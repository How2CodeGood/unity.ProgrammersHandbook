Larger Text,Smaller Text,Symbol
Preface ,NA,NA
Dedication,NA,NA
Chapter 1—Introduction to Neural Networks ,NA,NA
Neural Processing ,NA,NA
Neural Network ,NA,NA
Output of a Neuron ,NA,NA
Cash Register Game ,NA,NA
Weights ,NA,NA
Training ,NA,NA
Feedback ,NA,NA
Supervised or Unsupervised Learning ,NA,NA
Noise ,NA,NA
Memory ,NA,NA
Capsule of History ,NA,NA
Neural Network Construction ,NA,NA
Sample Applications ,NA,NA
Qualifying for a Mortgage ,NA,NA
Cooperation and Competition ,NA,NA
Example—A Feed-Forward Network ,NA,NA
Example—A Hopfield Network ,NA,NA
Hamming Distance ,NA,NA
Asynchronous Update ,NA,NA
Binary and Bipolar Inputs ,NA,NA
Bias ,NA,NA
Another Example for the Hopfield Network ,NA,NA
Summary,NA,NA
Chapter 2—C++ and Object Orientation ,NA,NA
Introduction to C++ ,NA,NA
Encapsulation,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (1 of 13) [21/11/02 21:56:36],NA
Data Hiding ,NA,NA
Constructors and Destructors as Special Functions of C++ ,NA,NA
Dynamic Memory Allocation ,NA,NA
Overloading ,NA,NA
Polymorphism and Polymorphic Functions ,NA,NA
Overloading Operators ,NA,NA
Inheritance ,NA,NA
Derived Classes ,NA,NA
Reuse of Code ,NA,NA
C++ Compilers ,NA,NA
Writing C++ Programs ,NA,NA
Summary,NA,NA
Chapter 3—A Look at Fuzzy Logic ,NA,NA
Crisp or Fuzzy Logic?,NA,NA
Fuzzy Sets ,NA,NA
Fuzzy Set Operations ,NA,NA
Union of Fuzzy Sets ,NA,NA
Intersection and Complement of Two Fuzzy Sets ,NA,NA
Applications of Fuzzy Logic ,NA,NA
Examples of Fuzzy Logic ,NA,NA
Commercial Applications ,NA,NA
Fuzziness in Neural Networks ,NA,NA
Code for the Fuzzifier ,NA,NA
Fuzzy Control Systems ,NA,NA
Fuzziness in Neural Networks ,NA,NA
Neural-Trained Fuzzy Systems ,NA,NA
Summary,NA,NA
Chapter 4—Constructing a Neural Network ,NA,NA
First Example for C++ Implementation ,NA,NA
Classes in C++ Implementation ,NA,NA
C++ Program for a Hopfield Network ,NA,NA
Header File for C++ Program for Hopfield Network ,NA,NA
Notes on the Header File Hop.h ,NA,NA
Source Code for the Hopfield Network,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (2 of 13) [21/11/02 21:56:36],NA
Comments on the C++ Program for Hopfield Network ,NA,NA
Output from the C++ Program for Hopfield Network ,NA,NA
Further Comments on the Program and Its Output A ,NA,NA
New Weight Matrix to Recall More Patterns ,NA,NA
Weight Determination ,NA,NA
Binary to Bipolar Mapping ,NA,NA
Pattern’s Contribution to Weight ,NA,NA
Autoassociative Network ,NA,NA
Orthogonal Bit Patterns ,NA,NA
Network Nodes and Input Patterns ,NA,NA
Second Example for C++ Implementation ,NA,NA
C++ Implementation of Perceptron Network ,NA,NA
Header File ,NA,NA
Implementation of Functions ,NA,NA
Source Code for Perceptron Network ,NA,NA
Comments on Your C++ Program ,NA,NA
Input/Output for percept.cpp ,NA,NA
Network Modeling ,NA,NA
Tic-Tac-Toe Anyone?,NA,NA
Stability and Plasticity ,NA,NA
Stability for a Neural Network ,NA,NA
Plasticity for a Neural Network ,NA,NA
Short-Term Memory and Long-Term Memory ,NA,NA
Summary,NA,NA
Chapter 5—A Survey of Neural Network Models ,NA,NA
Neural Network Models ,NA,NA
Layers in a Neural Network ,NA,NA
Single-Layer Network ,NA,NA
XOR Function and the Perceptron ,NA,NA
Linear Separability ,NA,NA
A Second Look at the XOR Function: Multilayer Perceptron ,NA,NA
Example of the Cube Revisited ,NA,NA
Strategy ,NA,NA
Details ,NA,NA
Performance of the Perceptron,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (3 of 13) [21/11/02 21:56:36],NA
Other Two-layer Networks ,NA,NA
Many Layer Networks ,NA,NA
Connections Between Layers ,NA,NA
Instar and Outstar ,NA,NA
Weights on Connections ,NA,NA
Initialization of Weights ,NA,NA
A Small Example ,NA,NA
Initializing Weights for Autoassociative Networks ,NA,NA
Weight Initialization for Heteroassociative Networks ,NA,NA
"On Center, Off Surround ",NA,NA
Inputs ,NA,NA
Outputs ,NA,NA
The Threshold Function ,NA,NA
The Sigmoid Function ,NA,NA
The Step Function ,NA,NA
The Ramp Function ,NA,NA
Linear Function ,NA,NA
Applications ,NA,NA
Some Neural Network Models ,NA,NA
Adaline and Madaline ,NA,NA
Backpropagation ,NA,NA
Figure for Backpropagation Network ,NA,NA
Bidirectional Associative Memory ,NA,NA
Temporal Associative Memory ,NA,NA
Brain-State-in-a-Box ,NA,NA
Counterpropagation ,NA,NA
Neocognitron ,NA,NA
Adaptive Resonance Theory ,NA,NA
Summary,NA,NA
Chapter 6—Learning and Training ,NA,NA
Objective of Learning ,NA,NA
Learning and Training ,NA,NA
Hebb’s Rule ,NA,NA
Delta Rule ,NA,NA
Supervised Learning,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (4 of 13) [21/11/02 21:56:36],NA
Generalized Delta Rule ,NA,NA
Statistical Training and Simulated Annealing ,NA,NA
Radial Basis-Function Networks ,NA,NA
Unsupervised Networks ,NA,NA
Self-Organization ,NA,NA
Learning Vector Quantizer ,NA,NA
Associative Memory Models and One-Shot Learning ,NA,NA
Learning and Resonance ,NA,NA
Learning and Stability ,NA,NA
Training and Convergence ,NA,NA
Lyapunov Function ,NA,NA
Other Training Issues ,NA,NA
Adaptation ,NA,NA
Generalization Ability ,NA,NA
Summary,NA,NA
Chapter 7—Backpropagation ,NA,NA
Feedforward Backpropagation Network ,NA,NA
Mapping ,NA,NA
Layout ,NA,NA
Training ,NA,NA
Illustration: Adjustment of Weights of Connections from a Neuron in ,NA,NA
the Hidden Layer ,NA,NA
Illustration: Adjustment of Weights of Connections from a Neuron in ,NA,NA
the Input Layer ,NA,NA
Adjustments to Threshold Values or Biases ,NA,NA
Another Example of Backpropagation Calculations ,NA,NA
Notation and Equations ,NA,NA
Notation ,NA,NA
Equations ,NA,NA
C++ Implementation of a Backpropagation Simulator ,NA,NA
A Brief Tour of How to Use the Simulator ,NA,NA
C++ Classes and Class Hierarchy ,NA,NA
Summary,NA,NA
Chapter 8—BAM: Bidirectional Associative Memory,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (5 of 13) [21/11/02 21:56:36],NA
Introduction ,NA,NA
Inputs and Outputs ,NA,NA
Weights and Training ,NA,NA
Example ,NA,NA
Recall of Vectors ,NA,NA
Continuation of Example ,NA,NA
Special Case—Complements ,NA,NA
C++ Implementation ,NA,NA
Program Details and Flow ,NA,NA
Program Example for BAM ,NA,NA
Header File ,NA,NA
Source File ,NA,NA
Program Output ,NA,NA
Additional Issues ,NA,NA
Unipolar Binary Bidirectional Associative Memory ,NA,NA
Summary,NA,NA
Chapter 9—FAM: Fuzzy Associative Memory ,NA,NA
Introduction ,NA,NA
Association ,NA,NA
FAM Neural Network ,NA,NA
Encoding ,NA,NA
Example of Encoding ,NA,NA
Recall ,NA,NA
C++ Implementation ,NA,NA
Program details ,NA,NA
Header File ,NA,NA
Source File ,NA,NA
Output ,NA,NA
Summary,NA,NA
Chapter 10—Adaptive Resonance Theory (ART) ,NA,NA
Introduction ,NA,NA
The Network for ART1 ,NA,NA
A Simplified Diagram of Network Layout ,NA,NA
Processing in ART1,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (6 of 13) [21/11/02 21:56:36],NA
Special Features of the ART1 Model ,NA,NA
Notation for ART1 Calculations ,NA,NA
Algorithm for ART1 Calculations ,NA,NA
Initialization of Parameters ,NA,NA
Equations for ART1 Computations ,NA,NA
Other Models ,NA,NA
C++ Implementation ,NA,NA
A Header File for the C++ Program for the ART1 Model ,NA,NA
Network ,NA,NA
A Source File for C++ Program for an ART1 Model Network ,NA,NA
Program Output ,NA,NA
Summary,NA,NA
Chapter 11—The Kohonen Self-Organizing Map ,NA,NA
Introduction ,NA,NA
Competitive Learning ,NA,NA
Normalization of a Vector ,NA,NA
Lateral Inhibition ,NA,NA
The Mexican Hat Function ,NA,NA
Training Law for the Kohonen Map ,NA,NA
Significance of the Training Law ,NA,NA
The Neighborhood Size and Alpha ,NA,NA
C++ Code for Implementing a Kohonen Map ,NA,NA
The Kohonen Network ,NA,NA
Modeling Lateral Inhibition and Excitation ,NA,NA
Classes to be Used ,NA,NA
Revisiting the Layer Class ,NA,NA
A New Layer Class for a Kohonen Layer ,NA,NA
Implementation of the Kohonen Layer and Kohonen ,NA,NA
Network ,NA,NA
Flow of the Program and the main() Function ,NA,NA
Flow of the Program ,NA,NA
Results from Running the Kohonen Program ,NA,NA
A Simple First Example ,NA,NA
Orthogonal Input Vectors Example ,NA,NA
Variations and Applications of Kohonen Networks ,NA,NA
Using a Conscience,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (7 of 13) [21/11/02 21:56:36],NA
LVQ: Learning Vector Quantizer ,NA,NA
Counterpropagation Network ,NA,NA
Application to Speech Recognition ,NA,NA
Summary,NA,NA
Chapter 12—Application to Pattern Recognition ,NA,NA
Using the Kohonen Feature Map ,NA,NA
An Example Problem: Character Recognition ,NA,NA
C++ Code Development ,NA,NA
Changes to the Kohonen Program ,NA,NA
Testing the Program ,NA,NA
Generalization versus Memorization ,NA,NA
Adding Characters ,NA,NA
Other Experiments to Try ,NA,NA
Summary,NA,NA
Chapter 13—Backpropagation II ,NA,NA
Enhancing the Simulator ,NA,NA
Another Example of Using Backpropagation ,NA,NA
Adding the Momentum Term ,NA,NA
Code Changes ,NA,NA
Adding Noise During Training ,NA,NA
One Other Change—Starting Training from a Saved Weight File ,NA,NA
Trying the Noise and Momentum Features ,NA,NA
Variations of the Backpropagation Algorithm ,NA,NA
Applications ,NA,NA
Summary,NA,NA
Chapter 14—Application to Financial Forecasting ,NA,NA
Introduction ,NA,NA
Who Trades with Neural Networks?,NA,NA
Developing a Forecasting Model ,NA,NA
The Target and the Timeframe ,NA,NA
Domain Expertise ,NA,NA
Gather the Data ,NA,NA
Pre processing the Data for the Network,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (8 of 13) [21/11/02 21:56:36],NA
Reduce Dimensionality ,NA,NA
Eliminate Correlated Inputs Where Possible ,NA,NA
Design a Network Architecture ,NA,NA
The Train/Test/Redesign Loop ,NA,NA
Forecasting the S&P 500 ,NA,NA
Choosing the Right Outputs and Objective ,NA,NA
Choosing the Right Inputs ,NA,NA
Choosing a Network Architecture ,NA,NA
Preprocessing Data ,NA,NA
A View of the Raw Data ,NA,NA
Highlight Features in the Data ,NA,NA
Normalizing the Range ,NA,NA
The Target ,NA,NA
Storing Data in Different Files ,NA,NA
Training and Testing ,NA,NA
Using the Simulator to Calculate Error ,NA,NA
Only the Beginning ,NA,NA
What’s Next?,NA,NA
Technical Analysis and Neural Network Preprocessing ,NA,NA
Moving Averages ,NA,NA
Momentum and Rate of Change ,NA,NA
Relative Strength Index ,NA,NA
Percentage R ,NA,NA
Herrick Payoff Index ,NA,NA
MACD,NA,NA
“Stochastics”,NA,NA
On-Balance Volume ,NA,NA
Accumulation-Distribution ,NA,NA
What Others Have Reported ,NA,NA
Can a Three-Year-Old Trade Commodities?,NA,NA
Forecasting Treasury Bill and Treasury Note Yields ,NA,NA
Neural Nets versus Box-Jenkins Time-Series Forecasting ,NA,NA
Neural Nets versus Regression Analysis ,NA,NA
Hierarchical Neural Network ,NA,NA
The Walk-Forward Methodology of Market Prediction ,NA,NA
Dual Confirmation Trading System,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (9 of 13) [21/11/02 21:56:36],NA
A Turning Point Predictor ,NA,NA
The S&P 500 and Sunspot Predictions ,NA,NA
A Critique of Neural Network Time-Series Forecasting for ,NA,NA
Trading ,NA,NA
Resource Guide for Neural Networks and Fuzzy Logic in Finance ,NA,NA
Magazines ,NA,NA
Books ,NA,NA
Book Vendors ,NA,NA
Consultants ,NA,NA
Historical Financial Data Vendors ,NA,NA
Preprocessing Tools for Neural Network Development ,NA,NA
Genetic Algorithms Tool Vendors ,NA,NA
Fuzzy Logic Tool Vendors ,NA,NA
Neural Network Development Tool Vendors ,NA,NA
Summary,NA,NA
Chapter 15—Application to Nonlinear Optimization ,NA,NA
Introduction ,NA,NA
Neural Networks for Optimization Problems ,NA,NA
Traveling Salesperson Problem ,NA,NA
The TSP in a Nutshell ,NA,NA
Solution via Neural Network ,NA,NA
Example of a Traveling Salesperson Problem for Hand ,NA,NA
Calculation ,NA,NA
Neural Network for Traveling Salesperson Problem ,NA,NA
Network Choice and Layout ,NA,NA
Inputs ,NA,NA
"Activations, Outputs, and Their Updating ",NA,NA
Performance of the Hopfield Network ,NA,NA
C++ Implementation of the Hopfield Network for the Traveling ,NA,NA
Salesperson Problem ,NA,NA
Source File for Hopfield Network for Traveling Salesperson ,NA,NA
Problem ,NA,NA
Output from Your C++ Program for the Traveling Salesperson ,NA,NA
Problem ,NA,NA
Other Approaches to Solve the Traveling Salesperson Problem,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (10 of 13) [21/11/02 21:56:36],NA
Optimizing a Stock Portfolio ,NA,NA
Tabu Neural Network ,NA,NA
Summary,NA,NA
Chapter 16—Applications of Fuzzy Logic ,NA,NA
Introduction ,NA,NA
A Fuzzy Universe of Applications ,NA,NA
Section I: A Look at Fuzzy Databases and Quantification ,NA,NA
Databases and Queries ,NA,NA
Relations in Databases ,NA,NA
Fuzzy Scenarios ,NA,NA
Fuzzy Sets Revisited ,NA,NA
Fuzzy Relations ,NA,NA
Matrix Representation of a Fuzzy Relation ,NA,NA
Properties of Fuzzy Relations ,NA,NA
Similarity Relations ,NA,NA
Resemblance Relations ,NA,NA
Fuzzy Partial Order ,NA,NA
Fuzzy Queries ,NA,NA
Extending Database Models ,NA,NA
Example ,NA,NA
Possibility Distributions ,NA,NA
Example ,NA,NA
Queries ,NA,NA
"Fuzzy Events, Means and Variances ",NA,NA
Example: XYZ Company Takeover Price ,NA,NA
Probability of a Fuzzy Event ,NA,NA
Fuzzy Mean and Fuzzy Variance ,NA,NA
Conditional Probability of a Fuzzy Event ,NA,NA
Conditional Fuzzy Mean and Fuzzy Variance ,NA,NA
Linear Regression a la Possibilities ,NA,NA
Fuzzy Numbers ,NA,NA
Triangular Fuzzy Number ,NA,NA
Linear Possibility Regression Model ,NA,NA
Section II: Fuzzy Control ,NA,NA
Designing a Fuzzy Logic Controller,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (11 of 13) [21/11/02 21:56:36],NA
Step One: Defining Inputs and Outputs for the FLC ,NA,NA
Step ,NA,NA
Two: Fuzzify the Inputs ,NA,NA
Step Three: Set Up Fuzzy Membership Functions for the ,NA,NA
Output(s) ,NA,NA
Step Four: Create a Fuzzy Rule Base ,NA,NA
Step Five: Defuzzify the Outputs ,NA,NA
Advantages and Disadvantages of Fuzzy Logic Controllers ,NA,NA
Summary,NA,NA
Chapter 17—Further Applications ,NA,NA
Introduction ,NA,NA
Computer Virus Detector ,NA,NA
Mobile Robot Navigation ,NA,NA
A Classifier ,NA,NA
A Two-Stage Network for Radar Pattern Classification ,NA,NA
Crisp and Fuzzy Neural Networks for Handwritten Character ,NA,NA
Recognition ,NA,NA
Noise Removal with a Discrete Hopfield Network ,NA,NA
Object Identification by Shape ,NA,NA
Detecting Skin Cancer ,NA,NA
EEG Diagnosis ,NA,NA
Time Series Prediction with Recurrent and Nonrecurrent Networks ,NA,NA
Security Alarms ,NA,NA
Circuit Board Faults ,NA,NA
Warranty Claims ,NA,NA
Writing Style Recognition ,NA,NA
Commercial Optical Character Recognition ,NA,NA
ART-EMAP and Object Recognition ,NA,NA
Summary,NA,NA
References ,NA,NA
Appendix A ,NA,NA
Appendix B ,NA,NA
Glossary ,NA,NA
Index,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...ic/C++_Neural_Networks_and_Fuzzy_Logic/ewtoc.html (12 of 13) [21/11/02 21:56:36],NA
Table of Contents,NA,NA
Preface,NA,NA
The number of models available in neural network literature is quite large. Very often the ,NA,NA
treatment is mathematical and complex. This book provides illustrative examples in C++ ,NA,NA
that the reader can use as a basis for further experimentation. A key to learning about ,NA,NA
"neural networks to appreciate their inner workings is to experiment. Neural networks, in ",NA,NA
"the end, are fun to learn about and discover. Although the language for description used is ",NA,NA
"C++, you will not find extensive class libraries in this book. With the exception of the ",NA,NA
"backpropagation simulator, you will find fairly simple example programs for many ",NA,NA
different neural network architectures and paradigms. Since backpropagation is widely ,NA,NA
"used and also easy to tame, a simulator is provided with the capacity to handle large input ",NA,NA
data sets. You use the simulator in one of the chapters in this book to solve a financial ,NA,NA
forecasting problem. You will find ample room to expand and experiment with the code ,NA,NA
presented in this book. ,NA,NA
There are many different angles to neural networks and fuzzy logic. The fields are ,NA,NA
expanding rapidly with ever-new results and applications. This book presents many of the ,NA,NA
"different neural network topologies, including the BAM, the Perceptron, Hopfield memory, ",NA,NA
"ART1, Kohonen’s Self-Organizing map, Kosko’s Fuzzy Associative memory, and, of ",NA,NA
"course, the Feedforward Backpropagation network (aka Multilayer Perceptron). You ",NA,NA
should get a fairly broad picture of neural networks and fuzzy logic with this book. At the ,NA,NA
"same time, you will have real code that shows you example usage of the models, to solidify ",NA,NA
your understanding. This is especially useful for the more complicated neural network ,NA,NA
architectures like the Adaptive Resonance Theory of Stephen Grossberg (ART).,NA,NA
The subjects are covered as follows:,NA,NA
•,NA,NA
  Chapter 1 gives you an overview of neural network terminology and ,NA,NA
nomenclature. You discover that neural nets are capable of solving complex ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...gic/C++_Neural_Networks_and_Fuzzy_Logic/about.html (1 of 4) [21/11/02 21:56:37],NA
problems with parallel computational architectures. The Hopfield network and ,NA,NA
feedforward network are introduced in this chapter. ,NA,NA
•,NA,NA
  Chapter 2 introduces C++ and object orientation. You learn the benefits of object-,NA,NA
oriented programming and its basic concepts. ,NA,NA
•,NA,NA
"  Chapter 3 introduces fuzzy logic, a technology that is fairly synergistic with ",NA,NA
neural network problem solving. You learn about math with fuzzy sets as well as ,NA,NA
how you can build a simple fuzzifier in C++. ,NA,NA
•,NA,NA
"  Chapter 4 introduces you to two of the simplest, yet very representative, models ",NA,NA
"of: the Hopfield network, the Perceptron network, and their C++ implementations. ",NA,NA
•,NA,NA
  Chapter 5 is a survey of neural network models. This chapter describes the ,NA,NA
"features of several models, describes ",NA,NA
threshold,NA,NA
" functions, and develops concepts in ",NA,NA
neural networks. ,NA,NA
•,NA,NA
  Chapter 6 focuses on learning and training paradigms. It introduces the concepts ,NA,NA
"of supervised and unsupervised learning, self-organization and topics including ",NA,NA
"backpropagation of errors, radial basis function networks, and conjugate gradient ",NA,NA
methods. ,NA,NA
•,NA,NA
  Chapter 7 goes through the construction of a backpropagation simulator. You will ,NA,NA
find this simulator useful in later chapters also. C++ classes and features are detailed ,NA,NA
in this chapter. ,NA,NA
•,NA,NA
  Chapter 8 covers the Bidirectional Associative memories for associating pairs of ,NA,NA
patterns. ,NA,NA
•,NA,NA
  Chapter 9 introduces Fuzzy Associative memories for associating pairs of fuzzy ,NA,NA
sets. ,NA,NA
•,NA,NA
  Chapter 10 covers the Adaptive Resonance Theory of Grossberg. You will have a ,NA,NA
chance to experiment with a program that illustrates the working of this theory. ,NA,NA
•,NA,NA
  ,NA,NA
Chapters 11 and 12 discuss the Self-Organizing map of Teuvo Kohonen and its ,NA,NA
application to pattern recognition. ,NA,NA
•,NA,NA
"  Chapter 13 continues the discussion of the backpropagation simulator, with ",NA,NA
enhancements made to the simulator to include momentum and noise during ,NA,NA
training. ,NA,NA
•,NA,NA
"  Chapter 14 applies backpropagation to the problem of financial forecasting, ",NA,NA
discusses setting up a backpropagation network with 15 input variables and 200 test ,NA,NA
cases to run a simulation. The problem is approached via a systematic 12-step ,NA,NA
approach for preprocessing data and setting up the problem. You will find a number ,NA,NA
of examples of financial forecasting highlighted from the literature. A resource guide ,NA,NA
for neural networks in finance is included for people who would like more ,NA,NA
information about this area. ,NA,NA
•,NA,NA
  Chapter 15 deals with nonlinear optimization with a thorough discussion of the ,NA,NA
Traveling Salesperson problem. You learn the formulation by Hopfield and the ,NA,NA
approach of Kohonen. ,NA,NA
•,NA,NA
  Chapter 16 treats two application areas of fuzzy logic: fuzzy control systems and ,NA,NA
fuzzy databases. This chapter also expands on fuzzy relations and fuzzy set theory ,NA,NA
with several examples. ,NA,NA
•,NA,NA
  Chapter 17 discusses some of the latest applications using neural networks and ,NA,NA
fuzzy logic. ,NA,NA
"In this second edition, we have followed readers’ suggestions and included more ",NA,NA
"explanations and material, as well as updated the material with the latest information and ",NA,NA
research. We have also corrected errors and omissions from the first edition. ,NA,NA
"Neural networks are now a subject of interest to professionals in many fields, and also a ",NA,NA
"tool for many areas of problem solving. The applications are widespread in recent years, ",NA,NA
and the fruits of these applications are being reaped by many from diverse fields. This ,NA,NA
methodology has become an alternative to modeling of some physical and nonphysical ,NA,NA
"systems with scientific or mathematical basis, and also to expert systems methodology. ",NA,NA
One of the reasons for it is that absence of full information is not as big a problem in neural ,NA,NA
networks as it is in the other methodologies mentioned earlier. The results are sometimes ,NA,NA
"astounding, even phenomenal, with neural networks, and the effort is at times relatively ",NA,NA
"modest to achieve such results. Image processing, vision, financial market analysis, and ",NA,NA
optimization are among the many areas of application of neural networks. To think that the ,NA,NA
modeling of neural networks is one of modeling a system that attempts to mimic human ,NA,NA
learning is somewhat exciting. Neural networks can learn in an unsupervised learning mode. ,NA,NA
"Just as human brains can be trained to master some situations, neural networks can be ",NA,NA
trained to recognize patterns and to do optimization and other tasks.,NA,NA
"In the early days of interest in neural networks, the researchers were mainly biologists and ",NA,NA
"psychologists. Serious research now is done by not only biologists and psychologists, but ",NA,NA
"by professionals from computer science, electrical engineering, computer engineering, ",NA,NA
"mathematics, and physics as well. The latter have either joined forces, or are doing ",NA,NA
"independent research parallel with the former, who opened up a new and promising field ",NA,NA
for everyone.,NA,NA
"In this book, we aim to introduce the subject of neural networks as directly and simply as ",NA,NA
possible for an easy understanding of the methodology. Most of the important neural ,NA,NA
"network architectures are covered, and we earnestly hope that our efforts have succeeded ",NA,NA
in presenting this subject matter in a clear and useful fashion.,NA,NA
"We welcome your comments and suggestions for this book, from errors and oversights, to ",NA,NA
suggestions for improvements to future printings at the following E-mail addresses:,NA,NA
V. Rao,NA,NA
 rao@cse.bridgeport.edu,NA,NA
H. Rao,NA,NA
 ViaSW@aol.com,NA,NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...gic/C++_Neural_Networks_and_Fuzzy_Logic/about.html (4 of 4) [21/11/02 21:56:37]",NA
Table of Contents,NA,NA
Dedication,NA,NA
To the memory of ,NA,NA
"Vydehamma, Annapurnamma, Anandarao, Madanagopalarao, Govindarao, and ",NA,NA
Rajyalakshamma.,NA,NA
Acknowledgments,NA,NA
We thank everyone at MIS:Press/Henry Holt and Co. who has been associated with this ,NA,NA
"project for their diligence and support, namely, the Technical Editors of this edition and ",NA,NA
"the first edition for their suggestions and feedback; Laura Lewin, the Editor, and all of the ",NA,NA
other people at MIS:Press for making the book a reality. ,NA,NA
"We would also like to thank Dr. Tarek Kaylani for his helpful suggestions, Professor R. ",NA,NA
"Haskell, and our other readers who wrote to us, and Dr. V. Rao’s students whose ",NA,NA
suggestions were helpful. Please E-mail us more feedback!,NA,NA
"Finally, thanks to Sarada and Rekha for encouragement and support. Most of all, thanks to ",NA,NA
Rohini and Pranav for their patience and understanding through many lost evenings and ,NA,NA
weekends.,NA,NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathematic.../C++_Neural_Networks_and_Fuzzy_Logic/about_author.html [21/11/02 21:56:40]",NA
Previous Table of Contents Next,NA,NA
Chapter 1 ,NA,NA
Introduction to Neural Networks ,NA,NA
Neural Processing,NA,NA
How do you recognize a face in a crowd? How does an economist predict the direction of ,NA,NA
"interest rates? Faced with problems like these, the human brain uses a web of ",NA,NA
interconnected processing elements called ,NA,NA
neurons,NA,NA
 to process information. Each neuron is ,NA,NA
autonomous and independent; it does its work ,NA,NA
asynchronously,NA,NA
", that is, without any ",NA,NA
"synchronization to other events taking place. The two problems posed, namely recognizing ",NA,NA
"a face and forecasting interest rates, have two important characteristics that distinguish ",NA,NA
"them from other problems: First, the problems are complex, that is, you can’t devise a ",NA,NA
simple,NA,NA
 step-by-step ,NA,NA
algorithm,NA,NA
" or precise formula to give you an answer; and second, the ",NA,NA
data provided to resolve the problems is equally complex and may be noisy or incomplete. ,NA,NA
You could have forgotten your glasses when you’re trying to recognize that face. The ,NA,NA
economist may have at his or her disposal thousands of pieces of data that may or may not ,NA,NA
be relevant to his or her forecast on the economy and on interest rates.,NA,NA
The vast processing power inherent in biological neural structures has inspired the study of ,NA,NA
the structure itself for hints on organizing human-made computing structures. ,NA,NA
Artificial ,NA,NA
neural networks,NA,NA
", the subject of this book, covers the way to organize synthetic neurons to ",NA,NA
"solve the same kind of difficult, complex problems in a similar manner as we think the ",NA,NA
human brain may. This chapter will give you a sampling of the terms and nomenclature ,NA,NA
used to talk about neural networks. These terms will be covered in more depth in the ,NA,NA
chapters to follow.,NA,NA
Neural Network,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/001-003.html (1 of 3) [21/11/02 21:56:42],NA
A ,NA,NA
neural network,NA,NA
 is a computational structure inspired by the study of biological neural ,NA,NA
"processing. There are many different types of neural networks, from relatively simple to ",NA,NA
"very complex, just as there are many theories on how biological neural processing works. ",NA,NA
We will begin with a discussion of a ,NA,NA
layered feed-forward,NA,NA
 type of neural network and ,NA,NA
branch out to other paradigms later in this chapter and in other chapters.,NA,NA
A layered feed-forward neural network has ,NA,NA
"layers,",NA,NA
 or subgroups of processing elements. A ,NA,NA
layer of processing elements makes independent computations on data that it receives and ,NA,NA
passes the results to another layer. The next layer may in turn make its independent ,NA,NA
"computations and pass on the results to yet another layer. Finally, a subgroup of one or ",NA,NA
more processing elements determines the output from the network. Each processing ,NA,NA
element makes its computation based upon a weighted sum of its inputs. The first layer is ,NA,NA
the ,NA,NA
input layer,NA,NA
 and the last the ,NA,NA
output layer,NA,NA
. The layers that are placed between the first and ,NA,NA
the last layers are the ,NA,NA
hidden layers,NA,NA
. The processing elements are seen as units that are ,NA,NA
"similar to the neurons in a human brain, and hence, they are referred to as ",NA,NA
cells,NA,NA
", ",NA,NA
neuromimes,NA,NA
", or ",NA,NA
artificial neurons,NA,NA
. A ,NA,NA
threshold function,NA,NA
 is sometimes used to qualify the ,NA,NA
output of a neuron in the output layer. Even though our subject matter deals with artificial ,NA,NA
"neurons, we will simply refer to them as neurons. Synapses between neurons are referred to ",NA,NA
as ,NA,NA
connections,NA,NA
", which are represented by edges of a directed graph in which the nodes are ",NA,NA
the artificial neurons.,NA,NA
Figure 1.1 is a layered feed-forward neural network. The circular nodes represent neurons. ,NA,NA
"Here there are three layers, an input layer, a hidden layer, and an output layer. The directed ",NA,NA
graph mentioned shows the connections from nodes from a given layer to other nodes in ,NA,NA
other layers. Throughout this book you will see many variations on the number and types of ,NA,NA
layers.,NA,NA
Figure 1.1,NA,NA
  A typical neural network.,NA,NA
Output of a Neuron,NA,NA
"Basically, the ",NA,NA
internal activation,NA,NA
 or raw output of a neuron in a neural network is a ,NA,NA
"weighted sum of its inputs, but a threshold function is also used to determine the final ",NA,NA
"value, or the output. When the output is 1, the neuron is said to ",NA,NA
fire,NA,NA
", and when it is 0, the ",NA,NA
"neuron is considered not to have fired. When a threshold function is used, different results ",NA,NA
"of activations, all in the same interval of values, can cause the same final output value. ",NA,NA
"This situation helps in the sense that, if precise input causes an activation of 9 and noisy ",NA,NA
"input causes an activation of 10, then the output works out the same as if noise is filtered ",NA,NA
out.,NA,NA
"To put the description of a neural network in a simple and familiar setting, let us describe ",NA,NA
"an example about a daytime game show on television, ",NA,NA
The Price is Right,NA,NA
.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/001-003.html (3 of 3) [21/11/02 21:56:42]",NA
Previous Table of Contents Next,NA,NA
Cash Register Game,NA,NA
A contestant in ,NA,NA
The Price is Right,NA,NA
 is sometimes asked to play the ,NA,NA
Cash Register Game,NA,NA
. A ,NA,NA
"few products are described, their prices are unknown to the contestant, and the contestant ",NA,NA
has to declare how many units of each item he or she would like to (pretend to) buy. If the ,NA,NA
"total purchase does not exceed the amount specified, the contestant wins a special prize. ",NA,NA
"After the contestant announces how many items of a particular product he or she wants, the ",NA,NA
"price of that product is revealed, and it is rung up on the cash register. The contestant must ",NA,NA
"be careful, in this case, that the total does not exceed some nominal value, to earn the ",NA,NA
"associated prize. We can now cast the whole operation of this game, in terms of a neural ",NA,NA
"network, called a ",NA,NA
Perceptron,NA,NA
", as follows.",NA,NA
"Consider each product on the shelf to be a neuron in the input layer, with its input being the ",NA,NA
unit price of that product. The cash register is the single neuron in the output layer. The ,NA,NA
only connections in the network are between each of the neurons (products displayed on the ,NA,NA
shelf) in the input layer and the output neuron (the cash register). This arrangement is ,NA,NA
"usually referred to as a neuron, the cash register in this case, being an ",NA,NA
instar,NA,NA
 in neural ,NA,NA
"network terminology. The contestant actually determines these connections, because when ",NA,NA
"the contestant says he or she wants, say five, of a specific product, the contestant is thereby ",NA,NA
assigning a weight of 5 to the connection between that product and the cash register. The ,NA,NA
total bill for the purchases by the contestant is nothing but the weighted sum of the unit ,NA,NA
prices of the different products offered. For those items the contestant does not choose to ,NA,NA
"purchase, the implicit weight assigned is 0. The application of the dollar limit to the bill is ",NA,NA
"just the application of a threshold, except that the threshold value should not be exceeded ",NA,NA
"for the outcome from this network to favor the contestant, winning him or her a good prize. ",NA,NA
"In a Perceptron, the way the threshold works is that an output neuron is supposed to fire if ",NA,NA
its activation value ,NA,NA
exceeds,NA,NA
 the threshold value.,NA,NA
Weights,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/003-005.html (1 of 3) [21/11/02 21:56:42],NA
The weights used on the connections between different layers have much significance in ,NA,NA
the working of the neural network and the characterization of a network. The following ,NA,NA
actions are possible in a neural network: ,NA,NA
1.,NA,NA
  Start with one set of weights and run the network. (NO TRAINING) ,NA,NA
2.,NA,NA
"  Start with one set of weights, run the network, and modify some or all the ",NA,NA
"weights, and run the network again with the new set of weights. Repeat this process ",NA,NA
until some predetermined goal is met. (TRAINING) ,NA,NA
Training,NA,NA
"Since the output(s) may not be what is expected, the weights may need to be altered. Some ",NA,NA
rule then needs to be used to determine how to alter the weights. There should also be a ,NA,NA
criterion to specify when the process of successive modification of weights ceases. This ,NA,NA
"process of changing the weights, or rather, updating the weights, is called ",NA,NA
training,NA,NA
. A ,NA,NA
network in which learning is employed is said to be subjected to ,NA,NA
training,NA,NA
. Training is an ,NA,NA
external process or regimen. Learning is the desired process that takes place internal to the ,NA,NA
network.,NA,NA
Feedback,NA,NA
"If you wish to train a network so it can recognize or identify some predetermined patterns, ",NA,NA
"or evaluate some function values for given arguments, it would be important to have ",NA,NA
"information fed back from the output neurons to neurons in some layer before that, to ",NA,NA
enable further processing and adjustment of weights on the connections. Such feedback can ,NA,NA
"be to the input layer or a layer between the input layer and the output layer, sometimes ",NA,NA
labeled the ,NA,NA
hidden layer,NA,NA
". What is fed back is usually the error in the output, modified ",NA,NA
appropriately according to some useful paradigm. The process of feedback continues ,NA,NA
through the subsequent cycles of operation of the neural network and ceases when the ,NA,NA
training is completed.,NA,NA
Supervised or Unsupervised Learning,NA,NA
A network can be subject to ,NA,NA
supervised,NA,NA
 or ,NA,NA
unsupervised,NA,NA
 learning. The learning would be ,NA,NA
"supervised if external criteria are used and matched by the network output, and if not, the ",NA,NA
learning is unsupervised. This is one broad way to divide different neural network ,NA,NA
approaches. Unsupervised approaches are also termed ,NA,NA
self-organizing,NA,NA
. There is more ,NA,NA
"interaction between neurons, typically with feedback and intralayer connections between ",NA,NA
neurons promoting self-organization.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/003-005.html (2 of 3) [21/11/02 21:56:42],NA
Supervised networks are a little more straightforward to conceptualize than unsupervised ,NA,NA
networks. You apply the inputs to the supervised network along with an expected ,NA,NA
"response, much like the Pavlovian conditioned stimulus and response regimen. You mold ",NA,NA
the network with stimulus-response pairs. A stock market forecaster may present economic ,NA,NA
data (the ,NA,NA
stimulus,NA,NA
) along with metrics of stock market performance (the ,NA,NA
response,NA,NA
) to the ,NA,NA
neural network to the present and attempt to predict the future once training is complete.,NA,NA
"You provide unsupervised networks with only stimulus. You may, for example, want an ",NA,NA
"unsupervised network to correctly classify parts from a conveyor belt into part numbers, ",NA,NA
providing an image of each part to do the classification (the stimulus). The unsupervised ,NA,NA
"network in this case would act like a look-up memory that is indexed by its contents, or a ",NA,NA
Content-Addressable-Memory (CAM),NA,NA
.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/003-005.html (3 of 3) [21/11/02 21:56:42]",NA
Previous Table of Contents Next,NA,NA
Noise,NA,NA
"Noise is perturbation, or a deviation from the actual. A data set used to train a neural ",NA,NA
"network may have inherent noise in it, or an image may have random speckles in it, for ",NA,NA
example. The response of the neural network to noise is an important factor in determining ,NA,NA
"its suitability to a given application. In the process of training, you may apply a metric to ",NA,NA
your neural network to see how well the network has learned your training data. In cases ,NA,NA
"where the metric stabilizes to some meaningful value, whether the value is acceptable to ",NA,NA
"you or not, you say that the network ",NA,NA
converges,NA,NA
. You may wish to introduce noise ,NA,NA
"intentionally in training to find out if the network can learn in the presence of noise, and if ",NA,NA
the network can converge on noisy data.,NA,NA
Memory,NA,NA
"Once you train a network on a set of data, suppose you continue training the network with ",NA,NA
new data. Will the network forget the intended training on the original set or will it ,NA,NA
remember? This is another angle that is approached by some researchers who are ,NA,NA
interested in preserving a network’s ,NA,NA
long-term memory(LTM,NA,NA
) as well as its ,NA,NA
short-term ,NA,NA
memory (STM),NA,NA
. Long-term memory is memory associated with learning that persists for the ,NA,NA
long term. Short-term memory is memory associated with a neural network that decays in ,NA,NA
some time interval.,NA,NA
Capsule of History,NA,NA
You marvel at the capabilities of the human brain and find its ways of processing ,NA,NA
information unknown to a large extent. You find it awesome that very complex situations ,NA,NA
are discerned at a far greater speed than what a computer can do. ,NA,NA
"Warren McCulloch and Walter Pitts formulated in 1943 a model for a nerve cell, a neuron, ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/005-007.html (1 of 3) [21/11/02 21:56:43],NA
"during their attempt to build a theory of self-organizing systems. Later, Frank Rosenblatt ",NA,NA
"constructed a Perceptron, an arrangement of processing elements representing the nerve ",NA,NA
cells into a network. His network could recognize simple shapes. It was the advent of ,NA,NA
different models for different applications.,NA,NA
Those working in the field of artificial intelligence (AI) tried to hypothesize that you can ,NA,NA
model thought processes using some symbols and some rules with which you can ,NA,NA
transform the symbols.,NA,NA
A limitation to the symbolic approach is related to how knowledge is representable. A piece ,NA,NA
"of information is localized, that is, available at one location, perhaps. It is not distributed ",NA,NA
over many locations. You can easily see that distributed knowledge leads to a faster and ,NA,NA
greater inferential process. Information is less prone to be damaged or lost when it is ,NA,NA
distributed than when it is localized. Distributed information processing can be fault ,NA,NA
"tolerant to some degree, because there are multiple sources of knowledge to apply to a ",NA,NA
"given problem. Even if one source is cut off or destroyed, other sources may still permit ",NA,NA
"solution to a problem. Further, with subsequent learning, a solution may be remapped into a ",NA,NA
new organization of distributed processing elements that exclude a faulty processing ,NA,NA
element.,NA,NA
"In neural networks, information may impact the activity of more than one neuron. ",NA,NA
Knowledge is distributed and lends itself easily to parallel computation. Indeed there are ,NA,NA
many research activities in the field of hardware design of neural network processing ,NA,NA
"engines that exploit the parallelism of the neural network paradigm. Carver Mead, a ",NA,NA
"pioneer in the field, has suggested analog VLSI (very large scale integration) circuit ",NA,NA
implementations of neural networks.,NA,NA
Neural Network Construction,NA,NA
There are three aspects to the construction of a neural network: ,NA,NA
1.Structure,NA,NA
—the architecture and topology of the neural network ,NA,NA
2.Encoding,NA,NA
—the method of changing weights ,NA,NA
3.Recall,NA,NA
—the method and capacity to retrieve information ,NA,NA
Let’s cover the first one—,NA,NA
structure,NA,NA
. This relates to how many layers the network should ,NA,NA
"contain, and what their functions are, such as for input, for output, or for feature extraction. ",NA,NA
Structure also encompasses how interconnections are made between neurons in the ,NA,NA
"network, and what their functions are.",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/005-007.html (2 of 3) [21/11/02 21:56:43],NA
The second aspect is ,NA,NA
encoding,NA,NA
. Encoding refers to the paradigm used for the determination ,NA,NA
of and changing of weights on the connections between neurons. In the case of the ,NA,NA
"multilayer feed-forward neural network, you initially can define weights by randomization. ",NA,NA
"Subsequently, in the process of training, you can use the ",NA,NA
backpropagation algorithm,NA,NA
", which ",NA,NA
is a means of updating weights starting from the output backwards. When you have ,NA,NA
"finished training the multilayer feed-forward neural network, you are finished with ",NA,NA
encoding since weights do not change after training is completed.,NA,NA
"Finally, ",NA,NA
recall,NA,NA
 is also an important aspect of a neural network. Recall refers to getting an ,NA,NA
"expected output for a given input. If the same input as before is presented to the network, ",NA,NA
the same corresponding output as before should result. The type of recall can characterize ,NA,NA
the network as being ,NA,NA
autoassociative,NA,NA
 or ,NA,NA
heteroassociative,NA,NA
. Autoassociation is the ,NA,NA
"phenomenon of associating an input vector with itself as the output, whereas ",NA,NA
heteroassociation is that of recalling a related vector given an input vector. You have a ,NA,NA
"fuzzy remembrance of a phone number. Luckily, you stored it in an autoassociative neural ",NA,NA
"network. When you apply the fuzzy remembrance, you retrieve the actual phone number. ",NA,NA
This is a use of autoassociation. Now if you want the individual’s name associated with a ,NA,NA
"given phone number, that would require heteroassociation. Recall is closely related to the ",NA,NA
concepts of STM and LTM introduced earlier.,NA,NA
The three aspects to the construction of a neural network mentioned above essentially ,NA,NA
distinguish between different neural networks and are part of their design process.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/005-007.html (3 of 3) [21/11/02 21:56:43]",NA
Previous Table of Contents Next,NA,NA
Sample Applications,NA,NA
One application for a neural network is ,NA,NA
"pattern classification,",NA,NA
 or ,NA,NA
pattern matching,NA,NA
. The ,NA,NA
"patterns can be represented by binary digits in the discrete cases, or real numbers ",NA,NA
representing analog signals in continuous cases. Pattern classification is a form of ,NA,NA
establishing an autoassociation or heteroassociation. Recall that associating different ,NA,NA
patterns is building the type of association called heteroassociation. If you input a ,NA,NA
"corrupted or modified pattern A to the neural network, and receive the true pattern A, this is ",NA,NA
termed autoassociation. What use does this provide? Remember the example given at the ,NA,NA
"beginning of this chapter. In the human brain example, say you want to recall a face in a ",NA,NA
crowd and you have a hazy remembrance (input). What you want is the actual image. ,NA,NA
"Autoassociation, then, is useful in recognizing or retrieving patterns with possibly ",NA,NA
incomplete information as input. What about heteroassociation? Here you associate A with ,NA,NA
"B. Given A, you get B and sometimes vice versa. You could store the face of a person and ",NA,NA
"retrieve it with the person’s name, for example. It’s quite common in real circumstances to ",NA,NA
"do the opposite, and sometimes not so well. You recall the face of a person, but can’t place ",NA,NA
the name.,NA,NA
Qualifying for a Mortgage,NA,NA
"Another sample application, which is in fact in the works by a U.S. government agency, is ",NA,NA
to devise a neural network to produce a quick response credit rating of an individual trying ,NA,NA
to qualify for a mortgage. The problem to date with the application process for a mortgage ,NA,NA
has been the staggering amount of paperwork and filing details required for each ,NA,NA
"application. Once information is gathered, the response time for knowing whether or not ",NA,NA
your mortgage is approved has typically taken several weeks. All of this will change. The ,NA,NA
proposed neural network system will allow the complete application and approval process ,NA,NA
"to take three hours, with approval coming in five minutes of entering all of the information ",NA,NA
"required. You enter in the applicant’s employment history, salary information, credit ",NA,NA
"information, and other factors and apply these to a trained neural network. The neural ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/008-009.html (1 of 3) [21/11/02 21:56:44],NA
"network, based on prior training on thousands of case histories, looks for patterns in the ",NA,NA
applicant’s profile and then produces a yes or no rating of worthiness to carry a particular ,NA,NA
mortgage. Let’s now continue our discussion of factors that distinguish neural network ,NA,NA
models from each other. ,NA,NA
Cooperation and Competition,NA,NA
We will now discuss ,NA,NA
cooperation,NA,NA
 and ,NA,NA
competition,NA,NA
. Again we start with an example feed ,NA,NA
forward neural network. If the network consists of a single input layer and an output layer ,NA,NA
"consisting of a single neuron, then the set of weights for the connections between the input ",NA,NA
layer neurons and the output neuron are given in a ,NA,NA
weight vector,NA,NA
. For three inputs and one ,NA,NA
"output, this could be ",NA,NA
W,NA,NA
 = {w,1,NA
", w",2,NA
", w",3,NA
" }. When the output layer has more than one neuron, ",NA,NA
the output is not just one value but is also a vector. In such a situation each neuron in one ,NA,NA
"layer is connected to each neuron in the next layer, with weights assigned to these ",NA,NA
interconnections. Then the weights can all be given together in a two-dimensional ,NA,NA
weight ,NA,NA
matrix,NA,NA
", which is also sometimes called a ",NA,NA
correlation matrix.,NA,NA
 When there are in-between ,NA,NA
"layers such as a hidden layer or a so-called Kohonen layer or a Grossberg layer, the ",NA,NA
interconnections are made between each neuron in one layer and every neuron in the next ,NA,NA
"layer, and there will be a corresponding correlation matrix. ",NA,NA
Cooperation,NA,NA
 or ,NA,NA
competition,NA,NA
 or ,NA,NA
"both can be imparted between network neurons in the same layer, through the choice of the ",NA,NA
right sign of weights for the connections. Cooperation is the attempt between neurons in ,NA,NA
one neuron aiding the prospect of firing by another. Competition is the attempt between ,NA,NA
neurons to individually excel with higher output. ,NA,NA
Inhibition,NA,NA
", a mechanism used in ",NA,NA
"competition, is the attempt between neurons in one neuron decreasing the prospect of ",NA,NA
"another neuron’s firing. As already stated, the vehicle for these phenomena is the ",NA,NA
"connection weight. For example, a positive weight is assigned for a connection between ",NA,NA
"one node and a cooperating node in that layer, while a negative weight is assigned to inhibit ",NA,NA
a competitor.,NA,NA
"To take this idea to the connections between neurons in consecutive layers, we would ",NA,NA
assign a positive weight to the connection between one node in one layer and its nearest ,NA,NA
"neighbor node in the next layer, whereas the connections with distant nodes in the other ",NA,NA
layer will get negative weights. The negative weights would indicate competition in some ,NA,NA
cases and inhibition in others. To make at least some of the discussion and the concepts a ,NA,NA
"bit clearer, we preview two example neural networks (there will be more discussion of ",NA,NA
these networks in the chapters that follow): the feed-forward network and the Hopfield ,NA,NA
network.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/008-009.html (2 of 3) [21/11/02 21:56:44],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/008-009.html (3 of 3) [21/11/02 21:56:44]",NA
Previous Table of Contents Next,NA,NA
Example—A Feed-Forward Network,NA,NA
"A sample feed-forward network, as shown in Figure 1.2, has five neurons arranged in three ",NA,NA
layers: two neurons (labeled ,NA,NA
x,1,NA
 and ,NA,NA
x,2,NA
") in layer 1, two neurons (labeled ",NA,NA
x,3,NA
 and ,NA,NA
x,4,NA
) in layer ,NA,NA
"2, and one neuron (labeled ",NA,NA
x,5,NA
) in layer 3. There are arrows connecting the neurons together. ,NA,NA
This is the direction of information flow. A feed-forward network has ,NA,NA
information flowing forward only. Each arrow that connects neurons has a ,NA,NA
weight ,NA,NA
"associated with it (like, ",NA,NA
w,31,NA
 for example). You calculate the ,NA,NA
"state, ",NA,NA
"x,",NA,NA
 of each neuron by ,NA,NA
summing the weighted values that flow into a neuron. The state of the neuron is the output ,NA,NA
value of the neuron and remains the same until the neuron receives new information on its ,NA,NA
inputs.,NA,NA
Figure 1.2,NA,NA
  A feed-forward neural network with topology 2-2-1.,NA,NA
"For example, for ",NA,NA
x,3,NA
 and ,NA,NA
x,5,NA
:,NA,NA
x,3,NA
 = w,23,NA
x,2,NA
 + w,13,NA
x,1,NA
x,5,NA
 = w,35,NA
x,3,NA
 + w,45,NA
x,4,NA
"We will formalize the equations in Chapter 7, which details one of the training algorithms ",NA,NA
for the feed-forward network called ,NA,NA
Backpropagation,NA,NA
.,NA,NA
Note that you present information to this network at the leftmost nodes (layer 1) called the ,NA,NA
input layer,NA,NA
". You can take information from any other layer in the network, but in most ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/009-012.html (1 of 3) [21/11/02 21:56:45],NA
"cases do so from the rightmost node(s), which make up the ",NA,NA
output layer,NA,NA
. Weights are ,NA,NA
"usually determined by a supervised training algorithm, where you present examples to the ",NA,NA
network and adjust weights appropriately to achieve a desired response. Once you have ,NA,NA
"completed training, you can use the network without changing weights, and note the ",NA,NA
response for inputs that you apply. Note that a detail not yet shown is a nonlinear scaling ,NA,NA
function that limits the range of the weighted sum. This scaling function has the effect of ,NA,NA
clipping very large values in positive and negative directions for each neuron so that the ,NA,NA
cumulative summing that occurs across the network stays within reasonable bounds. ,NA,NA
Typical real number ranges for neuron inputs and outputs are –1 to +1 or 0 to +1. You will ,NA,NA
see more about this network and applications for it in Chapter 7. Now let us contrast this ,NA,NA
"neural network with a completely different type of neural network, the Hopfield network, ",NA,NA
and present some simple applications for the ,NA,NA
Hopfield network,NA,NA
.,NA,NA
Example—A Hopfield Network,NA,NA
"The neural network we present is a Hopfield network, with a single layer. We place, in this ",NA,NA
"layer, four neurons, each connected to the rest, as shown in Figure 1.3. Some of the ",NA,NA
"connections have a positive weight, and the rest have a negative weight. The network will ",NA,NA
"be presented with two input patterns, one at a time, and it is supposed to recall them. The ",NA,NA
inputs would be binary patterns having in each component a 0 or 1. If two patterns of equal ,NA,NA
"length are given and are treated as vectors, their ",NA,NA
dot product,NA,NA
 is obtained by first ,NA,NA
multiplying corresponding components together and then adding these products. Two ,NA,NA
vectors are said to be ,NA,NA
"orthogonal,",NA,NA
 if their dot product is 0. The mathematics involved in ,NA,NA
"computations done for neural networks include matrix multiplication, transpose of a matrix, ",NA,NA
"and transpose of a vector. Also see Appendix B. The inputs (which are stable, stored ",NA,NA
patterns) to be given should be orthogonal to one another.,NA,NA
Figure 1.3,NA,NA
  Layout of a Hopfield network.,NA,NA
The two patterns we want the network to recall are ,NA,NA
A,NA,NA
" = (1, 0, 1, 0) and ",NA,NA
B,NA,NA
" = (0, 1, 0, 1), ",NA,NA
which you can verify to be orthogonal. Recall that two vectors ,NA,NA
A,NA,NA
 and ,NA,NA
B,NA,NA
 are orthogonal if ,NA,NA
their dot product is equal to zero. This is true in this case since,NA,NA
 A,1,NA
B,1,NA
 + A,2,NA
 B,2,NA
 + A,3,NA
B,3,NA
 + A,4,NA
B,4,NA
 = (1x0 + 0x1 + 1x0 + 0x1) = 0,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/009-012.html (2 of 3) [21/11/02 21:56:45],NA
The following matrix ,NA,NA
W,NA,NA
 gives the weights on the connections in the network.,NA,NA
 0   -3    3   -3,NA,NA
 -3    0   -3    3 ,NA,NA
W =   3   -3    0   -3,NA,NA
 -3    3   -3    0,NA,NA
"We need a threshold function also, and we define it as follows. The threshold value [theta] ",NA,NA
is 0. ,NA,NA
f(t) = ,NA,NA
{,NA,NA
 1  if t >= [theta],NA,NA
 0  if t < [theta],NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/009-012.html (3 of 3) [21/11/02 21:56:45]",NA
Previous Table of Contents Next,NA,NA
We have four neurons in the only layer in this network. We need to compute the activation ,NA,NA
of each neuron as the weighted sum of its inputs. The activation at the first node is the dot ,NA,NA
product of the input vector and the first column of the weight matrix (0 -3 3 -3). We get the ,NA,NA
activation at the other nodes similarly. The output of a neuron is then calculated by ,NA,NA
evaluating the threshold function at the activation of the neuron. So if we present the input ,NA,NA
vector ,NA,NA
A,NA,NA
", the dot product works out to 3 and f(3) = 1. Similarly, we get the dot products of ",NA,NA
"the second, third, and fourth nodes to be –6, 3, and –6, respectively. The corresponding ",NA,NA
"outputs therefore are 0, 1, and 0. This means that the output of the network is the vector (1, ",NA,NA
"0, 1, 0), same as the input pattern. The network has recalled the pattern as presented, or we ",NA,NA
can say that pattern ,NA,NA
A,NA,NA
" is stable, since the output is equal to the input. When ",NA,NA
B,NA,NA
" is presented, ",NA,NA
the dot product obtained at the first node is –6 and the output is 0. The outputs for the rest ,NA,NA
"of the nodes taken together with the output of the first node gives (0, 1, 0, 1), which means ",NA,NA
that the network has stable recall for ,NA,NA
B,NA,NA
 also.,"NOTE:  
 In Chapter 4, a method of determining the weight matrix for the Hopfield 
 network given a set of input vectors is presented.",NA
So far we have presented easy cases to the network—vectors that the Hopfield network was ,NA,NA
specifically designed (through the choice of the weight matrix) to recall. What will the ,NA,NA
network give as output if we present a pattern different from both ,NA,NA
A,NA,NA
 and ,NA,NA
B,NA,NA
? Let ,NA,NA
C,NA,NA
" = (0, 1, ",NA,NA
"0, 0) be presented to the network. The activations would be –3, 0, –3, 3, making the outputs ",NA,NA
"0, 1, 0, 1, which means that ",NA,NA
B,NA,NA
 achieves stable recall. This is quite interesting. Suppose we ,NA,NA
did intend to input ,NA,NA
B,NA,NA
 and we made a slight error and ended up presenting ,NA,NA
C,NA,NA
", instead. The ",NA,NA
network did what we wanted and recalled ,NA,NA
B,NA,NA
. But why not ,NA,NA
A,NA,NA
"? To answer this, let us ask is ",NA,NA
C,NA,NA
 ,NA,NA
closer to ,NA,NA
A,NA,NA
 or ,NA,NA
B,NA,NA
? How do we compare? We use the distance formula for two four-,NA,NA
"dimensional points. If (a, b, c, d) and (e, f, g, h) are two four-dimensional points, the ",NA,NA
distance between them is:,NA,NA
[radic][(a – e)2 + (b – f)2 + (c – g)2 + (d – h)2 ],NA,NA
The distance between ,NA,NA
A,NA,NA
 and ,NA,NA
C,NA,NA
" is [radic]3, whereas the distance between ",NA,NA
B,NA,NA
 and ,NA,NA
C,NA,NA
 is just 1. ,NA,NA
So since ,NA,NA
B,NA,NA
" is closer in this sense, ",NA,NA
B,NA,NA
 was recalled rather than ,NA,NA
A,NA,NA
. You may verify that if we ,NA,NA
do the same exercise with ,NA,NA
D,NA,NA
" = (0, 0, 1, 0), we will see that the network recalls ",NA,NA
A,NA,NA
", which is ",NA,NA
closer than ,NA,NA
B,NA,NA
 to ,NA,NA
D,NA,NA
.,NA,NA
Hamming Distance,NA,NA
"When we talk about closeness of a bit pattern to another bit pattern, the ",NA,NA
Euclidean distance ,NA,NA
"need not be considered. Instead, the ",NA,NA
Hamming distance,NA,NA
" can be used, which is much easier ",NA,NA
"to determine, since it is the number of bit positions in which the two patterns being ",NA,NA
"compared differ. Patterns being strings, the Hamming distance is more appropriate than the ",NA,NA
Euclidean distance.,"NOTE:  
 The weight matrix 
 W
  we gave in this example is not the only weight matrix that 
 would enable the network to recall the patterns A and B correctly. You can see that if we 
 replace each of 3 and –3 in the matrix by say, 2 and –2, respectively, the resulting matrix 
 would also facilitate the same performance from the network. For more details, consult 
 Chapter 4.",NA
Asynchronous Update,NA,NA
The Hopfield network is a ,NA,NA
recurrent,NA,NA
 network. This means that outputs from the network are ,NA,NA
"fed back as inputs. This is not apparent from Figure 1.3, but is clearly seen from Figure 1.4.",NA,NA
The Hopfield network always stabilizes to a fixed point. There is a very important detail ,NA,NA
"regarding the Hopfield network to achieve this stability. In the examples thus far, we have ",NA,NA
"not had a problem getting a stable output from the network, so we have not presented this ",NA,NA
detail of network operation. This detail is the need to update the network ,NA,NA
asynchronously,NA,NA
. ,NA,NA
"This means that changes do not occur simultaneously to outputs that are fed back as inputs, ",NA,NA
but rather occur for one vector component at a time. The true operation of the Hopfield ,NA,NA
network follows the procedure below for input vector ,NA,NA
Invec,NA,NA
 and output vector ,NA,NA
Outvec,NA,NA
:,NA,NA
1.,NA,NA
"  Apply an input, Invec, to the network, and initialize ",NA,NA
Outvec = Invec ,NA,NA
2.,NA,NA
  Start with i = 1 ,NA,NA
3.,NA,NA
  Calculate Value,i,NA
 = DotProduct ( Invec,"i,",NA
 Column,i,NA
 of Weight matrix) ,NA,NA
4.,NA,NA
  Calculate ,NA,NA
Outvec,i,NA
 = ,NA,NA
f,NA,NA
(,NA,NA
Value,i,NA
) where ,NA,NA
f,NA,NA
 is the threshold function discussed ,NA,NA
previously ,NA,NA
5.,NA,NA
  Update the input to the network with component ,NA,NA
Outvec,i,NA
6.,NA,NA
"  Increment i, and repeat steps 3, 4, 5, and 6 until ",NA,NA
Invec,NA,NA
 = ,NA,NA
Outvec,NA,NA
 (note that when i ,NA,NA
"reaches its maximum value, it is then next reset to 1 for the cycle to continue) ",NA,NA
"Now let’s see how to apply this procedure. Building on the last example, we now input ",NA,NA
E,NA,NA
 = ,NA,NA
"(1, 0, 0, 1), which is at an equal distance from ",NA,NA
A,NA,NA
 and ,NA,NA
B.,NA,NA
 Without applying the ,NA,NA
"asynchronous procedure above, but instead using the shortcut procedure we’ve been using ",NA,NA
"so far, you would get an output ",NA,NA
F,NA,NA
" = (0, 1, 1, 0). This vector, ",NA,NA
F,NA,NA
", as subsequent input would ",NA,NA
result in ,NA,NA
E,NA,NA
 as the output. This is incorrect since the network oscillates between two states. ,NA,NA
We have updated the entire input vector synchronously.,NA,NA
Now let’s apply asynchronous update. For input ,NA,NA
E,NA,NA
", (1,0,0,1) we arrive at the following ",NA,NA
"results detailed for each update step, in Table 1.1.",NA,NA
Table 1.1,NA,NA
Example of Asynchronous Update for the Hopfield Network ,NA,NA
Step ,NA,NA
i ,NA,NA
Invec ,NA,NA
Column of ,NA,NA
Weight vector ,NA,NA
Value ,NA,NA
Outvec ,NA,NA
notes ,NA,NA
0 ,NA,NA
1 ,NA,NA
1001 ,NA,NA
0 -3 3 -3 ,NA,NA
-3 ,NA,NA
1001 ,NA,NA
initialization : set ,NA,NA
Outvec = Invec = Input ,NA,NA
pattern ,NA,NA
1 ,NA,NA
1001 ,NA,NA
0001 ,NA,NA
column 1 of Outvec ,NA,NA
changed to 0 ,NA,NA
2 ,NA,NA
2 ,NA,NA
0001 ,NA,NA
-3 0 -3 3 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
column 2 of Outvec ,NA,NA
changed to 1 ,NA,NA
3 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
3 -3 0 -3 ,NA,NA
-6 ,NA,NA
0101 ,NA,NA
column 3 of Outvec ,NA,NA
stays as 0 ,NA,NA
4 ,NA,NA
4 ,NA,NA
0101 ,NA,NA
-3 3 -3 0 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
column 4 of Outvec ,NA,NA
stays as 1 ,NA,NA
5 ,NA,NA
1 ,NA,NA
0101 ,NA,NA
0 -3 3 -3 ,NA,NA
-6 ,NA,NA
0101 ,NA,NA
column 1 stable as 0 ,NA,NA
6 ,NA,NA
2 ,NA,NA
0101 ,NA,NA
-3 0 -3 3 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
column 2 stable as 1 ,NA,NA
7 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
3 -3 0 -3 ,NA,NA
-6 ,NA,NA
0101 ,NA,NA
column 3 stable as 0 ,NA,NA
8 ,NA,NA
4 ,NA,NA
0101 ,NA,NA
-3 3 -3 0 ,NA,NA
3 ,NA,NA
0101 ,NA,NA
column 4 stable as 1; ,NA,NA
stable recalled pattern = ,NA,NA
0101 ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Binary and Bipolar Inputs,NA,NA
Two types of inputs that are used in neural networks are ,NA,NA
binary,NA,NA
 and ,NA,NA
bipolar,NA,NA
 inputs. We ,NA,NA
"have already seen examples of binary input. Bipolar inputs have one of two values, 1 and –",NA,NA
"1. There is clearly a one-to-one mapping or correspondence between them, namely having -",NA,NA
1 of bipolar correspond to a 0 of binary. In determining the weight matrix in some ,NA,NA
"situations where binary strings are the inputs, this mapping is used, and when the output ",NA,NA
"appears in bipolar values, the inverse transformation is applied to get the corresponding ",NA,NA
binary string. A simple example would be that the binary string 1 0 0 1 is mapped onto the ,NA,NA
bipolar string 1 –1 –1 1; while using the inverse transformation on the bipolar string –1 1 –,NA,NA
"1 –1, we get the binary string 0 1 0 0.",NA,NA
Bias,NA,NA
The use of threshold value can take two forms. One we showed in the example. The ,NA,NA
"activation is compared to the threshold value, and the neuron fires if the threshold value is ",NA,NA
"attained or exceeded. The other way is to add a value to the activation itself, in which case ",NA,NA
it is called the ,NA,NA
bias,NA,NA
", and then determining the output of the neuron. We will encounter bias ",NA,NA
and ,NA,NA
gain,NA,NA
 later.,NA,NA
Another Example for the Hopfield Network,NA,NA
You will see in Chapter 12 an application of ,NA,NA
Kohonen’s feature map,NA,NA
 for pattern ,NA,NA
recognition. Here we give an example of pattern association using a Hopfield network. The ,NA,NA
patterns are some characters. A pattern representing a character becomes an input to a ,NA,NA
Hopfield network through a bipolar vector. This bipolar vector is generated from the pixel ,NA,NA
"(picture element) grid for the character, with an assignment of a 1 to a black pixel and a -1 ",NA,NA
to a pixel that is white. A grid size such as 5x7 or higher is usually employed in these ,NA,NA
"approaches. The number of pixels involved will then be 35 or more, which determines the ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/015-020.html (1 of 4) [21/11/02 21:56:48],NA
dimension of a bipolar vector for the character pattern.,NA,NA
"We will use, for simplicity, a 3x3 grid for character patterns in our example. This means ",NA,NA
"the Hopfield network has 9 neurons in the only layer in the network. Again for simplicity, ",NA,NA
we use two ,NA,NA
exemplar patterns,NA,NA
", or reference patterns, which are given in Figure 1.5. ",NA,NA
"Consider the pattern on the left as a representation of the character “plus”, +, and the one ",NA,NA
"on the right that of “minus”, ",NA,NA
-,NA,NA
 .,NA,NA
Figure 1.5,NA,NA
  The “plus” pattern and “minus” pattern.,NA,NA
"The bipolar vectors that represent the characters in the figure, reading the character pixel ",NA,NA
"patterns row by row, left to right, and top to bottom, with a 1 for black and -1 for white ",NA,NA
"pixels, are C+ = (-1, 1, -1, 1, 1, 1, -1, 1, -1), and C- = (-1, -1, -1, 1, 1, 1, -1, -1, -1). The ",NA,NA
weight matrix ,NA,NA
W,NA,NA
 is:,NA,NA
 0  0  2 -2 -2 -2  2  0  2,NA,NA
 0  0  0  0  0  0  0  2  0,NA,NA
 2  0  0 -2 -2 -2  2  0  2,NA,NA
 2  0 -2  0  2  2 -2  0 -2 ,NA,NA
W=    2  0 -2  2  0  2 -2  0 -2,NA,NA
 2  0 -2  2  2  0 -2  0 -2,NA,NA
 2  0  2 -2 -2 -2  0  0  2,NA,NA
 0  2  0  0  0  0  0  0  0,NA,NA
 2  0  2 -2 -2 -2  2  0  0,NA,NA
"The activations with input C+ are given by the vector (-12, 2, -12, 12, 12, 12, -12, 2, -12). ",NA,NA
"With input C-, the activations vector is (-12, -2, -12, 12, 12, 12, -12, -2, -12). ",NA,NA
When this Hopfield network uses the threshold function,NA,NA
f(x) = ,NA,NA
{,NA,NA
 1  if x >= 0,NA,NA
 -1  if x [le] 0,NA,NA
"the corresponding outputs will be C+ and C-, respectively, showing the stable recall of the ",NA,NA
"exemplar vectors, and establishing an autoassociation for them. When the output vectors ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/015-020.html (2 of 4) [21/11/02 21:56:48],NA
"are used to construct the corresponding characters, you get the original character patterns. ",NA,NA
Let us now input the character pattern in Figure 1.6.,NA,NA
Figure 1.6,NA,NA
  Corrupted “minus” pattern.,NA,NA
We will call the corresponding bipolar vector ,NA,NA
A,NA,NA
" = (1, -1, -1, 1, 1, 1, -1, -1, -1). You get the ",NA,NA
"activation vector (-12, -2, -8, 4, 4, 4, -8, -2, -8) giving the output vector, ",NA,NA
C,NA,NA
"- = (-1, -1, -1, 1, ",NA,NA
"1, 1, -1, -1, -1). In other words, the character ",NA,NA
-,NA,NA
", corrupted slightly, is recalled as the ",NA,NA
character ,NA,NA
-,NA,NA
 by the Hopfield network. The intended pattern is recognized.,NA,NA
We now input a bipolar vector that is different from the vectors corresponding to the ,NA,NA
"exemplars, and see whether the network can store the corresponding pattern. The vector ",NA,NA
we choose is ,NA,NA
B,NA,NA
" = (1, -1, 1, -1, -1, -1, 1, -1, 1). The corresponding neuron activations are ",NA,NA
"given by the vector (12, -2, 12, -4, -4, -4, 12, -2, 12) which causes the output to be the ",NA,NA
"vector (1, -1, 1, -1, -1, -1, 1, -1, 1), same as B. An additional pattern, which is a 3x3 grid ",NA,NA
"with only the corner pixels black, as shown in Figure 1.7, is also recalled since it is ",NA,NA
"autoassociated, by this Hopfield network.",NA,NA
Figure 1.7,NA,NA
  Pattern result.,NA,NA
"If we omit part of the pattern in Figure 1.7, leaving only the top corners black, as in Figure ",NA,NA
"1.8, we get the bipolar vector ",NA,NA
D,NA,NA
" = (1, -1, 1, -1, -1, -1, -1, -1, -1). You can consider this also ",NA,NA
as an incomplete or corrupted version of the pattern in Figure 1.7. The network activations ,NA,NA
"turn out to be (4, -2, 4, -4, -4, -4, 8, -2, 8) and give the output (1, -1, 1, -1, -1, -1, 1, -1, 1), ",NA,NA
which is B.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/015-020.html (3 of 4) [21/11/02 21:56:48],NA
Figure 1.8,NA,NA
  A partly lost Pattern of Figure 1.7.,NA,NA
Summary,NA,NA
In this chapter we introduced a neural network as a collection of processing elements ,NA,NA
distributed over a finite number of layers and interconnected with positive or negative ,NA,NA
"weights, depending on whether cooperation or competition (or inhibition) is intended. The ",NA,NA
activation of a neuron is basically a weighted sum of its inputs. A threshold function ,NA,NA
determines the output of the network. There may be layers of neurons in between the input ,NA,NA
"layer and the output layer, and some such middle layers are referred to as hidden layers, ",NA,NA
"others by names such as Grossberg or Kohonen layers, named after the researchers Stephen ",NA,NA
"Grossberg and Teuvo Kohonen, who proposed them and their function. ",NA,NA
"Modification of the weights is the process of training the network, and a network subject to ",NA,NA
this process is said to be learning during that phase of the operation of the network. In some ,NA,NA
"network operations, a feedback operation is used in which the current output is treated as ",NA,NA
modified input to the same network. ,NA,NA
"You have seen a couple of examples of a Hopfield network, one of them for pattern ",NA,NA
recognition.,NA,NA
Neural networks can be used for problems that can’t be solved with a known formula and ,NA,NA
for problems with incomplete or noisy data. Neural networks seem to have the capacity to ,NA,NA
"recognize patterns in the data presented to it, and are thus useful in many types of pattern ",NA,NA
recognition problems.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch01/015-020.html (4 of 4) [21/11/02 21:56:48]",NA
Previous Table of Contents Next,NA,NA
Chapter 2 ,NA,NA
C++ and Object Orientation ,NA,NA
Introduction to C++,NA,NA
C++ is an ,NA,NA
object-oriented,NA,NA
 programming language built on the base of the C language. This ,NA,NA
"chapter gives you a very brief introduction to C++, touching on many important aspects of ",NA,NA
"C++, so you would be able to follow our presentations of the C++ implementations of ",NA,NA
neural network models and write your own C++ programs.,NA,NA
The C++ language is a superset of the C language. You could write C++ programs like C ,NA,NA
"programs (a few of the programs in this book are like that), or you could take advantage of ",NA,NA
the ,NA,NA
object-oriented,NA,NA
 features of C++ to write object-oriented programs (like the ,NA,NA
backpropagation simulator of Chapter 7). What makes a programming language or ,NA,NA
"programming methodology object oriented? Well, there are several indisputable pillars of ",NA,NA
object orientation. These features stand out more than any other as far as object orientation ,NA,NA
goes. They are ,NA,NA
"encapsulation, data hiding, overloading, polymorphism,",NA,NA
 and the grand-,NA,NA
daddy of them all: ,NA,NA
inheritance.,NA,NA
 Each of the pillars of object-orientation will be discussed in ,NA,NA
"the coming sections, but before we tackle these, we need to answer the question, What does ",NA,NA
"all this object-oriented stuff buy me ? By using the object-oriented features of C++, in ",NA,NA
conjunction with ,NA,NA
Object-Oriented Analysis and Design(OOAD),NA,NA
", which is a methodology ",NA,NA
"that fully utilizes object orientation, you can have well-packaged, reusable, extensible, and ",NA,NA
reliable programs and program segments. It’s beyond the scope of this book to discuss ,NA,NA
"OOAD, but it’s recommended you read Booch or Rumbaugh to get more details on OOAD ",NA,NA
and how and why to change your programming style forever! See the reference section in ,NA,NA
the back of this book for more information on these readings. Now let’s get back to ,NA,NA
discussing the great object-oriented features of C++.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/021-023.html (1 of 3) [21/11/02 21:56:49],NA
Encapsulation,NA,NA
In C++ you have the facility to encapsulate data and the operations that manipulate that ,NA,NA
"data, in an appropriate object. This enables the use of these collections of data and ",NA,NA
"function, called ",NA,NA
objects,NA,NA
" , in programs other than the program for which they were ",NA,NA
"originally created. With objects, just as with the traditional concept of subroutines, you ",NA,NA
make functional blocks of code. You still have language-supported abstractions such as ,NA,NA
scope and separate compilation available. This is a rudimentary form of encapsulation. ,NA,NA
"Objects carry encapsulation a step further. With objects, you define not only the way a ",NA,NA
"function operates, or its ",NA,NA
implementation,NA,NA
", but also the way an object can be accessed, or its ",NA,NA
interface.,NA,NA
" You can specify access differently for different entities. For example, you could ",NA,NA
make function ,NA,NA
do_operation(),NA,NA
 contained inside ,NA,NA
Object A,NA,NA
 accessible to ,NA,NA
Object B,NA,NA
 but not to ,NA,NA
Object C.,NA,NA
 This access qualification can also be used for data members inside an object. ,NA,NA
The encapsulation of data and the intended operations on them prevents the data from being ,NA,NA
subjected to operations not meant for them. This is what really makes objects reusable and ,NA,NA
portable! The operations are usually given in the form of functions operating upon the data ,NA,NA
items. Such functions are also called ,NA,NA
methods,NA,NA
 in some object-oriented programming ,NA,NA
languages. The data items and the functions that manipulate them are combined into a ,NA,NA
structure called a ,NA,NA
class,NA,NA
. A class is an abstract data type. When you make an instance of a ,NA,NA
"class, you have an object. This is no different than when you instantiate an integer type to ",NA,NA
create variables ,NA,NA
i,NA,NA
 and ,NA,NA
j.,NA,NA
" For example, you design a class called ",NA,NA
"ElectronicBook,",NA,NA
 with a data element called ,NA,NA
ArrayofPages,NA,NA
. When you instantiate your class ,NA,NA
you make objects of ,NA,NA
type,NA,NA
 ElectronicBook. Suppose that you create two of these called ,NA,NA
EB_Geography,NA,NA
 and ,NA,NA
EB_History,NA,NA
. Every object that is instantiated has its own data ,NA,NA
"member inside it, referred to by ",NA,NA
ArrayOfPages,NA,NA
.,NA,NA
Data Hiding,NA,NA
Related to the idea of encapsulation is the concept of ,NA,NA
data hiding,NA,NA
. Encapsulation hides the ,NA,NA
data from other classes and functions in other classes. Going back to the ,NA,NA
ElectronicBook ,NA,NA
"class, you could define functions like ",NA,NA
GetNextPage,NA,NA
", ",NA,NA
GetPreviousPage,NA,NA
", and ",NA,NA
GetCurrentPage,NA,NA
 as the only means of accessing information in the ,NA,NA
ArrayofPages,NA,NA
 data ,NA,NA
"member, by functions that access the ",NA,NA
ElectronicBook,NA,NA
 object. Although there may be a ,NA,NA
hundred and one other attributes and data elements in the class ,NA,NA
ElectronicBook,NA,NA
", these are ",NA,NA
"all hidden from view. This makes programs more reliable, since publishing a specific ",NA,NA
interface to an object prevents inadvertent access to data in ways that were not designed or ,NA,NA
"accounted for. In C++, the access to an object, and its encapsulated data and functions is ",NA,NA
"treated very carefully, by the use of keywords ",NA,NA
private,NA,NA
", ",NA,NA
protected,NA,NA
", and ",NA,NA
public,NA,NA
. One has the ,NA,NA
"opportunity to make access specifications for data objects and functions as being private, or ",NA,NA
"protected, or public while defining a class. Only when the declaration is made as public ",NA,NA
do other functions and objects have access to the object and its components without ,NA,NA
"question. On the other hand, if the declaration happens to be as private, there is no ",NA,NA
"possibility of such access. When the declaration given is as protected, then the access to ",NA,NA
"data and functions in a class by others is not as free as when it is public, nor as restricted as ",NA,NA
"when it is private. You can declare one class as derived from another class, which will be ",NA,NA
discussed shortly. So-called derived classes and the declaring class do get the access to the ,NA,NA
components of the object that are declared protected. One class that is not a derived class of ,NA,NA
a second class can get access to data items and functions of the second class if it is declared ,NA,NA
as a ,NA,NA
friend,NA,NA
 class in the second. The three types of declarations of access ,NA,NA
"specification can be different for different components of an object. For example, some of ",NA,NA
"the data items could be declared public, some private, and the others protected. The same ",NA,NA
"situation can occur with the functions in an object. When no explicit declaration is made, ",NA,NA
the default specification is as private.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/021-023.html (3 of 3) [21/11/02 21:56:49]",NA
Previous Table of Contents Next,NA,NA
Constructors and Destructors as Special Functions of C++,NA,NA
Constructors,NA,NA
 and ,NA,NA
destructors,NA,NA
 are special functions in C++. They define how an object is ,NA,NA
created and destroyed. You cannot have a class defined in a C++ program without ,NA,NA
declaring and defining at least one constructor for it. You may omit declaring and then ,NA,NA
defining a destructor only because the compiler you use will create a default destructor. ,NA,NA
"More than one constructor, but only one destructor, can be declared for a class.",NA,NA
Constructors are for the creation of an object of a class and for initializing it. C++ requires ,NA,NA
that every function has a return type. The only exceptions are constructors and destructors. ,NA,NA
A constructor is given the same name as the class for which it is a constructor. It may take ,NA,NA
arguments or it may not need them. Different constructors for the same class differ in the ,NA,NA
number and types of arguments they take. It is a good idea to provide for each class at least ,NA,NA
a default constructor that does not take any arguments and does not do anything except ,NA,NA
create an object of that class type. A constructor is called at the time an object of its class is ,NA,NA
needed to be created.,NA,NA
"A destructor also is given the same name as the class for which it is a destructor, but with ",NA,NA
"the tilde (~) preceding the name. Typically, what is done in a destructor is to have ",NA,NA
statements that ask the system to delete the various data structures created for the class. ,NA,NA
This helps to free-up allocated memory for those data structures. A destructor is called ,NA,NA
when the object created earlier is no longer needed in the program.,NA,NA
Dynamic Memory Allocation,NA,NA
C++ has keywords ,NA,NA
new,NA,NA
 and ,NA,NA
delete,NA,NA
", which are used as a pair in that order, though separated ",NA,NA
by other statements of the program. They are for making dynamic allocation of memory at ,NA,NA
the time of creation of a class object and for freeing-up such allocated memory when it is ,NA,NA
no longer needed. You create space on the heap with the use of new. This obviates the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/023-025.html (1 of 3) [21/11/02 21:56:50],NA
"need in C++ for malloc, which is the function for dynamic memory allocation used in C.",NA,NA
Overloading,NA,NA
Encapsulation of data and functions would also allow you to use the same function name in ,NA,NA
two different objects. The use of a name for a function more than once does not have to be ,NA,NA
only in different object declarations. Within the same object one can use the same name for ,NA,NA
"functions with different functionality, if they can be distinguished in terms of either their ",NA,NA
return type or in terms of their argument types and number. This feature is called ,NA,NA
overloading,NA,NA
". For example, if two different types of variables are data items in an object, a ",NA,NA
"commonly named function can be the addition, one for each of the two types of ",NA,NA
variables—thus taking advantage of overloading. Then the function addition is said to be ,NA,NA
overloaded. But remember that the function main is just about the only function that cannot ,NA,NA
be overloaded.,NA,NA
Polymorphism and Polymorphic Functions,NA,NA
A ,NA,NA
polymorphic,NA,NA
 function is a function whose name is used in different ways in a program. It ,NA,NA
"can be also declared virtual, if the intention is late binding. This enables it to be bound at ",NA,NA
run time. Late binding is also referred to as dynamic binding. An advantage in declaring a ,NA,NA
"function in an object as virtual is that, if the program that uses this object calls that function ",NA,NA
"only conditionally, there is no need to bind the function early, during the ",NA,NA
compilation of the program. It will be bound only if the condition is met and the call of the ,NA,NA
"function takes place. For example, you could have a polymorphic function called ",NA,NA
draw() ,NA,NA
"that is associated with different graphical objects, like for rectangle, circle, and sphere. The ",NA,NA
"details or methods of the functions are different, but the name ",NA,NA
draw(),NA,NA
 is common. If you ,NA,NA
now have a collection of these objects and pick up an arbitrary object without knowing ,NA,NA
"exactly what it is (via a pointer, for example), you can still invoke the draw function for the ",NA,NA
object and be assured that the right draw function will be bound to the object and called.,NA,NA
Overloading Operators,NA,NA
"You can overload operators in addition to overloading functions. As a matter of fact, the ",NA,NA
system defined left shift operator ,NA,NA
<<,NA,NA
 is also overloaded in C++ when used with ,NA,NA
"cout,",NA,NA
 the ,NA,NA
C++ variation of the C language ,NA,NA
printf,NA,NA
 function. There is a similar situation with the right ,NA,NA
shift operator >> in C++ when used with ,NA,NA
"cin,",NA,NA
 the C++ variation of the C language ,NA,NA
scanf ,NA,NA
function. You can take any operator and overload it. But you want to be cautious and not ,NA,NA
"overdo it, and also you do not create confusion when you overload an operator. The ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/023-025.html (2 of 3) [21/11/02 21:56:50],NA
guiding principle in this regard is the creation of a code-saving and time-saving facility ,NA,NA
while maintaining simplicity and clarity. Operator overloading is especially useful for ,NA,NA
doing normal arithmetic on nonstandard data types. You could overload the multiplication ,NA,NA
"symbol to work with complex numbers, for example.",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/023-025.html (3 of 3) [21/11/02 21:56:50]",NA
Previous Table of Contents Next,NA,NA
Inheritance,NA,NA
The primary distinction for C++ from C is that C++ has classes. Objects are defined in ,NA,NA
"classes. Classes themselves can be data items in other classes, in which case one class ",NA,NA
"would be an element of another class. Of course, then one class is a member, which brings ",NA,NA
"with it its own data and functions, in the second class. This type of relationship is referred ",NA,NA
to as a “has-a” relationship: Object A has an Object B inside it. ,NA,NA
A relationship between classes can be established not only by making one class a member ,NA,NA
of another but also by the process of deriving one class from another. One class can be ,NA,NA
"derived from another class, which becomes its base class. Then a hierarchy of classes is ",NA,NA
"established, and a sort of parent–child relationship between classes is established. The ",NA,NA
"derived class inherits, from the base class, some of the data members and functions. This ",NA,NA
type of relationship is referred to as an “is-a” relationship. You could have class ,NA,NA
Rectangle ,NA,NA
be derived from class ,NA,NA
Shape,NA,NA
", since ",NA,NA
Rectangle,NA,NA
 is a ,NA,NA
Shape,NA,NA
". Naturally, if a class A is derived ",NA,NA
"from a class B, and if B itself is derived from a class C, then A inherits from both B and C. ",NA,NA
A class can be derived from more than one class. This is how ,NA,NA
multiple inheritance,NA,NA
 occurs. ,NA,NA
Inheritance is a powerful mechanism for creating base functionality that is passed onto ,NA,NA
next generations for further enhancement or modification.,NA,NA
Derived Classes,NA,NA
"When one class has some members declared in it as protected, then such members would ",NA,NA
"be hidden from other classes, but not from the derived classes. In other words, deriving one ",NA,NA
class from another is a way of accessing the protected members of the parent class by the ,NA,NA
derived class. We then say that the derived class is inheriting from the parent class those ,NA,NA
members in the parent class that are declared as protected or public. ,NA,NA
"In declaring a derived class from another class, access or visibility specification can be ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/025-027.html (1 of 3) [21/11/02 21:56:51],NA
"made, meaning that such derivation can be public or the default case, private. Table 2.1 ",NA,NA
shows the consequences of such specification when deriving one class from another.,NA,NA
Table 2.1,NA,NA
Visibility of Base Class Members in Derived Class ,NA,NA
Derivation ,NA,NA
Specification ,NA,NA
Base Class ,NA,NA
Specification ,NA,NA
Derived Class Access ,NA,NA
private(default) ,NA,NA
private ,NA,NA
none ,NA,NA
public ,NA,NA
protected ,NA,NA
"full access, private in derived class ",NA,NA
public ,NA,NA
"full access, public in derived class ",NA,NA
private ,NA,NA
none ,NA,NA
protected ,NA,NA
"full access, protected in derived class ",NA,NA
public ,NA,NA
"full access, public in derived class ",NA,NA
Reuse of Code,NA,NA
C++ is also attractive for the extendibility of the programs written in it and for the reuse ,NA,NA
"opportunity, thanks to the features in C++ such as inheritance and polymorphism ",NA,NA
mentioned earlier. A new programming project cannot only reuse classes that are created ,NA,NA
"for some other program, if they are appropriate, but can extend another program with ",NA,NA
additional classes and functions as deemed necessary. You could inherit from an existing ,NA,NA
class hierarchy and only change functionality where you need to. ,NA,NA
C++ Compilers,NA,NA
"All of the programs in this book have been compiled and tested with Turbo C++, Borland ",NA,NA
"C++, Microsoft C/C++, and Microsoft Visual C++. These are a few of the popular ",NA,NA
commercial C++ compilers available. You should be able to use most other commercial ,NA,NA
C++ compilers also. All of the programs should also port easily to other operating systems ,NA,NA
"like Unix and the Mac, because they are all character-based programs. ",NA,NA
Previous Table of Contents Next,NA,NA
Previous Table of Contents Next,NA,NA
Writing C++ Programs,NA,NA
"Before one starts writing a C++ program for a particular problem, one has to have a clear ",NA,NA
picture of the various parameters and variables that would be part of the problem definition ,NA,NA
"and/or its solution. In addition, it should be clear as to what manipulations need to be ",NA,NA
performed during the solution process. Then one carefully determines what classes are ,NA,NA
needed and what relationships they have to each other in a hierarchy of classes. Think ,NA,NA
"about is-a and has-a relationships to see where classes need to defined, and which classes ",NA,NA
could be derived from others. It would be far more clear to the programmer at this point in ,NA,NA
the program plan what the data and function access specifications should be and so on. The ,NA,NA
typical compilation error messages a programmer to C++ may encounter are stating that a ,NA,NA
"particular function or data is not a member of a particular class, or that it is not accessible to ",NA,NA
"a class, or that a constructor was not available for a particular class. When function ",NA,NA
"arguments at declaration and at the place the function is called do not match, either for ",NA,NA
"number or for types or both, the compiler thinks of them as two different functions. The ",NA,NA
compiler does not find the definition and/or declaration of one of the two and has reason to ,NA,NA
complain. This type of error in one line of code may cause the compiler to alert you that ,NA,NA
"several other errors are also present, perhaps some in terms of improper punctuation. In that ",NA,NA
"case, remedying the fundamental error that was pointed out would straighten many of the ",NA,NA
other argued matters. ,NA,NA
The following list contains a few additional particulars you need to keep in mind when ,NA,NA
writing C++ programs.,NA,NA
•,NA,NA
  A member x of an object A is referred to with A.x just as done with structure ,NA,NA
elements in C. ,NA,NA
•,NA,NA
"  If you declare a class B, then the constructor function is also named B. B has no ",NA,NA
"return type. If this constructor takes, say, one argument of type integer, you define ",NA,NA
the constructor using the syntax: ,NA,NA
B::B(int){whatever the function does};,NA,NA
•,NA,NA
"  If you declare a member function C of class B, where return type of C is, say, ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/027-030.html (1 of 3) [21/11/02 21:56:51],NA
"float and C takes two arguments, one of type float, and the other int, then you define ",NA,NA
C with the syntax: ,NA,NA
"float B::C(float,int){whatever the function does};•",NA,NA
  If you ,NA,NA
"declare a member function D of class B, where D does not return any value and ",NA,NA
"takes no arguments, you define D using the syntax: ",NA,NA
void B::D( ){whatever the ,NA,NA
function does};,NA,NA
•,NA,NA
"  If G is a class derived from, say, class B previously mentioned, you declare G ",NA,NA
using the syntax: ,NA,NA
class G:B,NA,NA
. The constructor for G is defined using the syntax: ,NA,NA
G::G(arguments of G):B(int){whatever the function does},NA,NA
". If, on the other hand, ",NA,NA
"G is derived from, say, class B as well as class T, then you declare G using the ",NA,NA
syntax: ,NA,NA
"class G:B,T",NA,NA
. ,NA,NA
•,NA,NA
"  If one class is declared as derived from more than one other class, that is, if there ",NA,NA
"are more than one base class for it, the derivations specification can be different or ",NA,NA
the same. Thus the class may be derived from one class publicly and at the same ,NA,NA
time from another class privately. ,NA,NA
•,NA,NA
  If you have declared a global variable ,NA,NA
y,NA,NA
" external to a class B, and if you also have a ",NA,NA
data member ,NA,NA
y,NA,NA
" in the class B, you can use the external ",NA,NA
y,NA,NA
 with the reference symbol ::. ,NA,NA
Thus ::,NA,NA
y,NA,NA
" refers to the global variable, whereas y, within a member function of B, or ",NA,NA
B.y refers to the data member of B. This way polymorphic functions can also be ,NA,NA
distinguished from each other. ,NA,NA
"This is by no means a comprehensive list of features, but more a brief list of important ",NA,NA
constructs in C++. You will see examples of C++ usage in later chapters. ,NA,NA
Summary,NA,NA
A few highlights of the C++ language are presented. ,NA,NA
•,NA,NA
  C++ is an object-oriented language with full compatibility with the C language. ,NA,NA
•,NA,NA
  ,NA,NA
You create classes in C++ that encapsulate data and functions that operate on the ,NA,NA
"data, and hiding data where a public interface is not needed. ",NA,NA
•,NA,NA
  You can create hierarchies of classes with the facility of inheritance. ,NA,NA
Polymorphism is a feature that allows you to apply a function to a task according to ,NA,NA
the object the function is operating on. ,NA,NA
•,NA,NA
"  Another feature in C++ is overloading of operators, which allows you to create ",NA,NA
new functionality for existing operators in a different context. ,NA,NA
•,NA,NA
"  Overall, C++ is a powerful language fitting the object-oriented paradigm that ",NA,NA
enables software reuse and enhanced reliability. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch02/027-030.html (3 of 3) [21/11/02 21:56:51]",NA
Previous Table of Contents Next,NA,NA
Chapter 3 ,NA,NA
A Look at Fuzzy Logic ,NA,NA
Crisp or Fuzzy Logic?,NA,NA
Logic deals with true and false. A ,NA,NA
proposition,NA,NA
 can be true on one occasion and false on ,NA,NA
another. “Apple is a red fruit” is such a proposition. If you are holding a Granny Smith ,NA,NA
"apple that is green, the proposition that apple is a red fruit is false. On the other hand, if ",NA,NA
"your apple is of a red delicious variety, it is a red fruit and the proposition in reference is ",NA,NA
"true. If a proposition is true, it has a truth value of 1; if it is false, its truth value is 0. These ",NA,NA
are the only possible truth values. Propositions can be combined to generate other ,NA,NA
"propositions, by means of logical operations.",NA,NA
"When you say it will rain today or that you will have an outdoor picnic today, you are ",NA,NA
making statements with certainty. Of course your statements in this case can be either true ,NA,NA
"or false. The truth values of your statements can be only 1, or 0. Your statements then can ",NA,NA
be said to be ,NA,NA
crisp,NA,NA
.,NA,NA
"On the other hand, there are statements you cannot make with such certainty. You may be ",NA,NA
"saying that you think it will rain today. If pressed further, you may be able to say with a ",NA,NA
"degree of certainty in your statement that it will rain today. Your level of certainty, ",NA,NA
"however, is about 0.8, rather than 1. This type of situation is what ",NA,NA
fuzzy logic,NA,NA
 was ,NA,NA
developed to model. Fuzzy logic deals with propositions that can be true to a certain ,NA,NA
"degree—somewhere from 0 to 1. Therefore, a proposition’s truth value indicates the degree ",NA,NA
of certainty about which the proposition is true. The degree of certainity sounds like a ,NA,NA
"probability (perhaps subjective probability), but it is not quite the same. Probabilities for ",NA,NA
"mutually exclusive events cannot add up to more than 1, but their fuzzy values may. ",NA,NA
Suppose that the probability of a cup of coffee being hot is 0.8 and the probability of the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/031-033.html (1 of 3) [21/11/02 21:56:52],NA
cup of coffee being cold is 0.2. These probabilities must add up to 1.0. Fuzzy values do not ,NA,NA
need to add up to 1.0. The truth value of a proposition that a cup of coffee is hot is 0.8. The ,NA,NA
truth value of a proposition that the cup of coffee is cold can be 0.5. There is no restriction ,NA,NA
on what these truth values must add up to.,NA,NA
Fuzzy Sets,NA,NA
Fuzzy logic is best understood in the context of ,NA,NA
set membership,NA,NA
. Suppose you are ,NA,NA
assembling a set of rainy days. Would you put today in the set? When you deal only with ,NA,NA
"crisp statements that are either true or false, your inclusion of today in the set of rainy days ",NA,NA
"is based on certainty. When dealing with fuzzy logic, you would include today in the set of ",NA,NA
rainy days via an ,NA,NA
ordered pair,NA,NA
", such as (today, 0.8). The first member in such an ordered ",NA,NA
pair is ,NA,NA
a candidate for inclusion,NA,NA
" in the set, and the second member is a value between 0 and ",NA,NA
"1, inclusive, called the ",NA,NA
degree of membership,NA,NA
 in the set. The inclusion of the degree of ,NA,NA
membership in the set makes it convenient for developers to come up with a set theory ,NA,NA
"based on fuzzy logic, just as regular set theory is developed. Fuzzy sets are sets in which ",NA,NA
members are presented as ordered pairs that include information on degree of membership. ,NA,NA
"A traditional set of, say, ",NA,NA
k,NA,NA
" elements, is a special case of a fuzzy set, where each of those ",NA,NA
k ,NA,NA
"elements has 1 for the degree of membership, and every other element in the universal set ",NA,NA
"has a degree of membership 0, for which reason you don’t bother to list it.",NA,NA
Fuzzy Set Operations,NA,NA
The usual operations you can perform on ordinary sets are ,NA,NA
union,NA,NA
", in which you take all the ",NA,NA
elements that are in one set or the other; and ,NA,NA
intersection,NA,NA
", in which you take the elements ",NA,NA
"that are in both sets. In the case of fuzzy sets, taking a union is finding the degree of ",NA,NA
"membership that an element should have in the new fuzzy set, which is the union of two ",NA,NA
fuzzy sets.,NA,NA
If ,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
", and d are such that their degrees of membership in the fuzzy set A are 0.9, 0.4, ",NA,NA
"0.5, and 0, respectively, then the fuzzy set A is given by the ",NA,NA
fit vector,NA,NA
" (0.9, 0.4, 0.5, 0). The ",NA,NA
components of this fit vector are called ,NA,NA
fit values,NA,NA
 of ,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
", and ",NA,NA
d,NA,NA
.,NA,NA
Union of Fuzzy Sets,NA,NA
Consider a union of two traditional sets and an element that belongs to only one of those ,NA,NA
"sets. Earlier you saw that if you treat these sets as fuzzy sets, this element has a degree of ",NA,NA
membership of 1 in one case and 0 in the other since it belongs to one set and not the other. ,NA,NA
Yet you are going to put this element in the union. The criterion you use in this action has ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/031-033.html (2 of 3) [21/11/02 21:56:52],NA
"to do with degrees of membership. You need to look at the two degrees of membership, ",NA,NA
"namely, 0 and 1, and pick the higher value of the two, namely, 1. In other words, what you ",NA,NA
want for the degree of membership of an element when listed in the union of two fuzzy ,NA,NA
"sets, is the maximum value of its degrees of membership within the two fuzzy sets forming ",NA,NA
a union. ,NA,NA
If ,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
", and ",NA,NA
d,NA,NA
" have the respective degrees of membership in fuzzy sets A, B as A = (0.9, ",NA,NA
"0.4, 0.5, 0) and B = (0.7, 0.6, 0.3, 0.8), then A [cup] B = (0.9, 0.6, 0.5, 0.8).",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/031-033.html (3 of 3) [21/11/02 21:56:52]",NA
Previous Table of Contents Next,NA,NA
Intersection and Complement of Two Fuzzy Sets,NA,NA
"Analogously, the degree of membership of an element in the intersection of two fuzzy sets ",NA,NA
is the ,NA,NA
minimum,NA,NA
", or the smaller value of its degree of membership individually in the two ",NA,NA
"sets forming the intersection. For example, if today has 0.8 for degree of membership in the ",NA,NA
set of rainy days and 0.5 for degree of membership in the set of days of work ,NA,NA
"completion, then today belongs to the set of rainy days on which work is completed to a ",NA,NA
"degree of 0.5, the smaller of 0.5 and 0.8.",NA,NA
"Recall the fuzzy sets A and B in the previous example. A = (0.9, 0.4, 0.5, 0) and B = (0.7, ",NA,NA
"0.6, 0.3, 0.8). A[cap]B, which is the intersection of the fuzzy sets A and B, is obtained by ",NA,NA
"taking, in each component, the smaller of the values found in that component in A and in ",NA,NA
"B. Thus A[cap]B = (0.7, 0.4, 0.3, 0).",NA,NA
"The idea of a universal set is implicit in dealing with traditional sets. For example, if you ",NA,NA
"talk of the set of married persons, the universal set is the set of all persons. Every other set ",NA,NA
you consider in that context is a subset of the universal set. We bring up this matter of ,NA,NA
"universal set because when you make the complement of a traditional set A, you need to ",NA,NA
"put in every element in the universal set that is not in A. The complement of a fuzzy set, ",NA,NA
"however, is obtained as follows. In the case of fuzzy sets, if the degree of membership is ",NA,NA
"0.8 for a member, then that member is not in that set to a degree of 1.0 – 0.8 = 0.2. So you ",NA,NA
can set the degree of membership in the complement fuzzy set to the complement with ,NA,NA
"respect to 1. If we return to the scenario of having a degree of 0.8 in the set of rainy days, ",NA,NA
then today has to have 0.2 membership degree in the set of nonrainy or clear days.,NA,NA
"Continuing with our example of fuzzy sets A and B, and denoting the complement of A by ",NA,NA
"A’, we have A’ = (0.1, 0.6, 0.5, 1) and B’ = (0.3, 0.4, 0.7, 0.2). Note that A’ [cup] B’ = ",NA,NA
"(0.3, 0.6, 0.7, 1), which is also the complement of A [cap] B. You can similarly verify that ",NA,NA
"the complement of A [cup] B is the same as A’ [cap] B’. Furthermore, A [cup] A’ = (0.9, ",NA,NA
"0.6, 0.5, 1) and A [cap] A’ = (0.1, 0.4, 0.5, 0), which is not a vector of zeros only, as would ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/033-035.html (1 of 3) [21/11/02 21:56:53],NA
"be the case in conventional sets. In fact, A and A’ will be equal in the sense that their fit ",NA,NA
"vectors are the same, if each component in the fit vector is equal to 0.5.",NA,NA
Applications of Fuzzy Logic,NA,NA
"Applications of fuzzy sets and fuzzy logic are found in many fields, including artificial ",NA,NA
"intelligence, engineering, computer science, operations research, robotics, and pattern ",NA,NA
recognition. These fields are also ripe for applications for neural networks. So it seems ,NA,NA
natural that fuzziness should be introduced in neural networks themselves. Any area where ,NA,NA
"humans need to indulge in making decisions, fuzzy sets can find a place, since information ",NA,NA
on which decisions are to be based may not always be complete and the reliability of the ,NA,NA
supposed values of the underlying parameters is not always certain. ,NA,NA
Examples of Fuzzy Logic,NA,NA
"Let us say five tasks have to be performed in a given period of time, and each task requires ",NA,NA
one person dedicated to it. Suppose there are six people capable of doing these tasks. As ,NA,NA
"you have more than enough people, there is no problem in scheduling this work and getting ",NA,NA
"it done. Of course who gets assigned to which task depends on some criterion, such as total ",NA,NA
"time for completion, on which some optimization can be done. But suppose these six ",NA,NA
people are not necessarily available during the particular period of time in question. ,NA,NA
"Suddenly, the equation is seen in less than crisp terms. The availability of the people is ",NA,NA
fuzzy-valued. Here is an example of an assignment problem where fuzzy sets can be used. ,NA,NA
Commercial Applications,NA,NA
Many commercial uses of fuzzy logic exist today. A few examples are listed here: ,NA,NA
•,NA,NA
"  A subway in Sendai, Japan uses a fuzzy controller to control a subway car. This ",NA,NA
controller has outperformed human and conventional controllers in giving a smooth ,NA,NA
ride to passengers in all terrain and external conditions. ,NA,NA
•,NA,NA
  Cameras and camcorders use fuzzy logic to adjust autofocus mechanisms and to ,NA,NA
cancel the jitter caused by a shaking hand. ,NA,NA
•,NA,NA
  Some automobiles use fuzzy logic for different control applications. Nissan has ,NA,NA
"patents on fuzzy logic braking systems, transmission controls, and fuel injectors. ",NA,NA
GM uses a fuzzy transmission system in its Saturn vehicles. ,NA,NA
•,NA,NA
  FuziWare has developed and patented a fuzzy spreadsheet called ,NA,NA
FuziCalc,NA,NA
 that ,NA,NA
allows users to incorporate fuzziness in their data. ,NA,NA
•,NA,NA
  Software applications to search and match images for certain pixel regions of ,NA,NA
interest have been developed. Avian Systems has a software package called ,NA,NA
FullPixelSearch,NA,NA
. ,NA,NA
•,NA,NA
  A stock market charting and research tool called ,NA,NA
SuperCharts,NA,NA
 from Omega ,NA,NA
"Research, uses fuzzy logic in one of its modules to determine whether the market is ",NA,NA
"bullish, bearish, or neutral. ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/033-035.html (3 of 3) [21/11/02 21:56:53]",NA
Previous Table of Contents Next,NA,NA
Fuzziness in Neural Networks,NA,NA
There are a number of ways fuzzy logic can be used with neural networks. Perhaps the ,NA,NA
simplest way is to use a ,NA,NA
fuzzifier,NA,NA
 function to preprocess or post-process data for a neural ,NA,NA
"network. This is shown in Figure 3.1, where a neural network has a preprocessing fuzzifier ",NA,NA
that converts data into fuzzy data for application to a neural network.,NA,NA
Figure 3.1,NA,NA
  A neural network with fuzzy preprocessor.,NA,NA
Let us build a simple fuzzifier based on an application to predict the direction of the stock ,NA,NA
"market. Suppose that you wish to fuzzify one set of data used in the network, the Federal ",NA,NA
"Reserve’s fiscal policy, in one of four fuzzy categories: very accommodative, ",NA,NA
"accommodative, tight, or very tight. Let us suppose that the raw data that we need to ",NA,NA
fuzzify is the discount rate and the interest rate that the Federal Reserve controls to set the ,NA,NA
"fiscal policy. Now, a low discount rate usually indicates a loose fiscal policy, but this ",NA,NA
"depends not only on the observer, but also on the political climate. There is a probability, ",NA,NA
for a given discount rate that you will find two people who offer different categories for the ,NA,NA
"Fed fiscal policy. Hence, it is appropriate to fuzzify the data, so that the data we present to ",NA,NA
the neural network is like what an observer would see. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/035-037.html (1 of 3) [21/11/02 21:56:54],NA
Figure 3.2 shows the fuzzy categories for different interest rates. Note that the category ,NA,NA
"tight has the largest range. At any given interest rate level, you could have one possible ",NA,NA
"category or several. If only one interest rate is present on the graph, this indicates that ",NA,NA
"membership in that fuzzy set is 1.0. If you have three possible fuzzy sets, there is a ",NA,NA
"requirement that membership add up to 1.0. For an interest rate of 8%, you have some ",NA,NA
chance of finding this in the tight category or the accommodative category. To find out the ,NA,NA
"percentage probability from the graph, take the height of each curve at a given interest rate ",NA,NA
"and normalize this to a one-unit length. At 8%, the tight category is about 0.8 unit in height, ",NA,NA
"and accommodative is about 0.3 unit in height. The total is about 1.1 units, and the ",NA,NA
"probability of the value being tight is then 0.8/1.1 = 0.73, while the probability of the value ",NA,NA
being accommodative is 0.27.,NA,NA
Figure 3.2,NA,NA
  Fuzzy categories for Federal Reserve policy based on the Fed discount rate.,NA,NA
Code for the Fuzzifier,NA,NA
Let’s develop C++ code to create a simple fuzzifier. A class called ,NA,NA
category,NA,NA
 is defined in ,NA,NA
"Listing 3.1. This class encapsulates the data that we need to define, the categories in Figure ",NA,NA
3.2. There are three private data members called ,NA,NA
lowval,NA,NA
", ",NA,NA
"midval,",NA,NA
 and ,NA,NA
highval,NA,NA
. These ,NA,NA
"represent the values on the graph that define the category triangle. In the tight category, the ",NA,NA
"lowval is 5.0, the midval is 8.5, and the highval is 12.0. The category class allows you to ",NA,NA
"instantiate a category object and assign parameters to it to define it. Also, there is a string ",NA,NA
called ,NA,NA
name,NA,NA
" that identifies the category, e.g. “tight.” Various member functions are used to ",NA,NA
interface to the private data members. There is ,NA,NA
"setval(),",NA,NA
" for example, which lets you set the ",NA,NA
"value of the three parameters, while ",NA,NA
gethighval(),NA,NA
 returns the value of the parameter ,NA,NA
highval. The function ,NA,NA
getshare(),NA,NA
 returns the relative value of membership in a category ,NA,NA
"given an input. In the example discussed earlier, with the number 8.0 as the Fed discount ",NA,NA
"rate and the category tight defined according to the graph in Figure 3.2, ",NA,NA
getshare(),NA,NA
 would ,NA,NA
"return 0.8. Note that this is not yet normalized. Following this example, the ",NA,NA
getshare() ,NA,NA
value from the accommodative category would also be used to determine the membership ,NA,NA
weights. These weights define a probability in a given category. A random number ,NA,NA
generator is used to define a value that is used to select a fuzzy category based on the ,NA,NA
probabilities defined.,NA,NA
Listing 3.1,NA,NA
 fuzzfier.h,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/035-037.html (2 of 3) [21/11/02 21:56:54],NA
"// fuzzfier.h V. Rao, H. Rao ",NA,NA
// program to fuzzify data,NA,NA
class category ,NA,NA
{ ,NA,NA
private:,NA,NA
 char name[30];,NA,NA
" float   lowval,highval,midval;",NA,NA
public:,NA,NA
 category(){};,NA,NA
 void setname(char *);,NA,NA
 char * getname();,NA,NA
" void setval(float&,float&,float&); ",NA,NA
float getlowval();,NA,NA
 float getmidval();,NA,NA
 float gethighval();,NA,NA
 float getshare(const float&);,NA,NA
 ~category(){};,NA,NA
};,NA,NA
int randnum(int);,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/035-037.html (3 of 3) [21/11/02 21:56:54]",NA
Previous Table of Contents Next,NA,NA
Let’s look at the implementation file in Listing 3.2. ,NA,NA
Listing 3.2,NA,NA
 fuzzfier.cpp,NA,NA
"// fuzzfier.cpp      V. Rao, H. Rao ",NA,NA
// program to fuzzify data,NA,NA
#include <iostream.h> ,NA,NA
#include <stdlib.h> ,NA,NA
#include <time.h> ,NA,NA
#include <string.h> ,NA,NA
#include <fuzzfier.h>,NA,NA
void category::setname(char *n) ,NA,NA
{ ,NA,NA
"strcpy(name,n); ",NA,NA
},NA,NA
char * category::getname() ,NA,NA
{ ,NA,NA
return name; ,NA,NA
},NA,NA
"void category::setval(float &h, float &m, float &l) ",NA,NA
{ ,NA,NA
highval=h; ,NA,NA
midval=m; ,NA,NA
lowval=l; ,NA,NA
},NA,NA
float category::getlowval(),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (1 of 6) [21/11/02 21:56:55],NA
{ ,NA,NA
return lowval; ,NA,NA
},NA,NA
float category::getmidval() ,NA,NA
{ ,NA,NA
return midval; ,NA,NA
},NA,NA
float category::gethighval() ,NA,NA
{ ,NA,NA
return highval; ,NA,NA
},NA,NA
float category::getshare(const float & input) ,NA,NA
{ ,NA,NA
// this member function returns the relative membership ,NA,NA
"// of an input in a category, with a maximum of 1.0",NA,NA
float output; ,NA,NA
"float midlow, highmid;",NA,NA
midlow=midval-lowval; ,NA,NA
highmid=highval-midval;,NA,NA
"// if outside the range, then output=0 ",NA,NA
if ((input <= lowval) || (input >= highval)),NA,NA
 output=0;,NA,NA
else,NA,NA
 {,NA,NA
 if (input > midval),NA,NA
 output=(highval-input)/highmid;,NA,NA
 else,NA,NA
 if (input==midval),NA,NA
 output=1.0;,NA,NA
 else,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (2 of 6) [21/11/02 21:56:55],NA
 output=(input-lowval)/midlow;,NA,NA
 } ,NA,NA
return output;,NA,NA
},NA,NA
int randomnum(int maxval) ,NA,NA
{ ,NA,NA
// random number generator ,NA,NA
// will return an integer up to maxval,NA,NA
srand ((unsigned)time(NULL)); ,NA,NA
return rand() % maxval; ,NA,NA
},NA,NA
void main() ,NA,NA
{ ,NA,NA
// a fuzzifier program that takes category information: ,NA,NA
"// lowval, midval and highval and category name ",NA,NA
// and fuzzifies an input based on ,NA,NA
// the total number of categories and the membership // ,NA,NA
in each category,NA,NA
"int i=0,j=0,numcat=0,randnum; ",NA,NA
"float l,m,h, inval=1.0;",NA,NA
"char input[30]=""               ""; ",NA,NA
category * ptr[10]; ,NA,NA
float relprob[10]; ,NA,NA
"float total=0, runtotal=0;",NA,NA
//input the category information; terminate with `done';,NA,NA
while (1),NA,NA
 {,NA,NA
" cout << ""\nPlease type in a category name, e.g. Cool\n""; ",NA,NA
"cout << ""Enter one word without spaces\n"";",NA,NA
" cout << ""When you are done, type `done' :\n\n"";",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (3 of 6) [21/11/02 21:56:55],NA
 ptr[i]= new category;,NA,NA
 cin >> input;,NA,NA
 if ((input[0]=='d' && input[1]=='o' &&,NA,NA
 input[2]=='n' && input[3]=='e')) break;,NA,NA
 ptr[i]->setname(input);,NA,NA
" cout << ""\nType in the lowval, midval and highval\n""; ",NA,NA
"cout << ""for each category, separated by spaces\n""; ",NA,NA
"cout << "" e.g. 1.0 3.0 5.0 :\n\n"";",NA,NA
 cin >> l >> m >> h;,NA,NA
" ptr[i]->setval(h,m,l);",NA,NA
 i++;,NA,NA
 },NA,NA
numcat=i; // number of categories,NA,NA
// Categories set up: Now input the data to fuzzify ,NA,NA
"cout <<""\n\n""; ",NA,NA
"cout << ""===================================\n""; ",NA,NA
"cout << ""==Fuzzifier is ready for data==\n""; ",NA,NA
"cout << ""===================================\n"";",NA,NA
while (1),NA,NA
 {,NA,NA
" cout << ""\ninput a data value, type 0 to terminate\n"";",NA,NA
 cin >> inval;,NA,NA
 if (inval == 0) break;,NA,NA
 // calculate relative probabilities of ,NA,NA
//   input being in each category,NA,NA
 total=0;,NA,NA
 for (j=0;j<numcat;j++),NA,NA
 {,NA,NA
 relprob[j]=100*ptr[j]->getshare(inval);,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (4 of 6) [21/11/02 21:56:55],NA
 total+=relprob[j];,NA,NA
 },NA,NA
 if (total==0),NA,NA
 {,NA,NA
" cout << ""data out of range\n""; ",NA,NA
exit(1);,NA,NA
 },NA,NA
 randnum=randomnum((int)total);,NA,NA
 j=0;,NA,NA
 runtotal=relprob[0];,NA,NA
 while ((runtotal<randnum)&&(j<numcat)) ,NA,NA
{,NA,NA
 j++;,NA,NA
 runtotal += relprob[j];,NA,NA
 },NA,NA
" cout << ""\nOutput fuzzy category is ==> "" <<",NA,NA
" ptr[j]->getname()<<""<== \n"";",NA,NA
" cout <<""category\t""<<""membership\n""; ",NA,NA
"cout <<""---------------\n"";",NA,NA
 for (j=0;j<numcat;j++),NA,NA
 {,NA,NA
" cout << ptr[j]->getname()<<""\t\t""<<",NA,NA
" (relprob[j]/total) <<""\n"";",NA,NA
 ,NA,NA
},NA,NA
 },NA,NA
"cout << ""\n\nAll done. Have a fuzzy day !\n"";",NA,NA
},NA,NA
This program first sets up all the categories you define. These could be for the example we ,NA,NA
"choose or any example you can think of. After the categories are defined, you can start ",NA,NA
entering data to be fuzzified. As you enter data you see the probability aspect come into ,NA,NA
"play. If you enter the same value twice, you may end up with different categories! You will ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (5 of 6) [21/11/02 21:56:55],NA
"see sample output shortly, but first a technical note on how the weighted probabilities are ",NA,NA
set up. The best way to explain it is with an example. Suppose that you have defined three ,NA,NA
"categories, A, B, and C. Suppose that category A has a relative membership of 0.8, ",NA,NA
"category B of 0.4, and category C of 0.2. In the program, these numbers are first multiplied ",NA,NA
"by 100, so you end up with A=80, B=40, and C=20. Now these are stored in a vector with ",NA,NA
an index ,NA,NA
j,NA,NA
 initialized to point to the first category. Let’s say that these three numbers ,NA,NA
represent three adjacent number bins that are joined together. Now pick a random number ,NA,NA
"to index into the bin that has its maximum value of (80+40+20). If the number is 100, then ",NA,NA
"it is greater than 80 and less than (80+40), you end up in the second bin that represents B. ",NA,NA
"Does this scheme give you weighted probabilities? Yes it does, since the size of the bin ",NA,NA
(given a uniform distribution of random indexes into it) determines the probability of ,NA,NA
"falling into the bin. Therefore, the probability of falling into bin A is 80/(80+40+20).",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/037-043.html (6 of 6) [21/11/02 21:56:55]",NA
Previous Table of Contents Next,NA,NA
Sample output from the program is shown below. Our input is in italic; computer output is ,NA,NA
not. The categories defined by the graph in Figure 3.2 are entered in this example. Once the ,NA,NA
"categories are set up, the first data entry of 4.0 gets fuzzified to the accommodative ",NA,NA
category. Note that the memberships are also presented in each category. The same value is ,NA,NA
"entered again, and this time it gets fuzzified to the very accommodative category. For the ",NA,NA
"last data entry of 12.5, you see that only the very tight category holds membership for this ",NA,NA
value. In all cases you will note that the memberships add up to 1.0. ,NA,NA
fuzzfier,NA,NA
"Please type in a category name, e.g. Cool ",NA,NA
Enter one word without spaces ,NA,NA
"When you are done, type `done' :",NA,NA
v.accommodative,NA,NA
Type ,NA,NA
in ,NA,NA
the ,NA,NA
"lowval, ",NA,NA
midval ,NA,NA
and ,NA,NA
"highval for each category, separated ",NA,NA
by spaces e.g. 1.0 3.0 5.0 :,NA,NA
0 3 6,NA,NA
"Please type in a category name, e.g. Cool ",NA,NA
Enter one word without spaces ,NA,NA
"When you are done, type `done' :",NA,NA
accommodative,NA,NA
Type ,NA,NA
in ,NA,NA
the ,NA,NA
"lowval, ",NA,NA
midval ,NA,NA
and ,NA,NA
"highval for each category, separated ",NA,NA
by spaces e.g. 1.0 3.0 5.0 :,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/043-048.html (1 of 5) [21/11/02 21:56:57],NA
3 6 9,NA,NA
"Please type in a category name, e.g. Cool ",NA,NA
Enter one word without spaces ,NA,NA
"When you are done, type `done' :",NA,NA
tight,NA,NA
Type ,NA,NA
in ,NA,NA
the ,NA,NA
"lowval, ",NA,NA
midval ,NA,NA
and ,NA,NA
"highval for each category, separated ",NA,NA
by spaces e.g. 1.0 3.0 5.0 :,NA,NA
5 8.5 12,NA,NA
"Please type in a category name, e.g. Cool ",NA,NA
Enter one word without spaces ,NA,NA
"When you are done, type `done' :",NA,NA
v.tight,NA,NA
Type ,NA,NA
in ,NA,NA
the ,NA,NA
"lowval, ",NA,NA
midval ,NA,NA
and ,NA,NA
"highval for each category, separated ",NA,NA
by spaces e.g. 1.0 3.0 5.0 :,NA,NA
10 12 14,NA,NA
"Please type in a category name, e.g. Cool ",NA,NA
Enter one word without spaces ,NA,NA
"When you are done, type `done' :",NA,NA
done,NA,NA
=================================== ,NA,NA
==Fuzzifier is ready for data== ,NA,NA
===================================,NA,NA
"input a data value, type 0 to terminate ",NA,NA
4.0,NA,NA
Output fuzzy category is ==> accommodative<== ,NA,NA
category   membership,NA,NA
-----------------------------,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/043-048.html (2 of 5) [21/11/02 21:56:57],NA
v.accommodative      0.666667 ,NA,NA
accommodative        0.333333 ,NA,NA
tight        0 ,NA,NA
v.tight      0,NA,NA
"input a data value, type 0 to terminate ",NA,NA
4.0,NA,NA
Output fuzzy category is ==> v.accommodative<== ,NA,NA
category   membership,NA,NA
-----------------------------,NA,NA
v.accommodative      0.666667 ,NA,NA
accommodative        0.333333 ,NA,NA
tight        0 ,NA,NA
v.tight      0,NA,NA
"input a data value, type 0 to terminate ",NA,NA
7.5,NA,NA
Output fuzzy category is ==> accommodative<== ,NA,NA
category   membership,NA,NA
-----------------------------,NA,NA
v.accommodative      0 ,NA,NA
accommodative        0.411765 ,NA,NA
tight        0.588235 ,NA,NA
v.tight      0,NA,NA
"input a data value, type 0 to terminate ",NA,NA
11.0,NA,NA
 Output fuzzy category is ==> tight<== ,NA,NA
category   membership,NA,NA
-----------------------------,NA,NA
v.accommodative      0 ,NA,NA
accommodative        0 ,NA,NA
tight        0.363636 ,NA,NA
v.tight      0.636364,NA,NA
"input a data value, type 0 to terminate ",NA,NA
12.5,NA,NA
Output fuzzy category is ==> v.tight<==,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/043-048.html (3 of 5) [21/11/02 21:56:57],NA
category   membership,NA,NA
-----------------------------,NA,NA
v.accommodative      0,NA,NA
accommodative        0,NA,NA
tight        0,NA,NA
v.tight      1,NA,NA
"input a data value, type 0 to terminate",NA,NA
0,NA,NA
All done. Have a fuzzy day !,NA,NA
Fuzzy Control Systems,NA,NA
The most widespread use of fuzzy logic today is in ,NA,NA
fuzzy control,NA,NA
 applications. You can use ,NA,NA
fuzzy logic to make your air conditioner cool your room. Or you can design a subway ,NA,NA
system to use fuzzy logic to control the braking system for smooth and accurate stops. A ,NA,NA
control system is a closed-loop system that typically controls a machine to achieve a ,NA,NA
"particular desired response, given a number of environmental inputs. A fuzzy control ",NA,NA
"system is a closed-loop system that uses the process of fuzzification, as shown in the ",NA,NA
"Federal Reserve policy program example, to generate fuzzy inputs to an ",NA,NA
inference engine,NA,NA
", ",NA,NA
"which is a knowledge base of actions to take. The inverse process, called ",NA,NA
defuzzification,NA,NA
", is ",NA,NA
"also used in a fuzzy control system to create crisp, real values to apply to the machine or ",NA,NA
"process under control. In Japan, fuzzy controllers have been used to control many ",NA,NA
"machines, including washing machines and camcorders.",NA,NA
Figure 3.3 shows a diagram of a fuzzy control system. The major parts of this closed-loop ,NA,NA
system are:,NA,NA
Figure 3.3,NA,NA
  Diagram of a fuzzy control system.,NA,NA
•machine under control,NA,NA
"—this is the machine or process that you are controlling, ",NA,NA
"for example, a washing machine ",NA,NA
•outputs,NA,NA
"—these are the measured response behaviors of your machine, for example, ",NA,NA
the temperature of the water ,NA,NA
•fuzzy outputs,NA,NA
"—these are the same outputs passed through a fuzzifier, for ",NA,NA
"example, hot or very cold ",NA,NA
•inference engine/fuzzy rule base,NA,NA
—an inference engine converts fuzzy outputs to ,NA,NA
actions to take by accessing fuzzy rules in a fuzzy rule base. An example of a fuzzy ,NA,NA
"rule: IF the output is very cold, THEN increase the water temperature setting by a ",NA,NA
very large amount ,NA,NA
•fuzzy inputs,NA,NA
"—these are the fuzzy actions to perform, such as increase the water ",NA,NA
temperature setting by a very large amount ,NA,NA
•inputs,NA,NA
"—these are the (crisp) dials on the machine to control its behavior, for ",NA,NA
"example, water temperature setting = 3.423, converted from fuzzy inputs with a ",NA,NA
defuzzifier ,NA,NA
The key to development of a fuzzy control system is to iteratively construct a fuzzy rule ,NA,NA
base that yields the desired response from your machine. You construct these fuzzy rules ,NA,NA
from knowledge about the problem. In many cases this is very intuitive and gives you a ,NA,NA
robust control system in a very short amount of time. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/043-048.html (5 of 5) [21/11/02 21:56:57]",NA
Previous Table of Contents Next,NA,NA
Fuzziness in Neural Networks,NA,NA
Fuzziness can enter neural networks to define the weights from fuzzy sets. A comparison ,NA,NA
between expert systems and fuzzy systems is important to understand in the context of ,NA,NA
neural networks. Expert systems are based on crisp rules. Such crisp rules may not always ,NA,NA
be available. Expert systems have to consider an exhaustive set of possibilities. Such sets ,NA,NA
"may not be known beforehand. When crisp rules are not possible, and when it is not known ",NA,NA
"if the possibilities are exhaustive, the expert systems approach is not a good one. ",NA,NA
"Some neural networks, through the features of training and learning, can function in the ",NA,NA
presence of unexpected situations. Therein neural networks have an advantage over expert ,NA,NA
"systems, and they can manage with far less information than expert systems need.",NA,NA
One form of fuzziness in neural networks is called a ,NA,NA
fuzzy cognitive map,NA,NA
. A fuzzy ,NA,NA
cognitive map is like a dynamic state machine with fuzzy states. A traditional ,NA,NA
state ,NA,NA
machine,NA,NA
 is a machine with defined states and outputs associated with each state. ,NA,NA
Transitions from state to state take place according to input events or ,NA,NA
stimuli,NA,NA
. A fuzzy ,NA,NA
cognitive map looks like a state machine but has fuzzy states (not just 1 or 0). You have a ,NA,NA
"set of weights along each transition path, and these weights can be learned from a set of ",NA,NA
training data.,NA,NA
Our treatment of fuzziness in neural networks is with the discussion of the fuzzy ,NA,NA
"associative memory, abbreviated as ",NA,NA
FAM,NA,NA
", which, like the fuzzy cognitive map, was ",NA,NA
developed by Bart Kosko. The FAM and the C++ implementation are discussed in Chapter ,NA,NA
9.,NA,NA
Neural-Trained Fuzzy Systems,NA,NA
So far we have considered how fuzzy logic plays a role in neural networks. The converse ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/048-050.html (1 of 3) [21/11/02 21:56:57],NA
"relationship, neural networks in fuzzy systems, is also an active area of research. In order to ",NA,NA
"build a fuzzy system, you must have a set of membership rules for fuzzy categories. It is ",NA,NA
sometimes difficult to deduce these membership rules with a given set of complex data. ,NA,NA
Why not use a neural network to define the fuzzy rules for you? A neural network is good ,NA,NA
at discovering relationships and patterns in data and can be used to preprocess data in a ,NA,NA
"fuzzy system. Further, a neural network that can learn new relationships with new input ",NA,NA
data can be used to refine fuzzy rules to create a fuzzy adaptive system. Neural trained ,NA,NA
"fuzzy systems are being used in many commercial applications, especially in Japan: ",NA,NA
•,NA,NA
  The Laboratory for International Fuzzy Engineering Research (LIFE) in ,NA,NA
"Yokohama, Japan has a backpropagation neural network that derives fuzzy rules and ",NA,NA
membership functions. The LIFE system has been successfully applied to a foreign-,NA,NA
exchange trade support system with approximately 5000 fuzzy rules. ,NA,NA
•,NA,NA
  Ford Motor ,NA,NA
Company has developed trainable fuzzy systems for automobile idle-speed control. ,NA,NA
•,NA,NA
  National Semiconductor Corporation has a software product called ,NA,NA
NeuFuz,NA,NA
 that ,NA,NA
supports the generation of fuzzy rules with a neural network for control ,NA,NA
applications. ,NA,NA
•,NA,NA
  A number of Japanese consumer and industrial products use neural networks with ,NA,NA
"fuzzy systems, including vacuum cleaners, rice cookers, washing machines, and ",NA,NA
photocopying machines. ,NA,NA
•,NA,NA
  AEG Corporation of Germany uses a neural-network-trained fuzzy control system ,NA,NA
for its water- and energy-conserving washing machine. After the machine is loaded ,NA,NA
"with laundry, it measures the water level with a pressure sensor and infers the ",NA,NA
amount of laundry in the machine by the speed and volume of water. A total of 157 ,NA,NA
rules were generated by a neural network that was trained on data correlating the ,NA,NA
amount of laundry with the measurement of water level on the sensor. ,NA,NA
Summary,NA,NA
"In this chapter, you read about fuzzy logic, fuzzy sets, and simple operations on fuzzy sets. ",NA,NA
"Fuzzy logic, unlike Boolean logic, has more than two on or off categories to describe ",NA,NA
"behavior of systems. You use membership values for data in fuzzy categories, which may ",NA,NA
"overlap. In this chapter, you also developed a fuzzifier program in C++ that takes crisp ",NA,NA
"values and converts them to fuzzy values, based on categories and memberships that you ",NA,NA
"define. For use with neural networks, fuzzy logic can serve as a post-processing or pre-",NA,NA
processing filter. Kosko developed neural networks that use fuzziness and called them ,NA,NA
fuzzy associative memories,NA,NA
", which will be discussed in later chapters. You also read about ",NA,NA
how neural networks can be used in fuzzy systems to define membership functions and ,NA,NA
fuzzy rules.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/048-050.html (2 of 3) [21/11/02 21:56:57],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch03/048-050.html (3 of 3) [21/11/02 21:56:57]",NA
Previous Table of Contents Next,NA,NA
Chapter 4 ,NA,NA
Constructing a Neural Network ,NA,NA
First Example for C++ Implementation,NA,NA
The neural network we presented in Chapter 1 is an example of a Hopfield network with a ,NA,NA
single layer. Now we present a C++ implementation of this network. Suppose we place four ,NA,NA
"neurons, all connected to one another on this layer, as shown in Figure 4.1. Some of these ",NA,NA
connections have a positive weight and the rest have a negative weight. You may recall ,NA,NA
"from the earlier presentation of this example, that we used two input patterns to determine ",NA,NA
"the weight matrix. The network recalls them when the inputs are presented to the network, ",NA,NA
one at a time. These inputs are binary and orthogonal so that their stable recall is assured. ,NA,NA
Each component of a binary input pattern is either a 0 or a 1. Two vectors are orthogonal ,NA,NA
when their ,NA,NA
dot product,NA,NA
—the sum of the products of their corresponding components—is ,NA,NA
zero. An example of a binary input pattern is 1 0 1 0 0. An example of a pair of orthogonal ,NA,NA
"vectors is (0, 1, 0, 0, 1) and (1, 0, 0, 1, 0). An example of a pair of vectors that are not ",NA,NA
"orthogonal is (0, 1, 0, 0, 1) and (1, 1, 0, 1, 0). These last two vectors have a dot product of ",NA,NA
"1, different from 0.",NA,NA
Figure 4.1,NA,NA
  Layout of a Hopfield Network,NA,NA
The two patterns we want the network to have stable recall for are ,NA,NA
A,NA,NA
" = (1, 0, 1, 0) and ",NA,NA
B,NA,NA
 = ,NA,NA
"(0, 1, 0, 1). The weight matrix ",NA,NA
W,NA,NA
 is given as follows:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/051-053.html (1 of 3) [21/11/02 21:56:59],NA
 0    -3     3     -3,NA,NA
 W     =  -3     0    -3      3,NA,NA
 3    -3     0     -3,NA,NA
 -3     3    -3      0,"NOTE:  
 The positive links (values with positive signs) tend to encourage agreement in a 
 stable configuration, whereas negative links (values with negative signs) tend to 
 discourage agreement in a stable configuration.",NA
We need a ,NA,NA
threshold,NA,NA
" function also, and we define it using a threshold value, [theta], as ",NA,NA
follows:,NA,NA
 f(t) =  ,NA,NA
{,NA,NA
 1  if t >= [theta],NA,NA
 0  if t < [theta],NA,NA
The threshold value [theta] is used as a cut-off value for the activation of a neuron to enable ,NA,NA
"it to fire. The activation should equal or exceed the threshold value for the neuron to fire, ",NA,NA
"meaning to have output 1. For our Hopfield network, [theta] is taken as 0. There are four ",NA,NA
neurons in the only layer in this network. The first node’s output is the output of the ,NA,NA
threshold,NA,NA
 function. The argument for the ,NA,NA
threshold,NA,NA
 function is the activation of the node. ,NA,NA
And the activation of the node is the dot product of the input vector and the first column of ,NA,NA
"the weight matrix. So if the input vector is A, the dot product becomes 3, and ",NA,NA
f,NA,NA
(3) = 1. And ,NA,NA
"the dot products of the second, third, and fourth nodes become –6, 3, and –6, respectively. ",NA,NA
"The corresponding outputs therefore are 0, 1, and 0. This means that the output of the ",NA,NA
"network is the vector (1, 0, 1, 0), which is the same as the input pattern. Therefore, the ",NA,NA
"network has recalled the pattern as presented. When B is presented, the dot product ",NA,NA
obtained at the first node is –6 and the output is 0. The activations of all the four nodes ,NA,NA
together with the ,NA,NA
threshold,NA,NA
" function give (0, 1, 0, 1) as output from the network, which ",NA,NA
means that the network recalled B as well. The weight matrix worked well with both input ,NA,NA
"patterns, and we do not need to modify it.",NA,NA
Classes in C++ Implementation,NA,NA
"In our C++ implementation of this network, there are the following classes: a ",NA,NA
network ,NA,NA
"class, and a ",NA,NA
neuron,NA,NA
" class. In our implementation, we create the network with four neurons, ",NA,NA
"and these four neurons are all connected to one another. A neuron is not self-connected, ",NA,NA
"though. That is, there is no edge in the directed graph representing the network, where the ",NA,NA
"edge is from one node to itself. But for simplicity, we could pretend that such a connection ",NA,NA
"exists carrying a weight of 0, so that the weight matrix has 0’s in its principal diagonal.",NA,NA
The functions that determine the neuron activations and the network output are declared ,NA,NA
public,NA,NA
. Therefore they are visible and accessible without restriction. The activations of the ,NA,NA
neurons are calculated with functions defined in the neuron class. When there are more than ,NA,NA
"one layer in a neural network, the outputs of neurons in one layer become the inputs for ",NA,NA
neurons in the next layer. In order to facilitate passing the outputs from one layer as inputs ,NA,NA
"to another layer, our C++ implementations compute the neuron outputs in the ",NA,NA
network,NA,NA
 ,NA,NA
class. For this reason the ,NA,NA
threshold,NA,NA
 function is made a member of the ,NA,NA
network ,NA,NA
class. We ,NA,NA
"do this for the Hopfield network as well. To see if the network has achieved correct recall, ",NA,NA
"you make comparisons between the presented pattern and the network output, component ",NA,NA
by component.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/051-053.html (3 of 3) [21/11/02 21:56:59]",NA
C++ Program for a Hopfield Network,"For convenience every C++ program has two components: One is the header file with all of the class 
 declarations and lists of include library files; the other is the source file that includes the header file and the 
 detailed descriptions of the member functions of the classes declared in the header file. You also put the 
 function 
 main
  in the source file. Most of the computations are done by class member functions, when class 
 objects are created in the function 
 main
 , and calls are made to the appropriate functions. The header file has an 
 .
 h
  (or 
 .hpp
 ) extension, as you know, and the source file has a 
 .cpp
  extension, to indicate that it is a C++ code 
 file. It is possible to have the contents of the header file written at the beginning of the 
 .cpp
  file and work with 
 one file only, but separating the declarations and implementations into two files allows you to change the 
 implementation of a class(
 .cpp
 ) without changing the interface to the class (
 .h
 ).
  
 Header File for C++ Program for Hopfield Network
  
 Listing 4.1 contains Hop.h, the header file for the C++ program for the Hopfield network. The include files 
 listed in it are the stdio.h, iostream.h, and math.h. The iostream.h file contains the declarations and details of 
 the C++ streams for input and output. A 
 network
  class and a 
 neuron
  class, are declared in Hop.h. The data 
 members and member functions are declared within each class, and their accessibility is specified by the 
 keywords 
 protected
  or 
 public
 .
  
 Listing 4.1
  Header file for C++ program for Hopfield network.
  
 //Hop.h      V. Rao, H. Rao 
  
 //Single layer Hopfield Network with 4 neurons
  
 #include <stdio.h> 
  
 #include <iostream.h> 
  
 #include <math.h>
  
 class neuron 
  
 { 
  
 protected:
  
  
  int activation;
  
  
  friend class network; 
  
 public:
  
  
  int weightv[4];
  
  
  neuron() {};
  
  
  neuron(int *j) ;
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/053-058.html (1 of 4) [21/11/02 21:56:59]",NA
Previous Table of Contents Next,NA,NA
Comments on the C++ Program for Hopfield Network,NA,NA
Note the use of the output stream operator ,NA,NA
cout<<,NA,NA
 to output text strings or numerical output. C++ has ,NA,NA
istream,NA,NA
 and ,NA,NA
ostream,NA,NA
 classes from which the ,NA,NA
iostream,NA,NA
 class is derived. The standard input and output ,NA,NA
streams are ,NA,NA
cin,NA,NA
 and ,NA,NA
cout,NA,NA
", respectively, used, correspondingly, with the operators >> and <<. Use of ",NA,NA
cout ,NA,NA
for the output stream is much simpler than the use of the C function ,NA,NA
printf,NA,NA
". As you can see, there is no ",NA,NA
"formatting suggested for output. However, there is a provision that allows you to format the output, while ",NA,NA
using ,NA,NA
cout,NA,NA
.,NA,NA
Also note the way comments are introduced in the program. The line with comments should start with a ,NA,NA
"double slash //. Unlike C, the comment does not have to end with a double slash. Of course, if the ",NA,NA
"comments extend to subsequent lines, each such line should have a double slash at the start. You can still ",NA,NA
"use the pair, /* at the beginning with */ at the end of lines of comments, as you do in C. If the comment ",NA,NA
"continues through many lines, the C facility will be handier to delimit the comments.",NA,NA
The neurons in the network are members of the network class and are identified by the abbreviation ,NA,NA
nrn,NA,NA
. ,NA,NA
"The two patterns, 1010 and 0101, are presented to the network one at a time in the program.",NA,NA
Output from the C++ Program for Hopfield Network,NA,NA
"The output from this program is as follows and is self-explanatory. When you run this program, you’re ",NA,NA
"likely to see a lot of output whiz by, so in order to leisurely look at the output, use redirection. Type ",NA,NA
Hop ,NA,NA
> filename,NA,NA
", and your output will be stored in a file, which you can edit with any text editor or list by ",NA,NA
using the ,NA,NA
type filename | more,NA,NA
 command.,NA,NA
THIS PROGRAM IS FOR A HOPFIELD NETWORK WITH A SINGLE LAYER OF 4 FULLY,NA,NA
INTERCONNECTED NEURONS. THE NETWORK SHOULD RECALL THE PATTERNS 1010 AND,NA,NA
0101 CORRECTLY.,NA,NA
 nrn[0].weightv[0] is  0,NA,NA
 nrn[0].weightv[1] is  -3,NA,NA
 nrn[0].weightv[2] is  3,NA,NA
 nrn[0].weightv[3] is  -3,NA,NA
activation is 3,NA,NA
output value is  1,NA,NA
 nrn[1].weightv[0] is  -3,NA,NA
 nrn[1].weightv[1] is  0,NA,NA
 nrn[1].weightv[2] is  -3,NA,NA
 nrn[1].weightv[3] is  3 ,NA,NA
activation is -6 ,NA,NA
output value is  0,NA,NA
 nrn[2].weightv[0] is  3,NA,NA
 nrn[2].weightv[1] is  -3,NA,NA
 nrn[2].weightv[2] is  0,NA,NA
 nrn[2].weightv[3] is  -3 ,NA,NA
activation is 3 ,NA,NA
output value is  1,NA,NA
 nrn[3].weightv[0] is  -3,NA,NA
 nrn[3].weightv[1] is  3,NA,NA
 nrn[3].weightv[2] is  -3,NA,NA
 nrn[3].weightv[3] is  0 ,NA,NA
activation is -6 ,NA,NA
output value is  0,NA,NA
 pattern= 1  output = 1  component matches ,NA,NA
pattern= 0  output = 0  component matches ,NA,NA
pattern= 1  output = 1  component matches ,NA,NA
pattern= 0  output = 0  component matches,NA,NA
 nrn[0].weightv[0] is  0,NA,NA
 nrn[0].weightv[1] is  -3,NA,NA
 nrn[0].weightv[2] is  3,NA,NA
 nrn[0].weightv[3] is  -3 ,NA,NA
activation is -6 ,NA,NA
output value is  0,NA,NA
 nrn[1].weightv[0] is  -3,NA,NA
 nrn[1].weightv[1] is  0,NA,NA
 nrn[1].weightv[2] is  -3,NA,NA
 nrn[1].weightv[3] is  3 ,NA,NA
activation is 3 ,NA,NA
output value is  1,NA,NA
 nrn[2].weightv[0] is  3,NA,NA
 nrn[2].weightv[1] is  -3,NA,NA
 nrn[2].weightv[2] is  0,NA,NA
 nrn[2].weightv[3] is  -3 ,NA,NA
activation is -6 ,NA,NA
output value is  0,NA,NA
 nrn[3].weightv[0] is  -3,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/058-061.html (2 of 3) [21/11/02 21:57:00],NA
 nrn[3].weightv[1] is  3,NA,NA
 nrn[3].weightv[2] is  -3,NA,NA
 nrn[3].weightv[3] is  0 ,NA,NA
activation is 3 ,NA,NA
output value is  1,NA,NA
 pattern= 0  output = 0  component matches ,NA,NA
pattern= 1  output = 1  component matches ,NA,NA
pattern= 0  output = 0  component matches ,NA,NA
pattern= 1  output = 1  component matches,NA,NA
Further Comments on the Program and Its Output,NA,NA
Let us recall our previous discussion of this example in Chapter 1. What does the network give as output if ,NA,NA
"we present a pattern different from both A and B? If C = (0, 1, 0, 0) is the input pattern, the activation (dot ",NA,NA
"products) would be –3, 0, –3, 3 making the outputs (next state) of the neurons 0,1,0,1, so that B would be ",NA,NA
"recalled. This is quite interesting, because if we intended to input B, and we made a slight error and ended ",NA,NA
"up presenting C instead, the network would recall B. You can run the program by changing the pattern to 0, ",NA,NA
"1, 0, 0 and compiling again, to see that the B pattern is recalled. ",NA,NA
Another element about the example in Chapter 1 is that the weight matrix W is not the only weight matrix ,NA,NA
that would enable the network to recall the patterns A and B correctly. If we replace the 3 and –3 in the ,NA,NA
"matrix with 2 and –2, respectively, the resulting matrix would facilitate the same performance from the ",NA,NA
"network. One way for you to check this is to change the wt1, wt2, wt3, wt4 given in the program ",NA,NA
"accordingly, and compile and run the program again. The reason why both of the weight matrices work is ",NA,NA
"that they are closely related. In fact, one is a scalar (constant) multiple of the other, that is, if you multiply ",NA,NA
"each element in the matrix by the same scalar, namely 2/3, you get the corresponding matrix in cases where ",NA,NA
"3 and –3 are replaced with 2 and –2, respectively.",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/058-061.html (3 of 3) [21/11/02 21:57:00]",NA
Previous Table of Contents Next,NA,NA
A New Weight Matrix to Recall More Patterns,NA,NA
Let’s continue to discuss this example. Suppose we are interested in having the patterns E = ,NA,NA
"(1, 0, 0, 1) and F = (0, 1, 1, 0) also recalled correctly, in addition to the patterns A and B. ",NA,NA
"In this case we would need to train the network and come up with a learning algorithm, ",NA,NA
which we will discuss in more detail later in the book. We come up with the matrix ,NA,NA
W,NA,NA
"1, ",NA,NA
which follows.,NA,NA
 0    -5     4    4,NA,NA
 W,1,NA
 =   -5     0     4    4,NA,NA
 4     4     0   -5,NA,NA
 4     4    -5    0,NA,NA
"Try to use this modification of the weight matrix in the source program, and then compile ",NA,NA
"and run the program to see that the network successfully recalls all four patterns A, B, E, ",NA,NA
and F. ,"NOTE:  
 The C++ implementation shown does not include the asynchronous update 
 feature mentioned in Chapter 1, which is not necessary for the patterns presented. The 
 coding of this feature is left as an exercise for the reader.",NA
Weight Determination,NA,NA
You may be wondering about how these weight matrices were developed in the previous ,NA,NA
"example, since so far we’ve only discussed how the network does its job, and how to ",NA,NA
implement the model. You have learned that the choice of weight matrix is not necessarily ,NA,NA
unique. But you want to be assured that there is some established way besides trial and ,NA,NA
"error, in which to construct a weight matrix. You can go about this in the following way. ",NA,NA
Binary to Bipolar Mapping,NA,NA
Let’s look at the previous example. You have seen that by replacing each 0 in a binary ,NA,NA
"string with a –1, you get the corresponding bipolar string. If you keep all 1’s the same and ",NA,NA
"replace each 0 with a –1, you will have a formula for the above option. You can apply the ",NA,NA
following function to each bit in the string: ,NA,NA
 f(x) = 2x – 1,"NOTE:  
 When you give the binary bit 
 x
 , you get the corresponding bipolar character 
 f
 (
 x
 )",NA
"For inverse mapping, which turns a bipolar string into a binary string, you use the ",NA,NA
following function: ,NA,NA
 f(x) =  (x + 1) / 2,"NOTE:  
 When you give the bipolar character 
 x
 , you get the corresponding binary bit 
 f
 (
 x
 )",NA
Pattern’s Contribution to Weight,NA,NA
"Next, we work with the bipolar versions of the input patterns. You take each pattern to be ",NA,NA
"recalled, one at a time, and determine its contribution to the weight matrix of the network. ",NA,NA
The contribution of each pattern is itself a matrix. The size of such a matrix is the same as ,NA,NA
"the weight matrix of the network. Then add these contributions, in the way matrices are ",NA,NA
"added, and you end up with the weight matrix for the network, which is also referred to as ",NA,NA
the ,NA,NA
correlation matrix.,NA,NA
" Let us find the contribution of the pattern A = (1, 0, 1, 0):",NA,NA
"First, we notice that the binary to bipolar mapping of A = (1, 0, 1, 0) gives the vector (1, ",NA,NA
"–1, 1, –1).",NA,NA
"Then we take the transpose, and multiply, the way matrices are multiplied, and we see the ",NA,NA
following:,NA,NA
 1  [1   -1   1   -1]       1   -1   1   -,NA,NA
1 1                     =   -1    1  -1    ,NA,NA
1 1                          1   -1   1   ,NA,NA
-1 1                         -1    1  -1    ,NA,NA
1,NA,NA
Now subtract 1 from each element in the main diagonal (that runs from top left to bottom ,NA,NA
right). This operation gives the same result as subtracting the identity matrix from the given ,NA,NA
"matrix, obtaining 0’s in the main diagonal. The resulting matrix, which is given next, is the ",NA,NA
"contribution of the pattern (1, 0, 1, 0) to the weight matrix. ",NA,NA
 0      -1      1     -1,NA,NA
 -1       0     -1      1,NA,NA
 1      -1      0     -1,NA,NA
 -1       1     -1      0,NA,NA
"Similarly, we can calculate the contribution from the pattern B = (0, 1, 0, 1) by verifying ",NA,NA
"that pattern B’s contribution is the same matrix as pattern A’s contribution. Therefore, the ",NA,NA
matrix of weights for this exercise is the matrix ,NA,NA
W,NA,NA
 shown here.,NA,NA
 0     -2      2      -2,NA,NA
 W  =    -2      0     -2       2,NA,NA
 2     -2      0      -2,NA,NA
 -2      2     -2       0,NA,NA
You can now optionally apply an arbitrary scalar multiplier to all the entries of the matrix if ,NA,NA
you wish. This is how we had previously obtained the +/- 3 values instead of +/- 2 values ,NA,NA
shown above. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Autoassociative Network,NA,NA
The Hopfield network just shown has the feature that the network associates an input ,NA,NA
pattern with itself in recall. This makes the network an ,NA,NA
autoassociative,NA,NA
 network. The ,NA,NA
patterns used for determining the proper weight matrix are also the ones that are ,NA,NA
autoassociatively recalled. These patterns are called the ,NA,NA
exemplars,NA,NA
. A pattern other than an ,NA,NA
"exemplar may or may not be recalled by the network. Of course, when you present the ",NA,NA
"pattern 0 0 0 0, it is stable, even though it is not an exemplar pattern.",NA,NA
Orthogonal Bit Patterns,NA,NA
You may be wondering how many patterns the network with four nodes is able to recall. ,NA,NA
Let us first consider how many different bit patterns are orthogonal to a given bit pattern. ,NA,NA
This question really refers to bit patterns in which at least one bit is equal to 1. A little ,NA,NA
"reflection tells us that if two bit patterns are to be orthogonal, they cannot both have 1’s in ",NA,NA
"the same position, since the dot product would need to be 0. In other words, a bitwise ",NA,NA
logical ,NA,NA
AND,NA,NA
 operation of the two bit patterns has to result in a 0. This suggests the ,NA,NA
following. If a pattern P has ,NA,NA
k,NA,NA
", less than 4, bit positions with 0 (and so 4-",NA,NA
k,NA,NA
 bit positions ,NA,NA
"with 1), and if pattern Q is to be orthogonal to P, then Q can have 0 or 1 in those ",NA,NA
k ,NA,NA
"positions, but it must have only 0 in the rest 4-",NA,NA
k,NA,NA
 positions. Since there are two choices for ,NA,NA
each of the ,NA,NA
k,NA,NA
" positions, there are 2",k,NA
 possible patterns orthogonal to P. This number 2,k,NA
 of ,NA,NA
patterns includes the pattern with all zeroes. So there really are 2,k,NA
–1 non-zero patterns ,NA,NA
orthogonal to P. Some of these 2,k,NA
–1 patterns are not orthogonal to each other. As an ,NA,NA
"example, P can be the pattern 0 1 0 0, which has ",NA,NA
k,NA,NA
 = 3 positions with 0. There are 2,3,NA
–1=7 ,NA,NA
"nonzero patterns orthogonal to 0 1 0 0. Among these are patterns 1 0 1 0 and 1 0 0 1, ",NA,NA
"which are not orthogonal to each other, since their dot product is 1 and not 0.",NA,NA
Network Nodes and Input Patterns,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/064-065.html (1 of 2) [21/11/02 21:57:02],NA
"Since our network has four neurons in it, it also has four nodes in the ",NA,NA
directed graph,NA,NA
 that ,NA,NA
represents the network. These are laterally connected because connections are established ,NA,NA
from node to node. They are lateral because the nodes are all in the same layer. We started ,NA,NA
"with the patterns A = (1, 0, 1, 0) and B = (0, 1, 0, 1) as the exemplars. If we take any other ",NA,NA
"nonzero pattern that is orthogonal to A, it will have a 1 in a position where B also has a 1. ",NA,NA
"So the new pattern will not be orthogonal to B. Therefore, the orthogonal set of patterns ",NA,NA
that contains A and B can have only those two as its elements. If you remove B from the ,NA,NA
"set, you can get (at most) two others to join A to form an orthogonal set. They are the ",NA,NA
"patterns (0, 1, 0, 0) and (0, 0, 0, 1).",NA,NA
"If you follow the procedure described earlier to get the correlation matrix, you will get the ",NA,NA
following weight matrix:,NA,NA
 0     -1      3      -1,NA,NA
W,NA,NA
  =   -1      0     -1      -1,NA,NA
 3     -1      0      -1,NA,NA
 -1     -1     -1       0,NA,NA
"With this matrix, pattern A is recalled, but the zero pattern (0, 0, 0, 0) is obtained for the ",NA,NA
"two patterns (0, 1, 0, 0) and (0, 0, 0, 1). Once the zero pattern is obtained, its own recall ",NA,NA
will be stable. ,NA,NA
Second Example for C++ Implementation,NA,NA
Recall the cash register game from the show ,NA,NA
The Price is Right,NA,NA
", used as one of the ",NA,NA
examples in Chapter 1. This example led to the description of the Perceptron neural ,NA,NA
network. We will now resume our discussion of the Perceptron model and follow up with ,NA,NA
its C++ implementation. Keep the cash register game example in mind as you read the ,NA,NA
following C++ implementation of the Perceptron model. Also note that the input signals in ,NA,NA
"this example are not necessarily binary, but they may be real numbers. It is because the ",NA,NA
prices of the items the contestant has to choose are real numbers (dollars and cents). A ,NA,NA
Perceptron has one layer of input neurons and one layer of output neurons. Each input layer ,NA,NA
neuron is connected to each neuron in the output layer.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/064-065.html (2 of 2) [21/11/02 21:57:02]",NA
Previous Table of Contents Next,NA,NA
Comments on Your C++ Program,NA,NA
Notice the use of input stream operator ,NA,NA
cin>>,NA,NA
" in the C++ program, instead of the C function ",NA,NA
scanf,NA,NA
 in ,NA,NA
several places. The ,NA,NA
iostream,NA,NA
 class in C++ was discussed earlier in this chapter. The program works ,NA,NA
like this:,NA,NA
"First, the network input neurons are given their connection weights, and then an input vector is presented ",NA,NA
"to the input layer. A threshold value is specified, and the output neuron does the weighted sum of its ",NA,NA
"inputs, which are the outputs of the input layer neurons. This weighted sum is the activation of the output ",NA,NA
"neuron, and it is compared with the threshold value, and the output neuron fires (output is 1) if the ",NA,NA
threshold value is not greater than its activation. It does not fire (output is 0) if its activation is smaller ,NA,NA
"than the threshold value. In this implementation, neither supervised nor unsupervised training is ",NA,NA
incorporated.,NA,NA
Input/Output for percept.cpp,NA,NA
"There are two data files used in this program. One is for setting up the weights, and the other for setting ",NA,NA
"up the input vectors. On the command line, you enter the program name followed by the weight file ",NA,NA
name and the input file name. For this discussion (also on the accompanying disk for this book) create a ,NA,NA
"file called weight.dat, which contains the following data: ",NA,NA
 2.0 3.0 3.0 2.0,NA,NA
 3.0 0.0 6.0 2.0,NA,NA
These are two weight vectors. Create also an input file called input.dat with the two data vectors ,NA,NA
below: ,NA,NA
 1.95 0.27 0.69 1.25,NA,NA
 0.30 1.05 0.75 0.19,NA,NA
"During the execution of the program, you are first prompted for the number of vectors that are used (in ",NA,NA
"this case, 2), then for a threshold value for the input/weight vectors (use 7.0 in both cases). You will then ",NA,NA
see the following output. Note that the user input is in italic. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/071-075.html (1 of 5) [21/11/02 21:57:04],NA
 percept weight.dat input.dat,NA,NA
THIS PROGRAM IS FOR A PERCEPTRON NETWORK WITH AN INPUT LAYER OF 4 ,NA,NA
"NEURONS, EACH CONNECTED TO THE OUTPUT NEURON.",NA,NA
THIS EXAMPLE TAKES REAL NUMBERS AS INPUT SIGNALS ,NA,NA
please enter the number of weights/vectors,NA,NA
2 ,NA,NA
this is vector # 1 ,NA,NA
"please enter a threshold value, eg 7.0 ",NA,NA
7.0,NA,NA
weight for neuron 1 is  2           activation is 3.9 ,NA,NA
weight for neuron 2 is  3           activation is 0.81 ,NA,NA
weight for neuron 3 is  3           activation is 2.07 ,NA,NA
weight for neuron 4 is  2           activation is 2.5,NA,NA
activation is  9.28,NA,NA
the output neuron activation exceeds the threshold value of 7 ,NA,NA
output value is 1,NA,NA
this is vector # 2 ,NA,NA
"please enter a threshold value, eg 7.0 ",NA,NA
7.0,NA,NA
weight for neuron 1 is  3           activation is 0.9 ,NA,NA
weight for neuron 2 is  0           activation is 0 ,NA,NA
weight for neuron 3 is  6           activation is 4.5 ,NA,NA
weight for neuron 4 is  2           activation is 0.38,NA,NA
activation is  5.78 ,NA,NA
the output neuron activation is smaller than the threshold value of 7 ,NA,NA
output value is 0,NA,NA
"Finally, try adding a data vector of (1.4, 0.6, 0.35, 0.99) to the data file. Add a weight vector of ( 2, 6, 8, ",NA,NA
3) to the weight file and use a threshold value of 8.25 to see the result. You can use other values to ,NA,NA
experiment also. ,NA,NA
Network Modeling,NA,NA
"So far, we have considered the construction of two networks, the Hopfield memory and the Perceptron. ",NA,NA
What are other considerations (which will be discussed in more depth in the chapters to follow) that you ,NA,NA
should keep in mind ? ,NA,NA
Some of the considerations that go into the modeling of a neural network for an application are:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/071-075.html (2 of 5) [21/11/02 21:57:04],NA
nature of inputs,NA,NA
 fuzzy,NA,NA
 binary,NA,NA
 analog,NA,NA
 crisp,NA,NA
 binary,NA,NA
 analog,NA,NA
number of inputs,NA,NA
nature of outputs,NA,NA
 fuzzy,NA,NA
 binary,NA,NA
 analog,NA,NA
 crisp,NA,NA
 binary,NA,NA
 analog,NA,NA
number of outputs,NA,NA
nature of the application,NA,NA
 to complete patterns (recognize corrupted patterns) ,NA,NA
to classify patterns,NA,NA
 to do an optimization,NA,NA
 to do approximation,NA,NA
 to perform data clustering,NA,NA
 to compute functions,NA,NA
dynamics,NA,NA
adaptive,NA,NA
learning,NA,NA
training,NA,NA
 with exemplars,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/071-075.html (3 of 5) [21/11/02 21:57:04],NA
 without exemplars ,NA,NA
self-organizing ,NA,NA
nonadaptive ,NA,NA
learning ,NA,NA
training,NA,NA
 with exemplars,NA,NA
 without exemplars ,NA,NA
self-,NA,NA
organizing,NA,NA
hidden layers,NA,NA
number,NA,NA
 fixed,NA,NA
 variable ,NA,NA
sizes,NA,NA
 fixed,NA,NA
 variable ,NA,NA
processing,NA,NA
 additive,NA,NA
 multiplicative ,NA,NA
hybrid,NA,NA
 additive and multiplicative ,NA,NA
combining other approaches,NA,NA
 expert systems,NA,NA
 genetic algorithms,NA,NA
"Hybrid models, as indicated above, could be of the variety of combining neural network approach with ",NA,NA
expert system methods or of combining additive and multiplicative processing paradigms. ,NA,NA
Decision support systems are amenable to approaches that combine neural networks with expert ,NA,NA
systems. An example of a hybrid model that combines different modes of processing by neurons is the ,NA,NA
Sigma Pi,NA,NA
" neural network, wherein one layer of neurons uses summation in aggregation and the next ",NA,NA
layer of neurons uses multiplicative processing.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/071-075.html (4 of 5) [21/11/02 21:57:04],NA
"A hidden layer, if only one, in a neural network is a layer of neurons that operates in between the input ",NA,NA
layer and the output layer of the network. Neurons in this layer receive inputs from those in the input ,NA,NA
layer and supply their outputs as the inputs to the neurons in the output layer. When a hidden layer ,NA,NA
"comes in between other hidden layers, it receives input and supplies input to the respective hidden layers.",NA,NA
"In modeling a network, it is often not easy to determine how many, if any, hidden layers, and of what ",NA,NA
"sizes, are needed in the model. Some approaches, like genetic algorithms—which are paradigms ",NA,NA
"competing with neural network approaches in many situations but nevertheless can be cooperative, as ",NA,NA
"here—are at times used to make a determination on the needed or optimum, as the case may be, ",NA,NA
"numbers of hidden layers and/or the neurons in those hidden layers. In what follows, we outline one ",NA,NA
such application.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/071-075.html (5 of 5) [21/11/02 21:57:04]",NA
Previous Table of Contents Next,NA,NA
Tic-Tac-Toe Anyone?,NA,NA
David Fogel describes evolutionary general problem solving and uses the familiar game of ,NA,NA
Tic-Tac-Toe as an example. The idea is to come up with optimal strategies in playing this ,NA,NA
"game. The first player’s marker is an X, and the second player’s marker is an O. Whoever ",NA,NA
gets three of his or her markers in a row or a column or a diagonal before the other player ,NA,NA
"does, wins. Shrewd players manage a draw position, if their equally shrewd opponent ",NA,NA
thwarts their attempts to win. A draw position is one where neither player has three of his ,NA,NA
"or her markers in a row, or a column, or a diagonal. ",NA,NA
"The board can be described by a vector of nine components, each of which is a three-valued ",NA,NA
number. Imagine the squares of the board for the game as taken in sequence row by row ,NA,NA
"from top to bottom. Allow a 1 to show the presence of an X in that square, a 0 to indicate a ",NA,NA
"blank there, and a -1 to correspond to an O. This is an example of a coding for the status of ",NA,NA
"the board. For example, (-1, 0, 1, 0, -1, 0, 1, 1, -1) is a winning position for the second ",NA,NA
"player, because it corresponds to the board looking as below.",NA,NA
 O       X,NA,NA
 O,NA,NA
 X   X   O,NA,NA
"A neural network for this problem will have an input layer with nine neurons, as each input ",NA,NA
pattern has nine components. There would be some hidden layers. But the example is with ,NA,NA
"one hidden layer. The output layer also contains nine neurons, so that one cycle of ",NA,NA
"operation of the network shows what the best configuration of the board is to be, given a ",NA,NA
"particular input. Of course, during this cycle of operation, all that needs to be determined is ",NA,NA
"which blank space, indicated by a 0 in the input, should be changed to 1, if strategy is being ",NA,NA
worked for player 1. None of the 1’s and -1’s is to be changed. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/076-077.html (1 of 3) [21/11/02 21:57:04],NA
"In this particular example, the neural network architecture itself is dynamic. The network ",NA,NA
"expands or contracts according to some rules, which are described next.",NA,NA
Fogel describes the network as an evolving network in the sense that the number of neurons ,NA,NA
in the hidden layer changed with a probability of 0.5. A node was equally likely to be added ,NA,NA
"or deleted. Since the number of unmarked squares dwindles after each play, this kind of ",NA,NA
"approach with varying numbers of neurons in the network seems to be reasonable, and ",NA,NA
interesting.,NA,NA
"The initial set of weights is random values between -0.5 and 0.5, inclusive, according to a ",NA,NA
uniform distribution. Bias and threshold values also come from this distribution. The ,NA,NA
sigmoid function:,NA,NA
1/(1 + ,NA,NA
e,-x,NA
),NA,NA
"is used also, to determine the outputs. ",NA,NA
"Weights and biases were changed during the network operation training cycles. Thus, the ",NA,NA
network had a learning phase. (You will read more on learning in Chapter 6.) This network ,NA,NA
"is adaptive, since it changes its architecture. Other forms of adaptation in neural networks ",NA,NA
are in changing parameter values for a fixed architecture. (See Chapter 6.) The results of ,NA,NA
"the experiment Fogel describes show that you need nine neurons in the hidden layer also, ",NA,NA
for your network to be the best for this problem. They also purged any strategy that was ,NA,NA
likely to lose.,NA,NA
Fogel’s emphasis is on the evolutionary aspect of an adaptive process or experiment. Our ,NA,NA
interest in this example is primarily due to the fact that an adaptive neural network is used.,NA,NA
"The choice of Tic-Tac-Toe, while being a simple and all too familiar game, is in the genre ",NA,NA
of much more complicated games. These games ask a player to place a marker in some ,NA,NA
"position in a given array, and as players take turns doing so, some criterion determines if it ",NA,NA
"is a draw, or who won. Unlike in Tic-Tac-Toe, the criterion by which one wins may not be ",NA,NA
known to the players.,NA,NA
Stability and Plasticity,NA,NA
We discuss now a few other considerations in neural network modeling by introducing ,NA,NA
short-term memory and long-term memory concepts. Neural network training is usually ,NA,NA
done in an ,NA,NA
iterative,NA,NA
" way, meaning that the procedure is repeated a certain number of times. ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/076-077.html (2 of 3) [21/11/02 21:57:04],NA
These iterations are referred to as ,NA,NA
cycles.,NA,NA
" After each cycle, the input used may remain the ",NA,NA
"same or change, or the weights may remain the same or change. Such change is based on ",NA,NA
"the output of a completed cycle. If the number of cycles is not preset, and the network is ",NA,NA
"allowed to go through cycles until some other criterion is met, the question of whether or ",NA,NA
"not the termination of the iterative process occurs eventually, arises naturally.",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/076-077.html (3 of 3) [21/11/02 21:57:04]",NA
Previous Table of Contents Next,NA,NA
Stability for a Neural Network,NA,NA
Stability,NA,NA
 refers to such convergence that facilitates an end to the iterative process. For ,NA,NA
"example, if any two consecutive cycles result in the same output for the network, then there ",NA,NA
"may be no need to do more iterations. In this case, convergence has occurred, and the ",NA,NA
"network has stabilized in its operation. If weights are being modified after each cycle, then ",NA,NA
convergence of weights would constitute stability for the network.,NA,NA
"In some situations, it takes many more iterations than you desire, to have output in two ",NA,NA
consecutive cycles to be the same. Then a tolerance level on the convergence criterion can ,NA,NA
"be used. With a tolerance level, you accomplish early but satisfactory termination of the ",NA,NA
operation of the network.,NA,NA
Plasticity for a Neural Network,NA,NA
"Suppose a network is trained to learn some patterns, and in this process the weights are ",NA,NA
adjusted according to an algorithm. After learning these patterns and encountering a new ,NA,NA
"pattern, the network may modify the weights in order to learn the new pattern. But what if ",NA,NA
the new weight structure is not responsive to the new pattern? Then the network does not ,NA,NA
possess ,NA,NA
plasticity,NA,NA
—the ability to deal satisfactorily with new short-term memory (STM) ,NA,NA
while retaining long-term memory (LTM). Attempts to endow a network with plasticity ,NA,NA
may have some adverse effects on the stability of your network.,NA,NA
Short-Term Memory and Long-Term Memory,NA,NA
We alluded to short-term memory (STM) and long-term memory (LTM) in the previous ,NA,NA
paragraph. STM is basically the information that is currently and perhaps temporarily ,NA,NA
"being processed. It is manifested in the patterns that the network encounters. LTM, on the ",NA,NA
"other hand, is information that is already stored and is not being currently processed. In a ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/077-080.html (1 of 2) [21/11/02 21:57:05],NA
"neural network, STM is usually characterized by patterns and LTM is characterized by the ",NA,NA
connections’ weights. The weights determine how an input is processed in the network to ,NA,NA
"yield output. During the cycles of operation of a network, the weights may change. After ",NA,NA
"convergence, they represent LTM, as the weight levels achieved are stable. ",NA,NA
Summary,NA,NA
"You saw in this chapter, the C++ implementations of a simple Hopfield network and of a ",NA,NA
simple Perceptron network. What have not been included in them is an automatic iteration ,NA,NA
and a learning algorithm. They were not necessary for the examples that were used in this ,NA,NA
"chapter to show C++ implementation, the emphasis was on the method of implementation. ",NA,NA
"In a later chapter, you will read about the learning algorithms and examples of how to ",NA,NA
implement some of them. ,NA,NA
Considerations in modeling a neural network are presented in this chapter along with an ,NA,NA
outline of how Tic-Tac-Toe is used as an example of an adaptive neural network model.,NA,NA
"You also were introduced to the following concepts: stability, plasticity, short-term ",NA,NA
"memory, and long-term memory (discussed further in later chapters). Much more can be ",NA,NA
"said about them, in terms of the so-called noise-saturation dilemma, or stability–plasticity ",NA,NA
"dilemma and what research has developed to address them (for further reading, see ",NA,NA
References).,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch04/077-080.html (2 of 2) [21/11/02 21:57:05]",NA
Previous Table of Contents Next,NA,NA
Chapter 5 ,NA,NA
A Survey of Neural Network Models ,NA,NA
Neural Network Models,NA,NA
"You were introduced in the preceding pages to the Perceptron model, the Feedforward ",NA,NA
"network, and the Hopfield network. You learned that the differences between the models ",NA,NA
"lie in their architecture, encoding, and recall. We aim now to give you a comprehensive ",NA,NA
picture of these and other neural network models. We will show details and ,NA,NA
implementations of some networks in later chapters. ,NA,NA
"The models we briefly review in this chapter are the Perceptron, Hopfield, Adaline, Feed-",NA,NA
"Forward Backpropagation, Bidirectional Associative Memory, Brain-State-in-a-Box, ",NA,NA
"Neocognitron, Fuzzy Associative Memory, ART1, and ART2. C++ implementations of ",NA,NA
some of these and the role of fuzzy logic in some will be treated in the subsequent ,NA,NA
"chapters. For now, our discussion will be about the distinguishing characteristics of a ",NA,NA
neural network. We will follow it with the description of some of the models.,NA,NA
Layers in a Neural Network,NA,NA
"A neural network has its neurons divided into subgroups, or fields, and elements in each ",NA,NA
"subgroup are placed in a row, or a column, in the diagram depicting the network. Each ",NA,NA
subgroup is then referred to as a layer of neurons in the network. A great many models of ,NA,NA
"neural networks have two layers, quite a few have one layer, and some have three or more ",NA,NA
"layers. A number of additional, so-called hidden layers are possible in some networks, ",NA,NA
"such as the Feed-forward backpropagation network. When the network has a single layer, ",NA,NA
"the input signals are received at that layer, processing is done by its neurons, and output is ",NA,NA
"generated at that layer. When more than one layer is present, the first field is for the ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/081-083.html (1 of 3) [21/11/02 21:57:06],NA
neurons that supply the input signals for the neurons in the next layer. ,NA,NA
"Every network has a layer of input neurons, but in most of the networks, the sole purpose ",NA,NA
"of these neurons is to feed the input to the next layer of neurons. However, there are ",NA,NA
"feedback connections, or recurrent connections in some networks, so that the neurons in ",NA,NA
"the input layer may also do some processing. In the Hopfield network you saw earlier, the ",NA,NA
input and output layers are the same. If any layer is present between the input and output ,NA,NA
"layers, it may be referred to as a hidden layer in general, or as a layer with a special name ",NA,NA
taken after the researcher who proposed its inclusion to achieve certain performance from ,NA,NA
the network. Examples are the Grossberg and the Kohonen layers. The number of hidden ,NA,NA
layers is not limited except by the scope of the problem being addressed by the neural ,NA,NA
network.,NA,NA
A layer is also referred to as a ,NA,NA
field.,NA,NA
" Then the different layers can be designated as field A, ",NA,NA
"field B, and so on, or shortly, F",A,NA
", F",B,NA
.,NA,NA
Single-Layer Network,NA,NA
A neural network with a single layer is also capable of processing for some important ,NA,NA
"applications, such as integrated circuit implementations or assembly line control. The most ",NA,NA
common capability of the different models of neural networks is pattern recognition. But ,NA,NA
"one network, called the ",NA,NA
Brain-State-in-a-Box,NA,NA
", which is a single-layer neural network, can ",NA,NA
do pattern completion. ,NA,NA
Adaline,NA,NA
" is a network with A and B fields of neurons, but ",NA,NA
aggregation,NA,NA
 or processing of input signals is done only by the field B neurons.,NA,NA
The Hopfield network is a single-layer neural network. The Hopfield network makes an ,NA,NA
association between different patterns (heteroassociation) or associates a pattern with itself ,NA,NA
(autoassociation). You may characterize this as being able to recognize a given pattern. The ,NA,NA
idea of viewing it as a case of pattern recognition becomes more relevant if a pattern is ,NA,NA
"presented with some noise, meaning that there is some slight deformation in the pattern, ",NA,NA
and if the network is able to relate it to the correct pattern.,NA,NA
"The Perceptron technically has two layers, but has only one group of weights. We ",NA,NA
therefore still refer to it as a single-layer network. The second layer consists solely of the ,NA,NA
"output neuron, and the first layer consists of the neurons that receive input(s). Also, the ",NA,NA
"neurons in the same layer, the input layer in this case, are not interconnected, that is, no ",NA,NA
"connections are made between two neurons in that same layer. On the other hand, in the ",NA,NA
"Hopfield network, there is no separate output layer, and hence, it is strictly a single-layer ",NA,NA
"network. In addition, the neurons are all fully connected with one another.",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/081-083.html (2 of 3) [21/11/02 21:57:06],NA
"Let us spend more time on the single-layer Perceptron model and discuss its limitations, ",NA,NA
and thereby motivate the study of multilayer networks.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/081-083.html (3 of 3) [21/11/02 21:57:06]",NA
Previous Table of Contents Next,NA,NA
XOR Function and the Perceptron,NA,NA
The ability of a Perceptron in evaluating functions was brought into question when Minsky ,NA,NA
and Papert proved that a simple function like XOR (the logical function exclusive or) could ,NA,NA
"not be correctly evaluated by a Perceptron. The XOR logical function, f(A,B), is as follows: ",NA,NA
A ,NA,NA
B ,NA,NA
"f(A,B)= XOR(A,B) ",NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
0 ,NA,NA
"To summarize the behavior of the XOR, if both inputs are the same value, the output is 0, ",NA,NA
otherwise the output is 1. ,NA,NA
Minsky and Papert showed that it is impossible to come up with the proper set of weights ,NA,NA
for the neurons in the single layer of a simple Perceptron to evaluate the XOR function. ,NA,NA
"The reason for this is that such a Perceptron, one with a single layer of neurons, requires ",NA,NA
"the function to be evaluated, to be linearly separable by means of the function values. The ",NA,NA
concept of ,NA,NA
linear separability,NA,NA
 is explained next. But let us show you first why the simple ,NA,NA
perceptron fails to compute this function.,NA,NA
"Since there are two arguments for the XOR function, there would be two neurons in the ",NA,NA
"input layer, and since the function’s value is one number, there would be one output ",NA,NA
"neuron. Therefore, you need two weights ",NA,NA
w,1,NA
 and ,NA,NA
w,2,NA
" ,and a threshold value ",NA,NA
,NA,NA
. Let us now ,NA,NA
look at the conditions to be satisfied by the ,NA,NA
w,NA,NA
’s and the ,NA,NA
,NA,NA
 so that the outputs corresponding ,NA,NA
to given inputs would be as for the XOR function.,NA,NA
First the output should be 0 if inputs are 0 and 0. The activation works out as 0. To get an ,NA,NA
"output of 0, you need 0 < ",NA,NA
,NA,NA
. This is your first condition. Table 5.1 shows this and two other ,NA,NA
"conditions you need, and why.",NA,NA
Table 5.1,NA,NA
Conditions on Weights ,NA,NA
Input ,NA,NA
Activation ,NA,NA
Output ,NA,NA
Needed Condition ,NA,NA
"0, 0 ",NA,NA
0 ,NA,NA
0 ,NA,NA
0 < ,NA,NA
,NA,NA
"1, 0 ",NA,NA
w,1,NA
1 ,NA,NA
w,1,NA
 > ,NA,NA
,NA,NA
"0, 1 ",NA,NA
w,2,NA
1 ,NA,NA
w,2,NA
 > ,NA,NA
,NA,NA
"1, 1 ",NA,NA
w,1,NA
 + w,2,NA
0 ,NA,NA
w,1,NA
 + w,2,NA
< ,NA,NA
,NA,NA
"From the first three conditions, you can deduce that the sum of the two weights has to be ",NA,NA
greater than ,NA,NA
,NA,NA
", which has to be positive itself. Line 4 is inconsistent with lines 1, 2, and 3, ",NA,NA
since line 4 requires the sum of the two weights to be less than ,NA,NA
,NA,NA
. This affirms the ,NA,NA
contention that it is not possible to compute the XOR function with a simple perceptron. ,NA,NA
"Geometrically, the reason for this failure is that the inputs (0, 1) and (1, 0) with which you ",NA,NA
"want output 1, are situated diagonally opposite each other, when plotted as points in the ",NA,NA
"plane, as shown below in a diagram of the output (1=T, 0=F):",NA,NA
 F     T,NA,NA
 T     F ,NA,NA
You can’t separate the T’s and the F’s with a straight line. This means that you cannot ,NA,NA
"draw a line in the plane in such a way that neither (1, 1) ->F nor (0, 0)->F is on the same ",NA,NA
"side of the line as (0, 1) ->T and (1, 0)-> T. ",NA,NA
Linear Separability ,NA,NA
"What linearly separable means is, that a type of a linear barrier or a separator—a line in the ",NA,NA
"plane, or a plane in the three-dimensional space, or a hyperplane in higher ",NA,NA
"dimensions—should exist, so that the set of inputs that give rise to one value for the ",NA,NA
"function all lie on one side of this barrier, while on the other side lie the inputs that do not ",NA,NA
yield that value for the function. A ,NA,NA
hyperplane,NA,NA
" is a surface in a higher dimension, but with a ",NA,NA
linear equation defining it much the same way a line in the plane and a plane in the three-,NA,NA
dimensional space are defined.,NA,NA
"To make the concept a little bit clearer, consider a problem that is similar but, let us ",NA,NA
"emphasize, not the same as the XOR problem.",NA,NA
Imagine a cube of 1-unit length for each of its edges and lying in the positive octant in a ,NA,NA
xyz-rectangular coordinate system with one corner at the origin. The other corners or ,NA,NA
"vertices are at points with coordinates (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, ",NA,NA
"0), and (1, 1, 1). Call the origin O, and the seven points listed as A, B, C, D, E, F, and G, ",NA,NA
respectively. Then any two faces opposite to each other are linearly separable because you ,NA,NA
can define the separating plane as the plane halfway between these two faces and also ,NA,NA
parallel to these two faces.,NA,NA
"For example, consider the faces defined by the set of points O, A, B, and C and by the set ",NA,NA
"of points D, E, F, and G. They are parallel and 1 unit apart, as you can see in Figure 5.1. ",NA,NA
The separating plane for these two faces can be seen to be one of many possible ,NA,NA
"planes—any plane in between them and parallel to them. One example, for simplicity, is ",NA,NA
"the plane that passes through the points (1/2, 0, 0), (1/2, 0, 1), (1/2, 1, 0), and (1/2, 1, 1). Of ",NA,NA
"course, you need only specify three of those four points because a plane is uniquely ",NA,NA
determined by three points that are not all on the same line. So if the first set of points ,NA,NA
"corresponds to a value of say, +1 for the function, and the second set to a value of –1, then ",NA,NA
"a single-layer Perceptron can determine, through some training algorithm, the correct ",NA,NA
"weights for the connections, even if you start with the weights being initially all 0.",NA,NA
Figure 5.1,NA,NA
  Separating plane.,NA,NA
"Consider the set of points O, A, F, and G. This set of points cannot be linearly separated ",NA,NA
"from the other vertices of the cube. In this case, it would be impossible for the single-layer ",NA,NA
Perceptron to determine the proper weights for the neurons in evaluating the type of ,NA,NA
function we have been discussing. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/083-085.html (4 of 4) [21/11/02 21:57:07]",NA
Previous Table of Contents Next,NA,NA
A Second Look at the XOR Function: Multilayer Perceptron,NA,NA
"By introducing a set of cascaded Perceptrons, you have a Perceptron network, with an input ",NA,NA
"layer, middle or hidden layer, and an output layer. You will see that the multilayer ",NA,NA
"Perceptron can evaluate the XOR function as well as other logic functions (AND, OR, ",NA,NA
"MAJORITY, etc.). The absence of the separability that we talked about earlier is overcome ",NA,NA
"by having a second stage, so to speak, of connection weights. ",NA,NA
You need two neurons in the input layer and one in the output layer. Let us put a hidden ,NA,NA
layer with two neurons. Let ,NA,NA
w,11,NA
", ",NA,NA
w,12,NA
", ",NA,NA
w,21,NA
", and ",NA,NA
w,22,NA
", be the weights on connections from the ",NA,NA
input neurons to the hidden layer neurons. Let v,1,NA
", v",2,NA
" , be the weights on the connections ",NA,NA
from the hidden layer neurons to the outout neuron.,NA,NA
We will select the w’s (weights) and the threshold values ,NA,NA
,1,NA
" , and ",NA,NA
,2,NA
 at the hidden layer ,NA,NA
"neurons, so that the input (0, 0) generates the output vector (0, 0), and the input vector (1, ",NA,NA
"1) generates (1, 1), while the inputs (1, 0) and (0, 1) generate (0, 1) as the hidden layer ",NA,NA
"output. The inputs to the output layer neurons would be from the set {(0, 0), (1, 1), (0, 1)}. ",NA,NA
"These three vectors are separable, with (0, 0), and (1, 1) on one side of the separating line, ",NA,NA
"while (0, 1) is on the other side.",NA,NA
We will select the ,NA,NA
 ,NA,NA
s (weights) and ,NA,NA
,NA,NA
", the threshold value at the output neuron, so as to ",NA,NA
"make the inputs (0, 0) and (1, 1) cause an output of 0 for the network, and an output of 1 is ",NA,NA
"caused by the input (0, 1). The network layout within the labels of weights and threshold ",NA,NA
values inside the nodes representing hidden layer and output neurons is shown in Figure ,NA,NA
5.1a. Table 5.2 gives the results of operation of this network.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/085-088.html (1 of 3) [21/11/02 21:57:08],NA
Figure 5.1a,NA,NA
  Example network.,NA,NA
Table 5.2,NA,NA
Results for the Perceptron with One Hidden Layer. ,NA,NA
Input ,NA,NA
Hidden Layer ,NA,NA
Activations ,NA,NA
Hidden Layer ,NA,NA
Outputs ,NA,NA
Output Neuron ,NA,NA
activaton ,NA,NA
Output of ,NA,NA
network ,NA,NA
"(0, 0) ",NA,NA
"(0, 0) ",NA,NA
"(0, 0) ",NA,NA
0 ,NA,NA
0 ,NA,NA
"(1, 1) ",NA,NA
"(0.3, 0.6) ",NA,NA
"(1, 1) ",NA,NA
0 ,NA,NA
0 ,NA,NA
"(0, 1) ",NA,NA
"(0.15, 0.3) ",NA,NA
"(0, 1) ",NA,NA
0.3 ,NA,NA
1 ,NA,NA
"(1, 0) ",NA,NA
"(0.15, 0.3) ",NA,NA
"(0, 1) ",NA,NA
0.3 ,NA,NA
1 ,NA,NA
"It is clear from Table 5.2, that the above perceptron with a hidden layer does compute the ",NA,NA
XOR function successfully. ,"Note:  
 The activation should exceed the threshold value for a neuron to fire. Where the 
 output of a neuron is shown to be 0, it is because the internal activation of that neuron fell 
 short of its threshold value.",NA
Example of the Cube Revisited,NA,NA
"Let us return to the example of the cube with vertices at the origin O, and the points labeled ",NA,NA
"A, B, C, D, E, F, and G. Suppose the set of vertices O, A, F, and G give a value of 1 for the ",NA,NA
"function to be evaluated, and the other vertices give a –1. The two sets are not linearly ",NA,NA
separable as mentioned before. A simple Perceptron cannot evaluate this function. ,NA,NA
Can the addition of another layer of neurons help? The answer is yes. What would be the ,NA,NA
role of this additional layer? The answer is that it will do the final processing for the ,NA,NA
problem after the previous layer has done some preprocessing. This can do two separations ,NA,NA
in the sense that the set of eight vertices can be separated—or partitioned—into three ,NA,NA
"separable subsets. If this partitioning can also help collect within each subset, like vertices, ",NA,NA
"meaning those that map onto the same value for the function, the network will succeed in ",NA,NA
its task of evaluating the function when the aggregation and thresholding is done at the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/085-088.html (2 of 3) [21/11/02 21:57:08],NA
output neuron.,NA,NA
Strategy,NA,NA
So the strategy is first to consider the set of vertices that give a value of +1 for the function ,NA,NA
and determine the minimum number of subsets that can be identified to be each separable ,NA,NA
from the rest of the vertices. It is evident that since the vertices O and A lie on one edge of ,NA,NA
"the cube, they can form one subset that is separable. The other two vertices, viz., F and one ",NA,NA
"for G, which correspond to the value +1 for the function, can form a second subset that is ",NA,NA
"separable, too. We need not bother with the last four vertices from the point of view of ",NA,NA
"further partitioning that subset. It is clear that one new layer of three neurons, one of which ",NA,NA
"fires for the inputs corresponding to the vertices O and A, one for F, and G, and the third ",NA,NA
"for the rest, will then facilitate the correct evaluation of the function at the output neuron. ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/085-088.html (3 of 3) [21/11/02 21:57:08]",NA
Previous Table of Contents Next,NA,NA
Details,NA,NA
"Table 5.3 lists the vertices and their coordinates, together with a flag that indicates to which ",NA,NA
subset in the partitioning the vertex belongs. Note that you can think of the action of the ,NA,NA
Multilayer Perceptron as that of evaluating the intersection and union of linearly separable ,NA,NA
subsets. ,NA,NA
Table 5.3,NA,NA
Partitioning of Vertices of a Cube ,NA,NA
Vertex ,NA,NA
Coordinates ,NA,NA
Subset ,NA,NA
O ,NA,NA
"(0,0,0) ",NA,NA
1 ,NA,NA
A ,NA,NA
"(0,0,1) ",NA,NA
1 ,NA,NA
B ,NA,NA
"(0,1,0) ",NA,NA
2 ,NA,NA
C ,NA,NA
"(0,1,1) ",NA,NA
2 ,NA,NA
D ,NA,NA
"(1,0,0) ",NA,NA
2 ,NA,NA
E ,NA,NA
"(1,0,1) ",NA,NA
2 ,NA,NA
F ,NA,NA
"(1,1,0) ",NA,NA
3 ,NA,NA
G ,NA,NA
"(1,1,1) ",NA,NA
3 ,NA,NA
"The network, which is a two-layer Perceptron, meaning two layers of weights, has three ",NA,NA
neurons in the first layer and one output neuron in the second layer. Remember that we are ,NA,NA
counting those layers in which the neurons do the aggregation of the signals coming into ,NA,NA
them using the connection weights. The first layer with the three neurons is what is ,NA,NA
"generally described as the hidden layer, since the second layer is not hidden and is at the ",NA,NA
extreme right in the layout of the neural network. Table 5.4 gives an example of the ,NA,NA
weights you can use for the connections between the input neurons and the hidden layer ,NA,NA
"neurons. There are three input neurons, one for each coordinate of the vertex of the cube. ",NA,NA
Table 5.4,NA,NA
Weights for Connections Between Input Neurons and Hidden Layer Neurons. ,NA,NA
Input Neuron # ,NA,NA
Hidden Layer Neuron # Connection Weight ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
2 ,NA,NA
0.1 ,NA,NA
1 ,NA,NA
3 ,NA,NA
-1 ,NA,NA
2 ,NA,NA
1 ,NA,NA
1 ,NA,NA
2 ,NA,NA
2 ,NA,NA
-1 ,NA,NA
2 ,NA,NA
3 ,NA,NA
-1 ,NA,NA
3 ,NA,NA
1 ,NA,NA
0.2 ,NA,NA
3 ,NA,NA
2 ,NA,NA
0.3 ,NA,NA
3 ,NA,NA
3 ,NA,NA
0.6 ,NA,NA
"Now we give, in Table 5.5, the weights for the connections between the three hidden-layer ",NA,NA
neurons and the output neuron. ,NA,NA
Table 5.5,NA,NA
Weights for Connection Between the Hidden-Layer Neurons and the Output ,NA,NA
Neuron ,NA,NA
Hidden Layer Neuron # ,NA,NA
Connection Weight ,NA,NA
0.6 ,NA,NA
1 ,NA,NA
3 ,NA,NA
0.3 ,NA,NA
3 ,NA,NA
0.6 ,NA,NA
It is not apparent whether or not these weights will do the job. To determine the activations ,NA,NA
"of the hidden-layer neurons, you need these weights, and you also need the threshold value ",NA,NA
"at each neuron that does processing. A hidden-layer neuron will fire, that is, will output a ",NA,NA
"1, if the weighted sum of the signals it receives is greater than the threshold value. If the ",NA,NA
"output neuron fires, the function value is taken as +1, and if it does not fire, the function ",NA,NA
value is –1. Table 5.6 gives the threshold values. Figure 5.1b shows the neural network ,NA,NA
with connection weights and threshold values. ,NA,NA
Figure 5.1b,NA,NA
  Neural Network for Cube Example,NA,NA
Table 5.6,NA,NA
Threshold Values ,NA,NA
Layer ,NA,NA
Neuron ,NA,NA
Threshold Value ,NA,NA
1.8 ,NA,NA
hidden ,NA,NA
1 ,NA,NA
hidden ,NA,NA
2 ,NA,NA
0.05 ,NA,NA
hidden ,NA,NA
3 ,NA,NA
-0.2 ,NA,NA
output ,NA,NA
1 ,NA,NA
0.5 ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Performance of the Perceptron,NA,NA
"When you input the coordinates of the vertex G, which has 1 for each coordinate, the first ",NA,NA
hidden-layer neuron aggregates these inputs and gets a value of 2.2. Since 2.2 is more than ,NA,NA
"the threshold value of the first neuron in the hidden layer, that neuron fires, and its output ",NA,NA
of 1 becomes an input to the output neuron on the connection with weight 0.6. But you ,NA,NA
need the activations of the other hidden-layer neurons as well. Let us describe the ,NA,NA
performance with coordinates of G as the inputs to the network. Table 5.7 describes this. ,NA,NA
Table 5.7,NA,NA
Results with Coordinates of Vertex G as Input ,NA,NA
Vertex/ ,NA,NA
Coordinates ,NA,NA
Hidden ,NA,NA
Layer ,NA,NA
Weighted ,NA,NA
Sum ,NA,NA
Comment ,NA,NA
Activation Contribution to ,NA,NA
Output ,NA,NA
Sum ,NA,NA
"G:1,1,1 ",NA,NA
1 ,NA,NA
2.2 ,NA,NA
>1.8 ,NA,NA
1 ,NA,NA
0.6 ,NA,NA
0.6 ,NA,NA
2 ,NA,NA
-0.8 ,NA,NA
<0.05 ,NA,NA
0 ,NA,NA
0 ,NA,NA
3 ,NA,NA
-1.4 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"The weighted sum at the output neuron is 0.6, and it is greater than the threshold value 0.5. ",NA,NA
"Therefore, the output neuron fires, and at the vertex G, the function is evaluated to have a ",NA,NA
value of +1. ,NA,NA
Table 5.8 shows the performance of the network with the rest of the vertices of the cube. ,NA,NA
"You will notice that the network computes a value of +1 at the vertices, O, A, F, and G, ",NA,NA
and a –1 at the rest.,NA,NA
Table 5.8,NA,NA
Results with Other Inputs ,NA,NA
"O :0, 0, 0 ",NA,NA
Hidden ,NA,NA
Layer ,NA,NA
Neuron# ,NA,NA
Weighted ,NA,NA
Sum ,NA,NA
Comment ,NA,NA
Activation Contribution ,NA,NA
to Output ,NA,NA
Sum ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
"A :0, 0, 1 ",NA,NA
2 ,NA,NA
0 ,NA,NA
<0.05 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0.6,*,NA
3 ,NA,NA
0 ,NA,NA
>-0.2 ,NA,NA
1 ,NA,NA
0.6 ,NA,NA
1 ,NA,NA
0.2 ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"B :0, 1, 0 ",NA,NA
2 ,NA,NA
0.3 ,NA,NA
>0.05 ,NA,NA
1 ,NA,NA
0.3 ,NA,NA
0.9,*,NA
3 ,NA,NA
0.6 ,NA,NA
>-0.2 ,NA,NA
1 ,NA,NA
0.6 ,NA,NA
1 ,NA,NA
1 ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"C :0, 1, 1 ",NA,NA
2 ,NA,NA
-1 ,NA,NA
<0.05 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
3 ,NA,NA
-1 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1.2 ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"D :1, 0, 0 ",NA,NA
2 ,NA,NA
0.2 ,NA,NA
>0.05 ,NA,NA
1 ,NA,NA
0.3 ,NA,NA
0.3 ,NA,NA
3 ,NA,NA
-0.4 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"E :1, 0, 1 ",NA,NA
2 ,NA,NA
.1 ,NA,NA
>0.05 ,NA,NA
1 ,NA,NA
0.3 ,NA,NA
0.3 ,NA,NA
3 ,NA,NA
-1 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1.2 ,NA,NA
<1.8 ,NA,NA
0 ,NA,NA
0 ,NA,NA
"F :1, 1, 0 ",NA,NA
2 ,NA,NA
0.4 ,NA,NA
>0.05 ,NA,NA
1 ,NA,NA
0.3 ,NA,NA
0.3 ,NA,NA
3 ,NA,NA
-0.4 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
2 ,NA,NA
>1.8 ,NA,NA
1 ,NA,NA
0.6 ,NA,NA
2 ,NA,NA
-0.9 ,NA,NA
<0.05 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0.6,*,NA
3 ,NA,NA
-2 ,NA,NA
<-0.2 ,NA,NA
0 ,NA,NA
0 ,"*
 The output neuron fires, as this value is greater than 0.5 (the threshold value); the 
  
 function value is +1.",NA
Other Two-layer Networks,NA,NA
Many important neural network models have two layers. The Feedforward ,NA,NA
"backpropagation network, in its simplest form, is one example. Grossberg and Carpenter’s ",NA,NA
ART1 paradigm uses a two-layer network. The Counterpropagation network has a ,NA,NA
"Kohonen layer followed by a Grossberg layer. Bidirectional Associative Memory, (BAM), ",NA,NA
"Boltzman Machine, Fuzzy Associative Memory, and Temporal Associative Memory are ",NA,NA
"other two-layer networks. For autoassociation, a single-layer network could do the job, but ",NA,NA
"for heteroassociation or other such mappings, you need at least a two-layer network. We ",NA,NA
will give more details on these models shortly. ,NA,NA
Many Layer Networks,NA,NA
"Kunihiko Fukushima’s Neocognitron, noted for identifying handwritten characters, is an ",NA,NA
example of a network with several layers. Some previously mentioned networks can also be ,NA,NA
multilayered from the addition of more hidden layers. It is also possible to combine two or ,NA,NA
more neural networks into one network by creating appropriate connections between layers ,NA,NA
of one subnetwork to those of the others. This would certainly create a multilayer network. ,NA,NA
Connections Between Layers,NA,NA
You have already seen some difference in the way connections are made between neurons ,NA,NA
"in a neural network. In the Hopfield network, every neuron was connected to every other in ",NA,NA
"the one layer that was present in the network. In the Perceptron, neurons within the same ",NA,NA
"layer were not connected with one another, but the connections were between the neurons ",NA,NA
"in one layer and those in the next layer. In the former case, the connections are described as ",NA,NA
"being lateral. In the latter case, the connections are forward and the signals are fed forward ",NA,NA
within the network. ,NA,NA
"Two other possibilities also exist. All the neurons in any layer may have extra connections, ",NA,NA
with each neuron connected to itself. The second possibility is that there are connections ,NA,NA
"from the neurons in one layer to the neurons in a previous layer, in which case there is both ",NA,NA
"forward and backward signal feeding. This occurs, if feedback is a feature for the network ",NA,NA
model. The type of layout for the network neurons and the type of connections between the ,NA,NA
neurons constitute the architecture of the particular model of the neural network.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/090-093.html (3 of 3) [21/11/02 21:57:10]",NA
Previous Table of Contents Next,NA,NA
Instar and Outstar,NA,NA
Outstar,NA,NA
 and ,NA,NA
instar,NA,NA
 are terms defined by Stephen Grossberg for ways of looking at neurons ,NA,NA
in a network. A neuron in a web of other neurons receives a large number of inputs from ,NA,NA
"outside the neuron’s boundaries. This is like an inwardly radiating star, hence, the term ",NA,NA
"instar. Also, a neuron may be sending its output to many other destinations in the network. ",NA,NA
In this way it is acting as an outstar. Every neuron is thus simultaneously both an instar and ,NA,NA
an outstar. As an instar it receives stimuli from other parts of the network or from outside ,NA,NA
the network. Note that the neurons in the input layer of a network primarily have ,NA,NA
"connections away from them to the neurons in the next layer, and thus behave mostly as ",NA,NA
outstars. Neurons in the output layer have many connections coming to it and thus behave ,NA,NA
mostly as instars. A neural network performs its work through the constant interaction of ,NA,NA
instars and outstars.,NA,NA
A layer of instars can constitute a competitive layer in a network. An outstar can also be ,NA,NA
described as a source node with some associated sink nodes that the source feeds to. ,NA,NA
Grossberg identifies the source input with a conditioned stimulus and the sink inputs with ,NA,NA
unconditioned stimuli. Robert Hecht-Nielsen’s Counterpropagation network is a model ,NA,NA
built with instars and outstars.,NA,NA
Weights on Connections,NA,NA
Weight assignments on connections between neurons not only indicate the strength of the ,NA,NA
signal that is being fed for aggregation but also the type of interaction between the two ,NA,NA
neurons. The type of interaction is one of cooperation or of competition. The cooperative ,NA,NA
"type is suggested by a positive weight, and the competition by a negative weight, on the ",NA,NA
connection. The positive weight connection is meant for what is called ,NA,NA
"excitation,",NA,NA
 while ,NA,NA
the negative weight connection is termed an ,NA,NA
inhibition.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/093-096.html (1 of 4) [21/11/02 21:57:12],NA
Initialization of Weights,NA,NA
Initializing the network weight structure is part of what is called the ,NA,NA
encoding phase,NA,NA
 of a ,NA,NA
"network operation. The encoding algorithms are several, differing by model and by ",NA,NA
application. You may have gotten the impression that the weight matrices used in the ,NA,NA
examples discussed in detail thus far have been arbitrarily determined; or if there is a ,NA,NA
"method of setting them up, you are not told what it is.",NA,NA
It is possible to start with randomly chosen values for the weights and to let the weights be ,NA,NA
adjusted appropriately as the network is run through successive iterations. This would make ,NA,NA
"it easier also. For example, under supervised training, if the error between the desired and ",NA,NA
"computed output is used as a criterion in adjusting weights, then one may as well set the ",NA,NA
initial weights to zero and let the training process take care of the rest. The small example ,NA,NA
that follows illustrates this point.,NA,NA
A Small Example,NA,NA
"Suppose you have a network with two input neurons and one output neuron, with forward ",NA,NA
"connections between the input neurons and the output neuron, as shown in Figure 5.2. The ",NA,NA
"network is required to output a 1 for the input patterns (1, 0) and (1, 1), and the value 0 for ",NA,NA
"(0, 1) and (0, 0). There are only two connection weights ",NA,NA
w,1,NA
 and ,NA,NA
w,2,NA
.,NA,NA
Figure 5.2,NA,NA
  Neural network with forward connections.,NA,NA
"Let us set initially both weights to 0, but you need a threshold function also. Let us use the ",NA,NA
"following threshold function, which is slightly different from the one used in a previous ",NA,NA
example: ,NA,NA
f(x) = ,NA,NA
{,NA,NA
1 if x > 0 ,NA,NA
0 if x ,NA,NA
,NA,NA
 0 ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/093-096.html (2 of 4) [21/11/02 21:57:12],NA
The reason for modifying this function is that if ,NA,NA
f,NA,NA
(,NA,NA
x,NA,NA
) has value 1 when ,NA,NA
x,NA,NA
" = 0, then no matter ",NA,NA
"what the weights are, the output will work out to 1 with input (0, 0). This makes it ",NA,NA
impossible to get a correct computation of any function that takes the value 0 for the ,NA,NA
"arguments (0, 0).",NA,NA
Now we need to know by what procedure we adjust the weights. The procedure we would ,NA,NA
apply for this example is as follows.,NA,NA
•,NA,NA
  If the output with input pattern (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
") is as desired, then do not adjust the weights. ",NA,NA
•,NA,NA
  If the output with input pattern (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
") is smaller than what it should be, then ",NA,NA
increment each of ,NA,NA
w,1,NA
 and ,NA,NA
w,2,NA
 by 1. ,NA,NA
•,NA,NA
  If the output with input pattern (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
") is greater than what it should be, then ",NA,NA
subtract 1 from ,NA,NA
w,1,NA
 if the product ,NA,NA
aw,1,NA
" is smaller than 1, and adjust ",NA,NA
w,2,NA
 similarly. ,NA,NA
"Table 5.9 shows what takes place when we follow these procedures, and at what values the ",NA,NA
weights settle. ,NA,NA
Table 5.9,NA,NA
Adjustment of Weights ,NA,NA
step ,NA,NA
w,1,NA
w,2,NA
a ,NA,NA
b ,NA,NA
activation ,NA,NA
output ,NA,NA
comment ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
0 ,NA,NA
0 ,NA,NA
desired output is 1; ,NA,NA
increment both w’s ,NA,NA
0 ,NA,NA
0 ,NA,NA
2 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
2 ,NA,NA
1 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
3 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
4 ,NA,NA
1 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
output is 1; it ,NA,NA
should be 0. ,NA,NA
5 ,NA,NA
1 ,NA,NA
0 ,NA,NA
0 ,NA,NA
1 ,NA,NA
0 ,NA,NA
0 ,NA,NA
subtract 1 from w,2,NA
6 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
7 ,NA,NA
1 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
0 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
8 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
1 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
9 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
0 ,NA,NA
1 ,NA,NA
1 ,NA,NA
output is what it ,NA,NA
should be ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
"Table 5.9 shows that the network weight vector changed from an initial vector (0, 0) to the ",NA,NA
"final weight vector (1, 0) in eight iterations. This example is not of a network for pattern ",NA,NA
"matching. If you think about it, you will realize that the network is designed to fire if the ",NA,NA
"first digit in the pattern is a 1, and not otherwise. An analogy for this kind of a problem is ",NA,NA
"determining if a given image contains a specific object in a specific part of the image, such ",NA,NA
as a dot should occur in the letter i. ,NA,NA
If the initial weights are chosen somewhat prudently and to make some particular ,NA,NA
"relevance, then the speed of operation can be increased in the sense of convergence being ",NA,NA
"achieved with fewer iterations than otherwise. Thus, encoding algorithms are important. ",NA,NA
We now present some of the encoding algorithms.,NA,NA
Initializing Weights for Autoassociative Networks,NA,NA
Consider a network that is to associate each input pattern with itself and which gets binary ,NA,NA
"patterns as inputs. Make a bipolar mapping on the input pattern. That is, replace each 0 by –",NA,NA
1. Call the mapped pattern the vector ,NA,NA
x,NA,NA
", when written as a column vector. The transpose, ",NA,NA
"the same vector written as a row vector, is ",NA,NA
x,T,NA
. You will get a matrix of order the size of ,NA,NA
x ,NA,NA
when you form the product ,NA,NA
xx,T,NA
. Obtain similar matrices for the other patterns you want the ,NA,NA
"network to store. Add these matrices to give you the matrix of weights to be used initially, ",NA,NA
as we did in Chapter 4. This process can be described with the following equation:,NA,NA
W,NA,NA
 = ,NA,NA
,i,NA
x,i,NA
x,iT,NA
Weight Initialization for Heteroassociative Networks,NA,NA
Consider a network that is to associate one input pattern with another pattern and that gets ,NA,NA
"binary patterns as inputs. Make a bipolar mapping on the input pattern. That is, replace ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/096-098.html (1 of 3) [21/11/02 21:57:13],NA
each 0 by –1. Call the mapped pattern the vector ,NA,NA
x,NA,NA
 when written as a column vector. Get a ,NA,NA
similar bipolar mapping for the corresponding associated pattern. Call it ,NA,NA
y,NA,NA
. You will get a ,NA,NA
matrix of size ,NA,NA
x,NA,NA
 by size ,NA,NA
y,NA,NA
 when you form the product xy,T,NA
. Obtain similar matrices for the ,NA,NA
other patterns you want the network to store. Add these matrices to give you the matrix of ,NA,NA
weights to be used initially. The following equation restates this process:,NA,NA
W,NA,NA
 = ,NA,NA
,i,NA
x,i,NA
y,iT,NA
"On Center, Off Surround",NA,NA
In one of the many interesting paradigms you encounter in neural network models and ,NA,NA
"theory, is the strategy ",NA,NA
winner takes all,NA,NA
". Well, if there should be one winner emerging from a ",NA,NA
"crowd of neurons in a particular layer, there needs to be competition. Since everybody is ",NA,NA
"for himself in such a competition, in this case every neuron for itself, it would be necessary ",NA,NA
to have lateral connections that indicate this circumstance. The lateral connections from any ,NA,NA
"neuron to the others should have a negative weight. Or, the neuron with the highest ",NA,NA
activation is considered the winner and only its weights are modified in the training ,NA,NA
"process, leaving the weights of others the same. Winner takes all means that only one ",NA,NA
neuron in that layer fires and the others do not. This can happen in a hidden layer or in the ,NA,NA
output layer.,NA,NA
"In another situation, when a particular category of input is to be identified from among ",NA,NA
"several groups of inputs, there has to be a subset of the neurons that are dedicated to seeing ",NA,NA
"it happen. In this case, inhibition increases for distant neurons, whereas excitation ",NA,NA
"increases for the neighboring ones, as far as such a subset of neurons is concerned. The ",NA,NA
phrase ,NA,NA
"on center, off surround",NA,NA
 describes this phenomenon of distant inhibition and near ,NA,NA
excitation.,NA,NA
"Weights also are the prime components in a neural network, as they reflect on the one hand ",NA,NA
"the memory stored by the network, and on the other hand the basis for learning and ",NA,NA
training.,NA,NA
Inputs,NA,NA
You have seen that mutually orthogonal or almost orthogonal patterns are required as stable ,NA,NA
"stored patterns for the Hopfield network, which we discussed before for pattern matching. ",NA,NA
Similar restrictions are found also with other neural networks. Sometimes it is not a ,NA,NA
"restriction, but the purpose of the model makes natural a certain type of input. Certainly, in ",NA,NA
"the context of pattern classification, binary input patterns make problem setup ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/096-098.html (2 of 3) [21/11/02 21:57:13],NA
"simpler. Binary, bipolar, and analog signals are the varieties of inputs. Networks that accept ",NA,NA
"analog signals as inputs are for continuous models, and those that require binary or bipolar ",NA,NA
"inputs are for discrete models. Binary inputs can be fed to networks for continuous models, ",NA,NA
but analog signals cannot be input to networks for discrete models (unless they are ,NA,NA
"fuzzified). With input possibilities being discrete or analog, and the model possibilities ",NA,NA
"being discrete or continuous, there are potentially four situations, but one of them where ",NA,NA
analog inputs are considered for a discrete model is untenable. ,NA,NA
An example of a continuous model is where a network is to adjust the angle by which the ,NA,NA
steering wheel of a truck is to be turned to back up the truck into a parking space. If a ,NA,NA
"network is supposed to recognize characters of the alphabet, a means of discretization of a ",NA,NA
character allows the use of a discrete model.,NA,NA
What are the types of inputs for problems like image processing or handwriting analysis? ,NA,NA
"Remembering that artificial neurons, as processing elements, do aggregation of their inputs ",NA,NA
"by using connection weights, and that the output neuron uses a ",NA,NA
threshold,NA,NA
" function, you ",NA,NA
know that the inputs have to be numerical. A handwritten character can be superimposed on ,NA,NA
"a grid, and the input can consist of the cells in each row of the grid where a part of the ",NA,NA
"character is present. In other words, the input corresponding to one character will be a set ",NA,NA
of binary or gray-scale sequences containing one sequence for each row of the grid. A 1 in ,NA,NA
a particular position in the sequence for a row shows that the corresponding pixel is ,NA,NA
"present(black) in that part of the grid, while 0 shows it is not. The size of the grid has to be ",NA,NA
"big enough to accommodate the largest character under study, as well as the most complex ",NA,NA
features.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/096-098.html (3 of 3) [21/11/02 21:57:13]",NA
Previous Table of Contents Next,NA,NA
Outputs,NA,NA
"The output from some neural networks is a spatial pattern that can include a bit pattern, in ",NA,NA
"some a binary function value, and in some others an analog signal. The type of mapping ",NA,NA
"intended for the inputs determines the type of outputs, naturally. The output could be one ",NA,NA
"of classifying the input data, or finding associations between patterns of the same ",NA,NA
dimension as the input. ,NA,NA
The ,NA,NA
threshold,NA,NA
 functions do the final mapping of the activations of the output neurons into ,NA,NA
the network outputs. But the outputs from a single cycle of operation of a neural network ,NA,NA
"may not be the final outputs, since you would iterate the network into further cycles of ",NA,NA
"operation until you see convergence. If convergence seems possible, but is taking an awful ",NA,NA
"lot of time and effort, that is, if it is too slow to learn, you may assign a tolerance level and ",NA,NA
settle for the network to achieve near convergence.,NA,NA
The Threshold Function,NA,NA
"The output of any neuron is the result of thresholding, if any, of its internal activation, ",NA,NA
"which, in turn, is the weighted sum of the neuron’s inputs. Thresholding sometimes is done ",NA,NA
for the sake of scaling down the activation and mapping it into a meaningful output for the ,NA,NA
"problem, and sometimes for adding a bias. Thresholding (scaling) is important for ",NA,NA
multilayer networks to preserve a meaningful range across each layer’s operations. The ,NA,NA
most often used ,NA,NA
threshold,NA,NA
 function is the ,NA,NA
sigmoid,NA,NA
 function. A ,NA,NA
step,NA,NA
 function or a ,NA,NA
ramp ,NA,NA
function or just a ,NA,NA
linear,NA,NA
" function can be used, as when you simply add the bias to the ",NA,NA
activation. The ,NA,NA
sigmoid,NA,NA
" function accomplishes mapping the activation into the interval [0, ",NA,NA
1]. The equations are given as follows for the different threshold functions just mentioned.,NA,NA
The Sigmoid Function,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/098-102.html (1 of 4) [21/11/02 21:57:14],NA
More than one function goes by the name ,NA,NA
sigmoid,NA,NA
 function. They differ in their formulas ,NA,NA
and in their ranges. They all have a graph similar to a stretched letter ,NA,NA
s,NA,NA
. We give below two ,NA,NA
"such functions. The first is the hyperbolic tangent function with values in (–1, 1). The ",NA,NA
second is the logistic function and has values between 0 and 1. You therefore choose the ,NA,NA
one that fits the range you want. The graph of the ,NA,NA
sigmoid logistic,NA,NA
 function is given in Fig. ,NA,NA
5.3.,NA,NA
1.,NA,NA
  f(x) = tanh(x) = ( e,x,NA
 - e,-x,NA
) / (e,x,NA
 + e,-x,NA
) ,NA,NA
2.,NA,NA
  f(x) = 1 / (1+ e,-x,NA
) ,NA,NA
"Note that the first function here, the hyperbolic tangent function, can also be written, as 1 - ",NA,NA
2,NA,NA
e,-x,NA
 / (,NA,NA
e,x,NA
 + ,NA,NA
e,-x,NA
 ) after adding and also subtracting ,NA,NA
e,-x,NA
" to the numerator, and then ",NA,NA
simplifying. If now you multiply in the second term both numerator and denominator by ,NA,NA
e,x,NA
", you get 1 - 2/ (",NA,NA
e,2x,NA
 + 1). As ,NA,NA
x,NA,NA
 approaches -,NA,NA
 ,NA,NA
", this function goes to -1, and as ",NA,NA
x,NA,NA
approaches +,NA,NA
 ,NA,NA
", it goes to +1. On the other hand, the second function here, the ",NA,NA
sigmoid ,NA,NA
logistic,NA,NA
" function, goes to 0 as ",NA,NA
x,NA,NA
 approaches -,NA,NA
 ,NA,NA
", and to +1 as ",NA,NA
x,NA,NA
 approaches +,NA,NA
 ,NA,NA
. You can see ,NA,NA
this if you rewrite 1 / (1+ ,NA,NA
e,-x,NA
) as 1 - 1 / (1+ ,NA,NA
e,x,NA
"), after manipulations similar to those above.",NA,NA
You can think of equation 1 as the bipolar equivalent of binary equation 2. Both functions ,NA,NA
have the same shape.,NA,NA
Figure 5.3 is the graph of the ,NA,NA
sigmoid logistic,NA,NA
 function (number 2 of the preceding list).,NA,NA
Figure 5.3,NA,NA
  The ,NA,NA
sigmoid,NA,NA
 function. ,NA,NA
The Step Function,NA,NA
The ,NA,NA
step,NA,NA
 function is also frequently used as a ,NA,NA
threshold,NA,NA
 function. The function is 0 to start ,NA,NA
with and remains so to the left of some ,NA,NA
threshold,NA,NA
 value ,NA,NA
,NA,NA
. A jump to 1 occurs for the value ,NA,NA
of the function to the right of ,NA,NA
,NA,NA
", and the function then remains at the level 1. In general, a ",NA,NA
step,NA,NA
 function can have a finite number of points at which jumps of equal or unequal size ,NA,NA
"occur. When the jumps are equal and at many points, the graph will resemble a staircase. ",NA,NA
We are interested in a ,NA,NA
step,NA,NA
" function that goes from 0 to 1 in one step, as soon as the ",NA,NA
argument exceeds the ,NA,NA
threshold,NA,NA
 value ,NA,NA
,NA,NA
. You could also have two values other than 0 and 1 ,NA,NA
in defining the range of values of such a ,NA,NA
step,NA,NA
 function. A graph of the ,NA,NA
step,NA,NA
 function follows ,NA,NA
in Figure 5.4.,NA,NA
Figure 5.4,NA,NA
  The ,NA,NA
step,NA,NA
 function. ,"Note:  
 You can think of a 
 sigmoid
  function as a 
 fuzzy step
  function.",NA
The Ramp Function,NA,NA
To describe the ,NA,NA
ramp,NA,NA
" function simply, first consider a step function that makes a jump ",NA,NA
"from 0 to 1 at some point. Instead of letting it take a sudden jump like that at one point, let ",NA,NA
"it gradually gain in value, along a straight line (looks like a ramp), over a finite interval ",NA,NA
"reaching from an initial 0 to a final 1. Thus, you get a ",NA,NA
ramp,NA,NA
 function. You can think of a ,NA,NA
ramp,NA,NA
 function as a piecewise linear approximation of a sigmoid. The graph of a ,NA,NA
ramp ,NA,NA
function is illustrated in Figure 5.5.,NA,NA
Figure 5.5,NA,NA
  Graph of a ,NA,NA
ramp,NA,NA
 function. ,NA,NA
Linear Function,NA,NA
A ,NA,NA
linear,NA,NA
 function is a simple one given by an equation of the form:,NA,NA
f(x) = ,NA,NA
,NA,NA
x + ,NA,NA
,NA,NA
When ,NA,NA
,NA,NA
" = 1, the application of this ",NA,NA
threshold,NA,NA
 function amounts to simply adding a bias ,NA,NA
equal to ,NA,NA
,NA,NA
 to the sum of the inputs.,NA,NA
Applications,NA,NA
"As briefly indicated before, the areas of application generally include auto- and ",NA,NA
"heteroassociation, pattern recognition, data compression, data completion, signal filtering, ",NA,NA
"image processing, forecasting, handwriting recognition, and optimization. The type of ",NA,NA
"connections in the network, and the type of learning algorithm used must be chosen ",NA,NA
"appropriate to the application. For example, a network with lateral connections can do ",NA,NA
"autoassociation, while a feed-forward type can do forecasting. ",NA,NA
Some Neural Network Models,NA,NA
Adaline and Madaline,NA,NA
Adaline,NA,NA
 is the acronym for ,NA,NA
adaptive linear element,NA,NA
", due to Bernard Widrow and Marcian ",NA,NA
"Hoff. It is similar to a Perceptron. Inputs are real numbers in the interval [–1,+1], and ",NA,NA
learning is based on the criterion of minimizing the average squared error. Adaline has a ,NA,NA
high capacity to store patterns. ,NA,NA
Madaline,NA,NA
 stands for many Adalines and is a neural network ,NA,NA
"that is widely used. It is composed of field A and field B neurons, and there is one ",NA,NA
connection from each field A neuron to each field B neuron. Figure 5.6 shows a diagram of ,NA,NA
the Madaline.,NA,NA
Figure 5.6,NA,NA
  The Madaline model.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/098-102.html (4 of 4) [21/11/02 21:57:14]",NA
Previous Table of Contents Next,NA,NA
Backpropagation,NA,NA
The ,NA,NA
Backpropagation,NA,NA
 training algorithm for training feed-forward networks was ,NA,NA
"developed by Paul Werbos, and later by Parker, and Rummelhart and McClelland. This ",NA,NA
"type of network configuration is the most common in use, due to its ease of training. It is ",NA,NA
estimated that over 80% of all neural network projects in development use ,NA,NA
"backpropagation. In backpropagation, there are two phases in its learning cycle, one to ",NA,NA
"propagate the input pattern through the network and the other to adapt the output, by ",NA,NA
changing the weights in the network. It is the error signals that are backpropagated in the ,NA,NA
network operation to the hidden layer(s). The portion of the error signal that a hidden-layer ,NA,NA
neuron receives in this process is an estimate of the contribution of a particular neuron to ,NA,NA
"the output error. Adjusting on this basis the weights of the connections, the squared error, ",NA,NA
"or some other metric, is reduced in each cycle and finally minimized, if possible.",NA,NA
Figure for Backpropagation Network,NA,NA
"You will find in Figure 7.1 in Chapter 7, the layout of the nodes that represent the neurons ",NA,NA
"in a feedforward Backpropagation network and the connections between them. For now, ",NA,NA
"you try your hand at drawing this layout based on the following description, and compare ",NA,NA
your drawing with Figure 7.1. There are three fields of neurons. The connections are ,NA,NA
forward and are from each neuron in a layer to every neuron in the next layer. There are no ,NA,NA
lateral or recurrent connections. Labels on connections indicate weights. Keep in mind that ,NA,NA
"the number of neurons is not necessarily the same in different layers, and this fact should be ",NA,NA
evident in the notation for the weights. ,NA,NA
Bidirectional Associative Memory,NA,NA
"Bidirectional Associative Memory, (BAM), and other models described in this section ",NA,NA
were developed by Bart Kosko. BAM is a network with feedback connections from the ,NA,NA
output layer to the input layer. It associates a member of the set of input patterns with a ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/102-105.html (1 of 3) [21/11/02 21:57:15],NA
"member of the set of output patterns that is the closest, and thus it does heteroassociation. ",NA,NA
"The patterns can be with binary or bipolar values. If all possible input patterns are known, ",NA,NA
the matrix of connection weights can be determined as the sum of matrices obtained by ,NA,NA
taking the matrix product of an input vector (as a column vector) with its transpose ,NA,NA
(written as a row vector). ,NA,NA
The pattern obtained from the output layer in one cycle of operation is fed back at the input ,NA,NA
layer at the start of the next cycle. The process continues until the network stabilizes on all ,NA,NA
the input patterns. The stable state so achieved is described as ,NA,NA
resonance,NA,NA
", a concept used in ",NA,NA
the Adaptive Resonance Theory.,NA,NA
"You will find in Figure 8.1 in Chapter 8, the layout of the nodes that represent the neurons ",NA,NA
in a BAM network and the connections between them. There are two fields of neurons. The ,NA,NA
network is fully connected with feedback connections and forward connections. There are ,NA,NA
no lateral or recurrent connections.,NA,NA
"Fuzzy Associative memories are similar to Bidirectional Associative memories, except that ",NA,NA
association is established between fuzzy patterns. Chapter 9 deals with Fuzzy Associative ,NA,NA
memories.,NA,NA
Temporal Associative Memory,NA,NA
Another type of associative memory is ,NA,NA
temporal associative memory,NA,NA
". Amari, a pioneer in ",NA,NA
"the field of neural networks, constructed a Temporal Associative Memory model that has ",NA,NA
feedback connections between the input and output layers. The forte of this model is that it ,NA,NA
can store and retrieve spatiotemporal patterns. An example of a spatiotemporal pattern is a ,NA,NA
waveform of a speech segment.,NA,NA
Brain-State-in-a-Box,NA,NA
"Introduced by James Anderson and others, this network differs from the single-layer fully ",NA,NA
connected Hopfield network in that ,NA,NA
Brain-State-in-a-Box,NA,NA
 uses what we call recurrent ,NA,NA
"connections as well. Each neuron has a connection to itself. With target patterns available, a ",NA,NA
modified ,NA,NA
Hebbian learning,NA,NA
 rule is used. The adjustment to a connection weight is ,NA,NA
proportional to the product of the desired output and the error in the computed output. You ,NA,NA
"will see more on Hebbian learning in Chapter 6. This network is adept at noise tolerance, ",NA,NA
and it can accomplish pattern completion. Figure 5.7 shows a Brain-State-in-a-Box ,NA,NA
network.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/102-105.html (2 of 3) [21/11/02 21:57:15],NA
Figure 5.7,NA,NA
"  A Brain-State-in-a-Box, network.",NA,NA
What’s in a Name?,NA,NA
More like what’s in the box? Suppose you find the following: there is a square box and its ,NA,NA
"corners are the locations for an entity to be. The entity is not at one of the corners, but is at ",NA,NA
some point inside the box. The next position for the entity is determined by working out the ,NA,NA
"change in each coordinate of the position, according to a weight matrix, and a ",NA,NA
squashing function. This process is repeated until the entity settles down at some position. ,NA,NA
The choice of the weight matrix is such that when the entity reaches a corner of the square ,NA,NA
"box, its position is stable and no more movement takes place. You would perhaps guess ",NA,NA
that the entity finally settles at the corner nearest to the initial position of it within the box. ,NA,NA
It is said that this kind of an example is the reason for the name Brain-State-in-a-Box for ,NA,NA
the model. Its forte is that it represents linear transformations. Some type of association of ,NA,NA
patterns can be achieved with this model. If an incomplete pattern is associated with a ,NA,NA
"completed pattern, it would be an example of autoassociation. ",NA,NA
Counterpropagation,NA,NA
"This is a neural network model developed by Robert Hecht-Nielsen, that has one or two ",NA,NA
"additional layers between the input and output layers. If it is one, the middle layer is a ",NA,NA
"Grossberg layer with a bunch of outstars. In the other case, a Kohonen layer, or a self-",NA,NA
"organizing layer, follows the input layer, and in turn is followed by a Grossberg layer of ",NA,NA
outstars. The model has the distinction of considerably reducing training time. With this ,NA,NA
"model, you gain a tool that works like a look-up table. ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/102-105.html (3 of 3) [21/11/02 21:57:15]",NA
Previous Table of Contents Next,NA,NA
Neocognitron,NA,NA
"Compared to all other neural network models, Fukushima’s Neocognitron is more complex ",NA,NA
and ambitious. It demonstrates the advantages of a multilayered network. The ,NA,NA
Neocognitron is one of the best models for recognizing handwritten symbols. Many pairs of ,NA,NA
layers called the ,NA,NA
S layer,NA,NA
", for ",NA,NA
simple layer,NA,NA
", and ",NA,NA
C layer,NA,NA
", for ",NA,NA
complex layer,NA,NA
", are used. Within ",NA,NA
"each S layer are several planes containing simple cells. Similarly, there are within each C ",NA,NA
"layer, an equal number of planes containing complex cells. The input layer does not have ",NA,NA
this arrangement and is like an input layer in any other neural network.,NA,NA
The number of planes of simple cells and of complex cells within a pair of S and C layers ,NA,NA
"being the same, these planes are paired, and the complex plane cells process the outputs of ",NA,NA
the simple plane cells. The simple cells are trained so that the response of a simple cell ,NA,NA
corresponds to a specific portion of the input image. If the same part of the image occurs ,NA,NA
"with some distortion, in terms of scaling or rotation, a different set of simple cells responds ",NA,NA
to it. The complex cells output to indicate that some simple cell they correspond to did fire. ,NA,NA
"While simple cells respond to what is in a contiguous region in the image, complex cells ",NA,NA
"respond on the basis of a larger region. As the process continues to the output layer, the C-",NA,NA
"layer component of the output layer responds, corresponding to the entire image presented ",NA,NA
in the beginning at the input layer.,NA,NA
Adaptive Resonance Theory,NA,NA
ART1 is the first model for adaptive resonance theory for neural networks developed by ,NA,NA
Gail Carpenter and Stephen Grossberg. This theory was developed to address the ,NA,NA
stability–plasticity dilemma. The network is supposed to be plastic enough to learn an ,NA,NA
"important pattern. But at the same time it should remain stable when, in short-term ",NA,NA
"memory, it encounters some distorted versions of the same pattern. ",NA,NA
"ART1 model has A and B field neurons, a gain, and a reset as shown in Figure 5.8. There ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/105-108.html (1 of 3) [21/11/02 21:57:16],NA
are top-down and bottom-up connections between neurons of fields A and B. The neurons ,NA,NA
"in field B have lateral connections as well as recurrent connections. That is, every neuron in ",NA,NA
"this field is connected to every other neuron in this field, including itself, in addition to the ",NA,NA
"connections to the neurons in field A. The external input (or bottom-up signal), the top-",NA,NA
"down signal, and the gain constitute three elements of a set, of which at least two should be ",NA,NA
a +1 for the neuron in the A field to fire. This is what is termed the ,NA,NA
two-thirds rule,NA,NA
. ,NA,NA
"Initially, therefore, the gain would be set to +1. The idea of a single winner is also ",NA,NA
"employed in the B field. The gain would not contribute in the top-down phase; actually, it ",NA,NA
will inhibit. The two-thirds rule helps move toward stability once ,NA,NA
"resonance,",NA,NA
 or ,NA,NA
"equilibrium, is obtained. A ",NA,NA
vigilance parameter,NA,NA
,NA,NA
 is used to determine the parameter reset. ,NA,NA
Vigilance parameter corresponds to what degree the resonating category can be predicted. ,NA,NA
The part of the system that contains gain is called the ,NA,NA
attentional subsystem,NA,NA
", whereas the ",NA,NA
"rest, the part that contains reset, is termed the ",NA,NA
orienting subsystem,NA,NA
. The top-down activity ,NA,NA
"corresponds to the orienting subsystem, and the bottom-up activity relates to the attentional ",NA,NA
subsystem.,NA,NA
Figure 5.8,NA,NA
  The ART1 network.,NA,NA
"In ART1, classification of an input pattern in relation to stored patterns is attempted, and if ",NA,NA
"unsuccessful, a new stored classification is generated. Training is unsupervised. There are ",NA,NA
two versions of training: slow and fast. They differ in the extent to which the weights are ,NA,NA
given the time to reach their eventual values. Slow training is governed by differential ,NA,NA
"equations, and fast training by algebraic equations. ",NA,NA
"ART2 is the analog counterpart of ART1, which is for discrete cases. These are self-",NA,NA
"organizing neural networks, as you can surmise from the fact that training is present but ",NA,NA
unsupervised. The ART3 model is for recognizing a coded pattern through a parallel ,NA,NA
"search, and is developed by Carpenter and Grossberg. It tries to emulate the activities of ",NA,NA
chemical transmitters in the brain during what can be construed as a parallel search for ,NA,NA
pattern recognition.,NA,NA
Summary,NA,NA
"The basic concepts of neural network layers, connections, weights, inputs, and outputs ",NA,NA
have been discussed. An example of how adding another layer of neurons in a network can ,NA,NA
solve a problem that could not be solved without it is given in detail. A number of neural ,NA,NA
"network models are introduced briefly. Learning and training, which form the basis of ",NA,NA
"neural network behavior has not been included here, but will be discussed in the following ",NA,NA
chapter. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch05/105-108.html (3 of 3) [21/11/02 21:57:16]",NA
Previous Table of Contents Next,NA,NA
Chapter 6 ,NA,NA
Learning and Training ,NA,NA
"In the last chapter, we presented an overview of different neural network models. In this ",NA,NA
"chapter, we continue the broad discussion of neural networks with two important topics: ",NA,NA
Learning and Training. Here are key questions that we would like to answer: ,NA,NA
•,NA,NA
  How do neural networks learn? ,NA,NA
•,NA,NA
  What does it mean for a network to learn ? ,NA,NA
•,NA,NA
  What differences are there between supervised and unsupervised learning ? ,NA,NA
•,NA,NA
  What training regimens are in common use for neural networks? ,NA,NA
Objective of Learning,NA,NA
"There are many varieties of neural networks. In the final analysis, as we have discussed ",NA,NA
"briefly in Chapter 4 on network modeling, all neural networks do one or more of the ",NA,NA
following : ,NA,NA
•,NA,NA
  Pattern classification ,NA,NA
•,NA,NA
  Pattern completion ,NA,NA
•,NA,NA
  Optimization ,NA,NA
•,NA,NA
  Data clustering ,NA,NA
•,NA,NA
  Approximation ,NA,NA
•,NA,NA
  Function evaluation ,NA,NA
"A neural network, in any of the previous tasks, maps a set of inputs to a set of outputs. This ",NA,NA
nonlinear mapping can be thought of as a multidimensional ,NA,NA
mapping surface,NA,NA
. The ,NA,NA
objective of learning ,NA,NA
is to mold the mapping surface according to a desired response,NA,NA
", ",NA,NA
either with or without an explicit training process.,NA,NA
Learning and Training,NA,NA
"A network can learn when training is used, or the network can learn also in the absence of ",NA,NA
training. The difference between ,NA,NA
supervised,NA,NA
 and ,NA,NA
unsupervised training,NA,NA
" is that, in the former ",NA,NA
"case, external prototypes are used as target outputs for specific inputs, and the network is ",NA,NA
given a learning algorithm to follow and calculate new connection weights that bring the ,NA,NA
output closer to the target output. Unsupervised learning is the sort of learning that takes ,NA,NA
"place without a teacher. For example, when you are finding your way out of a labyrinth, no ",NA,NA
teacher is present. You learn from the responses or events that develop as you try to feel ,NA,NA
"your way through the maze. For neural networks, in the unsupervised case, a learning ",NA,NA
"algorithm may be given but target outputs are not given. In such a case, data input to the ",NA,NA
network gets clustered together; similar input stimuli cause similar responses.,NA,NA
When a neural network model is developed and an appropriate learning algorithm is ,NA,NA
"proposed, it would be based on the theory supporting the model. Since the dynamics of the ",NA,NA
"operation of the neural network is under study, the learning equations are initially ",NA,NA
"formulated in terms of differential equations. After solving the differential equations, and ",NA,NA
"using any initial conditions that are available, the algorithm could be simplified to consist ",NA,NA
of an algebraic equation for the changes in the weights. These simple forms of learning ,NA,NA
equations are available for your neural networks.,NA,NA
"At this point of our discussion you need to know what learning algorithms are available, ",NA,NA
and what they look like. We will now discuss two main rules for learning—,NA,NA
Hebbian ,NA,NA
"learning,",NA,NA
 used with unsupervised learning and the ,NA,NA
"delta rule,",NA,NA
 used with supervised learning. ,NA,NA
Adaptations of these by simple modifications to suit a particular context generate many ,NA,NA
"other learning rules in use today. Following the discussion of these two rules, we present ",NA,NA
variations for each of the two classes of learning: supervised learning and ,NA,NA
unsupervised learning.,NA,NA
Hebb’s Rule,NA,NA
Learning algorithms are usually referred to as ,NA,NA
learning rules,NA,NA
. The foremost such rule is due ,NA,NA
"to Donald Hebb. Hebb’s rule is a statement about how the firing of one neuron, which has a ",NA,NA
"role in the determination of the activation of another neuron, affects the first neuron’s ",NA,NA
"influence on the activation of the second neuron, especially if it is done in a repetitive ",NA,NA
"manner. As a learning rule, Hebb’s observation translates into a formula for the difference ",NA,NA
"in a connection weight between two neurons from one iteration to the next, as a constant ",NA,NA
[mu] times the product of activations of the two neurons. How a connection weight is to be ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/109-111.html (2 of 3) [21/11/02 21:57:17],NA
"modified is what the learning rule suggests. In the case of Hebb’s rule, it is adding the ",NA,NA
quantity [mu],NA,NA
a,i,NA
a,j,NA
", where ",NA,NA
a,i,NA
 is the activation of the ,NA,NA
i,NA,NA
"th neuron, and ",NA,NA
a,j,NA
 is the activation of the ,NA,NA
j,NA,NA
th neuron to the connection weight between the ,NA,NA
i,NA,NA
th and ,NA,NA
j,NA,NA
th neurons. The constant [mu] ,NA,NA
itself is referred to as the ,NA,NA
learning rate,NA,NA
. The following equation using the notation just ,NA,NA
"described, states it succinctly:",NA,NA
[Delta]w,ij,NA
 = [mu]a,i,NA
a,j,NA
"As you can see, the learning rule derived from Hebb’s rule is quite simple and is used in ",NA,NA
both simple and more involved networks. Some modify this rule by replacing the quantity ,NA,NA
a,i,NA
 with its deviation from the average of all ,NA,NA
a,NA,NA
"s and, similarly, replacing ",NA,NA
a,j,NA
 by a ,NA,NA
corresponding quantity. Such rule variations can yield rules better suited to different ,NA,NA
situations.,NA,NA
"For example, the output of a neural network being the activations of its output layer ",NA,NA
"neurons, the Hebbian learning rule in the case of a perceptron takes the form of adjusting ",NA,NA
the weights by adding [mu] times the difference between the output and the target. ,NA,NA
Sometimes a situation arises where some unlearning is required for some neurons. In this ,NA,NA
case a reverse Hebbian rule is used in which the quantity [mu],NA,NA
a,i,NA
a,j,NA
 is subtracted from the ,NA,NA
"connection weight under question, which in effect is employing a negative learning rate.",NA,NA
"In the Hopfield network of Chapter 1, there is a single layer with all neurons fully ",NA,NA
interconnected. Suppose each neuron’s output is either ,NA,NA
a,NA,NA
 + 1 or ,NA,NA
a,NA,NA
 – 1. If we take [mu] = 1 ,NA,NA
"in the Hebbian rule, the resulting modification of the connection weights can be described ",NA,NA
"as follows: add 1 to the weight, if both neuron outputs match, that is, both are +1 or –1. ",NA,NA
"And if they do not match (meaning one of them has output +1 and the other has –1), then ",NA,NA
subtract 1 from the weight.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/109-111.html (3 of 3) [21/11/02 21:57:17]",NA
Previous Table of Contents Next,NA,NA
Delta Rule,NA,NA
The delta rule is also known as the ,NA,NA
least mean squared,NA,NA
 error rule (LMS). You first calculate the ,NA,NA
"square of the errors between the target or desired values and computed values, and then take the ",NA,NA
"average to get the mean squared error. This quantity is to be minimized. For this, realize that it is a ",NA,NA
"function of the weights themselves, since the computation of output uses them. The set of values of ",NA,NA
weights that minimizes the mean squared error is what is needed for the next cycle of operation of ,NA,NA
"the neural network. Having worked this out mathematically, and having compared the weights thus ",NA,NA
"found with the weights actually used, one determines their difference and gives it in the delta rule, ",NA,NA
"each time weights are to be updated. So the delta rule, which is also the rule used first by Widrow ",NA,NA
"and Hoff, in the context of learning in neural networks, is stated as an equation defining the change ",NA,NA
in the weights to be affected.,NA,NA
Suppose you fix your attention to the weight on the connection between the ,NA,NA
i,NA,NA
th neuron in one layer ,NA,NA
and the ,NA,NA
j,NA,NA
th neuron in the next layer. At time ,NA,NA
t,NA,NA
", this weight is ",NA,NA
w,ij,NA
(,NA,NA
t,NA,NA
") . After one cycle of operation, this ",NA,NA
weight becomes ,NA,NA
w,ij,NA
(,NA,NA
t,NA,NA
 + 1). The difference between the two is ,NA,NA
w,ij,NA
(,NA,NA
t,NA,NA
 + 1) - ,NA,NA
w,ij,NA
(,NA,NA
t,NA,NA
"), and is denoted by ",NA,NA
[Delta],NA,NA
w,ij,NA
 . The delta rule then gives [Delta],NA,NA
w,ij,NA
 as :,NA,NA
[Delta]w,ij,NA
 = 2[mu]x,i,NA
(desired output value – computed output value),j,NA
"Here, [mu] is the learning rate, which is positive and much smaller than 1, and ",NA,NA
x,i,NA
 is the ,NA,NA
i,NA,NA
th ,NA,NA
component of the input vector.,NA,NA
Supervised Learning,NA,NA
Supervised neural network paradigms to be discussed include : ,NA,NA
•,NA,NA
  Perceptron ,NA,NA
•,NA,NA
  Adaline ,NA,NA
•,NA,NA
  Feedforward Backpropagation network ,NA,NA
•,NA,NA
  Statistical trained networks (Boltzmann/Cauchy machines) ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/111-115.html (1 of 3) [21/11/02 21:57:19],NA
•,NA,NA
  Radial basis function networks ,NA,NA
The Perceptron and the Adaline use the delta rule; the only difference is that the Perceptron has ,NA,NA
"binary output, while the Adaline has continuous valued output. The Feedforward Backpropagation ",NA,NA
network uses the ,NA,NA
generalized delta rule,NA,NA
", which is described next.",NA,NA
Generalized Delta Rule,NA,NA
"While the delta rule uses local information on error, the generalized delta rule uses error ",NA,NA
information that is not local. It is designed to minimize the total of the squared errors of the output ,NA,NA
"neurons. In trying to achieve this minimum, the steepest descent method, which uses the gradient of ",NA,NA
"the weight surface, is used. (This is also used in the delta rule.) For the next error calculation, the ",NA,NA
"algorithm looks at the gradient of the error surface, which gives the direction of the largest slope on ",NA,NA
the error surface. This is used to determine the direction to go to try to minimize the error. ,NA,NA
"The algorithm chooses the negative of this gradient, which is the direction of steepest descent. ",NA,NA
"Imagine a very hilly error surface, with peaks and valleys that have a wide range of magnitude. ",NA,NA
Imagine starting your search for minimum error at an arbitrary point. By choosing the negative ,NA,NA
"gradient on all iterations, you eventually end up at a valley. You cannot know, however, if this ",NA,NA
valley is the global minimum or a local minimum. Getting stuck in a local minimum is one well-,NA,NA
known potential problem of the steepest descent method. You will see more on the generalized ,NA,NA
delta rule in the chapter on backpropagation (Chapter 7). ,NA,NA
Statistical Training and Simulated Annealing,NA,NA
"The Boltzmann machine (and Cauchy machine) uses probabilities and statistical theory, along with ",NA,NA
an energy function representing temperature. The learning is probabilistic and is called ,NA,NA
simulated ,NA,NA
annealing,NA,NA
". At different temperature levels, a different number of iterations in processing are used, ",NA,NA
and this constitutes an annealing schedule. Use of probability distributions is for the goal of reaching ,NA,NA
a state of global minimum of energy. Boltzmann distribution and Cauchy distribution are probability ,NA,NA
"distributions used in this process. It is obviously desirable to reach a global minimum, rather than ",NA,NA
settling down at a local minimum.,NA,NA
Figure 6.1 clarifies the distinction between a local minimum and a global minimum. In this figure ,NA,NA
you find the graph of an energy function and points A and B. These points show that the energy ,NA,NA
"levels there are smaller than the energy levels at any point in their vicinity, so you can say they ",NA,NA
"represent points of minimum energy. The overall or global minimum, as you can see, is at point B, ",NA,NA
"where the energy level is smaller than that even at point A, so A corresponds only to a local ",NA,NA
"minimum. It is desirable to get to B and not get stopped at A itself, in the pursuit of a minimum for ",NA,NA
"the energy function. If point C is reached, one would like the further movement to be toward B and ",NA,NA
"not A. Similarly, if a point near A is reached, the subsequent movement should avoid reaching or ",NA,NA
settling at A but carry on to B. Perturbation techniques are useful for these considerations.,NA,NA
Figure 6.1,NA,NA
  Local and global minima.,NA,NA
Clamping Probabilities,NA,NA
"Sometimes in simulated annealing, first a subset of the neurons in the network are associated with ",NA,NA
"some inputs, and another subset of neurons are associated with some outputs, and these are ",NA,NA
"clamped with probabilities, which are not changed in the learning process. Then the rest of the ",NA,NA
network is subjected to adjustments. Updating is not done for the clamped units in the network. ,NA,NA
This training procedure of Geoffrey Hinton and Terrence Sejnowski provides an extension of the ,NA,NA
Boltzmann technique to more general networks. ,NA,NA
Radial Basis-Function Networks,NA,NA
"Although details of radial basis functions are beyond the scope of this book, it is worthwhile to ",NA,NA
contrast the learning characteristics for this type of neural network model. Radial basis-function ,NA,NA
networks in topology look similar to feedforward networks. Each neuron has an output to input ,NA,NA
"characteristic that resembles a radial function (for two inputs, and thus two dimensions). ",NA,NA
"Specifically, the output ",NA,NA
h,NA,NA
(x) is as follows:,NA,NA
h(,NA,NA
x,NA,NA
)  = exp [ (,NA,NA
x - u),2,NA
 / 2[sigma],2,NA
   ],NA,NA
"Here, ",NA,NA
x,NA,NA
" is the input vector, ",NA,NA
u,NA,NA
" is the mean, and [sigma] is the standard deviation of the output ",NA,NA
response curve of the neuron. Radial basis function (RBF) networks have rapid training time ,NA,NA
(orders of magnitude faster than backpropagation) and do not have problems with local minima as ,NA,NA
"backpropagation does. RBF networks are used with supervised training, and typically only the ",NA,NA
"output layer is trained. Once training is completed, a RBF network may be slower to use than a ",NA,NA
"feedforward Backpropagation network, since more computations are required to arrive at an output.",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/111-115.html (3 of 3) [21/11/02 21:57:19]",NA
Previous Table of Contents Next,NA,NA
Unsupervised Networks,NA,NA
Unsupervised neural network paradigms to be discussed include: ,NA,NA
•,NA,NA
  Hopfield Memory ,NA,NA
•,NA,NA
  Bidirectional associative memory ,NA,NA
•,NA,NA
  Fuzzy associative memory ,NA,NA
•,NA,NA
  Learning vector quantizer ,NA,NA
•,NA,NA
  Kohonen self-organizing map ,NA,NA
•,NA,NA
  ART1 ,NA,NA
Self-Organization,NA,NA
Unsupervised learning and self-organization are closely related. Unsupervised learning was ,NA,NA
"mentioned in Chapter 1, along with supervised learning. Training in supervised learning ",NA,NA
takes the form of external exemplars being provided. The network has to compute the ,NA,NA
correct weights for the connections for neurons in some layer or the other. Self-,NA,NA
organization implies unsupervised learning. It was described as a characteristic of a neural ,NA,NA
"network model, ART1, based on adaptive resonance theory (to be covered in Chapter 10). ",NA,NA
"With the winner-take-all criterion, each neuron of field B learns a distinct classification. ",NA,NA
"The winning neuron in a layer, in this case the field B, is the one with the largest ",NA,NA
"activation, and it is the only neuron in that layer that is allowed to fire. Hence, the name ",NA,NA
winner take all. ,NA,NA
"Self-organization means self-adaptation of a neural network. Without target outputs, the ",NA,NA
closest possible response to a given input signal is to be generated. Like inputs will cluster ,NA,NA
together. The connection weights are modified through different iterations of network ,NA,NA
"operation, and the network capable of self-organizing creates on its own the closest ",NA,NA
possible set of outputs for the given inputs. This happens in the model in Kohonen’s self-,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/115-117.html (1 of 3) [21/11/02 21:57:20],NA
organizing map.,NA,NA
Kohonen’s ,NA,NA
Linear Vector Quantizer,NA,NA
 (LVQ) described briefly below is later extended as a ,NA,NA
"self-organizing feature map. Self-organization is also learning, but without supervision; it is ",NA,NA
a case of self-training. Kohonen’s topology preserving maps illustrate self-organization by ,NA,NA
"a neural network. In these cases, certain subsets of output neurons respond to certain ",NA,NA
"subareas of the inputs, so that the firing within one subset of neurons indicates the presence ",NA,NA
of the corresponding subarea of the input. This is a useful paradigm in applications such as ,NA,NA
speech recognition. The winner-take-all strategy used in ART1 also facilitates self-,NA,NA
organization.,NA,NA
Learning Vector Quantizer,NA,NA
Suppose the goal is the classification of input vectors. Kohonen’s Vector Quantization is a ,NA,NA
method in which you first gather a finite number of vectors of the dimension of your input ,NA,NA
vector. Kohonen calls these ,NA,NA
codebook vectors,NA,NA
. You then assign groups of these codebook ,NA,NA
"vectors to the different classes under the classification you want to achieve. In other words, ",NA,NA
"you make a correspondence between the codebook vectors and classes, or, partition the set ",NA,NA
of codebook vectors by classes in your classification.,NA,NA
"Now examine each input vector for its distance from each codebook vector, and find the ",NA,NA
nearest or closest codebook vector to it. You identify the input vector with the class to ,NA,NA
which the codebook vector belongs.,NA,NA
"Codebook vectors are updated during training, according to some algorithm. Such an ",NA,NA
"algorithm strives to achieve two things: (1), a codebook vector closest to the input vector is ",NA,NA
"brought even closer to it; and (two), a codebook vector indicating a different class is made ",NA,NA
more distant from the input vector.,NA,NA
"For example, suppose (2, 6) is an input vector, and (3, 10) and (4, 9) are a pair of codebook ",NA,NA
"vectors assigned to different classes. You identify (2, 6) with the class to which (4, 9) ",NA,NA
"belongs, since (4, 9) with a distance of [radic]13 is closer to it than (3, 10) whose distance ",NA,NA
"from (2, 6) is [radic]17. If you add 1 to each component of (3, 10) and subtract 1 from each ",NA,NA
"component of (4, 9), the new distances of these from (2, 6) are [radic]29 and [radic]5, ",NA,NA
"respectively. This shows that (3, 10) when changed to (4, 11) becomes more distant from ",NA,NA
"your input vector than before the change, and (4, 9) is changed to (3, 8), which is a bit ",NA,NA
"closer to (2, 6) than (4, 9) is.",NA,NA
Training continues until all input vectors are classified. You obtain a stage where the ,NA,NA
classification for each input vector remains the same as in the previous cycle of training. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/115-117.html (2 of 3) [21/11/02 21:57:20],NA
This is a process of self-organization.,NA,NA
The Learning Vector Quantizer (LVQ) of Kohonen is a self-organizing network. It ,NA,NA
classifies input vectors on the basis of a set of stored or reference vectors. The B field ,NA,NA
neurons are also called ,NA,NA
grandmother cells,NA,NA
", each of which represents a specific class in the ",NA,NA
reference vector set. Either supervised or unsupervised learning can be used with this ,NA,NA
network. (See Figure 6.2.),NA,NA
Figure 6.2,NA,NA
  Layout for Learning Vector Quantizer.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/115-117.html (3 of 3) [21/11/02 21:57:20]",NA
Previous Table of Contents Next,NA,NA
Associative Memory Models and One-Shot Learning,NA,NA
"The Hopfield memory, Bidirectional Associative memory and Fuzzy Associative memory ",NA,NA
"are all unsupervised networks that perform pattern completion, or pattern association. That ",NA,NA
"is, with corrupted or missing information, these memories are able to recall or complete an ",NA,NA
expected output. Gallant calls the training method used in these networks as ,NA,NA
one-shot ,NA,NA
"learning, since you determine the weight matrix as a function of the completed patterns you ",NA,NA
wish to recall just once. An example of this was shown in Chapter 4 with ,NA,NA
determination of weights for the Hopfield memory.,NA,NA
Learning and Resonance,NA,NA
ART1 is the first neural network model based on adaptive resonance theory of Carpenter ,NA,NA
and Grossberg. When you have a pair of patterns such that when one of them is input to a ,NA,NA
"neural network the output turns out to be the other pattern in the pair, and if this happens ",NA,NA
"consistently in both directions, then you may describe it as resonance. We discuss in ",NA,NA
Chapter 8 bidirectional associative memories and resonance. By the time training is ,NA,NA
"completed, and learning is through, many other pattern pairs would have been presented to ",NA,NA
the network as well. If changes in the short-term memory do not disturb or affect the long-,NA,NA
"term memory, the network shows adaptive resonance. The ART1 model is designed to ",NA,NA
maintain it. Note that this discussion relates largely to stability. ,NA,NA
Learning and Stability,NA,NA
"Learning, convergence, and stability are matters of much interest. As ",NA,NA
learning,NA,NA
 is taking ,NA,NA
"place, you want to know if the process is going to halt at some appropriate point, which is a ",NA,NA
question of ,NA,NA
convergence,NA,NA
. Is what is learned ,NA,NA
stable,NA,NA
", or will the network have to learn all over ",NA,NA
"again, as each new event occurs? These questions have their answers within a mathematical ",NA,NA
model with differential equations developed to describe a learning algorithm. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/117-119.html (1 of 3) [21/11/02 21:57:21],NA
Proofs showing stability are part of the model inventor’s task. One particular tool that aids ,NA,NA
"in the process of showing convergence is the idea of state energy, or cost, to describe ",NA,NA
whether the direction the process is taking can lead to convergence.,NA,NA
The ,NA,NA
Lyapunov,NA,NA
" function, discussed later in this chapter, is found to provide the right energy ",NA,NA
"function, which can be minimized during the operation of the neural network. This function ",NA,NA
"has the property that the value gets smaller with every change in the state of the system, ",NA,NA
thus assuring that a minimum will be reached eventually. The Lyapunov function is ,NA,NA
"discussed further because of its significant utility for neural network models, but briefly ",NA,NA
"because of the high level of mathematics involved. Fortunately, simple forms are derived ",NA,NA
and put into learning algorithms for neural networks. The high-level mathematics is used in ,NA,NA
making the proofs to show the viability of the models.,NA,NA
"Alternatively, temperature relationships can be used, as in the case of the Boltzmann ",NA,NA
"machine, or any other well-suited cost function such as a function of distances used in the ",NA,NA
formulation of the ,NA,NA
Traveling Salesman Problem,NA,NA
", in which the total distance for the tour of ",NA,NA
"the traveling salesman is to be minimized, can be employed. The Traveling Salesman ",NA,NA
"Problem is important and well-known. A set of cities is to be visited by the salesman, each ",NA,NA
"only once, and the aim is to devise a tour that minimizes the total distance traveled. The ",NA,NA
search continues for an efficient algorithm for this problem. Some algorithms solve the ,NA,NA
problem in a large number but not all of the situations. A neural network formulation can ,NA,NA
also work for the Traveling Salesman Problem. You will see more about this in Chapter 15.,NA,NA
Training and Convergence,NA,NA
"Suppose you have a criterion such as energy to be minimized or cost to be decreased, and ",NA,NA
you know the optimum level for this criterion. If the network achieves the optimum value ,NA,NA
"in a finite number of steps, then you have convergence for the operation of the network. ",NA,NA
"Or, if you are making pairwise associations of patterns, there is the prospect of ",NA,NA
"convergence if after each cycle of the network operation, the number of errors is ",NA,NA
decreasing. ,NA,NA
"It is also possible that convergence is slow, so much so that it may seem to take forever to ",NA,NA
"achieve the convergence state. In that case, you should specify a tolerance value and ",NA,NA
"require that the criterion be achieved within that tolerance, avoiding a lot of computing ",NA,NA
time. You may also introduce a ,NA,NA
momentum,NA,NA
 parameter to further change the weight and ,NA,NA
thereby speed up the convergence. One technique used is to add a portion of the previous ,NA,NA
change in weight.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/117-119.html (2 of 3) [21/11/02 21:57:21],NA
"Instead of converging, the operation may result in oscillations. The weight structure may ",NA,NA
keep changing back and forth; learning will never cease. Learning algorithms need to be ,NA,NA
analyzed in terms of convergence as being an essential algorithm property.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/117-119.html (3 of 3) [21/11/02 21:57:21]",NA
Previous Table of Contents Next,NA,NA
Lyapunov Function,NA,NA
"Neural networks are dynamic systems in the learning and training phase of their operation, ",NA,NA
"and convergence is an essential feature, so it was necessary for the researchers developing ",NA,NA
the models and their learning algorithms to find a provable criterion for convergence in a ,NA,NA
"dynamic system. The Lyapunov function, mentioned previously, turned out to be the most ",NA,NA
convenient and appropriate function. It is also referred to as the energy function. The ,NA,NA
function decreases as the system states change. Such a function needs to be found and ,NA,NA
watched as the network operation continues from cycle to cycle. Usually it involves a ,NA,NA
quadratic form. The least mean squared error is an example of such a function. Lyapunov ,NA,NA
function usage assures a system stability that cannot occur without convergence. It is ,NA,NA
"convenient to have one value, that of the Lyapunov function specifying the system ",NA,NA
"behavior. For example, in the Hopfield network, the energy function is a constant times the ",NA,NA
sum of products of outputs of different neurons and the connection weight between them. ,NA,NA
"Since pairs of neuron outputs are multiplied in each term, the entire expression is a ",NA,NA
quadratic form. ,NA,NA
Other Training Issues,NA,NA
"Besides the applications for which a neural network is intended, and depending on these ",NA,NA
"applications, you need to know certain aspects of the model. The length of encoding time ",NA,NA
and the length of learning time are among the important considerations. These times could ,NA,NA
be long but should not be prohibitive. It is important to understand how the network ,NA,NA
"behaves with new inputs; some networks may need to be trained all over again, but some ",NA,NA
"tolerance for distortion in input patterns is desirable, where relevant. Restrictions on the ",NA,NA
format of inputs should be known. ,NA,NA
An advantage of neural networks is that they can deal with nonlinear functions better than ,NA,NA
"traditional algorithms can. The ability to store a number of patterns, or needing more and ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/119-122.html (1 of 3) [21/11/02 21:57:22],NA
more neurons in the output field with an increasing number of input patterns are the kind ,NA,NA
of aspects addressing the capabilities of a network and also its limitations.,NA,NA
Adaptation,NA,NA
Sometimes neural networks are used as ,NA,NA
adaptive filters,NA,NA
", the motivation for such an ",NA,NA
architecture being selectivity. You want the neural network to classify each input pattern ,NA,NA
into its appropriate category. Adaptive models involve changing of connection weights ,NA,NA
"during all their operations, while nonadaptive ones do not alter the weights after the phase ",NA,NA
of learning with exemplars. The Hopfield network is often used in modeling a neural ,NA,NA
"network for optimization problems, and the Backpropagation model is a popular choice in ",NA,NA
most other applications. Neural network models are distinguishable sometimes by their ,NA,NA
"architecture, sometimes by their adaptive methods, and sometimes both. Methods for ",NA,NA
"adaptation, where adaptation is incorporated, assume great significance in the description ",NA,NA
and utility of a neural network model.,NA,NA
"For adaptation, you can modify parameters in an architecture during training, such as the ",NA,NA
learning rate in the backpropagation training method for example. A more radical approach ,NA,NA
is to modify the architecture itself during training. New neural network paradigms change ,NA,NA
the number or layers and the number of neurons in a layer during training. These node ,NA,NA
adding or pruning algorithms are termed ,NA,NA
constructive algorithms,NA,NA
. (See Gallant for more ,NA,NA
details.),NA,NA
Generalization Ability,NA,NA
The analogy for a neural network presented at the beginning of the chapter was that of a ,NA,NA
multidimensional mapping surface that maps inputs to outputs. For each unseen input with ,NA,NA
"respect to a training set, the ",NA,NA
generalization,NA,NA
 ability of a network determines how well the ,NA,NA
mapping surface renders the new input in the output space. A stock market forecaster must ,NA,NA
"generalize well, otherwise you lose money in unseen market conditions. The opposite of ",NA,NA
generalization is ,NA,NA
memorization.,NA,NA
" A pattern recognition system for images of handwriting, ",NA,NA
should be able to generalize a letter A that is handwritten in several different ways by ,NA,NA
"different people. If the system memorizes, then you will not recognize the letter A in all ",NA,NA
"cases, but instead will categorize each letter A variation separately. The trick to achieve ",NA,NA
"generalization is in network architecture, design, and training methodology. You do not ",NA,NA
"want to overtrain your neural network on expected outcomes, but rather should accept a ",NA,NA
slightly worse than minimum error on your training set data. You will learn more about ,NA,NA
generalization in Chapter 14.,NA,NA
Summary,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/119-122.html (2 of 3) [21/11/02 21:57:22],NA
Learning and training are important issues in applying neural networks. Two broad ,NA,NA
categories of network learning are supervised and unsupervised learning. Supervised ,NA,NA
learning provides example outputs to compare to while unsupervised learning does not. ,NA,NA
"During supervised training, external prototypes are used as target outputs and the network ",NA,NA
is given a learning algorithm to follow and calculate new connection weights that bring the ,NA,NA
output closer to the target output. You can refer to networks using unsupervised learning as ,NA,NA
"self-organizing networks, since no external information or guidance is used in learning. ",NA,NA
Several neural network paradigms were presented in this chapter along with their learning ,NA,NA
and training characteristics. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch06/119-122.html (3 of 3) [21/11/02 21:57:22]",NA
Previous Table of Contents Next,NA,NA
Chapter 7 ,NA,NA
Backpropagation ,NA,NA
Feedforward Backpropagation Network,NA,NA
The feedforward backpropagation network is a very popular model in neural networks. It ,NA,NA
"does not have feedback connections, but errors are backpropagated during training. Least ",NA,NA
mean squared error is used. Many applications can be formulated for using a feedforward ,NA,NA
"backpropagation network, and the methodology has been a model for most multilayer ",NA,NA
"neural networks. Errors in the output determine measures of hidden layer output errors, ",NA,NA
which are used as a basis for adjustment of connection weights between the input and ,NA,NA
hidden layers. Adjusting the two sets of weights between the pairs of layers and ,NA,NA
recalculating the outputs is an iterative process that is carried on until the errors fall below a ,NA,NA
tolerance level. Learning rate parameters scale the adjustments to weights. A momentum ,NA,NA
parameter can also be used in scaling the adjustments from a previous iteration and adding ,NA,NA
to the adjustments in the current iteration. ,NA,NA
Mapping,NA,NA
The feedforward backpropagation network maps the input vectors to output vectors. Pairs ,NA,NA
of input and output vectors are chosen to train the network first. Once training is ,NA,NA
"completed, the weights are set and the network can be used to find outputs for new inputs. ",NA,NA
"The dimension of the input vector determines the number of neurons in the input layer, and ",NA,NA
the number of neurons in the output layer is determined by the dimension of the outputs. If ,NA,NA
there are ,NA,NA
k,NA,NA
 neurons in the input layer and ,NA,NA
m,NA,NA
" neurons in the output layer, then this network ",NA,NA
can make a mapping from ,NA,NA
k,NA,NA
-dimensional space to an ,NA,NA
m,NA,NA
"-dimensional space. Of course, what ",NA,NA
that mapping is depends on what pair of patterns or vectors are used as exemplars to train ,NA,NA
"the network, which determine the network weights. Once trained, the network gives ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/123-125.html (1 of 3) [21/11/02 21:57:23],NA
you the image of a new input vector under this mapping. Knowing what mapping you want ,NA,NA
the feedforward backpropagation network to be trained for implies the dimensions of the ,NA,NA
"input space and the output space, so that you can determine the numbers of neurons to have ",NA,NA
in the input and output layers.,NA,NA
Layout,NA,NA
The architecture of a feedforward backpropagation network is shown in Figure 7.1. While ,NA,NA
"there can be many hidden layers, we will illustrate this network with only one hidden ",NA,NA
"layer. Also, the number of neurons in the input layer and that in the output layer are ",NA,NA
"determined by the dimensions of the input and output patterns, respectively. It is not easy ",NA,NA
to determine how many neurons are needed for the hidden layer. In order to avoid ,NA,NA
"cluttering the figure, we will show the layout in Figure 7.1 with five input neurons, three ",NA,NA
"neurons in the hidden layer, and four output neurons, with a few representative ",NA,NA
connections. ,NA,NA
Figure 7.1,NA,NA
  Layout of a feedforward backpropagation network.,NA,NA
"The network has three fields of neurons: one for input neurons, one for hidden processing ",NA,NA
"elements, and one for the output neurons. As already stated, connections are for feed ",NA,NA
forward activity. There are connections from every neuron in field A to every one in field ,NA,NA
"B, and, in turn, from every neuron in field B to every neuron in field C. Thus, there are two ",NA,NA
"sets of weights, those figuring in the activations of hidden layer neurons, and those that ",NA,NA
"help determine the output neuron activations. In training, all of these weights are adjusted ",NA,NA
by considering what can be called a ,NA,NA
cost function,NA,NA
 in terms of the error in the computed ,NA,NA
output pattern and the desired output pattern.,NA,NA
Training,NA,NA
"The feedforward backpropagation network undergoes supervised training, with a finite ",NA,NA
number of pattern pairs consisting of an input pattern and a desired or target output pattern. ,NA,NA
An input pattern is presented at the input layer. The neurons here pass the pattern ,NA,NA
"activations to the next layer neurons, which are in a hidden layer. The outputs of the hidden ",NA,NA
"layer neurons are obtained by using perhaps a bias, and also a threshold function with the ",NA,NA
activations determined by the weights and the inputs. These hidden layer outputs become ,NA,NA
"inputs to the output neurons, which process the inputs using an optional ",NA,NA
bias,NA,NA
 and a ,NA,NA
threshold,NA,NA
 function. The final output of the network is determined by the activations from ,NA,NA
the output layer.,NA,NA
"The computed pattern and the input pattern are compared, a function of this error for each ",NA,NA
"component of the pattern is determined, and adjustment to weights of connections between ",NA,NA
"the hidden layer and the output layer is computed. A similar computation, still based on the ",NA,NA
"error in the output, is made for the connection weights between the input and hidden layers. ",NA,NA
The procedure is repeated with each pattern pair assigned for training the network. Each ,NA,NA
pass through all the training patterns is called a ,NA,NA
cycle,NA,NA
 or an ,NA,NA
epoch.,NA,NA
 The process is then ,NA,NA
repeated as many cycles as needed until the error is within a prescribed tolerance.,"There can be more than one learning rate parameter used in training in a feedforward 
 backpropagation network. You can use one with each set of weights between consecutive 
 layers.",NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Illustration: Adjustment of Weights of Connections from a Neuron in ,NA,NA
the Hidden Layer,NA,NA
We will be as specific as is needed to make the computations clear. First recall that the activation of a ,NA,NA
neuron in a layer other than the input layer is the sum of products of its inputs and the weights ,NA,NA
corresponding to the connections that bring in those inputs. Let us discuss the ,NA,NA
j,NA,NA
th neuron in the hidden ,NA,NA
layer. Let us be specific and say ,NA,NA
j,NA,NA
" = 2. Suppose that the input pattern is (1.1, 2.4, 3.2, 5.1, 3.9) and the ",NA,NA
"target output pattern is (0.52, 0.25, 0.75, 0.97). Let the weights be given for the second hidden layer ",NA,NA
"neuron by the vector (–0.33, 0.07, –0.45, 0.13, 0.37). The activation will be the quantity:",NA,NA
 (-0.33 * 1.1) + (0.07 * 2.4) + (-0.45 * 3.2) + (0.13 * 5.1) ,NA,NA
+ (0.37 * 3.9) = 0.471,NA,NA
"Now add to this an optional bias of, say, 0.679, to give 1.15. If we use the ",NA,NA
sigmoid,NA,NA
 function given by:,NA,NA
" 1 / ( 1+ exp(-x) ),",NA,NA
with ,NA,NA
x,NA,NA
" = 1.15, we get the output of this hidden layer neuron as 0.7595.","We are taking values to a few decimal places only for illustration, unlike the precision that can be 
 obtained on a computer.",NA
"We need the computed output pattern also. Let us say it turns out to be actual =(0.61, 0.41, 0.57, 0.53), ",NA,NA
"while the desired pattern is desired =(0.52, 0.25, 0.75, 0.97). Obviously, there is a discrepancy between ",NA,NA
"what is desired and what is computed. The component-wise differences are given in the vector, desired - ",NA,NA
"actual = (-0.09, -0.16, 0.18, 0.44). We use these to form another vector where each component is a ",NA,NA
"product of the error component, corresponding computed pattern component, and the complement of the ",NA,NA
"latter with respect to 1. For example, for the first component, error is –0.09, computed pattern ",NA,NA
"component is 0.61, and its complement is 0.39. Multiplying these together (0.61*0.39*-0.09), we get -",NA,NA
"0.02. Calculating the other components similarly, we get the vector (–0.02, –0.04, 0.04, 0.11). The ",NA,NA
"desired–actual vector, which is the error vector multiplied by the actual output vector, gives you a value ",NA,NA
"of error reflected back at the output of the hidden layer. This is scaled by a value of (1-output vector), ",NA,NA
which is the first derivative of the output activation function for numerical stability). You will see the ,NA,NA
formulas for this process later in this chapter. ,NA,NA
The backpropagation of errors needs to be carried further. We need now the weights on the connections ,NA,NA
"between the second neuron in the hidden layer that we are concentrating on, and the different output ",NA,NA
"neurons. Let us say these weights are given by the vector (0.85, 0.62, –0.10, 0.21). The error of the ",NA,NA
"second neuron in the hidden layer is now calculated as below, using its output.",NA,NA
 error = 0.7595 * (1 - 0.7595) * ( (0.85 * -0.02) + (0.62 * -0.04),NA,NA
 + ( -0.10 * 0.04) + (0.21 * 0.11)) = -0.0041.,NA,NA
"Again, here we multiply the error (e.g., -0.02) from the output of the current layer, by the output value ",NA,NA
(0.7595) and the value (1-0.7595). We use the weights on the connections between neurons to work ,NA,NA
backwards through the network. ,NA,NA
"Next, we need the ",NA,NA
learning rate parameter,NA,NA
 for this layer; let us set it as 0.2. We multiply this by the ,NA,NA
"output of the second neuron in the hidden layer, to get 0.1519. Each of the components of the vector (–",NA,NA
"0.02, –0.04, 0.04, 0.11) is multiplied now by 0.1519, which our latest computation gave. The result is a ",NA,NA
vector that gives the adjustments to the weights on the connections that go from the second neuron in ,NA,NA
"the hidden layer to the output neurons. These values are given in the vector (–0.003, –0.006, 0.006, ",NA,NA
"0.017). After these adjustments are added, the weights to be used in the next cycle on the connections ",NA,NA
between the second neuron in the hidden layer and the output neurons become those in the vector ,NA,NA
"(0.847, 0.614, –0.094, 0.227).",NA,NA
Illustration: Adjustment of Weights of Connections from a Neuron in ,NA,NA
the Input Layer,NA,NA
Let us look at how adjustments are calculated for the weights on connections going from the ,NA,NA
i,NA,NA
th neuron ,NA,NA
in the input layer to neurons in the hidden layer. Let us take specifically ,NA,NA
i,NA,NA
" = 3, for illustration.",NA,NA
Much of the information we need is already obtained in the previous discussion for the second hidden ,NA,NA
"layer neuron. We have the errors in the computed output as the vector (–0.09, –0.16, 0.18, 0.44), and we ",NA,NA
"obtained the error for the second neuron in the hidden layer as –0.0041, which was not used above. Just ",NA,NA
"as the error in the output is propagated back to assign errors for the neurons in the hidden layer, those ",NA,NA
errors can be propagated to the input layer neurons.,NA,NA
"To determine the adjustments for the weights on connections between the input and hidden layers, we ",NA,NA
"need the errors determined for the outputs of hidden layer neurons, a learning rate parameter, and the ",NA,NA
"activations of the input neurons, which are just the input values for the input layer. Let us take the ",NA,NA
learning rate parameter to be 0.15. Then the weight adjustments for the connections from the third input ,NA,NA
neuron to the hidden layer neurons are obtained by multiplying the particular hidden layer neuron’s ,NA,NA
output error by the learning rate parameter and by the input component from the input neuron. The ,NA,NA
adjustment for the weight on the connection from the third input neuron to the second hidden layer ,NA,NA
"neuron is 0.15 * 3.2 * –0.0041, which works out to –0.002.",NA,NA
"If the weight on this connection is, say, –0.45, then adding the adjustment of -0.002, we get the modified ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/125-127.html (2 of 3) [21/11/02 21:57:24],NA
"weight of –0.452, to be used in the next iteration of the network operation. Similar calculations are made ",NA,NA
to modify all other weights as well.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/125-127.html (3 of 3) [21/11/02 21:57:24]",NA
Previous Table of Contents Next,NA,NA
Adjustments to Threshold Values or Biases,NA,NA
"The bias or the threshold value we added to the activation, before applying the ",NA,NA
threshold ,NA,NA
"function to get the output of a neuron, will also be adjusted based on the error being ",NA,NA
propagated back. The needed values for this are in the previous discussion.,NA,NA
The adjustment for the threshold value of a neuron in the output layer is obtained by ,NA,NA
multiplying the calculated error (not just the difference) in the output at the output neuron ,NA,NA
and the learning rate parameter used in the adjustment calculation for weights at this layer. ,NA,NA
"In our previous example, we have the learning rate parameter as 0.2, and the error vector as ",NA,NA
"(–0.02, –0.04, 0.04, 0.11), so the adjustments to the threshold values of the four output ",NA,NA
"neurons are given by the vector (–0.004, –0.008, 0.008, 0.022). These adjustments are ",NA,NA
added to the current levels of threshold values at the output neurons.,NA,NA
The adjustment to the threshold value of a neuron in the hidden layer is obtained similarly ,NA,NA
by multiplying the learning rate with the computed error in the output of the hidden layer ,NA,NA
"neuron. Therefore, for the second neuron in the hidden layer, the adjustment to its ",NA,NA
"threshold value is calculated as 0.15 * –0.0041, which is –0.0006. Add this to the current ",NA,NA
"threshold value of 0.679 to get 0.6784, which is to be used for this neuron in the next ",NA,NA
training pattern for the neural network.,NA,NA
Another Example of Backpropagation Calculations,NA,NA
"You have seen, in the preceding sections, the details of calculations for one particular ",NA,NA
neuron in the hidden layer in a feedforward backpropagation network with five input ,NA,NA
"neurons and four neurons in the output layer, and two neurons in the hidden layer. ",NA,NA
You are going to see all the calculations in the C++ implementation later in this chapter. ,NA,NA
"Right now, though, we present another example and give the complete picture of the ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/127-129.html (1 of 4) [21/11/02 21:57:25],NA
calculations done in one completed iteration or cycle of backpropagation.,NA,NA
"Consider a feedforward backpropagation network with three input neurons, two neurons in ",NA,NA
"the hidden layer, and three output neurons. The weights on connections from the input ",NA,NA
"neurons to the neurons in the hidden layer are given in Matrix M-1, and those from the ",NA,NA
neurons in the hidden layer to output neurons are given in Matrix M-2.,NA,NA
We calculate the output of each neuron in the hidden and output layers as follows. We add ,NA,NA
a bias or threshold value to the activation of a neuron (call this result ,NA,NA
x,NA,NA
) and use the ,NA,NA
sigmoid,NA,NA
 function below to get the output.,NA,NA
 f(x) = 1/ (1 + e,-x,NA
),NA,NA
Learning parameters used are 0.2 for the connections between the hidden layer neurons ,NA,NA
and output neurons and 0.15 for the connections between the input neurons and the ,NA,NA
neurons in the hidden layer. These values as you recall are the same as in the previous ,NA,NA
"illustration, to make it easy for you to follow the calculations by comparing them with ",NA,NA
similar calculations in the preceding sections. ,NA,NA
"The input pattern is ( 0.52, 0.75, 0.97 ), and the desired output pattern is ( 0.24, 0.17, 0.65). ",NA,NA
The initial weight matrices are as follows:,NA,NA
M-1,NA,NA
 Matrix of weights from input layer to hidden layer,NA,NA
 0.6     - 0.4,NA,NA
 0.2       0.8,NA,NA
 - 0.5       0.3,NA,NA
M-2,NA,NA
 Matrix of weights from hidden layer to output layer,NA,NA
 -0.90       0.43       0.25,NA,NA
 0.11     - 0.67     - 0.75,NA,NA
"The threshold values (or bias) for neurons in the hidden layer are 0.2 and 0.3, while those ",NA,NA
"for the output neurons are 0.15, 0.25, and 0.05, respectively. ",NA,NA
Table 7.1 presents all the results of calculations done in the first iteration. You will see ,NA,NA
modified or new weight matrices and threshold values. You will use these and the original ,NA,NA
input vector and the desired output vector to carry out the next iteration.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/127-129.html (2 of 4) [21/11/02 21:57:25],NA
Table 7.1,NA,NA
Backpropagation Calculations ,NA,NA
Item ,NA,NA
I-1 ,NA,NA
I-2 ,NA,NA
I-3 ,NA,NA
H-1 ,NA,NA
H-2 ,NA,NA
O-1 ,NA,NA
O-2 ,NA,NA
O-3 ,NA,NA
Input ,NA,NA
0.52 ,NA,NA
0.75 ,NA,NA
0.97 ,NA,NA
0.6 ,NA,NA
- 0.4 ,NA,NA
0.24 ,NA,NA
0.17 ,NA,NA
0.65 ,NA,NA
Desired Output ,NA,NA
M-1 Row 1 ,NA,NA
M-1 Row 2 ,NA,NA
0.2 ,NA,NA
0.8 ,NA,NA
- 0.90 ,NA,NA
0.43 ,NA,NA
0.25 ,NA,NA
M-1 Row 3 ,NA,NA
- 0.5 ,NA,NA
0.3 ,NA,NA
M-2 Row 1 ,NA,NA
M-2 Row 2 ,NA,NA
0.2 ,NA,NA
0.3 ,NA,NA
0.11 ,NA,NA
- 0.67 ,NA,NA
- 0.75 ,NA,NA
Threshold ,NA,NA
0.15 ,NA,NA
0.25 ,NA,NA
0.05 ,NA,NA
Activation - H ,NA,NA
- 0.023 0.683 ,NA,NA
- 0.410 - 0.254 - 0.410 ,NA,NA
Activation + ,NA,NA
Threshold -H ,NA,NA
0.177 ,NA,NA
0.983 ,NA,NA
0.544 ,NA,NA
0.728 ,NA,NA
Output -H ,NA,NA
Complement ,NA,NA
0.456 ,NA,NA
0.272 ,NA,NA
Activation -O ,NA,NA
Activation + ,NA,NA
Threshold -O ,NA,NA
- 0.260 - 0.004 - 0.360 ,NA,NA
Output -O ,NA,NA
0.435 ,NA,NA
0.499 ,NA,NA
0.411 ,NA,NA
Complement ,NA,NA
0.565 ,NA,NA
0.501 ,NA,NA
0.589 ,NA,NA
Diff. from ,NA,NA
Target ,NA,NA
- 0.195 - 0.329 0.239 ,NA,NA
Computed Error -,NA,NA
O ,NA,NA
- 0.048 - 0.082 0.058 ,NA,NA
Computed Error -,NA,NA
H ,NA,NA
0.0056 0.0012 ,NA,NA
Adjustment to ,NA,NA
Threshold ,NA,NA
0.0008 0.0002 -0.0096 -0.0164 0.0116 ,NA,NA
Adjustment to M-,NA,NA
2 Column 1 ,NA,NA
Adjustment to M-,NA,NA
2 Column 2 ,NA,NA
-0.0005 -0.0070 ,NA,NA
0.0007 0.0008 ,NA,NA
Adjustment to M-,NA,NA
2 Column 3 ,NA,NA
0.0008 0.0011 ,NA,NA
- 0.91 ,NA,NA
0.412 ,NA,NA
0.262 ,NA,NA
New Matrix M-2 ,NA,NA
Row 1 ,NA,NA
New Matrix M-2 ,NA,NA
Row 2 ,NA,NA
0.096 ,NA,NA
- 0.694 - 0.734 ,NA,NA
New Threshold ,NA,NA
Values -O ,NA,NA
0.1404 0.2336 0.0616 ,NA,NA
Adjustment to M-,NA,NA
1 Row 1 ,NA,NA
Adjustment to M-,NA,NA
1 Row 2 ,NA,NA
Adjustment to M-,NA,NA
1 Row 3 ,NA,NA
New Matrix M-1 ,NA,NA
Row 1 ,NA,NA
New Matrix M-1 ,NA,NA
Row 2 ,NA,NA
New Matrix M-1 ,NA,NA
Row 3 ,NA,NA
New Threshold ,NA,NA
Values -H ,NA,NA
0.0004 -0.0001 ,NA,NA
0.0006 0.0001 ,NA,NA
0.0008 0.0002 ,NA,NA
0.6004 - 0.4 ,NA,NA
0.2006 0.8001 ,NA,NA
-0.4992 0.3002 ,NA,NA
0.2008 0.3002 ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
"The top row in the table gives headings for the columns. They are, Item, I-1, I-2, I-3 (I-k ",NA,NA
"being for input layer neuron k); H-1, H-2 (for hidden layer neurons); and O-1, O-2, O-3 ",NA,NA
(for output layer neurons). ,NA,NA
"In the first column of the table, M-1 and M-2 refer to weight matrices as above. Where an ",NA,NA
"entry is appended with -H, like in Output -H, the information refers to the hidden layer. ",NA,NA
"Similarly, -O refers to the output layer, as in Activation + threshold -O.",NA,NA
"The next iteration uses the following information from the previous iteration, which you ",NA,NA
"can identify from Table 7.1. The input pattern is ( 0.52, 0.75, 0.97 ), and the desired output ",NA,NA
"pattern is ( 0.24, 0.17, 0.65). The current weight matrices are as follows:",NA,NA
M-1,NA,NA
 Matrix of weights from input layer to hidden layer:,NA,NA
 0.6004   - 0.4,NA,NA
 0.2006     0.8001,NA,NA
 - 0.4992     0.3002,NA,NA
M-2,NA,NA
 Matrix of weights from hidden layer to output layer:,NA,NA
 -0.910       0.412      0.262,NA,NA
 0.096      -0.694     -0.734,NA,NA
"The threshold values (or bias) for neurons in the hidden layer are 0.2008 and 0.3002, while ",NA,NA
"those for the output neurons are 0.1404, 0.2336, and 0.0616, respectively. ",NA,NA
You can keep the learning parameters as 0.15 for connections between input and hidden ,NA,NA
"layer neurons, and 0.2 for connections between the hidden layer neurons and output ",NA,NA
"neurons, or you can slightly modify them. Whether or not to change these two parameters ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/129-133.html (1 of 3) [21/11/02 21:57:25],NA
"is a decision that can be made perhaps at a later iteration, having obtained a sense of how ",NA,NA
the process is converging.,NA,NA
If you are satisfied with the rate at which the computed output pattern is getting close to the ,NA,NA
"target output pattern, you would not change these learning rates. If you feel the ",NA,NA
"convergence is much slower than you would like, then the learning rate parameters can be ",NA,NA
adjusted slightly upwards. It is a subjective decision both in terms of when (if at all) and to ,NA,NA
what new levels these parameters need to be revised.,NA,NA
Notation and Equations,NA,NA
You have just seen an example of the process of training in the feedforward ,NA,NA
"backpropagation network, described in relation to one hidden layer neuron and one input ",NA,NA
"neuron. There were a few vectors that were shown and used, but perhaps not made easily ",NA,NA
identifiable. We therefore introduce some notation and describe the equations that were ,NA,NA
implicitly used in the example. ,NA,NA
Notation,NA,NA
Let us talk about two matrices whose elements are the weights on connections. One matrix ,NA,NA
"refers to the interface between the input and hidden layers, and the second refers to that ",NA,NA
between the hidden layer and the output layer. Since connections exist from each neuron in ,NA,NA
"one layer to every neuron in the next layer, there is a vector of weights on the connections ",NA,NA
"going out from any one neuron. Putting this vector into a row of the matrix, we get as many ",NA,NA
rows as there are neurons from which connections are established. ,NA,NA
Let M,1,NA
 and M,2,NA
 be these matrices of weights. Then what does ,NA,NA
M,1,NA
[,NA,NA
i,NA,NA
][,NA,NA
j,NA,NA
] represent? It is the ,NA,NA
weight on the connection from the ,NA,NA
i,NA,NA
th input neuron to the ,NA,NA
j,NA,NA
th neuron in the hidden layer. ,NA,NA
"Similarly, ",NA,NA
M,2,NA
[,NA,NA
i,NA,NA
][,NA,NA
j,NA,NA
] denotes the weight on the connection from the ,NA,NA
i,NA,NA
th neuron in the hidden ,NA,NA
layer and the ,NA,NA
j,NA,NA
th output neuron.,NA,NA
"Next, we will use ",NA,NA
x,NA,NA
", ",NA,NA
y,NA,NA
", ",NA,NA
z,NA,NA
" for the outputs of neurons in the input layer, hidden layer, and ",NA,NA
"output layer, respectively, with a subscript attached to denote which neuron in a given ",NA,NA
layer we are referring to. Let ,NA,NA
P,NA,NA
" denote the desired output pattern, with ",NA,NA
p,i,NA
 as the ,NA,NA
components. Let ,NA,NA
m,NA,NA
" be the number of input neurons, so that according to our notation, (",NA,NA
x,NA,NA
"1, ",NA,NA
x,NA,NA
"2, …, ",NA,NA
x,NA,NA
m) will denote the input pattern. If ,NA,NA
P,NA,NA
" has, say, ",NA,NA
r,NA,NA
" components, the output layer ",NA,NA
needs ,NA,NA
r,NA,NA
 neurons. Let the number of hidden layer neurons be ,NA,NA
n,NA,NA
. Let ,NA,NA
,h,NA
 be the learning rate ,NA,NA
"parameter for the hidden layer, and ",NA,NA
,"o
 ",NA
", that for the output layer. Let ",NA,NA
,NA,NA
 with the appropriate ,NA,NA
"subscript represent the threshold value or bias for a hidden layer neuron, and ",NA,NA
,NA,NA
 with an ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/129-133.html (2 of 3) [21/11/02 21:57:25],NA
appropriate subscript refer to the threshold value of an output neuron.,NA,NA
Let the errors in output at the output layer be denoted by ,NA,NA
ej,NA,NA
s and those at the hidden layer ,NA,NA
by ,NA,NA
t,i,NA
’s. If we use a ,NA,NA
 ,NA,NA
" prefix of any parameter, then we are looking at the change in or ",NA,NA
"adjustment to that parameter. Also, the thresholding function we would use is the ",NA,NA
sigmoid,NA,NA
"function, ",NA,NA
f,NA,NA
(,NA,NA
x,NA,NA
) = 1 / (1 + exp(–,NA,NA
x,NA,NA
)).,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Equations ,NA,NA
Output of ,NA,NA
j,NA,NA
th hidden layer neuron:,NA,NA
y,j,NA
 = f( (,NA,NA
©,i,NA
 x,i,NA
M,1,NA
[ i ][ j ] ) + ,NA,NA
,NA,NA
j ) ,NA,NA
(7.1) ,NA,NA
Output of ,NA,NA
j,NA,NA
th output layer neuron:,NA,NA
z,j,NA
 = f( (,NA,NA
©,i,NA
 y,i,NA
M,2,NA
[ i ][ j ] ) + ,NA,NA
,j,NA
 ) ,NA,NA
(7.2) ,NA,NA
Ith component of vector of output differences: ,NA,NA
desired value - computed value = P,i,NA
 – z,i,NA
Ith component of output error at the output layer: ,NA,NA
e,i,NA
 = ( P,i,NA
 - z,i,NA
) ,NA,NA
(7.3) ,NA,NA
Ith component of output error at the hidden layer:,NA,NA
t,i,NA
 = y,i,NA
 (1 - y,i,NA
 ) (,NA,NA
©,NA,NA
j M,2,NA
[ i ][ j ] e,j,NA
),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/133-135.html (1 of 4) [21/11/02 21:57:26],NA
(7.4) ,NA,NA
Adjustment for weight between ,NA,NA
i,NA,NA
th neuron in hidden layer and ,NA,NA
j,NA,NA
th output neuron:,NA,NA
 ,NA,NA
M,2,NA
[ i ][ j ] = ,NA,NA
,o,NA
 y,i,NA
e,j,NA
(7.5) ,NA,NA
Adjustment for weight between ,NA,NA
i,NA,NA
th input neuron and ,NA,NA
j,NA,NA
th neuron in hidden layer: ,NA,NA
M,1,NA
[ i ][ j ] = ,NA,NA
,h,NA
x,i,NA
t,j,NA
(7.6) ,NA,NA
Adjustment to the threshold value or bias for the ,NA,NA
j,NA,NA
th output neuron:,NA,NA
 ,j,NA
 = ,NA,NA
,o,NA
 e,j,NA
Adjustment to the threshold value or bias for the ,NA,NA
j,NA,NA
th hidden layer neuron:,NA,NA
,j,NA
 = ,NA,NA
,h,NA
 e,j,NA
For use of momentum parameter ,NA,NA
,NA,NA
" (more on this parameter in Chapter 13), instead of ",NA,NA
"equations 7.5 and 7.6, use:",NA,NA
 ,NA,NA
M,2,NA
[ i ][ j ] ( t ) = ,NA,NA
,o,NA
 y,i,NA
e,j,NA
 + ,NA,NA
 ,NA,NA
M,2,NA
[ i ][ j ] ( t - 1 ) ,NA,NA
(7.7) ,NA,NA
and,NA,NA
 ,NA,NA
M,1,NA
[ i ][ j ] ( t ) = ,NA,NA
,h,NA
 x,i,NA
t,j,NA
 + ,NA,NA
 ,NA,NA
M,1,NA
[ i ][ j ] (t - 1) ,NA,NA
(7.8) ,NA,NA
C++ Implementation of a Backpropagation Simulator,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/133-135.html (2 of 4) [21/11/02 21:57:26],NA
The backpropagation simulator of this chapter has the following design objectives: ,NA,NA
1.,NA,NA
  Allow the user to specify the number and size of all layers. ,NA,NA
2.,NA,NA
  Allow the use of one or more hidden layers. ,NA,NA
3.,NA,NA
  Be able to save and restore the state of the network. ,NA,NA
4.,NA,NA
  Run from an arbitrarily large training data set or test data set. ,NA,NA
5.,NA,NA
  Query the user for key network and simulation parameters. ,NA,NA
6.,NA,NA
  ,NA,NA
Display key information at the end of the simulation. ,NA,NA
7.,NA,NA
  Demonstrate the use of some C++ features. ,NA,NA
A Brief Tour of How to Use the Simulator,NA,NA
"In order to understand the C++ code, let us have an overview of the functioning of the ",NA,NA
program. ,NA,NA
There are two modes of operation in the simulator. The user is queried first for which ,NA,NA
mode of operation is desired. The modes are ,NA,NA
Training,NA,NA
 mode and ,NA,NA
Nontraining,NA,NA
 mode (,NA,NA
Test ,NA,NA
mode).,NA,NA
Training Mode,NA,NA
"Here, the user provides a training file in the current directory called training.dat. This file ",NA,NA
"contains exemplar pairs, or patterns. Each pattern has a set of inputs followed by a set of ",NA,NA
"outputs. Each value is separated by one or more spaces. As a convention, you can use a ",NA,NA
few extra spaces to separate the inputs from the outputs. Here is an example of a ,NA,NA
training.dat file that contains two patterns: ,NA,NA
 0.4 0.5 0.89           -0.4 -0.8,NA,NA
 0.23 0.8 -0.3          0.6 0.34,NA,NA
"In this example, the first pattern has inputs 0.4, 0.5, and 0.89, with an expected output of –",NA,NA
"0.4 and –0.8. The second pattern has inputs of 0.23, 0.8, and –0.3 and outputs of 0.6 and ",NA,NA
"0.34. Since there are three inputs and two outputs, the input layer size for the network must ",NA,NA
be three neurons and the output layer size must be two neurons. Another file that is used in ,NA,NA
training is the weights file. Once the simulator reaches the error tolerance that was specified ,NA,NA
"by the user, or the maximum number of iterations, the simulator saves the state of the ",NA,NA
"network, by saving all of its weights in a file called ",NA,NA
weights.dat.,NA,NA
 This file can then be used ,NA,NA
subsequently in another run of the simulator in Nontraining mode. To provide some idea of ,NA,NA
"how the network has done, information about the total and average error is ",NA,NA
"presented at the end of the simulation. In addition, the output generated by the network for ",NA,NA
the last pattern vector is provided in an output file called ,NA,NA
output.dat.,NA,NA
Nontraining Mode (Test Mode),NA,NA
"In this mode, the user provides test data to the simulator in a file called ",NA,NA
test.dat.,NA,NA
 This file ,NA,NA
"contains only input patterns. When this file is applied to an already trained network, an ",NA,NA
"output.dat file is generated, which contains the outputs from the network for all of the input ",NA,NA
"patterns. The network goes through one cycle of operation in this mode, covering all the ",NA,NA
"patterns in the test data file. To start up the network, the weights file, weights.dat is read to ",NA,NA
initialize the state of the network. The user must provide the same network size parameters ,NA,NA
used to train the network.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/133-135.html (4 of 4) [21/11/02 21:57:26]",NA
Previous Table of Contents Next,NA,NA
Operation,NA,NA
The first thing to do with your simulator is to train a network with an architecture you ,NA,NA
choose. You can select the number of layers and the number of hidden layers for your ,NA,NA
network. Keep in mind that the input and output layer sizes are dictated by the input ,NA,NA
patterns you are presenting to the network and the outputs you seek from the network. Once ,NA,NA
"you decide on an architecture, perhaps a simple three-layer network with one hidden layer, ",NA,NA
you prepare training data for it and save the data in the training.dat file. After this you are ,NA,NA
ready to train. You provide the simulator with the following information: ,NA,NA
•,NA,NA
  The mode (select 1 for training) ,NA,NA
•,NA,NA
"  The values for the error tolerance and the learning rate parameter, lambda or beta ",NA,NA
•,NA,NA
  ,NA,NA
"The maximum number of cycles, or passes through the training data you’d like to ",NA,NA
try ,NA,NA
•,NA,NA
"  The number of layers (between three and five, three implies one hidden layer, ",NA,NA
while five implies three hidden layers) ,NA,NA
•,NA,NA
"  The size for each layer, from the input to the output ",NA,NA
The simulator then begins training and reports the current cycle number and the average ,NA,NA
error for each cycle. ,NA,NA
You should watch the error to see that it is on the whole decreasing ,NA,NA
with time,NA,NA
". If it is not, you should restart the simulation, because this will start with a brand ",NA,NA
"new set of random weights and give you another, possibly better, solution. Note that there ",NA,NA
will be legitimate periods where the error ,NA,NA
may,NA,NA
 increase for some time. Once the simulation ,NA,NA
"is done you will see information about the number of cycles and patterns used, and the total ",NA,NA
and average error that resulted. The weights are saved in the weights.dat file. You can ,NA,NA
rename this file to use this particular state of the network later. You can infer the size and ,NA,NA
"number of layers from the information in this file, as will be shown in the next section for ",NA,NA
the ,NA,NA
weights.dat,NA,NA
 file format. You can have a peek at the ,NA,NA
output.dat,NA,NA
 file to see the kind of ,NA,NA
training result you have achieved. To get a full-blown accounting of each pattern and the ,NA,NA
"match to that pattern, copy the training file to the test file and delete the output information ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/135-137.html (1 of 3) [21/11/02 21:57:27],NA
from it. You can then run ,NA,NA
Test,NA,NA
 mode to get a full list of all the input stimuli and responses ,NA,NA
in the output.dat file.,NA,NA
Summary of Files Used in the Backpropagation Simulator,NA,NA
"Here is a list of the files for your reference, as well as what they are used for. ",NA,NA
•weights.dat,NA,NA
 You can look at this file to see the weights for the network. It shows ,NA,NA
"the layer number followed by the weights that feed into the layer. The first layer, or ",NA,NA
"input layer, layer zero, does not have any weights associated with it. An example of ",NA,NA
"the weights.dat file is shown as follows for a network with three layers of sizes 3, 5, ",NA,NA
and 2. Note that the row width for layer ,NA,NA
n,NA,NA
 matches the column length for layer ,NA,NA
n,NA,NA
 + 1: ,NA,NA
1 -0.199660 -0.859660 -0.339660 -0.25966 0.520340,NA,NA
1  0.292860 -0.487140 0.212860 -0.967140 -0.427140,NA,NA
1  0.542106 -0.177894 0.322106 -0.977894 0.562106,NA,NA
2 -0.175350 -0.835350,NA,NA
2 -0.330167 -0.250167,NA,NA
2  0.503317 0.283317,NA,NA
2 -0.477158 0.222842,NA,NA
2 -0.928322 -0.388322,NA,NA
"In this weights file the row width for layer 1 is 5, corresponding to the output of that ",NA,NA
"(middle) layer. The input for the layer is the column length, which is 3, just as ",NA,NA
"specified. For layer 2, the output size is the row width, which is 2, and the input size ",NA,NA
"is the column length, 5, which is the same as the output for the middle layer. You ",NA,NA
can read the weights file to find out how things look. ,NA,NA
•training.dat,NA,NA
 This file contains the input patterns for training. You can have as large ,NA,NA
a file as you’d like without degrading the performance of the simulator. The ,NA,NA
simulator caches data in memory for processing. This is to improve the speed of the ,NA,NA
"simulation since disk accesses are expensive in time. A data buffer, which has a ",NA,NA
maximum size specified in a ,NA,NA
#define,NA,NA
" statement in the program, is filled with data ",NA,NA
from the training.dat file whenever data is needed. The format for the training.dat ,NA,NA
file has been shown in the ,NA,NA
Training,NA,NA
 mode section. ,NA,NA
•test.dat,NA,NA
 The test.dat file is just like the training.dat file but without expected ,NA,NA
outputs. You use this file with a trained neural network in ,NA,NA
Test,NA,NA
 mode to see what ,NA,NA
responses you get for untrained data. ,NA,NA
•output.dat,NA,NA
 The output.dat file contains the results of the simulation. In ,NA,NA
Test,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/135-137.html (2 of 3) [21/11/02 21:57:27],NA
"mode, the input and output vectors are shown for all pattern vectors. In the ",NA,NA
Simulator,NA,NA
" mode, the expected output is also shown, but only the last vector in the ",NA,NA
"training set is presented, since the training set is usually quite large. ",NA,NA
Shown here is an example of an output file in ,NA,NA
Training,NA,NA
 mode: ,NA,NA
for input vector: ,NA,NA
0.400000  -0.400000 ,NA,NA
output vector is: ,NA,NA
0.880095 ,NA,NA
expected output vector is: ,NA,NA
0.900000,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/135-137.html (3 of 3) [21/11/02 21:57:27]",NA
Previous Table of Contents Next,NA,NA
C++ Classes and Class Hierarchy,NA,NA
"So far, you have learned how we address most of the objectives outlined for this program. The only ",NA,NA
objective left involves the demonstration of some C++ features. In this program we use a class ,NA,NA
hierarchy with the ,NA,NA
inheritance,NA,NA
" feature. Also, we use ",NA,NA
polymorphism,NA,NA
 with dynamic binding and ,NA,NA
function overloading with static binding.,NA,NA
First let us look at the class hierarchy used for this program (see Figure 7.2). An ,NA,NA
abstract,NA,NA
 class is a ,NA,NA
"class that is never meant to be instantiated as an object, but serves as a base class from which others ",NA,NA
can inherit functionality and interface definitions. The layer class is such a class. You will see ,NA,NA
"shortly that one of its functions is set = zero, which indicates that this class is an ",NA,NA
abstract base class,NA,NA
. ,NA,NA
From the layer class are two branches. One is the ,NA,NA
input_layer,NA,NA
" class, and the other is the ",NA,NA
output_layer,NA,NA
 class. The middle layer class is very much like the output layer in function and so ,NA,NA
inherits from the ,NA,NA
output_layer,NA,NA
 class.,NA,NA
Figure 7.2,NA,NA
  Class hierarchy used in the backpropagation simulator.,NA,NA
Function overloading can be seen in the definition of the ,NA,NA
calc_error(),NA,NA
 function. It is used in the ,NA,NA
input_layer,NA,NA
" with no parameters, while it is used in the ",NA,NA
output_layer,NA,NA
 (which the ,NA,NA
input_layer ,NA,NA
"inherits from) with one parameter. Using the same function name is not a problem, and this is ",NA,NA
referred to as ,NA,NA
overloading,NA,NA
". Besides function overloading, you may also have operator overloading, ",NA,NA
"which is using an operator that performs some familiar function like + for addition, for another ",NA,NA
"function, say, vector addition.",NA,NA
When you have overloading with the same parameters and the keyword ,NA,NA
virtual,NA,NA
", then you have the ",NA,NA
potential for ,NA,NA
dynamic binding,NA,NA
", which means that you determine which overloaded function to ",NA,NA
execute at run time and not at compile time. Compile time binding is referred to as ,NA,NA
static binding,NA,NA
. If ,NA,NA
you put a bunch of C++ objects in an array of pointers to the ,NA,NA
base,NA,NA
" class, and then go through a loop ",NA,NA
"that indexes each pointer and executes an overloaded virtual function that pointer is pointing to, then ",NA,NA
you will be using dynamic binding. This is exactly the case in the function ,NA,NA
"calc_out(),",NA,NA
 which is ,NA,NA
declared with the virtual keyword in the ,NA,NA
layer,NA,NA
 base class. Each descendant of layer can provide a ,NA,NA
version of ,NA,NA
"calc_out(),",NA,NA
" which differs in functionality from the base class, and the correct function ",NA,NA
will be selected at run time based on the object’s identity. In this case ,NA,NA
"calc_out(),",NA,NA
 which is a ,NA,NA
"function to calculate the outputs for each layer, is different for the input layer than for the other two ",NA,NA
types of layers.,NA,NA
Let’s look at some details in the header file in Listing 7.1:,NA,NA
Listing 7.1,NA,NA
 Header file for the backpropagation simulator,NA,NA
"// layer.h            V.Rao, H. Rao ",NA,NA
// header file for the layer class hierarchy and ,NA,NA
// the network class ,NA,NA
#define MAX_LAYERS    5 ,NA,NA
#define MAX_VECTORS   100 ,NA,NA
class network; ,NA,NA
class layer ,NA,NA
{ ,NA,NA
protected:,NA,NA
 int num_inputs;,NA,NA
 int num_outputs;,NA,NA
 float *outputs;// pointer to array of outputs,NA,NA
" float *inputs; // pointer to array of inputs, which",NA,NA
 // are outputs of some other layer,NA,NA
 ,NA,NA
friend network; ,NA,NA
public:,NA,NA
 virtual void calc_out()=0; ,NA,NA
}; ,NA,NA
class input_layer: public layer ,NA,NA
{ ,NA,NA
private: ,NA,NA
public:,NA,NA
" input_layer(int, int);",NA,NA
 ~input_layer();,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/137-142.html (2 of 4) [21/11/02 21:57:28],NA
 virtual void calc_out();,NA,NA
};,NA,NA
class middle_layer;,NA,NA
class output_layer:   public layer ,NA,NA
{ ,NA,NA
protected:,NA,NA
 float * weights;,NA,NA
 float * output_errors;    // array of errors at output,NA,NA
 float ,NA,NA
* back_errors;      // array of errors back-propagated,NA,NA
 float * ,NA,NA
expected_values;  // to inputs,NA,NA
 friend network;,NA,NA
public:,NA,NA
};,NA,NA
" output_layer(int, int);",NA,NA
 ~output_layer();,NA,NA
 virtual void calc_out();,NA,NA
 void calc_error(float &);,NA,NA
 void randomize_weights();,NA,NA
 void update_weights(const float); ,NA,NA
void list_weights();,NA,NA
" void write_weights(int, FILE *); ",NA,NA
"void read_weights(int, FILE *); ",NA,NA
void list_errors();,NA,NA
 void list_outputs();,NA,NA
class middle_layer:   public output_layer ,NA,NA
{,NA,NA
private:,NA,NA
public:,NA,NA
" middle_layer(int, ",NA,NA
int);,NA,NA
 ~middle_layer();,NA,NA
 void calc_error(); ,NA,NA
};,NA,NA
class network,NA,NA
{,NA,NA
private:,NA,NA
 layer *layer_ptr[MAX_LAYERS];,NA,NA
 int number_of_layers;,NA,NA
 int layer_size[MAX_LAYERS];,NA,NA
 float *buffer;,NA,NA
 fpos_t position;,NA,NA
 unsigned training;,NA,NA
public:,NA,NA
 network();,NA,NA
 ~network();,NA,NA
 void set_training(const unsigned &);,NA,NA
 unsigned get_training_value();,NA,NA
 void get_layer_info();,NA,NA
 void set_up_network();,NA,NA
 void randomize_weights();,NA,NA
 void update_weights(const float);,NA,NA
 void write_weights(FILE *);,NA,NA
 void read_weights(FILE *);,NA,NA
 void list_weights();,NA,NA
 void write_outputs(FILE *);,NA,NA
 void list_outputs();,NA,NA
 void list_errors();,NA,NA
 void forward_prop();,NA,NA
 void backward_prop(float &);,NA,NA
 int fill_IObuffer(FILE *);,NA,NA
 void set_up_pattern(int);,NA,NA
};,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch07/137-142.html (4 of 4) [21/11/02 21:57:28]",NA
Details of the Backpropagation Header File,"At the top of the file, there are two 
 #define
  statements, which are used to set the maximum number of layers 
 that can be used, currently five, and the maximum number of training or test vectors that can be read into an 
 I/O buffer. This is currently 100. You can increase the size of the buffer for better speed at the cost of 
 increased memory usage.
  
 The following are definitions in the 
 layer
  base class. Note that the number of inputs and outputs are 
 protected
  data members, which means that they can be accessed freely by descendants of the class.
  
 int num_inputs;
  
 int num_outputs;
  
 float *outputs;      // pointer to array of outputs
  
 float *inputs;       // pointer to array of inputs, which
  
  // are outputs of some other layer
  
 friend network;
  
 There are also two pointers to arrays of floats in this class. They are the pointers to the outputs in a given 
 layer and the inputs to a given layer. To get a better idea of what a layer encompasses, Figure 7.3 shows 
 you a small feedforward backpropagation network, with a dotted line that shows you the three layers for 
 that network. A layer contains neurons and weights. The layer is responsible for calculating its output 
 (
 calc_out()),
  stored in the 
 float * outputs
  array, and errors 
 (calc_error())
  for each of its respective 
 neurons. The errors are stored in another array called 
 float * output_errors
  defined in the 
 output
  class. 
  
 Note that the 
 input
  class does not have any weights associated with it and therefore is a special case. It 
 does not need to provide any data members or function members related to errors or backpropagation. The 
 only purpose of the input layer is to store data to be forward propagated to the next layer.
  
  
 Figure 7.3
   Organization of layers for backpropagation program.
  
 With the output layer, there are a few more arrays present. First, for storing backpropagated errors, there is 
 an array called 
 float * back_errors
 . There is a weights array called 
 float * weights
 , and finally, for storing 
 the expected values that initiate the error calculation process, there is an array called 
 float *",NA
Details of the Backpropagation Implementation File,"The implementation of the classes and methods is the next topic. Let’s look at the layer.cpp file in Listing 
 7.2. 
  
 Listing 7.2
  layer.cpp implementation file for the backpropagation simulator
  
 // layer.cpp           V.Rao, H.Rao 
  
 // compile for floating point hardware if available 
 #include <stdio.h> 
  
 #include <iostream.h> 
  
 #include <stdlib.h> 
  
 #include <math.h> 
  
 #include <time.h> 
  
 #include ""layer.h""
  
 inline float squash(float input) 
  
 // squashing function 
  
 // use sigmoid -- can customize to something 
 // else if desired; can add a bias term too 
 //",NA
A Look at the Functions in the layer.cpp File,"The following is a listing of the functions in the layer.cpp file along with a brief statement of each one's 
 purpose. 
  
 •void set_training(const unsigned &)
  Sets the value of the private data member, training; use 1 
 for training mode, and 0 for test mode. 
  
 •unsigned get_training_value()
  Gets the value of the training constant that gives the mode in use. 
 •void get_layer_info()
  Gets information about the number of layers and layer sizes from the user. 
 •void set_up_network()
  This routine sets up the connections between layers by assigning pointers 
 appropriately. 
  
 •void randomize_weights()
  At the beginning of the training process, this routine is used to 
 randomize all of the weights in the network. 
  
 •void update_weights(const float)
  As part of training, weights are updated according to the 
 learning law used in backpropagation. 
  
 •void write_weights(FILE *)
  This routine is used to write weights to a file. 
  
 •void read_weights(FILE *)
  This routine is used to read weights into the network from a file. 
  
 •void list_weights()
  This routine can be used to list weights while a simulation is in progress. 
 •void write_outputs(FILE *)
  This routine writes the outputs of the network to a file. 
  
 •void list_outputs()
  This routine can be used to list the outputs of the network while a simulation is 
 in progress. 
  
 •void list_errors()
  Lists errors for all layers while a simulation is in progress. 
  
 •void forward_prop()
  Performs the forward propagation. 
  
 •void backward_prop(float &)
  Performs the backward error propagation. 
  
 •int fill_IObuffer(FILE *)
  This routine fills the internal IO buffer with data from the training or 
 test data sets. 
  
 •void set_up_pattern(int)
  This routine is used to set up one pattern from the IO buffer for 
 training. 
  
 •inline float squash(float input)
  This function performs the sigmoid function. 
  
 •inline float randomweight (unsigned unit)
  This routine returns a random weight between –1 and 1; 
 use 1 to initialize the generator, and 0 for all subsequent calls. 
  
  
 Note that the functions 
 squash(float)
  and 
 randomweight(unsigned)
  are declared inline. This means that the 
 function's source code is inserted wherever it appears. This increases code size, but also increases speed 
 because a function call, which is expensive, is avoided.",NA
Previous Table of Contents Next,NA,NA
"The backprop.cpp file implements the simulator controls. First, data is accepted from the user for ",NA,NA
network parameters. Assuming ,NA,NA
Training,NA,NA
" mode is used, the training file is opened and data is read ",NA,NA
from the file to fill the IO buffer. Then the main loop is executed where the network processes ,NA,NA
"pattern by pattern to complete a cycle, which is one pass through the entire training data set. (The ",NA,NA
"IO buffer is refilled as required during this process.) After executing one cycle, the file pointer is ",NA,NA
reset to the beginning of the file and another cycle begins. The simulator continues with cycles ,NA,NA
until one of the two fundamental criteria is met:,NA,NA
1.,NA,NA
  The maximum cycle count specified by the user is exceeded. ,NA,NA
2.,NA,NA
  The average error per pattern for the latest cycle is less than the error tolerance ,NA,NA
specified by the user. ,NA,NA
"When either of these occurs, the simulator stops and reports out the error achieved, and saves ",NA,NA
weights in the weights.dat file and one output vector in the output.dat file. ,NA,NA
In ,NA,NA
Test,NA,NA
" mode, exactly one cycle is processed by the network and outputs are written to the ",NA,NA
output.dat file. At the beginning of the simulation in ,NA,NA
Test,NA,NA
" mode, the network is set up with weights ",NA,NA
"from the weights.dat file. To simplify the program, the user is requested to enter the number of ",NA,NA
"layers and size of layers, although you could have the program figure this out from the weights ",NA,NA
file.,NA,NA
Compiling and Running the Backpropagation Simulator,NA,NA
Compiling the backprop.cpp file will compile the simulator since layer.cpp is included in ,NA,NA
"backprop.cpp. To run the simulator, once you have created an executable (using 80X87 floating ",NA,NA
"point hardware if available), you type in ",NA,NA
backprop,NA,NA
 and see the following screen (user input in ,NA,NA
italic):,NA,NA
C++ Neural Networks and Fuzzy Logic,NA,NA
 Backpropagation simulator,NA,NA
 version 1,NA,NA
Please enter ,NA,NA
1,NA,NA
" for TRAINING on, or ",NA,NA
0,NA,NA
 for off:,NA,NA
Use training to change weights according to your ,NA,NA
expected outputs. Your training.dat file should contain ,NA,NA
a set of inputs and expected outputs. The number of ,NA,NA
inputs determines the size of the first (input) layer ,NA,NA
while the number of outputs determines the size of the ,NA,NA
last (output) layer :,NA,NA
1,NA,NA
-> Training mode is *ON*. weights will be saved ,NA,NA
in the file weights.dat at the end of the ,NA,NA
current set of input (training) data,NA,NA
 Please enter in the error_tolerance,NA,NA
" -- between 0.001 to 100.0, try 0.1 to start --",NA,NA
"and the learning_parameter, beta",NA,NA
" -- between 0.01 to 1.0, try 0.5 to start --",NA,NA
 separate entries by a space,NA,NA
 example: 0.1 0.5 sets defaults mentioned :,NA,NA
0.2 0.25,NA,NA
Please enter the maximum cycles for the simulation ,NA,NA
A cycle is one pass through the data set.,NA,NA
Try a value of 10 to start with ,NA,NA
Please enter in the number of layers for your network.,NA,NA
You can have a minimum of three to a maximum of five.,NA,NA
three implies one hidden layer; five implies three hidden layers:,NA,NA
3,NA,NA
Enter in the layer sizes separated by spaces.,NA,NA
"For a network with three neurons in the input layer, ",NA,NA
"two neurons in a hidden layer, and four neurons in the ",NA,NA
"output layer, you would enter: ",NA,NA
3 2 4,NA,NA
.,NA,NA
You can have up to three hidden layers for five maximum entries :,NA,NA
2 2 1,NA,NA
1        0.353248 ,NA,NA
2        0.352684 ,NA,NA
3        0.352113 ,NA,NA
4        0.351536,NA,NA
5        0.350954 ,NA,NA
...,NA,NA
299      0.0582381 ,NA,NA
300      0.0577085,NA,NA
------------------------,NA,NA
 done:   results in file output.dat,NA,NA
 training: last vector only,NA,NA
 not ,NA,NA
training: full cycle,NA,NA
 weights saved in file ,NA,NA
weights.dat-->average error per cycle = 0.20268 <-,NA,NA
-,NA,NA
-->error last cycle = 0.0577085 <--,NA,NA
->error last cycle per pattern= 0.0577085 <--,NA,NA
------>total cycles = 300 <--,NA,NA
------>total patterns = 300 <--,"The cycle number and the average error per pattern is displayed as the simulation progresses (not 
 all values shown). You can monitor this to make sure the simulator is converging on a solution. If 
 the error does not seem to decrease beyond a certain point, but instead drifts or blows up, then you 
 should start the simulator again with a new starting point defined by the random weights 
 initializer. Also, you could try decreasing the size of the learning rate parameter. Learning may be 
 slower, but this may allow a better minimum to be found.",NA
This example shows just one pattern in the training set with two inputs and one output. The ,NA,NA
results along with the (one) last pattern are shown as follows from the file output.dat: ,NA,NA
for input vector: ,NA,NA
0.400000  -0.400000 ,NA,NA
output vector is: ,NA,NA
0.842291 ,NA,NA
expected output vector is: ,NA,NA
0.900000,NA,NA
"The match is pretty good, as can be expected, since the optimization is easy for the network; there ",NA,NA
is only one pattern to worry about. Let’s look at the final set of weights for this simulation in ,NA,NA
weights.dat. These weights were obtained by updating the weights for 300 cycles with the ,NA,NA
learning law: ,NA,NA
 1 0.175039 0.435039,NA,NA
 1 -1.319244 -0.559244,NA,NA
 2 0.358281,NA,NA
 2 2.421172,NA,NA
We’ll leave the backpropagation simulator for now and return to it in a later chapter for further ,NA,NA
exploration. You can experiment a number of different ways with the simulator: ,NA,NA
•,NA,NA
  Try a different number of layers and layer sizes for a given problem. ,NA,NA
•,NA,NA
  Try different learning rate parameters and see its effect on convergence and training ,NA,NA
time. ,NA,NA
•,NA,NA
  Try a very large learning rate parameter (should be between 0 and 1); try a number over 1 ,NA,NA
and note the result. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Summary,NA,NA
"In this chapter, you learned about one of the most powerful neural network algorithms ",NA,NA
"called backpropagation. Without having feedback connections, propagating only errors ",NA,NA
"appropriately to the hidden layer and input layer connections, the algorithm uses the so-",NA,NA
called generalized delta rule and trains the network with exemplar pairs of patterns. It is ,NA,NA
difficult to determine how many hidden-layer neurons are to be provided for. The number ,NA,NA
"of hidden layers could be more than one. In general, the size of the hidden layer(s) is ",NA,NA
related to the features or distinguishing characteristics that should be discerned from the ,NA,NA
data. Our example in this chapter relates to a simple case where there is a single hidden ,NA,NA
"layer. The outputs of the output neurons, and therefore of the network, are vectors with ",NA,NA
"components between 0 and 1, since the ",NA,NA
thresholding,NA,NA
 function is the ,NA,NA
sigmoid,NA,NA
 function. ,NA,NA
"These values can be scaled, if necessary, to get values in another interval.",NA,NA
"Our example does not relate to any particular function to be computed by the network, but ",NA,NA
"inputs and outputs were randomly chosen. What this can tell you is that, if you do not know ",NA,NA
"the functional equation between two sets of vectors, the feedback backpropagation network ",NA,NA
"can find the mapping for any vector in the domain, even if the functional equation is not ",NA,NA
"found. For all we know, that function could be nonlinear as well.",NA,NA
There is one important fact you need to remember about the backpropagation algorithm,NA,NA
. ,NA,NA
Its steepest descent procedure in training does not guarantee finding a global or overall ,NA,NA
"minimum, it can find only a local minimum of the energy surface.",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathemati...C++_Neural_Networks_and_Fuzzy_Logic/ch07/176-176.html [21/11/02 21:57:33]",NA
Previous Table of Contents Next,NA,NA
Chapter 8 ,NA,NA
BAM: Bidirectional Associative Memory ,NA,NA
Introduction,NA,NA
The ,NA,NA
Bidirectional Associative Memory,NA,NA
 (,NA,NA
BAM,NA,NA
) model has a neural network of two layers and is ,NA,NA
"fully connected from each layer to the other. That is, there are feedback connections from the ",NA,NA
"output layer to the input layer. However, the weights on the connections between any two given ",NA,NA
neurons from different layers are the same. You may even consider it to be a single bidirectional ,NA,NA
connection with a single weight. The matrix of weights for the connections from the output layer ,NA,NA
to the input layer is simply the transpose of the matrix of weights for the connections between the ,NA,NA
input and output layer. If we denote the matrix for forward connection weights by ,NA,NA
W,NA,NA
", then ",NA,NA
W,T,NA
 is ,NA,NA
"the matrix of weights for the output layer to input layer connections. As you recall, the transpose ",NA,NA
of a matrix is obtained simply by interchanging the rows and the columns of the matrix.,NA,NA
"There are two layers of neurons, an input layer and an output layer. There are no lateral ",NA,NA
"connections, that is, no two neurons within the same layer are connected. ",NA,NA
Recurrent,NA,NA
" connections, ",NA,NA
"which are feedback connections to a neuron from itself, may or may not be present. The ",NA,NA
"architecture is quite simple. Figure 8.1 shows the layout for this neural network model, using ",NA,NA
only three input neurons and two output neurons. There are feedback connections from Field A to ,NA,NA
Field B and vice-versa. This figure also indicates the presence of inputs and outputs at each of the ,NA,NA
two fields for the bidirectional associative memory network. Connection weights are also shown ,NA,NA
"as labels on only a few connections in this figure, to avoid cluttering. The general case is ",NA,NA
analogous.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/179-182.html (1 of 4) [21/11/02 21:57:34],NA
Figure 8.1,NA,NA
  Layout of a BAM network,NA,NA
Inputs and Outputs,NA,NA
"The input to a BAM network is a vector of real numbers, usually in the set { –1, +1 }. The output ",NA,NA
"is also a vector of real numbers, usually in the set { –1, +1 }, with the same or different ",NA,NA
"dimension from that of the input. These vectors can be considered patterns, and the network makes ",NA,NA
"heteroassociation of patterns. If the output is required to be the same as input, then you are asking ",NA,NA
"the network to make autoassociation, which it does, and it becomes a special case of the general ",NA,NA
activity of this type of neural network. ,NA,NA
"For inputs and outputs that are not outside the set containing just –1 and +1, try following this ",NA,NA
"next procedure. You can first make a mapping into binary numbers, and then a mapping of each ",NA,NA
"binary digit into a bipolar digit. For example, if your inputs are first names of people, each ",NA,NA
"character in the name can be replaced by its ASCII code, which in turn can be changed to a ",NA,NA
"binary number, and then each binary digit 0 can be replaced by –1. For example, the ASCII code ",NA,NA
"for the letter R is 82, which is 1010010, as a binary number. This is mapped onto the bipolar ",NA,NA
"string 1 –1 1 –1 –1 1 –1. If a name consists of three characters, their ASCII codes in binary can ",NA,NA
be concatenated or juxtaposed and the corresponding bipolar string obtained. This bipolar string ,NA,NA
can also be looked upon as a vector of bipolar characters.,NA,NA
Weights and Training,NA,NA
"BAM does not modify weights during its operation, and as mentioned in Chapter 6, like the ",NA,NA
"Hopfield network, uses one-shot training. The adaptive variety of BAM, called the ",NA,NA
Adaptive ,NA,NA
Bidirectional Associative Memory,NA,NA
", (",NA,NA
ABAM),NA,NA
 undergoes supervised iterative training. BAM needs ,NA,NA
some exemplar pairs of vectors. The pairs used as exemplars are those that require ,NA,NA
"heteroassociation. The weight matrix, there are two, but one is just the transpose of the other as ",NA,NA
"already mentioned, is constructed in terms of the exemplar vector pairs.",NA,NA
The use of exemplar vectors is a one-shot learning—to determine what the weights should be. ,NA,NA
"Once weights are so determined, and an input vector is presented, a potentially associated vector ",NA,NA
"is output. It is taken as input in the opposite direction, and its potentially associated vector is ",NA,NA
"obtained back at the input layer. If the last vector found is the same as what is originally input, ",NA,NA
"then there is resonance. Suppose the vector B is obtained at one end, as a result of C being input ",NA,NA
at the other end. If B in turn is input during the next cycle of operation at the end where it was ,NA,NA
"obtained, and produces C at the opposite end, then you have a pair of heteroassociated vectors. ",NA,NA
This is what is basically happening in a BAM neural network.,"NOTE:  
 The BAM and Hopfield memories are closely related. You can think of the Hopfield 
 memory as a special case of the BAM.",NA
"What follow are the equations for the determination of the weight matrix, when the ",NA,NA
k,NA,NA
 pairs of ,NA,NA
exemplar vectors are denoted by (,NA,NA
 X,i,NA
", Y",i,NA
"), ",NA,NA
i,NA,NA
 ranging from 1 to ,NA,NA
k,NA,NA
. Note that ,NA,NA
T,NA,NA
 in the superscript of a ,NA,NA
matrix stands for the transpose of the matrix. While you interchange the rows and columns to get ,NA,NA
"the transpose of a matrix, you write a column vector as a row vector, and vice versa to get the ",NA,NA
transpose of a vector. The following equations refer to the vector pairs after their components are ,NA,NA
"changed to bipolar values, only for obtaining the weight matrix ",NA,NA
W,NA,NA
. Once ,NA,NA
W,NA,NA
" is obtained, further ",NA,NA
use of these exemplar vectors is made in their original form.,NA,NA
W,NA,NA
 =  X,1T,NA
 Y,1,NA
 + ... + X,kT,NA
 Y,k,NA
and ,NA,NA
W,T,NA
 =  Y,1T,NA
X,1,NA
 + ... + Y,kT,NA
 X,k,NA
Example,NA,NA
Suppose you choose two pairs of vectors as possible exemplars. Let them be: ,NA,NA
 X,1,NA
" = (1, 0, 0, 1), Y",1,NA
"= (0, 1, 1)",NA,NA
and ,NA,NA
 X,2,NA
" = (0, 1, 1, 0), Y",2,NA
" = (1, 0, 1)",NA,NA
"These you change into bipolar components and get, respectively, (1, –1, –1, 1), (–1, 1, 1), (–1, 1, ",NA,NA
"1, –1), and (1, –1, 1). ",NA,NA
 1 [-1 1 1]   -1 [1 -1 1]   -1  1  1     -1  1 -1    -2  2 0,NA,NA
 W = -1           + 1          =  1 -1 -1  +   1 -1  1  =  2 -2 0,NA,NA
 -1             1             1 -1 -1      1 -1  1     2 -2 0,NA,NA
 1            -1            -1  1  1     -1  1 -1    -2  2 0,NA,NA
and ,NA,NA
 -2    2    2    -2,NA,NA
W,T,NA
 =    2   -2   -2     2,NA,NA
 0    0    0     0,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/179-182.html (4 of 4) [21/11/02 21:57:34]",NA
Previous Table of Contents Next,NA,NA
You may think that the last column of ,NA,NA
W,NA,NA
 being all zeros presents a problem in that when the input is ,NA,NA
X,1,NA
" and whatever the output, when this output is presented back in the backward direction, it does not ",NA,NA
produce ,NA,NA
X,1,NA
", which does not have a zero in the last component. There is a ",NA,NA
thresholding,NA,NA
 function ,NA,NA
needed here. The ,NA,NA
thresholding,NA,NA
 function is transparent when the activations are all either +1 or –1. It ,NA,NA
"then just looks like you do an inverse mapping from bipolar values to binary, which is done by ",NA,NA
"replacing each –1 by a 0. When the activation is zero, you simply leave the output of that neuron as it ",NA,NA
was in the previous cycle or iteration. We now present the ,NA,NA
thresholding,NA,NA
 function for BAM outputs.,NA,NA
 1  if  y,j,NA
 > 0   1   if  x,i,NA
 > 0,NA,NA
b,j,NA
|,t+1,NA
 =  b,j,NA
|,t,NA
    if  y,j,NA
 = 0   and   a,i,NA
|,t+1,NA
 =   a,i,NA
|,t,NA
  if  x,i,NA
 = 0,NA,NA
 0   if  y,j,NA
 < 0                                     0   if  x,i,NA
 < 0,NA,NA
where ,NA,NA
x,i,NA
 and ,NA,NA
y,j,NA
 are the activations of neurons ,NA,NA
i,NA,NA
 and ,NA,NA
j,NA,NA
" in the input layer and output layer, respectively, ",NA,NA
and ,NA,NA
b,j,NA
|,t,NA
 refers to the output of the ,NA,NA
j,NA,NA
th neuron in the output layer in the cycle ,NA,NA
t,NA,NA
", while ",NA,NA
a,i,NA
|,t,NA
 refers to the ,NA,NA
output of the ,NA,NA
i,NA,NA
th neuron in the input layer in the cycle ,NA,NA
t,NA,NA
". Note that at the start, the ",NA,NA
a,i,NA
 and ,NA,NA
b,j,NA
 values are ,NA,NA
the same as the corresponding components in the exemplar pair being used.,NA,NA
If ,NA,NA
X,1,NA
" = (1, 0, 0, 1) is presented to the input neurons, their activations are given by the vector (–4, 4, 0). ",NA,NA
"The output vector, after using the ",NA,NA
threshold,NA,NA
" function just described is (0, 1, 1). The last component here ",NA,NA
"is supposed to be the same as the output in the previous cycle, since the corresponding activation value ",NA,NA
is 0. Since ,NA,NA
X,1,NA
 and ,NA,NA
Y,1,NA
" are one exemplar pair, the third component of ",NA,NA
Y,1,NA
 is what we need as the third ,NA,NA
"component of the output in the current cycle of operation; therefore, we fed ",NA,NA
X,1,NA
 and received ,NA,NA
Y,1,NA
. If we ,NA,NA
feed ,NA,NA
Y,1,NA
" at the other end (B field) , the activations in the A field will be (2, –2, –2, 2), and the output ",NA,NA
"vector will be (1, 0, 0, 1), which is ",NA,NA
X,1,NA
.,NA,NA
With A field input ,NA,NA
X,2,NA
" you get B field activations (4, -4, 0), giving the output vector as (1, 0, 1), which is ",NA,NA
Y,2,NA
. Thus ,NA,NA
X,2,NA
 and ,NA,NA
Y,2,NA
 are heteroassociated with each other.,NA,NA
Let us modify our ,NA,NA
X,1,NA
" to be (1, 0, 1, 1). Then the weight matrix ",NA,NA
W,NA,NA
 becomes,NA,NA
 1  [-1  1  1]       -1  [1  -1  1]       -2    2    0,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/182-184.html (1 of 3) [21/11/02 21:57:35],NA
 W =   -1              + 1               =    2   -2    0,NA,NA
 1                    1                    0    0    2,NA,NA
 1                   -1                   -2    2    0,NA,NA
and ,NA,NA
 -2     2     0    -2,NA,NA
W,T,NA
 =    2    -2     0     2,NA,NA
 0     0     2     0,NA,NA
Now this is a different set of two exemplar vector pairs. The pairs are ,NA,NA
X,1,NA
" = (1, 0, 1, 1), ",1,NA
" = (0, 1, 1), and ",NA,NA
X,2,NA
" = (0, 1, 1, 0), ",NA,NA
Y,2,NA
" = (1, 0, 1). Naturally, the weight matrix is different, as is its transpose, ",NA,NA
"correspondingly. As stated before, the weights do not change during the operation of the network with ",NA,NA
whatever inputs presented to it. The results are as shown in Table 8.1 below.,NA,NA
Table 8.1,NA,NA
Results for the Example ,NA,NA
Input vector ,NA,NA
activation ,NA,NA
output vector ,NA,NA
X,1,NA
" = (1, 0, 1, 1) ",NA,NA
"(-4, 4, 2) ",NA,NA
"(0, 1, 1) = Y",1,NA
"(2, -2, 2) ",NA,NA
X,2,NA
" = (0, 1, 1, 0) ",NA,NA
"(1, 0, 1) = Y",2,NA
Y,1,NA
" = (0, 1, 1) ",NA,NA
"(2, -2, 2, 2) ",NA,NA
"(1, 0, 1, 1) = X",1,NA
"(-2, 2, 2, -2) ",NA,NA
Y,2,NA
" = (1, 0, 1) ",NA,NA
"(0, 1, 1,0) = X",2,NA
You may think that you will encounter a problem when you input a new vector and one of the neurons ,NA,NA
"has activation 0. In the original example, you did find this situation when you got the third output ",NA,NA
neuron’s activation as 0. The thresholding function asked you to use the same output for this neuron as ,NA,NA
"existed in the earlier time cycle. So you took it to be 1, the third component in (0, 1, 1). But if your ",NA,NA
input vector is a new ,NA,NA
X,NA,NA
 vector for which you are trying to find an associated ,NA,NA
Y,NA,NA
" vector, then you do not ",NA,NA
have a ,NA,NA
Y,NA,NA
 component to fall back on when the activation turns out to be 0. How then can you use the ,NA,NA
thresholding,NA,NA
 function as stated? What guidance do you have in this situation? If you keep track of the ,NA,NA
"inputs used and outputs received thus far, you realize that the ",NA,NA
Field B,NA,NA
 (where you get your ,NA,NA
Y,NA,NA
 vector) ,NA,NA
"neurons are in some state, meaning that they had some outputs perhaps with some training vector. If ",NA,NA
"you use that output component as the one existing in the previous cycle, you have no problem in using ",NA,NA
the ,NA,NA
thresholding,NA,NA
 function.,NA,NA
"As an example, consider the input vector ",NA,NA
X,3,NA
" = ( 1, 0, 0, 0), with which the activations of neurons in ",NA,NA
Field B,NA,NA
" would be (-2, 2, 0). The first component of the output vector is clearly 0, and the second ",NA,NA
clearly 1. The third component is what is in doubt. Considering the last row of the table where ,NA,NA
Y,2,NA
gives the state of the neurons in ,NA,NA
Field B,NA,NA
", you can accept 1, the last component of ",NA,NA
Y,2,NA
", as the value you ",NA,NA
get from the thresholding function corresponding to the activation value 0. So the output would be the ,NA,NA
"vector (0, 1, 1), which is ",NA,NA
Y,1,NA
. But ,NA,NA
Y,1,NA
 is heteroassociated with ,NA,NA
X,1,NA
". Well, it means that ",NA,NA
X,3,NA
" = ( 1, 0, 0, 0) ",NA,NA
is not heteroassociated with any ,NA,NA
X,NA,NA
 vector.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/182-184.html (3 of 3) [21/11/02 21:57:35]",NA
Previous Table of Contents Next,NA,NA
Recall of Vectors,NA,NA
When ,NA,NA
X,1,NA
" is presented at the input layer, the activation at the output layer will give ( –4, 4, ",NA,NA
2) to which we apply the t,NA,NA
hresholding,NA,NA
" function, which replaces a positive value by 1, and ",NA,NA
a negative value by 0.,NA,NA
"This then gives us the vector (0, 1, 1) as the output, which is the same as our ",NA,NA
Y,1,NA
. Now ,NA,NA
Y,1,NA
is ,NA,NA
"passed back to the input layer through the feedback connections, and the activation of the ",NA,NA
"input layer becomes the vector (2, –2, 2, 2), which after thresholding gives the output ",NA,NA
"vector (1, 0, 1, 1), same as ",NA,NA
X,1,NA
. When ,NA,NA
X,2,NA
" is presented at the input layer, the activation at the ",NA,NA
"output layer will give (2, –2, 2) to which the ",NA,NA
thresholding,NA,NA
" function, which replaces a ",NA,NA
"positive value by 1 and a negative value by 0, is applied. This then gives the vector (1, 0, 1) ",NA,NA
"as the output, which is the same as ",NA,NA
Y,2,NA
. Now ,NA,NA
Y,2,NA
 is passed back to the input layer through ,NA,NA
"the feedback connections to get the activation of the input layer as (–2, 2, 2, –2), which ",NA,NA
"after thresholding gives the output vector (0, 1, 1, 0), which is ",NA,NA
X,2,NA
.,"The two vector pairs chosen here for encoding worked out fine, and the BAM network 
 with four neurons in Field A and three neurons in Field B is all set for finding a vector 
 under heteroassociation with a given input vector.",NA
Continuation of Example,NA,NA
Let us now use the vector ,NA,NA
X,3,NA
" = (1, 1, 1, 1). The vector ",NA,NA
Y,3,NA
" = (0, 1, 1) is obtained at the ",NA,NA
output layer. But the next step in which we present ,NA,NA
Y,3,NA
 in the backward direction does not ,NA,NA
produce ,NA,NA
X,3,NA
", instead it gives an ",NA,NA
X,1,NA
" = (1, 0, 1, 1). We already have ",NA,NA
X,1,NA
 associated with ,NA,NA
Y,1,NA
. ,NA,NA
This means that ,NA,NA
X,3,NA
 is not associated with any vector in the output space. On the other ,NA,NA
"hand, if instead of getting ",NA,NA
X,1,NA
 we obtained a different ,NA,NA
X,4,NA
" vector, and if this in the feed ",NA,NA
forward operation produced a different ,NA,NA
Y,NA,NA
" vector, then we repeat the operation of the ",NA,NA
network until no changes occur at either end. Then we will have possibly a new pair of ,NA,NA
vectors under the heteroassociation established by this BAM network.,NA,NA
Special Case—Complements,NA,NA
If a pair of (distinct) patterns ,NA,NA
X,NA,NA
 and ,NA,NA
Y,NA,NA
" are found to be heteroassociated by BAM, and if ",NA,NA
you input the complement of ,NA,NA
X,NA,NA
", complement being obtained by interchanging the 0’s and ",NA,NA
1’s in ,NA,NA
X,NA,NA
", BAM will show that the complement of ",NA,NA
Y,NA,NA
 is the pattern associated with the ,NA,NA
complement of ,NA,NA
X,NA,NA
. An example will be seen in the illustrative run of the program for C++ ,NA,NA
"implementation of BAM, which follows.",NA,NA
C++ Implementation,NA,NA
"In our C++ implementation of a discrete bidirectional associative memory network, we ",NA,NA
create classes for ,NA,NA
neuron,NA,NA
 and ,NA,NA
network,NA,NA
. Other classes created are called ,NA,NA
exemplar,NA,NA
", ",NA,NA
assocpair,NA,NA
", ",NA,NA
potlpair,NA,NA
", for the exemplar pair of vectors, associated pair of vectors, and ",NA,NA
"potential pairs of vectors, respectively, for finding heteroassociation between them. We ",NA,NA
could have made one class of ,NA,NA
pairvect,NA,NA
 for a pair of vectors and derived the exemplar and ,NA,NA
so on from it. The ,NA,NA
network,NA,NA
 class is declared as a ,NA,NA
friend,NA,NA
 class in these other classes. Now ,NA,NA
"we present the header and source files, called bamntwrk.h and bamntwrk.cpp. Since we ",NA,NA
"reused our previous code from the Hopfield network of Chapter 4, there are a few data ",NA,NA
members of classes that we did not put to explicit use in the program. We call the ,NA,NA
neuron ,NA,NA
class ,NA,NA
bmneuron,NA,NA
 to remind us of BAM.,NA,NA
Program Details and Flow,NA,NA
A neuron in the first layer is referred to as ,NA,NA
anrn,NA,NA
", and the number of neurons in this layer is ",NA,NA
referred to as ,NA,NA
anmbr,NA,NA
. We give the name,NA,NA
bnrn,NA,NA
" to the array of neurons in the second layer, ",NA,NA
and ,NA,NA
bnmbr,NA,NA
 denotes the size of that array. The sequence of operations in the program is as ,NA,NA
follows:,NA,NA
•,NA,NA
"  We ask the user to input the exemplar vectors, and we transform them into their ",NA,NA
bipolar versions. The ,NA,NA
trnsfrm ( ),NA,NA
 function in the exemplar class is for this purpose. ,NA,NA
•,NA,NA
  We give the network the ,NA,NA
X,NA,NA
" vector, in its bipolar version, in one exemplar pair. We ",NA,NA
find the activations of the elements of ,NA,NA
bnrn,NA,NA
 array and get corresponding output ,NA,NA
vector as a binary pattern. If this is the ,NA,NA
Y,NA,NA
" in the exemplar pair, the network has made ",NA,NA
"a desired association in one direction, and we go on to the next.step. ",NA,NA
"Otherwise we have a potential associated pair, one of which is ",NA,NA
X,NA,NA
 and the other is ,NA,NA
what we just got as the output vector in the opposite layer. We say potential ,NA,NA
associated pair because we have the next step to confirm the association. ,NA,NA
•,NA,NA
  We run the ,NA,NA
bnrn,NA,NA
 array through the transpose of the weight matrix and calculate ,NA,NA
the outputs of the ,NA,NA
anrn,NA,NA
" array elements. If , as a result, we get the vector ",NA,NA
X,NA,NA
 as the ,NA,NA
anrn,NA,NA
" array, we found an associated pair, ",NA,NA
"(X, Y).",NA,NA
" Otherwise, we repeat the two steps ",NA,NA
just described until we find an associated pair. ,NA,NA
•,NA,NA
  We now work with the next pair of exemplar vectors in the same manner as ,NA,NA
"above, to find an associated pair. ",NA,NA
•,NA,NA
"  We assign serial numbers, denoted by the variable ",NA,NA
idn,NA,NA
", to the associated pairs so ",NA,NA
we can print them all together at the end of the program. The pair is called ,NA,NA
"(X, Y) ",NA,NA
where ,NA,NA
X,NA,NA
 produces ,NA,NA
Y,NA,NA
 through the weight matrix ,NA,NA
W,NA,NA
", and ",NA,NA
Y,NA,NA
 produces ,NA,NA
X,NA,NA
 through the ,NA,NA
weight matrix which is the transpose of ,NA,NA
W.,NA,NA
•,NA,NA
"  A flag is used to have value 0 until confirmation of association is obtained, when ",NA,NA
the value of the flag changes to 1. ,NA,NA
•,NA,NA
  Functions ,NA,NA
compr1,NA,NA
 and ,NA,NA
compr2,NA,NA
 in the network class verify if the potential pair is ,NA,NA
indeed an associated pair and set the proper value of the flag mentioned above. ,NA,NA
•,NA,NA
  ,NA,NA
Functions ,NA,NA
comput1,NA,NA
 and ,NA,NA
comput2,NA,NA
 in the network class carry out the calculations to ,NA,NA
"get the activations and then find the output vector, in the proper directions of the ",NA,NA
bidirectional associative memory network. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Program Example for BAM,NA,NA
"For our illustration run, we provided for six neurons in the input layer and five in the output ",NA,NA
"layer. We used three pairs of exemplars for encoding. We used two additional input vectors, ",NA,NA
one of which is the complement of the ,NA,NA
X,NA,NA
" of an exemplar pair, after the encoding is done, to ",NA,NA
"see what association will be established in these cases, or what recall will be made by the ",NA,NA
BAM network.,NA,NA
Header File,NA,NA
"As expected, the complement of the ",NA,NA
Y,NA,NA
 of the exemplar is found to be associated with the ,NA,NA
complement of the ,NA,NA
X,NA,NA
" of that pair. When the second input vector is presented, however, a new ",NA,NA
"pair of associated vectors is found. After the code is presented, we list the computer output ",NA,NA
also.,NA,NA
Listing 8.1,NA,NA
 bamntwrk.h,NA,NA
"//bamntwrk.h   V. Rao,  H. Rao",NA,NA
//Header file for BAM network program,NA,NA
#include <iostream.h> ,NA,NA
#include <math.h> ,NA,NA
#include <stdlib.h> ,NA,NA
#define MXSIZ 10  // determines the maximum size of the network,NA,NA
class bmneuron ,NA,NA
{,NA,NA
 protected:,NA,NA
 int nnbr;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/187-189.html (1 of 4) [21/11/02 21:57:37],NA
" int inn,outn;",NA,NA
 int output;,NA,NA
 int activation;,NA,NA
 int outwt[MXSIZ];,NA,NA
 char *name;,NA,NA
 friend class network;,NA,NA
public:,NA,NA
 bmneuron() { };,NA,NA
" void getnrn(int,int,int,char *); };",NA,NA
class exemplar ,NA,NA
{ ,NA,NA
protected:,NA,NA
" int xdim,ydim;",NA,NA
" int v1[MXSIZ],v2[MXSIZ];",NA,NA
" int u1[MXSIZ],u2[MXSIZ];",NA,NA
 friend class network;,NA,NA
 friend class mtrx;,NA,NA
public:,NA,NA
 exemplar() { };,NA,NA
" void getexmplr(int,int,int *,int *);",NA,NA
 void prexmplr();,NA,NA
 void trnsfrm();,NA,NA
 void prtrnsfrm(); ,NA,NA
};,NA,NA
class asscpair ,NA,NA
{ ,NA,NA
protected:,NA,NA
" int xdim,ydim,idn;",NA,NA
" int v1[MXSIZ],v2[MXSIZ];",NA,NA
 friend class network;,NA,NA
public:,NA,NA
 asscpair() { };,NA,NA
" void getasscpair(int,int,int);",NA,NA
 void prasscpair(); ,NA,NA
};,NA,NA
class potlpair,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/187-189.html (2 of 4) [21/11/02 21:57:37],NA
{ ,NA,NA
protected:,NA,NA
" int xdim,ydim;",NA,NA
" int v1[MXSIZ],v2[MXSIZ];",NA,NA
 friend class network;,NA,NA
public:,NA,NA
 potlpair() { };,NA,NA
" void getpotlpair(int,int);",NA,NA
 void prpotlpair(); ,NA,NA
};,NA,NA
class network ,NA,NA
{ ,NA,NA
public:,NA,NA
" int  anmbr,bnmbr,flag,nexmplr,nasspr,ninpt;",NA,NA
" bmneuron (anrn)[MXSIZ],(bnrn)[MXSIZ];",NA,NA
 exemplar (e)[MXSIZ];,NA,NA
 asscpair (as)[MXSIZ];,NA,NA
 potlpair (pp)[MXSIZ];,NA,NA
" int outs1[MXSIZ],outs2[MXSIZ];",NA,NA
" int mtrx1[MXSIZ][MXSIZ],mtrx2[MXSIZ][MXSIZ];",NA,NA
};,NA,NA
 network() { };,NA,NA
" void getnwk(int,int,int,int [][6],int [][5]); ",NA,NA
"void compr1(int,int);",NA,NA
" void compr2(int,int);",NA,NA
 void prwts();,NA,NA
 void iterate();,NA,NA
 void findassc(int *);,NA,NA
 void asgninpt(int *);,NA,NA
" void asgnvect(int,int *,int *);",NA,NA
 void comput1();,NA,NA
 void comput2();,NA,NA
 void prstatus();,NA,NA
Source File,NA,NA
The program source file is presented as follows. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/187-189.html (3 of 4) [21/11/02 21:57:37],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/187-189.html (4 of 4) [21/11/02 21:57:37]",NA
Additional Issues,"If you desire to use vectors with real number components, instead of binary numbers, you can do so. Your model 
 is then called a 
 Continuous Bidirectional Associative Memory
 . A matrix 
 W
  and its transpose are used for the 
 connection weights. However, the matrix 
 W
  is not formulated as we described so far. The matrix is arbitrarily 
 chosen, and kept constant. The 
 thresholding
  function is also chosen as a continuous function, unlike the one used 
 before. The changes in activations of neurons during training are according to extensions the Cohen-Grossberg 
 paradigm.
  
 Michael A. Cohen and Stephen Grossberg developed their model for autoassociation, with a symmetric matrix of 
 weights. Stability is ensured by the Cohen-Grossberg theorem; there is no learning.
  
 If, however, you desire the network to learn new pattern pairs, by modifying the weights so as to find association 
 between new pairs, you are designing what is called an 
 Adaptive Bidirectional Associative Memory
  (
 ABAM
 ).
  
 The law that governs the weight changes determines the type of ABAM you have, namely, the 
 Competitive 
 ABAM, or the 
 Differential Hebbian
  ABAM, or the 
 Differential Competitive
  ABAM. Unlike in the ABAM 
 model, which is additive type, some products of outputs from the two layers, or the derivatives of the threshold 
 functions are used in the other models.
  
 Here we present a brief description of a model, which is a variation of the BAM. It is called 
 UBBAM
  (
 Unipolar 
 Binary Bidirectional Associative Memory
 ).",NA
Unipolar Binary Bidirectional Associative Memory,"T. C. B. Yu and R. J. Mears describe a design of unipolar binary bidirectional associative memory, and its 
 implementation with a Smart Advanced Spatial Light Modulator (SASLM). The SASLM device is a ferroelectric 
 liquid crystal spatial light modulator. It is driven by a silicon CMOS backplane. We use their paper to present 
 some features of a unipolar binary bidirectional associative memory, and ignore the hardware implementation of 
 it. 
  
 Recall the procedure by which you determine the weight matrix 
 W
  for a BAM network, as described in the 
 previous pages. As a first step, you convert each vector in the exemplar pairs into its bipolar version. If 
 X
  and 
 Y 
 are an exemplar pair (in bipolar versions), you take the product 
 X
 T
  Y
  and add it to similar products from other 
 exemplar pairs, to get a weight matrix 
 W.
  Some of the elements of the matrix 
 W
  may be negative numbers. In the 
 unipolar context you do not have negative values and positive values at the same time. Only one of them is 
 allowed. Suppose you do not want any negative numbers; then one way of remedying the situation is by adding a 
 large enough constant to each element of the matrix. You cannot choose to add to only the negative numbers that 
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/211-213.html (1 of 3) [21/11/02 21:57:40]",NA
Previous Table of Contents Next,NA,NA
"The example of Yu and Mears, consists of eight 8-component vectors in both directions of ",NA,NA
"the Bidirectional Associative Memory model. You can take these vectors, as they do, to ",NA,NA
define the pixel values in a 8x8 grid of pixels. What is being accomplished is the ,NA,NA
association of two spatial patterns. In terms of the binary numbers that show the pixel ,NA,NA
"values, the patterns are shown in Figure 8.2. Call them Pattern I and Pattern II. ",NA,NA
Figure 8.2,NA,NA
"  Two patterns, Pattern I and Pattern II, given by pixel values: 1 for black, 0 for ",NA,NA
white.,NA,NA
"Instead of Pattern I, they used a corrupted form of it as given in Figure 8.3. There was no ",NA,NA
"problem in the network finding the associated pair (Pattern I, Pattern II). ",NA,NA
Figure 8.3,NA,NA
  Pattern I and corrupted Pattern I.,NA,NA
"In Figure 8.3, the corrupted version of Pattern I differs from Pattern I, in 10 of the 64 ",NA,NA
"places, a corruption of 15.6%. This corruption percentage is cited as the limit below which, ",NA,NA
"Yu and Mears state, heteroassociative recall is obtained. Thus noise elimination to a certain ",NA,NA
"extent is possible with this model. As we have seen with the Hopfield memory, an ",NA,NA
"application of associative memory is pattern completion, where you are presented a ",NA,NA
corrupted version of a pattern and you recall the true pattern. This is autoassociation. In the ,NA,NA
"case of BAM, you have heteroassociative re call with a corrupted input. ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/213-216.html (1 of 2) [21/11/02 21:57:41],NA
Summary,NA,NA
"In this chapter, bidirectional associative memories are presented. The development of these ",NA,NA
memories is largely due to Kosko. They share with Adaptive Resonance Theory the feature ,NA,NA
of resonance between the two layers in the network. The bidirectional associative ,NA,NA
"memories (BAM) network finds heteroassociation between binary patterns, and these are ",NA,NA
converted to bipolar values to determine the connection weight matrix. Even though there ,NA,NA
"are connections in both directions between neurons in the two layers, essentially only one ",NA,NA
weight matrix is involved. You use the transpose of this weight matrix for the connections ,NA,NA
"in the opposite direction. When one input at one end leads to some output at the other end, ",NA,NA
"which in turn leads to output that is the same as the previous input, resonance is reached ",NA,NA
and an associated pair is found. ,NA,NA
The continuous bidirectional associative memory extends the binary model to the ,NA,NA
continuous case. Adaptive bidirectional memories of different flavors are the result of ,NA,NA
incorporating different learning paradigms. A unipolar binary version of BAM is also ,NA,NA
presented as an application of the BAM for pattern completion.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch08/213-216.html (2 of 2) [21/11/02 21:57:41]",NA
Previous Table of Contents Next,NA,NA
Chapter 9 ,NA,NA
FAM: Fuzzy Associative Memory ,NA,NA
Introduction,NA,NA
"In this chapter, you we will learn about fuzzy sets and their elements, both for input and ",NA,NA
output of an associative neural network. Every element of a fuzzy set has a degree of ,NA,NA
"membership in the set. Unless this degree of membership is 1, an element does not belong ",NA,NA
to the set (in the sense of elements of an ordinary set belonging to the set). In a neural ,NA,NA
"network of a fuzzy system the inputs, the outputs, and the connection weights all belong ",NA,NA
"fuzzily to the spaces that define them. The weight matrix will be a fuzzy matrix, and the ",NA,NA
activations of the neurons in such a network have to be determined by rules of fuzzy logic ,NA,NA
and fuzzy set operations. ,NA,NA
An expert system uses what are called ,NA,NA
crisp rules,NA,NA
 and applies them sequentially. The ,NA,NA
advantage in casting the same problem in a fuzzy system is that the rules you work with do ,NA,NA
"not have to be crisp, and the processing is done in parallel. What the fuzzy systems can ",NA,NA
"determine is a fuzzy association. These associations can be modified, and the underlying ",NA,NA
"phenomena better understood, as experience is gained. That is one of the reasons for their ",NA,NA
growing popularity in applications. When we try to relate two things through a process of ,NA,NA
"trial and error, we will be implicitly and intuitively establishing an association that is ",NA,NA
gradually modified and perhaps bettered in some sense. Several fuzzy variables may be ,NA,NA
present in such an exercise. That we did not have full knowledge at the beginning is not a ,NA,NA
hindrance; there is some difference in using probabilities and using fuzzy logic as well. The ,NA,NA
degree of membership assigned for an element of a set does not have to be as firm as the ,NA,NA
assignment of a probability.,NA,NA
"The degree of membership is, like a probability, a real number between 0 and 1. The closer ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch09/217-220.html (1 of 4) [21/11/02 21:57:42],NA
"it is to 1, the less ambiguous is the membership of the element in the set concerned. ",NA,NA
"Suppose you have a set that may or may not contain three elements, say, ",NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", and ",NA,NA
c,NA,NA
. Then ,NA,NA
the fuzzy set representation of it would be by the ordered triple (,NA,NA
m,a,NA
", ",NA,NA
m,b,NA
", ",NA,NA
m,c,NA
"), which is ",NA,NA
called the ,NA,NA
fit vector,NA,NA
", and its components are called ",NA,NA
fit values,NA,NA
". For example, the triple (0.5, ",NA,NA
"0.5, 0.5) shows that each of ",NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", and ",NA,NA
c,NA,NA
", have a membership equal to only one-half. This ",NA,NA
triple itself will describe the fuzzy set. It can also be thought of as a point in the three-,NA,NA
dimensional space. None of such points will be outside the unit cube. When the number of ,NA,NA
"elements is higher, the corresponding points will be on or inside the unit hypercube.",NA,NA
"It is interesting to note that this fuzzy set, given by the triple (0.5, 0.5, 0.5), is its own ",NA,NA
"complement, something that does not happen with regular sets. The complement is the set ",NA,NA
that shows the degrees of nonmembership.,NA,NA
The ,NA,NA
height,NA,NA
" of a fuzzy set is the maximum of its fit values, and the fuzzy set is said to be ",NA,NA
normal,NA,NA
", if its height is 1. The fuzzy set with fit vector (0.3, 0.7, 0.4) has height 0.7, and it ",NA,NA
"is not a normal fuzzy set. However, by introducing an additional dummy component with ",NA,NA
"fit value 1, we can extend it into a normal fuzzy set. The desirability of normalcy of a ",NA,NA
fuzzy set will become apparent when we talk about recall in fuzzy associative memories.,NA,NA
The subset relationship is also different for fuzzy sets from the way it is defined for regular ,NA,NA
"sets. For example, if you have a fuzzy set given by the triple (0.3, 0.7, 0.4), then any fuzzy ",NA,NA
set with a triple (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
) such that ,NA,NA
a,NA,NA
,NA,NA
" 0.3, ",NA,NA
b,NA,NA
,NA,NA
" 0.7, and ",NA,NA
c,NA,NA
,NA,NA
" 0.4, is its fuzzy subset. For ",NA,NA
"example, the fuzzy set given by the triple (0.1, 0.7, 0) is a subset of the fuzzy set (0.3, 0.7, ",NA,NA
0.4).,NA,NA
Association,NA,NA
"Consider two fuzzy sets, one perhaps referring to the popularity of (or interest in) an ",NA,NA
"exhibition and the other to the price of admission. The popularity could be very high, high, ",NA,NA
"fair, low, or very low. Accordingly, the fit vector will have five components. The price of ",NA,NA
"admission could be high, modest, or low. Its fit vector has three components. A fuzzy ",NA,NA
"associative memory system then has the association (popularity, price), and the fuzzy set ",NA,NA
pair encodes this association. ,NA,NA
We describe the encoding process and the recall in the following sections. Once encoding ,NA,NA
"is completed, associations between subsets of the first fuzzy set and subsets of the second ",NA,NA
fuzzy set are also established by the same fuzzy associative memory system.,NA,NA
FAM Neural Network,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch09/217-220.html (2 of 4) [21/11/02 21:57:42],NA
"The neural network for a fuzzy associative memory has an input and an output layer, with ",NA,NA
"full connections in both directions, just as in a BAM neural network. Figure 9.1 shows the ",NA,NA
"layout. To avoid cluttering, the figure shows the network with three neurons in field A and ",NA,NA
"two neurons in field B, and only some of the connection weights. The general case is ",NA,NA
analogous. ,NA,NA
Figure 9.1,NA,NA
  Fuzzy associative memory neural network.,"This figure is the same as Figure 8.1 in Chapter 8, since the model is the same, except 
 that fuzzy vectors are used with the current model.",NA
Encoding,NA,NA
Encoding for fuzzy associative memory systems is similar in concept to the encoding ,NA,NA
"process for bidirectional associative memories, with some differences in the operations ",NA,NA
"involved. In the BAM encoding, bipolar versions of binary vectors are used just for ",NA,NA
encoding. Matrices of the type ,NA,NA
X,iT,NA
Y,i,NA
 are added together to get the connection weight ,NA,NA
matrix. There are two basic operations with the elements of these vectors. They are ,NA,NA
multiplication and addition of products. There is no conversion of the fit values before the ,NA,NA
encoding by the fuzzy sets. The multiplication of elements is replaced by the operation of ,NA,NA
taking the ,NA,NA
minimum,NA,NA
", and addition is replaced by the operation of taking the ",NA,NA
maximum,NA,NA
.,NA,NA
There are two methods for encoding. The method just described is what is called max–min ,NA,NA
composition. It is used to get the connection weight matrix and also to get the outputs of ,NA,NA
neurons in the fuzzy associative memory neural network. The second method is called ,NA,NA
correlation–product encoding. It is obtained the same way as a BAM connection weight ,NA,NA
"matrix is obtained. Max–min composition is what is frequently used in practice, and we ",NA,NA
will confine our attention to this method.,NA,NA
Previous Table of Contents Next,NA,NA
Previous Table of Contents Next,NA,NA
Example of Encoding,NA,NA
"Suppose the two fuzzy sets we use to encode have the fit vectors ( 0.3, 0.7, 0.4, 0.2) and (0.4, 0.3, 0.9). ",NA,NA
Then the matrix ,NA,NA
W,NA,NA
 is obtained by using max–min composition as follows.,NA,NA
" 0.3 [0.4 0.3 0.9] min(0.3,0.4) min(0.3,0.3) min(0.3,0.9) 0.3 0.3 0.3 ",NA,NA
"W=0.7              =min(0.7,0.4) min(0.7,0.3) min(0.7,0.9)=0.4 0.3 0.7 ",NA,NA
"0.4               min(0.4,0.4) min(0.4,0.3) min(0.4,0.9) 0.4 0.3 0.4 ",NA,NA
"0.2               min(0.2,0.4) min(0.2,0.3) min(0.2,0.9) 0.2 0.2 0.2",NA,NA
Recall for the Example,NA,NA
"If we input the fit vector (0.3, 0.7, 0.4, 0.2), the output (",NA,NA
b,1,NA
", ",NA,NA
b,2,NA
", ",NA,NA
b,3,NA
") is determined as follows, using ",NA,NA
b,j,NA
 = ,NA,NA
max( min(,NA,NA
a,1,NA
", ",NA,NA
w,"1
 j",NA
"), …, min(",NA,NA
a,m,NA
", ",NA,NA
w,"m
 j",NA
"), where ",NA,NA
m,NA,NA
" is the dimension of the ‘a’ fit vector, and ",NA,NA
w,ij,NA
 is the ,NA,NA
i,NA,NA
"th row, ",NA,NA
j,NA,NA
th column element of the matrix ,NA,NA
W.,NA,NA
 b,1,NA
" = max(min(0.3, 0.3), min(0.7, 0.4), min(0.4, 0.4),",NA,NA
" min(0.2, 0.2)) =  max(0.3, 0.4, 0.4, 0.2) = 0.4 ",NA,NA
b,2,NA
" = max(min(0.3, 0.3), min(0.7, 0.3), min(0.4, 0.3),",NA,NA
" min(0.2, 0.2)) = max( 0.3, 0.3, 0.3, 0.2 ) = 0.3 ",NA,NA
"b3 = max(min(0.3, 0.3), min(0.7, 0.7), min(0.4, 0.4),",NA,NA
" min(0.2, 0.2)) = max (0.3, 0.7, 0.4, 0.2) = 0.7",NA,NA
"The output vector (0.4, 0.3, 0.7) is not the same as the second fit vector used, namely (0.4, 0.3, 0.9), but it is ",NA,NA
"a subset of it, so the recall is not perfect. If you input the vector (0.4, 0.3, 0.7) in the opposite direction, ",NA,NA
using the transpose of the matrix ,NA,NA
W,NA,NA
", the output is (0.3, 0.7, 0.4, 0.2), showing resonance. If on the other ",NA,NA
"hand you input (0.4, 0.3, 0.9) at that end, the output vector is (0.3, 0.7, 0.4, 0.2), which in turn causes in the ",NA,NA
"other direction an output of (0.4, 0.3, 0.7) at which time there is resonance. Can we foresee these results? ",NA,NA
The following section explains this further.,NA,NA
Recall,NA,NA
Let us use the operator o to denote max–min composition. Perfect recall occurs when the weight matrix is ,NA,NA
obtained using the max–min composition of fit vectors ,NA,NA
U,NA,NA
 and ,NA,NA
V,NA,NA
 as follows:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch09/220-223.html (1 of 3) [21/11/02 21:57:43],NA
(i)U,NA,NA
 o ,NA,NA
W,NA,NA
 = ,NA,NA
V,NA,NA
 if and only if height (U) ,NA,NA
,NA,NA
 height (V). ,NA,NA
(ii)V,NA,NA
 o ,NA,NA
W,T,NA
 = ,NA,NA
U,NA,NA
 if and only if height (V) ,NA,NA
,NA,NA
 height (U). ,NA,NA
Also note that if ,NA,NA
X,NA,NA
 and ,NA,NA
Y,NA,NA
 are arbitrary fit vectors with the same dimensions as ,NA,NA
U,NA,NA
 and ,NA,NA
V,NA,NA
", then:",NA,NA
(iii)X,NA,NA
 o ,NA,NA
W,NA,NA
,NA,NA
V. ,NA,NA
(iv)Y,NA,NA
 o ,NA,NA
W,T,NA
,NA,NA
U.,"A 
 
  B is the notation to say A is a subset of B.",NA
"In the previous example, height of (0.3, 0.7, 0.4, 0.2) is 0.7, and height of (0.4, 0.3, 0.9) is 0.9. Therefore ",NA,NA
"(0.4, 0.3, 0.9) as input, produced (0.3, 0.7, 0.4, 0.2) as output, but (0.3, 0.7, 0.4, 0.2) as input, produced only ",NA,NA
"a subset of (0.4, 0.3, 0.9). That both (0.4, 0.3, 0.7) and (0.4, 0.3, 0.9) gave the same output, (0.3, 0.7, 0.4, ",NA,NA
"0.2) is in accordance with the corollary to the above, which states that if (X, Y) is a fuzzy associated ",NA,NA
"memory, and if X is a subset of X’, then (X’, Y) is also a fuzzy associated memory. ",NA,NA
C++ Implementation,NA,NA
"We use the classes we created for BAM implementation in C++, except that we call the neuron class ",NA,NA
"fzneuron,",NA,NA
 and we do not need some of the methods or functions in the ,NA,NA
network,NA,NA
" class. The header file, the ",NA,NA
"source file, and the output from an illustrative run of the program are given in the following. The header ",NA,NA
file is called ,NA,NA
fuzzyam.hpp,NA,NA
", and the source file is called ",NA,NA
fuzzyam.cpp,NA,NA
.,NA,NA
Program details,NA,NA
The program details are analogous to the program details given in Chapter 8. The computations are done ,NA,NA
"with fuzzy logic. Unlike in the nonfuzzy version, a single exemplar fuzzy vector pair is used here. There ",NA,NA
"are no transformations to bipolar versions, since the vectors are fuzzy and not binary and crisp. ",NA,NA
A neuron in the first layer is referred to as ,NA,NA
anrn,NA,NA
", and the number of neurons in this layer is referred to as ",NA,NA
anmbr,NA,NA
. ,NA,NA
bnrn,NA,NA
 is the name we give to the array of neurons in the second layer and ,NA,NA
bnmbr,NA,NA
 denotes the size ,NA,NA
of that array. The sequence of operations in the program are as follows:,NA,NA
•,NA,NA
  We ask the user to input the exemplar vector pair. ,NA,NA
•,NA,NA
  We give the network the ,NA,NA
X,NA,NA
" vector, in the exemplar pair. We find the activations of the elements ",NA,NA
of ,NA,NA
bnrn,NA,NA
 array and get corresponding output vector as a binary pattern. If this is the ,NA,NA
Y,NA,NA
 in the ,NA,NA
"exemplar pair, the network has made a desired association in one direction, and we go on to the ",NA,NA
"next.step. Otherwise we have a potential associated pair, one of which is ",NA,NA
X,NA,NA
 and the other is what ,NA,NA
we just got as the output vector in the opposite layer. We say potential associated pair because we ,NA,NA
have the next step to confirm the association. ,NA,NA
•,NA,NA
  We run the ,NA,NA
bnrn,NA,NA
 array through the transpose of the weight matrix and calculate the outputs of ,NA,NA
the ,NA,NA
anrn,NA,NA
" array elements. If, as a result, we get the vector ",NA,NA
X,NA,NA
 as the ,NA,NA
anrn,NA,NA
" array, we found an ",NA,NA
"associated pair, ",NA,NA
"(X, Y).",NA,NA
" Otherwise, we repeat the two steps just described until we find an ",NA,NA
associated pair. ,NA,NA
•,NA,NA
"  We now work with the next pair of exemplar vectors in the same manner as above, to find an ",NA,NA
associated pair. ,NA,NA
•,NA,NA
"  We assign serial numbers, denoted by the variable ",NA,NA
idn,NA,NA
", to the associated pairs so we can print ",NA,NA
them all together at the end of the program. The pair is called ,NA,NA
"(X, Y)",NA,NA
 where ,NA,NA
X,NA,NA
 produces ,NA,NA
Y,NA,NA
 through ,NA,NA
the weight matrix ,NA,NA
"W,",NA,NA
 and ,NA,NA
Y,NA,NA
 produces ,NA,NA
X,NA,NA
" through the weight matrix, which is the transpose of ",NA,NA
W.•,NA,NA
  A ,NA,NA
"flag is used to have value 0 until confirmation of association is obtained, when the value of the flag ",NA,NA
changes to 1. ,NA,NA
•,NA,NA
  Functions ,NA,NA
compr1,NA,NA
 and ,NA,NA
compr2,NA,NA
 in the network class verify if the potential pair is indeed an ,NA,NA
associated pair and set the proper value of the flag mentioned above. ,NA,NA
•,NA,NA
  Functions ,NA,NA
comput1,NA,NA
 and ,NA,NA
comput2,NA,NA
 in the network class carry out the calculations to get the ,NA,NA
"activations and then find the output vector, in the respective directions of the fuzzy associative ",NA,NA
memory network. ,NA,NA
A lot of the code from the bidirectional associative memory (BAM) is used for the FAM. Here are the ,NA,NA
"listings, with comments added where there are differences between this code and the code for the BAM of ",NA,NA
Chapter 8. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Summary,"In this chapter, bidirectional associative memories are presented for fuzzy subsets. The development of these 
 is largely due to Kosko. They share the feature of resonance between the two layers in the network with 
 Adaptive Resonance theory. Even though there are connections in both directions between neurons in the 
 two layers, only one weight matrix is involved. You use the transpose of this weight matrix for the 
 connections in the opposite direction. When one input at one end leads to some output at the other, which in 
 turn leads to output same as the previous input, resonance is reached and an associated pair is found. In the 
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch09/235-242.html (5 of 6) [21/11/02 21:57:45]",NA
Previous Table of Contents Next,NA,NA
Chapter 10 ,NA,NA
Adaptive Resonance Theory (ART) ,NA,NA
Introduction,NA,NA
Grossberg’s ,NA,NA
Adaptive Resonance Theory,NA,NA
", developed further by Grossberg and Carpenter, is ",NA,NA
for the categorization of patterns using the ,NA,NA
competitive learning,NA,NA
 paradigm. It introduces a ,NA,NA
gain control,NA,NA
 and a ,NA,NA
reset,NA,NA
 to make certain that learned categories are retained even while new ,NA,NA
categories are learned and thereby addresses the plasticity–stability dilemma.,NA,NA
Adaptive Resonance Theory makes much use of a competitive learning paradigm. A ,NA,NA
criterion is developed to facilitate the occurrence of ,NA,NA
winner-take-all,NA,NA
 phenomenon. A single ,NA,NA
"node with the largest value for the set criterion is declared the winner within its layer, and it ",NA,NA
"is said to classify a pattern class. If there is a tie for the winning neuron in a layer, then an ",NA,NA
"arbitrary rule, such as the first of them in a serial order, can be taken as the winner.",NA,NA
The neural network developed for this theory establishes a system that is made up of two ,NA,NA
"subsystems, one being the attentional subsystem, and this contains the unit for gain control. ",NA,NA
"The other is an orienting subsystem, and this contains the unit for reset. During the ",NA,NA
"operation of the network modeled for this theory, patterns emerge in the attentional ",NA,NA
subsystem and are called traces of STM (,NA,NA
short-term memory,NA,NA
). Traces of LTM (,NA,NA
long-term ,NA,NA
memory,NA,NA
) are in the connection weights between the input layer and output layer.,NA,NA
"The network uses processing with feedback between its two layers, until resonance occurs. ",NA,NA
Resonance occurs when the output in the first layer after feedback from the second layer ,NA,NA
matches the original pattern used as input for the first layer in that processing cycle. A ,NA,NA
"match of this type does not have to be perfect. What is required is that the degree of match, ",NA,NA
"measured suitably, exceeds a predetermined level, termed ",NA,NA
vigilance parameter,NA,NA
. Just as a ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/243-246.html (1 of 4) [21/11/02 21:57:46],NA
photograph matches the likeness of the subject to a greater degree when the granularity is ,NA,NA
"higher, the pattern match gets finer when the vigilance parameter is closer to 1.",NA,NA
The Network for ART1,NA,NA
The neural network for the adaptive resonance theory or ART1 model consists of the ,NA,NA
following: ,NA,NA
•,NA,NA
"  A layer of neurons, called the F",1,NA
 layer (input layer or comparison layer) ,NA,NA
•,NA,NA
  ,NA,NA
A node for each layer as a gain control unit ,NA,NA
•,NA,NA
"  A layer of neurons, called the F",2,NA
 layer (,NA,NA
output,NA,NA
 layer or ,NA,NA
recognition,NA,NA
 layer) ,NA,NA
•,NA,NA
  A node as a reset unit ,NA,NA
•,NA,NA
  Bottom-up connections from F,1,NA
 layer to F,2,NA
 layer ,NA,NA
•,NA,NA
  Top-down connections from F,2,NA
 layer to F,1,NA
 layer ,NA,NA
•,NA,NA
  Inhibitory connection (negative weight) form F,2,NA
 layer to gain control ,NA,NA
•,NA,NA
  ,NA,NA
Excitatory connection (positive weight) from gain control to a layer ,NA,NA
•,NA,NA
  ,NA,NA
Inhibitory connection from F,1,NA
 layer to reset node ,NA,NA
•,NA,NA
  Excitatory connection from reset node to F,2,NA
 layer ,NA,NA
A Simplified Diagram of Network Layout,NA,NA
Figure 10.1,NA,NA
  simplified diagram of the neural network for an ART1 model.,NA,NA
Processing in ART1,NA,NA
"The ART1 paradigm, just like the Kohonen Self-Organizing Map to be introduced in ",NA,NA
"Chapter 11, performs data clustering on input data; like inputs are clustered together into a ",NA,NA
"category. As an example, you can use a data clustering algorithm such as ART1 for ",NA,NA
Optical ,NA,NA
Character Recognition,NA,NA
" (OCR), where you try to match different samples of a letter to its ",NA,NA
ASCII equivalent. Particular attention is made in the ART1 paradigm to ensure that old ,NA,NA
information is not thrown away while new information is assimilated.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/243-246.html (2 of 4) [21/11/02 21:57:46],NA
"An input vector, when applied to an ART1 system, is first compared to existing patterns in ",NA,NA
the system. If there is a close enough match within a specified tolerance (as indicated by a ,NA,NA
"vigilance parameter), then that stored pattern is made to resemble the input pattern further ",NA,NA
and the classification operation is complete. If the input pattern does not resemble any of ,NA,NA
"the stored patterns in the system, then a new category is created with a new stored pattern ",NA,NA
that resembles the input pattern.,NA,NA
Special Features of the ART1 Model,NA,NA
One special feature of an ART1 model is that a ,NA,NA
two-thirds rule,NA,NA
 is necessary to determine ,NA,NA
the activity of neurons in the F,1,NA
 layer. There are three input sources to each neuron in layer ,NA,NA
F,1,NA
". They are the external input, the output of gain control, and the outputs of F",2,NA
 layer ,NA,NA
neurons. The F,1,NA
neurons will not fire unless at least two of the three inputs are active. The ,NA,NA
gain control unit and the two-thirds rule together ensure proper response from the input ,NA,NA
layer neurons. A second feature is that a vigilance parameter is used to determine the ,NA,NA
"activity of the reset unit, which is activated whenever there is no match found among ",NA,NA
existing patterns during classification.,NA,NA
Notation for ART1 Calculations,NA,NA
Let us list the various symbols we will use to describe the operation of a neural network ,NA,NA
for an ART1 model: ,NA,NA
w,ij,NA
v,ji,NA
a,i,NA
b,j,NA
x,i,NA
y,j,NA
z,i,NA
,NA,NA
m,NA,NA
Weight on the connection from the ,NA,NA
i,NA,NA
th neuron in the F,1,NA
 layer to the ,NA,NA
j,NA,NA
th ,NA,NA
neuron in the F,2,NA
 layer ,NA,NA
Weight on the connection from the ,NA,NA
j,NA,NA
th neuron in the F,2,NA
 layer to the ,NA,NA
i,NA,NA
th ,NA,NA
neuron on the F,1,NA
 layer ,NA,NA
Activation of the ,NA,NA
i,NA,NA
th neuron in the F,1,NA
 layer ,NA,NA
Activation of the ,NA,NA
j,NA,NA
th neuron in the F,2,NA
 layer ,NA,NA
Output of the ,NA,NA
i,NA,NA
th neuron in the F,1,NA
 layer ,NA,NA
Output of the ,NA,NA
j,NA,NA
th neuron in the F,2,NA
 layer ,NA,NA
Input to the ,NA,NA
i,NA,NA
th neuron in F,1,NA
 layer from F,2,NA
 layer ,NA,NA
"Vigilance parameter, positive and no greater than 1 (0<",NA,NA
,NA,NA
 1) ,NA,NA
Number of neurons in the F,1,NA
 layer ,NA,NA
n ,NA,NA
Number of neurons in the F,2,NA
 layer ,NA,NA
I ,NA,NA
Input vector ,NA,NA
S,i,NA
Sum of the components of the input vector ,NA,NA
S,x,NA
Sum of the outputs of neurons in the F,1,NA
 layer ,NA,NA
A,NA,NA
", ",NA,NA
C,NA,NA
", ",NA,NA
D ,NA,NA
Parameters with positive values or zero ,NA,NA
L ,NA,NA
Parameter with value greater than 1 ,NA,NA
B ,NA,NA
Parameter with value less than D + 1 but at least as large as either D or 1 ,NA,NA
r ,NA,NA
Index of winner of competition in the F,2,NA
 layer ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/243-246.html (4 of 4) [21/11/02 21:57:46]",NA
Previous Table of Contents Next,NA,NA
Algorithm for ART1 Calculations,NA,NA
The ART1 equations are not easy to follow. We follow the description of the algorithm found in James ,NA,NA
"A. Freeman and David M. Skapura. The following equations, taken in the order given, describe the ",NA,NA
steps in the algorithm. Note that ,NA,NA
binary,NA,NA
 input patterns are used in ART1.,NA,NA
Initialization of Parameters,NA,NA
w,ij,NA
should be positive and less than ,NA,NA
L,NA,NA
 / ( ,NA,NA
m,NA,NA
 - 1 + ,NA,NA
L,NA,NA
) ,NA,NA
v,ji,NA
should be greater than ( ,NA,NA
B,NA,NA
 - 1 ) / ,NA,NA
D,NA,NA
a,i,NA
 = -,NA,NA
B,NA,NA
 / ( 1 + ,NA,NA
C,NA,NA
 ) ,NA,NA
Equations for ART1 Computations,NA,NA
"When you read below the equations for ART1 computations, keep in mind the following ",NA,NA
considerations. If a subscript ,NA,NA
i,NA,NA
" appears on the left-hand side of the equation, it means that there are ",NA,NA
m ,NA,NA
"such equations, as the subscript i varies from 1 to m. Similarly, if instead a subscript ",NA,NA
j,NA,NA
" occurs, then there ",NA,NA
are ,NA,NA
n,NA,NA
 such equations as ,NA,NA
j,NA,NA
 ranges from 1 to ,NA,NA
n,NA,NA
. The equations are used in the order they are given. They ,NA,NA
"give a step-by-step description of the following algorithm. All the variables, you recall, are defined in the ",NA,NA
"earlier section on notation. For example, I is the input vector.",NA,NA
F,1,NA
 layer calculations:,NA,NA
 a,i,NA
 = I,i,NA
 / ( 1 + A ( I,i,NA
 + B ) + C ),NA,NA
 x,i,NA
 = 1 if a,i,NA
 > 0,NA,NA
 0 if a,i,NA
,NA,NA
 0,NA,NA
F,2,NA
 layer calculations:,NA,NA
 b,j,NA
 = ,NA,NA
©,NA,NA
 w,ij,NA
 x,i,NA
", the summation being on i from 1 to m",NA,NA
 y,j,NA
 = 1 if jth neuron has the largest activation value in the F,2,NA
 layer,NA,NA
 = 0 if jth neuron is not the winner in F,2,NA
 layer,NA,NA
Top-down inputs:,NA,NA
 z,i,NA
 = ,NA,NA
©,NA,NA
v,ji,NA
y,j,NA
", the summation being on j from 1 to n (You will ",NA,NA
notice that exactly one term is nonzero),NA,NA
F,1,NA
 layer calculations:,NA,NA
 a,i,NA
 = ( I,i,NA
 + D z,i,NA
 - B ) / ( 1 + A ( I,i,NA
 + D z,i,NA
 ) + C ) ,NA,NA
x,i,NA
 = 1 if a,i,NA
 > 0,NA,NA
 = 0 if a,i,NA
,NA,NA
 0,NA,NA
Checking with vigilance parameter:,NA,NA
If ( ,NA,NA
S,x,NA
 / ,NA,NA
S,I,NA
 ) <,NA,NA
©,NA,NA
", set ",NA,NA
y,j,NA
 = 0 for all ,NA,NA
j,NA,NA
", including the winner ",NA,NA
r,NA,NA
 in ,NA,NA
F,2,NA
" layer, and consider the ",NA,NA
j,NA,NA
th neuron ,NA,NA
"inactive (this step is reset, skip remaining steps).",NA,NA
If ( ,NA,NA
S,x,NA
 / ,NA,NA
S,I,NA
 ) ,NA,NA
©,NA,NA
", then continue.",NA,NA
Modifying top-down and bottom-up connection weight for winner r:,NA,NA
 v,ir,NA
  = ( L / ( S,x,NA
 + L -1 ) if x,i,NA
 = 1,NA,NA
 = 0 if x,i,NA
 = 0,NA,NA
 w,ri,NA
  = 1 if x,i,NA
 = 1,NA,NA
 = 0 if x,i,NA
 = 0,NA,NA
"Having finished with the current input pattern, we repeat these steps with a new input pattern. We lose ",NA,NA
the index ,NA,NA
r,NA,NA
 given to one neuron as a winner and treat all neurons in the F,2,NA
 layer with their original ,NA,NA
indices (subscripts).,NA,NA
The above presentation of the algorithm is hoped to make all the steps as clear as possible. The process is ,NA,NA
"rather involved. To recapitulate, first an input vector is presented to the F",1,NA
" layer neurons, their ",NA,NA
"activations are determined, and then the ",NA,NA
threshold,NA,NA
 function is used. The outputs of the F,1,NA
 layer neurons ,NA,NA
constitute the inputs to the F,2,NA
" layer neurons, from which a winner is designated on the basis of the largest ",NA,NA
"activation. The winner only is allowed to be active, meaning that the output is 1 for the winner and 0 for ",NA,NA
"all the rest. The equations implicitly incorporate the use of the 2/3 rule that we mentioned earlier, and ",NA,NA
they also incorporate the way the gain control is used. The gain control is designed to have a ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/246-250.html (2 of 4) [21/11/02 21:57:47],NA
value 1 in the phase of determining the activations of the neurons in the F,2,NA
 layer and 0 if either there is ,NA,NA
no input vector or output from the F,2,NA
 layer is propagated to the F,1,NA
 layer.,NA,NA
Other Models,NA,NA
"Extensions of an ART1 model, which is for binary patterns, are ART2 and ART3. Of these, ART2 ",NA,NA
"model categorizes and stores analog-valued patterns, as well as binary patterns, while ART3 addresses ",NA,NA
computational problems of hierarchies. ,NA,NA
C++ Implementation,NA,NA
"Again, the algorithm for ART1 processing as given in Freeman and Skapura is followed for our C++ ",NA,NA
implementation. Our objective in programming ART1 is to provide a feel for the workings of this ,NA,NA
"paradigm with a very simple program implementation. For more details on the inner workings of ART1, ",NA,NA
"you are encouraged to consult Freeman and Skapura, or other references listed at the back of the book. ",NA,NA
A Header File for the C++ Program for the ART1 Model Network,NA,NA
The header file for the C++ program for the ART1 model network is art1net.hpp. It contains the ,NA,NA
"declarations for two classes, an ",NA,NA
artneuron,NA,NA
" class for neurons in the ART1 model, and a ",NA,NA
network,NA,NA
" class, ",NA,NA
which is declared as a ,NA,NA
friend,NA,NA
 class in the ,NA,NA
artneuron,NA,NA
 class. Functions declared in the ,NA,NA
network,NA,NA
 class ,NA,NA
"include one to do the iterations for the network operation, finding the winner in a given iteration, and ",NA,NA
one to inquire if reset is needed.,NA,NA
"//art1net.h   V. Rao,  H. Rao ",NA,NA
//Header file for ART1 model network program,NA,NA
#include <iostream.h> ,NA,NA
#define MXSIZ 10,NA,NA
class artneuron ,NA,NA
{,NA,NA
protected:,NA,NA
 int nnbr;,NA,NA
" int inn,outn;",NA,NA
 int output;,NA,NA
 double activation;,NA,NA
 double outwt[MXSIZ];,NA,NA
 char *name;,NA,NA
 friend class network;,NA,NA
public:,NA,NA
 artneuron() { };,NA,NA
" void getnrn(int,int,int,char *);",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/246-250.html (3 of 4) [21/11/02 21:57:47],NA
};,NA,NA
class network ,NA,NA
{ ,NA,NA
public:,NA,NA
" int  anmbr,bnmbr,flag,ninpt,sj,so,winr;",NA,NA
" float ai,be,ci,di,el,rho;",NA,NA
" artneuron (anrn)[MXSIZ],(bnrn)[MXSIZ];",NA,NA
" int outs1[MXSIZ],outs2[MXSIZ];",NA,NA
 int lrndptrn[MXSIZ][MXSIZ];,NA,NA
" double acts1[MXSIZ],acts2[MXSIZ];",NA,NA
" double mtrx1[MXSIZ][MXSIZ],mtrx2[MXSIZ][MXSIZ];",NA,NA
};,NA,NA
 network() { };,NA,NA
" void getnwk(int,int,float,float,float,float,float); ",NA,NA
void prwts1();,NA,NA
 void prwts2();,NA,NA
" int winner(int k,double *v,int);",NA,NA
 void practs1();,NA,NA
 void practs2();,NA,NA
 void prouts1();,NA,NA
 void prouts2();,NA,NA
" void iterate(int *,float,int);",NA,NA
 void asgninpt(int *);,NA,NA
 void comput1(int);,NA,NA
 void comput2(int *);,NA,NA
 void prlrndp();,NA,NA
 void inqreset(int);,NA,NA
 void adjwts1();,NA,NA
 void adjwts2();,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch10/246-250.html (4 of 4) [21/11/02 21:57:47]",NA
Previous Table of Contents Next,NA,NA
A Source File for C++ Program for an ART1 Model Network,NA,NA
The implementations of the functions declared in the header file are contained in the source file ,NA,NA
for the C++ program for an ART1 model network. It also has the ,NA,NA
main,NA,NA
" function, which contains ",NA,NA
"specifications of the number of neurons in the two layers of the network, the values of the ",NA,NA
"vigilance and other parameters, and the input vectors. Note that if there are ",NA,NA
n,NA,NA
" neurons in a layer, ",NA,NA
"they are numbered serially from 0 to n–1, and not from 1 to ",NA,NA
n,NA,NA
 in the C++ program. ,NA,NA
The source file is called art1net.cpp. It is set up with six neurons in the F,1,NA
 layer and seven ,NA,NA
neurons in the ,NA,NA
F,2,NA
 layer. The ,NA,NA
main,NA,NA
 function also contains the parameters needed in the ,NA,NA
algorithm.,NA,NA
"To initialize the bottom-up weights, we set each weight to be –0.1 + ",NA,NA
L,NA,NA
/(,NA,NA
m,NA,NA
 – 1 + ,NA,NA
L,NA,NA
) so that it is ,NA,NA
greater than 0 and less than ,NA,NA
L,NA,NA
/(,NA,NA
m,NA,NA
 – 1 + ,NA,NA
L,NA,NA
"), as suggested before. Similarly, the top-down weights ",NA,NA
are initialized by setting each of them to 0.2 + (,NA,NA
B,NA,NA
 – 1)/,NA,NA
D,NA,NA
 so it would be greater than (,NA,NA
B,NA,NA
 – 1)/,NA,NA
D,NA,NA
. ,NA,NA
Initial activations of the F,1,NA
 layer neurons are each set to –,NA,NA
B,NA,NA
/(1 + ,NA,NA
C,NA,NA
"), as suggested earlier.",NA,NA
A ,NA,NA
restrmax,NA,NA
 function is defined to compute the maximum in an array when one of the array ,NA,NA
elements is not desired to be a candidate for the maximum. This facilitates the removal of the ,NA,NA
current winner from competition when reset is needed. Reset is needed when the degree of ,NA,NA
match is of a smaller magnitude than the vigilance parameter.,NA,NA
The function ,NA,NA
iterate,NA,NA
 is a ,NA,NA
member,NA,NA
 function of the ,NA,NA
network,NA,NA
 class and does the processing for ,NA,NA
the network. The ,NA,NA
inqreset,NA,NA
 function of the ,NA,NA
network,NA,NA
 class compares the ,NA,NA
vigilance,NA,NA
 parameter ,NA,NA
with the degree of match.,NA,NA
"//art1net.cpp  V. Rao, H. Rao",NA,NA
//Source file for ART1 network program,NA,NA
"#include ""art1net.h""",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (1 of 10) [21/11/02 21:57:48],NA
"int restrmax(int j,double *b,int k) ",NA,NA
{,NA,NA
" int i,tmp;",NA,NA
 for(i=0;i<j;i++){,NA,NA
 if(i !=k),NA,NA
 {tmp = i;,NA,NA
 i = j;},NA,NA
 },NA,NA
 for(i=0;i<j;i++){,NA,NA
 if( (i != tmp)&&(i != k)),NA,NA
 {if(b[i]>b[tmp]) tmp = i;}},NA,NA
 return tmp;,NA,NA
 },NA,NA
"void artneuron::getnrn(int m1,int m2,int m3, char *y) ",NA,NA
{ ,NA,NA
int i; ,NA,NA
name = y; ,NA,NA
nnbr = m1; ,NA,NA
outn = m2; ,NA,NA
inn  = m3;,NA,NA
for(i=0;i<outn;++i){,NA,NA
 outwt[i] = 0 ;,NA,NA
 },NA,NA
output = 0; ,NA,NA
activation = 0.0; ,NA,NA
},NA,NA
" void network::getnwk(int k,int l,float aa,float bb,float",NA,NA
" cc,float dd,float ll) ",NA,NA
{ ,NA,NA
anmbr = k; ,NA,NA
bnmbr = l; ,NA,NA
ninpt = 0; ,NA,NA
ai = aa; ,NA,NA
be = bb;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (2 of 10) [21/11/02 21:57:48],NA
ci = cc; ,NA,NA
di = dd; ,NA,NA
el = ll; ,NA,NA
"int i,j; ",NA,NA
flag = 0;,NA,NA
"char *y1=""ANEURON"", *y2=""BNEURON"" ;",NA,NA
for(i=0;i<anmbr;++i){,NA,NA
" anrn[i].artneuron::getnrn(i,bnmbr,0,y1);}",NA,NA
for(i=0;i<bnmbr;++i){,NA,NA
" bnrn[i].artneuron::getnrn(i,0,anmbr,y2);}",NA,NA
"float tmp1,tmp2,tmp3; ",NA,NA
tmp1 = 0.2 +(be - 1.0)/di; ,NA,NA
tmp2 = -0.1 + el/(anmbr - 1.0 +el); ,NA,NA
tmp3 = - be/(1.0 + ci);,NA,NA
for(i=0;i<anmbr;++i){,NA,NA
 anrn[i].activation = tmp3;,NA,NA
 acts1[i] = tmp3;,NA,NA
 for(j=0;j<bnmbr;++j){,NA,NA
 },NA,NA
 mtrx1[i][j]  = tmp1;,NA,NA
 mtrx2[j][i] = tmp2;,NA,NA
 anrn[i].outwt[j] = mtrx1[i][j]; ,NA,NA
bnrn[j].outwt[i] = mtrx2[j][i]; ,NA,NA
},NA,NA
prwts1(); ,NA,NA
prwts2(); ,NA,NA
practs1(); ,NA,NA
"cout<<""\n""; ",NA,NA
},NA,NA
"int network::winner(int k,double *v,int kk){ ",NA,NA
int t1;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (3 of 10) [21/11/02 21:57:48],NA
"t1 = restrmax(k,v,kk); ",NA,NA
return t1; ,NA,NA
} ,NA,NA
void network::prwts1() ,NA,NA
{ ,NA,NA
"int i3,i4; ",NA,NA
"cout<<""\nweights for F1 layer neurons: \n""; ",NA,NA
for(i3=0;i3<anmbr;++i3){,NA,NA
 for(i4=0;i4<bnmbr;++i4){,NA,NA
" cout<<anrn[i3].outwt[i4]<<""  "";}",NA,NA
" cout<<""\n""; } ",NA,NA
"cout<<""\n""; ",NA,NA
} ,NA,NA
void network::prwts2() ,NA,NA
{ ,NA,NA
"int i3,i4; ",NA,NA
"cout<<""\nweights for F2 layer neurons: \n""; ",NA,NA
for(i3=0;i3<bnmbr;++i3){,NA,NA
 for(i4=0;i4<anmbr;++i4){,NA,NA
" cout<<bnrn[i3].outwt[i4]<<""  "";};",NA,NA
" cout<<""\n"";  } ",NA,NA
"cout<<""\n""; ",NA,NA
} ,NA,NA
void network::practs1() ,NA,NA
{ ,NA,NA
int j; ,NA,NA
"cout<<""\nactivations of F1 layer neurons: \n""; ",NA,NA
for(j=0;j<anmbr;++j){,NA,NA
" cout<<acts1[j]<<""   "";}",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (4 of 10) [21/11/02 21:57:48],NA
"cout<<""\n""; ",NA,NA
},NA,NA
void network::practs2() ,NA,NA
{ ,NA,NA
int j; ,NA,NA
"cout<<""\nactivations of F2 layer neurons: \n"";",NA,NA
for(j=0;j<bnmbr;++j){,NA,NA
" cout<<acts2[j]<<""   "";}",NA,NA
"cout<<""\n""; ",NA,NA
},NA,NA
void network::prouts1() ,NA,NA
{ ,NA,NA
int j; ,NA,NA
"cout<<""\noutputs of F1 layer neurons: \n"";",NA,NA
for(j=0;j<anmbr;++j){,NA,NA
" cout<<outs1[j]<<""   "";}",NA,NA
"cout<<""\n""; ",NA,NA
},NA,NA
void network::prouts2() ,NA,NA
{ ,NA,NA
int j; ,NA,NA
"cout<<""\noutputs of F2 layer neurons: \n"";",NA,NA
for(j=0;j<bnmbr;++j){,NA,NA
" cout<<outs2[j]<<""   "";}",NA,NA
"cout<<""\n""; ",NA,NA
},NA,NA
void network::asgninpt(int *b) ,NA,NA
{ ,NA,NA
int j; ,NA,NA
sj = so = 0; ,NA,NA
"cout<<""\nInput vector is:\n"" ;",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (5 of 10) [21/11/02 21:57:48],NA
for(j=0;j<anmbr;++j){,NA,NA
" cout<<b[j]<<"" "";}",NA,NA
"cout<<""\n"";",NA,NA
for(j=0;j<anmbr;++j){,NA,NA
 sj += b[j];,NA,NA
 anrn[j].activation = b[j]/(1.0 +ci +ai*(b[j]+be)); ,NA,NA
acts1[j] = anrn[j].activation;,NA,NA
 if(anrn[j].activation > 0) anrn[j].output = 1;,NA,NA
 else,NA,NA
 anrn[j].output = 0;,NA,NA
 outs1[j] = anrn[j].output;,NA,NA
 so += anrn[j].output;,NA,NA
 },NA,NA
practs1(); ,NA,NA
prouts1(); ,NA,NA
},NA,NA
void network::inqreset(int t1) ,NA,NA
{ ,NA,NA
int jj; ,NA,NA
flag = 0; ,NA,NA
jj = so/sj; ,NA,NA
"cout<<""\ndegree of match: ""<<jj<<"" vigilance:  ""<<rho<<""\n"";",NA,NA
if( jj > rho ) flag = 1;,NA,NA
 else,NA,NA
" {cout<<""winner is ""<<t1;",NA,NA
" cout<<"" reset required \n"";}",NA,NA
},NA,NA
void network::comput1(int k) ,NA,NA
{ ,NA,NA
int j;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (6 of 10) [21/11/02 21:57:48],NA
for(j=0;j<bnmbr;++j){,NA,NA
 int ii1;,NA,NA
 double c1 = 0.0;,NA,NA
" cout<<""\n"";",NA,NA
 for(ii1=0;ii1<anmbr;++ii1){,NA,NA
 c1 += outs1[ii1] * mtrx2[j][ii1]; ,NA,NA
},NA,NA
 bnrn[j].activation = c1;,NA,NA
 acts2[j] = c1;};,NA,NA
"winr = winner(bnmbr,acts2,k); ",NA,NA
"cout<<""winner is ""<<winr; ",NA,NA
for(j=0;j<bnmbr;++j){,NA,NA
 if(j == winr) bnrn[j].output = 1;,NA,NA
 else bnrn[j].output =  0;,NA,NA
 outs2[j] = bnrn[j].output;,NA,NA
 },NA,NA
practs2(); ,NA,NA
prouts2(); ,NA,NA
},NA,NA
void network::comput2(int *b) ,NA,NA
{ ,NA,NA
double db[MXSIZ]; ,NA,NA
double tmp; ,NA,NA
so = 0; ,NA,NA
"int i,j;",NA,NA
for(j=0;j<anmbr;++j){,NA,NA
 db[j] =0.0;,NA,NA
 for(i=0;i<bnmbr;++i){,NA,NA
 db[j] += mtrx1[j][i]*outs2[i];};,NA,NA
 tmp = b[j] + di*db[j];,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (7 of 10) [21/11/02 21:57:48],NA
 acts1[j] = (tmp - be)/(ci +1.0 +ai*tmp);,NA,NA
 anrn[j].activation = acts1[j];,NA,NA
 if(anrn[j].activation > 0) anrn[j].output = 1;,NA,NA
 else anrn[j].output = 0;,NA,NA
 outs1[j] = anrn[j].output;,NA,NA
 so += anrn[j].output;,NA,NA
 } ,NA,NA
"cout<<""\n""; ",NA,NA
practs1(); ,NA,NA
prouts1(); ,NA,NA
} ,NA,NA
void network::adjwts1() ,NA,NA
{ ,NA,NA
int i; ,NA,NA
for(i=0;i<anmbr;++i){,NA,NA
 if(outs1[i] >0) {mtrx1[i][winr]  = 1.0;},NA,NA
 else,NA,NA
 {mtrx1[i][winr] = 0.0;},NA,NA
 anrn[i].outwt[winr] = mtrx1[i][winr];} ,NA,NA
prwts1(); ,NA,NA
} ,NA,NA
void network::adjwts2() ,NA,NA
{ ,NA,NA
int i; ,NA,NA
"cout<<""\nwinner is ""<<winr<<""\n""; ",NA,NA
for(i=0;i<anmbr;++i){,NA,NA
 if(outs1[i] > 0) {mtrx2[winr][i] = el/(so + el -1);},NA,NA
 else,NA,NA
 {mtrx2[winr][i] = 0.0;},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (8 of 10) [21/11/02 21:57:48],NA
 bnrn[winr].outwt[i]  = mtrx2[winr][i];},NA,NA
prwts2(); ,NA,NA
},NA,NA
"void network::iterate(int *b,float rr,int kk) ",NA,NA
{ ,NA,NA
int j; ,NA,NA
rho = rr; ,NA,NA
flag = 0;,NA,NA
asgninpt(b); ,NA,NA
comput1(kk); ,NA,NA
comput2(b); ,NA,NA
inqreset(winr);,NA,NA
if(flag == 1){,NA,NA
 ninpt ++;,NA,NA
 adjwts1();,NA,NA
 adjwts2();,NA,NA
 int j3;,NA,NA
 for(j3=0;j3<anmbr;++j3){,NA,NA
 lrndptrn[ninpt][j3] = b[j3];},NA,NA
 prlrndp();,NA,NA
 },NA,NA
else,NA,NA
 {,NA,NA
 for(j=0;j<bnmbr;++j){,NA,NA
 outs2[j] = 0;,NA,NA
 bnrn[j].output = 0;},NA,NA
},NA,NA
" iterate(b,rr,winr);",NA,NA
 },NA,NA
void network::prlrndp(),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (9 of 10) [21/11/02 21:57:48],NA
{ ,NA,NA
int j; ,NA,NA
"cout<<""\nlearned vector # ""<<ninpt<<""  :\n"";",NA,NA
for(j=0;j<anmbr;++j){,NA,NA
" cout<<lrndptrn[ninpt][j]<<""  "";}",NA,NA
"cout<<""\n""; ",NA,NA
},NA,NA
void main() ,NA,NA
{ ,NA,NA
"int ar = 6, br = 7, rs = 8; ",NA,NA
"float aa = 2.0,bb = 2.5,cc = 6.0,dd = 0.85,ll = 4.0,rr =",NA,NA
 ,NA,NA
0.95; ,NA,NA
"int inptv[][6]={0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,1,0,1,0,\",NA,NA
" 1,0};",NA,NA
"cout<<""\n\nTHIS PROGRAM IS FOR AN -ADAPTIVE RESONANCE THEORY\",NA,NA
" 1 - NETWORK.\n""; ",NA,NA
"cout<<""THE NETWORK IS SET UP FOR ILLUSTRATION WITH ""<<ar<<"" \",NA,NA
" INPUT NEURONS,\n""; ",NA,NA
"cout<<"" AND ""<<br<<"" OUTPUT NEURONS.\n"";",NA,NA
static network bpn; ,NA,NA
"bpn.getnwk(ar,br,aa,bb,cc,dd,ll) ; ",NA,NA
"bpn.iterate(inptv[0],rr,rs); ",NA,NA
"bpn.iterate(inptv[1],rr,rs); ",NA,NA
"bpn.iterate(inptv[2],rr,rs); ",NA,NA
"bpn.iterate(inptv[3],rr,rs); ",NA,NA
},NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch10/250-261.html (10 of 10) [21/11/02 21:57:48]",NA
Program Output,"Four 
 input
  vectors are used in the trial run of the program, and these are specified in the main function. The 
 output is self-explanatory. We have included only in this text some comments regarding the output. These 
 comments are enclosed within strings of asterisks. They are not actually part of the program output. Table 10.1 
 shows a summarization of the categorization of the inputs done by the network. Keep in mind that the 
 numbering of the neurons in any layer, which has 
 n
  neurons, is from 0 to 
 n
  – 1, and not from 1 to 
 n
 .
  
 Table 10.1
 Categorization of Inputs 
  
  
 input 
  
 winner in F
 2
  layer 
  
 0 1 0 0 0 0 
  
 0, no reset 
  
 1 0 1 0 1 0 
  
 1, no reset 
  
 0 0 0 0 1 0 
  
 1, after reset 2 
  
 1 0 1 0 1 0 
  
 1, after reset 3 
  
  
 The input pattern 0 0 0 0 1 0 is considered a subset of the pattern 1 0 1 0 1 0 in the sense that in whatever 
 position the first pattern has a 1, the second pattern also has a 1. Of course, the second pattern has 1’s in other 
 positions as well. At the same time, the pattern 1 0 1 0 1 0 is considered a superset of the pattern 0 0 0 0 1 0. The 
 reason that the pattern 1 0 1 0 1 0 is repeated as input after the pattern 0 0 0 0 1 0 is processed, is to see what 
 happens with this superset. In both cases, the degree of match falls short of the vigilance parameter, and a reset 
 is needed. 
  
 Here’s the output of the program:
  
 THIS PROGRAM IS FOR AN ADAPTIVE RESONANCE THEORY
  
 1-NETWORK. THE NETWORK IS SET UP FOR ILLUSTRATION WITH SIX INPUT NEURONS
  
 AND SEVEN OUTPUT NEURONS.
  
 *************************************************************
  
 Initialization of connection weights and F1 layer activations. F1 layer
  
 connection weights are all chosen to be equal to a random value subject
  
 to the conditions given in the algorithm. Similarly, F2 layer connection
  
 weights are all chosen to be equal to a random value subject to the
  
 conditions given in the algorithm.",NA
Previous Table of Contents Next,NA,NA
Summary,NA,NA
This chapter presented the basics of the Adaptive Resonance Theory of Grossberg and ,NA,NA
Carpenter and a C++ implementation of the neural network modeled for this theory. It is an ,NA,NA
elegant theory that addresses the stability–plasticity dilemma. The network relies on ,NA,NA
resonance. It is a self-organizing network and does categorization by associating individual ,NA,NA
neurons of the F,2,NA
" layer with individual patterns. By employing a so-called 2/3 rule, it ",NA,NA
ensures stability in learning patterns.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathemati...C++_Neural_Networks_and_Fuzzy_Logic/ch10/269-269.html [21/11/02 21:57:50]",NA
Previous Table of Contents Next,NA,NA
Chapter 11 ,NA,NA
The Kohonen Self-Organizing Map ,NA,NA
Introduction,NA,NA
"This chapter discusses one type of unsupervised competitive learning, the ",NA,NA
Kohonen feature ,NA,NA
map,NA,NA
", or ",NA,NA
self-organizing map,NA,NA
 (,NA,NA
SOM,NA,NA
"). As you recall, in unsupervised learning there are no ",NA,NA
"expected outputs presented to a neural network, as in a supervised training algorithm such ",NA,NA
"as backpropagation. Instead, a network, by its self-organizing properties, is able to infer ",NA,NA
relationships and learn more as more inputs are presented to it. One advantage to this ,NA,NA
scheme is that you can expect the system to change with changing conditions and inputs. ,NA,NA
The system constantly learns. The Kohonen SOM is a neural network system developed by ,NA,NA
Teuvo Kohonen of Helsinki University of Technology and is often used to classify inputs ,NA,NA
"into different categories. Applications for feature maps can be traced to many areas, ",NA,NA
including speech recognition and robot motor control.,NA,NA
Competitive Learning,NA,NA
A Kohonen feature map may be used by itself or as a layer of another neural network. A ,NA,NA
Kohonen layer is composed of neurons that compete with each other. Like in Adaptive ,NA,NA
"Resonance Theory, the Kohonen SOM is another case of using a winner-take-all strategy. ",NA,NA
Inputs are fed into each of the neurons in the Kohonen layer (from the input layer). Each ,NA,NA
neuron determines its output according to a weighted sum formula: ,NA,NA
Output = ,NA,NA
©,NA,NA
 w,ij,NA
 x,i,NA
"The weights and the inputs are usually normalized, which means that the magnitude of the ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/271-274.html (1 of 4) [21/11/02 21:57:51],NA
weight and input vectors are set equal to one. The neuron with the largest output is the ,NA,NA
winner. This neuron has a final output of 1. All other neurons in the layer have an output of ,NA,NA
zero. Differing input patterns end up firing different winner neurons. Similar or identical ,NA,NA
input patterns classify to the same output neuron. You get like inputs clustered together. In ,NA,NA
"Chapter 12, you will see the use of a Kohonen network in pattern classification.",NA,NA
Normalization of a Vector,NA,NA
"Consider a vector, ",NA,NA
A,NA,NA
 = ,NA,NA
ax,NA,NA
 + ,NA,NA
by,NA,NA
 + ,NA,NA
cz,NA,NA
. The normalized vector ,NA,NA
A’,NA,NA
 is obtained by dividing each ,NA,NA
component of ,NA,NA
A,NA,NA
 by the square root of the sum of squares of all the components. In other ,NA,NA
words each component is multiplied by 1/ [radic](,NA,NA
a,2,NA
 + ,NA,NA
b,2,NA
 + ,NA,NA
c,2,NA
). Both the weight vector and ,NA,NA
the input vector are normalized during the operation of the Kohonen feature map. The ,NA,NA
reason for this is the training law uses subtraction of the weight vector from the input ,NA,NA
vector. Using normalization of the values in the subtraction reduces both vectors to a unit-,NA,NA
"less status, and hence, makes the subtraction of like quantities possible. You will learn ",NA,NA
more about the training law shortly.,NA,NA
Lateral Inhibition,NA,NA
Lateral inhibition,NA,NA
 is a process that takes place in some biological neural networks. Lateral ,NA,NA
"connections of neurons in a given layer are formed, and squash distant neighbors. The ",NA,NA
"strength of connections is inversely related to distance. The positive, supportive ",NA,NA
connections are termed as ,NA,NA
excitatory,NA,NA
" while the negative, squashing connections are termed ",NA,NA
inhibitory,NA,NA
.,NA,NA
A biological example of lateral inhibition occurs in the human vision system.,NA,NA
The Mexican Hat Function,NA,NA
"Figure 11.1 shows a function, called the ",NA,NA
mexican hat,NA,NA
" function, which shows the ",NA,NA
relationship between the connection strength and the distance from the winning neuron. ,NA,NA
The effect of this function is to set up a competitive environment for learning. Only ,NA,NA
winning neurons and their neighbors participate in learning for a given input pattern.,NA,NA
Figure 11.1,NA,NA
  The ,NA,NA
mexican hat,NA,NA
 function showing lateral inhibition. ,NA,NA
Training Law for the Kohonen Map,NA,NA
The training law for the Kohonen feature map is straightforward. The change in weight ,NA,NA
"vector for a given output neuron is a gain constant, alpha, multiplied by the difference ",NA,NA
between the input vector and the old weight vector: ,NA,NA
W,new,NA
 = ,NA,NA
W,old,NA
 + alpha * (Input -,NA,NA
W,old,NA
),NA,NA
Both the old weight vector and the input vector are normalized to unit length. Alpha is a ,NA,NA
gain constant between 0 and 1.,NA,NA
Significance of the Training Law,NA,NA
"Let us consider the case of a two-dimensional input vector. If you look at a unit circle, as ",NA,NA
"shown in Figure 11.2, the effect of the training law is to try to align the weight vector and ",NA,NA
the input vector. Each pattern attempts to nudge the weight vector closer by a fraction ,NA,NA
determined by alpha. For three dimensions the surface becomes a unit sphere instead of a ,NA,NA
circle. For higher dimensions you term the surface a ,NA,NA
hypersphere,NA,NA
. It is not necessarily ideal ,NA,NA
to have perfect alignment of the input and weight vectors. You use neural networks for ,NA,NA
"their ability to recognize patterns, but also to generalize input data sets. By aligning all ",NA,NA
"input vectors to the corresponding winner weight vectors, you are essentially ",NA,NA
memorizing ,NA,NA
"the input data set classes. It may be more desirable to come close, so that noisy or ",NA,NA
incomplete inputs may still trigger the correct classification.,NA,NA
Figure 11.2,NA,NA
  The training law for the Kohonen map as shown on a unit circle.,NA,NA
The Neighborhood Size and Alpha,NA,NA
"In the Kohonen map, a parameter called the ",NA,NA
neighborhood size,NA,NA
 is used to model the effect ,NA,NA
of the ,NA,NA
mexican hat,NA,NA
 function. Those neurons that are within the distance specified by the ,NA,NA
neighborhood size participate in training and weight vector changes; those that are outside ,NA,NA
this distance do not participate in learning. The neighborhood size typically is started as an ,NA,NA
initial value and is decreased as the input pattern cycles continue. This process tends to ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/271-274.html (3 of 4) [21/11/02 21:57:51],NA
support the winner-take-all strategy by eventually singling out a winner neuron for a given ,NA,NA
pattern.,NA,NA
Figure 11.3 shows a linear arrangement of neurons with a neighborhood size of 2. The ,NA,NA
hashed central neuron is the winner. The darkened adjacent neurons are those that will ,NA,NA
participate in training.,NA,NA
Figure 11.3,NA,NA
  Winner neuron with a neighborhood size of 2 for a Kohonen map.,NA,NA
"Besides the neighborhood size, ",NA,NA
alpha,NA,NA
 typically is also reduced during simulation. You will ,NA,NA
see these features when we develop a Kohonen map program.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/271-274.html (4 of 4) [21/11/02 21:57:51]",NA
Previous Table of Contents Next,NA,NA
C++ Code for Implementing a Kohonen Map,NA,NA
The C++ code for the Kohonen map draws on much of the code developed for the backpropagation ,NA,NA
simulator. The Kohonen map is a much simpler program and may not rely on as large a data set for input. ,NA,NA
"The Kohonen map program uses only two files, an input file and an output file. In order to use the program, ",NA,NA
you must create an input data set and save this in a file called ,NA,NA
input.dat,NA,NA
. The output file is called ,NA,NA
kohonen.dat,NA,NA
 and is saved in your current working directory. You will get more details shortly on the ,NA,NA
formats of these files.,NA,NA
The Kohonen Network,NA,NA
"The Kohonen network has two layers, an ",NA,NA
input,NA,NA
 layer and a ,NA,NA
Kohonen output,NA,NA
 layer. (See Figure 11.4). The ,NA,NA
input layer is a size determined by the user and must match the size of each row (pattern) in the input data ,NA,NA
file.,NA,NA
Figure 11.4,NA,NA
  A Kohonen network.,NA,NA
Modeling Lateral Inhibition and Excitation,NA,NA
The ,NA,NA
mexican hat,NA,NA
 function shows positive values for an immediate neighborhood around the neuron and ,NA,NA
negative values for distant neurons. A true method of modeling would incorporate mutual excitation or ,NA,NA
support for neurons that are within the neighborhood (with this excitation increasing for nearer neurons) ,NA,NA
"and inhibition for distant neurons outside the neighborhood. For the sake of computational efficiency, we ",NA,NA
model lateral inhibition and excitation by looking at the maximum output for the output neurons and ,NA,NA
making that output belong to a winner neuron. Other outputs are inhibited by setting their outputs to zero. ,NA,NA
"Training, or weight update, is performed on all outputs that are within a neighborhood size distance from ",NA,NA
the winner neuron. Neurons outside the neighborhood do not participate in training. The true way of ,NA,NA
modeling lateral inhibition would be too expensive since the number of lateral connections is quite large. ,NA,NA
You will find that this approximation will lead to a network with many if not all of the properties of a true ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/274-279.html (1 of 4) [21/11/02 21:57:52],NA
modeling approach of a Kohonen network.,NA,NA
Classes to be Used,NA,NA
"We use many of the classes from the backpropagation simulator. We require only two layers, the input ",NA,NA
layer and the Kohonen layer. We make a new layer class called the ,NA,NA
Kohonen layer,NA,NA
" class, and a new ",NA,NA
network,NA,NA
 class called the ,NA,NA
Kohonen_network,NA,NA
.,NA,NA
Revisiting the Layer Class,NA,NA
The ,NA,NA
layer,NA,NA
" class needs to be slightly modified, as shown in Listing 11.1.",NA,NA
Listing 11.1,NA,NA
 Modification of layer.h,NA,NA
"// layer.h             V.Rao, H. Rao ",NA,NA
// header file for the layer class hierarchy and ,NA,NA
// the network class,NA,NA
#define MAX_LAYERS    5 ,NA,NA
#define MAX_VECTORS   100,NA,NA
class network; ,NA,NA
class Kohonen_network;,NA,NA
class layer ,NA,NA
{,NA,NA
protected:,NA,NA
 int num_inputs;,NA,NA
 int num_outputs;,NA,NA
 float *outputs;// pointer to array of outputs,NA,NA
" float *inputs; // pointer to array of inputs, which",NA,NA
 // are outputs of some other layer,NA,NA
 friend network; ,NA,NA
friend Kohonen_network; // update for Kohonen model,NA,NA
public:,NA,NA
}; ,NA,NA
...,NA,NA
 virtual void calc_out()=0;,NA,NA
Here the changes are indicated in italic. You notice that the ,NA,NA
Kohonen_network,NA,NA
 is made a friend to the ,NA,NA
layer,NA,NA
" class, so that the ",NA,NA
Kohonen_network,NA,NA
 can have access to the data of a layer.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/274-279.html (2 of 4) [21/11/02 21:57:52],NA
A New Layer Class for a Kohonen Layer,NA,NA
The next step to take is to create a ,NA,NA
Kohonen_layer,NA,NA
 class and a ,NA,NA
Kohonen_network,NA,NA
 class. This is shown in ,NA,NA
Listing 11.2.,NA,NA
Listing 11.2,NA,NA
 The Kohonen_layer class and Kohonen_network class in layerk.h,NA,NA
"// layerk.h           V.Rao, H. Rao ",NA,NA
// header file for the Kohonen layer and ,NA,NA
// the Kohonen network,NA,NA
class Kohonen_network;,NA,NA
class Kohonen_layer: public layer ,NA,NA
{,NA,NA
protected:,NA,NA
 float * weights;,NA,NA
 int winner_index;,NA,NA
 float win_distance;,NA,NA
 int neighborhood_size;,NA,NA
 friend Kohonen_network;,NA,NA
public:,NA,NA
" Kohonen_layer(int, int, int);",NA,NA
 ~Kohonen_layer();,NA,NA
 virtual void calc_out();,NA,NA
 void randomize_weights();,NA,NA
 void update_neigh_size(int);,NA,NA
 void update_weights(const float);,NA,NA
 void list_weights();,NA,NA
 void list_outputs();,NA,NA
 float get_win_dist();,NA,NA
};,NA,NA
class Kohonen_network,NA,NA
{,NA,NA
private:,NA,NA
 layer *layer_ptr[2];,NA,NA
 int layer_size[2];,NA,NA
 int neighborhood_size;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/274-279.html (3 of 4) [21/11/02 21:57:52],NA
public:,NA,NA
 Kohonen_network();,NA,NA
 ~Kohonen_network();,NA,NA
 void get_layer_info();,NA,NA
 void set_up_network(int);,NA,NA
 void randomize_weights();,NA,NA
 void update_neigh_size(int);,NA,NA
 void update_weights(const float); ,NA,NA
void list_weights();,NA,NA
 void list_outputs();,NA,NA
 void get_next_vector(FILE *); ,NA,NA
void process_next_pattern();,NA,NA
 float get_win_dist();,NA,NA
 int get_win_index();,NA,NA
};,NA,NA
The ,NA,NA
Kohonen_layer,NA,NA
 is derived from the ,NA,NA
layer,NA,NA
" class, so it has pointers inherited that point to a set of ",NA,NA
outputs and a set of inputs. Let’s look at some of the functions and member variables.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/274-279.html (4 of 4) [21/11/02 21:57:52]",NA
Previous Table of Contents Next,NA,NA
Kohonen_layer:,NA,NA
•float * weights,NA,NA
 Pointer to the weights matrix. ,NA,NA
•int winner_index,NA,NA
" Index value of the output, which is the winner. ",NA,NA
•float win_distance,NA,NA
 The Euclidean distance of the winner weight vector from the input ,NA,NA
vector. ,NA,NA
•int neighborhood_size,NA,NA
 The size of the neighborhood. ,NA,NA
"•Kohonen_layer(int, int, int)",NA,NA
" Constructor for the layer: inputs, outputs, and the ",NA,NA
neighborhood size. ,NA,NA
•~Kohonen_layer(),NA,NA
 Destructor. ,NA,NA
•virtual void calc_out(),NA,NA
 The function to calculate the outputs; for the Kohonen layer this ,NA,NA
models lateral competition. ,NA,NA
•void randomize_weights(),NA,NA
 A function to initialize weights with random normal values. ,NA,NA
•void update_neigh_size(nt),NA,NA
 This function updates the neighborhood size with a new ,NA,NA
value. ,NA,NA
•void update_weights(const float),NA,NA
 This function updates the weights according to the ,NA,NA
"training law using the passed parameter, alpha. ",NA,NA
•void list_weights(),NA,NA
 This function can be used to list the weight matrix. ,NA,NA
•void list_outputs(),NA,NA
 This function is used to write outputs to the output file. ,NA,NA
•float get_win_dist(),NA,NA
 Returns the Euclidean distance between the winner weight vector and ,NA,NA
the input vector. ,NA,NA
Kohonen_network:,NA,NA
•layer *layer_ptr[2],NA,NA
" Pointer array; element 0 points to the input layer, element 1 points to ",NA,NA
the Kohonen layer. ,NA,NA
•int layer_size[2],NA,NA
 Array of layer sizes for the two layers. ,NA,NA
•int neighborhood_size,NA,NA
 The current neighborhood size. ,NA,NA
•Kohonen_network(),NA,NA
 Constructor. ,NA,NA
•~Kohonen_network(),NA,NA
 Destructor. ,NA,NA
•void get_layer_info(),NA,NA
 Gets information about the layer sizes. ,NA,NA
•void set_up_network(int),NA,NA
 Connects layers and sets up the Kohonen map. ,NA,NA
•void randomize_weights(),NA,NA
 Creates random normalized weights. ,NA,NA
•void update_neigh_size(int),NA,NA
 Changes the neighborhood size. ,NA,NA
•void update_weights(const float),NA,NA
 Performs weight update according to the training law. ,NA,NA
•void list_weights(),NA,NA
 Can be used to list the weight matrix. ,NA,NA
•void list_outputs(),NA,NA
 Can be used to list outputs. ,NA,NA
•void get_next_vector(FILE *),NA,NA
 Function gets another input vector from the input file. ,NA,NA
•void process_next_pattern(),NA,NA
 Applies pattern to the Kohonen map. ,NA,NA
•float get_win_dist(),NA,NA
 Returns the winner’s distance from the input vector. ,NA,NA
•int get_win_index(),NA,NA
 Returns the index of the winner. ,NA,NA
Implementation of the Kohonen Layer and Kohonen Network,NA,NA
Listing 11.3 shows the ,NA,NA
layerk.cpp,NA,NA
" file, which has the implementation of the functions outlined.",NA,NA
Listing 11.3,NA,NA
 Implementation file for the Kohonen layer and Kohonen network :layerk.cpp,NA,NA
"// layerk.cpp          V.Rao, H.Rao ",NA,NA
// compile for floating point hardware if ,NA,NA
available #include “layer.cpp”,NA,NA
#include “layerk.h”,NA,NA
// ————————————————————-,NA,NA
//                          Kohonen layer ,NA,NA
//—————————————————————,NA,NA
"Kohonen_layer::Kohonen_layer(int i, int o, int",NA,NA
 init_neigh_size) ,NA,NA
{ ,NA,NA
num_inputs=i; ,NA,NA
num_outputs=o; ,NA,NA
neighborhood_size=init_neigh_size; ,NA,NA
weights = new float[num_inputs*num_outputs]; ,NA,NA
outputs = new float[num_outputs]; ,NA,NA
},NA,NA
Kohonen_layer::~Kohonen_layer() ,NA,NA
{ ,NA,NA
delete [num_outputs*num_inputs] weights; ,NA,NA
delete [num_outputs] outputs; ,NA,NA
} ,NA,NA
void Kohonen_layer::calc_out() ,NA,NA
{ ,NA,NA
// implement lateral competition ,NA,NA
// choose the output with the largest // ,NA,NA
value as the winner; neighboring ,NA,NA
// outputs participate in next weight,NA,NA
// update. Winner’s output is 1 ,NA,NA
while // all other outputs are zero,NA,NA
"int i,j,k; ",NA,NA
float accumulator=0.0; ,NA,NA
float maxval; ,NA,NA
winner_index=0; ,NA,NA
maxval=-1000000;,NA,NA
for (j=0; j<num_outputs; j++),NA,NA
 {,NA,NA
 for (i=0; i<num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 if (weights[k+j]*weights[k+j] > 1000000.0),NA,NA
 {,NA,NA
 cout << “weights are blowing up\n”;,NA,NA
 cout << “try a smaller learning constant\n”;,NA,NA
 cout << “e.g. beta=0.02,NA,NA
 aborting...\n”;,NA,NA
 exit(1);,NA,NA
 },NA,NA
 outputs[j]=weights[k+j]*(*(inputs+i));,NA,NA
 accumulator+=outputs[j];,NA,NA
 },NA,NA
 // no squash function,NA,NA
 outputs[j]=accumulator;,NA,NA
 if (outputs[j] > maxval),NA,NA
 {,NA,NA
 maxval=outputs[j];,NA,NA
 winner_index=j;,NA,NA
 },NA,NA
 accumulator=0;,NA,NA
 },NA,NA
 // set winner output to 1,NA,NA
 outputs[winner_index]=1.0;,NA,NA
 // now zero out all other outputs,NA,NA
 for (j=0; j< winner_index; j++),NA,NA
 outputs[j]=0;,NA,NA
 for (j=num_outputs-1; j>winner_index; j—),NA,NA
 outputs[j]=0;,NA,NA
},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (3 of 10) [21/11/02 21:57:54],NA
void ,NA,NA
Kohonen_layer::randomize_weights() { ,NA,NA
"int i, j, k; ",NA,NA
const unsigned first_time=1;,NA,NA
const unsigned not_first_time=0; ,NA,NA
float discard; ,NA,NA
float norm;,NA,NA
discard=randomweight(first_time);,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 weights[k+j]=randomweight(not_first_time);,NA,NA
 },NA,NA
 },NA,NA
// now need to normalize the weight vectors ,NA,NA
// to unit length ,NA,NA
// a weight vector is the set of weights for ,NA,NA
// a given output,NA,NA
for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 norm=0;,NA,NA
 for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 norm+=weights[k+j]*weights[k+j]; ,NA,NA
},NA,NA
 norm = 1/((float)sqrt((double)norm));,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 weights[k+j]*=norm;,NA,NA
 },NA,NA
 },NA,NA
},NA,NA
void Kohonen_layer::update_neigh_size(int new_neigh_size),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (4 of 10) [21/11/02 21:57:54],NA
{ ,NA,NA
neighborhood_size=new_neigh_size; ,NA,NA
},NA,NA
void Kohonen_layer::update_weights(const float alpha) ,NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
"int start_index, stop_index; ",NA,NA
// learning law: weight_change = ,NA,NA
//             alpha*(input-weight) ,NA,NA
//      zero change if input and weight ,NA,NA
// vectors are aligned ,NA,NA
// only update those outputs that ,NA,NA
// are within a neighborhood’s distance ,NA,NA
// from the last winner ,NA,NA
start_index = winner_index -,NA,NA
 neighborhood_size;,NA,NA
if (start_index < 0),NA,NA
 start_index =0;,NA,NA
stop_index = winner_index +,NA,NA
 neighborhood_size;,NA,NA
if (stop_index > num_outputs-1),NA,NA
 stop_index = num_outputs-1;,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=start_index; j<=stop_index; j++),NA,NA
 weights[k+j] +=,NA,NA
 alpha*((*(inputs+i))-weights[k+j]);,NA,NA
 },NA,NA
},NA,NA
void Kohonen_layer::list_weights() ,NA,NA
{ ,NA,NA
"int i, j, k;",NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
" cout << “weight[“<<i<<”,”<<",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (5 of 10) [21/11/02 21:57:54],NA
},NA,NA
 },NA,NA
 j<<”] is: “<<weights[k+j];,NA,NA
void Kohonen_layer::list_outputs() ,NA,NA
{ ,NA,NA
int i; ,NA,NA
for (i=0; i< num_outputs; i++),NA,NA
 {,NA,NA
 cout << “outputs[“<<i<<,NA,NA
 “] is: “<<outputs[i];,NA,NA
 } ,NA,NA
} ,NA,NA
float Kohonen_layer::get_win_dist() ,NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
j=winner_index; ,NA,NA
float accumulator=0; ,NA,NA
float * win_dist_vec = new float [num_inputs]; ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 win_dist_vec[i]=(*(inputs+i))-,NA,NA
weights[k+j];,NA,NA
 ,NA,NA
accumulator+=win_dist_vec[i]*win_dist_vec[i];,NA,NA
 } ,NA,NA
win_distance =(float)sqrt((double)accumulator); ,NA,NA
delete [num_inputs]win_dist_vec; ,NA,NA
return win_distance; ,NA,NA
} ,NA,NA
Kohonen_network::Kohonen_network() ,NA,NA
{ ,NA,NA
} ,NA,NA
Kohonen_network::~Kohonen_network() ,NA,NA
{ ,NA,NA
},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (6 of 10) [21/11/02 21:57:54],NA
void ,NA,NA
Kohonen_network::get_layer_info() { ,NA,NA
int i;,NA,NA
//—————————————————————,NA,NA
// ,NA,NA
//    Get layer sizes for the Kohonen network ,NA,NA
// ,NA,NA
// ————————————————————-,NA,NA
cout << “ Enter in the layer sizes separated by spaces.\n”; ,NA,NA
cout << “ A Kohonen network has an input layer \n”; ,NA,NA
cout << “ followed by a Kohonen (output) layer \n”;,NA,NA
for (i=0; i<2; i++),NA,NA
 {,NA,NA
 cin >> layer_size[i];,NA,NA
 },NA,NA
// ———————————————————————————,NA,NA
// size of layers: ,NA,NA
//             input_layer       layer_size[0] ,NA,NA
//             Kohonen_layer      layer_size[1] ,NA,NA
//———————————————————————————-,NA,NA
} ,NA,NA
void Kohonen_network::set_up_network(int nsz) ,NA,NA
{ ,NA,NA
int i;,NA,NA
// set up neighborhood size ,NA,NA
neighborhood_size = nsz;,NA,NA
//———————————————————————————-,NA,NA
// Construct the layers ,NA,NA
// ,NA,NA
//———————————————————————————-,NA,NA
"layer_ptr[0] = new input_layer(0,layer_size[0]);",NA,NA
layer_ptr[1] =,NA,NA
" new Kohonen_layer(layer_size[0],",NA,NA
" layer_size[1],neighborhood_size);",NA,NA
for (i=0;i<2;i++),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (7 of 10) [21/11/02 21:57:54],NA
 {,NA,NA
 if (layer_ptr[i] == 0),NA,NA
 {,NA,NA
 cout << “insufficient memory\n”;,NA,NA
 cout << “use a smaller architecture\n”;,NA,NA
 exit(1);,NA,NA
 },NA,NA
 },NA,NA
//———————————————————————————-,NA,NA
// Connect the layers ,NA,NA
// ,NA,NA
//———————————————————————————-,NA,NA
// set inputs to previous layer outputs for the Kohonen ,NA,NA
layer layer_ptr[1]->inputs = layer_ptr[0]->outputs;,NA,NA
},NA,NA
void Kohonen_network::randomize_weights() ,NA,NA
{,NA,NA
((Kohonen_layer *)layer_ptr[1]),NA,NA
 ->randomize_weights(); ,NA,NA
},NA,NA
void Kohonen_network::update_neigh_size(int n) ,NA,NA
{ ,NA,NA
((Kohonen_layer *)layer_ptr[1]),NA,NA
 ->update_neigh_size(n); ,NA,NA
},NA,NA
void Kohonen_network::update_weights(const float ,NA,NA
a) { ,NA,NA
((Kohonen_layer *)layer_ptr[1]),NA,NA
 ->update_weights(a); ,NA,NA
},NA,NA
void Kohonen_network::list_weights() ,NA,NA
{ ,NA,NA
((Kohonen_layer *)layer_ptr[1]),NA,NA
 ->list_weights(); ,NA,NA
},NA,NA
void Kohonen_network::list_outputs() ,NA,NA
{ ,NA,NA
((Kohonen_layer *)layer_ptr[1]),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (8 of 10) [21/11/02 21:57:54],NA
},NA,NA
 ->list_outputs();,NA,NA
void Kohonen_network::get_next_vector(FILE * ,NA,NA
ifile) { ,NA,NA
int i; ,NA,NA
float normlength=0; ,NA,NA
int num_inputs=layer_ptr[1]->num_inputs; ,NA,NA
float *in = layer_ptr[1]->inputs; ,NA,NA
// get a vector and normalize it ,NA,NA
for (i=0; i<num_inputs; i++),NA,NA
 {,NA,NA
" fscanf(ifile,”%f”,(in+i));",NA,NA
 normlength += (*(in+i))*(*(in+i));,NA,NA
 } ,NA,NA
"fscanf(ifile,”\n”); ",NA,NA
normlength = 1/(float)sqrt((double)normlength); ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 (*(in+i)) *= normlength;,NA,NA
 },NA,NA
},NA,NA
void Kohonen_network::process_next_pattern() ,NA,NA
{,NA,NA
 layer_ptr[1]->calc_out(); ,NA,NA
},NA,NA
float Kohonen_network::get_win_dist() ,NA,NA
{ ,NA,NA
float  retval; ,NA,NA
retval=((Kohonen_layer ,NA,NA
*)layer_ptr[1]),NA,NA
 ->get_win_dist();,NA,NA
return retval; ,NA,NA
},NA,NA
int Kohonen_network::get_win_index() ,NA,NA
{ ,NA,NA
return ((Kohonen_layer ,NA,NA
*)layer_ptr[1]),NA,NA
 ->winner_index; ,NA,NA
},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (9 of 10) [21/11/02 21:57:54],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch11/279-291.html (10 of 10) [21/11/02 21:57:54]",NA
Flow of the Program and the main() Function,"The 
 main()
  function is contained in a file called 
 kohonen.cpp
 , which is shown in Listing 11.4. To compile 
 this program, you need only compile and make this main file, kohonen.cpp. Other files are included in this.
  
 Listing 11.4
  The main implementation file, kohonen.cpp for the Kohonen Map program
  
 // kohonen.cpp        V. Rao, H. Rao 
  
 // Program to simulate a Kohonen map
  
 #include “layerk.cpp”
  
 #define INPUT_FILE “input.dat”
  
 #define OUTPUT_FILE “kohonen.dat”
  
 #define dist_tol   0.05
  
 void main() 
  
 {
  
 int neighborhood_size, period; 
  
 float avg_dist_per_cycle=0.0; 
  
 float dist_last_cycle=0.0; 
  
 float avg_dist_per_pattern=100.0; // for the latest cycle 
 float dist_last_pattern=0.0; 
  
 float total_dist; 
  
 float alpha; 
  
 unsigned startup; 
  
 int max_cycles; 
  
 int patterns_per_cycle=0;
  
 int total_cycles, total_patterns; 
  
 // create a network object 
  
 Kohonen_network knet;
  
 FILE * input_file_ptr, * output_file_ptr;
  
 // open input file for reading 
  
 if ((input_file_ptr=fopen(INPUT_FILE,”r”))==NULL)
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/291-296.html (1 of 5) [21/11/02 21:57:55]",NA
Previous Table of Contents Next,NA,NA
Flow of the Program,NA,NA
The flow of the program is very similar to the backpropagation simulator. The criterion for ending the ,NA,NA
simulation in the Kohonen program is the ,NA,NA
average winner distance,NA,NA
. This is a Euclidean distance measure ,NA,NA
between the input vector and the winner’s weight vector. This distance is the square root of the sum of ,NA,NA
the squares of the differences between individual vector components between the two vectors.,NA,NA
Results from Running the Kohonen Program,NA,NA
"Once you compile the program, you need to create an input file to try it. We will first use a very simple ",NA,NA
input file and examine the results. ,NA,NA
A Simple First Example,NA,NA
"Let us create an input file, input.dat, which contains only two arbitrary vectors: ",NA,NA
0.4 0.98 0.1 0.2 ,NA,NA
0.5 0.22 0.8 0.9 ,"The file contains two four-dimensional vectors. We expect to see output that contains a different winner 
 neuron for each of these patterns. If this is the case, then the Kohonen map has assigned different 
 categories for each of the input vectors, and, in the future, you can expect to get the same winner 
 classification for vectors that are close to or equal to these vectors.",NA
"By running the Kohonen map program, you will see the following output (user input is italic): ",NA,NA
Please enter initial values for:,NA,NA
"alpha (0.01-1.0),",NA,NA
and the neighborhood size (integer between 0 and 50),NA,NA
"separated by spaces, e.g. 0.3 5",NA,NA
0.3 5 ,NA,NA
"Now enter the period, which is the ",NA,NA
number of cycles after which the values ,NA,NA
for alpha the neighborhood size are decremented ,NA,NA
"choose an integer between 1 and 500 , e.g. 50 ",NA,NA
50 ,NA,NA
Please enter the maximum cycles for the simulation ,NA,NA
A cycle is one pass through the data set.,NA,NA
Try a value of 500 to start with ,NA,NA
500,NA,NA
 Enter in the layer sizes separated by spaces.,NA,NA
 A Kohonen network has an input layer,NA,NA
 followed by a Kohonen (output) layer ,NA,NA
4 10,NA,NA
——————————————————————————,NA,NA
 done,NA,NA
——>average dist per cycle = 0.544275 <——-——,NA,NA
>dist last cycle = 0.0827523 <——-,NA,NA
->dist last cycle per pattern= 0.0413762 <——-,NA,NA
——————>total cycles = 11 <——-,NA,NA
——————>total patterns = 22 <——-,NA,NA
——————————————————————————,NA,NA
The layer sizes are given as 4 for the input layer and 10 for the Kohonen layer. You should choose the ,NA,NA
size of the Kohonen layer to be larger than the number of distinct patterns that you think are in the input ,NA,NA
data set. One of the outputs reported on the screen is the distance for the last cycle per pattern. This value ,NA,NA
"is listed as 0.04, which is less than the terminating value set at the top of the kohonen.cpp file of 0.05. ",NA,NA
"The map converged on a solution. Let us look at the file, kohonen.dat, the output file, to see the mapping ",NA,NA
to winner indexes: ,NA,NA
cycle     pattern     win index     neigh_size    ,NA,NA
avg_dist_per_pattern——————————————————————————————————————————————————,NA,NA
————————————————————0         0           1             5             ,NA,NA
100.000000 ,NA,NA
0         1           3             5             100.000000 ,NA,NA
1         2           1             5             0.304285 ,NA,NA
1         3           3             5             0.304285 ,NA,NA
2         4           1             5             0.568255 ,NA,NA
2         5           3             5             0.568255 ,NA,NA
3         6           1             5             0.542793 ,NA,NA
3         7           8             5             0.542793 ,NA,NA
4         8           1             5             0.502416 ,NA,NA
4         9           8             5             0.502416 ,NA,NA
5         10          1             5             0.351692 ,NA,NA
5         11          8             5             0.351692 ,NA,NA
6         12          1             5             0.246184,NA,NA
6         13          8             5             ,NA,NA
0.246184 7         14          1             5             ,NA,NA
0.172329 7         15          8             5             ,NA,NA
0.172329 8         16          1             5             ,NA,NA
0.120630 8         17          8             5             ,NA,NA
0.120630 9         18          1             5             ,NA,NA
0.084441 9         19          8             5             ,NA,NA
0.084441 10        20          1             5             ,NA,NA
0.059109 10        21          8             5             ,NA,NA
0.059109,NA,NA
"In this example, the neighborhood size stays at its initial value of 5. In the first column you see the cycle ",NA,NA
"number, and in the second the pattern number. Since there are two patterns per cycle, you see the cycle ",NA,NA
number repeated twice for each cycle. ,"The Kohonen map was able to find two distinct winner neurons for each of the patterns. One has winner 
 index 1 and the other index 8.",NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Orthogonal Input Vectors Example,NA,NA
"For a second example, look at Figure 11.5, where we choose input vectors on a two-dimensional unit ",NA,NA
circle that are 90° apart. The input.dat file should look like the following: ,NA,NA
 1  0,NA,NA
 0  1,NA,NA
-1  0,NA,NA
 0 -1,NA,NA
Figure 11.5,NA,NA
  Orthogonal input vectors.,NA,NA
"Using the same parameters for the Kohonen network, but with layer sizes of 2 and 10, what result would ",NA,NA
"you expect? The output file, kohonen.dat, follows: ",NA,NA
cycle     pattern     win index    neigh_size      ,NA,NA
avg_dist_per_pattern———————————————————————————————————————————————————,NA,NA
————————————————————0         0           4            5               ,NA,NA
100.000000 ,NA,NA
0         1           0            5               100.000000 ,NA,NA
0         2           9            5               100.000000 ,NA,NA
0         3           3            5               100.000000 ,NA,NA
1         4           4            5               0.444558 ,NA,NA
1         5           0            5               0.444558,NA,NA
497       1991        6            0               0.707107 ,NA,NA
498       1992        0            0               0.707107 ,NA,NA
498       1993        0            0               0.707107 ,NA,NA
498       1994        6            0               0.707107 ,NA,NA
498       1995        6            0               0.707107 ,NA,NA
499       1996        0            0               0.707107 ,NA,NA
499       1997        0            0               0.707107,NA,NA
499       1998        6            0               0.707107 ,NA,NA
499       1999        6            0               0.707107,NA,NA
You can see that this example doesn’t quite work. Even though the neighborhood size gradually got ,NA,NA
"reduced to zero, the four inputs did not get categorized to different outputs. The winner distance became ",NA,NA
"stuck at the value of 0.707, which is the distance from a vector at 45°. In other words, the map generalizes a ",NA,NA
"little too much, arriving at the middle value for all of the input vectors. ",NA,NA
"You can fix this problem by starting with a smaller neighborhood size, which provides for less ",NA,NA
"generalization. By using the same parameters and a neighborhood size of 2, the following output is ",NA,NA
obtained.,NA,NA
cycle     pattern     win index    neigh_size    ,NA,NA
avg_dist_per_pattern—————————————————————————————————————————————————,NA,NA
————————————————————0         0           5            2             ,NA,NA
100.000000 ,NA,NA
0         1           6            2             100.000000 ,NA,NA
0         2           4            2             100.000000 ,NA,NA
0         3           9            2             100.000000 ,NA,NA
1         4           0            2             0.431695 ,NA,NA
1         5           6            2             0.431695 ,NA,NA
1         6           3            2             0.431695 ,NA,NA
1         7           9            2             0.431695 ,NA,NA
2         8           0            2             0.504728 ,NA,NA
2         9           6            2             0.504728 ,NA,NA
2         10          3            2             0.504728 ,NA,NA
2         11          9            2             0.504728 ,NA,NA
3         12          0            2             0.353309 ,NA,NA
3         13          6            2             0.353309 ,NA,NA
3         14          3            2             0.353309 ,NA,NA
3         15          9            2             0.353309 ,NA,NA
4         16          0            2             0.247317 ,NA,NA
4         17          6            2             0.247317 ,NA,NA
4         18          3            2             0.247317 ,NA,NA
4         19          9            2             0.247317 ,NA,NA
5         20          0            2             0.173122 ,NA,NA
5         21          6            2             0.173122 ,NA,NA
5         22          3            2             0.173122 ,NA,NA
5         23          9            2             0.173122 ,NA,NA
6         24          0            2             0.121185 ,NA,NA
6         25          6            2             0.121185 ,NA,NA
6         26          3            2             0.121185 ,NA,NA
6         27          9            2             0.121185 ,NA,NA
7         28          0            2             0.084830 ,NA,NA
7         29          6            2             0.084830 ,NA,NA
7         30          3            2             0.084830 ,NA,NA
7         31          9            2             0.084830 ,NA,NA
8         32          0            2             0.059381,NA,NA
8         33          6            2             0.059381 ,NA,NA
8         34          3            2             0.059381 ,NA,NA
8         35          9            2             0.059381,NA,NA
"For this case, the network quickly converges on a unique winner for each of the four input patterns, and ",NA,NA
the distance criterion is below the set criterion within eight cycles. You can experiment with other input ,NA,NA
data sets and combinations of Kohonen network parameters. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/299-301.html (3 of 3) [21/11/02 21:57:56]",NA
Previous Table of Contents Next,NA,NA
Variations and Applications of Kohonen Networks,NA,NA
There are many variations of the Kohonen network. Some of these will be briefly ,NA,NA
discussed in this section. ,NA,NA
Using a Conscience,NA,NA
DeSieno has used a ,NA,NA
conscience,NA,NA
" factor in a Kohonen network. For a winning neuron, if the ",NA,NA
neuron is winning more than a fair share of the time (roughly more than 1/,NA,NA
n,NA,NA
", where ",NA,NA
n,NA,NA
 is the ,NA,NA
"number of neurons), then this neuron has a threshold that is applied temporarily to allow ",NA,NA
other neurons the chance to win. The purpose of this modification is to allow more uniform ,NA,NA
weight distribution while learning is taking place.,NA,NA
LVQ: Learning Vector Quantizer,NA,NA
You have read about LVQ (Learning Vector Quantizer) in previous chapters. In light of the ,NA,NA
"Kohonen map, it should be pointed out that the LVQ is simply a supervised version of the ",NA,NA
Kohonen network. Inputs and expected output categories are presented to the network for ,NA,NA
"training. You get data clustered, just as a Kohonen network, according to the similarity to ",NA,NA
other data inputs. ,NA,NA
Counterpropagation Network,NA,NA
"A neural network topology, called a ",NA,NA
counterpropagation,NA,NA
" network, is a combination of a ",NA,NA
Kohonen layer with a Grossberg layer. This network was developed by Robert Hecht-,NA,NA
"Nielsen and is useful for prototyping of systems, with a fairly rapid training time compared ",NA,NA
"to backpropagation. The Kohonen layer provides for categorization, while the Grossberg ",NA,NA
layer allows for Hebbian conditioned learning. Counterpropagation has been used ,NA,NA
successfully in data compression applications for images. Compression ratios of 10:1 to ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch11/301-303.html (1 of 3) [21/11/02 21:57:57],NA
"100:1 have been obtained, using a lossy compression scheme that codes the image with a ",NA,NA
technique called ,NA,NA
vector quantization,NA,NA
", where the image is broken up into representative ",NA,NA
subimage vectors. The statistics of these vectors is such that you find that a large part of the ,NA,NA
image can be adequately represented by a subset of all the vectors. The vectors with the ,NA,NA
"highest frequency of occurrence are coded with the shortest bit strings, hence you achieve ",NA,NA
data compression.,NA,NA
Application to Speech Recognition,NA,NA
Kohonen created a ,NA,NA
phonetic typewriter,NA,NA
 by classifying speech waveforms of different ,NA,NA
phonemes of Finnish speech into different categories using a Kohonen SOM. The ,NA,NA
Kohonen phoneme map used 50 samples of each phoneme for calibration. These samples ,NA,NA
caused excitation in a neighborhood of cells more strongly than in other cells. A ,NA,NA
neighborhood was labeled with the particular phoneme that caused excitation. For an ,NA,NA
"utterance of speech made to the network, the exact neighborhoods that were active during ",NA,NA
"the utterance were noted, and for how long, and in what sequence. Short excitations were ",NA,NA
taken as transitory sounds. The information obtained from the network was then pieced ,NA,NA
together to find out the words in the utterance made to the network.,NA,NA
Summary,NA,NA
"In this chapter, you have learned about one of the important types of competitive learning ",NA,NA
called Kohonen feature map. The most significant points of this discussion are outlined as ,NA,NA
follows: ,NA,NA
•,NA,NA
  The Kohonen feature map is an example of an unsupervised neural network that ,NA,NA
is mainly used as a classifier system or data clustering system. As more inputs are ,NA,NA
"presented to this network, the network improves its learning and is able to adapt to ",NA,NA
changing inputs. ,NA,NA
•,NA,NA
  The training law for the Kohonen network tries to align the weight vectors along ,NA,NA
the same direction as input vectors. ,NA,NA
•,NA,NA
  The Kohonen network models lateral competition as a form of self-organization. ,NA,NA
One winner neuron is derived for each input pattern to categorize that input. ,NA,NA
•,NA,NA
  Only neurons within a certain distance (neighborhood) from the winner are ,NA,NA
allowed to participate in training for a given input pattern. ,NA,NA
Previous Table of Contents Next,NA,NA
Chapter 12 ,NA,NA
Application to Pattern Recognition ,NA,NA
Using the Kohonen Feature Map,"In this chapter, you will use the Kohonen program developed in Chapter 11 to recognize patterns. You will 
 modify the Kohonen program for the display of patterns. 
  
 An Example Problem: Character Recognition
  
 The problem that is presented in this chapter is to recognize or categorize alphabetic characters. You will input 
 various alphabetic characters to a Kohonen map and train the network to recognize these as separate categories. 
 This program can be used to try other experiments that will be discussed at the end of this chapter.",NA
Representing Characters,"Each character is represented by a 5×7 grid of pixels. We use the graphical printing characters of the IBM 
 extended ASCII character set to show a grayscale output for each pixel. To represent the letter A, for example, 
 you could use the pattern shown in Figure 12.1. Here the blackened boxes represent value 1, while empty boxes 
 represent a zero. You can represent all characters this way, with a binary map of 35 pixel values. 
  
  
 Figure 12.1
   Representation of the letter A with a 5×7 pattern.
  
 The letter A is represented by the values: 
  
 0 0 
 1
  0 0 
  
 0 
 1
  0 
 1
  0 
  
 1
  0 0 0 
 1
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch12/305-308.html (1 of 4) [21/11/02 21:57:58]",NA
Monitoring the Weights,"We will present the Kohonen map with many such characters and find the response in output. You will be able 
 to watch the Kohonen map as it goes through its cycles and learns the input patterns. At the same time, you 
 should be able to watch the weight vectors for the winner neurons to see the pattern that is developing in the 
 weights. Remember that for a Kohonen map the weight vectors tend to become aligned with the input vectors. 
 So after a while, you will notice that the weight vector for the input will resemble the input pattern that you are 
 categorizing.",NA
Representing the Weight Vector,"Although on and off values are fine for the input vectors mentioned, you need to see grayscale values for the 
 weight vector. This can be accomplished by quantizing the weight vector into four bins, each represented by a 
 different ASCII graphic character, as shown in Table 12.1. 
  
 Table 12.1
 Quantizing the Weight Vector 
  
  
 <= 0 
  
 White rectangle (space) 
  
 0 < weight <= 0.25 
  
 Light-dotted rectangle 
  
 0.25 < weight <= 0.50 
  
 Medium-dotted rectangle 
  
 0.50 < weight <= 0.75 
  
 Dark-dotted rectangle 
  
 weight > 0.75 
  
 Black rectangle 
  
  
 The ASCII values for the graphics characters to be used are listed in Table 12.2. 
  
 Table 12.2
 ASCII Values for Rectangle Graphic Characters 
  
  
 White rectangle 
  
 255 
  
 Light-dotted rectangle 
  
 176",NA
C++ Code Development,"The changes to the Kohonen program are relatively minor. The following listing indicates these changes. 
  
 Changes to the Kohonen Program
  
 The first change to make is to the Kohonen_network class definition. This is in the file, layerk.h, shown in 
 Listing 12.1. 
  
 Listing 12.1
  Updated layerk.h file
  
 class Kohonen_network
  
 {
  
 private:
  
  layer *layer_ptr[2];
  
  int layer_size[2];
  
  int neighborhood_size;
  
 public:
  
  
  Kohonen_network();
  
  
  ~Kohonen_network();
  
  
  void get_layer_info();
  
  
  void set_up_network(int);
  
  
  void randomize_weights();
  
  
  void update_neigh_size(int);
  
  
  void update_weights(const float);
  
  void list_weights();
  
  
  void list_outputs();
  
  
  void get_next_vector(FILE *);
  
  
  void process_next_pattern();
  
  
  float get_win_dist();
  
  
  int get_win_index(); 
  
  
  
 void display_input_char(); 
  
  
  
 void display_winner_weights();
  
 };",NA
Previous Table of Contents Next,NA,NA
Testing the Program,NA,NA
Let us run the example that we have created an input file for. We have an input.dat file with the ,NA,NA
characters A and X defined. A run of the program with these inputs is shown as follows: ,NA,NA
Please enter initial values for: ,NA,NA
"alpha (0.01-1.0), ",NA,NA
and the neighborhood size (integer between 0 and ,NA,NA
"50) separated by spaces, e.g., 0.3 5 ",NA,NA
0.3 5 ,NA,NA
"Now enter the period, which is the ",NA,NA
number of cycles after which the values ,NA,NA
for alpha the neighborhood size are decremented ,NA,NA
"choose an integer between 1 and 500, e.g., 50 ",NA,NA
50 ,NA,NA
Please enter the maximum cycles for the simulation ,NA,NA
A cycle is one pass through the data set.,NA,NA
Try a value of 500 to start with ,NA,NA
500 ,NA,NA
Enter in the layer sizes separated by spaces.,NA,NA
A Kohonen network has an input ,NA,NA
layer ,NA,NA
followed ,NA,NA
by ,NA,NA
a ,NA,NA
Kohonen ,NA,NA
(output) ,NA,NA
layer ,NA,NA
35 100,NA,NA
The output of the program is contained in file kohonen.dat as usual. This shows the following result. ,NA,NA
cycle   pattern    win index   neigh_size    avg_dist_per_pattern——,NA,NA
—————————————————————————————————————————————————————————————————0       ,NA,NA
0          42          5             100.000000 ,NA,NA
0       1          47          5             100.000000 ,NA,NA
1       2          42          5             0.508321 ,NA,NA
1       3          47          5             0.508321 ,NA,NA
2       4          40          5             0.742254,NA,NA
2       5          47          5             ,NA,NA
0.742254 3       6          40          5             ,NA,NA
0.560121 3       7          47          5             ,NA,NA
0.560121 4       8          40          5             ,NA,NA
0.392084 4       9          47          5             ,NA,NA
0.392084 5       10         40          5             ,NA,NA
0.274459 5       11         47          5             ,NA,NA
0.274459 6       12         40          5             ,NA,NA
0.192121 6       13         47          5             ,NA,NA
0.192121 7       14         40          5             ,NA,NA
0.134485 7       15         47          5             ,NA,NA
0.134485 8       16         40          5             ,NA,NA
0.094139 8       17         47          5             ,NA,NA
0.094139 9       18         40          5             ,NA,NA
0.065898 9       19         47          5             ,NA,NA
0.065898 10      20         40          5             ,NA,NA
0.046128 10      21         47          5             ,NA,NA
0.046128 11      22         40          5             ,NA,NA
0.032290 11      23         47          5             ,NA,NA
0.032290 12      24         40          5             ,NA,NA
0.022603 12      25         47          5             ,NA,NA
0.022603 13      26         40          5             ,NA,NA
0.015822 13      27         47          5             ,NA,NA
0.015822 14      28         40          5             ,NA,NA
0.011075 14      29         47          5             ,NA,NA
0.011075 15      30         40          5             ,NA,NA
0.007753 15      31         47          5             ,NA,NA
0.007753 16      32         40          5             ,NA,NA
0.005427 16      33         47          5             ,NA,NA
0.005427 17      34         40          5             ,NA,NA
0.003799 17      35         47          5             ,NA,NA
0.003799 18      36         40          5             ,NA,NA
0.002659 18      37         47          5             ,NA,NA
0.002659 19      38         40          5             ,NA,NA
0.001861 19      39         47          5             ,NA,NA
0.001861 20      40         40          5             ,NA,NA
0.001303 20      41         47          5             ,NA,NA
0.001303,NA,NA
"The tolerance for the distance was set to be 0.001 for this program, and the program was able to ",NA,NA
converge to this value. Both of the inputs were successfully classified into two different winning ,NA,NA
output neurons. In Figures 12.2 and 12.3 you see two snapshots of the input and weight vectors that ,NA,NA
"you will find with this program. The weight vector resembles the input as you can see, but it is not ",NA,NA
an exact replication. ,NA,NA
Figure 12.2,NA,NA
  Sample screen output of the letter A from the input and weight vectors.,NA,NA
Figure 12.3,NA,NA
  Sample screen output of the letter X from the input and weight vectors.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch12/316-320.html (3 of 3) [21/11/02 21:58:01]",NA
Previous Table of Contents Next,NA,NA
Generalization versus Memorization,NA,NA
"As mentioned in Chapter 11, you actually don’t desire the exact replication of the input pattern ",NA,NA
for the weight vector. This would amount to memorizing of the input patterns with no capacity ,NA,NA
for generalization. ,NA,NA
"For example, a typical use of this alphabet classifier system would be to use it to process noisy ",NA,NA
"data, like handwritten characters. In such a case, you would need a great deal of latitude in ",NA,NA
scoping a class for a letter A.,NA,NA
Adding Characters,NA,NA
The next step of the program is to add characters and see what categories they end up in. There ,NA,NA
"are many alphabetic characters that look alike, such as H and B for example. You can expect the ",NA,NA
Kohonen classifier to group these like characters into the same class. ,NA,NA
"We now modify the input.dat file to add the characters H, B, and I. The new input.dat file is ",NA,NA
shown as follows.,NA,NA
0 0 1 0 0   0 1 0 1 0  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1,NA,NA
 1 0 0 0 1,NA,NA
1 0 0 0 1   0 1 0 1 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 1 0 1 0,NA,NA
 1 0 0 0 1,NA,NA
1 0 0 0 1   1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1,NA,NA
 1 0 0 0 1,NA,NA
1 1 1 1 1   1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1,NA,NA
 1 1 1 1 1,NA,NA
0 0 1 0 0   0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0,NA,NA
 0 0 1 0 0,NA,NA
The output using this input file is shown as follows. ,NA,NA
—————————————————————————-,NA,NA
 done,NA,NA
——>average dist per cycle = 0.732607 <—-,NA,NA
——>dist last cycle = 0.00360096 <—-,NA,NA
->dist last cycle per pattern= 0.000720192 <—-,NA,NA
——————>total cycles = 37 <—-,NA,NA
——————>total patterns = 185 <—-,NA,NA
—————————————————————————-,NA,NA
The file kohonen.dat with the output values is now shown as follows. ,NA,NA
cycle   pattern    win index   neigh_size    ,NA,NA
avg_dist_per_pattern—————————————————————————————————————————————,NA,NA
————————————————————0       0          69          5             ,NA,NA
100.000000 ,NA,NA
0       1          93          5             100.000000 ,NA,NA
0       2          18          5             100.000000 ,NA,NA
0       3          18          5             100.000000 ,NA,NA
0       4          78          5             100.000000 ,NA,NA
1       5          69          5             0.806743 ,NA,NA
1       6          93          5             0.806743 ,NA,NA
1       7          18          5             0.806743 ,NA,NA
1       8          18          5             0.806743 ,NA,NA
1       9          78          5             0.806743 ,NA,NA
2       10         69          5             0.669678 ,NA,NA
2       11         93          5             0.669678 ,NA,NA
2       12         18          5             0.669678 ,NA,NA
2       13         18          5             0.669678 ,NA,NA
2       14         78          5             0.669678 ,NA,NA
3       15         69          5             0.469631 ,NA,NA
3       16         93          5             0.469631 ,NA,NA
3       17         18          5             0.469631 ,NA,NA
3       18         18          5             0.469631 ,NA,NA
3       19         78          5             0.469631 ,NA,NA
4       20         69          5             0.354791 ,NA,NA
4       21         93          5             0.354791 ,NA,NA
4       22         18          5             0.354791 ,NA,NA
4       23         18          5             0.354791 ,NA,NA
4       24         78          5             0.354791 ,NA,NA
5       25         69          5             0.282990 ,NA,NA
5       26         93          5             0.282990 ,NA,NA
5       27         18          5             0.282990 ,NA,NA
...,NA,NA
35      179        78          5             0.001470,NA,NA
36      180        69          5             0.001029 ,NA,NA
36      181        93          5             0.001029 ,NA,NA
36      182        13          5             0.001029 ,NA,NA
36      183        19          5             0.001029 ,NA,NA
36      184        78          5             0.001029,NA,NA
"Again, the network does not find a problem in classifying these vectors. ","Until cycle 21, both the 
 H
  and the 
 B
  were classified as output neuron 18. The ability to distinguish 
 these vectors is largely due to the small tolerance we have assigned as a termination criterion.",NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Other Experiments to Try,NA,NA
"You can try other experiments with the program. For example, you can repeat the input file ",NA,NA
"but with the order of the entries changed. In other words, you can present the same inputs a ",NA,NA
number of times in different order. This actually helps the Kohonen network train faster. ,NA,NA
You can try applying garbled versions of the characters to see if the network distinguishes ,NA,NA
"them. Just as in the backpropagation program, you can save the weights in a weight file to ",NA,NA
"freeze the state of training, and then apply new inputs. You can enter all of the characters ",NA,NA
from ,NA,NA
A,NA,NA
 to ,NA,NA
Z,NA,NA
 and see the classification that results. Do you need to train on all of the ,NA,NA
characters or a subset? You can change the size of the Kohonen layer. How many neurons ,NA,NA
do you need to recognize the complete alphabet?,NA,NA
There is no restriction on using digital inputs of 1 and 0 as we had used. You can apply ,NA,NA
grayscale analog values. The program will display the input pattern according to the ,NA,NA
"quantization levels that were set. This set can be expanded, and you can use a graphics ",NA,NA
"interface to display more levels. You can then try pattern recognition of arbitrary images, ",NA,NA
but remember that processing time will increase rapidly with the number of neurons used. ,NA,NA
"The number of input neurons you choose is dictated by the image resolution, unless you ",NA,NA
filter and/or subsample the image before presenting it to the network. Filtering is the ,NA,NA
process of using a type of averaging function applied to groups of pixels. Subsampling is ,NA,NA
the process of choosing a lower-output image resolution by selecting fewer pixels than a ,NA,NA
"source image. If you start with an image that is 100 × 100 pixels, you can subsample this ",NA,NA
"image 2:1 in each direction to obtain an image that is one-fourth the size, or 50 × 50 pixels. ",NA,NA
Whether you throw away every other pixel to get this output resolution or apply a filter is ,NA,NA
up to you. You could average every two pixels to get one output pixel as an example of a ,NA,NA
very simple filter.,NA,NA
Summary,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch12/322-323.html (1 of 2) [21/11/02 21:58:02],NA
The following list highlights the important Kohonen program features which you learned ,NA,NA
in this chapter. ,NA,NA
•,NA,NA
  This chapter presented a simple character recognition program using a Kohonen ,NA,NA
feature map. ,NA,NA
•,NA,NA
  The input vectors and the weight vectors were displayed to show convergence ,NA,NA
and note similarity between the two vectors. ,NA,NA
•,NA,NA
"  As training progresses, the weight vector for the winner neuron resembles the ",NA,NA
input character map. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Chapter 13 ,NA,NA
Backpropagation II ,NA,NA
Enhancing the Simulator,NA,NA
"In Chapter 7, you developed a backpropagation simulator. In this chapter, you will put it to use with ",NA,NA
examples and also add some new features to the simulator: a term called ,NA,NA
momentum,NA,NA
", and the capability ",NA,NA
of adding noise to the inputs during simulation. There are many variations of the algorithm that try to ,NA,NA
"alleviate two problems with backpropagation. First, like other neural networks, there is a strong ",NA,NA
"possibility that the solution found with backpropagation is not a global error minimum, but a local one. ",NA,NA
"You may need to shake the weights a little by some means to get out of the local minimum, and ",NA,NA
possibly arrive at a lower minimum. The second problem with backpropagation is speed. The ,NA,NA
algorithm is very slow at learning. There are many proposals for speeding up the search process. ,NA,NA
Neural networks are inherently parallel processing architectures and are suited for simulation on ,NA,NA
parallel processing hardware. While there are a few plug-in neural net or digital signal processing ,NA,NA
"boards available in the market, the low-cost simulation platform of choice remains the personal ",NA,NA
computer. Speed enhancements to the training algorithm are therefore very buffernecessary.,NA,NA
Another Example of Using Backpropagation,NA,NA
"Before modifying the simulator to add features, let’s look at the same problem we used the Kohonen ",NA,NA
"map to analyze in Chapter 12. As you recall, we would like to be able to distinguish alphabetic ",NA,NA
"characters by assigning them to different bins. For backpropagation, we would apply the inputs and ",NA,NA
train the network with anticipated responses. Here is the input file that we used for distinguishing five ,NA,NA
"different characters, ",NA,NA
A,NA,NA
", ",NA,NA
X,NA,NA
", ",NA,NA
H,NA,NA
", ",NA,NA
B,NA,NA
", and ",NA,NA
I,NA,NA
:,NA,NA
0 0 1 0 0  0 1 0 1 0  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1 ,NA,NA
1 0 0 0 1  0 1 0 1 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 1 0 1 0,NA,NA
1 0 0 0 1  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1,NA,NA
1 1 1 1 1  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1,NA,NA
0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0,NA,NA
1 0 0 0 1 ,NA,NA
1 0 0 0 1 ,NA,NA
1 0 0 0 1 ,NA,NA
1 1 1 1 1 ,NA,NA
0 0 1 0 0,NA,NA
Each line has a 5x7 dot representation of each character. Now we need to name each of the output ,NA,NA
categories. We can assign a simple 3-bit representation as follows: ,NA,NA
A ,NA,NA
000 ,NA,NA
X ,NA,NA
010 ,NA,NA
H ,NA,NA
100 ,NA,NA
B ,NA,NA
101 ,NA,NA
I ,NA,NA
111 ,NA,NA
Let’s train the network to recognize these characters. The ,NA,NA
training.dat,NA,NA
 file looks like the following.,NA,NA
0 0 1 0 0  0 1 0 1 0  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1 ,NA,NA
1 0 0 0 1  0 1 0 1 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 1 0 1 0 ,NA,NA
1 0 0 0 1  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1 ,NA,NA
1 1 1 1 1  1 0 0 0 1  1 0 0 0 1  1 1 1 1 1  1 0 0 0 1  1 0 0 0 1 ,NA,NA
0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0,NA,NA
1 0 0 0 1  0 0 0 ,NA,NA
1 0 0 0 1  0 1 0 ,NA,NA
1 0 0 0 1  1 0 0 ,NA,NA
1 1 1 1 1  1 0 1 ,NA,NA
0 0 1 0 0  1 1 1,NA,NA
"Now you can start the simulator. Using the parameters (beta = 0.1, tolerance = 0.001, and max_cycles ",NA,NA
"= 1000) and with three layers of size 35 (input), 5 (middle), and 3 (output), you will get a typical result ",NA,NA
like the following. ,NA,NA
-------------------------- -,NA,NA
 done:   results in file output.dat,NA,NA
 training: last vector only,NA,NA
 not training: full cycle,NA,NA
 weights saved in file weights.dat,NA,NA
-->average error per cycle = 0.035713<--,NA,NA
-->error last cycle = 0.008223 <--,NA,NA
->error last cycle per pattern= 0.00164455 <--,NA,NA
------>total cycles = 1000 <--,NA,NA
------>total patterns = 5000 <--,NA,NA
---------------------------,NA,NA
The simulator stopped at the 1000 maximum cycles specified in this case. Your results will be ,NA,NA
different since the weights start at a random point. Note that the tolerance specified was nearly met. ,NA,NA
Let us see how close the output came to what we wanted. Look at the ,NA,NA
output.dat,NA,NA
 file. You can see ,NA,NA
the match for the last pattern as follows:,NA,NA
for input vector: ,NA,NA
0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000 ,NA,NA
output vector is: ,NA,NA
0.999637  0.998721  0.999330 ,NA,NA
expected output vector is: ,NA,NA
1.000000  1.000000  1.000000,NA,NA
-----------,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/325-327.html (3 of 3) [21/11/02 21:58:03]",NA
Previous Table of Contents Next,NA,NA
"To see the outputs of all the patterns, we need to copy the ",NA,NA
training.dat,NA,NA
 file to the ,NA,NA
test.dat,NA,NA
 file and ,NA,NA
rerun the simulator in ,NA,NA
Test,NA,NA
 mode. Remember to delete the expected output field once you copy the ,NA,NA
file.,NA,NA
Running the simulator in ,NA,NA
Test,NA,NA
 mode (0) shows the following result in the output.dat file:,NA,NA
for input vector: ,NA,NA
0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
0.000000  1.000000  0.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  1.000000  0.000000  0.000000  0.000000  1.000000  1.000000 ,NA,NA
1.000000  1.000000  1.000000  1.000000  1.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  1.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
output vector is: ,NA,NA
0.005010  0.002405  0.000141,NA,NA
-----------,NA,NA
for input vector: ,NA,NA
1.000000  0.000000  0.000000  0.000000  1.000000  0.000000  1.000000 ,NA,NA
0.000000  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000 ,NA,NA
1.000000  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
output vector is: ,NA,NA
0.001230  0.997844  0.000663,NA,NA
-----------,NA,NA
for input vector: ,NA,NA
1.000000  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000 ,NA,NA
0.000000  0.000000  1.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  1.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
output vector is: ,NA,NA
0.995348  0.000253  0.002677,NA,NA
-----------,NA,NA
for input vector:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/327-330.html (1 of 4) [21/11/02 21:58:04],NA
1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  0.000000 ,NA,NA
0.000000  0.000000  1.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000 ,NA,NA
output vector is: ,NA,NA
0.999966  0.000982  0.997594,NA,NA
-----------,NA,NA
for input vector: ,NA,NA
0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000 ,NA,NA
output vector is: ,NA,NA
0.999637  0.998721  0.999330,NA,NA
-----------,NA,NA
"The training patterns are learned very well. If a smaller tolerance is used, it would be possible to ",NA,NA
complete the learning in fewer cycles. What happens if we present a foreign character to the network? ,NA,NA
Let us create a new test.dat file with two entries for the letters ,NA,NA
M,NA,NA
 and ,NA,NA
J,NA,NA
", as follows:",NA,NA
1 0 0 0 1  1 1 0 1 1  1 0 1 0 1  1 0 0 0 1  1 0 0 0 1  1 0 0 0 1 ,NA,NA
0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0  0 0 1 0 0,NA,NA
1 0 0 0 1 ,NA,NA
0 1 1 1 1,NA,NA
The results should show each foreign character in the category closest to it. The middle layer of the ,NA,NA
"network acts as a feature detector. Since we specified five neurons, we have given the network the ",NA,NA
freedom to define five features in the input training set to use to categorize inputs. The results in the ,NA,NA
output.dat file are shown as follows. ,NA,NA
for input vector: ,NA,NA
1.000000  0.000000  0.000000  0.000000  1.000000  1.000000  1.000000 ,NA,NA
0.000000  1.000000  1.000000  1.000000  0.000000  1.000000  0.000000 ,NA,NA
1.000000  1.000000  0.000000  0.000000  0.000000  1.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000 ,NA,NA
0.000000  1.000000  1.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
output vector is: ,NA,NA
0.963513  0.000800  0.001231,NA,NA
-----------,NA,NA
for input vector: ,NA,NA
0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000 ,NA,NA
1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/327-330.html (2 of 4) [21/11/02 21:58:04],NA
0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000,NA,NA
0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000 ,NA,NA
0.000000  0.000000  0.000000  1.000000  1.000000  1.000000  1.000000,NA,NA
output vector is:,NA,NA
0.999469  0.996339  0.999157,NA,NA
-----------,NA,NA
"In the first pattern, an ",NA,NA
M,NA,NA
 is categorized as an ,NA,NA
H,NA,NA
", whereas in the second pattern, a ",NA,NA
J,NA,NA
 is categorized as an ,NA,NA
I,NA,NA
", as expected. The case of the first pattern seems reasonable since the ",NA,NA
H,NA,NA
 and ,NA,NA
M,NA,NA
 share many pixels in ,NA,NA
common.,NA,NA
Other Experiments to Try,NA,NA
There are many other experiments you could try in order to get a better feel for how to train and use a ,NA,NA
backpropagation neural network. ,NA,NA
•,NA,NA
"  You could use the ASCII 8-bit code to represent each character, and try to train the network. ",NA,NA
You could also code all of the alphabetic characters and see if it’s possible to distinguish all of ,NA,NA
them. ,NA,NA
•,NA,NA
"  You can garble a character, to see if you still get the correct output. ",NA,NA
•,NA,NA
"  You could try changing the size of the middle layer, and see the effect on training time and ",NA,NA
generalization ability. ,NA,NA
•,NA,NA
  You could change the tolerance setting to see the difference between an overtrained and ,NA,NA
"undertrained network in generalization capability. That is, given a foreign pattern, is the ",NA,NA
"network able to find the closest match and use that particular category, or does it arrive at a ",NA,NA
new category altogether? ,NA,NA
We will return to the same example after enhancing the simulator with momentum and noise addition ,NA,NA
capability. ,NA,NA
Adding the Momentum Term,NA,NA
A simple change to the training law that sometimes results in much faster training is the addition of a ,NA,NA
momentum,NA,NA
 term. The training law for backpropagation as implemented in the simulator is:,NA,NA
Weight change = Beta * output_error * input,NA,NA
Now we add a term to the weight change equation as follows: ,NA,NA
Weight change = Beta * output_error * input +,NA,NA
 Alpha*previous_weight_change,NA,NA
"The second term in this equation is the momentum term. The weight change, in the absence of error, ",NA,NA
"would be a constant multiple by the previous weight change. In other words, the weight change ",NA,NA
continues in the direction it was heading. The momentum term is an attempt to try to keep the weight ,NA,NA
"change process moving, and thereby not get stuck in local minimas. ",NA,NA
Code Changes,NA,NA
"The effected files to implement this change are the layer.cpp file, to modify the ",NA,NA
update_weights() ,NA,NA
member function of the ,NA,NA
output_layer,NA,NA
" class, and the main backprop.cpp file to read in the value for ",NA,NA
alpha,NA,NA
 and pass it to the ,NA,NA
member,NA,NA
 function. There is some additional storage needed for storing ,NA,NA
"previous weight changes, and this affects the layer.h file. The momentum term could be implemented ",NA,NA
in two ways:,NA,NA
1.,NA,NA
  Using the weight change for the previous pattern. ,NA,NA
2.,NA,NA
  Using the weight change accumulated over the previous cycle. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
"Although both of these implementations are valid, the second is particularly useful, since it ",NA,NA
"adds a term that is significant for all patterns, and hence would contribute to global error ",NA,NA
reduction. We implement the second choice by accumulating the value of the current cycle ,NA,NA
weight changes in a vector called ,NA,NA
cum_deltas,NA,NA
. The past cycle weight changes are stored in a ,NA,NA
vector called ,NA,NA
past_deltas,NA,NA
. These are shown as follows in a portion of the layer.h file.,NA,NA
class output_layer:   public layer ,NA,NA
{ ,NA,NA
protected:,NA,NA
 float * weights;,NA,NA
 float * output_errors; // array of errors at output ,NA,NA
float * back_errors; // array of errors back-propagated ,NA,NA
float * expected_values;      // to inputs,NA,NA
 float * cum_deltas;   // for momentum,NA,NA
 float * past_deltas;  // for momentum,NA,NA
 friend network; ,NA,NA
...,NA,NA
Changes to the layer.cpp File,NA,NA
The implementation file for the ,NA,NA
layer,NA,NA
 class changes in the ,NA,NA
output_layer::update_weights() ,NA,NA
routine and the constructor and destructor for ,NA,NA
output_layer,NA,NA
". First, here is the constructor for ",NA,NA
output_layer,NA,NA
. Changes are highlighted in italic.,NA,NA
"output_layer::output_layer(int ins, int outs) ",NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
num_inputs=ins;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (1 of 6) [21/11/02 21:58:05],NA
num_outputs=outs; ,NA,NA
weights = new float[num_inputs*num_outputs]; ,NA,NA
output_errors = new float[num_outputs]; ,NA,NA
back_errors = new float[num_inputs]; ,NA,NA
outputs = new float[num_outputs]; ,NA,NA
expected_values = new float[num_outputs]; ,NA,NA
cum_deltas = new float[num_inputs*num_outputs]; ,NA,NA
past_deltas = new float[num_inputs*num_outputs]; ,NA,NA
if ((weights==0)||(output_errors==0)||(back_errors==0),NA,NA
 ||(outputs==0)||(expected_values==0),NA,NA
 ||(past_deltas==0)||(cum_deltas==0)),NA,NA
 {,NA,NA
" cout << ""not enough memory\n"";",NA,NA
" cout << ""choose a smaller architecture\n"";",NA,NA
 ,NA,NA
exit(1);,NA,NA
 } ,NA,NA
// zero cum_deltas and past_deltas matrix ,NA,NA
for (i=0; i< num_inputs; i,NA,NA
++,NA,NA
),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j,NA,NA
++,NA,NA
),NA,NA
 {,NA,NA
 cum_deltas[k,NA,NA
+,NA,NA
j]=0;,NA,NA
 past_deltas[k,NA,NA
+,NA,NA
j=0;,NA,NA
 },NA,NA
 } ,NA,NA
},NA,NA
The destructor simply deletes the new vectors: ,NA,NA
output_layer::~output_layer() ,NA,NA
{ ,NA,NA
// some compilers may require the array ,NA,NA
// size in the delete statement; those ,NA,NA
// conforming to Ansi C++ will not ,NA,NA
delete [num_outputs*num_inputs] weights; ,NA,NA
delete [num_outputs] output_errors; ,NA,NA
delete [num_inputs] back_errors; ,NA,NA
delete [num_outputs] outputs; ,NA,NA
delete [num_outputs*num_inputs] past_deltas; ,NA,NA
delete [num_outputs*num_inputs] cum_deltas;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (2 of 6) [21/11/02 21:58:05],NA
},NA,NA
Now let’s look at the ,NA,NA
update_weights(),NA,NA
 routine changes:,NA,NA
"void output_layer::update_weights(const float beta, ",NA,NA
const float alpha) ,NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
float delta; ,NA,NA
// learning law: weight_change = ,NA,NA
//             beta*output_error*input + alpha*past_delta ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 { ,NA,NA
delta=beta*output_errors[j]*(*(inputs,NA,NA
+,NA,NA
i)) ,NA,NA
+,NA,NA
alpha*past_deltas[k,NA,NA
+,NA,NA
j];,NA,NA
 weights[k,NA,NA
+,NA,NA
j] ,NA,NA
+,NA,NA
= delta;,NA,NA
 cum_deltas[k,NA,NA
+,NA,NA
j],NA,NA
+,NA,NA
=delta; // current cycle,NA,NA
 },NA,NA
 },NA,NA
},NA,NA
The change to the training law amounts to calculating a delta and adding it to the cumulative ,NA,NA
total of weight changes in ,NA,NA
cum_deltas,NA,NA
. At some point (at the start of a new cycle) you need to ,NA,NA
set the ,NA,NA
past_deltas,NA,NA
 vector to the ,NA,NA
cum_delta,NA,NA
 vector. Where does this occur? Since the layer ,NA,NA
"has no concept of cycle, this must be done at the network level. There is a network level ",NA,NA
function called ,NA,NA
update_momentum,NA,NA
 at the beginning of each cycle that in turns calls a layer ,NA,NA
level function of the same name. The layer level function swaps the ,NA,NA
past_deltas,NA,NA
 vector and ,NA,NA
the ,NA,NA
cum_deltas,NA,NA
" vector, and reinitializes the ",NA,NA
cum_deltas,NA,NA
 vector to zero. We need to return to ,NA,NA
the layer.h file to see changes that are needed to define the two functions ,NA,NA
mentioned.,NA,NA
class output_layer:   public layer ,NA,NA
{ ,NA,NA
protected:,NA,NA
 float * weights;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (3 of 6) [21/11/02 21:58:05],NA
 float * output_errors; // array of errors at output ,NA,NA
float * back_errors; // array of errors back-propagated ,NA,NA
float * expected_values;     // to inputs,NA,NA
 float * cum_deltas;   // for momentum,NA,NA
 float * past_deltas;   // for momentum,NA,NA
 friend network; ,NA,NA
public:,NA,NA
};,NA,NA
" output_layer(int, int);",NA,NA
 ~output_layer();,NA,NA
 virtual void calc_out();,NA,NA
 void calc_error(float &);,NA,NA
 void randomize_weights();,NA,NA
" void update_weights(const float, const float); ",NA,NA
void update_momentum();,NA,NA
 void list_weights();,NA,NA
" void write_weights(int, FILE *);",NA,NA
" void read_weights(int, FILE *);",NA,NA
 void list_errors();,NA,NA
 void list_outputs();,NA,NA
class network,NA,NA
{,NA,NA
private:,NA,NA
layer *layer_ptr[MAX_LAYERS];,NA,NA
 int number_of_layers;,NA,NA
 int layer_size[MAX_LAYERS];,NA,NA
 float *buffer;,NA,NA
 fpos_t position;,NA,NA
 unsigned training;,NA,NA
public:,NA,NA
 network();,NA,NA
 ~network();,NA,NA
 void set_training(const unsigned &);,NA,NA
 ,NA,NA
unsigned get_training_value();,NA,NA
 void get_layer_info();,NA,NA
 void set_up_network();,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (4 of 6) [21/11/02 21:58:05],NA
 void randomize_weights();,NA,NA
" void update_weights(const float, const float); ",NA,NA
void update_momentum();,NA,NA
 ...,NA,NA
At both the ,NA,NA
network,NA,NA
 and ,NA,NA
output_layer,NA,NA
 class levels the function prototype for the ,NA,NA
update_momentum,NA,NA
 member functions are highlighted. The implementation for these ,NA,NA
functions are shown as follows from the ,NA,NA
layer.cpp,NA,NA
 class.,NA,NA
void output_layer::update_momentum() { ,NA,NA
// This function is called when a ,NA,NA
// new cycle begins; the past_deltas // ,NA,NA
pointer is swapped with the ,NA,NA
// cum_deltas pointer. Then the contents ,NA,NA
// pointed to by the cum_deltas pointer ,NA,NA
// is zeroed out.,NA,NA
"int i, j, k; ",NA,NA
float * temp;,NA,NA
// swap ,NA,NA
temp = past_deltas; ,NA,NA
past_deltas=cum_deltas; ,NA,NA
cum_deltas=temp;,NA,NA
// zero cum_deltas matrix ,NA,NA
// for new cycle ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 cum_deltas[k+j]=0;,NA,NA
 } ,NA,NA
},NA,NA
void network::update_momentum() ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ,NA,NA
((output_layer *)layer_ptr[i]),NA,NA
 -,NA,NA
>update_momentum();,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (5 of 6) [21/11/02 21:58:05],NA
},NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/330-336.html (6 of 6) [21/11/02 21:58:05]",NA
Adding Noise During Training,"Another approach to breaking out of local minima as well as to enhance generalization ability is to introduce 
 some noise in the inputs during training. A random number is added to each input component of the input 
 vector as it is applied to the network. This is scaled by an overall noise factor, 
 NF
 , which has a 0 to 1 range. 
 You can add as much noise to the simulation as you want, or not any at all, by choosing 
 NF
  = 0. 
  
 When you are close to a solution and have reached a satisfactory minimum, you don’t want noise at that 
 time to interfere with convergence to the minimum. We implement a noise factor that decreases with the 
 number of cycles, as shown in the following excerpt from the backprop.cpp file.
  
 // update NF 
  
 // gradually reduce noise to zero 
  
 if (total_cycles>0.7*max_cycles)
  
  
  new_NF = 0; 
  
 else if (total_cycles>0.5*max_cycles)
  
  
  new_NF = 0.25*NF; 
  
 else if (total_cycles>0.3*max_cycles)
  
  
  new_NF = 0.50*NF; 
  
 else if (total_cycles>0.1*max_cycles)
  
  
  new_NF = 0.75*NF;
  
 backp.set_NF(new_NF);
  
 The noise factor is reduced at regular intervals. The new noise factor is updated with the network class 
 function called 
 set_NF(float).
  There is a member variable in the network class called 
 NF
  that holds the 
 current value for the noise factor. The noise is added to the inputs in the 
 input_layer
  member function 
 calc_out()
 .
  
 Another reason for using noise is to prevent memorization by the network. You are effectively presenting a 
 different input pattern with each cycle so it becomes hard for the network to memorize patterns.",NA
One Other Change—Starting Training from a Saved Weight File,"Shortly, we will look at the complete listings for the backpropagation simulator. There is one other 
 enhancement to discuss. It is often useful in long simulations to be able to start from a known point, which is 
 from an already saved set of weights. This is a simple change in the backprop.cpp program, which is well",NA
Previous Table of Contents Next,NA,NA
Listing 13.2,NA,NA
 Layer.cpp file updated to include noise and momentum,NA,NA
"// layer.cpp          V.Rao, H.Rao ",NA,NA
// added momentum and noise,NA,NA
// compile for floating point hardware if available ,NA,NA
#include <stdio.h> ,NA,NA
#include <iostream.h> ,NA,NA
#include <stdlib.h> ,NA,NA
#include <math.h> ,NA,NA
#include <time.h> ,NA,NA
"#include ""layer.h""",NA,NA
inline float squash(float input) ,NA,NA
// squashing function ,NA,NA
// use sigmoid — can customize to something ,NA,NA
// else if desired; can add a bias term too ,NA,NA
// ,NA,NA
{ ,NA,NA
if (input < -50),NA,NA
 return 0.0; ,NA,NA
else   if (input > 50),NA,NA
 return 1.0;,NA,NA
 else return (float)(1/(1+exp(-(double)input)));,NA,NA
},NA,NA
inline float randomweight(unsigned init) ,NA,NA
{ ,NA,NA
int num; ,NA,NA
// random number generator ,NA,NA
// will return a floating point ,NA,NA
// value between -1 and 1,NA,NA
if (init==1)  // seed the generator,NA,NA
 srand ((unsigned)time(NULL));,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (1 of 17) [21/11/02 21:58:07],NA
num=rand() % 100;,NA,NA
return 2*(float(num/100.00))-1; ,NA,NA
},NA,NA
// the next function is needed for Turbo C++ // ,NA,NA
and Borland C++ to link in the appropriate // ,NA,NA
functions for fscanf floating point formats: ,NA,NA
static void force_fpf() ,NA,NA
{,NA,NA
" float x, *y;",NA,NA
 y=&x;,NA,NA
 x=*y; ,NA,NA
},NA,NA
// ---------------------,NA,NA
//                            input layer ,NA,NA
//---------------------,NA,NA
"input_layer::input_layer(int i, int o) {",NA,NA
num_inputs=i; ,NA,NA
num_outputs=o;,NA,NA
outputs = new float[num_outputs]; ,NA,NA
orig_outputs = new float[num_outputs]; ,NA,NA
if ((outputs==0)||(orig_outputs==0)),NA,NA
 {,NA,NA
" cout << ""not enough memory\n"";",NA,NA
" cout << ""choose a smaller architecture\n""; ",NA,NA
exit(1);,NA,NA
 },NA,NA
noise_factor=0;,NA,NA
},NA,NA
input_layer::~input_layer() ,NA,NA
{ ,NA,NA
delete [num_outputs] outputs; ,NA,NA
delete [num_outputs] orig_outputs; ,NA,NA
},NA,NA
void input_layer::calc_out() ,NA,NA
{ ,NA,NA
//add noise to inputs ,NA,NA
// randomweight returns a random number ,NA,NA
// between -1 and 1,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (2 of 17) [21/11/02 21:58:07],NA
int i; ,NA,NA
for (i=0; i<num_outputs; i++),NA,NA
 outputs[i] =orig_outputs[i]*,NA,NA
 (1+noise_factor*randomweight(0));,NA,NA
},NA,NA
void input_layer::set_NF(float noise_fact) ,NA,NA
{ ,NA,NA
noise_factor=noise_fact; ,NA,NA
},NA,NA
// ---------------------,NA,NA
//                            output layer ,NA,NA
//---------------------,NA,NA
"output_layer::output_layer(int ins, int outs) ",NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
num_inputs=ins; ,NA,NA
num_outputs=outs; ,NA,NA
weights = new float[num_inputs*num_outputs]; ,NA,NA
output_errors = new float[num_outputs]; ,NA,NA
back_errors = new float[num_inputs]; ,NA,NA
outputs = new float[num_outputs]; ,NA,NA
expected_values = new float[num_outputs]; ,NA,NA
cum_deltas = new float[num_inputs*num_outputs]; ,NA,NA
past_deltas = new float[num_inputs*num_outputs]; ,NA,NA
if ((weights==0)||(output_errors==0)||(back_errors==0),NA,NA
 ||(outputs==0)||(expected_values==0),NA,NA
 ||(past_deltas==0)||(cum_deltas==0)),NA,NA
 {,NA,NA
" cout << ""not enough memory\n"";",NA,NA
" cout << ""choose a smaller architecture\n"";",NA,NA
 ,NA,NA
exit(1);,NA,NA
 },NA,NA
// zero cum_deltas and past_deltas matrix ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 cum_deltas[k+j]=0;,NA,NA
 past_deltas[k+j]=0;,NA,NA
 },NA,NA
 } ,NA,NA
},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (3 of 17) [21/11/02 21:58:07],NA
output_layer::~output_layer() ,NA,NA
{ ,NA,NA
// some compilers may require the array ,NA,NA
// size in the delete statement; those ,NA,NA
// conforming to Ansi C++ will not ,NA,NA
delete [num_outputs*num_inputs] weights; ,NA,NA
delete [num_outputs] output_errors; ,NA,NA
delete [num_inputs] back_errors; ,NA,NA
delete [num_outputs] outputs; ,NA,NA
delete [num_outputs*num_inputs] past_deltas; ,NA,NA
delete [num_outputs*num_inputs] cum_deltas;,NA,NA
},NA,NA
void output_layer::calc_out() ,NA,NA
{ ,NA,NA
"int i,j,k; ",NA,NA
float accumulator=0.0;,NA,NA
for (j=0; j<num_outputs; j++),NA,NA
 {,NA,NA
 for (i=0; i<num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 if (weights[k+j]*weights[k+j] > 1000000.0),NA,NA
 {,NA,NA
" cout << ""weights are blowing up\n"";",NA,NA
" cout << ""try a smaller learning constant\n"";",NA,NA
" cout << ""e.g. beta=0.02    aborting...\n"";",NA,NA
 exit(1);,NA,NA
 },NA,NA
 outputs[j]=weights[k+j]*(*(inputs+i));,NA,NA
 accumulator+=outputs[j];,NA,NA
 },NA,NA
 // use the sigmoid squash function,NA,NA
 outputs[j]=squash(accumulator);,NA,NA
 accumulator=0;,NA,NA
 },NA,NA
},NA,NA
void output_layer::calc_error(float & error) ,NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
float accumulator=0;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (4 of 17) [21/11/02 21:58:07],NA
float total_error=0; ,NA,NA
for (j=0; j<num_outputs; j++),NA,NA
 {,NA,NA
 output_errors[j] = expected_values[j]-outputs[j];,NA,NA
 total_error+=output_errors[j];,NA,NA
 },NA,NA
error=total_error;,NA,NA
for (i=0; i<num_inputs; i++) ,NA,NA
{ ,NA,NA
k=i*num_outputs; ,NA,NA
for (j=0; j<num_outputs; j++),NA,NA
 {,NA,NA
 back_errors[i]=,NA,NA
 weights[k+j]*output_errors[j];,NA,NA
 accumulator+=back_errors[i];,NA,NA
 },NA,NA
 back_errors[i]=accumulator;,NA,NA
 accumulator=0;,NA,NA
 // now multiply by derivative of,NA,NA
" // sigmoid squashing function, which is",NA,NA
 // just the input*(1-input),NA,NA
 back_errors[i]*=(*(inputs+i))*(1-(*(inputs+i)));,NA,NA
 },NA,NA
},NA,NA
void output_layer::randomize_weights() ,NA,NA
{ ,NA,NA
"int i, j, k; ",NA,NA
const unsigned first_time=1;,NA,NA
const unsigned not_first_time=0; ,NA,NA
float discard;,NA,NA
discard=randomweight(first_time);,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 weights[k+j]=randomweight(not_first_time);,NA,NA
 } ,NA,NA
},NA,NA
"void output_layer::update_weights(const float beta,",NA,NA
 const float alpha),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (5 of 17) [21/11/02 21:58:07],NA
{ ,NA,NA
"int i, j, k; ",NA,NA
float delta;,NA,NA
// learning law: weight_change = ,NA,NA
//             beta*output_error*input + alpha*past_delta,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 delta=beta*output_errors[j]*(*(inputs+i)),NA,NA
 +alpha*past_deltas[k+j];,NA,NA
 weights[k+j] += delta;,NA,NA
 cum_deltas[k+j]+=delta; // current cycle,NA,NA
 },NA,NA
 },NA,NA
},NA,NA
void output_layer::update_momentum() { ,NA,NA
// This function is called when a ,NA,NA
// new cycle begins; the past_deltas // ,NA,NA
pointer is swapped with the ,NA,NA
// cum_deltas pointer. Then the contents ,NA,NA
// pointed to by the cum_deltas pointer ,NA,NA
// is zeroed out.,NA,NA
"int i, j, k; ",NA,NA
float * temp;,NA,NA
// swap ,NA,NA
temp = past_deltas; ,NA,NA
past_deltas=cum_deltas; ,NA,NA
cum_deltas=temp;,NA,NA
// zero cum_deltas matrix ,NA,NA
// for new cycle ,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 cum_deltas[k+j]=0;,NA,NA
 } ,NA,NA
},file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (6 of 17) [21/11/02 21:58:07],NA
void output_layer::list_weights() ,NA,NA
{ ,NA,NA
"int i, j, k;",NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
" cout << ""weight[""<<i<<"",""<<",NA,NA
" j<<""] is: ""<<weights[k+j];",NA,NA
 },NA,NA
},NA,NA
void output_layer::list_errors() ,NA,NA
{ ,NA,NA
"int i, j;",NA,NA
for (i=0; i< num_inputs; i++),NA,NA
" cout << ""backerror[""<<i<<",NA,NA
" ""] is : ""<<back_errors[i]<<""\n""; ",NA,NA
for (j=0; j< num_outputs; j++),NA,NA
" cout << ""outputerrors[""<<j<<",NA,NA
" ""] is: ""<<output_errors[j]<<""\n"";",NA,NA
},NA,NA
"void output_layer::write_weights(int layer_no,",NA,NA
 FILE * weights_file_ptr) ,NA,NA
{ ,NA,NA
"int i, j, k;",NA,NA
// assume file is already open and ready for ,NA,NA
// writing,NA,NA
// prepend the layer_no to all lines of data ,NA,NA
// format: ,NA,NA
"//             layer_no   weight[0,0] weight[0,1] ... ",NA,NA
"//             layer_no   weight[1,0] weight[1,1] ... ",NA,NA
//             ...,NA,NA
for (i=0; i< num_inputs; i++),NA,NA
 {,NA,NA
" fprintf(weights_file_ptr,""%i "",layer_no);",NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
" fprintf(weights_file_ptr,""%f "",",NA,NA
 weights[k+j]);,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (7 of 17) [21/11/02 21:58:07],NA
 },NA,NA
" fprintf(weights_file_ptr,""\n"");",NA,NA
 },NA,NA
},NA,NA
"void output_layer::read_weights(int layer_no,",NA,NA
 FILE * weights_file_ptr) ,NA,NA
{ ,NA,NA
"int i, j, k;",NA,NA
// assume file is already open and ready for ,NA,NA
// reading,NA,NA
// look for the prepended layer_no ,NA,NA
// format: ,NA,NA
"//             layer_no       weight[0,0] weight[0,1] ... ",NA,NA
"//             layer_no       weight[1,0] weight[1,1] ... ",NA,NA
//             ...,NA,NA
while (1),NA,NA
 {,NA,NA
" fscanf(weights_file_ptr,""%i"";,&j);",NA,NA
 if ((j==layer_no)|| (feof(weights_file_ptr))),NA,NA
 break;,NA,NA
 else,NA,NA
 {,NA,NA
 while (fgetc(weights_file_ptr) != `\n'),NA,NA
 {;}// get rest of line,NA,NA
 },NA,NA
 },NA,NA
if (!(feof(weights_file_ptr))),NA,NA
 {,NA,NA
 // continue getting first line i=0;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 },NA,NA
" fscanf(weights_file_ptr,""%f"",",NA,NA
 &weights[j]); // i*num_outputs,NA,NA
 = 0,NA,NA
" fscanf(weights_file_ptr,""\n"");",NA,NA
 // now get the other lines,NA,NA
 for (i=1; i< num_inputs; i++),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (8 of 17) [21/11/02 21:58:07],NA
 {,NA,NA
" fscanf(weights_file_ptr,",NA,NA
" ”%i”,&layer_no);",NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
" fscanf(weights_file_ptr,”%f”,",NA,NA
 &weights[k+j]);,NA,NA
 },NA,NA
 },NA,NA
" fscanf(weights_file_ptr,”\n”);",NA,NA
 },NA,NA
else cout << “end of file reached\n”;,NA,NA
},NA,NA
void output_layer::list_outputs() ,NA,NA
{ ,NA,NA
int j;,NA,NA
for (j=0; j< num_outputs; j++),NA,NA
 {,NA,NA
 cout << “outputs[“<<j,NA,NA
 <<”] is: “<<outputs[j]<<”\n”; ,NA,NA
},NA,NA
},NA,NA
// ————————————————————-,NA,NA
//                           middle layer ,NA,NA
//—————————————————————,NA,NA
"middle_layer::middle_layer(int i, int o): ",NA,NA
"output_layer(i,o) ",NA,NA
{ ,NA,NA
},NA,NA
middle_layer::~middle_layer() ,NA,NA
{ ,NA,NA
delete [num_outputs*num_inputs] weights; ,NA,NA
delete [num_outputs] output_errors; ,NA,NA
delete [num_inputs] back_errors; ,NA,NA
delete [num_outputs] outputs; ,NA,NA
},NA,NA
void middle_layer::calc_error() ,NA,NA
{,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (9 of 17) [21/11/02 21:58:07],NA
"int i, j, k; ",NA,NA
float accumulator=0;,NA,NA
for (i=0; i<num_inputs; i++),NA,NA
 {,NA,NA
 k=i*num_outputs;,NA,NA
 for (j=0; j<num_outputs; j++),NA,NA
 {,NA,NA
 back_errors[i]=,NA,NA
 weights[k+j]*(*(output_errors+j));,NA,NA
 accumulator+=back_errors[i];,NA,NA
 },NA,NA
 back_errors[i]=accumulator;,NA,NA
 accumulator=0;,NA,NA
 // now multiply by derivative of,NA,NA
" // sigmoid squashing function, which is",NA,NA
 // just the input*(1-input),NA,NA
 back_errors[i]*=(*(inputs+i))*(1-(*(inputs+i))); },NA,NA
},NA,NA
network::network() ,NA,NA
{ ,NA,NA
position=0L; ,NA,NA
},NA,NA
network::~network() ,NA,NA
{ ,NA,NA
"int i,j,k; ",NA,NA
i=layer_ptr[0]->num_outputs;// inputs ,NA,NA
j=layer_ptr[number_of_layers-1]->num_outputs; //outputs ,NA,NA
k=MAX_VECTORS;,NA,NA
delete [(i+j)*k]buffer; ,NA,NA
},NA,NA
void network::set_training(const unsigned & value) ,NA,NA
{ ,NA,NA
training=value; ,NA,NA
},NA,NA
unsigned network::get_training_value() ,NA,NA
{ ,NA,NA
return training; ,NA,NA
},NA,NA
void network::get_layer_info(),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (10 of 17) [21/11/02 21:58:07],NA
{ ,NA,NA
int i;,NA,NA
//—————————————————————,NA,NA
// ,NA,NA
//      Get layer sizes for the network ,NA,NA
// ,NA,NA
// ————————————————————-,NA,NA
cout << “ Please enter in the number of layers for your net work.\n”; ,NA,NA
cout << “ You can have a minimum of 3 to a maximum of 5. \n”; ,NA,NA
cout << “ 3 implies 1 hidden layer; 5 implies 3 hidden layers : \n\n”;,NA,NA
cin >> number_of_layers;,NA,NA
cout << “ Enter in the layer sizes separated by spaces.\n”; ,NA,NA
"cout << “ For a network with 3 neurons in the input layer,\n”; ",NA,NA
"cout << “ 2 neurons in a hidden layer, and 4 neurons in the\n”; cout ",NA,NA
"<< “ output layer, you would enter: 3 2 4 .\n”; ",NA,NA
"cout << “ You can have up to 3 hidden layers,for five maximum entries ",NA,NA
:\n\n”;,NA,NA
for (i=0; i<number_of_layers; i++),NA,NA
 {,NA,NA
 cin >> layer_size[i];,NA,NA
 },NA,NA
// ———————————————————————————,NA,NA
// size of layers: ,NA,NA
//    input_layer            layer_size[0] ,NA,NA
//    output_layer           layer_size[number_of_layers-1] ,NA,NA
//    middle_layers          layer_size[1] ,NA,NA
//    optional: layer_size[number_of_layers-3] ,NA,NA
//    optional: layer_size[number_of_layers-2] ,NA,NA
//———————————————————————————-,NA,NA
},NA,NA
void network::set_up_network() ,NA,NA
{ ,NA,NA
"int i,j,k; ",NA,NA
//———————————————————————————-,NA,NA
// Construct the layers ,NA,NA
// ,NA,NA
//———————————————————————————-,NA,NA
"layer_ptr[0] = new input_layer(0,layer_size[0]); ",NA,NA
for (i=0;i<(number_of_layers-1);i++),NA,NA
 {,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (11 of 17) [21/11/02 21:58:07],NA
 layer_ptr[i+1] =,NA,NA
" new middle_layer(layer_size[i],layer_size[i+1]); ",NA,NA
},NA,NA
layer_ptr[number_of_layers-1] = new ,NA,NA
"output_layer(layer_size[number_of_layers-2], layer_size[number_of_ ",NA,NA
layers-1]);,NA,NA
for (i=0;i<(number_of_layers-1);i++),NA,NA
 {,NA,NA
 if (layer_ptr[i] == 0),NA,NA
 {,NA,NA
 cout << “insufficient memory\n”;,NA,NA
 cout << “use a smaller architecture\n”;,NA,NA
 exit(1);,NA,NA
 },NA,NA
 },NA,NA
//———————————————————————————-,NA,NA
// Connect the layers ,NA,NA
// ,NA,NA
//———————————————————————————-,NA,NA
"// set inputs to previous layer outputs for all layers, ",NA,NA
//             except the input layer,NA,NA
for (i=1; i< number_of_layers; i++),NA,NA
 layer_ptr[i]->inputs = layer_ptr[i-1]->outputs;,NA,NA
"// for back_propagation, set output_errors to next layer //             ",NA,NA
back_errors for all layers except the output //             ,NA,NA
layer and input layer,NA,NA
for (i=1; i< number_of_layers -1; i++),NA,NA
 ((output_layer *)layer_ptr[i])->output_errors =,NA,NA
 ,NA,NA
((output_layer *)layer_ptr[i+1])->back_errors;,NA,NA
// define the IObuffer that caches data from ,NA,NA
// the datafile ,NA,NA
i=layer_ptr[0]->num_outputs;// inputs ,NA,NA
j=layer_ptr[number_of_layers-1]->num_outputs; //outputs ,NA,NA
k=MAX_VECTORS;,NA,NA
buffer=new,NA,NA
 float[(i+j)*k]; ,NA,NA
if (buffer==0),NA,NA
 {,NA,NA
 cout << “insufficient memory for buffer\n”; ,NA,NA
exit(1);,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (12 of 17) [21/11/02 21:58:07],NA
},NA,NA
 },NA,NA
void network::randomize_weights() ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ((output_layer *)layer_ptr[i]),NA,NA
 -,NA,NA
>randomize_weights(); },NA,NA
"void network::update_weights(const float beta, const float alpha) ",NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ((output_layer *)layer_ptr[i]),NA,NA
" ->update_weights(beta,alpha); ",NA,NA
},NA,NA
void network::update_momentum() ,NA,NA
{ ,NA,NA
int i; ,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ((output_layer *)layer_ptr[i]),NA,NA
 ->update_momentum(); },NA,NA
void network::write_weights(FILE * weights_file_ptr) ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ((output_layer *)layer_ptr[i]),NA,NA
" ->write_weights(i,weights_file_ptr); ",NA,NA
},NA,NA
void network::read_weights(FILE * weights_file_ptr) ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 ((output_layer *)layer_ptr[i]),NA,NA
" ->read_weights(i,weights_file_ptr); ",NA,NA
},NA,NA
void network::list_weights() ,NA,NA
{,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (13 of 17) [21/11/02 21:58:07],NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 {,NA,NA
 cout << “layer number : “ <<i<< “\n”; ,NA,NA
((output_layer *)layer_ptr[i]),NA,NA
 ->list_weights();,NA,NA
 } ,NA,NA
},NA,NA
void network::list_outputs() ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 {,NA,NA
 cout << “layer number : “ <<i<< “\n”; ,NA,NA
((output_layer *)layer_ptr[i]),NA,NA
 ->list_outputs();,NA,NA
 } ,NA,NA
},NA,NA
void network::write_outputs(FILE *outfile) ,NA,NA
{ ,NA,NA
"int i, ins, outs; ",NA,NA
ins=layer_ptr[0]->num_outputs; ,NA,NA
outs=layer_ptr[number_of_layers-1]->num_outputs; ,NA,NA
float temp;,NA,NA
"fprintf(outfile,”for input vector:\n”);",NA,NA
for (i=0; i<ins; i++),NA,NA
 {,NA,NA
 temp=layer_ptr[0]->outputs[i]; ,NA,NA
"fprintf(outfile,”%f  “,temp); }",NA,NA
"fprintf(outfile,”\noutput vector is:\n”);",NA,NA
for (i=0; i<outs; i++),NA,NA
 {,NA,NA
 temp=layer_ptr[number_of_layers-1]-> ,NA,NA
outputs[i];,NA,NA
" fprintf(outfile,”%f  “,temp);",NA,NA
 },NA,NA
if (training==1) ,NA,NA
{,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (14 of 17) [21/11/02 21:58:07],NA
"fprintf(outfile,”\nexpected output vector is:\n”);",NA,NA
for (i=0; i<outs; i++),NA,NA
 {,NA,NA
 temp=((output_layer *)(layer_ptr[number_of_layers-1]))-> ,NA,NA
expected_values[i];,NA,NA
" fprintf(outfile,”%f  “,temp);",NA,NA
},NA,NA
 },NA,NA
"fprintf(outfile,”\n———————————\n”);",NA,NA
},NA,NA
void network::list_errors() ,NA,NA
{ ,NA,NA
int i;,NA,NA
for (i=1; i<number_of_layers; i++),NA,NA
 {,NA,NA
 cout << “layer number : “ <<i<< “\n”; ,NA,NA
((output_layer *)layer_ptr[i]),NA,NA
 ->list_errors();,NA,NA
 } ,NA,NA
},NA,NA
int network::fill_IObuffer(FILE * inputfile) ,NA,NA
{ ,NA,NA
// this routine fills memory with ,NA,NA
"// an array of input, output vectors ",NA,NA
// up to a maximum capacity of ,NA,NA
// MAX_INPUT_VECTORS_IN_ARRAY ,NA,NA
// the return value is the number of read // ,NA,NA
vectors,NA,NA
"int i, k, count, veclength;",NA,NA
"int ins, outs;",NA,NA
ins=layer_ptr[0]->num_outputs;,NA,NA
outs=layer_ptr[number_of_layers-1]->num_outputs;,NA,NA
if (training==1),NA,NA
 veclength=ins+outs; ,NA,NA
else,NA,NA
 veclength=ins;,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (15 of 17) [21/11/02 21:58:07],NA
count=0; ,NA,NA
while  ((count<MAX_VECTORS)&&,NA,NA
 (!feof(inputfile))),NA,NA
 {,NA,NA
 k=count*(veclength);,NA,NA
 for (i=0; i<veclength; i++),NA,NA
 {,NA,NA
" fscanf(inputfile,”%f”,&buffer[k+i]);",NA,NA
 },NA,NA
" fscanf(inputfile,”\n”);",NA,NA
 count++;,NA,NA
 },NA,NA
if (!(ferror(inputfile))),NA,NA
 return count; ,NA,NA
else return -1; // error condition,NA,NA
},NA,NA
void network::set_up_pattern(int buffer_index) ,NA,NA
{ ,NA,NA
// read one vector into the network ,NA,NA
"int i, k; ",NA,NA
"int ins, outs;",NA,NA
ins=layer_ptr[0]->num_outputs; ,NA,NA
outs=layer_ptr[number_of_layers-1]->num_outputs; ,NA,NA
if (training==1),NA,NA
 k=buffer_index*(ins+outs); ,NA,NA
else,NA,NA
 k=buffer_index*ins;,NA,NA
for (i=0; i<ins; i++),NA,NA
 ((input_layer*)layer_ptr[0]),NA,NA
 ->orig_outputs[i]=buffer[k+i]; ,NA,NA
if (training==1) ,NA,NA
{,NA,NA
 for (i=0; i<outs; i++),NA,NA
 ((output_layer *)layer_ptr[number_of_layers-1])->,NA,NA
 expected_values[i]=buffer[k+i+ins]; ,NA,NA
},NA,NA
},NA,NA
void network::forward_prop() ,NA,NA
{ ,NA,NA
int i; ,NA,NA
for (i=0; i<number_of_layers; i++),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (16 of 17) [21/11/02 21:58:07],NA
},NA,NA
 {,NA,NA
 layer_ptr[i]->calc_out(); //polymorphic ,NA,NA
// function,NA,NA
 },NA,NA
void network::backward_prop(float & toterror) { ,NA,NA
int i; ,NA,NA
// error for the output layer ,NA,NA
((output_layer*)layer_ptr[number_of_layers-1])->,NA,NA
 calc_error(toterror);,NA,NA
// error for the middle layer(s) ,NA,NA
for (i=number_of_layers-2; i>0; i—),NA,NA
 {,NA,NA
 ((middle_layer*)layer_ptr[i])->,NA,NA
 calc_error();,NA,NA
 },NA,NA
},NA,NA
void network::set_NF(float noise_fact) ,NA,NA
{ ,NA,NA
((input_layer*)layer_ptr[0])->set_NF(noise_fact); ,NA,NA
},NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...Neural_Networks_and_Fuzzy_Logic/ch13/340-362.html (17 of 17) [21/11/02 21:58:07]",NA
The New and Final backprop.cpp File,"The last file to present is the backprop.cpp file. This is shown in Listing 13.3. 
  
 Listing 13.3
  Implementation file for the backpropagation simulator, with noise and momentum backprop.cpp
  
 // backprop.cpp      V. Rao, H. Rao 
  
 #include “layer.cpp”
  
 #define TRAINING_FILE   “training.dat”
  
 #define WEIGHTS_FILE “weights.dat”
  
 #define OUTPUT_FILE   “output.dat”
  
 #define TEST_FILE   “test.dat”
  
 void main() 
  
 {
  
 float error_tolerance=0.1; 
  
 float total_error=0.0; 
  
 float avg_error_per_cycle=0.0; 
  
 float error_last_cycle=0.0; 
  
 float avgerr_per_pattern=0.0; // for the latest cycle 
 float error_last_pattern=0.0; 
  
 float learning_parameter=0.02; 
  
 float alpha; // momentum parameter 
  
 float NF; // noise factor 
  
 float new_NF;
  
 unsigned temp, startup, start_weights; 
  
 long int vectors_in_buffer; 
  
 long int max_cycles; 
  
 long int patterns_per_cycle=0;
  
 long int total_cycles, total_patterns; 
  
 int i;
  
 // create a network object 
  
 network backp;
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/362-372.html (1 of 8) [21/11/02 21:58:08]",NA
Previous Table of Contents Next,NA,NA
Trying the Noise and Momentum Features,NA,NA
"You can test out the version 2 simulator, which you just compiled with the example that ",NA,NA
you saw at the beginning of the chapter. You will find that there is a lot of trial and error in ,NA,NA
finding optimum values for ,NA,NA
alpha,NA,NA
", the noise factor, and ",NA,NA
beta,NA,NA
. This is true also for the ,NA,NA
"middle layer size and the number of middle layers. For some problems, the addition of ",NA,NA
"momentum makes convergence much faster. For other problems, you may not find any ",NA,NA
noticeable difference. An example run of the five-character recognition problem discussed ,NA,NA
at the beginning of this chapter resulted in the following results with ,NA,NA
beta,NA,NA
" = 0.1, ",NA,NA
tolerance ,NA,NA
"= 0.001, ",NA,NA
alpha,NA,NA
" = 0.25, ",NA,NA
NF,NA,NA
" = 0.1, and the layer sizes kept at 35 5 3.",NA,NA
—————————————————————————-,NA,NA
 done:   results in file output.dat,NA,NA
 training: last vector only,NA,NA
 not training: full cycle,NA,NA
 weights saved in file weights.dat,NA,NA
——>average error per cycle = 0.02993<—-,NA,NA
——>error last cycle = 0.00498<—-,NA,NA
->error last cycle per pattern= 0.000996 <—-,NA,NA
——————>total cycles = 242 <—-,NA,NA
——————>total patterns = 1210 <—-,NA,NA
—————————————————————————-,"The network was able to converge on a better solution (in terms of error measurement) in 
 one-fourth the number of cycles. You can try varying 
 alpha
  and 
 NF
  to see the effect on 
 overall simulation time. You can now start from the same initial starting weights by 
 specifying a value of 1 for the starting weights question. For large values of 
 alpha
  and 
 beta
 , the network usually will not converge, and the weights will get unacceptably large",NA
Variations of the Backpropagation Algorithm,NA,NA
Backpropagation is a versatile neural network algorithm that very often leads to success. ,NA,NA
Its Achilles heel is the slowness at which it converges for certain problems. Many ,NA,NA
variations of the algorithm exist in the literature to try to improve convergence speed and ,NA,NA
robustness. Variations have been proposed in the following portions of the algorithm: ,NA,NA
•Adaptive parameters.,NA,NA
 You can set rules that modify ,NA,NA
alpha,NA,NA
", the momentum ",NA,NA
"parameter, and ",NA,NA
beta,NA,NA
", the learning parameter, as the simulation progresses. For ",NA,NA
"example, you can reduce beta whenever a weight change does not reduce the error. ",NA,NA
"You can consider undoing the particular weight change, setting ",NA,NA
alpha,NA,NA
 to ,NA,NA
zero,NA,NA
 and ,NA,NA
redoing the weight change with the new value of ,NA,NA
beta,NA,NA
. ,NA,NA
•Use other minimum search routines besides steepest descent.,NA,NA
" For example, ",NA,NA
"you could use Newton’s method for finding a minimum, although this would be a ",NA,NA
fairly slow process. Other examples include the use of conjugate gradient methods ,NA,NA
"or Levenberg-Marquardt optimization, both of which would result in very rapid ",NA,NA
training. ,NA,NA
•Use different cost functions.,NA,NA
 Instead of calculating the error (as ,NA,NA
"expected—actual output), you could determine another cost function that you want ",NA,NA
to minimize. ,NA,NA
•Modify the architecture.,NA,NA
 You could use partially connected layers instead of ,NA,NA
"fully connected layers. Also, you can use a recurrent network, that is, one in which ",NA,NA
some outputs feed back as inputs. ,NA,NA
Applications,NA,NA
Backpropagation remains the king of neural network architectures because of its ease of ,NA,NA
use and wide applicability. A few of the notable applications in the literature will be cited ,NA,NA
as examples. ,NA,NA
•NETTalk.,NA,NA
" In 1987, Sejnowski and Rosenberg developed a network connected to a ",NA,NA
"speech synthesizer that was able to utter English words, being trained to produce ",NA,NA
phonemes from English text. The architecture consisted of an input layer window of ,NA,NA
seven characters. The characters were part of English text that was scrolled by. The ,NA,NA
network was trained to pronounce the letter at the center of the window. The middle ,NA,NA
"layer had 80 neurons, while the output layer consisted of 26 neurons. With 1024 ",NA,NA
"training patterns and 10 cycles, the network started making intelligible speech, ",NA,NA
"similar to the process of a child learning to talk. After 50 cycles, the network was ",NA,NA
about 95% accurate. You could purposely damage the network with the removal of ,NA,NA
"neurons, but this did not cause performance to drop off a cliff; instead, the ",NA,NA
performance degraded gracefully. There was rapid recovery with retraining using ,NA,NA
fewer neurons also. This shows the fault tolerance of neural networks. ,NA,NA
•Sonar target recognition.,NA,NA
 Neural nets using backpropagation have been used to ,NA,NA
identify different types of targets using the frequency signature (with a Fast Fourier ,NA,NA
transform) of the reflected signal. ,NA,NA
•Car navigation.,NA,NA
 Pomerleau developed a neural network that is able to navigate a ,NA,NA
"car based on images obtained from a camera mounted on the car’s roof, and a range ",NA,NA
finder that coded distances in grayscale. The 30×32 pixel image and the 8×32 range ,NA,NA
finder image were fed into a hidden layer of size 29 feeding an output layer of 45 ,NA,NA
neurons. The output neurons were arranged in a straight line with each side ,NA,NA
"representing a turn to a particular direction (right or left), while the center neurons ",NA,NA
represented “drive straight ahead.” After 1200 road images were trained on the ,NA,NA
"network, the neural network driver was able to negotiate a part of the Carnegie-",NA,NA
"Mellon campus at a speed of about 3 miles per hour, limited only by the speed of the ",NA,NA
real-time calculations done on a trained network in the Sun-3 computer in the car. ,NA,NA
•Image compression.,NA,NA
" G.W. Cottrell, P. Munro, and D. Zipser used ",NA,NA
backpropagation to compress images with the result of an 8:1 compression ratio. ,NA,NA
"They used standard backpropagation with 64 input neurons (8×8 pixels), 16 hidden ",NA,NA
"neurons, and 64 output neurons equal to the inputs. This is called ",NA,NA
self-supervised ,NA,NA
backpropagation,NA,NA
 and represents an autoassociative network. The compressed signal ,NA,NA
"is taken from the hidden layer. The input to hidden layer comprised the compressor, ",NA,NA
while the hidden to output layer forms a decompressor. ,NA,NA
•Image recognition.,NA,NA
 Le Cun reported a backpropagation network with three hidden ,NA,NA
layers that could recognize handwritten postal zip codes. He used a 16×16 array of ,NA,NA
"pixel to represent each handwritten digit and needed to encode 10 outputs, each of ",NA,NA
which represented a digit from 0 to 9. One interesting aspect of this work is that the ,NA,NA
hidden layers were not fully connected. The network was set up with blocks of ,NA,NA
neurons in the first two hidden layers set up as feature detectors for different parts of ,NA,NA
the previous layer. All the neurons in the block were set up to have the same weights ,NA,NA
as those from the previous layer. This is called ,NA,NA
weight sharing,NA,NA
. Each block would ,NA,NA
sample a different part of the previous layer’s image. The first hidden layer had 12 ,NA,NA
"blocks of 8×8 neurons, whereas the second hidden layer had 12 blocks of 4×4 ",NA,NA
neurons. The third hidden layer was fully connected and consisted of 30 neurons. ,NA,NA
There were 1256 neurons. The network was trained on 7300 examples and tested on ,NA,NA
2000 cases with error rates of 1% on training set and 5% on the test set. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/372-375.html (3 of 4) [21/11/02 21:58:09],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch13/372-375.html (4 of 4) [21/11/02 21:58:09]",NA
Previous Table of Contents Next,NA,NA
Summary,NA,NA
"You explored further the backpropagation algorithm in this chapter, continuing the ",NA,NA
discussion in Chapter 7. ,NA,NA
•,NA,NA
  A momentum term was added to the training law and was shown to result in ,NA,NA
much faster convergence in some cases. ,NA,NA
•,NA,NA
  A noise term was added to inputs to allow training to take place with random ,NA,NA
"noise applied. This noise was made to decrease with the number of cycles, so that ",NA,NA
final stage learning could be done in a noise-free environment. ,NA,NA
•,NA,NA
  The final version of the backpropagation simulator was constructed and used on ,NA,NA
the example from Chapter 12. Further application of the simulator will be made in ,NA,NA
Chapter 14. ,NA,NA
•,NA,NA
"  Several applications with the backpropagation algorithm were outlined, showing ",NA,NA
the wide applicability of this algorithm. ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Chapter 14 ,NA,NA
Application to Financial Forecasting ,NA,NA
Introduction,NA,NA
"In Chapters 7 and 13, the backpropagation simulator was developed. In this chapter, you ",NA,NA
will use the simulator to tackle a complex problem in financial forecasting. The application ,NA,NA
of neural networks to financial forecasting and modeling has been very popular over the ,NA,NA
last few years. Financial journals and magazines frequently mention the use of neural ,NA,NA
"networks, and commercial tools and simulators are quite widespread. ",NA,NA
This chapter gives you an overview of typical steps used in creating financial forecasting ,NA,NA
"models. Many of the steps will be simplified, and so the results will not, unfortunately, be ",NA,NA
"good enough for real life application. However, this chapter will hopefully serve as an ",NA,NA
introduction to the field with some added pointers for further reading and resources for ,NA,NA
those who want more detailed information.,NA,NA
Who Trades with Neural Networks?,NA,NA
There has been a great amount of interest on Wall Street for neural networks. Bradford ,NA,NA
"Lewis runs two Fidelity funds in part with the use of neural networks. Also, LBS Capital ",NA,NA
"Management (Peoria, Illinois) manages part of its portfolio with neural networks. ",NA,NA
"According to Barron’s (February 27, 1995), LBS’s $150 million fund beat the averages by ",NA,NA
"three percentage points a year since 1992. Each weekend, neural networks are retrained ",NA,NA
"with the latest technical and fundamental data including P/E ratios, earnings results and ",NA,NA
interest rates. Another of LBS’s models has done worse than the S&P 500 for the past five ,NA,NA
years however. In the book ,NA,NA
Virtual Trading,NA,NA
", Jeffrey Katz states that most of the successful ",NA,NA
neural network systems are proprietary and not publicly heard of. Clients who use neural ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/377-379.html (1 of 3) [21/11/02 21:58:11],NA
"networks usually don’t want anyone else to know what they are doing, for fear of losing ",NA,NA
their competitive edge. Firms put in many person-years of engineering design with a lot of ,NA,NA
CPU cycles to achieve practical and profitable results. Let’s look at the process:,NA,NA
Developing a Forecasting Model,NA,NA
"There are many steps in building a forecasting model, as listed below. ",NA,NA
1.,NA,NA
  Decide on what your target is and develop a neural network (following these ,NA,NA
steps) for each target. ,NA,NA
2.,NA,NA
  Determine the time frame that you wish to forecast. ,NA,NA
3.,NA,NA
  Gather information about the problem domain. ,NA,NA
4.,NA,NA
  Gather the needed data and get a feel for each inputs relationship to the target. ,NA,NA
5.,NA,NA
  Process the data to highlight features for the network to discern. ,NA,NA
6.,NA,NA
  Transform the data as appropriate. ,NA,NA
7.,NA,NA
"  Scale and bias the data for the network, as needed. ",NA,NA
8.,NA,NA
  Reduce the dimensionality of the input data as much as possible. ,NA,NA
9.,NA,NA
"  Design a network architecture (topology, # layers, size of layers, parameters, ",NA,NA
learning paradigm). ,NA,NA
10.,NA,NA
  Go through the train/test/redesign loop for a network. ,NA,NA
11.,NA,NA
"  Eliminate correlated inputs as much as possible, while in step 10. ",NA,NA
12.,NA,NA
  Deploy your network on new data and test it and refine it as necessary. ,NA,NA
"Once you develop a forecasting model, you then must integrate this into your ",NA,NA
trading ,NA,NA
system.,NA,NA
" A neural network can be designed to predict direction, or magnitude, or maybe just ",NA,NA
turning points in a particular market or something else. Avner Mandelman of Cereus ,NA,NA
"Investments (Los Altos Hills, California) uses a long-range trained neural network to tell ",NA,NA
him when the market is making a top or bottom (,NA,NA
Barron’s,NA,NA
", December 14, 1992).",NA,NA
Now let’s expand on the twelve aspects of model building:,NA,NA
The Target and the Timeframe,NA,NA
What should the output of your neural network forecast? Let’s say you want to predict the ,NA,NA
"stock market. Do you want to predict the S&P 500? Or, do you want to predict the direction ",NA,NA
of the S&P 500 perhaps? You could predict the volatility of the S&P 500 too (maybe if ,NA,NA
"you’re an options player). Further, like Mr. Mandelman, you could only want to predict ",NA,NA
"tops and bottoms, say, for the Dow Jones Industrial Average. You need to decide on the ",NA,NA
market or markets and also on your specific objectives. ,NA,NA
Another crucial decision is the timeframe you want to predict forward. It is easier to create ,NA,NA
neural network models for longer term predictions than it is for shorter term predictions. ,NA,NA
"You can see a lot of market noise, or seemingly random, chaotic variations at smaller and ",NA,NA
smaller timescale resolutions that might explain this. Another reason is that the ,NA,NA
macroeconomic forces that ,NA,NA
fundamentally,NA,NA
" move market over long periods, move slowly. ",NA,NA
"The U.S. dollar makes multiyear trends, shaped by economic policy of governments ",NA,NA
"around the world. For a given error tolerance, a one-year forecast, or one-month forecast ",NA,NA
will take less effort with a neural network than a one-day forecast will.,NA,NA
Domain Expertise,NA,NA
So far we’ve talked about the target and the timeframe. Now one other important aspect of ,NA,NA
model building is knowledge of the domain. If you want to create an effective predictive ,NA,NA
"model of the weather, then you need to know or be able to guess about the factors that ",NA,NA
influence weather. The same holds true for the stock market or other financial market. In ,NA,NA
"order to create a real tradable Treasury bond trading system, you need to have a good idea ",NA,NA
"of what really drives the market and works— i.e., talk to a Tbond trader and encapsulate his ",NA,NA
domain expertise! ,NA,NA
Gather the Data,NA,NA
"Once you know the factors that influence the target output, you can gather raw data. If you ",NA,NA
"are predicting the S&P 500, then you may consider Treasury yields, 3-month T-bill yields, ",NA,NA
"and earnings as some of the factors. Once you have the data, you can do scatter plots to see ",NA,NA
if there is some correlation between the input and the target output. If you are not satisfied ,NA,NA
"with the plot, you may consider a different input in its place. ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/377-379.html (3 of 3) [21/11/02 21:58:11]",NA
Previous Table of Contents Next,NA,NA
Pre processing the Data for the Network,NA,NA
"Surprising as it may sound, you are most likely going to spend about 90% of your time, as ",NA,NA
"a neural network developer, in massaging and transforming data into meaningful form for ",NA,NA
training your network. We actually defined three substeps in this area of preprocessing in ,NA,NA
our master list: ,NA,NA
•,NA,NA
  Highlight features ,NA,NA
•,NA,NA
  Transform ,NA,NA
•,NA,NA
  Scale and bias ,NA,NA
Highlighting Features in the Input Data,NA,NA
"You should present the neural network, as much as possible, with an easy way to find ",NA,NA
"patterns in your data. For time series data, like stock market prices over time, you may ",NA,NA
consider presenting quantities like rate of change and acceleration (the first and second ,NA,NA
derivatives of your input) as examples. Other ways to highlight data is to magnify certain ,NA,NA
"occurrences. For example, if you consider Central bank intervention as an important ",NA,NA
"qualifier to foreign exchange rates, then you may include as an input to your network, a ",NA,NA
"value of 1 or 0, to indicate the presence or lack of presence of Central bank intervention. ",NA,NA
Now if you further consider the activity of the U.S. Federal Reserve bank to be important ,NA,NA
"by itself, then you may wish to highlight that, by separating it out as another 1/0 input. ",NA,NA
Using 1/0 coding to separate composite effects is called ,NA,NA
thermometer encoding,NA,NA
.,NA,NA
There is a whole body of study of market behavior called ,NA,NA
Technical Analysis,NA,NA
 from which ,NA,NA
you may also wish to present ,NA,NA
technical studies,NA,NA
 on your data. There is a wide assortment of ,NA,NA
"mathematical technical studies that you perform on your data (see references), such as ",NA,NA
moving averages to smooth data as an example. There are also pattern recognition studies ,NA,NA
"you can use, like the “double-top” formation, which purportedly results in a high ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/379-382.html (1 of 3) [21/11/02 21:58:12],NA
"probability of significant decline. To be able to recognize such a pattern, you may wish to ",NA,NA
present a mathematical function that aids in the identification of the double-top.,NA,NA
You may want to de-emphasize unwanted noise in your input data. If you see a spike in ,NA,NA
"your data, you can lessen its effect, by passing it through a moving average filter for ",NA,NA
example. You should be careful about introducing excessive lag in the resulting data ,NA,NA
though.,NA,NA
Transform the Data If Appropriate,NA,NA
"For time series data, you may consider using a ",NA,NA
Fourier transform,NA,NA
 to move to the frequency-,NA,NA
phase plane. This will uncover periodic cyclic information if it exists. The Fourier ,NA,NA
transform will decompose the input discrete data series into a series of frequency spikes ,NA,NA
that measure the relevance of each frequency component. If the stock market indeed ,NA,NA
"follows the so-called January effect, where prices typically make a run up, then you would ",NA,NA
expect a strong yearly component in the frequency spectrum. Mark Jurik suggests sampling ,NA,NA
"data with intervals that catch different cycle periods, in his paper on neural network data ",NA,NA
preparation (see references ).,NA,NA
You can use other signal processing techniques such as filtering. Besides the frequency ,NA,NA
"domain, you can also consider moving to other spaces, such as with using the wavelet ",NA,NA
transform. You may also analyze the chaotic component of the data with chaos measures. ,NA,NA
It’s beyond the scope of this book to discuss these techniques. (Refer to the Resources ,NA,NA
section of this chapter for more information.) If you are developing short-term trading ,NA,NA
"neural network systems, these techniques may play a significant role in your preprocessing ",NA,NA
"effort. All of these techniques will provide new ways of looking at your data, for possible ",NA,NA
features to detect in other domains.,NA,NA
Scale Your Data,NA,NA
Neurons like to see data in a particular input range to be most effective. If you present data ,NA,NA
like the S&P 500 that varies from 200 to 550 (as the S&P 500 has over the years) will not ,NA,NA
"be useful, since the middle layer of neurons have a Sigmoid Activation function that ",NA,NA
"squashes large inputs to either 0 or +1. In other words, you should choose data that fit a ",NA,NA
"range that does not saturate, or overwhelm the network neurons. Choosing inputs from –1 ",NA,NA
"to 1 or 0 to 1 is a good idea. By the same token, you should normalize the expected values ",NA,NA
for the outputs to the 0 to 1 sigmoidal range. ,NA,NA
It is important to pay attention to the number of input values in the data set that are close to ,NA,NA
"zero. Since the weight change law is proportional to the input value, then a close to zero ",NA,NA
"input will mean that that weight will not participate in learning! To avoid such situations, ",NA,NA
"you can add a constant bias to your data to move the data closer to 0.5, where the neurons ",NA,NA
respond very well.,NA,NA
Reduce Dimensionality,NA,NA
You should try to eliminate inputs wherever possible. This will reduce the ,NA,NA
dimensionality ,NA,NA
of the problem and make it easier for your neural network to generalize. Suppose that you ,NA,NA
"have three inputs, ",NA,NA
x,NA,NA
", ",NA,NA
y,NA,NA
 and ,NA,NA
z,NA,NA
" and one output, ",NA,NA
o,NA,NA
. Now suppose that you find that all of your ,NA,NA
inputs are restricted only to one plane. You could redefine axes such that you have ,NA,NA
x,NA,NA
’ and ,NA,NA
y,NA,NA
’ ,NA,NA
for the new plane and map your inputs to the new coordinates. This changes the number of ,NA,NA
"inputs to your problem to 2 instead of 3, without any loss of information. This is illustrated ",NA,NA
in Figure 14.1.,NA,NA
Figure 14.1,NA,NA
  Reducing dimensionality from three to two dimensions.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/379-382.html (3 of 3) [21/11/02 21:58:12]",NA
Previous Table of Contents Next,NA,NA
Generalization versus Memorization,NA,NA
"If your overall goal is beyond pattern classification, you need to track your network’s ",NA,NA
ability to generalize. Not only should you look at the overall error with the training set that ,NA,NA
"you define, but you should set aside some training examples as part of a test set (and do not ",NA,NA
"train with them), with which you can see whether or not the network is able to correctly ",NA,NA
"predict. If the network responds poorly to your test set, you know that you have ",NA,NA
"overtrained, or you can say the network “memorized” the training patterns. If you look at ",NA,NA
"the arbitrary curve-fitting analogy in Figure 14.2, you see curves for a generalized fit, ",NA,NA
"labeled G, and an overfit, labeled O. In the case of the overfit, any data point outside of the ",NA,NA
training data results in highly erroneous prediction. Your test data will certainly show you ,NA,NA
large error in the case of an overfitted model. ,NA,NA
Figure 14.2,NA,NA
  General (G) versus over fitting (0) of data.,NA,NA
Another way to consider this issue is in terms of ,NA,NA
Degrees Of Freedom (DOF),NA,NA
. For the ,NA,NA
polynomial:,NA,NA
y= a0 + a1x + a2x2 + anxn...,NA,NA
the DOF equals the number of coefficients ,NA,NA
a,NA,NA
"0, ",NA,NA
a,NA,NA
1 ... ,NA,NA
an,NA,NA
", which is ",NA,NA
N,NA,NA
 + 1. So for the equation ,NA,NA
of a line (,NA,NA
y,NA,NA
=,NA,NA
a,NA,NA
0 + ,NA,NA
a1x,NA,NA
"), the DOF would be 2. For a parabola, this would be 3 and so on. The ",NA,NA
objective to not overfit data can be restated as an objective to obtain the function with the ,NA,NA
"least DOF that fits the data adequately. For neural network models, the larger the number of ",NA,NA
"trainable weights (which is a function of the number of inputs and the architecture), the ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/382-385.html (1 of 3) [21/11/02 21:58:13],NA
larger the DOF. Be careful with having too many (unimportant) inputs. You may find ,NA,NA
"terrific results with your training data, but extremely poor results with your test data.",NA,NA
Eliminate Correlated Inputs Where Possible,NA,NA
You have seen that getting to the minimum number of inputs for a given problem is ,NA,NA
important in terms of minimizing DOF and simplifying your model. Another way to reduce ,NA,NA
dimensionality is to look for correlated inputs and to ,NA,NA
carefully,NA,NA
 eliminate ,NA,NA
"redundancy. For example, you may find that the Swiss franc and German mark are highly ",NA,NA
correlated over a certain time period of interest. You may wish to eliminate one of these ,NA,NA
inputs to reduce dimensionality. You have to be careful in this process though. You may ,NA,NA
"find that a seemingly redundant piece of information is actually very important. Mark Jurik, ",NA,NA
"of Jurik Consulting, in his paper on data preprocessing, suggests that one of the best ways ",NA,NA
to determine if an input is really needed is to construct neural network models with and ,NA,NA
without the input and choose the model with the best error on training and test data. ,NA,NA
"Although very iterative, you can try eliminating as many inputs as possible this way and be ",NA,NA
assured that you haven’t eliminated a variable that really made a difference.,NA,NA
Another approach is ,NA,NA
sensitivity analysis,NA,NA
", where you vary one input a little, while holding all ",NA,NA
others constant and note the effect on the output. If the effect is small you eliminate that ,NA,NA
"input. This approach is flawed because in the real world, all the inputs ",NA,NA
are not constant. ,NA,NA
Jurik’s approach is more time consuming but will lead to a better model.,NA,NA
The process of ,NA,NA
"decorrelation,",NA,NA
" or eliminating correlated inputs, can also utilize a linear ",NA,NA
algebra technique called ,NA,NA
principal component analysis.,NA,NA
 The result of principal component ,NA,NA
analysis is a minimum set of variables that contain the maximum information. For further ,NA,NA
"information on principal component analysis, you should consult a statistics reference or ",NA,NA
research two methods of principal component analysis: the ,NA,NA
Karhunen-Loev transform,NA,NA
 and ,NA,NA
the ,NA,NA
Hotelling transform,NA,NA
.,NA,NA
Design a Network Architecture,NA,NA
Now it’s time to actually design the neural network. For the backpropagation feed-forward ,NA,NA
"neural network we have designed, this means making the following choices: ",NA,NA
1.,NA,NA
  The number of hidden layers. ,NA,NA
2.,NA,NA
  The size of hidden layers. ,NA,NA
3.,NA,NA
"  The learning constant, beta([beta]). ",NA,NA
4.,NA,NA
"  The momentum parameter, alpha([alpha]). ",NA,NA
5.,NA,NA
  The form of the squashing function (does not have to be the sigmoid). ,NA,NA
6.,NA,NA
"  The starting point, that is, initial weight matrix. ",NA,NA
7.,NA,NA
  The addition of noise. ,NA,NA
"Some of the parameters listed can be made to vary with the number of cycles executed, ",NA,NA
"similar to the current implementation of noise. For example, you can start with a learning ",NA,NA
constant [beta] that is large and reduce this constant as learning progresses. This allows ,NA,NA
rapid initial learning in the beginning of the process and may speed up the overall ,NA,NA
simulation time. ,NA,NA
The Train/Test/Redesign Loop,NA,NA
Much of the process of determining the best parameters for a given application is trial and ,NA,NA
error. You need to spend a great deal of time evaluating different options to find the best fit ,NA,NA
for your problem. You may literally create hundreds if not thousands of networks either ,NA,NA
manually or automatically to search for the best solution. Many commercial neural network ,NA,NA
programs use ,NA,NA
genetic algorithms,NA,NA
 to help to automatically arrive at an optimum network. A ,NA,NA
genetic algorithm makes up possible solutions to a problem from a set of starting ,NA,NA
genes.,NA,NA
 ,NA,NA
"Analogous to biological evolution, the algorithm combines genetic ",NA,NA
"solutions with a predefined set of operators to create new generations of solutions, who ",NA,NA
survive or perish depending on their ability to solve the problem. The key benefit of genetic ,NA,NA
algorithms (GA) is the ability to traverse an enormous search space for a possibly optimum ,NA,NA
solution. You would program a GA to search for the number of hidden layers and other ,NA,NA
"network parameters, and gradually evolve a neural network solution. Some vendors use a ",NA,NA
"GA only to assign a starting set of weights to the network, instead of randomizing the ",NA,NA
weights to start you off near a good solution.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/382-385.html (3 of 3) [21/11/02 21:58:13]",NA
Previous Table of Contents Next,NA,NA
Now let’s review the steps: ,NA,NA
1.Split your data,NA,NA
". First, divide you data set into three pieces, a training set, a test ",NA,NA
"set and a blind test set. Use about 80% of your data records for your training set, ",NA,NA
10% for your test set and 10% for your blind test set. ,NA,NA
2.Train and test,NA,NA
". Next, start with a network topology and train your network on ",NA,NA
"your training set data. When you have reached a satisfactory minimum error, save ",NA,NA
your weights and apply your trained network to the test data and note the error. Now ,NA,NA
restart the process with the same network topology for a different set of initial ,NA,NA
weights and see if you can achieve a better error on training and test sets. ,NA,NA
Reasoning: you may have found a local minimum on your first attempt and ,NA,NA
"randomizing the initial weights will start you off to a different, maybe better ",NA,NA
solution. ,NA,NA
3.Eliminate correlated inputs,NA,NA
. You may optionally try at this point to see if you ,NA,NA
"can eliminate correlated inputs, as mentioned before, by iteratively removing each ",NA,NA
input and noting the best error you can achieve on the training and test sets for each ,NA,NA
of these cases. Choose the case that leads to the best error and eliminate the input (if ,NA,NA
any) that achieved it. You can repeat this whole process again to try to eliminate ,NA,NA
another input variable. ,NA,NA
4.Iteratively train and test,NA,NA
. Now you can try other network parameters and repeat ,NA,NA
the train and test process to achieve a better result. ,NA,NA
5.Deploy your network,NA,NA
. You now can use the blind test data set to see how your ,NA,NA
"optimized network performs. If the error is not satisfactory, then you need to re-",NA,NA
enter the design phase or the train and test phase. ,NA,NA
6.Revisit your network design when conditions change,NA,NA
. You need to retrain your ,NA,NA
network when you have reason to think that you have new information relevant to ,NA,NA
the problem you are modeling. If you have a neural network that tries to predict the ,NA,NA
"weekly change in the S&P 500, then you likely will need to retrain your network at ",NA,NA
"least once a month, if not once a week. If you find that the network no longer ",NA,NA
"generalizes well with the new information, you need to re-enter the design ",NA,NA
phase. ,NA,NA
"If this sounds like a lot of work, it is! Now, let’s try our luck at forecasting by going ",NA,NA
through a subset of the steps outlined: ,NA,NA
Forecasting the S&P 500,NA,NA
"The S&P 500 index is a widely followed stock average, like the Dow Jones Industrial ",NA,NA
Average (DJIA). It has a broader representation of the stock market since this average is ,NA,NA
"based on 500 stocks, whereas the DJIA is based on only 30. The problem to be approached ",NA,NA
"in this chapter is to predict the S&P 500 index, given a variety of indicators and data for ",NA,NA
prior weeks. ,NA,NA
Choosing the Right Outputs and Objective,NA,NA
Our objective is to forecast the S&P 500 ten weeks from now. Whereas the objective may ,NA,NA
"be to predict the level of the S&P 500, it is important to simplify the job of the network by ",NA,NA
asking for a change in the level rather than for the absolute level of the index. What you ,NA,NA
want to do is give the network the ability to fit the problem at hand conveniently in the ,NA,NA
"output space of the output layer. Practically speaking, you know that the output from the ",NA,NA
"network cannot be outside of the 0 to 1 range, since we have used a sigmoid activation ",NA,NA
function. You could take the S&P 500 index and scale this absolute price level to this ,NA,NA
"range, for example. However, you will likely end up with very small numbers that have a ",NA,NA
"small range of variability. The difference from week to week, on the other hand, has a ",NA,NA
"much smaller overall range, and when these differences are scaled to the 0 to 1 range, you ",NA,NA
have much more variability. ,NA,NA
The output we choose is the change in the S&P 500 from the current week to 10 weeks ,NA,NA
from now as a percentage of the current week’s value.,NA,NA
Choosing the Right Inputs,NA,NA
The inputs to the network need to be weekly changes of indicators that have some ,NA,NA
"relevance to the S&P 500 index. This is a complex forecasting problem, and we can only ",NA,NA
guess at some of the relationships. This is one of the inherent strengths of using neural nets ,NA,NA
"for forecasting; if a relationship is weak, the network will learn to ignore it automatically. ",NA,NA
Be cognizant that you do want to minimize the DOF as mentioned before though. In this ,NA,NA
"example, we choose a data set that represents the state of the financial markets and the ",NA,NA
economy. The inputs chosen are listed as: ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/385-388.html (2 of 3) [21/11/02 21:58:14],NA
•,NA,NA
"  Previous price action in the S&P 500 index, including the close or final value of ",NA,NA
the index ,NA,NA
•,NA,NA
"  Breadth indicators for the stock market, including the number of advancing issues ",NA,NA
and declining issues for the stocks in the New York Stock Exchange (NYSE) ,NA,NA
•,NA,NA
  ,NA,NA
"Other technical indicators, including the number of new highs and new lows ",NA,NA
achieved in the week for the NYSE market. This gives some indication about the ,NA,NA
strength of an uptrend or downtrend. ,NA,NA
•,NA,NA
"  Interest rates, including short-term interest rates in the Three-Month Treasury Bill ",NA,NA
"Yield, and long-term rates in the 30-Year Treasury Bond Yield. ",NA,NA
Other possible inputs could have been government statistics like the Consumer Price ,NA,NA
"Index, Housing starts, and the Unemployment Rate. These were not chosen because long- ",NA,NA
and short-term interest rates tend to encompass this data already. ,NA,NA
You are encouraged to experiment with other inputs and ideas. All of the data mentioned ,NA,NA
"can be obtained in the public domain, such as from financial publications (",NA,NA
Barron’s,NA,NA
", ",NA,NA
Investor’s Business Daily,NA,NA
", ",NA,NA
Wall Street Journal,NA,NA
) and from ftp sites on the Internet for the ,NA,NA
"Department of Commerce and the Federal Reserve, as well as from commercial vendors ",NA,NA
(see the Resource Guide at the end of the chapter). There are new sources cropping up on ,NA,NA
the Internet all the time. A sampling of World Wide Web addresses for commercial and ,NA,NA
noncommercial sources include:,NA,NA
•,NA,NA
"  FINWeb, ",NA,NA
http://riskweb.bus.utexas.edu/finweb.html,NA,NA
•,NA,NA
"  Chicago Mercantile Exchange, ",NA,NA
http://www.cme.com/cme,NA,NA
•,NA,NA
"  SEC Edgar Database, ",NA,NA
http://town.hall.org/edgar/edgar.html,NA,NA
•,NA,NA
"  CTSNET Business & Finance Center, ",NA,NA
http://www.cts.com/cts/biz/,NA,NA
•,NA,NA
"  QuoteCom, ",NA,NA
http://www.quote.com,NA,NA
•,NA,NA
"  Philadelphia Fed, ",NA,NA
http://compstat.wharton.upenn.edu:8001/~siler/ fedpage.html,NA,NA
•,NA,NA
"  Ohio state Financial Data Finder, ",NA,NA
http://cob.ohio-state.edu/dept/fin/ osudata.html,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Choosing a Network Architecture,NA,NA
The input and output layers are fixed by the number of inputs and outputs we are using. In ,NA,NA
"our case, the output is a single number, the expected change in the S&P 500 index 10 ",NA,NA
weeks from now. The input layer size will be dictated by the number of inputs we have ,NA,NA
after preprocessing. You will see more on this soon. The middle layers can be either 1 or 2. ,NA,NA
It is best to choose the smallest number of neurons possible for a given problem to allow ,NA,NA
"for generalization. If there are too many neurons, you will tend to get memorization of ",NA,NA
patterns. We will use one hidden layer. The size of the first hidden layer is generally ,NA,NA
recommended as between one-half to three times the size of the input layer. If a second ,NA,NA
"hidden layer is present, you may have between three and ten times the number of output ",NA,NA
neurons. The best way to determine optimum size is by trial and error. ,"NOTE:  
 You should try to make sure that there are enough training examples for your 
 trainable weights. In other words, your architecture may be dictated by the number of 
 input training examples, or 
 facts,
  you have. In an ideal world, you would want to have 
 about 10 or more facts for each weight. For a 10-10-1 architecture, there are (10X10 + 
 10X1 = 110 weights), so you should aim for about 1100 facts. The smaller the ratio of 
 facts to weights, the more likely you will be undertraining your network, which will lead 
 to very poor generalization capability.",NA
Preprocessing Data,NA,NA
"We now begin the preprocessing effort. As mentioned before, this will likely be where ",NA,NA
"you, the neural network designer, will spend most of your time. ",NA,NA
A View of the Raw Data,NA,NA
Let’s look at the raw data for the problem we want to solve. There are a couple of ways we ,NA,NA
can start preprocessing the data to reduce the number of inputs and enhance the variability ,NA,NA
of the data: ,NA,NA
•,NA,NA
  Use Advances/Declines ratio instead of each value separately. ,NA,NA
•,NA,NA
  ,NA,NA
Use New Highs/New Lows ratio instead of each value separately. ,NA,NA
We are left with the following indicators: ,NA,NA
1.,NA,NA
  Three-Month Treasury Bill Yield ,NA,NA
2.,NA,NA
  30-Year Treasury Bond Yield ,NA,NA
3.,NA,NA
  NYSE Advancing/Declining issues ,NA,NA
4.,NA,NA
  NYSE New Highs/New Lows ,NA,NA
5.,NA,NA
  S&P 500 closing price ,NA,NA
"Raw data for the period from January 4, 1980 to October 28, 1983 is taken as the training ",NA,NA
"period, for a total of 200 weeks of data. The following 50 weeks are kept on reserve for a ",NA,NA
test period to see if the predictions are valid outside of the training interval. The last date of ,NA,NA
"this period is October 19, 1984. Let’s look at the raw data now. (You get data on the disk ",NA,NA
"available with this book that covers the period from January, 1980 to December, 1992.) In ",NA,NA
"Figures 14.3 through 14.5, you will see a number of these indicators plotted over the ",NA,NA
training plus test intervals: ,NA,NA
•,NA,NA
  Figure 14.3 shows you the S&P 500 stock index. ,NA,NA
Figure 14.3,NA,NA
  The S&P 500 Index for the period of interest.,NA,NA
•,NA,NA
  Figure 14.4 shows long-term bonds and short-term 3-month T-bill interest rates. ,NA,NA
Figure 14.4,NA,NA
  Long-term and short-term interest rates.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/388-393.html (2 of 4) [21/11/02 21:58:15],NA
•,NA,NA
"  Figure 14.5 shows some breadth indicators on the NYSE, the number of ",NA,NA
"advancing stocks/number of declining stocks, as well as the ratio of new highs to ",NA,NA
new lows on the NYSE ,NA,NA
Figure 14.5,NA,NA
  Breadth indicators on the NYSE: Advancing/Declining issues and ,NA,NA
New Highs/New Lows.,NA,NA
A sample of a few lines looks like the following data in Table 14.1. Note that the order of ,NA,NA
parameters is the same as listed above. ,NA,NA
Table 14.1,NA,NA
Raw Data ,NA,NA
Date ,NA,NA
3Mo ,NA,NA
TBills ,NA,NA
30YrTBonds ,NA,NA
NYSE-,NA,NA
Adv/Dec ,NA,NA
NYSE-,NA,NA
NewH/NewL ,NA,NA
SP-Close ,NA,NA
1/4/80 ,NA,NA
12.11 ,NA,NA
9.64 ,NA,NA
4.209459 ,NA,NA
2.764706 ,NA,NA
106.52 ,NA,NA
1/11/80 ,NA,NA
11.94 ,NA,NA
9.73 ,NA,NA
1.649573 ,NA,NA
21.28571 ,NA,NA
109.92 ,NA,NA
1/18/80 ,NA,NA
11.9 ,NA,NA
9.8 ,NA,NA
0.881335 ,NA,NA
4.210526 ,NA,NA
111.07 ,NA,NA
1/25/80 ,NA,NA
12.19 ,NA,NA
9.93 ,NA,NA
0.793269 ,NA,NA
3.606061 ,NA,NA
113.61 ,NA,NA
2/1/80 ,NA,NA
12.04 ,NA,NA
10.2 ,NA,NA
1.16293 ,NA,NA
2.088235 ,NA,NA
115.12 ,NA,NA
2/8/80 ,NA,NA
12.09 ,NA,NA
10.48 ,NA,NA
1.338415 ,NA,NA
2.936508 ,NA,NA
117.95 ,NA,NA
2/15/80 ,NA,NA
12.31 ,NA,NA
10.96 ,NA,NA
0.338053 ,NA,NA
0.134615 ,NA,NA
115.41 ,NA,NA
2/22/80 ,NA,NA
13.16 ,NA,NA
11.25 ,NA,NA
0.32381 ,NA,NA
0.109091 ,NA,NA
115.04 ,NA,NA
2/29/80 ,NA,NA
13.7 ,NA,NA
12.14 ,NA,NA
1.676895 ,NA,NA
0.179245 ,NA,NA
113.66 ,NA,NA
3/7/80 ,NA,NA
15.14 ,NA,NA
12.1 ,NA,NA
0.282591 ,NA,NA
0 ,NA,NA
106.9 ,NA,NA
3/14/80 ,NA,NA
15.38 ,NA,NA
12.01 ,NA,NA
0.690286 ,NA,NA
0.011628 ,NA,NA
105.43 ,NA,NA
3/21/80 ,NA,NA
15.05 ,NA,NA
11.73 ,NA,NA
0.486267 ,NA,NA
0.027933 ,NA,NA
102.31 ,NA,NA
3/28/80 ,NA,NA
16.53 ,NA,NA
11.67 ,NA,NA
5.247191 ,NA,NA
0.011628 ,NA,NA
100.68 ,NA,NA
4/3/80 ,NA,NA
15.04 ,NA,NA
12.06 ,NA,NA
0.983562 ,NA,NA
0.117647 ,NA,NA
102.15 ,NA,NA
4/11/80 ,NA,NA
14.42 ,NA,NA
11.81 ,NA,NA
1.565854 ,NA,NA
0.310345 ,NA,NA
103.79 ,NA,NA
4/18/80 ,NA,NA
13.82 ,NA,NA
11.23 ,NA,NA
1.113287 ,NA,NA
0.146341 ,NA,NA
100.55 ,NA,NA
4/25/80 ,NA,NA
12.73 ,NA,NA
10.59 ,NA,NA
0.849807 ,NA,NA
0.473684 ,NA,NA
105.16 ,NA,NA
5/2/80 ,NA,NA
10.79 ,NA,NA
10.42 ,NA,NA
1.147465 ,NA,NA
1.857143 ,NA,NA
105.58 ,NA,NA
5/9/80 ,NA,NA
9.73 ,NA,NA
10.15 ,NA,NA
0.513052 ,NA,NA
0.473684 ,NA,NA
104.72 ,NA,NA
5/16/80 ,NA,NA
8.6 ,NA,NA
9.7 ,NA,NA
1.342444 ,NA,NA
6.75 ,NA,NA
107.35 ,NA,NA
5/23/80 ,NA,NA
8.95 ,NA,NA
9.87 ,NA,NA
3.110825 ,NA,NA
26 ,NA,NA
110.62 ,NA,NA
Highlight Features in the Data,NA,NA
"For each of the five inputs, we want use a function to highlight rate of change types of ",NA,NA
features. We will use the following function (as originally proposed by Jurik) for this ,NA,NA
purpose. ,NA,NA
ROC(n) = (input(t) - BA(t - n)) / (input(t)+ BA(t - n)),NA,NA
where: input(t) is the input’s current value and BA(,NA,NA
t,NA,NA
 - ,NA,NA
n,NA,NA
) is a five unit block average of ,NA,NA
adjacent values centered around the value ,NA,NA
n,NA,NA
 periods ago.,NA,NA
Now we need to decide how many of these features we need. Since we are making a ,NA,NA
"prediction 10 weeks into the future, we will take data as far back as 10 weeks also. This ",NA,NA
"will be ROC(10). We will also use one other rate of change, ROC(3). We have now added ",NA,NA
"5*2 = 10 inputs to our network, for a total of 15. All of the preprocessing can be done with ",NA,NA
a spreadsheet.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Here’s what we get (Table 14.2) after doing the block averages. Example : BA3MoBills ,NA,NA
for 1/18/80 = (3MoBills(1/4/80) + 3MoBills(1/11/80) + 3MoBills(1/18/80) + ,NA,NA
3MoBills(1/25/80) + 3MoBills(2/1/80))/5. ,NA,NA
Table 14.2,NA,NA
Data after Doing Block Averages ,NA,NA
Date ,NA,NA
3MoBills ,NA,NA
LngBonds ,NA,NA
NYSE- ,NA,NA
1/4/80 ,NA,NA
12.11 ,NA,NA
9.64 ,NA,NA
4.209459 ,NA,NA
1/11/80 ,NA,NA
11.94 ,NA,NA
9.73 ,NA,NA
1.649573 ,NA,NA
1/18/80 ,NA,NA
11.9 ,NA,NA
9.8 ,NA,NA
0.881335 ,NA,NA
1/25/80 ,NA,NA
12.19 ,NA,NA
9.93 ,NA,NA
0.793269 ,NA,NA
2/1/80 ,NA,NA
12.04 ,NA,NA
10.2 ,NA,NA
1.16293 ,NA,NA
2/8/80 ,NA,NA
12.09 ,NA,NA
10.48 ,NA,NA
1.338415 ,NA,NA
2/15/80 ,NA,NA
12.31 ,NA,NA
10.96 ,NA,NA
0.338053 ,NA,NA
2/22/80 ,NA,NA
13.16 ,NA,NA
11.25 ,NA,NA
0.32381 ,NA,NA
2/29/80 ,NA,NA
13.7 ,NA,NA
12.14 ,NA,NA
1.676895 ,NA,NA
3/7/80 ,NA,NA
15.14 ,NA,NA
12.1 ,NA,NA
0.282591 ,NA,NA
3/14/80 ,NA,NA
15.38 ,NA,NA
12.01 ,NA,NA
0.690286 ,NA,NA
3/21/80 ,NA,NA
15.05 ,NA,NA
11.73 ,NA,NA
0.486267 ,NA,NA
3/28/80 ,NA,NA
16.53 ,NA,NA
11.67 ,NA,NA
5.247191 ,NA,NA
4/3/80 ,NA,NA
15.04 ,NA,NA
12.06 ,NA,NA
0.983562 ,NA,NA
4/11/80 ,NA,NA
14.42 ,NA,NA
11.81 ,NA,NA
1.565854 ,NA,NA
4/18/80 ,NA,NA
13.82 ,NA,NA
11.23 ,NA,NA
1.113287 ,NA,NA
4/25/80 ,NA,NA
12.73 ,NA,NA
10.59 ,NA,NA
0.849807 ,NA,NA
5/2/80 ,NA,NA
10.79 ,NA,NA
10.42 ,NA,NA
1.147465 ,NA,NA
5/9/80 ,NA,NA
9.73 ,NA,NA
10.15 ,NA,NA
0.513052 ,NA,NA
5/16/80 ,NA,NA
8.6 ,NA,NA
9.7 ,NA,NA
1.342444 ,NA,NA
5/23/80 ,NA,NA
8.95 ,NA,NA
9.87 ,NA,NA
3.110825 ,NA,NA
NYSE-Adv/Dec ,NA,NA
SP-Close ,NA,NA
NewH/NewL ,NA,NA
BA3MoB ,NA,NA
BALngBnd ,NA,NA
2.764706 ,NA,NA
106.52 ,NA,NA
12.036 ,NA,NA
9.86 ,NA,NA
21.28571 ,NA,NA
109.92 ,NA,NA
4.210526 ,NA,NA
111.07 ,NA,NA
3.606061 ,NA,NA
113.61 ,NA,NA
12.032 ,NA,NA
10.028 ,NA,NA
2.088235 ,NA,NA
115.12 ,NA,NA
12.106 ,NA,NA
10.274 ,NA,NA
2.936508 ,NA,NA
117.95 ,NA,NA
12.358 ,NA,NA
10.564 ,NA,NA
0.134615 ,NA,NA
115.41 ,NA,NA
12.66 ,NA,NA
11.006 ,NA,NA
0.109091 ,NA,NA
115.04 ,NA,NA
13.28 ,NA,NA
11.386 ,NA,NA
0.179245 ,NA,NA
113.66 ,NA,NA
13.938 ,NA,NA
11.692 ,NA,NA
0 ,NA,NA
106.9 ,NA,NA
14.486 ,NA,NA
11.846 ,NA,NA
0.011628 ,NA,NA
105.43 ,NA,NA
15.16 ,NA,NA
11.93 ,NA,NA
0.027933 ,NA,NA
102.31 ,NA,NA
15.428 ,NA,NA
11.914 ,NA,NA
0.011628 ,NA,NA
100.68 ,NA,NA
15.284 ,NA,NA
11.856 ,NA,NA
0.117647 ,NA,NA
102.15 ,NA,NA
14.972 ,NA,NA
11.7 ,NA,NA
0.310345 ,NA,NA
103.79 ,NA,NA
14.508 ,NA,NA
11.472 ,NA,NA
0.146341 ,NA,NA
100.55 ,NA,NA
13.36 ,NA,NA
11.222 ,NA,NA
0.473684 ,NA,NA
105.16 ,NA,NA
12.298 ,NA,NA
10.84 ,NA,NA
1.857143 ,NA,NA
105.58 ,NA,NA
11.134 ,NA,NA
10.418 ,NA,NA
0.473684 ,NA,NA
104.72 ,NA,NA
10.16 ,NA,NA
10.146 ,NA,NA
6.75 ,NA,NA
107.35 ,NA,NA
7.614 ,NA,NA
8.028 ,NA,NA
26 ,NA,NA
110.62 ,NA,NA
5.456 ,NA,NA
5.944 ,NA,NA
BAA/D ,NA,NA
BAH/L ,NA,NA
BAClose ,NA,NA
1.739313 ,NA,NA
6.791048 ,NA,NA
111.248 ,NA,NA
1.165104 ,NA,NA
6.825408 ,NA,NA
113.534 ,NA,NA
0.9028 ,NA,NA
2.595189 ,NA,NA
114.632 ,NA,NA
0.791295 ,NA,NA
1.774902 ,NA,NA
115.426 ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/393-394.html (2 of 3) [21/11/02 21:58:16],NA
0.968021 ,NA,NA
1.089539 ,NA,NA
115.436 ,NA,NA
0.791953 ,NA,NA
0.671892 ,NA,NA
113.792 ,NA,NA
0.662327 ,NA,NA
0.086916 ,NA,NA
111.288 ,NA,NA
0.69197 ,NA,NA
0.065579 ,NA,NA
108.668 ,NA,NA
1.676646 ,NA,NA
0.046087 ,NA,NA
105.796 ,NA,NA
1.537979 ,NA,NA
0.033767 ,NA,NA
103.494 ,NA,NA
1.794632 ,NA,NA
0.095836 ,NA,NA
102.872 ,NA,NA
1.879232 ,NA,NA
0.122779 ,NA,NA
101.896 ,NA,NA
1.95194 ,NA,NA
0.211929 ,NA,NA
102.466 ,NA,NA
1.131995 ,NA,NA
0.581032 ,NA,NA
103.446 ,NA,NA
1.037893 ,NA,NA
0.652239 ,NA,NA
103.96 ,NA,NA
0.993211 ,NA,NA
1.94017 ,NA,NA
104.672 ,NA,NA
1.392719 ,NA,NA
7.110902 ,NA,NA
106.686 ,NA,NA
1.222757 ,NA,NA
7.016165 ,NA,NA
85.654 ,NA,NA
0.993264 ,NA,NA
6.644737 ,NA,NA
64.538 ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
"Now let’s look at the rest of this table, which is made up of the new 10 values of ROC ",NA,NA
indicators (Table 14.3). ,NA,NA
Table 14.3,NA,NA
Added Rate of Change (ROC) Indicators ,NA,NA
Date ,NA,NA
ROC3_3Mo ROC3_Bond ROC10_AD ROC3_H/L ,NA,NA
ROC3_SPC ,NA,NA
1/4/80 ,NA,NA
1/11/80 ,NA,NA
1/18/80 ,NA,NA
1/25/80 ,NA,NA
2/1/80 ,NA,NA
2/8/80 ,NA,NA
0.002238 ,NA,NA
0.030482 ,NA,NA
-0.13026 ,NA,NA
-0.39625 ,NA,NA
0.029241 ,NA,NA
2/15/80 ,NA,NA
0.011421 ,NA,NA
0.044406 ,NA,NA
-0.55021 ,NA,NA
-0.96132 ,NA,NA
0.008194 ,NA,NA
2/22/80 ,NA,NA
0.041716 ,NA,NA
0.045345 ,NA,NA
-0.47202 ,NA,NA
-0.91932 ,NA,NA
0.001776 ,NA,NA
2/29/80 ,NA,NA
0.0515 ,NA,NA
0.069415 ,NA,NA
0.358805 ,NA,NA
-0.81655 ,NA,NA
-0.00771 ,NA,NA
3/7/80 ,NA,NA
0.089209 ,NA,NA
0.047347 ,NA,NA
-0.54808 ,NA,NA
-1 ,NA,NA
-0.03839 ,NA,NA
3/14/80 ,NA,NA
0.073273 ,NA,NA
0.026671 ,NA,NA
-0.06859 ,NA,NA
-0.96598 ,NA,NA
-0.03814 ,NA,NA
3/21/80 ,NA,NA
0.038361 ,NA,NA
0.001622 ,NA,NA
-0.15328 ,NA,NA
-0.51357 ,NA,NA
-0.04203 ,NA,NA
3/28/80 ,NA,NA
0.065901 ,NA,NA
-0.00748 ,NA,NA
0.766981 ,NA,NA
-0.69879 ,NA,NA
-0.03816 ,NA,NA
4/3/80 ,NA,NA
-0.00397 ,NA,NA
0.005419 ,NA,NA
-0.26054 ,NA,NA
0.437052 ,NA,NA
-0.01753 ,NA,NA
4/11/80 ,NA,NA
-0.03377 ,NA,NA
-0.00438 ,NA,NA
0.008981 ,NA,NA
0.437052 ,NA,NA
-0.01753 ,NA,NA
4/18/80 ,NA,NA
-0.0503 ,NA,NA
-0.02712 ,NA,NA
-0.23431 ,NA,NA
0.803743 ,NA,NA
0.001428 ,NA,NA
4/25/80 ,NA,NA
-0.08093 ,NA,NA
-0.0498 ,NA,NA
-0.37721 ,NA,NA
0.58831 ,NA,NA
0.015764 ,NA,NA
5/2/80 ,NA,NA
-0.14697 ,NA,NA
-0.04805 ,NA,NA
-0.25956 ,NA,NA
0.795146 ,NA,NA
0.014968 ,NA,NA
5/9/80 ,NA,NA
-0.15721 ,NA,NA
-0.05016 ,NA,NA
-0.37625 ,NA,NA
-0.10178 ,NA,NA
0.00612 ,NA,NA
5/16/80 ,NA,NA
-0.17695 ,NA,NA
-0.0555 ,NA,NA
0.127944 ,NA,NA
0.823772 ,NA,NA
0.016043 ,NA,NA
5/23/80 ,NA,NA
-0.10874 ,NA,NA
-0.02701 ,NA,NA
0.515983 ,NA,NA
0.86112 ,NA,NA
0.027628 ,NA,NA
ROC10_3Mo ROC10_Bnd ROC10_AD ROC10_HL ROC10_SP ,NA,NA
0.15732 ,NA,NA
0.084069 ,NA,NA
0.502093 ,NA,NA
-0.99658 ,NA,NA
-0.04987 ,NA,NA
0.111111 ,NA,NA
0.091996 ,NA,NA
-0.08449 ,NA,NA
-0.96611 ,NA,NA
-0.05278 ,NA,NA
0.087235 ,NA,NA
0.069553 ,NA,NA
0.268589 ,NA,NA
-0.78638 ,NA,NA
-0.04964 ,NA,NA
0.055848 ,NA,NA
0.030559 ,NA,NA
0.169062 ,NA,NA
-0.84766 ,NA,NA
-0.06888 ,NA,NA
0.002757 ,NA,NA
-0.01926 ,NA,NA
-0.06503 ,NA,NA
-0.39396 ,NA,NA
-0.04658 ,NA,NA
-0.10345 ,NA,NA
-0.0443 ,NA,NA
0.183309 ,NA,NA
0.468658 ,NA,NA
-0.03743 ,NA,NA
-0.17779 ,NA,NA
-0.0706 ,NA,NA
-0.127 ,NA,NA
0.689919 ,NA,NA
-0.03041 ,NA,NA
-0.25496 ,NA,NA
-0.0996 ,NA,NA
0.319735 ,NA,NA
0.980756 ,NA,NA
-0.0061 ,NA,NA
-0.25757 ,NA,NA
-0.0945 ,NA,NA
0.299569 ,NA,NA
0.996461 ,NA,NA
0.02229 ,"NOTE:  
 Note that you don’t get completed rows until 3/28/90, since we have a ROC 
 indicator dependent on a Block Average value 10 weeks before it. The first block average 
 value is generated 1/1/80, two weeks after the start of the data set. All of this indicates that 
 you will need to discard the first 12 values in the dataset to get complete rows, also called 
 complete 
 facts
 .",NA
Normalizing the Range,NA,NA
We now have values in the original five data columns that have a very large range. We ,NA,NA
would like to reduce the range by some method. We use the following function: ,NA,NA
new value = (old value - Mean)/ (Maximum Range),NA,NA
This relates the distance from the mean for a value in a column as a fraction of the ,NA,NA
Maximum range for that column. You should note the value of the Maximum range and ,NA,NA
"Mean, so that you can un-normalize the data when you get a result.",NA,NA
The Target,NA,NA
"We’ve taken care of all our inputs, which number 15. The final piece of information is the ",NA,NA
target. The objective as stated at the beginning of this exercise is to predict the percentage ,NA,NA
"change 10 weeks into the future. We need to time shift the S&P 500 close 10 weeks back, ",NA,NA
and then calculate the value as a percentage change as follows: ,NA,NA
Result = 100 X ((S&P 10 weeks ahead) - (S&P this week))/(S&P this week).,NA,NA
This gives us a value that varies between -14.8 to and + 33.7. This is not in the form we ,NA,NA
"need yet. As you recall, the output comes from a ",NA,NA
sigmoid,NA,NA
 function that is restricted to 0 to ,NA,NA
+1. We will first add 14.8 to all values and then scale them by a factor of 0.02. This will ,NA,NA
result in a scaled target that varies from 0 to 1.,NA,NA
scaled target = (result + 14.8) X 0.02,NA,NA
The final data file with the scaled target shown along with the scaled original six columns ,NA,NA
of data is shown in Table 14.4.,NA,NA
Table 14.4,NA,NA
Normalized Ranges for Original Columns and Scaled Target ,NA,NA
Date ,NA,NA
S_3MOBill ,NA,NA
S_LngBnd ,NA,NA
S_A/D ,NA,NA
3/28/80 ,NA,NA
0.534853 ,NA,NA
-0.01616 ,NA,NA
0.765273 ,NA,NA
4/3/80 ,NA,NA
0.391308 ,NA,NA
0.055271 ,NA,NA
-0.06356 ,NA,NA
4/11/80 ,NA,NA
0.331578 ,NA,NA
0.009483 ,NA,NA
0.049635 ,NA,NA
4/18/80 ,NA,NA
0.273774 ,NA,NA
-0.09674 ,NA,NA
-0.03834 ,NA,NA
4/25/80 ,NA,NA
0.168765 ,NA,NA
-0.21396 ,NA,NA
-0.08956 ,NA,NA
5/2/80 ,NA,NA
-0.01813 ,NA,NA
-0.2451 ,NA,NA
-0.0317 ,NA,NA
5/9/80 ,NA,NA
-0.12025 ,NA,NA
-0.29455 ,NA,NA
-0.15503 ,NA,NA
5/16/80 ,NA,NA
-0.22912 ,NA,NA
-0.37696 ,NA,NA
0.006205 ,NA,NA
5/23/80 ,NA,NA
-0.1954 ,NA,NA
-0.34583 ,NA,NA
0.349971 ,NA,NA
S_H/L ,NA,NA
S_SPC ,NA,NA
Result ,NA,NA
Scaled Target ,NA,NA
-0.07089 ,NA,NA
-0.51328 ,NA,NA
12.43544 ,NA,NA
0.544709 ,NA,NA
-0.07046 ,NA,NA
-0.49236 ,NA,NA
12.88302 ,NA,NA
0.55366 ,NA,NA
-0.06969 ,NA,NA
-0.46901 ,NA,NA
9.89498 ,NA,NA
0.4939 ,NA,NA
-0.07035 ,NA,NA
-0.51513 ,NA,NA
15.36549 ,NA,NA
0.60331 ,NA,NA
-0.06903 ,NA,NA
-0.44951 ,NA,NA
11.71548 ,NA,NA
0.53031 ,NA,NA
-0.06345 ,NA,NA
-0.44353 ,NA,NA
11.61205 ,NA,NA
0.528241 ,NA,NA
-0.06903 ,NA,NA
-0.45577 ,NA,NA
16.53934 ,NA,NA
0.626787 ,NA,NA
-0.04372 ,NA,NA
-0.41833 ,NA,NA
12.51048 ,NA,NA
0.54621 ,NA,NA
0.033901 ,NA,NA
-0.37179 ,NA,NA
9.573314 ,NA,NA
0.487466 ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Storing Data in Different Files,NA,NA
You need to place the first 200 lines in a training.dat file (provided for you in the ,NA,NA
accompanying diskette) and the subsequent 40 lines of data in the another test.dat file for ,NA,NA
use in testing. You will read more about this shortly. There is also more data than this ,NA,NA
provided on this diskette in raw form for you to do further experiments. ,NA,NA
Training and Testing,NA,NA
"With the training data available, we set up a simulation. The number of inputs are 15, and ",NA,NA
the number of outputs is 1. A total of three layers are used with a middle layer of size 5. ,NA,NA
This number should be made as small as possible with acceptable results. The optimum ,NA,NA
"sizes and number of layers can only be found by much trial and error. After each run, you ",NA,NA
can look at the error from the training set and from the test set. ,NA,NA
Using the Simulator to Calculate Error,NA,NA
You obtain the error for the test set by running the simulator in ,NA,NA
Training,NA,NA
 mode (you need ,NA,NA
to temporarily copy the test data with expected outputs to the training.dat file) for one cycle ,NA,NA
"with weights loaded from the weights file. Since this is the last and only cycle, weights do ",NA,NA
"not get modified, and you can get a reading of the average error. Refer to Chapter 13 for ",NA,NA
more information on the simulator’s ,NA,NA
Test,NA,NA
 and ,NA,NA
Training,NA,NA
 modes. This approach has been ,NA,NA
taken with five runs of the simulator for 500 cycles each. Table 14.5 summarizes the results ,NA,NA
along with the parameters used. The error gets better and better with each run up to run # 4. ,NA,NA
"For run #5, the training set error decreases, but the test set error increases, indicating the ",NA,NA
"onset of memorization. Run # 4 is used for the final network results, showing test set RMS ",NA,NA
error of 13.9% and training set error of 6.9%.,NA,NA
Table 14.5,NA,NA
Results of Training the Backpropagation Simulator for Predicting the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/396-399.html (1 of 4) [21/11/02 21:58:18],NA
Percentage Change in the S&P 500 Index ,NA,NA
Run# ,NA,NA
Tolerance ,NA,NA
Beta ,NA,NA
Alpha NF ,NA,NA
max ,NA,NA
cycles ,NA,NA
cycles ,NA,NA
run ,NA,NA
training set ,NA,NA
error ,NA,NA
test set ,NA,NA
error ,NA,NA
1 ,NA,NA
0.001 ,NA,NA
0.5 ,NA,NA
0.001 ,NA,NA
0.0005 500 ,NA,NA
500 ,NA,NA
0.150938 ,NA,NA
0.25429 ,NA,NA
2 ,NA,NA
0.001 ,NA,NA
0.4 ,NA,NA
0.001 ,NA,NA
0.0005 500 ,NA,NA
500 ,NA,NA
0.114948 ,NA,NA
0.185828 ,NA,NA
3 ,NA,NA
0.001 ,NA,NA
0.3 ,NA,NA
0 ,NA,NA
0 ,NA,NA
500 ,NA,NA
500 ,NA,NA
0.0936422 ,NA,NA
0.148541 ,NA,NA
4 ,NA,NA
0.001 ,NA,NA
0.2 ,NA,NA
0 ,NA,NA
0 ,NA,NA
500 ,NA,NA
500 ,NA,NA
0.068976 ,NA,NA
0.139230 ,NA,NA
5 ,NA,NA
0.001 ,NA,NA
0.1 ,NA,NA
0 ,NA,NA
0 ,NA,NA
500 ,NA,NA
500 ,NA,NA
0.0621412 ,NA,NA
0.143430 ,"NOTE:  
 If you find the test set error does not decrease much, whereas the training set 
 error continues to make substantial progress, then this means that memorization is 
 starting to set in (run#5 in example). It is important to monitor the test set(s) that you are 
 using while you are training to make sure that good, generalized learning is occurring 
 versus memorizing or overfitting the data. In the case shown, the test set error continued 
 to improve until run#5, where the test set error degraded. You need to revisit the 12-step 
 process to forecasting model design to make any further improvements beyond what was 
 achieved.",NA
"To see the exact correlation, you can copy any period you’d like, with the expected value ",NA,NA
"output fields deleted, to the test.dat file. Then you run the simulator in ",NA,NA
Test,NA,NA
 mode and get ,NA,NA
the output value from the simulator for each input vector. You can then compare this with ,NA,NA
the expected value in your training set or test set.,NA,NA
"Now that you’re done, you need to un-normalize the data back to get the answer in terms of ",NA,NA
the change in the S&P 500 index. What you’ve accomplished is a way in which you can get ,NA,NA
"data from a financial newspaper, like ",NA,NA
Barron’s or Investor’s Business Daily,NA,NA
", and feed the ",NA,NA
current week’s data into your trained neural network to get a prediction of what the S&P ,NA,NA
500 index is likely to do ten weeks from now.,NA,NA
Here are the steps to un-normalize:,NA,NA
1.,NA,NA
"  Take the predicted scaled target value and calculate, the result value as ",NA,NA
Result = ,NA,NA
(Scaled target/0.02) - 14.8 ,NA,NA
2.,NA,NA
  Take the result from step 1 (which is the percentage change 10 weeks from now) ,NA,NA
"and calculate the projected value, ",NA,NA
Projected S&P 10 weeks from now = (This ,NA,NA
week’s S&P value)(1+ Result/100),NA,NA
Only the Beginning,NA,NA
This is only a very brief illustration (not meant for trading !) of what you can do with ,NA,NA
"neural networks in financial forecasting. You need to further analyze the data, provide more ",NA,NA
"predictive indicators, and optimize/redesign your neural network architecture to get better ",NA,NA
"generalization and lower error. You need to present many, many more test cases ",NA,NA
representing different market conditions to have a robust predictor that can be traded with. ,NA,NA
A graph of the expected and predicted output for the test set and the training set is shown in ,NA,NA
"Figure 14.6. Here, the normalized values are used for the output. Note that the error is ",NA,NA
about 13.9% on average over the test set and 6.9% over the training set. You can see that ,NA,NA
"the test set did well in the beginning, but showed great divergence in the last few weeks. ",NA,NA
Figure 14.6,NA,NA
  Comparison of predicted versus actual for the training and test data sets,NA,NA
.,NA,NA
The preprocessing steps shown in this chapter should serve as one example of the kinds of ,NA,NA
steps you can use. There are a vast variety of analysis and statistical methods that can be ,NA,NA
"used in preprocessing. For applying fuzzy data, you can use a program like the fuzzifier ",NA,NA
program that was developed in Chapter 3 to preprocess some of the data. ,NA,NA
What’s Next?,NA,NA
There are many other experiments you can do from here on. The example chosen was in the ,NA,NA
field of financial forecasting. But you could certainly try the simulator on other problems ,NA,NA
like sales forecasting or perhaps even weather forecasting. The key to all of the applications ,NA,NA
"though, is how you present and enhance data, and working through parameter selection by ",NA,NA
"trial and error. Before concluding this chapter, we will cover more topics in preprocessing ",NA,NA
and present some case studies in financial forecasting. You should consider the suggestions ,NA,NA
made in the 12-step approach to forecasting model design and research some of the ,NA,NA
resources listed at the end of this chapter if you have more interest in this area. ,NA,NA
Previous Table of Contents Next,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/396-399.html (3 of 4) [21/11/02 21:58:18],NA
Technical Analysis and Neural Network Preprocessing,"We cannot overstate the importance of preprocessing in developing a forecasting model. There is a large body 
 of information related to the study of financial market behavior called Technical Analysis. You can use the 
 mathematical studies defined by Technical Analysis to preprocess your input data to reveal predictive features. 
  
 We will present a sampling of Technical Analysis studies that can be used, with formulae and graphs. 
  
 Moving Averages
  
 Moving averages are used very widely to capture the underlying trend of a price move. Moving averages are 
 simple filters that average data over a moving window. Popular moving averages include 5-, 10-, and 20 period 
 moving averages. The formula is shown below for a simple moving average, SMA: 
  
  SMA
 t
  = ( P
 t
  + P
 t-1
  + ...  P
 t-n
 )/ n
  
  where  n = the number of time periods back
  
  P
 -n
 = price at n time periods back
  
 An exponential moving average is a weighted moving average that places more weight on the most recent data. 
 The formula for this indicator, EMA is as follows: 
  
  EMA
 t
  = (1 - a)P
 t
  + a ( EMA
 t-1
 )
  
  where  a = smoothing constant  (typical 0.10)
  
  P
 t
 = price at time t
  
 Momentum and Rate of Change
  
 Momentum is really velocity, or rate of price change with time. The formula for this is 
  
  M
 t
  =  ( P
 t
   -  P
 t-a
   )
  
  where  a = lookback parameter
  
  for a 5-day momentum value, a = 5
  
 The Rate of Change indicator is actually a ratio. It is the current price divided by the price some interval, 
 a
 , ago 
 divided by a constant. Specifically,
  
  ROC = P
 t
  / P
 t-a
   x 1000",NA
Previous Table of Contents Next,NA,NA
On-Balance Volume ,NA,NA
The on-balance volume (OBV) indicator was created to try to uncover accumulation and ,NA,NA
distribution patterns of large player in the stock market. This is a cumulative sum of ,NA,NA
"volume data, specified as follows: ",NA,NA
If today’s close is greater than yesterday’s close ,NA,NA
OBV,t,NA
 = OBV,t-1,NA
 + 1 ,NA,NA
If today’s close is less than yesterday’s close ,NA,NA
OBV,t,NA
 = OBV,t-1,NA
 - 1 ,NA,NA
The absolute value of the index is not important; attention is given only to the direction ,NA,NA
and trend.,NA,NA
Accumulation-Distribution ,NA,NA
This indicator does for price what OBV does for volume. ,NA,NA
If today’s close is greater than yesterday’s close: ,NA,NA
AD,t,NA
 = AD,t-1,NA
 + (Close,t,NA
 - Low,t,NA
) ,NA,NA
If today’s close is less than yesterday’s close ,NA,NA
AD,t,NA
 = AD,t-1,NA
 + (High,t,NA
 - Close,t,NA
),file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/402-406.html (1 of 4) [21/11/02 21:58:21],NA
Now let’s examine how these indicators look. Figure 14.7 shows a ,NA,NA
"bar chart,",NA,NA
 which is a ,NA,NA
"chart of price data versus time, along with the following indicators:",NA,NA
•,NA,NA
  Ten-unit moving average ,NA,NA
•,NA,NA
  Ten-unit exponential moving average ,NA,NA
•,NA,NA
  Momentum ,NA,NA
•,NA,NA
  MACD ,NA,NA
•,NA,NA
  Percent R ,NA,NA
Figure 14.7,NA,NA
  Five minute bar chart of the S&P 500 Sept 95 Futures contract with several ,NA,NA
technical indicators displayed.,NA,NA
The time period shown is 5 minute bars for the S&P 500 September 1995 Futures contract. ,NA,NA
"The top of each bar indicates the highest value (“high”) for that time interval, the bottom ",NA,NA
"indicates the lowest value(“low”), and the horizontal lines on the bar indicate the initial ",NA,NA
(“open”) and final (“close”) values for the time interval. ,NA,NA
Figure 14.8 shows another bar chart for Intel Corporation stock for the period from ,NA,NA
"December 1994 to July 1995, with each bar representing a day of activity. The following ",NA,NA
indicators are displayed also.,NA,NA
•,NA,NA
  Rate of Change ,NA,NA
•,NA,NA
  Relative Strength ,NA,NA
•,NA,NA
  Stochastics ,NA,NA
•,NA,NA
  Accumulation-Distribution ,NA,NA
Figure 14.8,NA,NA
  Daily bar chart of Intel Corporation with several technical indicators ,NA,NA
displayed.,NA,NA
You have seen a few of the hundreds of technical indicators that have been invented to ,NA,NA
date. New indicators are being created rapidly as the field of Technical Analysis gains ,NA,NA
"popularity and following. There are also pattern recognition studies, such as formations that ",NA,NA
"resemble flags or pennants as well as more exotic types of studies, like ",NA,NA
Elliot wave ,NA,NA
counts. ,NA,NA
"You can refer to books on Technical Analysis (e.g., Murphy) for more information about ",NA,NA
these and other studies.,NA,NA
Neural preprocessing with Technical Analysis tools as well as with traditional engineering ,NA,NA
"analysis tools such as Fourier series, Wavelets, and Fractals can be very useful in finding ",NA,NA
predictive patterns for forecasting.,NA,NA
What Others Have Reported,NA,NA
"In this final section of the chapter, we outline some case studies documented in periodicals ",NA,NA
"and books, to give you an idea of the successes or failures to date with neural networks in ",NA,NA
financial forecasting. Keep in mind that the very best (= most profitable) results are usually ,NA,NA
"never reported (so as not to lose a competitive edge) ! Also, remember that the market ",NA,NA
inefficiencies exploited yesterday may no longer be the same to exploit today. ,NA,NA
Can a Three-Year-Old Trade Commodities?,NA,NA
"Well, Hillary Clinton can certainly trade commodities, but a three-year-old, too? In his ",NA,NA
"paper, “Commodity Trading with a Three Year Old,” J. E. Collard describes a neural ",NA,NA
network with the supposed intelligence of a three-year-old. The application used a ,NA,NA
feedforward backpropagation network with a 37-30-1 architecture. The network was ,NA,NA
trained to buy (“go long”) or sell (“go short”) in the live cattle commodity futures market. ,NA,NA
"The training set consisted of 789 facts for trading days in 1988, 1989, 1990, and 1991. ",NA,NA
Each input vector consisted of 18 fundamental indicators and six market technical variables ,NA,NA
"(Open, High, Low, Close, Open Interest, Volume). The network could be trained for the ",NA,NA
correct output on all but 11 of the 789 facts. ,NA,NA
The fully trained network was used on 178 subsequent trading days in 1991. The ,NA,NA
cumulative profit increased from $0 to $1547.50 over this period by trading one live cattle ,NA,NA
contract. The largest loss in a trade was $601.74 and the largest gain in a trade was ,NA,NA
$648.30.,NA,NA
Forecasting Treasury Bill and Treasury Note Yields,NA,NA
Milam Aiken designed a feedforward backpropagation network that predicted Treasury ,NA,NA
Bill Rates and compared the forecast he obtained with forecasts made by top U.S. ,NA,NA
"economists. The results showed the neural network, given the same data, made better ",NA,NA
predictions (.18 versus .71 absolute error). Aiken used 250 economic data series to see ,NA,NA
correlation to T-Bills and used only the series that showed leading correlation: Dept. of ,NA,NA
"Commerce Index of Leading Economic Indicators, the Center for International Business ",NA,NA
"Cycle Research (CIBCR) Short Leading Composite Index, and the CIBCR Long Leading ",NA,NA
Composite Index. Prior data for these three indicators for the past four years (total 12 ,NA,NA
inputs) was used to predict the average annual T-Bill rate (one output) for the current year. ,NA,NA
Guido Deboeck and Masud Cader designed profitable trading systems for two-year and 10-,NA,NA
year treasury securities. They used feedforward neural networks with a learning algorithm ,NA,NA
called ,NA,NA
extended-delta-bar-delta,NA,NA
 (,NA,NA
EDBD,NA,NA
"), which is a variant of backpropagation. Training ",NA,NA
samples composed of 100 facts were selected from 1120 trading days spanning from July 1 ,NA,NA
"1989 to June 30, 1992. The test period consisted of more than 150 trading days from July 1, ",NA,NA
"1992 to December 30, 1992. Performance on the test set was monitored every ",NA,NA
N ,NA,NA
"thousand training cycles, and the training procedure was stopped when performance ",NA,NA
degraded on the test set. (This is the same procedure we used when developing a model for ,NA,NA
the S&P 500.),NA,NA
A criterion used to judge model performance was the ratio of the average profit divided by ,NA,NA
the maximum ,NA,NA
"drawdown,",NA,NA
 which is the largest unrealized loss seen during the trading ,NA,NA
period. A portfolio of separate designed trading systems for two-year and 10-year securities ,NA,NA
"gave the following performance: Over a period of 4.5 years, the portfolio had 133 total ",NA,NA
trades with 65% profitable trades and the maximum drawdown of 64 ,NA,NA
"basis points,",NA,NA
 or ,NA,NA
thousands of units for bond yields. The total gain was 677 basis points over that period with ,NA,NA
a maximum gain in one trade of 52 basis points and maximum loss in one trade of 47 basis ,NA,NA
points.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/402-406.html (4 of 4) [21/11/02 21:58:21]",NA
Previous Table of Contents Next,NA,NA
The stability and robustness of this system was checked by using over 1000 moving time windows of 3-,NA,NA
"month, 6-month, and 12-month duration over the 4.5-year interval and noting the standard deviations in ",NA,NA
profits and maximum drawdown. The maximum drawdown varied from 30 to 48 basis points. ,NA,NA
Neural Nets versus Box-Jenkins Time-Series Forecasting,NA,NA
Ramesh Sharda and Rajendra Patil used a standard 12-12-1 feedforward backpropagation network and ,NA,NA
compared the results with Box-Jenkins methodology for time-series forecasting. Box-Jenkins forecasting ,NA,NA
is traditional time-series forecasting technique. The authors used 75 different time series for evaluation. ,NA,NA
The results showed that neural networks achieved better MAPE (mean absolute percentage error) with a ,NA,NA
mean over all 75 time series MAPEs of 14.67 versus 15.94 for the Box-Jenkins approach. ,NA,NA
Neural Nets versus Regression Analysis,NA,NA
Leorey Marquez et al. compared neural network modeling with standard regression analysis. The authors ,NA,NA
used a feedforward backpropagation network with a structure of 1-6-1. They used three functional forms ,NA,NA
found in regression analysis: ,NA,NA
1.,NA,NA
  Y = B0 + B1 X + e ,NA,NA
2.,NA,NA
  Y = B0 + B1 log(X) + e ,NA,NA
3.,NA,NA
  Y = B0 + B1/X + e ,NA,NA
"For each of these forms, 100 pairs of (x,y) data were generated for this “true” model. ",NA,NA
Now the neural network was trained on these 100 pairs of data. An additional 100 data points were ,NA,NA
generated by the network to test the forecasting ability of the network. The results showed that the neural ,NA,NA
"network achieved a MAPE within 0.6% of the true model, which is a very good result. The neural ",NA,NA
network model approximated the linear model best. An experiment was also done with intentional mis-,NA,NA
"specification of some data points. The neural network model did well in these cases also, but ",NA,NA
comparatively worse for the reciprocal model case.,NA,NA
Hierarchical Neural Network,NA,NA
Mendelsohn developed a multilevel neural network as shown in Figure 14.9. Here five neural networks ,NA,NA
are arranged such that four network outputs feed that final network. The four networks are trained to ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/406-409.html (1 of 3) [21/11/02 21:58:22],NA
"produce the High, Low, short-term trend, and medium-term trend for a particular financial instrument. ",NA,NA
The final network takes these four outputs as input and produces a turning point indicator. ,NA,NA
Figure 14.9,NA,NA
  Hierarchical neural network system to predict turning points.,NA,NA
Each network was trained and tested with 1200 fact days spanning 1988 to 1992 (33% used for testing). ,NA,NA
Preprocessing was accomplished by using differences of the inputs and with some technical analysis ,NA,NA
studies: ,NA,NA
•,NA,NA
  Moving averages ,NA,NA
•,NA,NA
  Exponential moving averages ,NA,NA
•,NA,NA
  Stochastic indicators ,NA,NA
"For the network that produces a predicted High value, the average error ranged between 7.04% and 7.65% ",NA,NA
"for various financial markets over the test period, including Treasury Bonds, Eurodollar, Japanese Yen, and ",NA,NA
S&P 500 futures contracts. ,NA,NA
The Walk-Forward Methodology of Market Prediction,NA,NA
A methodology that is sometimes used in neural network design is ,NA,NA
walk-forward,NA,NA
 training and testing. This ,NA,NA
"means that you choose an interval of time (e.g., six months) over which you train a neural network and test ",NA,NA
the network over a subsequent interval. You then move the training window and testing window forward ,NA,NA
"one month, for example, and repeat the exercise. You do this for the time period of interest to see your ",NA,NA
forecasting results. The advantage of this approach is that you maximize the network’s ability to model the ,NA,NA
recent past in making a prediction. The disadvantage is that the network forget characteristics of the market ,NA,NA
that happened prior to the training window.,NA,NA
Takashi Kimoto et al. used the walk forward methodology in designing a trading system for Fujitsu and ,NA,NA
"Nikko Securities. They also, like Mendelsohn, use a hierarchical neural network composed of individual ",NA,NA
"feedforward neural networks. Prediction of the TOPIX, which is the Japanese equivalent of the Dow Jones ",NA,NA
"Industrial Average, was performed for 33 months from January 1987 to September 1980. Four networks ",NA,NA
were used in the first level of the hierarchy trained on price data and economic data. The results were fed to ,NA,NA
a final network that generated buy and sell signals. The performance of the trading system achieved a result ,NA,NA
that is 20% better than a buy and hold strategy for the TOPIX.,NA,NA
Dual Confirmation Trading System,NA,NA
"Jeremy Konstenius, discusses a trading system for the S&P 400 index with a ",NA,NA
holographic neural network,NA,NA
", ",NA,NA
which is unlike the feedforward backpropagation neural network. The holographic network uses complex ,NA,NA
"numbers for data input and output from neurons, which are mathematically more complex than ",NA,NA
feedforward network neurons. The author uses two trained networks to forecast the next day’s direction ,NA,NA
based on data for the past 10 days. Each network uses input data that is ,NA,NA
"detrended,",NA,NA
 by subtracting a moving ,NA,NA
average from the data. Network 1 uses detrended closing values. Network 2 uses detrended High ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/406-409.html (2 of 3) [21/11/02 21:58:22],NA
"values. If both networks agree, or confirm each other, then a trade is made. There is no trade otherwise.",NA,NA
Network 1 showed an accuracy of 61.9% for the five-month test period (the training period spanned two ,NA,NA
"years prior to the test period), while Network 2 also showed an accuracy of 61.9%. Using the two ",NA,NA
"networks together, Konstenius achieved an accuracy of 65.82%.",NA,NA
A Turning Point Predictor,NA,NA
This neural network approach is discussed by Michitaka Kosaka et al. (1991). ,NA,NA
They discuss applying the feedforward backpropagation network to develop buy/sell signals for securities. ,NA,NA
"You would gather time-series data on stock prices, and want to find trends in the data so that changes in the ",NA,NA
"direction of the trend provide you the turning points, which you interpret as signals to buy or sell.",NA,NA
You will need to list these factors that you think have any influence on the price of a security you are ,NA,NA
studying. You need to also determine how you measure these factors. You then formulate a nonlinear ,NA,NA
function combining the factors on your list and the past however many prices of your security (your time ,NA,NA
series data).,NA,NA
"The function has the form, as Michitaka Kosaka, et al. (1991) put it,",NA,NA
" p(t + h) = F(x(t), x(t -1), ... , f",1,NA
", f",2,NA
", ... )",NA,NA
 where,NA,NA
 f,1,NA
", f",2,NA
", represent factors on your list,",NA,NA
" x(t) is the price of your stock at time t,",NA,NA
" p(t + h) is the turning point of security price at time t + h, and",NA,NA
" p(t + h) = -1 for a turn from downward to upward,",NA,NA
" p(t + h) = +1 for a turn from upward to downward,",NA,NA
 p(t + h) = 0 for no change and therefore no turn,NA,NA
Here you vary ,NA,NA
h,NA,NA
" through the values 1, 2, etc. as you move into the future one day (time period) at a time. ",NA,NA
Note that the detailed form of the function ,NA,NA
F,NA,NA
 is not given. This is for you to set up as you see fit.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
You can set up a similar function for ,NA,NA
x,NA,NA
(,NA,NA
t + h,NA,NA
),NA,NA
",",NA,NA
 the stock price at time ,NA,NA
t + h,NA,NA
", and have a ",NA,NA
separate network computing it using the backpropagation paradigm. You will then be ,NA,NA
"generating future prices of the stock and the future buy/sell signals hand in hand, but ",NA,NA
parallel.,NA,NA
"Michitaka Kosaka, et al. (1991) report that they used time-series data over five years to ",NA,NA
"identify the network model, and time-series data over one year to evaluate the model’s ",NA,NA
"forecasting performance, with a success rate of 65% for turning points.",NA,NA
The S&P 500 and Sunspot Predictions,NA,NA
Michael Azoff in his book on time-series forecasting with neural networks (see references) ,NA,NA
creates neural network systems for predicting the S&P 500 index as well as for predicting ,NA,NA
"chaotic time series, such as sunspot occurrences. Azoff uses feedforward backpropagation ",NA,NA
"networks, with a training algorithm called ",NA,NA
"adaptive steepest descent,",NA,NA
 a variation of the ,NA,NA
standard algorithm,NA,NA
.,NA,NA
" For the sunspot time series, and an architecture of 6-5-1, and a ratio of ",NA,NA
"training vectors to trainable weights of 5.1, he achieves training set error of 12.9% and test ",NA,NA
set error of 21.4%. This series was composed of yearly sunspot numbers for the years 1706 ,NA,NA
to 1914. Six years of consecutive annual data were input to the network.,NA,NA
One network Azoff used to forecast the S&P 500 index was a 17-7-1 network. The training ,NA,NA
"vectors to trainable weights ratio was 6.1. The achieved training set error was 3.29%, and ",NA,NA
"on the test set error was 4.67%. Inputs to this network included price data, a ",NA,NA
volatility ,NA,NA
"indicator, which is a function of the range of price movement, and a ",NA,NA
random walk ,NA,NA
"indicator, a technical analysis study.",NA,NA
A Critique of Neural Network Time-Series Forecasting for Trading,NA,NA
"Michael de la Maza and Deniz Yuret, managers for the Redfire Capital Management ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/409-412.html (1 of 3) [21/11/02 21:58:23],NA
"Group, suggest that risk-adjusted return, and not mean-squared error should be the metric to ",NA,NA
optimize in a neural network application for trading. They also point out that with neural ,NA,NA
"networks, like with statistical methods such as linear regression, data facts that seem ",NA,NA
unexplainable can’t be ignored even if you want them to be. There is no equivalent for a ,NA,NA
"“don’t care,” condition for the output of a neural network. This type of condition may be an ",NA,NA
important option for trading environments that have no “discoverable regularity” as the ,NA,NA
"authors put it, and therefore are really not tradable. Some solutions to the two problems ",NA,NA
posed are given as follows: ,NA,NA
•,NA,NA
"  Use an algorithm other than backpropagation, which allows for maximization of ",NA,NA
"risk-adjusted return, such as simulated annealing or genetic algorithms. ",NA,NA
•,NA,NA
  Transform the data input to the network so that minimizing mean-squared error ,NA,NA
becomes equivalent to maximizing risk-adjusted return. ,NA,NA
•,NA,NA
  Use a hierarchy (see hierarchical neural network earlier in this section) of neural ,NA,NA
"networks, with each network responsible for detecting features or regularities from ",NA,NA
one component of the data. ,NA,NA
Resource Guide for Neural Networks and Fuzzy Logic in ,NA,NA
Finance,NA,NA
Here is a sampling of resources compiled from trade literature: ,"NOTE:  
 We do not take responsibility for any errors or omissions.",NA
Magazines,NA,NA
Technical Analysis of Stocks and Commodities ,NA,NA
"Technical Analysis, Inc., 3517 S.W. Alaska St., Seattle, WA 98146. ",NA,NA
Futures ,NA,NA
"Futures Magazine, 219 Parkade, Cedar Falls, IA 50613. ",NA,NA
AI in Finance ,NA,NA
"Miller Freeman Inc, 600 Harrison St., San Francisco, CA 94107 ",NA,NA
NeuroVest Journal ,NA,NA
"P.O. Box 764, Haymarket, VA 22069 ",NA,NA
IEEE Transactions on Neural Networks ,NA,NA
"IEEE Service Center, 445 Hoes Lane, P.O. Box 1331, Piscataway, NJ 08855 ",NA,NA
"Particularly worthwhile is an excellent series of articles by consultant Murray Ruggiero Jr., ",NA,NA
in ,NA,NA
Futures,NA,NA
 magazine on neural network design and trading system design in issues ,NA,NA
spanning July ‘94 through June ‘95.,NA,NA
Books,NA,NA
"Azoff, Michael",NA,NA
", Neural Network Time Series Forecasting of Financial Markets",NA,NA
", ",NA,NA
"John Wiley and Sons, New York, 1994. ",NA,NA
"Lederman, Jess, ",NA,NA
Virtual Trading,NA,NA
", Probus Publishing, 1995. ",NA,NA
"Trippi, Robert, ",NA,NA
Neural Networks in Finance and Investing,NA,NA
", Probus Publishing, ",NA,NA
1993. ,NA,NA
Book Vendors,NA,NA
"Traders Press, Inc.",NA,NA
 (800) 927-8222 ,NA,NA
"P.O. Box 6206, Greenville, SC 29606 ",NA,NA
Traders’ Library,NA,NA
 (800) 272-2855 ,NA,NA
"9051 Red Branch Rd., Suite M, Columbia, MD 21045 ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Consultants,NA,NA
Mark Jurik ,NA,NA
Jurik Research ,NA,NA
"P.O. Box 2379, Aptos, CA 95001 ",NA,NA
Hy Rao ,NA,NA
"Via Software Inc, v: (609) 275-4786, fax: (609) 799-7863 ",NA,NA
"BEI Suite 480, 660 Plainsboro Rd., Plainsboro, NJ 08536 ",NA,NA
ViaSW@aol.com ,NA,NA
Mendelsohn Enterprises Inc. ,NA,NA
25941 Apple Blossom Lane ,NA,NA
"Wesley Chapel, FL 33544 ",NA,NA
Murray Ruggiero Jr. ,NA,NA
"Ruggiero Associates, ",NA,NA
"East Haven, CT ",NA,NA
The Schwartz Associates,NA,NA
 (800) 965-4561 ,NA,NA
"801 West El Camino Real, Suite 150, Mountain View, CA 94040 ",NA,NA
Historical Financial Data Vendors,NA,NA
CSI,NA,NA
 (800) CSI-4727 ,NA,NA
"200 W. Palmetto Park Rd., Boca Raton, FL 33432 ",NA,NA
Free Financial Network,NA,NA
", New York (212) 838-6324 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/412-416.html (1 of 5) [21/11/02 21:58:24],NA
Genesis Financial Data Services,NA,NA
 (800) 808-DATA ,NA,NA
"411 Woodmen, Colorado Springs, CO 80991 ",NA,NA
Pinnacle Data Corp,NA,NA
. (800) 724-4903 ,NA,NA
"460 Trailwood Ct., Webster, NY 14580 ",NA,NA
Stock Data Corp,NA,NA
. (410) 280-5533 ,NA,NA
"905 Bywater Rd., Annapolis, MD 21401 ",NA,NA
Technical Tools Inc,NA,NA
. (800) 231-8005 ,NA,NA
"334 State St., Suite 201, Los Altos, CA 94022 ",NA,NA
Tick Data Inc,NA,NA
. (800) 822-8425 ,NA,NA
"720 Kipling St., Suite 115, Lakewood, CO 80215 ",NA,NA
Worden Bros,NA,NA
"., ",NA,NA
Inc.,NA,NA
 (800) 776-4940 ,NA,NA
"4905 Pine Cone Dr., Suite 12, Durham, NC 27707 ",NA,NA
Preprocessing Tools for Neural Network Development,NA,NA
NeuralApp Preprocessor for Windows ,NA,NA
"Via Software Inc., v: (609) 275-4786 fax: (609) 799-7863 ",NA,NA
"BEI Suite 480, 660 Plainsboro Rd., Plainsboro, NJ 08536 ",NA,NA
ViaSW@aol.com ,NA,NA
Stock Prophet ,NA,NA
Future Wave Software (310) 540-5373 ,NA,NA
"1330 S. Gertruda Ave., Redondo Beach, CA 90277 ",NA,NA
Wavesamp & Data Decorrelator & Reducer ,NA,NA
TSA (800) 965-4561 ,NA,NA
"801 W. El Camino Real, #150, Mountain. View, CA 94040 ",NA,NA
Genetic Algorithms Tool Vendors,NA,NA
C Darwin ,NA,NA
ITANIS International Inc. ,NA,NA
"1737 Holly Lane, Pittsburgh, PA 15216 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/412-416.html (2 of 5) [21/11/02 21:58:24],NA
EOS ,NA,NA
Man Machine Interfaces Inc. (516) 249-4700 ,NA,NA
"555 Broad Hollow Rd., Melville, NY 11747 ",NA,NA
Evolver ,NA,NA
"Axcelis, Inc. (206) 632-0885 ",NA,NA
"4668 Eastern Ave. N., Seattle, WA 98103 ",NA,NA
Fuzzy Logic Tool Vendors,NA,NA
CubiCalc ,NA,NA
HyperLogic Corp. (619) 746-2765 ,NA,NA
"1855 East Valley Pkwy., Suite 210, Escondido, CA 92027 ",NA,NA
TILSHELL ,NA,NA
Togai InfraLogic Inc. ,NA,NA
"5 Vanderbilt, Irvine, CA 92718 ",NA,NA
Neural Network Development Tool Vendors,NA,NA
Braincel ,NA,NA
Promised Land Technologies (203) 562-7335 195 ,NA,NA
"Church St., 8th Floor, New Haven, CT 06510 ",NA,NA
BrainMaker ,NA,NA
California Scientific Software (916) 478-9040 ,NA,NA
"10024 Newtown Rd., Nevada City, CA 95959 ",NA,NA
"ForecastAgent for Windows, ForecastAgent for Windows 95 ",NA,NA
"Via Software Inc., v: (609) 275-4786 fax: (609) 799-7863 BEI ",NA,NA
"Suite 480, 660 Plainsboro Rd., Plainsboro, NJ 08536 ",NA,NA
ViaSW@aol.com ,NA,NA
InvestN 32 ,NA,NA
"RaceCom, Inc. (800) 638-8088 ",NA,NA
"555 West Granada Blvd., Suite E-10, Ormond Beach, FL 32714 ",NA,NA
"NetCaster, DataCaster ",NA,NA
Maui Analysis & Synthesis Technologies (808) 875-2516 ,NA,NA
"590 Lipoa Pkwy., Suite 226, Kihei, HI 96753 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/412-416.html (3 of 5) [21/11/02 21:58:24],NA
NeuroForecaster ,NA,NA
NIBS Pte. Ltd. (65) 344-2357 ,NA,NA
"62 Fowlie Rd., Republic of Singapore 1542 ",NA,NA
NeuroShell ,NA,NA
Ward Systems Group (301)662-7950 ,NA,NA
"Executive Park West, 5 Hillscrest Dr., Frederick, MD 21702 ",NA,NA
NeuralWorks Predict ,NA,NA
NeuralWare Inc. (412) 787-8222 ,NA,NA
"202 Park West Dr., Pittsburgh, PA 15276 ",NA,NA
N-Train ,NA,NA
Scientific Consultant Services (516) 696-3333 ,NA,NA
"20 Stagecoach Rd., Selden, NY 11784 ",NA,NA
Summary,NA,NA
This chapter presented a neural network application in financial forecasting. As an example ,NA,NA
"of the steps needed to develop a neural network forecasting model, the change in the ",NA,NA
Standard & Poor’s 500 stock index was predicted 10 weeks out based on weekly data for ,NA,NA
five indicators. Some examples of preprocessing of data for the network were shown as ,NA,NA
well as issues in training. ,NA,NA
"At the end of the training period, it was seen that memorization was taking place, since the ",NA,NA
"error in the test data degraded, whereas the error in the training set improved. It is ",NA,NA
important to monitor the error in the test data (without weight changes) while you are ,NA,NA
training to ensure that generalization ability is maintained. The final network resulted in ,NA,NA
average RMS error of 6.9 % over the training set and 13.9% error over the test set.,NA,NA
This chapter’s example in forecasting highlights the ease of use and wide applicability of ,NA,NA
"the backpropagation algorithm for large, complex problems and data sets. Several ",NA,NA
examples of research in financial forecasting were presented with a number of ideas and ,NA,NA
real-life methodologies presented.,NA,NA
Technical Analysis was briefly discussed with examples of studies that can be useful in ,NA,NA
preprocessing data for neural networks.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/412-416.html (4 of 5) [21/11/02 21:58:24],NA
A Resource guide was presented for further information on financial applications of neural ,NA,NA
networks.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch14/412-416.html (5 of 5) [21/11/02 21:58:24]",NA
Previous Table of Contents Next,NA,NA
Chapter 15 ,NA,NA
Application to Nonlinear Optimization ,NA,NA
Introduction,NA,NA
"Nonlinear optimization is an area of operations research, and efficient algorithms for some ",NA,NA
"of the problems in this area are hard to find. In this chapter, we describe the traveling ",NA,NA
salesperson problem and discuss how this problem is formulated as a nonlinear ,NA,NA
optimization problem in order to use neural networks (Hopfield and Kohonen) to find an ,NA,NA
optimum solution. We start with an explanation of the concepts of ,NA,NA
linear,NA,NA
", ",NA,NA
integer linear ,NA,NA
and ,NA,NA
nonlinear,NA,NA
 optimization.,NA,NA
An optimization problem has an ,NA,NA
objective,NA,NA
 function and a set of ,NA,NA
constraints,NA,NA
 on the ,NA,NA
variables. The problem is to find the values of the variables that lead to an optimum value ,NA,NA
"for the objective function, while satisfying all the constraints. The ",NA,NA
objective,NA,NA
 function may ,NA,NA
"be a linear function in the variables, or it may be a nonlinear function. For example, it could ",NA,NA
"be a function expressing the total cost of a particular production plan, or a function giving ",NA,NA
the net profit from a group of products that share a given set of resources. The objective ,NA,NA
"may be to find the minimum value for the objective function, if, for example, it represents ",NA,NA
"cost, or to find the maximum value of a profit function. The resources shared by the ",NA,NA
products in their manufacturing are usually in limited supply or have some other ,NA,NA
restrictions on their availability. This consideration leads to the specification of the ,NA,NA
constraints for the problem.,NA,NA
Each constraint is usually in the form of an equation or an inequality. The left side of such ,NA,NA
"an equation or inequality is an expression in the variables for the problem, and the right-",NA,NA
hand side is a constant. The constraints are said to be linear or nonlinear depending on ,NA,NA
whether the expression on the left-hand side is a linear function or nonlinear function of ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/417-419.html (1 of 3) [21/11/02 21:58:25],NA
the variables. A ,NA,NA
linear programming problem,NA,NA
 is an optimization problem with a ,NA,NA
linear ,NA,NA
objective function as well as a set of ,NA,NA
linear,NA,NA
 constraints. An ,NA,NA
integer linear,NA,NA
 programming ,NA,NA
problem is a linear programming problem where the variables are required to have integer ,NA,NA
values. A ,NA,NA
nonlinear optimization problem,NA,NA
 has one or more of the constraints nonlinear ,NA,NA
and/or the objective function is nonlinear.,NA,NA
Here are some examples of statements that specify objective functions and constraints:,NA,NA
•,NA,NA
  Linear objective function: Maximize ,NA,NA
Z,NA,NA
 = 3,NA,NA
X,1,NA
 + 4,NA,NA
X,2,NA
 + 5.7,NA,NA
X,3,NA
•,NA,NA
  ,NA,NA
Linear equality constraint: 13,NA,NA
X,1,NA
 - 4.5,NA,NA
X,2,NA
 + 7,NA,NA
X,3,NA
 = 22 ,NA,NA
•,NA,NA
  Linear inequality constraint: 3.6,NA,NA
X,1,NA
 + 8.4,NA,NA
X,2,NA
 - 1.7,NA,NA
X,3,NA
,NA,NA
 10.9 ,NA,NA
•,NA,NA
  ,NA,NA
Nonlinear objective function: Minimize ,NA,NA
Z,NA,NA
 = 5,NA,NA
X,2,NA
 + 7,NA,NA
XY,NA,NA
 + ,NA,NA
Y,2,NA
•,NA,NA
  ,NA,NA
Nonlinear equality constraint: 4,NA,NA
X,NA,NA
 + 3,NA,NA
XY,NA,NA
 + 7,NA,NA
Y,NA,NA
 + 2,NA,NA
Y,2,NA
 = 37.6 ,NA,NA
•,NA,NA
  ,NA,NA
Nonlinear inequality constraint: 4.8,NA,NA
X,NA,NA
 + 5.3,NA,NA
XY,NA,NA
 + 6.2,NA,NA
Y,2,NA
,NA,NA
 34.56 ,NA,NA
An example of a linear programming problem is the ,NA,NA
blending problem,NA,NA
. An example of a ,NA,NA
blending problem is that of making different flavors of ice cream blending different ,NA,NA
"ingredients, such as sugar, a variety of nuts, and so on, to produce different amounts of ice ",NA,NA
cream of many flavors. The objective in the problem is to find the amounts of individual ,NA,NA
"flavors of ice cream to produce with given supplies of all the ingredients, so the total profit ",NA,NA
is maximized.,NA,NA
A nonlinear optimization problem example is the ,NA,NA
quadratic programming problem,NA,NA
. The ,NA,NA
constraints are all linear but the objective function is a quadratic form. A quadratic form is ,NA,NA
an expression of two variables with 2 for the sum of the exponents of the two variables in ,NA,NA
each term.,NA,NA
"An example of a quadratic programming problem, is a simple investment strategy problem ",NA,NA
that can be stated as follows. You want to invest a certain amount in a growth stock and in ,NA,NA
"a speculative stock, achieving at least 25% return. You want to limit your investment in the ",NA,NA
speculative stock to no more than 40% of the total investment. You figure that the ,NA,NA
"expected return on the growth stock is 18%, while that on the speculative stock is 38%. ",NA,NA
Suppose G and S represent the proportion of your investment in the growth stock and the ,NA,NA
"speculative stock, respectively. So far you have specified the following constraints. These ",NA,NA
are linear constraints:,NA,NA
G,NA,NA
 + ,NA,NA
S,NA,NA
 = 1 ,NA,NA
This says the proportions add up to 1. ,NA,NA
S,NA,NA
,NA,NA
 0.4 ,NA,NA
1.18,NA,NA
G,NA,NA
 + 1.38,NA,NA
S,NA,NA
,NA,NA
 1.25 ,NA,NA
This says the proportion invested in speculative stock is no ,NA,NA
more than 40%. ,NA,NA
This says the expected return from these investments should ,NA,NA
be at least 25%. ,NA,NA
Now the objective function needs to be specified. You have specified already the expected ,NA,NA
return you want to achieve. Suppose that you are a conservative investor and want to ,NA,NA
minimize the variance of the return. The variance works out as a quadratic form. Suppose it ,NA,NA
is determined to be: ,NA,NA
 2G,2,NA
  + 3S,2,NA
 - GS,NA,NA
"This quadratic form, which is a function of G and S, is your objective function that you ",NA,NA
want to minimize subject to the (linear) constraints previously stated. ,NA,NA
Neural Networks for Optimization Problems,NA,NA
It is possible to construct a neural network to find the values of the variables that ,NA,NA
correspond to an optimum value of the ,NA,NA
objective,NA,NA
" function of a problem. For example, the ",NA,NA
neural networks that use the ,NA,NA
Widrow-Hoff learning rule,NA,NA
 find the minimum value of the ,NA,NA
error,NA,NA
 function using the ,NA,NA
least mean squared error,NA,NA
. Neural networks such as the ,NA,NA
feedforward backpropagation network use the ,NA,NA
steepest descent,NA,NA
 method for this purpose and ,NA,NA
"find a local minimum of the error, if not the global minimum. On the other hand, the ",NA,NA
Boltzmann machine or the Cauchy machine uses statistical methods and probabilities and ,NA,NA
achieves success in finding the global minimum of an ,NA,NA
error,NA,NA
 function. So we have an idea ,NA,NA
of how to go about using a neural network to find an optimum value of a function. The ,NA,NA
question remains as to how the constraints of an optimization problem should be treated in ,NA,NA
a neural network operation. A good example in answer to this question is the ,NA,NA
traveling ,NA,NA
salesperson problem,NA,NA
. Let’s discuss this example next.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Traveling Salesperson Problem,NA,NA
The traveling salesperson problem is well-known in optimization. Its mathematical ,NA,NA
"formulation is simple, and one can state a simple solution strategy also. Such a strategy is ",NA,NA
"often impractical, and as yet, there is no efficient algorithm for this problem that ",NA,NA
consistently works in all instances. The traveling salesperson problem is one among the so- ,NA,NA
called ,NA,NA
NP-complete problems,NA,NA
", about which you will read more in what follows. That means ",NA,NA
that any algorithm for this problem is going to be impractical with certain examples. The ,NA,NA
neural network approach tends to give solutions with less computing time than other ,NA,NA
available algorithms for use on a digital computer. The problem is defined as follows. A ,NA,NA
traveling salesperson has a number of cities to visit. The sequence in which the salesperson ,NA,NA
visits different cities is called a ,NA,NA
tour,NA,NA
. A tour should be such that every city on the list is ,NA,NA
"visited once and only once, except that he returns to the city from which he starts. The goal ",NA,NA
"is to find a tour that minimizes the total distance the salesperson travels, among all the tours ",NA,NA
that satisfy this criterion.,NA,NA
A simple strategy for this problem is to enumerate all feasible tours—a tour is feasible if it ,NA,NA
satisfies the criterion that every city is visited but once—to calculate the total distance for ,NA,NA
"each tour, and to pick the tour with the smallest total distance. This simple strategy ",NA,NA
"becomes impractical if the number of cities is large. For example, if there are 10 cities for ",NA,NA
"the traveling salesperson to visit (not counting home), there are 10! = 3,628,800 possible ",NA,NA
"tours, where 10! denotes the factorial of 10—the product of all the integers from 1 to 10—",NA,NA
and is the number of distinct permutations of the 10 cities. This number grows to over 6.2 ,NA,NA
"billion with only 13 cities in the tour, and to over a trillion with 15 cities.",NA,NA
The TSP in a Nutshell,NA,NA
For ,NA,NA
n,NA,NA
" cities to visit, let ",NA,NA
X,ij,NA
 be the variable that has value 1 if the salesperson goes from city ,NA,NA
i,NA,NA
 ,NA,NA
to city ,NA,NA
j,NA,NA
 and value 0 if the salesperson does not go from city ,NA,NA
i,NA,NA
 to city ,NA,NA
j,NA,NA
. Let ,NA,NA
d,ij,NA
 be the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/419-421.html (1 of 3) [21/11/02 21:58:26],NA
distance from city ,NA,NA
i,NA,NA
 to city ,NA,NA
j,NA,NA
. The traveling salesperson problem (TSP) is stated as follows:,NA,NA
Minimize the linear objective function: ,NA,NA
subject to:,NA,NA
" for each j=1, …, n (linear constraint)",NA,NA
" for each i=1, …, n (linear constraint)",NA,NA
•X,ij,NA
 for 1 for all i and j (integer constraint) ,NA,NA
This is a 0-1 integer linear programming problem. ,NA,NA
Solution via Neural Network,NA,NA
This section shows how the linear and integer constraints of the TSP are absorbed into an ,NA,NA
objective function that is nonlinear for solution via Neural network. ,NA,NA
The first consideration in the formulation of an optimization problem is the identification of ,NA,NA
the underlying variables and the type of values they can have. In a traveling salesperson ,NA,NA
"problem, each city has to be visited once and only once, except the city started from. ",NA,NA
Suppose you take it for granted that the last leg of the tour is the travel between the last city ,NA,NA
"visited and the city from which the tour starts, so that this part of the tour need not be ",NA,NA
explicitly included in the formulation. Then with ,NA,NA
n,NA,NA
" cities to be visited, the only information ",NA,NA
needed for any city is the position of that city in the order of visiting cities in the tour. This ,NA,NA
suggests that an ordered ,NA,NA
n,NA,NA
"-tuple is associated with each city with some element equal to 1, ",NA,NA
and the rest of the ,NA,NA
n,NA,NA
" – 1 elements equal to 0. In a neural network representation, this ",NA,NA
requires n neurons associated with one city. Only one of these ,NA,NA
n,NA,NA
 neurons corresponding to ,NA,NA
"the position of the city, in the order of cities in the tour, fires or has output 1. Since there ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/419-421.html (2 of 3) [21/11/02 21:58:26],NA
are ,NA,NA
n,NA,NA
" cities to be visited, you need ",NA,NA
n,2,NA
 neurons in the network. If these neurons are all ,NA,NA
"arranged in a square array, you need a single 1 in each row and in each column of this ",NA,NA
array to indicate that each city is visited but only once.,NA,NA
Let ,NA,NA
x,ij,NA
 be the variable to denote the fact that city ,NA,NA
i,NA,NA
 is the ,NA,NA
j,NA,NA
th city visited in a tour. Then ,NA,NA
x,ij,NA
 is ,NA,NA
the output of the jth neuron in the array of neurons corresponding to the ,NA,NA
i,NA,NA
th city. You have ,NA,NA
n,2,NA
" such variables, and their values are binary, 0 or 1. In addition, only ",NA,NA
n,NA,NA
 of these variables ,NA,NA
"should have value 1 in the solution. Furthermore, exactly one of the ",NA,NA
x,NA,NA
’s with the same first ,NA,NA
subscript (value of ,NA,NA
i,NA,NA
) should have value 1. It is because a given city can occupy only one ,NA,NA
"position in the order of the tour. Similarly, exactly one of the ",NA,NA
x,NA,NA
’s with the same second ,NA,NA
subscript (value of ,NA,NA
j,NA,NA
) should have value 1. It is because a given position in the tour can be ,NA,NA
only occupied by one city. These are the constraints in the problem. How do you then ,NA,NA
describe the tour? We take as the starting city for the tour to be city 1 in the array of cities. ,NA,NA
A tour can be given by the sequence ,NA,NA
1,NA,NA
", ",NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
", …, ",NA,NA
"q,",NA,NA
 indicating that the cities visited in the ,NA,NA
"tour in order starting at 1 are, ",NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", ",NA,NA
c,NA,NA
", …, ",NA,NA
q,NA,NA
 and back to 1. Note that the sequence of ,NA,NA
subscripts ,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", …, ",NA,NA
q,NA,NA
" is a permutation of 2, 3, … ",NA,NA
n,NA,NA
" – 1, ",NA,NA
x,a1,NA
=1,NA,NA
", ",NA,NA
x,b2,NA
=1,NA,NA
", etc.",NA,NA
"Having frozen city 1 as the first city of the tour, and noting that distances are symmetric, ",NA,NA
the distinct number of tours that satisfy the constraints is not ,NA,NA
n,NA,NA
"!, when there are ",NA,NA
n,NA,NA
 cities in ,NA,NA
"the tour as given earlier. It is much less, namely, ",NA,NA
n,NA,NA
!/2,NA,NA
n,NA,NA
. Thus when ,NA,NA
n,NA,NA
" is 10, the number of ",NA,NA
"distinct feasible tours is 10!/20, which is 181,440. If ",NA,NA
n,NA,NA
" is 15, it is still over 43 billion, and it ",NA,NA
exceeds a trillion with 17 cities in the tour. Yet for practical purposes there is not much ,NA,NA
"comfort knowing that for the case of 13 cities, 13! is over 6.2 billion and 13!/26 is only ",NA,NA
239.5 million—it is still a tough combinatorial problem.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/419-421.html (3 of 3) [21/11/02 21:58:26]",NA
Previous Table of Contents Next,NA,NA
"The cost of the tour is the total distance traveled, and it is to be minimized. The total ",NA,NA
distance traveled in the tour is the sum of the distances from each city to the next. The ,NA,NA
objective,NA,NA
 function has one term that corresponds to the total distance traveled in the tour. ,NA,NA
"The other terms, one for each constraint, in the ",NA,NA
objective,NA,NA
" function are expressions, each ",NA,NA
attaining a minimum value if and only if the corresponding constraint is satisfied. The ,NA,NA
objective,NA,NA
 function then takes the following form. Hopfield and Tank formulated the ,NA,NA
"problem as one of minimizing energy. Thus it is, customary to refer to the value of the ",NA,NA
"objective function of this problem, while using Hopfield-like neural network for its ",NA,NA
"solution, as the energy level, ",NA,NA
E,NA,NA
", of the network. The goal is to minimize this energy level.",NA,NA
In formulating the equation for ,NA,NA
E,NA,NA
", one uses constant parameters ",NA,NA
A,1,NA
", ",NA,NA
A,2,NA
", ",NA,NA
A,3,NA
", and ",NA,NA
A,4,NA
 as ,NA,NA
coefficients in different terms of the expression on the right-hand side of the equation. The ,NA,NA
equation that defines ,NA,NA
E,NA,NA
 is given as follows. Note that the notation in this equation includes ,NA,NA
d,ij,NA
 for the distance from city ,NA,NA
i,NA,NA
 to city ,NA,NA
j,NA,NA
.,NA,NA
E = A,1,NA
©,i,NA
©,k,NA
©,"j
 
 k",NA
x,ik,NA
x,ij,NA
 + A,2,NA
©,i,NA
©,k,NA
 x,i,NA
©,k,NA
©,"j
 
 k",NA
x,ki,NA
x,ji,NA
 + A,3,NA
[( ,NA,NA
©,i,NA
©,k,NA
x,ik,NA
) - n],2,NA
 + A,4,NA
©,k,NA
©,k,NA
©,"j
 
 k",NA
©,i,NA
d,kj,NA
x,ki,NA
(x,"j,i+1",NA
 + x,"j,i-1",NA
),NA,NA
Our first observation at this point is that ,NA,NA
E,NA,NA
 is a ,NA,NA
nonlinear,NA,NA
 function of the ,NA,NA
x,NA,NA
"’s, as you have ",NA,NA
quadratic terms in it. So this formulation of the traveling salesperson problem renders it a ,NA,NA
nonlinear optimization problem,NA,NA
.,NA,NA
All the summations indicated by the occurrences of the summation symbol ,NA,NA
©,NA,NA
", range from 1 ",NA,NA
to ,NA,NA
n,NA,NA
 for the values of their respective indices. This means that the same summand such as ,NA,NA
x,12,NA
x,33,NA
 also as ,NA,NA
x,33,NA
x,12,NA
", appears twice with only the factors interchanged in their order of ",NA,NA
"occurrence in the summand. For this reason, many authors use an additional factor of 1/2 ",NA,NA
for each term in the expression for ,NA,NA
E,NA,NA
". However, when you minimize a quantity ",NA,NA
z,NA,NA
 with a ,NA,NA
given set of values for the variables in the expression for ,NA,NA
z,NA,NA
", the same values for these ",NA,NA
variables minimize any whole or fractional multiple of ,NA,NA
z,NA,NA
", as well.",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/421-423.html (1 of 2) [21/11/02 21:58:27],NA
The third summation in the first term is over the index ,NA,NA
j,NA,NA
", from 1 to ",NA,NA
n,NA,NA
", but excluding ",NA,NA
whatever value ,NA,NA
k,NA,NA
 has. This prevents you from using something like ,NA,NA
x,12,NA
x,12,NA
". Thus, the first ",NA,NA
term is an abbreviation for the sum of ,NA,NA
n,2,NA
(,NA,NA
n,NA,NA
 – 1) terms with no two factors in a summand ,NA,NA
equal. This term is included to correspond to the constraint that no more than one neuron in ,NA,NA
"the same row can output a 1. Thus, you get 0 for this term with a valid solution. This is also ",NA,NA
true for the second term in the right-hand side of the equation for ,NA,NA
E,NA,NA
. Note that for any value ,NA,NA
of the index ,NA,NA
i,NA,NA
", ",NA,NA
x,ii,NA
" has value 0, since you are not making a move like, from city ",NA,NA
i,NA,NA
 to the same ,NA,NA
city ,NA,NA
i,NA,NA
 in any of the tours you consider as a solution to this problem. The third term in the ,NA,NA
expression for ,NA,NA
E,NA,NA
" has a minimum value of 0, which is attained if and only if exactly ",NA,NA
n,NA,NA
 of the ,NA,NA
n,2,NA
x,NA,NA
’s have value 1 and the rest 0.,NA,NA
"The last term expresses the goal of finding a tour with the least total distance traveled, ",NA,NA
indicating the shortest tour among all possible tours for the traveling salesperson. Another ,NA,NA
important issue about the values of the subscripts on the right-hand side of the equation for ,NA,NA
"E is, what happens to ",NA,NA
i,NA,NA
" + 1, for example, when ",NA,NA
i,NA,NA
 is already equal to ,NA,NA
n,NA,NA
", and to ",NA,NA
i,NA,NA
"–1, when ",NA,NA
i,NA,NA
 is ,NA,NA
equal to 1. The ,NA,NA
i,NA,NA
 + 1 and ,NA,NA
i,NA,NA
" – 1 seem like impossible values, being out of their allowed range ",NA,NA
from 1 to ,NA,NA
n,NA,NA
. The trick is to replace these values with their moduli with respect to ,NA,NA
n,NA,NA
. This ,NA,NA
"means, that the value ",NA,NA
n,NA,NA
" + 1 is replaced with 1, and the value 0 is replaced with ",NA,NA
n,NA,NA
 in the ,NA,NA
situations just described.,NA,NA
"Modular values are obtained as follows. If we want, say 13 modulo 5, we subtract 5 as ",NA,NA
"many times as possible from 13, until the remainder is a value between 0 and 4, 4 being 5 – ",NA,NA
"1. Since we can subtract 5 twice from 13 to get a remainder of 3, which is between 0 and 4, ",NA,NA
3 is the value of 13 modulo 5. Thus (,NA,NA
n,NA,NA
 + 3) modulo ,NA,NA
n,NA,NA
" is 3, as previously noted. Another ",NA,NA
"way of looking at these results is that 3 is 13 modulo 5 because, if you subtract 3 from 13, ",NA,NA
"you get a number divisible by 5, or which has 5 as a factor. Subtracting 3 from ",NA,NA
n,NA,NA
 + 3 gives ,NA,NA
you ,NA,NA
n,NA,NA
", which has ",NA,NA
n,NA,NA
 as a factor. So 3 is (,NA,NA
n,NA,NA
 + 3) modulo ,NA,NA
n,NA,NA
". In the case of –1, by subtracting ",NA,NA
(,NA,NA
n,NA,NA
"– 1) from it, we get -",NA,NA
n,NA,NA
", which can be divided by ",NA,NA
n,NA,NA
 getting –1. So (,NA,NA
n,NA,NA
 – 1) is the value of (–,NA,NA
1) modulo ,NA,NA
n,NA,NA
.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/421-423.html (2 of 2) [21/11/02 21:58:27]",NA
Previous Table of Contents Next,NA,NA
Example of a Traveling Salesperson Problem for Hand Calculation,NA,NA
"Suppose there are four cities in the tour. Call these cities, ",NA,NA
C,1,NA
", ",NA,NA
C,2,NA
", ",NA,NA
C,3,NA
", and ",NA,NA
C,4,NA
. Let the matrix of ,NA,NA
distances be the following matrix ,NA,NA
D,NA,NA
.,NA,NA
 0   10   14    7,NA,NA
 D =  10    0    6   12,NA,NA
 14    6    0    9,NA,NA
 7   12    9    0,NA,NA
"From our earlier discussion on the number of valid and distinct tours, we infer that there are just three ",NA,NA
"such tours. Since it is such a small number, we can afford to enumerate the three tours, find the energy ",NA,NA
"values associated with them, and pick the tour that has the smallest energy level among the three. The ",NA,NA
three tours are: ,NA,NA
•Tour 1,NA,NA
". 1 - 2 - 3 - 4 - 1 In this tour city 2 is visited first, followed by city 3, from where the ",NA,NA
"salesperson goes to city 4, and then returns to city 1. For city 2, the corresponding ordered array is ",NA,NA
"(1, 0, 0, 0), because city 2 is the first in this permutation of cities. Then ",NA,NA
x,21,NA
" = 1, ",NA,NA
x,22,NA
" = 0, ",NA,NA
x,23,NA
" = 0, ",NA,NA
x,24,NA
" = 0. Also (0, 1, 0, 0), (0, 0, 1, 0), and (0, 0, 0, 1) correspond to cities 3, 4, and 1, ",NA,NA
respectively. The total distance of the tour is ,NA,NA
d,12,NA
 + ,NA,NA
d,23,NA
 + ,NA,NA
d,34,NA
 + ,NA,NA
d,41,NA
= 10 + 6 + 9 + 7 = 32. ,NA,NA
•Tour 2,NA,NA
. 1 - 3 - 4 - 2 - 1 ,NA,NA
•Tour 3,NA,NA
. 1 - 4 - 2 - 3 - 1 ,NA,NA
"There seems to be some discrepancy here. If there is one, we need an explanation. The discrepancy is ",NA,NA
that we can find many more tours that should be valid because no city is visited more than once. You ,NA,NA
may say they are distinct from the three previously listed. Some of these additional tours are: ,NA,NA
•Tour 4,NA,NA
. 1 - 2 - 4 - 3 - 1 ,NA,NA
•Tour 5,NA,NA
. 3 - 2 - 4 - 1 - 3 ,NA,NA
•Tour 6,NA,NA
. 2 - 1 - 4 - 3 - 2 ,NA,NA
There is no doubt that these three tours are distinct from the first set of three tours. And in each of these ,NA,NA
"three tours, every city is visited exactly once, as required in the problem. So they are valid tours as well. ",NA,NA
"Why did our formula give us 3 for the value of the number of possible valid tours, while we ",NA,NA
are able to find 6? ,NA,NA
"The answer lies in the fact that if two valid tours are symmetric and have the same energy level, because ",NA,NA
"they have the same value for the total distance traveled in the tour, one of them is in a sense redundant, ",NA,NA
"or one of them can be considered degenerate, using the terminology common to this context. As long as ",NA,NA
"they are valid and give the same total distance, the two tours are not individually interesting, and any one ",NA,NA
"of the two is enough to have. By simple inspection, you find the total distances for the six listed tours. ",NA,NA
"They are 32 for tour 1, 32 also for tour 6, 45 for each of tours 2 and 4, and 39 for each of tours 3 and 5. ",NA,NA
"Notice also that tour 6 is not very different from tour 1. Instead of starting at city 1 as in tour 1, if you ",NA,NA
"start at city 2 and follow tour 1 from there in reverse order of visiting cities, you get tour 6. Therefore, ",NA,NA
the distance covered is the same for both these tours. You can find similar relationships for other pairs of ,NA,NA
"tours that have the same total distance for the tour. Either by reversing the order of cities in a tour, or by ",NA,NA
"making a circular permutation of the order of the cities in a tour, you get another tour with the same total ",NA,NA
"distance. This way you can find tours. Thus, only three distinct total distances are possible, and among ",NA,NA
them 32 is the lowest. The tour 1 - 2 - 3 - 4 - 1 is the shortest and is an optimum solution for this ,NA,NA
"traveling salesperson problem. There is an alternative optimum solution, namely tour 6, with 32 for the ",NA,NA
"total length of the tour. The problem is to find an optimal tour, and not to find all optimal tours if more ",NA,NA
than one exist. That is the reason only three distinct tours are suggested by the formula for the number of ,NA,NA
distinct valid tours in this case.,NA,NA
The formula we used to get the number of valid and distinct tours to be 3 is based on the elimination of ,NA,NA
"such symmetry. To clarify all this discussion, you should determine the energy levels of all your six tours ",NA,NA
"identified earlier, hoping to find pairs of tours with identical energy levels.",NA,NA
"Note that the first term on the right-hand side of the equation results in 0 for a valid tour, as this term is ",NA,NA
"to ensure there is no more than a single 1 in each row. That is, in any summand in the first term, at ",NA,NA
"least one of the factors, ",NA,NA
x,ik,NA
 or ,NA,NA
x,ij,NA
", where ",NA,NA
k,NA,NA
,NA,NA
j,NA,NA
" has to be 0 for a valid tour. So all those summands are 0, ",NA,NA
"making the first term itself 0. Similarly, the second term is 0 for a valid tour, because in any summand ",NA,NA
"at least one of the factors, ",NA,NA
x,ki,NA
 or ,NA,NA
x,ji,NA
", where ",NA,NA
k,NA,NA
,NA,NA
j,NA,NA
" has to be 0 for a valid tour. In all, exactly 4 of the 16 ",NA,NA
x,NA,NA
’s ,NA,NA
"are each 1, making the total of the ",NA,NA
x,NA,NA
’s 4. This causes the third term to be 0 for a valid tour. These ,NA,NA
"observations make it clear that it does not matter for valid tours, what values are assigned to the ",NA,NA
parameters ,NA,NA
A,1,NA
", ",NA,NA
A,2,NA
", and ",NA,NA
A,3,NA
". Assigning large values for these parameters would cause the energy levels, ",NA,NA
"for tours that are not valid, to be much larger than the energy levels for valid tours. Thereby, these tours ",NA,NA
become unattractive for the solution of the traveling salesperson problem. Let us use the value 1/2 for ,NA,NA
the parameter ,NA,NA
A,4,NA
.,NA,NA
"Let us demonstrate the calculation of the value for the last term in the equation for E, in the case of ",NA,NA
tour 1. Recall that the needed equation is,NA,NA
E = A,1,NA
©,i,NA
©,k,NA
©,"j
 
 k",NA
 x,ik,NA
x,ij,NA
 + A,2,NA
©,i,NA
©,k,NA
©,"j
 
 k",NA
x,ki,NA
x,ji,NA
 + A,3,NA
[( ,NA,NA
©,i,NA
©,k,NA
x,jk,NA
) - n],2,NA
 + A,4,NA
©,k,NA
©,"j
 
 k",NA
©,i,NA
d,kj,NA
x,ki,NA
(x,"j,i+1",NA
 + x,"j,i-1",NA
),NA,NA
The last term expands as given in the following calculation:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/423-426.html (2 of 4) [21/11/02 21:58:28],NA
 A,4,NA
{ ,d,NA
12[,x,NA
12(,x,NA
23 +,x,NA
21) + ,x,NA
13(,x,NA
24 + ,x,NA
22) + ,x,NA
14(,x,NA
21 + ,x,NA
23)] + ,d,NA
13[,x,NA
12(,x,NA
33 + ,x,NA
31) + ,x,NA
13(,x,NA
34 + ,x,NA
32) + ,x,NA
14(,x,NA
31 + ,x,NA
33)] + ,d,NA
14[,x,NA
12(,x,NA
43 + ,x,NA
41) + ,x,NA
13(,x,NA
44 + ,x,NA
42) + ,x,NA
14(,x,NA
41 + ,x,NA
43)] + ,d,NA
21[,x,NA
21(,x,NA
12 + ,x,NA
14) + ,x,NA
23(,x,NA
14 + ,x,NA
12) + ,x,NA
24(,x,NA
11 + ,x,NA
13)] + ,d,NA
23[,x,NA
21(,x,NA
32 + ,x,NA
34) + ,x,NA
23(,x,NA
34 + ,x,NA
32) + ,x,NA
24(,x,NA
31 + ,x,NA
33)] + ,d,NA
24[,x,NA
21(,x,NA
42 + ,x,NA
44) + ,x,NA
23(,x,NA
44 + ,x,NA
42) + ,x,NA
24(,x,NA
41 + ,x,NA
43)] + ,d,NA
31[,x,NA
31(,x,NA
12 + ,x,NA
14) + ,x,NA
32(,x,NA
13 + ,x,NA
11) + ,x,NA
34(,x,NA
11 + ,x,NA
13)] + ,d,NA
32[,x,NA
31(,x,NA
22 + ,x,NA
24) + ,x,NA
32(,x,NA
23 + ,x,NA
21) + ,x,NA
34(,x,NA
21 + ,x,NA
23)] + ,d,NA
34[,x,NA
31(,x,NA
42 + ,x,NA
44) + ,x,NA
32(,x,NA
43 + ,x,NA
41) + ,x,NA
34(,x,NA
41 + ,x,NA
43)] + ,d,NA
41[,x,NA
41(,x,NA
12 + ,x,NA
14) + ,x,NA
42(,x,NA
13 + ,x,NA
11) + ,x,NA
43(,x,NA
14 + ,x,NA
12)] + ,d,NA
42[,x,NA
41(,x,NA
22 + ,x,NA
24) + ,x,NA
42(,x,NA
23 + ,x,NA
21) + ,x,NA
43(,x,NA
24 + ,x,NA
22)] + ,d,NA
43[,x,NA
41(,x,NA
32 + ,x,NA
34) + ,x,NA
42(,x,NA
33 + ,x,NA
31) + ,x,NA
43(,x,NA
34 + ,x,NA
32)] },NA,NA
"When the respective values are substituted for the tour 1 - 2 - 3 - 4 - 1, the previous calculation ",NA,NA
becomes: ,NA,NA
 1/2{10[0(0 + 1) + 0(0 + 0) + 1(1 + 0)] + 14[0(0 + 0) + 0(0 + 1) + ,NA,NA
1(0 + 0)] +,NA,NA
 7[0(1 + 0) + 0(0 + 0) + 1(0 + 1)] + 10[1(0 + 1) + 0(1 + 0) + 0(0 + ,NA,NA
0)] +,NA,NA
 6[1(1 + 0) + 0(0 + 1) + 0(0 + 0)] + 12[1(0 + 0) + 0(0 + 0) + 0(0 + ,NA,NA
1)] +,NA,NA
 14[0(0 + 1) + 1(0 + 0) + 0(0 + 0)] + 6[0(0 + 0) + 1(0 + 1) + 0(1 + ,NA,NA
0)] +,NA,NA
 9[0(0 + 0) + 1(1 + 0) + 0(0 + 1)] + 7[0(0 + 1) + 0(0 + 0) + 1(1 + ,NA,NA
0)] +,NA,NA
 12[0(0 + 0) + 0(0 + 1) + 1(0 + 0)] + 9[0(1 + 0) + 0(0 + 0) + 1(0 + ,NA,NA
1)]},NA,NA
 = 1/2( 10 + 0 + 7 + 10 + 6 + 0 + 0 + 6 + 9 + 7 + 0 + 9),NA,NA
 = 1/2(64),NA,NA
 = 32,NA,NA
"Table 15.1 contains the values we get for the fourth term on the right-hand side of the equation, and for ",NA,NA
"E, with the six listed tours. ",NA,NA
Table 15.1,NA,NA
Energy Levels for Six of the Valid Tours ,NA,NA
Tour # ,NA,NA
Non-Zero x’s ,NA,NA
Value for the Last ,NA,NA
Term ,NA,NA
Energy Level ,NA,NA
Comment ,NA,NA
1 ,NA,NA
x,14,NA
", x",21,NA
", x",32,NA
", x",43,NA
32 ,NA,NA
32 ,NA,NA
1 - 2 - 3 - 4 - 1 tour ,NA,NA
2 ,NA,NA
45 ,NA,NA
45 ,NA,NA
1 - 3 - 4 - 2 - 1 tour ,NA,NA
x,14,NA
", x",23,NA
", x",31,NA
", x",42,NA
3 ,NA,NA
x,14,NA
", x",22,NA
", x",33,NA
", x",41,NA
39 ,NA,NA
39 ,NA,NA
1 - 4 - 2 - 3 - 1 tour ,NA,NA
4 ,NA,NA
x,14,NA
", x",21,NA
", x",33,NA
", x",42,NA
45 ,NA,NA
45 ,NA,NA
1 - 2 - 4 - 3 - 1 tour ,NA,NA
5 ,NA,NA
x,13,NA
", x",21,NA
", x",34,NA
", x",42,NA
39 ,NA,NA
39 ,NA,NA
3 - 2 - 4 - 1 - 3 tour ,NA,NA
6 ,NA,NA
32 ,NA,NA
32 ,NA,NA
2 - 1 - 4 - 3 - 2 tour ,NA,NA
x,11,NA
", x",24,NA
", x",33,NA
", x",42,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Neural Network for Traveling Salesperson Problem,NA,NA
Hopfield and Tank used a neural network to solve a traveling salesperson problem. The ,NA,NA
solution you get may not be optimal in certain instances. But by and large you may get a ,NA,NA
solution close to an optimal solution. One cannot find for the traveling salesperson problem ,NA,NA
"a consistently efficient method of solution, as it has been proved to belong to a set of ",NA,NA
problems called NP-complete problems. NP-complete problems have the distinction that ,NA,NA
"there is no known algorithm that is efficient and practical, and there is little likelihood that ",NA,NA
such an algorithm will be developed in the future. This is a caveat to keep in mind when ,NA,NA
using a neural network to solve a traveling salesperson problem. ,NA,NA
Network Choice and Layout,NA,NA
We will describe the use of a Hopfield network to attempt to solve the traveling ,NA,NA
salesperson problem. There are ,NA,NA
n,2,NA
 neurons in the network arranged in a two-dimensional ,NA,NA
array of ,NA,NA
n,NA,NA
 neurons per row and ,NA,NA
n,NA,NA
 per column. The network is fully connected. The ,NA,NA
connections in the network in each row and in each column are lateral connections. The ,NA,NA
layout of the neurons in the network with their connections is shown in Figure 15.1 for the ,NA,NA
"case of three cities, for illustration. To avoid cluttering, the connections between ",NA,NA
diagonally opposite neurons are not shown.,NA,NA
Figure 15.1,NA,NA
  Layout of a Hopfield network for the traveling salesperson problem.,NA,NA
The most important task on hand then is finding an appropriate connection weight matrix. ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/426-429.html (1 of 4) [21/11/02 21:58:29],NA
It is constructed taking into account that nonvalid tours should be prevented and valid tours ,NA,NA
"should be preferred. A consideration in this regard is, for example, no two neurons in the ",NA,NA
same row (column) should fire in the same cycle of operation of the network. As a ,NA,NA
"consequence, the lateral connections should be for inhibition and not for excitation. ",NA,NA
"In this context, the ",NA,NA
Kronecker delta,NA,NA
 function is used to facilitate simple notation. The ,NA,NA
Kronecker delta,NA,NA
" function has two arguments, which are given usually as subscripts of the ",NA,NA
symbol ,NA,NA
,NA,NA
. By definition ,NA,NA
,ik,NA
 has value 1 if ,NA,NA
i,NA,NA
 = ,NA,NA
k,NA,NA
", and 0 if ",NA,NA
i,NA,NA
,NA,NA
k,NA,NA
". That is, the two subscripts ",NA,NA
"should agree for the Kronecker delta to have value 1. Otherwise, its value is 0.",NA,NA
"We refer to the neurons with two subscripts, one for the city it refers to, and the other for ",NA,NA
"the order of that city in the tour. Therefore, an element of the weight matrix for a ",NA,NA
"connection between two neurons needs to have four subscripts, with a comma after two of ",NA,NA
the subscripts. An example is w,"ik,lj",NA
 referring to the weight on the connection between the ,NA,NA
(,NA,NA
ik,NA,NA
) neuron and the (l,NA,NA
j,NA,NA
) neuron. The value of this weight is set as follows:,NA,NA
W,"ik,lj",NA
 = -A,1,NA
,il,NA
(1-,NA,NA
,kj,NA
)-A,2,NA
,kj,NA
(1-,NA,NA
,kj,NA
(1-,NA,NA
,il,NA
)-A,3,NA
-A,4,NA
 d,il,NA
(,NA,NA
,"j, k+1",NA
 + ,NA,NA
,"j,k-1",NA
),NA,NA
Here the negative signs indicate inhibition through the lateral connections in a row or ,NA,NA
column. The -A,3,NA
 is a term for global inhibition.,NA,NA
Inputs,NA,NA
The inputs to the network are chosen arbitrarily. If as a consequence of the choice of the ,NA,NA
"inputs, the activations work out to give outputs that add up to the number of cities, an initial ",NA,NA
"solution for the problem, a legal tour, will result. A problem may also arise that the network ",NA,NA
"will get stuck at a local minimum. To avoid such an occurrence, random noise can be ",NA,NA
added. Usually you take as the input at each neuron a constant times the number of cities ,NA,NA
"and adjust this adding a random number, which can differ for different neurons. ",NA,NA
"Activations, Outputs, and Their Updating",NA,NA
We denote the activation of the neuron in the ,NA,NA
i,NA,NA
th row and ,NA,NA
j,NA,NA
th column by ,NA,NA
a,ij,NA
", and the output ",NA,NA
is denoted by ,NA,NA
x,ij,NA
. A time constant ,NA,NA
,NA,NA
", and a gain ",NA,NA
,NA,NA
 are used as well. A constant ,NA,NA
m,NA,NA
 is another ,NA,NA
"parameter used. Also, ",NA,NA
 ,NA,NA
"t denotes the increment in time, from one cycle to the next. Keep in ",NA,NA
mind that the index for the summation ,NA,NA
©,NA,NA
" ranges from 1 to n, the number of cities. Excluded ",NA,NA
values of the index are shown by the use of the symbol ,NA,NA
,NA,NA
.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/426-429.html (2 of 4) [21/11/02 21:58:29],NA
The change in the activation is then given by ,NA,NA
 ,NA,NA
a,ij,NA
", where:",NA,NA
 ,NA,NA
a,ij,NA
 = ,NA,NA
 ,NA,NA
t (Term,1,NA
 + Term,2,NA
 + Term,3,NA
 + Term,4,NA
 + Term,5,NA
) ,NA,NA
Term,1,NA
 = - a,ij,NA
/,NA,NA
,NA,NA
Term,2,NA
 = - A,"1k
 
 j",NA
x,ik,NA
Term,3,NA
 = - A,2,NA
©,"k
 
 i",NA
x,kj,NA
Term,4,NA
 = - A,3,NA
(,NA,NA
©,i,NA
©,k,NA
x,ik,NA
 - m) ,NA,NA
Term,5,NA
 = - A,4,NA
©,"k
 
 i",NA
d,ik,NA
(x,"k,j+1",NA
 + x,"k,j-1",NA
) ,NA,NA
"To update the activation of the neuron in the ith row and jth column, you take: ",a,NA
ijnew = ,a,NA
ijold + ,NA,NA
 ,a,NA
ij ,NA,NA
The output of a neuron in the ith row and jth column is calculated from: ,NA,NA
x,in,NA
 = (1 + tanh(,NA,NA
,NA,NA
a,ij,NA
))/2,"NOTE: 
  
 , which is the original sigmoid function",NA
The function used here is the hyperbolic tangent function. The gain parameter mentioned ,NA,NA
earlier ,NA,NA
,NA,NA
" is. The output of each neuron is calculated after updating the activation. Ideally, ",NA,NA
"you want to get the outputs as 0’s and 1’s, preferably a single one for each row and each ",NA,NA
"column, to represent a tour that satisfies the conditions of the problem. But the hyperbolic ",NA,NA
"tangent function gives a real number, and you have to settle for a close enough value to 1 ",NA,NA
"or 0. You may get, for example, 0.96 instead of 1, or 0.07 instead of 0. The solution is to ",NA,NA
"be obtained from such values by rounding up or down so that 1 or 0 will be used, as the ",NA,NA
case may be,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/426-429.html (4 of 4) [21/11/02 21:58:29]",NA
Previous Table of Contents Next,NA,NA
Performance of the Hopfield Network,NA,NA
Let us now look at Hopfield and Tank’s approach at solving the TSP. ,NA,NA
Hopfield and Tank Example,NA,NA
The Hopfield network’s use in solving the traveling salesperson problem is a pioneering ,NA,NA
effort in the use of the neural network approach for this problem. Hopfield and Tank’s ,NA,NA
"example is for a problem with 10 cities. The parameters used were, ",NA,NA
A,1,NA
"= 500, ",NA,NA
A,2,NA
" = 500, ",NA,NA
A,3,NA
"= 200, ",NA,NA
A,4,NA
" = 500, ",NA,NA
,NA,NA
" = 1, ",NA,NA
,NA,NA
" = 50, and ",NA,NA
m,NA,NA
 = 15. A good solution corresponding to a local ,NA,NA
minimum for ,NA,NA
E,NA,NA
" is the expected, if not the best, solution (global minimum). An annealing ",NA,NA
"process could be considered to move out of any local minimum. As was mentioned before, ",NA,NA
the traveling salesperson problem is one of those problems for which a single approach ,NA,NA
cannot be found that will be successful in all cases. There isn’t very much guidance as to ,NA,NA
how to choose the parameters in general for the use of the Hopfield network to solve the ,NA,NA
traveling salesperson problem.,NA,NA
C++ Implementation of the Hopfield Network for the Traveling Salesperson ,NA,NA
Problem,NA,NA
We present a C++ program for the Hopfield network operation for the traveling ,NA,NA
"salesperson problem. The header file is in the Listing 15.1, and the source file is in the ",NA,NA
Listing 15.2. A ,NA,NA
tsneuron,NA,NA
 class is declared for a neuron and a ,NA,NA
network,NA,NA
 class for the ,NA,NA
network. The ,NA,NA
network,NA,NA
 class is declared a friend in the ,NA,NA
tsneuron,NA,NA
 class. The program ,NA,NA
"follows the procedure described for setting inputs, connection weights, and updating.",NA,NA
Program Details,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/429-432.html (1 of 4) [21/11/02 21:58:30],NA
The following is a listing of the characteristics of the C++ program along with definitions ,NA,NA
and/or functions. ,NA,NA
•,NA,NA
"  The number of cities, the number of iterations, and the distances between the ",NA,NA
cities are solicited from the user. ,NA,NA
•,NA,NA
  The distances are taken as ,NA,NA
integer,NA,NA
 values. If you want to use real numbers as ,NA,NA
"distances, the type for distance matrix needs to be changed to ",NA,NA
float,NA,NA
", and ",NA,NA
corresponding changes are needed for ,NA,NA
calcdist ( ),NA,NA
" function, etc. ",NA,NA
•tourcity,NA,NA
 and ,NA,NA
tourorder,NA,NA
 arrays keep track of the cities that have to be covered and ,NA,NA
the order in which it is to be done. ,NA,NA
•,NA,NA
  A neuron corresponds to each combination of a city and its order in the tour. The ,NA,NA
i,NA,NA
th city visited in the order ,NA,NA
j,NA,NA
" , is the neuron corresponding to the element ",NA,NA
j,NA,NA
 + ,NA,NA
i,NA,NA
*,NA,NA
n,NA,NA
", in ",NA,NA
the array for neurons. Here ,NA,NA
n,NA,NA
 is the number of cities. The ,NA,NA
i,NA,NA
 and the ,NA,NA
j,NA,NA
 vary from 0 to ,NA,NA
n,NA,NA
 – 1. There are ,NA,NA
n,2,NA
 neurons. ,NA,NA
•mtrx,NA,NA
 is the matrix giving the weights on the connections between the neurons. It ,NA,NA
is a square matrix of order ,NA,NA
n,2,NA
. ,NA,NA
•,NA,NA
  An input vector is generated at random in the function ,NA,NA
main ( ),NA,NA
", and is later ",NA,NA
referred to as ,NA,NA
ip,NA,NA
•asgninpt ( ),NA,NA
 function presents the input vector ,NA,NA
ip,NA,NA
 to the network and determines ,NA,NA
the initial activations of the neurons. ,NA,NA
•getacts ( ),NA,NA
 function updates the activations of the neurons after each iteration. ,NA,NA
•getouts ( ),NA,NA
 function computes the outputs after each iteration. ,NA,NA
la,NA,NA
 is used as ,NA,NA
abbreviation for lambda in its argument. ,NA,NA
•iterate ( ),NA,NA
 function carries out the number of iterations desired. ,NA,NA
•,NA,NA
  findtour ( ) function determines the tour orders of cities to be visited using the ,NA,NA
"outputs of the neurons. When used at the end of the iterations, it gives the solution ",NA,NA
obtained for the traveling salesperson problem. ,NA,NA
•,NA,NA
  calcdist ( ) function calculates the distance of the tour in the solution. ,NA,NA
Listing 15.1,NA,NA
 Header file for the C++ program for the Hopfield network for the traveling ,NA,NA
salesperson problem,NA,NA
"//trvslsmn.h  V. Rao,  H. Rao",NA,NA
#include <iostream.h>,NA,NA
#include <stdlib.h>,NA,NA
#include <math.h>,NA,NA
#include <stdio.h>,NA,NA
#define MXSIZ 11,NA,NA
class tsneuron,NA,NA
 {,NA,NA
 protected:,NA,NA
" int cit,ord;",NA,NA
 float output;,NA,NA
 float activation;,NA,NA
 friend class network;,NA,NA
 public:,NA,NA
 tsneuron() { };,NA,NA
" void getnrn(int,int);",NA,NA
 };,NA,NA
class network,NA,NA
 {,NA,NA
 public:,NA,NA
 int  citnbr;,NA,NA
" float pra,prb,prc,prd,totout,distnce;",NA,NA
 tsneuron (tnrn)[MXSIZ][MXSIZ];,NA,NA
 int dist[MXSIZ][MXSIZ];,NA,NA
 int tourcity[MXSIZ];,NA,NA
 int tourorder[MXSIZ];,NA,NA
 float outs[MXSIZ][MXSIZ];,NA,NA
 float acts[MXSIZ][MXSIZ];,NA,NA
 float mtrx[MXSIZ][MXSIZ];,NA,NA
 float citouts[MXSIZ];,NA,NA
 float ordouts[MXSIZ];,NA,NA
 network() { };,NA,NA
" void getnwk(int,float,float,float,float); ",NA,NA
void getdist(int);,NA,NA
 void findtour();,NA,NA
 void asgninpt(float *);,NA,NA
 void calcdist();,NA,NA
" void iterate(int,int,float,float,float); ",NA,NA
"void getacts(int,float,float);",NA,NA
 void getouts(float);,NA,NA
//print functions,NA,NA
 void prdist();,NA,NA
 void prmtrx(int);,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/429-432.html (3 of 4) [21/11/02 21:58:30],NA
 void ,NA,NA
prtour();,NA,NA
 void ,NA,NA
practs();,NA,NA
 void ,NA,NA
prouts();,NA,NA
 };,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Output from Your C++ Program for the Traveling Salesperson Problem,NA,NA
"A three-city tour problem is trivial, since there is just one value for the total distance no matter ",NA,NA
how you permute the cities for the order of the tour. In this case the natural order is itself an ,NA,NA
"optimal solution. The program is run for two cases, for illustration. The first run is for a ",NA,NA
"problem with four cities. The second one is for a five-city problem. By the way, the cities are ",NA,NA
numbered from 0 to ,NA,NA
n,NA,NA
 – 1. The same parameter values are used in the two runs. The number of ,NA,NA
"cities, and consequently, the matrix of distances were different. In the first run, the number of ",NA,NA
"iterations asked for is 30, and in the second run it is 40.",NA,NA
The solution you get for the four-city problem is not the one in natural order. The total distance ,NA,NA
of the tour is 32. The tour in the solution is 1 - 0 - 3 - 2 - 1. This tour is equivalent to the tour in ,NA,NA
"natural order, as you can see by starting at 0 and reading cyclically right to left in the previous ",NA,NA
sequence of cities.,NA,NA
"The solution you get for the five-city problem is the tour 1 - 2 - 0 - 4 - 3 - 1. This reads, starting ",NA,NA
"from 0 as either 0 - 4 - 3 - 1 - 2 - 0, or as 0 - 2 - 1 - 3 - 4 - 0. It is different from the tour in ",NA,NA
"natural order. It has a total distance of 73, as compared to the distance of 84 with the natural ",NA,NA
"order. It is not optimal though, as a tour of shortest distance is 0 - 4 - 2 - 1 - 3 - 0 with total ",NA,NA
distance 50.,NA,NA
"Can the program give you the shorter tour? Yes, the solution can be different if you run the ",NA,NA
"program again because each time you run the program, the input vector is different, as it is ",NA,NA
chosen at random. The parameter values given in the program are by guess.,NA,NA
Note that the seed for the random number generator is given in the statement,NA,NA
srand ((unsigned)time(NULL));,NA,NA
"The program gives you the order in the tour for each city. For example, if it says ",NA,NA
tourcity 1 tour ,NA,NA
order 2,NA,NA
", that means the second (tour) city is the city visited third (in tour order). Your tour ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/444-449.html (1 of 5) [21/11/02 21:58:32],NA
orders are also with values from 0 to ,NA,NA
n,NA,NA
" – 1, like the cities.",NA,NA
"The user input is in italic, and computer output is normal, as you have seen before.",NA,NA
Output for a Four-City Problem,NA,NA
"Please type number of cities, number of iterations ",NA,NA
4 30,NA,NA
0.097 0.0585 0.053 0.078                //input vector—there ,NA,NA
are 16 neurons in the network ,NA,NA
0.0725 0.0535 0.0585 0.0985 ,NA,NA
0.0505 0.061 0.0735 0.057 ,NA,NA
0.0555 0.075 0.0885 0.0925,NA,NA
type distance (integer) from city 0 to city 1 ,NA,NA
10 ,NA,NA
type distance (integer) from city 0 to city 2 ,NA,NA
14 ,NA,NA
type distance (integer) from city 0 to city 3 ,NA,NA
7,NA,NA
type distance (integer) from city 1 to city 2 ,NA,NA
6 ,NA,NA
type distance (integer) from city 1 to city 3 ,NA,NA
12,NA,NA
type distance (integer) from city 2 to city 3 ,NA,NA
9,NA,NA
 Distance Matrix ,NA,NA
0  10  14  7 ,NA,NA
10  0  6  12 ,NA,NA
14  6  0  9 ,NA,NA
7  12  9  0,NA,NA
 Weight Matrix              //16x16 matrix of weights. There ,NA,NA
are 16 neurons in the network.,NA,NA
-30  -70   -70  -70,NA,NA
-70  -630  -30  -630,NA,NA
-70  -870  -30  -70,NA,NA
-30  -70   -70  -630,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/444-449.html (2 of 5) [21/11/02 21:58:32],NA
-70   -30  -70  -,NA,NA
70,NA,NA
-630  -70  -630 -,NA,NA
30,NA,NA
-870  -70  -870 -,NA,NA
70,NA,NA
-70   -30  -70  -,NA,NA
30,NA,NA
-70  -70   -30  -70,NA,NA
-30  -630  -70  -630,NA,NA
-30  -870  -70  -70,NA,NA
-70  -70   -30  -630,NA,NA
-70   -70  -70   -,NA,NA
30,NA,NA
-630  -30  -630  -,NA,NA
70,NA,NA
-870  -30  -870  -,NA,NA
70,NA,NA
-630  -30  -630  -,NA,NA
30,NA,NA
-70  -630  -30  -630,NA,NA
-30  -70   -70  -70,NA,NA
-70  -390  -30  -630,NA,NA
-70  -630  -30  -70,NA,NA
-630  -70  -630  -,NA,NA
30,NA,NA
-70   -30  -70   -,NA,NA
70,NA,NA
-390  -70  -390  -,NA,NA
30,NA,NA
-630  -70  -630  -,NA,NA
70,NA,NA
-30  -630  -70  -630,NA,NA
-70  -70   -30  -70,NA,NA
-30  -390  -70  -630,NA,NA
-30  -630  -70  -70,NA,NA
-630  -30  -630  -,NA,NA
70,NA,NA
-70   -70  -70   -,NA,NA
30,NA,NA
-390  -30  -390  -,NA,NA
70,NA,NA
-870  -30  -870  -,NA,NA
70,NA,NA
-70  -870  -30  -,NA,NA
870,NA,NA
-70  -390  -30  -,NA,NA
390,NA,NA
-30  -70   -70  -,NA,NA
870,NA,NA
-70  -870  -30  -,NA,NA
390,NA,NA
-870  -70  -870  -,NA,NA
30,NA,NA
-390  -70  -390  -,NA,NA
30,NA,NA
-70   -30  -70   -,NA,NA
30,NA,NA
-870  -70  -870  -,NA,NA
30,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/444-449.html (3 of 5) [21/11/02 21:58:32],NA
-30  -870  -70  -,NA,NA
870,NA,NA
-30  -390  -70  -,NA,NA
390,NA,NA
-70  -70   -30  -,NA,NA
870,NA,NA
-30  -870  -70  -,NA,NA
390,NA,NA
-870  -30  -870  -,NA,NA
70,NA,NA
-390  -30  -390  -,NA,NA
70,NA,NA
-70   -70  -70   -,NA,NA
70,NA,NA
-450  -30  -450  -,NA,NA
70,NA,NA
-70  -450  -30  -,NA,NA
450,NA,NA
-70  -750  -30  -,NA,NA
750,NA,NA
-70  -570  -30  -,NA,NA
450,NA,NA
-70  -450  -30  -,NA,NA
750,NA,NA
-450  -70  -450  -,NA,NA
30,NA,NA
-750  -70  -750  -,NA,NA
30,NA,NA
-570  -70  -570  -,NA,NA
30,NA,NA
-450  -70  -450  -,NA,NA
30,NA,NA
-30  -450  -70  -,NA,NA
450,NA,NA
-30  -750  -70  -,NA,NA
750,NA,NA
-30  -570  -70  -,NA,NA
450,NA,NA
-30  -450  -70  -,NA,NA
750,NA,NA
-450  -30  -450  -,NA,NA
70,NA,NA
-750  -30  -750  -,NA,NA
70,NA,NA
-570  -30  -570  -,NA,NA
70,NA,NA
-70   -70  -70   -,NA,NA
30,NA,NA
initial activations,NA,NA
 the activations:,NA,NA
-333.680054  -215.280014  -182.320023  -,NA,NA
371.280029-255.199997  -207.580002  -205.920013  -,NA,NA
425.519989-258.560028  -290.360016  -376.320007  -,NA,NA
228.000031-278.609985  -363  -444.27005  -,NA,NA
377.400024,NA,NA
the outputs ,NA,NA
0.017913  0.070217  0.100848  0.011483 ,NA,NA
0.044685  0.076494  0.077913  0.006022 ,NA,NA
0.042995  0.029762  0.010816  0.060882 ,NA,NA
0.034115  0.012667  0.004815  0.010678,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/444-449.html (4 of 5) [21/11/02 21:58:32],NA
 30 iterations completed,NA,NA
 the activations:,NA,NA
-222.586884  -176.979172  -195.530823  -,NA,NA
380.166107-164.0271    -171.654053  -214.053177  -,NA,NA
421.249023-158.297867  -218.833755  -319.384827  -,NA,NA
245.097473-194.550751  -317.505554  -437.527283  -,NA,NA
447.651581 the outputs ,NA,NA
0.064704  0.10681  0.087355  0.010333 ,NA,NA
0.122569  0.113061  0.071184  0.006337 ,NA,NA
0.130157  0.067483  0.021194  0.050156 ,NA,NA
0.088297  0.021667  0.005218  0.004624 ,NA,NA
tourcity 0 tour order 1 ,NA,NA
tourcity 1 tour order 0 ,NA,NA
tourcity 2 tour order 3 ,NA,NA
tourcity 3 tour order 2,NA,NA
 the tour : ,NA,NA
1 ,NA,NA
0 ,NA,NA
3 ,NA,NA
2,NA,NA
 distance of tour is : 32,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/444-449.html (5 of 5) [21/11/02 21:58:32]",NA
Previous Table of Contents Next,NA,NA
Output for a Five-City Problem,NA,NA
"Please type number of cities, number of iterations ",NA,NA
5 40,NA,NA
0.0645 0.069 0.0595 0.0615 0.0825   //input vector—there are ,NA,NA
25 neurons in the network ,NA,NA
0.074 0.0865 0.056 0.095 0.06 ,NA,NA
0.0625 0.0685 0.099 0.0645 0.0615 ,NA,NA
0.056 0.099 0.065 0.093 0.051 ,NA,NA
0.0675 0.094 0.0595 0.0635 0.0515,NA,NA
type distance (integer) from city 0 to city 1 ,NA,NA
10 ,NA,NA
type distance (integer) from city 0 to city 2 ,NA,NA
14 ,NA,NA
type distance (integer) from city 0 to city 3 ,NA,NA
7 ,NA,NA
type distance (integer) from city 0 to city 4 ,NA,NA
6,NA,NA
type distance (integer) from city 1 to city 2 ,NA,NA
12 ,NA,NA
type distance (integer) from city 1 to city 3 ,NA,NA
9 ,NA,NA
type distance (integer) from city 1 to city 4 ,NA,NA
18,NA,NA
type distance (integer) from city 2 to city 3 ,NA,NA
24 ,NA,NA
type distance (integer) from city 2 to city 4 ,NA,NA
16,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/449-455.html (1 of 6) [21/11/02 21:58:33],NA
type distance (integer) from city 3 to city 4 ,NA,NA
32,NA,NA
 Distance Matrix ,NA,NA
0   10  14   7  6 ,NA,NA
10   0  12   9  18 ,NA,NA
14  12  0   24  16 ,NA,NA
7   9   24   0  32 ,NA,NA
6   18  16  32  0,NA,NA
Weight Matrix    //25x25 matrix of weights. There are 25 ,NA,NA
neurons in the network.,NA,NA
-30  -70   -70  -70   -70,NA,NA
-70  -630  -30  -30   -630,NA,NA
-70  -70   -30  -70   -70,NA,NA
-70  -630  -70  -630  -30,NA,NA
-30  -870  -70  -70   -30,NA,NA
-70   -30  -70   -70  -70,NA,NA
-630  -70  -630  -30  -30,NA,NA
-870  -70  -70   -30  -70,NA,NA
-70   -30  -630  -70  -630,NA,NA
-30   -30  -70   -70  -70,NA,NA
-70  -70   -30  -70   -,NA,NA
70,NA,NA
-30  -630  -70  -630  -,NA,NA
30,NA,NA
-30  -70   -70  -70   -,NA,NA
30,NA,NA
-70  -30   -30  -630  -,NA,NA
70,NA,NA
-630 -30   -70  -70   -,NA,NA
70,NA,NA
-70  -70   -70   -30   -70,NA,NA
-30  -30   -630  -70   -630,NA,NA
-30  -70   -70   -70   -70,NA,NA
-30  -630  -30   -30   -630,NA,NA
-70  -870  -70   -630  -30,NA,NA
-70   -70  -70   -70   -30,NA,NA
-630  -30  -30   -630  -70,NA,NA
-870  -70  -630  -30   -30,NA,NA
-630  -30  -70   -70   -70,NA,NA
-70   -70  -630  -70   -,NA,NA
630,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/449-455.html (2 of 6) [21/11/02 21:58:33],NA
-70  -630  -30  -30   -630,NA,NA
-30  -70   -70  -70   -70,NA,NA
-70  -630  -70  -630  -30,NA,NA
-30  -70   -30  -70   -70,NA,NA
-70  -750  -30  -630  -70,NA,NA
-630  -70  -630  -30  -30,NA,NA
-70   -30  -70   -70  -70,NA,NA
-750  -30  -630  -70  -630,NA,NA
-30   -70  -70   -30  -70,NA,NA
-70   -30  -30   -30  -630,NA,NA
-30  -630  -70   -630  -,NA,NA
30,NA,NA
-70  -70   -30   -70   -,NA,NA
70,NA,NA
-30  -30   -30   -630  -,NA,NA
70,NA,NA
-630 -70   -70   -70   -,NA,NA
30,NA,NA
-70  -30   -630  -30   -,NA,NA
30,NA,NA
-30  -30   -630  -70   -630,NA,NA
-70  -70   -70   -30   -70,NA,NA
-30  -630  -30   -30   -630,NA,NA
-70  -70   -70   -70   -70,NA,NA
-30  -750  -70   -870  -30,NA,NA
-630  -30  -30   -630  -70,NA,NA
-70   -70  -70   -70   -30,NA,NA
-750  -70  -870  -30   -30,NA,NA
-870  -70  -750  -30   -30,NA,NA
-750  -30  -870  -70   -,NA,NA
870,NA,NA
-70  -870  -30  -30   -870,NA,NA
-70  -750  -30  -30   -750,NA,NA
-30  -870  -70  -870  -30,NA,NA
-30  -750  -70  -750  -30,NA,NA
-30  -70   -30  -870  -70,NA,NA
-870  -70  -870  -30  -30,NA,NA
-750  -70  -750  -30  -30,NA,NA
-70   -30  -870  -70  -870,NA,NA
-30   -30  -750  -70  -750,NA,NA
-30   -70  -30   -30  -870,NA,NA
-30   -870  -70  -870  -30,NA,NA
-30   -750  -70  -750  -30,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/449-455.html (3 of 6) [21/11/02 21:58:33],NA
-70   -30   -30  -870  -,NA,NA
70,NA,NA
-870  -30   -30  -750  -,NA,NA
70,NA,NA
-750  -70   -870 -30   -,NA,NA
30,NA,NA
-30  -30   -870  -70   -,NA,NA
870,NA,NA
-30  -30   -750  -70   -,NA,NA
750,NA,NA
-70  -870  -30   -30   -,NA,NA
870,NA,NA
-70  -750  -30   -30   -,NA,NA
750,NA,NA
-70  -70   -70   -450  -30,NA,NA
-870  -30  -30   -870  -70,NA,NA
-750  -30  -30   -750  -70,NA,NA
-70   -70  -450  -30   -30,NA,NA
-450  -70  -570  -30   -30,NA,NA
-570  -70  -450  -70   -,NA,NA
450,NA,NA
-70  -450  -30  -30   -450,NA,NA
-70  -570  -30  -30   -570,NA,NA
-70  -450  -70  -450  -30,NA,NA
-30  -570  -70  -570  -30,NA,NA
-30  -1470 -30  -450  -70,NA,NA
-450  -70  -450  -30  -30,NA,NA
-570  -70  -570  -30  -30,NA,NA
-1470 -30  -450  -70  -450,NA,NA
-30   -30  -570  -70  -570,NA,NA
-30   -30  -30   -30  -450,NA,NA
-30  -450  -70   -450  -,NA,NA
30,NA,NA
-30  -570  -70   -570  -,NA,NA
30,NA,NA
-30  -30   -30   -450  -,NA,NA
70,NA,NA
-450 -30   -30   -570  -,NA,NA
70,NA,NA
-570 -30   -450  -30   -,NA,NA
30,NA,NA
-30  -30    -450  -70   -,NA,NA
450,NA,NA
-30  -30    -570  -70   -,NA,NA
570,NA,NA
-30  -450   -30   -30   -,NA,NA
450,NA,NA
-70  -570   -30   -30   -,NA,NA
570,NA,NA
-70  -1470  -70   -390  -30,NA,NA
-450   -30   -30    -450  -,NA,NA
70,NA,NA
-570   -30   -30    -570  -,NA,NA
70,NA,NA
-1470  -70   -390   -30   -,NA,NA
30,NA,NA
-390   -70   -1110  -30   -,NA,NA
30,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/449-455.html (4 of 6) [21/11/02 21:58:33],NA
-1110  -70   -390   -70   -390,NA,NA
-70  -390   -30  -30    -390,NA,NA
-70  -1110  -30  -30    -1110,NA,NA
-70  -390   -70  -390   -30,NA,NA
-30  -1110  -70  -1110  -30,NA,NA
-30  -990   -30  -390   -70,NA,NA
-390   -70  -390   -30  -30,NA,NA
-1110  -70  -1110  -30  -30,NA,NA
-990   -30  -390   -70  -390,NA,NA
-30    -30  -1110  -70  -1110,NA,NA
-30    -30  -30    -30  -390,NA,NA
-30    -390   -70   -390   -,NA,NA
30,NA,NA
-30    -1110  -70   -1110  -,NA,NA
30,NA,NA
-30    -30    -30   -390   -,NA,NA
70,NA,NA
-390   -30    -30   -1110  -,NA,NA
70,NA,NA
-1110  -30    -390  -30    -,NA,NA
30,NA,NA
-30  -30    -390   -70  -390,NA,NA
-30  -30    -1110  -70  -1110,NA,NA
-30  -390   -30    -30  -390,NA,NA
-70  -1110  -30    -30  -1110,NA,NA
-70  -990   -30    -30  -990,NA,NA
-390  -30  -30  -390   -,NA,NA
70,NA,NA
-1110 -30  -30  -1110  -,NA,NA
70,NA,NA
-990  -30  -30  -990   -,NA,NA
70,NA,NA
-1950 -30  -30  -1950  -,NA,NA
70,NA,NA
-70   -70  -70  -70    -,NA,NA
30,NA,NA
initial activations,NA,NA
 the activations:,NA,NA
-290.894989  -311.190002  -218.365005  -309.344971  -,NA,NA
467.774994-366.299957  -421.254944  -232.399963  -489.249969  ,NA,NA
-467.399994-504.375     -552.794983  -798.929871  -496.005005  ,NA,NA
-424.964935-374.639984  -654.389832  -336.049988  -612.870056  ,NA,NA
-405.450012-544.724976  -751.060059  -418.285034  -545.465027  ,NA,NA
-500.065063,NA,NA
the outputs ,NA,NA
0.029577  0.023333  0.067838  0.023843  ,NA,NA
0.003636 0.012181  0.006337  0.057932  0.002812  ,NA,NA
0.003652,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/449-455.html (5 of 6) [21/11/02 21:58:33],NA
0.002346  0.001314  6.859939e-05  0.002594  0.006062 ,NA,NA
0.011034  0.000389  0.017419  0.000639  0.00765 ,NA,NA
0.001447  0.000122  0.006565  0.001434  0.002471,NA,NA
40 iterations completed,NA,NA
 the activations:,NA,NA
-117.115494  -140.58519   -85.636215   -158.240143  -,NA,NA
275.021301-229.135956  -341.123871  -288.208496  -536.142212  ,NA,NA
-596.154297-297.832794  -379.722595  -593.842102  -440.377625  ,NA,NA
-442.091064-209.226883  -447.291016  -283.609589  -519.441101  ,NA,NA
-430.469696-338.93219   -543.509766  -386.950531  -538.633606  ,NA,NA
-574.604492,NA,NA
the outputs ,NA,NA
0.196963  0.156168  0.263543  0.130235  ,NA,NA
0.035562 0.060107  0.016407  0.030516  0.001604  ,NA,NA
0.000781 0.027279  0.010388  0.000803  0.005044  ,NA,NA
0.004942 0.07511   0.004644  0.032192  0.001959  ,NA,NA
0.005677 0.016837  0.001468  0.009533  0.001557  ,NA,NA
0.001012,NA,NA
tourcity 0 tour order 2,NA,NA
tourcity 1 tour order 0,NA,NA
tourcity 2 tour order 1,NA,NA
tourcity 3 tour order 4,NA,NA
tourcity 4 tour order 3,NA,NA
 the tour : ,NA,NA
1 ,NA,NA
2 ,NA,NA
0 ,NA,NA
4 ,NA,NA
3,NA,NA
 distance of tour is : 73,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Other Approaches to Solve the Traveling Salesperson Problem,NA,NA
The following describes a few other methods for solving the traveling salesperson ,NA,NA
problem. ,NA,NA
Anzai’s Presentation,NA,NA
Yuichiro Anzai describes the Hopfield network for the traveling salesperson problem in a ,NA,NA
"slightly different way. For one thing, a ",NA,NA
global inhibition term,NA,NA
 is not used. A ,NA,NA
threshold ,NA,NA
"value is associated with each neuron, added to the ",NA,NA
activation,NA,NA
", and taken as the average of ",NA,NA
A,1,NA
 and ,NA,NA
A,2,NA
", using our earlier notation. The ",NA,NA
energy,NA,NA
 function is formulated slightly ,NA,NA
"differently, as follows:",NA,NA
E = A,1,NA
©,1,NA
(,NA,NA
©,k,NA
 x,ik,NA
-1),2,NA
 + A,2,NA
©,i,NA
(,NA,NA
©,k,NA
 x,ki,NA
-1),2,NA
 + A,4,NA
©,k,NA
©,"j
 
 k",NA
©,"i
 
 k",NA
 d,kj,NA
 x,ki,NA
(x,"j,i+1",NA
 + x,"j,i-1",NA
),NA,NA
The first term is 0 if the sum of the outputs is 1 in each column. The same is true for the ,NA,NA
second term with regard to rows.,NA,NA
The output is calculated using a parameter ,NA,NA
,NA,NA
", here called the ",NA,NA
reference activation level,NA,NA
", as:",NA,NA
x,ij,NA
 = (1 + tan tanh(a,ij,NA
/,NA,NA
,NA,NA
))/2,NA,NA
The parameters used are ,NA,NA
A,1,NA
" = 1/2, ",NA,NA
A,2,NA
" = 1/2, ",NA,NA
A,4,NA
" = 1/2, ",NA,NA
 ,NA,NA
t,NA,NA
" = 1, ",NA,NA
,NA,NA
" = 1, and ",NA,NA
,NA,NA
 = 1. An attempt ,NA,NA
"is made to solve the problem for a tour of 10 cities. The solution obtained is not crisp, in ",NA,NA
"the sense that exactly one 1 occurs in each row and each column, there are values of ",NA,NA
varying magnitude with one dominating value in each row and column. The prominent ,NA,NA
value is considered to be part of the solution.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/455-457.html (1 of 3) [21/11/02 21:58:34],NA
Kohonen’s Approach for the Traveling Salesperson Problem,NA,NA
Kohonen’s self-organizing maps can be used for the traveling salesperson problem. We ,NA,NA
summarize the discussion of this approach described in Eric Davalo’s and Patrick Naim’s ,NA,NA
work. Each city considered for the tour is referenced by its ,NA,NA
x,NA,NA
 and ,NA,NA
y,NA,NA
 coordinates. To each ,NA,NA
"city there corresponds a neuron. The neurons are placed in a single array, unlike the two-",NA,NA
dimensional array used in the Hopfield approach. The first and the last neurons in the array ,NA,NA
are considered to be neighbors.,NA,NA
"There is a weight vector for each neuron, and it also has two components. The weight ",NA,NA
"vector of a neuron is the image of the neuron in the map, which is sought to self-organize. ",NA,NA
"There are as many input vectors as there are cities, and the coordinate pair of a city ",NA,NA
constitutes an input vector. A neuron with a weight vector closest to the input vector is ,NA,NA
"selected. The weights of neurons in a neighborhood of the selected neuron are modified, ",NA,NA
others are not. A gradually reducing scale factor is also used for the modification of ,NA,NA
weights.,NA,NA
"One neuron is created first, and its weight vector has 0 for its components. Other neurons ",NA,NA
"are created one at a time, at each iteration of learning. Neurons may also be destroyed. The ",NA,NA
creation of the neuron and destruction of the neuron suggest adding a city provisionally to ,NA,NA
the final list in the tour and dropping a city also provisionally from that list. Thus the ,NA,NA
possibility of assigning any neuron to two inputs or two cities is prevented. The same is ,NA,NA
true about assigning two neurons to the same input.,NA,NA
"As the input vectors are presented to the network, if an unselected neuron falls in the ",NA,NA
"neighborhood of the closest twice, it is created. If a created neuron is not selected in three ",NA,NA
"consecutive iterations for modification of weights, along with others being modified, it is ",NA,NA
destroyed.,NA,NA
That a tour of shortest distance results from this network operation is apparent from the fact ,NA,NA
that the closest neurons are selected. It is reported that experimental results are very ,NA,NA
"promising. The computation time is small, and solutions somewhat close to the optimal are ",NA,NA
"obtained, if not the optimal solution itself. As was before about the traveling salesperson ",NA,NA
"problem, this is an NP-complete problem and near efficient (leading to suboptimal ",NA,NA
"solutions, but faster) approaches to it should be accepted.",NA,NA
Algorithm for Kohonen’s Approach,NA,NA
A gain parameter ,NA,NA
,NA,NA
 and a scale factor ,NA,NA
q,NA,NA
 are used while modifying the weights. A value ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/455-457.html (2 of 3) [21/11/02 21:58:34],NA
between 0.02 and 0.2 was tried in previous examples for ,NA,NA
q,NA,NA
. A distance of a neuron from the ,NA,NA
selected neuron is defined to be an integer between 0 and ,NA,NA
n,NA,NA
" – 1, where ",NA,NA
n,NA,NA
 is the number of ,NA,NA
cities for the tour. This means that these distances are not necessarily the actual distances ,NA,NA
between the cities. They could be made representative of the actual distances in some way. ,NA,NA
One such attempt is described in the following discussion on C++ implementation. This ,NA,NA
distance is denoted by ,NA,NA
d,j,NA
 for neuron ,NA,NA
j,NA,NA
. A ,NA,NA
squashing,NA,NA
 function similar to the ,NA,NA
Gaussian ,NA,NA
density,NA,NA
 function is also used.,NA,NA
The details of the algorithm are in a paper referenced in Davalo. The steps of the algorithm ,NA,NA
to the extent given by Davalo are:,NA,NA
•,NA,NA
  Find the weight vector for which the distance from the input vector is the smallest ,NA,NA
•,NA,NA
  Modify the weights using ,NA,NA
w,jnew,NA
 = w,jold,NA
 + (l,new,NA
 - w,jold,NA
)g(,NA,NA
,NA,NA
",d",j,NA
"), where g(",NA,NA
,NA,NA
",dj) = exp(- dj",2,NA
/,NA,NA
,NA,NA
) /,NA,NA
 ,NA,NA
•,NA,NA
  Reset ,NA,NA
,NA,NA
 as ,NA,NA
,NA,NA
 (1 - ,NA,NA
q,NA,NA
) ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
C++ Implementation of Kohonen’s Approach,"Our C++ implementation of this algorithm (described above) is with small modifications. We create but do 
 not destroy neurons explicitly. That is, we do not count the number of consecutive iterations in which a 
 neuron is not selected for modification of weights. This is a consequence of our not defining a neighborhood 
 of a neuron. Our example is for a problem with five neurons, for illustration, and because of the small number 
 of neurons involved, the entire set is considered a neighborhood of each neuron. 
  
 When all but one neuron are created, the remaining neuron is created without any more work with the 
 algorithm, and it is assigned to the input, which isn’t corresponded yet to a neuron. After creating 
 n
  – 1 
 neurons, only one unassigned input should remain.
  
 In our C++ implementation, the distance matrix for the distances between neurons, in our example, is given 
 as follows, following the stipulation in the algorithm that these values should be integers between 0 and 
 n
  – 
 1.
  
  0 1 2 3 4
  
  1 0 1 2 3
  
 d
  =    2 1 0 1 2
  
  3 2 1 0 1
  
  4 3 2 1 0
  
 We also ran the program by replacing the previous matrix with the following matrix and obtained the same 
 solution. The actual distances between the cities are about four times the corresponding values in this matrix, 
 more or less. We have not included the output from this second run of the program. 
  
  0 1 3 3 2
  
  1 0 3 2 1
  
 d
  =   3 3 0 4 2
  
  3 2 4 0 1
  
  2 1 2 1 0
  
 In our implementation, we picked a function similar to the 
 Gaussian density
  function as the 
 squashing 
 function. The 
 squashing
  function used is:
  
 f(d,
 
 ) = exp( -d
 2
 /
 
 ) / 
  
  (2 )",NA
Header File for C++ Program for Kohonen’s Approach,"Listing 15.3 contains the header file for this program, and listing 15.4 contains the corresponding source file: 
  
 Listing 15.3
  Header file for C++ program for Kohonen’s approach
  
 //tsp_kohn.h V.Rao, H.Rao 
  
 #include<iostream.h> 
  
 #include<math.h>
  
 #define MXSIZ 10 
  
 #define pi 3.141592654
  
 class city_neuron
  
  
  {
  
  
  protected:
  
  
  double x,y;
  
  
  int mark,order,count;
  
  
  double weight[2];
  
  
  friend class tspnetwork;
  
  
  public:
  
  
  city_neuron(){};
  
  
  void get_neuron(double,double);
  
  };
  
 class tspnetwork
  
  
  {
  
  
  protected:
  
  
  int chosen_city,order[MXSIZ];
  
  
  double gain,input[MXSIZ][2];
  
  
  int citycount,index,d[MXSIZ][MXSIZ];
  
  
  double gain_factor,diffsq[MXSIZ];
  
  
  city_neuron (cnrn)[MXSIZ];
  
  
  public:
  
  
  tspnetwork(int,double,double,double,double*,double*);
  
  void 
 get_input(double*,double*);
  
  
  void get_d();
  
  
  void find_tour();
  
  
  void associate_city();
  
  
  void modify_weights(int,int);
  
  
  double wtchange(int,int,double,double);
  
  
  void print_d();
  
  
  void print_input();
  
  
  void print_weights();
  
  
  void print_tour();
  
  
  };
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/457-467.html (2 of 8) [21/11/02 21:58:35]",NA
Source File Listing,"The following is the source file listing for the Kohonen approach to the traveling salesperson problem. 
  
 Listing 15.4
  Source file for C++ program for Kohonen’s approach
  
 //tsp_kohn.cpp  V.Rao, H.Rao 
  
 #include “tsp_kohn.h”
  
 void city_neuron::get_neuron(double a,double b) 
 {
  
  x = a;
  
  y = b;
  
  mark = 0;
  
  count = 0;
  
  weight[0] = 0.0;
  
  weight[1] = 0.0;
  
  };
  
 tspnetwork::tspnetwork(int k,double f,double q,double h, 
 double *ip0,double *ip1)
  
  {
  
  int i;
  
  gain = h;
  
  gain_factor = f;
  
  citycount = k;
  
  // distances between neurons as integers between 0 and n-1
  
  get_d();
  
  print_d();
  
  cout<<”\n”;
  
  // input vectors
  
  get_input(ip0,ip1);
  
  print_input();
  
  // neurons in the network
  
  for(i=0;i<citycount;++i)
  
  {
  
  order[i] = citycount+1;
  
  diffsq[i] = q;
  
  cnrn[i].get_neuron(ip0[i],ip1[i]); 
 cnrn[i].order = citycount +1;
  
  }
  
  }
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/457-467.html (3 of 8) [21/11/02 21:58:35]",NA
Previous Table of Contents Next,NA,NA
Output from a Sample Program Run,NA,NA
"The program, as mentioned, is created for the Kohonen approach to the traveling ",NA,NA
salesperson problem for five cities. There is no user input from the keyboard. All ,NA,NA
parameter values are given to the program with appropriate statements in the function ,NA,NA
"main. A scale factor of 0.05 is given to apply to the gain parameter, which is given as 1. ",NA,NA
"Initially, the distance of each neuron weight vector from an input vector is set at 1000, to ",NA,NA
"facilitate finding the closest for the first time. The cities with coordinates (7,3), (4,6), ",NA,NA
"(14,13), (0,12), (5,10) are specified for input vectors. ",NA,NA
"The tour found is not the one in natural order, namely 0 ",NA,NA
¡,NA,NA
 1 ,NA,NA
¡,NA,NA
 2 ,NA,NA
¡,NA,NA
 3 ,NA,NA
¡,NA,NA
 4 ,NA,NA
¡,NA,NA
" 0, with a ",NA,NA
distance of 43.16. The tour found has the order 0 ,NA,NA
¡,NA,NA
 3 ,NA,NA
¡,NA,NA
 1 ,NA,NA
¡,NA,NA
 4 ,NA,NA
¡,NA,NA
 2 ,NA,NA
¡,NA,NA
" 0, which covers a ",NA,NA
"distance of 44.43, which is slightly higher, as shown in Figure 15.2. The best tour, 0 ",NA,NA
¡,NA,NA
 2 ,NA,NA
¡,NA,NA
4 ,NA,NA
¡,NA,NA
 3 ,NA,NA
¡,NA,NA
 1 ,NA,NA
¡,NA,NA
 0 has a total distance of 38.54.,NA,NA
Figure 15.2,NA,NA
  City placement and tour found for TSP.,NA,NA
"Table 15.2 gives for the five-city example, the 12 (5!/10) distinct tour distances and ",NA,NA
"corresponding representative tours. These are not generated by the program, but by ",NA,NA
enumeration and calculation by hand. This table is provided here for you to see the ,NA,NA
different solutions for this five-city example of the traveling salesperson problem. ,NA,NA
Table 15.2,NA,NA
Distances and Representative Tours for Five-City Example ,NA,NA
Distance ,NA,NA
Tour ,NA,NA
Comment ,NA,NA
49.05 ,NA,NA
0-3-2-1-4-0 ,NA,NA
worst case ,NA,NA
47.59 ,NA,NA
0-3-1-2-4-0 ,NA,NA
tour given by the program ,NA,NA
45.33 ,NA,NA
0-2-1-4-3-0 ,NA,NA
44.86 ,NA,NA
0-2-3-1-4-0 ,NA,NA
44.43 ,NA,NA
0-3-1-4-2-0 ,NA,NA
44.30 ,NA,NA
0-2-1-3-4-0 ,NA,NA
optimal tour ,NA,NA
43.29 ,NA,NA
0-1-4-2-3-0 ,NA,NA
43.16 ,NA,NA
0-1-2-3-4-0 ,NA,NA
42.73 ,NA,NA
0-1-2-4-3-0 ,NA,NA
42.26 ,NA,NA
0-1-3-2-4-0 ,NA,NA
40.00 ,NA,NA
0-1-4-3-2-0 ,NA,NA
38.54 ,NA,NA
0-2-4-3-1-0 ,NA,NA
"There are 12 different distances you can get for tours with these cities by hand calculation, ",NA,NA
and four of these are higher and seven are lower than the one you find from this program. ,NA,NA
"The worst case tour (0 [rarr] 3 [rarr] 2 [rarr] 1 [rarr] 4 [rarr] 0) gives a distance of 49.05, ",NA,NA
"and the best, as you saw above, 38.54. The solution from the program is at about the middle ",NA,NA
"of the best and worst, in terms of total distance traveled. ",NA,NA
The output of the program being all computer generated is given below as follows:,NA,NA
d: 0   1   2   3   ,NA,NA
4 ,NA,NA
d: 1   0   1   2   ,NA,NA
3 ,NA,NA
d: 2   1   0   1   ,NA,NA
2 ,NA,NA
d: 3   2   1   0   ,NA,NA
1 ,NA,NA
d: 4   3   2   1   ,NA,NA
0,NA,NA
input : 7   3 ,NA,NA
input : 4   6 ,NA,NA
input : 14  13 ,NA,NA
input : 0   12 ,NA,NA
input : 5   10,NA,NA
weight: 1.595769   2.393654,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/467-470.html (2 of 4) [21/11/02 21:58:36],NA
weight: 3.289125e-09   4.933688e-09 ,NA,NA
weight: 2.880126e-35   4.320189e-35 ,NA,NA
weight: 1.071429e-78   1.607143e-78 ,NA,NA
weight: 1.693308e-139  2.539961e-,NA,NA
139,NA,NA
weight: 1.595769   2.393654 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 2.880126e-35   4.320189e-35 ,NA,NA
weight: 1.071429e-78   1.607143e-78 ,NA,NA
weight: 1.693308e-139  2.539961e-139,NA,NA
weight: 1.595769   2.393654 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 1.071429e-78   1.607143e-78 ,NA,NA
weight: 1.693308e-139  2.539961e-139,NA,NA
weight: 1.595769   2.393654 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 1.693308e-139   2.539961e-139,NA,NA
weight: 1.595769   2.393654 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625 ,NA,NA
weight: 5.585192   5.18625,NA,NA
tour : 0 –> 3–> 1 –> 4 –> 2–> 0,NA,NA
"(7, 3) –> (0, 12) –> (4, 6) –> (5, 10) –> (14, 13) –> (7, 3)",NA,NA
Optimizing a Stock Portfolio,NA,NA
Development of a neural network approach to a stock selection process in securities trading ,NA,NA
is similar to the application of neural networks to nonlinear optimization problems. ,NA,NA
The seminal work of Markowitz in making a mathematical formulation of an ,NA,NA
objective ,NA,NA
function in the context of ,NA,NA
portfolio selection,NA,NA
 forms a basis for such a development. There is ,NA,NA
"risk to be minimized or put a cap on, and there are profits to be maximized. Investment ",NA,NA
capital is a limited resource naturally.,NA,NA
The ,NA,NA
objective,NA,NA
 function is formulated in such a way that the optimal portfolio minimizes the ,NA,NA
objective,NA,NA
 function. There would be a term in the objective function involving the product ,NA,NA
of each pair of stock prices. The covariance of that pair of prices is also used in the ,NA,NA
objective,NA,NA
 function. A product renders the objective function a quadratic. There would of ,NA,NA
"course be some linear terms as well, and they represent the individual stock prices with the ",NA,NA
stock’s average return as coefficient in each such term. You already get the idea that this ,NA,NA
"optimization problem falls into the category of quadratic programming problems, which ",NA,NA
result in real number values for the variables in the optimal solution. Some other terms ,NA,NA
would also be included in the objective function to make sure that the constraints of the ,NA,NA
problem are satisfied.,NA,NA
A practical consideration is that a real number value for the amount of a stock may be ,NA,NA
"unrealistic, as fractional numbers of stocks may not be purchased. It makes more sense to ",NA,NA
ask that the variables be taking 0 or 1 only. The implication then is that either you buy a ,NA,NA
"stock, in which case you include it in the portfolio, or you do not buy at all. This is what is ",NA,NA
usually called a ,NA,NA
zero-one programming problem,NA,NA
. You also identify it as a combinatorial ,NA,NA
problem.,NA,NA
You already saw a combinatorial optimization problem in the traveling salesperson ,NA,NA
problem. The constraints were incorporated into special terms in the ,NA,NA
objective,NA,NA
" function, so ",NA,NA
that the only function to be computed is the ,NA,NA
objective,NA,NA
 function.,NA,NA
Deeming the ,NA,NA
objective,NA,NA
" function as giving the energy of a network in a given state, the ",NA,NA
simulated annealing paradigm and the Hopfield network can be used to solve the problem. ,NA,NA
"You then have a neural network in which each neuron represents a stock, and the size of the ",NA,NA
layer is determined by the number of stocks in the pool from which you want to build your ,NA,NA
stock portfolio. The paradigm suggested here strives to minimize the energy of the ,NA,NA
machine. The ,NA,NA
objective,NA,NA
 function needs therefore to be stated for minimization to get the ,NA,NA
best portfolio possible.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/467-470.html (4 of 4) [21/11/02 21:58:36]",NA
Previous Table of Contents Next,NA,NA
Tabu Neural Network,NA,NA
Tabu search,NA,NA
", popularized by Fred Glover with his contributions, is a paradigm that has ",NA,NA
been used successfully in many optimization problems. It is a method that can steer a ,NA,NA
"search procedure from a limited domain to an extended domain, so as to seek a solution ",NA,NA
that is better than a local minimum or a local maximum.,NA,NA
Tabu search ,NA,NA
(,NA,NA
"TS), suggests that an adaptive memory and a ",NA,NA
responsive exploration,NA,NA
 need to ,NA,NA
be part of an algorithm. Responsive exploration exploits the information derivable from a ,NA,NA
"selected strategy. Such information may be more substantial, even if the selected strategy is ",NA,NA
"in some sense a bad strategy, than what you can get even in a good strategy that is based on ",NA,NA
randomness. It is because there is an opportunity provided by such information to ,NA,NA
intelligently modify the strategy. You can get some clues as to how you can modify the ,NA,NA
strategy.,NA,NA
"When you have a paradigm that incorporates adaptive memory, you see the relevance of ",NA,NA
associating a neural network:. a TANN is a Tabu neural network. Tabu search and ,NA,NA
Kohonen’s self-organizing map have a common approach in that they work with ,NA,NA
"“neighborhoods.” As a new neighborhood is determined, TS prohibits some of the earlier ",NA,NA
"solutions, as it classifies them as ",NA,NA
tabu,NA,NA
. Such solutions contain attributes that are identified ,NA,NA
as ,NA,NA
tabu active,NA,NA
.,NA,NA
"Tabu search, has STM and LTM components as well. The short-term memory is ",NA,NA
sometimes called ,NA,NA
recency-based,NA,NA
 memory. While this may prove adequate to find good ,NA,NA
"solutions, the inclusion of long-term memory makes the search method much more potent. ",NA,NA
It also does not necessitate longer runs of the search process.,NA,NA
Some of the examples of applications using Tabu search are:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/470-472.html (1 of 2) [21/11/02 21:58:37],NA
•,NA,NA
  Training neural nets with the reactive Tabu search ,NA,NA
•,NA,NA
  Tabu Learning: a neural network search method for solving nonconvex ,NA,NA
optimization problems ,NA,NA
•,NA,NA
  Massively parallel Tabu search for the quadratic assignment problem ,NA,NA
•,NA,NA
  Connection machine implementation of a Tabu search algorithm for the traveling ,NA,NA
salesman problem ,NA,NA
•,NA,NA
  A Tabu search procedure for multicommodity location/allocation with balancing ,NA,NA
requirements ,NA,NA
Summary,NA,NA
The traveling salesperson problem is presented in this chapter as an example of nonlinear ,NA,NA
optimization with neural networks. Details of formulation are given of the energy function ,NA,NA
and its evaluation. The approaches to the solution of the traveling salesperson problem ,NA,NA
using a Hopfield network and using a Kohonen self-organizing map are presented. C++ ,NA,NA
programs are included for both approaches. ,NA,NA
The output with the C++ program for the Hopfield network refers to examples of four- and ,NA,NA
five-city tours. The output with the C++ program for the Kohonen approach is given for a ,NA,NA
"tour of five cities, for illustration. The solution obtained is good, if not optimal. The ",NA,NA
problem with the Hopfield approach lies in the selection of appropriate values for the ,NA,NA
parameters. Hopfield’s choices are given for his 10-city tour problem. The same values for ,NA,NA
the parameters may not work for the case of a different number of cities. The version of this ,NA,NA
approach given by Anzai is also discussed briefly.,NA,NA
Use of neural networks for nonlinear optimization as applied to portfolio selection is also ,NA,NA
presented in this chapter. You are introduced to Tabu search and its use in optimization ,NA,NA
with neural computing.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch15/470-472.html (2 of 2) [21/11/02 21:58:37]",NA
Previous Table of Contents Next,NA,NA
Chapter 16 ,NA,NA
Applications of Fuzzy Logic ,NA,NA
Introduction,NA,NA
"Up until now, we have discussed how fuzzy logic could be used in conjunction with neural ",NA,NA
networks: We looked at a fuzzifier in Chapter 3 that takes crisp input data and creates fuzzy ,NA,NA
"outputs, which then could be used as inputs to a neural network. In chapter 9, we used ",NA,NA
fuzzy logic to create a special type of associative memory called a FAM (fuzzy associative ,NA,NA
"memory). In this chapter, we focus on applications of fuzzy logic by itself. This chapter ",NA,NA
starts with an overview of the different types of application areas for fuzzy logic. We then ,NA,NA
"present two application domains of fuzzy logic: fuzzy control systems, and fuzzy databases ",NA,NA
"and quantification. In these sections, we also introduce some more concepts in fuzzy logic ",NA,NA
theory. ,NA,NA
A Fuzzy Universe of Applications,NA,NA
Fuzzy logic is being applied to a wide variety of problems. The most pervasive field of ,NA,NA
"influence is in control systems, with the rapid acceptance of fuzzy logic controllers (FLCs) ",NA,NA
for machine and process control. There are a number of other areas where fuzzy logic is ,NA,NA
"being applied. Here is a brief list adapted from Yan, et al., with examples in each area: ",NA,NA
•Biological and Medical Sciences,NA,NA
" Fuzzy logic based diagnosis systems, cancer ",NA,NA
"research, fuzzy logic based manipulation of prosthetic devices, fuzzy logic based ",NA,NA
"analysis of movement disorders, etc. ",NA,NA
•Management and Decision Support,NA,NA
" Fuzzy logic based factory site selection, ",NA,NA
"fuzzy logic aided military decision making (sounds scary, but remember that the ",NA,NA
"fuzzy in fuzzy logic applies to the imprecision in the data and not in the logic), ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/473-475.html (1 of 3) [21/11/02 21:58:38],NA
"fuzzy logic based decision making for marketing strategies, etc. ",NA,NA
•Economics and Finance,NA,NA
" Fuzzy modeling of complex marketing systems, fuzzy ",NA,NA
"logic based trading systems, fuzzy logic based cost-benefit analysis, fuzzy logic ",NA,NA
"based investment evaluation, etc. ",NA,NA
•Environmental Science,NA,NA
" Fuzzy logic based weather prediction, fuzzy logic based ",NA,NA
"water quality control, etc. ",NA,NA
•Engineering and Computer Science,NA,NA
" Fuzzy database systems, fuzzy logic based ",NA,NA
"prediction of earthquakes, fuzzy logic based automation of nuclear plant control, ",NA,NA
"fuzzy logic based computer network design, fuzzy logic based evaluation of ",NA,NA
"architectural design, fuzzy logic control systems, etc. ",NA,NA
•Operations Research,NA,NA
" Fuzzy logic based scheduling and modeling, fuzzy logic ",NA,NA
"based allocation of resources, etc. ",NA,NA
•Pattern Recognition and Classification,NA,NA
" Fuzzy logic based speech recognition, ",NA,NA
"fuzzy logic based handwriting recognition, fuzzy logic based facial characteristic ",NA,NA
"analysis, fuzzy logic based military command analysis, fuzzy image search, etc. ",NA,NA
•Psychology,NA,NA
" Fuzzy logic based analysis of human behavior, criminal investigation ",NA,NA
"and prevention based on fuzzy logic reasoning, etc. ",NA,NA
•Reliability and Quality Control,NA,NA
" Fuzzy logic based failure diagnosis, production ",NA,NA
"line monitoring and inspection, etc. ",NA,NA
"We will now move to one of the two application domains that we will discuss in depth, ",NA,NA
"Fuzzy Databases. Later in the chapter, we examine the second application domain, Fuzzy ",NA,NA
Control Systems. ,NA,NA
Section I: A Look at Fuzzy Databases and Quantification,NA,NA
"In this section, we want to look at some ways in which fuzzy logic may be applied to ",NA,NA
databases and operations with databases. Standard databases have ,NA,NA
crisp,NA,NA
" data sets, and you ",NA,NA
create unambiguous relations over the data sets. You also make queries that are specific and ,NA,NA
that do not have any ambiguity. Introducing ambiguity in one or more of these aspects of ,NA,NA
standard databases leads to ideas of how fuzzy logic can be applied to databases. Such ,NA,NA
application of fuzzy logic could mean that you get databases that are easier to query and ,NA,NA
"easier to interface to. A fuzzy search, where search criteria are not precisely bounded, may ",NA,NA
be more appropriate than a crisp search. You can recall any number of occasions when you ,NA,NA
"tend to make ambiguous queries, since you are not certain of what you need. You also tend ",NA,NA
to make ambiguous queries when a “ball park” value is sufficient for your purposes.,NA,NA
"In this section, you will also learn some more concepts in fuzzy logic. You will see these ",NA,NA
concepts introduced where they arise in the discussion of ideas relating to fuzzy databases. ,NA,NA
You may at times see somewhat of a digression in the middle of the fuzzy database ,NA,NA
discussion to fuzzy logic topics. You may skip to the area where the fuzzy database ,NA,NA
discussion is resumed and refer back to the skipped areas whenever you feel the need to ,NA,NA
get clarification of a concept.,NA,NA
"We will start with an example of a standard database, relations and queries. We then point ",NA,NA
out some of the ways in which fuzziness can be introduced.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/473-475.html (3 of 3) [21/11/02 21:58:38]",NA
Previous Table of Contents Next,NA,NA
Databases and Queries,NA,NA
Imagine that you are interested in the travel business. You may be trying to design special ,NA,NA
"tours in different countries with your own team of tour guides, etc. , and you want to ",NA,NA
"identify suitable persons for these positions. Initially, let us say, you are interested in their ",NA,NA
"own experiences in traveling, and the knowledge they possess, in terms of geography, ",NA,NA
"customs, language, and special occasions, etc. The information you want to keep in your ",NA,NA
"database may be something like, who the person is, the person’s citizenship, to where the ",NA,NA
"person traveled, when such travel occurred, the length of stay at that destination, the ",NA,NA
"person’s languages, the languages the person understands, the number of trips the person ",NA,NA
"made to each place of travel, etc. Let us use some abbreviations: ",NA,NA
cov,NA,NA
—country visited ,NA,NA
lov,NA,NA
—length of visit (days) ,NA,NA
nov,NA,NA
—number of visits including previous visits ,NA,NA
ctz,NA,NA
—citizenship ,NA,NA
yov,NA,NA
—year of visit ,NA,NA
lps,NA,NA
—language (other than mother tongue) with proficiency to speak ,NA,NA
lpu,NA,NA
—language with only proficiency to understand ,NA,NA
hs,NA,NA
"—history was studied (1—yes, 0—no) ",NA,NA
Typical entries may appear as noted in Table 16.1. ,NA,NA
Table 16.1,NA,NA
Example Database ,NA,NA
Name ,NA,NA
age ,NA,NA
ctz ,NA,NA
cov ,NA,NA
lov ,NA,NA
nov ,NA,NA
yov ,NA,NA
lps ,NA,NA
lpu ,NA,NA
hs ,NA,NA
John Smith ,NA,NA
35 ,NA,NA
U.S. ,NA,NA
India ,NA,NA
4 ,NA,NA
1 ,NA,NA
1994 ,NA,NA
Italian ,NA,NA
Hindi ,NA,NA
1 ,NA,NA
John Smith ,NA,NA
35 ,NA,NA
U.S. ,NA,NA
Italy ,NA,NA
7 ,NA,NA
2 ,NA,NA
1991 ,NA,NA
1 ,NA,NA
John Smith ,NA,NA
35 ,NA,NA
U.S. ,NA,NA
Japan ,NA,NA
3 ,NA,NA
1 ,NA,NA
1993 ,NA,NA
0 ,NA,NA
When a query is made to list persons that visited India or Japan after 1992 for 3 or more ,NA,NA
"days, John Smith’s two entries will be included. The conditions stated for this query are ",NA,NA
"straightforward, with lov [ge] 3 and yov > 1992 and (cov = India or cov = Japan). ",NA,NA
Relations in Databases,NA,NA
"A relation from this database may be the set of quintuples, (name, age, cov, lov, yov). ",NA,NA
"Another may be the set of triples, (name, ctz, lps). The quintuple (John Smith, 35, India, 4, ",NA,NA
"1994) belongs to the former relation, and the triple (John Smith, U.S., Italian) belongs to ",NA,NA
"the latter. You can define other relations, as well. ",NA,NA
Fuzzy Scenarios,NA,NA
Now the query part may be made fuzzy by asking to list young persons who recently ,NA,NA
visited Japan or India for a few days. John Smith’s entries may or may not be included this ,NA,NA
"time since it is not clear if John Smith is considered young, or whether 1993 is considered ",NA,NA
"recent, or if 3 days would qualify as a few days for the query. This modification of the ",NA,NA
query illustrates one of three scenarios in which fuzziness can be introduced into databases ,NA,NA
and their use. ,NA,NA
"This is the case where the database and relations are standard, but the queries may be fuzzy. ",NA,NA
"The other cases are: one where the database is fuzzy, but the queries are standard with no ",NA,NA
ambiguity; and one where you have both a fuzzy database and some fuzzy queries.,NA,NA
Fuzzy Sets Revisited,NA,NA
We will illustrate the concept of fuzziness in the case where the database and the queries ,NA,NA
"have fuzziness in them. Our discussion is guided by the reference Terano, Asai, and ",NA,NA
"Sugeno. First, let us review and recast the concept of a fuzzy set in a slightly different ",NA,NA
notation. ,NA,NA
If ,NA,NA
"a, b, c,",NA,NA
 and ,NA,NA
d,NA,NA
 are in the set ,NA,NA
A,NA,NA
" with 0.9, 0.4, 0.5, 0, respectively, as degrees of ",NA,NA
"membership, and in ",NA,NA
B,NA,NA
" with 0.9, 0.6, 0.3, 0.8, respectively, we give these fuzzy sets ",NA,NA
A,NA,NA
 and ,NA,NA
B,NA,NA
 ,NA,NA
as ,NA,NA
A,NA,NA
" = { 0.9/a, 0.4/b, 0.5/c} and ",NA,NA
B,NA,NA
" = {0.9/a, 0.6/b, 0.3/c, 0.8/d}. Now ",NA,NA
A,NA,NA
[cup]B = {0.9/,NA,NA
a,NA,NA
", ",NA,NA
0.6/,NA,NA
b,NA,NA
", 0.5/",NA,NA
c,NA,NA
", 0.8/",NA,NA
d,NA,NA
} since you take the larger of the degrees of membership in ,NA,NA
A,NA,NA
 and ,NA,NA
B,NA,NA
 for ,NA,NA
"each element. Also, ",NA,NA
A,NA,NA
[cap]B = {0.9/,NA,NA
a,NA,NA
", 0.4/",NA,NA
b,NA,NA
", 0.3/",NA,NA
c,NA,NA
} since you now take the smaller of the ,NA,NA
degrees of membership in ,NA,NA
A,NA,NA
 and ,NA,NA
B,NA,NA
 for each element. Since ,NA,NA
d,NA,NA
 has ,NA,NA
0,NA,NA
 as degree of ,NA,NA
membership in ,NA,NA
A,NA,NA
 (it is therefore not listed in ,NA,NA
A,NA,NA
"), it is not listed in ",NA,NA
A,NA,NA
[cap],NA,NA
B.,NA,NA
"Let us impart fuzzy values (FV) to each of the attributes, age, lov, nov, yov, and hs by ",NA,NA
defining the sets in Table 16.2.,NA,NA
Table 16.2,NA,NA
Fuzzy Values for Example Sets ,NA,NA
Fuzzy Value ,NA,NA
Set ,NA,NA
FV(age) ,NA,NA
"{ very young, young, somewhat old, old } ",NA,NA
FV(nov) ,NA,NA
"{ never, rarely, quite a few, often, very often } ",NA,NA
FV(lov) ,NA,NA
"{ barely few days, few days, quite a few days, many days } ",NA,NA
FV(yov) ,NA,NA
"{distant past, recent past, recent } ",NA,NA
FV(hs) ,NA,NA
"{ barely, adequately, quite a bit, extensively } ",NA,NA
"The attributes of name, citizenship, country of visit are clearly not candidates for having ",NA,NA
"fuzzy values. The attributes of lps, and lpu, which stand for language in which speaking ",NA,NA
"proficiency and language in which understanding ability exist, can be coupled into another ",NA,NA
attribute called ,NA,NA
flp,NA,NA
 (foreign language proficiency) with fuzzy values. We could have ,NA,NA
introduced in the original list an attribute called ,NA,NA
lpr,NA,NA
 ( language with proficiency to read) ,NA,NA
along with ,NA,NA
lps,NA,NA
 and ,NA,NA
lpu,NA,NA
". As you can see, these three can be taken together into the fuzzy-",NA,NA
valued attribute of foreign language proficiency. We give below the fuzzy values of flp.,NA,NA
" FV(flp) = {not proficient, barely proficient, adequate,",NA,NA
" proficient, very proficient }",NA,NA
"Note that each fuzzy value of each attribute gives rise to a fuzzy set, which depends on the ",NA,NA
elements you consider for the set and their degrees of membership. ,NA,NA
Previous Table of Contents Next,NA,NA
Previous Table of Contents Next,NA,NA
Properties of Fuzzy Relations,NA,NA
"A relation on a set, that is a subset of a Cartesian product of some set with itself, may have some ",NA,NA
interesting properties. It may be ,NA,NA
reflexive,NA,NA
. For this you need to have 1 for the degree of membership ,NA,NA
of each main diagonal entry. Our example here is evidently not reflexive.,NA,NA
A relation may be ,NA,NA
symmetric,NA,NA
. For this you need the degrees of membership of each pair of entries ,NA,NA
"symmetrically situated to the main diagonal to be the same value. For example (Jeff, Mike) and ",NA,NA
"(Mike, Jeff) should have the same degree of membership. Here they do not, so our example of a ",NA,NA
relation is not symmetric.,NA,NA
A relation may be ,NA,NA
antisymmetric,NA,NA
. This requires that if a is different from b and the degree of ,NA,NA
membership of the ordered pair (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
") is not 0, then its mirror image, the ordered pair (",NA,NA
b,NA,NA
", ",NA,NA
a,NA,NA
"), should ",NA,NA
"have 0 for degree of membership. In our example, both (Steve, Mike) and (Mike, Steve) have ",NA,NA
"positive values for degree of membership; therefore, the relation much_more_educated over the set ",NA,NA
"{Jeff, Steve, Mike} is not antisymmetric also.",NA,NA
A relation may be ,NA,NA
transitive,NA,NA
". For transitivity of a relation, you need the following condition, ",NA,NA
"illustrated with our set {Jeff, Steve, Mike}. For brevity, let us use r in place of ",NA,NA
"much_more_educated, the name of the relation:",NA,NA
 min (m,r,NA
"(Jeff, Steve) , m",r,NA
"(Steve, Mike) )[le]m",r,NA
"(Jeff, Mike)",NA,NA
 min (m,r,NA
"(Jeff, Mike) , m",r,NA
"(Mike, Steve) )[le]m",r,NA
"(Jeff, Steve)",NA,NA
 min (m,r,NA
"(Steve, Jeff) , m",r,NA
"(Jeff, Mike) )[le]m",r,NA
"(Steve, Mike)",NA,NA
 min (m,r,NA
"(Steve, Mike) , m",r,NA
"(Mike, Jeff) )[le]m",r,NA
"(Steve, Jeff)",NA,NA
 min (m,r,NA
"(Mike, Jeff) , m",r,NA
"(Jeff, Steve) )[le]m",r,NA
"(Mike, Steve)",NA,NA
 min (m,r,NA
"(Mike, Steve) , m",r,NA
"(Steve, Jeff) )[le]m",r,NA
"(Mike, Jeff)",NA,NA
"In the above listings, the ordered pairs on the left-hand side of an occurrence of [le] are such that the ",NA,NA
"second member of the first ordered pair matches the first member of the second ordered pair, and also ",NA,NA
"the right-hand side ordered pair is made up of the two nonmatching elements, in the same order. ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/479-482.html (1 of 3) [21/11/02 21:58:41],NA
"In our example,",NA,NA
 min (m,r,NA
"(Jeff, Steve) , m",r,NA
"(Steve, Mike) ) = min (0.2, 0.3) = 0.2",NA,NA
 m,r,NA
"(Jeff, Mike) = 0.7 > 0.2",NA,NA
"For this instance, the required condition is met. But in the following: ",NA,NA
 min (m,r,NA
"(Jeff, Mike), m",r,NA
"(Mike, Steve) ) = min (0.7, 0.6) = 0.6",NA,NA
 m,r,NA
"(Jeff, Steve) = 0.2 < 0.6",NA,NA
"The required condition is violated, so the relation much_more_educated is not transitive. ","NOTE:  
 If a condition defining a property of a relation is not met even in one instance, the relation 
 does not possess that property. Therefore, the relation in our example is not reflexive, not symmetric, 
 not even antisymmetric, and not transitive.",NA
"If you think about it, it should be clear that when a relation on a set of more than one element is ",NA,NA
"symmetric, it cannot be antisymmetric also, and vice versa. But a relation can be both not symmetric ",NA,NA
"and not antisymmetric at the same time, as in our example. ",NA,NA
"An example of reflexive, symmetric, and transitive relation is given by the following matrix:",NA,NA
 1     0.4   0.8,NA,NA
 0.4   1     0.4,NA,NA
 0.8   0.4   1,NA,NA
Similarity Relations,NA,NA
"A reflexive, symmetric, and transitive fuzzy relation is said to be a ",NA,NA
fuzzy equivalence relation,NA,NA
. Such a ,NA,NA
relation is also called a ,NA,NA
similarity relation,NA,NA
. When you have a similarity relation ,NA,NA
s,NA,NA
", you can define the ",NA,NA
similarity class,NA,NA
 of an element ,NA,NA
x,NA,NA
 of the domain as the fuzzy set in which the degree of membership of ,NA,NA
y,NA,NA
 ,NA,NA
in the domain is ,NA,NA
m,s,NA
(,NA,NA
x,NA,NA
", ",NA,NA
y,NA,NA
). The similarity class of ,NA,NA
x,NA,NA
 with the relation ,NA,NA
s,NA,NA
 can be denoted by [,NA,NA
x,NA,NA
],s,NA
.,NA,NA
Resemblance Relations,NA,NA
"Do you think similarity and resemblance are one and the same? If x is similar to y, does it mean that ",NA,NA
x resembles y? Or does the answer depend on what sense is used to talk of similarity or of ,NA,NA
"resemblance? In everyday jargon, Bill may be similar to George in the sense of holding high office, ",NA,NA
but does Bill resemble George in financial terms? Does this prompt us to look at a ‘resemblance ,NA,NA
relation’ and distinguish it from the ‘similarity relation’? Of course. ,NA,NA
"Recall that a fuzzy relation that is reflexive, symmetric, and also transitive is called similarity ",NA,NA
"relation. It helps you to create similarity classes. If the relation lacks any one of the three properties, it ",NA,NA
"is not a similarity relation. But if it is only not transitive, meaning it is both reflexive and symmetric, ",NA,NA
"it is still not a similarity relation, but it is a resemblance relation. An example of a resemblance ",NA,NA
"relation, call it ",NA,NA
t,NA,NA
", is given by the following matrix.",NA,NA
Let the domain have elements ,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
", and ",NA,NA
c,NA,NA
:,NA,NA
 1     0.4   0.8 ,NA,NA
t,NA,NA
 =  0.4   1     0.5,NA,NA
 0.8   0.5   1,NA,NA
"This fuzzy relation is clearly reflexive, and symmetric, but it is not transitive. For example: ",NA,NA
 min (m,t,NA
"(a, c) , m",t,NA
"(c, b) ) = min (0.8, 0.5) = 0.5 ,",NA,NA
but the following: ,NA,NA
 m,t,NA
"(a, b) = 0.4 < 0.5 ,",NA,NA
"is a violation of the condition for transitivity. Therefore, ",NA,NA
t,NA,NA
" is not a similarity relation, but it certainly is ",NA,NA
a resemblance relation.,NA,NA
Fuzzy Partial Order,NA,NA
One last definition is that of a ,NA,NA
fuzzy partial order,NA,NA
". A fuzzy relation that is reflexive, antisymmetric, ",NA,NA
and transitive is a fuzzy partial order. It differs from a similarity relation by requiring antisymmetry ,NA,NA
"instead of symmetry. In the context of crisp sets, an equivalence relation that helps to generate ",NA,NA
equivalence,NA,NA
" classes is also a reflexive, symmetric, and transitive relation. But those ",NA,NA
equivalence ,NA,NA
"classes are disjoint, unlike similarity classes with fuzzy relations. With crisp sets, you can define a ",NA,NA
"partial order, and it serves as a basis for making comparison of elements in the domain with one ",NA,NA
another.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/479-482.html (3 of 3) [21/11/02 21:58:41]",NA
Previous Table of Contents Next,NA,NA
Fuzzy Queries,NA,NA
"At this point, our digression from the discussion of fuzzy data bases is finished. Let us now ",NA,NA
"recall, for immediate reference, the entries in the definitions we listed earlier in Table 16.3: ",NA,NA
Table 16.3,NA,NA
Fuzzy Values for Example Sets ,NA,NA
Fuzzy Value ,NA,NA
Set ,NA,NA
FV(age) ,NA,NA
"{ very young, young, somewhat old, old } ",NA,NA
FV(nov) ,NA,NA
"{ never, rarely, quite a few, often, very often } ",NA,NA
FV(lov) ,NA,NA
"{ barely few days, few days, quite a few days, many days } ",NA,NA
FV(yov) ,NA,NA
"{distant past, recent past, recent } ",NA,NA
FV(hs) ,NA,NA
"{ barely, adequately, quite a bit, extensively } ",NA,NA
FV(flp) ,NA,NA
"{not proficient, barely proficient, adequate, proficient, very ",NA,NA
proficient ,NA,NA
} ,NA,NA
"If you refer to Tables 16.1 and 16.3, you will see that a fuzzy query defines a fuzzy set. For ",NA,NA
"example, suppose you ask for a list of young people with enough knowledge of Italy, and ",NA,NA
"who know Italian almost like a native. You want the intersection of the fuzzy sets, ",NA,NA
age_young,NA,NA
", ",NA,NA
hs_adequately,NA,NA
", and ",NA,NA
flp_very proficient,NA,NA
. John Smith is in the fuzzy set ,NA,NA
age_young,NA,NA
" with a 0.75 degree of membership, as noted before. Suppose he is in ",NA,NA
hs_adequately,NA,NA
" with a 0.2 degree of membership, and in ",NA,NA
flp_very proficient,NA,NA
 with a 0.9 ,NA,NA
degree of membership. We then put him in the fuzzy set for this query. The degree of ,NA,NA
"membership for John Smith is the smallest value among 0.75, 0.2, and 0.9, since we are ",NA,NA
taking the intersection of fuzzy sets. So 0.2/John Smith will be part of the enumeration of ,NA,NA
the query fuzzy set.,NA,NA
Note that you can use unknown in place of the value of an attribute for an item in the ,NA,NA
"database. If you do not know John Smith’s age, you can enter unknown in that field of John ",NA,NA
Smith’s record. You may say his age is ,NA,NA
unknown,NA,NA
", even though you have a rough idea that ",NA,NA
he is about 35. You would then be able to assign some reasonable degrees of ,NA,NA
membership of John Smith in fuzzy sets like ,NA,NA
age_young,NA,NA
 or ,NA,NA
age_somewhat old,NA,NA
.,NA,NA
Extending Database Models,NA,NA
"One way of extending a model into a fuzzy model, as far as databases are concerned, is to ",NA,NA
make use of similarity relations and to extend the operations with them as Buckles and ,NA,NA
"Perry do, such as ",NA,NA
PROJECT,NA,NA
. First there are the ,NA,NA
domains,NA,NA
", for the database. In our example ",NA,NA
"relating to travel and tour guides above, the domains are:",NA,NA
"D1 = { John Smith, ... }, the set of persons included in the database, ",NA,NA
"D2 = {India, Italy, Japan, ... }, the set of countries of visit, ",NA,NA
"D3 = {Hindi, Italian, Japanese, ... }, the set of foreign languages, ",NA,NA
"D4 = {U.S., ... }, the set of countries of citizenship, ",NA,NA
"D5 = set of ages, ",NA,NA
"D6 = set of years, ",NA,NA
"D7 = set of number of visits, ",NA,NA
D8 = set of length of visits. ,NA,NA
"Note that the enumerated sets are shown with ‘...’ in the sets, to indicate that there may be ",NA,NA
"more entries listed, but we are not giving a complete list. In practice, you will make a ",NA,NA
"complete enumeration unlike in our example. The domains D5, D6, D7, and D8 can also ",NA,NA
"be given with all their elements listed, since in practice, these sets are finite. Conceptually ",NA,NA
"though, they are infinite sets; for example, D6 is the set of positive integers. ",NA,NA
"Next, you have the similarity relations that you define. And then there are the operations.",NA,NA
Example,NA,NA
"Consider a standard database given below. It can be called also the relation R1, from set ",NA,NA
"D1 = {Georgette, Darrell, Ernie , Grace } to set D2 = {Spanish, Italian, French, Japanese, ",NA,NA
"Chinese, Russian} as shown in Table 16.4: ",NA,NA
Table 16.4,NA,NA
"Example for Relation R1, with Domains D1 and D2 ",NA,NA
D1 ,NA,NA
D2 ,NA,NA
Georgette ,NA,NA
Spanish ,NA,NA
Georgette ,NA,NA
Italian ,NA,NA
Darrell ,NA,NA
French ,NA,NA
Darrell ,NA,NA
Spanish ,NA,NA
Darrell ,NA,NA
Japanese ,NA,NA
Ernie ,NA,NA
Spanish ,NA,NA
Ernie ,NA,NA
Russian ,NA,NA
Grace ,NA,NA
Chinese ,NA,NA
Grace ,NA,NA
French ,NA,NA
Let’s say a fuzzy database has D1 and D2 as domains. Suppose you have a similarity ,NA,NA
relation S1 on domain D1 given by the matrix: ,NA,NA
 1     0.4   0.8   0.7,NA,NA
 0.4   1     0.5   0.3,NA,NA
 0.8   0.5   1     0.4,NA,NA
 0.7   0.3   0.4   1,NA,NA
Recall that the entries in this matrix represent the degrees of membership of ordered pairs ,NA,NA
"in the relation S1. For example, the first row third column element, which is 0.8, in the ",NA,NA
"matrix refers to the degree of membership of the pair (Georgette, Ernie), since the rows ",NA,NA
"and columns refer to Georgette, Darrell, Ernie and Grace in that order. ",NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
The result of the operation: ,NA,NA
PROJECT (R1 over D1) with LEVEL(D1) = 0.6 ,NA,NA
is the relation R2 given in Table 16.5. ,NA,NA
Table 16.5,NA,NA
"Relation R2, which is the Result of PROJECT Operation ",NA,NA
D1 ,NA,NA
"{Georgette, Ernie} ",NA,NA
"{Georgette, Grace} ",NA,NA
Darrell ,NA,NA
"This projection operation with a condition on the level works as follows. First, the column ",NA,NA
"for D1 is to be picked, of the two columns that R1 shows. Repetitions are removed. The ",NA,NA
"condition says that if two elements of D1 have a similarity of 0.6 or higher, they should be ",NA,NA
"treated as the same. Even though similarity levels of pairs (Georgette, Ernie) and ",NA,NA
"(Georgette, Grace) are both greater than 0.6, the pair (Ernie, Grace) has similarity of 0.4 ",NA,NA
only so that we do not treat the three as the same. ,NA,NA
"This type of table is not constructed in the standard database model, so this is part of an ",NA,NA
extension of the standard model.,NA,NA
"Suppose you recast the information in the relation R1 and call it R3, shown in Table 16.6.",NA,NA
Table 16.6,NA,NA
"Example for Relation R3, with Domains D1 and D2 ",NA,NA
D1D2 ,NA,NA
"Georgette {Spanish, Italian} ",NA,NA
"Darrell {French, Spanish, Japanese} ",NA,NA
"Grace {Chinese, French} ",NA,NA
"Ernie {Russian, Spanish} ",NA,NA
This kind of a table also is not found in standard databases (where there are groups with ,NA,NA
"more than one element used in the relation), and is an example of an ",NA,NA
extended model,NA,NA
.,NA,NA
Possibility Distributions,NA,NA
As an alternative to using similarity relations for introducing fuzziness into a database ,NA,NA
"model, you can, following Umano, et al., use a ",NA,NA
possibility distribution-relational model,NA,NA
. ,NA,NA
The possibility distributions represent the fuzzy values of attributes in the data. An ,NA,NA
"example of a possibility distribution is the fuzzy set you saw before, ",NA,NA
nov_rarely,NA,NA
" = {0.7/1, ",NA,NA
"0.2/2}, where ",NA,NA
nov_rarely,NA,NA
 stands for number of visits considered to be “rarely.”,NA,NA
Example,NA,NA
An example of a database on the lines of this model is shown in Table 16.7: ,NA,NA
Table 16.7,NA,NA
Example of Possibility Distribution Relational Model ,NA,NA
Name ,NA,NA
Number of Visits Outside ,NA,NA
the U.S. ,NA,NA
Citizenship ,NA,NA
Name of Companion on ,NA,NA
Latest Visit ,NA,NA
Peter ,NA,NA
3 ,NA,NA
U.S. ,NA,NA
Barbara ,NA,NA
Roberto ,NA,NA
"{10, 11}",p,NA
Spain ,NA,NA
Anne ,NA,NA
Andre ,NA,NA
unknown ,NA,NA
Carol ,NA,NA
2 ,NA,NA
Raj ,NA,NA
14 ,NA,NA
"{India, U.S.}",p,NA
Uma ,NA,NA
Alan ,NA,NA
unknown ,NA,NA
U.S. ,NA,NA
undefined ,NA,NA
James ,NA,NA
many ,NA,NA
U.K. ,NA,NA
null ,NA,NA
A standard database cannot look like this. Entries like ,NA,NA
many,NA,NA
 and ,NA,NA
"{10, 11}",p,NA
 clearly suggest ,NA,NA
fuzziness. The entry ,NA,NA
"{10, 11}",p,NA
",",NA,NA
 is a ,NA,NA
possibility distribution,NA,NA
", suggesting that the number of ",NA,NA
"visits outside the United States made by Roberto is either 10 or 11. Similarly, Raj’s ",NA,NA
"citizenship is India or United States, but not dual citizenship in both. Andre’s citizenship ",NA,NA
"and Alan’s number of visits outside the United States are not known, and they can have any ",NA,NA
values. The possibilities cannot be narrowed down as in the case of Raj’s citizenship and ,NA,NA
Roberto’s frequency of visits outside the United States The entry ,NA,NA
undefined,NA,NA
 is used for ,NA,NA
"Alan because he always traveled alone, he never took a companion.",NA,NA
James’ number of visits is fuzzy. He traveled many times. A fuzzy set for many will ,NA,NA
"provide the possibility distribution. It can be defined, for example, as:",NA,NA
" many = {0.2/6, 0.5/7, 0.8/8, 1/9, 1/10, ...}",NA,NA
The name of the companion on James’ latest visit outside the United States is entered as ,NA,NA
null,NA,NA
" because we do not know on the one hand whether he never took a companion, in ",NA,NA
which case we could have used ,NA,NA
undefined,NA,NA
" as in Alan’s case, and on the other whom he ",NA,NA
"took as a companion if he did take one, in which case we could have used ",NA,NA
unknown.,NA,NA
"Simply put, we use ",NA,NA
null,NA,NA
 when we do not know enough to use either ,NA,NA
unknown,NA,NA
 or ,NA,NA
undefined.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Queries,NA,NA
Let us turn our attention now to how queries are answered with this type of a database model. Suppose ,NA,NA
you want a list of U.S. citizens in your database. Peter and Alan clearly satisfy this condition on ,NA,NA
citizenship. Andre and Raj can only be said to possibly satisfy this condition. But Roberto and James ,NA,NA
clearly do not satisfy the given condition. This query itself is crisp and not fuzzy (either you belong to ,NA,NA
"the list of U.S. citizens or you don’t). The answer therefore should be a crisp set, meaning that unless ",NA,NA
"the degree of membership is 1, you will not list an element. So you get the set containing Peter and ",NA,NA
Alan only. ,NA,NA
A second query could be for people who made more than a few visits outside the United States Here the ,NA,NA
query is fuzzy. James with many visits outside United States seems to clearly satisfy the given ,NA,NA
condition. It is perhaps reasonable to assume that each element in the fuzzy set for ,NA,NA
many,NA,NA
 appears in the ,NA,NA
fuzzy set ,NA,NA
more than a few,NA,NA
 with a degree of membership 1. Andre’s 2 may be a number that merits 0 ,NA,NA
degree of membership in ,NA,NA
more than a few,NA,NA
. The other numbers in the database are such that they ,NA,NA
possibly satisfy the given condition to different degrees. You can see that the answer to this fuzzy query ,NA,NA
"is a fuzzy set. Now, we switch gears a little, to talk more on fuzzy theory. This will help with material ",NA,NA
to follow.,NA,NA
"Fuzzy Events, Means and Variances",NA,NA
"Let us introduce you to fuzzy events, fuzzy means, and fuzzy variances. These concepts are basic to ",NA,NA
make a study of ,NA,NA
fuzzy quantification,NA,NA
 theories. You will see how a variable’s probability distribution ,NA,NA
and its fuzzy set are used together. We will use an example to show how fuzzy means and fuzzy ,NA,NA
variances are calculated.,NA,NA
Example: XYZ Company Takeover Price,NA,NA
Suppose you are a shareholder of company XYZ and that you read in the papers that its takeover is a ,NA,NA
prospect. Currently the company shares are selling at $40 a share. You read about a hostile takeover by ,NA,NA
a group prepared to pay $100 a share for XYZ. Another company whose business is related to XYZ’s ,NA,NA
business and has been on friendly terms with XYZ is offering $85 a share. The employees of XYZ are ,NA,NA
concerned about their job security and have organized themselves in preparing to buy the company ,NA,NA
collectively for $60 a share. The buyout price of the company shares is a variable with these ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/487-489.html (1 of 3) [21/11/02 21:58:43],NA
"three possible values, viz., 100, 85, and 60. The board of directors of XYZ have to make the ultimate ",NA,NA
"decision regarding whom they would sell the company. The probabilities are 0.3, 0.5, and 0.2 ",NA,NA
"respectively, that the board decides to sell at $100 to the first group, to sell at $85 to the second, and to ",NA,NA
let the employees buy out at $60. ,NA,NA
"Thus, you get the probability distribution of the takeover price to be as follows:",NA,NA
price,NA,NA
100 ,NA,NA
85 ,NA,NA
60 ,NA,NA
probability,NA,NA
0.3 ,NA,NA
0.5 ,NA,NA
0.2 ,NA,NA
"From standard probability theory, this distribution gives a mean(or expected price) of: ",NA,NA
 100 x 0.3 + 85 x 0.5 + 60 x 0.2 = 84.5,NA,NA
and a variance of : ,NA,NA
 (100-84.5),2,NA
 x 0.3 +(85-84.5),2,NA
 x 0.5 + (60-84.5),2,NA
 x 0.2 = 124.825,NA,NA
Suppose now that a security analyst specializing in takeover situations feels that the board hates a ,NA,NA
hostile takeover but to some extent they cannot resist the price being offered. The analyst also thinks ,NA,NA
"that the board likes to keep some control over the company, which is possible if they sell the company ",NA,NA
to their friendly associate company. The analyst recognizes that the Board is sensitive to the fears and ,NA,NA
"apprehensions of their loyal employees with whom they built a healthy relationship over the years, and ",NA,NA
are going to consider the offer from the employees. ,NA,NA
The analyst’s feelings are reflected in the following fuzzy set:,NA,NA
" {0.7/100, 1/85, 0.5/60}",NA,NA
"You recall that this notation says, the degree of membership of 100 in this set is 0.7, that of 85 is 1, ",NA,NA
and the degree of membership is 0.5 for the value 60. ,NA,NA
The fuzzy set obtained in this manner is also called a fuzzy event. A different analyst may define a ,NA,NA
"different fuzzy event with the takeover price values. You, as a shareholder, may have your own ",NA,NA
intuition that suggests a different fuzzy event. Let us stay with the previous fuzzy set that we got from a ,NA,NA
"security analyst, and give it the name A.",NA,NA
Probability of a Fuzzy Event,NA,NA
"At this point, we can calculate the probability for the fuzzy event A by using the takeover prices as the ",NA,NA
basis to correspond the probabilities in the probability distribution and the degrees of membership in ,NA,NA
"A. In other words, the degrees of membership, 0.7, 1, and 0.5 are treated as having the probabilities ",NA,NA
"0.3, 0.5, and 0.2, respectively. But we want the probability of the fuzzy event A, which we calculate ",NA,NA
as the expected ,NA,NA
degree of membership,NA,NA
 under the probability distribution we are using.,NA,NA
Our calculation gives the following:,NA,NA
 0.7 x 0.3 + 1 x 0.5 + 0.5 x 0.2 = 0.21 + 0.5 + 0.1 = 0.81,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/487-489.html (3 of 3) [21/11/02 21:58:43]",NA
Linear Regression a la Possibilities,"When you see the definitions of fuzzy means and fuzzy variances, you may think that regression analysis can 
 also be dealt with in the realm of fuzzy logic. In this section we discuss what approach is being taken in this 
 regard. 
  
 First, recall what regression analysis usually means. You have a set of x- values and a corresponding set of 
 y 
 values, constituting a number of sample observations on variables 
 X
  and 
 Y
 . In determining a linear regression of 
 Y
  
 on 
 X
 ,
  you are taking 
 Y
  as the dependent variable, and 
 X
  as the independent variable, and the linear regression of 
 Y
  
 on 
 X
  is a linear equation expressing 
 Y
  in terms of 
 X
 . This equation gives you the line ‘closest’ to the sample 
 points (the scatter diagram) in some sense. You determine the coefficients in the equation as those values that 
 minimize the sum of squares of deviations of the actual 
 y
  values from the 
 y
  values from the line. Once the 
 coefficients are determined, you can use the equation to estimate the value of 
 Y
  for any given value of 
 X
 . People 
 use regression equations for forecasting.
  
 Sometimes you want to consider more than one independent variable, because you feel that there are more than 
 one variable which collectively can explain the variations in the value of the dependent variable. This is your 
 multiple regression model
 . Choosing your independent variables is where you show your modeling expertise 
 when you want to explain what happens to 
 Y
 , as 
 X
  varies.
  
 In any case, you realize that it is an optimization problem as well, since the minimization of the sum of squares 
 of deviations is involved. Calculus is used to do this for Linear Regression. Use of calculus methods requires 
 certain continuity properties. When such properties are not present, then some other method has to be used for 
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/489-492.html (2 of 3) [21/11/02 21:58:44]",NA
Previous Table of Contents Next,NA,NA
Fuzzy Numbers,NA,NA
A fuzzy number is an ordered pair of numbers ,NA,NA
"(a, b)",NA,NA
 with respect to a reference function ,NA,NA
"L, ",NA,NA
which gives you the membership function. Here ,NA,NA
b,NA,NA
 has to be a positive number.,NA,NA
Here are the properties of ,NA,NA
"L,",NA,NA
 the reference function:,NA,NA
1.,NA,NA
  It is a function of one variable and is symmetric about ,NA,NA
0,NA,NA
". That is, ",NA,NA
L,NA,NA
(,NA,NA
x,NA,NA
) = ,NA,NA
L,NA,NA
(-,NA,NA
x,NA,NA
). ,NA,NA
2.,NA,NA
  It has a value of ,NA,NA
1,NA,NA
 at ,NA,NA
x = 0,NA,NA
". In other words, ",NA,NA
L,NA,NA
(0) = 1. ,NA,NA
3.,NA,NA
"  It is generally decreasing, when ",NA,NA
x,NA,NA
 is ,NA,NA
0,NA,NA
" or a positive number, meaning that its ",NA,NA
"value drops as the value of its argument is increased. For example, ",NA,NA
L,NA,NA
(2) < ,NA,NA
L,NA,NA
(1). ,NA,NA
Thus ,NA,NA
L,NA,NA
(,NA,NA
x,NA,NA
) has its values less than 1 for positive values of ,NA,NA
x,NA,NA
. It does not make sense ,NA,NA
to have negative numbers as values of ,NA,NA
"L,",NA,NA
 and so you ignore the values of ,NA,NA
x,NA,NA
 that ,NA,NA
cause such values for ,NA,NA
L,NA,NA
. ,NA,NA
4.,NA,NA
  The maximum value for the function is ,NA,NA
"1,",NA,NA
 at ,NA,NA
x = 0,NA,NA
. It has a sort of a bell-shaped ,NA,NA
curve. ,NA,NA
If ,NA,NA
A,NA,NA
 is a fuzzy number with the ordered pair (,NA,NA
a,NA,NA
", ",NA,NA
b,NA,NA
) with ,NA,NA
b,NA,NA
" > 0, and if the reference function ",NA,NA
is ,NA,NA
L,NA,NA
", you do the following:",NA,NA
You write the fuzzy number as ,NA,NA
"A = (a, b)",L,NA
.,NA,NA
 You get the membership of any element ,NA,NA
"x,",NA,NA
 by ,NA,NA
taking the quantity ,NA,NA
(x - a) / b,NA,NA
 (which reminds you of how you get a ,NA,NA
z-score,NA,NA
"), and evaluate ",NA,NA
the reference function ,NA,NA
L,NA,NA
 at this argument. That is:,NA,NA
 m,A,NA
(x) = ,NA,NA
L,NA,NA
( (x-a) / b),NA,NA
Examples of a reference function ,NA,NA
L,NA,NA
 are:,NA,NA
Example 1: ,NA,NA
L,NA,NA
(,NA,NA
x,NA,NA
") = max ( 0, 1- ",NA,NA
x,2,NA
) ,NA,NA
You obtain the following shown in Table 16.8. ,NA,NA
Table 16.8,NA,NA
Reference Function L ,NA,NA
x ,NA,NA
L(x) ,NA,NA
0 ,NA,NA
-2 ,NA,NA
-1 ,NA,NA
0 ,NA,NA
-,NA,NA
0.5 ,NA,NA
0.75 ,NA,NA
0 ,NA,NA
1 ,NA,NA
0.5 ,NA,NA
0.75 ,NA,NA
1 ,NA,NA
0 ,NA,NA
2 ,NA,NA
0 ,NA,NA
This function is not ,NA,NA
strictly,NA,NA
 decreasing though. It remains a constant at 0 for ,NA,NA
x,NA,NA
 > 1.,NA,NA
Example 2: ,NA,NA
L,NA,NA
(,NA,NA
x,NA,NA
) = 1/ (1 + ,NA,NA
x,2,NA
 ) ,NA,NA
You get the values shown in Table 16.9. ,NA,NA
Table 16.9,NA,NA
Reference Function L ,NA,NA
x ,NA,NA
L(x) ,NA,NA
0.02 ,NA,NA
-7 ,NA,NA
-2 ,NA,NA
0.2 ,NA,NA
-1 ,NA,NA
0.5 ,NA,NA
-,NA,NA
0.5 ,NA,NA
0.8 ,NA,NA
0 ,NA,NA
1 ,NA,NA
0.5 ,NA,NA
0.8 ,NA,NA
1 ,NA,NA
0.5 ,NA,NA
2 ,NA,NA
0.2 ,NA,NA
7 ,NA,NA
0.02 ,NA,NA
"Let us now determine the membership of 3 in the fuzzy number A = (4, 10)",L,NA
", where ",NA,NA
L,NA,NA
 is ,NA,NA
"the function in the second example above, viz., 1/ (1 + ",NA,NA
x,2,NA
).,NA,NA
"First, you get (",NA,NA
x,NA,NA
- 4) / 10 = (3 - 4) / 10 = - 0.1. Use this as the argument of the reference ,NA,NA
function ,NA,NA
L,NA,NA
. 1/ (1 + (- 0.1),2,NA
 ) gives 0.99. This is expressed as follows:,NA,NA
 m,A,NA
(3) = L( (3-4) / 10) = 1/ (1 + (- 0.1),2,NA
 ) = 0.99,NA,NA
"You can verify the values, ",NA,NA
m,A,NA
"(0) = 0.862, and ",NA,NA
m,A,NA
(10) = 0.735.,NA,NA
Triangular Fuzzy Number,NA,NA
"With the right choice of a reference function, you can get a symmetrical fuzzy number ",NA,NA
A,NA,NA
", ",NA,NA
such that when you plot the membership function in ,NA,NA
"A,",NA,NA
 you get a triangle containing the ,NA,NA
pairs ,NA,NA
(,NA,NA
x,NA,NA
", ",NA,NA
m,A,NA
(,NA,NA
x,NA,NA
")),",NA,NA
 with ,NA,NA
m,A,NA
(,NA,NA
x,NA,NA
) > 0.,NA,NA
 An example is ,NA,NA
A,NA,NA
" = (5, 8)",L,NA
", where ",NA,NA
"L= max(1 - |x|, 0).",NA,NA
The numbers ,NA,NA
x,NA,NA
 that have positive values for ,NA,NA
m,A,NA
(,NA,NA
x,NA,NA
") are in the interval ( -3, 13 ). Also, ",NA,NA
m,A,NA
( -,NA,NA
"3 ) = 0, and ",NA,NA
m,A,NA
"(13) = 0. However, ",NA,NA
m,A,NA
(,NA,NA
x,NA,NA
) has its maximum value at ,NA,NA
x,NA,NA
" = 5. Now, if ",NA,NA
x,NA,NA
 is less ,NA,NA
"than -3 or greater than 13, the value of ",NA,NA
L,NA,NA
" is zero, and you do not consider such a number ",NA,NA
for membership in ,NA,NA
A.,NA,NA
 So all the elements for which membership in ,NA,NA
A,NA,NA
 is nonzero are in the ,NA,NA
triangle.,NA,NA
This ,NA,NA
triangular fuzzy number,NA,NA
" is shown in Figure 16.1. The height of the triangle is 1, and ",NA,NA
"the width is 16, twice the number 8, midpoint of the base is at 5. The pair of numbers 5 and ",NA,NA
8 are the ones defining the symmetrical fuzzy number ,NA,NA
A,NA,NA
. The vertical axis gives the ,NA,NA
"membership, so the range for this is from 0 to 1.",NA,NA
Figure 16.1,NA,NA
  Triangular membership function.,NA,NA
Linear Possibility Regression Model,NA,NA
Assume that you have ,NA,NA
(n + 1)-tuples,NA,NA
 of values of ,NA,NA
x,1,NA
", ... x",n,NA
",",NA,NA
 and ,NA,NA
y,NA,NA
". That is, for each ",NA,NA
"i, i ",NA,NA
ranging from ,NA,NA
1,NA,NA
 to ,NA,NA
"k,",NA,NA
 you have ,NA,NA
(x,1,NA
", ... , x",n,NA
", y),",NA,NA
 which are ,NA,NA
k,NA,NA
 sample observations on ,NA,NA
X,1,NA
", ... , ",NA,NA
X,n,NA
",",NA,NA
 and ,NA,NA
Y,NA,NA
. The linear possibility regression model is formulated differently depending ,NA,NA
upon whether the data collected is crisp or fuzzy.,NA,NA
"Let us give such a model below, for the case with crisp data. Then the fuzziness lies in the ",NA,NA
"coefficients in the model. You use symmetrical fuzzy numbers, ",NA,NA
A,j,NA
 = (a,j,NA
", b",j,NA
),L,NA
.,NA,NA
 The linear ,NA,NA
possibility regression model is formulated as:,NA,NA
Y,j,NA
 = A,0,NA
 + A,1,NA
X,j1,NA
 + ... + A,n,NA
X,jn,NA
The value of ,NA,NA
Y,NA,NA
" from the model is a fuzzy number, since it is a function of the fuzzy ",NA,NA
coefficients. The fuzzy coefficients ,NA,NA
A,j,NA
are chosen as those that minimize the width (the base ,NA,NA
of the triangle) of the fuzzy number ,NA,NA
Y,j,NA
.,NA,NA
 But ,NA,NA
A,j,NA
 is also determined by how big the ,NA,NA
membership of observed ,NA,NA
y,j,NA
" is to be, in the fuzzy number ",NA,NA
Y,j,NA
 .,NA,NA
 This last observation provides ,NA,NA
a constraint for the linear programming problem which needs to be solved to find the linear ,NA,NA
possibility regression. You select a value ,NA,NA
"d,",NA,NA
 and ask that ,NA,NA
m,Y,NA
(y),NA,NA
 [ge] ,NA,NA
d,NA,NA
.,NA,NA
We close this section by observing that linear possibility regression gives triangular fuzzy ,NA,NA
numbers for ,NA,NA
"Y,",NA,NA
" the dependent variable. It is like doing interval estimation, or getting a ",NA,NA
"regression band. Readers who are seriously interested in this topic should refer to Terano, ",NA,NA
et al. (see references).,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/492-496.html (4 of 4) [21/11/02 21:58:45]",NA
Previous Table of Contents Next,NA,NA
Section II: Fuzzy Control,NA,NA
"This section discusses the fuzzy logic controller (FLC), its application and design. Fuzzy ",NA,NA
"control is used in a variety of machines and processes today, with widespread application ",NA,NA
"especially in Japan. A few of the applications in use today are in the list in Table 16.10, ",NA,NA
"adapted from Yan, et al. ",NA,NA
Table 16.10,NA,NA
Applications of Fuzzy Logic Controllers (FLCs) and Functions Performed ,NA,NA
Application ,NA,NA
Video camcorder ,NA,NA
Washing machine ,NA,NA
Television ,NA,NA
Motor control ,NA,NA
Subway train ,NA,NA
Vacuum cleaner ,NA,NA
Hot water heater ,NA,NA
FLC function(s) ,NA,NA
Determine best focusing and lighting when there is movement ,NA,NA
in the picture ,NA,NA
"Adjust washing cycle by judging the dirt, size of the load, and ",NA,NA
type of fabric ,NA,NA
"Adjust brightness, color, and contrast of picture to please ",NA,NA
viewers ,NA,NA
Improve the accuracy and range of motion control under ,NA,NA
unexpected conditions ,NA,NA
Increase the stable drive and enhance the stop accuracy by ,NA,NA
evaluating the passenger traffic conditions. Provide a smooth ,NA,NA
start and smooth stop. ,NA,NA
Adjust the vacuum cleaner motor power by judging the ,NA,NA
amount of dust and dirt and the floor characteristics ,NA,NA
Adjust the heating element power according to the ,NA,NA
temperature and the quantity of water being used ,NA,NA
Helicopter control ,NA,NA
Determine the best operation actions by judging human ,NA,NA
instructions and the flying conditions including wind speed ,NA,NA
and direction ,NA,NA
Designing a Fuzzy Logic Controller,NA,NA
A fuzzy logic controller diagram was shown in Chapter 3. Let us redraw it now and discuss ,NA,NA
"a design example. Refer to Figure 16.2. For the purpose of discussion, let us assume that ",NA,NA
"this FLC controls a hot water heater. The hot water heater has a knob, HeatKnob(0-10) on ",NA,NA
"it to control the heating element power, the higher the value, the hotter it gets, with a value ",NA,NA
of 0 indicating the heating element is turned off. There are two sensors in the hot water ,NA,NA
"heater, one to tell you the temperature of the water (TempSense), which varies from 0 to ",NA,NA
"125° C, and the other to tell you the level of the water in the tank ",NA,NA
"(LevelSense), which varies from 0 = empty to 10 = full. Assume that there is an automatic ",NA,NA
flow control that determines how much cold water (at temperature 10° C) flows into the ,NA,NA
"tank from the main water supply; whenever the level of the water gets below 40, the flow ",NA,NA
"control turns on, and turns off when the level of the water gets above 95. ",NA,NA
Figure 16.2,NA,NA
  Fuzzy control of a water heater.,NA,NA
The design objective can be stated as: ,NA,NA
"Keep the water temperature as close to 80° C as possible, in spite of changes in the ",NA,NA
"water flowing out of the tank, and cold water flowing into the tank.",NA,NA
Step One: Defining Inputs and Outputs for the FLC,NA,NA
The range of values that inputs and outputs may take is called the ,NA,NA
universe of discourse. ,NA,NA
"We need to define the universe of discourse for all of the inputs and outputs of the FLC, ",NA,NA
which are all crisp values. Table 16.11 shows the ranges:,NA,NA
Table 16.11,NA,NA
Universe of Discourse for Inputs and Outputs for FLC ,NA,NA
Name ,NA,NA
Input/Output ,NA,NA
Minimum value ,NA,NA
Maximum value ,NA,NA
LevelSense ,NA,NA
I ,NA,NA
0 ,NA,NA
10 ,NA,NA
HeatKnob ,NA,NA
O ,NA,NA
0 ,NA,NA
10 ,NA,NA
TempSense ,NA,NA
I ,NA,NA
0 ,NA,NA
125 ,NA,NA
Step Two: Fuzzify the Inputs,NA,NA
The inputs to the FLC are the LevelSense and the TempSense. We can use triangular ,NA,NA
"membership functions to fuzzify the inputs, just as we did in Chapter 3, when we ",NA,NA
constructed the fuzzifier program. There are some general guidelines you can keep in mind ,NA,NA
when you determine the range of the fuzzy variables as related to the crisp inputs (adapted ,NA,NA
"from Yan, et al.): ",NA,NA
1.,NA,NA
  Symmetrically distribute the fuzzified values across the universe of discourse. ,NA,NA
2.,NA,NA
  Use an odd number of fuzzy sets for each variable so that some set is assured to ,NA,NA
be in the middle. The use of 5 to 7 sets is fairly typical. ,NA,NA
3.,NA,NA
  Overlap adjacent sets (by 15% to 25% typically) . ,NA,NA
Both the input variables LevelSense and TempSense are restricted to positive values. We ,NA,NA
use the following fuzzy sets to describe them: ,NA,NA
"XSmall, Small, Medium, Large, XLarge ",NA,NA
"In Table 16.12 and Figure 16.3, we show the assignment of ranges and triangular fuzzy ",NA,NA
"membership functions for LevelSense. Similarly, we assign ranges and triangular fuzzy ",NA,NA
membership functions for TempSense in Table 16.13 and Figure 16.4. The optimization of ,NA,NA
these assignments is often done through trial and error for achieving optimum performance ,NA,NA
of the FLC. ,NA,NA
Table 16.12,NA,NA
Fuzzy Variable Ranges for LevelSense ,NA,NA
Crisp Input Range ,NA,NA
Fuzzy Variable ,NA,NA
0–2 ,NA,NA
XSmall ,NA,NA
1.5–4 ,NA,NA
Small ,NA,NA
3–7 ,NA,NA
Medium ,NA,NA
6–8.5 ,NA,NA
Large ,NA,NA
7.5–10 ,NA,NA
XLarge ,NA,NA
Figure 16.3,NA,NA
  Fuzzy membership functions for LevelSense.,NA,NA
Table 16.13,NA,NA
Fuzzy Variable Ranges for TempSense ,NA,NA
Crisp Input Range ,NA,NA
Fuzzy Variable ,NA,NA
0–20 ,NA,NA
XSmall ,NA,NA
10–35 ,NA,NA
Small ,NA,NA
30–75 ,NA,NA
Medium ,NA,NA
60–95 ,NA,NA
Large ,NA,NA
85–125 ,NA,NA
XLarge ,NA,NA
Figure 16.4,NA,NA
  Fuzzy membership functions for TempSense.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Previous Table of Contents Next,NA,NA
Step Three: Set Up Fuzzy Membership Functions for the Output(s),NA,NA
"In our example, we have just one output, which is the HeatKnob. We need to assign fuzzy ",NA,NA
memberships to this variable just as we did for the inputs. This is shown in Table 16.14 ,NA,NA
and Figure 16.5. We use different variable names to make the example clearer later on. ,NA,NA
Table 16.14,NA,NA
Fuzzy Variable Ranges for HeatKnob ,NA,NA
Crisp Input Range ,NA,NA
Fuzzy Variable ,NA,NA
0–2 ,NA,NA
VeryLittle ,NA,NA
1.5–4 ,NA,NA
ALittle ,NA,NA
3–7 ,NA,NA
AGoodAmount ,NA,NA
6–8.5 ,NA,NA
ALot ,NA,NA
7.5–10 ,NA,NA
AWholeLot ,NA,NA
Figure 16.5,NA,NA
  Fuzzy membership functions for the output HeatKnob.,NA,NA
Step Four: Create a Fuzzy Rule Base,NA,NA
"Now that you have the inputs and the output defined in terms of fuzzy variables, you need ",NA,NA
"only specify what actions to take under what conditions; that is, you need to construct a set ",NA,NA
of rules that describe the operation of the FLC. These rules usually take the form of ,NA,NA
"IF–THEN rules and can be obtained from a human expert (heuristics), or can be supplied ",NA,NA
from a neural network that infers the rules from behavior of the system. We mentioned this ,NA,NA
idea in Chapter 3. ,NA,NA
"Let us construct a rule base for our design example. For the two inputs, we define the ",NA,NA
matrix shown in Table 16.15. Our heuristic guidelines in determining this matrix are the ,NA,NA
following statements and their converses:,NA,NA
1.,NA,NA
"  When the temperature is low, the HeatKnob should be set higher than when the ",NA,NA
temperature is high. ,NA,NA
2.,NA,NA
"  When the volume of water is low, the HeatKnob does not need to be as high as ",NA,NA
when the volume of water is high. ,"NOTE:  
 In FLCs, we do not need to specify all the boxes in the matrix. That is perfectly 
 fine. No entry signifies that no action is taken, for example, in the column for 
  
 SenseTemp=XL, no action is required since the temperature is already at or above the 
 target temperature.",NA
Table 16.15,NA,NA
"Fuzzy Rule Base for the Example Design, Output HeatKnob ",NA,NA
SenseTemp-> ,NA,NA
Sense Level ,NA,NA
XS ,NA,NA
S ,NA,NA
M ,NA,NA
L ,NA,NA
XL ,NA,NA
\/ ,NA,NA
XS ,NA,NA
AGoodAmount ,NA,NA
ALittle ,NA,NA
VeryLittle ,NA,NA
VeryLittle ,NA,NA
S ,NA,NA
ALot ,NA,NA
AGoodAmount ,NA,NA
VeryLittle ,NA,NA
M ,NA,NA
AWholeLot ,NA,NA
ALot ,NA,NA
AGoodAmount ,NA,NA
VeryLittle ,NA,NA
L ,NA,NA
AWholeLot ,NA,NA
ALot ,NA,NA
ALot ,NA,NA
ALittle ,NA,NA
XL ,NA,NA
AWholeLot ,NA,NA
ALot ,NA,NA
ALot ,NA,NA
AGoodAmount ,NA,NA
Let us examine a couple of typical entries in the table: For SenseLevel = Medium (M) and ,NA,NA
"SenseTemp = XSmall (XS), the output is HeatKnob = AWholeLot. Now for the same ",NA,NA
"temperature, as the water level rises, the setting on HeatKnob also should rise to ",NA,NA
"compensate for the added volume of water. You can see that for SenseLevel = Large(L), ",NA,NA
"and SenseTemp = XSmall(XS), the output is HeatKnob = AWholeLot. You can verify that ",NA,NA
the rest of the table is created by similar reasoning. ,NA,NA
Creating IF–THEN Rules,NA,NA
We can now translate the table entries into IF - THEN rules. We take these directly from ,NA,NA
Table 16.15: ,NA,NA
1.,NA,NA
  IF SenseTemp IS XSmall AND SenseLevel IS XSmall THEN SET HeatKnob ,NA,NA
TO AGoodAmount ,NA,NA
2.,NA,NA
  IF SenseTemp IS XSmall AND SenseLevel IS Small THEN SET HeatKnob TO ,NA,NA
ALot ,NA,NA
3.,NA,NA
  IF SenseTemp IS XSmall AND SenseLevel IS Medium THEN SET HeatKnob ,NA,NA
TO AWholeLot ,NA,NA
4.,NA,NA
  IF SenseTemp IS XSmall AND SenseLevel IS Large THEN SET HeatKnob TO ,NA,NA
AWholeLot ,NA,NA
5.,NA,NA
  IF SenseTemp IS XSmall AND SenseLevel IS XLarge THEN SET HeatKnob ,NA,NA
TO AWholeLot ,NA,NA
6.,NA,NA
  IF SenseTemp IS Small AND SenseLevel IS XSmall THEN SET HeatKnob TO ,NA,NA
ALittle ,NA,NA
7.,NA,NA
  IF SenseTemp IS Small AND SenseLevel IS Small THEN SET HeatKnob TO ,NA,NA
AGoodAmount ,NA,NA
8.,NA,NA
  IF SenseTemp IS Small AND SenseLevel IS Medium THEN SET HeatKnob TO ,NA,NA
ALot ,NA,NA
9.,NA,NA
  IF SenseTemp IS Small AND SenseLevel IS Large THEN SET HeatKnob TO ,NA,NA
ALot ,NA,NA
10.,NA,NA
  IF SenseTemp IS Small AND SenseLevel IS XLarge THEN SET HeatKnob TO ,NA,NA
ALot ,NA,NA
11.,NA,NA
  IF SenseTemp IS Medium AND SenseLevel IS XSmall THEN SET HeatKnob ,NA,NA
TO VeryLittle ,NA,NA
12.,NA,NA
  IF SenseTemp IS Medium AND SenseLevel IS Small THEN SET HeatKnob ,NA,NA
TO VeryLittle ,NA,NA
13.,NA,NA
  IF SenseTemp IS Medium AND SenseLevel IS Medium THEN SET HeatKnob ,NA,NA
TO AGoodAmount ,NA,NA
14.,NA,NA
  IF SenseTemp IS Medium AND SenseLevel IS Large THEN SET HeatKnob ,NA,NA
TO ALot ,NA,NA
15.,NA,NA
  IF SenseTemp IS Medium AND SenseLevel IS XLarge THEN SET HeatKnob ,NA,NA
TO ALot ,NA,NA
16.,NA,NA
  IF SenseTemp IS Large AND SenseLevel IS Small THEN SET HeatKnob TO ,NA,NA
VeryLittle ,NA,NA
17.,NA,NA
  IF SenseTemp IS Large AND SenseLevel IS Medium THEN SET HeatKnob ,NA,NA
TO VeryLittle ,NA,NA
18.,NA,NA
  IF SenseTemp IS Large AND SenseLevel IS Large THEN SET HeatKnob TO ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/500-504.html (3 of 4) [21/11/02 21:58:48],NA
ALittle ,NA,NA
19.,NA,NA
  IF SenseTemp IS Large AND SenseLevel IS XLarge THEN SET HeatKnob ,NA,NA
TO AGoodAmount ,NA,NA
Remember that the output and inputs to the fuzzy rule base are fuzzy variables. For any ,NA,NA
"given crisp input value, there may be fuzzy membership in several fuzzy input variables ",NA,NA
(determined by the fuzzification step). And each of these fuzzy input variable activations ,NA,NA
will cause different fuzzy output cells to ,NA,NA
fire,NA,NA
", or be activated. This brings us to the final ",NA,NA
"step, defuzzification of the output into a crisp value.",NA,NA
Step Five: Defuzzify the Outputs,NA,NA
"In order to control the HeatKnob, we need to obtain a crisp dial setting. So far, we have ",NA,NA
"several of the IF–THEN rules of the fuzzy rule base firing at once, because the inputs have ",NA,NA
been fuzzified. How do we arrive at a single crisp output number ? There are actually ,NA,NA
"several different strategies for this; we will consider two of the most common, the ",NA,NA
center of ,NA,NA
area,NA,NA
 (,NA,NA
COA,NA,NA
) or ,NA,NA
centroid,NA,NA
" method, and the ",NA,NA
fuzzy Or,NA,NA
 method. The easiest way to understand ,NA,NA
the process is with an example.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/500-504.html (4 of 4) [21/11/02 21:58:48]",NA
Previous Table of Contents Next,NA,NA
"Assume that at a particular point in time, LevelSense = 7.0 and TempSense = 65. These are ",NA,NA
the crisp inputs directly from the sensors. With fuzzification (refer to Chapter 3 for a ,NA,NA
"review), assume that you get the following fuzzy memberships: ",NA,NA
crisp input — LevelSense = 7.0 ,NA,NA
fuzzy outputs with membership values - ,NA,NA
Medium: 0.4 ,NA,NA
Large: 0.6 ,NA,NA
all others : 0.0 ,NA,NA
crisp input — TempSense=65 ,NA,NA
fuzzy outputs with membership values - ,NA,NA
Medium: 0.75 ,NA,NA
Large: 0.25 ,NA,NA
all others: 0.0 ,NA,NA
This results in four rules firing: ,NA,NA
1.,NA,NA
  TempSense = Medium (0.75) AND LevelSense = Medium (0.4) ,NA,NA
2.,NA,NA
  TempSense = Large (0.25) AND LevelSense = Medium (0.4) ,NA,NA
3.,NA,NA
  ,NA,NA
TempSense = Medium (0.75) AND LevelSense = Large (0.6) ,NA,NA
4.,NA,NA
  ,NA,NA
TempSense = Large (0.25) AND LevelSense = Large (0.6) ,NA,NA
"First you must determine, for each of the AND clauses in the IF–THEN rules, what the ",NA,NA
output should be. This is done by the ,NA,NA
conjunction,NA,NA
 or minimum operator. So for each of ,NA,NA
these rules you have the following firing strengths:,NA,NA
1.,NA,NA
  (0.75) ,NA,NA
,NA,NA
 (0.4) = 0.4 ,NA,NA
2.,NA,NA
  (0.25) ,NA,NA
,NA,NA
 (0.4) = 0.25 ,NA,NA
3.,NA,NA
  (0.75) ,NA,NA
,NA,NA
 (0.6) = 0.6 ,NA,NA
4.,NA,NA
  (0.25) ,NA,NA
,NA,NA
 (0.6) = 0.25 ,NA,NA
"By using the fuzzy rule base and the strengths assigned previously, we find the rules ",NA,NA
recommend the following output values (with strengths) for HeatKnob: ,NA,NA
1.,NA,NA
  AGoodAmount (0.4) ,NA,NA
2.,NA,NA
  VeryLittle (0.25) ,NA,NA
3.,NA,NA
  ALot (0.6) ,NA,NA
4.,NA,NA
  ALittle (0.25) ,NA,NA
"Now we must combine the recommendations to arrive at a single crisp value. First, we will ",NA,NA
use the fuzzy Or method of defuzzification. Here we use a ,NA,NA
disjunction,NA,NA
 or maximum ,NA,NA
operator to combine the values. We obtain the following:,NA,NA
 (0.4) • (0.25) • (0.6) • (0.25) = 0.6,NA,NA
The crisp output value for HeatKnob would then be this membership value multiplied by ,NA,NA
"the range of the output variable, or (0.6) (10-0) = 6.0. ",NA,NA
Another way of combining the outputs is with the centroid method. With the centroid ,NA,NA
"method, there are two variations, the ",NA,NA
overlap composition,NA,NA
 method and the ,NA,NA
additive ,NA,NA
composition,NA,NA
 method,NA,NA
.,NA,NA
" To review, we have the following output values and strengths.",NA,NA
1.,NA,NA
  AGoodAmount (0.4) ,NA,NA
2.,NA,NA
  VeryLittle (0.25) ,NA,NA
3.,NA,NA
  ALot (0.6) ,NA,NA
4.,NA,NA
  ALittle (0.25) ,NA,NA
We use the strength value and fill in the particular triangular membership function to that ,NA,NA
"strength level. For example, for the first rule, we fill the ",NA,NA
triangular membership,NA,NA
" function, ",NA,NA
AGoodAmount,NA,NA
 to the 0.4 level. We then cut off the top of the triangle (above 0.4). Next ,NA,NA
we do the same for the other rules. Finally we align the truncated figures as shown in ,NA,NA
Figure 16.6 and combine them according to the overlap composition method or the additive ,NA,NA
composition method (Kosko). You can see the difference in these two ,NA,NA
composition methods in Figure 16.6. The overlap method simply superimposes all of the ,NA,NA
truncated triangles onto the same area. You can lose information with this method. The ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/504-508.html (2 of 5) [21/11/02 21:58:49],NA
additive method adds the geometrical figures on top of each other.,NA,NA
Figure 16.6,NA,NA
  Defuzzification with the Centroid approach: Overlap and Additive ,NA,NA
composition.,NA,NA
"In order to get a crisp value, you take the centroid or ",NA,NA
"center of gravity,",NA,NA
 of the resulting ,NA,NA
geometrical figure. Let us do this for the overlap method figure. The centroid is a straight ,NA,NA
edge that can be placed through the figure to have it perfectly balanced; there is equal area ,NA,NA
"of the figure on either side of the straight edge, as shown in Figure 16.7. Splitting up the ",NA,NA
"geometry into pieces and summing all area contributions on either side of the centroid, we ",NA,NA
get a value of 5.2 for this example. This is already in terms of the crisp output value range:,NA,NA
 HeatKnob = 5.2,NA,NA
Figure 16.7,NA,NA
  Finding the centroid.,NA,NA
This completes the design for the simple example we chose. We conclude with a list of the ,NA,NA
advantages and disadvantages of FLCs. ,NA,NA
Advantages and Disadvantages of Fuzzy Logic Controllers,NA,NA
The following list adapted from McNeill and Thro shows the advantages and ,NA,NA
disadvantages to FLCs for control systems as compared to more traditional control ,NA,NA
systems. ,NA,NA
Advantages:,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/504-508.html (3 of 5) [21/11/02 21:58:49],NA
•,NA,NA
"  Relates input to output in linguistic terms, easily understood ",NA,NA
•,NA,NA
  Allows for rapid prototyping because the system designer doesn’t need to know ,NA,NA
everything about the system before starting ,NA,NA
•,NA,NA
  Cheaper because they are easier to design ,NA,NA
•,NA,NA
  Increased robustness ,NA,NA
•,NA,NA
  Simplify knowledge acquisition and representation ,NA,NA
•,NA,NA
  A few rules encompass great complexity ,NA,NA
•,NA,NA
  Can achieve less overshoot and oscillation ,NA,NA
•,NA,NA
  Can achieve steady state in a shorter time interval ,NA,NA
Disadvantages:,NA,NA
•,NA,NA
  Hard to develop a model from a fuzzy system ,NA,NA
•,NA,NA
  Require more fine tuning and simulation before operational ,NA,NA
•,NA,NA
  Have a stigma associated with the word ,NA,NA
fuzzy,NA,NA
 (at least in the Western world); ,NA,NA
engineers and most other people are used to crispness and shy away from fuzzy ,NA,NA
control and fuzzy decision making ,NA,NA
Summary,NA,NA
Fuzzy logic applications are many and varied. You got an overview of the different ,NA,NA
"applications areas that exist for fuzzy logic, from the control of washing machines to fuzzy ",NA,NA
logic based cost benefit analysis. Further you got details on two application domains: fuzzy ,NA,NA
databases and fuzzy control. ,NA,NA
This chapter dealt with extending database models to accommodate fuzziness in data ,NA,NA
"attribute values and in queries as well. You saw fuzzy relations; in particular, similarity and ",NA,NA
"resemblance relations, and similarity classes were reviewed. You found how ",NA,NA
possibility distributions help define fuzzy databases. You also learned what fuzzy events ,NA,NA
"are and how to calculate fuzzy means, fuzzy variances, and fuzzy conditional expectations. ",NA,NA
Concepts related to linear possibility regression model were presented.,NA,NA
The chapter presented the design of a simple fuzzy logic control (FLC) system to regulate ,NA,NA
the temperature of water in a hot water heater. The components of the FLC were discussed ,NA,NA
along with design procedures. The advantages of FLC design include rapid prototyping ,NA,NA
ability and the capability to solve very nonlinear control problems without knowing details ,NA,NA
of the nonlinearities.,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/504-508.html (4 of 5) [21/11/02 21:58:49],NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch16/504-508.html (5 of 5) [21/11/02 21:58:49]",NA
Previous Table of Contents Next,NA,NA
Chapter 17 ,NA,NA
Further Applications ,NA,NA
Introduction,NA,NA
"In this chapter, we present the outlines of some applications of neural networks and fuzzy ",NA,NA
logic. Most of the applications fall into a few main categories according to the paradigms ,NA,NA
they are based on. We offer a sampling of topics of research as found in the current ,NA,NA
"literature, but there are literally thousands of applications of neural networks and fuzzy ",NA,NA
"logic in science, technology and business with more and more applications added as time ",NA,NA
goes on. ,NA,NA
Some applications of neural networks are for adaptive control. Many such applications ,NA,NA
benefit from adding fuzziness also. Steering a car or backing up a truck with a fuzzy ,NA,NA
controller is an example. A large number of applications are based on the backpropagation ,NA,NA
training model. Another category of applications deals with classification. Some ,NA,NA
applications based on expert systems are augmented with a neural network approach. ,NA,NA
Decision support systems are sometimes designed this way. Another category is made up of ,NA,NA
"optimizers,",NA,NA
 whose purpose is to find the maximum or the minimum of a function.,"NOTE:  
 You will find other neural network applications related to finance presented 
 toward the end of Chapter 14.",NA
Computer Virus Detector,NA,NA
IBM Corporation has applied neural networks to the problem of detecting and correcting ,NA,NA
computer viruses. IBM’s AntiVirus program detects and eradicates new viruses ,NA,NA
automatically. It works on boot-sector types of viruses and keys off of the stereotypical ,NA,NA
behaviors that viruses usually exhibit. The feedforward backpropagation neural network ,NA,NA
was used in this application. New viruses discovered are used in the training set for later ,NA,NA
versions of the program to make them “smarter.” The system was modeled after ,NA,NA
knowledge about the human immune system: IBM uses a decoy program to “attract” a ,NA,NA
"potential virus, rather than have the virus attack the user’s files. These decoy programs are ",NA,NA
then immediately tested for infection. If the behavior of the decoy program seems like the ,NA,NA
"program was infected, then the virus is detected on that program and removed wherever it’s ",NA,NA
found. ,NA,NA
Mobile Robot Navigation,NA,NA
"C. Lin and C. Lee apply a multivalued Boltzmann machine, modeled by them, using an ",NA,NA
"artificial magnetic field approach. They define attractive and repulsive magnetic fields, ",NA,NA
"corresponding to goal position and obstacle, respectively. The weights on the connections ",NA,NA
in the Boltzmann machine are none other than the magnetic fields. ,NA,NA
They divide a two-dimensional traverse map into small grid cells. Given the goal cell and ,NA,NA
"obstacle cells, the problem is to navigate the two-dimensional mobile robot from an ",NA,NA
"unobstructed cell to the goal quickly, without colliding with any obstacle. An attracting ",NA,NA
artificial magnetic field is built for the goal location. They also build a repulsive artificial ,NA,NA
"magnetic field around the boundary of each obstacle. Each neuron, a grid cell, will point to ",NA,NA
"one of its eight neighbors, showing the direction for the movement of the robot. In other ",NA,NA
"words, the Boltzmann machine is adapted to become a compass for the mobile robot.",NA,NA
A Classifier,NA,NA
James Ressler and Marijke Augusteijn study the use of neural networks to the problem of ,NA,NA
weapon to target assignment. The neural network is used as a filter to remove unfeasible ,NA,NA
"assignments, where feasibility is determined in terms of the weapon’s ability to hit a given ",NA,NA
target if fired at a specific instant. The large number of weapons and threats along with the ,NA,NA
limitation on the amount of time lend significance to the need for reducing the number of ,NA,NA
assignments to consider. ,NA,NA
"The network’s role here is classifier, as it needs to separate the infeasible assignments from ",NA,NA
"the feasible ones. Learning has to be quick, and so Ressler and Augusteijn prefer to use an ",NA,NA
architecture called the ,NA,NA
cascade-correlation,NA,NA
" learning architecture, over backpropagation ",NA,NA
learning. Their network is dynamic in that the number of hidden layer neurons is ,NA,NA
determined during the training phase. This is part of a class of algorithms that change the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/511-513.html (2 of 3) [21/11/02 21:58:50],NA
architecture of the network during training.,NA,NA
A Two-Stage Network for Radar Pattern Classification,NA,NA
Mohammad Ahmadian and Russell Pimmel find it convenient to use a multistage neural ,NA,NA
"network configuration, a two-stage network in particular, for classifying patterns. The ",NA,NA
patterns they study are geometrical features of simulated radar targets. ,NA,NA
"Feature extraction is done in the first stage, while classification is done in the second. ",NA,NA
"Moreover, the first stage is made up of several networks, each for extracting a different ",NA,NA
estimable feature. Backpropagation is used for learning in the first stage. They use a single ,NA,NA
network in the second stage. The effect of noise is also studied.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/511-513.html (3 of 3) [21/11/02 21:58:50]",NA
Previous Table of Contents Next,NA,NA
Crisp and Fuzzy Neural Networks for Handwritten ,NA,NA
Character Recognition,NA,NA
"Paul Gader, Magdi Mohamed, and Jung-Hsien Chiang combine a fuzzy neural network and ",NA,NA
a crisp neural network for the recognition of handwritten alphabetic characters. They use ,NA,NA
backpropagation for the crisp neural network and a clustering algorithm called K-nearest ,NA,NA
neighbor for the fuzzy network. Their consideration of a fuzzy network in this study is ,NA,NA
"prompted by their belief that if some ambiguity is possible in deciphering a character, such ",NA,NA
"ambiguity should be accurately represented in the output. For example, a handwritten “u” ",NA,NA
"could look like a “v” or “u.” If present, the authors feel that this ambiguity should be ",NA,NA
translated to the classifier output. ,NA,NA
Feature extraction was accomplished as follows: character images of size 24x16 pixels ,NA,NA
were used. The first stage of processing extracted eight feature images from the input ,NA,NA
"image, two for each direction (north, northeast, northwest, and east). Each feature image ",NA,NA
uses an integer at each location that represents the length of the longest bar that fits at that ,NA,NA
point in that direction. These are referred to as ,NA,NA
bar features.,NA,NA
 Next 8x8 overlapping zones ,NA,NA
are used on the feature images to derive feature vectors. These are made by taking the ,NA,NA
summed values of the values in a zone and dividing this by the maximum possible value in ,NA,NA
"the zone. Each feature image results in a 15,120 element feature vectors.",NA,NA
"Data was obtained from the U.S. Postal Office, consisting of 250 characters. Results ",NA,NA
"showed 97.5% and 95.6% classification rates on training and test sets, respectively, for the ",NA,NA
"neural network. The fuzzy network resulted in 94.7% and 93.8% classification rates, where ",NA,NA
the desired output for many characters was set to ambiguous.,NA,NA
Noise Removal with a Discrete Hopfield Network,NA,NA
"Arun Jagota applies what is called a HcN, a special case of a discrete Hopfield network, to ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/513-515.html (1 of 3) [21/11/02 21:58:51],NA
the problem of recognizing a degraded printed word. HcN is used to process the output of ,NA,NA
"an Optical Character Recognizer, by attempting to remove noise. A dictionary of words is ",NA,NA
stored in the HcN and searched. ,NA,NA
Object Identification by Shape,NA,NA
"C. Ganesh, D. Morse, E. Wetherell, and J. Steele used a neural network approach to an ",NA,NA
"object identification system, based on the shape of an object and independent of its size. A ",NA,NA
two-dimensional grid of ultrasonic data represents the height profile of an object. The data ,NA,NA
grid is compressed into a smaller set that retains the essential features. Backpropagation is ,NA,NA
used. Recognition on the order of approximately 70% is achieved. ,NA,NA
Detecting Skin Cancer,NA,NA
"F. Ercal, A. Chawla, W. Stoecker, and R. Moss study a neural network approach to the ",NA,NA
diagnosis of malignant melanoma. They strive to discriminate tumor images as malignant ,NA,NA
or benign. There are as many as three categories of benign tumors to be distinguished from ,NA,NA
malignant melanoma. Color images of skin tumors are used in the study. Digital images of ,NA,NA
tumors are classified. Backpropagation is used. Two approaches are taken to reduce ,NA,NA
"training time. The first approach involves using fewer hidden layers, and the second ",NA,NA
involves randomization of the order of presentation of the training set. ,NA,NA
EEG Diagnosis,NA,NA
"Fred Wu, Jeremy Slater, R. Eugene Ramsay, and Lawrence Honig use a feedforward ",NA,NA
backpropagation neural network as a classifier in EEG diagnosis. They compare the ,NA,NA
performance of the neural network classifier to that of a nearest neighbor classifier. The ,NA,NA
neural network classifier shows a classifier accuracy of 75% for Multiple Sclerosis patients ,NA,NA
versus 65% for the nearest neighbor algorithm. ,NA,NA
Time Series Prediction with Recurrent and Nonrecurrent ,NA,NA
Networks,NA,NA
Sathyanarayan Rao and Sriram Sethuraman take a recurrent neural network and a ,NA,NA
feedforward network and train then in parallel. A recurrent neural network has feedback ,NA,NA
connections from the output neurons back to input neurons to model the storage of ,NA,NA
temporal information. A modified backpropagation algorithm is used for training the ,NA,NA
"recurrent network, called the ",NA,NA
real-time recurrent learning algorithm.,NA,NA
 They have the ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/513-515.html (2 of 3) [21/11/02 21:58:51],NA
"recurrent neural network store past information, and the feedforward network do the ",NA,NA
learning of nonlinear dependencies on the current samples. They use this scheme because ,NA,NA
"the recurrent network takes more than one time period to evaluate its output, whereas the ",NA,NA
feedforward network does not. This hybrid scheme overcomes the latency problem for the ,NA,NA
"recurrent network, providing immediate nonlinear evaluation from input to output.",NA,NA
Security Alarms,NA,NA
Deborah Frank and J. Bryan Pletta study the application of neural networks for alarm ,NA,NA
classification based on their operation under varying weather conditions. Performance ,NA,NA
degradation of a security system when the environment changes is a cause for losing ,NA,NA
confidence in the system itself. This problem is more acute with portable security systems. ,NA,NA
"They investigated the problem using several networks, ranging from backpropagation to ",NA,NA
"learning vector quantization. Data was collected using many scenarios, with and without ",NA,NA
"the coming of an intruder, which can be a vehicle or a human.",NA,NA
They found a 98% probability of detection and 9% nuisance alarm rate over all weather ,NA,NA
conditions.,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/513-515.html (3 of 3) [21/11/02 21:58:51]",NA
Previous Table of Contents Next,NA,NA
Circuit Board Faults,NA,NA
"Anthony Mason, Nicholas Sweeney, and Ronald Baer studied the neural network approach ",NA,NA
"in two laboratory experiments and one field experiment, in diagnosing faults in circuit ",NA,NA
boards. ,NA,NA
Test point readings were expressed as one vector. A fault vector was also defined with ,NA,NA
elements representing possible faults. The two vectors became a training pair. ,NA,NA
Backpropagation was used.,NA,NA
Warranty Claims,NA,NA
Gary Wasserman and Agus Sudjianto model the prediction of warranty claims with neural ,NA,NA
networks. The nonlinearity in the data prompted this approach. ,NA,NA
The motivation for the study comes from the need to assess warranty costs for a company ,NA,NA
that offers extended warranties for its products. This is another application that uses ,NA,NA
backpropagation. The architecture used was 2-10-1.,NA,NA
Writing Style Recognition,NA,NA
J. Nellis and T. Stonham developed a neural network character recognition system that ,NA,NA
adapts dynamically to a writing style. ,NA,NA
"They use a hybrid neural network for hand-printed character recognition, that integrates ",NA,NA
image processing and neural network architectures. The neural network uses random access ,NA,NA
memory (RAM) to model the functionality of an individual neuron. The authors use a ,NA,NA
transform called the ,NA,NA
five-way,NA,NA
" image processing transform on the input image, which is of ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/515-518.html (1 of 3) [21/11/02 21:58:51],NA
size 32x32 pixels. The transform converts the high spatial frequency data in a character into ,NA,NA
"four low frequency representations. What they achieve by this are position invariance, and ",NA,NA
"a ratio of black to white pixels approaching 1, rotation invariance, and capability to detect ",NA,NA
and correct breaks within characters. The transformed data are input to the neural network ,NA,NA
that is used as a classifier and is called a ,NA,NA
discriminator,NA,NA
.,NA,NA
A particular writing style that has less variability and therefore fewer subclasses is needed ,NA,NA
"to classify the style. Network size will also reduce confusion, and conflicts lessen.",NA,NA
Commercial Optical Character Recognition,NA,NA
Optical character recognition (OCR) is one of the most successful commercial applications ,NA,NA
"of neural networks. Caere Corporation brought out its neural network product in 1992, after ",NA,NA
"studying more than 100,000 examples of fax documents. Caere’s AnyFax technology ",NA,NA
combines neural networks with expert systems to extract character information from Fax or ,NA,NA
"scanned images. Calera, another OCR vendor, started using neural networks in 1984 and ",NA,NA
also benefited from using a very large (more than a million variations of alphanumeric ,NA,NA
characters) training set. ,NA,NA
ART-EMAP and Object Recognition,NA,NA
A neural network architecture called ART-EMAP (Gail Carpenter and William Ross) ,NA,NA
integrates Adaptive Resonance Theory (ART) with spatial and temporal evidence ,NA,NA
integration for predictive mapping (EMAP). The result is a system capable of complex 3-D ,NA,NA
object recognition. A vision system that samples two-dimensional perspectives of a three-,NA,NA
dimensional object is created that results in 98% correct recognition with an average of 9.2 ,NA,NA
"views presented on noiseless test data, and 92% recognition with an average of 11.2 views ",NA,NA
presented on noisy test data. The ART-EMAP system is an extension of ,NA,NA
"ARTMAP,",NA,NA
 which is ,NA,NA
a neural network architecture that performs supervised learning of recognition categories ,NA,NA
and multidimensional maps in response to input vectors. A fuzzy logic extension of ,NA,NA
ARTMAP is called ,NA,NA
Fuzzy ARTMAP,NA,NA
", which incorporates two fuzzy modules in the ART ",NA,NA
system.,NA,NA
Summary,NA,NA
A sampling of current research and commercial applications with neural networks and ,NA,NA
fuzzy logic technology is presented. Neural networks are applied toward a wide variety of ,NA,NA
"problems, from aiding medical diagnosis to detecting circuit faults in printed circuit board ",NA,NA
manufacturing. Some of the problem areas where neural networks and fuzzy logic have ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/515-518.html (2 of 3) [21/11/02 21:58:51],NA
been successfully applied are: ,NA,NA
•,NA,NA
  Filtering ,NA,NA
•,NA,NA
  Image processing ,NA,NA
•,NA,NA
  Intelligent control ,NA,NA
•,NA,NA
  Machine vision ,NA,NA
•,NA,NA
  Motion analysis ,NA,NA
•,NA,NA
  Optimization ,NA,NA
•,NA,NA
  Pattern recognition ,NA,NA
•,NA,NA
  Prediction ,NA,NA
•,NA,NA
  Time series analysis ,NA,NA
•,NA,NA
  Speech synthesis ,NA,NA
•,NA,NA
  Machine learning and robotics ,NA,NA
•,NA,NA
  Decision support systems ,NA,NA
•,NA,NA
  Classification ,NA,NA
•,NA,NA
  Data compression ,NA,NA
•,NA,NA
  Functional approximation ,NA,NA
The use of fuzzy logic and neural networks in software and hardware systems can only ,NA,NA
increase! ,NA,NA
Previous Table of Contents Next,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem..._Neural_Networks_and_Fuzzy_Logic/ch17/515-518.html (3 of 3) [21/11/02 21:58:51]",NA
Table of Contents,NA,NA
References,NA,NA
"Ahmadian, Mohamad, and Pimmel, Russell, “Recognizing Geometrical Features of ",NA,NA
"Simulated Targets with Neural Networks,” Conference Proceedings of the 1992 ",NA,NA
"Artificial Neural Networks in Engineering Conference, V.3, pp. 409–411. ",NA,NA
"Aleksander, Igor, and Morton, Helen, ",NA,NA
An Introduction to Neural Computing,NA,NA
", ",NA,NA
"Chapman and Hall, London, 1990. ",NA,NA
"Aiken, Milam, “Forecasting T-Bill Rates with a Neural Net,” Technical Analysis of ",NA,NA
"Stocks and Commodities, May 1995, Technical Analysis Inc., Seattle. ",NA,NA
"Anderson, James, and Rosenfeld, Edward, eds., ",NA,NA
Neurocomputing: Foundations of ,NA,NA
Research,NA,NA
", MIT Press, Cambridge, MA, 1988. ",NA,NA
"Anzai, Yuichiro, ",NA,NA
Pattern Recognition and Machine Learning,NA,NA
", Academic Press, ",NA,NA
"Englewood Cliffs, NJ, 1992. ",NA,NA
"Azoff, E. Michael, ",NA,NA
Neural Network Time Series Forecasting of Financial Markets,NA,NA
", ",NA,NA
"John Wiley & Sons, New York, 1994. ",NA,NA
"Bauer, Richard J., ",NA,NA
Genetic Algorithms and Investment Strategies,NA,NA
", John Wiley & ",NA,NA
"Sons, New York, 1994. ",NA,NA
"Booch, Grady, ",NA,NA
"Object Oriented Design with Applications,",NA,NA
" Benjamin-Cummings, ",NA,NA
"Redwood City, CA, 1991. ",NA,NA
"Carpenter, Gail A., and Ross, William D., “ART-EMAP: A Neural Network ",NA,NA
"Architecture for Object Recognition by Evidence Accumulation,” ",NA,NA
IEEE ,NA,NA
Transactions on Neural Networks,NA,NA
", Vol. 6., No. 4, July 1995, pp. 805–818. ",NA,NA
"Colby, Robert W., and Meyers, Thomas A., ",NA,NA
The Encyclopedia of Technical Market ,NA,NA
Indicators,NA,NA
", Business One Irwin, Homewood, IL, 1988. ",NA,NA
"Collard, J. E., “Commodity Trading with a Three Year Old,” ",NA,NA
Neural Networks in ,NA,NA
Finance and Investing,NA,NA
", pp. 411–420, Probus Publishing, Chicago, 1993. ",NA,NA
"Cox, Earl, ",NA,NA
The Fuzzy Systems Handbook,NA,NA
", Academic Press, Boston, 1994. ",NA,NA
"Dagli, Cihan, et al., eds., ",NA,NA
Intelligent Engineering Systems through Artificial Neural ,NA,NA
Networks,NA,NA
", Volume 2, ASME Press, New York, 1992. ",NA,NA
"Davalo, Eric, and Naim, Patrick, ",NA,NA
Neural Networks,NA,NA
", MacMillan, New York, 1991. ",NA,NA
"Ercal, F., et al., “Diagnosing Malignant Melanoma Using a Neural Network,” ",NA,NA
Conference Proceedings of the 1992 Artificial Neural Networks in Engineering ,NA,NA
"Conference, V.3, pp. 553–555. ",NA,NA
"Frank, Deborah, and Pletta, J. Bryan, “Neural Network Sensor Fusion for Security ",NA,NA
"Applications,” Conference Proceedings of the 1992 Artificial Neural Networks in ",NA,NA
"Engineering Conference, V.3, pp. 745–748. ",NA,NA
"Freeman, James A., and Skapura, David M., ",NA,NA
"Neural Networks Algorithms, ",NA,NA
"Applications, and Programming Techniques",NA,NA
", Addison-Wesley, Reading, MA, 1991. ",NA,NA
"Gader, Paul, et al., “Fuzzy and Crisp Handwritten Character Recognition Using ",NA,NA
"Neural Networks,” Conference Proceedings of the 1992 Artificial Neural Networks ",NA,NA
"in Engineering Conference, V.3, pp. 421–424. ",NA,NA
"Ganesh, C., et al., “A Neural Network-Based Object Identification System,” ",NA,NA
Conference Proceedings of the 1992 Artificial Neural Networks in Engineering ,NA,NA
"Conference, V.3, pp. 471–474. ",NA,NA
"Glover, Fred, ORSA CSTS Newsletter Vol 15, No 2, Fall 1994. ",NA,NA
"Goldberg, David E., ",NA,NA
"Genetic Algorithms in Search, Optimization and Machine ",NA,NA
Learning,NA,NA
", Addison-Wesley, Reading, MA, 1989. ",NA,NA
"Grossberg, Stephen, et al., Introduction and Foundations, Lecture Notes, Neural ",NA,NA
"Network Courses and Conference, Boston University, May 1992. ",NA,NA
"Hammerstrom, Dan, “Neural Networks at Work,” ",NA,NA
IEEE Spectrum,NA,NA
", New York, June ",NA,NA
1993. ,NA,NA
"Hertz, John, Krogh, Anders, and Palmer, Richard, ",NA,NA
Introduction to the Theory of ,NA,NA
Neural Computation,NA,NA
", Addison-Wesley, Reading, MA, 1991. ",NA,NA
"Jagota, Arun, “Word Recognition with a Hopfield-Style Net,” Conference ",NA,NA
"Proceedings of the 1992 Artificial Neural Networks in Engineering Conference, ",NA,NA
"V.3, pp. 445–448. ",NA,NA
"Johnson, R. Colin, “Accuracy Moves OCR into the Mainstream,” ",NA,NA
Electronic ,NA,NA
Engineering Times,NA,NA
", CMP Publications, Manhasset, NY, January 16, 1995. ",NA,NA
"Johnson, R. Colin, “Making the Neural-Fuzzy Connection,” ",NA,NA
Electronic Engineering ,NA,NA
Times,NA,NA
", CMP Publications, Manhasset, NY, September 27, 1993. ",NA,NA
"Johnson, R. Colin, “Neural Immune System Nabs Viruses,” ",NA,NA
Electronic Engineering ,NA,NA
Times,NA,NA
", CMP Publications, Manhasset, NY, May 8, 1995. ",NA,NA
"Jurik, Mark, “The Care and Feeding of a Neural Network,” ",NA,NA
Futures Magazine,NA,NA
", Oster ",NA,NA
"Communications, Cedar Falls, IA, October 1992. ",NA,NA
"Kimoto, Takashi, et al., “Stock Market Prediction System with Modular Neural ",NA,NA
"Networks,” ",NA,NA
Neural Networks in Finance and Investing,NA,NA
", pp. 343–356, Probus ",NA,NA
"Publishing, Chicago, 1993. ",NA,NA
"Kline, J., and Folger, T.A., ",NA,NA
"Fuzzy Sets, Uncertainty and Information",NA,NA
", Prentice Hall, ",NA,NA
"New York, 1988. ",NA,NA
"Konstenius, Jeremy G., “Trading the S&P with a Neural Network,” Technical ",NA,NA
"Analysis of Stocks and Commodities, October 1994, Technical Analysis Inc., ",NA,NA
Seattle. ,NA,NA
"Kosaka, M., et al., “Applications of Fuzzy Logic/Neural Network to Securities ",NA,NA
"Trading Decision Support System,” Conference Proceedings of the 1991 IEEE ",NA,NA
"International Conference on Systems, Man and Cybernetics, V.3, pp. 1913–1918. ",NA,NA
"Kosko, Bart, and Isaka, Satoru, ",NA,NA
Fuzzy Logic,NA,NA
", Scientific American, New York, July ",NA,NA
1993. ,NA,NA
"Kosko, Bart, ",NA,NA
Neural Networks and Fuzzy Systems: A Dynamical Systems Approach ,NA,NA
to Machine Intelligence,NA,NA
", Prentice-Hall, New York, 1992. ",NA,NA
"Laing, Jonathan, “New Brains: How Smart Computers are Beating the Stock ",NA,NA
"Market,” ",NA,NA
Barron’s,NA,NA
", February 27, 1995. ",NA,NA
"Lederman, Jess, and Klein, Robert, eds., ",NA,NA
Virtual Trading,NA,NA
", Probus Publishing, ",NA,NA
"Chicago, 1995. ",NA,NA
"Lin, C.T. and Lee, C.S.G, “A Multi-Valued Boltzman Machine”, IEEE Transactions ",NA,NA
"on Systems, Man, and Cybernetics, Vol. 25, No. 4, April 1995 pp. 660-668. ",NA,NA
"MacGregor, Ronald J., ",NA,NA
Neural and Brain Modeling,NA,NA
", Academic Press, Englewood ",NA,NA
"Cliffs, NJ, 1987. ",NA,NA
"Mandelman, Avner, “The Computer’s Bullish! A Money Manager’s Love Affair ",NA,NA
"with Neural Network Programs,” ",NA,NA
Barron’s,NA,NA
", December 14, 1992. ",NA,NA
"Maren, Alianna, Harston, Craig, and Pap, Robert, ",NA,NA
Handbook of Neural Computing ,NA,NA
Applications,NA,NA
", Academic Press, Englewood Cliffs, NJ, 1990. ",NA,NA
"Marquez, Leorey, et al., “Neural Network Models as an Alternative to Regression,” ",NA,NA
Neural Networks in Finance and Investing,NA,NA
", pp. 435–449, Probus Publishing, ",NA,NA
"Chicago, 1993. ",NA,NA
"Mason, Anthony et al., “Diagnosing Faults in Circuit Boards—A Neural Net ",NA,NA
"Approach,” Conference Proceedings of the 1992 Artificial Neural Networks in ",NA,NA
"Engineering Conference, V.3, pp. 839–843. ",NA,NA
"McNeill, Daniel, and Freiberger, Paul, ",NA,NA
Fuzzy Logic,NA,NA
", Simon & Schuster, New York, ",NA,NA
1993. ,NA,NA
"McNeill, F. Martin, and Thro, Ellen, ",NA,NA
Fuzzy Logic: A Practical Approach,NA,NA
", Academic ",NA,NA
"Press, Boston, 1994. ",NA,NA
"Murphy, John J., ",NA,NA
Intermarket Technical Analysis,NA,NA
", John Wiley & Sons, New York, ",NA,NA
1991. ,NA,NA
"Murphy, John J., ",NA,NA
Technical Analysis of the Futures Markets,NA,NA
", NYIF Corp., New ",NA,NA
"York, 1986. ",NA,NA
"Nellis, J., and Stonham, T.J., “A Neural Network Character Recognition System that ",NA,NA
"Dynamically Adapts to an Author’s Writing Style,” Conference Proceedings of the ",NA,NA
"1992 Artificial Neural Networks in Engineering Conference, V.3, pp. 975–979. ",NA,NA
"Peters, Edgar E., ",NA,NA
Fractal Market Analysis: Applying Chaos Theory to Investment ,NA,NA
and Economics,NA,NA
", John Wiley & Sons, New York, 1994. ",NA,NA
"Ressler, James L., and Augusteijn, Marijke F., “Weapon Target Assignment ",NA,NA
"Accessibility Using Neural Networks,” Conference Proceedings of the 1992 ",NA,NA
"Artificial Neural Networks in Engineering Conference, V.3, pp. 397–399. ",NA,NA
"Rao, Satyanaryana S,, and Sethuraman, Sriram, “A Modified Recurrent Learning ",NA,NA
"Algorithm for Nonlinear Time Series Prediction,” Conference Proceedings of the ",NA,NA
"1992 Artificial Neural Networks in Engineering Conference, V.3, pp. 725–730. ",NA,NA
"Rao, Valluru, and Rao, Hayagriva, ",NA,NA
Power Programming Turbo C++,NA,NA
", MIS:Press, ",NA,NA
"New York, 1992. ",NA,NA
"Ruggiero, Murray, “How to Build an Artificial Trader,” ",NA,NA
Futures Magazine,NA,NA
", Oster ",NA,NA
"Communications, Cedar Falls, IA, September 1994. ",NA,NA
"Ruggiero, Murray, “How to Build a System Framework,” ",NA,NA
Futures Magazine,NA,NA
", Oster ",NA,NA
"Communications, Cedar Falls, IA, November 1994. ",NA,NA
"Ruggiero, Murray, “Nothing Like Net for Intermarket Analysis,” ",NA,NA
Futures Magazine,NA,NA
", ",NA,NA
"Oster Communications, Cedar Falls, IA, May 1995. ",NA,NA
"Ruggiero, Murray, “Putting the Components before the System,” ",NA,NA
Futures Magazine,NA,NA
", ",NA,NA
"Oster Communications, Cedar Falls, IA, October 1994. ",NA,NA
"Ruggiero, Murray, “Training Neural Networks for Intermarket Analysis,” ",NA,NA
Futures ,NA,NA
Magazine,NA,NA
", Oster Communications, Cedar Falls, IA, August 1994. ",NA,NA
"Rumbaugh, James, et al, ",NA,NA
"Object-Oriented Modeling and Design,",NA,NA
" Prentice-Hall Inc, ",NA,NA
"Englewood Cliffs, New Jersey, 1991. ",NA,NA
"Sharda, Ramesh, and Patil, Rajendra, “A Connectionist Approach to Time Series ",NA,NA
"Prediction: An Empirical Test,” ",NA,NA
Neural Networks in Finance and Investing,NA,NA
", pp. ",NA,NA
"451–463, Probus Publishing, Chicago, 1993. ",NA,NA
"Sherry, Clifford, “Are Your Inputs Correlated?” Technical Analysis of Stocks and ",NA,NA
"Commodities, February 1995, Technical Analysis Inc., Seattle. ",NA,NA
"Simpson, Patrick K., ",NA,NA
"Artificial Neural Systems: Foundations, Paradigms, ",NA,NA
Applications and Implementations,NA,NA
", Pergamon Press, London, 1990. ",NA,NA
"Soucek, Branko, and Soucek, Marina, ",NA,NA
Neural and Massively Parallel Computers: ,NA,NA
The Sixth Generation,NA,NA
", John Wiley & Sons, New York, 1988. ",NA,NA
"Stein, Jon, “The Trader’s Guide to Technical Indicators,” ",NA,NA
Futures Magazine,NA,NA
", Oster ",NA,NA
"Communications, Cedar Falls, IA, August 1990. ",NA,NA
"Terano, Toshiro, et al., ",NA,NA
Fuzzy Systems Theory and Its Applications,NA,NA
", Academic Press, ",NA,NA
"Boston, 1993. ",NA,NA
"Trippi, Robert, and Turban, Efraim, eds., ",NA,NA
Neural Networks in Finance and ,NA,NA
Investing,NA,NA
", Probus Publishing, Chicago, 1993. ",NA,NA
"Wasserman, Gary S., and Sudjianto, Agus, Conference Proceedings of the 1992 ",NA,NA
"Artificial Neural Networks in Engineering Conference, V.3, pp. 901–904. ",NA,NA
"Wasserman, Philip D., ",NA,NA
Advanced Methods in Neural Computing,NA,NA
", Van Nostrand ",NA,NA
"Reinhold, New York, 1993. ",NA,NA
"Wasserman, Philip D., ",NA,NA
Neural Computing,NA,NA
", Van Nostrand Reinhold, New York, ",NA,NA
1989. ,NA,NA
"Wu, Fred, et al., “Neural Networks for EEG Diagnosis,” Conference Proceedings of ",NA,NA
"the 1992 Artificial Neural Networks in Engineering Conference, V.3, pp. 559–565. ",NA,NA
"Yager, R., ed., ",NA,NA
Fuzzy Sets and Applications: Selected Papers by L.Z. Zadeh,NA,NA
", Wiley-",NA,NA
"Interscience, New York, 1987. ",NA,NA
"Yan, Jun, Ryan, Michael, and Power, James, ",NA,NA
Using Fuzzy Logic,NA,NA
", Prentice-Hall, New ",NA,NA
"York, 1994. ",NA,NA
"Yoon, Y., and Swales, G., “Predicting Stock Price Performance: A Neural Network ",NA,NA
"Approach,” ",NA,NA
Neural Networks in Finance and Investing,NA,NA
", pp. 329–339, Probus ",NA,NA
"Publishing, Chicago, 1993. ",NA,NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Table of Contents,NA,NA
Appendix A ,NA,NA
Compiling Your Programs ,NA,NA
"All of the programs included in the book have been compiled and tested on Turbo C++, ",NA,NA
"Borland C++, and Microsoft C++/Visual C++ with either the small or medium memory ",NA,NA
"model. You should not have any problems in using other compilers, since standard I/O ",NA,NA
routines are used. Your target should be a DOS executable. With the backpropagation ,NA,NA
"simulator of Chapters 7, 13, and 14 you may run into a memory shortage situation. You ",NA,NA
should unload any TSR (Terminate and Stay Resident) programs and/or choose smaller ,NA,NA
architectures for your networks. By going to more hidden layers with fewer neurons per ,NA,NA
"layer, you may be able to reduce the overall memory requirements. ",NA,NA
"The programs in this book make heavy use of floating point calculations, and you should ",NA,NA
"compile your programs to take advantage of a math coprocessor, if you have one installed ",NA,NA
in your computer.,NA,NA
The organization of files on the accompanying diskette are according to chapter number. ,NA,NA
You will find relevant versions of files in the corresponding chapter directory.,NA,NA
"Most of the files are self-contained, or include other needed files in them. You will not ",NA,NA
require a makefile to build the programs. Load the main file for example for ,NA,NA
"backpropagation, the backprop.cpp file, into the development environment editor for your ",NA,NA
compiler and build a .exe file. That’s it!,NA,NA
Table of Contents,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...++_Neural_Networks_and_Fuzzy_Logic/appendix-a.html (1 of 2) [21/11/02 21:59:02],NA
Table of Contents,NA,NA
Appendix B ,NA,NA
Mathematical Background ,NA,NA
Dot Product or Scalar Product of Two Vectors,NA,NA
Given vectors ,NA,NA
U,NA,NA
 and ,NA,NA
V,NA,NA
", where ",NA,NA
U,NA,NA
 = (,NA,NA
u,1,NA
", …, ",NA,NA
u,n,NA
) and ,NA,NA
V,NA,NA
 = (,NA,NA
v,1,NA
", …, ",NA,NA
v,n,NA
"), their dot product or scalar ",NA,NA
product is ,NA,NA
U,NA,NA
 • ,NA,NA
V,NA,NA
 = ,NA,NA
u,1,NA
v,1,NA
 +… + ,NA,NA
u,n,NA
v,n,NA
 = ,NA,NA
©,NA,NA
u,i,NA
 v,i,NA
.,NA,NA
Matrices and Some Arithmetic Operations on Matrices,NA,NA
A real matrix is a rectangular array of real numbers. A matrix with ,NA,NA
m,NA,NA
 rows and ,NA,NA
n,NA,NA
 columns is ,NA,NA
referred to as an ,NA,NA
m,NA,NA
x,NA,NA
n,NA,NA
 matrix. The element in the ,NA,NA
i,NA,NA
th row and ,NA,NA
j,NA,NA
th column of the matrix is referred ,NA,NA
to as the ,NA,NA
ij,NA,NA
 element of the matrix and is denoted by ,NA,NA
a,ij,NA
.,NA,NA
The transpose of a matrix ,NA,NA
M,NA,NA
 is denoted by ,NA,NA
M,T,NA
. The element in the ,NA,NA
i,NA,NA
th row and ,NA,NA
j,NA,NA
th column of ,NA,NA
M,T,NA
is the same as the element of ,NA,NA
M,NA,NA
 in its ,NA,NA
j,NA,NA
th row and ,NA,NA
i,NA,NA
th column. ,NA,NA
M,T,NA
 is obtained from ,NA,NA
M,NA,NA
 by ,NA,NA
interchanging the rows and columns of ,NA,NA
M,NA,NA
". For example, if",NA,NA
 2  7 -3              2  4,NA,NA
M,NA,NA
" =          , then ",NA,NA
M,T,NA
 =  7  0,NA,NA
 4  0  9             -3  9,NA,NA
If ,NA,NA
X,NA,NA
 is a vector with ,NA,NA
m,NA,NA
" components, ",NA,NA
x,1,NA
", …, ",NA,NA
x,m,NA
", then it can be written as a column vector with ",NA,NA
"components listed one below another. It can be written as a row vector, ",NA,NA
X =,NA,NA
 (,NA,NA
x,1,NA
", …, ",NA,NA
x,m,NA
). The ,NA,NA
"transpose of a row vector is the column vector with the same components, and the transpose of a ",NA,NA
column vector is the corresponding row vector.,NA,NA
"The addition of matrices is possible if they have the same size, that is, the same number of rows ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...++_Neural_Networks_and_Fuzzy_Logic/appendix-b.html (1 of 4) [21/11/02 21:59:03],NA
and same number of columns. Then you just add the ,NA,NA
ij,NA,NA
 elements of the two matrices to get the ,NA,NA
ij ,NA,NA
"elements of the sum matrix. For example,",NA,NA
 3 -4 5    5 2 -3      8 -2  2,NA,NA
 +         =,NA,NA
 2  3 7    6 0  4      8  3 11,NA,NA
"Multiplication is defined for a given pair of matrices, only if a condition on their respective sizes ",NA,NA
"is satisfied. Then too, it is not a commutative operation. This means that if you exchange the ",NA,NA
"matrix on the left with the matrix on the right, the multiplication operation may not be defined, ",NA,NA
"and even if it is, the result may not be the same as before such an exchange. ",NA,NA
The condition to be satisfied for multiplying the matrices ,NA,NA
A,NA,NA
", ",NA,NA
B,NA,NA
 as ,NA,NA
AB,NA,NA
" is, that the number of ",NA,NA
columns in ,NA,NA
A,NA,NA
 is equal to the number of rows in ,NA,NA
B,NA,NA
. Then to get the ,NA,NA
ij,NA,NA
 element of the product matrix ,NA,NA
AB,NA,NA
", you take the ",NA,NA
i,NA,NA
th row of ,NA,NA
A,NA,NA
 as one vector and the ,NA,NA
j,NA,NA
th column of ,NA,NA
B,NA,NA
 as a second vector and do a ,NA,NA
"dot product of the two. For example, the two matrices given previously to illustrate the addition ",NA,NA
of two matrices are not compatible for multiplication in whichever order you take them. It is ,NA,NA
"because there are three columns in each, which is different from the number of rows, which is 2 ",NA,NA
in each. Another example is given as follows.,NA,NA
 3 -4 5              5  6,NA,NA
 Let A =            and   B =  2  0,NA,NA
 2  3 7             -3  4,NA,NA
Then ,NA,NA
AB,NA,NA
 and ,NA,NA
BA,NA,NA
" are both defined, ",NA,NA
AB,NA,NA
" is a 2x2 matrix, whereas ",NA,NA
BA,NA,NA
 is 3x3.,NA,NA
 -8  38              27 -2 67,NA,NA
 Also AB =         and BA  =    6 -8 10,NA,NA
 -5  40              -1 24 13,NA,NA
Lyapunov Function,NA,NA
A ,NA,NA
Lyapunov,NA,NA
" function is a function that decreases with time, taking on non-negative values. It is ",NA,NA
used to correspond between the state variables of a system and real numbers. The state of the ,NA,NA
"system changes as time changes, and the function decreases. Thus, the ",NA,NA
Lyapunov,NA,NA
 function ,NA,NA
decreases with each change of state of the system.,NA,NA
We can construct a simple example of a function with the property of decreasing with each change ,NA,NA
"of state as follows. Suppose a real number, ",NA,NA
x,NA,NA
", represents the state of a dynamic system at time ",NA,NA
t,NA,NA
. ,NA,NA
Also suppose that ,NA,NA
x,NA,NA
 is bounded for any ,NA,NA
t,NA,NA
 by a positive real number ,NA,NA
M,NA,NA
. That means ,NA,NA
x,NA,NA
 is less than ,NA,NA
M,NA,NA
 ,NA,NA
for every value of ,NA,NA
t,NA,NA
.,NA,NA
"Then the function,",NA,NA
" f(x,t) = exp(-|x|/(M+|x|+t))",NA,NA
is non-negative and decreases with increasing ,NA,NA
t,NA,NA
.,NA,NA
Local Minimum,NA,NA
A function ,NA,NA
f,NA,NA
(,NA,NA
x,NA,NA
) is defined to have a local minimum at ,NA,NA
y,NA,NA
", with a value ",NA,NA
z,NA,NA
", if",NA,NA
" f(y) = z, and f(x) ",NA,NA
,NA,NA
" z, for each x, such that there exists",NA,NA
 a positive real number h such that y – h ,NA,NA
,NA,NA
 x ,NA,NA
,NA,NA
 y + h.,NA,NA
"In other words, there is no other value of ",NA,NA
x,NA,NA
 in a neighborhood of ,NA,NA
y,NA,NA
", where the value of the ",NA,NA
function is smaller than ,NA,NA
z,NA,NA
.,NA,NA
There can be more than one local minimum for a function in its domain. A ,NA,NA
Step,NA,NA
 function (with a ,NA,NA
graph resembling a staircase) is a simple example of a function with an infinite number of points ,NA,NA
in its domain with local minima.,NA,NA
Global Minimum,NA,NA
A function ,NA,NA
f,NA,NA
(,NA,NA
x,NA,NA
) is defined to have a global minimum at ,NA,NA
y,NA,NA
", with a value ",NA,NA
z,NA,NA
", if",NA,NA
" f(y) = z, and f(x) ",NA,NA
,NA,NA
" z, for each x in the domain of",NA,NA
 the function f.,NA,NA
"In other words, there is no other value of ",NA,NA
x,NA,NA
 in the domain of the function ,NA,NA
f,NA,NA
", where the value of the ",NA,NA
function is smaller than ,NA,NA
z,NA,NA
". Clearly, a global minimum is also a local minimum, but a local ",NA,NA
minimum may not be a global minimum.,NA,NA
There can be more than one global minimum for a function in its domain. The trigonometric ,NA,NA
function ,NA,NA
f,NA,NA
(,NA,NA
x,NA,NA
) = sin,NA,NA
x,NA,NA
 is a simple example of a function with an infinite number of points with ,NA,NA
global minima. You may recall that sin(3,NA,NA
,NA,NA
"/ 2), sin (7",NA,NA
,NA,NA
"/ 2), and so on are all –1, the smallest ",NA,NA
value for the sine function.,NA,NA
Kronecker Delta Function,NA,NA
The Kronecker delta function is a function of two variables. It has a value of 1 if the two ,NA,NA
"arguments are equal, and 0 if they are not. Formally, ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...++_Neural_Networks_and_Fuzzy_Logic/appendix-b.html (3 of 4) [21/11/02 21:59:03],NA
,NA,NA
"(x,y)=",NA,NA
 1 if x = y,NA,NA
 0 if x ,NA,NA
,NA,NA
 y,NA,NA
Gaussian Density Distribution,NA,NA
"The Gaussian Density distribution, also called the Normal distribution, has a density function of ",NA,NA
"the following form. There is a constant parameter c, which can have any positive value. ",NA,NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathem...++_Neural_Networks_and_Fuzzy_Logic/appendix-b.html (4 of 4) [21/11/02 21:59:03]",NA
Table of Contents,NA,NA
Glossary,NA,NA
A,NA,NA
Activation ,NA,NA
The weighted sum of the inputs to a neuron in a neural network. ,NA,NA
Adaline ,NA,NA
Adaptive linear element machine. ,NA,NA
Adaptive Resonance Theory ,NA,NA
"Theory developed by Grossberg and Carpenter for categorization of patterns, and to ",NA,NA
address the stability–plasticity dilemma. ,NA,NA
Algorithm ,NA,NA
A step-by-step procedure to solve a problem. ,NA,NA
Annealing ,NA,NA
A process for preventing a network from being drawn into a local minimum. ,NA,NA
ART ,NA,NA
(Adaptive Resonance Theory) ART1 is the result of the initial development of this ,NA,NA
theory for binary inputs. Further developments led to ART2 for analog inputs. ,NA,NA
ART3 is the latest. ,NA,NA
Artificial neuron ,NA,NA
The primary object in an artificial neural network to mimic the neuron activity of ,NA,NA
the brain. The artificial neuron is a processing element of a neural network. ,NA,NA
Associative memory ,NA,NA
Activity of associating one pattern or object with itself or another. ,NA,NA
Autoassociative ,NA,NA
Making a correspondence of one pattern or object with itself. ,NA,NA
B,NA,NA
Backpropagation ,NA,NA
A neural network training algorithm for feedforward networks where the errors at ,NA,NA
the output layer are propagated back to the layer before in learning. If the previous ,NA,NA
"layer is not the input layer, then the errors at this hidden layer are propagated back ",NA,NA
to the layer before. ,NA,NA
BAM ,NA,NA
Bidirectional Associative Memory network model. ,NA,NA
Bias ,NA,NA
A value added to the activation of a neuron. ,NA,NA
Binary digit ,NA,NA
A value of 0 or 1. ,NA,NA
Bipolar value ,NA,NA
A value of –1 or +1. ,NA,NA
Boltzmann machine ,NA,NA
A neural network in which the outputs are determined with probability ,NA,NA
distributions. Trained and operated using simulated annealing. ,NA,NA
Brain-State-in-a-Box ,NA,NA
"Anderson’s single-layer, laterally connected neural network model. It can work ",NA,NA
with inputs that have noise in them or are incomplete. ,NA,NA
C,NA,NA
Cauchy machine ,NA,NA
"Similar to the Boltzmann machine, except that a Cauchy distribution is used for ",NA,NA
probabilities. ,NA,NA
Cognitron ,NA,NA
The forerunner to the Neocognitron. A network developed to recognize characters. ,NA,NA
Competition ,NA,NA
A process in which a winner is selected from a layer of neurons by some criterion. ,NA,NA
Competition suggests inhibition reflected in some connection weights being ,NA,NA
assigned a negative value. ,NA,NA
Connection ,NA,NA
A means of passing inputs from one neuron to another. ,NA,NA
Connection weight ,NA,NA
A numerical label associated with a connection and used in a weighted sum of ,NA,NA
inputs. ,NA,NA
Constraint ,NA,NA
"A condition expressed as an equation or inequality, which has to be satisfied by the ",NA,NA
variables. ,NA,NA
Convergence,NA,NA
Crisp,NA,NA
Termination of a process with a final result. ,NA,NA
The opposite of fuzzy—usually a specific numerical quantity or value for an entity. ,NA,NA
D,NA,NA
Delta rule ,NA,NA
"A rule for modification of connection weights, using both the output and the error ",NA,NA
obtained. It is also called the ,NA,NA
LMS rule,NA,NA
. ,NA,NA
E,NA,NA
Energy function ,NA,NA
A function of outputs and weights in a neural network to determine the state of the ,NA,NA
"system, e.g., Lyapunov function. ",NA,NA
Excitation ,NA,NA
Providing positive weights on connections to enable outputs that cause a neuron to ,NA,NA
fire. ,NA,NA
Exemplar ,NA,NA
An example of a pattern or object used in training a neural network. ,NA,NA
Expert system ,NA,NA
A set of formalized rules that enable a system to perform like an expert. ,NA,NA
F,NA,NA
FAM ,NA,NA
Fuzzy Associative Memory network. Makes associations between fuzzy sets. ,NA,NA
Feedback ,NA,NA
The process of relaying information in the opposite direction to the original. ,NA,NA
Fit vector ,NA,NA
A vector of values of degree of membership of elements of a fuzzy set. ,NA,NA
Fully connected network ,NA,NA
A neural network in which every neuron has connections to all other neurons. ,NA,NA
Fuzzy ,NA,NA
"As related to a variable, the opposite of crisp. A fuzzy quantity represents a range of ",NA,NA
"value as opposed to a single numeric value, e.g., “hot” vs. 89.4°. ",NA,NA
Fuzziness ,NA,NA
"Different concepts having an overlap to some extent. For example, descriptions of ",NA,NA
fair and cool temperatures may have an overlap of a small interval of temperatures. ,NA,NA
Fuzzy Associative Memory ,NA,NA
A neural network model to make association between fuzzy sets. ,NA,NA
Fuzzy equivalence relation ,NA,NA
"A fuzzy relation (relationship between fuzzy variables) that is reflexive, symmetric, ",NA,NA
and transitive. ,NA,NA
Fuzzy partial order ,NA,NA
"A fuzzy relation (relationship between fuzzy variables) that is reflexive, ",NA,NA
"antisymmetric, and transitive. ",NA,NA
G,NA,NA
Gain ,NA,NA
Sometimes a numerical factor to enhance the activation. Sometimes a connection ,NA,NA
for the same purpose. ,NA,NA
Generalized Delta rule ,NA,NA
A rule used in training of networks such as backpropagation training where hidden ,NA,NA
layer weights are modified with backpropagated error. ,NA,NA
Global minimum ,NA,NA
A point where the value of a function is no greater than the value at any other point ,NA,NA
in the domain of the function. ,NA,NA
H,NA,NA
Hamming distance ,NA,NA
The number of places in which two binary vectors differ from each other. ,NA,NA
Hebbian learning ,NA,NA
A learning algorithm in which Hebb’s rule is used. The change in connection ,NA,NA
weight between two neurons is taken as a constant times the product of their ,NA,NA
outputs. ,NA,NA
Heteroassociative ,NA,NA
Making an association between two distinct patterns or objects. ,NA,NA
Hidden layer ,NA,NA
An array of neurons positioned in between the input and output layers. ,NA,NA
Hopfield network ,NA,NA
"A single layer, fully connected, autoassociative neural network. ",NA,NA
I,NA,NA
Inhibition,NA,NA
The attempt by one neuron to diminish the chances of firing by another neuron. ,NA,NA
Input layer ,NA,NA
An array of neurons to which an external input or signal is presented. ,NA,NA
Instar ,NA,NA
A neuron that has no connections going from it to other neurons. ,NA,NA
L,NA,NA
Lateral connection ,NA,NA
A connection between two neurons that belong to the same layer. ,NA,NA
Layer ,NA,NA
An array of neurons positioned similarly in a network for its operation. ,NA,NA
Learning ,NA,NA
The process of finding an appropriate set of connection weights to achieve the goal ,NA,NA
of the network operation. ,NA,NA
Linearly separable ,NA,NA
Two subsets of a linear set having a linear barrier (hyperplane) between the two of ,NA,NA
them. ,NA,NA
LMS rule ,NA,NA
"Least mean squared error rule, with the aim of minimizing the average of the ",NA,NA
squared error. Same as the Delta rule. ,NA,NA
Local minimum ,NA,NA
A point where the value of the function is no greater than the value at any other ,NA,NA
point in its neighborhood. ,NA,NA
Long-term memory (LTM) ,NA,NA
Encoded information that is retained for an extended period. ,NA,NA
Lyapunov function ,NA,NA
A function that is bounded below and represents the state of a system that decreases ,NA,NA
with every change in the state of the system. ,NA,NA
M,NA,NA
Madaline ,NA,NA
A neural network in which the input layer has units that are Adalines. It is a ,NA,NA
multiple-Adaline. ,NA,NA
Mapping ,NA,NA
A correspondence between elements of two sets. ,NA,NA
N,NA,NA
Neural network ,NA,NA
"A collection of processing elements arranged in layers, and a collection of ",NA,NA
"connection edges between pairs of neurons. Input is received at one layer, and ",NA,NA
output is produced at the same or at a different layer. ,NA,NA
Noise ,NA,NA
Distortion of an input. ,NA,NA
Nonlinear optimization ,NA,NA
Finding the best solution for a problem that has a nonlinear function in its objective ,NA,NA
or in a constraint. ,NA,NA
O,NA,NA
On center off surround ,NA,NA
Assignment of excitatory weights to connections to nearby neurons and inhibitory ,NA,NA
weights to connections to distant neurons. ,NA,NA
Orthogonal vectors ,NA,NA
Vectors whose dot product is 0. ,NA,NA
Outstar ,NA,NA
A neuron that has no incoming connections. ,NA,NA
P,NA,NA
Perceptron ,NA,NA
A neural network for linear pattern matching. ,NA,NA
Plasticity ,NA,NA
Ability to be stimulated by new inputs and learn new mappings or modify existing ,NA,NA
ones. ,NA,NA
R,NA,NA
Resonance ,NA,NA
The responsiveness of two neurons in different layers in categorizing an input. An ,NA,NA
equilibrium in two directions. ,NA,NA
S,NA,NA
Saturation ,NA,NA
A condition of limitation on the frequency with which a neuron can fire. ,NA,NA
Self-organization ,NA,NA
A process of partitioning the output layer neurons to correspond to individual ,NA,NA
"patterns or categories, also called unsupervised learning or clustering. ",NA,NA
Short-term memory (STM) ,NA,NA
The storage of information that does not endure long after removal of the ,NA,NA
corresponding input. ,NA,NA
Simulated annealing ,NA,NA
An algorithm by which changes are made to decrease energy or temperature or cost. ,NA,NA
Stability ,NA,NA
Convergence of a network operation to a steady-state solution. ,NA,NA
Supervised learning ,NA,NA
A learning process in which the exemplar set consists of pairs of inputs and desired ,NA,NA
outputs. ,NA,NA
T,NA,NA
Threshold value ,NA,NA
A value used to compare the activation of a neuron to determine if the neuron fires ,NA,NA
or not. Sometimes a bias value is added to the activation of a neuron to allow the ,NA,NA
threshold value to be zero in determining the neuron’s output. ,NA,NA
Training ,NA,NA
The process of helping in a neural network to learn either by providing input/output ,NA,NA
"stimuli (supervised training) or by providing input stimuli (unsupervised training), ",NA,NA
and allowing weight change updates to occur. ,NA,NA
U,NA,NA
Unsupervised learning ,NA,NA
"Learning in the absence of external information on outputs, also called self-",NA,NA
organization or clustering. ,NA,NA
V,NA,NA
Vigilance parameter ,NA,NA
A parameter used in Adaptive Resonance Theory. It is used to selectively prevent ,NA,NA
the activation of a subsystem in the network. ,NA,NA
W,NA,NA
Weight,NA,NA
"A number associated with a neuron or with a connection between two neurons, ",NA,NA
which is used in aggregating the outputs to determine the activation of a neuron. ,NA,NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.",NA
Table of Contents,NA,NA
Index,NA,NA
A,NA,NA
ABAM ,NA,NA
see,NA,NA
 Adaptive Bi-directional Associative Memory ,NA,NA
"abstract class, 138 ",NA,NA
"abstract data type, 22 ",NA,NA
"accumulation-distribution, 402, 404, ",NA,NA
"activation, 3, 18, 89 ",NA,NA
"zero, 182 ",NA,NA
"Adaline, 81, 82, 102, 103, 112 ",NA,NA
"adaptation, 77, 120 ",NA,NA
"Adaptive Bi-directional Associative Memory, 181, 212 ",NA,NA
"Competitive, 212 ",NA,NA
Differential ,NA,NA
"Competitive, 212 ",NA,NA
"Hebbian, 212 ",NA,NA
"adaptive filters, 120 ",NA,NA
"adaptive linear element, 102 ",NA,NA
"adaptive models, 120 ",NA,NA
"adaptive parameters, 373 ",NA,NA
"adaptive resonance, 118 ",NA,NA
"Adaptive Resonance Theory I, 104, 107, 108, 115, 117, 243 ",NA,NA
"algorithm for calculations, 246 ",NA,NA
"equations for, 246 ",NA,NA
"F1 layer calculations, 247 ",NA,NA
"F2 layer calculations, 247 ",NA,NA
"modifying connection weights, 248 ",NA,NA
"Top-down inputs, 247 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (1 of 33) [21/11/02 21:59:06],NA
"two-thirds rule, 244, 245 ",NA,NA
"Adaptive Resonance Theory II, 248 ",NA,NA
"Adaptive Resonance Theory III, 248 ",NA,NA
"additive composition method, 506 ",NA,NA
"advancing issues, 387 ",NA,NA
"aggregation, 82, 87, 98 ",NA,NA
"Ahmadian, 513 ",NA,NA
"Aiken, 405 ",NA,NA
"algorithm, 1 ",NA,NA
"backpropagation, 7, 103, 271, 373, 375 ",NA,NA
"constructive, 121 ",NA,NA
"data clustering, 245 ",NA,NA
"encoding, 94, 96 ",NA,NA
"learning algorithm , 61, 79, 102, 118 ",NA,NA
"adaptive steepest descent, 410 ",NA,NA
"alpha, 273, 274, 330, 372, 373, 384 ",NA,NA
"alphabet classifier system, 320 ",NA,NA
"Amari, 104 ",NA,NA
"analog, 73, 74, 322 ",NA,NA
"signal, 8 ",NA,NA
"AND operation , 64 ",NA,NA
"Anderson, 105 ",NA,NA
annealing ,NA,NA
"process, 430 ",NA,NA
schedule 113 ,NA,NA
"simulated annealing, 113, 114 ",NA,NA
"Anzai, 456, 472 ",NA,NA
"application, 102, 374, 511 ",NA,NA
"nature of , 74 ",NA,NA
"approximation, 109 ",NA,NA
"architecture, 7, 81, 120, 373 ",NA,NA
"Artificial Intelligence , 6, 34 ",NA,NA
"artificial neural network , 1 ",NA,NA
ART1 ,NA,NA
see,NA,NA
 Adaptive Resonance Theory I ,NA,NA
ART2 ,NA,NA
see,NA,NA
 Adaptive Resonance Theory II ,NA,NA
ART3 ,NA,NA
see,NA,NA
 Adaptive Resonance Theory III ,NA,NA
"artneuron class, 249 ",NA,NA
"ASCII, 305, 306, 307, 329 ",NA,NA
"graphic characters, 306 ",NA,NA
assocpair class ,NA,NA
"in BAM network, 186 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (2 of 33) [21/11/02 21:59:06],NA
"association, 218 ",NA,NA
"asynchronously , 1, 14 ",NA,NA
"asynchronous update, 13, 62 ",NA,NA
"attentional subsystem, 107, 243 ",NA,NA
"Augusteijn, 512 ",NA,NA
"autoassociation , 7, 8, 82,, 92, 102, 180 ",NA,NA
"autoassociative network, 7, 64, 97, 375 ",NA,NA
"average winner distance, 296 ",NA,NA
"Azoff, 410 ",NA,NA
B,NA,NA
"backpropagated errors, 144 ",NA,NA
"Backpropagation, 10, 103, 104, 120, 123, 302, 325, 329, 374 ",NA,NA
"algorithm, 7, 103, 271, 373, 375 ",NA,NA
"beta, 330 ",NA,NA
"calculating error, 396 ",NA,NA
"calculations, 128, 130 ",NA,NA
"changing beta while training, 337 ",NA,NA
"choosing middle layer size, 372 ",NA,NA
"convergence, 372 ",NA,NA
"momentum term, 330 ",NA,NA
"noise factor, 336 ",NA,NA
"self-supervised, 375 ",NA,NA
"simulator, 134, 173, 337, 375, 377, 396 ",NA,NA
"training and testing, 396 ",NA,NA
"training law, 333 ",NA,NA
"variations of , 373 ",NA,NA
"Baer, 516 ",NA,NA
BAM ,NA,NA
see,NA,NA
 Bi-directional Associative Memory ,NA,NA
bar ,NA,NA
"chart, 403, 404 ",NA,NA
"features, 513 ",NA,NA
"Barron’s, 377, 378, 388 ",NA,NA
"base class , 25, 28, 138 ",NA,NA
"beta, 136, 337, 372, 373, 384 ",NA,NA
"bias, 16, 77, 125, 128, 378 ",NA,NA
"Bi-directional Associative Memory, 81, 92, 104, 115, 117, 179, 185, 215 ",NA,NA
"connection weight matrix, 212 ",NA,NA
"continuous, 211 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (3 of 33) [21/11/02 21:59:06],NA
"inputs, 180 ",NA,NA
"network, 104 ",NA,NA
"outputs, 180 ",NA,NA
"training, 181 ",NA,NA
"Unipolar Binary, 212 ",NA,NA
"bin, 325 ",NA,NA
"binary , 8, 15, 51, 65, 73, 74, 104 ",NA,NA
"input pattern, 51, 98 ",NA,NA
"patterns 11, 97 ",NA,NA
"string, 16, 62 ",NA,NA
"binary to bipolar mapping, 62, 63 ",NA,NA
binding ,NA,NA
"dynamic binding , 24, 139 ",NA,NA
"late binding , 24 ",NA,NA
"static binding, 139 ",NA,NA
"bipolar, 15, 17, 104 ",NA,NA
"mapping, 97 ",NA,NA
"string, 16, 62, 180 ",NA,NA
"bit pattern, 13 ",NA,NA
"Blending problem, 418 ",NA,NA
"block averages, 393 ",NA,NA
"bmneuron class, 186 ",NA,NA
"Boltzmann distribution, 113 ",NA,NA
"Boltzmann machine, 92, 112, 113, 118, 419, 512 ",NA,NA
"Booch, 21 ",NA,NA
"boolean logic, 50 ",NA,NA
bottom-up ,NA,NA
"connection weight, 248 ",NA,NA
"connections, 107, 244 ",NA,NA
"Box-Jenkins methodology, 406 ",NA,NA
"Brain-State-in-a-Box, 81, 82, 105 ",NA,NA
"breadth, 387, 389 ",NA,NA
"Buckles, 484 ",NA,NA
"buy/sell signals, 409, 410 ",NA,NA
C,NA,NA
"cache, 137 ",NA,NA
"Cader, 406 ",NA,NA
CAM ,NA,NA
see,NA,NA
 Content-Addressable-Memory ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (4 of 33) [21/11/02 21:59:06],NA
"Carpenter, 92, 107, 117, 243, 269, 517 ",NA,NA
"car navigation, 374 ",NA,NA
"cartesian product, 479 ",NA,NA
"cascade-correlation, 512 ",NA,NA
"Cash Register game, 3, 65 ",NA,NA
"categorization of inputs, 261 ",NA,NA
"category, 37 ",NA,NA
"Cauchy distribution, 113 ",NA,NA
"Cauchy machine, 112, 113, 419 ",NA,NA
cells ,NA,NA
"complex cells, 106 ",NA,NA
"simple cells, 106 ",NA,NA
"center of area method, 504 ",NA,NA
"centroid, 507, 508 ",NA,NA
"character recognition, 305 ",NA,NA
characters ,NA,NA
"alphabetic, 305 ",NA,NA
"ASCII, 306, 307 ",NA,NA
"garbled, 322 ",NA,NA
"graphic, 306, 307 ",NA,NA
"handwritten, 320 ",NA,NA
"Chawla, 514 ",NA,NA
"Chiang, 513 ",NA,NA
"cin, 25, 58, 71 ",NA,NA
"clamping probabilities, 114 ",NA,NA
"Clinton, Hillary, 405 ",NA,NA
"C language, 21 ",NA,NA
"class, 22 ",NA,NA
"abstract class, 138 ",NA,NA
"base class, 25, 28, 138, 139 ",NA,NA
"derived class, 23, 25, 26, 144 ",NA,NA
"friend class, 23 ",NA,NA
"hierarchy, 27, 138, 139 ",NA,NA
"input_layer class, 138 ",NA,NA
"iostream class, 71 ",NA,NA
"network class, 53, 66 ",NA,NA
"output_layer class, 138 ",NA,NA
"parent class, 26 ",NA,NA
"classification, 322 ",NA,NA
"C layer, 106 ",NA,NA
"codebook vectors, 116 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (5 of 33) [21/11/02 21:59:06],NA
"Cohen, 212 ",NA,NA
"Collard, 405 ",NA,NA
"column vector, 97 ",NA,NA
"combinatorial problem, 422 ",NA,NA
"comparison layer, 244 ",NA,NA
"competition, 9, 94, 97 ",NA,NA
"competitive learning, 243 ",NA,NA
compilers ,NA,NA
"C++ compilers, 27 ",NA,NA
"compilation error messages, 27 ",NA,NA
"complement, 33, 185, 201, 202 ",NA,NA
"complex cells, 106 ",NA,NA
composition ,NA,NA
"max-min, 220 ",NA,NA
"compressor, 375 ",NA,NA
"Computer Science, 34 ",NA,NA
conditional fuzzy ,NA,NA
"mean, 491 ",NA,NA
"variance, 491 ",NA,NA
"conjugate gradient methods, 373 ",NA,NA
"conjunction operator, 505 ",NA,NA
"connections, 2, 93, 102 ",NA,NA
"bottom-up, 107 ",NA,NA
"excitatory, 272 ",NA,NA
"feedback , 82 ",NA,NA
"inhibitory , 272 ",NA,NA
"lateral, 93, 97, 107, 272, 276 ",NA,NA
"recurrent, 82, 107, 179 ",NA,NA
"top-down, 107 ",NA,NA
"connection weight matrix, 220 ",NA,NA
"connection weights, 89, 98 ",NA,NA
"conscience factor, 302 ",NA,NA
"constraints, 417 ",NA,NA
"constructor, 23, 28, 55, 66 ",NA,NA
"default constructor, 23 ",NA,NA
"Consumer Price Index, 387 ",NA,NA
"Content-Addressable-Memory, 5 ",NA,NA
continuous ,NA,NA
"Bi-directional Associative Memory, 211 ",NA,NA
"models, 98 ",NA,NA
"convergence, 78, 79, 96, 118, 119, 132, 323, 372, 373, 375 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (6 of 33) [21/11/02 21:59:06],NA
"cooperation, 9, 94 ",NA,NA
"correlation matrix, 9, 63 ",NA,NA
"correlation-product encoding, 220 ",NA,NA
"cost function, 124, 373 ",NA,NA
"Cottrell, 374 ",NA,NA
"counterpropagation, 106 ",NA,NA
"network, 92, 93, 302 ",NA,NA
"cout, 25, 58 ",NA,NA
"C++, 21 ",NA,NA
"classes, 138 ",NA,NA
"code, 36 ",NA,NA
"comments, 58 ",NA,NA
"compilers, 27 ",NA,NA
"implementation, 185 ",NA,NA
"crisp, 31, 73, 74 ",NA,NA
"data sets, 475 ",NA,NA
"rules, 48, 217 ",NA,NA
"values, 50 ",NA,NA
"cube, 84, 87, 89, 90 ",NA,NA
"cum_deltas, 331 ",NA,NA
"cycle, 78, 125 ",NA,NA
"learning cycle, 103 ",NA,NA
"cyclic information, 380 ",NA,NA
D,NA,NA
"data biasing, 378 ",NA,NA
"data hiding, 21, 22 ",NA,NA
"data clustering, 109, 245 ",NA,NA
"data completion, 102 ",NA,NA
"data compression, 102, 302 ",NA,NA
"Deboeck, 406 ",NA,NA
"Davalo, 457 ",NA,NA
"de la Maza, 410 ",NA,NA
"Decision support systems, 75 ",NA,NA
"declining issues, 387 ",NA,NA
"decompressor, 375 ",NA,NA
"decorrelation, 384 ",NA,NA
default ,NA,NA
"constructor, 23, 66 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (7 of 33) [21/11/02 21:59:06],NA
"destructor, 23 ",NA,NA
"defuzzification, 504, 506 ",NA,NA
"degenerate tour, 424 ",NA,NA
degree of ,NA,NA
"certainty, 31 ",NA,NA
"membership, 32, 477 ",NA,NA
"degrees of freedom, 383 ",NA,NA
"delete, 24, 144 ",NA,NA
"delta rule, 110-113 ",NA,NA
"derived class, 23, 25, 26, 144 ",NA,NA
"descendant, 139, 143 ",NA,NA
"DeSieno, 302 ",NA,NA
"destructor, 23, 24 ",NA,NA
"digital signal processing boards, 325 ",NA,NA
"dimensionality, 381, 382, 384 ",NA,NA
"directed graph, 65 ",NA,NA
"discount rate, 35 ",NA,NA
"discretization of a character, 98 ",NA,NA
"discrete models, 98 ",NA,NA
"discriminator, 517 ",NA,NA
"disjunction operator, 506 ",NA,NA
"display_input_char function, 308 ",NA,NA
"display_winner_weights function, 308 ",NA,NA
distance ,NA,NA
"Euclidean, 13 ",NA,NA
"Hamming, 13 ",NA,NA
DJIA ,NA,NA
see,NA,NA
 Dow Jones Industrial Average ,NA,NA
DOF ,NA,NA
see,NA,NA
 degrees of freedom ,NA,NA
"domains, 479, 484 ",NA,NA
"dot product, 11, 12, 51, 64, 65 ",NA,NA
"Dow Jones Industrial Average, 378, 386 ",NA,NA
"dual confirmation trading system, 408 ",NA,NA
"dynamic allocation of memory, 24 ",NA,NA
"dynamic binding, 24, 139 ",NA,NA
dynamics ,NA,NA
"adaptive, 74 ",NA,NA
"nonadaptive, 74 ",NA,NA
E,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (8 of 33) [21/11/02 21:59:06],NA
EMA ,NA,NA
see,NA,NA
 exponential moving average ,NA,NA
"encapsulate, 29 ",NA,NA
"encapsulation, 21, 22 ",NA,NA
"encode, 375 ",NA,NA
"encoding, 7, 81, 220 ",NA,NA
"algorithm, 94, 96 ",NA,NA
"correlation-product, 220 ",NA,NA
"phase, 94 ",NA,NA
"thermometer, 380 ",NA,NA
"time, 120 ",NA,NA
"energy, 422 ",NA,NA
"function, 119 ",NA,NA
"level, 113, 422 ",NA,NA
"surface, 177 ",NA,NA
"Engineering, 34 ",NA,NA
"epoch, 125 ",NA,NA
"Ercal, 514 ",NA,NA
"error signal, 103 ",NA,NA
"error surface, 113 ",NA,NA
"error tolerance, 136 ",NA,NA
"Euclidean distance, 13, 280, 296 ",NA,NA
"excitation, 94, 98, 276, 303 ",NA,NA
"excitatory connections, 244, 272 ",NA,NA
"exclusive or, 83 ",NA,NA
"exemplar, 181-183, 201 ",NA,NA
"class in BAM network, 186 ",NA,NA
"pairs, 135, 177 ",NA,NA
"patterns, 135, 177 ",NA,NA
"exemplar pattern, 16, 64 ",NA,NA
"exemplars, 64, 65, 74, 75, 115, 181 ",NA,NA
"Expert systems, 48, 75, 217 ",NA,NA
"exponential moving average, 399 ",NA,NA
"extended (database) model, 486 ",NA,NA
"extended-delta-bar-delta, 406 ",NA,NA
F,NA,NA
"factorial, 420 ",NA,NA
FAM ,NA,NA
see,NA,NA
 Fuzzy Associative Memories ,NA,NA
"Fast Fourier Transform, 374 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (9 of 33) [21/11/02 21:59:06],NA
"fault tolerance, 374 ",NA,NA
"feature detector, 328 ",NA,NA
"feature extraction, 7, 513 ",NA,NA
"Federal Reserve, 388, 388 ",NA,NA
"feedback, 4, 5, 93 ",NA,NA
"connections, 123, 179 ",NA,NA
feed forward ,NA,NA
"Backpropagation, 81, 92, 112, 115, 123, 384, ",NA,NA
"network , 145, 406, 409, 511, ",NA,NA
"architecture, 124, ",NA,NA
"layout, 124, ",NA,NA
"network, 10 ",NA,NA
"operation, 185 ",NA,NA
"type, 2 ",NA,NA
"field, 82 ",NA,NA
"filter, 322 ",NA,NA
"financial forecasting, 377 ",NA,NA
"fire, 3, 71, 87, 89 ",NA,NA
"first derivative, 380 ",NA,NA
"fiscal policy, 36 ",NA,NA
"fit values, 32, 217 ",NA,NA
"fit vector, 32, 217, 221 ",NA,NA
"floating point calculations, 519 ",NA,NA
"compilation for, 519 ",NA,NA
"F1 layer, 245 ",NA,NA
"calculations, 247 ",NA,NA
"Fogel, 76, 77 ",NA,NA
"forecasting, 102 ",NA,NA
"model, 378 ",NA,NA
"T-Bill yields, 405 ",NA,NA
"T-Note yields, 405 ",NA,NA
"forward, 93, 104 ",NA,NA
"Fourier transform, 380 ",NA,NA
"Frank, 515 ",NA,NA
"Freeman, 246, 248 ",NA,NA
frequency ,NA,NA
"component, 380 ",NA,NA
"signature, 374 ",NA,NA
"frequency spikes, 380 ",NA,NA
"friend class, 23, 66, 68 ",NA,NA
"F2 layer, 245 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (10 of 33) [21/11/02 21:59:06],NA
"calculations, 247 ",NA,NA
"Fukushima, 92, 106 ",NA,NA
function ,NA,NA
"constructor function, 28 ",NA,NA
"energy function, 119 ",NA,NA
"evaluation, 109 ",NA,NA
"fuzzy step function, 101 ",NA,NA
"hyperbolic tangent function, 100 ",NA,NA
"linear function, 99, 102 ",NA,NA
"logistic function, 86, 100 ",NA,NA
"Lyapunov function, 118, 119 ",NA,NA
"member function, 28, 308 ",NA,NA
"objective, 417 ",NA,NA
"overloading, 25, 139 ",NA,NA
"ramp function, 99, 101, 102 ",NA,NA
"reference function, 493 ",NA,NA
sigmoid ,NA,NA
"function, 99, 100, 126, 129, 133, 164, 177, 395 ",NA,NA
"logistic function, 100 ",NA,NA
"step function, 99, 101 ",NA,NA
"threshold function, 52, 95, 99, 101 ",NA,NA
"XOR function, 83-85, 87 ",NA,NA
"fuzzifier, 35, 36, 47 ",NA,NA
"program, 50 ",NA,NA
"fuzziness, 48, 50 ",NA,NA
"fuzzy adaptive system, 49 ",NA,NA
"fuzzy ARTMAP, 517 ",NA,NA
"fuzzy association, 217 ",NA,NA
"Fuzzy Associative Memories, 49, 50, 81, 92, 104, 115, 117, 217, 218, 473 ",NA,NA
"encoding, 219, 220 ",NA,NA
"Fuzzy Cognitive Map, 48, 49 ",NA,NA
"fuzzy conditional expectations, 490, 509 ",NA,NA
"fuzzy control , 497, 509 ",NA,NA
"system, 47, 48, 473 ",NA,NA
"fuzzy controller, 47 ",NA,NA
"fuzzy database, 473, 475, 509 ",NA,NA
"fuzzy expected value, 490 ",NA,NA
"fuzzy equivalence relation, 481 ",NA,NA
"fuzzy events, 488, 509 ",NA,NA
"conditional probability of, 491 ",NA,NA
"probability of , 490 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (11 of 33) [21/11/02 21:59:06],NA
"fuzzy inputs, 47, 73 ",NA,NA
"fuzzy logic, 31, 34, 50, 473 ",NA,NA
"controller, 473, 497, 509 ",NA,NA
"fuzzy matrix, 217 ",NA,NA
"fuzzy means, 488, 490, 509 ",NA,NA
"fuzzy numbers, 493, 496 ",NA,NA
"triangular, 496 ",NA,NA
"fuzzy OR method, 505 ",NA,NA
"fuzzy outputs, 47, 74 ",NA,NA
"fuzzy quantification, 488 ",NA,NA
"fuzzy queries, 483, 488 ",NA,NA
"fuzzy relations, 479, 509 ",NA,NA
"matrix representation, 479 ",NA,NA
"fuzzy rule base, 502-504 ",NA,NA
"fuzzy rules, 47, 50 ",NA,NA
"fuzzy set, 32, 50, 218, 477, 488 ",NA,NA
"complement, 218 ",NA,NA
"height, 218 ",NA,NA
"normal, 218 ",NA,NA
"operations, 32, 217 ",NA,NA
"fuzzy systems, 50, 217 ",NA,NA
"fuzzy-valued, 34 ",NA,NA
"fuzzy values, 477 ",NA,NA
"fuzzy variances, 488, 490, 509 ",NA,NA
"fzneuron class, 221 ",NA,NA
G,NA,NA
"Gader, 513 ",NA,NA
"gain , 107 ",NA,NA
"constant, 273 ",NA,NA
"parameter, 429, 467 ",NA,NA
"gain control, 243, 248 ",NA,NA
"unit, 244 ",NA,NA
"Gallant, 117 ",NA,NA
"Ganesh, 514 ",NA,NA
"Gaussian density function, 458, 459, 524 ",NA,NA
"generalization, 121, 320, 382 ",NA,NA
"ability, 121, 336 ",NA,NA
"generalized delta rule, 112, 176 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (12 of 33) [21/11/02 21:59:06],NA
"genetic algorithms, 75, 76, 385 ",NA,NA
global ,NA,NA
"minimum, 113, 177 ",NA,NA
"variable, 28 ",NA,NA
"Glover, 471 ",NA,NA
"gradient, 112, 113 ",NA,NA
"grandmother cells, 117 ",NA,NA
"gray scale, 305, 306, 322, 374 ",NA,NA
"grid, 214, 305 ",NA,NA
"Grossberg, 19, 92, 93, 107, 117, 243, 269 ",NA,NA
"Grossberg layer, 9, 19, 82, 92, 106, 302 ",NA,NA
H,NA,NA
"Hamming distance, 13, 201, 202 ",NA,NA
"handwriting analysis, 98 ",NA,NA
"handwriting recognition, 102 ",NA,NA
"handwritten characters, 92 ",NA,NA
"heap, 144 ",NA,NA
"Hebb, 110 ",NA,NA
Hebbian ,NA,NA
"conditioned learning, 302 ",NA,NA
"learning, 105, 110 ",NA,NA
"Hebb’s rule, 110, 111 ",NA,NA
"Hecht-Nielsen, 93, 106, 302 ",NA,NA
"Herrick Payoff Index, 401 ",NA,NA
"heteroassociation, 7, 8, 82, 92, 102, 104, 180, 181 ",NA,NA
"heteroassociative network, 7, 97 ",NA,NA
"hidden layer, 2, 4, 75, 86, 89 ",NA,NA
"hierarchical neural network, 407 ",NA,NA
"hierarchies of classes, 27, 29 ",NA,NA
"Hinton, 114 ",NA,NA
"Hoff, 102, 112 ",NA,NA
"holographic neural network, 408 ",NA,NA
"Honig, 515 ",NA,NA
"Hopfield, 422, 427, 429 ",NA,NA
"memory, 73, 115, 117, 181 ",NA,NA
"Hopfield network, 9, 11-14, 16, 19, 51, 79, 81, 82, 93, 111, 119, 120, 181, 472 ",NA,NA
"Hotelling transform, 384 ",NA,NA
"Housing Starts, 387 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (13 of 33) [21/11/02 21:59:06],NA
"hybrid models, 75 ",NA,NA
"hyperbolic tangent function, 429 ",NA,NA
"hypercube, 218 ",NA,NA
"unit, 218 ",NA,NA
"hyperplane, 84 ",NA,NA
"hypersphere, 273 ",NA,NA
I,NA,NA
"image, 106, 302 ",NA,NA
"compression, 374 ",NA,NA
"processing, 98, 102 ",NA,NA
"five-way transform, 516 ",NA,NA
"recognition, 375 ",NA,NA
"resolution, 322 ",NA,NA
"implementation of functions, 67 ",NA,NA
"ineuron class, 66 ",NA,NA
"inference engine, 47 ",NA,NA
"inheritance, 21, 25, 26, 138 ",NA,NA
"multiple inheritance, 26 ",NA,NA
"inhibition, 9, 94, 98 ",NA,NA
"global, 428, 456 ",NA,NA
"lateral, 272, 276 ",NA,NA
"inhibitory connection, 272 ",NA,NA
initialization of ,NA,NA
"bottom-up weights, 250 ",NA,NA
"parameters, 246 ",NA,NA
"top-down weights, 250 ",NA,NA
"weights, 94 ",NA,NA
"inline, 165 ",NA,NA
"input, 98 ",NA,NA
"binary input, 98 ",NA,NA
"bipolar input, 98 ",NA,NA
"layer, 2, 10 ",NA,NA
"nature of , 73 ",NA,NA
"number of , 74 ",NA,NA
"patterns, 51, 65 ",NA,NA
"signals, 65 ",NA,NA
"space, 124 ",NA,NA
"vector, 53, 71, 272, 112 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (14 of 33) [21/11/02 21:59:06],NA
"input/output, 71 ",NA,NA
"inqreset function, 251 ",NA,NA
"instar, 93 ",NA,NA
"interactions, 94 ",NA,NA
"interconnections, 7 ",NA,NA
"interest rate, 387 ",NA,NA
"internal activation , 3 ",NA,NA
"intersection, 32, 33 ",NA,NA
"inverse mapping, 62, 182 ",NA,NA
"Investor’s Business Daily, 388 ",NA,NA
"iostream, 54, 71 ",NA,NA
"istream, 58 ",NA,NA
"iterative process, 78 ",NA,NA
J,NA,NA
"Jagota, 514 ",NA,NA
"January effect, 380 ",NA,NA
"Jurik, 381, 384, 392 ",NA,NA
K,NA,NA
"Karhunen-Loev transform, 384 ",NA,NA
"Katz, 377 ",NA,NA
"Kimoto, 408 ",NA,NA
"kohonen.dat file, 275, 298, 300, 317 ",NA,NA
"Kohonen, 19, 116, 117, 245, 271, 303, 456 ",NA,NA
"Kohonen feature map, 16, 271, 273, 303, 305, 323 ",NA,NA
"conscience factor, 302 ",NA,NA
"neighborhood size, 280, 299, 300 ",NA,NA
"training law, 273 ",NA,NA
"Kohonen layer, 9, 19, 82, 92, 106, 298, 302, 322 ",NA,NA
"class, 276 ",NA,NA
"Kohonen network, 275, 276, 280, 300, 303, 322 ",NA,NA
"applications of, 302, ",NA,NA
"Kohonen output layer, 275 ",NA,NA
"Kohonen Self-Organizing Map, 115, 456, 471, 472 ",NA,NA
"Kosaka, 409, ",NA,NA
"Kosko, 49, 50, 104, 215, 242, 506 ",NA,NA
"Kostenius, 408, 409 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (15 of 33) [21/11/02 21:59:06],NA
"Kronecker delta function 428, 524 ",NA,NA
L,NA,NA
"lambda, 136, 433 ",NA,NA
"late binding, 24 ",NA,NA
"lateral, 93 ",NA,NA
"lateral competition, 303 ",NA,NA
"laterally connected, 65 ",NA,NA
"lateral connections, 93, 97, 107, 272, 276 ",NA,NA
"lateral inhibition, 272, 276 ",NA,NA
"layer, 2, 81 ",NA,NA
"C layer, 106 ",NA,NA
"comparison, 244 ",NA,NA
"complex layer, 106 ",NA,NA
"F1, 244 ",NA,NA
"F2, 244 ",NA,NA
"Grossberg layer, 82, 92, 302 ",NA,NA
"hidden layer, 75, 81, 86, 89 ",NA,NA
"input layer, 2, 3, 82 ",NA,NA
"Kohonen layer, 82, 92, 302, 322 ",NA,NA
"middle layer, 329, 372 ",NA,NA
"output layer, 2, 82 ",NA,NA
"recognition, 244 ",NA,NA
"S layer, 106 ",NA,NA
"simple layer, 106 ",NA,NA
"layout, 52, 86, 124 ",NA,NA
"ART1, 244 ",NA,NA
"BAM , 180 ",NA,NA
"Brain-State-in-a-Box, 105 ",NA,NA
"FAM, 219 ",NA,NA
"Hopfield network, 11 ",NA,NA
"for TSP, 427 ",NA,NA
"LVQ, 117 ",NA,NA
"Madaline model, 103 ",NA,NA
"LBS Capital Management, 377 ",NA,NA
"learning, 4, 74, 98, 109, 110, 117, 118 ",NA,NA
"algorithm, 61, 79, 102, 118 ",NA,NA
"cycle, 103 ",NA,NA
"Hebbian, 105, 110 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (16 of 33) [21/11/02 21:59:06],NA
"one-shot, 117 ",NA,NA
"probabilistic, 113 ",NA,NA
"rate(parameter), 111, 112, 123, 125, 127, 136, 175 ",NA,NA
"supervised learning, 5, 110, 112, 117, 121 ",NA,NA
"time, 120 ",NA,NA
unsupervised ,NA,NA
"competitive learning, 271 ",NA,NA
"learning, 5, 110, 117, 121 ",NA,NA
"Learning Vector Quantizer, 115-117, 302 ",NA,NA
"least mean squared error, 111, 119, 123, 419 ",NA,NA
"rule, 111 ",NA,NA
"Le Cun, 375 ",NA,NA
"Lee, 512 ",NA,NA
"Levenberg-Marquardt optimization, 373 ",NA,NA
"Lewis, 377 ",NA,NA
"Lin, 512 ",NA,NA
"linear function, 99, 102 ",NA,NA
"linear possibility regression model, 493, 496, 509 ",NA,NA
"linear programming, 417 ",NA,NA
"integer, 417 ",NA,NA
"linearly separable, 83- 85 ",NA,NA
LMS ,NA,NA
see,NA,NA
 least mean squared error rule ,NA,NA
"local minimum, 113, 177, 325 ",NA,NA
logic ,NA,NA
"boolean logic, 50 ",NA,NA
"fuzzy logic, 31, 34, 50, 473 ",NA,NA
"logical operations, 31 ",NA,NA
"AND, 64 ",NA,NA
"logistic function, 86, 100 ",NA,NA
"Long-term memory, 6, 77- 79, 118, 472 ",NA,NA
"traces of, 243 ",NA,NA
look-up ,NA,NA
"memory, 5 ",NA,NA
"table, 106 ",NA,NA
LTM ,NA,NA
see,NA,NA
 Long-term memory ,NA,NA
LVQ ,NA,NA
see,NA,NA
 Learning Vector Quantizer ,NA,NA
"Lyapunov Function, 118, 119 ",NA,NA
M,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (17 of 33) [21/11/02 21:59:06],NA
MACD ,NA,NA
see,NA,NA
 moving average convergence divergence ,NA,NA
"Madaline, 102, 103 ",NA,NA
"main diagonal, 63, 480 ",NA,NA
"malignant melanoma, 514 ",NA,NA
"malloc, 24 ",NA,NA
"Mandelman, 378 ",NA,NA
MAPE ,NA,NA
see,NA,NA
 mean absolute percentage error ,NA,NA
"mapping, 123, 180 ",NA,NA
"binary to bipolar, 62, 63 ",NA,NA
"inverse, 62, 182 ",NA,NA
"nonlinear, 109 ",NA,NA
"real to binary, 180 ",NA,NA
"mapping surface, 109 ",NA,NA
"Markowitz, 470 ",NA,NA
"Marquez, 406 ",NA,NA
"Mason, 516 ",NA,NA
"matrix, 97, 521 ",NA,NA
"addition, 521 ",NA,NA
"correlation matrix, 9 ",NA,NA
"fuzzy, 217 ",NA,NA
"multiplication, 11 ",NA,NA
"product, 104, 522 ",NA,NA
"transpose, 11 ",NA,NA
"weight matrix, 97 ",NA,NA
"max_cycles, 326 ",NA,NA
"maximum, 33, 219 ",NA,NA
"max-min composition, 220, 221 ",NA,NA
"McClelland, 103 ",NA,NA
"McCulloch, 6 ",NA,NA
"McNeill, 508 ",NA,NA
"mean absolute percentage error, 406 ",NA,NA
"mean squared error, 111, 410 ",NA,NA
"Mears, 212, 214 ",NA,NA
"membership, 32 ",NA,NA
"functions, 50 ",NA,NA
"triangular, 499, 506 ",NA,NA
"rules, 49 ",NA,NA
"memorization, 121, 320, 336, 382, 397 ",NA,NA
"memorizing inputs, 273, 320 ",NA,NA
"memory, 98 ",NA,NA
"adaptive, 471 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (18 of 33) [21/11/02 21:59:06],NA
"fuzzy associative, 218, 221 ",NA,NA
"long-term memory, 6, 77-79, 118, 472 ",NA,NA
"recency based, 471 ",NA,NA
"short-term memory, 6, 77, 78, 79, 107, 118, 471 ",NA,NA
"Mendelsohn, 407, 408 ",NA,NA
"methods, 22, 145 ",NA,NA
"metric, 5, 103 ",NA,NA
"mexican hat function, 272, 273, 274, 276 ",NA,NA
"middle layer, 329, 372 ",NA,NA
choosing size 372 ,NA,NA
"minimum, 33, 219 ",NA,NA
"global, 113, 177 ",NA,NA
"local, 113, 177, 325 ",NA,NA
"Minsky, 83 ",NA,NA
model ,NA,NA
"ART1, 245 ",NA,NA
"continuous, 98 ",NA,NA
"discrete, 98 ",NA,NA
"Perceptron model, 65, 68, 81, 83 ",NA,NA
"modulo, 423 ",NA,NA
"Mohamed, 513 ",NA,NA
"momentum, 325, 330, 337, 372, 400 ",NA,NA
"implementing, 331 ",NA,NA
"parameter, 119, 123, 134, 384 ",NA,NA
"term, 330, 375 ",NA,NA
"Morse, 514 ",NA,NA
"Moss, 514 ",NA,NA
"moving average convergence divergence, 401 ",NA,NA
"moving averages, 380, 399 ",NA,NA
"simple, 399 ",NA,NA
"weighted, 399 ",NA,NA
"multilayered, 92 ",NA,NA
"network, 106 ",NA,NA
"multilayer feed-forward neural network, 7 ",NA,NA
"multilayer networks, 123 ",NA,NA
"multiple inheritance, 26 ",NA,NA
"Munro, 374 ",NA,NA
N,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (19 of 33) [21/11/02 21:59:06],NA
NF ,NA,NA
see,NA,NA
 noise factor ,NA,NA
"NP-complete, 419, 427, 457 ",NA,NA
NYSE ,NA,NA
see,NA,NA
 New York Stock Exchange ,NA,NA
"Naim, 457 ",NA,NA
"neighborhood, 276, 303, 457 ",NA,NA
"size, 274, 280, 299, 300 ",NA,NA
"neighbors, 272 ",NA,NA
"Neocognitron, 81, 92, 106 ",NA,NA
"Nellis, 516 ",NA,NA
"NETTalk, 374 ",NA,NA
network ,NA,NA
"adaptive, 77 ",NA,NA
"architecture, 77, 384, 388 ",NA,NA
"autoassociative network, 97 ",NA,NA
"backpropagation network, 329 ",NA,NA
"bi-directional associative memory network, 104,, 88 ",NA,NA
"Brain-State-in-a-Box network, 105 ",NA,NA
"class, 53, 66 ",NA,NA
"heteroassociative networks, 97 ",NA,NA
"Hopfield network, 9, 11-14, 16, 19, 51, 79, 81, 82, 93, 111, 119, 120, 181, ",NA,NA
472 ,NA,NA
"layout, 86 ",NA,NA
"modeling, 73 ",NA,NA
"multilayer network, 123 ",NA,NA
"nodes, 65 ",NA,NA
"Perceptron network, 65, 66, 68, 79 ",NA,NA
"radial basis function networks, 112, 114, 115 ",NA,NA
RBF networks ,NA,NA
see,NA,NA
 radial basis function networks ,NA,NA
"self-organizing , 269 ",NA,NA
"statistical trained networks, 112 ",NA,NA
"NeuFuz, 49 ",NA,NA
"neural network, 1, 2 ",NA,NA
"algorithms, 176 ",NA,NA
"artificial neural network, 1 ",NA,NA
"autoassociative, 375 ",NA,NA
"biological, 272 ",NA,NA
"counterpropagation, 302 ",NA,NA
"fault tolerance of, 374 ",NA,NA
"FAM, 218 ",NA,NA
"hierarchical, 407 ",NA,NA
"holographic, 408 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (20 of 33) [21/11/02 21:59:06],NA
"Kohonen, 271, 322 ",NA,NA
"multilayer, 123 ",NA,NA
"Perceptron, 65 ",NA,NA
"plug-in boards, 325 ",NA,NA
"self-organizing, 107, 269 ",NA,NA
"Tabu, 471 ",NA,NA
"two-layer, 92 ",NA,NA
"neural-trained fuzzy systems, 49 ",NA,NA
neuromimes 2 ,NA,NA
"neuron, 1, 3 ",NA,NA
"input neurons, 82 ",NA,NA
"output neuron, 99 ",NA,NA
"new, 24, 144 ",NA,NA
"Newton’s method, 373 ",NA,NA
"New York Stock Exchange, 387 ",NA,NA
"new highs, 389 ",NA,NA
"new lows, 389 ",NA,NA
"noise, 5, 330, 336, 337, 372, 375 ",NA,NA
"random, 375 ",NA,NA
"noise factor, 336 ",NA,NA
"noise-saturation dilemma, 79 ",NA,NA
"noise tolerance, 105 ",NA,NA
"noisy data, 320 ",NA,NA
"nonlinear function, 120 ",NA,NA
"nonlinear mapping, 109 ",NA,NA
"nonlinear scaling function, 10 ",NA,NA
"nonlinear optimization, 417, 422, 472 ",NA,NA
"nontraining mode, 135 ",NA,NA
"Normal distribution, 524 ",NA,NA
"normal fuzzy set, 218 ",NA,NA
"normalization of a vector, 272 ",NA,NA
"normalized inputs, 271, 381 ",NA,NA
"normalized weights, 280 ",NA,NA
"notation, 132 ",NA,NA
"nprm parameter, 433 ",NA,NA
"number bins, 43 ",NA,NA
NYSE ,NA,NA
see,NA,NA
 New York Stock Exchange ,NA,NA
O,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (21 of 33) [21/11/02 21:59:06],NA
"object, 22 ",NA,NA
"objective function, 417 ",NA,NA
"object-orientation, 21 ",NA,NA
"object-oriented programming language, 21 ",NA,NA
"Object-Oriented Analysis and Design, 21 ",NA,NA
"on-balance volume, 402 ",NA,NA
"on center, off surround, 97, 98 ",NA,NA
"one-shot learning, 117 ",NA,NA
"oneuron class, 66 ",NA,NA
"Operations Research, 34 ",NA,NA
"operator overloading, 25, 139 ",NA,NA
optical character ,NA,NA
"recognition, 245 ",NA,NA
"recognizer, 514 ",NA,NA
"optimization, 102, 109, 417 ",NA,NA
"nonlinear, 417, 422, 472 ",NA,NA
"stock portfolio, 470 ",NA,NA
"ordered pair, 32 ",NA,NA
"organization of layers for backpropagation program, 144 ",NA,NA
"orienting subsystem, 107, 243 ",NA,NA
"orthogonal, 11, 12, 51, 64, 98 ",NA,NA
"bit patterns, 64 ",NA,NA
"input vectors, 299 ",NA,NA
"set, 65 ",NA,NA
"ostream, 58 ",NA,NA
"output, 99 ",NA,NA
"layer, 2, 10 ",NA,NA
"nature of , 74 ",NA,NA
"number of , 74 ",NA,NA
"space, 124 ",NA,NA
"stream, 58 ",NA,NA
"outstar, 93, 106 ",NA,NA
"overfitting of data, 383 ",NA,NA
"overlap composition method, 506 ",NA,NA
"overloading, 21, 24 ",NA,NA
"function overloading, 25, 139 ",NA,NA
"operator overloading, 25, 139 ",NA,NA
"overtrained network, 329 ",NA,NA
P,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (22 of 33) [21/11/02 21:59:06],NA
"Papert, 83 ",NA,NA
"Parker, 103 ",NA,NA
"partitioning, 87, 88 ",NA,NA
"past_deltas, 331 ",NA,NA
"Patil, 406 ",NA,NA
pattern ,NA,NA
"association, 16 ",NA,NA
"binary pattern, 11, 97 ",NA,NA
"bit pattern, 99 ",NA,NA
"character, 17 ",NA,NA
"classification, 8, 98, 109 ",NA,NA
"completion, 105, 109 ",NA,NA
"matching, 8, 98 ",NA,NA
"recognition, 16, 34, 102, 108, 305, 322 ",NA,NA
"studies, 380 ",NA,NA
"system, 121 ",NA,NA
"spatial, 99, 214 ",NA,NA
"Pavlovian, 5 ",NA,NA
"Perceptron, 3, 4, 66, 73, 82, 87, 90, 93, 102, 112 ",NA,NA
"model, 65, 68, 81, 83 ",NA,NA
"multilayer Perceptron, 85, 88 ",NA,NA
"network, 65, 66, 68, 79 ",NA,NA
"single-layer Perceptron, 85 ",NA,NA
"permutations, 420 ",NA,NA
"Perry, 484 ",NA,NA
"perturbation , 5, 113 ",NA,NA
"phoneme, 303, 374 ",NA,NA
"phonetic typewriter, 303 ",NA,NA
"Pimmel, 513 ",NA,NA
"Pitts, 6 ",NA,NA
"pixel, 16, 322, 329, 374 ",NA,NA
"values, 214, 305 ",NA,NA
"plastic, 107 ",NA,NA
"plasticity, 78, 79 ",NA,NA
"plasticity-stability dilemma, 243 ",NA,NA
"Pletta, 515 ",NA,NA
"polymorphic function, 24, 28 ",NA,NA
"polymorphism, 21, 24, 27, 138 ",NA,NA
"Pomerleau, 374 ",NA,NA
"portfolio selection, 470, 472 ",NA,NA
"possibility distributions, 486, 487, 509 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (23 of 33) [21/11/02 21:59:06],NA
"relational model, 486, 487 ",NA,NA
"postprocess, 35 ",NA,NA
"postprocessing, 50 ",NA,NA
"filter , 50 ",NA,NA
potlpair class ,NA,NA
"in BAM network, 186 ",NA,NA
"preprocess, 35, 50 ",NA,NA
"preprocessing , 87, 379, 399 ",NA,NA
"data, 389 ",NA,NA
"filter, 50 ",NA,NA
"fuzzifier, 35 ",NA,NA
"Price is Right, 3, 65 ",NA,NA
"principal component analysis, 384 ",NA,NA
"private, 23, 26 ",NA,NA
"probabilities, 31, 419 ",NA,NA
"probability, 31, 43 ",NA,NA
"distributions, 113 ",NA,NA
processing ,NA,NA
"additive, 75 ",NA,NA
"hybrid, 75 ",NA,NA
"multiplicative, 75 ",NA,NA
"PROJECT operation, 485 ",NA,NA
"proposition, 31 ",NA,NA
"protected, 23, 26, 54, 143 ",NA,NA
"public, 23, 26, 53, 54 ",NA,NA
Q,NA,NA
"quadratic form, 119, 120, 418 ",NA,NA
"quadratic programming problem, 418 ",NA,NA
"quantification, 473, 475 ",NA,NA
"quantization levels, 322 ",NA,NA
"queries, 475, 476, 488 ",NA,NA
"fuzzy, 483, 488 ",NA,NA
R,NA,NA
"radial basis function networks, 112, 114, 115 ",NA,NA
"ramp function, 99, 101,, 102 ",NA,NA
"Ramsay, 515 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (24 of 33) [21/11/02 21:59:06],NA
"random number generator, 37 ",NA,NA
"range, 394 ",NA,NA
"normalized, 394, 395 ",NA,NA
"rate of change, 392, 400, 404 ",NA,NA
"function, 392 ",NA,NA
"indicator, 393, 394 ",NA,NA
"real-time recurrent learning algorithm, 515 ",NA,NA
"recall, 7, 81, 184, 220 ",NA,NA
"BAM , 184 ",NA,NA
"FAM, 220, 221 ",NA,NA
"recency based memory, 471 ",NA,NA
"recognition layer, 244 ",NA,NA
"recurrent, 13, 179 ",NA,NA
"recurrent connections, 82, 107, 179 ",NA,NA
reference ,NA,NA
"activation level, 456 ",NA,NA
"function, 493 ",NA,NA
"regression analysis, 406 ",NA,NA
"risk-adjusted return, 410 ",NA,NA
"relations, 476 ",NA,NA
"antisymmetric, 480, 481 ",NA,NA
"reflexive, 480, 481 ",NA,NA
"resemblance, 509 ",NA,NA
"similarity, 481, 509 ",NA,NA
"symmetric, 480, 481 ",NA,NA
"transitive, 480, 481 ",NA,NA
"relative strength index, 400, 404 ",NA,NA
"remainder, 423 ",NA,NA
"reset, 243, 247, 251, 262 ",NA,NA
"reset node, 244 ",NA,NA
"reset unit, 244 ",NA,NA
"resonance, 104, 107, 117, 118, 181, 215, 243, 269 ",NA,NA
"responsive exploration, 471 ",NA,NA
"Ressler, 512 ",NA,NA
"restrmax function, 251 ",NA,NA
"return type, 23 ",NA,NA
"reuse, 26 ",NA,NA
"Robotics, 34 ",NA,NA
ROC ,NA,NA
see,NA,NA
 rate of change ,NA,NA
"Rosenberg, 374 ",NA,NA
"Ross, 517 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (25 of 33) [21/11/02 21:59:06],NA
"row vector, 97, 104 ",NA,NA
RSI ,NA,NA
see,NA,NA
 relative strength index ,NA,NA
rule ,NA,NA
"delta, 110, 111 ",NA,NA
"generalized delta, 112 ",NA,NA
"Hebbian, 111 ",NA,NA
"Hebb’s, 110 ",NA,NA
rules ,NA,NA
"fuzzy rules, 50 ",NA,NA
"Rumbaugh, 21 ",NA,NA
"Rummelhart, 103 ",NA,NA
S,NA,NA
S&P 500 Index ,NA,NA
see,NA,NA
 Standard and Poor’s 500 Index ,NA,NA
"Sathyanarayan Rao, 515 ",NA,NA
"saturate, 381 ",NA,NA
"scalar, 61 ",NA,NA
"scalar multiplier, 64 ",NA,NA
"second derivative, 380 ",NA,NA
"Sejnowski, 114, 374 ",NA,NA
"self-adaptation, 115 ",NA,NA
"self-connected, 53 ",NA,NA
"self-organization, 5, 6, 74, 115, 116 ",NA,NA
"self-organizing feature map, 116 ",NA,NA
"Self-Organizing Map, 245, 271 ",NA,NA
"self-organizing neural networks, 107, 117, 121 self-",NA,NA
"supervised backpropagation, 375 ",NA,NA
"sensitivity analysis, 384 ",NA,NA
"separable, 84, 86, 88 ",NA,NA
"linearly separable, 83, 84 ",NA,NA
"subsets, 87 ",NA,NA
"separability, 84, 86 ",NA,NA
separating ,NA,NA
"line, 86 ",NA,NA
"plane, 85 ",NA,NA
"Sethuraman, 515 ",NA,NA
"set membership, 32 ",NA,NA
"Sharda, 406 ",NA,NA
"shift operator, 25 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (26 of 33) [21/11/02 21:59:06],NA
"Short Term Memory, 6, 77, 78, 79, 107, 118, 471 ",NA,NA
"traces of, 243 ",NA,NA
"Sigma Pi neural network , 75 ",NA,NA
sigmoid ,NA,NA
"activation function, 381, 387 ",NA,NA
"function, 77, 99, 126, 129, 133, 164, 177, 395 ",NA,NA
"squashing, 381 ",NA,NA
"signal filtering, 102 ",NA,NA
signals ,NA,NA
"analog, 98 ",NA,NA
"similarity, 486 ",NA,NA
"class, 481, 509 ",NA,NA
"level, 486 ",NA,NA
"relation, 481 ",NA,NA
"simple cells, 106 ",NA,NA
"simple moving average, 399 ",NA,NA
"simulated annealing, 113, 114 ",NA,NA
"simulator, 372, 396 ",NA,NA
"controls, 173 ",NA,NA
"mode, 138 ",NA,NA
"Skapura, 246, 248 ",NA,NA
"Slater, 515 ",NA,NA
"S layer, 106 ",NA,NA
SMA ,NA,NA
see,NA,NA
 simple moving average ,NA,NA
SOM ,NA,NA
see,NA,NA
 Self-Organizing Map ,NA,NA
"sonar target recognition, 374 ",NA,NA
spatial ,NA,NA
"pattern, 99, 214 ",NA,NA
"temporal pattern, 105, ",NA,NA
speech ,NA,NA
"recognition, 303, ",NA,NA
"synthesizer, 374, ",NA,NA
"spike, 380 ",NA,NA
"squared error, 103 ",NA,NA
squashing ,NA,NA
"function, 384, 458, 459 ",NA,NA
"stable, 79, 107 ",NA,NA
"stability 78, 79, 118 ",NA,NA
"and plasticity, 77 ",NA,NA
"stability-plasticity dilemma, 79, 107,, 269 ",NA,NA
STM ,NA,NA
see,NA,NA
 Short Term Memory ,file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (27 of 33) [21/11/02 21:59:06],NA
"Standard and Poor’s 500 Index, 377, 378 ",NA,NA
"forecasting, 386 ",NA,NA
"standard I/O routines, 519 ",NA,NA
"state energy, 118 ",NA,NA
"state machine, 48 ",NA,NA
"static binding, 139 ",NA,NA
"Steele, 514 ",NA,NA
"steepest descent, 112, 113, 177, 373 ",NA,NA
"step function, 99, 101 ",NA,NA
"stochastics, 402, 404 ",NA,NA
"Stoecker, 514 ",NA,NA
"Stonham, 516 ",NA,NA
string ,NA,NA
"binary, 62 ",NA,NA
"bipolar, 62 ",NA,NA
"structure, 7, 7 ",NA,NA
"subsample, 322 ",NA,NA
"subset, 221 ",NA,NA
subsystem ,NA,NA
"attentional, 107, 243 ",NA,NA
"orienting, 107, 243 ",NA,NA
"Sudjianto, 516 ",NA,NA
"summand, 422 ",NA,NA
"summation symbol, 422 ",NA,NA
"supervised , 109 ",NA,NA
"learning, 5, 110, 112, 115, 117, 121 ",NA,NA
"training 94, 110, 115, 121, 125 ",NA,NA
"Sweeney , 516 ",NA,NA
"symbolic approach, 6 ",NA,NA
T,NA,NA
TSR ,NA,NA
see,NA,NA
 Terminate and Stay Resident ,NA,NA
"Tabu , 471 ",NA,NA
"active, 471 ",NA,NA
"neural network, 471 ",NA,NA
"search, 471, 472 ",NA,NA
"Tank, 422, 427, 429 ",NA,NA
"target 378, 395 ",NA,NA
"outputs, 110, 115 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (28 of 33) [21/11/02 21:59:06],NA
"patterns, 105 ",NA,NA
"scaled, 395 ",NA,NA
"tau, 433 ",NA,NA
"technical analysis, 399 ",NA,NA
"temperature, 118 ",NA,NA
"Temporal Associative Memory, 92 ",NA,NA
"Terano, 496 ",NA,NA
"Terminate and Stay Resident programs, 519 ",NA,NA
"terminating value, 298 ",NA,NA
"termination criterion, 322 ",NA,NA
"test.dat file, 327, 328 ",NA,NA
"test mode, 135, 137, 138, 164, 173, 327, 396 ",NA,NA
"Thirty-year Treasury Bond Rate, 387 ",NA,NA
"Three-month Treasury Bill Rate, 387 ",NA,NA
threshold ,NA,NA
"function, 2, 3, 12, 17, 19, 52, 95, 99, 101, 125, 183 ",NA,NA
"value, 16, 52, 66, 77, 86, 87, 90, 101, 128, 456 ",NA,NA
"thresholding, 87, 185 ",NA,NA
"function, 133, 177, 182, 184, 214 ",NA,NA
"Thro, 508 ",NA,NA
"Tic-Tac-Toe, 76, 79 ",NA,NA
"time lag, 380 ",NA,NA
"time series forecasting, 406, 410 ",NA,NA
"time shifting, 395 ",NA,NA
"timeframe, 378 ",NA,NA
"tolerance, 119, 125, 173, 245, 318, 322, 328, 329, 372 ",NA,NA
"level, 78, 123 ",NA,NA
"value, 119 ",NA,NA
top-down ,NA,NA
"connection weight , 248 ",NA,NA
"connections, 107, 244 ",NA,NA
"top-down inputs, 247 ",NA,NA
"topology, 7 ",NA,NA
"Topology Preserving Maps, 116 ",NA,NA
"tour, 420 ",NA,NA
"traces, 243 ",NA,NA
"of STM, 243 ",NA,NA
"of LTM, 243 ",NA,NA
trading ,NA,NA
"commodities, 405, ",NA,NA
"system, 378 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (29 of 33) [21/11/02 21:59:06],NA
"dual confirmation, 408 ",NA,NA
"training, 4, 74, 75, 98, 109, 110, 119, 181, 396 ",NA,NA
"fast, 107 ",NA,NA
"law, 272, 273, 274, 330, 333 ",NA,NA
"mode, 135, 137, 138, 164, 173, 396 ",NA,NA
"supervised, 94, 110, 115 ",NA,NA
"slow, 107 ",NA,NA
"time, 329 ",NA,NA
"unsupervised, 107, 110 ",NA,NA
transpose ,NA,NA
"of a matrix, 11, 179, 181, 183 ",NA,NA
"of a vector, 11, 63, 97, 181 ",NA,NA
"traveling salesperson(salesman) problem, 118, 119, 419 ",NA,NA
"hand calculation, 423 ",NA,NA
"Hopfield network solution-Hopfield, 427 ",NA,NA
"Hopfield network solution-Anzai, 456 ",NA,NA
"Kohonen network solution, 456 ",NA,NA
"triple, 217 ",NA,NA
"truth value, 31 ",NA,NA
"tsneuron class, 430 ",NA,NA
TS ,NA,NA
see,NA,NA
 Tabu search ,NA,NA
TSP ,NA,NA
see,NA,NA
 traveling salesperson problem ,NA,NA
"turning point predictor, 409 ",NA,NA
"turning points, 407 ",NA,NA
"two-layer networks, 92 ",NA,NA
"two-thirds rule, 107, 244, 245, 269 ",NA,NA
U,NA,NA
"Umano, 486 ",NA,NA
"Unemployment Rate, 387 ",NA,NA
"undertrained network, 329 ",NA,NA
"uniform distribution, 77 ",NA,NA
"union, 32 ",NA,NA
"Unipolar Binary Bi-directional Associative Memory, 212 ",NA,NA
unit ,NA,NA
"circle, 299 ",NA,NA
"hypercube, 218 ",NA,NA
"unit length, 273 ",NA,NA
"universal set, 33 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (30 of 33) [21/11/02 21:59:06],NA
"universe of discourse, 498, 499 ",NA,NA
"unsupervised , 107 ",NA,NA
"competitive learning, 271 ",NA,NA
"learning, 5, 110, 115, 117, 121 ",NA,NA
"training, 107, 110 ",NA,NA
V,NA,NA
value ,NA,NA
"fit value, 32 ",NA,NA
"threshold value, 16, 52, 66, 77, 86, 87, 90, 101, 128, 456 ",NA,NA
variable ,NA,NA
"external, 28 ",NA,NA
"global, 28 ",NA,NA
"vector, 17 ",NA,NA
"codebook vectors, 116 ",NA,NA
"column vector, 97, 104, 181 ",NA,NA
"fit vector, 32, 33 ",NA,NA
"heterassociated, 181 ",NA,NA
"input vector, 53, 71, 272, 112 ",NA,NA
"normalization of, 272 ",NA,NA
"potentially associated, 181 ",NA,NA
"quantization, 302 ",NA,NA
"row vector, 97, 181 ",NA,NA
"weight vector, 9, 96 ",NA,NA
"vector pairs, 181 ",NA,NA
"vertex, 88 ",NA,NA
"vertices, 88 ",NA,NA
"vigilance parameter, 107, 243, 245, 247, 262 ",NA,NA
"virtual, 24, 139 ",NA,NA
"trading, 377 ",NA,NA
"visibility, 26 ",NA,NA
"visible, 53 ",NA,NA
W,NA,NA
"walk-forward methodology, 408 ",NA,NA
"Wall Street Journal, 388 ",NA,NA
"Wasserman, 516 ",NA,NA
"weight matrix, 9, 17, 51, 65, 97, 181, 183 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (31 of 33) [21/11/02 21:59:06],NA
"weight sharing, 375 ",NA,NA
"weight surface, 113 ",NA,NA
"weight update, 276 ",NA,NA
"weight vector, 9, 96 ",NA,NA
"quantizing, 307 ",NA,NA
"weighted sum, 2, 3, 19, 271 ",NA,NA
"weights , 4, 181 ",NA,NA
"bottom-up, 250 ",NA,NA
"connection , 89, 98 ",NA,NA
"top-down, 250 ",NA,NA
"Werbos, 103 ",NA,NA
"Wetherell, 514 ",NA,NA
"Widrow, 102, 112 ",NA,NA
"winner indexes, 298 ",NA,NA
"winner, 98 ",NA,NA
"neuron, 323 ",NA,NA
"winner-take-all, 97, 98, 115, 116, 243, 271, 274 ",NA,NA
"World Wide Web, 388 ",NA,NA
"Wu, 515 ",NA,NA
X,NA,NA
"XOR function, 83-85, 87 ",NA,NA
Y,NA,NA
"Yan, 473, 497 ",NA,NA
"Yu, 212, 214 ",NA,NA
"Yuret, 410 ",NA,NA
Z,NA,NA
"zero pattern, 65 ",NA,NA
"zero-one programming problem, 471 ",NA,NA
"Zipser, 374 ",file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (32 of 33) [21/11/02 21:59:06],NA
Table of Contents,"Copyright © 
 IDG Books Worldwide, Inc.
  
 file:///H:/edonkey/docs/c/(ebook-pdf)%20-%20mathe...+_Neural_Networks_and_Fuzzy_Logic/book-index.html (33 of 33) [21/11/02 21:59:06]",NA
