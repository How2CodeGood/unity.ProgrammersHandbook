Larger Text,Smaller Text,Symbol
Data Structures and ,NA,NA
Algorithm Analysis,NA,NA
Edition 3.2 (C++ Version),NA,NA
Clifford A. Shaffer,"Department of Computer 
 Science 
  
 Virginia Tech 
  
 Blacksburg, VA 24061
  
 January 2, 2012 
  
 Update 3.2.0.3 
  
 For a list of changes, see
  
 http://people.cs.vt.edu/˜shaffer/Book/errata.html
  
 Copyright © 2009-2012 by Clifford A. Shaffer.
  
 This document is made freely available in PDF form for educational and 
 other non-commercial use. You may make copies of this file and 
  
 redistribute in electronic form without charge. You may extract 
 portions of this document provided that the front page, including the 
 title, author, and this notice are included. Any commercial use of this 
 document requires the written consent of the author. The author can be 
 reached at 
  
 shaffer@cs.vt.edu.
  
 If you wish to have a printed version of this document, print copies 
 are published by Dover Publications 
  
 (see
  http://store.doverpublications.com/048648582x.html
 ).
  
 Further information about this text is available at 
  
 http://people.cs.vt.edu/˜shaffer/Book/.",NA
Contents,"Preface
  
 xiii
  
  
 I
  
 Preliminaries
  
 1
  
 1
  
 Data Structures and Algorithms
  
 3
  
 2
  
 1.1 
 A Philosophy of Data Structures 
 4 
 1.1.1 
 The Need for Data Structures 
 4 
 1.1.2 
 Costs and Benefits 
 6 
 1.2 
 Abstract Data Types and Data Structures 
 8 
 1.3 
 Design Patterns 
 12 
 1.3.1 
 Flyweight 
 13 
 1.3.2 
 Visitor 
 13 
 1.3.3 
 Composite 
 14 
 1.3.4 
 Strategy 
 15 
 1.4 
 Problems, Algorithms, and Programs 
 16 
 1.5 
 Further Reading 
 18 
 1.6 
 Exercises 
 20 
 Mathematical Preliminaries
  
 25
  
 2.1 
 Sets and Relations 
 25 
 2.2 
 Miscellaneous Notation 
 29 
 2.3 
 Logarithms 
 31 
 2.4 
 Summations and Recurrences 
 32 
 2.5 
 Recursion 
 36 
 2.6 
 Mathematical Proof Techniques 
 38 
 iii",NA
Preface,"We study data structures so that we can learn to write more efficient 
 programs. But why must programs be efficient when new computers are 
 faster every year? The reason is that our ambitions grow with our 
 capabilities. Instead of rendering efficiency needs obsolete, the modern 
 revolution in computing power and storage capability merely raises the 
 efficiency stakes as we attempt more complex tasks. 
 The quest for program efficiency need not and should not conflict with 
 sound design and clear coding. Creating efficient programs has little to do 
 with “program-ming tricks” but rather is based on good organization of 
 information and good al-gorithms. A programmer who has not mastered the 
 basic principles of clear design is not likely to write efficient programs. 
 Conversely, concerns related to develop-ment costs and maintainability 
 should not be used as an excuse to justify inefficient performance. Generality 
 in design can and should be achieved without sacrificing performance, but 
 this can only be done if the designer understands how to measure 
 performance and does so as an integral part of the design and 
 implementation pro-cess. Most computer science curricula recognize that 
 good programming skills be-gin with a strong emphasis on fundamental 
 software engineering principles. Then, once a programmer has learned the 
 principles of clear program design and imple-mentation, the next step is to 
 study the effects of data organization and algorithms on program efficiency. 
 Approach:
  This book describes many techniques for representing data. 
 These techniques are presented within the context of the following 
 principles: 
 1.
  Each data structure and each algorithm has costs and benefits. 
 Practitioners need a thorough understanding of how to assess costs 
 and benefits to be able to adapt to new design challenges. This 
 requires an understanding of the principles of algorithm analysis, and 
 also an appreciation for the significant effects of the physical medium 
 employed (e.g., data stored on disk versus main memory). 
 2.
  Related to costs and benefits is the notion of tradeoffs. For example, it 
 is quite common to reduce time requirements at the expense of an 
 increase in space requirements, or vice versa. Programmers face 
 tradeoff issues regularly in all 
 xiii",NA
PART I ,NA,NA
Preliminarie,NA,NA
s,NA,NA
1,NA,NA
Data Structures and Algorithms,"How many cities with more than 250,000 people lie within 500 miles of 
 Dallas, Texas? How many people in my company make over $100,000 per 
 year? Can we connect all of our telephone customers with less than 1,000 
 miles of cable? To answer questions like these, it is not enough to have the 
 necessary information. We must organize that information in a way that 
 allows us to find the answers in time to satisfy our needs. 
 Representing information is fundamental to computer science. The 
 primary purpose of most computer programs is not to perform calculations, 
 but to store and retrieve information — usually as fast as possible. For this 
 reason, the study of data structures and the algorithms that manipulate 
 them is at the heart of computer science. And that is what this book is about 
 — helping you to understand how to structure information to support 
 efficient processing. 
 This book has three primary goals. The first is to present the commonly 
 used data structures. These form a programmer’s basic data structure 
 “toolkit.” For many problems, some data structure in the toolkit provides a 
 good solution. 
 The second goal is to introduce the idea of tradeoffs and reinforce the 
 concept that there are costs and benefits associated with every data 
 structure. This is done by describing, for each data structure, the amount of 
 space and time required for typical operations. 
 The third goal is to teach how to measure the effectiveness of a data 
 structure or algorithm. Only through such measurement can you determine 
 which data structure in your toolkit is most appropriate for a new problem. 
 The techniques presented also allow you to judge the merits of new data 
 structures that you or others might invent. 
 There are often many approaches to solving a problem. How do we 
 choose between them? At the heart of computer program design are two 
 (sometimes con-flicting) goals: 
 1.
  To design an algorithm that is easy to understand, code, and debug. 
 2.
  To design an algorithm that makes efficient use of the computer’s resources. 
 3",NA
1.1 ,NA,NA
A Philosophy of Data Structures,"1.1.1 The Need for Data Structures
  
 You might think that with ever more powerful computers, program 
 efficiency is becoming less important. After all, processor speed and memory 
 size still con-tinue to improve. Won’t any efficiency problem we might have 
 today be solved by tomorrow’s hardware? 
 As we develop more powerful computers, our history so far has always 
 been to use that additional computing power to tackle more complex 
 problems, be it in the form of more sophisticated user interfaces, bigger 
 problem sizes, or new problems previously deemed computationally 
 infeasible. More complex problems demand more computation, making the 
 need for efficient programs even greater. Worse yet, as tasks become more 
 complex, they become less like our everyday experience. Today’s computer 
 scientists must be trained to have a thorough understanding of the 
 principles behind efficient program design, because their ordinary life 
 experiences often do not apply when designing computer programs. 
 In the most general sense, a data structure is any data representation 
 and its associated operations. Even an integer or floating point number 
 stored on the com-puter can be viewed as a simple data structure. More 
 commonly, people use the term “data structure” to mean an organization or",NA
1.2,NA,NA
Abstract Data Types and Data Structures,"The previous section used the terms “data item” and “data structure” 
 without prop-erly defining them. This section presents terminology and 
 motivates the design process embodied in the three-step approach to 
 selecting a data structure. This mo-tivation stems from the need to manage 
 the tremendous complexity of computer programs. 
 A
  type
  is a collection of values. For example, the Boolean type consists of 
 the values
  true
  and
  false
 . The integers also form a type. An integer is a
  
 simple type
  because its values contain no subparts. A bank account record 
 will typically contain several pieces of information such as name, address, 
 account number, and account balance. Such a record is an example of an
  
 aggregate type
  or
  composite type
 . A
  data item
  is a piece of information or 
 a record whose value is drawn from a type. A data item is said to be a
  
 member
  of a type. 
 A
  data type
  is a type together with a collection of operations to 
 manipulate the type. For example, an integer variable is a member of the 
 integer data type. Addition is an example of an operation on the integer data 
 type. 
 A distinction should be made between the logical concept of a data type 
 and its physical implementation in a computer program. For example, there 
 are two tra-ditional implementations for the list data type: the linked list 
 and the array-based list. The list data type can therefore be implemented 
 using a linked list or an ar-ray. Even the term “array” is ambiguous in that it 
 can refer either to a data type or an implementation. “Array” is commonly 
 used in computer programming to mean a contiguous block of memory 
 locations, where each memory location stores one fixed-length data item. By 
 this meaning, an array is a physical data structure. However, array can also 
 mean a logical data type composed of a (typically ho-mogeneous) collection",NA
1.3,NA,NA
Design Patterns,"At a higher level of abstraction than ADTs are abstractions for describing the 
 design of programs — that is, the interactions of objects and classes. 
 Experienced software designers learn and reuse patterns for combining 
 software components. These have come to be referred to as
  design 
 patterns
 . 
 A design pattern embodies and generalizes important design concepts 
 for a recurring problem. A primary goal of design patterns is to quickly 
 transfer the knowledge gained by expert designers to newer programmers. 
 Another goal is to allow for efficient communication between programmers. 
 It is much easier to discuss a design issue when you share a technical 
 vocabulary relevant to the topic. 
  
 Specific design patterns emerge from the realization that a particular 
 design problem appears repeatedly in many contexts. They are meant to 
 solve real prob-lems. Design patterns are a bit like templates.  
 They 
 describe the structure for a design solution, with the details filled in for any",NA
1.4 ,NA,NA
"Problems, Algorithms, and Programs","Programmers commonly deal with problems, algorithms, and computer 
 programs. These are three distinct concepts. 
 Problems:
  As your intuition would suggest, a
  problem
  is a task to be 
 performed. It is best thought of in terms of inputs and matching outputs. A 
 problem definition should not include any constraints on
  how
  the problem is 
 to be solved. The solution method should be developed only after the 
 problem is precisely defined and thor-oughly understood. However, a 
 problem definition should include constraints on the resources that may be 
 consumed by any acceptable solution. For any problem to be solved by a 
 computer, there are always such constraints, whether stated or implied. For 
 example, any computer program may use only the main memory and disk 
 space available, and it must run in a “reasonable” amount of time. 
 Problems can be viewed as functions in the mathematical sense. A
  
 function 
 is a matching between inputs (the
  domain
 ) and outputs (the
  
 range
 ). An input to a function might be a single value or a collection of 
 information. The values making up an input are called the
  parameters
  of 
 the function. A specific selection of values for the parameters is called an
  
 instance
  of the problem. For example, the input parameter to a sorting 
 function might be an array of integers. A particular array of integers, with a 
 given size and specific values for each position in the array, would be an 
 instance of the sorting problem. Different instances might generate the same 
 output. However, any problem instance must always result in the same 
 output every time the function is computed using that particular input. 
 This concept of all problems behaving like mathematical functions might 
 not match your intuition for the behavior of computer programs. You might 
 know of programs to which you can give the same input value on two 
 separate occasions, and two different outputs will result. For example, if you 
 type “
 date
 ” to a typical UNIX command line prompt, you will get the current 
 date. Naturally the date will be different on different days, even though the 
 same command is given. However, there is obviously more to the input for 
 the date program than the command that you type to run the program. The 
 date program computes a function. In other words, on any particular day 
 there can only be a single answer returned by a properly running date 
 program on a completely specified input. For all computer programs, the 
 output is completely determined by the program’s full set of inputs. Even 
 a“random number generator” is completely determined by its inputs 
 (although some random number generating systems appear to get around",NA
1.5 ,NA,NA
Further Reading,"An early authoritative work on data structures and algorithms was the 
 series of books
  The Art of Computer Programming
  by Donald E. Knuth, with 
 Volumes 1 and 3 being most relevant to the study of data structures [Knu97, 
 Knu98]. A mod-ern encyclopedic approach to data structures and algorithms 
 that should be easy to understand once you have mastered this book is
  
 Algorithms
  by Robert Sedge-wick [Sed11]. For an excellent and highly 
 readable (but more advanced) teaching introduction to algorithms, their 
 design, and their analysis, see
  Introduction to Al-gorithms: A Creative 
 Approach
  by Udi Manber [Man89]. For an advanced, en-cyclopedic approach, 
 see
  Introduction to Algorithms
  by Cormen, Leiserson, and Rivest [CLRS09]. 
 Steven S. Skiena’s
  The Algorithm Design Manual
  [Ski10] pro-vides pointers to",NA
1.6 ,NA,NA
Exercises,"The exercises for this chapter are different from those in the rest of the 
 book. Most of these exercises are answered in the following chapters. 
 However, you should 
 not
  look up the answers in other parts of the book. 
 These exercises are intended to make you think about some of the issues to 
 be covered later on. Answer them to the best of your ability with your 
 current knowledge. 
 1.1
  Think of a program you have used that is unacceptably slow. Identify 
 the spe-cific operations that make the program slow. Identify other 
 basic operations that the program performs quickly enough. 
 1.2
  Most programming languages have a built-in integer data type. 
 Normally this representation has a fixed size, thus placing a limit on 
 how large a value can be stored in an integer variable. Describe a 
 representation for integers that has no size restriction (other than the 
 limits of the computer’s available main memory), and thus no practical 
 limit on how large an integer can be stored. Briefly show how your 
 representation can be used to implement the operations of addition, 
 multiplication, and exponentiation. 
 1.3
  Define an ADT for character strings. Your ADT should consist of typical 
 functions that can be performed on strings, with each function defined 
 in terms of its input and output. Then define two different physical 
 representa-tions for strings. 
 1.4
  Define an ADT for a list of integers. First, decide what functionality 
 your ADT should provide. Example 1.4 should give you some ideas. 
 Then, spec-ify your ADT in
  C
 ++
 in the form of an abstract class 
 declaration, showing the functions, their parameters, and their return 
 types. 
 1.5
  Briefly describe how integer variables are typically represented on a 
 com-puter. (Look up one’s complement and two’s complement",NA
2,NA,NA
Mathematical Preliminaries,"This chapter presents mathematical notation, background, and techniques 
 used throughout the book. This material is provided primarily for review 
 and reference. You might wish to return to the relevant sections when you 
 encounter unfamiliar notation or mathematical techniques in later chapters. 
 Section 2.7 on estimation might be unfamiliar to many readers. 
 Estimation is not a mathematical technique, but rather a general engineering 
 skill. It is enor-mously useful to computer scientists doing design work, 
 because any proposed solution whose estimated resource requirements fall 
 well outside the problem’s re-source constraints can be discarded 
 immediately, allowing time for greater analysis of more promising solutions.",NA
2.1 ,NA,NA
Sets and Relations,"The concept of a set in the mathematical sense has wide application in 
 computer science. The notations and techniques of set theory are commonly 
 used when de-scribing and implementing algorithms because the 
 abstractions associated with sets often help to clarify and simplify algorithm 
 design. 
 A
  set
  is a collection of distinguishable
  members
  or
  elements
 . The 
 members are typically drawn from some larger population known as the
  
 base type
 . Each member of a set is either a
  primitive element
  of the base 
 type or is a set itself. There is no concept of duplication in a set. Each value 
 from the base type is either in the set or not in the set. For example, a set 
 named
  P
  might consist of the three integers 7, 11, and 42. In this case,
  P
 ’s 
 members are 7, 11, and 42, and the base type is integer. 
 Figure 2.1 shows the symbols commonly used to express sets and their 
 rela-tionships. Here are some examples of this notation in use. First define 
 two sets,
  P 
 and
  Q
 . 
 P
  =
  {
 2
 ,
  3
 ,
  5
 }, 
 Q
  =
  {
 5
 ,
  10
 }.
  
 25",NA
2.2,NA,NA
Miscellaneous Notation,"Units of measure:
  I use the following notation for units of measure. “B” will 
 be used as an abbreviation for bytes, “b” for bits, “KB” for kilobytes (2
 10
 = 
 1024 bytes), “MB” for megabytes (2
 20
 bytes), “GB” for gigabytes (2
 30
 bytes), 
 and“ms” for milliseconds (a millisecond is 
 1000
 of a second). Spaces are not 
 placed be-tween the number and the unit abbreviation when a power of two 
 is intended. Thus a disk drive of size 25 gigabytes (where a gigabyte is 
 intended as 2
 30
 bytes) will be written as “25GB.” Spaces are used when a 
 decimal value is intended. An amount of 2000 bits would therefore be 
 written “2 Kb” while “2Kb” represents 2048 bits. 2000 milliseconds is 
 written as 2000 ms. Note that in this book large amounts of storage are 
 nearly always measured in powers of two and times in powers of ten. 
 Factorial function:
  The
  factorial
  function, written
  n
 ! for
  n
  an integer 
 greater than 0, is the product of the integers between 1 and
  n
 , inclusive. 
 Thus, 5! = 
 1
  ·
  2
  ·
  3
  ·
  4
  ·
  5 = 120. As a special case, 0! = 1. The factorial function grows quickly as
  n
  
 becomes larger. Because computing the factorial function directly 
 is a time-consuming process, it can be useful to have an equation that provides a 
 good approximation. Stirling’s approximation states that
  n
 !
  ≈√
 2
 πn
 (
 n e
 )
 n
 , where 
 e ≈
  2
 .
 71828 (
 e
  is the base for the system of natural logarithms).
 3
 Thus we see that while
  
 n
 ! grows slower than
  n
 n
 (because
 √
 2
 πn/e
 n
 <
  1), it grows faster than
  c
 n
 for 
 any positive integer constant
  c
 . 
 Permutations:
  A
  permutation
  of a sequence
  S
  is simply the members of
  S
  
 ar-ranged in some order. For example, a permutation of the integers 1 
 through
  n 
 would be those values arranged in some order. If the sequence 
 contains
  n
  distinct members, then there are
  n
 ! different permutations for the 
 sequence. This is because there are
  n
  choices for the first member in the 
 permutation; for each choice of first member there are
  n −
  1 choices for the 
 second member, and so on. Sometimes one would like to obtain a
  random 
 permutation
  for a sequence, that is, one of the 
 n
 ! possible permutations is 
 selected in such a way that each permutation has equal probability of being",NA
2.3 ,NA,NA
Logarithms,"A
  logarithm
  of base
  b
  for value
  y
  is the power to which
  b
  is raised to get
  y
 . 
 Nor-mally, this is written as log
 b
  y
  =
  x
 . Thus, if log
 b
  y
  =
  x
  then
  b
 x
 =
  y
 , and
  b
 log
 b
 y
 =
  
 y
 . 
 Logarithms are used frequently by programmers. Here are two typical uses. 
 Example 2.6
  Many programs require an encoding for a collection of 
 ob-jects. What is the minimum number of bits needed to represent
  n
  
 distinct 
 code values? The answer is
  ∈
 log
 2
  n∈
  bits. For example, if you have 
 1000 codes to store, you will require at least
  ∈
 log
 2
  1000
 ∈
  = 10 bits to 
 have 1000 different codes (10 bits provide 1024 distinct code 
 values). 
 Example 2.7
  Consider the binary search algorithm for finding a 
 given value within an array sorted by value from lowest to highest. 
 Binary search first looks at the middle element and determines if the 
 value being searched for is in the upper half or the lower half of the 
 array. The algorithm then continues splitting the appropriate 
 subarray in half until the desired value is found. (Binary search is 
 described in more detail in Section 3.5.) How many times can an 
 array of size
  n
  be split in half until only one element remains in the 
 final subarray? The answer is
  ∈
 log
 2
  n∈
  times. 
 In this book, nearly all logarithms used have a base of two. This is 
 because data structures and algorithms most often divide things in half, or 
 store codes with binary bits. Whenever you see the notation log
  n
  in this 
 book, either log
 2
  n
  is meant or else the term is being used asymptotically and 
 so the actual base does not matter. Logarithms using any base other than 
 two will show the base explicitly. 
  
 Logarithms have the following properties, for any positive values of
  m
 ,
  n
 , 
 and 
 r
 , and any positive integers
  a
  and
  b
 . 
 1.
  log(
 nm
 ) = log
  n
  + log
  m
 .  
 2.
  log(
 n/m
 ) = log
  n −
  log
  m
 .  
 3.
  log(
 n
 r
 ) =
  r
  log
  n
 .",NA
2.4 ,NA,NA
Summations and Recurrences,"Most programs contain loop constructs. When analyzing running time costs 
 for programs with loops, we need to add up the costs for each time the loop 
 is executed. This is an example of a
  summation
 . Summations are simply the 
 sum of costs for some function applied to a range of parameter values. 
 Summations are typically written with the following “Sigma” notation: 
 n
  
 f
 (
 i
 )
 .
  
 i
 =1
  
 This notation indicates that we are summing the value of
  f
 (
 i
 ) over some 
 range of (integer) values. The parameter to the expression and its initial 
 value are indicated below thesymbol. Here, the notation
  i
  = 1 indicates that 
 the parameter is
  i
  and that it begins with the value 1. At the top of thesymbol 
 is the expression
  n
 . This indicates the maximum value for the parameter
  i
 . 
 Thus, this notation means to sum the values of
  f
 (
 i
 ) as
  i
  ranges across the 
 integers from 1 through
  n
 . This can also be 
 4
 These properties are the idea behind the slide rule. Adding two numbers can be viewed 
 as joining two lengths together and measuring their combined length. Multiplication is not so 
 easily done. However, if the numbers are first converted to the lengths of their logarithms, 
 then those lengths can be added and the inverse logarithm of the resulting length gives the 
 answer for the multiplication (this is simply logarithm property (1)). A slide rule measures 
 the length of the logarithm for the numbers, lets you slide bars representing these lengths to",NA
Recursion,Chap. 2 Mathematical Preliminaries,NA
2.5,"An algorithm is
  recursive
  if it calls itself to do part of its work. For this 
 approach to be successful, the “call to itself” must be on a smaller problem 
 then the one originally attempted. In general, a recursive algorithm must 
 have two parts: the 
 base case
 , which handles a simple input that can be 
 solved without resorting to a recursive call, and the recursive part which 
 contains one or more recursive calls to the algorithm where the parameters 
 are in some sense “closer” to the base case than those of the original call. 
 Here is a recursive
  C
 ++
 function to compute the factorial of
  n
 . A trace of
  fact
 ’s 
 execution for a small value of
  n
  is presented in Section 4.2.4. 
 long fact(int n) { 
  
 // Compute n! recursively 
  
  
 // To fit n! into a long variable, we require n <= 12 
  
 Assert((n >= 0) 
 && (n <= 12), ""Input out of range""); 
  
 if (n <= 1) return 1; // Base case: 
 return base solution 
  
 return n * fact(n-1); 
  
 // Recursive call 
 for n > 1 
  
 }
  
 of the base cases computes a solution for the problem. If
  n >
  1, then
  fact
  calls The first 
 two lines of the function constitute the base cases. If
  n ≤
  1, then one 
 a function that knows how to find the factorial of
  n −
  1. Of course, the 
 function that knows how to compute the factorial of
  n −
  1 happens to be
  fact
  
 itself. But we should not think too hard about this while writing the 
 algorithm. The design for recursive algorithms can always be approached in 
 this way. First write the base cases. Then think about solving the problem by 
 combining the results of one or more smaller — but similar — subproblems. 
 If the algorithm you write is correct, then certainly you can rely on it 
 (recursively) to solve the smaller subproblems. The secret to success is: Do 
 not worry about
  how
  the recursive call solves the subproblem. Simply accept 
 that it
  will
  solve it correctly, and use this result to in turn correctly solve the 
 original problem. What could be simpler? 
 Recursion has no counterpart in everyday, physical-world problem 
 solving. The concept can be difficult to grasp because it requires you to think 
 about problems in a new way. To use recursion effectively, it is necessary to 
 train yourself to stop analyzing the recursive process beyond the recursive 
 call. The subproblems will take care of themselves. You just worry about the 
 base cases and how to recombine the subproblems. 
 The recursive version of the factorial function might seem unnecessarily 
 com-plicated to you because the same effect can be achieved by using a
  
 while
  loop. Here is another example of recursion, based on a famous puzzle 
 called “Towers of Hanoi.” The natural algorithm to solve this problem has 
 multiple recursive calls. It cannot be rewritten easily using
  while
  loops.",NA
2.6 ,NA,NA
Mathematical Proof Techniques,"Solving any problem has two distinct parts: the investigation and the 
 argument. Students are too used to seeing only the argument in their 
 textbooks and lectures. But to be successful in school (and in life after 
 school), one needs to be good at both, and to understand the differences 
 between these two phases of the process. To solve the problem, you must 
 investigate successfully. That means engaging the problem, and working 
 through until you find a solution. Then, to give the answer to your client 
 (whether that “client” be your instructor when writing answers on a 
 homework assignment or exam, or a written report to your boss), you need 
 to be able to make the argument in a way that gets the solution across 
 clearly and succinctly. The argument phase involves good technical writing 
 skills — the ability to make a clear, logical argument. 
 Being conversant with standard proof techniques can help you in this 
 process. Knowing how to write a good proof helps in many ways. First, it 
 clarifies your thought process, which in turn clarifies your explanations.",NA
2.7 ,NA,NA
Estimation,"One of the most useful life skills that you can gain from your computer 
 science training is the ability to perform quick estimates. This is sometimes 
 known as “back of the napkin” or “back of the envelope” calculation. Both 
 nicknames suggest that only a rough estimate is produced. Estimation 
 techniques are a standard part of engineering curricula but are often 
 neglected in computer science. Estimation is no substitute for rigorous, 
 detailed analysis of a problem, but it can serve to indicate when a rigorous 
 analysis is warranted: If the initial estimate indicates that the solution is 
 unworkable, then further analysis is probably unnecessary. 
 Estimation can be formalized by the following three-step process: 
 1.
  Determine the major parameters that affect the problem. 
 2.
  Derive an equation that relates the parameters to the problem. 
 3.
  Select values for the parameters, and apply the equation to yield an 
 estimated  
 solution. 
 When doing estimations, a good way to reassure yourself that the 
 estimate is reasonable is to do it in two different ways. In general, if you 
 want to know what comes out of a system, you can either try to estimate that 
 directly, or you can estimate what goes into the system (assuming that what 
 goes in must later come out). If both approaches (independently) give 
 similar answers, then this should build confidence in the estimate. 
 When calculating, be sure that your units match. For example, do not add 
 feet and pounds. Verify that the result is in the correct units. Always keep in 
 mind that the output of a calculation is only as good as its input. The more 
 uncertain your valuation for the input parameters in Step 3, the more 
 uncertain the output value. However, back of the envelope calculations are 
 often meant only to get an answer within an order of magnitude, or perhaps 
 within a factor of two. Before doing an estimate, you should decide on 
 acceptable error bounds, such as within 25%, within a factor of two, and so 
 forth. Once you are confident that an estimate falls within your error 
 bounds, leave it alone! Do not try to get a more precise estimate than 
 necessary for your purpose. 
 Example 2.18
  How many library bookcases does it take to store 
 books containing one million pages? I estimate that a 500-page book 
 requires one inch on the library shelf (it will help to look at the size of 
 any handy book), yielding about 200 feet of shelf space for one 
 million pages. If a shelf is 4 feet wide, then 50 shelves are required. If 
 a bookcase contains",NA
2.8,NA,NA
Further Reading,"Most of the topics covered in this chapter are considered part of Discrete 
 Math-ematics. An introduction to this field is
  Discrete Mathematics with 
 Applications 
 by Susanna S. Epp [Epp10]. An advanced treatment of many 
 mathematical topics useful to computer scientists is
  Concrete Mathematics: A 
 Foundation for Computer Science
  by Graham, Knuth, and Patashnik [GKP94]. 
 See “Technically Speaking” from the February 1995 issue of
  IEEE 
 Spectrum 
 [Sel95] for a discussion on the standard for indicating units of 
 computer storage used in this book.",NA
2.9 ,NA,NA
Exercises,"2.1
  For each relation below, explain why the relation does or does not 
 satisfy  each of the properties reflexive, symmetric, antisymmetric, and 
 transitive. 
 (a)
  “isBrotherOf” on the set of people. 
 (b)
  “isFatherOf” on the set of people. 
 (c)
  The relation
  R
  =
  {∈x, y∈ | x
 2
 +
  y
 2
 = 1
 }
  for real numbers
  x
  and
  y
 . 
 (d)
  The relation
  R
  =
  {∈x, y∈ | x
 2
 =
  y
 2
 }
  for real numbers
  x
  and
  y
 .  
 (e)
  The relation
  R
  =
  {∈x, y∈ | x
  mod
  y
  = 0
 }
  for
  x, y ∈ {
 1
 ,
  2
 ,
  3
 ,
  4
 }
 . 
 (f)
  The 
 empty relation
  ∈
  (i.e., the relation with no ordered pairs for which it 
 is true) on the set of integers. 
 it is true) on the empty set. 
 (g)
  The empty relation
  ∈
  (i.e., the relation with no 
 ordered pairs for which 
 2.2
  For each of the following relations, either prove that it is an 
 equivalence  
 relation or prove that it is not an equivalence relation. 
 (a)
  For integers
  a
  and
  b
 ,
  a ≡ b
  if and only if
  a
  +
  b
  is even. 
 (b)
  For integers
  a
  and
  b
 ,
  a ≡ b
  if and only if
  a
  +
  b
  is odd.",NA
3,NA,NA
Algorithm Analysis,"How long will it take to process the company payroll once we complete our 
 planned merger? Should I buy a new payroll program from vendor X or 
 vendor Y? If a particular program is slow, is it badly implemented or is it 
 solving a hard problem? Questions like these ask us to consider the difficulty 
 of a problem, or the relative efficiency of two or more approaches to solving 
 a problem. 
 This chapter introduces the motivation, basic notation, and fundamental 
 tech-niques of algorithm analysis. We focus on a methodology known as
  
 asymptotic 
 algorithm 
 analysis
 , 
 or 
 simply
  
 asymptotic 
 analysis
 . 
 Asymptotic analysis attempts to estimate the resource consumption of an 
 algorithm. It allows us to compare the relative costs of two or more 
 algorithms for solving the same problem. Asymptotic analysis also gives 
 algorithm designers a tool for estimating whether a proposed solution is 
 likely to meet the resource constraints for a problem before they imple-ment 
 an actual program. After reading this chapter, you should understand 
 • the concept of a growth rate, the rate at which the cost of an algorithm 
 grows  as the size of its input grows; 
 • the concept of upper and lower bounds for a growth rate, and how to 
 estimate  
 these bounds for a simple program, algorithm, or 
 problem; and 
 • the difference between the cost of an algorithm (or program) and the 
 cost of  a problem. 
 The chapter concludes with a brief discussion of the practical difficulties 
 encoun-tered when empirically measuring the cost of a program, and some 
 principles for code tuning to improve program efficiency.",NA
3.1 ,NA,NA
Introduction,"How do you compare two algorithms for solving some problem in terms of 
 effi-ciency? We could implement both algorithms as computer programs and 
 then run",NA
3.2 ,NA,NA
"Best, Worst, and Average Cases","Consider the problem of finding the factorial of
  n
 . For this problem, there is 
 only one input of a given “size” (that is, there is only a single instance for 
 each size of 
 n
 ). Now consider our largest-value sequential search algorithm 
 of Example 3.1, which always examines every array value. This algorithm 
 works on many inputs of a given size
  n
 . That is, there are many possible 
 arrays of any given size. However, no matter what array of size
  n
  that the 
 algorithm looks at, its cost will always be the same in that it always looks at 
 every element in the array one time. 
 For some algorithms, different inputs of a given size require different 
 amounts of time. For example, consider the problem of searching an array 
 containing
  n 
 integers to find the one with a particular value
  K
  (assume that
  K
  
 appears exactly once in the array). The
  sequential search
  algorithm begins 
 at the first position in the array and looks at each value in turn until
  K
  is 
 found. Once
  K
  is found, the algorithm stops. This is different from the 
 largest-value sequential search algorithm of Example 3.1, which always 
 examines every array value. 
 There is a wide range of possible running times for the sequential search 
 alg-orithm. The first integer in the array could have value
  K
 , and so only one 
 integer is examined. In this case the running time is short. This is the
  best 
 case
  for this algorithm, because it is not possible for sequential search to 
 look at less than one value. Alternatively, if the last position in the array 
 contains
  K
 , then the running time is relatively long, because the algorithm 
 must examine
  n
  values. This is the 
 worst case
  for this algorithm, because 
 sequential search never looks at more than 
 n
  values. If we implement 
 sequential search as a program and run it many times on many different 
 arrays of size
  n
 , or search for many different values of
  K
  within the same 
 array, we expect the algorithm on average to go halfway through the array 
 before finding the value we seek. On average, the algorithm examines about
  
 n/
 2 values. We call this the
  average case
  for this algorithm. 
 When analyzing an algorithm, should we study the best, worst, or 
 average case? Normally we are not interested in the best case, because this 
 might happen only rarely and generally is too optimistic for a fair 
 characterization of the algorithm’s running time. In other words, analysis 
 based on the best case is not likely to be representative of the behavior of 
 the algorithm. However, there are rare instances where a best-case analysis 
 is useful — in particular, when the best case has high probability of 
 occurring. In Chapter 7 you will see some examples where taking advantage 
 of the best-case running time for one sorting algorithm makes a second 
 more efficient. 
 How about the worst case? The advantage to analyzing the worst case is 
 that you know for certain that the algorithm must perform at least that well. 
 This is es-pecially important for real-time applications, such as for the",NA
3.3 ,NA,NA
"A Faster Computer, or a Faster Algorithm?","Imagine that you have a problem to solve, and you know of an algorithm 
 whose running time is proportional to
  n
 2
 . Unfortunately, the resulting 
 program takes ten times too long to run. If you replace your current 
 computer with a new one that is ten times faster, will the
  n
 2
 algorithm 
 become acceptable? If the problem size remains the same, then perhaps the 
 faster computer will allow you to get your work done quickly enough even 
 with an algorithm having a high growth rate. But a funny thing happens to 
 most people who get a faster computer. They don’t run the same problem 
 faster. They run a bigger problem! Say that on your old computer you were 
 content to sort 10,000 records because that could be done by the computer 
 during your lunch break. On your new computer you might hope to sort 
 100,000 records in the same time. You won’t be back from lunch any sooner,",NA
3.4 ,NA,NA
Asymptotic Analysis,"Despite the larger constant for the curve labeled 10
 n
  in Figure 3.1, 
 2
 n
 2
 crosses it at the relatively small value of
  n
  = 5. What if we double the 
 value of the constant in front of the linear equation? As shown in the graph, 
 20
 n
  is surpassed by 2
 n
 2
 once
  n
  = 10. The additional factor of two for the 
 linear growth rate does not much matter. It only doubles the
  x
 -coordinate 
 for the intersection point. In general, changes to a constant factor in either 
 equation only shift
  where
  the two curves cross, not
  whether
  the two curves 
 cross. 
 When you buy a faster computer or a faster compiler, the new problem 
 size that can be run in a given amount of time for a given growth rate is 
 larger by the same factor, regardless of the constant on the running-time 
 equation. The time curves for two algorithms with different growth rates 
 still cross, regardless of their running-time equation constants. For these 
 reasons, we usually ignore the con-stants when we want an estimate of the 
 growth rate for the running time or other resource requirements of an 
 algorithm. This simplifies the analysis and keeps us thinking about the most 
 important aspect: the growth rate. This is called
  asymp-totic algorithm 
 analysis
 . To be precise, asymptotic analysis refers to the study of an 
 algorithm as the input size “gets big” or reaches a limit (in the calculus 
 sense). However, it has proved to be so useful to ignore all constant factors 
 that asymptotic analysis is used for most algorithm comparisons. 
 It is not always reasonable to ignore the constants. When comparing 
 algorithms meant to run on small values of
  n
 , the constant can have a large 
 effect. For exam-ple, if the problem is to sort a collection of exactly five 
 records, then an algorithm designed for sorting thousands of records is 
 probably not appropriate, even if its asymptotic analysis indicates good 
 performance. There are rare cases where the constants for two algorithms 
 under comparison can differ by a factor of 1000 or more, making the one 
 with lower growth rate impractical for most purposes due to its large 
 constant. Asymptotic analysis is a form of “back of the envelope” esti-mation 
 for algorithm resource consumption. It provides a simplified model of the 
 running time or other resource needs of an algorithm. This simplification 
 usually helps you understand the behavior of your algorithms. Just be aware 
 of the limi-tations to asymptotic analysis in the rare situation where the 
 constant is important. 
 3.4.1 Upper Bounds
  
 Several terms are used to describe the running-time equation for an 
 algorithm. These terms — and their associated symbols — indicate precisely 
 what aspect of the algorithm’s behavior is being described. One is the
  upper",NA
3.5,NA,NA
Calculating the Running Time for a Program,"This section presents the analysis for several simple code fragments. 
 Example 3.9
  We begin with an analysis of a simple assignment to an 
 integer variable. 
 a = b;
  
 Because the assignment statement takes constant time, it is Θ(1). 
 Example 3.10
  Consider a simple
  for
  loop. 
 sum = 0; 
  
 for (i=1; i<=n; i++) 
  
  
 sum += n;
  
 The first line is Θ(1). The
  for
  loop is repeated
  n
  times. The third 
 line takes constant time so, by simplifying rule (4) of Section 3.4.4, 
 the total cost for executing the two lines making up the
  for
  loop is 
 Θ(
 n
 ). By rule (3), the cost of the entire code fragment is also Θ(
 n
 ). 
 Example 3.11
  We now analyze a code fragment with several
  for
  
 loops, some of which are nested. 
 sum = 0; 
  
 for (i=1; i<=n; i++) 
  
  
 for (j=1; j<=i; j++) 
  
  
 sum++; 
  
 for (k=0; k<n; k++) 
  
  
 A[k] = k;
  
 // First for loop 
  
 // 
  
 is a double loop
  
 // Second for loop
  
 This code fragment has three separate statements: the first 
 assignment statement and the two
  for
  loops. Again the assignment 
 statement takes constant time; call it
  c
 1
 . The second
  for
  loop is just 
 like the one in Exam-ple 3.10 and takes
  c
 2
 n
  = Θ(
 n
 ) time. 
 The first
  for
  loop is a double loop and requires a special 
 technique. We work from the inside of the loop outward. The 
 expression
  sum++
  requires constant time; call it
  c
 3
 . Because the 
 inner
  for
  loop is executed
  i
  times, by",NA
Analyzing Problems,Chap. 3 Algorithm Analysis,NA
3.6,"You most often use the techniques of “algorithm” analysis to analyze an 
 algorithm, or the instantiation of an algorithm as a program. You can also 
 use these same techniques to analyze the cost of a problem. It should make 
 sense to you to say that the upper bound for a problem cannot be worse 
 than the upper bound for the best algorithm that we know for that problem. 
 But what does it mean to give a lower bound for a problem? 
 Consider a graph of cost over all inputs of a given size
  n
  for some 
 algorithm for a given problem. Define
  A
  to be the collection of all algorithms 
 that solve the problem (theoretically, there are an infinite number of such 
 algorithms). Now, consider the collection of all the graphs for all of the 
 (infinitely many) algorithms in
  A
 . The worst case lower bound is the
  least
  of 
 all the
  highest
  points on all the graphs. 
 It is much easier to show that an algorithm (or program) is in Ω(
 f
 (
 n
 )) 
 than it is to show that a problem is in Ω(
 f
 (
 n
 )). For a problem to be in Ω(
 f
 (
 n
 )) 
 means that
  every
  algorithm that solves the problem is in Ω(
 f
 (
 n
 )), even 
 algorithms that we have not thought of! 
 So far all of our examples of algorithm analysis give “obvious” results, 
 with big-Oh always matching Ω. To understand how big-Oh, Ω, and Θ 
 notations are properly used to describe our understanding of a problem or 
 an algorithm, it is best to consider an example where you do not already 
 know a lot about the problem. 
 Let us look ahead to analyzing the problem of sorting to see how this 
 process works. What is the least possible cost for any sorting algorithm in 
 the worst case? The algorithm must at least look at every element in the 
 input, just to determine that the input is truly sorted. Thus, any sorting 
 algorithm must take at least
  cn
  time. For many problems, this observation 
 that each of the
  n
  inputs must be looked at leads to an easy Ω(
 n
 ) lower 
 bound. 
 In your previous study of computer science, you have probably seen an 
 example of a sorting algorithm whose running time is in O(
 n
 2
 ) in the worst 
 case. The simple Bubble Sort and Insertion Sort algorithms typically given as 
 examples in a first year programming course have worst case running times 
 in O(
 n
 2
 ). Thus, the problem of sorting can be said to have an upper bound in 
 O(
 n
 2
 ). How do we close the gap between Ω(
 n
 ) and O(
 n
 2
 )? Can there be a 
 better sorting algorithm? If you can think of no algorithm whose worst-case 
 growth rate is better than O(
 n
 2
 ), and if you have discovered no analysis 
 technique to show that the least cost for the problem of sorting in the worst 
 case is greater than Ω(
 n
 ), then you cannot know for sure whether or not 
 there is a better algorithm.",NA
3.7 ,NA,NA
Common Misunderstandings,"Asymptotic analysis is one of the most intellectually difficult topics that 
 undergrad-uate computer science majors are confronted with. Most people 
 find growth rates and asymptotic analysis confusing and so develop 
 misconceptions about either the concepts or the terminology. It helps to 
 know what the standard points of confusion are, in hopes of avoiding them. 
 One problem with differentiating the concepts of upper and lower 
 bounds is that, for most algorithms that you will encounter, it is easy to 
 recognize the true growth rate for that algorithm. Given complete 
 knowledge about a cost function, the upper and lower bound for that cost 
 function are always the same. Thus, the distinction between an upper and a 
 lower bound is only worthwhile when you have incomplete knowledge 
 about the thing being measured. If this distinction is still not clear, reread 
 Section 3.6. We use Θ-notation to indicate that there is no meaningful 
 difference between what we know about the growth rates of the upper and 
 lower bound (which is usually the case for simple algorithms). 
 It is a common mistake to confuse the concepts of upper bound or lower 
 bound on the one hand, and worst case or best case on the other. The best, 
 worst, or average cases each give us a concrete input instance (or concrete 
 set of instances) that we can apply to an algorithm description to get a cost 
 measure. The upper and lower bounds describe our understanding of the
  
 growth rate
  for that cost measure. So to define the growth rate for an 
 algorithm or problem, we need to determine what we are measuring (the 
 best, worst, or average case) and also our description for what we know 
 about the growth rate of that cost measure (big-Oh, Ω, or Θ). 
 The upper bound for an algorithm is not the same as the worst case for 
 that algorithm for a given input of size
  n
 . What is being bounded is not the 
 actual cost (which you can determine for a given value of
  n
 ), but rather the
  
 growth rate
  for the",NA
3.8,NA,NA
Multiple Parameters,"Sometimes the proper analysis for an algorithm requires multiple 
 parameters to de-scribe the cost. To illustrate the concept, consider an 
 algorithm to compute the rank ordering for counts of all pixel values in a 
 picture. Pictures are often represented by a two-dimensional array, and a 
 pixel is one cell in the array. The value of a pixel is either the code value for 
 the color, or a value for the intensity of the picture at that pixel. Assume that 
 each pixel can take any integer value in the range 0 to
  C −
  1. The problem is 
 to find the number of pixels of each color value and then sort the color 
 values with respect to the number of times each value appears in the picture. 
 Assume that the picture is a rectangle with
  P
  pixels. A pseudocode algorithm 
 to solve the problem follows. 
 for (i=0; i<C; i++) 
  
 // Initialize count 
  
  
 count[i] = 0; 
  
 for (i=0; i<P; i++) 
  
 // Look at all of the pixels 
  
 count[value(i)]++; // Increment a pixel value count sort(count, C); 
  
 // Sort pixel value counts
  
 In this example,
  count
  is an array of size
  C
  that stores the number of pixels 
 for each color value. Function
  value(i)
  returns the color value for pixel
  i
 . 
 The time for the first
  for
  loop (which initializes
  count
 ) is based on the 
 num-ber of colors,
  C
 . The time for the second loop (which determines the 
 number of pixels with each color) is Θ(
 P
 ). The time for the final line, the call 
 to
  sort
 , de-pends on the cost of the sorting algorithm used. From the 
 discussion of Section 3.6, we can assume that the sorting algorithm has cost",NA
3.9 ,NA,NA
Space Bounds,"Besides time, space is the other computing resource that is commonly of 
 concern to programmers. Just as computers have become much faster over 
 the years, they have also received greater allotments of memory. Even so, 
 the amount of available disk space or main memory can be significant 
 constraints for algorithm designers. 
 The analysis techniques used to measure space requirements are similar 
 to those used to measure time requirements. However, while time 
 requirements are nor-mally measured for an algorithm that manipulates a 
 particular data structure, space requirements are normally determined for 
 the data structure itself. The concepts of asymptotic analysis for growth 
 rates on input size apply completely to measuring space requirements. 
 Example 3.16
  What are the space requirements for an array of
  n
  
 inte-gers? If each integer requires
  c
  bytes, then the array requires
  cn
  
 bytes, which is Θ(
 n
 ). 
 Example 3.17
  Imagine that we want to keep track of friendships 
 between 
 n
  people. We can do this with an array of size
  n × n
 . Each 
 row of the array represents the friends of an individual, with the 
 columns indicating who has that individual as a friend. For example, 
 if person
  j
  is a friend of person 
 i
 , then we place a mark in column
  j
  of 
 row
  i
  in the array. Likewise, we should also place a mark in column
  i",NA
3.10 ,NA,NA
Speeding Up Your Programs,"In practice, there is not such a big difference in running time between an 
 algorithm with growth rate Θ(
 n
 ) and another with growth rate Θ(
 n
  log
  n
 ). 
 There is, however, an enormous difference in running time between 
 algorithms with growth rates ofΘ(
 n
  log
  n
 ) and Θ(
 n
 2
 ). As you shall see during 
 the course of your study of common data structures and algorithms, it is not 
 unusual that a problem whose obvious solu-tion requires Θ(
 n
 2
 ) time also 
 has a solution requiring Θ(
 n
  log
  n
 ) time. Examples include sorting and 
 searching, two of the most important computer problems.",NA
3.11 ,NA,NA
Empirical Analysis,"This chapter has focused on asymptotic analysis. This is an analytic tool, 
 whereby we model the key aspects of an algorithm to determine the growth 
 rate of the alg-orithm as the input size grows. As pointed out previously, 
 there are many limita-tions to this approach. These include the effects at 
 small problem size, determining the finer distinctions between algorithms 
 with the same growth rate, and the inher-ent difficulty of doing 
 mathematical modeling for more complex problems. 
 An alternative to analytical approaches are empirical ones. The most 
 obvious empirical approach is simply to run two competitors and see which 
 performs better. In this way we might overcome the deficiencies of 
 analytical approaches. 
 Be warned that comparative timing of programs is a difficult business, 
 often subject to experimental errors arising from uncontrolled factors 
 (system load, the language or compiler used, etc.). The most important point 
 is not to be biased in favor of one of the programs. If you are biased, this is 
 certain to be reflected in the timings. One look at competing software or 
 hardware vendors’ advertisements should convince you of this. The most 
 common pitfall when writing two programs to compare their performance is 
 that one receives more code-tuning effort than the other. As mentioned in 
 Section 3.10, code tuning can often reduce running time by a factor of ten. If 
 the running times for two programs differ by a constant factor regardless of 
 input size (i.e., their growth rates are the same), then differences in code 
 tuning might account for any difference in running time. Be suspicious of 
 empirical comparisons in this situation. 
 Another approach to analysis is simulation. The idea of simulation is to 
 model the problem with a computer program and then run it to get a result. 
 In the con-text of algorithm analysis, simulation is distinct from empirical 
 comparison of two competitors because the purpose of the simulation is to 
 perform analysis that might otherwise be too difficult. A good example of 
 this appears in Figure 9.10. This figure shows the cost for inserting or 
 deleting a record from a hash table under two different assumptions for the 
 policy used to find a free slot in the table. The
  y
  axes is the cost in number of 
 hash table slots evaluated, and the
  x
  axes is the percentage of slots in the 
 table that are full. The mathematical equations for these curves can be 
 determined, but this is not so easy. A reasonable alternative is to write 
 simple variations on hashing. By timing the cost of the program for various 
 loading con-ditions, it is not difficult to construct a plot similar to Figure 
 9.10. The purpose of this analysis is not to determine which approach to 
 hashing is most efficient, so we are not doing empirical comparison of 
 hashing alternatives. Instead, the purpose is to analyze the proper loading",NA
Further Reading,Chap. 3 Algorithm Analysis,NA
3.12,"Pioneering works on algorithm analysis include
  The Art of Computer 
 Programming 
 by Donald E. Knuth [Knu97, Knu98], and
  The Design and 
 Analysis of Computer Algorithms
  by Aho, Hopcroft, and Ullman [AHU74]. The 
 alternate definition forΩ comes from [AHU83]. The use of the notation “
 T
 (
 n
 ) 
 is in O(
 f
 (
 n
 ))” rather than the more commonly used “
 T
 (
 n
 ) = O(
 f
 (
 n
 ))” I derive 
 from Brassard and Bratley [BB96], though certainly this use predates them. 
 A good book to read for further information on algorithm analysis 
 techniques is
  Compared to What?
  by Gregory J.E. Rawlins [Raw92]. 
 Bentley [Ben88] describes one problem in numerical analysis for which, 
 be-tween 1945 and 1988, the complexity of the best known algorithm had 
 decreased from O(
 n
 7
 ) to O(
 n
 3
 ). For a problem of size
  n
  = 64, this is roughly 
 equivalent to the speedup achieved from all advances in computer hardware 
 during the same time period. 
 While the most important aspect of program efficiency is the algorithm, 
 much improvement can be gained from efficient coding of a program. As 
 cited by Freder-ick P. Brooks in
  The Mythical Man-Month
  [Bro95], an 
 efficient programmer can of-ten produce programs that run five times faster 
 than an inefficient programmer, even when neither takes special efforts to 
 speed up their code. For excellent and enjoy-able essays on improving your 
 coding efficiency, and ways to speed up your code when it really matters, see 
 the books by Jon Bentley [Ben82, Ben00, Ben88]. The situation described in 
 Example 3.18 arose when we were working on the project reported on in 
 [SU92]. 
 As an interesting aside, writing a correct binary search algorithm is not 
 easy. Knuth [Knu98] notes that while the first binary search was published 
 in 1946, the first bug-free algorithm was not published until 1962! Bentley 
 (“Writing Correct Programs” in [Ben00]) has found that 90% of the 
 computer professionals he tested could not write a bug-free binary search in 
 two hours.",NA
3.13 ,NA,NA
Exercises,"3.1
  For each of the six expressions of Figure 3.1, give the range of values of
  
 n 
  for which that expression is most efficient. 
 3.2
  Graph the following expressions. For each expression, state the range 
 of  values of
  n
  for which that expression is the most efficient. 
 4
 n
 2
  
 log
 3
  n
  
 3
 n
  
 20
 n
  
 2 
 log
 2
  n
  
 n
 2
 /
 3
  
 3.3
  Arrange the following expressions by growth rate from slowest to fastest. 
 4
 n
 2
  
 log
 3
  n
  
 n
 ! 
 3
 n
  
 20
 n
  
 2 
 log
 2
  n
  
 n
 2
 /
 3
  
 See Stirling’s approximation in Section 2.2 for help in classifying
  n
 !.",NA
3.14 ,NA,NA
Projects,NA,NA
PART II ,NA,NA
Fundamental Data Structures,93,NA
4,NA,NA
"Lists, Stacks, and Queues","If your program needs to store a few things — numbers, payroll records, or 
 job de-scriptions for example — the simplest and most effective approach 
 might be to put them in a list. Only when you have to organize and search 
 through a large number of things do more sophisticated data structures 
 usually become necessary. (We will study how to organize and search 
 through medium amounts of data in Chapters 5, 7, and 9, and discuss how to 
 deal with large amounts of data in Chapters 8–10.) Many applications don’t 
 require any form of search, and they do not require that any or-dering be 
 placed on the objects being stored. Some applications require processing in 
 a strict chronological order, processing objects in the order that they 
 arrived, or perhaps processing objects in the reverse of the order that they 
 arrived. For all these situations, a simple list structure is appropriate. 
 This chapter describes representations for lists in general, as well as two 
 impor-tant list-like structures called the stack and the queue. Along with 
 presenting these fundamental data structures, the other goals of the chapter 
 are to: (1) Give examples of separating a logical representation in the form 
 of an ADT from a physical im-plementation for a data structure. (2) Illustrate 
 the use of asymptotic analysis in the context of some simple operations that 
 you might already be familiar with. In this way you can begin to see how 
 asymptotic analysis works, without the complica-tions that arise when 
 analyzing more sophisticated algorithms and data structures. (3) Introduce 
 the concept and use of dictionaries. 
 We begin by defining an ADT for lists in Section 4.1. Two 
 implementations for the list ADT — the array-based list and the linked list 
 — are covered in detail and their relative merits discussed. Sections 4.2 and 
 4.3 cover stacks and queues, re-spectively. Sample implementations for each 
 of these data structures are presented. Section 4.4 presents the Dictionary 
 ADT for storing and retrieving data, which sets a context for implementing 
 search structures such as the Binary Search Tree of Section 5.4.",NA
Lists,"Chap. 4 Lists, Stacks, and Queues",NA
4.1,"We all have an intuitive understanding of what we mean by a “list.” Our first 
 step is to define precisely what is meant so that this intuitive understanding 
 can eventually be converted into a concrete data structure and its 
 operations. The most important concept related to lists is that of
  position
 . In 
 other words, we perceive that there is a first element in the list, a second 
 element, and so on. We should view a list as embodying the mathematical 
 concepts of a sequence, as defined in Section 2.1. 
 We define a
  list
  to be a finite, ordered sequence of data items known as
  
 ele-ments
 . “Ordered” in this definition means that each element has a 
 position in the list. (We will not use “ordered” in this context to mean that 
 the list elements are sorted by value.) Each list element has a data type. In 
 the simple list implemen-tations discussed in this chapter, all elements of 
 the list have the same data type, although there is no conceptual objection to 
 lists whose elements have differing data types if the application requires it 
 (see Section 12.1). The operations defined as part of the list ADT do not 
 depend on the elemental data type. For example, the list ADT can be used for 
 lists of integers, lists of characters, lists of payroll records, even lists of lists. 
 A list is said to be
  empty
  when it contains no elements. The number of 
 ele-ments currently stored is called the
  length
  of the list. The beginning of 
 the list is called the
  head
 , the end of the list is called the
  tail
 . There might or 
 might not be some relationship between the value of an element and its 
 position in the list. For example,
  sorted lists
  have their elements positioned 
 in ascending order of value, while
  unsorted lists
  have no particular 
 relationship between element values and positions. This section will 
 consider only unsorted lists. Chapters 7 and 9 treat the problems of how to 
 create and search sorted lists efficiently. 
 When presenting the contents of a list, we use the same notation as was 
 in-troduced for sequences in Section 2.1. To be consistent with
  C
 ++
 array 
 indexing, the first position on the list is denoted as 0. Thus, if there are
  n
  
 elements in the list, they are given positions 0 through
  n −
  1 as
  ∈a
 0
 , a
 1
 , ..., 
 a
 n−
 1
 ∈
 . The subscript indicates an element’s position within the list. Using 
 this notation, the empty list would appear as
  ∈∈
 . 
 Before selecting a list implementation, a program designer should first 
 consider what basic operations the implementation must support. Our 
 common intuition about lists tells us that a list should be able to grow and 
 shrink in size as we insert and remove elements. We should be able to insert 
 and remove elements from any-where in the list. We should be able to gain 
 access to any element’s value, either to read it or to change it. We must be 
 able to create and clear (or reinitialize) lists. It is also convenient to access 
 the next or previous element from the “current” one.",NA
4.2,NA,NA
Stacks,"The
  stack
  is a list-like structure in which elements may be inserted or 
 removed from only one end. While this restriction makes stacks less flexible 
 than lists, it also makes stacks both efficient (for those operations they can 
 do) and easy to im-plement. Many applications require only the limited form 
 of insert and remove operations that stacks provide. In such cases, it is more 
 efficient to use the sim-pler stack data structure rather than the generic list. 
 For example, the freelist of Section 4.1.2 is really a stack. 
 Despite their restrictions, stacks have many uses. Thus, a special 
 vocabulary for stacks has developed. Accountants used stacks long before",NA
4.3 ,NA,NA
Queues,"Like the stack, the
  queue
  is a list-like structure that provides restricted 
 access to its elements. Queue elements may only be inserted at the back 
 (called an
  enqueue 
 operation) and removed from the front (called a
  
 dequeue
  operation). Queues oper-ate like standing in line at a movie theater 
 ticket counter.
 1
 If nobody cheats, then newcomers go to the back of the line. 
 The person at the front of the line is the next to be served. Thus, queues 
 release their elements in order of arrival. Accountants have used queues 
 since long before the existence of computers. They call a queue a “FIFO” list, 
 which stands for “First-In, First-Out.” Figure 4.23 shows a sample queue 
 ADT. This section presents two implementations for queues: the array-based 
 queue and the linked queue. 
 4.3.1 Array-Based Queues
  
 The array-based queue is somewhat tricky to implement effectively. A 
 simple con-version of the array-based list implementation is not efficient. 
 Assume that there are
  n
  elements in the queue. By analogy to the array-
 based list implementation, we could require that all elements of the queue 
 be stored in the first
  n
  positions of the array. If we choose the rear element 
 of the queue to be in position 0, then
  dequeue
  operations require only Θ(1) 
 time because the front ele-ment of the queue (the one being removed) is the 
 last element in the array. However, 
 enqueue
  operations will require Θ(
 n
 ) 
 time, because the
  n
  elements currently in the queue must each be shifted 
 one position in the array. If instead we chose the rear element of the queue 
 to be in position
  n −
  1, then an
  enqueue
  operation is equivalent to an
  
 append
  operation on a list. This requires only Θ(1) time. But now, a
  
 dequeue
  operation requires Θ(
 n
 ) time, because all of the elements must be",NA
4.4 ,NA,NA
Dictionaries,"The most common objective of computer programs is to store and retrieve 
 data. Much of this book is about efficient ways to organize collections of data 
 records so that they can be stored and retrieved quickly. In this section we 
 describe a simple interface for such a collection, called a
  dictionary
 . The 
 dictionary ADT provides operations for storing records, finding records, and 
 removing records from the collection. This ADT gives us a standard basis for 
 comparing various data structures. 
 Before we can discuss the interface for a dictionary, we must first define 
 the concepts of a
  key
  and
  comparable
  objects. If we want to search for a 
 given record in a database, how should we describe what we are looking for? 
 A database record could simply be a number, or it could be quite 
 complicated, such as a payroll record with many fields of varying types. We 
 do not want to describe what we are looking for by detailing and matching 
 the entire contents of the record. If we knew every-thing about the record 
 already, we probably would not need to look for it. Instead, we typically 
 define what record we want in terms of a key value. For example, if 
 searching for payroll records, we might wish to search for the record that 
 matches a particular ID number. In this example the ID number is the
  search 
 key
 .",NA
4.5 ,NA,NA
Further Reading,"For more discussion on choice of functions used to define the
  List
  ADT, see 
 the work of the Reusable Software Research Group from Ohio State. Their 
 definition for the
  List
  ADT can be found in [SWH93]. More information 
 about designing such classes can be found in [SW94].",NA
4.6 ,NA,NA
Exercises,"4.1
  Assume a list has the following configuration: 
 ∈ |
  2
 ,
  23
 ,
  15
 ,
  5
 ,
  9
  ∈.",NA
4.7 ,NA,NA
Projects,"4.1
  A
  deque
  (pronounced “deck”) is like a queue, except that items may be 
 added and removed from both the front and the rear. Write either an 
 array-based or linked implementation for the deque. 
 4.2
  One solution to the problem of running out of space for an array-based 
 list implementation is to replace the array with a larger array 
 whenever the origi-nal array overflows. A good rule that leads to an 
 implementation that is both space and time efficient is to double the 
 current size of the array when there is an overflow. Re-implement the 
 array-based
  List
  class of Figure 4.2 to support this array-doubling rule. 
 4.3
  Use singly linked lists to implement integers of unlimited size. Each 
 node of the list should store one digit of the integer. You should 
 implement addition, subtraction, multiplication, and exponentiation 
 operations. Limit exponents to be positive integers. What is the 
 asymptotic running time for each of your operations, expressed in 
 terms of the number of digits for the two operands of each function? 
 4.4
  Implement doubly linked lists by storing the sum of the
  next
  and
  prev 
  
 pointers in a single pointer variable as described in Example 4.1. 
 4.5
  Implement a city database using unordered lists. Each database record 
 con-tains the name of the city (a string of arbitrary length) and the 
 coordinates of the city expressed as integer
  x
  and
  y
  coordinates. Your 
 database should allow records to be inserted, deleted by name or 
 coordinate, and searched by name or coordinate. Another operation 
 that should be supported is to print all records within a given distance 
 of a specified point. Implement the database using an array-based list 
 implementation, and then a linked list im-plementation. Collect 
 running time statistics for each operation in both im-plementations. 
 What are your conclusions about the relative advantages and 
 disadvantages of the two implementations? Would storing records on 
 the list in alphabetical order by city name speed any of the operations? 
 Would keeping the list in alphabetical order slow any of the 
 operations? 
 4.6
  Modify the code of Figure 4.18 to support storing variable-length 
 strings of at most 255 characters. The stack array should have type
  
 char
 . A string is represented by a series of characters (one character 
 per stack element), with",NA
5,NA,NA
Binary Trees,"The list representations of Chapter 4 have a fundamental limitation: Either 
 search or insert can be made efficient, but not both at the same time. Tree 
 structures permit both efficient access and update to large collections of 
 data. Binary trees in particular are widely used and relatively easy to 
 implement. But binary trees are useful for many things besides searching. 
 Just a few examples of applications that trees can speed up include 
 prioritizing jobs, describing mathematical expressions and the syntactic 
 elements of computer programs, or organizing the information needed to 
 drive data compression algorithms. 
 This chapter begins by presenting definitions and some key properties of 
 bi-nary trees. Section 5.2 discusses how to process all nodes of the binary 
 tree in an organized manner. Section 5.3 presents various methods for 
 implementing binary trees and their nodes. Sections 5.4 through 5.6 present 
 three examples of binary trees used in specific applications: the Binary 
 Search Tree (BST) for implementing dictionaries, heaps for implementing 
 priority queues, and Huffman coding trees for text compression. The BST, 
 heap, and Huffman coding tree each have distinctive structural features that 
 affect their implementation and use.",NA
5.1 ,NA,NA
Definitions and Properties,"A
  binary tree
  is made up of a finite set of elements called
  nodes
 . This set 
 either is empty or consists of a node called the
  root
  together with two 
 binary trees, called the left and right
  subtrees
 , which are disjoint from each 
 other and from the root. (Disjoint means that they have no nodes in 
 common.) The roots of these subtrees are
  children
  of the root. There is an
  
 edge
  from a node to each of its children, and a node is said to be the
  parent
  
 of its children. 
 If
  n
 1
 ,
  n
 2
 , ...,
  n
 k
  is a sequence of nodes in the tree such that
  n
 i
  is the parent 
 of 
 n
 i
 +1
  for 1
  ≤ i < k
 , then this sequence is called a
  path
  from
  n
 1
  to
  n
 k
 . The
  
 length 
 of the path is
  k −
  1. If there is a path from node
  R
  to node
  M
 , then
  R
  is",NA
5.2 ,NA,NA
Binary Tree Traversals,"Often we wish to process a binary tree by “visiting” each of its nodes, each 
 time performing a specific action such as printing the contents of the node. 
 Any process for visiting all of the nodes in some order is called a
  traversal
 . 
 Any traversal that lists every node in the tree exactly once is called an
  
 enumeration
  of the tree’s nodes. Some applications do not require that the 
 nodes be visited in any particular order as long as each node is visited 
 precisely once. For other applications, nodes must be visited in an order that 
 preserves some relationship. For example, we might wish to make sure that",NA
5.3 ,NA,NA
Binary Tree Node Implementations,"In this section we examine ways to implement binary tree nodes. We begin 
 with some options for pointer-based binary tree node implementations. 
 Then comes a discussion on techniques for determining the space 
 requirements for a given imple-mentation. The section concludes with an 
 introduction to the array-based imple-mentation for complete binary trees. 
 5.3.1 Pointer-Based Node Implementations
  
 By definition, all binary tree nodes have two children, though one or both 
 children can be empty. Binary tree nodes typically contain a value field, with 
 the type of the field depending on the application. The most common node 
 implementation includes a value field and pointers to the two children. 
 Figure 5.7 shows a simple implementation for the
  BinNode
  abstract 
 class, which we will name
  BSTNode
 . Class
  BSTNode
  includes a data 
 member of type 
 E
 , (which is the second template parameter) for the element 
 type. To support search structures such as the Binary Search Tree, an 
 additional field is included, with cor-responding access methods, to store a 
 key value (whose purpose is explained in Section 4.4). Its type is determined 
 by the first template parameter, named
  Key
 . Every
  BSTNode
  object also has 
 two pointers, one to its left child and another to its right child. Overloaded
  
 new
  and
  delete
  operators could be added to support a freelist, as described 
 in Section 4.1.2.Figure 5.8 illustrates the
  BSTNode
  imple-mentation. 
 Some programmers find it convenient to add a pointer to the node’s 
 parent, allowing easy upward movement in the tree. Using a parent pointer 
 is somewhat analogous to adding a link to the previous node in a doubly 
 linked list. In practice, the parent pointer is almost always unnecessary and 
 adds to the space overhead for the tree implementation. It is not just a 
 problem that parent pointers take space. More importantly, many uses of the 
 parent pointer are driven by improper under-standing of recursion and so 
 indicate poor programming. If you are inclined toward using a parent 
 pointer, consider if there is a more efficient implementation possible.",NA
5.4 ,NA,NA
Binary Search Trees,"Section 4.4 presented the dictionary ADT, along with dictionary 
 implementations based on sorted and unsorted lists. When implementing 
 the dictionary with an unsorted list, inserting a new record into the 
 dictionary can be performed quickly by putting it at the end of the list. 
 However, searching an unsorted list for a particular record requires Θ(
 n
 ) 
 time in the average case. For a large database, this is probably much too 
 slow. Alternatively, the records can be stored in a sorted list. If the list is",NA
5.5 ,NA,NA
Heaps and Priority Queues,"There are many situations, both in real life and in computing applications, 
 where we wish to choose the next “most important” from a collection of 
 people, tasks, or objects. For example, doctors in a hospital emergency room 
 often choose to see next the “most critical” patient rather than the one who 
 arrived first. When scheduling programs for execution in a multitasking 
 operating system, at any given moment there might be several programs 
 (usually called
  jobs
 ) ready to run. The next job selected is the one with the 
 highest
  priority
 . Priority is indicated by a particular value associated with 
 the job (and might change while the job remains in the wait list). 
 When a collection of objects is organized by importance or priority, we 
 call this a
  priority queue
 . A normal queue data structure will not implement 
 a prior-ity queue efficiently because search for the element with highest 
 priority will takeΘ(
 n
 ) time. A list, whether sorted or not, will also require 
 Θ(
 n
 ) time for either in-sertion or removal. A BST that organizes records by 
 priority could be used, with the total of
  n
  inserts and
  n
  remove operations 
 requiring Θ(
 n
  log
  n
 ) time in the average case. However, there is always the 
 possibility that the BST will become unbal-anced, leading to bad 
 performance. Instead, we would like to find a data structure that is 
 guaranteed to have good performance for this special application.",NA
5.6 ,NA,NA
Huffman Coding Trees,"The space/time tradeoff principle from Section 3.9 states that one can often 
 gain an improvement in space requirements in exchange for a penalty in 
 running time. There are many situations where this is a desirable tradeoff. A 
 typical example is storing files on disk. If the files are not actively used, the 
 owner might wish to compress them to save space. Later, they can be 
 uncompressed for use, which costs some time, but only once. 
 We often represent a set of items in a computer program by assigning a 
 unique code to each item. For example, the standard ASCII coding scheme 
 assigns a unique eight-bit value to each character. It takes a certain 
 minimum number of bits to provide unique codes for each character. For 
 example, it takes
  ∈
 log 128
 ∈
  or seven bits to provide the 128 unique codes 
 needed to represent the 128 symbols of the ASCII character set.
 5
  
 all codes will be the same length, as are ASCII codes. This is called a
  fixed-length 
 The 
 requirement for
  ∈
 log
  n∈
  bits to represent
  n
  unique code values assumes that 
 coding scheme. If all characters were used equally often, then a fixed-length 
 coding scheme is the most space efficient method. However, you are 
 probably aware that not all characters are used equally often in many 
 applications. For example, the various letters in an English language 
 document have greatly different frequencies of use. 
 Figure 5.23 shows the relative frequencies of the letters of the alphabet. 
 From this table we can see that the letter ‘E’ appears about 60 times more 
 often than the letter ‘Z.’ In normal ASCII, the words “DEED” and “MUCK” 
 require the same",NA
5.7 ,NA,NA
Further Reading,"See Shaffer and Brown [SB93] for an example of a tree implementation 
 where an internal node pointer field stores the value of its child instead of a 
 pointer to its child when the child is a leaf node. 
 Many techniques exist for maintaining reasonably balanced BSTs in the 
 face of an unfriendly series of insert and delete operations. One example is 
 the AVL tree of Adelson-Velskii and Landis, which is discussed by Knuth 
 [Knu98]. The AVL tree (see Section 13.2) is actually a BST whose insert and 
 delete routines reorganize the tree structure so as to guarantee that the 
 subtrees rooted by the children of any node will differ in height by at most 
 one. Another example is the splay tree [ST85], also discussed in Section 13.2. 
  
 See Bentley’s Programming Pearl “Thanks, Heaps” [Ben85, Ben88] for a 
 good discussion on the heap data structure and its uses. 
 The proof of Section 5.6.1 that the Huffman coding tree has minimum 
 external path weight is from Knuth [Knu97]. For more information on data 
 compression techniques, see
  Managing Gigabytes
  by Witten, Moffat, and Bell 
 [WMB99], and 
 Codes and Cryptography
  by Dominic Welsh [Wel88]. Tables 
 5.23 and 5.24 are derived from Welsh [Wel88].",NA
5.8 ,NA,NA
Exercises,"5.1
  Section 5.1.1 claims that a full binary tree has the highest number of 
 leaf  
 nodes among all trees with
  n
  internal nodes. Prove that this is 
 true. 
 5.2
  Define the
  degree
  of a node as the number of its non-empty children. 
 Prove by induction that the number of degree 2 nodes in any binary 
 tree is one less than the number of leaves. 
 5.3
  Define the
  internal path length
  for a tree as the sum of the depths of 
 all internal nodes, while the
  external path length
  is the sum of the 
 depths of all leaf nodes in the tree. Prove by induction that if tree
  T
  is a 
 full binary tree with
  n
  internal nodes,
  I
  is
  T
 ’s internal path length, and
  
 E
  is
  T
 ’s external path length, then
  E
  =
  I
  + 2
 n
  for
  n ≥
  0.",NA
5.9 ,NA,NA
Projects,"5.1
  Re-implement the composite design for the binary tree node class of 
 Fig- ure 5.11 using a flyweight in place of
  NULL
  pointers to empty nodes. 
 5.2
  One way to deal with the “problem” of
  NULL
  pointers in binary trees is 
 to use that space for some other purpose. One example is the
  
 threaded
  binary tree. Extending the node implementation of Figure 
 5.7, the threaded binary tree stores with each node two additional bit 
 fields that indicate if the child pointers
  lc
  and
  rc
  are regular pointers 
 to child nodes or threads. If
  lc 
 is not a pointer to a non-empty child 
 (i.e., if it would be
  NULL
  in a regular binary tree), then it instead stores 
 a pointer to the
  inorder predecessor
  of that node. The inorder 
 predecessor is the node that would be printed immediately before the 
 current node in an inorder traversal. If
  rc
  is not a pointer to a child, 
 then it instead stores a pointer to the node’s
  inorder successor
 . The 
 inorder successor is the node that would be printed immediately after 
 the current node in an inorder traversal. The main advantage of 
 threaded binary trees is that operations such as inorder traversal can 
 be implemented without using recursion or a stack. 
 Re-implement the BST as a threaded binary tree, and include a non-
 recursive version of the preorder traversal 
 5.3
  Implement a city database using a BST to store the database records. 
 Each database record contains the name of the city (a string of 
 arbitrary length) and the coordinates of the city expressed as integer
  
 x
 - and
  y
 -coordinates. The BST should be organized by city name. Your 
 database should allow records to be inserted, deleted by name or 
 coordinate, and searched by name or coordinate. Another operation 
 that should be supported is to print all records within a given distance 
 of a specified point. Collect running-time statistics for each operation. 
 Which operations can be implemented reason-ably efficiently (i.e., in 
 Θ(log
  n
 ) time in the average case) using a BST? Can the database 
 system be made more efficient by using one or more additional BSTs 
 to organize the records by location? 
 5.4
  Create a binary tree ADT that includes generic traversal methods that 
 take a visitor, as described in Section 5.2. Write functions
  count
  and
  
 BSTcheck 
 of Section 5.2 as visitors to be used with the generic 
 traversal method.",NA
6,NA,NA
Non-Binary Trees,"Many organizations are hierarchical in nature, such as the military and most 
 busi-nesses. Consider a company with a president and some number of vice 
 presidents who report to the president. Each vice president has some 
 number of direct sub-ordinates, and so on. If we wanted to model this 
 company with a data structure, it would be natural to think of the president 
 in the root node of a tree, the vice presi-dents at level 1, and their 
 subordinates at lower levels in the tree as we go down the organizational 
 hierarchy. 
 Because the number of vice presidents is likely to be more than two, this 
 com-pany’s organization cannot easily be represented by a binary tree. We 
 need instead to use a tree whose nodes have an arbitrary number of 
 children. Unfortunately, when we permit trees to have nodes with an 
 arbitrary number of children, they be-come much harder to implement than 
 binary trees. We consider such trees in this chapter. To distinguish them 
 from binary trees, we use the term
  general tree
 . 
 Section 6.1 presents general tree terminology. Section 6.2 presents a 
 simple representation for solving the important problem of processing 
 equivalence classes. Several pointer-based implementations for general 
 trees are covered in Section 6.3. Aside from general trees and binary trees, 
 there are also uses for trees whose in-ternal nodes have a fixed number
  K
  of 
 children where
  K
  is something other than two. Such trees are known as
  K
 -
 ary trees. Section 6.4 generalizes the properties of binary trees to
  K
 -ary 
 trees. Sequential representations, useful for applications such as storing 
 trees on disk, are covered in Section 6.5.",NA
6.1 ,NA,NA
General Tree Definitions and Terminology,"A
  tree T
  is a finite set of one or more nodes such that there is one designated 
 node 
 R
 , called the root of
  T
 . If the set (
 T
 −{R}
 ) is not empty, these nodes are 
 partitioned into
  n >
  0 disjoint subsets
  T
 0
 ,
  T
 1
 , ...,
  T
 n−
 1
 , each of which is a tree, 
 and whose roots
  R
 1
 ,
  R
 2
 , ...,
  R
 n
 , respectively, are children of
  R
 . The subsets
  T
 i
  
 (0
  ≤ i < n
 ) are said to be
  subtrees
  of
  T
 . These subtrees are ordered in that
  T
 i
  
 is said to come before",NA
6.2 ,NA,NA
The Parent Pointer Implementation,"Perhaps the simplest general tree implementation is to store for each node 
 only a pointer to that node’s parent. We will call this the
  parent pointer
  
 implementation. Clearly this implementation is not general purpose, 
 because it is inadequate for such important operations as finding the 
 leftmost child or the right sibling for a node. Thus, it may seem to be a poor 
 idea to implement a general tree in this way. However, the parent pointer 
 implementation stores precisely the information required to answer the 
 following, useful question: “Given two nodes, are they in the same tree?” To 
 answer the question, we need only follow the series of parent pointers from 
 each node to its respective root. If both nodes reach the same root, then they 
 must be in the same tree. If the roots are different, then the two nodes are 
 not in the same tree. The process of finding the ultimate root for a given 
 node we will call
  FIND
 . 
 The parent pointer representation is most often used to maintain a 
 collection of disjoint sets. Two disjoint sets share no members in common 
 (their intersection is empty). A collection of disjoint sets partitions some 
 objects such that every object is in exactly one of the disjoint sets. There are 
 two basic operations that we wish to support: 
 (1)
  determine if two objects are in the same set, 
 and  
 (2)
  merge two sets together. 
 Because two merged sets are united, the merging operation is called UNION 
 and the whole process of determining if two objects are in the same set and 
 then merging the sets goes by the name “UNION/FIND.” 
  
 To implement UNION/FIND, we represent each disjoint set with a 
 separate general tree. Two objects are in the same disjoint set if they are in 
 the same tree. Every node of the tree (except for the root) has precisely one 
 parent. Thus, each node requires the same space to represent it. The 
 collection of objects is typically stored in an array, where each element of the 
 array corresponds to one object, and each element stores the object’s value. 
 The objects also correspond to nodes in the various disjoint trees (one tree 
 for each disjoint set), so we also store the parent value with each object in 
 the array. Those nodes that are the roots of their respective trees store an 
 appropriate indicator. Note that this representation means that a single 
 array is being used to implement a collection of trees. This makes it easy to 
 merge trees together with UNION operations. 
 Figure 6.4 shows the parent pointer implementation for the general tree, 
 called 
 ParPtrTree
 . This class is greatly simplified from the declarations of 
 Figure 6.2 because we need only a subset of the general tree operations. 
 Instead of implement-ing a separate node class,
  ParPtrTree
  simply stores 
 an array where each array element corresponds to a node of the tree. Each",NA
6.3 ,NA,NA
General Tree Implementations,"We now tackle the problem of devising an implementation for general trees 
 that allows efficient processing for all member functions of the ADTs shown 
 in Fig-ure 6.2. This section presents several approaches to implementing 
 general trees. Each implementation yields advantages and disadvantages in 
 the amount of space required to store a node and the relative ease with 
 which key operations can be performed. General tree implementations 
 should place no restriction on how many",NA
6.4 ,K,NA
-ary Trees,"K
 -ary trees are trees whose internal nodes all have exactly
  K
  children. Thus, 
 a full binary tree is a 2-ary tree. The PR quadtree discussed in Section 13.3 is 
 an example of a 4-ary tree. Because
  K
 -ary tree nodes have a fixed number of 
 children, unlike general trees, they are relatively easy to implement. In 
 general,
  K
 -ary trees bear many similarities to binary trees, and similar 
 implementations can be used for 
 K
 -ary tree nodes. Note that as
  K
  becomes 
 large, the potential number of
  NULL 
 pointers grows, and the difference 
 between the required sizes for internal nodes and leaf nodes increases. 
 Thus, as
  K
  becomes larger, the need to choose separate implementations for 
 the internal and leaf nodes becomes more pressing.",NA
6.5 ,NA,NA
Sequential Tree Implementations,"Next we consider a fundamentally different approach to implementing trees. 
 The goal is to store a series of node values with the minimum information 
 needed to reconstruct the tree structure. This approach, known as a
  
 sequential
  tree imple-mentation, has the advantage of saving space because 
 no pointers are stored. It has",NA
6.6 ,NA,NA
Further Reading,"The expression log
 ∈
 n
  cited in Section 6.2 is closely related to the inverse of 
 Ack-ermann’s function. For more information about Ackermann’s function 
 and the cost of path compression for UNION/FIND, see Robert E. Tarjan’s 
 paper “On the effi-ciency of a good but not linear set merging algorithm” 
 [Tar75]. The article “Data Structures and Algorithms for Disjoint Set Union 
 Problems” by Galil and Italiano [GI91] covers many aspects of the 
 equivalence class problem. 
 Foundations of Multidimensional and Metric Data Structures
  by Hanan 
 Samet [Sam06] treats various implementations of tree structures in detail 
 within the con-text of
  K
 -ary trees. Samet covers sequential implementations 
 as well as the linked and array implementations such as those described in 
 this chapter and Chapter 5. While these books are ostensibly concerned with 
 spatial data structures, many of the concepts treated are relevant to anyone 
 who must implement tree structures.",NA
6.7 ,NA,NA
Exercises,"6.1
  Write an algorithm to determine if two general trees are identical. 
 Make the  
 algorithm as efficient as you can. Analyze your algorithm’s 
 running time. 
 6.2
  Write an algorithm to determine if two binary trees are 
 identical when the  
 ordering of the subtrees for a node is ignored. For 
 example, if a tree has root  
 node with value
  R
 , left child with value
  A
  
 and right child with value
  B
 , this  
 would be considered identical to 
 another tree with root node value
  R
 , left  
 child value
  B
 , and right 
 child value
  A
 . Make the algorithm as efficient as you  
 can. Analyze your 
 algorithm’s running time. How much harder would it be  
 to make 
 this algorithm work on a general tree? 
 6.3
  Write a postorder traversal function for general trees, similar to the 
 preorder  
 traversal function named
  preorder
  given in Section 6.1.2. 
 6.4
  Write a function that takes as input a general tree and returns the 
 number of nodes in that tree. Write your function to use the
  GenTree
  
 and
  GTNode 
 ADTs of Figure 6.2. 
 6.5
  Describe how to implement the weighted union rule efficiently. In 
 particular, describe what information must be stored with each node 
 and how this infor-mation is updated when two trees are merged. 
 Modify the implementation of Figure 6.4 to support the weighted 
 union rule. 
 6.6
  A potential alternative to the weighted union rule for combining two 
 trees is the height union rule. The height union rule requires that the 
 root of the tree with greater height become the root of the union. 
 Explain why the height union rule can lead to worse average time 
 behavior than the weighted union rule.",NA
6.8 ,NA,NA
Projects,"6.1
  Write classes that implement the general tree class declarations of 
 Figure 6.2 using the dynamic “left-child/right-sibling” representation 
 described in Sec-tion 6.3.4. 
 6.2
  Write classes that implement the general tree class declarations of 
 Figure 6.2 using the linked general tree implementation with child 
 pointer arrays of Fig-ure 6.12. Your implementation should support 
 only fixed-size nodes that do not change their number of children once 
 they are created. Then, re-implement these classes with the linked list 
 of children representation of Figure 6.13. How do the two 
 implementations compare in space and time efficiency and ease of 
 implementation? 
 6.3
  Write classes that implement the general tree class declarations of 
 Figure 6.2 using the linked general tree implementation with child 
 pointer arrays of Fig-ure 6.12. Your implementation must be able to 
 support changes in the num-ber of children for a node. When created, 
 a node should be allocated with only enough space to store its initial 
 set of children. Whenever a new child is added to a node such that the 
 array overflows, allocate a new array from free store that can store 
 twice as many children. 
 6.4
  Implement a BST file archiver. Your program should take a BST 
 created in main memory using the implementation of Figure 5.14 and 
 write it out to disk using one of the sequential representations of 
 Section 6.5. It should also be able to read in disk files using your 
 sequential representation and create the equivalent main memory 
 representation. 
 6.5
  Use the UNION/FIND algorithm to implement a solution to the 
 following problem. Given a set of points represented by their
  xy
 -
 coordinates, assign the points to clusters. Any two points are defined 
 to be in the same cluster if they are within a specified distance
  d
  of 
 each other. For the purpose of this problem, clustering is an",NA
PART III ,NA,NA
Sorting and Searching,229,NA
7,NA,NA
Internal Sorting,"We sort many things in our everyday lives: A handful of cards when playing 
 Bridge; bills and other piles of paper; jars of spices; and so on. And we have 
 many intuitive strategies that we can use to do the sorting, depending on 
 how many objects we have to sort and how hard they are to move around. 
 Sorting is also one of the most frequently performed computing tasks. We 
 might sort the records in a database so that we can search the collection 
 efficiently. We might sort the records by zip code so that we can print and 
 mail them more cheaply. We might use sorting as an intrinsic part of an 
 algorithm to solve some other problem, such as when computing the 
 minimum-cost spanning tree (see Section 11.5). 
 Because sorting is so important, naturally it has been studied intensively 
 and many algorithms have been devised. Some of these algorithms are 
 straightforward adaptations of schemes we use in everyday life. Others are 
 totally alien to how hu-mans do things, having been invented to sort 
 thousands or even millions of records stored on the computer. After years of 
 study, there are still unsolved problems related to sorting. New algorithms 
 are still being developed and refined for special-purpose applications. 
 While introducing this central problem in computer science, this chapter 
 has a secondary purpose of illustrating issues in algorithm design and 
 analysis. For example, this collection of sorting algorithms shows multiple 
 approaches to us-ing divide-and-conquer. In particular, there are multiple 
 ways to do the dividing: Mergesort divides a list in half; Quicksort divides a 
 list into big values and small values; and Radix Sort divides the problem by 
 working on one digit of the key at a time. Sorting algorithms can also 
 illustrate a wide variety of analysis techniques. We’ll find that it is possible 
 for an algorithm to have an average case whose growth rate is significantly 
 smaller than its worse case (Quicksort). We’ll see how it is possible to speed 
 up sorting algorithms (both Shellsort and Quicksort) by taking advantage of 
 the best case behavior of another algorithm (Insertion sort). We’ll see 
 several examples of how we can tune an algorithm for better performance. 
 We’ll see that special case behavior by some algorithms makes them a good 
 solution for",NA
7.1 ,NA,NA
Sorting Terminology and Notation,"Except where noted otherwise, input to the sorting algorithms presented in 
 this chapter is a collection of records stored in an array. Records are 
 compared to one another by means of a comparator class, as introduced in 
 Section 4.4. To simplify the discussion we will assume that each record has a 
 key field whose value is ex-tracted from the record by the comparator. The 
 key method of the comparator class is
  prior
 , which returns true when its 
 first argument should appear prior to its sec-ond argument in the sorted list. 
 We also assume that for every record type there is a
  swap
  function that can 
 interchange the contents of two records in the array(see the Appendix). 
 Given a set of records
  r
 1
 ,
  r
 2
 , ...,
  r
 n
  with key values
  k
 1
 ,
  k
 2
 , ...,
  k
 n
 , the
  Sorting 
 Problem
  is to arrange the records into any order
  s
  such that records
  r
 s
 1
 ,
  r
 s
 2
 , 
 ...,
  r
 s
 n 
 have keys obeying the property
  k
 s
 1
  ≤ k
 s
 2
  ≤ ... ≤ k
 s
 n
 . In other words, the 
 sorting problem is to arrange a set of records so that the values of their key 
 fields are in non-decreasing order. 
 As defined, the Sorting Problem allows input with two or more records 
 that have the same key value. Certain applications require that input not 
 contain duplicate key values. The sorting algorithms presented in this 
 chapter and in Chapter 8 can handle duplicate key values unless noted 
 otherwise. 
 When duplicate key values are allowed, there might be an implicit 
 ordering to the duplicates, typically based on their order of occurrence 
 within the input. It might be desirable to maintain this initial ordering 
 among duplicates. A sorting algorithm is said to be
  stable
  if it does not 
 change the relative ordering of records with identical key values. Many, but 
 not all, of the sorting algorithms presented in this chapter are stable, or can 
 be made stable with minor changes.",NA
7.2 ,NA,NA
Three,"Θ(
 n
 2
 )",NA
 Sorting Algorithms,"This section presents three simple sorting algorithms. While easy to 
 understand and implement, we will soon see that they are unacceptably 
 slow when there are many records to sort. Nonetheless, there are situations 
 where one of these simple algorithms is the best tool for the job. 
 7.2.1 Insertion Sort
  
 Imagine that you have a stack of phone bills from the past two years and that 
 you wish to organize them by date. A fairly natural way to do this might be 
 to look at the first two bills and put them in order. Then take the third bill 
 and put it into the right order with respect to the first two, and so on. As you 
 take each bill, you would add it to the sorted pile that you have already 
 made. This naturally intuitive process is the inspiration for our first sorting 
 algorithm, called
  Insertion Sort
 . Insertion Sort iterates through a list of 
 records. Each record is inserted in turn at the correct position within a 
 sorted list composed of those records already processed. The",NA
7.3 ,NA,NA
Shellsort,"The next sorting algorithm that we consider is called
  Shellsort
 , named after 
 its inventor, D.L. Shell. It is also sometimes called the
  diminishing 
 increment
  sort. Unlike Insertion and Selection Sort, there is no real life 
 intuitive equivalent to Shell-sort. Unlike the exchange sorts, Shellsort makes 
 comparisons and swaps between non-adjacent elements. Shellsort also 
 exploits the best-case performance of Inser-tion Sort. Shellsort’s strategy is 
 to make the list “mostly sorted” so that a final Insertion Sort can finish the 
 job. When properly implemented, Shellsort will give substantially better 
 performance than Θ(
 n
 2
 ) in the worst case. 
 Shellsort uses a process that forms the basis for many of the sorts 
 presented in the following sections: Break the list into sublists, sort them, 
 then recombine the sublists. Shellsort breaks the array of elements into 
 “virtual” sublists. Each sublist is sorted using an Insertion Sort. Another 
 group of sublists is then chosen and sorted, and so on.",NA
7.4 ,NA,NA
Mergesort,"A natural approach to problem solving is divide and conquer. In terms of 
 sorting, we might consider breaking the list to be sorted into pieces, process 
 the pieces, and then put them back together somehow. A simple way to do 
 this would be to split the list in half, sort the halves, and then merge the 
 sorted halves together. This is the idea behind
  Mergesort
 .",NA
7.5 ,NA,NA
Quicksort,"While Mergesort uses the most obvious form of divide and conquer (split the 
 list in half then sort the halves), it is not the only way that we can break 
 down the sorting problem. And we saw that doing the merge step for 
 Mergesort when using an array implementation is not so easy. So perhaps a 
 different divide and conquer strategy might turn out to be more efficient?",NA
7.6 ,NA,NA
Heapsort,"Our discussion of Quicksort began by considering the practicality of using a 
 binary search tree for sorting. The BST requires more space than the other 
 sorting meth-ods and will be slower than Quicksort or Mergesort due to the 
 relative expense of inserting values into the tree. There is also the possibility 
 that the BST might be un-balanced, leading to a Θ(
 n
 2
 ) worst-case running 
 time. Subtree balance in the BST is closely related to Quicksort’s partition 
 step. Quicksort’s pivot serves roughly the same purpose as the BST root 
 value in that the left partition (subtree) stores val-ues less than the pivot 
 (root) value, while the right partition (subtree) stores values greater than or 
 equal to the pivot (root). 
 A good sorting algorithm can be devised based on a tree structure more 
 suited to the purpose. In particular, we would like the tree to be balanced, 
 space efficient, and fast. The algorithm should take advantage of the fact that 
 sorting is a special-purpose application in that all of the values to be stored 
 are available at the start. This means that we do not necessarily need to 
 insert one value at a time into the tree structure. 
 Heapsort is based on the heap data structure presented in Section 5.5. 
 Heapsort has all of the advantages just listed. The complete binary tree is 
 balanced, its array representation is space efficient, and we can load all 
 values into the tree at once, taking advantage of the efficient
  buildheap
  
 function. The asymptotic perfor-mance of Heapsort is Θ(
 n
  log
  n
 ) in the best, 
 average, and worst cases. It is not as fast as Quicksort in the average case (by 
 a constant factor), but Heapsort has special properties that will make it 
 particularly useful when sorting data sets too large to fit in main memory, as 
 discussed in Chapter 8. 
 A sorting algorithm based on max-heaps is quite straightforward. First 
 we use the heap building algorithm of Section 5.5 to convert the array into 
 max-heap order. Then we repeatedly remove the maximum value from the 
 heap, restoring the heap property each time that we do so, until the heap is 
 empty. Note that each time we remove the maximum element from the heap, 
 it is placed at the end of the array. Assume the
  n
  elements are stored in array 
 positions 0 through
  n −
  1. After removing the maximum value from the heap",NA
7.7 ,NA,NA
Binsort and Radix Sort,"Imagine that for the past year, as you paid your various bills, you then 
 simply piled all the paperwork onto the top of a table somewhere. Now the 
 year has ended and its time to sort all of these papers by what the bill was 
 for (phone, electricity, rent, etc.) and date. A pretty natural approach is to 
 make some space on the floor, and as you go through the pile of papers, put 
 the phone bills into one pile, the electric bills into another pile, and so on. 
 Once this initial assignment of bills to piles is done (in one pass), you can 
 sort each pile by date relatively quickly because they are each fairly small. 
 This is the basic idea behind a Binsort. 
  
  Section 3.9 presented the following code fragment to sort a permutation 
 of the numbers 0 through
  n −
  1:",NA
7.8 ,NA,NA
An Empirical Comparison of Sorting Algorithms,"Which sorting algorithm is fastest? Asymptotic complexity analysis lets us 
 distin-guish between Θ(
 n
 2
 ) and Θ(
 n
  log
  n
 ) algorithms, but it does not help 
 distinguish between algorithms with the same asymptotic complexity. Nor 
 does asymptotic analysis say anything about which algorithm is best for",NA
7.9 ,NA,NA
Lower Bounds for Sorting,"This book contains many analyses for algorithms. These analyses generally 
 define the upper and lower bounds for algorithms in their worst and 
 average cases. For many of the algorithms presented so far, analysis has 
 been easy. This section con-siders a more difficult task — an analysis for the 
 cost of a
  problem
  as opposed to an 
 algorithm
 . The upper bound for a 
 problem can be defined as the asymptotic cost of the fastest known 
 algorithm. The lower bound defines the best possible efficiency for
  any
  
 algorithm that solves the problem, including algorithms not yet invented. 
 Once the upper and lower bounds for the problem meet, we know that no 
 future algorithm can possibly be (asymptotically) more efficient. 
 A simple estimate for a problem’s lower bound can be obtained by 
 measuring the size of the input that must be read and the output that must 
 be written. Certainly no algorithm can be more efficient than the problem’s 
 I/O time. From this we see that the sorting problem cannot be solved by
  any
  
 algorithm in less than Ω(
 n
 ) time because it takes at least
  n
  steps to read and 
 write the
  n
  values to be sorted. Alter-natively, any sorting algorithm must at 
 least look at every input vale to recognize whether the input values are in 
 sort order. So, based on our current knowledge of sorting algorithms and the 
 size of the input, we know that the
  problem
  of sorting is bounded by Ω(
 n
 ) 
 and O(
 n
  log
  n
 ). 
 Computer scientists have spent much time devising efficient general-
 purpose sorting algorithms, but no one has ever found one that is faster than",NA
7.10 ,NA,NA
Further Reading,"The definitive reference on sorting is Donald E. Knuth’s
  Sorting and 
 Searching 
 [Knu98]. A wealth of details is covered there, including optimal 
 sorts for small size
  n
  and special purpose sorting networks. It is a thorough 
 (although somewhat dated) treatment on sorting. For an analysis of 
 Quicksort and a thorough survey on its optimizations, see Robert 
 Sedgewick’s
  Quicksort
  [Sed80]. Sedgewick’s
  Al-gorithms
  [Sed11] discusses 
 most of the sorting algorithms described here and pays special attention to 
 efficient implementation. The optimized Mergesort version of Section 7.4 
 comes from Sedgewick. 
 While Ω(
 n
  log
  n
 ) is the theoretical lower bound in the worst case for 
 sorting, many times the input is sufficiently well ordered that certain 
 algorithms can take advantage of this fact to speed the sorting process. A 
 simple example is Insertion Sort’s best-case running time. Sorting 
 algorithms whose running time is based on the amount of disorder in the 
 input are called
  adaptive
 . For more information on adaptive sorting 
 algorithms, see “A Survey of Adaptive Sorting Algorithms” by Estivill-Castro 
 and Wood [ECW92].",NA
7.11 ,NA,NA
Exercises,"7.1
  Using induction, prove that Insertion Sort will always produce a sorted array.",NA
7.12 ,NA,NA
Projects,"7.1
  One possible improvement for Bubble Sort would be to add a flag 
 variable and a test that determines if an exchange was made during 
 the current iter-ation. If no exchange was made, then the list is sorted 
 and so the algorithm can stop early. This makes the best case 
 performance become O(
 n
 ) (because if the list is already sorted, then no",NA
8,NA,NA
File Processing and External,NA,NA
Sorting,"Earlier chapters presented basic data structures and algorithms that operate 
 on data stored in main memory. Some applications require that large 
 amounts of informa-tion be stored and processed — so much information 
 that it cannot all fit into main memory. In that case, the information must 
 reside on disk and be brought into main memory selectively for processing. 
 You probably already realize that main memory access is much faster 
 than ac-cess to data stored on disk or other storage devices. The relative 
 difference in access times is so great that efficient disk-based programs 
 require a different approach to algorithm design than most programmers 
 are used to. As a result, many program-mers do a poor job when it comes to 
 file processing applications. 
 This chapter presents the fundamental issues relating to the design of 
 algo-rithms and data structures for disk-based applications.
 1
 We begin with a 
 descrip-tion of the significant differences between primary memory and 
 secondary storage. Section 8.2 discusses the physical aspects of disk drives. 
 Section 8.3 presents ba-sic methods for managing buffer pools. Section 8.4 
 discusses the
  C
 ++
 model for random access to data stored on disk. Section 8.5 
 discusses the basic principles for sorting collections of records too large to 
 fit in main memory.",NA
8.1 ,NA,NA
Primary versus Secondary Storage,"Computer storage devices are typically classified into
  primary
  or
  main
  
 memory and
  secondary
  or
  peripheral
  storage. Primary memory usually 
 refers to
  Random
  
 1
 Computer technology changes rapidly. I provide examples of disk drive specifications and 
 other hardware performance numbers that are reasonably up to date as of the time when the 
 book was written. When you read it, the numbers might seem out of date. However, the basic 
 principles do not change. The approximate ratios for time, space, and cost between memory 
 and disk have remained surprisingly steady for over 20 years.",NA
8.2 ,NA,NA
Disk Drives,"A
  C
 ++
 programmer views a random access file stored on disk as a contiguous 
 series of bytes, with those bytes possibly combining to form data records. 
 This is called the
  logical
  file. The
  physical
  file actually stored on disk is 
 usually not a contiguous series of bytes. It could well be in pieces spread all 
 over the disk. The
  file manager
 , a part of the operating system, is 
 responsible for taking requests for data from a logical file and mapping 
 those requests to the physical location of the data on disk. Likewise, when 
 writing to a particular logical byte position with respect to the beginning of 
 the file, this position must be converted by the file manager into the 
 corresponding physical location on the disk. To gain some appreciation for 
 the the approximate time costs for these operations, you need to understand 
 the physical structure and basic workings of a disk drive. 
 Disk drives are often referred to as
  direct access
  storage devices. This 
 means that it takes roughly equal time to access any record in the file. This is 
 in contrast to
  sequential access
  storage devices such as tape drives, which 
 require the tape reader to process data from the beginning of the tape until 
 the desired position has been reached. As you will see, the disk drive is only 
 approximately direct access: At any given time, some records are more 
 quickly accessible than others. 
 8.2.1 Disk Drive Architecture
  
 A hard disk drive is composed of one or more round
  platters
 , stacked one 
 on top of another and attached to a central
  spindle
 . Platters spin 
 continuously at a constant rate. Each usable surface of each platter is 
 assigned a
  read/write head
  or
  I/O head
  through which data are read or 
 written, somewhat like the arrangement of a phonograph player’s arm 
 “reading” sound from a phonograph record. Unlike a phonograph needle, the 
 disk read/write head does not actually touch the surface of a hard disk. 
 Instead, it remains slightly above the surface, and any contact during normal 
 operation would damage the disk. This distance is very small, much smaller 
 than the height of a dust particle. It can be likened to a 5000-kilometer 
 airplane trip across the United States, with the plane flying at a height of one 
 meter! 
 A hard disk drive typically has several platters and several read/write 
 heads, as shown in Figure 8.2(a). Each head is attached to an
  arm
 , which 
 connects to the 
 boom
 .
 2
 The boom moves all of the heads in or out together. 
 When the heads are in some position over the platters, there are data on 
 each platter directly accessible",NA
8.3,NA,NA
Buffers and Buffer Pools,"Given the specifications of the disk drive from Example 8.1, we find that it 
 takes about 9
 .
 5+11
 .
 1
 ×
 1
 .
 5 = 26
 .
 2 ms to read one track of data on average. It 
 takes about 9
 .
 5+11
 .
 1
 /
 2+(1
 /
 256)
 ×
 11
 .
 1 = 15
 .
 1 ms on average to read a single 
 sector of data. This is a good savings (slightly over half the time), but less 
 than 1% of the data on the track are read. If we want to read only a single 
 byte, it would save us effectively no time over that required to read an entire 
 sector. For this reason, nearly all disk drives automatically read or write an 
 entire sector’s worth of information whenever the disk is accessed, even 
 when only one byte of information is requested. 
 Once a sector is read, its information is stored in main memory. This is 
 known as
  buffering
  or
  caching
  the information. If the next disk request is to 
 that same sector, then it is not necessary to read from disk again because the 
 information is already stored in main memory. Buffering is an example of 
 one method for minimizing disk accesses mentioned at the beginning of the 
 chapter: Bring off additional information from disk to satisfy future requests. 
 If information from files were accessed at random, then the chance that two 
 consecutive disk requests are to the same sector would be low. However, in 
 practice most disk requests are close to the location (in the logical file at 
 least) of the previous request. This means that the probability of the next 
 request “hitting the cache” is much higher than chance would indicate. 
 This principle explains one reason why average access times for new 
 disk drives are lower than in the past. Not only is the hardware faster, but 
 information is also now stored using better algorithms and larger caches 
 that minimize the number of times information needs to be fetched from 
 disk. This same concept is also used to store parts of programs in faster 
 memory within the CPU, using the CPU cache that is prevalent in modern 
 microprocessors. 
 Sector-level buffering is normally provided by the operating system and 
 is of-ten built directly into the disk drive controller hardware. Most 
 operating systems maintain at least two buffers, one for input and one for 
 output. Consider what would happen if there were only one buffer during a 
 byte-by-byte copy operation. The sector containing the first byte would be 
 read into the I/O buffer. The output operation would need to destroy the",NA
8.4 ,NA,NA
The Programmer’s View of Files,"The
  C
 ++
 programmer’s logical view of a random access file is a single stream 
 of bytes. Interaction with a file can be viewed as a communications channel 
 for issuing one of three instructions: read bytes from the current position in 
 the file, write bytes to the current position in the file, and move the current 
 position within the file. You do not normally see how the bytes are stored in 
 sectors, clusters, and so forth. The mapping from logical to physical 
 addresses is done by the file system, and sector-level buffering is done 
 automatically by the disk controller. 
 When processing records in a disk file, the order of access can have a 
 great effect on I/O time. A
  random access
  procedure processes records in 
 an order independent of their logical order within the file.
  Sequential 
 access
  processes records in order of their logical appearance within the file. 
 Sequential processing requires less seek time if the physical layout of the 
 disk file matches its logical layout, as would be expected if the file were 
 created on a disk with a high percentage of free space. 
 C
 ++
 provides several mechanisms for manipulating disk files. One of the 
 most commonly used is the
  fstream
  class. The following methods can be 
 used to manipulate information in the file.",NA
8.5 ,NA,NA
External Sorting,"We now consider the problem of sorting collections of records too large to 
 fit in main memory. Because the records must reside in peripheral or 
 external memory, such sorting methods are called
  external sorts
 . This is in 
 contrast to the internal sorts discussed in Chapter 7 which assume that the 
 records to be sorted are stored in main memory. Sorting large collections of 
 records is central to many applications, such as processing payrolls and 
 other large business databases. As a consequence, many external sorting 
 algorithms have been devised. Years ago, sorting algorithm designers sought 
 to optimize the use of specific hardware configurations, such as multiple 
 tape or disk drives. Most computing today is done on personal computers 
 and low-end workstations with relatively powerful CPUs, but only one or at 
 most two disk drives. The techniques presented here are geared toward 
 optimized pro-cessing on a single disk drive. This approach allows us to 
 cover the most important issues in external sorting while skipping many less 
 important machine-dependent details. Readers who have a need to 
 implement efficient external sorting algorithms that take advantage of more 
 sophisticated hardware configurations should consult the references in 
 Section 8.6. 
 When a collection of records is too large to fit in main memory, the only 
 prac-tical way to sort it is to read some records from disk, do some 
 rearranging, then write them back to disk. This process is repeated until the",NA
8.6 ,NA,NA
Further Reading,"A good general text on file processing is Folk and Zoellick’s
  File Structures: A 
 Conceptual Toolkit
  [FZ98]. A somewhat more advanced discussion on key 
 issues in file processing is Betty Salzberg’s
  File Structures: An Analytical 
 Approach
  [Sal88]. A great discussion on external sorting methods can be 
 found in Salzberg’s book. The presentation in this chapter is similar in spirit 
 to Salzberg’s. 
 For details on disk drive modeling and measurement, see the article by 
 Ruemm-ler and Wilkes, “An Introduction to Disk Drive Modeling” [RW94]. 
 See Andrew S. Tanenbaum’s
  Structured Computer Organization
  [Tan06] for 
 an introduction to computer hardware and organization. An excellent, 
 detailed description of mem-ory and hard disk drives can be found online at 
 “The PC Guide,” by Charles M. Kozierok [Koz05] (www.pcguide.com). The PC 
 Guide also gives detailed de-scriptions of the Microsoft Windows and UNIX 
 (Linux) file systems. 
 See “Outperforming LRU with an Adaptive Replacement Cache 
 Algorithm”by Megiddo and Modha [MM04] for an example of a more 
 sophisticated algorithm than LRU for managing buffer pools.",NA
Exercises,Chap. 8 File Processing and External Sorting,NA
8.7,"8.1
  Computer memory and storage prices change rapidly. Find out what 
 the current prices are for the media listed in Figure 8.1. Does your 
 information change any of the basic conclusions regarding disk 
 processing? 
 8.2
  Assume a disk drive from the late 1990s is configured as follows. The 
 to-tal storage is approximately 675MB divided among 15 surfaces. 
 Each sur-face has 612 tracks; there are 144 sectors/track, 512 
 bytes/sector, and 8 sec-tors/cluster. The disk turns at 3600 rpm. The 
 track-to-track seek time is 20 ms, and the average seek time is 80 ms. 
 Now assume that there is a 360KB file on the disk. On average, how 
 long does it take to read all of the data in the file? Assume that the first 
 track of the file is randomly placed on the disk, that the entire file lies 
 on adjacent tracks, and that the file completely fills each track on 
 which it is found. A seek must be performed each time the I/O head 
 moves to a new track. Show your calculations. 
 8.3
  Using the specifications for the disk drive given in Exercise 8.2, 
 calculate the expected time to read one entire track, one sector, and 
 one byte. Show your calculations. 
 8.4
  Using the disk drive specifications given in Exercise 8.2, calculate the 
 time  
 required to read a 10MB file assuming 
 (a)
  The file is stored on a series of contiguous tracks, as few tracks as 
 pos- 
 sible. 
 (b)
  The file is spread randomly across the disk in 4KB clusters. 
 Show your calculations. 
 8.5
  Assume that a disk drive is configured as follows. The total storage is 
 ap-proximately 1033MB divided among 15 surfaces. Each surface has 
 2100 tracks, there are 64 sectors/track, 512 bytes/sector, and 8 
 sectors/cluster. The disk turns at 7200 rpm. The track-to-track seek 
 time is 3 ms, and the average seek time is 20 ms. Now assume that 
 there is a 512KB file on the disk. On average, how long does it take to 
 read all of the data on the file? Assume that the first track of the file is 
 randomly placed on the disk, that the entire file lies on contiguous 
 tracks, and that the file completely fills each track on which it is found. 
 Show your calculations. 
 8.6
  Using the specifications for the disk drive given in Exercise 8.5, 
 calculate the expected time to read one entire track, one sector, and 
 one byte. Show your calculations. 
 8.7
  Using the disk drive specifications given in Exercise 8.5, calculate the 
 time  
 required to read a 10MB file assuming 
 (a)
  The file is stored on a series of contiguous tracks, as few tracks as 
 pos- 
 sible. 
 (b)
  The file is spread randomly across the disk in 4KB clusters.",NA
8.8 ,NA,NA
Projects,"8.1
  For a database application, assume it takes 10 ms to read a block from 
 disk, 1 ms to search for a record in a block stored in memory, and that 
 there is room in memory for a buffer pool of 5 blocks. Requests come 
 in for records, with the request specifying which block contains the 
 record. If a block is accessed, there is a 10% probability for each of the 
 next ten requests that the request will be to the same block. What will 
 be the expected performance improvement for each of the following 
 modifications to the system? 
 (a)
  Get a CPU that is twice as fast.",NA
9,NA,NA
Searching,"Organizing and retrieving information is at the heart of most computer 
 applica-tions, and searching is surely the most frequently performed of all 
 computing tasks. Search can be viewed abstractly as a process to determine 
 if an element with a par-ticular value is a member of a particular set. The 
 more common view of searching is an attempt to find the record within a 
 collection of records that has a particular key value, or those records in a 
 collection whose key values meet some criterion such as falling within a 
 range of values. 
  
 We can define searching formally as follows. Suppose that we have a 
 collection 
 L
  of
  n
  records of the form 
 (
 k
 1
 , I
 1
 )
 ,
  (
 k
 2
 , I
 2
 )
 , ...,
  (
 k
 n
 , I
 n
 ) 
 where
  I
 j
  is information associated with key
  k
 j
  from record
  j
  for 1
  ≤ j ≤ n
 . 
 Given a particular key value
  K
 , the
  search problem
  is to locate a record (
 k
 j
 , 
 I
 j
 ) in
  L 
 such that
  k
 j
  =
  K
  (if one exists).
  Searching
  is a systematic method for 
 locating the record (or records) with key value
  k
 j
  =
  K
 . 
 A
  successful
  search is one in which a record with key
  k
 j
  =
  K
  is found. An 
 unsuccessful
  search is one in which no record with
  k
 j
  =
  K
  is found (and no 
 such record exists). 
 An
  exact-match query
  is a search for the record whose key value 
 matches a specified key value. A
  range query
  is a search for all records 
 whose key value falls within a specified range of key values. 
 We can categorize search algorithms into three general approaches: 
 1.
  Sequential and list methods. 
 2.
  Direct access by key value 
 (hashing).  
 3.
  Tree indexing methods.",NA
9.1 ,NA,NA
Searching Unsorted and Sorted Arrays,"The simplest form of search has already been presented in Example 3.1: the 
 se-quential search algorithm. Sequential search on an unsorted list requires 
 Θ(
 n
 ) time in the worst case. 
 How many comparisons does linear search do on average? A major 
 consid-eration is whether
  K
  is in list
  L
  at all. We can simplify our analysis by 
 ignoring everything about the input except the position of
  K
  if it is found in
  
 L
 . Thus, we have 
 n
  + 1 distinct possible events: That
  K
  is in one of positions 0 
 to
  n −
  1 in
  L
  (each position having its own probability), or that it is not in
  L
  at 
 all. We can express the probability that
  K
  is not in
  L
  as 
 n
  
 P
 (
 K /∈
  L
 ) = 1
  −
  
 i
 =1
  
 P
 (
 K
  =
  L
 [
 i
 ]) 
 where
  P
 (
 x
 ) is the probability of event
  x
 . 
 For any position
  i
  in the list, we must look at
  i
  + 1 records to reach it. So we say Let
  p
 i
  be 
 the probability that
  K
  is in position
  i
  of
  L
  (indexed from 0 to
  n −
  1.",NA
9.2 ,NA,NA
Self-Organizing Lists,NA,NA
9.3 ,NA,NA
Bit Vectors for Representing Sets,"Determining whether a value is a member of a particular set is a special case 
 of searching for keys in a sequence of records. Thus, any of the search 
 methods discussed in this book can be used to check for set membership. 
 However, we can also take advantage of the restricted circumstances 
 imposed by this problem to develop another representation. 
 In the case where the set values fall within a limited range, we can 
 represent the set using a bit array with a bit position allocated for each 
 potential member. Those members actually in the set store a value of 1 in 
 their corresponding bit; those members not in the set store a value of 0 in 
 their corresponding bit. For example, consider the set of primes between 0 
 and 15. Figure 9.1 shows the corresponding bit array. To determine if a 
 particular value is prime, we simply check the corre-sponding bit. This 
 representation scheme is called a
  bit vector
  or a
  bitmap
 . The mark array 
 used in several of the graph algorithms of Chapter 11 is an example of such a 
 set representation. 
 If the set fits within a single computer word, then set union, intersection, 
 and difference can be performed by logical bit-wise operations. The union of 
 sets
  A 
 and
  B
  is the bit-wise OR function (whose symbol is
  |
  in
  C
 ++
 ). The 
 intersection of sets
  A
  and
  B
  is the bit-wise AND function (whose symbol is
  &
  
 in
  C
 ++
 ). For example, if we would like to compute the set of numbers 
 between 0 and 15 that are both prime and odd numbers, we need only 
 compute the expression 
 0011010100010100 & 0101010101010101
 .
  
 The set difference
  A − B
  can be implemented in
  C
 ++
 using the expression
  
 A&˜B 
 (
 ˜
  is the symbol for bit-wise negation). For larger sets that do not fit",NA
9.4 ,NA,NA
Hashing,"This section presents a completely different approach to searching arrays: 
 by direct access based on key value. The process of finding a record using 
 some computa-tion to map its key value to a position in the array is called
  
 hashing
 . Most hash-ing schemes place records in the array in whatever 
 order satisfies the needs of the address calculation, thus the records are not 
 ordered by value or frequency. The function that maps key values to 
 positions is called a
  hash function
  and will be denoted by
  h
 . The array that 
 holds the records is called the
  hash table
  and will be denoted by
  HT
 . A 
 position in the hash table is also known as a
  slot
 . The number of slots in 
 hash table
  HT
  will be denoted by the variable
  M
 , with slots numbered from 0 
 to
  M −
  1. The goal for a hashing system is to arrange things such that, for any 
 key value
  K
  and some hash function
  h
 ,
  i
  =
  h
 (
 K
 ) is a slot in the table such that 
 0
  ≤
  h
 (
 K
 )
  < M
 , and we have the key of the record stored at
  HT
 [
 i
 ] equal to 
 K
 . 
 Hashing is not good for applications where multiple records with the 
 same key value are permitted. Hashing is not a good method for answering 
 range searches. In other words, we cannot easily find all records (if any) 
 whose key values fall within a certain range. Nor can we easily find the 
 record with the minimum or maximum key value, or visit the records in key 
 order. Hashing is most appropriate for answer-ing the question, “What 
 record, if any, has key value
  K
 ?” For applications where access involves only 
 exact-match queries, hashing is usually the search method of choice because 
 it is extremely efficient when implemented correctly. As you will see in this 
 section, however, there are many approaches to hashing and it is easy to 
 devise an inefficient implementation. Hashing is suitable for both in-memory 
 and disk-based searching and is one of the two most widely used methods 
 for or-ganizing large databases stored on disk (the other is the B-tree, which 
 is covered in Chapter 10).",NA
9.5 ,NA,NA
Further Reading,"For a comparison of the efficiencies for various self-organizing techniques, 
 see Bentley and McGeoch, “Amortized Analysis of Self-Organizing Sequential 
 Search Heuristics” [BM85]. The text compression example of Section 9.2 
 comes from Bentley et al., “A Locally Adaptive Data Compression Scheme” 
 [BSTW86]. For more on Ziv-Lempel coding, see
  Data Compression: Methods 
 and Theory
  by James A. Storer [Sto88]. Knuth covers self-organizing lists and 
 Zipf distributions in Volume 3 of
  The Art of Computer Programming
 [Knu98]. 
  
 Introduction to Modern Information Retrieval
  by Salton and McGill 
 [SM83] is an excellent source for more information about document retrieval 
 techniques. 
 See the paper “Practical Minimal Perfect Hash Functions for Large 
 Databases”by Fox et al. [FHCD92] for an introduction and a good algorithm 
 for perfect hash-ing. 
 For further details on the analysis for various collision resolution 
 policies, see Knuth, Volume 3 [Knu98] and
  Concrete Mathematics: A 
 Foundation for Computer Science
  by Graham, Knuth, and Patashnik [GKP94]. 
 The model of hashing presented in this chapter has been of a fixed-size 
 hash table. A problem not addressed is what to do when the hash table gets 
 half full and more records must be inserted. This is the domain of dynamic 
 hashing methods. A good introduction to this topic is “Dynamic Hashing 
 Schemes” by R.J. Enbody and H.C. Du [ED88].",NA
9.6 ,NA,NA
Exercises,"9.1
  Create a graph showing expected cost versus the probability of an 
 unsuc-cessful search when performing sequential search (see Section 
 9.1). What",NA
9.7 ,NA,NA
Projects,"9.1
  Implement a binary search and the quadratic binary search of Section 9.1. 
 Run your implementations over a large range of problem sizes, timing 
 the results for each algorithm. Graph and compare these timing results. 
 9.2
  Implement the three self-organizing list heuristics count, move-to-
 front, and transpose. Compare the cost for running the three heuristics 
 on various input data. The cost metric should be the total number of 
 comparisons required when searching the list. It is important to 
 compare the heuristics using input data for which self-organizing lists 
 are reasonable, that is, on frequency dis-tributions that are uneven. 
 One good approach is to read text files. The list should store individual 
 words in the text file. Begin with an empty list, as was done for the text 
 compression example of Section 9.2. Each time a word is encountered 
 in the text file, search for it in the self-organizing list. If the",NA
10,NA,NA
Indexing,"Many large-scale computing applications are centered around data sets that 
 are too large to fit into main memory. The classic example is a large database 
 of records with multiple search keys, requiring the ability to insert, delete, 
 and search for records. Hashing provides outstanding performance for such 
 situations, but only in the limited case in which all searches are of the form 
 “find the record with key value
  K
 .” Many applications require more general 
 search capabilities. One exam-ple is a range query search for all records 
 whose key lies within some range. Other queries might involve visiting all 
 records in order of their key value, or finding the record with the greatest 
 key value. Hash tables are not organized to support any of these queries 
 efficiently. 
 This chapter introduces file structures used to organize a large collection 
 of records stored on disk. Such file structures support efficient insertion, 
 deletion, and search operations, for exact-match queries, range queries, and 
 largest/smallest key value searches. 
 Before discussing such file structures, we must become familiar with 
 some ba-sic file-processing terminology. An
  entry-sequenced file
  stores 
 records in the order that they were added to the file. Entry-sequenced files 
 are the disk-based equivalent to an unsorted list and so do not support 
 efficient search. The natural solution is to sort the records by order of the 
 search key. However, a typical database, such as a collection of employee or 
 customer records maintained by a business, might con-tain multiple search 
 keys. To answer a question about a particular customer might require a 
 search on the name of the customer. Businesses often wish to sort and 
 output the records by zip code order for a bulk mailing. Government 
 paperwork might require the ability to search by Social Security number. 
 Thus, there might not be a single “correct” order in which to store the 
 records. 
 Indexing
  is the process of associating a key with the location of a 
 correspond-ing data record. Section 8.5 discussed the concept of a key sort, 
 in which an
  index file
  is created whose records consist of key/pointer pairs. 
 Here, each key is asso-ciated with a pointer to a complete record in the main 
 database file. The index file",NA
10.1 ,NA,NA
Linear Indexing,"A
  linear index
  is an index file organized as a sequence of key/pointer pairs 
 where the keys are in sorted order and the pointers either (1) point to the 
 position of the complete record on disk, (2) point to the position of the 
 primary key in the primary index, or (3) are actually the value of the 
 primary key. Depending on its size, a linear index might be stored in main 
 memory or on disk. A linear index provides a number of advantages. It 
 provides convenient access to variable-length database records, because 
 each entry in the index file contains a fixed-length key field and a fixed-
 length pointer to the beginning of a (variable-length) record as shown in 
 Figure 10.1. A linear index also allows for efficient search and random access 
 to database records, because it is amenable to binary search. 
 If the database contains enough records, the linear index might be too 
 large to store in main memory. This makes binary search of the index more 
 expensive because many disk accesses would typically be required by the 
 search process. One solution to this problem is to store a second-level linear 
 index in main memory that indicates which disk block in the index file stores 
 a desired key. For example, the linear index on disk might reside in a series 
 of 1024-byte blocks. If each key/pointer pair in the linear index requires 8 
 bytes (a 4-byte key and a 4-byte pointer), then 128 key/pointer pairs are 
 stored per block. The second-level index, stored in main memory, consists of 
 a simple table storing the value of the key in the first position of each block 
 in the linear index file. This arrangement is shown in Figure 10.2. If the 
 linear index requires 1024 disk blocks (1MB), the second-level index 
 contains only 1024 entries, one per disk block. To find which disk block 
 contains a desired search key value, first search through the 1024-entry 
 table to find the greatest value less than or equal to the search key. This 
 directs the search to the proper block in the index file, which is then read 
 into memory. At this point, a binary search within this block will produce a 
 pointer to the actual record in the database. Because the",NA
10.2 ,NA,NA
ISAM,"How do we handle large databases that require frequent update? The main 
 problem with the linear index is that it is a single, large array that does not 
 adjust well to updates because a single update can require changing the 
 position of every key in the index. Inverted lists reduce this problem, but 
 they are only suitable for sec-ondary key indices with many fewer secondary 
 key values than records. The linear index would perform well as a primary 
 key index if it could somehow be broken into pieces such that individual 
 updates affect only a part of the index. This con-cept will be pursued 
 throughout the rest of this chapter, eventually culminating in the B
 +
 -tree, the 
 most widely used indexing method today. But first, we begin by studying 
 ISAM, an early attempt to solve the problem of large databases requiring 
 frequent update. Its weaknesses help to illustrate why the B
 +
 -tree works so 
 well. 
 Before the invention of effective tree indexing schemes, a variety of disk-
 based indexing methods were in use. All were rather cumbersome, largely 
 because no adequate method for handling updates was known. Typically, 
 updates would cause the index to degrade in performance. ISAM is one 
 example of such an index and was widely used by IBM prior to adoption of 
 the B-tree.",NA
10.3 ,NA,NA
Tree-based Indexing,"Linear indexing is efficient when the database is static, that is, when records 
 are inserted and deleted rarely or never. ISAM is adequate for a limited 
 number of updates, but not for frequent changes. Because it has essentially 
 two levels of indexing, ISAM will also break down for a truly large database 
 where the number of cylinders is too great for the top-level index to fit in 
 main memory. 
  
 In their most general form, database applications have the following 
 character-istics: 
 1.
  Large sets of records that are frequently 
 updated.  
 2.
  Search is by one or a combination of several 
 keys.  
 3.
  Key range queries or min/max queries are 
 used. 
 For such databases, a better organization must be found. One approach 
 would be to use the binary search tree (BST) to store primary and secondary 
 key indices. BSTs can store duplicate key values, they provide efficient 
 insertion and deletion as well as efficient search, and they can perform 
 efficient range queries. When there is enough main memory, the BST is a 
 viable option for implementing both primary and secondary key indices. 
 Unfortunately, the BST can become unbalanced. Even under relatively 
 good conditions, the depth of leaf nodes can easily vary by a factor of two. 
 This might not be a significant concern when the tree is stored in main 
 memory because the time required is still Θ(log
  n
 ) for search and update. 
 When the tree is stored on disk, however, the depth of nodes in the tree 
 becomes crucial. Every time a BST node
  B
  is visited, it is necessary to visit all 
 nodes along the path from the root to
  B
 . Each node on this path must be 
 retrieved from disk. Each disk access returns a block of information. If a 
 node is on the same block as its parent, then the cost to find that node is 
 trivial once its parent is in main memory. Thus, it is desirable to keep 
 subtrees together on the same block. Unfortunately, many times a node is 
 not on the same block as its parent. Thus, each access to a BST node could 
 potentially require that another block to be read from disk. Using a buffer 
 pool to store multiple blocks in memory can mitigate disk access problems if 
 BST accesses display good locality of reference. But a buffer pool cannot",NA
10.4 ,NA,NA
2-3 Trees,"This section presents a data structure called the 2-3 tree. The 2-3 tree is not 
 a binary tree, but instead its shape obeys the following definition: 
 1.
  A node contains one or two keys. 
 2.
  Every internal node has either two children (if it contains one key) or 
 three  children (if it contains two keys). Hence the name. 
 3.
  All leaves are at the same level in the tree, so the tree is always height 
 bal- 
 anced. 
 In addition to these shape properties, the 2-3 tree has a search tree 
 property analogous to that of a BST. For every node, the values of all 
 descendants in the left subtree are less than the value of the first key, while 
 values in the center subtree are greater than or equal to the value of the first 
 key. If there is a right subtree (equivalently, if the node stores two keys), 
 then the values of all descendants in the center subtree are less than the 
 value of the second key, while values in the right subtree are greater than or 
 equal to the value of the second key. To maintain these shape and search 
 properties requires that special action be taken when nodes are inserted 
 and deleted. The 2-3 tree has the advantage over the BST in that the 2-3 tree 
 can be kept height balanced at relatively low cost. 
 Figure 10.9 illustrates the 2-3 tree. Nodes are shown as rectangular 
 boxes with two key fields. (These nodes actually would contain complete 
 records or pointers to complete records, but the figures will show only the 
 keys.) Internal nodes with only two children have an empty right key field. 
 Leaf nodes might contain either one or two keys. Figure 10.10 is an 
 implementation for the 2-3 tree node. Class 
 TTNode
  is assumed to be a 
 private class of the the 2-3 tree class
  TTTree
 , and thus the data members of
  
 TTNode
  will be made public to simplify the presentation. 
 Note that this sample declaration does not distinguish between leaf and 
 internal nodes and so is space inefficient, because leaf nodes store three 
 pointers each. The",NA
10.5 ,NA,NA
B-Trees,"This section presents the B-tree. B-trees are usually attributed to R. Bayer 
 and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees 
 had re-",NA
10.6 ,NA,NA
Further Reading,"For an expanded discussion of the issues touched on in this chapter, see a 
 gen-eral file processing text such as
  File Structures: A Conceptual Toolkit
  by 
 Folk and Zoellick [FZ98]. In particular, Folk and Zoellick provide a good 
 discussion of the relationship between primary and secondary indices. The 
 most thorough dis-cussion on various implementations for the B-tree is the 
 survey article by Comer [Com79]. Also see [Sal88] for further details on 
 implementing B-trees. See Shaf-fer and Brown [SB93] for a discussion of 
 buffer pool management strategies for B
 +
 -tree-like data structures.",NA
10.7 ,NA,NA
Exercises,"10.1
  Assume that a computer system has disk blocks of 1024 bytes, and 
 that you are storing records that have 4-byte keys and 4-byte data 
 fields. The records are sorted and packed sequentially into the disk 
 file. 
 (a)
  Assume that a linear index uses 4 bytes to store the key and 4 
 bytes to store the block ID for the associated records. What is the 
 greatest number of records that can be stored in the file if a linear 
 index of size 256KB is used? 
 (b)
  What is the greatest number of records that can be stored in the 
 file if the linear index is also stored on disk (and thus its size is 
 limited only by the second-level index) when using a second-level 
 index of 1024 bytes (i.e., 256 key values) as illustrated by Figure 
 10.2? Each element of the second-level index references the 
 smallest key value for a disk block of the linear index. 
 10.2
  Assume that a computer system has disk blocks of 4096 bytes, and 
 that you are storing records that have 4-byte keys and 64-byte data 
 fields. The records are sorted and packed sequentially into the disk 
 file.",NA
10.8 ,NA,NA
Projects,"10.1
  Implement a two-level linear index for variable-length records as 
 illustrated by Figures 10.1 and 10.2. Assume that disk blocks are 1024 
 bytes in length. Records in the database file should typically range 
 between 20 and 200 bytes, including a 4-byte key value. Each record of 
 the index file should store a key value and the byte offset in the 
 database file for the first byte of the",NA
PART IV ,NA,NA
Advanced Data Structures,379,NA
11,NA,NA
Graphs,"Graphs provide the ultimate in data structure flexibility. Graphs can model 
 both real-world systems and abstract problems, so they are used in 
 hundreds of applica-tions. Here is a small sampling of the range of problems 
 that graphs are routinely applied to. 
 1.
  Modeling connectivity in computer and communications networks. 
 2.
  Representing a map as a set of locations with distances between 
 locations;  
 used to compute shortest routes between locations. 
 3.
  Modeling flow capacities in transportation networks. 
 4.
  Finding a path from a starting condition to a goal condition; for 
 example, in  
 artificial intelligence problem solving. 
 5.
  Modeling computer algorithms, showing transitions from one program 
 state  
 to another. 
 6.
  Finding an acceptable order for finishing subtasks in a complex activity, 
 such  
 as constructing large buildings. 
 7.
  Modeling relationships such as family trees, business or military 
 organiza- 
 tions, and scientific taxonomies. 
 We begin in Section 11.1 with some basic graph terminology and then 
 define two fundamental representations for graphs, the adjacency matrix 
 and adjacency list. Section 11.2 presents a graph ADT and simple 
 implementations based on the adjacency matrix and adjacency list. Section 
 11.3 presents the two most commonly used graph traversal algorithms, 
 called depth-first and breadth-first search, with application to topological 
 sorting. Section 11.4 presents algorithms for solving some problems related 
 to finding shortest routes in a graph. Finally, Section 11.5 presents 
 algorithms for finding the minimum-cost spanning tree, useful for deter-
 mining lowest-cost connectivity in a network. Besides being useful and 
 interesting in their own right, these algorithms illustrate the use of some 
 data structures pre-sented in earlier chapters. 
 381",NA
11.1,NA,NA
Terminology and Representations,"A graph
  G
  = (
 V
 ,
  E
 ) consists of a set of vertices
  V
  and a set of edges
  E
 , such 
 that each edge in
  E
  is a connection between a pair of vertices in
  V
 .
 1
 The 
 number of vertices is written
  |
 V
 |
 , and the number of edges is written
  |
 E
 |
 .
  |
 E
 |
  
 can range from zero to a maximum of
  |
 V
 |
 2
 − |
 V
 |
 . A graph with relatively few 
 edges is called 
 sparse
 , while a graph with many edges is called
  dense
 . A 
 graph containing all possible edges is said to be
  complete
 . 
 A graph with edges directed from one vertex to another (as in Figure 
 11.1(b)) is called a
  directed graph
  or
  digraph
 . A graph whose edges are not 
 directed is called an
  undirected graph
  (as illustrated by Figure 11.1(a)). A 
 graph with labels associated with its vertices (as in Figure 11.1(c)) is called a
  
 labeled graph
 . Two vertices are
  adjacent
  if they are joined by an edge. Such 
 vertices are also called 
 neighbors
 . An edge connecting Vertices
  U
  and
  V
  is 
 written (
 U
 ,
  V
 ). Such an edge is said to be
  incident
  on Vertices
  U
  and
  V
 . 
 Associated with each edge may be a cost or
  weight
 . Graphs whose edges 
 have weights (as in Figure 11.1(c)) are said to be
  weighted
 . 
 A sequence of vertices
  v
 1
 ,
  v
 2
 , ...,
  v
 n
  forms a
  path
  of length
  n −
  1 if there 
 exist edges from
  v
 i
  to
  v
 i
 +1
  for 1
  ≤ i < n
 . A path is
  simple
  if all vertices on the 
 path are distinct. The
  length
  of a path is the number of edges it contains. A
  
 cycle
  is a path of length three or more that connects some vertex
  v
 1
  to itself. 
 A cycle is
  simple
  if the path is simple, except for the first and last vertices 
 being the same. 
 1
 Some graph applications require that a given pair of vertices can have multiple or parallel 
 edges connecting them, or that a vertex can have an edge to itself. However, the applications 
 discussed in this book do not require either of these special cases, so for simplicity we will 
 assume that they cannot occur.",NA
11.2 ,NA,NA
Graph Implementations,"We next turn to the problem of implementing a general-purpose graph class. 
 Fig-ure 11.5 shows an abstract class defining an ADT for graphs. Vertices are 
 defined by an integer index value. In other words, there is a Vertex 0, Vertex 
 1, and so on. We can assume that a graph application stores any additional 
 information of interest about a given vertex elsewhere, such as a name or 
 application-dependent value. Note that this ADT is not implemented using a 
 template, because it is the 
 Graph
  class users’ responsibility to maintain 
 information related to the vertices themselves. The
  Graph
  class need have 
 no knowledge of the type or content of the information associated with a 
 vertex, only the index number for that vertex. 
 Abstract class
  Graph
  has methods to return the number of vertices and 
 edges (methods
  n
  and
  e
 , respectively). Function
  weight
  returns the weight 
 of a given edge, with that edge identified by its two incident vertices. For 
 example, calling 
 weight(0, 4)
  on the graph of Figure 11.1 (c) would return 4. 
 If no such edge exists, the weight is defined to be 0. So calling
  weight(0, 2)
  
 on the graph of Figure 11.1 (c) would return 0. 
 Functions
  setEdge
  and
  delEdge
  set the weight of an edge and remove an 
 edge from the graph, respectively. Again, an edge is identified by its two 
 incident vertices.
  setEdge
  does not permit the user to set the weight to be 0, 
 because this value is used to indicate a non-existent edge, nor are negative 
 edge weights per-mitted. Functions
  getMark
  and
  setMark
  get and set, 
 respectively, a requested value in the
  Mark
  array (described below) for 
 Vertex
  V
 . 
 Nearly every graph algorithm presented in this chapter will require 
 visits to all neighbors of a given vertex. Two methods are provided to 
 support this. They work in a manner similar to linked list access functions. 
 Function
  first
  takes as input a vertex
  V
 , and returns the edge to the first 
 neighbor for
  V
  (we assume the neighbor list is sorted by vertex number). 
 Function
  next
  takes as input Vertices
  V1
  and
  V2 
 and returns the index for 
 the vertex forming the next edge with
  V1
  after
  V2
  on
  V1
 ’s edge list. Function
  
 next
  will return a value of
  n
  =
  |
 V
 |
  once the end of the edge list for
  V1
  has 
 been reached. The following line appears in many graph algorithms: 
 for (w = G=>first(v); w < G->n(); w = G->next(v,w))
  
 This
  for
  loop gets the first neighbor of
  v
 , then works through the remaining 
 neigh-bors of
  v
  until a value equal to
  G->n()
  is returned, signaling that all 
 neighbors of
  v
  have been visited. For example,
  first(1)
  in Figure 11.4 would",NA
11.3 ,NA,NA
Graph Traversals,"Often it is useful to visit the vertices of a graph in some specific order based 
 on the graph’s topology. This is known as a
  graph traversal
  and is similar in 
 concept to a tree traversal. Recall that tree traversals visit every node 
 exactly once, in some specified order such as preorder, inorder, or 
 postorder. Multiple tree traversals exist because various applications 
 require the nodes to be visited in a particular order. For example, to print a 
 BST’s nodes in ascending order requires an inorder traver-sal as opposed to 
 some other traversal. Standard graph traversal orders also exist. Each is 
 appropriate for solving certain problems. For example, many problems in 
 artificial intelligence programming are modeled using graphs. The problem 
 domain may consist of a large collection of states, with connections between 
 various pairs of states. Solving the problem may require getting from a 
 specified start state to a specified goal state by moving between states only 
 through the connections. Typi-cally, the start and goal states are not directly 
 connected. To solve this problem, the vertices of the graph must be searched 
 in some organized manner. 
 Graph traversal algorithms typically begin with a start vertex and 
 attempt to visit the remaining vertices from there. Graph traversals must 
 deal with a number of troublesome cases. First, it may not be possible to 
 reach all vertices from the start vertex. This occurs when the graph is not 
 connected. Second, the graph may contain cycles, and we must make sure 
 that cycles do not cause the algorithm to go into an infinite loop.",NA
11.4 ,NA,NA
Shortest-Paths Problems,"On a road map, a road connecting two towns is typically labeled with its distance. 
 We can model a road network as a directed graph whose edges are labeled with 
 real numbers. These numbers represent the distance (or other cost metric, 
 such as travel time) between two vertices. These labels may be called
  
 weights
 ,
  costs
 , or 
 distances
 , depending on the application. Given such a 
 graph, a typical problem is to find the total length of the shortest path 
 between two specified vertices. This 
 is not a trivial problem, because the shortest path may not be along the edge (if 
 any) connecting two vertices, but rather may be along a path involving one or more 
 intermediate vertices. For example, in Figure 11.16, the cost of the path from
  A
  to 
 B
  to
  D
  is 15. The cost of the edge directly from
  A
  to
  D
  is 20. The cost of the path 
 from
  A
  to
  C
  to
  B
  to
  D
  is 10. Thus, the shortest path from
  A
  to
  D
  is 10 (not along 
 the edge connecting
  A
  to
  D
 ). We use the notation d(
 A
 ,
  D
 ) = 10 to indicate that the 
 shortest distance from
  A
  to
  D
  is 10. In Figure 11.16, there is no path from
  E
  to
  B
 , so 
 we set d(
 E
 ,
  B
 ) =
  ∞
 . We define w(
 A
 ,
  D
 ) = 20 to be the weight of edge (
 A
 ,
  D
 ), that is, the 
 weight of the direct connection from
  A
  to
  D
 . Because there is no edge from 
 E
  to
  B
 , w(
 E
 ,
  B
 ) =
  ∞
 . Note that w(
 D
 ,
  A
 ) =
  ∞
  because the graph of Figure 11.16 is directed. 
 We assume that all weights are positive.",NA
11.5 ,NA,NA
Minimum-Cost Spanning Trees,"The
  minimum-cost spanning tree
  (MST) problem takes as input a 
 connected, undirected graph
  G
 , where each edge has a distance or weight 
 measure attached. The MST is the graph containing the vertices of
  G
  along 
 with the subset of
  G
 ’s edges that (1) has minimum total cost as measured by",NA
11.6 ,NA,NA
Further Reading,"Many interesting properties of graphs can be investigated by playing with the pro- 
 grams in the Stanford Graphbase. This is a collection of benchmark databases and 
 graph processing programs. The Stanford Graphbase is documented in [Knu94].",NA
11.7 ,NA,NA
Exercises,"11.1
  Prove by induction that a graph with
  n
  vertices has at most
  n
 (
 n−
 1)
 /
 2 
 edges. 
 11.2
  Prove the following implications regarding free trees. 
 (a)
  IF an undirected graph is connected and has no simple cycles, 
 THEN  
 the graph has
  |
 V
 | −
  1 edges.  
 (b)
  IF an undirected graph has
  |
 V
 | −
  1 edges and no cycles, THEN the 
 graph is connected.",NA
11.8 ,NA,NA
Projects,"11.1
  Design a format for storing graphs in files. Then implement two 
 functions: one to read a graph from a file and the other to write a 
 graph to a file. Test your functions by implementing a complete MST 
 program that reads an undi-rected graph in from a file, constructs the 
 MST, and then writes to a second file the graph representing the MST.",NA
12,NA,NA
Lists and Arrays Revisited,"Simple lists and arrays are the right tools for the many applications. Other 
 situa-tions require support for operations that cannot be implemented 
 efficiently by the standard list representations of Chapter 4. This chapter 
 presents a range of topics, whose unifying thread is that the data structures 
 included are all list- or array-like. These structures overcome some of the 
 problems of simple linked list and con-tiguous array representations. This 
 chapter also seeks to reinforce the concept of logical representation versus 
 physical implementation, as some of the “list” imple-mentations have quite 
 different organizations internally. 
 Section 12.1 describes a series of representations for multilists, which 
 are lists that may contain sublists. Section 12.2 discusses representations for 
 implementing sparse matrices, large matrices where most of the elements 
 have zero values. Sec-tion 12.3 discusses memory management techniques, 
 which are essentially a way of allocating variable-length sections from a 
 large array.",NA
12.1 ,NA,NA
Multilists,"Recall from Chapter 4 that a list is a finite, ordered sequence of items of the 
 form
 ∈x
 0
 , x
 1
 , ..., x
 n−
 1
 ∈
  where
  n ≥
  0. We can represent the empty list by
  NULL
  
 or
  ∈∈
 . In Chapter 4 we assumed that all list elements had the same data 
 type. In this section, we extend the definition of lists to allow elements to be 
 arbitrary in nature. In general, list elements are one of two types. 
 1.
  An
  atom
 , which is a data record of some type such as a number, 
 symbol, or  
 string. 
 2.
  Another list, which is called a
  sublist
 . 
 A list containing sublists will be written as 
 ∈x1, ∈y1, ∈a1, a2∈, y3∈, ∈z1, z2∈, x4∈.",NA
12.2 ,NA,NA
Matrix Representations,"Sometimes we need to represent a large, two-dimensional matrix where 
 many of the elements have a value of zero. One example is the lower 
 triangular matrix that results from solving systems of simultaneous 
 equations. A lower triangular matrix stores zero values at all positions [
 r
 ,
  c
 ] 
 such that
  r < c
 , as shown in Figure 12.6(a). Thus, the upper-right triangle of 
 the matrix is always zero. Another example is representing undirected 
 graphs in an adjacency matrix (see Project 11.2). Because all edges between 
 Vertices
  i
  and
  j
  go in both directions, there is no need to store both. Instead 
 we can just store one edge going from the higher-indexed vertex to",NA
12.3 ,NA,NA
Memory Management,"Most data structures are designed to store and access objects of uniform 
 size. A typical example would be an integer stored in a list or a queue. Some 
 applications require the ability to store variable-length records, such as a 
 string of arbitrary length. One solution is to store in the list or queue fixed-
 length pointers to the variable-length strings. This is fine for data structures 
 stored in main memory. But if the collection of strings is meant to be stored 
 on disk, then we might need to worry about where exactly these strings are 
 stored. And even when stored in main memory, something has to figure out",NA
12.4 ,NA,NA
Further Reading,"For information on LISP, see
  The Little LISPer
  by Friedman and Felleisen 
 [FF89]. Another good LISP reference is
  Common LISP: The Language
  by Guy 
 L. Steele [Ste90]. For information on Emacs, which is both an excellent text 
 editor and a programming environment, see the
  GNU Emacs Manual
  by 
 Richard Stallman",NA
12.5 ,NA,NA
Exercises,"12.1
  For each of the following bracket notation descriptions, draw the 
 equivalent  
 multilist in graphical form such as shown in Figure 12.2. 
  
  
 (a)
  ∈a, b, ∈c, d, e∈, ∈f, ∈g∈, h∈∈
  
  
 (b)
  ∈a, b, ∈c, d, L
 1:
 e∈, L
 1
 ∈
  
  
   
 (c)
  ∈L
 1:
 a, L
 1
 , ∈L
 2:
 b∈, L
 2
 , ∈L
 1
 ∈∈
  
 12.2 
   
 (a)
  Show the bracket notation for the list of Figure 
 12.19(a). 
 (b)
  Show the bracket notation for the list of Figure 
 12.19(b). 
 (c)
  Show the bracket notation for the list of 
 Figure 12.19(c). 
 12.3
  Given the linked representation of a pure list such as 
 ∈x
 1
 , ∈y
 1
 , y
 2
 , ∈z
 1
 , z
 2
 ∈, y
 4
 ∈, ∈w
 1
 , w
 2
 ∈, x
 4
 ∈,
  
 write an in-place reversal algorithm to reverse the sublists at all levels 
 in-cluding the topmost level. For this example, the result would be a 
 linked representation corresponding to 
 ∈x
 4
 , ∈w
 2
 , w
 1
 ∈, ∈y
 4
 , ∈z
 2
 , z
 1
 ∈, y
 2
 , y
 1
 ∈, x
 1
 ∈.",NA
12.6 ,NA,NA
Projects,NA,NA
13,NA,NA
Advanced Tree Structures,"This chapter introduces several tree structures designed for use in 
 specialized ap-plications. The trie of Section 13.1 is commonly used to store 
 and retrieve strings. It also serves to illustrate the concept of a key space 
 decomposition. The AVL tree and splay tree of Section 13.2 are variants on 
 the BST. They are examples of self-balancing search trees and have 
 guaranteed good performance regardless of the insertion order for records. 
 An introduction to several spatial data structures used to organize point 
 data by
  xy
 -coordinates is presented in Section 13.3. 
 Descriptions of the fundamental operations are given for each data 
 structure. One purpose for this chapter is to provide opportunities for class 
 programming projects, so detailed implementations are left to the reader.",NA
13.1 ,NA,NA
Tries,"Recall that the shape of a BST is determined by the order in which its data 
 records are inserted. One permutation of the records might yield a balanced 
 tree while another might yield an unbalanced tree, with the extreme case 
 becoming the shape of a linked list. The reason is that the value of the key 
 stored in the root node splits the key range into two parts: those key values 
 less than the root’s key value, and those key values greater than the root’s 
 key value. Depending on the relationship between the root node’s key value 
 and the distribution of the key values for the other records in the the tree, 
 the resulting BST might be balanced or unbalanced. Thus, the BST is an 
 example of a data structure whose organization is based on an 
 object space 
 decomposition
 , so called because the decomposition of the key range is 
 driven by the objects (i.e., the key values of the data records) stored in the 
 tree. 
 The alternative to object space decomposition is to predefine the 
 splitting posi-tion within the key range for each node in the tree. In other 
 words, the root could be predefined to split the key range into two equal 
 halves, regardless of the particular values or order of insertion for the data 
 records. Those records with keys in the lower half of the key range will be 
 stored in the left subtree, while those records",NA
13.2,NA,NA
Balanced Trees,"We have noted several times that the BST has a high risk of becoming 
 unbalanced, resulting in excessively expensive search and update 
 operations. One solution to this problem is to adopt another search tree 
 structure such as the 2-3 tree or the binary trie. An alternative is to modify 
 the BST access functions in some way to guarantee that the tree performs 
 well. This is an appealing concept, and it works well for heaps, whose access 
 functions maintain the heap in the shape of a complete binary tree. 
 Unfortunately, requiring that the BST always be in the shape of a complete 
 binary tree requires excessive modification to the tree during update, as 
 discussed in Section 10.3. 
 If we are willing to weaken the balance requirements, we can come up 
 with alternative update routines that perform well both in terms of cost for 
 the update and in balance for the resulting tree structure. The AVL tree 
 works in this way, using insertion and deletion routines altered from those 
 of the BST to ensure that, for every node, the depths of the left and right 
 subtrees differ by at most one. The AVL tree is described in Section 13.2.1. 
 A different approach to improving the performance of the BST is to not 
 require that the tree always be balanced, but rather to expend some effort 
 toward making the BST more balanced every time it is accessed. This is a 
 little like the idea of path compression used by the UNION/FIND algorithm 
 presented in Section 6.2. One example of such a compromise is called the
  
 splay tree
 . The splay tree is described in Section 13.2.2.",NA
13.3,NA,NA
Spatial Data Structures,"All of the search trees discussed so far — BSTs, AVL trees, splay trees, 2-3 
 trees, B-trees, and tries — are designed for searching on a one-dimensional 
 key. A typical example is an integer key, whose one-dimensional range can 
 be visualized as a number line. These various tree structures can be viewed 
 as dividing this one-dimensional number line into pieces. 
 Some databases require support for multiple keys. In other words, 
 records can be searched for using any one of several key fields, such as name 
 or ID number. Typically, each such key has its own one-dimensional index, 
 and any given search query searches one of these independent indices as 
 appropriate. 
 A multidimensional search key presents a rather different concept. 
 Imagine that we have a database of city records, where each city has a name 
 and an
  xy
 -coordinate. A BST or splay tree provides good performance for 
 searches on city name, which is a one-dimensional key. Separate BSTs could 
 be used to index the
  x
 -and
  y
 -coordinates. This would allow us to insert and 
 delete cities, and locate them by name or by one coordinate. However, 
 search on one of the two coordinates is not a natural way to view search in a 
 two-dimensional space. Another option is to combine the
  xy
 -coordinates 
 into a single key, say by concatenating the two coor-dinates, and index cities 
 by the resulting key in a BST. That would allow search by coordinate, but 
 would not allow for efficient two-dimensional
  range queries
  such as 
 searching for all cities within a given distance of a specified point. The 
 problem is that the BST only works well for one-dimensional keys, while a 
 coordinate is a two-dimensional key where neither dimension is more 
 important than the other. 
 Multidimensional range queries are the defining feature of a
  spatial 
 applica-tion
 . Because a coordinate gives a position in space, it is called a
  
 spatial attribute
 . To implement spatial applications efficiently requires the 
 use of
  spatial data struc-tures
 . Spatial data structures store data objects 
 organized by position and are an important class of data structures used in",NA
13.4 ,NA,NA
Further Reading,"PATRICIA tries and other trie implementations are discussed in
  Information 
 Re-trieval: Data Structures & Algorithms
 , Frakes and Baeza-Yates, eds. 
 [FBY92]. 
  
 See Knuth [Knu97] for a discussion of the AVL tree. For further reading 
 on splay trees, see “Self-adjusting Binary Search” by Sleator and Tarjan 
 [ST85]. 
 The world of spatial data structures is rich and rapidly evolving. For a 
 good introduction, see
  Foundations of Multidimensional and Metric Data 
 Structures
  by Hanan Samet [Sam06]. This is also the best reference for more 
 information on the PR quadtree. The k-d tree was invented by John Louis 
 Bentley. For further information on the k-d tree, in addition to [Sam06], see 
 [Ben75]. For information on using a quadtree to store arbitrary polygonal 
 objects, see [SH92]. 
 For a discussion on the relative space requirements for two-way versus 
 multi-way branching, see “A Generalized Comparison of Quadtree and 
 Bintree Storage Requirements” by Shaffer, Juvvadi, and Heath [SJH93].",NA
13.5 ,NA,NA
Exercises,"13.1
  Show the binary trie (as illustrated by Figure 13.1) for the following 
 collec- 
 tion of values: 42, 12, 100, 10, 50, 31, 7, 11, 99. 
 13.2
  Show the PAT trie (as illustrated by Figure 13.3) for the following 
 collection  of values: 42, 12, 100, 10, 50, 31, 7, 11, 99. 
 13.3
  Write the insertion routine for a binary trie as shown in Figure 13.1. 
 13.4
  Write the deletion routine for a binary trie as shown in Figure 13.1. 
 13.5
  
 (a)
  Show the result (including appropriate rotations) of inserting the 
 value  39 into the AVL tree on the left in Figure 13.4. 
 (b)
  Show the result (including appropriate rotations) of inserting the 
 value  300 into the AVL tree on the left in Figure 13.4. 
 (c)
  Show the result (including appropriate rotations) of inserting the 
 value  50 into the AVL tree on the left in Figure 13.4. 
 (d)
  Show the result (including appropriate rotations) of inserting the 
 value  1 into the AVL tree on the left in Figure 13.4. 
 13.6
  Show the splay tree that results from searching for value 75 in the 
 splay tree  of Figure 13.10(d). 
 13.7
  Show the splay tree that results from searching for value 18 in the 
 splay tree  of Figure 13.10(d). 
 13.8
  Some applications do not permit storing two records with duplicate 
 key val-ues. In such a case, an attempt to insert a duplicate-keyed 
 record into a tree structure such as a splay tree should result in a 
 failure on insert. What is the appropriate action to take in a splay tree 
 implementation when the insert routine is called with a duplicate-
 keyed record? 
 13.9
  Show the result of deleting point A from the k-d tree of Figure 13.11. 
 13.10
  
 (a)
  Show the result of building a k-d tree from the following points 
 (in-serted in the order given). A (20, 20), B (10, 30), C (25, 50), D 
 (35, 
 25), E (30, 45), F (30, 35), G (55, 40), H (45, 35), I (50, 30). 
 (b)
  Show the result of deleting point A from the tree you built in part (a). 
 13.11
  
 13.12
  
 (a)
  Show the result of deleting F from the PR quadtree of Figure 13.16. 
 (b)
  Show the result of deleting records E and F from the PR quadtree 
 of  Figure 13.16. 
 (a)
  Show the result of building a PR quadtree from the following 
 points (inserted in the order given). Assume the tree is 
 representing a space of 
 64 by 64 units. A (20, 20), B (10, 30), C (25, 50), D (35, 25), E (30, 
 45), F (30, 35), G (45, 25), H (45, 30), I (50, 30). 
 (b)
  Show the result of deleting point C from the tree you built in part (a).",NA
13.6 ,NA,NA
Projects,"13.1
  Use the trie data structure to devise a program to sort variable-length strings. 
 The program’s running time should be proportional to the total 
 number of letters in all of the strings. Note that some strings might be 
 very long while most are short. 
 13.2
  Define the set of
  suffix strings
  for a string
  S
  to be
  S
 ,
  S
  without its first 
 char-acter,
  S
  without its first two characters, and so on. For example, 
 the complete set of suffix strings for “HELLO” would be 
 {
 HELLO
 ,
  ELLO
 ,
  LLO
 ,
  LO
 ,
  O
 }.
  
 A
  suffix tree
  is a PAT trie that contains all of the suffix strings for a 
 given string, and associates each suffix with the complete string. The 
 advantage of a suffix tree is that it allows a search for strings using 
 “wildcards.” For example, the search key “TH*” means to find all 
 strings with “TH” as the first two characters. This can easily be done 
 with a regular trie. Searching for “*TH” is not efficient in a regular trie, 
 but it is efficient in a suffix tree.",NA
PART V ,NA,NA
Theory of ,NA,NA
Algorithms,NA,NA
14,NA,NA
Analysis Techniques,"Often it is easy to invent an equation to model the behavior of an algorithm 
 or data structure. Often it is easy to derive a closed-form solution for the 
 equation should it contain a recurrence or summation. But sometimes 
 analysis proves more difficult. It may take a clever insight to derive the right 
 model, such as the snow-plow argument for analyzing the average run 
 length resulting from Replacement Selection (Section 8.5.2). In this example, 
 once the snowplow argument is under-stood, the resulting equations follow 
 naturally. Sometimes, developing the model is straightforward but analyzing 
 the resulting equations is not. An example is the average-case analysis for 
 Quicksort. The equation given in Section 7.5 simply enu-merates all possible 
 cases for the pivot position, summing corresponding costs for the recursive 
 calls to Quicksort. However, deriving a closed-form solution for the resulting 
 recurrence relation is not as easy. 
 Many analyses of iterative algorithms use a summation to model the cost 
 of a loop. Techniques for finding closed-form solutions to summations are 
 presented in Section 14.1. The cost for many algorithms based on recursion 
 are best modeled by recurrence relations. A discussion of techniques for 
 solving recurrences is pro-vided in Section 14.2. These sections build on the 
 introduction to summations and recurrences provided in Section 2.4, so the 
 reader should already be familiar with that material. 
 Section 14.3 provides an introduction to the topic of
  amortized 
 analysis
 . Am-ortized analysis deals with the cost of a series of operations. 
 Perhaps a single operation in the series has high cost, but as a result the cost 
 of the remaining oper-ations is limited. Amortized analysis has been used 
 successfully to analyze several of the algorithms presented in previous 
 sections, including the cost of a series of UNION/FIND operations (Section 
 6.2), the cost of partition in Quicksort (Sec-tion 7.5), the cost of a series of 
 splay tree operations (Section 13.2), and the cost of a series of operations on 
 self-organizing lists (Section 9.2). Section 14.3 discusses the topic in more 
 detail.",NA
Summation Techniques,Chap. 14 Analysis Techniques,NA
14.1,"Consider the following simple summation. 
 n
  
 i.
  
 i
 =1
  
 In Section 2.6.3 it was proved by induction that this summation has the well-
 known closed form
  n
 (
 n
  + 1)
 /
 2. But while induction is a good technique for 
 proving that a proposed closed-form expression is correct, how do we find a 
 candidate closed-form expression to test in the first place? Let us try to think 
 through this problem from first principles, as though we had never seen it 
 before. 
 A good place to begin analyzing a summation it is to give an estimate of 
 its value for a given
  n
 . Observe that the biggest term for this summation is
  n
 , 
 and there are
  n
  terms being summed up. So the total must be less than
  n
 2
 . 
 Actually, most terms are much less than
  n
 , and the sizes of the terms grows 
 linearly. If we were to draw a picture with bars for the size of the terms, 
 their heights would form a line, and we could enclose them in a box
  n
  units 
 wide and
  n
  units high. It is easy to see from this that a closer estimate for the 
 summation is about (
 n
 2
 )
 /
 2. Having this estimate in hand helps us when 
 trying to determine an exact closed-form solution, because we will hopefully 
 recognize if our proposed solution is badly wrong. 
 Let us now consider some ways that we might hit upon an exact equation 
 for the closed form solution to this summation. One particularly clever 
 approach we can take is to observe that we can “pair up” the first and last 
 terms, the second and (
 n −
  1)th terms, and so on. Each pair sums to
  n
  + 1. 
 The number of pairs is
  n/
 2. Thus, the solution is
  n
 (
 n
 +1)
 /
 2. This is pretty, and 
 there is no doubt about it being correct. The problem is that it is not a useful 
 technique for solving many other summations. 
 Now let us try to do something a bit more general. We already 
 recognized that, because the largest term is
  n
  and there are
  n
  terms, the 
 summation is less than
  n
 2
 . If we are lucky, the closed form solution is a 
 polynomial. Using that as a working assumption, we can invoke a technique 
 called
  guess-and-test
 . We will guess that the closed-form solution for this 
 summation is a polynomial of the form 
 c
 1
 n
 2
 +
  c
 2
 n
  +
  c
 3
  for some constants
  c
 1
 ,
  
 c
 2
 , and
  c
 3
 . If this is true, then we can plug in the answers to small cases of the 
 summation to solve for the coefficients. For this example, substituting 0, 1, 
 and 2 for
  n
  leads to three simultaneous equations. Because the summation 
 when
  n
  = 0 is just 0,
  c
 3
  must be 0. For
  n
  = 1 and
  n
  = 2 we get the two 
 equations 
 c
 1
  +
  c
 2
  
 = 
 1",NA
14.2 ,NA,NA
Recurrence Relations,"Recurrence relations are often used to model the cost of recursive functions. 
 For example, the standard Mergesort (Section 7.4) takes a list of size
  n
 , splits 
 it in half, performs Mergesort on each half, and finally merges the two 
 sublists in
  n
  steps. 
 The cost for this can be modeled as 
 T
 (
 n
 ) = 2
 T
 (
 n/
 2) +
  n.
  
 In other words, the cost of the algorithm on input of size
  n
  is two times the 
 cost for input of size
  n/
 2 (due to the two recursive calls to Mergesort) plus
  n
  
 (the time to merge the sublists together again). 
 There are many approaches to solving recurrence relations, and we 
 briefly con-sider three here. The first is an estimation technique: Guess the 
 upper and lower bounds for the recurrence, use induction to prove the 
 bounds, and tighten as re-quired. The second approach is to expand the 
 recurrence to convert it to a summa-tion and then use summation 
 techniques. The third approach is to take advantage of already proven 
 theorems when the recurrence is of a suitable form. In particu-lar, typical 
 divide and conquer algorithms such as Mergesort yield recurrences of a 
 form that fits a pattern for which we have a ready solution. 
 14.2.1 Estimating Upper and Lower Bounds
  
 The first approach to solving recurrences is to guess the answer and then 
 attempt to prove it correct. If a correct upper or lower bound estimate is 
 given, an easy induction proof will verify this fact. If the proof is successful, 
 then try to tighten the bound. If the induction proof fails, then loosen the 
 bound and try again. Once the upper and lower bounds match, you are 
 finished. This is a useful technique when you are only looking for asymptotic 
 complexities. When seeking a precise closed-form solution (i.e., you seek the 
 constants for the expression), this method will probably be too much work. 
 Example 14.5
  Use the guessing technique to find the asymptotic 
 bounds for Mergesort, whose running time is described by the 
 equation 
 T
 (
 n
 ) = 2
 T
 (
 n/
 2) +
  n
 ;  
 T
 (2) = 1
 .
  
 We begin by guessing that this recurrence has an upper bound in 
 O(
 n
 2
 ). To be more precise, assume that 
 T
 (
 n
 )
  ≤ n
 2
 .",NA
Amortized Analysis,Chap. 14 Analysis Techniques,NA
14.3,"This section presents the concept of
  amortized analysis
 , which is the 
 analysis for a series of operations taken as a whole. In particular, amortized 
 analysis allows us to deal with the situation where the worst-case cost for
  n
  
 operations is less than 
 n
  times the worst-case cost of any one operation. 
 Rather than focusing on the indi-vidual cost of each operation independently 
 and summing them, amortized analysis looks at the cost of the entire series 
 and “charges” each individual operation with a share of the total cost. 
 We can apply the technique of amortized analysis in the case of a series 
 of se-quential searches in an unsorted array. For
  n
  random searches, the 
 average-case cost for each search is
  n/
 2, and so the
  expected
  total cost for 
 the series is
  n
 2
 /
 2. Unfortunately, in the worst case all of the searches would 
 be to the last item in the array. In this case, each search costs
  n
  for a total 
 worst-case cost of
  n
 2
 . Compare this to the cost for a series of
  n
  searches such 
 that each item in the array is searched for precisely once. In this situation, 
 some of the searches
  must
  be expensive, but also some searches
  must
  be 
 cheap. The total number of searches, in the best, av-erage, and worst case, 
 for this problem must be
 n 
 of two better than the more pessimistic analysis 
 that charges each operation in the 
 i
 =
 i
 i ≈ n
 2
 /
 2. This is a factor 
 series with its worst-case cost. 
 As another example of amortized analysis, consider the process of 
 increment-ing a binary counter. The algorithm is to move from the lower-
 order (rightmost) bit toward the high-order (leftmost) bit, changing 1s to 0s 
 until the first 0 is en-countered. This 0 is changed to a 1, and the increment 
 operation is done. Below is
  C
 ++
 code to implement the increment operation, 
 assuming that a binary number of length
  n
  is stored in array
  A
  of length
  n
 . 
 for (i=0; ((i<n) && (A[i] == 1)); i++) 
  
  
 A[i] = 0; 
  
 if (i < n) 
  
  
 A[i] = 1;
  
 what is the average cost for an increment operation in terms of the number of bits If we 
 count from 0 through 2
 n
 −
  1, (requiring a counter with at least
  n
  bits), 
 processed? Naive worst-case analysis says that if all
  n
  bits are 1 (except for 
 the high-order bit), then
  n
  bits need to be processed. Thus, if there are 
 2
 n
 increments, then the cost is
  n
 2
 n
 . However, this is much too high, because it 
 is rare for so many bits to be processed. In fact, half of the time the low-
 order bit is 0, and so only that bit is processed. One quarter of the time, the 
 low-order two bits are 01, and so only the low-order two bits are processed. 
 Another way to view this is that the low-order bit is always flipped, the bit to",NA
14.4 ,NA,NA
Further Reading,"A good introduction to solving recurrence relations appears in
  Applied 
 Combina-torics
  by Fred S. Roberts [Rob84]. For a more advanced treatment, 
 see
  Concrete Mathematics
  by Graham, Knuth, and Patashnik [GKP94]. 
 Cormen, Leiserson, and Rivest provide a good discussion on various 
 methods for performing amortized analysis in
  Introduction to Algorithms
  
 [CLRS09]. For an amortized analysis that the splay tree requires
  m
  log
  n
  time 
 to perform a series of
  m
  operations on
  n
  nodes when
  m > n
 , see “Self-
 Adjusting Binary Search Trees” by Sleator and Tarjan [ST85]. The proof for 
 Theorem 14.2 comes from“Amortized Analysis of Self-Organizing Sequential 
 Search Heuristics” by Bentley and McGeoch [BM85].",NA
14.5 ,NA,NA
Exercises,"14.1
  Use the technique of guessing a polynomial and deriving the 
 coefficients to  
 solve the summation  
  
  
 n 
  
  
  
  
 i
 2
 .
  
 i
 =1
  
 14.2
  Use the technique of guessing a polynomial and deriving the 
 coefficients to  
 solve the summation  
  
  
 n 
  
  
  
  
 i
 3
 .
  
 i
 =1
  
 14.3
  Find, and prove correct, a closed-form solution for 
 b 
  
  
 i
 2
 .
  
 i
 =
 a
  
 14.4
  Use subtract-and-guess or divide-and-guess to find the closed form 
 solution for the following summation. You must first find a pattern 
 from which to deduce a potential closed form solution, and then prove 
 that the proposed solution is correct. 
 n 
  
 i/
 2
 i
  
 i
 =1",NA
14.6 ,NA,NA
Projects,"14.1
  Implement the UNION/FIND algorithm of Section 6.2 using both path com- 
 pression and the weighted union rule. Count the total number of node ac- 
 cesses required for various series of equivalences to determine if the actual 
 performance of the algorithm matches the expected cost of Θ(
 n
  log
 ∈
 n
 ).",NA
15,NA,NA
Lower Bounds,"How do I know if I have a good algorithm to solve a problem? If my 
 algorithm runs in Θ(
 n
  log
  n
 ) time, is that good? It would be if I were sorting 
 the records stored in an array. But it would be terrible if I were searching 
 the array for the largest element. The value of an algorithm must be 
 determined in relation to the inherent complexity of the problem at hand. 
 In Section 3.6 we defined the upper bound for a problem to be the upper 
 bound of the best algorithm we know for that problem, and the lower bound 
 to be the tightest lower bound that we can prove over all algorithms for that 
 problem. While we usually can recognize the upper bound for a given 
 algorithm, finding the tightest lower bound for all possible algorithms is 
 often difficult, especially if that lower bound is more than the “trivial” lower 
 bound determined by measuring the amount of input that must be 
 processed. 
 The benefits of being able to discover a strong lower bound are 
 significant. In particular, when we can make the upper and lower bounds for 
 a problem meet, this means that we truly understand our problem in a 
 theoretical sense. It also saves us the effort of attempting to discover more 
 (asymptotically) efficient algorithms when no such algorithm can exist. 
 Often the most effective way to determine the lower bound for a problem 
 is to find a reduction to another problem whose lower bound is already 
 known. This is the subject of Chapter 17. However, this approach does not 
 help us when we cannot find a suitable “similar problem.” Our focus in this 
 chapter is discovering and proving lower bounds from first principles. Our 
 most significant example of a lower bounds argument so far is the proof 
 from Section 7.9 that the problem of sorting is O(
 n
  log
  n
 ) in the worst case. 
 Section 15.1 reviews the concept of a lower bound for a problem and 
 presents the basic “algorithm” for finding a good algorithm. Section 15.2 
 discusses lower bounds on searching in lists, both those that are unordered 
 and those that are or-dered. Section 15.3 deals with finding the maximum 
 value in a list, and presents a model for selection based on building a 
 partially ordered set. Section 15.4 presents",NA
15.1 ,NA,NA
Introduction to Lower Bounds Proofs,"The lower bound for the problem is the tightest (highest) lower bound that 
 we can prove
  for all possible algorithms
  that solve the problem.
 1
 This can be a 
 difficult bar, given that we cannot possibly know all algorithms for any 
 problem, because there are theoretically an infinite number. However, we 
 can often recognize a simple lower bound based on the amount of input that 
 must be examined. For example, we can argue that the lower bound for any 
 algorithm to find the maximum-valued element in an unsorted list must be 
 Ω(
 n
 ) because any algorithm must examine all of the inputs to be sure that it 
 actually finds the maximum value. 
 In the case of maximum finding, the fact that we know of a simple 
 algorithm that runs in O(
 n
 ) time, combined with the fact that any algorithm 
 needs Ω(
 n
 ) time, is significant. Because our upper and lower bounds meet 
 (within a constant factor), we know that we do have a “good” algorithm for 
 solving the problem. It is possible that someone can develop an 
 implementation that is a “little” faster than an existing one, by a constant 
 factor. But we know that its not possible to develop one that is 
 asymptotically better. 
 We must be careful about how we interpret this last statement, however. 
 The world is certainly better off for the invention of Quicksort, even though 
 Mergesort was available at the time. Quicksort is not asymptotically faster 
 than Mergesort, yet is not merely a “tuning” of Mergesort either. Quicksort is 
 a substantially different approach to sorting. So even when our upper and 
 lower bounds for a problem meet, there are still benefits to be gained from a 
 new, clever algorithm. 
 So now we have an answer to the question “How do I know if I have a 
 good algorithm to solve a problem?” An algorithm is good (asymptotically 
 speaking) if its upper bound matches the problem’s lower bound. If they 
 match, we know to stop trying to find an (asymptotically) faster algorithm. 
 What if the (known) upper bound for our algorithm does not match the 
 (known) lower bound for the problem? In this case, we might not know 
 what to do. Is our upper bound flawed, and the algorithm is really faster 
 than we can prove? Is our lower bound weak, and the true lower bound for 
 the problem is greater? Or is our algorithm simply not the best? 
 1
 Throughout this discussion, it should be understood that any mention of bounds must 
 specify what class of inputs are being considered. Do we mean the bound for the worst case",NA
15.2 ,NA,NA
Lower Bounds on Searching Lists,"In Section 7.9 we presented an important lower bounds proof to show that 
 the problem of sorting is Θ(
 n
  log
  n
 ) in the worst case. In Chapter 9 we 
 discussed a number of algorithms to search in sorted and unsorted lists, but 
 we did not provide any lower bounds proofs to this important problem. We 
 will extend our pool of techniques for lower bounds proofs in this section by 
 studying lower bounds for searching unsorted and sorted lists. 
 15.2.1 Searching in Unsorted Lists
  
 Given an (unsorted) list
  L
  of
  n
  elements and a search key
  K
 , we seek to 
 identify one element in
  L
  which has key value
  K
 , if any exists. For the rest of 
 this discussion, we will assume that the key values for the elements in
  L
  are 
 unique, that the set of all possible keys is totally ordered (that is, the 
 operations
  <
 , =, and
  >
  are defined for all pairs of key values), and that 
 comparison is our only way to find the relative ordering of two keys. Our 
 goal is to solve the problem using the minimum number of comparisons. 
 Given this definition for searching, we can easily come up with the 
 standard sequential search algorithm, and we can also see that the lower 
 bound for this prob-lem is “obviously”
  n
  comparisons. (Keep in mind that 
 the key
  K
  might not actually appear in the list.) However, lower bounds 
 proofs are a bit slippery, and it is in-structive to see how they can go wrong. 
 Theorem 15.1
  The lower bound for the problem of searching in an unsorted 
 list is n comparisons.
  
 3
 Recalling the advice to be suspicious of any lower bounds proof that argues a given 
 behavior“must” happen, this proof should be raising red flags. However, in this particular 
 case the problem is so constrained that there really is no (better) alternative to this particular 
 sequence of events.",NA
15.3 ,NA,NA
Finding the Maximum Value,"How can we find the
  i
 th largest value in a sorted list? Obviously we just go to 
 the 
 i
 th position. But what if we have an unsorted list? Can we do better than 
 to sort it? If we are looking for the minimum or maximum value, certainly 
 we can do better than sorting the list. Is this true for the second biggest 
 value? For the median value? In later sections we will examine those 
 questions. For this section, we will continue our examination of lower 
 bounds proofs by reconsidering the simple problem of finding the maximum 
 value in an unsorted list. 
 Here is a simple algorithm for finding the largest value. 
 // Return position of largest value in ""A"" of size ""n"" int largest(int A[], 
 int n) { 
  
  
 int currlarge = 0; // Holds largest element position 
  
 for (int i=1; 
 i<n; i++) // For each array element 
   
 if (A[currlarge] < A[i]) // if 
 A[i] is larger 
  
  
   
 currlarge = i; // 
  
  
 remember its position 
  
 return currlarge; // Return 
 largest position }
  
 Obviously this algorithm requires
  n
  comparisons. Is this optimal? It 
 should be intuitively obvious that it is, but let us try to prove it. (Before 
 reading further you might try writing down your own proof.)",NA
15.4 ,NA,NA
Adversarial Lower Bounds Proofs,"Our next problem will be finding the second largest in a collection of objects. 
 Con-sider what happens in a standard single-elimination tournament. Even 
 if we assume that the “best” team wins in every game, is the second best the 
 one that loses in the finals? Not necessarily. We might expect that the second 
 best must lose to the best, but they might meet at any time. 
 Let us go through our standard “algorithm for finding algorithms” by 
 first proposing an algorithm, then a lower bound, and seeing if they match. 
 Unlike our analysis for most problems, this time we are going to count the 
 exact number of comparisons involved and attempt to minimize this count. 
 A simple algorithm for finding the second largest is to first find the 
 maximum (in
  n −
  1 comparisons), discard it, and then find the maximum of 
 the remaining elements (in
  n −
  2 compar-isons) for a total cost of 2
 n −
  3 
 comparisons. Is this optimal? That seems doubtful, but let us now proceed to 
 the step of attempting to prove a lower bound. 
 Theorem 15.2
  The lower bound for finding the second largest value is
  2
 n −
  3
 .
  
 Proof:
  Any element that loses to anything other than the maximum cannot 
 be second. So, the only candidates for second place are those that lost to the 
 maximum. Function
  largest
  might compare the maximum element to
  n −
  1 
 others. Thus, we might need
  n −
  2 additional comparisons to find the second 
 largest.  
 2
  
  
 This proof is wrong. It exhibits the
  necessity fallacy
 : “Our algorithm 
 does something, therefore all algorithms solving the problem must do the 
 same.” 
 This leaves us with our best lower bounds argument at the 
 moment being that finding the second largest must cost at least as much as 
 finding the largest, or
  n−
 1. Let us take another try at finding a better 
 algorithm by adopting a strategy of divide and conquer. What if we break the 
 list into halves, and run
  largest
  on each half? We can then compare the two",NA
15.5 ,NA,NA
State Space Lower Bounds Proofs,"We now consider the problem of finding both the minimum and the 
 maximum from an (unsorted) list of values. This might be useful if we want 
 to know the range of a collection of values to be plotted, for the purpose of 
 drawing the plot’s scales. Of course we could find them independently in 2
 n 
 −
  2 comparisons. A slight modification is to find the maximum in
  n −
  1 
 comparisons, remove it from the list, and then find the minimum in
  n −
  2 
 further comparisons for a total of 2
 n −
  3 comparisons. Can we do better than 
 this? 
 Before continuing, think a moment about how this problem of finding the 
 mini-mum and the maximum compares to the problem of the last section, 
 that of finding the second biggest value (and by implication, the maximum). 
 Which of these two problems do you think is harder? It is probably not at all 
 obvious to you that one problem is harder or easier than the other. There is 
 intuition that argues for ei-ther case. On the one hand intuition might argue 
 that the process of finding the maximum should tell you something about 
 the second biggest value, more than that process should tell you about the 
 minimum value. On the other hand, any given comparison tells you 
 something about which of two can be a candidate for maximum value, and 
 which can be a candidate for minimum value, thus making progress in both 
 directions. 
 We will start by considering a simple divide-and-conquer approach to 
 finding the minimum and maximum. Split the list into two parts and find the 
 minimum and maximum elements in each part. Then compare the two 
 minimums and maximums to each other with a further two comparisons to 
 get the final result. The algorithm is shown in Figure 15.3. 
 The cost of this algorithm can be modeled by the following recurrence. 
 This is a rather interesting recurrence, and its solution ranges between 3
 n/
 2
 −
 2 
  
 T
 (
 n
 ) =
  
  
  
 1 
 T
 (
 ∈n/
 2
 ∈
 ) +
  T
 (
 ∈n/
 2
 ∈
 ) + 2  
  
 n
  = 2",NA
15.6 ,NA,NA
Finding the,i,NA
th Best Element,"We now tackle the problem of finding the
  i
 th best element in a list. As 
 observed earlier, one solution is to sort the list and simply look in the
  i
 th 
 position. However, this process provides considerably more information 
 than we need to solve the problem. The minimum amount of information 
 that we actually need to know can be visualized as shown in Figure 15.4. 
 That is, all we need to know is the
  i −
  1 items less than our desired value, and 
 the
  n − i
  items greater. We do not care about the relative order within the 
 upper and lower groups. So can we find the required information faster than 
 by first sorting? Looking at the lower bound, can we tighten that beyond the 
 trivial lower bound of
  n
  comparisons? We will focus on the specific question 
 of finding the median element (i.e., the element with rank
  n/
 2), because the 
 resulting algorithm can easily be modified to find the
  i
 th largest value for 
 any
  i
 . 
 Looking at the Quicksort algorithm might give us some insight into 
 solving the median problem. Recall that Quicksort works by selecting a pivot 
 value, partition-ing the array into those elements less than the pivot and 
 those greater than the pivot, and moving the pivot to its proper location in",NA
15.7 ,NA,NA
Optimal Sorting,"We conclude this section with an effort to find the sorting algorithm with the 
 ab-solute fewest possible comparisons. It might well be that the result will 
 not be practical for a general-purpose sorting algorithm. But recall our 
 analogy earlier to sports tournaments. In sports, a “comparison” between 
 two teams or individuals means doing a competition between the two. This 
 is fairly expensive (at least com-pared to some minor book keeping in a 
 computer), and it might be worth trading a fair amount of book keeping to 
 cut down on the number of games that need to be played. What if we want to",NA
15.8 ,NA,NA
Further Reading,"Much of the material in this book is also covered in many other textbooks on 
 data structures and algorithms. The biggest exception is that not many other 
 textbooks cover lower bounds proofs in any significant detail, as is done in 
 this chapter. Those that do focus on the same example problems (search and 
 selection) because it tells such a tight and compelling story regarding related 
 topics, while showing off the major techniques for lower bounds proofs. Two 
 examples of such textbooks are“Computer Algorithms” by Baase and Van 
 Gelder [BG00], and “Compared to What?” by Gregory J.E. Rawlins [Raw92]. 
 “Fundamentals of Algorithmics” by Brassard and Bratley [BB96] also covers 
 lower bounds proofs.",NA
15.9 ,NA,NA
Exercises,"15.1
  Consider the so-called “algorithm for algorithms” in Section 15.1. Is 
 this really an algorithm? Review the definition of an algorithm from 
 Section 1.4. Which parts of the definition apply, and which do not? Is 
 the “algorithm for algorithms” a heuristic for finding a good algorithm? 
 Why or why not? 
 15.2
  Single-elimination tournaments are notorious for their scheduling 
 difficul-ties. Imagine that you are organizing a tournament for
  n",NA
15.10 ,NA,NA
Projects,"15.1
  Implement the median-finding algorithm of Section 15.6. Then, modify this 
 algorithm to allow finding the
  i
 th element for any value
  i < n
 .",NA
16,NA,NA
Patterns of Algorithms,"This chapter presents several fundamental topics related to the theory of 
 algorithms. Included are dynamic programming (Section 16.1), randomized 
 algorithms (Sec-tion 16.2), and the concept of a transform (Section 16.3.5). 
 Each of these can be viewed as an example of an “algorithmic pattern” that is 
 commonly used for a wide variety of applications. In addition, Section 16.3 
 presents a number of nu-merical algorithms. Section 16.2 on randomized 
 algorithms includes the Skip List (Section 16.2.2). The Skip List is a 
 probabilistic data structure that can be used to implement the dictionary 
 ADT. The Skip List is no more complicated than the BST. Yet it often 
 outperforms the BST because the Skip List’s efficiency is not tied to the 
 values or insertion order of the dataset being stored.",NA
16.1 ,NA,NA
Dynamic Programming,"Consider again the recursive function for computing the
  n
 th Fibonacci number. 
 long fibr(int n) { // Recursive Fibonacci generator 
  
 // fibr(46) is 
 largest value that fits in a long 
  
 Assert((n > 0) && 
 (n < 47), ""Input out of range""); 
  
 if ((n == 1) || (n == 
 2)) return 1; // Base cases 
  
 return fibr(n-1) + 
 fibr(n-2); 
  
 // Recursion }
  
 The cost of this algorithm (in terms of function calls) is the size of the
  n
 th 
 Fi-bonacci number itself, which our analysis of Section 14.2 showed to be 
 exponential (approximately
  n
 1
 .
 62
 ). Why is this so expensive? Primarily 
 because two recursive calls are made by the function, and the work that they 
 do is largely redundant. That is, each of the two calls is recomputing most of 
 the series, as is each sub-call, and so on. Thus, the smaller values of the 
 function are being recomputed a huge number of times. If we could 
 eliminate this redundancy, the cost would be greatly reduced. The approach 
 that we will use can also improve any algorithm that spends most of its time 
 recomputing common subproblems. 
 517",NA
16.2 ,NA,NA
Randomized Algorithms,"In this section, we will consider how introducing randomness into our 
 algorithms might speed things up, although perhaps at the expense of 
 accuracy. But often we can reduce the possibility for error to be as low as we 
 like, while still speeding up the algorithm. 
 16.2.1 Randomized algorithms for finding large values
  
 In Section 15.1 we determined that the lower bound cost of finding the 
 maximum value in an unsorted list is Ω(
 n
 ). This is the least time needed to 
 be certain that we have found the maximum value. But what if we are willing 
 to relax our requirement for certainty? The first question is: What do we 
 mean by this? There are many aspects to “certainty” and we might relax the 
 requirement in various ways. 
 There are several possible guarantees that we might require from an 
 algorithm that produces
  X
  as the maximum value, when the true maximum is
  
 Y
  . So far we have assumed that we require
  X
  to equal
  Y
  . This is known as an 
 exact or deterministic algorithm to solve the problem. We could relax this 
 and require only that
  X
 ’s rank is “close to”
  Y
  ’s rank (perhaps within a fixed 
 distance or percentage). This is known as an approximation algorithm. We 
 could require that
  X
  is “usually”
 Y
  . This is known as a probabilistic algorithm. 
 Finally, we could require only that 
 X
 ’s rank is “usually” “close” to
  Y
  ’s rank. 
 This is known as a heuristic algorithm. 
  
 There are also different ways that we might choose to sacrifice reliability 
 for speed. These types of algorithms also have names. 
 1. Las Vegas Algorithms
 : We always find the maximum value, and 
 “usually”we find it fast. Such algorithms have a guaranteed result, but 
 do not guarantee fast running time.",NA
16.3 ,NA,NA
Numerical Algorithms,"This section presents a variety of algorithms related to mathematical 
 computations on numbers. Examples are activities like multiplying two 
 numbers or raising a number to a given power. In particular, we are 
 concerned with situations where built-in integer or floating-point 
 operations cannot be used because the values being operated on are too 
 large. Similar concerns arise for operations on polynomials or matrices. 
 Since we cannot rely on the hardware to process the inputs in a single 
 constant-time operation, we are concerned with how to most effectively 
 implement the op-eration to minimize the time cost. This begs a question as 
 to how we should apply our normal measures of asymptotic cost in terms of 
 growth rates on input size. First, what is an instance of addition or 
 multiplication? Each value of the operands yields a different problem 
 instance. And what is the input size when multiplying two numbers? If we 
 view the input size as two (since two numbers are input), then any non-
 constant-time algorithm has a growth rate that is infinitely high compared to 
 the growth of the input. This makes no sense, especially in light of the fact 
 that we know from grade school arithmetic that adding or multiplying 
 numbers does seem to get more difficult as the value of the numbers 
 involved increases. In fact, we know from standard grade school algorithms 
 that the cost of standard addition is linear on the number of digits being 
 added, and multiplication has cost
  n × m 
 when multiplying an
  m
 -digit 
 number by an
  n
 -digit number. 
 The number of digits for the operands does appear to be a key 
 consideration when we are performing a numeric algorithm that is sensitive 
 to input size. The number of digits is simply the log of the value, for a 
 suitable base of the log. Thus, for the purpose of calculating asymptotic 
 growth rates of algorithms, we will con-sider the “size” of an input value to",NA
16.4 ,NA,NA
Further Reading,"For further information on Skip Lists, see “Skip Lists: A Probabilistic 
 Alternative to Balanced Trees” by William Pugh [Pug90].",NA
16.5 ,NA,NA
Exercises,"16.1
  Solve Towers of Hanoi using a dynamic programming algorithm. 
 16.2
  There are six possible permutations of the lines 
 for (int k=0; k<G.n(); k++) 
  
 for (int i=0; i<G.n(); i++) 
  
  
 for (int j=0; j<G.n(); j++)
  
 in Floyd’s algorithm. Which ones give a correct algorithm? 
 16.3
  Show the result of running Floyd’s all-pairs shortest-paths algorithm 
 on the  
 graph of Figure 11.26. 
 16.4
  The implementation for Floyd’s algorithm given in Section 16.1.2 is 
 ineffi-cient for adjacency lists because the edges are visited in a bad 
 order when initializing array
  D
 . What is the cost of of this initialization 
 step for the adja-cency list? How can this initialization step be revised 
 so that it costs Θ(
 |
 V
 |
 2
 ) in the worst case? 
 16.5
  State the greatest possible lower bound that you can prove for the all-
 pairs  
 shortest-paths problem, and justify your answer. 
 16.6
  Show the Skip List that results from inserting the following values. 
 Draw the Skip List after each insert. With each value, assume the depth 
 of its corresponding node is as given in the list.",NA
16.6 ,NA,NA
Projects,"16.1
  Complete the implementation of the Skip List-based dictionary begun 
 in Sec- 
 tion 16.2.2. 
 16.2
  Implement both a standard Θ(
 n
 3
 ) matrix multiplication algorithm and 
 Stras-sen’s matrix multiplication algorithm (see Exercise 14.16.3.3). 
 Using empir-ical testing, try to estimate the constant factors for the 
 runtime equations of the two algorithms. How big must
  n
  be before 
 Strassen’s algorithm becomes more efficient than the standard 
 algorithm?",NA
17,NA,NA
Limits to Computation,"This book describes data structures that can be used in a wide variety of 
 problems, and many examples of efficient algorithms. In general, our search 
 algorithms strive to be at worst in O(log
  n
 ) to find a record, and our sorting 
 algorithms strive to be in O(
 n
  log
  n
 ). A few algorithms have higher 
 asymptotic complexity. Both Floyd’s all-pairs shortest-paths algorithm and 
 standard matrix multiply have running times of Θ(
 n
 3
 ) (though for both, the 
 amount of data being processed is Θ(
 n
 2
 )). 
 We can solve many problems efficiently because we have available (and 
 choose to use) efficient algorithms. Given any problem for which you know
  
 some
  alg-orithm, it is always possible to write an inefficient algorithm to 
 “solve” the problem. For example, consider a sorting algorithm that tests 
 every possible permutation of its input until it finds the correct permutation 
 that provides a sorted list. The running time for this algorithm would be 
 unacceptably high, because it is proportional to the number of permutations 
 which is
  n
 ! for
  n
  inputs. When solving the minimum-cost spanning tree 
 problem, if we were to test every possible subset of edges to see which 
 forms the shortest minimum spanning tree, the amount of work would 
 problems we have more clever algorithms that allow us to find answers 
 (relatively) be proportional to 2
 |
 E
 |
 for a graph with
  |
 E
 |
  edges. Fortunately, for 
 both of these quickly without explicitly testing every possible solution. 
 Unfortunately, there are many computing problems for which the best 
 possible algorithm takes a long time to run. A simple example is the Towers 
 of Hanoi problem, which requires 2
 n
 moves to “solve” a tower with
  n
  disks. It 
 is not possible for any computer program that solves the Towers of Hanoi 
 problem to run in less than Ω(2
 n
 ) time, because that many moves must be 
 printed out. 
 Besides those problems whose solutions
  must
  take a long time to run, 
 there are also many problems for which we simply do not know if there are 
 efficient algo-rithms or not. The best algorithms that we know for such 
 problems are very slow, but perhaps there are better ones waiting to be 
 discovered. Of course, while having a problem with high running time is bad,",NA
17.1 ,NA,NA
Reductions,"We begin with an important concept for understanding the relationships 
 between problems, called
  reduction
 . Reduction allows us to solve one 
 problem in terms of another. Equally importantly, when we wish to 
 understand the difficulty of a problem, reduction allows us to make relative 
 statements about upper and lower bounds on the cost of a problem (as 
 opposed to an algorithm or program). 
 Because the concept of a problem is discussed extensively in this 
 chapter, we want notation to simplify problem descriptions. Throughout this 
 chapter, a problem will be defined in terms of a mapping between inputs 
 and outputs, and the name of the problem will be given in all capital letters. 
 Thus, a complete definition of the sorting problem could appear as follows: 
 SORTING:  
  
 Input
 : A sequence of integers
  x
 0
 ,
  x
 1
 ,
  x
 2
 , ...,
  x
 n−
 1
 . 
 whenever
  i < j
 . 
 Output
 : A permutation
  y
 0
 ,
  y
 1
 ,
  y
 2
 , ...,
  y
 n−
 1
  of the sequence such 
 that
  y
 i
  ≤ y
 j
  
 When you buy or write a program to solve one problem, such as sorting, 
 you might be able to use it to help solve a different problem. This is known 
 in software engineering as
  software reuse
 . To illustrate this, let us consider 
 another problem. 
 PAIRING:  
  
 Input
 : Two sequences of integers
  X 
  
 =  (
 x
 0
 , x
 1
 , ..., x
 n−
 1
 ) and
  Y 
 = (
 y
 0
 , 
 y
 1
 , ..., y
 n−
 1
 ). 
 Output
 : A pairing of the elements in the two sequences such that the 
 least value in
  X
  is paired with the least value in
  Y
 , the next least value in
  X
  
 is paired with the next least value in
  Y
 , and so on.",NA
17.2 ,NA,NA
Hard Problems,"There are several ways that a problem could be considered hard. For 
 example, we might have trouble understanding the definition of the problem 
 itself. At the be-ginning of a large data collection and analysis project, 
 developers and their clients might have only a hazy notion of what their",NA
17.3 ,NA,NA
Impossible Problems,"Even the best programmer sometimes writes a program that goes into an 
 infinite loop. Of course, when you run a program that has not stopped, you 
 do not know for sure if it is just a slow program or a program in an infinite 
 loop. After “enough time,” you shut it down. Wouldn’t it be great if your 
 compiler could look at your program and tell you before you run it that it 
 will get into an infinite loop? To be more specific, given a program and a 
 particular input, it would be useful to know if executing the program on that 
 input will result in an infinite loop without actually running the program. 
 Unfortunately, the
  Halting Problem
 , as this is called, cannot be solved. 
 There will never be a computer program that can positively determine, for 
 an arbitrary program
  P
 , if
  P
  will halt for all input. Nor will there even be a 
 computer program that can positively determine if arbitrary program
  P
  will 
 halt for a specified input
  I
 . How can this be? Programmers look at programs 
 regularly to determine if they will halt. Surely this can be automated. As a 
 warning to those who believe any program can be analyzed in this way, 
 carefully examine the following code fragment before reading on. 
 while (n > 1) 
  
  
 if (ODD(n)) 
  
  
  
 n = 3 * n + 1; 
  
  
 else 
  
  
  
 n = n / 2;
  
 This is a famous piece of code. The sequence of values that is assigned to
  
 n 
 by this code is sometimes called the
  Collatz sequence
  for input value
  n
 . 
 Does this code fragment halt for all values of
  n
 ? Nobody knows the answer. 
 Every input that has been tried halts. But does it always halt? Note that for 
 this code fragment, because we do not know if it halts, we also do not know 
 an upper bound for its running time. As for the lower bound, we can easily 
 show Ω(log
  n
 ) (see Exercise 3.14). 
 Personally, I have faith that someday some smart person will completely 
 ana-lyze the Collatz function, proving once and for all that the code fragment 
 halts for all values of
  n
 . Doing so may well give us techniques that advance 
 our ability to do algorithm analysis in general. Unfortunately, proofs from
  
 computability
  — the branch of computer science that studies what is 
 impossible to do with a computer— compel us to believe that there will 
 always be another bit of program code that",NA
17.4 ,NA,NA
Further Reading,NA,NA
17.5 ,NA,NA
Exercises,"17.1
  Consider this algorithm for finding the maximum element in an array: 
 First sort the array and then select the last (maximum) element. What 
 (if anything) does this reduction tell us about the upper and lower 
 bounds to the problem of finding the maximum element in a sequence? 
 Why can we not reduce SORTING to finding the maximum element? 
 17.2
  Use a reduction to prove that squaring an
  n × n
  matrix is just as 
 expensive  
 (asymptotically) as multiplying two
  n × n
  matrices. 
 17.3
  Use a reduction to prove that multiplying two upper triangular
  n × n
  
 matri- 
 ces is just as expensive (asymptotically) as multiplying two 
 arbitrary
  n × n 
 matrices. 
 17.4 
  
 (a)
  Explain why computing the factorial of
  n
  by multiplying all values 
  
  
 from 1 to
  n
  together is an exponential time algorithm. 
 (b)
  Explain why computing an approximation to the factorial of
  n
  by 
 mak-ing use of Stirling’s formula (see Section 2.2) is a polynomial 
 time algorithm. 
 17.5
  Consider this algorithm for solving the K-CLIQUE problem. First, 
 generate all subsets of the vertices containing exactly
  k
  vertices. There 
 are
  O
 (
 n
 k
 ) such subsets altogether. Then, check whether any subgraphs 
 induced by these subsets is complete. If this algorithm ran in 
 polynomial time, what would be its significance? Why is this not a 
 polynomial-time algorithm for the K-CLIQUE problem? 
 17.6
  Write the 3 SAT expression obtained from the reduction of SAT to 3 
 SAT  described in Section 17.2.1 for the expression 
 (
 a
  +
  b
  +
  c
  +
  d
 )
  ·
  (
 d
 )
  ·
  (
 b
  +
  c
 )
  ·
  (
 a
  +
  b
 )
  ·
  (
 a
  +
  c
 )
  ·
  (
 b
 )
 .
  
 Is this expression satisfiable? 
 17.7
  Draw the graph obtained by the reduction of SAT to the K-CLIQUE 
 problem  given in Section 17.2.1 for the expression",NA
17.6 ,NA,NA
Projects,"17.1
  Implement VERTEX COVER; that is, given graph
  G
  and integer
  k
 , 
 answer the question of whether or not there is a vertex cover of size
  k
  
 or less. Begin",NA
PART VI ,NA,NA
APPENDIX,575,NA
A,NA,NA
Utility Functions,"Here are various utility functions used by the C++ example programs in this text. 
 // Return true iff ""x"" is even 
  
 inline bool EVEN(int x) { return (x % 2) == 0; }
  
 // Return true iff ""x"" is odd 
  
 inline bool ODD(int x) { return (x % 2) != 0; }
  
 // Assert: If ""val"" is false, print a message and terminate // the program 
  
 void Assert(bool val, string s) { 
  
  
 if (!val) { // Assertion failed -- close the program 
  
  
 cout << 
 ""Assertion Failed: "" << s << endl; 
  
  
  
 exit(-1); 
  
  
 } 
  
 }
  
 // Swap two elements in a generic array 
  
 template<typename E> 
  
 inline void swap(E A[], int i, int j) { 
  
  
 E temp = A[i]; 
  
  
 A[i] = A[j]; 
  
  
 A[j] = temp; 
  
 } 
  
 // Random number generator functions
  
 inline void Randomize() // Seed the generator 
  
 { 
 srand(1); }
  
 // Return a random value in range 0 to n-1 inline int 
 Random(int n) 
  
  
 { return rand() % (n); }
  
 577",NA
Bibliography,"[AG06] 
 Ken Arnold and James Gosling.
  The Java Programming Language
 . 
 Addison-Wesley, Reading, MA, USA, fourth edition, 2006. 
 [Aha00]  
 Dan Aharoni. Cogito, ergo sum! cognitive processes of students 
 deal- 
 ing with data structures. In
  Proceedings of SIGCSE’00
 , pages 26–
 30,  
 ACM Press, March 2000. 
 [AHU74]  Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman.
  The Design 
  
 and Analysis of Computer Algorithms
 . Addison-Wesley, Reading, 
 MA,  
 1974. 
 [AHU83]  Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman.
  Data Struc-
  
 tures and Algorithms
 . Addison-Wesley, Reading, MA, 1983. 
 [BB96]  
 G. Brassard and P. Bratley.
  Fundamentals of Algorithmics
 . 
 Prentice  
 Hall, Upper Saddle River, NJ, 1996. 
 John Louis Bentley. Multidimensional binary search trees used for [Ben75]  
 associative searching.
  Communications of the ACM
 , 18(9):509–517, 
 September 1975. ISSN: 0001-0782. 
 [Ben82]  
 John Louis Bentley.
  Writing Efficient Programs
 . Prentice Hall, 
 Upper  
 Saddle River, NJ, 1982. 
 [Ben84]  
 John Louis Bentley. Programming pearls: The back of the 
 envelope.  
 Communications of the ACM
 , 27(3):180–184, March 1984. 
 [Ben85]  
 John Louis Bentley. Programming pearls: Thanks, heaps.
  
 Communi-
  
 cations of the ACM
 , 28(3):245–250, March 1985. 
 [Ben86]  
 John Louis Bentley. Programming pearls: The envelope is back.
  
 Com-
  
 munications of the ACM
 , 29(3):176–182, March 1986. 
 John Bentley.
  More Programming Pearls: Confessions of a Coder
 . [Ben88]  
 Addison-Wesley, Reading, MA, 1988. 
 [Ben00]  
 John Bentley.
  Programming Pearls
 . Addison-Wesley, Reading, MA, 
  
 second edition, 2000. 
 [BG00]  
 Sara Baase and Allen Van Gelder.
  Computer Algorithms: Introduction
  
 to Design & Analysis
 .  
 edition, 2000. 
 Addison-Wesley, Reading, MA, USA, third 
 579",NA
Index,"80/20 rule, 319, 343 
 abstract data type (ADT), xiv, 8–12, 
 20,  
 21, 49, 95–99, 133–142, 144, 
  
 145, 155, 168, 169, 204–206, 
  
 213, 215, 223, 224, 285–290, 
  
 381, 386, 388, 421, 436, 464 
 abstraction, 10  
 accounting, 120, 128  
 Ackermann’s function, 223 
 implementation, 8, 9, 21  
 artificial intelligence, 381  
 assert
 , xvi  
 asymptotic analysis,
  see
  algorithm  
  
 analysis, asymptotic  
 ATM machine, 6  
 average-case analysis, 61–62  
 AVL tree, 196, 359, 437, 442–446, 
 464 
 back of the envelope, napkin,
  see
  
 activation record,
  see
  compiler,  
 estimating 
  
 activation record  
 backtracking, 561 
 aggregate type, 8  
 bag, 26, 49  
 algorithm analysis, xiii, 4, 55–91, 231  bank, 6–7 
 amortized,
  see
  amortized 
 analysis asymptotic, 4, 55, 56, 
 65–70, 95, 
 basic operation, 5, 6, 20, 21, 57, 58, 
 63 best fit,
  see
  memory management, 
 best 
 469 fit 
 empirical comparison, 55–56, 85,  best-case analysis, 61–62 
 232  
 big-Oh notation,
  see
  O notation 
 for program statements, 71–75  
 bin packing, 562 
 multiple parameters, 79–80 
 running time measures, 57  
 space requirements, 56, 80–
 82 
 binary search,
  see
  search, 
 binary binary search tree,
  see
  
 BST  
 binary tree, 151–201, 203 
 algorithm, definition of, 17–18 BST,
  see
  BST 
 all-pairs shortest paths, 521–523, 540, complete, 152, 153, 168, 179, 251 
 543 full, 152–155, 166, 167, 186, 196, 
 amortized analysis, 73, 114, 321, 469, 221 
  
 484–485, 487, 489, 
 490 approximation, 561 
 implementation, 151, 153, 
 196 node, 151, 155, 160–
 166 
 array 
 NULL
  pointers, 155 
 dynamic, 113, 489 overhead, 166 
 585",NA
