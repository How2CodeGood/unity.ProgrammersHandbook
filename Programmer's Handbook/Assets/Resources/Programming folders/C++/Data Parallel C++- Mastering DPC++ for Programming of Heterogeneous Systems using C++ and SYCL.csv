Larger Text,Smaller Text,Symbol
Data Parallel ,NA,NA
C++ ,NA,NA
Mastering DPC++ for ,NA,NA
Programming of ,NA,NA
Heterogeneous Systems using ,NA,NA
C++ and ,NA,NA
SYCL,NA,NA
—,NA,NA
James ,NA,NA
Reinders ,NA,NA
Ben ,NA,NA
Ashbaugh ,NA,NA
James ,NA,NA
Brodman ,NA,NA
Michael ,NA,NA
Kinsner ,NA,NA
John ,NA,NA
Pennycook ,NA,NA
Xinmin Tian,NA,NA
Data Parallel C++,NA,NA
Mastering DPC++ ,NA,NA
for Programming ,NA,NA
of Heterogeneous ,NA,NA
Systems ,NA,NA
using C++ and ,NA,NA
SYCL,NA,NA
James ,NA,NA
Reinders ,NA,NA
Ben Ashbaugh ,NA,NA
James ,NA,NA
Brodman ,NA,NA
Michael ,NA,NA
Kinsner ,NA,NA
John ,NA,NA
Pennycook ,NA,NA
Xinmin Tian,NA,NA
Table of Contents,"About the Authors xvii
  
 Preface xix
  
 Acknowledgments xxiii
  
 Chapter 1: Introduction1 
 Read the Book, Not the Spec 2 SYCL 121 vs 
 SYCL 2020, and DPC++ 3 Getting a DPC++ Compiler 4 Book GitHub 4 Hello, 
 World! and a SYCL Program Dissection 5 Queues and Actions 6 It Is All 
 About Parallelism 7 Throughput 7 Latency 8 Think Parallel 8 Amdahl and 
 Gustafson 9 Scaling 9 Heterogeneous Systems 10 Data-Parallel 
 Programming 11 Key Attributes of DPC++ and SYCL 12 Single-Source 12 
 Host 13 Devices 13
  
 iii",NA
About the Authors,"James Reinders 
 is a consultant with more than three decades of 
  
 experience in parallel computing and is an author/coauthor/editor of 
 ten technical books related to parallel programming. He has had the 
 great fortune to help make key contributions to two of the world’s fastest 
 computers (#1 on the TOP500 list) as well as many other 
 supercomputers and software developer tools. James finished 10,001 
 days (over 27 years) at Intel in mid- 2016, and he continues to write, 
 teach, program, and consult in areas related to parallel computing (HPC 
 and AI).
  
 Ben Ashbaugh 
 is a Software Architect at Intel Corporation where he has 
 worked for over 20 years developing software drivers for Intel graphics 
 products.  For the past 10 years, Ben has focused on parallel programming 
 models for general-purpose computation on graphics processors, 
  
 including SYCL and DPC++.  Ben is active in the Khronos SYCL, OpenCL, 
 and SPIR working groups, helping to define industry standards for parallel 
 programming, and he has authored numerous extensions to expose 
 unique Intel GPU features.
  
 James Brodman 
 is a software engineer at Intel Corporation working 
 on runtimes and compilers for parallel programming, and he is one of 
 the architects of DPC++.  He has a Ph.D. in Computer Science from the 
 University of Illinois at Urbana-Champaign.
  
 Michael Kinsner 
 is a Principal Engineer at Intel Corporation developing 
 parallel programming languages and models for a variety of architectures, 
 and he is one of the architects of DPC++.  He contributes extensively to 
 spatial programming models and compilers, and is an Intel representative 
 within The Khronos Group where he works on the SYCL and OpenCL",NA
Preface,"Address 
  
 Spaces
  
  queue
  
 USM
  
  devices
  
 memory
  
 host
  
 model
  
 atomics
  
  SIMD
  
 communication
  
 data parallelism
  
 scan
  
 vectors
  
 get_info<>
  
 pack/unpack
  
 gather/scatter
  
 thread
  
  FPGA 
  
 emulator
  
  stencil
  
 affinity
  
 This book is about programming for 
 data parallelism
  using C++. If you are 
 new to parallel programming, that is okay. If you have never heard of SYCL 
 or the DPC++ compiler, that is also okay.
  
 SYCL is an industry-driven Khronos standard adding data parallelism 
 to C++ for heterogeneous systems. DPC++ is an open source compiler 
 project that is based on SYCL, a few extensions, and broad heterogeneous 
 support that includes GPU, CPU, and FPGA support. All examples in this 
 book compile and work with DPC++ compilers.
  
 xix",NA
 Continuing to Evolve,"When this book project began in 2019, our vision for fully supporting C++ 
 with data parallelism required a number of extensions beyond the then- 
 current SYCL 1.2.1 standard. These extensions, supported by the DPC++ 
 compiler, included support for Unified Shared Memory (USM), sub-groups 
 to complete a three-level hierarchy throughout SYCL, anonymous 
 lambdas, and numerous programming simplifications.
  
 At the time that this book is being published (late 2020), a provisional 
 SYCL 2020 specification is available for public comment. The provisional 
 specification includes support for USM, sub-groups, anonymous lambdas, 
 and simplifications for coding (akin to C++17 CTAD). This book teaches 
 SYCL with extensions to approximate where SYCL will be in the future. 
 These extensions are implemented in the DPC++ compiler project. While 
 we expect changes to be small compared to the bulk of what this book 
 teaches, there will be changes with SYCL as the community helps refine it. 
 Important resources for updated information include the book GitHub and 
 errata that can be found from the web page for this book (
 www.apress.
  
 com/9781484255735
 ), as well as the oneAPI DPC++ language 
 reference (
 tinyurl.com/dpcppref
 ).
  
 xx",NA
 Structure of This Book,"It is hard to leap in and explain everything at once. In fact, it is impossible 
 as far as we know. Therefore, this book is a journey that takes us through 
 what we need to know to be an effective programmer with Data Parallel 
 C++.
  
  
 Chapter 
 1
  lays the first foundation by covering core concepts that are 
 either new or worth refreshing in our minds.
  
 Chapters 
 2
 –
 4
  lay a foundation of understanding for data-parallel 
 programming C++. When we finish with reading Chapters 
 1
 –
 4
 , 
  
 we will have a solid foundation for data-parallel programming in C++. 
 Chapters 
 1
 –
 4
  build on each other, and are best read in order.
  
 Ch 13-19
  
 patterns      tips       libraries 
 atomics        memory models 
 GPUs         CPUs         FPGAs
  
  
 Welcome to 
  
 DPC ++
  
 Ch  5-12
  
  error handling     scheduling 
 communication   vector    devices
  
 Ch  1-4
  
 queue       buffer         USM 
 accessor       kernel
  
 xxi",NA
Acknowledgments,"We all get to new heights by building on the work of others. Isaac Newton 
 gave credit for his success from “standing on the shoulders of giants.” We 
 would all be in trouble if this was not allowed.
  
 Perhaps there is no easy path to writing a new book on an exciting new 
 developments such as SYCL and DPC++. Fortunately, there are good people 
 who make that path easier—it is our great joy to thank them for their help!
  
  
 We are deeply thankful for all those whose work has helped make this 
 book possible, and we do wish to thank as many as we can recall by name. 
  
 If we stood on your shoulders and did not call you out by name, you can 
 know we are thankful, and please accept our apologies for any accidental 
 forgetfulness.
  
 A handful of people tirelessly read our early manuscripts and provided 
 insightful feedback for which we are very grateful. These reviewers include 
 Jefferson Amstutz, Thomas Applencourt, Alexey Bader, Gordon Brown, 
 Konstantin Bobrovsky, Robert Cohn, Jessica Davies, Tom Deakin, Abhishek 
 Deshmukh, Bill Dieter, Max Domeika, Todd Erdner, John Freeman, Joe 
 Garvey, Nithin George, Milind Girkar, Sunny Gogar, Jeff Hammond, 
  
 Tommy Hoffner, Zheming Jin, Paul Jurczak, Audrey Kertesz, Evgueny 
 Khartchenko, Jeongnim Kim, Rakshith Krishnappa, Goutham Kalikrishna 
 Reddy Kuncham, Victor Lomüller, Susan Meredith, Paul Petersen, Felipe De 
 Azevedo Piovezan, Ruyman Reyes, Jason Sewall, Byron Sinclair, 
  
 Philippe Thierry, and Peter Žužek.
  
 We thank the entire development team at Intel who created DPC++ 
 including its libraries and documentation, without which this book would 
 not be possible.
  
 xxiii",NA
CHAPTER 1,NA,NA
Introduction,"This chapter lays the foundation by covering core concepts, including 
 terminology, that are critical to have fresh in our minds as we learn how to 
 accelerate C++ programs using data parallelism.
  
 Data parallelism in C++ enables access to parallel resources in a 
 modern heterogeneous system. A single C++ application can use any 
 combination of devices—including GPUs, CPUs, FPGAs, and AI 
 Application-Specific Integrated Circuits (ASICs)—that are suitable to the 
 problems at hand.
  
 This book teaches data-parallel programming using C++ and SYCL.
  
 SYCL (pronounced 
 sickle
 ) is an industry-driven Khronos standard that 
 adds data parallelism to C++ for heterogeneous systems. SYCL programs 
 perform best when paired with SYCL-aware C++ compilers such as the 
 open source Data Parallel C++ (DPC++) compiler used in this book. SYCL is 
 not an acronym; SYCL is simply a name.
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_1
  
 1",NA
" Read the Book, Not the Spec","No one wants to be told “Go read the spec!” Specifications are hard to 
 read, and the SYCL specification is no different. Like every great language 
 specification, it is full of precision and light on motivation, usage, and 
 teaching. This book is a “study guide” to teach SYCL and use of the DPC++ 
 compiler.
  
 As mentioned in the Preface, this book cannot explain 
 everything at 
 once
 . Therefore, this chapter does what no other chapter will do: the code 
 examples contain programming constructs that go unexplained until 
 future chapters. We should try to not get hung up on understanding the 
 coding examples completely in Chapter 
 1
  and trust it will get better with 
 each chapter.
  
 1
  The DPC++ team is quick to point out that they hope all their extensions will 
 be considered, and hopefully accepted, by the SYCL standard at some time in 
 the future.
  
 2",NA
" SYCL 1.2.1 vs. SYCL 2020, and DPC++","As this book goes to press, the provisional SYCL 2020 specification is 
 available for public comments. In time, there will be a successor to the 
 current SYCL 1.2.1 standard. That anticipated successor has been 
 informally referred to as SYCL 2020. While it would be nice to say that this 
 book teaches SYCL 2020, that is not possible because that standard does 
 not yet exist.
  
 This book teaches SYCL with extensions, to approximate where SYCL 
 will be in the future.  These extensions are implemented in the DPC++ 
 compiler project. Almost all the extensions implemented in DPC++ exist as 
 new features in the provisional SYCL 2020 specification. Notable new 
 features that DPC++ supports are USM, sub- groups, syntax simplifications 
 enabled by C++17 (known as CTAD—class template argument deduction), 
 and the ability to use anonymous lambdas without having to name them.
  
  
 At publication time, 
 no
  SYCL compiler (including DPC++) exists that 
 implements 
 all
  the functionality in the SYCL 2020 provisional 
 specification.
  
 Some of the features used in this book are specific to the DPC++ 
 compiler. Many of these features were originally Intel extensions to 
  
 SYCL that have since been accepted into the SYCL 2020 provisional 
  
 specification, and in some cases their syntax has changed slightly during 
 the standardization process. Other features are still being developed or are 
 under discussion for possible inclusion in future SYCL standards, and their 
 syntax may similarly be modified. Such syntax changes are actually highly 
 desirable during language development, as we want features to evolve and 
 improve to address the needs of wider groups of developers and the 
 capabilities of a wide range of devices. All of the code samples in this book 
 use the DPC++ syntax to ensure compatibility with the DPC++ compiler.
  
 While endeavoring to approximate where SYCL is headed, there will 
 almost certainly need to be adjustments to information in this book to 
 align with the standard as it evolves. Important resources for updated",NA
 Getting a DPC++ Compiler,"DPC++ is available from a GitHub repository (
 github.com/intel/llvm
 ). 
 Getting started with DPC++ instructions, including how to build the open 
 source compiler with a clone from GitHub, can be found at 
 intel.github.io/ 
 llvm-docs/GetStartedGuide.html
 .
  
 There are also bundled versions of the DPC++ compiler, augmented 
 with additional tools and libraries for DPC++ programming and support, 
 available as part of a larger oneAPI project. The project brings broad 
 support for heterogeneous systems, which include libraries, debuggers, 
 and other tools, known as oneAPI. The oneAPI tools, including DPC++, 
 are available freely (
 oneapi.com/implementations
 ). The official oneAPI 
 DPC++ Compiler Documentation, including a list of extensions, can be 
 found at 
 intel.github.io/llvm-docs
 .",NA
"the online companion to this book, the ",NA,NA
oneapI dpC++ ,NA,NA
language reference online,NA,NA
", is a great resource for more ",NA,NA
formal details building upon what is taught in this book.,NA,NA
 Book GitHub,"Shortly we will encounter code in Figure 
 1-1
 . If we want to avoid typing it 
 all in, we can easily download all the examples in this book from a GitHub 
 repository (
 www.apress.com/9781484255735
 —look for Services for this 
 book: Source Code). The repository includes the complete code with build 
 files, since most code listings omit details that are repetitive or otherwise 
  
 4",NA
" Hello, World! and a SYCL ",NA,NA
Program Dissection,"Figure 
 1-1
  shows a sample SYCL program. Compiling with the DPC++ 
 compiler, and running it, results in the following being printed: 
  
  
 Hello, world! (and some additional text left to experience by running 
 it) 
  
 We will completely understand this particular example by the end of 
 Chapter 
 4
 . Until then, we can observe the single include of <CL/sycl. hpp> 
 (line 1) that is needed to define all the SYCL constructs. All SYCL 
 constructs live inside a namespace called sycl:
  
 5",NA
 Queues and Actions,"Chapter 
 2
  will discuss queues and actions, but we can start with a simple 
 explanation for now. Queues are the only connection that allows an 
 application to direct work to be done on a device. There are two types of 
 actions that can be placed into a queue: (a) code to execute and (b) 
 memory operations. Code to execute is expressed via either single_task, 
 parallel_for (used in Figure 
 1-1
 ), or parallel_for_work_group. Memory 
 operations perform copy operations between host and device or fill 
  
 6",NA
Queues connect us to devices.,NA,NA
We submit actions into these queues to request ,NA,NA
computational work and data movement.,NA,NA
actions happen asynchronously.,NA,NA
 It Is All About Parallelism,"Since programming in C++ for data parallelism is all about parallelism, 
 let’s start with this critical concept. The goal of parallel programming is 
 to compute something faster. It turns out there are two aspects to this: 
 increased throughput
  and 
 reduced latency
 .",NA
 Throughput,"Increasing throughput of a program comes when we get more work done 
 in a set amount of time. Techniques like pipelining may actually stretch 
 out the time necessary to get a single work-item done, in order to allow 
 overlapping of work that leads to more work-per-unit-of-time being 
  
 7",NA
 Latency,"What if we want to get one thing done faster—for instance, analyzing a 
 voice command and formulating a response? If we only cared about 
 throughput, the response time might grow to be unbearable. The concept 
 of latency reduction requires that we break up an item of work into 
 pieces that can be tackled in parallel. For throughput, image processing 
 might assign whole images to different processing units—in this case, our 
 goal may be optimizing for images per second. For latency, image 
 processing might assign each pixel within an image to different 
 processing cores—in this case, our goal may be maximizing pixels per 
 second from a single image.",NA
 Think Parallel,"Successful parallel programmers use both techniques in their 
 programming. This is the beginning of our quest to 
 Think Parallel
 .
  
 We want to adjust our minds to think first about where parallelism can 
 be found in our algorithms and applications. We also think about how 
 different ways of expressing the parallelism affect the performance we 
 ultimately achieve. That is a 
 lot
  to take in all at once. The quest to 
 Think 
 Parallel
  becomes a lifelong journey for parallel programmers. We can learn 
 a few tips here.
  
 8",NA
 Amdahl and Gustafson,"Amdahl’s Law, stated by the supercomputer pioneer Gene Amdahl in 1967, 
 is a formula to predict the theoretical maximum speed-up when using 
 multiple processors. Amdahl lamented that the maximum gain from 
 parallelism is limited to (1/(1-p)) where p is the fraction of the program 
 that runs in parallel. If we only run two-thirds of our program in parallel, 
 then the most that program can speed up is a factor of 3. We definitely 
 need that concept to sink in deeply! This happens because no matter how 
 fast we make that two-thirds of our program run, the other one-third still 
 takes the same time to complete. Even if we add one hundred GPUs, we 
 would only get a factor of 3 increase in performance.
  
 For many years, some viewed this as proof that parallel computing 
 would not prove fruitful. In 1988, John Gustafson presented an article 
 titled “Reevaluating Amdahl’s Law.” He observed that parallelism was not 
 used to speed up fixed workloads, but rather it was used to allow work to 
 be scaled up. Humans experience the same thing. One delivery person 
 cannot deliver a single package faster with the help of many more people 
 and trucks. However, a hundred people and trucks can deliver one 
 hundred packages more quickly than a single driver with a truck. Multiple 
 drivers will definitely increase throughput and will also generally reduce 
 latency for package deliveries. Amdahl’s Law tells us that a single driver 
 cannot deliver one package faster by adding ninety-nine more drivers with 
 their own trucks. Gustafson noticed the opportunity to deliver one 
 hundred packages faster with these extra drivers and trucks.",NA
 Scaling,"The word “scaling” appeared in our prior discussion. Scaling is a measure 
 of how much a program speeds up (simply referred to as “speed-up”) 
 when additional computing is available. Perfect speed-up happens if one 
 hundred packages are delivered in the same time as one package, 
  
 9",NA
 Heterogeneous Systems,"The phrase “heterogeneous system” snuck into the prior paragraph. For 
 our purposes, a heterogeneous system is any system which contains 
 multiple types of computational devices. For instance, a system with both a 
 Central Processing Unit (CPU) and a Graphics Processing Unit (GPU) is a 
 heterogeneous system. The CPU is often just called a processor, although 
 that can be confusing when we speak of all the processing units in a 
 heterogeneous system as compute processors. To avoid this confusion, 
 SYCL refers to processing units as 
 devices
 . Chapter 
 2
  will begin the 
  
 discussion of how to steer work (computations) to particular devices in a 
 heterogeneous system.
  
 GPUs have evolved to become high-performance computing devices 
 and therefore are sometimes referred to as General-Purpose GPUs, or 
 GPGPUs. For heterogeneous programming purposes, we can simply 
 assume we are programming such powerful GPGPUs and refer to them as 
 GPUs.
  
 Today, the collection of devices in a heterogeneous system can include 
 CPUs, GPUs, FPGAs (Field Programmable Gate Arrays), DSPs (Digital Signal 
 Processors), ASICs (Application-Specific Integrated Circuits), and AI chips 
 (graph, neuromorphic, etc.).",NA
SYCL was created to address the challenges of C++ data-,NA,NA
parallel programming for heterogeneous systems.,NA,NA
 Data-Parallel Programming,"The phrase “data-parallel programming” has been lingering unexplained 
 ever since the title of this book. Data-parallel programming focuses on 
 parallelism that can be envisioned as a bunch of data to operate on in 
 parallel. This shift in focus is like Gustafson vs. Amdahl. We need one 
 hundred packages to deliver (effectively lots of data) in order to divide 
 up the work among one hundred trucks with drivers. The key concept 
 comes down to what we should divide. Should we process whole images 
 or process them in smaller tiles or process them pixel by pixel? Should 
 we analyze a collection of objects as a single collection or a set of smaller 
 groupings of objects or object by object?
  
 11",NA
 Key Attributes of DPC++ and SYCL,"Every DPC++ (or SYCL) program is also a C++ program. Neither SYCL 
 nor DPC++ relies on any language changes to C++. Both can be fully 
 implemented with templates and lambda functions.
  
 The reason SYCL compilers
 2
  exist is to optimize code in a way that 
 relies on built-in knowledge of the SYCL specification. A standard C++ 
 compiler that lacks any built-in knowledge of SYCL cannot lead to the 
 same performance levels that are possible with a SYCL-aware 
 compiler.
  
  
 Next, we will examine the key attributes of DPC++ and SYCL: 
 single- 
 source
  style, host, devices, kernel code, and asynchronous task graphs.",NA
 Single-Source,"Programs can be single-source, meaning that the same translation unit
 3 
 contains both the code that defines the compute kernels to be executed on 
 devices and also the host code that orchestrates execution of those 
 compute kernels. Chapter 
 2
  begins with a more detailed look at this 
 capability. We can still divide our program source into different files and 
 translation units for host and device code if we want to, but the key is that 
 we don't have to!
  
 2
  It is probably more correct to call it a C++ compiler with support for SYCL. 
 3
  
 We could just say “file,” but that is not entirely correct here. A translation unit 
 is the actual input to the compiler, made from the source file after it has been 
 processed by the C preprocessor to inline header files and expand macros.",NA
 Host,"Every program starts by running on a host, and most of the 
 lines
  of code in 
 a program are usually for the host. Thus far, hosts have always been CPUs. 
 The standard does not require this, so we carefully describe it as a host. 
 This seems unlikely to be anything other than a CPU because the host 
 needs to fully support C++17 in order to support all DPC++ and SYCL 
 programs. As we will see shortly, devices do not need to support all of 
 C++17.",NA
 Devices,"Using multiple devices in a program is what makes it heterogeneous 
 programming. That’s why the word 
 device
  has been recurring in this 
 chapter since the explanation of heterogeneous systems a few pages 
 ago. 
  
 We already learned that the collection of devices in a heterogeneous 
 system can include GPUs, FPGAs, DSPs, ASICs, CPUs, and AI chips, but is 
 not limited to any fixed list.
  
 Devices are the target for 
 acceleration offload
  that SYCL promises. 
  
 The idea of offloading computations is generally to transfer work to a 
 device that can accelerate completion of the work. We have to worry about 
 making up for time lost moving data—a topic that needs to constantly be 
 on our minds.",NA
 Sharing Devices,"On a system with a device, such as a GPU, we can envision two or more 
 programs running and wanting to use a single device. They do not need 
 to be programs using SYCL or DPC++. Programs can experience delays in 
 processing by the device if another program is currently using it. This is 
 really the same philosophy used in C++ programs in general for CPUs. 
 Any system can be overloaded if we run too many active programs on our 
 CPU (mail, browser, virus scanning, video editing, photo editing, etc.) all 
 at once.",NA
 Kernel Code,"Code for a device is specified as kernels. This is a concept that is not unique 
 to SYCL or DPC++: it is a core concept in other offload acceleration 
 languages including OpenCL and CUDA.
  
 Kernel code has certain restrictions to allow broader device support 
 and massive parallelism. The list of features 
 not
  supported in kernel code 
 includes dynamic polymorphism, dynamic memory allocations (therefore 
 no object management using new or delete operators), static variables, 
 function pointers, runtime type information (RTTI), and exception 
  
 handling. No virtual member functions, and no variadic functions, are 
 allowed to be called from kernel code. Recursion is not allowed within 
 kernel code.
  
 Chapter 
 3
  will describe how memory allocations are done before and 
 after kernels are invoked, thereby making sure that kernels stay focused 
 on massively parallel computations. Chapter 
 5
  will describe handling of 
 exceptions that arise in connection with devices.
  
 The rest of C++ is fair game in a kernel, including lambdas, operator 
 overloading, templates, classes, and static polymorphism. We can also 
 share data with host (see Chapter 
 3
 ) and share the read-only values of 
 (non-global) host variables (via lambda captures).
  
 14",NA
 Kernel: Vector Addition (DAXPY),"Kernels should feel familiar to any programmer who has work on 
 computationally complex code. Consider implementing DAXPY, which 
 stands for “Double-precision A times X Plus Y.” A classic for decades. 
  
 Figure 
 1-2
  shows DAXPY implemented in modern Fortran, C/C++, and 
 SYCL. Amazingly, the computation lines (line 3) are virtually identical. 
  
 Chapters 
 4
  and 
 10
  will explain kernels in detail. Figure 
 1-2
  should help 
 remove any concerns that kernels are difficult to understand—they should 
 feel familiar even if the terminology is new to us.
  
 1. 
 2. 
 3. 
 4.
  
 ! Fortran loop 
  
 do 
 i = 
 1
 , n 
  
 z
 (i) = alpha * 
 x
 (i) + 
 y
 (i) end 
 do
  
  
 1. 
 2. 
 3. 
 4.
  
 // C++ loop 
  
 for 
 (
 int 
 i=
 0
 ;i<n;i++) { 
  
 z
 [i] = alpha * 
 x
 [i] + 
 y
 [i]; 
 }
  
 1. 
 2. 
 3. 
 4.
  
 // SYCL kernel 
  
 myq
 .
 parallel_for
 (
 range
 {n},[=](
 id
 <
 1
 > 
 i
 ) { 
 z
 [i] = alpha * 
 x
 [i] 
 + 
 y
 [i]; 
  
 }).
 wait
 ();
  
 Figure 1-2. 
 DAXPY computations in Fortran, C++, and SYCL",NA
 Asynchronous Task Graphs,"The asynchronous nature of programming with SYCL/DPC++ must 
 not 
 be 
 missed. Asynchronous programming is critical to understand for two 
 reasons: (1) proper use gives us better performance (better scaling), and 
 (2) mistakes lead to parallel programming errors (usually race conditions) 
 that make our applications unreliable.
  
 15",NA
 Race Conditions When We Make a Mistake,"In our first code example (Figure 
 1-1
 ), we specifically did a “wait” on line 
 18 to prevent line 20 from writing out the value from result before it was 
 available. We must keep this asynchronous behavior in mind. There is 
 another subtle thing done in that same code example—line 14 uses 
 std::memcpy to load the input. Since std::memcpy runs on the host, line 16 
 and later do not execute until line 15 has completed. After reading Chapter 
 3
 , we could be tempted to change this to use myQ.memcpy (using SYCL). 
  
 We have done exactly that in Figure 
 1-3
  in line 8. Since that is a queue 
 submission, there is no guarantee that it will complete before line 10. 
 This creates a 
 race condition
 , which is a type of parallel programming 
 bug. A race condition exists when two parts of a program access the same 
 data without coordination. Since we expect to write data using line 8 and 
 then read it in line 10, we do not want a race that might have line 17 
 execute",NA
Chapter ,NA,NA
4,NA,NA
 will tell us “lambdas not considered harmful.” We ,NA,NA
should be comfortable with lambda functions in order to use ,NA,NA
"dpC++, SYCL, and modern C++ well.",NA,NA
 C++ Lambda Functions,"A feature of modern C++ that is heavily used by parallel programming 
 techniques is the lambda function. Kernels (the code to run on a device) 
 can be expressed in multiple ways, the most common one being a lambda 
 function. Chapter 
 10
  discusses all the various forms that a kernel can take, 
 including lambda functions. Here we have a refresher on C++ lambda 
 functions plus some notes regarding use to define kernels. Chapter 
 10 
 expands on the kernel aspects after we have learned more about SYCL in 
 the intervening chapters.
  
 The code in Figure 
 1-3
  has a lambda function. We can see it because it 
 starts with the very definitive [=]. In C++, lambdas start with a square 
 bracket, and information before the closing square bracket denotes how to 
 capture
  variables that are used within the lambda but not explicitly passed 
 to it as parameters. For kernels, the capture must be 
 by value
  which is 
 denoted by the inclusion of an equals sign within the brackets.
  
 Support for lambda expressions was introduced in C++11. They are 
 used to create anonymous function objects (although we can assign them 
 to named variables) that can capture variables from the enclosing scope. 
  
 The basic syntax for a C++ lambda expression is
  
 [ capture-
 list
 ] ( 
 params 
 ) -> ret { body }
  
 18",NA
 Portability and Direct Programming,"Portability is a key objective for SYCL and DPC++; however, neither can 
  
 guarantee it. All a language and compiler can do is make portability a little 
  
 easier for us to achieve in our applications when we want to do so.
  
 Portability is a complex topic and includes the concept of 
 functional 
  
 portability
  as well as 
 performance portability
 . With functional portability, 
  
 we expect our program to compile and run equivalently on a wide variety 
  
 of platforms. With performance portability, we would like our program to 
  
 get reasonable performance on a wide variety of platforms. While that is 
  
 a pretty soft definition, the converse might be clearer—we do not want to 
  
 write a program that runs superfast on one platform only to find that it is 
  
 unreasonably slow on another. In fact, we’d prefer that it got the most out 
  
 of any platform that it is run upon. Given the wide variety of devices in a 
  
 heterogeneous system, performance portability requires non-trivial effort 
  
 from us as programmers.
  
 21",NA
 Concurrency vs. Parallelism,"The terms 
 concurrent
  and 
 parallel
  are not equivalent, although they are 
 sometimes misconstrued as such. It is important to know that any 
 programming consideration needed for concurrency is also important for 
 parallelism.
  
 The term concurrent refers to code that can be advancing but not 
 necessarily at the same instant. On our computers, if we have a Mail 
 program open and a Web Browser, then they are running concurrently. 
 Concurrency can happen on systems with only one processor, through a",NA
Tip,NA,NA
 any programming consideration needed for concurrency ,NA,NA
is also important for parallelism.,"The term parallel refers to code that can be advancing at the same 
 instant. Parallelism requires systems that can actually do more than one 
 thing at a time. A heterogeneous system can always do things in parallel, 
 by its very nature of having at least two compute devices. Of course, a SYCL 
 program does not require a heterogeneous system as it can run on a host- 
 only system. Today, it is highly unlikely that any host system is not capable 
 of parallel execution.
  
 Concurrent execution of code generally faces the same issues as 
 parallel execution of code, because any particular code sequence cannot 
 assume that it is the only code changing the world (data locations, I/O, 
 etc.).",NA
 Summary,"This chapter provided terminology needed for SYCL and DPC++ and 
 provided refreshers on key aspects of parallel programming and C++ that 
 are critical to SYCL and DPC++. Chapters 
 2
 , 
 3
 , and 
 4
  expand on three keys 
 to SYCL programming: devices need to be given work to do (send code to 
 run on them), be provided with data (send data to use on them), and have 
 a method of writing code (kernels).
  
 23",NA
CHAPTER 2,NA,NA
Where Code ,NA,NA
Executes,"Parallel programming is not really about driving in 
 the
  fast lane. It is 
 actually about driving fast in 
 all
  the lanes. This chapter is all about 
 enabling us to put our code everywhere that we can. We choose to enable 
 all the compute resources in a heterogeneous system whenever it makes 
 sense. Therefore, we need to know where those compute resources are 
 hiding (find them) and put them to work (execute our code on them).
  
 We can control 
 where
  our code executes—in other words, we can 
 control which devices are used for which kernels. SYCL provides a 
  
 framework for heterogeneous programming in which code can execute on 
 a mixture of a host CPU and devices. The mechanisms which determine 
 where code executes are important for us to understand and use.
  
 This chapter describes where code can execute, when it will execute, 
 and the mechanisms used to control the locations of execution. Chapter 
 3 
 will describe how to manage data so it arrives where we are executing our 
 code, and then Chapter 
 4
  returns to the code itself and discusses the 
 writing of kernels.",NA
 Single-Source,"SYCL programs can be single-source, meaning that the same translation 
 unit (typically a source file and its headers) contains both the code that 
 defines the compute kernels to be executed on SYCL devices and also the 
 host code that orchestrates execution of those kernels. Figure 
 2-1
  shows 
 these two code paths graphically, and Figure 
 2-2
  provides an example 
 application with the host and device code regions marked.
  
 Combining both device and host code into a single-source file (or 
 translation unit) can make it easier to understand and maintain a 
 heterogeneous application. The combination also provides improved 
 language type safety and can lead to more compiler optimizations of our 
 code.
  
  
 Figure 2-1. 
 Single-source code contains both host code (runs on 
 CPU) and device code (runs on SYCL devices)
  
 26",NA
 Host Code,"Applications contain C++ host code, which is executed by the CPU(s) on 
  
 which the operating system has launched the application. Host code is the 
  
 backbone of an application that defines and controls assignment of work 
  
 to available devices. It is also the interface through which we define the 
  
 data and dependences that should be managed by the runtime.
  
 Host code is standard C++ augmented with SYCL-specific constructs 
  
 and classes that are designed to be implementable as a C++ library. This 
  
 makes it easier to reason about what is allowed in host code (anything 
 that 
  
 is allowed in C++) and can simplify integration with build systems.
  
 27",NA
sYCL applications are standard C++ augmented with ,NA,NA
constructs that can be implemented as a C++ library.,NA,NA
a sYCL compiler may provide higher performance for a ,NA,NA
program by “understanding” these constructs.,"The host code in an application orchestrates data movement and 
 compute offload to devices, but can also perform compute-intensive work 
 itself and can use libraries like any C++ application.",NA
 Device Code,"Devices correspond to accelerators or processors that are conceptually 
 independent from the CPU that is executing host code. An implementation 
 must expose the host processor also as a device, as described later in this 
 chapter, but the host processor and devices should be thought of as 
 logically independent from each other. The host processor runs native C++ 
 code, while devices run device code.
  
 Queues are the mechanism through which work is submitted to a 
 device for future execution. There are three important properties of device 
 code to understand:
  
  1. 
 It executes asynchronously from the host code. 
 The host program submits device code to a device, 
 and the runtime tracks and starts that work only 
 when all dependences for execution are satisfied 
 (more on this in Chapter 
 3
 ). The host program 
 execution carries on before the submitted work is 
 started on a device, providing the property that 
 execution on devices is asynchronous to host 
 program execution, unless we explicitly tie the two 
 together.
  
 28",NA
 Choosing Devices,"To explore the mechanisms that let us control where device code will 
 execute, we’ll look at five use cases:
  
 Method#1: 
  
 Running device code 
 somewhere
 , when we 
  
 don’t care which device is used. This is often 
  
 the first step in development because it is the 
  
 simplest.
  
 29",NA
developers will typically debug their code as much as ,NA,NA
possible with Method#2 and only move to Methods #3–#5 ,NA,NA
when code has been tested as much as is practical with ,NA,NA
Method#2.,NA,NA
 Method#1: Run on a Device of Any ,NA,NA
Type,"When we don’t care where our device code will run, it is easy to let the 
 runtime pick for us. This automatic selection is designed to make it easy to 
 start writing and running code, when we don’t yet care about what device 
 is chosen. This device selection does 
 not
  take into account the code to be 
 run, so should be considered an arbitrary choice which likely won’t be 
 optimal.
  
 Before talking about choice of a device, even one that the 
  
 implementation has selected for us, we should first cover the mechanism 
 through which a program interacts with a device: the 
 queue
 .",NA
 Queues,"A queue is an abstraction to which actions are submitted for execution 
  
 on a single device. A simplified definition of the queue class is given 
  
 in Figures 
 2-3
  and 
 2-4
 . Actions are usually the launch of data-parallel 
  
 compute, although other commands are also available such as manual 
  
 control of data motion for when we want more control than the automatic 
  
 movement provided by the runtime. Work submitted to a queue can 
  
 execute after prerequisites tracked by the runtime are met, such as 
  
 availability of input data. These prerequisites are covered in Chapters 
 3
  
 and 
 8
 .
  
 class
  queue
  { 
  
 public: 
  
  
 // Create a queue associated with the default device 
  
 queue
 (
 const
  
 property_list
  = {}); 
  
  
 queue
 (
 const
  async_handler
 &
 , 
  
   
 const
  property_list
  = {});
  
 // Create a queue associated with an explicit device // A device selector 
 may be used in place of a device 
 queue
 (
 const device&
 , 
 const
  property_list
  
 = {}); 
  
 queue
 (
 const device&
 , 
 const
  async_handler
 &
 , 
  
  
 const
  property_list
  = {});
  
  
 // Create a queue associated with a device in a specific context 
  
 // A device selector 
 may be used in place of a device 
  
  
 queue
 (
 const context&
 , 
 const device&
 , 
  
   
 const
  property_list
  = {}); 
  
  
 queue
 (
 const context&
 , 
 const device&
 , 
  
   
 const
  async_handler
 &
 , 
  
   
 const
  property_list
  = {}); 
  
 };
  
 Figure 2-3. 
 Simplified definition of the constructors of the queue class
  
 31",NA
" Binding a Queue to a Device, When Any ",NA,NA
Device Will Do,"Figure 
 2-7
  is an example where the device that a queue should bind to is 
 not specified. The trivial queue constructor that does not take any 
 arguments (as in Figure 
 2-7
 ) simply chooses some available device 
 behind the scenes. SYCL guarantees that at least one device will always 
 be available—namely, the host device. The host device can run kernel 
 code and is an abstraction of the processor on which the host program is 
 executing so is always present.
  
 34",NA
 Method#2: Using the Host Device ,NA,NA
for Development and Debugging,"The host device can be thought of as enabling the host CPU to act as if it 
  
 was an independent device, allowing our device code to execute regardless 
  
 of the accelerators available in a system. We always have some processor 
  
 running the host program, so the host device is therefore always available 
  
 to our application. The host device provides a guarantee that device code 
  
 can always be run (no dependence on accelerator hardware) and has a few 
  
 primary uses:
  
 35",NA
 Method#3: Using a GPU (or ,NA,NA
Other Accelerators),"GPUs are showcased in the next example, but any type of accelerator 
 applies equally. To make it easy to target common classes of accelerators, 
 devices are grouped into several broad categories, and SYCL provides 
 built-in selector classes for them. To choose from a broad category of 
 device type such as “any GPU available in the system,” the corresponding 
 code is very brief, as described in this section.",NA
 Device Types,"There are two main categories of devices to which a queue can be bound:
  
  1. The host device, which has already been described.
  
  2. Accelerator devices such as a GPU, an FPGA, or a CPU 
 device, which are used to accelerate workloads in 
 our applications.",NA
 Accelerator Devices,"There are a few broad groups of accelerator types:
  
  1. CPU devices
  
  2. GPU devices
  
 38",NA
 Device Selectors,"Classes that must be bound to a specific device, such as the queue class, 
 have constructors that can accept a class derived from device_selector. 
  
 For example, the queue constructor is
  
 queue( const device_selector &deviceSelector,
  
  const 
 property_list &propList = {});
  
 There are five built-in selectors for the broad classes of common 
 devices:
  
 default_selector 
 host_selector 
  
 cpu_selector
  
 any device of the implementation’s 
 choosing. select the host device 
 (always available).
  
 select a device that identifies itself as a Cpu in 
  
 device queries.
  
 gpu_selector 
  
 select a device that identifies itself as a 
 Gpu in 
  
 device queries.
  
 accelerator_selector 
  
 select a device that identifies itself 
 as an 
  
 “accelerator,” which includes FpGas.
  
 One additional selector included in DPC++ (not available in SYCL) is 
 available by including the header  ""CL/sycl/intel/fpga_extensions.hpp"":
  
 INTEL::fpga_selector
  
 select a device that identifies itself as an FpGa.
  
 39",NA
 When Device Selection Fails,"If a gpu_selector is used when creating an object such as a queue and if 
 there are no GPU devices available to the runtime, then the selector throws 
 a runtime_error exception. This is true for all device selector classes in that 
 if no device of the required class is available, then a runtime_error 
 exception is thrown. It is reasonable for complex applications to catch that 
 error and instead acquire a less desirable (for the application/algorithm) 
 device class as an alternative. Exceptions and error handling are discussed 
 in more detail in Chapter 
 5
 .",NA
 Method#4: Using Multiple Devices,"As shown in Figures 
 2-5
  and 
 2-6
 , we can construct multiple queues in an 
 application. We can bind these queues to a single device (the sum of work 
 to the queues is funneled into the single device), to multiple devices, or to 
 some combination of these. Figure 
 2-13
  provides an example that creates 
 one queue bound to a GPU and another queue bound to an FPGA. The 
 corresponding mapping is shown graphically in Figure 
 2-14
 .
  
 43",NA
 Method#5: Custom (Very Specific) ,NA,NA
Device Selection,"We will now look at how to write a custom selector. In addition to 
 examples in this chapter, there are a few more examples shown in Chapter 
 12
 . The built-in device selectors are intended to let us get code up and 
 running quickly. Real applications usually require specialized selection of 
 a device, such as picking a desired GPU from a set of GPU types available in 
 a system. The device selection mechanism is easily extended to arbitrarily 
 complex logic, so we can write whatever code is required to choose the 
 device that we prefer.",NA
device_selector,NA,NA
 Base Class,"All device selectors derive from the abstract device_selector base class 
 and define the function call operator in the derived class:
  
 virtual int
  operator()
 (
 const device &
 dev
 ) 
 const
  { 
  
 ;
  /* User logic */ 
  
 }
  
 Defining this operator in a class that derives from device_selector is 
 all that is required to define any complexity of selection logic, once we 
 know three things:
  
  1. The function call operator is automatically called once 
 for each device that the runtime finds as accessible 
 to the application, including the host device.
  
  2. The operator returns an integer score each time that it 
 is invoked. The highest score across all available 
 devices is the device that the selector chooses.
  
  3. A negative integer returned by the function call 
 operator means that the device being considered 
 must not be chosen.
  
 45",NA
 Mechanisms to Score a Device,"We have many options to create an integer score corresponding to a 
  
 specific device, such as the following:
  
  1. Return a positive value for a specific device class.
  
  2. String match on a device name and/or device 
  
 vendor strings.
  
  3. Anything we can imagine in code leading to an 
  
 integer value, based on device or platform queries.
  
 For example, one possible approach to select an Intel Arria family 
  
 FPGA device is shown in Figure 
 2-15
 .
  
 class
  my_selector
  : 
 public
  device_selector
  { 
  
  
 public: 
  
   
 int
  operator()
 (
 const device &
 dev
 ) 
 const override
  { 
  
   
 if
  ( 
  
      
 dev
 .
 get_info
 <
 info
 ::
 device
 ::name>().
 find
 (
 ""Arria""
 ) 
   
  
   
 != 
 std
 ::
 string
 ::npos && 
  
      
 dev
 .
 get_info
 <
 info
 ::
 device
 ::vendor>().
 find
 (
 ""Intel""
 ) 
  
  
    
 != 
 std
 ::
 string
 ::npos) { 
  
     
 return
  1
 ; 
  
   
 } 
  
   
 return
  -
 1
 ; 
  
   
 } 
  
 };
  
 Figure 2-15. 
 Custom selector for Intel Arria FPGA device
  
 Chapter 
 12
  has more discussion and examples for device selection 
  
 (Figures 
 12-2
  and 
 12-3
 ) and discusses the get_info method in more depth.",NA
 Three Paths to Device Code Execution ,NA,NA
on CPU,"A potential source of confusion comes from the multiple mechanisms 
  
 through which a CPU can have code executed on it, as summarized in 
  
 Figure 
 2-16
 .",NA
 Creating Work on a Device,"Applications usually contain a combination of both host code and device 
 code. There are a few class members that allow us to submit device code 
 for execution, and because these work dispatch constructs are the only 
 way to submit device code, they allow us to easily distinguish device code 
 from host code.
  
 The remainder of this chapter introduces some of the work dispatch 
 constructs, with the goal to help us understand and identify the division 
 between device code and host code that executes natively on the host 
 processor.",NA
 Introducing the Task Graph,"A fundamental concept in the SYCL execution model is a graph of nodes. 
 Each node (unit of work) in this graph contains an action to be performed 
 on a device, with the most common action being a data-parallel device 
 kernel invocation. Figure 
 2-17
  shows an example graph with four nodes, 
 where each node can be thought of as a device kernel invocation.
  
 The nodes in Figure 
 2-17
  have dependence edges defining when it is 
 legal for a node’s work to begin execution. The dependence edges are 
 most commonly generated automatically from data dependences, 
 although there are ways for us to manually add additional custom 
 dependences when we want to. Node B in the graph, for example, has a 
 dependence edge from node A. This edge means that node A must 
 complete execution, and most",NA
 Where Is the Device Code?,"There are multiple mechanisms that can be used to define code that will be 
 executed on a device, but a simple example shows how to identify such 
 code. Even if the pattern in the example appears complex at first glance, 
 the pattern remains the same across all device code definitions so quickly 
 becomes second nature.
  
 The code passed as the final argument to the parallel_for, defined as a 
 lambda in Figure 
 2-18
 , is the device code to be executed on a device. The 
 parallel_for in this case is the construct that lets us distinguish device code 
 from host code. parallel_for is one of a small set of device dispatch 
 mechanisms, all members of the handler class, that define the code to be 
 executed on a device. A simplified definition of the handler class is given in 
 Figure 
 2-19
 .
  
 50",NA
 Actions,"The code in Figure 
 2-18
  contains a parallel_for, which defines work to be 
 performed on a device. The parallel_for is within a command group (CG) 
 submitted to a queue, and the queue defines the device on which the work 
 is to be performed. Within the command group, there are two categories 
 of code:
  
  1. 
 Exactly one call to an action
  that either queues 
 device code for execution or performs a manual 
 memory operation such as copy.
  
  2. 
 Host code
  that sets up dependences defining when it is 
 safe for the runtime to start execution of the work 
 defined in (1), such as creation of accessors to 
 buffers (described in Chapter 
 3
 ).
  
 The handler class contains a small set of member functions that 
 define the action to be performed when a task graph node is executed. 
 Figure 
 2- 21 
 summarizes these actions.
  
 53",NA
"a command group must have exactly one action within it, ",NA,NA
such as a kernel launch or explicit memory operation.,"The idea that code is executed asynchronously in the future is the 
 critical difference between code that runs on the CPU as part of the host 
 program and device code that will run in the future when dependences 
  
 54",NA
 Fallback,"Usually a command group is executed on the command queue to which we 
 have submitted it. However, there may be cases where the command 
 group fails to be submitted to a queue (e.g., when the requested size of 
 work is too large for the device’s limits) or when a successfully submitted 
 operation is unable to begin execution (e.g., when a hardware device has 
 failed). To handle such cases, it is possible to specify a fallback queue for 
 the command group to be executed on. The authors don’t recommend this 
 error management technique because it offers little control, and instead 
 we recommend catching and managing the initial error as is described in 
 Chapter 
 5
 . We briefly cover the fallback queue here because some people 
 prefer the style and it is a well-known part of SYCL.",NA
the fallback queue is enabled by passing a secondary queue ,NA,NA
to a submit call.  the authors recommend catching the ,NA,NA
"initial error and handling it, as described in Chapter ",NA,NA
5,NA,NA
", ",NA,NA
instead of using the fallback queue mechanism which ,NA,NA
offers less control.,NA,NA
 Summary,"In this chapter we provided an overview of queues, selection of the 
 device with which a queue will be associated, and how to create custom 
 device selectors. We also overviewed the code that executes on a device 
 asynchronously when dependences are met vs. the code that executes as 
 part of the C++ application host code. Chapter 
 3
  describes how to control 
 data movement.
  
 58",NA
CHAPTER 3,NA,NA
Data Management,"Supercomputer architects often lament that we need to “feed the beast.” 
 The phrase “feed the beast” refers to the “beast” of a computer we create 
 when we use lots of parallelism, and feeding data to it becomes a key 
 challenge to solve.
  
 Feeding a Data Parallel C++ program on a heterogeneous machine 
 requires some care to ensure data is where it needs to be when it needs 
 to be there. In a large program, that can be a lot of work. In a preexisting 
 C++ program, it can be a nightmare just to sort out how to manage all the 
 data movements needed.
  
 We will carefully explain the two ways to manage data: Unified Shared 
 Memory (USM) and buffers. USM is pointer based, which is familiar to C++ 
 programmers. Buffers offer a higher-level abstraction. Choice is good.
  
  
 We need to control the movement of data, and this chapter covers 
 options to do exactly that.
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_3
  
 61",NA
 Introduction,"Compute is nothing without data. The whole point of accelerating a 
 computation is to produce an answer more quickly. This means that one of 
 the most important aspects of data-parallel computations is how they 
 access data, and introducing accelerator devices into a machine further 
 complicates the picture. In traditional single-socket CPU-based systems, 
 we have a single memory. Accelerator devices often have their own 
 attached memories that cannot be directly accessed from the host. 
 Consequently, parallel programming models that support discrete devices 
 must provide mechanisms to manage these multiple memories and move 
 data between them.
  
 62",NA
 The Data Management Problem,"Historically, one of the advantages of shared memory models for parallel 
 programming is that they provide a single, shared view of memory. Having 
 this single view of memory simplifies life. We are not required to do 
 anything special to access memory from parallel tasks (aside from proper 
 synchronization to avoid data races). While some types of accelerator 
 devices (e.g., integrated GPUs) share memory with a host CPU, many 
 discrete accelerators have their own local memories separate from that of 
 the CPU as seen in Figure 
 3-1
 .
  
  
 Figure 3-1. 
 Multiple discrete memories",NA
 Device Local vs. Device Remote,"Programs running on a device perform better when reading and writing 
 data using memory attached directly to the device rather than remote 
 memories. We refer to accesses to a directly attached memory as 
 local 
 accesses. Accesses to another device’s memory are 
 remote
  accesses. 
 Remote accesses tend to be slower than local accesses because they must 
  
 63",NA
 Managing Multiple Memories,"Managing multiple memories can be accomplished, broadly, in two ways: 
 explicitly
  through our program or 
 implicitly
  by the runtime. Each method 
 has its advantages and drawbacks, and we may choose one or the other 
 depending on circumstances or personal preference.",NA
 Explicit Data Movement,"One option for managing multiple memories is to explicitly copy data 
 between different memories. Figure 
 3-2
  shows a system with a discrete 
 accelerator where we must first copy any data that a kernel will require 
 from the host memory to GPU memory. After the kernel computes results, 
 we must copy these results back to the CPU before the host program can 
 use that data.
  
 64",NA
 Implicit Data Movement,"The alternative to program-controlled explicit data movements are 
 implicit data movements controlled by a parallel runtime or driver. In this 
 case, instead of requiring explicit copies between different memories, the 
 parallel runtime is responsible for ensuring that data is transferred to the 
 appropriate memory before it is used.
  
 The advantage of implicit data movement is that it requires less effort 
 to get an application to take advantage of faster memory attached directly 
 to the device. All the heavy lifting is done automatically by the runtime. 
  
 This also reduces the opportunity to introduce errors into the program 
 since the runtime will automatically identify both when data transfers 
 must be performed and how much data must be transferred.
  
 The drawback of implicit data movement is that we have less or no 
 control over the behavior of the runtime’s implicit mechanisms. The 
 runtime will provide functional correctness but may not move data in an 
 optimal fashion that ensures maximal overlap of computation with data 
 transfer, and this could have a negative impact on program performance.
  
 65",NA
 Selecting the Right Strategy,"Picking the best strategy for a program can depend on many different 
 factors. Different strategies might be appropriate for different phases of 
 program development. We could even decide that the best solution is to 
 mix and match the explicit and implicit methods for different pieces of 
 the program. We might choose to begin using implicit data movement to 
 simplify porting an application to a new device. As we begin tuning the 
 application for performance, we might start replacing implicit data 
 movement with explicit in performance-critical parts of the code. 
  
 Future chapters will cover how data transfers can be overlapped 
 with computation in order to optimize performance.",NA
" USM, Buffers, and Images","There are three abstractions for managing memory: Unified Shared 
  
 Memory (USM), buffers, and images. USM is a pointer-based approach that 
 should be familiar to C/C++ programmers. One advantage of USM is easier 
 integration with existing C++ code that operates on pointers. Buffers, as 
 represented by the buffer template class, describe one-, two-, or three- 
 dimensional arrays. They provide an abstract view of memory that can be 
 accessed on either the host or a device. Buffers are not directly accessed by 
 the program and are instead used through accessor objects. Images act as a 
 special type of buffer that provides extra functionality specific to image 
 processing. This functionality includes support for special image formats, 
 reading of images using sampler objects, and more. Buffers and images are 
 powerful abstractions that solve many problems, but rewriting all 
 interfaces in existing code to accept buffers or accessors can be time- 
 consuming. Since the interface for buffers and images is largely the same, 
 the rest of this chapter will only focus on USM and buffers.
  
 66",NA
 Unified Shared Memory,"USM is one tool available to us for data management. USM is a pointer-
 based approach that should be familiar to C and C++ programmers who 
 use malloc or new to allocate data. USM simplifies life when porting 
 existing C/C++ code that makes heavy use of pointers. Devices that 
 support USM support a unified virtual address space. Having a unified 
 virtual address space means that any pointer value returned by a USM 
 allocation routine on the host will be a valid pointer value on the device. 
 We do not have to manually translate a host pointer to obtain the “device 
 version”—we see the same pointer value on both the host and device.
  
 A more detailed description of USM can be found in Chapter 
 6
 .",NA
 Accessing Memory Through Pointers,"Since not all memories are created equal when a system contains both 
 host memory and some number of device-attached local memories, USM 
 defines three different types of allocations: device, host, and shared. All 
 types of allocations are performed on the host. Figure 
 3-3
  summarizes the 
 characteristics of each allocation type.
  
  
 Figure 3-3. 
 USM allocation types
  
 67",NA
 USM and Data Movement,"USM supports both explicit and implicit data movement strategies, and 
 different allocation types map to different strategies. Device allocations 
 require us to explicitly move data between host and device, while host and 
 shared allocations provide implicit data movement.",NA
 Explicit Data Movement in USM,"Explicit data movement with USM is accomplished with device allocations 
 and a special memcpy() found in the queue and handler classes. We 
  
 enqueue memcpy() operations (actions) to transfer data either from the 
 host to the device or from the device to the host.
  
 68",NA
 Implicit Data Movement in USM,"Implicit data movement with USM is accomplished with host and shared
  
 allocations. With these types of allocations, we do not need to explicitly 
  
 insert copy operations to move data between host and device. Instead, 
  
 we simply access the pointers inside a kernel, and any required data 
  
 movement is performed automatically without programmer intervention 
  
 (as long as your device supports these allocations). This greatly 
 simplifies 
  
 porting of existing codes: simply replace any malloc or new with the 
  
 appropriate USM allocation functions (as well as the calls to free to 
  
 deallocate memory), and everything should just work.
  
 #include
  <CL/sycl.hpp> 
  
 using
  namespace
  sycl
 ; 
  
 constexpr int
  N = 
 42
 ;
  
 int
  main
 () { 
  
 queue
  Q; 
  
 int
  *host_array = 
 malloc_host
 <
 int
 >(N, Q); 
 int
  *shared_array = 
 malloc_shared
 <
 int
 >(N, Q);
  
 for
  (
 int
  i = 
 0
 ; i < N; i++) { 
  
  
 // Initialize hostArray on host 
  
  
 host_array
 [i] = i; 
  
 }
  
 // We will learn how to simplify this example later 
 Q
 .
 submit
 ([&](
 handler 
 &
 h
 ) { 
  
  
 h
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) { 
  
     
 // access sharedArray and hostArray on device 
   
   
 shared_array
 [i] = 
 host_array
 [i] + 
 1
 ; 
  
    
 }); 
  
  
 }); 
  
 Q
 .
 wait
 ();
  
 for
  (
 int
  i = 
 0
 ; i < N; i++) { 
  
  
 // access sharedArray on host 
  
  
 host_array
 [i] = 
 shared_array
 [i]; 
  
 }
  
 }
  
 free
 (shared_array, Q); 
  
 free
 (host_array, Q); 
  
 return
  0
 ;
  
 Figure 3-5. 
 USM implicit data movement
  
 70",NA
 Buffers,"The other abstraction provided for data management is the buffer object. 
 Buffers are a data abstraction that represent one or more objects of a given 
 C++ type. Elements of a buffer object can be a scalar data type (such as an 
 int, float, or double), a vector data type (Chapter 
 11
 ), or a user-defined 
 class or structure. Data structures in buffers must be C++ 
 trivially copyable
 , 
 which means that an object can be safely copied byte by byte where copy 
 constructors do not need to be invoked.
  
 While a buffer itself is a single object, the C++ type encapsulated by the 
 buffer could be an array that contains multiple objects. Buffers represent 
 data objects rather than specific memory addresses, so they cannot be 
 directly accessed like regular C++ arrays. Indeed, a buffer object might 
 map to multiple different memory locations on several different devices, or 
 even on the same device, for performance reasons. Instead, we use 
 accessor
  objects to read and write to buffers.
  
 A more detailed description of buffers can be found in Chapter 
 7
 .
  
 71",NA
 Creating Buffers,"Buffers can be created in a variety of ways. The simplest method is to 
 simply construct a new buffer with a range that specifies the size of the 
 buffer. However, creating a buffer in this fashion does not initialize its data, 
 meaning that we must first initialize the buffer through other means before 
 attempting to read useful data from it.
  
 Buffers can also be created from existing data on the host. This is done 
 by invoking one of the several constructors that take either a pointer to an 
 existing host allocation, a set of InputIterators, or a container that has 
 certain properties. Data is copied during buffer construction from the 
 existing host allocation into the buffer object’s host memory. A buffer may 
 also be created from an existing cl_mem object if we are using the SYCL 
 interoperability features with OpenCL.",NA
 Accessing Buffers,"Buffers may not be directly accessed by the host and device (except 
  
 through advanced and infrequently used mechanisms not described here). 
  
 Instead, we must create accessors in order to read and write to buffers. 
 Accessors provide the runtime with information about how we plan to use 
 the data in buffers, allowing it to correctly schedule data movement.
  
 72",NA
 Access Modes,"When creating an accessor, we can inform the runtime how we are going to 
 use it to provide more information for optimizations. We do this by 
 specifying an 
 access mode
 . Access modes are defined in the access::mode 
 enum 
  
 described in Figure 
 3-7
 . In the code example shown in Figure 
 3- 6
 , the 
 accessor myAccessor is created with the default access mode, 
 access::mode::read_ write. This lets the runtime know that we intend to 
 both read and write to the buffer through myAccessor. Access modes are 
 how the runtime is able to optimize implicit data movement. For example, 
 access::mode::read tells the runtime that the data needs to be available on 
 the device before this kernel can begin executing. If a kernel only reads 
 data through an accessor, there is no need to copy data back to the host 
 after the kernel has completed as we haven’t modified it. Likewise, 
 access::mode::write lets the runtime know that we will modify the 
 contents of a buffer and may need to copy the results back after 
 computation has ended.
  
 Creating accessors with the proper modes gives the runtime more 
 information about how we use data in our program. The runtime uses 
 accessors to order the uses of data, but it can also use this data to optimize 
 scheduling of kernels and data movement. The access modes and 
  
 optimization tags are described in greater detail in Chapter 
 7
 .",NA
 Ordering the Uses of Data,"Kernels can be viewed as asynchronous tasks that are submitted for 
 execution. These tasks must be submitted to a queue where they are 
 scheduled for execution on a device. In many cases, kernels must execute 
 in a specific order so that the correct result is computed. If obtaining the 
 correct result requires task A to execute before task B, we say that a 
 dependence
 1
  exists between tasks A and B.
  
 However, kernels are not the only form of task that must be scheduled. 
  
 Any data that is accessed by a kernel needs to be available on the device 
 before the kernel can start executing. These data dependences can create 
 additional tasks in the form of data transfers from one device to another. 
 Data transfer tasks may be either explicitly coded copy operations or more 
 commonly implicit data movements performed by the runtime.
  
 If we take all the tasks in a program and the dependences that exist 
 between them, we can use this to visualize the information as a graph. This 
 task graph is specifically a directed acyclic graph (DAG) where the nodes 
 are the tasks and the edges are the dependences. The graph is 
 directed 
 because dependences are one-way: task A must happen before task B. The 
 graph is 
 acyclic
  because it does not contain any cycles or paths from a node 
 that lead back to itself.
  
 In Figure 
 3-8
 , task A must execute before tasks B and C. Likewise, 
 B and C must execute before task D. Since B and C do not have a 
 dependence between each other, the runtime is free to execute them 
  
 1
  Note that you may see “dependence” and “dependences” sometimes spelled 
 “dependency” and “dependencies” in other texts. They mean the same thing, 
 but we are favoring the spelling used in several important papers on data flow 
 analysis. See 
 https://dl.acm.org/doi/pdf/10.1145/75277.75280
  and 
 https://dl.acm.org/doi/pdf/10.1145/113446.113449
 .
  
 75",NA
 In-order Queues,"The simplest option to order tasks is to submit them to an in-order 
 queue object. An in-order queue executes tasks in the order in which 
 they were submitted as seen in Figure 
 3-10
 . While the intuitive task 
 ordering of in-order queues provides an advantage in simplicity, it 
 provides a disadvantage in that the execution of tasks will serialize even 
 if no dependences exist between independent tasks. In-order queues are 
 useful when bringing up applications because they are simple, intuitive, 
 deterministic on execution ordering, and suitable for many codes.
  
 #include
  <CL/sycl.hpp> 
  
 using
  namespace
  sycl
 ; 
  
 constexpr int
  N = 
 4
 ; 
  
 int
  main
 () { 
  
  
 queue
  Q{
 property
 ::
 queue
 ::
 in_order
 ()};
  
 }
  
 // Task A 
  
 Q
 .
 submit
 ([&](
 handler&
  h
 ) { 
  
  
 h
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) {
  /*...*/
  }); });
  
 A
  
 // Task B 
  
 Q
 .
 submit
 ([&](
 handler&
  h
 ) { 
  
  
 h
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) {
  /*...*/
  }); });
  
 B
  
 // Task C 
  
 Q
 .
 submit
 ([&](
 handler&
  h
 ) { 
  
  
 h
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) {
  /*...*/
  }); });
  
 C
  
 return
  0
 ;
  
 Figure 3-10. 
 In-order queue usage
  
 77",NA
 Out-of-Order (OoO) Queues,"Since queue objects are out-of-order queues (unless created with the in- 
 order queue property), they must provide ways to order tasks submitted 
 to them. Queues order tasks by letting us inform the runtime of 
 dependences between them. These dependences can be specified, either 
 explicitly or implicitly, using 
 command groups
 .
  
 A command group is an object that specifies a task and its 
  
 dependences. Command groups are typically written as C++ lambdas 
 passed as an argument to the submit() method of a queue object. This 
 lambda’s only parameter is a reference to a handler object. The handler 
 object is used inside the command group to specify actions, create 
 accessors, and specify dependences.",NA
 Explicit Dependences with Events,"Explicit dependences between tasks look like the examples we have seen 
 (Figure 
 3-8
 ) where task A must execute before task B. Expressing 
 dependences in this way focuses on explicit ordering based on the 
  
 computations that occur rather than on the data accessed by the 
  
 computations. Note that expressing dependences between computations is 
 primarily relevant for codes that use USM since codes that use buffers 
 express most dependences via accessors. In Figures 
 3-4
  and 
 3-5
 , we simply 
 tell the queue to wait for all previously submitted tasks to finish before we 
 continue. Instead, we can express task dependences through 
 event
  objects. 
  
 When submitting a command group to a queue, the submit() method 
 returns an event object. These events can then be used in two ways.
  
 First, we can synchronize through the host by explicitly calling the 
 wait() method on an event. This forces the runtime to wait for the 
 task that generated the event to finish executing before host program 
 execution may continue. Explicitly waiting on events can be very 
 useful for debugging an application, but wait() can overly constrain 
 the asynchronous execution of tasks since it halts all execution on the 
  
 78",NA
 Implicit Dependences with Accessors,"Implicit dependences between tasks are created from data dependences. 
 Data dependences between tasks take three forms, shown in Figure 
 3-12
 .
  
 Read-after-Write 
  
 (RAW) 
  
 Write-after-Read 
  
 (WAR) 
  
 Write-after-
  
 Write(WAW)
  
 Figure 3-12. 
 Three forms of data dependences
  
 Data dependences are expressed to the runtime in two ways: 
 accessors and program order. Both must be used for the runtime to 
 properly compute data dependences. This is illustrated in Figures 
 3-13 
 and 
 3-14
 .
  
 80",NA
 Choosing a Data Management ,NA,NA
Strategy,"Selecting the right data management strategy for our applications is 
 largely a matter of personal preference. Indeed, we may begin with one 
 strategy and switch to another as our program matures. However, there 
 are a few useful guidelines to help us to pick a strategy that will serve our 
 needs.
  
 The first decision to make is whether we want to use explicit or 
 implicit data movement since this greatly affects what we need to do to 
 our program. Implicit data movement is generally an easier place to start 
 because all the data movement is handled for us, letting us focus on 
 expression of the computation.
  
 If we decide that we’d rather have full control over all data movement 
 from the beginning, then explicit data movement using USM device 
 allocations is where we want to start. We just need to be sure to add all the 
 necessary copies between host and devices!
  
 When selecting an implicit data movement strategy, we still have a 
 choice of whether to use buffers or USM host or shared pointers. Again, 
 this choice is a matter of personal preference, but there are a few 
 questions that could help guide us to one over the other. If we’re porting 
 an existing C/C++ program that uses pointers, USM might be an easier path 
 since most code won’t need to change. If data representation hasn’t guided 
 us to a preference, another question we can ask is how we would like to 
 express our dependences between kernels. If we prefer to think about data 
 dependences between kernels, choose buffers. If we prefer to think about 
 dependences as performing one computation before another and want to 
 express that using an in-order queue or with explicit events or waiting 
 between kernels, choose USM.",NA
 Handler Class: Key Members,"We have shown a number of ways to use the handler class. Figures 
 3-17 
 and 
 3-18
  provide a more detailed explanation of the key members of this 
 very important class. We have not yet used all these members, but they 
 will be used later in the book. This is as good a place as any to lay them out.
  
 A closely related class, the queue class, is similarly explained at the end 
 of Chapter 
 2
 . The online oneAPI DPC++ language reference provides an 
 even more detailed explanation of both classes.
  
 87",NA
 Summary,"In this chapter, we have introduced the mechanisms that address the 
 problems of data management and how to order the uses of data. 
  
 Managing access to different memories is a key challenge when using 
 accelerator devices, and we have different options to suit our needs.
  
 We provided an overview of the different types of dependences that 
 can exist between the uses of data, and we described how to provide 
 information about these dependences to queues so that they properly 
 order tasks.
  
 This chapter provided an overview of Unified Shared Memory and 
 buffers. We will explore all the modes and behaviors of USM in greater 
 detail in Chapter 
 6
 . Chapter 
 7
  will explore buffers more deeply, including 
 all the different ways to create buffers and control their behavior. Chapter 
 8
  will revisit the scheduling mechanisms for queues that control the 
 ordering of kernel executions and data movements.
  
  
 Open Access
  This chapter is licensed under the terms 
 of the Creative Commons Attribution 4.0 International 
  
 License (
 http://creativecommons.org/licenses/by/4.0/
 ), which permits 
 use, sharing, adaptation, distribution and reproduction in any medium or 
 format, as long as you give appropriate credit to the original author(s) and 
 the source, provide a link to the Creative Commons license and indicate if 
 changes were made.
  
 The images or other third party material in this chapter are included 
 in the chapter’s Creative Commons license, unless indicated otherwise in 
 a credit line to the material. If material is not included in the chapter’s 
 Creative Commons license and your intended use is not permitted by 
 statutory regulation or exceeds the permitted use, you will need to obtain 
 permission directly from the copyright holder.
  
 90",NA
CHAPTER 4,NA,NA
Expressing ,NA,NA
Parallelism,"USM
  
  queue
  
 host
  
  devices
  
 Now we can put together our first collection of puzzle pieces. We already 
 know how to place code (Chapter 
 2
 ) and data (Chapter 
 3
 ) on a device—
 all we must do now is engage in the art of deciding what to do with it. To 
 that end, we now shift to fill in a few things that we have conveniently 
 left out or glossed over so far. This chapter marks the transition from 
 simple teaching examples toward real-world parallel code and expands 
 upon details of the code samples we have casually shown in prior 
 chapters.",NA
 Parallelism Within Kernels,"Parallel kernels have emerged in recent years as a powerful means 
  
 of expressing data parallelism. The primary design goals of a kernel- based 
 approach are 
 portability
  across a wide range of devices and high 
 programmer 
 productivity
 . As such, kernels are typically not hard-coded to 
 work with a specific number or configuration of hardware resources (e.g., 
 cores, hardware threads, SIMD [Single Instruction, Multiple Data] 
 instructions). Instead, kernels describe parallelism in terms of abstract 
 concepts that an implementation (i.e., the combination of compiler and 
 runtime) can then map to the hardware parallelism available on a specific 
 target device. Although this mapping is implementation-defined, we can 
 (and should) trust implementations to select a mapping that is sensible 
 and capable of effectively exploiting hardware parallelism.
  
 92",NA
guaranteeing functional portability is not the same as ,NA,NA
guaranteeing high performance!,"There is a significant amount of diversity in the devices supported, 
 and we must remember that different architectures are designed and 
 optimized for different use cases. Whenever we hope to achieve the 
 highest levels of 
 performance
  on a specific device, we should always 
 expect that some additional manual optimization work will be 
 required—regardless of the programming language we’re using! 
 Examples of such device-specific optimizations include blocking for a 
 particular cache size, choosing a grain size that amortizes scheduling 
 overheads, making use of specialized instructions or hardware units, 
 and, most importantly, choosing an appropriate algorithm. Some of these 
 examples will be revisited in Chapters 
 15
 , 
 16
 , and 
 17
 .
  
 Striking the right balance between performance, portability, and 
 productivity during application development is a challenge that we must 
 all face—and a challenge that this book cannot address in its entirety. 
 However, we hope to show that DPC++ provides all the tools required to 
 maintain both generic portable code and optimized target-specific code 
 using a single high-level programming language. The rest is left as an 
 exercise to the reader!",NA
 Multidimensional Kernels,"The parallel constructs of many other languages are one-dimensional, 
 mapping work directly to a corresponding one-dimensional hardware 
 resource (e.g., number of hardware threads). Parallel kernels are 
  
 93",NA
 Loops vs. Kernels,"An iterative loop is an inherently serial construct: each iteration of the 
 loop is executed sequentially (i.e., in order). An optimizing compiler may 
 be able to determine that some or all iterations of a loop can execute in 
 parallel, but it must be conservative—if the compiler isn’t smart enough or 
 doesn’t have enough information to prove that parallel execution is always 
 safe, it must preserve the loop’s sequential semantics for correctness.
  
 for
  (
 int
  i = 
 0
 ; i < N; ++i) { 
  
  
 c
 [i] = 
 a
 [i] + 
 b
 [i]; 
  
 }
  
 Figure 4-2. 
 Expressing a vector addition as a serial loop
  
 Consider the loop in Figure 
 4-2
 , which describes a simple vector 
 addition. Even in a simple case like this, proving that the loop can be 
 executed in parallel is not trivial: parallel execution is only safe if c does 
 not overlap a or b, which in the general case cannot be proven without 
 a runtime check! In order to address situations like this, languages have 
 added features enabling us to provide compilers with extra information 
 that may simplify analysis (e.g., asserting that pointers do not overlap 
 with restrict) or to override all analysis altogether (e.g., declaring that 
 all iterations of a loop are independent or defining exactly how the loop 
 should be scheduled to parallel resources).
  
 The exact meaning of a 
 parallel loop
  is somewhat ambiguous—due to 
 overloading of the term by different parallel programming languages—
 but many common parallel loop constructs represent compiler 
  
 transformations applied to sequential loops. Such programming models 
  
 95",NA
the sooner that we can shift our thinking from parallel loops ,NA,NA
"to kernels, the easier it will be to write effective parallel ",NA,NA
programs using Data parallel C++.,96,NA
 Overview of Language Features,"Once we’ve decided to write a parallel kernel, we must decide what type of 
 kernel we want to launch and how to represent it in our program. There 
 are a multitude of ways to express parallel kernels, and we need to 
 familiarize ourselves with each of these options if we want to master the 
 language.",NA
 Separating Kernels from Host Code,"We have several alternative ways to separate host and device code, which 
 we can mix and match within an application: C++ lambda expressions or 
 function objects (functors), OpenCL C source strings, or binaries. Some of 
 these options were already covered in Chapter 
 2
 , and all of them will be 
 covered in more detail in Chapter 
 10
 .
  
 The fundamental concepts of expressing parallelism are shared by all 
 these options. For consistency and brevity, all the code examples in this 
 chapter express kernels using C++ lambdas.
  
 LAMBDAS NOT CONSIDERED HARMFUL
  
 there is no need to fully understand everything that the C++ 
 specification says about lambdas in order to get started with 
 DpC++—all we need to know is that the body of the lambda 
 represents the kernel and that variables captured (by value) will be 
 passed to the kernel as arguments.
  
 there is no performance impact arising from the use of lambdas 
 instead of more verbose mechanisms for defining kernels. a DpC++ 
 compiler always understands when a lambda represents the body 
 of a parallel kernel and can optimize for parallel execution 
 accordingly.
  
 For a refresher on C++ lambda functions, with notes about their use 
 in sYCl, see Chapter 
 1
 . For more specific details on using lambdas to 
 define kernels, see Chapter 
 10
 .",NA
 Different Forms of Parallel Kernels,"There are three different kernel forms, supporting different execution 
 models and syntax. It is possible to write portable kernels using any of the 
 kernel forms, and kernels written in any form can be tuned to achieve high 
 performance on a wide variety of device types. However, there will be 
 times when we may want to use a specific form to make a specific parallel 
 algorithm easier to express or to make use of an otherwise inaccessible 
 language feature.
  
 The first form is used for 
 basic
  data-parallel kernels and offers the 
 gentlest introduction to writing kernels. With basic kernels, we sacrifice 
 control over low-level features like scheduling in order to make the 
 expression of the kernel as simple as possible. How the individual kernel 
 instances are mapped to hardware resources is controlled entirely by the 
 implementation, and so as basic kernels grow in complexity, it becomes 
 harder and harder to reason about their performance.
  
 The second form extends basic kernels to provide access to low-level 
 performance-tuning features. This second form is known as 
 ND-range 
 (N-
 dimensional range) data parallel for historical reasons, and the most 
 important thing to remember is that it enables certain kernel instances to 
 be grouped together, allowing us to exert some control over data locality 
 and the mapping between individual kernel instances and the hardware 
 resources that will be used to execute them.
  
 The third form provides an alternative syntax to simplify the 
 expression of ND-range kernels using nested kernel constructs. This third 
 form is referred to as 
 hierarchical
  data parallel, referring to the hierarchy 
 of the nested kernel constructs that appear in user source code.
  
 We will revisit how to choose between the different kernel forms again 
 at the end of this chapter, once we’ve discussed their features in more 
 detail.
  
 98",NA
 Basic Data-Parallel Kernels,"The most basic form of parallel kernel is appropriate for operations that 
 are 
 embarrassingly parallel
  (i.e., operations that can be applied to every 
 piece of data completely independently and in any order). By using this 
 form, we give an implementation complete control over the scheduling of 
 work. It is thus an example of a 
 descriptive
  programming construct—we 
 describe
  that the operation is embarrassingly parallel, and all scheduling 
 decisions are made by the implementation.
  
 Basic data-parallel kernels are written in a Single Program, Multiple 
 Data (SPMD) style—a single “program” (the kernel) is applied to multiple 
 pieces of data. Note that this programming model still permits each 
 instance of the kernel to take different paths through the code, as a result 
 of data-dependent branches.
  
 One of the greatest strengths of a SPMD programming model is that it 
 allows the same “program” to be mapped to multiple levels and types of 
 parallelism, without any explicit direction from us. Instances of the same 
 program could be pipelined, packed together and executed with SIMD 
 instructions, distributed across multiple threads, or a mix of all three.",NA
 Understanding Basic Data-Parallel ,NA,NA
Kernels,"The execution space of a basic parallel kernel is referred to as its execution 
 range
 , and each instance of the kernel is referred to as an 
 item
 . This is 
 represented diagrammatically in Figure 
 4-4
 .",NA
 Writing Basic Data-Parallel Kernels,"Basic data-parallel kernels are expressed using the parallel_for function. 
  
 Figure 
 4-5
  shows how to use this function to express a vector addition, 
 which is our take on “Hello, world!” for parallel accelerator programming.
  
 100",NA
 Details of Basic Data-Parallel Kernels,"The functionality of basic data-parallel kernels is exposed via three C++ 
 classes: range, id, and item. We’ve already seen the range and id classes a 
 few times in previous chapters, but we revisit them here with a different 
 focus.",NA
 The range Class,"A range represents a one-, two-, or three-dimensional range. The 
  
 dimensionality of a range is a template argument and must therefore be 
 known at compile time, but its size in each dimension is dynamic and is 
 passed to the constructor at runtime. Instances of the range class are used 
 to describe both the execution ranges of parallel constructs and the sizes of 
 buffers.
  
  
 A simplified definition of the range class, showing the constructors and 
 various methods for querying its extent, is shown in Figure 
 4-9
 .
  
 103",NA
 The id Class,"An id represents an index into a one, two-, or three-dimensional range. 
 The definition of id is similar in many respects to range: its dimensionality 
 must also be known at compile time, and it may be used to index an 
 individual instance of a kernel in a parallel construct or an offset into a 
 buffer.
  
 As shown by the simplified definition of the id class in Figure 
 4-10
 , an 
 id is conceptually nothing more than a container of one, two, or three 
 integers. The operations available to us are also very simple: we can query 
 the component of an index in each dimension, and we can perform simple 
 arithmetic to compute new indices.
  
 Although we can construct an id to represent an arbitrary index, to 
 obtain the id associated with a specific kernel instance, we must accept it 
 (or an item containing it) as an argument to a kernel function. This id (or 
 values returned by its member functions) must be forwarded to any 
 function in which we want to query the index—there are not currently any 
 free functions for querying the index at arbitrary points in a program, but 
 this may be addressed by a future version of DPC++.
  
 104",NA
 The item Class,"An item represents an individual instance of a kernel function, 
  
 encapsulating both the execution range of the kernel and the instance’s 
 index within that range (using a range and an id, respectively). Like range 
 and id, its dimensionality must be known at compile time.
  
 A simplified definition of the item class is given in Figure 
 4-11
 . The 
 main difference between item and id is that item exposes additional 
 functions to query properties of the execution range (e.g., size, offset) and 
 a convenience function to compute a linearized index. As with id, the only 
 way to obtain the item associated with a specific kernel instance is to 
 accept it as an argument to a kernel function.
  
 105",NA
 Explicit ND-Range Kernels,"The second form of parallel kernel replaces the flat execution range of 
 basic data-parallel kernels with an execution range where items belong to 
 groups and is appropriate for cases where we would like to express some 
 notion of locality within our kernels. Different behaviors are defined and 
 guaranteed for different types of groups, giving us more insight into 
 and/or control over how work is mapped to specific hardware platforms.
  
 These explicit ND-range kernels are thus an example of a more 
 prescriptive
  parallel construct—we 
 prescribe
  a mapping of work to each 
 type of group, and the implementation must obey that mapping. However, 
 it is not completely prescriptive, as the groups themselves may execute in 
 any order and an implementation retains some freedom over how each 
 type of group is mapped to hardware resources. This combination of 
 prescriptive and descriptive programming enables us to design and tune 
 our kernels for locality without impacting their portability.
  
 Like basic data-parallel kernels, ND-range kernels are written in a 
 SPMD style where all work-items execute the same kernel ""program"" 
 applied to multiple pieces of data. The key difference is that each program 
  
 106",NA
 Understanding Explicit ND-Range Parallel ,NA,NA
Kernels,"The execution range of an ND-range kernel is divided into work-groups, 
 sub-groups, and work-items. The ND-range represents the total execution 
 range, which is divided into work-groups of uniform size (i.e., the work- 
 group size must divide the ND-range size exactly in each dimension). Each 
 work-group can be further divided by the implementation into sub-groups. 
 Understanding the execution model defined for work-items and each type 
 of group is an important part of writing correct and portable programs.
  
 Figure 
 4-12
  shows an example of an ND-range of size (8, 8, 8) divided 
 into 8 work-groups of size (4, 4, 4). Each work-group contains 16 one- 
 dimensional sub-groups of 4 work-items. Pay careful attention to the 
 numbering of the dimensions: sub-groups are always one-dimensional, 
 and so dimension 2 of the ND-range and work-group becomes dimension 0 
 of the sub-group.
  
  
  
  
  
  
  
  
  
 Figure 4-12. 
 Three-dimensional ND-range divided into work-groups, 
 sub-groups, and work-items",NA
 Work-Items,"Work-items represent the individual instances of a kernel function. In the 
 absence of other groupings, work-items can be executed in any order and 
 cannot communicate or synchronize with each other except by way of 
 atomic memory operations to global memory (see Chapter 
 19
 ).",NA
 Work-Groups,"The work-items in an ND-range are organized into work-groups. Work- 
 groups can execute in any order, and work-items in different work-groups 
 cannot communicate with each other except by way of atomic memory 
 operations to global memory (see Chapter 
 19
 ). However, the work-items 
 within a work-group have concurrent scheduling guarantees when certain 
 constructs are used, and this locality provides some additional 
 capabilities:
  
  1. Work-items in a work-group have access to 
 work-
 group local memory
 , which may be mapped to a 
 dedicated fast memory on some devices (see 
 Chapter 
 9
 ).",NA
 Sub-Groups,"On many modern hardware platforms, subsets of the work-items in a 
 work-group known as 
 sub-groups
  are executed with additional scheduling 
 guarantees. For example, the work-items in a sub-group could be executed 
 simultaneously as a result of compiler vectorization, and/or the sub- 
 groups themselves could be executed with forward progress guarantees 
 because they are mapped to independent hardware threads.
  
 When working with a single platform, it is tempting to bake 
  
 assumptions about these execution models into our codes, but this makes 
 them inherently unsafe and non-portable—they may break when moving 
 between different compilers or even when moving between different 
 generations of hardware from the same vendor!
  
 110",NA
 Writing Explicit ND-Range Data-Parallel ,NA,NA
Kernels,"Figure 
 4-13
  re-implements the matrix multiplication kernel that we saw 
 previously using the ND-range parallel kernel syntax, and the diagram in 
 Figure 
 4-14
  shows how the work in this kernel is mapped to the work-
 items in each work-group. Grouping our work-items in this way ensures 
 locality of access and hopefully improves cache hit rates: for example, the 
 work- group in Figure 
 4-14
  has a local range of (4, 4) and contains 16 
 work-items, but only accesses four times as much data as a single work-
 item—in other words, each value we load from memory can be reused four 
 times.",NA
 Details of Explicit ND-Range Data-,NA,NA
Parallel Kernels,"ND-range data-parallel kernels use different classes compared to basic 
 data-parallel kernels: range is replaced by nd_range, and item is replaced 
 by nd_item. There are also two new classes, representing the different 
 types of groups to which a work-item may belong: functionality tied to 
 work-groups is encapsulated in the group class, and functionality tied to 
 sub-groups is encapsulated in the sub_group class.
  
 113",NA
 The nd_range Class,"An nd_range represents a grouped execution range using two instances 
 of the range class: one denoting the global execution range and another 
 denoting the local execution range of each work-group. A simplified 
 definition of the nd_range class is given in Figure 
 4-15
 .
  
 It may be a little surprising that the nd_range class does not mention 
 sub-groups at all: the sub-group range is not specified during construction 
 and cannot be queried. There are two reasons for this omission. First, sub-
 groups are a low-level implementation detail that can be ignored for many 
 kernels. Second, there are several devices supporting exactly one valid 
 sub-group size, and specifying this size everywhere would be 
 unnecessarily verbose. All functionality related to sub-groups is 
  
 encapsulated in a dedicated class that will be discussed shortly.
  
 template
  <
 int 
 Dimensions
  = 
 1
 > 
  
 class
  nd_range
  { 
  
 public: 
  
  
 // Construct an nd_range from global and work-group local ranges 
  
 nd_range
 (
 range
 <
 Dimensions
 > 
 global
 , 
 range
 <
 Dimensions
 > 
 local
 ); 
  
 // Return the global 
 and work-group local ranges 
  
  
 range
 <
 Dimensions
 > 
 get_global_range
 () 
 const
 ; 
  
  
 range
 <
 Dimensions
 > 
 get_local_range
 () 
 const
 ; 
  
  
 // Return the number of work-groups in the global range 
  
  
 range
 <
 Dimensions
 > 
 get_group_range
 () 
 const
 ; 
  
 };
  
 Figure 4-15. 
 Simplified definition of the nd_range class",NA
 The nd_item Class,"An nd_item is the ND-range form of an item, again encapsulating the 
 execution range of the kernel and the item’s index within that range. 
 Where nd_item differs from item is in how its position in the range is 
 queried and represented, as shown by the simplified class definition in 
 Figure 
 4-16
 . 
  
 114",NA
 The group Class,"The group class encapsulates all functionality related to work-groups, and 
 a simplified definition is shown in Figure 
 4-17
 .
  
 template
  <
 int 
 Dimensions
  = 
 1
 > 
  
 class
  group
  { 
  
 public: 
  
  
 // Return the index of this group in the kernel's execution range 
  
 id
 <
 Dimensions
 > 
 get_id
 () 
 const
 ; 
  
  
 size_t
  get_id
 (
 int
  dimension
 ) 
 const
 ; 
  
  
 size_t
  get_linear_id
 () 
 const
 ; 
  
  
 // Return the number of groups in the kernel's execution range 
  
 range
 <
 Dimensions
 >
  
 get_group_range
 () 
 const
 ; 
  
  
 size_t
  get_group_range
 (
 int
  dimension
 ) 
 const
 ; 
  
  
 // Return the number of work-items in this group 
  
  
 range
 <
 Dimensions
 > 
 get_local_range
 () 
 const
 ; 
  
  
 size_t
  get_local_range
 (
 int
  dimension
 ) 
 const
 ; 
  
 };
  
 Figure 4-17. 
 Simplified definition of the group class
  
 Many of the functions that the group class provides each have 
  
 equivalent functions in the nd_item class: for example, calling group.get_ 
 id() is equivalent to calling item.get_group_id(), and calling group.
  
 get_local_range() is equivalent to calling item.get_local_range().
  
 If we’re not using any of the work-group functions exposed by the class, 
 should we still use it? Wouldn’t it be simpler to use the functions in 
  
 nd_item directly, instead of creating an intermediate group object? There is 
 a tradeoff here: using group requires us to write slightly more code, but 
 that code may be easier to read. For example, consider the code snippet in 
 Figure 
 4-18
 : it is clear that body expects to be called by all work-items in 
 the group, and it is clear that the range returned by get_local_range() in the 
 body of the parallel_for is the range of the group. The same code could very 
 easily be written using only nd_item, but it would likely be harder for 
 readers to follow.
  
 116",NA
 The sub_group Class,"The sub_group class encapsulates all functionality related to sub- 
  
 groups, and a simplified definition is shown in Figure 
 4-19
 . Unlike with 
  
 work-groups, the sub_group class is the only way to access sub-group 
  
 functionality; none of its functions are duplicated in nd_item. The queries 
  
 in the sub_group class are all interpreted relative to the calling work-item: 
  
 for example, get_local_id() returns the local index of the calling work- 
  
 item within its sub-group.
  
 class
  sub_group
  { 
  
 public: 
  
  
 // Return the index of the sub-group 
  
 id
 <
 1
 > 
 get_group_id
 () 
 const
 ;
  
 // Return the number of sub-groups in this item's parent work-group 
 range
 <
 1
 > 
 get_group_range
 () 
 const
 ;
  
 // Return the index of the work-item in this sub-group 
 id
 <
 1
 > 
 get_local_id
 () 
 const
 ;
  
 // Return the number of work-items in this sub-group 
 range
 <
 1
 > 
 get_local_range
 () 
 const
 ;
  
  
 // Return the maximum number of work-items in any 
  
 // sub-
 group in this item's parent work-group 
  
 range
 <
 1
 > 
 get_max_local_range
 () 
 const
 ; 
  
 };
  
 Figure 4-19. 
 Simplified definition of the sub_group class
  
 117",NA
 Hierarchical Parallel Kernels,"Hierarchical data-parallel kernels offer an experimental alternative syntax 
 for expressing kernels in terms of work-groups and work-items, where 
 each level of the hierarchy is programmed using a nested invocation of the 
 parallel_for function. This 
 top-down
  programming style is intended to be 
 similar to writing parallel loops and may feel more familiar than the 
 bottom-up
  programming style used by the other two kernel forms.
  
 One complexity of hierarchical kernels is that each nested invocation of 
 parallel_for creates a separate SPMD environment; each scope defines a 
 new “program” that should be executed by all parallel workers associated 
 with that scope. This complexity requires compilers to perform additional 
 analysis and can complicate code generation for some devices; compiler 
 technology for hierarchical parallel kernels on some platforms is still 
 relatively immature, and performance will be closely tied to the quality of a 
 particular compiler implementation.
  
 Since the relationship between a hierarchical data-parallel kernel and 
 the code generated for a specific device is compiler-dependent, 
 hierarchical kernels should be considered a more 
 descriptive
  construct 
 than explicit ND-range kernels. However, since hierarchical kernels retain 
 the ability to control the mapping of work to work-items and work-groups, 
 they remain more 
 prescriptive
  than basic kernels.
  
 118",NA
 Understanding Hierarchical Data-,NA,NA
Parallel Kernels,"The underlying execution model of hierarchical data-parallel kernels is the 
 same as the execution model of explicit ND-range data-parallel kernels. 
  
 Work-items, sub-groups, and work-groups have identical semantics 
 and execution guarantees.
  
 However, the different scopes of a hierarchical kernel are mapped by 
 the compiler to different execution resources: the outer scope is executed 
 once per work-group (as if executed by a single work-item), while the 
 inner scope is executed in parallel by work-items within the work-group. 
 The different scopes also control where in memory different variables 
 should be allocated, and the opening and closing of scopes imply work-
 group barriers (to enforce memory consistency).
  
 Although the work-items in a work-group are still divided into 
  
 sub-groups, the sub_group class cannot currently be accessed from a 
 hierarchical parallel kernel; incorporating the concept of sub-groups into 
 SYCL hierarchical parallelism requires more significant changes than 
 introducing a new class, and work in this area is ongoing.",NA
 Writing Hierarchical Data-Parallel ,NA,NA
Kernels,"In hierarchical kernels, the parallel_for function is replaced by the 
 parallel_for_work_group and parallel_for_work_item functions, which 
 correspond to work-group and work-item parallelism, respectively. Any 
 code in a parallel_for_work_group scope is executed only once per work-
 group, and variables allocated in a parallel_for_work_group scope are 
 visible to all work-items (i.e., they are allocated in work-group local 
 memory). Any code in a parallel_for_work_item scope is executed in 
 parallel by the work-items of the work-group, and variables allocated in a 
 parallel_for_work_item scope are visible to a single work-item (i.e., they 
 are allocated in work-item private memory).",NA
 Details of Hierarchical Data-Parallel ,NA,NA
Kernels,"Hierarchical data-parallel kernels reuse the group class from ND-range 
  
 data-parallel kernels, but replace nd_item with h_item. A new  private_
  
 memory class is introduced to provide tighter control over allocations in 
  
 parallel_for_work_group scope.",NA
 The h_item Class,"An h_item is a variant of item that is only available within a parallel_
  
 for_work_item scope. As shown in Figure 
 4-23
 , it provides a similar 
  
 interface to an nd_item, with one notable difference: the item’s index can 
  
 be queried relative to the physical execution range of a work-group 
 (with 
  
 get_physical_local_id()) or the logical execution range of a parallel_
  
 for_work_item construct (with get_logical_local_id()).
  
 122",NA
 The private_memory Class,"The private_memory class provides a mechanism to declare variables that 
 are private to each work-item, but which can be accessed across multiple 
 parallel_for_work_item constructs nested within the same parallel_ 
 for_work_group scope.
  
 This class is necessary because of how variables declared in different 
 hierarchical parallelism scopes behave: variables declared at the outer 
 scope are only private if the compiler can prove it is safe to make them so, 
 and variables declared at the inner scope are private to a logical work- 
 item rather than a physical one. It is impossible using scope alone for us to 
 convey that a variable is intended to be private for each physical work- 
 item.
  
 To see why this is a problem, let’s refer back to our matrix 
  
 multiplication kernels in Figure 
 4-22
 . The ib and jb variables are declared 
 at parallel_for_work_group scope and by default should be allocated in 
 work-group local memory! There’s a good chance that an optimizing 
 compiler would not make this mistake, because the variables are read-only 
 and their value is simple enough to compute redundantly on every work- 
 item, but the language makes no such guarantees. If we want to be certain 
  
 123",NA
 Mapping Computation to Work-Items,"Most of the code examples so far have assumed that each instance of a 
 kernel function corresponds to a single operation on a single piece of data. 
 This is a simple way to write kernels, but such a one-to-one mapping is not 
  
 124",NA
 One-to-One Mapping,"When we write kernels such that there is a one-to-one mapping of work to 
 work-items, those kernels must always be launched with a range or 
 nd_range with a size exactly matching the amount of work that needs to be 
 done. This is the most obvious way to write kernels, and in many cases, it 
 works very well—we can trust an implementation to map work-items to 
 hardware efficiently.
  
 However, when tuning for performance on a specific combination of 
 system and implementation, it may be necessary to pay closer attention to 
 low-level scheduling behaviors. The scheduling of work-groups to 
 compute resources is implementation-defined and could potentially be 
 dynamic
  (i.e., when a compute resource completes one work-group, the 
 next work-group it executes may come from a shared queue). The impact 
 of dynamic scheduling on performance is not fixed, and its significance 
 depends upon factors including the execution time of each instance of the 
 kernel function and whether the scheduling is implemented in software 
 (e.g., on a CPU) or hardware (e.g., on a GPU).",NA
 Many-to-One Mapping,"The alternative is to write kernels with a many-to-one mapping of work 
 to work-items. The 
 meaning
  of the range changes subtly in this case: the 
 range no longer describes the amount of work to be done, but rather the 
 number of workers to use. By changing the number of workers and the 
 amount of work assigned to each worker, we can fine-tune work 
 distribution to maximize performance.
  
 125",NA
 Choosing a Kernel Form,"Choosing between the different kernel forms is largely a matter of personal 
 preference and heavily influenced by prior experience with other parallel 
 programming models and languages.
  
 The other main reason to choose a specific kernel form is that it is the 
 only form to expose certain functionality required by a kernel. 
 Unfortunately, it can be difficult to identify which functionality will be 
 required before development begins—especially while we are still 
 unfamiliar with the different kernel forms and their interaction with 
 various classes.
  
 We have constructed two guides based on our own experience in order 
 to help us navigate this complex space. These guides should be considered 
 rules of thumb and are definitely not intended to replace our own 
  
 experimentation—the best way to choose between the different kernel 
 forms will always be to spend some time writing in each of them, in order 
 to learn which form is the best fit for our application and development 
 style.
  
  
 The first guide is the flowchart in Figure 
 4-26
 , which selects a 
 kernel form based on
  
  1. Whether we have previous experience with parallel 
 programming
  
  2. Whether we are writing a new code from scratch or are 
 porting an existing parallel program written in a 
 different language
  
  3. Whether our kernel is embarrassingly parallel, 
 already contains nested parallelism, or reuses data 
 between different instances of the kernel function
  
  4. Whether we are writing a new kernel in SYCL to 
 maximize performance or to improve the portability 
 of our code or because it provides a more productive 
 means of expressing parallelism than lower-level 
 languages",NA
 Summary,"This chapter introduced the basics of expressing parallelism in DPC++ and 
 discussed the strengths and weaknesses of each approach to writing data- 
 parallel kernels.
  
 DPC++ and SYCL provide support for many forms of parallelism, and 
 we hope that we have provided enough information to prepare readers 
 to dive in and start coding!
  
 We have only scratched the surface, and a deeper dive into many of 
 the concepts and classes introduced in this chapter is forthcoming: the 
 usage of local memory, barriers, and communication routines will be 
 covered in Chapter 
 9
 ; different ways of defining kernels besides using 
 lambda expressions will be discussed in Chapter 
 10
 ; detailed mappings 
 of the ND- range execution model to specific hardware will be explored 
 in Chapters 
 15
 , 
 16
 , and 
 17
 ; and best practices for expressing common 
 parallel patterns using DPC++ will be presented in Chapter 
 14
 .
  
 129",NA
CHAPTER 5,NA,NA
Error Handling,"Agatha Christie wrote in 1969 that “human error is nothing to what a 
 computer can do if it tries.” It is no mystery that we, as programmers, get 
 to clean up the mess. The mechanisms for error handling could catch 
 programmer errors that 
 others
  may make. Since we do not plan on making 
 mistakes ourselves, we can focus on using error handling to handle 
 conditions that may occur in the real world from 
 other
  causes.
  
 Detecting and dealing with unexpected conditions and errors can be 
 helpful during application development (think: the 
 other
  programmer who 
 works on the project who 
 does
  make mistakes), but more importantly play 
 a critical role in stable and safe production applications and libraries. 
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_5
  
 131",NA
 Safety First,"A core aspect of C++ error handling is that if we do nothing to handle an 
 error that has been detected (thrown), then the application will terminate 
 and indicate that something went wrong. This behavior allows us to write 
 applications without focusing on error management and still be confident 
 that errors will somehow be signaled to a developer or user. We’re not 
 suggesting that we should ignore error handling, of course! Production 
 applications should be written with error management as a core part of the 
 architecture, but applications often start development without such a 
 focus. C++ aims to make code which doesn’t handle errors still able to 
 observe errors, even when they are not dealt with explicitly.
  
 Since SYCL is Data Parallel C++, the same philosophy holds: if we do 
 nothing in our code to manage errors and an error is detected, an 
 abnormal termination of the program will occur to let us know that 
 something bad happened. Production applications should of course 
 consider error management as a core part of the software architecture, not 
 only reporting but often also recovering from error conditions.",NA
If we don’t add any error management code and an error ,NA,NA
"occurs, we will still see an abnormal program termination ",NA,NA
which is an indication to dig deeper.,132,NA
 Types of Errors,"C++ provides a framework for notification and handling of errors through 
 its exception mechanism. Heterogeneous programming requires an 
 additional level of error management beyond this, because some errors 
 occur on a device or when trying to launch work on a device. These errors 
 are typically decoupled in time from the host program’s execution, and as 
 such they don’t integrate cleanly with classic C++ exception handling 
 mechanisms. To solve this, there are additional mechanisms to make 
 asynchronous errors as manageable and controllable as regular C++ 
 exceptions.
  
 Figure 
 5-1
  shows two components of a typical application: (1) the host 
 code that runs sequentially and submits work to the task graph for future 
 execution and (2) the task graph which runs asynchronously from the host 
 program and executes kernels or other actions on devices when the 
 necessary dependences are met. The example shows a 
  
 parallel_for as the operation that executes asynchronously as part of the 
 task graph, but other operations are possible as well as discussed in 
 Chapters 
 3
 , 
 4
 , and 
 8
 .
  
 133",NA
 Let’s Create Some Errors!,"As examples for the remainder of this chapter and to allow us to 
  
 experiment, we’ll create both synchronous and asynchronous errors in the 
 following sections.",NA
 Synchronous Error,"#include
  <CL/sycl.hpp> 
  
 using
  namespace
  sycl
 ;
  
 int
  main
 () { 
  
  
 buffer
 <
 int
 > B{ 
 range
 {
 16
 } };
  
 // ERROR: Create sub-buffer larger than size of parent buffer // An exception is thrown 
 from within the buffer constructor 
 buffer
 <
 int
 > 
 B2
 (B, 
 id
 {
 8
 }, 
 range
 {
 16
 });
  
 }
  
 return
  0
 ;
  
 Example output: 
  
 terminate called after throwing an instance of 
  
 'cl::sycl::invalid_object_error' 
  
  
 what():  Requested sub-buffer size exceeds the size of the parent buffer 
  
 -30 (CL_INVALID_VALUE)
  
 Figure 5-2. 
 Creating a synchronous error
  
 135",NA
 Asynchronous Error,"Generating an asynchronous error is a bit trickier because 
  
 implementations work hard to detect and report errors synchronously 
 whenever possible. Synchronous errors are easier to debug because they 
 occur at a specific point of origin in the host program, so are preferred 
 whenever possible. One way to generate an asynchronous error for our 
 demonstration purpose, though, is to add a fallback/secondary queue to 
 our command group submission and to discard synchronous exceptions 
 that also happen to be thrown. Figure 
 5-3
  shows such code which 
 invokes our handle_async_error function to allow us to experiment. 
  
 Asynchronous errors can occur and be reported without a secondary/ 
 fallback queue, so note that the secondary queue is only part of the 
 example and in no way a requirement for asynchronous errors.
  
 136",NA
 Application Error Handling Strategy,"The C++ 
 exception
  features are designed to cleanly separate the point in a 
 program where an error is detected from the point where it may be 
 handled, and this concept fits very well with both synchronous and 
 asynchronous errors in SYCL. Through the throw and catch mechanisms, a 
 hierarchy of handlers can be defined which can be important in 
  
 production applications.
  
 Building an application that can handle errors in a consistent 
  
 and reliable way requires a strategy up front and a resulting software 
 architecture built for error management. C++ provides flexible tools to 
 implement many alternative strategies, but such architecture is beyond the 
 scope of this chapter. There are many books and other references devoted 
 to this topic, so we encourage looking to them for full coverage of C++ 
 error management strategies.
  
 This said, error detection and reporting doesn’t always need to 
  
 be production-scale. Errors in a program can be reliably detected and 
 reported through minimal code if the goal is simply to detect errors during 
 execution and to report them (but not necessarily to recover from them). 
 The following sections cover first what happens if we ignore error 
 handling and do nothing (the default behavior isn’t all that bad!), followed 
 by recommended error reporting that is simple to implement in basic 
 applications.",NA
 Ignoring Error Handling,"C++ and SYCL are designed to tell us that something went wrong even 
 when we don’t handle errors explicitly. The default result of unhandled 
 synchronous or asynchronous errors is abnormal program termination 
 which an operating system should tell us about. The following two 
 examples mimic the behavior that will occur if we do not handle a 
 synchronous and an asynchronous error, respectively.
  
 138",NA
 Synchronous Error Handling,"We keep this section very short because SYCL synchronous errors are just 
 C++ exceptions. Most of the additional error mechanisms added in SYCL 
 relate to asynchronous errors which we cover in the next section, but 
 synchronous errors are important because implementations try to detect 
 and report as many errors synchronously as possible, since they are 
 easier to reason about and handle.
  
 Synchronous errors defined by SYCL are a derived class from 
  
 std::exception of type sycl::exception, which allows us to catch the SYCL 
 errors specifically though a try-catch structure such as what we see in 
 Figure 
 5-6
 .
  
 try{ 
  
  
 // Do some SYCL work 
  
 } 
 catch
  (
 sycl
 ::exception &e) { 
  
  
 // Do something to output or handle the exceptinon 
  
  
 std
 ::cout << ""Caught sync SYCL exception: "" << 
 e
 .
 what
 () << ""
 \n
 ""; 
  
 return
  1
 ; 
  
 } 
  
 Figure 5-6. 
 Pattern to catch sycl::exception specifically
  
 140",NA
 Asynchronous Error Handling,"Asynchronous errors are detected by the SYCL runtime (or an underlying 
 backend), and the errors occur independently of execution of commands 
 in the host program. The errors are stored in lists internal to the SYCL 
  
 141",NA
 The Asynchronous Handler,"The asynchronous handler is a function that the application defines, which 
 is registered with SYCL contexts and/or queues. At the times defined by 
 the next section, if there are any unprocessed asynchronous exceptions 
 that are available to be handled, then the asynchronous handler is invoked 
 by the SYCL runtime and passed a list of these exceptions.
  
 The asynchronous handler is passed to a context or queue constructor 
 as a std::function and can be defined in ways such as a regular function, 
 lambda, or functor, depending on our preference. The handler must accept 
 a sycl::exception_list argument, such as in the example handler shown in 
 Figure 
 5-8
 .
  
 // Our simple asynchronous handler function 
  
 auto
  handle_async_error = [](
 exception_list
  elist
 ) { 
  
 for
  (
 auto
  &e : elist) { 
  
   
 try
 { 
 std
 ::rethrow_exception(e); } 
  
   
 catch
  ( 
 sycl
 ::exception& e ) { 
  
   
 std
 ::cout << ""ASYNC EXCEPTION!!
 \n
 ""; 
  
   
 std
 ::cout << 
 e
 .
 what
 () << 
 ""
 \n
 ""
 ; 
  
   
 } 
  
  
 } 
  
 };
  
 Figure 5-8. 
 Example asynchronous handler implementation defined as 
 a lambda
  
 142",NA
"In defining asynchronous handlers, most developers should ",NA,NA
define them on queues unless already explicitly managing ,NA,NA
contexts for other reasons.,"If an asynchronous handler is not defined for a queue or the queue’s 
 parent context and an asynchronous error occurs on that queue (or in 
 the context) that must be processed, then the default asynchronous 
 handler is invoked. The default handler operates as if it was coded as 
 shown in Figure 
 5-9
 .
  
 143",NA
Consider ,NA,NA
terminating ,NA,NA
applications ,NA,NA
within ,NA,NA
an ,NA,NA
"asynchronous handler, after outputting information about ",NA,NA
the ,NA,NA
"error, ",NA,NA
if ,NA,NA
comprehensive ,NA,NA
error ,NA,NA
recovery ,NA,NA
and ,NA,NA
management mechanisms are not in place.,NA,NA
 Invocation of the Handler,"The asynchronous handler is called by the runtime at specific times. 
  
 Errors aren’t reported immediately as they occur because management of 
 errors and safe application programming (particularly multithreaded) 
 would become more difficult and expensive if that was the case. The 
 asynchronous handler is instead called at the following very specific times:
  
  1. When the host program calls 
  
 queue::throw_asynchronous() on a specific queue
  
  2. When the host program calls 
  
 queue::wait_and_throw() on a specific queue
  
  3. When the host program calls 
  
 event::wait_and_throw() on a specific event
  
  4. When a queue is destroyed
  
  5. When a context is destroyed
  
 Methods 1–3 provide a mechanism for a host program to control 
 when asynchronous exceptions are handled, so that thread safety and 
 other details specific to an application can be managed. They effectively 
 provide controlled points at which asynchronous exceptions enter the 
 host program control flow and can be processed almost as if they were 
 synchronous errors.
  
 145",NA
 Errors on a Device,"The error detection and handling mechanisms discussed in this chapter 
 have been host-based. They are mechanisms through which the host 
 program can detect and deal with something that may have gone wrong 
 either in the host program or potentially during execution of kernels on 
 devices. What we have not covered is how to signal, from within the device 
 code that we write, that something has gone wrong. This omission is not a 
 mistake, but quite intentional.
  
 SYCL explicitly disallows C++ exception handling mechanisms (such 
 as throw) within device code, because there are performance costs for 
 some types of device that we usually don’t want to pay. If we detect that 
 something has gone wrong within our device code, we should signal the 
 error using existing non-exception-based techniques. For example, we 
 could write to a buffer that logs errors or return some invalid result from 
 our numeric calculation that we define to mean that an error occurred. 
 The right strategy in these cases is very application specific.
  
 146",NA
 Summary,"In this chapter, we introduced synchronous and asynchronous errors, 
 covered the default behavior to expect if we do nothing to manage 
 errors that might occur, and covered the mechanisms used to handle 
 asynchronous errors at controlled points in our application. Error 
 management strategies are a major topic in software engineering and a 
 significant percentage of the code written in many applications. SYCL 
 integrates with the C++ knowledge that we already have when it comes 
 to error handling and provides flexible mechanisms to integrate with 
 whatever our preferred error management strategy is.
  
  
 Open Access
  This chapter is licensed under the terms of 
 the Creative Commons Attribution 4.0 International 
  
 License (
 http://creativecommons.org/licenses/by/4.0/
 ), which permits 
 use, sharing, adaptation, distribution and reproduction in any medium or 
 format, as long as you give appropriate credit to the original author(s) and 
 the source, provide a link to the Creative Commons license and indicate if 
 changes were made.
  
 The images or other third party material in this chapter are included 
 in the chapter’s Creative Commons license, unless indicated otherwise in 
 a credit line to the material. If material is not included in the chapter’s 
 Creative Commons license and your intended use is not permitted by 
 statutory regulation or exceeds the permitted use, you will need to 
 obtain permission directly from the copyright holder.
  
 147",NA
CHAPTER 6,NA,NA
Unified ,NA,NA
Shared ,NA,NA
Memory,"The next two chapters provide a deeper look into how to manage data. 
 There are two different approaches that complement each other: Unified 
 Shared Memory (USM) and buffers. USM exposes a different level of 
 abstraction for memory than buffers—USM has pointers, and buffers are a 
 higher-level interface. This chapter focuses on USM. The next chapter will 
 focus on buffers.
  
 Unless we specifically know that we want to use buffers, USM is a good 
 place to start. USM is a pointer-based model that allows memory to be 
 read and written through regular C++ pointers.",NA
 Why Should We Use USM?,"Since USM is based on C++ pointers, it is a natural place to start for 
 existing pointer-based C++ codes. Existing functions that take pointers as 
 parameters continue to work without modification. In the majority of 
 cases, the only changes required are to replace existing calls to malloc or 
 new with USM-specific allocation routines that we will discuss later in 
 this chapter.",NA
 Allocation Types,"While USM is based on C++ pointers, not all pointers are created equal. 
  
 USM defines three different types of allocations, each with unique 
 semantics. A device may not support all types (or even 
 any
  type) of USM 
 allocation. We will learn how to query what a device supports later. The 
 three types of allocations and their characteristics are summarized in 
 Figure 
 6-1
 .
  
  
 Figure 6-1. 
 USM allocation types
  
 150",NA
 Device Allocations,"This first type of allocation is what we need in order to have a pointer into 
 a device’s attached memory, such as (G)DDR or HBM. Device allocations 
 can be read from or written to by kernels running on a device, but they 
 cannot be directly accessed from code executing on the host. Trying to 
 access a device allocation on the host can result in either incorrect data or 
 a program crashing due to an error. We must copy data between host and 
 device using the explicit USM memcpy mechanisms, which specify how 
 much data must be copied between two places, that will be covered later in 
 this chapter.",NA
 Host Allocations,"This second type of allocation is easier to use than device allocations since 
 we do not have to manually copy data between the host and the device. 
 Host allocations are allocations in host memory that are accessible on both 
 the host and the device. These allocations, while accessible on the device, 
 cannot migrate to the device’s attached memory. Instead, kernels that read 
 from or write to this memory do it 
 remotely
 , often over a slower bus such 
 as PCI-Express. This tradeoff between convenience and performance is 
 something that we must take into consideration. Despite the higher access 
 costs that host allocations can incur, there are still valid reasons to use 
 them. Examples include rarely accessed data or large data sets that cannot 
 fit inside device attached memory.",NA
 Shared Allocations,"The final type of allocation combines attributes of both device and host 
 allocations, combining the programmer convenience of host allocations 
 with the greater performance afforded by device allocations. Like host 
 allocations, shared allocations are accessible on both the host and device. 
  
 151",NA
 Allocating Memory,"USM allows us to allocate memory in a variety of different ways that 
 cater to different needs and preferences. However, before we go over all 
 the methods in greater detail, we should discuss how USM allocations 
 differ from regular C++ allocations.
  
 152",NA
 What Do We Need to Know?,"Regular C++ programs can allocate memory in multiple ways: new, malloc, 
 or allocators. No matter which syntax we prefer, memory allocation is 
 ultimately performed by the system allocator in the host operating system. 
  
 When we allocate memory in C++, the only concerns are “How much 
 memory do we need?” and “How much memory is available to allocate?” 
 However, USM requires extra information before an allocation can be 
 performed.
  
 First, USM allocation needs to specify which type of allocation is 
 desired: device, host, or shared. It is important to request the right type of 
 allocation in order to obtain the desired behavior for that allocation. 
  
 Next, every USM allocation must specify a context object against which the 
 allocation will be made. The context object hasn’t had a lot of discussion 
 yet, so it’s worth saying a little about it here. A context represents a device 
 or set of devices on which we can execute kernels. We can think of a 
 context as a convenient place for the runtime to stash some state about 
 what it’s doing. Programmers are not likely to directly interact with 
 contexts outside of passing them around in most DPC++ programs.
  
 USM allocations are not guaranteed to be usable across different 
 contexts—it is important that all USM allocations, queues, and kernels 
 share the same context object. Typically, we can obtain this context from 
 the queue being used to submit work to a device. Finally, device 
 allocations also require that we specify which device will provide the 
 memory for the allocation. This is important since we do not want to 
 oversubscribe the memory of our devices (unless the device is able to 
 support this—we will say more about that later in the chapter when we 
 discuss migration of data). USM allocation routines can be distinguished 
 from their C++ analogues by the addition of these extra parameters.
  
 153",NA
 Multiple Styles,"Sometimes, trying to please everyone with a single option proves to be an 
 impossible task, just as some people prefer coffee over tea, or emacs over 
 vi. If we ask programmers what an allocation interface should look like, we 
 will get several different answers back. USM embraces this diversity of 
 choice and provides several different flavors of allocation interfaces. These 
 different flavors are C-style, C++-style, and C++ allocator–style. We will 
 now discuss each and point out their similarities and differences.",NA
 Allocations à la C,"The first style of allocation functions (listed in Figure 
 6-2
 , later used in 
 examples shown in Figures 
 6-6
  and 
 6-7
 ) is modeled after memory 
 allocation in C: malloc functions that take a number of bytes to allocate and 
 return a void * pointer. This style of function is type agnostic. We must 
 specify the total number of bytes to allocate, which means if we want to 
 allocate N objects of type X, one must ask for N * sizeof(X) total bytes. 
  
 The returned pointer is of type void *, which means that we must then 
 cast it to an appropriate pointer to type X. This style is very simple but 
 can be verbose due to the size calculations and typecasting required.
  
 We can further divide this style of allocation into two categories: 
 named functions and single function. The distinction between these two 
 flavors is how we specify the desired type of USM allocation. With the 
 named functions (malloc_device, malloc_host, and malloc_shared), the type 
 of USM allocation is encoded in the function name. The single function 
 malloc requires the type of USM allocation to be specified as an additional 
 parameter. Neither flavor is better than the other, and the choice of which 
 to use is governed by our preference.
  
 We cannot move on without briefly mentioning alignment. Each 
 version of malloc also has an aligned_alloc counterpart. The malloc 
 functions return memory aligned to the default behavior of our device. 
  
 154",NA
 Allocations à la C++,"The next flavor of USM allocation functions (listed in Figure 
 6-3
 ) is very 
 similar to the first but with more of a C++ look and feel. We once again 
 have both named and single function versions of the allocation routines as 
 well as our default and user-specified alignment versions. The difference is 
 that now our functions are C++ templated functions that allocate Count 
 objects of type T and return a pointer of type T *. Taking advantage of 
 modern C++ simplifies things, since we no longer need to manually 
 calculate the total size of the allocation in bytes or cast the returned 
 pointer to the appropriate type. This also tends to yield a more compact 
 and less error-prone expression in code. However, we should note that 
 unlike “new” in C++, malloc-style interfaces do not invoke constructors for 
 the objects being allocated—we are simply allocating enough bytes to fit 
 that type.
  
 This flavor of allocation is a good place to start for new codes written 
 with USM in mind. The previous C-style is a good starting point for existing 
 C++ codes that already make heavy use of C or C++ malloc, to which we 
 will add the use of USM.
  
 156",NA
 C++ Allocators,"The final flavor of USM allocation (Figure 
 6-4
 ) embraces modern C++ 
  
 even more than the previous flavor. This flavor is based on the C++ 
  
 allocator interface, which defines objects that are used to perform 
  
 memory allocations either directly or indirectly inside a container such 
  
 157",NA
 Deallocating Memory,"Whatever a program allocates must eventually be deallocated. USM 
 defines a free method to deallocate memory allocated by one of the malloc 
 or aligned_malloc functions. This free method also takes the context in 
 which the memory was allocated as an extra parameter. The queue can 
 also be substituted for the context. If memory was allocated with a C++ 
 allocator object, it should also be deallocated using that object.
  
 constexpr int
  N = 
 42
 ;
  
 queue
  Q;
  
 // Allocate N floats
  
 // C-style 
  
 float
  *f1 = 
 static_cast
 <
 float
 *>(
 malloc_shared
 (N*
 sizeof
 (
 float
 ),Q));
  
 // C++-style 
  
 float
  *f2 = 
 malloc_shared
 <
 float
 >(N, Q);
  
 // C++-allocator-style 
  
 usm_allocator<
 float
 , 
 usm
 ::
 alloc
 ::shared> 
 alloc
 (Q); 
 float
  *f3 = 
 alloc
 .
 allocate
 (N);
  
 // Free our allocations 
  
 free
 (f1, 
 Q
 .
 get_context
 ()); 
  
 free
 (f2, Q); 
  
 alloc
 .
 deallocate
 (f3, N);
  
 Figure 6-5. 
 Three styles for allocation",NA
 Allocation Example,"In Figure 
 6-5
 , we show how to perform the same allocation using the 
 three styles just described. In this example, we allocate N single-
 precision floating-point numbers as shared allocations. The first 
 allocation f1 uses the C-style void * returning malloc routines. For this 
 allocation, we explicitly pass the device and context that we obtain from 
 the queue. 
  
 159",NA
 Data Management,"Now that we understand how to allocate memory using USM, we will 
 discuss how data is managed. We can look at this in two pieces: data 
 initialization and data movement.",NA
 Initialization,"Data initialization concerns filling our memory with values before we 
  
 perform computations on it. One example of a common initialization 
 pattern is to fill an allocation with zeroes before it is used. If we were to do 
 this using USM allocations, we could do it in a variety of ways. First, we 
 could write a kernel to do this. If our data set is particularly large or the 
 initialization requires complex calculations, this is a reasonable way to go 
 since the initialization can be performed in parallel (and it makes the 
 initialized data ready to go on the device). Second, we could implement 
 this as a loop over all the elements of an allocation that sets each to zero. 
 However, there is potentially a problem with this approach. A loop would 
 work fine for host and shared allocations since these are accessible on the 
 host. However, since device allocations are 
 not
  accessible on the host, a 
 loop in host code would not be able to write to them. This brings us to the 
 third option.",NA
 Data Movement,"Data movement is probably the most important aspect of USM to 
  
 understand. If the right data is not in the right place at the right time, our 
 program will produce incorrect results. USM defines two strategies that 
 we can use to manage data: explicit and implicit. The choice of which 
 strategy we want to use is related to the types of USM allocations our 
 hardware supports or that we want to use.",NA
 Explicit,"The first strategy USM offers is explicit data movement (Figure 
 6-6
 ). Here, 
 we must explicitly copy data between the host and device. We can do this 
 by invoking the memcpy method, found on both the handler and queue 
 classes. The memcpy method takes three arguments: a pointer to the 
  
 161",NA
 Implicit,"The second strategy that USM provides is implicit data movement 
  
 (example usage shown in Figure 
 6-7
 ). In this strategy, data movement 
  
 happens 
 implicitly
 , that is, without requiring input from us. With implicit 
  
 data movement, we do not need to insert calls to memcpy since we can 
  
 directly access the data through the USM pointers wherever we want to 
 use 
  
 it. Instead, it becomes the job of the system to ensure that the data will be 
  
 available in the correct location when it is being used.
  
 163",NA
Migration,"With explicit data movement, we control how much data movement 
 occurs. With implicit data movement, the system handles this for us, but it 
 might not do it as efficiently. The DPC++ runtime is not an oracle—it 
 cannot predict what data an application will access before it does it. 
 Additionally, pointer analysis remains a very difficult problem for 
 compilers, which may not be able to accurately analyze and identify every 
 allocation that might be used inside a kernel. Consequently, 
  
 implementations of the mechanisms for implicit data movement may 
 make different decisions based on the capabilities of the device that 
 supports USM, which affects both how shared allocations can be used and 
 how they perform.
  
 If a device is very capable, it might be able to migrate memory on 
 demand. In this case, data movement would occur after the host or 
  
 device attempts to access an allocation that is not currently in the desired 
 location. On-demand data greatly simplifies programming as it provides 
 the desired semantic that a USM shared pointer can be accessed anywhere 
 and just work. If a device cannot support on-demand migration (Chapter 
 12
  explains how to query a device for capabilities), it might still be able to 
 guarantee the same semantics with extra 
 restrictions
  on how shared 
 pointers can be used.
  
 The restricted form of USM shared allocations governs when and 
 where shared pointers may be accessed and how big shared allocations 
 can be. If a device cannot migrate memory on demand, that means the 
 runtime must be conservative and assume that a kernel might access 
 any allocation in its device attached memory. This brings a couple of 
 consequences.
  
 First, it means that the host and device should not try to access a 
 shared allocation at the same time. Applications should instead alternate 
 access in phases. The host can access an allocation, then a kernel can 
 compute using that data, and finally the host can read the results. 
  
 165",NA
Fine-Grained Control,"When a device supports on-demand migration of shared allocations, data 
 movement occurs after memory is accessed in a location where it is not 
 currently resident. However, a kernel can stall while waiting for the data 
 movement to complete. The next statement it executes may even cause 
 more data movement to occur and introduce additional latency to the 
 kernel execution.
  
 DPC++ gives us a way to modify the performance of the automatic 
 migration mechanisms. It does this by defining two functions: prefetch 
 and mem_advise. Figure 
 6-8
  shows a simple utilization of each. These 
 functions let us give hints to the runtime about how kernels will access 
 data so that the runtime can choose to start moving data 
 before
  a kernel 
 tries to access it. Note that this example uses the queue shortcut methods 
 that directly invoke parallel_for on the queue object instead of inside a 
 lambda passed to the submit method (a command group).
  
 166",NA
 Queries,"Finally, not all devices support every feature of USM. We should not 
 assume that all USM features are available if we want our programs to be 
 portable across different devices. USM defines several things that we can 
 query. 
  
 These queries can be separated into two categories: pointer queries 
 and device capability queries. Figure 
 6-9
  shows a simple utilization of 
 each.
  
 The pointer queries in USM answer two questions. The first question is 
 “What type of USM allocation does this pointer point to?” The  get_ 
 pointer_type function takes a pointer and DPC++ context and returns a 
 result of type usm::alloc, which can have four possible values: 
 host
 , 
 device
 , 
 shared
 , or 
 unknown
 . The second question is “What device was this USM 
 pointer allocated against?” We can pass a pointer and a context to the 
 function get_pointer_device and get back a device object. This is mostly 
 used with device or shared USM allocations since it does not make much 
 sense with host allocations.
  
 The second type of query provided by USM concerns the capabilities 
 of a device. USM extends the list of device information descriptors that 
 can be queried by calling get_info on a device object. These queries can 
 be used to test which types of USM allocations are supported by a device. 
  
 Additionally, we can query if shared allocations are restricted on the",NA
 Summary,"In this chapter, we’ve described Unified Shared Memory, a pointer-based 
 strategy for data management. We covered the three types of allocations 
 that USM defines. We discussed all the different ways that we can allocate 
 and deallocate memory with USM and how data movement can be either 
 explicitly controlled by us (the programmers) for device allocations or 
 implicitly controlled by the system for shared allocations. Finally, we 
 discussed how to query the different USM capabilities that a device 
 supports and how to query information about USM pointers in a program.
  
 Since we have not discussed synchronization in this book in detail 
 yet, there is more on USM in later chapters when we discuss scheduling, 
 communications, and synchronization. Specifically, we cover these 
 additional considerations for USM in Chapters 
 8
 , 
 9
 , and 
 19
 .
  
  
 In the next chapter, we will cover the second strategy for data 
 management: buffers.
  
 170",NA
CHAPTER 7,NA,NA
Buffers,"In this chapter, we will learn about the buffer abstraction. We learned 
 about Unified Shared Memory (USM), the pointer-based strategy for data 
 management, in the previous chapter. USM forces us to think about where 
 memory lives and what should be accessible where. The buffer abstraction 
 is a higher-level model that hides this from the programmer. Buffers 
 simply represent data, and it becomes the job of the runtime to manage 
 how the data is stored and moved in memory.
  
 This chapter presents an alternative approach to managing our data. 
  
 The choice between buffers and USM often comes down to personal 
 preference and the style of existing code, and applications are free to mix 
 and match the two styles in representation of different data within the 
 application.
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_7
  
 173",NA
 Buffers,"A buffer is a high-level abstraction for data. Buffers are not necessarily tied 
 to a single location or virtual memory address. Indeed, the runtime is free 
 to use many different locations in memory (even across different devices) 
 to represent a buffer, but the runtime must be sure to always give us a 
 consistent view of the data. A buffer is accessible on the host and on any 
 device.
  
 template
  <
 typename
  T
 , 
 int
  Dimensions
 , 
 AllocatorT
  allocator
 > 
 class
  buffer
 ;
  
 Figure 7-1. 
 Buffer class definition
  
 The buffer class is a template class with three template arguments, as 
 shown in Figure 
 7-1
 . The first template argument is the type of the object 
 that the buffer will contain. This type must be 
 trivially copyable
  as defined 
 by C++, which basically means that it is safe to copy this object byte by 
 byte without using any special copy or move constructors. The next 
 template",NA
 Creation,"In the following figures, we show several ways in which buffer objects 
 can be created. The choice of how to create buffers in application code is 
 a combination of how the buffer needs to be used and personal coding 
 preferences. Let’s walk through the example and look at each instance.
  
 // Create a buffer of 2x5 ints using the default allocator 
  
 buffer
 <
 int
 , 
 2
 , buffer_allocator> b1{
 range
 <
 2
 >{
 2
 , 
 5
 }}; 
  
 // Create a buffer of 2x5 ints using the default allocator 
  
 // and CTAD for range 
  
 buffer
 <
 int
 , 
 2
 > b2{
 range
 {
 2
 , 
 5
 }}; 
  
 // Create a buffer of 20 floats using a 
  
 // default-constructed std::allocator 
  
 buffer
 <
 float
 , 
 1
 , 
 std
 ::allocator<
 float
 >> b3{
 range
 {
 20
 }}; 
  
 // Create a buffer of 20 floats using a passed-in allocator 
  
 std
 ::allocator<
 float
 > myFloatAlloc; 
  
 buffer
 <
 float
 , 
 1
 , 
 std
 ::allocator<
 float
 >> b4{
 range
 (
 20
 ), myFloatAlloc};
  
 Figure 7-2. 
 Creating buffers, Part 1
  
 The first buffer we create in Figure 
 7-2
 , b1, is a two-dimensional buffer 
 of ten integers. We explicitly pass all template arguments, even explicitly 
 passing the default value of buffer_allocator as the allocator type. However, 
 using modern C++, we can express this much more compactly. Buffer b2 is 
 also a two-dimensional buffer of ten integers using the default allocator. 
 Here we make use of C++17’s class template argument deduction (CTAD) 
 to automatically infer template arguments we have to express. 
  
 175",NA
 Buffer Properties,"Buffers can also be created with special properties that alter their 
 behavior. 
  
 In Figure 
 7-5
 , we will walk through an example of the three different 
 optional buffer properties and discuss how they might be used. Note that 
 these properties are relatively uncommon in most codes.",NA
use_host_ptr,"The first property that may be optionally specified during buffer creation 
 is use_host_ptr. When present, this property requires the buffer to not 
 allocate any memory on the host, and any allocator passed or specified on 
 buffer construction is effectively ignored. Instead, the buffer must use the 
 memory pointed to by a host pointer that is passed to the constructor. 
 Note that this does not require the device to use the same memory to hold 
 the buffer’s data. A device is free to cache the contents of a buffer in its 
 attached memory. Also note that this property may only be used when a 
 host pointer is passed to the constructor. This option can be useful when 
 the program wants full control over all host memory allocations.
  
 In our example in Figure 
 7-5
 , we create a buffer b as we saw in our 
 previous examples. We next create buffer b1 and initialize it with a pointer 
 to myInts. We also pass the property use_host_ptr, which means that 
 buffer b1 will only use the memory pointed to by myInts and not allocate 
 any additional temporary storage.",NA
use_mutex,"The next property, use_mutex, concerns fine-grained sharing of memory 
 between buffers and host code. Buffer b2 is created using this property. 
 The property takes a reference to a mutex object that can later be queried 
 from the buffer as we see in the example. This property also requires a 
 host pointer be passed to the constructor, and it lets the runtime 
 determine when it is safe to access updated values in host code through 
 the provided",NA
context_bound,"The final property is shown in the creation of buffer b3 in our example. 
 Here, our buffer of 42 integers is created with the context_bound property. 
  
 The property takes a reference to a context object. Normally, a buffer is 
 free to be used on any device or context. However, if this property is 
 used, it locks the buffer to the specified context. Attempting to use the 
 buffer on another context will result in a runtime error. This could be 
 useful for debugging programs by identifying cases where a kernel might 
 be submitted to the wrong queue, for instance. In practice, we do not 
 expect to see this property used in many programs, and the ability for 
 buffers to be accessed on any device in any context is one of the most 
 powerful properties of the buffer abstraction (which this property 
 undoes).",NA
 What Can We Do with a Buffer?,"Many things can be done with buffer objects. We can query characteristics 
 of a buffer, determine if and where any data is written back to host 
 memory after the buffer is destroyed, or reinterpret a buffer as one with 
 different characteristics. One thing that cannot be done, however, is to 
 directly access the data that a buffer represents. Instead, we must create 
 accessor objects to access the data, and we will learn all about this later in 
 the chapter.
  
 181",NA
 Accessors,"Data represented by a buffer cannot be directly accessed through the 
 buffer object. Instead, we must create accessor objects that allow us to 
 safely access a buffer’s data. Accessors inform the runtime where and 
 how we want to access data, allowing the runtime to ensure that the right 
 data is in the right place at the right time. This is a very powerful concept, 
 especially when combined with the task graph that schedules kernels for 
 execution based in part on data dependences.
  
 182",NA
 Accessor Creation,"Figure 
 7-8
  shows an example program with everything that we need to 
 get started with accessors. In this example, we have three buffers, A, B, 
 and C. The first task we submit to the queue creates accessors to each 
 buffer and defines a kernel that uses these accessors to initialize the 
 buffers with some values. Each accessor is constructed with a reference 
 to the buffer it will access as well as the handler object defined by the 
 command group we’re submitting to the queue. This effectively binds the 
 accessor to the kernel we’re submitting as part of the command group. 
 Regular accessors are device accessors since they, by default, target 
 global buffers stored in device memory. This is the most common use 
 case.",NA
 What Can We Do with an Accessor?,"Many things can be done with an accessor object. However, the most 
 important thing we can do is spelled out in the accessor’s name—access 
 data. This is usually done through one of the accessor’s [] operators. We 
 use the [] operator in our examples in Figures 
 7-8
  and 
 7-10
 . This operator 
 takes either an id object that can properly index multidimensional data or 
 a single size_t. The second case is used when an accessor has more than 
 one dimension. It returns an object that is then meant to be indexed again 
 with [] until we arrive at a scalar value, and this would be of the form a[i] 
 [j] in a two-dimensional case. Remember that the ordering of accessor 
 dimensions follows the convention of C++ where the rightmost dimension 
 is the unit-stride dimension (iterates “fastest”).
  
 An accessor can also return a pointer to the underlying data. This 
 pointer can be accessed directly following normal C++ rules. Note that 
 there can be additional complexity involved with respect to the address 
 space of this pointer. Address spaces and their quirks will be discussed in 
 a later chapter.
  
 Many things can also be queried from an accessor object. Examples 
 include the number of elements accessible through the accessor, the size 
 in bytes of the region of the buffer it covers, or the range of data accessible.
  
 Accessors provide a similar interface to C++ containers and may be 
 used in many situations where containers may be passed. The container 
 interface supported by accessors includes the data method, which is 
 equivalent to get_pointer, and several flavors of forward and backward 
 iterators.
  
 191",NA
 Summary,"In this chapter, we have learned about buffers and accessors. Buffers are 
 an abstraction of data that hides the underlying details of memory 
 management from the programmer. They do this in order to provide a 
 simpler, higher-level abstraction. We went through several examples that 
 showed us the different ways to construct buffers as well as the different 
 optional properties that can be specified to alter their behavior. We 
 learned how to initialize a buffer with data from host memory as well as 
 how to write data back to host memory when we are done with a buffer.
  
 Since we should not access buffers directly, we learned how to access 
 the data in a buffer by using accessor objects. We learned the difference 
 between device accessors and host accessors. We discussed the different 
 access modes and targets and how they inform the runtime how and 
 where an accessor will be used by the program. We showed the simplest 
 way to use accessors using the default access modes and targets, and we 
 learned how to distinguish between a placeholder accessor and one that is 
 not. We then saw how to further optimize the example program by giving 
 the runtime more information about our accessor usage by adding access 
 tags to our accessor declarations. Finally, we covered many of the different 
 ways that accessors can be used in a program.
  
 In the next chapter, we will learn in greater detail how the runtime can 
 use the information we give it through accessors to schedule the execution 
 of different kernels. We will also see how this information informs the 
 runtime about when and how the data in buffers needs to be copied 
 between the host and a device. We will learn how we can explicitly control 
 data movement involving buffers—and USM allocations too.
  
 192",NA
CHAPTER 8,NA,NA
Scheduling ,NA,NA
Kernels and Data ,NA,NA
Movement,"We need to discuss our role as the concert master for our parallel 
  
 programs. The proper orchestration of a parallel program is a thing of 
 beauty—code running full speed without waiting for data, because we 
 have arranged for all data to arrive and depart at the proper times. Code 
 well-decomposed to keep the hardware maximally busy. It is the thing that 
 dreams are made of!
  
 Life in the fast lanes—not just one lane!—demands that we take 
 our work as the conductor seriously. In order to do that, we can think 
 of our job in terms of task graphs.",NA
 What Is Graph Scheduling?,"In Chapter 
 3
 , we discussed data management and ordering the uses of 
 data. That chapter described the key abstraction behind graphs in DPC++: 
 dependences. Dependences between kernels are fundamentally based on 
 what data a kernel accesses. A kernel needs to be certain that it reads the 
 correct data before it can compute its output.
  
 We described the three types of data dependences that are important 
 for ensuring correct execution. The first, Read-after-Write (RAW), occurs 
 when one task needs to read data produced by a different task. This type 
 of dependence describes the flow of data between two kernels. The second 
 type of dependence happens when one task needs to update data after 
 another task has read it. We call that type of dependence a Write-after- 
 Read (WAR) dependence. The final type of data dependence occurs when 
 two tasks try to write the same data. This is known as a Write-after-Write 
 (WAW) dependence.
  
 196",NA
 How Graphs Work in DPC++,"A command group can contain three different things: an action, its 
  
 dependences, and miscellaneous host code. Of these three things, the one 
 that is always required is the action since without it, the command group 
 really doesn’t do anything. Most command groups will also express 
 dependences, but there are cases where they may not. One such example 
 is the first action submitted in a program. It does not depend on anything 
 to begin execution; therefore, we would not specify any dependence. The 
 other thing that can appear inside a command group is arbitrary C++ code 
 that executes on the host. This is perfectly legal and can be useful to help 
 specify the action or its dependences, and this code is executed while the 
 command group is created (not later when the action is performed based 
 on dependences having been met).
  
 Command groups are typically expressed as a C++ lambda expression 
 passed to the submit method. Command groups can also be expressed 
 through shortcut methods on queue objects that take a kernel and set of 
 event-based dependences.
  
 197",NA
 Command Group Actions,"There are two types of actions that may be performed by a command 
 group: kernels and explicit memory operations. A command group may 
 only perform a single action. As we’ve seen in earlier chapters, kernels are 
 defined through calls to a parallel_for or single_task method and express 
 computations that we want to perform on our devices. Operations for 
 explicit data movement are the second type of action. Examples from USM 
 include memcpy, memset, and fill operations. Examples from buffers 
 include copy, fill, and update_host.",NA
 How Command Groups Declare ,NA,NA
Dependences,"The other main component of a command group is the set of dependences 
 that must be satisfied before the action defined by the group can execute. 
 DPC++ allows these dependences to be specified in several ways.
  
 If a program uses in-order DPC++ queues, the in-order semantics of the 
 queue specify implicit dependences between successively enqueued 
 command groups. One task cannot execute until the previously submitted 
 task has completed.
  
 Event-based dependences are another way to specify what must be 
 complete before a command group may execute. These event-based 
 dependences may be specified in two ways. The first way is used when a 
 command group is specified as a lambda passed to a queue’s submit 
 method. In this case, the programmer invokes the depends_on method of 
 the command group handler object, passing either an event or vector of 
 events as parameter. The other way is used when a command group is 
 created from the shortcut methods defined on the queue object. When the 
 programmer directly invokes parallel_for or single_task on a queue, an 
 event or vector of events may be passed as an extra parameter.
  
 198",NA
 Examples,"Now we will illustrate everything we’ve just learned with several 
 examples. We will present how one might express two different 
 dependence patterns in several ways. The two patterns we will illustrate 
 are linear dependence chains where one task executes after another and a 
 “Y” pattern where two independent tasks must execute before successive 
 tasks.
  
 Graphs for these dependence patterns can be seen in Figures 
 8-1 
 and 
 8-2
 . Figure 
 8-1
  depicts a linear dependence chain. The first node 
 represents the initialization of data, while the second node presents the 
 reduction operation that will accumulate the data into a single result. 
  
 Figure 
 8-2
  depicts a “Y” pattern where we independently initialize two 
 different pieces of data. After the data is initialized, an addition kernel 
 will sum the two vectors together. Finally, the last node in the graph 
 accumulates the result into a single value.
  
  
 Figure 8-1. 
 Linear dependence chain graph",NA
 When Are the Parts of a CG Executed?,"Since task graphs are asynchronous, it makes sense to wonder when 
 exactly command groups are executed. By now, it should be clear that 
 kernels may be executed as soon as their dependences have been satisfied, 
 but what happens with the host portion of a command group?
  
 When a command group is submitted to a queue, it is executed 
  
 immediately on the host (before the submit call returns). This host portion 
 of the command group is executed only once. Any kernel or explicit data 
 operation defined in the command group is enqueued for execution on the 
 device.",NA
 Data Movement,"Data movement is another very important aspect of graphs in DPC++ that 
 is essential for understanding application performance. However, it can 
 often be accidentally overlooked if data movement happens implicitly in 
 a program, either using buffers and accessors or using USM shared 
 allocations. Next, we will examine the different ways that data movement 
 can affect graph execution in DPC++.
  
 206",NA
 Explicit,"Explicit data movement has the advantage that it appears 
 explicitly
  in a 
 graph, making it obvious for programmers what goes on within execution 
 of a graph. We will separate explicit data operations into those for USM 
 and those for buffers.
  
 As we learned in Chapter 
 6
 , explicit data movement in USM occurs 
 when we need to copy data between device allocations and the host. This 
 is done with the memcpy method, found in both the queue and handler 
 classes. Submitting the action or command group returns an event that 
 can be used to order the copy with other command groups.
  
 Explicit data movement with buffers occurs by invoking either the 
 copy or update_host method of the command group handler object. The 
 copy method can be used to manually exchange data between host 
 memory and an accessor object on a device. This can be done for a variety 
 of reasons. A simple example is checkpointing a long-running sequence of 
 computations. With the copy method, data can be written from the device 
 to arbitrary host memory in a one-way fashion. If this were done using 
 buffers, most cases (i.e., those where the buffer was not created with 
 use_host_ptr) would require the data to first be copied to the host and 
 then from the buffer’s memory to the desired host memory.
  
 The update_host method is a very specialized form of copy. If a buffer 
 was created around a host pointer, this method will copy the data 
 represented by the accessor back to the original host memory. This can be 
 useful if a program manually synchronizes host data with a buffer that was 
 created with the special use_mutex property. However, this use case is not 
 likely to occur in most programs.
  
 207",NA
 Implicit,"Implicit data movement can have hidden consequences for command 
 groups and task graphs in DPC++. With implicit data movement, data is 
 copied between host and device either by the DPC++ runtime or by some 
 combination of hardware and software. In either case, copying occurs 
 without explicit input from the user. Let’s again look separately at the USM 
 and buffer cases.
  
 With USM, implicit data movement occurs with host and shared 
 allocations. As we learned in Chapter 
 6
 , host allocations do not really move 
 data so much as access it remotely, and shared allocations 
  
 may migrate between host and device. Since this migration happens 
 automatically, there is really nothing to think about with USM implicit data 
 movement and command groups. However, there are some nuances with 
 shared allocations worth keeping in mind.
  
 The prefetch operation works in a similar fashion to memcpy in order 
 to let the runtime begin migrating shared allocations before a kernel 
 attempts to use them. However, unlike memcpy where data must be 
 copied in order to ensure correct results, prefetches are often treated as 
 hints
  to the runtime to increase performance, and prefetches do not 
 invalidate pointer values in memory (as a copy would when copying to a 
 new address range). The program will still execute correctly if a prefetch 
 has not completed before a kernel begins executing, and so many codes 
 may choose to make command groups in a graph not depend on prefetch 
 operations since they are not a functional requirement.
  
 Buffers also carry some nuance. When using buffers, command groups 
 must construct accessors for buffers that specify how the data will be used. 
 These data dependences express the ordering between different command 
 groups and allow us to construct task graphs. However, command groups 
 with buffers sometimes fill another purpose: they specify the requirements 
 on data movement.
  
 208",NA
 Synchronizing with the Host,"The last topic we will discuss is how to synchronize graph execution with 
 the host. We have already touched on this throughout the chapter, but we 
 will now examine all the different ways a program can do this.
  
 The first method for host synchronization is one we’ve used in many of 
 our previous examples: waiting on a queue. Queue objects have two 
 methods, wait and wait_and_throw, that block execution until every 
 command group that was submitted to the queue has completed. This is a 
 very simple method that handles many common cases. However, it is 
 worth pointing out that this method is very coarse-grained. If finer-grained 
 synchronization is desired, one of the other approaches we will discuss 
 may be better suit an application’s needs.
  
 209",NA
 Summary,"In this chapter, we have learned about graphs and how they are built, 
 scheduled, and executed in DPC++. We went into detail on what command 
 groups are and what function they serve. We discussed the three things 
 that can be within a command group: dependences, an action, and 
  
 miscellaneous host code. We reviewed how to specify dependences 
  
 between tasks using events as well as through data dependences described 
 by accessors. We learned that the single action in a command group may 
 be either a kernel or an explicit memory operation, and we then looked at 
 several examples that showed the different ways we can construct 
 common execution graph patterns. Next, we reviewed how data movement 
 is an important part of DPC++ graphs, and we learned how it can appear 
 either explicitly or implicitly in a graph. Finally, we looked at all the ways 
 to synchronize the execution of a graph with the host.
  
 Understanding the program flow can enable us to understand the sort 
 of debug information that can be printed if we have runtime failures to 
 debug. Chapter 
 13
  has a table in the section “Debugging Runtime Failures” 
  
 211",NA
CHAPTER 9,NA,NA
Communication ,NA,NA
and ,NA,NA
Synchronization,"In Chapter 
 4
 , we discussed ways to express parallelism, either using 
 basic data-parallel kernels, explicit ND-range kernels, or hierarchical 
 parallel kernels. We discussed how basic data-parallel kernels apply the 
 same operation to every piece of data independently. We also discussed 
 how explicit ND-range kernels and hierarchical parallel kernels divide 
 the execution range into work-groups of work-items.
  
 In this chapter, we will revisit the question of how to break up a 
  
 problem into bite-sized chunks in our continuing quest to 
 Think Parallel
 . 
 This chapter provides more detail regarding explicit ND- range kernels 
 and hierarchical parallel kernels and describes how groupings of work-
 items may be used to improve the performance of some types of 
 algorithms. We will describe how groups of work-items provide additional 
 guarantees for",NA
 Work-Groups and Work-Items,"Recall from Chapter 
 4
  that explicit ND-range and hierarchical parallel 
 kernels organize work-items into work-groups and that the work-items in 
 a work-group are guaranteed to execute concurrently. This property is 
 important, because when work-items are guaranteed to execute 
 concurrently, the work-items in a work-group can cooperate to solve a 
 problem.
  
  
 Figure 9-1. 
 Two-dimensional ND-range of size (8, 8) divided into 
 four work-groups of size (4,4)
  
 Figure 
 9-1
  shows an ND-range divided into work-groups, where each 
 work-group is represented by a different color. The work-items in each 
 work-group are guaranteed to execute concurrently, so a work-item may 
 communicate with other work-items that share the same color.",NA
 Building Blocks for Efficient ,NA,NA
Communication,"This section describes building blocks that support efficient 
  
 communication between work-items in a group. Some are fundamental 
 building blocks that enable construction of custom algorithms, whereas 
 others are higher level and describe common operations used by many 
 kernels.",NA
 Synchronization via Barriers,"The most fundamental building block for communication is the 
 barrier 
 function. The barrier function serves two key purposes: 
  
  
 First, the barrier function synchronizes execution of work-items in a 
 group. By synchronizing execution, one work-item can ensure that 
 another work-item has completed an operation before using the result of 
 that operation. Alternatively, one work-item is given time to complete its 
 operation before another work-item uses the result of the operation.
  
 Second, the barrier function synchronizes how each work-item views 
 the state of memory. This type of synchronization operation is known as 
 enforcing 
 memory consistency
  or 
 fencing
  memory (more details in Chapter 
 19
 ). Memory consistency is at least as important as synchronizing 
 execution since it ensures that the results of memory operations",NA
 Work-Group Local Memory,"The work-group barrier function is sufficient to coordinate communication 
 among work-items in a work-group, but the communication itself must 
 occur through memory. Communication may occur through either",NA
Communication between work-items in a work-group can ,NA,NA
be more convenient and faster when using local memory!,"We can use the device query info::device::local_mem_type to 
 determine whether an accelerator has dedicated resources for local 
 memory or whether local memory is implemented as a software 
 abstraction of global memory. Please refer to Chapter 
 12
  for more 
 information about querying properties of a device and to Chapters 
 15
 , 
 16
 , and 
 17
  for more information about how local memory is typically 
 implemented for CPUs, GPUs, and FPGAs.",NA
 Using Work-Group Barriers and ,NA,NA
Local Memory,"Now that we have identified the basic building blocks for efficient 
 communication between work-items, we can describe how to express 
 work-group barriers and local memory in kernels. Remember that 
 communication between work-items requires a notion of work-item 
 grouping, so these concepts can only be expressed for ND-range kernels 
 and hierarchical kernels and are not included in the execution model for 
 basic data-parallel kernels.
  
 219",NA
"For many algorithms, it is helpful to think of local ",NA,NA
memory as an explicit cache.,"Figure 
 9-5
  is a modified diagram from Chapter 
 4
  showing a work-
 group consisting of a single row, which makes the algorithm using local 
 memory easier to understand. Observe that for elements in a row of the 
 result matrix, every result element is computed using a unique column of 
 data from one of the input matrices, shown in blue and orange. Because 
 there is no data sharing for this input matrix, it is not an ideal candidate for 
 local memory. Observe, though, that every result element in the row 
 accesses the exact same data in the other input matrix, shown in green. 
 Because this data is reused, it is an excellent candidate to benefit from 
 work-group local memory.
  
 221",NA
 Work-Group Barriers and Local ,NA,NA
Memory in ND- Range Kernels,"This section describes how work-group barriers and local memory are 
 expressed in ND-range kernels. For ND-range kernels, the representation 
 is explicit: a kernel declares and operates on a local accessor 
 representing an allocation in the local address space and calls a barrier 
 function to synchronize the work-items in a work-group.",NA
 Local Accessors,"To declare local memory for use in an ND-range kernel, use a 
 local 
  
 accessor
 . Like other accessor objects, a local accessor is constructed within 
 a command group handler, but unlike the accessor objects discussed in 
 Chapters 
 3
  and 
 7
 , a local accessor is not created from a buffer object. 
  
 Instead, a local accessor is created by specifying a type and a range 
 describing the number of elements of that type. Like other accessors, 
 local accessors may be one-dimensional, two-dimensional, or three- 
 dimensional. Figure 
 9-7
  demonstrates how to declare local accessors and 
 use them in a kernel.
  
 Remember that local memory is uninitialized when each work-group 
 begins and does not persist after each work-group completes. This means 
 that a local accessor must always be read_write, since otherwise a kernel 
 would have no way to assign the contents of local memory or view the 
 results of an assignment. Local accessors may optionally be atomic though, 
 in which case accesses to local memory via the accessor are performed 
 atomically. Atomic accesses are discussed in more detail in Chapter 
 19
 .
  
 223",NA
 Synchronization Functions,"To synchronize the work-items in an ND-range kernel work-group, call 
 the barrier function in the nd_item class. Because the barrier function is a 
 member of the nd_item class, it is only available to ND-range kernels and 
 is not available to basic data-parallel kernels or hierarchical kernels.
  
 The barrier function currently accepts one argument to describe the 
 memory spaces to synchronize or 
 fence
 , but the arguments to the barrier 
 function may change in the future as the memory model evolves in SYCL 
 and DPC++. In all cases though, the arguments to the barrier function 
 provide additional control regarding the memory spaces that are 
 synchronized or the 
 scope
  of the memory synchronization.
  
 When no arguments are passed to the barrier function, the barrier 
 function will use functionally correct and conservative defaults. The 
 code examples in this chapter use this syntax for maximum portability 
 and readability. For highly optimized kernels, it is recommended to 
 precisely describe which memory spaces or which work-items must be 
 synchronized, which may improve performance.
  
 224",NA
 A Full ND-Range Kernel Example,"Now that we know how to declare a local memory accessor and 
  
 synchronize access to it using a barrier function, we can implement 
  
 an ND-range kernel version of matrix multiplication that coordinates 
  
 communication among work-items in the work-group to reduce traffic to 
  
 global memory. The complete example is shown in Figure 
 9-8
 .
  
 // Traditional accessors, representing matrices in global memory: 
 accessor
  matrixA{bufA, 
 h}; 
  
 accessor
  matrixB{bufB, h}; 
  
 accessor
  matrixC{bufC, h};
  
 // Local accessor, for one matrix tile: 
 constexpr int
  
 tile_size = 
 16
 ; 
  
 local_accessor<
 int
 > tileA{tile_size, h};
  
 h
 .
 parallel_for
 ( 
  
  
 nd_range
 <
 2
 >{{M, N}, {
 1
 , tile_size}}, [=](
 nd_item
 <
 2
 > 
 item
 ) { 
   
 // Indices in 
 the global index space: 
  
   
 int
  m = 
 item
 .
 get_global_id
 ()[
 0
 ]; 
  
   
 int
  n = 
 item
 .
 get_global_id
 ()[
 1
 ];
  
 // Index in the local index space: 
  
 int
  i = 
 item
 .
 get_local_id
 ()[
 1
 ];
  
 T sum = 
 0
 ; 
  
 for
  (
 int
  kk = 
 0
 ; kk < K; kk += tile_size) { 
  
 // Load the matrix tile from matrix A, and synchronize // to ensure all work-
 items have a consistent view // of the matrix tile in local memory.
  
 tileA
 [i] = 
 matrixA
 [m][kk + i]; 
  
 item
 .
 barrier
 ();
  
 // Perform computation using the local memory tile, and // matrix B in 
 global memory.
  
 for
  (
 int
  k = 
 0
 ; k < tile_size; k++) 
  
  
 sum += 
 tileA
 [k] * 
 matrixB
 [kk + k][n];
  
 });
  
 }
  
 // After computation, synchronize again, to ensure all // reads from the 
 local memory tile are complete.
  
 item
 .
 barrier
 ();
  
 // Write the final result to global memory. 
 matrixC
 [m][n] = 
 sum;
  
 Figure 9-8. 
 Expressing a tiled matrix multiplication kernel with an 
  
 ND-range parallel_for and work-group local memory
  
 225",NA
 Work-Group Barriers and Local ,NA,NA
Memory in Hierarchical Kernels,"This section describes how work-group barriers and local memory are 
 expressed in hierarchical kernels. Unlike ND-range kernels, local memory 
 and barriers in hierarchical kernels are implicit, requiring no special 
 syntax or function calls. Some programmers will find the hierarchical 
 kernel representation more intuitive and easier to use, whereas other 
 programmers will appreciate the direct control provided by ND-range 
 kernels. In most cases, the same algorithms may be described using both 
 representations, so we can choose the representation that we find easiest 
 to develop and maintain.
  
 226",NA
 Scopes for Local Memory and Barriers,"Recall from Chapter 
 4
  that hierarchical kernels express two levels of 
  
 parallel execution through use of the parallel_for_work_group and 
  
 parallel_for_work_item functions. These two levels, or scopes, of parallel 
  
 execution are used to express whether a variable is in work-group local 
  
 memory and shared across all work-items in the work-group or whether 
  
 a variable is in per-work-item private memory that is not shared among 
  
 work-items. The two scopes are also used to synchronize the work-items 
 in 
  
 a work-group and to enforce memory consistency.
  
 Figure 
 9-9
  shows an example hierarchical kernel that declares a 
  
 variable at work-group scope in local memory, loads into it, and then uses 
  
 that variable in work-item scope. There is an implicit barrier between 
  
 the write into local memory at work-group scope and the read from local 
  
 memory at work-item scope.
  
 range
  group_size{
 16
 }; 
  
 range
  num_groups = size / group_size;
  
 h
 .
 parallel_for_work_group
 (num_groups, group_size, [=](
 group
 <
 1
 > 
 group
 ) { 
 // This variable is 
 declared at work-group scope, so 
  
 // it is allocated in local memory and accessible to 
  
 // all work-items.
  
 int
  localIntArr
 [
 16
 ];
  
 // There is an implicit barrier between code and variables // declared at work-
 group scope and the code and variables // at work-item scope.
  
 group
 .
 parallel_for_work_item
 ([&](
 h_item
 <
 1
 > 
 item
 ) { 
 auto
  index = 
 item
 .
 get_global_id
 (); 
  
 auto
  local_index = 
 item
 .
 get_local_id
 ();
  
 // The code at work-item scope can read and write the // variables declared 
 at work-group scope.
  
   
 localIntArr
 [local_index] = index + 
 1
 ; 
  
   
 data_acc
 [index] = 
 localIntArr
 [local_index]; 
  
 }); 
  
 });
  
 Figure 9-9. 
 Hierarchical kernel with a local memory variable
  
 227",NA
 A Full Hierarchical Kernel Example,"Now that we know how to express local memory and barriers in 
  
 hierarchical kernels, we can write a hierarchical kernel that implements 
 the same algorithm as the ND-range kernel in Figure 
 9-7
 . This kernel is 
 shown in Figure 
 9-10
 .
  
 Although the hierarchical kernel is very similar to the ND-range kernel, 
 there is one key difference: in the ND-range kernel, the results of the 
 matrix multiplication are accumulated into the per-work-item variable 
 sum before writing to the output matrix in memory, whereas the 
 hierarchical kernel accumulates into memory. We could accumulate into a 
 per-work- item variable in the hierarchical kernel as well, but this requires 
 a special private_memory syntax to declare per-work-item data at work-
 group scope, and one of the reasons we chose to use the hierarchical 
 kernel syntax was to avoid special syntax!",NA
hierarchical kernels do not require special syntax to declare ,NA,NA
"variables in work-group local memory, but they require ",NA,NA
special syntax to declare some variables in work-item ,NA,NA
private memory!,"To avoid the special per-work-item data syntax, it is a common pattern 
 for work-item loops in hierarchical kernels to write intermediate results to 
 either work-group local memory or global memory.
  
 228",NA
 Sub-Groups,"So far in this chapter, work-items have communicated with other work- 
 items in the work-group by exchanging data through work-group local 
 memory and by synchronizing via implicit or explicit barrier functions, 
 depending on how the kernel is written.
  
 In Chapter 
 4
 , we discussed another grouping of work-items. A sub- 
 group is an implementation-defined subset of work-items in a work-group 
 that execute together on the same hardware resources or with additional 
 scheduling guarantees. Because the implementation decides how to group 
 work-items into sub-groups, the work-items in a sub-group may be able to 
 communicate or synchronize more efficiently than the work-items in an 
 arbitrary work-group.
  
 This section describes the building blocks for communication 
 among work-items in a sub-group. Note that sub-groups are currently 
 implemented only for ND-range kernels and sub-groups are not 
 expressible through hierarchical kernels.",NA
 Synchronization via Sub-Group Barriers,"Just like how the work-items in a work-group in an ND-range kernel may 
 synchronize using a work-group barrier function, the work-items in a 
 sub-group may synchronize using a sub-group barrier function. Whereas 
 the work-items in a work-group synchronize by calling a group_barrier 
 function or the barrier function in the nd_item class, the work-items in a 
 sub-group synchronize by calling a group_barrier function or the barrier 
 function in a special sub_group class that may be queried from the 
 nd_item class, as shown in Figure 
 9-11
 .
  
 230",NA
 Exchanging Data Within a Sub-Group,"Unlike work-groups, sub-groups do not have a dedicated memory space 
 for exchanging data. Instead, work-items in the sub-group may exchange 
 data through work-group local memory, through global memory, or more 
 commonly by using sub-group 
 collective functions
 .
  
 As described previously, a 
 collective function
  is a function that 
 describes an operation performed by a group of work-items, not an 
 individual work-item, and because a barrier synchronization function is 
 an operation performed by a group of work-items, it is one example of a 
 collective function.
  
 Other collective functions express common communication patterns. 
  
 We will describe the semantics for many collective functions in detail 
 later in this chapter, but for now, we will briefly describe the broadcast 
 collective function that we will use to implement matrix multiplication 
 using sub-groups.
  
  
 The broadcast collective function takes a value from one work-item 
 in the group and communicates it to all other work-items in the group. 
  
 An example is shown in Figure 
 9-12
 . Notice that the semantics of the 
 broadcast function require that the local_id identifying which value in",NA
 A Full Sub-Group ND-Range Kernel ,NA,NA
Example,"Figure 
 9-14
  is a complete example that implements matrix multiplication 
  
 using sub-groups. Notice that this kernel requires no work-group local 
  
 memory or explicit synchronization and instead uses a sub-group 
  
 broadcast collective function to communicate the contents of the matrix 
  
 tile among work-items.
  
 // Note: This example assumes that the sub-group size is // greater than or equal 
 to the tile size!
  
 static const int
  tileSize = 
 4
 ;
  
 h
 .
 parallel_for
 ( 
  
  
 nd_range
 <
 2
 >{{M, N}, {
 1
 , tileSize}}, [=](
 nd_item
 <
 2
 > 
 item
 ) { 
   
 auto
  sg = 
 item
 .
 get_sub_group
 ();
  
 // Indices in the global index space: 
 int
  m = 
 item
 .
 get_global_id
 ()[
 0
 ]; 
  
 int
  n = 
 item
 .
 get_global_id
 ()[
 1
 ];
  
 // Index in the local index space: 
  
 int
  i = 
 item
 .
 get_local_id
 ()[
 1
 ];
  
 T sum = 
 0
 ; 
  
 for
  (
 int_fast64_t
  kk = 
 0
 ; kk < K; kk += tileSize) { 
 // Load the matrix tile 
 from matrix A.
  
 T tileA = 
 matrixA
 [m][kk + i];
  
 // Perform computation by broadcasting from the matrix 
  
 // tile and loading from matrix B in global memory.  The loop // variable k describes 
 which work-item in the sub-group to // broadcast data from.
  
 for
  (
 int
  k = 
 0
 ; k < tileSize; k++) 
  
   
 sum += 
 intel
 ::
 broadcast
 (sg, tileA, k) * 
 matrixB
 [kk + k][n]; }
  
   
 // Write the final result to global memory. 
  
  
 matrixC
 [m][n] = sum; 
  
  
 }); 
  
 });
  
 Figure 9-14. 
 Tiled matrix multiplication kernel expressed with ND- 
  
 range parallel_for and sub-group collective functions
  
 233",NA
 Collective Functions,"In the “Sub-Groups” section of this chapter, we described collective 
 functions and how collective functions express common communication 
 patterns. We specifically discussed the broadcast collective function, 
 which is used to communicate a value from one work-item in a group to 
 the other work-items in the group. This section describes additional 
 collective functions.
  
 Although the collective functions described in this section can be 
 implemented directly in our programs using features such as atomics, 
 work-group local memory, and barriers, many devices include dedicated 
 hardware to accelerate collective functions. Even when a device does not 
 include specialized hardware, vendor-provided implementations of 
 collective functions are likely tuned for the device they are running on, 
 so calling a built-in collective function will usually perform better than a 
 general-purpose implementation that we might write.",NA
use collective functions for common communication ,NA,NA
patterns to simplify code and increase performance!,"Many collective functions are supported for both work-groups and 
 sub-groups. Other collective functions are supported only for sub-
 groups.",NA
 Broadcast,"The broadcast function enables one work-item in a group to share the 
 value of a variable with all other work-items in the group. A diagram 
 showing how the broadcast function works can be found in Figure 
 9-12
 . 
  
 The broadcast function is supported for both work-groups and  sub- 
 groups.
  
 234",NA
 Votes,"The any_of and all_of functions (henceforth referred to collectively as 
 “vote” functions) enable work-items to compare the result of a Boolean 
 condition across their group: any_of returns true if the condition is true for 
 at least one work-item in the group, and all_of returns true only if the 
 condition is true for all work-items in the group. A comparison of these 
 two functions for an example input is shown in Figure 
 9-15
 .
  
  
  
  
 Figure 9-15. 
 Comparison of the any_of and all_of functions
  
  
 The any_of and all_of vote functions are supported for both work- 
 groups and sub-groups.",NA
 Shuffles,"One of the most useful features of sub-groups is the ability to communicate 
 directly between individual work-items without explicit memory 
  
 operations. In many cases, such as the sub-group matrix multiplication 
 kernel, these 
 shuffle
  operations enable us to remove work-group local 
 memory usage from our kernels and/or to avoid unnecessary repeated 
 accesses to global memory. There are several flavors of these shuffle 
 functions available.
  
 The most general of the shuffle functions is called shuffle, and as 
 shown in Figure 
 9-16
 , it allows for arbitrary communication between any 
 pair of work-items in the sub-group. This generality may come at a 
 performance cost, however, and we strongly encourage making use of the 
 more specialized shuffle functions wherever possible.
  
 235",NA
 Loads and Stores,"The sub-group load and store functions serve two purposes: first, 
  
 informing the compiler that all work-items in the sub-group are loading 
 contiguous data starting from the same (uniform) location in memory and, 
 second, enabling us to request optimized loads/stores of large amounts of 
 contiguous data.
  
  
 For an ND-range parallel_for, it may not be clear to the compiler 
 how addresses computed by different work-items relate to one another. 
  
 For example, as shown in Figure 
 9-20
 , accessing a contiguous block of",NA
 Summary,"This chapter discussed how work-items in a group may communicate and 
 cooperate to improve the performance of some types of kernels.
  
 We first discussed how ND-range kernels and hierarchical kernels 
 support grouping work-items into work-groups. We discussed how 
 grouping work-items into work-groups changes the parallel execution 
 model, guaranteeing that the work-items in a work-group execute 
 concurrently and enabling communication and synchronization.
  
 Next, we discussed how the work-items in a work-group may 
  
 synchronize using barriers and how barriers are expressed explicitly for 
 ND-range kernels or implicitly between work-group and work-item scopes 
 for",NA
CHAPTER 10,NA,NA
Defining Kernels,"Thus far in this book, our code examples have represented kernels 
  
 using C++ lambda expressions. Lambda expressions are a concise and 
 convenient way to represent a kernel right where it is used, but they are 
 not the only way to represent a kernel in SYCL. In this chapter, we will 
 explore various ways to define kernels in detail, helping us to choose a 
 kernel form that is most natural for our C++ coding needs.
  
 This chapter explains and compares three ways to represent a kernel:
  
 • Lambda expressions
  
 • Named function objects (functors)
  
 • Interoperability with kernels created via other 
 languages or APIs
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_10
  
 241",NA
 Why Three Ways to Represent a ,NA,NA
Kernel?,"Before we dive into the details, let’s start with a summary of why there 
 are three ways to define a kernel and the advantages and disadvantages 
 of each method. A useful summary is given in Figure 
 10-1
 .
  
 Bear in mind that a kernel is used to express a unit of computation 
 and that many instances of a kernel will usually execute in parallel on an 
 accelerator. SYCL supports multiple ways to express a kernel to integrate 
 naturally and seamlessly into a variety of codebases while executing 
 efficiently on a wide diversity of accelerator types.",NA
 Kernels As Lambda Expressions,"C++ lambda expressions, also referred to as 
 anonymous function objects
 , 
 unnamed function objects
 , 
 closures
 , or simply 
 lambdas
 , are a convenient 
 way to express a kernel right where it is used. This section describes how 
 to represent a kernel as a C++ lambda expression. This expands on the 
 introductory refresher on C++ lambda functions, in Chapter 
 1
 , which 
 included some coding samples with output.
  
 C++ lambda expressions are very powerful and have an expressive 
 syntax, but only a specific subset of the full C++ lambda expression syntax 
 is required (and supported) when expressing a kernel.
  
 h
 .
 parallel_for
 (size, 
  
  
 // This is the start of a kernel lambda expression: 
 [=](
 id
 <
 1
 > 
 i
 ) { 
  
   
 data_acc
 [i] = 
 data_acc
 [i] + 
 1
 ; 
  
  
 } 
  
  
 // This is the end of the kernel lambda expression.
  
 );
  
 Figure 10-2. 
 Kernel defined using a lambda expression",NA
 Elements of a Kernel Lambda Expression,"Figure 
 10-2
  shows a kernel written as a typical lambda expression—the 
 code examples so far in this book have used this syntax.
  
 The illustration in Figure 
 10-3
  shows more elements of a lambda 
 expression that may be used with kernels, but many of these elements are 
 not typical. In most cases, the lambda defaults are sufficient, so a typical 
 kernel lambda expression looks more like the lambda expression in 
 Figure 
 10-2
  than the more complicated lambda expression in Figure 
 10-3
 .
  
 244",NA
 Naming Kernel Lambda Expressions,"There is one more element that must be provided in some cases when a 
 kernel is written as a lambda expression: because lambda expressions are 
 anonymous
 , at times SYCL requires an explicit kernel name template 
 parameter to uniquely identify a kernel written as a lambda expression.
  
 // In this example, ""class Add"" names the kernel lambda:
  
 h
 .
 parallel_for
 <
 class
  Add
 >(size, [=](
 id
 <
 1
 > 
 i
 ) { 
  
 data_acc
 [i] = 
 data_acc
 [i] + 
 1
 ; 
  
 });
  
 Figure 10-4. 
 Naming kernel lambda expressions
  
 Naming a kernel lambda expression is a way for a host code compiler 
 to identify which kernel to invoke when the kernel was compiled by a 
 separate device code compiler. Naming a kernel lambda also enables 
 runtime introspection of a compiled kernel or building a kernel by name, 
 as shown in Figure 
 10-9
 .
  
 247",NA
When the kernel name template parameter is not ,NA,NA
"required, using unnamed kernel lambdas is preferred to ",NA,NA
reduce verbosity.,NA,NA
 Kernels As Named Function Objects,"Named function objects, also known as 
 functors
 , are an established pattern 
 in C++ that allows operating on an arbitrary collection of data while 
 maintaining a well-defined interface. When used to represent a kernel, the 
 member variables of a named function object define the state that the 
 kernel may operate on, and the overloaded function call operator() is 
 invoked for each work-item in the parallel execution space.
  
 248",NA
 Elements of a Kernel Named Function ,NA,NA
Object,"The code in Figure 
 10-6
  describes the elements of a kernel represented as 
 a named function object.",NA
 Interoperability with Other APIs,"When a SYCL implementation is built on top of another API, the 
  
 implementation may be able to interoperate with kernels defined using 
 mechanisms of the underlying API. This allows an application to easily and 
 incrementally integrate SYCL into existing codebases.
  
 Because a SYCL implementation may be layered on top of many other 
 APIs, the functionality described in this section is optional and may not 
 be supported by all implementations. The underlying API may even differ 
 depending on the specific device type or device vendor!
  
 Broadly speaking, an implementation may support two 
 interoperability mechanisms: from an API-defined source or intermediate 
 representation (IR) or from an API-specific handle. Of these two 
 mechanisms, the 
  
 ability to create a kernel from an API-defined source or intermediate 
 representation is more portable, since some source or IR formats are 
 supported by multiple APIs. For example, OpenCL C kernels may be 
 directly consumed by many APIs or may be compiled into some form 
 understood by an API, but it is unlikely that an API-specific kernel handle 
 from one API will be understood by a different API.",NA
remember that all forms of interoperability are optional!,NA,NA
Different sYCl implementations may support creating ,NA,NA
kernels from different api-specific handles—or not at all.,NA,NA
always check the documentation for details!,NA,NA
 Interoperability with API-Defined ,NA,NA
Source Languages,"With this form of interoperability, the contents of the kernel are described 
 as source code or using an intermediate representation that is not defined 
 by SYCL, but the kernel objects are still created using SYCL API calls. This 
 form of interoperability allows reuse of kernel libraries written in other 
 source languages or use of domain-specific languages (DSLs) that generate 
 code in an intermediate representation.
  
 An implementation must understand the kernel source code or 
 intermediate representation to utilize this form of interoperability. For 
 example, if the kernel is written using OpenCL C in source form, the 
 implementation must support building SYCL programs from OpenCL C 
 kernel source code.
  
  
 Figure 
 10-7
  shows how a SYCL kernel may be written as OpenCL C 
 kernel source code.
  
 // Note: This must select a device that supports interop! 
 queue
  Q{ cpu_selector{} 
 };
  
 program p{
 Q
 .
 get_context
 ()};
  
 p
 .
 build_with_source
 (
 R""CLC( 
  
  
  
 kernel void add(global int* data) { 
  
  
  
 int index = get_global_id(0); 
   
  
 data[index] = data[index] + 1; 
  
  
 } 
  
 )CLC""
 , 
  
 ""-cl-fast-relaxed-math""
 );
  
 std
 ::cout << 
 ""Running on device: "" 
  
  
 << 
 Q
 .
 get_device
 ().
 get_info
 <
 info
 ::
 device
 ::name>() << 
 ""
 \n
 ""
 ;
  
 Q
 .
 submit
 ([&](
 handler&
  h
 ) { 
  
  
 accessor
  data_acc {data_buf, h};
  
  
 h
 .
 set_args
 (data_acc); 
  
  
 h
 .
 parallel_for
 (size, 
 p
 .
 get_kernel
 (
 ""add""
 )); });
  
 Figure 10-7. 
 Kernel created from OpenCL C kernel source
  
 252",NA
 Interoperability with API-Defined Kernel ,NA,NA
Objects,"With this form of interoperability, the kernel objects themselves are 
 created in another API and then imported into SYCL. This form of 
 interoperability enables one part of an application to directly create 
 and use kernel objects using underlying APIs and another part of the 
 application to reuse the same kernels using SYCL APIs. The code in 
 Figure 
 10-8
  shows how a SYCL kernel may be created from an OpenCL 
 kernel object.",NA
 Kernels in Program Objects,"In prior sections, when kernels were either created from an API-defined 
 representation or from API-specific handles, the kernels were created in 
 two steps: first by creating a 
 program object
  and then by creating the 
 kernel from the program object. A program object is a collection of kernels 
 and the functions they call that are compiled as a unit.
  
 For kernels represented as lambda expressions or named function 
 objects, the program object containing the kernel is usually implicit and 
 invisible to an application. For applications that require more control, an 
 application can explicitly manage kernels and the program objects that 
 encapsulate them. To describe why this may be beneficial, it is helpful to 
 take a brief look at how many SYCL implementations manage just-in-time 
 (JIT) kernel compilation.
  
 While not required by the specification, many implementations 
 compile kernels “lazily.” This is usually a good policy since it ensures fast 
 application startup and does not unnecessarily compile kernels that are 
 never executed. The disadvantage of this policy is that the first use of a 
 kernel usually takes longer than subsequent uses, since it includes the time 
 needed to compile the kernel, plus the time needed to submit and execute 
 the kernel. For some complex kernels, the time needed to compile the 
 kernel can be significant, making it desirable to shift compilation to a 
 different point during application execution, such as when the application 
 is loading, or in a separate background thread.
  
 Some kernels may also benefit from implementation-defined “build 
 options” to precisely control how the kernel is compiled. For example, for 
 some implementations, it may be possible to instruct the kernel compiler 
 to use a math library with lower precision and better performance.
  
 To provide more control over when and how a kernel is compiled, an 
 application can explicitly request that a kernel be compiled before the 
 kernel is used, using specific build options. Then, the pre-compiled kernel 
 can be submitted into a queue for execution, like usual. Figure 
 10-9
  shows 
 how this works.",NA
 Summary,"In this chapter, we explored different ways to define kernels. We described 
 how to seamlessly integrate into existing C++ codebases by representing 
 kernels as C++ lambda expressions or named function objects. For new 
 codebases, we also discussed the pros and cons of the different kernel 
 representations, to help choose the best way to define kernels based on the 
 needs of the application or library.
  
 We also described how to interoperate with other APIs, either by 
 creating a kernel from an API-defined source language or intermediate 
 representation or by creating a kernel object from a handle to an API 
 representation of the kernel. Interoperability enables an application to 
 migrate from lower-level APIs to SYCL over time or to interface with 
 libraries written for other APIs.",NA
CHAPTER 11,NA,NA
Vectors,"Vectors are collections of data. These can be useful because parallelism in 
 our computers comes from collections of compute hardware, and data is 
 often processed in related groupings (e.g., the color channels in an RGB 
 pixel). Sound like a marriage made in heaven? It is so important, we’ll 
 spend a chapter discussing the merits of vector types and how to utilize 
 them. We will not dive into 
 vectorization
  in this chapter, since that varies 
 based on device type and implementations. Vectorization is covered in 
 Chapters 
 15
  and 
 16
 .
  
 This chapter seeks to address the following questions:
  
 • What are vector types?
  
 • How much do I really need to know about the vector 
 interface?
  
 • 
  
 Should vector types be used to express parallelism?
  
 259
  
 • 
  
 When should I use vector types?
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_11",NA
 How to Think About Vectors,"Vectors are a surprisingly controversial topic when we talk with parallel 
 programming experts, and in the authors’ experience, this is because 
 different people define and think about the term in different ways. 
  
  
 There are two broad ways to think about vector data types (a 
 collection of data):
  
  1. 
 As a convenience type
 , which groups data that you 
 might want to refer to and operate on as a group, for 
 example, grouping the color channels of a pixel (e.g., 
 RGB, YUV) into a single variable (e.g., float3), which 
 could be a vector. We could define a pixel class or 
 struct and define math operators like + on it, but 
 vector types conveniently do this for us out of the 
 box. Convenience types can be found in many shader 
 languages used to program GPUs, so this way of 
 thinking is already common among many GPU 
 developers.
  
  2. As a mechanism to describe how code 
 maps to a SIMD 
 instruction set
  in hardware. For example, in some 
 languages and implementations, operations on a 
 float8 could in theory map to an eight-lane SIMD 
 instruction in hardware. Vector types are used in 
 multiple languages as a convenient high-level 
 alternative to CPU-specific SIMD intrinsics for 
 specific instruction sets, so this way of thinking is 
 already common among many CPU developers.
  
 260",NA
 Vector Types,"Vector types in SYCL are cross-platform class templates that work 
  
 efficiently on devices as well as in host C++ code and allow sharing of 
 vectors between the host and its devices. Vector types include methods 
 that allow construction of a new vector from a swizzled set of component 
 elements, meaning that elements of the new vector can be picked in an 
 arbitrary order from elements of the old vector. vec is a vector type that 
 compiles down to the built-in vector types on target device backends, 
 where possible, and provides compatible support on the host.
  
  
 The vec class is templated on its number of elements and its element 
 type. The number of elements parameter, numElements, can be one of 1,",NA
 Vector Interface,"The functionality of vector types is exposed through the class vec. The 
  
 vec class represents a set of data elements that are grouped together. The 
 interfaces of the constructors, member functions, and non-member 
 functions of the vec class template are described in Figures 
 11-1
 , 
 11-4
 , and 
 11-5
 .
  
 The XYZW members listed in Figure 
 11-2
  are available only when 
 numElements <= 4. RGBA members are available only when numElements 
 == 4.
  
  
 The members lo, hi, odd, and even shown in Figure 
 11-3
  are available 
 only when numElements > 1.
  
 264",NA
 Load and Store Member Functions,"Vector load and store operations are members of the vec class for 
 loading and storing the elements of a vector. These operations can be to 
 or from an array of elements of the same type as the channels of the 
 vector. An example is shown in Figure 
 11-6
 .
  
 267",NA
 Swizzle Operations,"In 
 graphics
  applications, 
 swizzling
  means rearranging the data elements of 
 a vector. For example, if a = {1, 2, 3, 4,}, and knowing that the components 
 of a four-element vector can be referred to as {x, y, z, w}, we could write b 
 = a.wxyz(). The result in the variable b would be {4, 1, 2, 3}. This form of 
 code is common in 
 GPU
  applications where there is efficient hardware for 
 such operations. Swizzles can be performed in two ways:
  
 • By calling the swizzle member function of a vec, which 
 takes a variadic number of integer template arguments 
 between 0 and numElements-1, specifying swizzle indices
  
 • By calling one of the simple swizzle member functions 
 such as XYZW_SWIZZLE and RGBA_SWIZZLE
  
 Note that the simple swizzle functions are only available for up to four-
 element vectors and are only available when the macro SYCL_SIMPLE_ 
 SWIZZLES is defined before including sycl.hpp. In both cases, the return 
 type is always an instance of __swizzled_vec__, an implementation- defined 
 temporary class representing a swizzle of the original vec instance. 
  
 Both the swizzle member function template and the simple swizzle 
 member functions allow swizzle indexes to be repeated. Figure 
 11-7
  shows 
 a simple usage of __swizzled_vec__.
  
 269",NA
 Vector Execution Within a Parallel ,NA,NA
Kernel,"As described in Chapters 
 4
  and 
 9
 , a work-item is the leaf node of 
  
 the parallelism hierarchy and represents an individual instance of a 
  
 kernel function. Work-items can be executed in any order and cannot 
  
 communicate or synchronize with each other except through atomic 
  
 memory operations to local and global memory or through group 
  
 collective functions (e.g., shuffle, barrier).
  
 As described at the start of this chapter, a vector in DPC++ should be 
  
 interpreted as a convenience for us when writing code. Each vector is local 
  
 to a single work-item (instead of relating to vectorization in hardware) 
 and",NA
 Vector Parallelism,"Although vectors in source code within DPC++ should be interpreted as 
 convenience tools that are local to only a single work-item, this chapter on 
 vectors would not be complete without some mention of how SIMD 
 instructions in hardware operate. This discussion is not coupled to vectors 
 within our source code, but provides orthogonal background that will be 
 useful as we progress to the later chapters of this book that describe 
 specific device types (GPU, CPU, FPGA).
  
 Modern CPUs and GPUs contain SIMD instruction hardware that 
 operate on multiple data values contained in one vector register or a 
 register file. For example, with Intel x86 AVX-512 and other modern CPU 
 SIMD hardware, SIMD instructions can be used to exploit data parallelism. 
 On CPUs and GPUs that provide SIMD hardware, we can consider a vector 
 addition operation, for example, on an eight-element vector, as shown in 
 Figure 
 11-13
 .
  
  
  
  
  
  
  
  
  
  
  
 Figure 11-13. 
 SIMD addition with eight-way data parallelism
  
 The vector addition in this example could execute in a single 
 instruction on vector hardware, adding the vector registers vec_x and 
 vec_y in parallel with that SIMD instruction.
  
 Exposing potential parallelism in a hardware-agnostic way ensures 
 that our applications can scale up (or down) to fit the capabilities of 
 different platforms, including those with vector hardware instructions. 
  
 274",NA
 Summary,"There are multiple interpretations of the term 
 vector
  within programming 
 languages, and understanding the interpretation that a particular language 
 or compiler has been built around is important when we want to write 
 performant and scalable code. DPC++ and the DPC++ compiler have been 
 built around the idea that vectors in source code are convenience functions 
 local to a work-item and that implicit vectorization by the 
  
 compiler across work-items may map to SIMD instructions in the 
  
 hardware. When we want to write code which maps directly to vector 
 hardware explicitly, we should look to vendor documentation and future 
 extensions to SYCL and DPC++. Writing our kernels using multiple 
  
 work-items (e.g., ND-range) and relying on the compiler to vectorize 
 across work-items should be how most applications are written because 
 doing so leverages the powerful abstraction of SPMD, which provides an 
 easy-to-reason-about programming model, and that provides scalable 
 performance across devices and architectures.
  
 This chapter has described the vec interface, which offers convenience 
 out of the box when we have groupings of similarly typed data that we 
 want to operate on (e.g., a pixel with multiple color channels). It has also 
 touched briefly on SIMD instructions in hardware, to prepare us for more 
 detailed discussions in Chapters 
 15
  and 
 16
 .
  
 275",NA
CHAPTER 12,NA,NA
Device ,NA,NA
Information,"Chapter 
 2
  introduced us to the mechanisms that direct work to a 
 particular device—controlling 
 where code executes.
  In this chapter, we 
 explore how to adapt to the devices that are present at runtime.
  
 We want our programs to be portable. In order to be portable, we need 
 our programs to adapt to the capabilities of the device. We can 
 parameterize our programs to only use features that are present and to 
 tune our code to the particulars of devices. If our program is not designed 
 to adapt, then bad things can happen including slow execution or program 
 failures.
  
 Fortunately, the creators of the SYCL specification thought about this 
 and gave us interfaces to let us solve this problem. The SYCL specification 
 defines a device class that encapsulates a device on which kernels may be 
 executed. The ability to query the device class, so that our program can 
 adapt to the device characteristics and capabilities, is the heart of what 
 this chapter teaches.",NA
"Parameterizing a program can help with correctness, ",NA,NA
"portability, performance portability, and future ",NA,NA
proofing.,"This chapter dives into the most important queries and how to use 
 them effectively in our programs.
  
 Device-specific properties are queryable using get_info, but DPC++ 
 diverges from SYCL 1.2.1 in that it fully overloads get_info to alleviate the 
 need to use get_work_group_info for work-group information that is really 
 device-specific information. DPC++ does not support use of 
 get_work_group_info. This change means that device-specific kernel and 
 work-group properties are properly found as queries for device-specific 
 properties (get_info). This corrects a confusing historical anomaly still 
 present in SYCL 1.2.1 that was inherited from OpenCL.",NA
 Refining Kernel Code to Be ,NA,NA
More Prescriptive,"It is useful to consider that our coding, kernel by kernel, will fall 
 broadly into one of three categories:
  
 • Generic kernel code: Run anywhere, not tuned to a 
 specific class of device.
  
 • Device type–specific kernel code: Run on a type of 
 device (e.g., GPU, CPU, FPGA), not tuned to specific 
 models
  of a device type. This is very useful because 
  
 278",NA
it is our job as programmers to determine when different ,NA,NA
patterns (Chapter ,NA,NA
14,NA,NA
) are needed for different device ,NA,NA
types. We dedicate Chapters ,NA,NA
14,NA,NA
", ",NA,NA
15,NA,NA
", ",NA,NA
16,NA,NA
", and ",NA,NA
17,NA,NA
 to ,NA,NA
illuminating this important thinking.,"It is most common to start by implementing generic kernel code to get 
 it working. Chapter 
 2
  specifically talks about what methods are easiest to 
 debug when getting started with a kernel implementation. Once we have a 
 kernel working, we may evolve it to target the capabilities of a specific 
 device type or device model.
  
 Chapter 
 14
  offers a framework of thinking to consider parallelism 
 first, before we dive into device considerations. It is our choice of pattern 
 (aka algorithm) that dictates our code, and it is our job as programmers 
 to determine when different patterns are needed for different devices. 
  
 Chapters 
 15
  (GPU), 
 16
  (CPU), and 
 17
  (FPGA) dive more deeply into the 
 qualities that distinguish these device types and motivate a choice in 
 pattern to use. It is these qualities that motivate us to consider writing 
 distinct versions of kernels for different devices when the approaches 
 (pattern choice) on different device types differ.
  
 When we have a kernel written for a specific type of device (e.g., a 
 specific CPU, GPU, FPGA, etc.), it is logical to adapt it to specific vendors or 
 even models of such devices. Good coding style is to parameterize code 
 based on features (e.g., item size support found from a device query).",NA
Parameterizing makes the most sense when the algorithm is ,NA,NA
broadly the same but has been tuned for the capabilities of a ,NA,NA
specific device. Writing a different kernel is much cleaner ,NA,NA
"when using a completely different approach, pattern, or ",NA,NA
algorithm.,NA,NA
 How to Enumerate Devices and ,NA,NA
Capabilities,"Chapter 
 2
  enumerates and explains five methods for choosing a device on 
 which to execute. Essentially, Method#1 was the least prescriptive 
 run it 
 somewhere
 , and we evolve to the most prescriptive Method#5 which 
 considered executing on a fairly precise model of a device from a family of 
 devices. The enumerated methods in between gave a mix of flexibility and 
 prescriptiveness. Figures 
 12-1
 , 
 12-2
 , and 
 12-3
  help to illustrate how we 
 can select a device.",NA
 Custom Device Selector,"Figure 
 12-3
  uses a custom device selector. Custom device selectors were 
 first discussed in Chapter 
 2
  as Method#5 for choosing where our code 
 runs (Figure 
 2-15
 ). The custom device selector causes its operator(), 
 shown in Figure 
 12-3
 , to be invoked for each device available to the 
 application. The device selected is the one that receives the highest score.
 1
  
 In this example, we will have a little fun with our selector:
  
 1
  If our device selector returned only negative values, then the my_selector() 
 would throw a runtime_error exception as expected on non-GPU systems in 
 Figure 
 12-2
 . Since we return a positive value for the host, that cannot happen in 
 Figure 
 12-3
 .
  
 281",NA
Queries about devices rely on installed software (special ,NA,NA
"user-level drivers), to respond regarding a device. SYCL and ",NA,NA
"DPC++ rely on this, just as an operating system needs drivers ",NA,NA
to access hardware—it is not sufficient that the hardware ,NA,NA
simply be installed in a machine.,282,NA
 Being Curious: get_info<>,"In order for our program to “know” what devices are available at runtime, 
 we can have our program query available devices from the device class, 
 and then we can learn more details using get_info<> to inquire about a 
 specific device. We provide a simple program, called 
 curious
  (see 
  
 Figure 
 12-4
 ), that uses these interfaces to dump out information for us to 
 look at directly. This can be very useful for doing a sanity check when 
 developing or debugging a program that uses these interfaces. Failure of 
 this program to work as expected can often tell us that the software 
 drivers we need are not installed correctly. Figure 
 12-5
  shows sample 
 output from this program, with the high-level information about the 
 devices that are present.
  
 // Loop through available platforms 
  
 for
  (
 auto const
 & this_platform : 
 platform
 ::
 get_platforms
 () ) { 
  
 std
 ::cout << 
 ""Found 
 platform: "" 
  
   
 << 
 this_platform
 .
 get_info
 <
 info
 ::
 platform
 ::name>() << 
 ""
 \n
 ""
 ;
  
 }
  
 // Loop through available devices in this platform 
  
 for
  (
 auto const
 & this_device : 
 this_platform
 .
 get_devices
 () ) { 
  
 std
 ::cout << 
 "" Device: "" 
  
  
 << 
 this_device
 .
 get_info
 <
 info
 ::
 device
 ::name>() << 
 ""
 \n
 ""
 ; } 
  
 std
 ::cout << 
 ""
 \n
 ""
 ;
  
 Figure 12-4. 
 Simple use of device query mechanisms: curious.cpp
  
 285",NA
 Being More Curious: Detailed ,NA,NA
Enumeration Code,"We offer a program, which we have named verycurious.cpp (Figure 
 12-6
 ), 
  
 to illustrate some of the detailed information available using get_info<>. 
  
 Again, we find ourselves writing code like this to help when developing or 
  
 debugging a program. Figure 
 12-5
  shows sample output from this 
 program, 
  
 with the lower-level information about the devices that are present.
  
 Now that we have shown how to access the information, we will 
  
 discuss the information fields that prove the most important to query and 
  
 act upon in applications.",NA
 Inquisitive: get_info<>,"The has_extension() interface allows a program to test directly for 
  
 a feature, rather than having to walk through a list of extensions from 
 get_info <info::platform::extensions> as printed out by the previous code 
 examples. The SYCL 2020 provisional specification has defined new 
 mechanisms to query extensions and detailed aspects of devices, but we 
 don't cover those features (which are just being finalized) in this book. 
 Consult the 
 online oneAPI DPC++ language reference
  for more information.",NA
 Device Information Descriptors,"Our “curious” program examples, used earlier in this chapter, utilize the 
 most used SYCL device class member functions (i.e., is_host, is_cpu, 
 is_gpu, is_accelerator, get_info, has_extension). These member functions 
 are documented in the SYCL specification in a table titled 
  
 “Member functions of the SYCL device class” (in SYCL 1.2.1, it is Table 
 4.18).
  
 The “curious” program examples also queried for information using 
 the get_info member function. There is a set of queries that must be 
 supported by all SYCL devices, including a host device. The complete list of 
 such items is described in the SYCL specification in a table titled “Device 
 information descriptors” (in SYCL 1.2.1, it is Table 4.20).",NA
 Device-Specific Kernel ,NA,NA
Information Descriptors,"Like platforms and devices, we can query information about our kernels 
 using a get_info function. Such information (e.g., supported work-group 
 sizes, preferred work-group size, the amount of private memory required 
 per work-item) is device-specific, and so the get_info member function of 
 the kernel class accepts a device as an argument.
  
 288",NA
 The Specifics: Those of “Correctness”,"We will divide the specifics into information about necessary conditions 
 (correctness) and information useful for tuning but not necessary for 
 correctness.
  
 In this first correctness category, we will enumerate conditions that 
 should be met in order for kernels to launch properly. Failure to abide by 
 these device limitations will lead to program failures. Figure 
 12-7
  shows 
 how we can fetch a few of these parameters in a way that the values are 
 available for use in host code and in kernel code (via lambda capture). We 
 can modify our code to utilize this information; for instance, it could guide 
 our code on buffer sizing or work-group sizing.",NA
Submitting a kernel that does not satisfy these ,NA,NA
conditions will generate an error.,289,NA
 Device Queries,"device_type: cpu, gpu, accelerator, custom,
 2
 automatic, host, all. These are 
 most often tested by is_host(), is_cpu, is_gpu(), and so on (see Figure 
 12-
 6
 ):
  
 max_work_item_sizes: The maximum number of 
 work-items that are permitted in each dimension 
 of the work-group of the nd_range. The minimum 
 value is (1, 1, 1) for non-custom devices.
  
 max_work_group_size: The maximum number of 
 work-items that are permitted in a work-group 
 executing a kernel on a single compute unit. The 
 minimum value is 1.
  
 global_mem_size: The size of global memory in 
 bytes.
  
 local_mem_size: The size of local memory in bytes. 
 Except for custom devices, the minimum size is 32 K.
  
 2
  Custom devices are not discussed in this book. If we find ourselves 
 programming a device that identifies itself using the 
 custom
  type, we will need to 
 study the documentation for that device to learn more.
  
 290",NA
We advise avoiding max_compute_units in program logic.,291,NA
 Kernel Queries,"The mechanisms discussed in Chapter 
 10
 , under “Kernels in Program 
 Objects,” are needed to perform these kernel queries:
  
 work_group_size: Returns the maximum work- 
 group size that can be used to execute a kernel on a 
 specific device
  
 compile_work_group_size: Returns the work-group 
 size specified by a kernel if applicable; otherwise 
 returns (0, 0, 0)
  
 compile_sub_group_size: Returns the sub-group 
 size specified by a kernel if applicable; otherwise 
 returns 0
  
 compile_num_sub_groups: Returns the number 
 of sub-groups specified by a kernel if applicable; 
 otherwise returns 0
  
 max_sub_group_size: Returns the maximum sub- 
 group size for a kernel launched with the specified 
 work-group size
  
 max_num_sub_groups: Returns the maximum 
 number of sub-groups for a kernel
  
 292",NA
 The Specifics: Those of ,NA,NA
“Tuning/ Optimization”,"There are a few additional parameters that can be considered as fine- 
 tuning parameters for our kernels. These can be ignored without 
 jeopardizing the correctness of a program. These allow our kernels to 
 really utilize the particulars of the hardware for performance.",NA
Paying attention to the results of these queries can help ,NA,NA
when tuning for a cache (if it exists).,NA,NA
 Device Queries,"global_mem_cache_line_size: Size of global 
 memory cache line in bytes.
  
 global_mem_cache_size: Size of global memory 
 cache in bytes.
  
 local_mem_type: The type of local memory 
  
 supported. This can be info::local_mem_ 
  
 type::local implying dedicated local memory storage 
 such as SRAM or info::local_mem_ 
  
 type::global. The latter type means that local 
 memory is just implemented as an abstraction on 
 top of global memory with no performance gains. 
 For custom devices (only), the local memory type 
 can also be info::local_mem_type::none, indicating 
 local memory is not supported.
  
 293",NA
 Kernel Queries,"preferred_work_group_size: The preferred 
  
 work-group size for executing a kernel on a specific 
 device.
  
 preferred_work_group_size_multiple: The 
  
 preferred work-group size for executing a kernel 
 on a specific device",NA
 Runtime vs. Compile-Time ,NA,NA
Properties,"The queries described in this chapter are performed through runtime 
 APIs (get_info), meaning that the results are not known until 
  
 runtime. This covers many use cases, but the SYCL specification is also 
 undergoing work to provide compile-time querying of properties, when 
 they can be known by a toolchain, to allow more advanced programming 
 techniques such as templating of kernels based on properties of devices. 
 Compile- time adaptation of code based on queries is not possible with 
 the existing runtime queries, and this ability can be important for 
 advanced optimizations or when writing kernels that use some 
 extensions. The interfaces were not defined well enough yet at the time of 
 writing to describe those interfaces in this book, but we can look forward 
 to much more powerful query and code adaptation mechanisms that are 
 coming soon in SYCL and DPC++! 
  
 Look to the online oneAPI DPC++ language reference and the SYCL 
 specifications for updates.
  
 294",NA
 Summary,"The most portable programs will query the devices that are available in a 
 system and adjust their behavior based on runtime information. This 
 chapter opens the door to the rich set of information that is available to 
 allow such tailoring of our code to adjust to the hardware that is present at 
 runtime.
  
 Our programs can be made more portable, more performance 
 portable, and more future-proof by parameterizing our application to 
 adjust to the characteristics of the hardware. We can also test that the 
 hardware present falls within the bounds of any assumptions we have 
 made in the design of our program and either warn or abort when 
 hardware is found that lies outside the bounds of our assumptions.
  
  
 Open Access
  This chapter is licensed under the terms of 
 the Creative Commons Attribution 4.0 International 
  
 License (
 http://creativecommons.org/licenses/by/4.0/
 ), which permits 
 use, sharing, adaptation, distribution and reproduction in any medium or 
 format, as long as you give appropriate credit to the original author(s) and 
 the source, provide a link to the Creative Commons license and indicate if 
 changes were made.
  
 The images or other third party material in this chapter are included 
 in the chapter’s Creative Commons license, unless indicated otherwise in 
 a credit line to the material. If material is not included in the chapter’s 
 Creative Commons license and your intended use is not permitted by 
 statutory regulation or exceeds the permitted use, you will need to 
 obtain permission directly from the copyright holder.
  
 295",NA
CHAPTER 13,NA,NA
Practical Tips,"This chapter is home to a number of pieces of useful information, practical 
 tips, advice, and techniques that have proven useful when programming 
 SYCL and using DPC++. None of these topics are covered exhaustively, so 
 the intent is to raise awareness and encourage learning more as needed.",NA
 Getting a DPC++ Compiler and ,NA,NA
Code Samples,"Chapter 
 1
  covers how to get the DPC++ compiler (
 oneapi.com/ 
  
 implementations
  or 
 github.com/intel/llvm
 ) and where to get the code 
 samples (
 www.apress.com/9781484255735
 —look for Services for this 
 book: Source Code). This is mentioned again to emphasize how useful 
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_13
  
 297",NA
 Online Forum and Documentation,"The Intel Developer Zone hosts a forum for discussing the DPC++ 
  
 compiler, DPC++ Library (Chapter 
 18
 ), DPC++ Compatibility Tool (for 
 CUDA migration—discussed later in this chapter), and gdb included in the 
 oneAPI toolkit (this chapter touches on debugging too). This is an 
 excellent place to post questions about writing code, including suspected 
 compiler bugs. You will find posts from some of the book authors on this 
 forum doing exactly that, especially while writing this book. The forum is 
 available online at 
 https://software.intel.com/en-us/forums/oneapi-
 data- parallel-c-compiler
 .
  
 The online 
 oneAPI DPC++ language reference
  is a great resource to 
 find a complete list of the classes and member definitions, details on 
 compiler options, and more.",NA
 Platform Model,"A SYCL or DPC++ compiler is designed to act and feel like any other C++ 
 compiler we have ever used. A notable difference is that a regular C++ 
 compiler produces code only for a CPU. It is worth understanding the 
 inner workings, at a high level, that enable a compiler to produce code for 
 a host CPU 
 and
  devices.
  
 The platform model (Figure 
 13-1
 ), used by SYCL and DPC++, specifies 
 a host that coordinates and controls the compute work that is performed 
 on the devices. Chapter 
 2
  describes how to assign work to devices, and 
 Chapter 
 4
  dives into how to program devices. Chapter 
 12
  describes using 
 the platform model at various levels of specificity.
  
 298",NA
 Multiarchitecture Binaries,"Since our goal is to have a single-source code to support a heterogeneous 
 machine, it is only natural to want a single executable file to be the result.
  
 A multiarchitecture binary (aka a 
 fat binary
 ) is a single binary file that 
 has been expanded to include all the compiled and intermediate code 
 needed for our heterogeneous machine. The concept of multiarchitecture 
 binaries is not new. For example, some operating systems support 
  
 multiarchitecture 32-bit and 64-bit libraries and executables. A 
  
 multiarchitecture binary acts like any other a.out or A.exe we are used 
 to—but it contains everything needed for a heterogeneous machine. 
  
 This helps to automate the process of picking the right code to run for a 
 particular device. As we discuss next, one possible form of the device code 
 in a fat binary is an intermediate format that defers the final creation of 
 device instructions until runtime.",NA
 Compilation Model,"The single-source nature of SYCL and DPC++ allows compilations to act 
 and feel like regular C++ compilations. There is no need for us to invoke 
 additional passes for devices or deal with bundling device and host code. 
 That is all handled automatically for us by the compiler. Of course, 
 understanding the details of what is happening can be important for 
 several reasons. This is useful knowledge if we want to target specific 
 architectures more effectively, and it is important to understand if we 
 need to debug a failure happening in the compilation process.
  
 We will review the compilation model so that we are educated for 
 when that knowledge is needed. Since the compilation model supports 
 code that executes on both a host and potentially several devices 
 simultaneously, the commands issued by the compiler, linker, and other 
 supporting tools are more complicated than the C++ compilations we are 
 used to (targeting only one architecture). Welcome to the heterogeneous 
 world!",NA
DpC++ compilation can be “ahead-of-time” or “just-in-time.”,"By default, when we compile our code for most devices, the output for 
 device code is stored in an intermediate form. At runtime, the device 
 handler on the system will 
 just-in-time
  compile the intermediate form into 
 code to run on the device(s) to match what is available on the system.
  
 We can ask the compiler to compile ahead-of-time for specific devices 
 or classes of devices. This has the advantage of saving runtime, but it has 
 the disadvantage of added compile time and fatter binaries! Code that is 
 compiled ahead-of-time is not as portable as just-in-time because it cannot 
 adjust at runtime. We can include both in our binary to get the benefits of 
 both.
  
 Compiling for a specific device ahead-of-time also helps us to check at 
 build time that our program should work on that device. With just-in-time 
 compilation, it is possible that a program will fail to compile at runtime 
 (which can be caught using the mechanisms in Chapter 
 5
 ). There are a few 
 debugging tips for this in the upcoming “Debugging” section of this 
 chapter, and Chapter 
 5
  details how these errors can be caught at runtime 
 to avoid requiring that our applications abort.
  
 301",NA
...,"a.o
  
 Bundler
  
 a_fat.o
  
 Unbundler
  
 a.o",NA
...,"b.o
  
 a.IR
  
 b.o
  
 a.IR
  
 Bundler
  
 b_fat.o
  
 Unbundler
  
 a.IR
  
 b.IR
  
 b.IR
  
 b.IR
  
  
 Compile to object
  
  
 Link to executable
  
  
 Figure 13-3. 
 Compilation process: Offload bundler/unbundler",NA
 Adding SYCL to Existing C++ ,NA,NA
Programs,"Adding the appropriate exploitation of parallelism to an existing C++ 
 program is the first step to using SYCL. If a C++ application is already 
 exploiting parallel execution, that may be a bonus, or it may be a headache. 
 That is because the way we divide the work of an application into parallel 
 execution greatly affects what we can do with it. When programmers talk",NA
 Debugging,"This section conveys some modest debugging advice, to ease the 
 challenges unique to debugging a parallel program, especially one 
 targeting a heterogeneous machine.
  
 We should never forget that we have the option to debug our 
  
 applications while they are running on the host device. This debugging 
 tip is described as Method#2 in Chapter 
 2
 . Because the architectures of 
 devices often include fewer debugging hooks, tools can often probe code 
 on a host more precisely. Another advantage of running 
 everything
  on 
 the host is that many errors relating to synchronization will disappear, 
 including moving memory back and forth between the host and devices. 
  
 While we eventually need to debug all such errors, this can allow 
 incremental debugging so we can resolve some bugs before others.",NA
Debugging tip,NA,NA
 running on the host device is a powerful ,NA,NA
debugging tool.,"Parallel programming errors, specifically data races and deadlocks, are 
 generally easier for tools to detect and eliminate when running all code on 
 the host. Much to our chagrin, we will most often see program failures 
 from such parallel programming errors when running on a combination of 
 host and devices. When such issues strike, it is very useful to remember 
 that pulling back to host-only is a powerful debugging tool. Thankfully, 
 SYCL and DPC++ are carefully designed to keep this option available to us 
 and easy to access.
  
 305",NA
Debugging tip,NA,NA
" if a program is deadlocking, check that the ",NA,NA
host accessors are being destroyed properly.,"The following DPC++ compiler options are a good idea when we start 
 debugging:
  
 • -g: Put debug information in the output.
  
 • -ferror-limit=1: Maintain sanity when using C++ with 
 template libraries such as SYCL/DPC++.
  
 • -Werror -Wall -Wpedantic: Have the compiler 
  
 enforce good coding to help avoid producing incorrect 
 code to debug at runtime.
  
  
 We really do not need to get bogged down fixing pedantic warnings 
 just to use DPC++, so choosing to not use -Wpedantic is understandable.
  
 When we leave our code to be compiled just-in-time during runtime, 
 there is code we can inspect. This is 
 highly dependent
  on the layers used by 
 our compiler, so looking at the compiler documentation for suggestions is 
 a good idea.",NA
 Debugging Kernel Code,"While debugging kernel code, start by running on the host device (as 
 advised in Chapter 
 2
 ). The code for device selectors in Chapter 
 2
  can easily 
 be modified to accept runtime options, or compiler-time options, to 
 redirect work to the host device when we are debugging.
  
 When debugging kernel code, SYCL defines a C++-style stream that 
 can be used within a kernel (Figure 
 13-4
 ). DPC++ also offers an 
 experimental implementation of a C-style printf that has useful 
 capabilities, with some restrictions. Additional details are in the online 
 oneAPI DPC++ language reference
 .
  
 306",NA
 Debugging Runtime Failures,"When a runtime error occurs while compiling just-in-time, we are either 
 dealing with a compiler/runtime bug, or we have accidentally 
 programmed nonsense that was not detected until it tripped up the 
 runtime and created difficult-to-understand runtime error messages. 
  
 It can be a bit intimidating to dive into these bugs, but even a cursory look 
 may allow us to get a better idea of what caused a particular issue. It might 
 yield some additional knowledge that will guide us to avoid the issue, or it 
 may just help us submit a short bug report to the compiler team. Either 
 way, knowing that some tools exist to help can be important.
  
 307",NA
Debugging tip,NA,NA
  When other options are exhausted and we ,NA,NA
"need to debug a runtime issue, we look for dump tools that ",NA,NA
might give us hints toward the cause.,309,NA
 Initializing Data and Accessing ,NA,NA
Kernel Outputs,"In this section, we dive into a topic that causes confusion for new users of 
 SYCL and that leads to the most common (in our experience) first bugs 
 that we encounter as new SYCL developers.
  
 Put simply, when we create a buffer from a host memory allocation 
 (e.g., array or vector), we can’t access the host allocation directly until the 
 buffer has been destroyed. The buffer owns any host allocation passed to 
  
 310",NA
"if we construct a buffer from a host memory allocation, ",NA,NA
we must not directly access the host allocation until the ,NA,NA
"buffer has been destroyed! While it is alive, the buffer ",NA,NA
owns the allocation.,"A common bug appears when the host program accesses a host 
  
 allocation while a buffer still owns that allocation. All bets are off once this 
 happens, because we don’t know what the buffer is using the allocation for. 
 Don’t be surprised if the data is incorrect—the kernels that we’re trying to 
 read the output from may not have even started running yet! As described 
 in Chapters 
 3
  and 
 8
 , SYCL is built around an asynchronous task graph 
 mechanism. Before we try to use output data from task graph operations, 
 we need to be sure that we have reached synchronization points in the 
 code where the graph has executed and made data available to the host. 
 Both buffer destruction and creation of host accessors are operations that 
 cause this synchronization.
  
 Figure 
 13-6
  shows a common pattern of code that we often write, 
 where we cause a buffer to be destroyed by closing the block scope that it 
 was defined within. By causing the buffer to go out of scope and be 
 destroyed, we can then safely read kernel results through the original host 
 allocation that was passed to the buffer constructor.",NA
advanced users may prefer to use buffer destruction to ,NA,NA
return result data from kernels into a host memory ,NA,NA
"allocation. But for most users, and especially new ",NA,NA
"developers, it is recommended to use scoped host accessors.",313,NA
"prefer to use host accessors instead of scoping of buffers, ",NA,NA
especially ,NA,NA
when getting started.,"To avoid these bugs, we recommend using host accessors instead of 
  
 buffer scoping when getting started with SYCL and DPC++. Host accessors 
  
 provide access to a buffer from the host, and once their constructor has 
  
 314",NA
"When using host accessors, be sure that they are destroyed ",NA,NA
when no longer needed to unlock use of the buffer by ,NA,NA
kernels or other host accessors.,317,NA
 Multiple Translation Units,"When we want to call functions inside a kernel that are defined in a 
 different translational unit, those functions need to be labeled with SYCL_ 
 EXTERNAL. Without this attribute, the compiler will only compile a 
 function for use outside of device code (making it illegal to call that 
 external 
  
 function from within device code).
  
  
 There are a few restrictions on SYCL_EXTERNAL functions that do 
 not apply if we define the function within the same translation unit:
  
 • SYCL_EXTERNAL can only be used on functions.
  
 • SYCL_EXTERNAL functions cannot use raw pointers as 
 parameter or return types. Explicit pointer classes must be 
 used instead.
  
 • SYCL_EXTERNAL functions cannot call a parallel_for_ 
 work_item method.
  
 • SYCL_EXTERNAL functions cannot be called from within 
 a parallel_for_work_group scope.
  
 If we try to compile a kernel that is calling a function that is not inside 
 the same translation unit and is not declared with SYCL_EXTERNAL, then 
 we can expect a compile error similar to
  
 error: SYCL kernel cannot call an undefined function without 
 SYCL_EXTERNAL attribute
  
  
 If the function itself is compiled without a SYCL_EXTERNAL attribute, 
 we can expect to see either a link or runtime failure such as
  
 terminate called after throwing an instance of 
 'cl::sycl::compile_program_error' 
  
 ...error: undefined reference to ...
  
 319",NA
 Performance Implications of Multiple ,NA,NA
Translation Units,"An implication of the compilation model (see earlier in this chapter) is that 
 if we scatter our device code into multiple translation units, that may 
 trigger more invocations of just-in-time compilation than if our device code 
 is co-located. This is highly implementation dependent and is subject to 
 changes over time as implementations mature.
  
 Such effects on performance are minor enough to ignore through 
 most of our development work, but when we get to fine-tuning to 
 maximize code performance, there are two things we can consider to 
 mitigate these effects: (1) group device code together in the same 
 translation unit, and (2) use ahead-of-time compilation to avoid just-in-
 time compilation effects entirely. Since both of these require some effort 
 on our part, we only do this when we have finished our development 
 and are trying to squeeze every ounce of performance out of our 
 application. When we do resort to this detailed tuning, it is worth 
 testing changes to observe their effect on the exact SYCL 
 implementation that we are using.",NA
 When Anonymous Lambdas Need ,NA,NA
Names,"SYCL provides for assigning names defined as lambdas in case tools need it 
 and for debugging purposes (e.g., to enable displays in terms of user-
 defined names). Throughout most of this book, anonymous lambdas have 
 been used for kernels because names are not needed when using DPC++ 
 (except for passing of compile options as described with lambda naming",NA
 Migrating from CUDA to SYCL,"Migrating CUDA code to SYCL or DPC++ is not covered in detail in this 
 book. There are tools and resources available that explore doing this. 
 Migrating CUDA code is relatively straightforward since it is a kernel-based 
 approach to parallelism. Once written in SYCL or DPC++, the new program 
 is enhanced by its ability to target more devices than supported by CUDA 
 alone. The newly enhanced program can still be targeted to NVIDIA GPUs 
 using SYCL compilers with NVIDIA GPU support.
  
  
 Migrating to SYCL opens the door to the diversity of devices supported 
 by SYCL, which extends far beyond just GPUs.
  
 When using the DPC++ Compatibility Tool, the --report-type=
 value 
 option provides very useful statistics about the migrated code. One of the 
 reviewers of this book called it a “beautiful flag provided by Intel dpct.” 
 The --in-root option can prove very useful when migrating CUDA code 
 depending on source code organization of a project.
  
 321",NA
 Summary,"Popular culture today often refers to tips as 
 life hacks
 . Unfortunately, 
  
 programming culture often assigns a negative connotation to 
 hack
 , so the 
 authors refrained from naming this chapter “SYCL Hacks.” Undoubtedly, 
 this chapter has just touched the surface of what practical tips can be 
 given for using SYCL and DPC++. More tips can be shared by all of us on 
 the 
 online forum
  as we learn together how to make the most out of SYCL 
 with DPC++.
  
  
 Open Access
  This chapter is licensed under the terms of 
 the Creative Commons Attribution 4.0 International 
  
 License (
 http://creativecommons.org/licenses/by/4.0/
 ), which permits 
 use, sharing, adaptation, distribution and reproduction in any medium or 
 format, as long as you give appropriate credit to the original author(s) and 
 the source, provide a link to the Creative Commons license and indicate if 
 changes were made.
  
 The images or other third party material in this chapter are included 
 in the chapter’s Creative Commons license, unless indicated otherwise in 
 a credit line to the material. If material is not included in the chapter’s 
 Creative Commons license and your intended use is not permitted by 
 statutory regulation or exceeds the permitted use, you will need to 
 obtain permission directly from the copyright holder.",NA
CHAPTER 14,NA,NA
Common ,NA,NA
Parallel ,NA,NA
Patterns,"When we are at our best as programmers, we recognize patterns in our 
 work and apply techniques that are time proven to be the best solution. 
 Parallel programming is no different, and it would be a serious mistake 
 not to study the patterns that have proven to be useful in this space. 
 Consider the MapReduce frameworks adopted for Big Data applications; 
 their success stems largely from being based on two simple yet effective 
 parallel patterns—
 map
  and 
 reduce
 .
  
 There are a number of common patterns in parallel programming 
 that crop up time and again, independent of the programming language 
 that we’re using. These patterns are versatile and can be employed at 
 any level of parallelism (e.g., sub-groups, work-groups, full devices) and",NA
 Understanding the Patterns,"The patterns discussed here are a subset of the parallel patterns described 
 in the book 
 Structured Parallel Programming
  by McCool et al. We do not 
 cover the patterns related to 
 types
  of parallelism (e.g., fork-join, branch- 
 and- bound) but focus on the algorithmic patterns most useful for writing 
 data-parallel kernels.
  
 324",NA
 Map,"The map pattern is the simplest parallel pattern of all and will 
  
 be immediately familiar to readers with experience of functional 
  
 programming languages. As shown in Figure 
 14-2
 , each input element of a 
 range is independently 
 mapped
  to an output by applying some function. 
 Many data-parallel operations can be expressed as instances of the map 
 pattern (e.g., vector addition).
  
 325",NA
 Stencil,"The stencil pattern is closely related to the map pattern. As shown in 
 Figure 
 14-3
 , a function is applied to an input and a set of neighboring 
 inputs described by a 
 stencil
  to produce a single output. Stencil patterns 
 appear frequently in many domains, including scientific/engineering 
 applications (e.g., finite difference codes) and computer vision/machine 
 learning applications (e.g., image convolutions).",NA
 Reduction,"A reduction is a common parallel pattern which 
 combines
  partial results 
 from each instance of a kernel invocation using an operator that is 
  
 typically 
 associative
  and 
 commutative
  (e.g., addition). The most ubiquitous 
 examples of reductions are computing a sum (e.g., while computing a dot 
 product) or computing the minimum/maximum value (e.g., using 
 maximum velocity to set time-step size).
  
 328",NA
 Scan,"The scan pattern computes a generalized prefix sum using a binary 
 associative operator, and each element of the output represents a partial 
 result. A scan is said to be 
 inclusive
  if the partial sum for element 
 i
  is the 
 sum of all elements in the range [0, 
 i
 ] (i.e., the sum 
 including i
 ). A scan is 
 said to be 
 exclusive
  if the partial sum for element 
 i
  is the sum of all 
 elements in the range [0, 
 i
 ]) (i.e., the sum 
 excluding i
 ).
  
 At first glance, a scan appears to be an inherently serial operation, 
 since the value of each output depends on the value of the previous 
 output! While it is true that scan has less opportunities for parallelism 
 than other patterns (and may therefore be less scalable), Figure 
 14-5
  
 shows that it is possible to implement a parallel scan using multiple 
 sweeps over the same data.",NA
 Pack and Unpack,"The pack and unpack patterns are closely related to scans and are often 
 implemented on top of scan functionality. We cover them separately here 
 because they enable performant implementations of common operations 
 (e.g., appending to a list) that may not have an obvious connection to 
 prefix sums.",NA
 Pack,"The pack pattern, shown in Figure 
 14-6
 , discards elements of an input 
 range based on a Boolean condition, 
 packing
  the elements that are not 
 discarded into contiguous locations of the output range. This Boolean 
 condition could be a  pre- computed mask or could be computed online by 
 applying some function to each input element.
  
  
 Figure 14-6. 
 Pack pattern
  
 Like with scan, there is an inherently serial nature to the pack 
  
 operation. Given an input element to pack/copy, computing its location in 
 the output range requires information about how many prior elements 
  
 332",NA
 Unpack,"As shown in Figure 
 14-7
  (and as its name suggests), the unpack pattern is 
 the opposite of the pack pattern. Contiguous elements of an input range 
 are 
 unpacked
  into non-contiguous elements of an output range, leaving 
 other elements untouched. The most obvious use case for this pattern is to 
 unpack data that was previously packed, but it can also be used to fill in 
 “gaps” in data resulting from some previous computation.
  
  
 Figure 14-7. 
 Unpack pattern",NA
 Using Built-In Functions and ,NA,NA
Libraries,"Many of these patterns can be expressed directly using built-in 
  
 functionality of DPC++ or vendor-provided libraries written in DPC++. 
  
 Leveraging these functions and libraries is the best way to balance 
 performance, portability, and productivity in real large-scale software 
 engineering projects.",NA
 The DPC++ Reduction Library,"Rather than require that each of us maintain our own library of portable 
 and highly performant reduction kernels, DPC++ provides a convenient 
 abstraction for describing variables with reduction semantics. This 
  
 abstraction simplifies the expression of reduction kernels and makes the 
 fact that a reduction is being performed explicit, allowing implementations 
 to select between different reduction algorithms for different 
 combinations of device, data type, and reduction operation.
  
 h
 .
 parallel_for
 ( 
  
  
 nd_range<
 1
 >{N, B}, 
  
  
 reduction
 (sum, 
 plus
 <>()), 
  
  
 [=](nd_item<
 1
 > 
 it
 , auto& 
 sum
 ) { 
  
   
 int i = 
 it
 .
 get_global_id
 (
 0
 ); 
  
   
 sum += 
 data
 [i]; 
  
 });
  
 Figure 14-8. 
 Reduction expressed as an ND-range data-parallel 
 kernel using the reduction library
  
 The kernel in Figure 
 14-8
  shows an example of using the reduction 
 library. Note that the kernel body doesn’t contain any reference to 
  
 reductions—all we must specify is that the kernel contains a reduction 
 which combines instances of the sum variable using the plus functor. This 
 provides enough information for an implementation to automatically 
 generate an optimized reduction sequence.
  
 At the time of writing, the reduction library only supports kernels with 
 a single reduction variable. Future versions of DPC++ are expected to 
 support kernels which perform more than one reduction simultaneously, 
 by specifying multiple reductions between the nd_range and functor 
 arguments passed into parallel_for and taking multiple reducers as 
 arguments to the kernel functor.
  
 The result of a reduction is not guaranteed to be written back 
 to the original variable until the kernel has completed. Apart from 
 this restriction, accessing the result of a reduction behaves 
 identically to 
  
 334",NA
 The ,NA,NA
reduction,NA,NA
 Class,"The reduction class is the interface we use to describe the reductions 
 present in a kernel. The only way to construct a reduction object is to use 
 one of the functions shown in Figure 
 14-9
 .
  
 template
  <
 typename
  T
 , 
 typename
  BinaryOperation
 > 
  
 unspecified
  reduction
 (
 T
 *
  variable
 , 
 BinaryOperation
  combiner
 );
  
 template
  <
 typename
  T
 , 
 typename
  BinaryOperation
 > 
  
 unspecified
  reduction
 (
 T
 *
  variable
 , 
 T
  identity
 , 
 BinaryOperation
  combiner
 );
  
 Figure 14-9. 
 Function prototypes of the reduction function
  
 The first version of the function allows us to specify the reduction 
 variable and the operator used to combine the contributions from each 
 work-item. The second version allows us to provide an optional identity 
 value associated with the reduction operator—this is an optimization for 
 user-defined reductions, which we will revisit later.",NA
 The ,NA,NA
reducer,NA,NA
 Class,"An instance of the reducer class encapsulates a reduction variable, 
 exposing a limited interface ensuring that we cannot update the reduction 
 variable in any way that an implementation could consider to be unsafe. 
  
 A simplified definition of the reducer class is shown in Figure 
 14-10
 . 
  
 Like the reduction class, the precise definition of the reducer class is 
 implementation-defined—a reducer's type will depend on how the 
 reduction is being performed, and it is important to know this at compile 
 time in order to maximize performance. However, the functions and 
 operators that allow us to update the reduction variable are well-defined 
 and are guaranteed to be supported by any DPC++ implementation.
  
 template
  <
 typename
  T
 , 
  
   
 typename
  BinaryOperation
 , 
  
   
 /* implementation-defined */
 > 
  
 class
  reducer
  { 
  
  
 // Combine partial result with reducer's value 
  
 void
  
 combine
 (
 const
  T
 &
  partial
 ); 
  
 };
  
 // Other operators are available for standard binary operations 
 template
  <
 typename
  T
 > 
  
 auto&
  operator +=
 (
 reducer
 <
 T
 ,
 plus
 ::<
 T
 >>
 &
 , 
 const
  T
 &
 );
  
 Figure 14-10. 
 Simplified definition of the reducer class
  
 336",NA
 User-Defined Reductions,"Several common reduction algorithms (e.g., a tree reduction) do not see 
 each work-item directly update a single shared variable, but instead 
 accumulate some partial result in a private variable that will be combined 
 at some point in the future. Such private variables introduce a problem: 
 how should the implementation initialize them? Initializing variables to the 
 first contribution from each work-item has potential performance 
 ramifications, since additional logic is required to detect and handle 
 uninitialized variables. Initializing variables to the identity of the reduction 
 operator instead avoids the performance penalty but is only possible when 
 the identity is known.
  
 DPC++ implementations can only automatically determine the correct 
 identity value to use when a reduction is operating on simple arithmetic 
 types and the reduction operator is a standard functor (e.g., plus). For 
 user-defined reductions (i.e., those operating on user-defined types and/or 
 using user-defined functors), we may be able to improve performance by 
 specifying the identity value directly.
  
 337",NA
 oneAPI DPC++ Library,"The C++ Standard Template Library (STL) contains several algorithms 
 which correspond to the parallel patterns discussed in this chapter. The 
 algorithms in the STL typically apply to sequences specified by pairs of 
 iterators and—starting with C++17—support an 
 execution policy
  
 argument denoting whether they should be executed sequentially or in 
 parallel.
  
 The oneAPI DPC++ Library (oneDPL) leverages this execution 
 policy argument to provide a high-productivity approach to parallel 
 programming that leverages kernels written in DPC++ under the hood. 
  
 If an application can be expressed solely using functionality of the STL 
 algorithms, oneDPL makes it possible to make use of the accelerators in 
 our systems without writing a single line of DPC++ kernel code!
  
 The table in Figure 
 14-12
  shows how the algorithms available in 
 the STL relate to the parallel patterns described in this chapter and to 
 legacy serial algorithms (available before C++17) where appropriate. A 
 more detailed explanation of how to use these algorithms in a DPC++ 
 application can be found in Chapter 
 18
 .",NA
 Group Functions,"Support for parallel patterns in DPC++ device code is provided by a 
  
 separate library of 
 group functions
 . These group functions exploit the 
 parallelism of a specific group of work-items (i.e., a work-group or a sub- 
 group) to implement common parallel algorithms at limited scope and can 
 be used as building blocks to construct other more complex algorithms.
  
 Like oneDPL, the syntax of the group functions in DPC++ is based on 
 that of the algorithm library in C++. The first argument to each function 
 accepts a group or sub_group object in place of an execution policy, and 
 any restrictions from the C++ algorithms apply. Group functions are 
 performed collaboratively by all the work-items in the specified group and 
 so must be treated similarly to a group barrier—all work-items in the 
 group must encounter the same algorithm in converged control flow (i.e., 
 all work-items in the group must similarly encounter or not encounter the 
 algorithm call), and all work-items must provide the same function 
 arguments in order to ensure that they agree on the operation being 
 performed.
  
 340",NA
 Direct Programming,"Although we recommend leveraging libraries wherever possible, we can 
 learn a lot by looking at how each pattern 
 could
  be implemented using 
 “native” DPC++ kernels.
  
 The kernels in the remainder of this chapter should not be expected to 
 reach the same level of performance as highly tuned libraries but 
  
 are useful in developing a greater understanding of the capabilities of 
 DPC++—and may even serve as a starting point for prototyping new 
 library functionality.
  
 USE VENDOR-PROVIDED LIBRARIES!
  
 When a vendor provides a library implementation of a function, it 
 is almost always beneficial to use it rather than re-implementing 
 the function as a kernel!",NA
 Map,"Owing to its simplicity, the map pattern can be implemented directly as a 
 basic parallel kernel. The code shown in Figure 
 14-13
  shows such an 
 implementation, using the map pattern to compute the square root of each 
 input element in a range.
  
 341",NA
 Stencil,"Implementing a stencil directly as a multidimensional basic data-parallel 
  
 kernel with multidimensional buffers, as shown in Figure 
 14-14
 , is 
  
 straightforward and easy to understand.
  
 id
 <
 2
 > 
 offset
 (
 1
 , 
 1
 ); 
  
 h
 .
 parallel_for
 (stencil_range, offset, [=](
 id
 <
 2
 > 
 idx
 ) { 
  
 int
  i = 
 idx
 [
 0
 ]; 
  
  
 int
  j = 
 idx
 [
 1
 ];
  
  
 float
  self = 
 input
 [i][j]; 
  
  
 float
  north = 
 input
 [i -
  1
 ][j]; 
  
  
 float
  east = 
 input
 [i][j + 
 1
 ]; 
  
  
 float
  south = 
 input
 [i + 
 1
 ][j]; 
  
  
 float
  west = 
 input
 [i][j -
  1
 ]; 
  
  
 output
 [i][j] = (self + north + east + south + west) / 
 5.0f
 ; });
  
 Figure 14-14. 
 Implementing the stencil pattern in a data-parallel 
  
 kernel
  
 However, this expression of the stencil pattern is very naïve and should 
  
 not be expected to perform very well. As mentioned earlier in the chapter, 
  
 it is well-known that leveraging locality (via spatial or temporal blocking) 
 is 
  
 required to avoid repeated reads of the same data from memory. A simple 
  
 example of spatial blocking, using work-group local memory, is shown in 
  
 Figure 
 14-15
 .
  
 342",NA
 Reduction,"It is possible to implement reduction kernels in DPC++ by leveraging 
 language features that provide synchronization and communication 
 capabilities between work-items (e.g., atomic operations, work-group and 
 sub-group functions, sub-group shuffles). The kernels in Figures 
 14-16
  and 
 14-17
  show two possible reduction implementations: a naïve reduction 
 using a basic parallel_for and an atomic operation for every work-item; 
 and a slightly smarter reduction that exploits locality using an ND-range 
 parallel_for and a work-group reduce function, respectively. We will revisit 
 these atomic operations in more detail in Chapter 
 19
 .
  
 Q
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) { 
  
 atomic_ref< 
  
  
  
 int
 , 
  
  
  
 memory_order
 ::relaxed, 
  
  
  
 memory_scope
 ::system, 
  
  
  
 access
 ::
 address_space
 ::global_space>(*sum) += 
 data
 [i]; }).
 wait
 ();
  
 Figure 14-16. 
 Implementing a naïve reduction expressed as a 
 data- parallel kernel
  
 344",NA
 Scan,"As we saw earlier in this chapter, implementing a parallel scan requires 
 multiple sweeps over the data, with synchronization occurring 
  
 between each sweep. Since DPC++ does not provide a mechanism for 
 synchronizing all work-items in an ND-range, a direct implementation 
 of a device-wide scan must be implemented using multiple kernels that 
 communicate partial results through global memory.
  
 345",NA
 Pack and Unpack,"Pack and unpack are also known as gather and scatter operations. These 
 operations handle differences in how data is arranged in memory and how 
 we wish to present it to the compute resources.",NA
 Pack,"Since pack depends on an exclusive scan, implementing a pack that 
 applies to all elements of an ND-range must also take place via global 
 memory and over the course of several kernel enqueues. However, there 
 is a common use case for pack that does not require the operation to be 
 applied over all elements of an ND-range—namely, applying a pack only 
 across items in a specific work-group or sub-group.
  
  
 The snippet in Figure 
 14-21
  shows how to implement a group 
 pack operation on top of an exclusive scan.
  
 uint32_t
  index = 
 exclusive_scan
 (g, (
 uint32_t
 ) predicate, 
 plus
 <>()); 
 if
  (predicate) 
  
  
 dst
 [index] = value;
  
 Figure 14-21. 
 Implementing a group pack operation on top of an 
 exclusive scan
  
 The code in Figure 
 14-22
  demonstrates how such a pack operation 
 could be used in a kernel to build a list of elements which require some 
 additional postprocessing (in a future kernel). The example shown is based 
 on a real kernel from molecular dynamics simulations: the work-items in 
  
 348",NA
 Unpack,"As with pack, we can implement unpack using scan. Figure 
 14-23
  
 shows how to implement a sub-group unpack operation on top of an 
 exclusive scan.
  
 uint32_t
  index = 
 exclusive_scan
 (sg, (
 uint32_t
 ) predicate, 
 plus
 <>()); 
 return
  (predicate) ? 
 new_value
 [index] : original_value;
  
 Figure 14-23. 
 Implementing a sub-group unpack operation on top of 
 an exclusive scan
  
 The code in Figure 
 14-24
  demonstrates how such a sub-group 
 unpack operation could be used to improve load balancing in a kernel 
 with divergent control flow (in this case, computing the Mandelbrot set). 
 Each work-item is assigned a separate pixel to compute and iterates until 
 convergence or a maximum number of iterations is reached. An unpack 
 operation is then used to replace completed pixels with new pixels.
  
 // Keep iterating as long as one work-item has work to do 
  
 while
  (
 any_of
 (sg, i < Nx)) { 
  
  
 uint32_t
  converged = 
  
   
 next_iteration
 (params, i, j, count, cr, ci, zr, zi, mandelbrot); 
  
 if
  (
 any_of
 (sg, 
 converged)) {
  
 // Replace pixels that have converged using an unpack // Pixels that haven't 
 converged are not replaced 
  
 uint32_t
  index = 
 exclusive_scan
 (sg, converged, 
 plus
 <>()); i = (converged) ? iq + 
 index : i; 
  
 iq += 
 reduce
 (sg, converged, 
 plus
 <>());
  
 }
  
 }
  
 // Reset the iterator variables for the new i 
 if
  (converged) 
  
 reset
 (params, i, j, count, cr, ci, zr, zi);
  
 Figure 14-24. 
 Using a sub-group unpack operation to improve load 
 balancing for kernels with divergent control flow
  
 350",NA
 Summary,"This chapter has demonstrated how to implement some of the most 
 common parallel patterns using DPC++ and SYCL features, including 
 built-in functions and libraries.
  
 The SYCL and DPC++ ecosystems are still developing, and we expect 
 to uncover new best practices for these patterns as developers gain more 
 experience with the language and from the development of production- 
 grade applications and libraries.",NA
 For More Information,"• 
 Structured Parallel Programming: Patterns for Efficient 
 Computation
  by Michael McCool, Arch Robison, and James 
 Reinders, © 2012, published by Morgan Kaufmann, ISBN 
 978-0-124-15993-8
  
 • Intel oneAPI DPC++ Library Guide, 
 https://software.intel.com/en-us/ oneapi-
 dpcpp- library-guide
  
 • Algorithms library, C++ Reference, 
 https:// 
 en.cppreference.com/w/cpp/algorithm
  
 351",NA
CHAPTER 15,NA,NA
Programming for ,NA,NA
GPUs,"Over the last few decades, Graphics Processing Units (GPUs) have evolved 
 from specialized hardware devices capable of drawing images on a screen 
 to general-purpose devices capable of executing complex parallel kernels. 
 Nowadays, nearly every computer includes a GPU alongside a traditional 
 CPU, and many programs may be accelerated by offloading part of a 
 parallel algorithm from the CPU to the GPU.
  
 In this chapter, we will describe how a typical GPU works, how GPU 
 software and hardware execute a SYCL application, and tips and 
 techniques to keep in mind when we are writing and optimizing parallel 
 kernels for a GPU.",NA
 Performance Caveats,"As with any processor type, GPUs differ from vendor to vendor or even 
 from product generation to product generation; therefore, best practices 
 for one device may not be best practices for a different device. The advice 
 in this chapter is likely to benefit many GPUs, both now and in the future, 
 but…",NA
"To achieve optimal performance for a particular GPU, ",NA,NA
always consult the GPU vendor’s documentation!,"Links to documentation from many GPU vendors are provided at the 
 end of this chapter.",NA
 How GPUs Work,"This section describes how typical GPUs work and how GPUs differ from 
 other accelerator types.",NA
 GPU Building Blocks,"Figure 
 15-1
  shows a very simplified GPU consisting of three high-level 
 building blocks:
  
  1. Execution resources: A GPU’s execution resources are the 
 processors that perform computational work. 
 Different GPU vendors use different names for their 
 execution resources, but all modern GPUs consist of 
 multiple programmable processors. The processors 
 may be 
 heterogeneous
  and specialized for particular 
 tasks, or they may be 
 homogeneous
  and 
 interchangeable. Processors for most modern GPUs 
 are 
 homogeneous
  and interchangeable.",NA
 Simpler Processors (but More of Them),"Traditionally, when performing graphics operations, GPUs process large 
 batches of data. For example, a typical game frame or rendering workload 
 involves thousands of vertices that produce millions of pixels per frame. 
  
 To maintain interactive frame rates, these large batches of data must 
 be processed as quickly as possible.
  
 A typical GPU design tradeoff is to eliminate features from the 
  
 processors forming the execution resources that accelerate single- 
  
 threaded performance and to use these savings to build additional 
 processors, as shown in Figure 
 15-2
 . For example, GPU processors may 
 not include sophisticated out-of-order execution capabilities or branch 
 prediction logic used by other types of processors. Due to these tradeoffs, 
 a single data element may be processed on a GPU slower than it would on 
 another processor, but the larger number of processors enables GPUs to 
 process many data elements quickly and efficiently.
  
  
  
  
  
  
  
  
  
  
 Figure 15-2. 
 GPU processors are simpler, but there are more of them
  
 356",NA
 Expressing Parallelism,"To improve the performance of this kernel for both CPUs and GPUs, we 
 can instead submit a range of data elements to process in parallel, by 
 converting one of the loops to a parallel_for. For the matrix 
  
 multiplication kernel, we can choose to submit a range of data elements 
 representing either of the two outermost loops. In Figure 
 15-5
 , we’ve 
 chosen to process rows of the result matrix in parallel.
  
 h
 .
 parallel_for
 (
 range
 {M}, [=](
 id
 <
 1
 > 
 idx
 ) { 
  
 int
  m = 
 idx
 [
 0
 ];
  
  
 for
  (
 int
  n = 
 0
 ; n < N; n++) { 
  
   
 T sum = 
 0
 ; 
  
   
 for
  (
 int
  k = 
 0
 ; k < K; k++) 
  
   
 sum += 
 matrixA
 [m * K + k] * 
 matrixB
 [k * N + n]; 
  
  
 matrixC
 [m * N + n] = sum; 
  
  
 } 
  
 });
  
 Figure 15-5. 
 Somewhat-parallel matrix multiplication
  
 CHOOSING HOW TO PARALLELIZE
  
 Choosing which dimension to parallelize is one very important way 
 to tune an application for both GPUs and other device types. 
 subsequent sections in this chapter will describe some of the 
 reasons why parallelizing in one dimension may perform better 
 than parallelizing in a different dimension.
  
 Even though the somewhat-parallel kernel is very similar to the 
 single task kernel, it should run better on a CPU and much better on a 
 GPU. As shown in Figure 
 15-6
 , the parallel_for enables work-items 
 representing rows of the result matrix to be processed on multiple 
 processor resources in parallel, so all execution resources stay busy.
  
 359",NA
 Expressing More Parallelism,"We can parallelize the matrix multiplication kernel even more by choosing 
 to process both outer loops in parallel. Because parallel_for can express 
 parallel loops over up to three dimensions, this is straightforward, as 
 shown in Figure 
 15-7
 . In Figure 
 15-7
 , note that both the range passed to 
 parallel_for and the item representing the index in the parallel execution 
 space are now two-dimensional.
  
 360",NA
 Simplified Control Logic (SIMD ,NA,NA
Instructions),"Many GPU processors optimize control logic by leveraging the fact that 
 most data elements tend to take the same control flow path through a 
 kernel. For example, in the matrix multiplication kernel, each data element 
 executes the innermost loop the same number of times since the loop 
 bounds are invariant.
  
 When data elements take the same control flow path through a kernel, 
 a processor may reduce the costs of managing an instruction stream by 
 sharing control logic among multiple data elements and executing them as 
 a group. One way to do this is to implement a 
 Single Instruction, Multiple 
 Data
  or 
 SIMD
  instruction set, where multiple data elements are processed 
 simultaneously by a single instruction.",NA
Kernels benefit from parallelism across processors and ,NA,NA
parallelism within processors!,NA,NA
 Predication and Masking,"Sharing an instruction stream among multiple data elements works well 
 so long as all data elements take the same path through conditional code 
 in a kernel. When data elements take different paths through conditional 
 code, control flow is said to 
 diverge
 . When control flow diverges in a SIMD 
 instruction stream, usually both control flow paths are executed, with 
 some channels masked off or 
 predicated
 . This ensures correct behavior, 
 but the correctness comes at a performance cost since channels that are 
 masked do not perform useful work.
  
 To show how predication and masking works, consider the kernel in 
 Figure 
 15-10
 , which multiplies each data element with an “odd” index by 
 two and increments each data element with an “even” index by one.
  
 h
 .
 parallel_for
 (array_size, [=](
 id
 <
 1
 > 
 i
 ) { 
  
 auto
  condition 
 = 
 i
 [
 0
 ] & 
 1
 ; 
  
  
 if
  (condition) 
  
   
 dataAcc
 [i] = 
 dataAcc
 [i] * 
 2
 ;
  // odd 
  
 else 
  
   
 dataAcc
 [i] = 
 dataAcc
 [i] + 
 1
 ;
  // even 
 });
  
 Figure 15-10. 
 Kernel with divergent control flow
  
 364",NA
 SIMD Efficiency,"SIMDefficiency
  measures how well a SIMD instruction stream performs 
 compared to equivalent scalar instruction streams. In Figure 
 15-11
 , 
 since control flow partitioned the channels into two equal groups, each 
 instruction in the divergent control flow executes with half efficiency. 
 In a worst-case scenario, for highly divergent kernels, efficiency may be 
 reduced by a factor of the processor’s SIMD width.
  
 365",NA
 SIMD Efficiency and Groups of Items,"All kernels in this chapter so far have been basic data-parallel kernels that 
 do not specify any grouping of items in the execution range, which gives an 
 implementation freedom to choose the best grouping for a device. For 
 example, a device with a wider SIMD width may prefer a larger grouping, 
 but a device with a narrower SIMD width may be fine with smaller 
 groupings.
  
 When a kernel is an ND-range kernel with explicit groupings of work- 
 items, care should be taken to choose an ND-range work-group size that 
 maximizes SIMD efficiency. When a work-group size is not evenly divisible 
 by a processor’s SIMD width, part of the work-group may execute with 
 channels disabled for the entire duration of the kernel. The kernel 
 preferred_work_group_size_multiple query can be used to choose an 
 efficient work-group size. Please refer to Chapter 
 12
  for more information 
 on how to query properties of a device.
  
 Choosing a work-group size consisting of a single work-item will likely 
 perform very poorly since many GPUs will implement a single-work-item 
 work-group by masking off all SIMD channels except for one. For example, 
 the kernel in Figure 
 15-12
  will likely perform much worse than the very 
 similar kernel in Figure 
 15-5
 , even though the only significant difference 
 between the two is a change from a basic data-parallel kernel to an 
  
 inefficient single-work-item ND-range kernel (nd_range<1>{M, 1}).
  
 366",NA
 Switching Work to Hide Latency,"Many GPUs implement one more technique to simplify control logic, 
 maximize execution resources, and improve performance: instead of 
 executing a single instruction stream on a processor, many GPUs allow 
 multiple instruction streams to be resident on a processor simultaneously.
  
 Having multiple instruction streams resident on a processor is 
  
 beneficial because it gives each processor a choice of work to execute. If 
 one instruction stream is performing a long-latency operation, such as a 
 read from memory, the processor can switch to a different instruction 
 stream that is ready to run instead of waiting for the operation to 
 complete. With enough instruction streams, by the time that the processor 
 switches back to the original instruction stream, the long-latency 
 operation may have completed without requiring the processor to wait at 
 all.
  
 Figure 
 15-13
  shows how a processor uses multiple simultaneous 
 instruction streams to hide latency and improve performance. Even 
 though the first instruction stream took a little longer to execute with 
 multiple streams, by switching to other instruction streams, the processor 
 was able to find work that was ready to execute and never needed to idly 
 wait for the long operation to complete.",NA
 Offloading Kernels to GPUs,"This section describes how an application, the SYCL runtime library, and 
 the GPU software driver work together to offload a kernel on GPU 
 hardware. The diagram in Figure 
 15-14
  shows a typical software stack 
 with these layers of abstraction. In many cases, the existence of these 
 layers is transparent to an application, but it is important to understand 
 and account for them when debugging or profiling our application.
  
 SYCL Application
  
  
 SYCL Runtime Library
  
  
 GPU Software Driver
  
  
 GPU Hardware
  
 Figure 15-14. 
 Offloading parallel kernels to GPUs (simplified)",NA
 SYCL Runtime Library,"The SYCL runtime library is the primary software library that SYCL 
 applications interface with. The runtime library is responsible for 
  
 implementing classes such as queues, buffers, and accessors and the 
 member functions of these classes. Parts of the runtime library may be in 
 header files and hence directly compiled into the application executable. 
  
 Other parts of the runtime library are implemented as library functions, 
 which are linked with the application executable as part of the application 
 build process. The runtime library is usually not device-specific, and the 
 same runtime library may orchestrate offload to CPUs, GPUs, FPGAs, or 
 other devices.
  
 369",NA
 GPU Software Drivers,"Although it is theoretically possible that a SYCL runtime library could 
 offload directly to a GPU, in practice, most SYCL runtime libraries interface 
 with a GPU software driver to submit work to a GPU.
  
 A GPU software driver is typically an implementation of an API, such as 
 OpenCL, Level Zero, or CUDA. Most of a GPU software driver is 
 implemented in a user-mode driver library that the SYCL runtime calls 
 into, and the user-mode driver may call into the operating system or a 
 kernel-mode driver to perform system-level tasks such as allocating 
 memory or submitting work to the device. The user-mode driver may also 
 invoke other user-mode libraries; for example, the GPU driver may invoke 
 a GPU compiler to just-in-time compile a kernel from an intermediate 
 representation to GPU ISA (Instruction Set Architecture). These software 
 modules and the interactions between them are shown in Figure 
 15-15
 .
  
 SYCL Runtime Library
  
 API Calls
  
 GPU Software User-Mode Driver
  
 User-Mode Support 
  
 Libraries or Compilers
  
 User Mode
  
 Kernel Mode
  
 Operating Systems Services 
  
 GPU Software Kernel-Mode Driver
  
 Software
  
 Hardware
  
 GPU Hardware
  
 Figure 15-15. 
 Typical GPU software driver modules
  
 370",NA
 GPU Hardware,"When the runtime library or the GPU software user-mode driver is 
  
 explicitly requested to submit work or when the GPU software 
 heuristically determines that work should begin, it will typically call 
 through the 
  
 operating system or a kernel-mode driver to start executing work on the 
 GPU. In some cases, the GPU software user-mode driver may submit work 
 directly to the GPU, but this is less common and may not be supported by 
 all devices or operating systems.
  
 When the results of work executed on a GPU are consumed by the host 
 processor or another accelerator, the GPU must issue a signal to indicate 
 that work is complete. The steps involved in work completion are very 
 similar to the steps for work submission, executed in reverse: the GPU may 
 signal the operating system or kernel-mode driver that it has finished 
 execution, then the user-mode driver will be informed, and finally the 
 runtime library will observe that work has completed via GPU software 
 API calls.
  
 Each of these steps introduces latency, and in many cases, the runtime 
 library and the GPU software are making a tradeoff between lower latency 
 and higher throughput. For example, submitting work to the GPU more 
 frequently may reduce latency, but submitting frequently may also reduce 
 throughput due to per-submission overheads. Collecting large batches of 
 work increases latency but amortizes submission overheads over more 
 work and introduces more opportunities for parallel execution. 
  
 The runtime and drivers are tuned to make the right tradeoff and usually 
 do a good job, but if we suspect that driver heuristics are submitting 
 work inefficiently, we should consult documentation to see if there are 
 ways to override the default driver behavior using API-specific or even 
 implementation-specific mechanisms.",NA
 Beware the Cost of Offloading!,"Although SYCL implementations and GPU vendors are continually 
  
 innovating and optimizing to reduce the cost of offloading work to a GPU, 
 there will always be overhead involved both when starting work on a GPU 
 and observing results on the host or another device. When choosing where 
 to execute an algorithm, consider both the benefit of executing an 
 algorithm on a device and the cost of moving the algorithm and any data 
 that it requires to the device. In some cases, it may be most efficient to 
 perform a parallel operation using the host processor—or to execute a 
 serial part of an algorithm inefficiently on the GPU—to avoid the overhead 
 of moving an algorithm from one processor to another.",NA
Consider the performance of our algorithm as a whole—it ,NA,NA
may be most efficient to execute part of an algorithm ,NA,NA
inefficiently on one device than to transfer execution to ,NA,NA
another device!,NA,NA
 Transfers to and from Device Memory,"On GPUs with dedicated memory, be especially aware of transfer costs 
 between dedicated GPU memory and memory on the host or another 
 device. Figure 
 15-16
  shows typical memory bandwidth differences 
 between different memory types in a system.
  
 372",NA
 GPU Kernel Best Practices,"The previous sections described how the dispatch parameters passed to a 
 parallel_for affect how kernels are assigned to GPU processor resources 
 and the software layers and overheads involved in executing a kernel on a 
 GPU. 
  
 This section describes best practices when a kernel is executing on a GPU.
  
 Broadly speaking, kernels are either 
 memory bound
 , meaning that their 
 performance is limited by data read and write operations into or out of the 
 execution resources on the GPU, or are 
 compute bound
 , meaning that their 
 performance is limited by the execution resources on the GPU. A good first 
 step when optimizing a kernel for a GPU—and many other processors!—is 
 to determine whether our kernel is memory bound or compute bound, 
 since the techniques to improve a memory-bound kernel frequently will 
 not benefit a compute-bound kernel and vice versa. GPU vendors often 
 provide profiling tools to help make this determination.",NA
Different optimization techniques are needed depending ,NA,NA
whether our kernel is memory bound or compute bound!,"Because GPUs tend to have many processors and wide SIMD widths, 
 kernels tend to be memory bound more often than they are compute 
 bound. If we are unsure where to start, examining how our kernel accesses 
 memory is a good first step.",NA
 Accessing Global Memory,"Efficiently accessing global memory is critical for optimal application 
 performance, because almost all data that a work-item or work-group 
 operates on originates in global memory. If a kernel operates on global 
 memory inefficiently, it will almost always perform poorly. Even though 
 GPUs often include dedicated hardware 
 gather
  and 
 scatter
  units for",NA
 Accessing Work-Group Local Memory,"In the previous section, we described how accesses to global memory 
 benefit from 
 locality
 , to maximize cache performance. As we saw, in some 
 cases we can design our algorithm to efficiently access memory, such as 
 by choosing to parallelize in one dimension instead of another. This 
 technique isn’t possible in all cases, however. This section describes how 
 we can use work-group local memory to efficiently support more 
 memory access patterns.
  
 Recall from Chapter 
 9
  that work-items in a work-group can cooperate 
 to solve a problem by communicating through work-group local memory 
 and synchronizing using work-group barriers. This technique is especially 
 beneficial for GPUs, since typical GPUs have specialized hardware 
  
 to implement both barriers and work-group local memory. Different GPU 
 vendors and different products may implement work-group local memory 
 differently, but work-group local memory frequently has two benefits 
 compared to global memory: local memory may support higher bandwidth 
 and lower latency than accesses to global memory, even when the global 
 memory access hits a cache, and local memory is often divided into 
 different memory regions, called 
 banks
 . So long as each work-item in a 
 group accesses a different bank, the local memory access executes with full 
 performance. Banked accesses allow local memory to support far more 
 access patterns with peak performance than global memory.",NA
"for maximum global memory performance, minimize the ",NA,NA
number of cache lines accessed.,NA,NA
"for maximum local memory performance, minimize the ",NA,NA
number of bank conflicts!,"A summary of access patterns and expected performance for global 
 memory and local memory is shown in Figure 
 15-20
 . Assume that 
  
 when ptr points to global memory, the pointer is aligned to the size of a 
 GPU cache line. The best performance when accessing global memory 
 can be achieved by accessing memory consecutively starting from a 
 cache-aligned address. Accessing an unaligned address will likely lower 
 global memory performance because the access may require accessing 
 additional cache lines. Because accessing an unaligned local address will 
 not result in additional bank conflicts, the local memory performance is 
 unchanged.
  
 The strided case is worth describing in more detail. Accessing every 
 other element in global memory requires accessing more cache lines and 
 will likely result in lower performance. Accessing every other element in 
 local memory may result in bank conflicts and lower performance, but 
 only if the number of banks is divisible by two. If the number of banks is 
 odd, this case will operate at full performance also.
  
 379",NA
 Avoiding Local Memory Entirely ,NA,NA
with Sub-Groups,"As discussed in Chapter 
 9
 , sub-group collective functions are an 
  
 alternative way to exchange data between work-items in a group. For 
 many GPUs, a sub-group represents a collection of work-items processed 
 by a 
  
 380",NA
 Optimizing Computation Using Small Data ,NA,NA
Types,"This section describes techniques to optimize kernels after eliminating or 
 reducing memory access bottlenecks. One very important perspective to 
 keep in mind is that GPUs have traditionally been designed to draw 
 pictures on a screen. Although pure computational capabilities of GPUs 
 have evolved and improved over time, in some areas their graphics 
 heritage is still apparent.
  
 Consider support for kernel data types, for example. Many GPUs are 
 highly optimized for 32-bit floating-point operations, since these 
 operations tend to be common in graphics and games. For algorithms that 
 can cope with lower precision, many GPUs also support a lower-precision 
 16-bit floating-point type that trades precision for faster processing. 
  
 Conversely, although many GPUs do support 64-bit double-precision 
 floating-point operations, the extra precision will come at a cost, and 32-bit 
 operations usually perform much better than their 64-bit equivalents.
  
 The same is true for integer data types, where 32-bit integer data types 
 typically perform better than 64-bit integer data types and 16-bit integers 
 may perform even better still. If we can structure our computation to use 
 smaller integers, our kernel may perform faster. One area to pay careful 
 attention to are addressing operations, which typically operate on 64-bit 
 size_t data types, but can sometimes be rearranged to perform most of the 
 calculation using 32-bit data types. In some local memory cases, 16 bits of 
 indexing is sufficient, since most local memory allocations are small.",NA
 Optimizing Math Functions,"Another area where a kernel may trade off accuracy for performance 
  
 involves SYCL built-in functions. SYCL includes a rich set of math functions 
 with well-defined accuracy across a range of inputs. Most GPUs do not 
 support these functions natively and implement them using a long 
 sequence of other instructions. Although the math function 
 implementations are typically well-optimized for a GPU, if our application 
 can tolerate lower accuracy, we should consider a different 
 implementation with lower 
  
 accuracy and higher performance instead. Please refer to Chapter 
 18
  for 
 more information about SYCL built-in functions.
  
 For commonly used math functions, the SYCL library includes fast or 
 native function variants with reduced or implementation-defined 
 accuracy requirements. For some GPUs, these functions can be an order 
 of magnitude faster than their precise equivalents, so they are well worth 
 considering if they have enough precision for an algorithm. For example, 
 many image postprocessing algorithms have well-defined inputs and can 
 tolerate lower accuracy and hence are good candidates for using fast or 
 native math functions.",NA
"if an algorithm can tolerate lower precision, we can use ",NA,NA
smaller data types or lower-precision math functions to ,NA,NA
increase performance!,NA,NA
 Specialized Functions and Extensions,"One final consideration when optimizing a kernel for a GPU are 
  
 specialized instructions that are common in many GPUs. As one example, 
 nearly all GPUs support a mad or fma multiply-and-add instruction that 
 performs two operations in a single clock. GPU compilers are generally 
 very good at identifying and optimizing individual multiplies and adds to 
 use a single instruction instead, but SYCL also includes mad and fma",NA
 Summary,"In this chapter, we started by describing how typical GPUs work and how 
 GPUs are different than traditional CPUs. We described how GPUs are 
 optimized for large amounts of data, by trading processor features that 
 accelerate a single instruction stream for additional processors.
  
 We described how GPUs process multiple data elements in parallel 
 using wide SIMD instructions and how GPUs use predication and masking 
 to execute kernels with complex flow control using SIMD instructions. 
  
 We discussed how predication and masking can reduce SIMD efficiency 
 and decrease performance for kernels that are highly divergent and how 
 choosing to parallelize along one dimension vs. another may reduce SIMD 
 divergence.
  
 Because GPUs have so many processing resources, we discussed how it 
 is important to give GPUs enough work to keep occupancy high. We also 
 described how GPUs use instruction streams to hide latency, making it 
 even more crucial to give GPUs lots of work to execute.
  
 Next, we discussed the software and hardware layers involved in 
 offloading a kernel to a GPU and the costs of offloading. We discussed how 
 it may be more efficient to execute an algorithm on a single device than it 
 is to transfer execution from one device to another.
  
 383",NA
 For More Information,"There is much more to learn about GPU programming, and this chapter 
 just scratched the surface!
  
 GPU specifications and white papers are a great way to learn more 
 about specific GPUs and GPU architectures. Many GPU vendors provide 
 very detailed information about their GPUs and how to program them.
  
  
 At the time of this writing, relevant reading about major GPUs can be 
 found on 
 software.intel.com
 , 
 devblogs.nvidia.com
 , and 
 amd.com
 .
  
 Some GPU vendors have open source drivers or driver components. 
 When available, it can be instructive to inspect or step through driver code, 
 to get a sense for which operations are expensive or where overheads may 
 exist in an application.
  
 This chapter focused entirely on traditional accesses to global memory 
 via buffer accessors or Unified Shared Memory, but most GPUs also include 
 a fixed-function texture sampler that can accelerate operations on images. 
 For more information about images and samplers, please refer to the SYCL 
 specification.
  
 384",NA
CHAPTER 16,NA,NA
Programming ,NA,NA
for CPUs,"Kernel programming originally became popular as a way to program GPUs. 
 As kernel programming is generalized, it is important to understand how 
 our style of programming affects the mapping of our code to a CPU.
  
  
 The CPU has evolved over the years. A major shift occurred around 
 2005 when performance gains from increasing clock speeds diminished. 
  
 Parallelism arose as the favored solution—instead of increasing clock 
 speeds, CPU producers introduced multicore chips. Computers became 
 more effective in performing multiple tasks at the same time!
  
  
 While multicore prevailed as the path for increasing hardware 
 performance, releasing that gain in software required non-trivial effort. 
  
 Multicore processors required developers to come up with different 
 algorithms so the hardware improvements could be noticeable, and this 
 was not always easy. The more cores that we have, the harder it is to keep 
 them efficiently busy. DPC++ is one of the programming languages that 
 address these challenges, with many constructs that help to exploit various 
 forms of parallelism on CPUs (and other architectures).
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_16
  
 387",NA
 Performance Caveats,"DPC++ paves a portable path to parallelize our applications or to develop 
 parallel applications from scratch. The application performance of a 
 program, when run on CPUs, is largely dependent upon the following 
 factors:
  
 • The underlying performance of the single invocation 
 and execution of kernel code
  
 • The percentage of the program that runs in a parallel 
 kernel and its scalability
  
 • CPU utilization, effective data sharing, data locality, 
 and load balancing
  
 • The amount of synchronization and communication 
 between work-items
  
 • The overhead introduced to create, resume, manage, 
 suspend, destroy, and synchronize the threads that work-
 items execute on, which is made worse by the number of 
 serial-to-parallel or parallel-to-serial transitions
  
 • Memory conflicts caused by shared memory or falsely 
 shared memory
  
 • Performance limitations of shared resources such as 
 memory, write combining buffers, and memory 
 bandwidth
  
 388",NA
"to achieve optimal performance on a CpU, understand ",NA,NA
as many characteristics of the CpU architecture as ,NA,NA
possible!,NA,NA
 The Basics of a General-Purpose CPU,"Emergence and rapid advancements in multicore CPUs have driven 
 substantial acceptance of shared memory parallel computing platforms. 
  
 CPUs offer parallel computing platforms at laptop, desktop, and 
  
 server levels, making them ubiquitous and exposing performance 
  
 almost everywhere. The most common form of CPU architecture is 
  
 cache-coherent Non-Uniform Memory Access (cc-NUMA), which is 
 characterized by access times not being completely uniform. Even many 
 small dual-socket general-purpose CPU systems have this kind of memory 
 system. This architecture has become dominant because the number of 
 cores in a processor, as well as the number of sockets, continues to 
 increase.
  
 In a cc-NUMA CPU system, each socket connects to a subset of the 
 total memory in the system. A cache-coherent interconnect glues all of 
 the sockets together and provides a single system view for 
 programmers. 
  
 Such a memory system is scalable, because the aggregate memory 
  
 bandwidth scales with the number of sockets in the system. The benefit 
 of the interconnect is that an application has transparent access to all of 
 the memory in the system, regardless of where the data resides. 
 However, there is a cost: the latency to access data and instructions, from 
 memory is no longer consistent (e.g., fixed access latency). The latency 
 instead depends",NA
 The Basics of SIMD Hardware,"In 1996, the first widely deployed SIMD (Single Instruction, Multiple Data 
 according to Flynn’s taxonomy) instruction set was MMX extensions on 
 top of the x86 architecture. Many SIMD instruction set extensions have 
 since followed both on Intel architectures and more broadly across the 
 industry. A CPU core carries out its job by executing instructions, and 
  
 391",NA
 Exploiting Thread-Level Parallelism,"To improve the performance of the STREAM Triad kernel for both CPUs 
 and GPUs, we can compute on a range of data elements that can be 
 processed in parallel, by converting the loop to a parallel_for kernel.
  
 A STREAM Triad kernel may be trivially executed on a CPU by 
 submitting it into a queue for a parallel execution. The body of this 
 STREAM Triad DPC++ parallel kernel looks exactly like the body of the 
 STREAM Triad loop that executes in serial C++ on the CPU, as shown in 
 Figure 
 16-6
 .
  
 398",NA
 Thread Affinity Insight,"Thread affinity designates the CPU cores on which specific threads execute. 
 Performance can suffer if a thread moves around among cores, for 
 instance, if threads do not execute on the same core, cache locality can 
 become an inefficiency if data ping-pongs between different cores.
  
 The DPC++ runtime library supports several schemes for binding 
 threads to core(s) through environment variables DPCPP_CPU_CU_ 
 AFFINITY, DPCPP_CPU_PLACES, DPCPP_CPU_NUM_CUS, and DPCPP_ 
 CPU_SCHEDULE, which are not defined by SYCL.
  
 The first of these is the environment variable 
 DPCPP_CPU_CU_AFFINITY. Tuning using these environment variable 
 controls is simple and low cost and can have large impact for many 
 applications. The description of this environment variable is shown in 
 Figure 
 16-8
 .",NA
 (,"tid
  
 mod 
 numH
 T",NA
),+,NA
 ,NA,NA
(,"tid
  
 mod 
 numSocke
 t",NA
),"´ 
 numHT
  
 close boundHT
  = 
 tid
  
 mo
 d",NA
(,"numSocke
 t
  
 ´ 
 numH
 T",NA
),"where
  
 • tid denotes a software thread identifier.
  
 • boundHT denotes a hyper-thread (logical core) that 
 thread tid is bound to.
  
 • numHT denotes the number of hyper-threads per socket.
  
 • numSocket denotes the number of sockets in the 
 system.
  
 Assume that we run a program with eight threads on a dual-core 
 dual- socket hyper-threading system—in other words, we have four 
 cores for a total of eight hyper-threads to program. Figure 
 16-9
  shows 
 examples of how threads can map to the hyper-threads and cores for 
 different DPCPP_CPU_CU_AFFINITY settings.
  
 402",NA
 Be Mindful of First Touch to Memory,"Memory is stored where it is first touched (used). Since the initialization 
 loop in our example is not parallelized, it is executed by the host thread in 
 serial, resulting in all the memory being associated with the socket that the 
 host thread is running on. Subsequent access by other sockets will then 
 access data from memory attached to the initial socket (used for the 
 initialization), which is clearly undesirable for performance. We can 
 achieve a higher performance on the STREAM Triad kernel by parallelizing 
 the initialization loop to control the first touch effect across sockets, as 
 shown in Figure 
 16-10
 .
  
 template
  <
 typename
  T
 > 
  
 void
  init
 (
 queue &
 deviceQueue
 , 
 T
 *
  VA
 , 
 T
 *
  VB
 , 
 T
 *
  VC
 , 
 size_t
  array_size
 ) { 
  
 range
 <
 1
 > 
 numOfItems{array_size}; 
  
  
 buffer
 <T, 
 1
 > 
 bufferA
 (VA, numOfItems); 
  
  
 buffer
 <T, 
 1
 > 
 bufferB
 (VB, numOfItems); 
  
  
 buffer
 <T, 
 1
 > 
 bufferC
 (VC, numOfItems); 
  
  
 auto
  queue_event = 
 deviceQueue
 .
 submit
 ([&](
 handler&
  cgh
 ) { 
  
   
 auto
  aA = 
 bufA
 .
 template
  get_access
 <
 sycl_write
 >(cgh); 
  
   
 auto
  aB = 
 bufB
 .
 template
  get_access
 <
 sycl_write
 >(cgh); 
  
   
 auto
  aC = 
 bufC
 .
 template
  get_access
 <
 sycl_write
 >(cgh); 
  
   
 cgh
 .
 parallel_for
 <
 class
  Init
 <
 T
 >>(numOfItems, [=](id<
 1
 > wi) { 
  
   
 aA
 [wi] = 
 2.0
 ; 
 aB
 [wi] = 
 1.0
 ; 
 aC
 [wi] = 
 0.0
 ; 
  
   
 }); 
  
  
 });
  
 }
  
 queue_event
 .
 wait
 ();
  
 Figure 16-10. 
 STREAM Triad parallel initialization kernel to control 
 first touch effects
  
 405",NA
DpC++ parallel kernels benefit from thread-level ,NA,NA
parallelism across cores and hyper-threads!,NA,NA
 SIMD Vectorization on CPU,"While a well-written DPC++ kernel without cross-work-item dependences 
 can run in parallel effectively on a CPU, we can also apply vectorization to 
 DPC++ kernels to leverage SIMD hardware, similarly to the GPU 
  
 support described in Chapter 
 15
 . Essentially, CPU processors may 
  
 optimize memory loads, stores, and operations using SIMD instructions by 
 leveraging the fact that most data elements are often in contiguous 
 memory and take the same control flow paths through a data-parallel 
 kernel. For example, in a kernel with a statement a[i] = a[i] + b[i], each 
 data element executes with same instruction stream 
 load
 , 
 load
 , 
 add
 , and 
 store
  by sharing hardware logic among multiple data elements and 
 executing them as a group, which may be mapped naturally onto a 
 hardware’s SIMD instruction set. Specifically, multiple data elements can 
 be processed simultaneously by a single instruction.
  
 The number of data elements that are processed simultaneously by a 
 single instruction is sometimes referred to as the vector length (or SIMD 
 width) of the instruction or processor executing it. In Figure 
 16-11
 , our 
 instruction stream runs with four-way SIMD execution.
  
 406",NA
 Ensure SIMD Execution Legality,"Semantically, the DPC++ execution model ensures that SIMD execution can 
 be applied to any kernel, and a set of work-items in each work- group (i.e., 
 a sub-group) may be executed concurrently using SIMD instructions. 
  
 Some implementations may instead choose to execute loops within a 
 kernel using SIMD instructions, but this is possible if and only if all original 
 data dependences are preserved, or data dependences are resolved by the 
 compiler based on privatization and reduction semantics.
  
 A single DPC++ kernel execution can be transformed from processing 
 of a single work-item to a set of work-items using SIMD instructions 
 within the work-group. Under the ND-range model, the fastest-growing 
 (unit-stride) dimension is selected by the compiler vectorizer on which 
 to generate SIMD code. Essentially, to enable vectorization given an ND- 
 range, there should be no cross-work-item dependences between any 
 two work-items in the same sub-group, or the compiler needs to 
 preserve cross-work-item forward dependences in the same sub-group.
  
 407",NA
 SIMD Masking and Cost,"In real applications, we can expect conditional statements such as an if 
 statement, conditional expressions such as a = b > a? a: b, loops with a 
 variable number of iterations, switch statements, and so on. Anything that 
 is conditional may lead to scalar control flows not executing the same code 
 paths and, just like on a GPU (Chapter 
 15
 ), can lead to decreased 
  
 409",NA
 Avoid Array-of-Struct for SIMD Efficiency,"AOS (Array-of-Struct) structures lead to gathers and scatters, which 
  
 can both impact SIMD efficiency and introduce extra bandwidth and latency 
 for memory accesses. The presence of a hardware gather-scatter 
 mechanism does not eliminate the need for this transformation—gather- 
 scatter accesses commonly need significantly higher bandwidth and latency 
 than contiguous loads. Given an AOS data layout of struct {float x; float y; 
 float z; float w;} a[4], consider a kernel operating on it as shown in Figure 
 16-15
 .
  
 411",NA
 Data Type Impact on SIMD Efficiency,"C++ programmers often use integer data types whenever they know that 
 the data fits into a 32-bit signed type, often leading to code such as
  
 int id = get_global_id(0); a[id] = b[id] + c[id];
  
 However, given that the return type of the get_global_id(0) is size_t 
 (unsigned integer, often 64-bit)
 , in some cases, the conversion reduces the 
 optimization that a compiler can legally perform. This can then lead to 
 SIMD gather/scatter instructions when the compiler vectorizes the code in 
 the kernel, for example
  
 413",NA
 SIMD Execution Using single_task,"Under a single task execution model, optimizations related to the vector 
 types and functions depend on the compiler. The compiler and runtime are 
 given a freedom either to enable explicit SIMD execution or to choose 
 scalar execution within the single_task kernel, and the result will depend 
 on the compiler implementation. For instance, the DPC++ CPU compiler 
 honors vector types and generates SIMD instructions for CPU SIMD 
 execution. The vec load, store, and swizzle function will perform 
 operations directly on vector variables, informing the compiler that data 
 elements are accessing contiguous data starting from the same (uniform) 
 location in memory and enabling us to request optimized loads/stores of 
 contiguous data.
  
 415",NA
 Summary,"To get the most out of thread-level parallelism and SIMD vector-level 
 parallelism on CPUs, we need to keep the following goals in mind:
  
 • Be familiar with all types of DPC++ parallelism and the 
 underlying CPU architectures we wish to target.
  
 • Exploit the right amount of parallelism, not more and not 
 less, at a thread level that best matches hardware 
 resources. Use vendor tooling, such as analyzers and 
 profilers, to help guide our tuning work to achieve this.
  
 • Be mindful of thread affinity and memory first touch 
 impact on program performance.
  
 • Design data structures with a data layout, alignment, and 
 data width such that the most frequently executed 
 calculations can access memory in a SIMD-friendly manner 
 with maximum SIMD parallelism.
  
 • Be mindful of balancing the cost of masking vs. code 
 branches.
  
 417",NA
CHAPTER 17,NA,NA
Programmi,NA,NA
ng for ,NA,NA
FPGAs,NA,NA
 FPGA ,NA,NA
emulator,"Kernel-based programming originally became popular as a way to access 
 GPUs. Since it has now been generalized across many types of accelerators, 
 it is important to understand how our style of programming affects the 
 mapping of code to an FPGA as well.
  
  
 Field Programmable Gate Arrays (FPGAs) are unfamiliar to the 
 majority of software developers, in part because most desktop computers 
 don’t 
  
 include an FPGA alongside the typical CPU and GPU. But FPGAs 
 are
  worth 
 knowing about because they offer advantages in many applications. The 
 same questions need to be asked as we would of other accelerators, such 
 as “When should I use an FPGA?”, “What parts of my applications should be 
 offloaded to FPGA?”, and “How do I write code that performs well on an 
 FPGA?”
  
  
 This chapter gives us the knowledge to start answering those 
  
 questions, at least to the point where we can decide whether an FPGA is",NA
 Performance Caveats,"As with any processor or accelerator, FPGA devices differ from vendor to 
 vendor or even from product generation to product generation; 
 therefore, best practices for one device may not be best practices for a 
 different device. The advice in this chapter is likely to benefit many FPGA 
 devices, both now and in the future, however…",NA
"…to achieve optimal performance for a particular FPGA, ",NA,NA
always consult the vendor’s documentation!,NA,NA
 How to Think About FPGAs,"FPGAs are commonly classified as a 
 spatial
  architecture. They benefit from 
 very different coding styles and forms of parallelism than devices that use 
 an Instruction Set Architecture (ISA), including CPUs and GPUs, which are 
  
 420",NA
 Pipeline Parallelism,"Another question that often arises from Figure 
 17-2
  is how the spatial 
 implementation of a program relates to a clock frequency and how quickly 
 a program will execute from start to finish. In the example shown, it’s easy 
 to believe that data could be loaded from memory, have multiplication and 
 addition operations performed, and have the result stored back into 
 memory, quite quickly. As the program becomes larger, potentially with 
 tens of thousands of operations across the FPGA device, it becomes 
 apparent that for all of the instructions to operate one after the other 
 (operations often depend on results produced by previous operations), it 
 might take significant time given the processing delays introduced by each 
 operation.
  
 Intermediate results between operations are updated (propagated) 
 over time in a spatial architecture as shown in Figure 
 17-3
 . For example, 
 the load executes and then passes its result into the multiplier, whose 
 result is then passed into the adder and so on. After some amount of time, 
 the intermediate data has propagated all the way to the end of the chain of 
 operations, and the final result is available or stored to memory.
  
 424",NA
*,NA,NA
+,"Figure 17-4. 
 Pipelining of a computation: Stages execute in 
 parallel
  
 When a spatial implementation is pipelined, it becomes extremely 
 efficient in the same way as a factory assembly line. Each pipeline stage 
 performs only a small amount of the overall work, but it does so quickly 
 and then begins to work on the next unit of work immediately afterward. 
 It takes many clock cycles for a 
 single
  computation to be processed by the 
 pipeline, from start to finish, but the pipeline can compute 
 many
  different 
 instances of the computation on different data simultaneously.
  
 When enough work starts executing in the pipeline, over enough 
 consecutive clock cycles, then every single pipeline stage and therefore 
 operation in the program can perform useful work during every 
  
 clock cycle, meaning that the entire spatial device performs work 
 simultaneously. This is one of the powers of spatial architectures—the 
 entire device can execute work in parallel, all of the time. We call this 
 pipeline parallelism
 .",NA
Pipeline parallelism is the primary form of parallelism ,NA,NA
exploited on FPGAs to achieve performance.,426,NA
 Kernels Consume Chip “Area”,"In existing implementations, each kernel in a DPC++ application generates 
 a spatial pipeline that consumes some resources of the FPGA (we can think 
 about this as 
 space
  or 
 area
  on the device), which is conceptually shown in 
 Figure 
 17-5
 .",NA
 When to Use an FPGA,"Like any accelerator architecture, predicting when an FPGA is the right 
 choice of accelerator vs. an alternative often comes down to knowledge 
 of the architecture, the application characteristics, and the system 
 bottlenecks. This section describes some of the characteristics of an 
 application to consider.",NA
 Lots and Lots of Work,"Like most modern compute accelerators, achieving good performance 
 requires a large amount of work to be performed. If computing a single 
 result from a single element of data, then it may not be useful to 
 leverage 
  
 428",NA
 Custom Operations or Operation Widths,"FPGAs were originally designed to perform efficient integer and bitwise 
 operations and to act as glue logic that could adapt interfaces of other 
 chips to work with each other. Although FPGAs have evolved into 
  
 computational powerhouses instead of just glue logic solutions, they are 
 still very efficient at bitwise operations, integer math operations on custom 
 data widths or types, and operations on arbitrary bit fields in packet 
 headers.
  
 The fine-grained architecture of an FPGA, described at the end of this 
 chapter, means that novel and arbitrary data types can be efficiently 
 implemented. For example, if we need a 33-bit integer multiplier or a 129-
 bit adder, FPGAs can provide these custom operations with great 
 efficiency. Because of this flexibility, FPGAs are commonly employed in 
 rapidly evolving domains, such as recently in machine learning, where the 
 data widths and operations have been changing faster than can be built 
 into ASICs.
  
 429",NA
 Scalar Data Flow,"An important aspect of FPGA spatial pipelines, apparent from Figure 
 17-4
 , 
 is that the intermediate data between operations not only stays on-chip (is 
 not stored to external memory), but that intermediate data between each 
 pipeline stage has dedicated storage registers. FPGA parallelism comes 
 from pipelining of computation such that many operations are being 
 executed concurrently, each at a different stage of the pipeline. This is 
 different from vector architectures where multiple computations are 
 executed as lanes of a shared vector instruction.
  
 The scalar nature of the parallelism in a spatial pipeline is important 
 for many applications, because it still applies even with tight data 
 dependences across the units of work. These data dependences can be 
 handled without loss of performance, as we will discuss later in this 
 chapter when talking about loop-carried dependences. The result is that 
 spatial pipelines, and therefore FPGAs, are compelling for algorithms 
 where data dependences across units of work (such as work-items) can’t 
 be broken and fine-grained communication must occur. Many optimization 
 techniques for other accelerators focus on breaking these dependences 
 though various techniques or managing communication at controlled 
 scales through features such as sub-groups. FPGAs can instead perform 
 well with communication from tight dependences and should be 
 considered for classes of algorithms where such patterns exist.
  
 LOOPS ARE FINE!
  
 A common misconception on data flow architectures is that loops 
 with either fixed or dynamic iteration counts lead to poor data flow 
 performance, because they aren’t simple feed-forward pipelines. At 
 least with the intel DPC++ and FPGA toolchains, this is not true. Loop 
 iterations can instead be a good way to produce high occupancy 
 within the pipeline, and the compilers are built around the concept 
 of allowing multiple loop iterations to execute in an overlapped way.",NA
 Low Latency and Rich Connectivity,"More conventional uses of FPGAs which take advantage of the rich input 
 and output transceivers on the devices apply equally well for developers 
 using DPC++. For example, as shown in Figure 
 17-6
 , some FPGA 
  
 accelerator cards have network interfaces that make it possible to stream 
 data directly into the device, process it, and then stream the result directly 
 back to the network. Such systems are often sought when processing 
 latency needs to be minimized and where processing through operating 
 system network stacks is too slow or needs to be offloaded.
  
  
  
 Figure 17-6. 
 Low-latency I/O streaming: FPGA connects network 
 data and computation tightly
  
 The opportunities are almost limitless when considering direct 
 input/ output through FPGA transceivers, but the options do come down 
 to what is available on the circuit board that forms an accelerator. 
 Because of the dependence on a specific accelerator card and variety of 
 such uses, aside from describing the pipe language constructs in a coming 
 section, this chapter doesn’t dive into these applications. We should 
 instead read the vendor documentation associated with a specific 
 accelerator card or search for an accelerator card that matches our 
 specific interface needs.",NA
 Customized Memory Systems,"Memory systems on an FPGA, such as function private memory or work- 
 group local memory, are built out of small blocks of on-chip memory. This 
 is important because each memory system is custom built for the specific 
 portion of an algorithm or kernel using it. FPGAs have significant on-chip 
 memory bandwidth, and combined with the formation of custom memory 
 systems, they can perform very well on applications that have atypical 
 memory access patterns and structures. Figure 
 17-7
  shows some of the 
 optimizations that can be performed by the compiler when a memory 
 system is implemented on an FPGA.
  
  
 Figure 17-7. 
 FPGA memory systems are customized by the compiler 
 for our specific code
  
 Other architectures such as GPUs have fixed memory structures that 
 are easy to reason about by experienced developers, but that can also be 
 hard to optimize around in many cases. Many optimizations on other 
 accelerators are focused around memory pattern modification to avoid 
 bank conflicts, for example. If we have algorithms that would benefit from 
 a custom memory structure, such as a different number of access ports per 
 bank or an unusual number of banks, then FPGAs can offer immediate 
 advantages. Conceptually, the difference is between writing code to use a 
 fixed memory system efficiently (most other accelerators) and having the 
 memory system custom designed by the compiler to be efficient with our 
 specific code (FPGA).
  
 432",NA
 Running on an FPGA,"There are two steps to run a kernel on an FPGA (as with any ahead-of-time 
 compilation accelerator):
  
  1. Compiling the source to a binary which can be run on 
 our hardware of interest
  
  2. Selecting the correct accelerator that we are 
 interested in at runtime
  
  
 To compile kernels so that they can run on FPGA hardware, we can use 
 the command line:
  
 dpcpp -fintelfpga my_source_code.cpp -Xshardware
  
 This command tells the compiler to turn all kernels in my_source_ 
 code.cpp into binaries that can run on an Intel FPGA accelerator and then 
 to package them within the host binary that is generated. When we execute 
 the host binary (e.g., by running ./a.out on Linux), the runtime will 
 automatically program any attached FPGA as required, before executing 
 the submitted kernels, as shown in Figure 
 17-8
 .
  
 433",NA
1,NA,NA
2,"01101 
  
 10101
  
 FPGA binary
  
 01101 
  
 10101
  
 Kernel 2
  
 Programming is automatic! The DPC++ runtime 
 programs the FPGA device behind the scenes 
 when needed, before a kernel runs on it.
  
 Figure 17-8. 
 FPGA programmed automatically at runtime",NA
FPGA programming binaries are embedded within the ,NA,NA
compiled DPC++ executable that we run on the host.  ,NA,NA
the FPGA is ,NA,NA
automatically configured behind the scenes for us.,NA,NA
When we run a host program and submit the first kernel for ,NA,NA
"execution on an FPGA, there might be a slight delay before the ",NA,NA
"kernel begins executing, while the FPGA is programmed.  ",NA,NA
resubmitting kernels for additional executions won’t see the ,NA,NA
same delay because the kernel is already programmed to the ,NA,NA
device and ready to run.,"Selection of an FPGA device at runtime was covered in Chapter 
 2
 . We 
  
 need to tell the host program where we want kernels to run because there 
  
 are typically multiple accelerator options available, such as a CPU and 
  
 GPU, in addition to the FPGA. To quickly recap one method to select an 
  
 FPGA during program execution, we can use code like that in Figure 
 17-9
 .",NA
 Compile Times,"Rumors abound that compiling designs for an FPGA can take a long time, 
 much 
  
 longer than compiling for ISA-based accelerators. The rumors are true! The 
 end 
  
 of this chapter overviews the fine-grained architectural elements of an 
 FPGA 
  
 that lead to both the advantages of an FPGA and the computationally 
 intensive 
  
 compilation (place-and-route optimizations) that can take hours in some 
 cases.
  
 The compile time from source code to FPGA hardware execution 
  
 is long enough that we don’t want to develop and iterate on our code 
  
 exclusively in hardware. FPGA development flows offer several stages 
  
 that minimize the number of hardware compilations, to make us 
  
 productive despite the hardware compile times. Figure 
 17-10
  shows 
  
 the typical stages, where most of our time is spent on the early steps 
  
 that provide fast turnaround and rapid iteration.",NA
 The FPGA Emulator,"Emulation is primarily used to functionally debug our application, to make 
 sure that it behaves as expected and produces correct results. There is no 
 reason to do this level of development on actual FPGA hardware where 
 compile times are longer. The emulation flow is activated by removing the 
 -Xshardware flag from the dpcpp compilation command and at the same 
 time using the INTEL::fpga_emulator_selector instead of the 
 INTEL::fpga_selector in our host code. We would compile using a command 
 like
  
 dpcpp -fintelfpga my_source_code.cpp
  
 Simultaneously, we would choose the FPGA emulator at runtime 
 using code such as in Figure 
 17-11
 . By using fpga_emulator_selector, 
 which uses the host processor to emulate an FPGA, we maintain a rapid 
 development and debugging process before we have to commit to the 
 lengthier compile for actual FPGA hardware.
  
 437",NA
 FPGA Hardware Compilation Occurs “Ahead-of-,NA,NA
Time”,"The 
 Full Compile and Hardware Profiling
  stage in Figure 
 17-10
  is an 
 ahead- 
 of- time
  compile in SYCL terminology. This means that the compilation of 
 the kernel to a device binary occurs when we initially compile our program 
 and not when the program is submitted to a device to be run. On an FPGA, 
 this is particularly important because
  
 438",NA
 Writing Kernels for FPGAs,"Once we have decided to use an FPGA for our application or even just 
 decided to try one out, having an idea of how to write code to see good 
 performance is important. This section describes topics that highlight 
 important concepts and covers a few topics that often cause confusion, to 
 make getting started faster.",NA
 Exposing Parallelism,"We have already looked at how pipeline parallelism is used to efficiently 
 perform work on an FPGA. Another simple pipeline example is shown in 
 Figure 
  17- 12
 .
  
  
  
 Figure 17-12. 
 Simple pipeline with five stages: Six clock cycles to 
 process an element of data
  
 440",NA
 Keeping the Pipeline Busy Using ND-Ranges,"The ND-range hierarchical execution model was described in Chapter 
 4
 . 
  
 Figure 
 17-15
  illustrates the key concepts: an ND-range execution model 
 where there is a hierarchical grouping of work-items, and where a work-
 item is the primitive unit of work that a kernel defines. This model was 
 originally developed to enable efficient programming of GPUs where work-
 items may execute concurrently at various levels of the execution model 
 hierarchy. To match the type of work that GPU hardware is efficient at, ND- 
 range work-items do not frequently communicate with each other in most 
 applications.
  
  
  
  
  
  
  
  
  
 Figure 17-15. 
 ND-range execution model: A hierarchical grouping of 
 work-items",NA
if we can structure our algorithm so that work-items ,NA,NA
"don’t need to communicate much (or at all), then nD-",NA,NA
range is a great way to generate work to keep the spatial ,NA,NA
pipeline full!,"A good example of a kernel that is efficient with an ND-range feeding 
 the pipeline is a random number generator, where creation of numbers in 
 the sequence is independent of the previous numbers generated.
  
 Figure 
 17-17
  shows an ND-range kernel that will call the random 
 number generation function once for each work-item in the 16 × 16 × 16 
 range. Note how the random number generation function takes the work- 
 item id as input.
  
 h
 .
 parallel_for
 ({
 16
 ,
 16
 ,
 16
 }, [=](auto 
 I
 ) { 
  
  
 output
 [I] = generate_random_number_from_ID(I); });
  
 Figure 17-17. 
 Multiple work-item (16 × 16 × 16) invocation of a 
 random number generator
  
 The example shows a parallel_for invocation that uses a range, with 
 only a global size specified. We can alternately use the parallel_for 
 invocation style that takes an nd_range, where both the global work size 
 and local work-group sizes are specified. FPGAs can very efficiently 
 implement work-group local memory from on-chip resources, so feel free 
 to use work-groups whenever they make sense, either because we want 
 work-group local memory or because having work-group IDs available 
 simplifies our code.
  
 445",NA
 Pipelines Do Not Mind Data Dependences!,"One of the challenges when programming vector architectures (e.g., GPUs) 
 where some work-items execute together as lanes of vector instructions is 
 structuring an algorithm to be efficient without extensive communication 
 between work-items. Some algorithms and applications lend themselves 
 well to vector hardware, and some don’t. A common cause of a poor 
 mapping is an algorithmic need for extensive sharing of data, due to data 
 dependences with other computations that are in some sense neighbors. 
  
 Sub-groups address some of this challenge on vector architectures by 
 providing efficient communication between work-items in the same sub- 
 group, as described in Chapter 
 14
 .
  
 FPGAs play an important role for algorithms that can’t be decomposed 
 into independent work. FPGA spatial pipelines are not vectorized across 
 work-items, but instead execute consecutive work-items across pipeline 
 stages. This implementation of the parallelism means that fine-grained 
 communication between work-items (even those in different work-groups) 
 can
  be implemented easily and efficiently within the spatial pipeline!
  
 446",NA
*,if,NA
+,NA,NA
-,NA,NA
*,"Figure 17-19. 
 Backward communication enables efficient data 
 dependence communication
  
 The ability to pass data backward (to an earlier stage in the pipeline) 
 is key to spatial architectures, but it isn’t obvious how to write code that 
 takes advantage of it. There are two approaches that make expressing 
 this pattern easy:
  
  1. Loops
  
  2. Intra-kernel pipes with ND-range kernels
  
 The second option is based on pipes that we describe later in this 
 chapter, but it isn’t nearly as common as loops so we mention it for 
 completeness, but don’t detail it here. Vendor documentation provides 
 more details on the pipe approach, but it’s easier to stick to loops which 
 are described next unless there is a reason to do otherwise.
  
 448",NA
 Spatial Pipeline Implementation of a Loop,"A loop is a natural fit when programming an algorithm that has data 
 dependences. Loops frequently express dependences across iterations, 
 even in the most basic loop examples where the counter that determines 
 when the loop should exit is carried across iterations (variable i in Figure 
 17-20
 ).
  
 int a = 
 0
 ; 
  
 for
  (int i=
 0
 ; i < size; i++) { 
  
  
 a = a + i; 
  
 }
  
 Figure 17-20. 
 Loop with two loop-carried dependences (i.e., i and a)
  
 In the simple loop of Figure 
 17-20
 , it is understood that the value of a 
 which is on the right-hand side of a= a + i reflects the value stored by the 
 previous loop iteration or the initial value if it’s the first iteration of the 
 loop. When a spatial compiler implements a loop, iterations of the loop can 
 be used to fill the stages of the pipeline as shown in Figure 
 17-21
 . 
  
 Notice that the queue of work which is ready to start now contains loop 
 iterations, not work-items!
  
 449",NA
 Loop Initiation Interval,"Conceptually, we probably think of iterations of a loop in C++ as executing 
 one after another, as shown in Figure 
 17-23
 . That’s the programming 
 model and is the right way to think about loops. In implementation, 
 though, compilers are free to perform many optimizations as long as most 
 behavior (i.e., defined and race-free behavior) of the program doesn’t 
 observably change. Regardless of compiler optimizations, what matters is 
 that the loop appears to execute 
 as if
  Figure 
 17-23
  is how it happened.
  
 451",NA
 Pipes,"An important concept in spatial and other architectures is a first-in 
 first- out (FIFO) buffer. There are many reasons that FIFOs are 
 important, but two properties are especially useful when thinking 
 about programming:
  
  1. There is 
 implicit control information carried 
 alongside the data
 . These signals tell us whether 
 the FIFO is empty or full and can be useful when 
 decomposing a problem into independent pieces.
  
  2. FIFOs have 
 storage capacity
 . This can make it easier to 
 achieve performance in the presence of dynamic 
 behaviors such as highly variable latencies when 
 accessing memory.
  
 Figure 
 17-28
  shows a simple example of a FIFO’s operation.
  
  
  
  
  
  
  
  
  
  
  
 Figure 17-28. 
 Example operation of a FIFO over time
  
 456",NA
Use type aliases to identify pipes.  this simplifies code and ,NA,NA
prevents accidental creation of unexpected pipes.,"Pipes have a min_capacity parameter. It defaults to 0 which is 
  
 automatic selection
 , but if specified, it guarantees that at least that number 
 of words can be written to the pipe without any being read out. This 
 parameter is useful when
  
  1. Two kernels communicating with a pipe do 
 not
  run at 
 the same time, and we need enough capacity in the 
 pipe for a first kernel to write all of its outputs 
 before a second kernel starts to run and reads from 
 the pipe.
  
  2. If kernels generate or consume data in bursts, then 
 adding capacity to a pipe can provide isolation 
 between the kernels, decoupling their performance 
 from each other. For example, a kernel producing 
  
 460",NA
 Blocking and Non-blocking Pipe Accesses,"Like most FIFO interfaces, pipes have two styles of interface: 
 blocking
  and 
 non-blocking
 . Blocking accesses wait (block/pause execution!) for the 
 operation to succeed, while non-blocking accesses return immediately 
 and set a Boolean value indicating whether the operation succeeded.
  
 The definition of success is simple: If we are reading from a pipe and 
 there was data available to read (the pipe wasn’t empty), then the read 
 succeeds. If we are writing and the pipe wasn’t already full, then the write 
 succeeds. Figure 
 17-33
  shows both forms of access member functions of 
 the pipe class. We see the member functions of a pipe that allow it to be 
 written to or read from. Recall that accesses to pipes can be blocking or 
 non-blocking.
  
 // Blocking 
  
 T
  read
 (); 
  
 void
  write
 ( const
  T
  &
 data
  );
  
 // Non-blocking 
  
 T
  read
 ( bool &success_code ); 
  
 void
  write
 ( const
  T
  &data, bool &success_code ); 
  
 Figure 17-33. 
 Member functions of a pipe that allow it to be written 
 to or read from
  
 461",NA
 For More Information on Pipes,"We could only scratch the surface of pipes in this chapter, but we should 
 now have an idea of what they are and the basics of how to use them. FPGA 
 vendor documentation has a lot more information and many examples of 
 their use in different types of applications, so we should look there if we 
 think that pipes are relevant for our particular needs.",NA
 Custom Memory Systems,"When programming for most accelerators, much of the optimization effort 
 tends to be spent making memory accesses more efficient. The same is true 
 of FPGA designs, particularly when input and output data pass through off-
 chip memory.
  
  
 There are two main reasons that memory accesses on an FPGA can be 
 worth optimizing:
  
  1. To reduce required bandwidth, particularly if some 
  
 of that bandwidth is used inefficiently
  
  2. To modify access patterns on a memory that is 
  
 leading 
 to unnecessary stalls in the spatial pipeline
  
 462",NA
 Some Closing Topics,"When talking with developers who are getting started with FPGAs, we find 
 that it often helps to understand at a high level the components that make 
 up the device and also to mention clock frequency which seems to be a 
 point of confusion. We close this chapter with these topics.",NA
 FPGA Building Blocks,"To help with an understanding of the tool flows (particularly compile 
 time), it is worth mentioning the building blocks that make up an 
  
 FPGA. These building blocks are abstracted away through DPC++ 
  
 and SYCL, and knowledge of them plays no part in typical application 
 development (at least in the sense of making code functional). Their 
 existence does, however, factor into development of an intuition for spatial 
 architecture optimization and tool flows, and occasionally in advanced 
 optimizations when choosing the ideal data type for our application, for 
 example.
  
 465",NA
 Clock Frequency,"FPGAs are extremely flexible and configurable, and that configurability 
 comes with some cost to the frequency that an FPGA runs at compared 
 with an equivalent design hardened into a CPU or any other fixed compute 
 architecture. But this is not a problem! The spatial architecture of an FPGA 
 more than makes up for the clock frequency because there are so many 
 independent operations occurring simultaneously, spread across the area 
 of the FPGA. Simply put, the frequency of an FPGA is lower 
  
 467",NA
Rule of thumb,NA,NA
 try not to exceed 90% of any resources on an ,NA,NA
FPGA and certainly not more than 90% of multiple resources.  ,NA,NA
exceeding may lead to exhaustion of routing resources which ,NA,NA
"leads to lower operating frequencies, unless we are willing to ",NA,NA
dive into lower-level FPGA details to counteract this.,NA,NA
 Summary,"In this chapter, we have introduced how pipelining maps an algorithm to 
 the FPGA’s spatial architecture. We have also covered concepts that can 
 help us to decide whether an FPGA is useful for our applications and that 
 can help us get up and running developing code faster. From this starting 
 point, we should be in good shape to browse vendor programming and 
 optimization manuals and to start writing FPGA code! FPGAs provide 
 performance and enable applications that wouldn’t make sense on other 
 accelerators, so we should keep them near the front of our mental toolbox!
  
 468",NA
CHAPTER 18,NA,NA
Libraries,"We have spent the entire book promoting the art of 
 writing our own code
 . 
  
 Now we finally acknowledge that some great programmers have already 
 written code that we can just use. Libraries are the best way to get our 
 work done. This is not a case of being lazy—it is a case of having better 
 things to do than reinvent the work of others. This is a puzzle piece worth 
 having.
  
 The open source DPC++ project includes some libraries. These 
  
 libraries can help us continue to use libstdc++, libc++, and MSVC library 
 functions even within our kernel code. The libraries are included as part of 
 DPC++ and the oneAPI products from Intel. These libraries are not tied to 
 the DPC++ compiler so they can be used with any SYCL compiler.
  
 The DPC++ library provides an alternative for programmers who 
 create heterogeneous applications and solutions. Its APIs are based on 
 familiar standards—C++ STL, Parallel STL (PSTL), and SYCL—to provide 
 high- productivity APIs to programmers. This can minimize programming 
 effort across CPUs, GPUs, and FPGAs while leading to high-performance 
 parallel applications that are portable.
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-5574-
 2_18
  
 471",NA
 Built-In Functions,"DPC++ provides a rich set of SYCL built-in functions with respect to 
 various data types. These built-in functions are available in the sycl 
 namespace on host and device with low-, medium-, and high-precision 
 support for the target devices based on compiler options, for example, the 
 -mfma, -ffast-math, and -ffp-contract=fast provided by the DPC++ 
 compiler. These built-in functions on host and device can be classified as in 
 the following:
  
 • Floating-point math functions: asin, acos, log, sqrt, floor, 
 etc. listed in Figure 
 18-2
 .
  
 • Integer functions: abs, max, min, etc. listed in 
 Figure 
 18-3
 .
  
 • Common functions: clamp, smoothstep, etc. listed in 
 Figure 
 18-4
 .
  
 • Geometric functions: cross, dot, distance, etc. listed in 
 Figure 
 18-5
 .
  
 • Relational functions: isequal, isless, isfinite, etc. listed in 
 Figure 
 18-6
 .
  
 472",NA
 Use the sycl:: Prefix with Built-In ,NA,NA
Functions,"The SYCL built-in functions should be invoked with an explicit 
  
 sycl:: prepended to the name. With the current SYCL specification, 
 calling just sqrt() is not guaranteed to invoke the SYCL built-in on all 
 implementations even if “using namespace sycl;” has been used.",NA
sYCL built-in functions should always be invoked with an ,NA,NA
explicit sycl:: in front of the built-in name. Failure to follow ,NA,NA
this advice may result in strange and non-portable results.,"If a built-in function name conflicts with a non-templated function in 
 our application, in many implementations (including DPC++), our function 
 will prevail, thanks to C++ overload resolution rules that prefer a non-
 templated function over a templated one. However, if our code has a 
 function name that is the same as a built-in name, the most portable thing 
 to do is either avoid using namespace sycl; or make sure no actual conflict 
 happens. Otherwise, some SYCL compilers will refuse to compile the code 
 due to an unresolvable conflict within their implementation. Such a 
 conflict will not be silent. Therefore, if our code compiles today, we can 
 safely ignore the possibility of future problems.
  
 474",NA
 DPC++ Library,"The DPC++ library consists of the following components:
  
 • A set of tested C++ standard APIs—we simply need to 
 include the corresponding C++ standard header files and 
 use the std namespace.
  
 • Parallel STL that includes corresponding header files. We 
 simply use #include <dpstd/...> to include them. 
  
 The DPC++ library uses namespace dpstd for the 
 extended API classes and functions.
  
 478",NA
 Standard C++ APIs in DPC++,"The DPC++ library contains a set of tested standard C++ APIs. The basic 
  
 functionality for a number of C++ standard APIs has been developed so 
  
 that these APIs can be employed in device kernels similar to how they are 
  
 employed in code for a typical C++ host application. Figure 
 18-7
  shows an 
  
 example of how to use std::swap in device code.
  
 class
  KernelSwap
 ; 
  
 std
 ::array <
 int
 ,
 2
 > arr{
 8
 ,
 9
 }; 
  
 buffer
 <
 int
 > buf{arr};
  
 { 
 }
  
 host_accessor
  host_A
 (buf); 
  
 std
 ::cout << 
 ""Before: ""
  << 
 host_A
 [
 0
 ] << 
 "", ""
  << 
 host_A
 [
 1
 ] << 
 ""
 \n
 ""
 ; 
 // End scope of host_A so that 
 upcoming kernel can operate on buf
  
 queue
  Q; 
  
 Q
 .
 submit
 ([&](
 handler &
 h
 ) { 
  
  
 accessor
  A{buf, h}; 
  
  
 h
 .
 single_task
 ([=]() { 
  
  
 // Call std::swap!
  
  
 std
 ::
 swap
 (
 A
 [
 0
 ], 
 A
 [
 1
 ]); 
  
  
 }); 
  
 });
  
 host_accessor
  host_B
 (buf); 
  
 std
 ::cout << 
 ""After:  ""
  << 
 host_B
 [
 0
 ] << 
 "", ""
  << 
 host_B
 [
 1
 ] << 
 ""
 \n
 ""
 ;
  
 Figure 18-7. 
 Using std::swap in device code
  
 479",NA
"to achieve cross-architecture portability, if a std function is ",NA,NA
not marked with “Y” in Figure ,NA,NA
18-8,NA,NA
", we need to keep ",NA,NA
portability in mind when we write device functions!,482,NA
 DPC++ Parallel STL,"Parallel STL is an implementation of the C++ standard library algorithms 
 with support for execution policies, as specified in the ISO/IEC 
 14882:2017 standard, commonly called C++17. The existing 
 implementation also supports the unsequenced execution policy specified 
 in Parallelism TS version 2 and proposed for the next version of the C++ 
 standard in the C++ working group paper P1001R1.
  
 When using algorithms and execution policies, specify the namespace 
 std::execution if there is no vendor-specific implementation of the C++17 
 standard library or pstl::execution otherwise.
  
 For any of the implemented algorithms, we can pass one of the values 
 seq, unseq, par, or par_unseq as the first parameter in a call to the 
 algorithm to specify the desired execution policy. The policies have the 
 following meanings:
  
 Execution 
 Policy
  
 Meaning
  
 seq 
  
 unseq
  
 sequential execution.
  
 Unsequenced siMD execution. this policy requires that 
 all 
  
 functions provided are safe to execute in siMD.
  
 par 
  
 par_unseq
  
 parallel execution by multiple 
 threads. Combined effect of 
 unseq and par.
  
 Parallel STL for DPC++ is extended with support for DPC++ devices 
 using special execution policies. The DPC++ execution policy specifies 
 where and how a Parallel STL algorithm runs. It inherits a standard C++ 
 execution policy, encapsulates a SYCL device or queue, and allows us to 
 set an optional kernel name. DPC++ execution policies can be used with 
 all standard C++ algorithms that support execution policies according to 
 the C++17 standard.
  
 483",NA
 DPC++ Execution Policy,"Currently, only the parallel unsequenced policy (par_unseq) is supported 
 by the DPC++ library. In order to use the DPC++ execution policy, there 
 are three steps:
  
  1. Add #include <dpstd/execution> into our code.
  
  2. Create a policy object by providing a standard policy 
 type, a class type for a unique kernel name as a 
 template argument (optional), and one of the 
 following constructor arguments:
  
 • A SYCL queue
  
 • A SYCL device
  
 • A SYCL device selector
  
 • An existing policy object with a different kernel 
 name
  
  3. Pass the created policy object to a Parallel STL 
  
 algorithm.
  
 A dpstd::execution::default_policy object is a predefined device_ policy 
 created with a default kernel name and default queue. This can be used to 
 create custom policy objects or passed directly when invoking an 
 algorithm if the default choices are sufficient.
  
 Figure 
 18-9
  shows examples that assume use of the using namespace 
 dpstd::execution; directive when referring to policy classes and functions.
  
 484",NA
 FPGA Execution Policy,"The fpga_device_policy class is a DPC++ policy tailored to achieve better 
 performance of parallel algorithms on FPGA hardware devices. Use the 
 policy when running the application on FPGA hardware or an FPGA 
 emulation device:
  
  1. Define the _PSTL_FPGA_DEVICE macro to run on 
 FPGA devices and additionally _PSTL_FPGA_EMU 
 to run on an FPGA emulation device.
  
  2. Add #include <dpstd/execution> to our code.
  
  3. Create a policy object by providing a class type for a 
 unique kernel name and an unroll factor (see 
 Chapter 
 17
 ) as template arguments (both optional) 
 and one of the following constructor arguments:
  
 • A SYCL queue constructed for 
 the FPGA selector
  (the 
 behavior is undefined with any other device type)
  
 • An existing FPGA policy object with a different 
 kernel name and/or unroll factor
  
  4. Pass the created policy object to a Parallel STL algorithm.
  
 485",NA
 Using DPC++ Parallel STL,"In order to use the DPC++ Parallel STL, we need to include Parallel STL 
 header files by adding a subset of the following set of lines. These lines are 
 dependent on the algorithms we intend to use:
  
 • #include <dpstd/algorithm>
  
 • #include <dpstd/numeric>
  
 • #include <dpstd/memory>
  
 486",NA
 Using Parallel STL with USM,"The following examples describe two ways to use the Parallel STL 
  
 algorithms in combination with USM:
  
 • Through USM pointers
  
 • Through USM allocators
  
 If we have a USM allocation, we can pass the pointers to the start and 
  
 (one past the) end of the allocation to a parallel algorithm. It is important 
  
 to be sure that the execution policy and the allocation itself were created 
  
 for the same queue or context, to avoid undefined behavior at runtime.
  
 490",NA
 Error Handling with DPC++ Execution ,NA,NA
Policies,"As detailed in Chapter 
 5
 , the DPC++ error handling model supports two 
 types of errors. With 
 synchronous
  errors, the runtime throws exceptions, 
 while 
 asynchronous
  errors are only processed in a user-supplied error 
 handler at specified times during program execution.
  
  
 For Parallel STL algorithms executed with DPC++ policies, handling of 
 all errors, synchronous or asynchronous, is a responsibility of the caller. 
  
 Specifically
  
 • No exceptions are thrown explicitly by algorithms.
  
 • Exceptions thrown by the runtime on the host CPU, 
 including DPC++ synchronous exceptions, are passed 
 through to the caller.
  
 • DPC++ asynchronous errors are not handled by the 
 Parallel STL, so must be handled (if any handling is 
 desired) by the calling application.
  
 To process DPC++ asynchronous errors, the queue associated with 
 a DPC++ policy must be created with an error handler object. The 
 predefined policy objects (default_policy and others) have no error 
 handlers, so we should create our own policies if we need to process 
 asynchronous errors.",NA
 Summary,"The DPC++ library is a companion to the DPC++ compiler. It helps us with 
 solutions for portions of our heterogeneous applications, using pre-built 
 and tuned libraries for common functions and parallel patterns. The 
 DPC++ library allows explicit use of the C++ STL API within kernels, it 
 streamlines cross-architecture programming with Parallel STL algorithm 
 extensions, and it increases the successful application of parallel",NA
CHAPTER 19,NA,NA
Memory ,NA,NA
Model and ,NA,NA
Atomics,"Memory consistency
  is not an esoteric concept if we want to be good 
 parallel programmers. It is a critical piece of our puzzle, helping us to 
 ensure that data is where we need it when we need it and that its values 
 are what we are expecting. This chapter brings to light key things we need 
 to master in order to ensure our program hums along correctly. This topic 
 is not unique to SYCL or to DPC++.
  
 Having a basic understanding of the memory (consistency) model of 
 a programming language is necessary for 
 any
  programmer who wants to 
 allow concurrent updates to memory (whether those updates originate 
 from multiple work-items in the same kernel, multiple devices, or both). 
 This is true regardless of how memory is allocated, and the content of",NA
 What Is in a Memory Model?,"This section expands upon the motivation for programming languages to 
 contain a memory model and introduces a few core concepts that parallel 
 programmers should familiarize themselves with:
  
 • Data races and synchronization
  
 • Barriers and fences
  
 • Atomic operations
  
 • Memory ordering
  
 Understanding these concepts at a high level is necessary to appreciate 
 their expression and usage in C++, SYCL, and DPC++. Readers with 
  
 extensive experience in parallel programming, especially using C++, may 
 wish to skip ahead.
  
 497",NA
 Data Races and Synchronization,"The 
 operations
  that we write in our programs typically do not map directly 
 to a single hardware instruction or micro-operation. A simple addition 
 operation such as data[i] += x may be broken down into a sequence of 
 several instructions or micro-operations:
  
  1. Load data[i] from memory into a temporary 
  
 (register).
  
  2. Compute the result of adding x to data[i].
  
  3. Store the result back to data[i].
  
 This is not something that we need to worry about when developing 
 sequential applications—the three stages of the addition will be executed 
 in the order that we expect, as depicted in Figure 
 19-1
 .
  
 tmp = data [i]
  
 tmp += x
  
 data [i] = tmp
  
 Figure 19-1. 
 Sequential execution of data[i] += x broken into three 
 separate operations
  
 Switching to parallel application development introduces an extra 
 level of complexity: if we have multiple operations being applied to the 
 same data concurrently, how can we be certain that their view of that data 
 is consistent? Consider the situation shown in Figure 
 19-2
 , where two 
 executions of data[i] += x have been interleaved. If the two executions 
  
 498",NA
Massively parallel applications should not be concerned ,NA,NA
with the exact order in which individual work-items ,NA,NA
execute!,NA,NA
 Barriers and Fences,"One way to prevent data races between work-items in the same group is to 
 introduce synchronization across different program instances using work- 
 group barriers and appropriate memory fences. We could use a work- 
 group barrier to order our updates of data[i] as shown in Figure 
 19-5
 , and 
 an updated version of our example kernel is given in Figure 
 19-6
 . Note that 
 because a work-group barrier does not synchronize work-items in 
 different groups, our simple example is only guaranteed to execute 
 correctly if we limit ourselves to a single work-group!
  
 tmp = data [i]
  
 tmp += x
  
 data [i] = tmp
  
 tmp = data [i]
  
 tmp += x
  
 data [i] = tmp
  
 Figure 19-5. 
 Two instances of data[i] += x separated by a barrier
  
 501",NA
 Atomic Operations,"Atomic operations enable concurrent access to a memory location without 
 introducing a data race. When multiple atomic operations access the same 
 memory, they are guaranteed not to overlap. Note that this guarantee does 
 not apply if only some of the accesses use atomics and that it is our 
 responsibility as programmers to ensure that we do not concurrently 
 access the same data using operations with different atomicity guarantees.",NA
Mixing atomic and non-atomic operations on the same ,NA,NA
memory location(s) at the same time results in ,NA,NA
undefined behavior!,"If our simple addition is expressed using atomic operations, the result 
 may look like Figure 
 19-8
 —each update is now an indivisible chunk of 
 work, and our application will always produce the correct result. The 
 corresponding code is shown in Figure 
 19-7
 —we will revisit the atomic_ 
 ref class and the meaning of its template arguments later in the chapter.
  
 int
 * data = 
 malloc_shared
 <
 int
 >(N, Q); 
 std
 ::
 fill
 (data, 
 data + N, 
 0
 );
  
 Q
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) { 
  
   
 int
  j = i % M; 
  
   
 atomic_ref<
 int
 , 
 memory_order
 ::relaxed, 
 memory_scope
 ::system, 
  
    
 access
 ::
 address_space
 ::global_space> 
 atomic_data
 (
 data
 [j]); 
  
  
 atomic_data += 
 1
 ; 
  
 }).
 wait
 ();
  
 for
  (
 int
  i = 
 0
 ; i < N; ++i) { 
  
  
 std
 ::cout << 
 ""data [""
  << i << 
 ""] = ""
  << 
 data
 [i] << 
 ""
 \n
 ""
 ; }
  
 Figure 19-7. 
 Avoiding a data race using atomic operations
  
 503",NA
 Memory Ordering,"Even within a sequential application, optimizing compilers and the 
 hardware are free to re-order operations if they do not change the 
 observable behavior of an application. In other words, the application 
 must behave 
 as if
  it ran exactly as it was written by the programmer.
  
 504",NA
 The Memory Model,"The chapter so far has introduced the concepts required to understand the 
 memory model. The remainder of the chapter explains the memory model 
 in detail, including
  
 • How to express the memory ordering requirements of 
 our kernels",NA
 The memory_order Enumeration Class,"The memory model exposes different memory orders through six values 
 of 
  
 the memory_order enumeration class, which can be supplied as arguments 
  
 to fences and atomic operations. Supplying a memory order argument 
  
 to an operation tells the compiler what memory ordering guarantees are 
  
 required for all other memory operations (to any address) 
 relative to that 
  
 operation
 , as explained in the following:
  
 508",NA
 The memory_scope Enumeration Class,"The standard C++ memory model assumes that applications execute on a 
 single device with a single address space. Neither of these assumptions 
 holds for DPC++ applications: different parts of the application execute on 
 different devices (i.e., a host device and one or more accelerator devices); 
 each device has multiple address spaces (i.e., private, local, and global); 
 and the global address space of each device may or may not be disjoint 
 (depending on USM support).
  
 In order to address this, DPC++ extends the C++ notion of memory 
 order to include the 
 scope
  of an atomic operation, denoting the minimum 
 set of work-items to which a given memory ordering constraint applies. 
 The set of scopes are defined by way of a memory_scope enumeration 
 class:
  
 • memory_scope::work_item
  
 The memory ordering constraint applies only to 
 the calling work-item. This scope is only useful for 
 image operations, as all other operations within a 
 work-item are already guaranteed to execute in 
 program order.
  
 • memory_scope::sub_group, memory_scope::work_ 
 group
  
 The memory ordering constraint applies only to 
 work- items in the same sub-group or work-group 
 as the calling work-item.
  
 • memory_scope::device
  
 The memory ordering constraint applies only to 
 work- items executing on the same device as the 
 calling work- item.",NA
 Querying Device Capabilities,"To ensure compatibility with devices supported by previous versions of 
 SYCL and to maximize portability, DPC++ supports OpenCL 1.2 devices 
 and other hardware that may not be capable of supporting the full C++ 
 memory model (e.g., certain classes of embedded devices). DPC++ 
 provides device queries to help us reason about the memory order(s) 
 and memory scope(s) supported by the devices available in a system:
  
 512",NA
 Barriers and Fences,"All previous usages of barriers and fences in the book so far have 
 ignored the issue of memory order and scope, by relying on default 
 behavior.
  
 Every group barrier in DPC++ acts as an acquire-release fence to all 
 address spaces accessible by the calling work-item and makes preceding 
 writes visible to at least all other work-items in the same group. This 
 ensures memory consistency within a group of work-items after a barrier, 
 in line with our intuition of what it means to synchronize (and the 
  
 definition of the 
 synchronizes-with
  relation in C++).
  
 The atomic_fence function gives us more fine-grained control than 
 this, allowing work-items to execute fences with a specified memory 
 order and scope. Group barriers in future versions of DPC++ may 
 similarly accept an optional argument to adjust the memory scope of the 
 acquire- release fences associated with a barrier.
  
 514",NA
 Atomic Operations in DPC++,"DPC++ provides support for many kinds of atomic operations on a variety 
 of data types. All devices are guaranteed to support atomic versions of 
 common operations (e.g., loads, stores, arithmetic operators), as well as the 
 atomic 
 compare-and-swap
  operations required to implement lock-free 
 algorithms. The language defines these operations for all fundamental 
 integer, floating-point, and pointer types—all devices must support these 
 operations for 32-bit types, but 64-bit-type support is optional.",NA
 The atomic Class,"The std::atomic class from C++11 provides an interface for creating and 
 operating on atomic variables. Instances of the atomic class own their data, 
 cannot be moved or copied, and can only be updated using atomic 
 operations. These restrictions significantly reduce the chances of using the 
 class incorrectly and introducing undefined behavior. Unfortunately, they 
 also prevent the class from being used in DPC++ kernels—it is impossible 
 to create atomic objects on the host and transfer them to the device! We 
 are free to continue using std::atomic in our host code, but attempting to 
 use it inside of device kernels will result in a compiler error.
  
 ATOMIC CLASS DEPRECATED IN SYCL 2020 AND DPC++
  
 the syCl 1.2.1 specification included a cl::sycl::atomic class that is 
 loosely based on the std::atomic class from C++11. We say 
 loosely 
 because there are some differences between the interfaces of the two 
 classes, most notably that the syCl 1.2.1 version does not own its 
 data and defaults to a relaxed memory ordering.
  
 the cl::sycl::atomic class is fully supported by dpC++, but its use is 
 discouraged to avoid confusion. We recommend that the atomic_ref 
 class (covered in the next section) be used in its place.
  
 515",NA
 The atomic_ref Class,"The std::atomic_ref class from C++20 provides an alternative interface 
  
 for atomic operations which provides greater flexibility than std::atomic. 
  
 The biggest difference between the two classes is that instances of 
  
 std::atomic_ref do not own their data but are instead constructed from 
  
 an existing non-atomic variable. Creating an atomic reference effectively 
  
 acts as a promise that the referenced variable will only be accessed 
  
 atomically for the lifetime of the reference. These are exactly the 
 semantics 
  
 needed by DPC++, since they allow us to create non-atomic data on the 
  
 host, transfer that data to the device, and treat it as atomic data only after 
  
 it has been transferred. The atomic_ref class used in DPC++ kernels is 
  
 therefore based on std::atomic_ref.
  
 We say 
 based on
  because the DPC++ version of the class includes three 
  
 additional template arguments as shown in Figure 
 19-11
 .
  
 template
  <
 typename
  T
 , 
  
     
 memory_order
  DefaultOrder
 , 
  
     
 memory_scope
  DefaultScope
 , 
  
     
 access
 ::
 address_space
  AddressSpace
 > 
  
 class
  atomic_ref
  { 
  
 public: 
  
   
 using
  value_type
  = 
 T
 ; 
  
   
 static constexpr size_t
  required_alignment = 
  
    
 /* implementation-defined */
 ; 
  
   
 static constexpr bool
  is_always_lock_free = 
  
    
 /* implementation-defined */
 ; 
  
   
 static constexpr
  memory_order default_read_order = 
  
    
 memory_order_traits
 <
 DefaultOrder
 >::read_order; 
  
   
 static constexpr
  memory_order default_write_order = 
  
    
 memory_order_traits
 <
 DefaultOrder
 >::write_order; 
  
   
 static constexpr
  memory_order default_read_modify_write_order = 
  
  
  
 DefaultOrder; 
  
   
 static constexpr
  memory_scope default_scope = DefaultScope;
  
  
 explicit
  atomic_ref
 (
 T
 &
  obj
 ); 
  
  
 atomic_ref
 (
 const
  atomic_ref
 &
  ref
 ) 
 noexcept
 ; };
  
 Figure 19-11. 
 Constructors and static members of the atomic_ref 
 class
  
 516",NA
 Using Atomics with Buffers,"As discussed in the previous section, there is no way in DPC++ to allocate 
  
 atomic data and move it between the host and device. To use atomic 
  
 operations in conjunction with buffers, we must create a buffer of non- 
  
 atomic data to be transferred to the device and then access that data 
  
 through an atomic reference.
  
 Q
 .
 submit
 ([&](
 handler&
  h
 ) { 
  
  
 accessor
  acc{buf, h}; 
  
  
 h
 .
 parallel_for
 (N, [=](
 id
 <
 1
 > 
 i
 ) { 
  
   
 int
  j = i % M; 
  
   
 atomic_ref<
 int
 , 
 memory_order
 ::relaxed, 
 memory_scope
 ::system, 
  
  
  
 access
 ::
 address_space
 ::global_space> 
 atomic_acc
 (
 acc
 [j]); 
  
  
 atomic_acc += 
 1
 ; 
  
  
 }); 
  
 });
  
 Figure 19-15. 
 Accessing a buffer via an explicitly created atomic_ref
  
 520",NA
 Using Atomics with Unified Shared Memory,"As shown in Figure 
 19-17
  (reproduced from Figure 
 19-7
 ), we can construct 
 atomic references from data stored in USM in exactly the same way as we 
 could for buffers. Indeed, the only difference between this code and the 
 code shown in Figure 
 19-15
  is that the USM code does not require buffers 
 or accessors.
  
 q
 .
 parallel_for
 (
 range
 <
 1
 >(N), [=](
 size_t
  i
 ) { 
  
  
 int
  j = i % M; 
  
  
 atomic_ref<
 int
 , 
 memory_order
 ::relaxed, 
 memory_scope
 ::system, 
  
   
 access
 ::
 address_space
 ::global_space> 
 atomic_data
 (
 data
 [j]); 
  
 atomic_data += 
 1
 ; 
  
 }).
 wait
 ();
  
 Figure 19-17. 
 Accessing a USM allocation via an explicitly created 
 atomic_ref
  
 There is no way of using only standard DPC++ features to mimic the 
 shorthand syntax provided by atomic accessors for USM pointers. 
 However, we expect that a future version of DPC++ will provide a 
 shorthand built on top of the mdspan class that has been proposed for 
 C++23.
  
 522",NA
 Using Atomics in Real Life,"The potential usages of atomics are so broad and varied that it would be 
 impossible for us to provide an example of each usage in this book. We 
 have included two representative examples, with broad applicability 
 across domains:
  
  1. Computing a histogram
  
  2. Implementing device-wide synchronization",NA
 Computing a Histogram,"The code in Figure 
 19-18
  demonstrates how to use relaxed atomics in 
 conjunction with work-group barriers to compute a histogram. The kernel 
 is split by the barriers into three phases, each with their own atomicity 
 requirements. Remember that the barrier acts both as a synchronization 
 point and an acquire-release fence—this ensures that any reads and writes 
 in one phase are visible to all work-items in the work-group in later 
 phases.
  
 The first phase sets the contents of some work-group local memory to 
 zero. The work-items in each work-group update independent locations in 
 work-group local memory by design—race conditions cannot occur, and 
 no atomicity is required.
  
 The second phase accumulates partial histogram results in local 
  
 memory. Work-items in the same work-group may update the same 
  
 locations in work-group local memory, but synchronization can be 
 deferred until the end of the phase—we can satisfy the atomicity 
 requirements using memory_order::relaxed and 
 memory_scope::work_group.
  
 The third phase contributes the partial histogram results to the 
 total stored in global memory. Work-items in the same work-group 
 are guaranteed to read from independent locations in work-group 
 local memory, but may update the same locations in global memory—
 we",NA
 Implementing Device-Wide ,NA,NA
Synchronization,"Back in Chapter 
 4
 , we warned against writing kernels that attempt to 
 synchronize work-items across work-groups. However, we fully expect 
 several readers of this chapter will be itching to implement their own 
 device-wide synchronization routines atop of atomic operations and that 
 our warnings will be ignored.",NA
device-wide synchronization is currently not portable and is ,NA,NA
best left to expert programmers.  Future versions of the ,NA,NA
language will address this.,"The code discussed in this section is dangerous and should not 
  
 be expected to work on all devices, because of potential differences in 
 scheduling and concurrency guarantees. The memory ordering guarantees 
 provided by atomics are orthogonal to forward progress guarantees; and, 
 at the time of writing, work-group scheduling in SYCL and DPC++ is 
 completely implementation-defined. Formalizing the concepts and 
 terminology required to discuss execution models and scheduling 
  
 guarantees is currently an area of active academic research, and future 
 versions of DPC++ are expected to build on this work to provide additional 
 scheduling queries and controls. For now, these topics should be 
  
 considered expert-only.
  
  
 Figure 
 19-19
  shows a simple implementation of a device-wide latch (a 
 single-use barrier), and Figure 
 19-20
  shows a simple example of its usage. 
  
 Each work-group elects a single work-item to signal arrival of the group 
 at the latch and await the arrival of other groups using a naïve spin-loop, 
 while the other work-items wait for the elected work-item using a work- 
 group barrier. It is this spin-loop that makes device-wide synchronization 
 unsafe; if any work-groups have not yet begun executing or the currently 
 executing work-groups are not scheduled fairly, the code may deadlock.",NA
relying on memory order alone to implement ,NA,NA
synchronization primitives may lead to deadlocks in the ,NA,NA
absence of independent forward progress guarantees!,"For the code to work correctly, the following three conditions must 
 hold:
  
  1. The atomic operations must use memory orders at 
 least as strict as those shown, in order to guarantee 
 that the correct fences are generated.
  
  2. Each work-group in the ND-range must be capable of 
 making forward progress, in order to avoid 
  
 a single work-group spinning in the loop from 
 starving a work-group that has yet to increment the 
 counter.
  
  3. The device must be capable of executing all work-
 groups in the ND- range concurrently, in order to 
 ensure that all work-groups in the ND- range 
 eventually reach the latch.
  
 526",NA
 Summary,"This chapter provided a high-level introduction to memory model and 
  
 atomic classes. Understanding how to use (and how not to use!) these 
  
 classes is key to developing correct, portable, and efficient parallel 
  
 programs.
  
 528",NA
 For More Information,"• A. Williams, 
 C++ Concurrency in Action: Practical 
 Multithreading
 , Manning, 2012, 978-1933988771
  
 • H. Sutter, “atomic<> Weapons: The C++ Memory Model 
 and Modern Hardware”, 
 https://herbsutter.
  
 com/2013/02/11/atomic-weapons-the-c-
 memory-model-and-modern-hardware/
  
 • H-J. Boehm, “Temporarily discourage memory_order_ 
 consume,” 
 http://wg21.link/p0371
  
 • C++ Reference, “std::atomic,”
  
 https://en.cppreference.com/w/cpp/atomic/atomic
  
 • C++ Reference, “std::atomic_ref,”
  
 https://en.cppreference.com/w/cpp/atomic/ 
 atomic_ref
  
 529",NA
 EPILOGUE,NA,NA
Future ,NA,NA
Direction of ,NA,NA
DPC++,"Take a moment now to feel the peace and calm of knowing that we 
 finally understand everything about programming using SYCL and 
 DPC++. All the puzzle pieces have fallen into place.
  
 Before we get too comfortable, let’s note that this book was written 
 at an exciting time for SYCL and DPC++. It has been a period of 
  
 rapid development that coincided with the release of the first DPC++ 
 specification and the SYCL 2020 provisional specification. We’ve 
  
 endeavored to ensure that the code samples, in all previous chapters, 
 compile with the open source DPC++ compiler at the time that this book 
 was sent to publication (Q3 2020) and execute on a wide range of 
 hardware. However, the future-looking code shown in this epilogue does 
 not compile with any compiler as of mid-2020.",NA
 Alignment with C++20 and C++23,"Maintaining close alignment between SYCL, DPC++, and ISO C++ has two 
 advantages. First, it enables SYCL and DPC++ to leverage the newest and 
 greatest features of standard C++ to improve developer productivity. 
 Second, it increases the chances of heterogeneous programming features 
  
 532",NA
 Address Spaces,"As we have seen in earlier chapters, there are some cases in which 
  
 otherwise simple codes are complicated by the existence of memory 
 spaces. We are free to use regular C++ pointers in most places, but at other 
 times are required to use the multi_ptr class and explicitly specify which 
  
 address space(s) their code is expected to support.
  
 534",NA
 Extension and Specialization ,NA,NA
Mechanism,"Chapter 
 12
  introduced an expressive set of queries enabling the host 
  
 to extract information about a device at runtime. These queries enable 
  
 runtime parameters such as work-group size to be tuned for a specific 
  
 device and for different kernels implementing different algorithms to be 
  
 dispatched to different types of device.
  
 Future versions are expected to augment these runtime queries with 
  
 compile-time queries, allowing code to be specialized based on whether 
  
 an implementation understands a vendor extension. Figure 
 EP-5
  shows 
  
 how the preprocessor could be used to detect whether the compiler 
  
 supports a specific vendor extension.
  
 #ifdef SYCL_EXT_INTEL_SUB_GROUPS 
  
 sycl
 ::
 ext
 ::
 intel
 ::
 sub_group 
 sg = 
 it
 .
 get_sub_group
 (); 
 #endif
  
 Figure EP-5. 
 Checking for Intel sub-group extension compiler 
  
 support with #ifdef
  
 536",NA
 Hierarchical Parallelism,"As we noted back in Chapter 
 4
 , we consider the hierarchical parallelism in 
 older versions of SYCL to be an experimental feature and expect it to be 
 slower than basic data-parallel and ND-range kernels in its adoption of 
 new language features.
  
 There are a 
 lot
  of new language features in DPC++ and SYCL 2020, and 
 several of them are currently incompatible with hierarchical parallelism 
 (e.g., sub-groups, group algorithms, reductions). Closing this gap would 
 help to improve programmer productivity and would enable more 
  
 compact syntax for some simple cases. The code in Figure 
 EP-7
  shows a 
  
 537",NA
 Summary,"There is already a lot of excitement around SYCL and DPC++, and this is 
 just the beginning! We (as a community) have a long path ahead of us, 
 and it will take significant continued effort to distil the best practices for 
 heterogeneous programming and to design new language features that 
 strike the desired balance between performance, portability, and 
 productivity.
  
 538",NA
 For More Information,"• Khronos SYCL Registry, 
 www.khronos.org/registry/ 
 SYCL/
  
 • J. Hoberock et al., “A Unified Executors Proposal for 
 C++,” 
 http://wg21.link/p0443
  
 • H. Carter Edwards et al., “mdspan: A Non-Owning 
 Multidimensional Array Reference,” 
 http://wg21.
  
 link/p0009
  
 • D. Hollman et al., “Production-Quality mdspan 
 Implementation,” 
 https://github.com/kokkos/ 
 mdspan
  
 539",NA
Index ,NA,NA
A ,NA,NA
B,"accelerator_selector, 
 39 
  
 Accessors, 
 see 
 Buffers, accessors 
 Actions, 
 53–54 
  
 Address spaces, 
 534–536
  
 Barrier function, 
 215,509 
  
  
 in ND-range kernels, 
 223 
  
 in hierarchical kernels, 
 226 
 Broadcast function, 
 234
  
 Ahead-of-time (AOT) 
  
 Buffers, 
 66
  
  
  
 compilation, 
 301 
  
  
 vs.
  just-in-time (JIT), 
 301 
  
 all_of function, 
 235 
  
 Amdahl’s Law, 
 9 
  
 Anonymous function objects, 
 see 
  
  
 Lambda function 
  
 any_of function, 
 235 
  
 Asynchronous errors, 
 136–142 
 Asynchronous Task Graphs, 
 15
  
  
 access modes, 
 74 
  
  
 accessors, 
 72–74 
  
  
 context_bound, 
 181 
  
  
 host memory, 
 182 
  
  
 use_host_ptr, 
 180,181 
  
  
 use_mutex, 
 180–181 
  
 build_with_kernel_type, 
 256 
 Built-in functions, 
 472–478
  
 atomic_fence function, 
 514 
 Atomic operations",NA
C,"atomic_fence, 
 513 
  
 atomic_ref class, 
 503 
  
 data race, 
 17,305,498–500 
  
 device-wide synchronization, 
  
 Central Processing Unit (CPU, 
 46–48,387–417 
  
 Choosing devices, 
 29 
  
 Collective functions, 
 217,234
  
 525–528 
  
 broadcast, 
 234
  
 std:atomic class, 
 515 
  
 std:atomic_ref class, 
 516–
 520
  
 load and store, 
 238,239 
 shuffles, 
 235–238
  
 Unified Shared Memory, 
 522
  
 vote, 
 235
  
 541
  
 © Intel Corporation 2021 
  
 J. Reinders et al., 
 Data Parallel C++
 , 
 https://doi.org/10.1007/978-1-4842-
 5574-2",NA
D,"function, 
 119–
 122 
  
 private_memory 
  
  
 Data management 
 buffers, 
 66 
  
 explicit, 
 64,65
  
 class, 
 123,124 
  
 loops 
 vs
 . kernels, 
 95,96 
  
 multidimensional kernels, 
  
 images, 
 66 
  
 93–95
  
 implicit, 
 65 
  
 strategy selection, 
 66,86,87 
 USM, 
 66,149–171,522 
  
  
 advantage of, 
 66 
  
  
 allocations, 
 67,68 
  
  
 explicit data 
  
  
  
 movement, 
 68,69 
  
  
 implicit data 
  
  
  
 movement, 
 70,71
  
 542",NA
E,"pipes, 
 456–461 
  
 First-in first-out (FIFO), 
 456,457
  
  
 Error handling, 
 131–146 
  
 Event, 
 78,198 
  
 Extension and specialization 
  
 mechanism, 
 536–537
  
 fpga_selector, 
 39 
  
 FPGA emulation, 
 436 
  
 Functions, built-in, 
 472–478 
 functors, 
 see 
 Named function 
 objects",NA
F,"Fallback, 
 56–58 
  
 Fences, 
 496 
  
 Fencing memory, 
 215 
  
 Field Programmable Gate Arrays",NA
G,"get_access, 
 185 
  
 get_global_id(), 
 115 
 get_info, 
 285
  
  
 (FPGAs), 
 43–44,419–469
  
 get_local_id(), 
 115",NA
H,"Handler class, 
 50–51,87–89 
  
 Heterogeneous Systems, 
 10–11 
 Hierarchical parallelism, 
 118–
 124, 
  
  
 537,538 
  
 Host code, 
 27 
  
 Host device, 
 48 
  
  
 development and debugging, 
  
  
  
 35–38 
  
  
 fallback queue, 
 56–58 
  
 host_selector, 
 39
  
 half-precision 
  
  
 floating-point, 
 382",NA
I,"predication, 
 364 
  
  
 masking, 
 364 
  
  
 offloading kernels 
  
  
  
 abstraction, 
 369 
  
  
  
 cost of, 
 372,373 
  
  
  
 software drivers, 
 370 
  
  
  
 SYCL runtime library, 
 369 
  
 profiling kernels, 
 378 
  
 Graph scheduling 
  
  
 command group
  
 id class, 
 103 
  
 In-order queues, 
 77 
  
 Initializing data, 
 310,311,313, 
  
  
 315–318 
  
 Initiation interval, 
 451 
  
 Intermediate representation (IR), 
  
 252,253 
  
 Interoperability, 
 241,251–254 
  
 item class, 
 105
  
 actions, 
 198 
  
 event-based dependences, 
  
 198,199",NA
J ,"Just-in-time (JIT), 
 301
  
  
  
 host synchronization, 
 209–211 
 GPU, 
 see 
 Graphics Processing 
  
 vs.
  ahead-of-time (AOT), 
 301
  
  
 Units 
  
 Graph scheduling, 
 196",NA
K,"group class, 
 116–117 
  
 Group functions, 
 340,341 
 Gustafson, 
 9
  
 544
  
 Kernels, 
 241–257 
  
  
 advantages and 
  
  
  
 disadvantages, 
 242",NA
M,"malloc functions, 
 154,155 
  
 Map pattern, 
 325,326,341,342 
  
 mem_advise(), 
 168 
  
 memcpy, 
 151,163,208 
  
 Memory allocation, 
 61–89 
  
 Memory consistency, 
 215, 
  
  
 496–506 
  
 Memory Fence, 
 226 
  
 Memory model, 
 224,497,506,507 
  
 barriers and fences, 
  
  
  
  
 501,502,514 
  
  
 C++ and SYCL/DPC++, 
 508 
  
  
 data races and synchronization,",NA
L,"498–500 
  
 definition, 
 497
  
  
 Lambda function, 
 18–21,244–248 
 Latency and Throughput, 
 7–8
  
 memory consistency, 
 495,496 
 memory_order enumeration 
  
 Libraries 
  
 class, 
 508–510
  
 built-in functions, 
 472–474 
 common functions, 
 477 
 geometric functions, 
 477 
 host and device, 
 472
  
 memory_scope enumeration 
  
 class, 
 511,512 
  
 ordering, 
 504–506 
  
 querying device capabilities, 
  
 integer functions, 
 476 
  
 512–514
  
 math functions, 
 475 
  
 memory_order enumeration class, 
  
 relational functions, 
 478 508–510
  
 load() member function, 
 268 
  
 memory_scope enumeration class, 
  
 Local Accessor, 
 223 511,512
  
 Local Memory, 
 217–219 
  
  
 in ND-Range kernels, 
 223 
  
 in hierarchical kernels, 
 226 
 Loop initiation interval, 
 451 
 Loop pipelining, 
 449
  
 memset function, 
 161 
  
 Multiarchitecture binaries, 
 300 
 Multidimensional Kernels, 
 93–95 
 Multiple translation 
  
 units, 
 319,320
  
 545",NA
N,"Named function objects, 
 248–251 
 ND-range kernels, 
 106–107,113 
  
 example, 
 225–226
  
  
 USM, 
 490,491 
  
 Pipes, 
 456 
  
 Pipeline parallelism, 
 424 
  
 Platform model 
  
  
 compilation model, 
 300–303",NA
O,"host device, 
 299 
  
 multiarchitecture binary, 
 300
  
  
 oneAPI DPC++ Library 
  
  
 (oneDPL), 
 339 
  
 Out-of-order (OoO) queues, 
 78
  
 SYCL and DPC++, 
 298 
 Portability, 
 21 
  
 prefetch (), 
 167 
  
 Program build options, 
 256",NA
P ,"Pack, 
 332,333,348,349 
 parallel_for, 
 118",NA
Q ,"Queries
  
  
 parallel_for_work_group 
  
  
  
 function, 
 227 
  
 parallel_for_work_item function, 
 227 
 Parallel patterns, 
 323–351 
  
  
 map, 
 325,326
  
 device information, 
 290–292 
  
 kernel information, 
 292,294 
  
 local memory type, 
 217 
  
 memory model, 
 506–507 
  
 unified shared memory, 
 168–
 170
  
 pack, 
 332,333 
  
 Queues
  
 properties, 
 324 
  
 reduction, 
 328–330 
 scan, 
 330,331 
  
 stencil, 
 326–328 
  
 unpack, 
 333
  
 binding to a device, 
 34 
  
 definition, 
 31,32 
  
 device_selector class, 
 34,39,40 
 multiple queues, 
 33,34
  
 Parallel STL (PSTL) 
  
 algorithms, 
 486",NA
R,"DPC++ execution policy, 
 484,485 
 dpstd :binary_search 
 algorithm, 
  
 Race Condition, 
 16 
  
 Reduction library, 
 334–337
  
 489,490 
  
 Reduction patterns, 
 328–330,
  
 FPGA execution policy, 
 485,486 
  
 344,345
  
 requirements, 
 487 
  
 Run time type information 
  
 std:fill function, 
 487,488 
  
 (RTTI), 
 29",NA
S,"Sample code download, 
 3 
  
 Scaling, 
 9–10 
  
 Scan patterns, 
 330,331,345–348 
 Selecting devices, 
 29–30 
  
 set_final_data, 
 182 
  
 set_write_back, 
 182 
  
 shared allocation, 
 151 
  
 Shuffle functions, 
 235–238 
  
 Single Program, Multiple Data 
  
  
 (SPMD), 
 99 
  
 Single-Source, 
 12,26–27 
  
 Standard Template Library 
  
  
  
 (STL), 
 339 
  
 std::function, 
 142 
  
 Stencil pattern, 
 326–
 328,342,344 
 store() member 
 function, 
 268 
  
 Sub-Groups, 
 110–112,230 
  
  
 compiler optimizations, 
 238 
  
 loads and stores, 
 238 
  
  
 sub_group class, 
 117–118 
  
 SYCL versions, 
 3 
  
 Synchronous errors, 
 135,136, 
  
  
  
 140,141
  
  
 in-order queue object, 
 77 
  
 OoO queues, 
 78 
  
  
 simple task graph, 
 75 
  
 Throughput and Latency, 
 7–
 8 
 throw_asynchronous(), 
 145 
 Translation units, 
 319–
 320 
  
 try-catch structure, 
 140",NA
U,"Unified shared memory (USM), 
 67, 
  
  
 149–170,522 
  
  
 aligned_malloc functions, 
 159 
  
 allocations, 
 67,68 
  
  
 data initialization, 
 160,161 
  
  
 data movement, 
 see 
 Data 
  
  
  
   
 movement 
  
  
 definition, 
 150 
  
  
 device allocation, 
 151 
  
  
 explicit data movement, 
 68,69 
  
 host allocation, 
 151 
  
  
 implicit data movement, 
 70,71 
  
 malloc, 
 67 
  
  
 unified virtual address, 
 67 
  
  
 memory allocation 
  
  
  
 C++ allocator-style, 
 154,",NA
T ,"Task graph, 
 48–49,82–85,196–211 
  
 DAG, 
 75
  
 157,158 
  
 C++-style, 
 154,156,157 
 C-style, 
 154,155 
  
 deallocation, 
 159,160
  
  
 disjoint dependence, 
 76,77 
 execution, 
 75 
  
 explicit dependences, 
 78,79 
 implicit dependences, 
 80–85
  
  
 new, malloc, 
  
  
  
 or allocators, 
 153 
  
 queries, 
 168–170 
  
 shared allocation, 
 151,152",NA
V ,NA,NA
"W, X, Y, Z","vec class, 
 263,264 
  
 wait(), 
 78
  
 Vectors, 
 259–275 
  
  
 explicit vector code, 
 262,263 
  
 features and hardware, 
 261 
  
 load and store 
  
  
  
 operations, 
 267,268
  
 548
  
 wait_and_throw(), 
 145 
  
 Work Groups, 
 108–110,214–
 215 
 Work-group local memory, 
  
  
 217–222,378–380 
  
 Work-Item, 
 107,214–215",NA
