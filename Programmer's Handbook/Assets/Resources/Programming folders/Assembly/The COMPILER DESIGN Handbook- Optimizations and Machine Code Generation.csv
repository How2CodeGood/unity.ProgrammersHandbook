Larger Text,Smaller Text,Symbol
The ,NA,NA
CO DESIG ,NA,NA
Handbook ,NA,NA
M PILER ,NA,NA
N,NA,NA
Optimizations and ,NA,NA
Machine Code ,NA,NA
Generation,SECOND EDITION,NA
The ,NA,NA
CO DESIG ,NA,NA
Handbook ,NA,NA
M PILER ,NA,NA
N,NA,NA
Optimizations and ,NA,NA
Machine Code ,NA,NA
Generation,NA,NA
SECOND EDITION ,NA,NA
Edited by ,NA,NA
Y.N Priti Shankar . ,NA,NA
Srikant,"Boca Raton   London   New York
  
 CRC Press is an imprint of the 
  
 Taylor & Francis Group, an 
 informa
  business",NA
Table of Contents,NA,NA
1,"Worst-Case Execution Time and Energy Analysis  
 Tulika Mitra, Abhik Roychoudhury
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  1
 -
 1",NA
2,NA,NA
3,NA,NA
 ,NA,NA
4,NA,NA
 ,NA,NA
5,NA,NA
6,NA,NA
 ,NA,NA
7,NA,NA
 ,NA,NA
8,NA,NA
9,NA,NA
10,NA,NA
11,"Static Program Analysis for Security  
 K. Gopinath
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  2
 -1
  
 Compiler-Aided Design of Embedded Computers  
 Aviral Shrivastava, Nikil Dutt
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  3
 -1
  
 Whole Execution Traces and Their Use in Debugging  
 Xiangyu Zhang, Neelam Gupta, Rajiv Gupta
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  4
 -1
  
 Optimizations for Memory Hierarchies  
 Easwaran Raman, David I. August
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  5
 -1
  
 Garbage Collection Techniques  
 Amitabha Sanyal, Uday P. Khedker
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  6
 -1
  
 Energy-Aware Compiler Optimizations  
 Y. N. Srikant, K. Ananda Vardhan
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  7
 -1
  
 Statistical and Machine Learning Techniques in Compiler Design  
 Kapil Vaswani
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  8
 -1
  
 Type Systems: Advances and Applications  
 Jens Palsberg, Todd Millstein
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 9
 -1
  
 Dynamic Compilation  
 Evelyn Duesterwald
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  10
 -1
  
 The Static Single Assignment Form: Construction and 
 Application to Program Optimization 
 J. Prakash Prabhu, Priti Shankar, Y. N. Srikant
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  11
 -1",NA
12,"Shape Analysis and Applications  
 Thomas Reps, Mooly Sagiv, Reinhard Wilhelm
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  12
 -
 1
  
 v",NA
1,NA,NA
3 ,NA,NA
1,NA,NA
4 ,NA,NA
1,NA,NA
5,"Optimizations for Object-Oriented Languages  
 Andreas Krall, Nigel Horspool
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  13
 -1
  
 Program Slicing  
 G. B. Mund, Rajib Mall
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  14
 -1
  
 Computations on Iteration Spaces  
 Sanjay Rajopadhye, Lakshminarayanan Renganarayana, Gautam 
 Gupta,
  
 Michelle Mills Strout
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  15
 -1",NA
1,NA,NA
6 ,NA,NA
1,NA,NA
7 ,NA,NA
1,NA,NA
8,"Architecture Description Languages for Retargetable Compilation  
 Wei Qin, Sharad Malik
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  16
 -1
  
 Instruction Selection Using Tree Parsing  
 Priti Shankar
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  17
 -1
  
 A Retargetable Very Long Instruction Word Compiler 
 Framework for Digital Signal Processors 
 Subramanian Rajagopalan, Sharad Malik
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  18
 -1",NA
1,NA,NA
9 ,NA,NA
2,NA,NA
0 ,NA,NA
2,NA,NA
1,"Instruction Scheduling  
 R. Govindarajan
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  19
 -1
  
 Advances in Software Pipelining  
 Hongbo Rong, R. Govindarajan
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  20
 -1
  
 Advances in Register Allocation Techniques  
 V. Krishna Nandivada
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  21
 -1
  
 Index
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  I
 -1
  
 vi",NA
1 ,NA,NA
Worst-Case Execution ,NA,NA
Time and Energy ,NA,NA
Analysis,"1.1 
 1.2
  
 Introduction
  ..........................................
 1
 -
 1 
 Programming-
 Language-Level WCET Analysis
  .........
 1
 -
 4
  
 WCET Calculation
  
 •
  Infeasible Path Detection and
  
 Exploitation
  
 Tulika Mitra 
  
 and 
  
 Abhik Roychoudhury 
  
 Department of Computer Science, 
 School of Computing, 
  
 National University of Singapore, 
 Singapore 
  
 tulika@comp.nus.edu.sg and 
  
 abhik@comp.nus.edu.sg",NA
1.1 ,NA,NA
Introduction,"1.3
  
 1.4
  
 1.5
  
 1.6
  
 Micro-Architectural Modeling
  .........................
 1
 -
 16
  
 Sources of Timing Unpredictability
 •
  Timing Anomaly
  
 •
  Overview of Modeling Techniques
 •
  Integrated Approach
  
 Based on ILP
 •
  Integrated Approach Based on Timing Schema
  
 •
  Separated Approach Based on Abstract Interpretation
  
 •
  A Separated Approach That Avoids State Enumeration
  
 Worst-Case Energy Estimation
  ........................
 1
 -
 35
  
 Backgroun
 d
  
 •
  Analysis Technique
  
 •
  Accuracy and Scalability
  
 Existing WCET Analysis Tools
  .........................
 1
 -
 41 
 Conclusions
  ..........................................
 1
 -
 42 
 Integration with 
 Schedulability Analysis
 •
  System-Level 
  
 Analysis
 •
  Retargetable WCET Analysis
 •
  Time-Predictable 
  
 System Design
 •
  WCET-Centric Compiler Optimizations
  
 References
  ..................................................
 1
 -
 44
  
 Timingpredictabilityisextremelyimportantforhardreal-
 timeembeddedsystemsemployedinapplication domains such as automotive electronics and 
 avionics. Schedulability analysis techniques can guarantee the satisfiability of timing constraints 
 for systems consisting of multiple concurrent tasks. One of the key inputs required for the 
 schedulability analysis is the worst-case execution time (WCET) of each of the tasks. WCET of a 
 task on a target processor is defined as its maximum execution time across all possible inputs.
  
 Figure 1.1a and Figure 1.2a show the variation in execution time of a
  quick sort
  program 
 on a simple and complex processor, respectively. The program sorts a five-element array. The 
 figures show the distribution of execution time (in processor cycles) for all possible 
 permutations of the array elements as inputs. The maximum execution time across all the inputs 
 is the WCET of the program. This simple example illustrates the inherent difficulty of finding the 
 WCET value:
  
 1
 -
 1",NA
1.2,NA,NA
Programming-Language-Level WCET Analysis,"We now proceed to discuss static analysis methods for estimating the WCET of a program. For 
 WCET analysis of a program, the first issue that needs to be determined is the program 
 representation on which the analysis will work. Earlier works [73] have used the
  syntax tree
  
 where the (nonleaf) nodes correspond to programming-language-level control structures. The 
 leaves correspond to basic blocks — maximal fragments of code that do not involve any control 
 transfer. Subsequently, almost all work on WCET analysis has used the
  control flow graph
 . The 
 nodes of a control flow graph (CFG) correspond to basic blocks, and the edges correspond to 
 control transfer between basic blocks. When we construct the CFG of a program, a separate copy 
 of the CFG of a function
  f
  is created for every distinct call site of
  f
  in the program",NA
1.3,"FIGURE 1.7
  
 A control flow graph fragment for illustrating infeasible path representation.",NA
Micro-Architectural Modeling,"The execution time of a basic block
  B
  in a program executing on a particular processor depends 
 on (a) the number of instructions in
  B
 , (b) the execution cycles per instruction in
  B
 , and (c) the 
 clock period of the processor. Let a basic block
  B
  contain the sequence of instructions ⟨
 I
 1
 ,
  I
 2
 ,
  . . .
  ,
  
 I
 N
 ⟩. For a simple micro-controller (e.g., TI MSP430), the execution latency of any instruction type 
 is a constant. Let
  latency
 (
 I
 i
 ) be a constant denoting the execution cycles of instruction
  I
 i
 . Then the 
 execution time of the basic block 
 B
  can be expressed as
  
  N
  
 time
 (
 B
 ) =
  
 i
 =1
  
 latency
 (
 I
 i
 )
  
 ×
  period
  
 (1.1)
  
 where
  period
  is the clock period of the processor. Thus, for a simple micro-controller, the 
 execution time of a basic block is also a constant and is trivial to compute. For this reason, initial 
 work on timing analysis [67, 73] concentrated mostly on program path analysis and ignored the 
 processor architecture.
  
 However, the increasing computational demand of the embedded systems led to the 
 deployment of processors with complex micro-architectural features. These processors employ 
 aggressive pipelining, caching, branch prediction, and other features [33] at the architectural 
 level to enhance performance. While the increasing architectural complexity significantly 
 improves the average-case performance of an application, it leads to a high degree of timing 
 unpredictability. The execution cycle
  latency
 (
 I
 i
 ) of an instruction
  I
 i
  in Equation 1.1 is no longer a 
 constant; instead it depends on the execution context of the instruction. For example, in the 
 presence of a cache, the execution time of an instruction depends on whether the processor 
 encounters a cache hit or a cache misses while fetching the instruction from the memory 
 hierarchy. Moreover, the large difference between the cache hit and miss latency implies that 
 assuming all memory accesses to be cache misses will lead to overly pessimistic timing estimates. 
 Any effective estimation technique should obtain a safe but tight bound on the number of cache 
 misses.",NA
1.4,NA,NA
Worst-Case Energy Estimation,"In this section, we present a static analysis technique to estimate safe and tight bounds for the 
 worst-case energy consumption of a program on a particular processor. The presentation in this 
 section is based on [36].
  
 Traditional power simulators, such as Wattch [9] and SimplePower [96], perform cycle-by-
 cycle power estimation and then add them up to obtain total energy consumption. Clearly, we 
 cannot use cycle-accurate estimation to compute the worst-case energy bound, as it would 
 essentially require us to simulate all possible scenarios (which is too expensive). The other 
 method [75, 88] is to use fixed per-instruction energy but it fails to capture the effects of cache 
 miss and branch prediction. Instead, worst-case energy analysis is based on the key observation 
 that the energy consumption of a program can be separated out into the following time-
 dependent and time-independent components:",NA
1.5,NA,NA
Existing WCET Analysis Tools,"There are some commercial and research prototype tools for WCET analysis. We discuss them in 
 this section. The most well known and extensively used commercial WCET analyzer is the aiT 
 tool 
 [1] 
 from 
 AbsIntAngewandteInformatik.aiTtakesinacodesnippetinexecutableformandcomputesitsWCET.Th
 e analyzer uses a two-phased approach where micro-architectural modeling is performed first 
 followed by path analysis. It employs abstract interpretation for cache/pipeline analysis and 
 estimates an upper bound on the execution time of each basic block. These execution time 
 bounds of basic blocks are then combined using ILP to estimate the WCET of the entire program. 
 Versions of aiT are available for various platforms including Motorola PowerPC, Motorola 
 ColdFire, ARM, and so on. The aiT tool is not open-source; so the user cannot change the analyzer 
 code to model timing effects of new processor platforms. The main strength of the aiT tool is its 
 detailed modeling of complex micro-architectures. It is probably the only WCET estimation tool 
 to have a full modeling of the processor micro-architecture for a complex real-life processor like 
 Motorola ColdFire [22] and Motorola PowerPC [32].
  
 Another commercial WCET analyzer is the Bound-T tool [87], which also takes in binary 
 executable programs. It concentrates mainly on program path analysis and does not model cache, 
 complex pipeline, or branch prediction. In path analysis, an important focus of the tool is 
 inferring loop bounds, for which it extensivelyusesthewell-knownOmega-calculator[66].Bound-
 ThasbeentargetedtowardIntel8051series 
 micro-controllers,AnalogDevicesADSP-
 21020DSP,andATMELERC32SPARCV7-basedplatforms.Like aiT, Bound-T is not open-source.
  
 The Chronos WCET analyzer [44] incorporates timing models of different micro-architectural 
 features present in modern processors. In particular, it models both in-order and out-of-order 
 pipelines, instruc-tion caches, dynamic branch prediction, and their interactions. The modeling of 
 different architectural features is parameterizable. Chronos is a completely open-source 
 distribution especially suited to the needs of the research community. This allows the researcher 
 to modify and extend the tool for his or her individual needs. Current state-of-the-art WCET 
 analyzers, such as aiT [1], are commercial tools that do not provide the source code. Unlike other 
 WCET analyzers, Chronos is
  not
  targeted toward one or more commercial embedded processors. 
 Instead, it is built on top of the freely available SimpleScalar simulator infrastructure. 
 SimpleScalar is a widely popular cycle-accurate architectural simulator that allows the user",NA
1.6,NA,NA
Conclusions,"Inthischapter,wehaveprimarilydiscussedsoftwaretimingandenergyanalysisofanisolatedtaskexecu
 ting on a target processor without interruption. This is an important problem and forms the 
 building blocks of more complicated performance analysis techniques. As we have seen, the main 
 steps of software timing and energy analysis are (a) program path analysis and (b) micro-
 architectural modeling. We have also discussed a number of analysis methods that either 
 perform an integrated analysis of the two steps or separate the two steps. It has been observed 
 that integrated analysis methods are not scalable to large programs [94], and hence separated 
 approaches for timing analysis may have a better chance of being integrated into compilers. 
 Finally, we outline here some possible future research directions.
  
 1.6.1 Integration with Schedulability Analysis
  
 The timing and energy analysis methods discussed in this chapter assume
  uninterrupted
  
 execution of a program. In reality, a program (or “task,” using the terminology of the real-time 
 systems community) may get preempted because of interrupts. The major impact of task 
 preemption is on the performance of the instruction and data caches. Let
  T
 l
  be a lower-priority 
 task that gets preempted by a higher-priority task 
 T
 h
 . When
  T
 l
  resumes execution, some of its 
 cache blocks have been replaced by
  T
 h
 . Clearly, if the WCET analysis does not anticipate this 
 preemption, the resulting timing guarantee will not be safe.
  Cache-related preemption delay
  [42, 
 58] analysis derives an upper bound on the number of additional cache misses per preemption.",NA
Acknowledgments,"Portions of this chapter were excerpted from R. Jayaseelan, T. Mitra, and X. Li, 2006, “Estimating 
 the worst-case energy consumption of embedded software,” in
  Proceedings of the 12th IEEE Real-
 Time and Embedded Technology and Applications Symposium (RTAS)
 , pages 81–90, and adapted 
 from X. Li, A. Roychoudhury, and T. Mitra, 2006, “Modeling out-of-order processors for WCET 
 analysis,”
  Real-Time Systems
 , 34(3): 195–227.
  
 The authors would like to acknowledge Ramkumar Jayaseelan for preparing the figures in the 
 introduc-tion section.
  
 References
  
 1. AbsInt Angewandte Informatik GmbH. aiT: Worst case execution time analyzer. http://www. 
  
 absint.com/ait/.
  
 2. P. Altenbernd. 1996. On the false path problem in hard real-time programs. In
  Proceedings of 
 the 
  
 Eighth Euromicro Workshop on Real-Time Systems (ECRTS)
 , 102–07.
  
 3. A. Anantaraman, K. Seth, K. Patil, E. Rotenberg, and F. Mueller. 2003. Virtual simple 
 architecture 
 (VISA):Exceedingthecomplexitylimitinsafereal-
 timesystems.In
 Proceedingsofthe30thIEEE/ACM International Symposium on Computer 
 Architecture (ISCA)
 , 350–61.
  
 4. R. Arnold, F. Mueller, D. B. Whalley, and M. G. Harmon. 1994. Bounding worst-case instruction 
 cache performance. In
  Proceedings of the 15th IEEE Real-Time Systems Symposium (RTSS)
 , 172–
 81. 5. I. Bate and R. Reutemann. 2004. Worst-case execution time analysis for dynamic branch 
 predictors. In
  Proceedings of the 16th Euromicro Conference on Real-Time Systems (ECRTS)
 , 
 215–22.
  
 6. G. Bernat, A. Colin, and S. M. Petters. 2002. WCET analysis of probabilistic hard real-time 
 systems. 
  
 In
  Proceedings of the 23rd IEEE Real-Time Systems Symposium (RTSS)
 , 279–88.
  
 7. R. Bodik, R. Gupta, and M. L. Soffa. 1997. Refining data flow information using infeasible paths.
  
 In
  Proceedings of the 6th European Software Engineering Conference
  held jointly with the
  
 5th ACM SIGSOFT International Symposium on Foundations of Software Engineering 
 ESEC/FSE
 , Vol. 1301 of 
 Lecture Notes in Computer Science
 , 361–77. New York: Springer.
  
 8. F.BodinandI.Puaut.2005.AWCET-orientedstaticbranchpredictionschemeforreal-timesystems. 
  
 In
  Proceedings of the 17th Euromicro Conference on Real-Time Systems
 , 33–40.
  
 9. D. Brooks, V. Tiwari, and M. Martonosi. 2000. Wattch: A framework for architectural-level 
 power analysis and optimizations. In
  Proceedings of the 27th Annual ACM/IEEE 
 International Symposium on Computer Architecture (ISCA)
 , 83–94.
  
 10. D. Burger and T. Austin. 1997. The SimpleScalar tool set, version 2.0. Technical Report CS-
 TR-
  
 1997-1342, University of Wisconsin, Madison.
  
 11. C. Burgui´ere and C. Rochange. 2005. A contribution to branch prediction modeling in 
 WCET analysis. In
  Proceedings of the IEEE Design, Automation and Test in Europe Conference 
 and Exposition
 , Vol. 1, 612–17.
  
 12. K. Chen, S. Malik, and D. I. August. 2001. Retargetable static timing analysis for embedded 
 software. 
  
 In
  Proceedings of IEEE/ACM International Symposium on System Synthesis (ISSS)
 .
  
 13. E. M. Clarke, E. A. Emerson, and A. P. Sistla. 1986. Automatic verification of finite-state 
 concurrent systems using temporal logic specifications.
  ACM Transactions on Programming 
 Languages and Systems
  8(2):244–63.",NA
2 ,NA,NA
Static Program,NA,NA
Analysis for Security,"2.1
  
 2.
 2 
 2.
 3 
 2.
 4
  
 Introduction
  ..........................................
 2
 -
 1
  
 A Dramatic Failure of Static Analysis: Ken Thompson’s Trojan
  
 Horse
 •
  Detecting Viruses
 •
  A Case Study for Static Analysis:
  
 GSM Security Hole
 •
  Obfuscation
  
 Static Analysis of Buffer Overflows
  .....................
 2
 -
 8
  
 Static Analysis of Safety of Mobile Code
 ................
 2
 -
 9
  
 Static Analysis of Access Control Policies
  ...............
 2
 -
 9
  
 Case Studies
 •
  Dynamic Access Control
 •
  Retrofitting Simple
  
 MAC Models for SELinux
 •
  Model Checking Access Control on
  
  
 K. Gopinath 
  
 Department of Computer Science and 
 Automation, 
  
 Indian Institute of Science, 
  
 Bangalore, India 
  
 gopi@csa.iisc.ernet.in",NA
Abstract,"the Internet: Trust Management
  
 2.5
  
 2.6
  
 Language-Based Security
  ..............................
 2
 -
 15 
 The Type-Based 
 Approach
 •
  Java Information Flow (Jif) 
  
 Language
 •
  A Case Study: VSR 
  
 Future Work
  ..........................................
 2
 -
 24
  
 References
  ..................................................
 2
 -
 25
  
 In this chapter, we discuss static analysis of the security of a system. First, we give a brief 
 background on what types of static analysis are feasible in principle and then move on to what is 
 practical. We next discuss static analysis of buffer overflow and mobile code, followed by access 
 control. Finally, we discuss static analysis of information flow expressed in a language that has 
 been annotated with flow policies.",NA
2.1,NA,NA
Introduction,"Analyzing a program for security holes is an important part of the current computing landscape, 
 as security has not been an essential ingredient in a program’s design for quite some time. With 
 the critical importance of a secure program becoming clearer in the recent past, designs based on 
 explicit security policies are likely to gain prominence.
  
 Static analysis of a program is one technique to detect security holes. Compared to monitoring 
 an execu-tion at runtime (which may not have the required coverage), a static analysis — even if 
 incomplete because of loss of precision — potentially gives an analysis on all runs possible 
 instead of just the ones seen so far.
  
 However, security analysis of an arbitrary program is extremely hard. First, what security 
 means is often unspecified or underspecified. The definition is either too strict and cannot cope 
 with the “commonsense”requirement or too broad and not useful. For example, one definition of 
 security involves the notion of“noninterference” [24]. If it is very strict, even cryptoanalytically 
 strong encryption and decryption does
  
 2
 -
 1",NA
2.2,NA,NA
Static Analysis of Buffer Overflows,"Since the advent of the Morris worm in 1988, buffer overflow techniques to compromise systems 
 have been widely used. Most recently, the SQL slammer worm in 2003, using a small UDP packet 
 (376B), compromised 90% of all target machines worldwide in less than 10 minutes.
  
 Since buffer overflow on a stack can be avoided, for example, by preventing the return address 
 from being overwritten by the “malicious” input string, array bounds checking the input 
 parameters by the callee is one technique. Because of the cost of this check, it is useful to explore 
 compile time approaches that eliminate the check through program analysis. Wagner et al. [56] 
 use static analysis (integer range analysis), but it has false positives due to imprecision in pointer 
 analysis, interprocedural analysis, and so on and a lack of information on dynamically allocated 
 sizes.
  
 CCured [46] uses static analysis to insert runtime checks to create a type-safe version of C 
 program. CCured classifies C pointers into SAFE, SEQ, or WILD pointers. SAFE pointers require 
 only a null check. SEQ pointers require a bounds check, as these are involved in pointer 
 arithmetic, but the pointed object is known statically, while WILD ones require a bounds check as 
 well as a runtime check, as it is not known what type of objects they point to at runtime. For such 
 dynamically typed pointers, we cannot rely on the static type; instead, we need, for example, 
 runtime tags to differentiate pointers from nonpointers.
  
 Ganapathy et al. [23] solve linear programming problems arising out of modeling C string 
 programs as linear programs to identify buffer overruns. Constraints result from buffer 
 declarations, assignments, and function call/returns. C source is first analyzed by a tool that 
 builds a program-dependence graph for each procedure, an interprocedural CFG, ASTs for 
 expressions, along with points-to and side-effect information. A constraint generator then 
 generates four constraints for each pointer to a buffer (between max/min buffer allocation and 
 max/min buffer index used), four constraints on each index assignment (between previous and 
 new values as well as for the highest and lowest values), two for each buffer declaration, and so 
 on. A taint analysis next attempts to identify and remove any uninitialized constraint variables to 
 make it easy for the constraint solvers.
  
 Using LP solvers, the best possible estimate of the number of bytes used and allocated for each 
 buffer in any execution is computed. Based on these values, buffer overruns are inferred. Some 
 false positives are possible because of the flow-insensitive approach followed; these have to be 
 manually resolved. Since infeasible linear programs are possible, they use an algorithm to 
 identify irreducibly inconsistent sets. After such sets of constraints are removed, further 
 processing is done before solvers are employed. This approach also employs techniques to make 
 program analysis context sensitive.
  
 Engler et al. [19] use a “metacompilation” (MC) approach to catch potential security holes. For 
 ex-ample, any use of “untrusted input”
 6
 could be a potential security hole. Since a compiler 
 potentially has information about such input variables, a compiler can statically infer some of the 
 problematic uses and flag them. To avoid hardwiring some of these inferences, the MC approach 
 allows implementers to add rules to the compiler in the form of high-level system-specific 
 checkers. Jaeger et al. [31] use a similar approach to make SELinux aware of two trust levels to 
 make information flow analysis possible; as of now, it is not possible. We will discuss this further 
 in Section 2.4.3.
  
  
 6
 Examples in the Linux kernel code are system call parameters, routines that copy data from user space, 
 and network data.",NA
2.3,NA,NA
Static Analysis of Safety of Mobile Code,"The importance of safe executable code embedded in web pages (such as Javascript), applications 
 (as macros in spreadsheets), OS kernel (such as drivers, packet filters [44], and profilers such as 
 DTrace [51]), cell phones, and smartcards is increasing every day. With unsafe code (especially 
 one that is a Trojan), it is possible to get elevated privileges that can ultimately compromise the 
 system. Recently, Google Desktop search [47] could be used to compromise a machine (to make 
 all of its local files available outside, for example) in the presence of a malicious Java applet, as 
 Java allows an applet to connect to its originating host.
  
 The most simple model is the “naive” sandbox model where there are restrictions such as 
 limited access to the local file system for the code, but this is often too restrictive. A better 
 sandbox model is that of executing the code in a virtual machine implemented either as an OS 
 abstraction or as a software isolation layer and using emulation. In the latter solution, the safety 
 property of the programming language and the access checks in the software isolation layer are 
 used to guarantee security.
  
 Since object-oriented (OO) languages such as Java and C# have been designed for making 
 possible“secure” applets, we will consider OO languages here. Checking whether a method has 
 access 
 permissions 
 maynotbelocal.Onceweuseaprogramminglanguagewithfunctioncalls,thecallstackhasinformationo
 n 
 thecurrentcallingsequence.Dependingonthispath,amethodmayormaynothavethepermissions.Stac
 k 
 inspectioncanbecarriedouttoprotectthecalleefromthecallerbyensuringthattheuntrustedcallerhast
 he right credentials to call a higher-privileged or trusted callee. However, it does not protect the 
 caller from the callee in the case of callback or event-based systems. We need to compute the 
 intersection of permissions of all methods invoked per thread and base access based on this 
 intersection. This protects in both directions.
  
 StaticanalysiscanbecarriedouttochecksecurityloopholesintroducedbyextensibilityinOOlanguag
 es. Such holes can be introduced through subclassing that overrides methods that check for 
 corner cases important for security. We can detect potential security holes by using a 
 combination of model checking and abstract interpretation: First, compute all the possible 
 execution histories; pushdown systems can be used for representation. Next, use temporal logic 
 to express properties of interest (for example, a method from an applet cannot call a method 
 from another applet). If necessary, use abstract interpretation and model checking to check 
 properties of interest.
  
 Another approach is that of the proof-carrying code (PCC). Here, mobile code is accompanied 
 by a proof that the code follows the security policy. As a detailed description of the above 
 approaches for the safety of mobile code is given in the first edition of this book [36], we will not 
 discuss it here further.",NA
2.4,NA,NA
Static Analysis of Access Control Policies,"Lampson [38] introduced access control as a mapping from
  {
 entity, resource, op
 }
  to
  {
 permit, 
 deny
 }
  (as commonly used in operating systems). Later models have introduced structure for 
 entities 
 such 
 as 
 roles 
 (“role-
 basedaccesscontrol”)andintroducedanooptohandletheabilitytomodelaccesscontrolmodularly by 
 allowing multiple rules to fire:
  {
 role, resource, op
 }
  to
  {
 permit, deny, noop
 }
 . Another significant 
 advance is access control with anonymous entities: the subject of trust management, which we 
 discuss in Section 2.4.4.
  
 Starting from the early simple notion, theoretical analysis in the HRU system [29] of access control has",NA
2.5,NA,NA
Language-Based Security,"As discussed earlier, current operating systems are much bigger than current compilers, so it is 
 worthwhile to make the compiler part of the TCB rather than an OS. If it is possible to express 
 security policies using a programming language that can be statically analyzed, a compiler as part 
 of a TCB makes eminent sense.
  
 The main goal of language-based security is to check the
  noninterference
  property, that is, to 
 detect all possible leakages of some sensitive information through computation, timing channels, 
 termination chan-nels, I/O channels, and so on. However, the noninterference property is too 
 restrictive to express security policies, since many programs do
  leak
  some information. For 
 example, sensitive data after encryption can be leaked to the outside world, which is agreeable 
 with respect to security as long as the encryption is effective. Hence, the noninterference 
 property has to be
  relaxed
  by some mechanisms like
  declassification
 .
  
 Note that static approaches cannot quantify the leakage of information, as the focus is on 
 whether 
 a 
 programviolatedsomedesiredpropertywithrespecttoinformationflow.Itispossibletouseadynamica
 p-proach that quantifies the amount of information leaked by a program as the entropy of the",NA
2.6,NA,NA
Future Work,"Current frameworks such as Jif have not been sufficiently developed. We foresee the following 
 evolution: 
  
 r
  The analysis in Section 2.5.1 assumed that typing can reveal interesting 
 properties. This needs to be extended to a more general static analysis that deals with values in 
 addition to types as well as
  
 when the language is extended to include arrays and so on. Essentially, the analysis should be 
 able to handle array types with affine index functions. This goes beyond the analysis possible 
 with type 
 r
  Incorporate pointers and heaps in the analysis. Use dataflow analysis on lattices 
 but with complex 
 r
  Integrate static analysis (such as abstract interpretation and compiler 
 dataflow analysis) with model 
 checkingtoanswerquestionssuchas:Whatistheleast/mostprivilegeavariableshouldhavetosatisf
 y some constraint? This may be coupled with techniques such as counterexample guided 
 abstraction refinement [13]. This, however, requires considerable machinery. Consider, for 
 example, a fully developed system such as the SLAM software model checking [3]. It uses 
 counterexample-driven",NA
3 ,NA,NA
Compiler-Aided ,NA,NA
Design of Embedded ,NA,NA
Computers,"Aviral Shrivastava
  
 3.1
  
 Introduction
  ..........................................
 3
 -
 1
  
 3.2
  
 Compiler-Aided Design of Embedded Systems
  .........
 3
 -
 2
  
 Design Constraints on Embedded Systems
 •
 Highly 
 Customized Designs of Embedded Systems
 •
  Compilers for 
 Embedded Systems
 •
  Compiler-Assisted Embedded System 
 3.3
  
 Design
  
 Horizontally Partitioned Cache
  ........................
 3
 -
 8
  
 Department of Computer Science and
  
 Engineering,
  
 3.4
  
 Compiler for Horizontally Partitioned Cache
 ...........
 3
 -
 9
  
 School of Computing and Informatics,
  
 HPC Compiler Framework
 •
  Experimental Framework
  
 •
  Simple Greedy Heuristics Work Well for Energy Optimization
  
 •
  Optimizing for Energy Is Different Than Optimizing for
  
 Arizona State University, Tempe, AZ
  
 Aviral.Shrivastava@asu.edu
  
  
 Nikil Dutt 
  
 Center for Embedded Computer Systems, 
 Donald Bren School of Informatics and 
 Computer Sciences, 
  
 University of California, Irvine, CA 
  
 dutt@uci.edu",NA
3.1 ,NA,NA
Introduction,"Performance
  
 3.5
  
 3.6
  
 Compiler-in-the-Loop HPC Design
 ....................
 3
 -
 17 
 Exhaustive 
 Exploration
 •
  HPC CIL DSE Heuristics
  
 •
  Importance of Compiler-in-the-Loop DSE 
  
 Summary
  .............................................
 3
 -
 24
  
 References
  ..................................................
 3
 -
 24
  
 Embeddedsystemsarecomputingplatformsthatareusedinsideaproductwhosemainfunctionisdiffer
 ent than general-purpose computing. Cell phones and multipoint fuel injection systems in cars 
 are examples of embedded systems. Embedded systems are characterized by application-specific 
 and multidimensional design constraints. While decreasing time to market and the need for 
 frequent 
 upgrades 
 are 
 pushing 
 embedded 
 system 
 designs 
 toward 
 programmable 
 implementations, these stringent requirements demand that designs be
  highly customized
 . To 
 customize embedded systems, standard design features of general-purpose processors are often 
 omitted, and several new features are introduced in embedded systems to meet all the design 
 constraints simultaneously.
  
 Consequently, software development for embedded systems has become a very challenging 
 task. Tra-ditionally humans used to code for embedded systems directly in assembly language, 
 but now with the software content reaching multimillion lines of code and increasing at the rate 
 of 100 times every decade, compilers have the onus of generating code for embedded systems. 
 With embedded system designs still being manually customized, compilers have a dual 
 responsibility: first to exploit the novel architectural features in the embedded systems, and 
 second to avoid the loss due to missing standard architectural features. Existing compiler 
 technology falls tragically short of these goals.
  
 3
 -
 1",NA
3.2,NA,NA
Compiler-Aided Design of Embedded Systems,"The fundamental difference between an embedded system and a general-purpose computer 
 system is in the usage of the system. An embedded system is very
  application specific
 . Typically a 
 set of applications are installed on an embedded system, and the embedded system continues to 
 execute those applications throughout its lifetime, while general-purpose computing systems are 
 designed to be much more flexible to allow and enable rapid evolution in the application set. For 
 example, the multipoint fuel injection systems in automobiles are controlled by embedded 
 systems, which are manufactured and installed when the car is made. Throughout the life of the 
 car, the embedded system performs no other task than controlling the multipoint fuel injection 
 into the engine. In contrast, a general-purpose computer performs a variety of tasks that change 
 very frequently. We continuously install new games, word processing software, text editing 
 software, movie players, simulation tools, and so on, on our desktop PCs. With the popularity of 
 automatic updating features in PCs, upgrading has become more frequent than ever before. It is 
 the application-specific nature of embedded systems that allows us to perform more aggressive 
 optimizations through customization.
  
 3.2.1 Design Constraints on Embedded Systems
  
 Most design constraints on the embedded systems come from the environment in which the 
 embedded system will operate. Embedded systems are characterized by
  application-specific, 
 stringent,
  and
  multidi-mensional
  design constraints:
  
 Application-specific design constraints:
  The design constraints on embedded systems differ 
 widely; they are very application specific. For instance, the embedded system used in 
 interplanetary",NA
3.3,NA,NA
Horizontally Partitioned Cache,"Caches are one of the major contributors of not only system power and performance, but also of 
 the embedded processor area and cost. In the Intel XScale [17], caches comprise approximately 
 90% of the transistor count and 60% of the area and consume approximately 15% of the 
 processor power [3]. As a result, several hardware, software, and cooperative techniques have 
 been proposed to improve the effectiveness of caches.
  
 Horizontallypartitionedcachesareonesuchfeature.HPCswereoriginallyproposedin1995byGonz
 alez et al. [6] for performance improvement. HPCs are a popular microarchitectural feature and 
 have been deployed in several current processors such as the popular Intel StrongArm [16] and 
 the Intel XScale [17]. However, compiler techniques to exploit them are still in their nascent 
 stages.
  
 A horizontally partitioned cache architecture maintains multiple caches at the same level of 
 hierarchy, but each memory address is mapped to exactly one cache. For example, the Intel 
 XScale contains two data caches, a 32KB main cache and a 2KB mini-cache. Each virtual page can 
 be mapped to either of the data caches, depending on the attributes in the page table entry in the 
 data memory management unit. Henceforth in this paper we will call the additional cache the 
 mini-cache and the original cache the main cache.
  
 The original idea behind such cache organization is the observation that array accesses in 
 loops often have low temporal locality. Each value of an array is used for a while and then not 
 used for a long time. Such array accesses sweep the cache and evict the existing data (like 
 frequently accessed stack data) out of the cache. The problem is worse for high-associativity 
 caches that typically employ first-in-first-out page replacement policy. Mapping such array 
 accesses to the small mini-cache reduces the pollution in the main cache and prevents thrashing, 
 leading to performance improvements. Thus, a horizontally partitioned cache is a simple, yet 
 powerful, architectural feature to improve performance. Consequently, most existing approaches 
 for partitioning data between the horizontally partitioned caches aim at improving performance.
  
 In addition to performance improvement, horizontally partitioned caches also result in a 
 reduction in the energy consumption due to two effects. First, reduction in the total number of 
 misses results in reduced energy consumption. Second, since the size of the mini-cache is 
 typically small, the energy consumed per access in the mini-cache is less than that in the large 
 main cache. Therefore, diverting some memory accesses to the mini-cache leads to a decrease in 
 the total energy consumption. Note that the first effect is in line with the performance goal and 
 was therefore targeted by traditional performance improvement optimizations. However, the 
 second effect is orthogonal to performance improvement. Therefore, energy reduction by the 
 second effect was not considered by traditional performance-oriented techniques. As we show in 
 this paper, the second effect (of a smaller mini-cache) can lead to energy improvements even in 
 the presence of slight performance degradation. Note that this is where the goals of performance 
 improvement and energy improvement diverge.",NA
3.4,"FIGURE 3.5
  
 Compiler framework.",NA
Compiler for Horizontally Partitioned Cache,"3.4.1
  
 HPC Compiler Framework
  
 The problem of energy optimization for HPCs can be translated into a data partitioning problem. 
 The data memory that the program accesses is divided into pages, and each page can be 
 independently and exclusively mapped to exactly one of the caches. The compiler’s job is then to 
 find the mapping of the data memory pages to the caches that leads to minimum energy 
 consumption.
  
 As shown in Figure 3.5, we first compile the application and generate the executable. The
  page 
 access information extractor
  calculates the number of times each page is accessed during the 
 execution of the program. Then it sorts the pages in decreasing order of accesses to the pages. 
 The complexity of simulation used to compute the number of accesses to each page and sorting 
 the pages is O[
 n
  +
  m
  log(
 m
 )], where
  n 
 is the number of data memory accesses, and
  m
  is the 
 number of pages accessed by the application.
  
 The
  data partitioning heuristic
  finds the best mapping of pages to the caches that minimizes 
 the energy consumption of the target embedded platform. The
  data partitioning heuristic
  can be 
 tuned to obtain the best-performing, or minimal energy, data partition by changing the cost 
 function
  performance/energy estimator
 .
  
 The executable together with the page mapping are then loaded by the operating system of the 
 target platform for optimized execution of the application.
  
 3.4.2 Experimental Framework
  
 We have developed a framework to evaluate data partitioning algorithms to optimize the 
 memory latency or the memory subsystem energy consumption of applications. We have 
 modified
  sim-safe
  simulator from the SimpleScalar toolset [2] to obtain the number of accesses to 
 each data memory page. This implements our
  page access information extractor
  in Figure 3.5. To 
 estimate the performance/energy of an application for a given mapping of data memory pages to 
 the main cache and the mini-cache, we have developed performance and energy models of the 
 memory subsystem of a popular PDA, the HP iPAQ h4300 [14].
  
 Figure 3.6 shows the memory subsystem of the iPAQ that we have modeled. The iPAQ uses the 
 Intel PXA255 processor [15] with the XScale core [17], which has a 32KB main cache and 2KB",NA
3.5,NA,NA
Compiler-in-the-Loop HPC Design,"So far we have seen that HPC is a very effective microarchitectural technique to reduce the 
 energy con-sumption of the processor. The energy savings achieved are very sensitive to the HPC 
 configuration; that is, if we change the HPC configuration, the page partitioning should also 
 change.
  
 In the traditional DSE techniques, for example, SO DSE, the binary and the page mapping are 
 kept the same, and the binary with the page mapping is executed on different HPC configurations. 
 This strategy is not useful for HPC DSE, since it does not make sense to use the same page 
 mapping after changing the HPC parameters. Clearly, the HPC parameters should be explored 
 with the CIL during the exploration. To evaluate HPC parameters, the page mapping should be set 
 to the given HPC configuration.
  
 Our CIL DSE framework to explore HPC parameters is depicted in Figure 3.16. The CIL DSE 
 framework is centered around a textual description of the processor. For our purposes, the 
 processor description contains information about (a) HPC parameters, (b) the memory 
 subsystem energy models, and (c) the processor and memory delay models.
  
 We use the OMN page partitioning heuristic and generate a binary executable along with the 
 page mapping. The page mapping specifies to which cache (main or mini) each data memory 
 page is mapped. The compiler is tuned to generate page mappings that lead to the minimum 
 memory subsystem energy consumption. The executable and the page mapping are both fed into 
 a simulator that estimates the runtime and the energy consumption of the memory subsystem.
  
 The Design Space Walker performs HPC design space exploration by updating the HPC design 
 parame-ters in the processor description. The mini-cache, which is configured by Design Space 
 Walker, is specified using two attributes: the mini-cache size and the mini-cache associativity. For 
 our experiments, we vary cache size from 256 bytes to 32 KB, in exponents of 2. We explore the 
 whole range of mini-cache associa-tivities, that is, from direct mapped to fully associative. We do 
 not model the mini-cache configurations
  
 Application 
  
 FIGURE 3.16
  
 HPC 
  
 Compiler 
  
 Processor 
  
 Description
  
 mini-cache 
  
 params
  
 Delay Model 
  
 Executable 
  
 Page Mapping 
  
 Simulator 
  
 Energy Model",NA
3.6,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Summary,"Embedded systems are characterized by stringent, application-specific, multidimensional 
 constraints on their designs. These constraints, along with the shrinking time to market and 
 frequent upgrade needs of embedded systems, are responsible for programmable embedded 
 systems that are
  highly customized
 . While code generation for these highly customized embedded 
 systems is a challenge, it is also very rewarding in the sense that an architecture-sensitive 
 compilation technique can have significant impact on the system power, performance, and so on. 
 Given the importance of the compiler on the system design parameters, it is reasonable for the 
 compiler to take part in designing the embedded system. While it is possible to use ad hoc 
 methods to include the
  compiler effects
  while designing an embedded system, a systematic 
 method-ology to design embedded processors is needed. This chapter introduced the CIL design 
 methodology, which systematically includes the compiler in the embedded system DSE. Our 
 methodology requires an architecture-sensitive compiler. To evaluate a design point in the 
 embedded system design space, the ap-plication code is first compiled for the embedded system 
 design and is then executed on the embedded system model to estimate the various design 
 parameters (e.g., power, performance, etc.). Owing to the lack of compiler technology for 
 embedded systems, most often, first an architecture-sensitive compilation technique needs to be 
 developed, and only then can it be used for CIL design of the embedded processor. In the chapter 
 we first developed a compilation technique for HPCs, which can result in a 50% reduc-tion in the 
 energy consumption of the memory subsystem. When we use this compilation technique in our 
 CIL approach, we can come up with HPC parameters that result in an 80% reduction in the 
 energy consumption by the memory subsystem, demonstrating the need and usefulness of our 
 approach.
  
 References
  
 1. M. R. Barbacci. 1981. Instruction set processor specifications (ISPS): The notation and its 
 applica-
  
 tions.
  IEEE Transactions on Computing
  C-30(1):24–40. New York: IEEE Press.
  
 2. D. Burger and T. M. Austin. 1997. The SimpleScalar tool set, version 2.0.
  SIGARCH Computer 
  
 Architecture News
  25(3):13–25.
  
 3. L. T. Clark, E. J. Hoffman, M. Biyani, Y. Liao, S. Strazdus, M. Morrow, K. E. Velarde, and M. A. Yarch.
  
 2001. An embedded 32-b microprocessor core for low-power and high-performance 
 applications. 
 IEEE J. Solid State Circuits
  36(11):1599–608. New York: IEEE Press.
  
 4. 
 PaulC.Clements.1996.Asurveyofarchitecturedescriptionlanguages.In
 ProceedingsofInternational 
  
 Workshop on Software Specification and Design (IWSSD)
 , 16–25.
  
 5. A.Fauth,M.Freericks,andA.Knoll.1993.Generationofhardwaremachinemodelsfrominstruction 
  
 set descriptions. In
  IEEE Workshop on VLSI Signal Processing
 , 242–50.
  
 6. A. Gonzalez, C. Aliagas, and M. Valero. 1995. A data cache with multiple caching strategies 
 tuned to different types of locality. In
  ICS ’95: Proceedings of the 9th International 
 Conference on Supercom-puting
 , 338–47. New York: ACM Press.
  
 7. M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and R. B. Brown. 2001. 
 MiBench: A free, commercially representative embedded benchmark suite. In
  IEEE 
 Workshop in Workload Characterization
 .
  
 8. J.Gyllenhaal,B.Rau,andW.Hwu.1996.HMDESversion2.0specification.Tech.Rep.IMPACT-96-3, 
  
 IMPACT Research Group, Univ. of Illinois, Urbana.
  
 9. G. Hadjiyiannis, S. Hanono, and S. Devadas. 1997. ISDL: An instruction set description 
 language for retargetability. In
  Proceedings of Design Automation Conference (DAC)
 , 299–
 302. New York: IEEE Press.
  
 10. A. Halambi, P. Grun, V. Ganesh, A. Khare, N. Dutt, and A. Nicolau. 1999. EXPRESSION: A 
 language for architecture exploration through compiler/simulator retargetability. In
  
 Proceedings of Design Automation and Test in Europe
 . New York: IEEE Press.
  
 11. A. Halambi, A. Shrivastava, P. Biswas, N. Dutt, and A. Nicolau. 2002. A design space 
 exploration framework for reduced bit-width instruction set architecture (risa) design. In
  
 ISSS ’02: Proceedings of the 15th International Symposium on System Synthesis
 , 120–25. New 
 York: ACM Press.",NA
4 ,NA,NA
Whole Execution ,NA,NA
Traces and Their ,NA,NA
Use in Debugging,"Xiangyu Zhang
  
 Department of Computer Sciences, 
 Purdue University, 
  
 West Lafayette, IN 
  
 xyzhang@cs.purdue.edu
  
 4.1
  
 Introduction
  ..........................................
 4
 -
 1
  
  
 Neelam Gupta
  
 Department of Computer Sciences, 
 University of Arizona, Tuscon, AZ 
 ngupta@cs.arizona.edu
  
 Rajiv Gupta
  
 Department of Computer Science 
  
 and Engineering, 
  
 University of California, Riverside, CA 
 gupta@cs.ucr.edu",NA
Abstract,"4.2
  
 Whole Execution Traces
  ...............................
 4
 -
 2
  
 Timestamped WET 
 Representation
  
 •
  Compressing Whole
  
 Execution Traces
  
 4.3
  
 Using WET in Debugging
  .............................
 4
 -
 8
  
 Dynamic Program 
 Slicing
  
 •
  Dynamic Matching of Program
  
 Versions
  
 4.4 Concluding Remarks
  ..................................
 4
 -
 16 
 References
  
 ..................................................
 4
 -
 16
  
 Profiling techniques have greatly advanced in recent years. Extensive amounts of dynamic 
 information can be collected (e.g., control flow, address and data values, data, and control 
 dependences), and sophisticated dynamic analysis techniques can be employed to assist in 
 improving the performance and reliability of software. In this chapter we describe a novel 
 representation called
  whole execution traces
  that can hold a vast amount of dynamic information 
 in a form that provides easy access to this information during dynamic analysis. We demonstrate 
 the use of this representation in locating faulty code in programs through dynamic-slicing- and 
 dynamic-matching-based analysis of dynamic information generated by failing runs of faulty 
 programs.",NA
4.1,NA,NA
Introduction,"Program profiles have been analyzed to identify program characteristics that researchers have 
 then exploited to guide the design of superior compilers and architectures. Because of the large 
 amounts of dynamic information generated during a program execution, techniques for space-
 efficient represen-tation and time-efficient analysis of the information are needed. To limit the 
 memory required to store different types of profiles, lossless compression techniques for several 
 different types of profiles have been developed. Compressed representations of
  control flow
  
 traces can be found in [15, 30]. These profiles can be analyzed for the presence of hot program 
 paths or traces [15] that have been exploited for performing path-sensitive optimization and 
 prediction techniques [3, 9, 11, 21].
  Value profiles
  have been compressed using value predictors 
 [4] and used to perform code specialization, data compression, and value encoding [5, 16, 20, 31].
  
 Address profiles
  have also been compressed [6] and used for identifying hot data streams that
  
 4
 -
 1",NA
4.2,NA,NA
Whole Execution Traces,"WET for a program execution is a comprehensive set of profile data that captures the complete 
 functional execution history of a program run. It includes the following dynamic information:
  
 Control flow profile:
  The control flow profile captures the complete control flow path taken 
 during a 
  
 single program run.
  
 Value profile:
  This profile captures the values that are computed and referenced by each 
 executed 
  
 statement. Values may correspond to data values or addresses.
  
 Dependence profile:
  The dependence profile captures the information about data and control 
 depen-dences exercised during a program run. A data dependence represents the flow of a 
 value from the statement that defines it to the statement that uses it as an operand. A 
 control dependence between two statements indicates that the execution of a statement 
 depends on the branch outcome of a predicate in another statement.
  
 The above information tells what statements were executed and in what order (control flow 
 profile), what operands and addresses were referenced as well as what results were produced 
 during each statement execution (value profile), and the statement executions on which a given 
 statement execution is data and control dependent (dependence profile).
  
 4.2.1 Timestamped WET Representation
  
 WET is essentially a static representation of the program that is labeled with dynamic profile 
 information. This organization provides direct access to all of the relevant profile information 
 associated with every execution instance of every statement. A statement in WET can correspond 
 to a source-level statement, intermediate-level statement, or machine instruction.
  
 To represent profile information of every execution instance of every statement, it is clearly 
 necessary to distinguish between execution instances of statements. The WET representation 
 distinguishes 
 between 
 executioninstancesofastatementbyassigningunique
 timestamps
 tothem[30].Togeneratethetimesta
 mps a
  time
  counter is maintained that is initialized to one and each time a basic block is executed, 
 the current value of
  time
  is assigned as a timestamp to the current execution instances of all the 
 statements within the basic block, and then
  time
  is incremented by one. Timestamps assigned in 
 this fashion essentially remember the ordering of all statements executed during a program 
 execution. The notion of timestamps is the key to representing and accessing the dynamic 
 information contained in WET.",NA
4.3,NA,NA
Using WET in Debugging,"In this section we consider two debugging scenarios and demonstrate how WET-based analysis 
 can be employed to assist in fault location in both scenarios. In the first scenario we have a 
 program that fails to produce the correct output for a given input, and it is our goal to assist the 
 programmer in locating the faulty code. In the second scenario we are given two versions of a 
 program that should behave the same but do not do so on a given input, and our goal is to help 
 the programmer locate the point at which the behavior of the two versions diverges. The 
 programmer can then use this information to correct one of the versions.
  
 4.3.1 Dynamic Program Slicing
  
 Let us consider the following scenario for fault location. Given a failed run of a program, our goal 
 is to identify a fault candidate set, that is, a small subset of program statements that includes the 
 faulty code whose execution caused the program to fail. Thus, we assume that the fact that the 
 program has failed is known because either the program crashed or it produced an output that 
 the user has determined to be",NA
4.4,NA,NA
Concluding Remarks,"The emphasis of earlier research on profiling techniques was separately studying single types of 
 profiles (control flow, address, value, or dependence) and capturing only a subset of profile 
 information of a given kind (e.g., hot control flow paths, hot data streams). However, recent 
 advances in profiling enable us to simultaneously capture and compactly represent complete 
 profiles of all types. In this chapter we described the WET representation that simultaneously 
 captures complete profile information of several types of profile data. We demonstrated how 
 such rich profiling data can serve as the basis of powerful dynamic analysis techniques. In 
 particular, we described how dynamic slicing and dynamic matching can be performed efficiently 
 and used to greatly assist a programmer in locating faulty code under two debugging scenarios.
  
 References
  
 1. H. Agrawal and J. Horgan. 1990. Dynamic program slicing. In
  ACM SIGPLAN Conference on 
  
 Programming Language Design and Implementation
 , 246–56. New York, NY: ACM Press.
  
 2. T. Ball and J. Larus. 1996. Efficient path profiling. In
  IEEE/ACM International Symposium on 
  
 Microarchitecture
 , 46–57. Washington, DC, IEEE Computer Society.
  
 3. R. Bodik, R. Gupta, and M. L. Soffa. 1998. Complete removal of redundant expressions. In
  
 ACM SIG-PLAN Conference on Programming Language Design and Implementation
 , 1–14, 
 Montreal, Canada. New York, NY: ACM Press.
  
 4. 
 M.BurtscherandM.Jeeradit.2003.Compressingextendedprogramtracesusingvaluepredictors.
 In 
 International Conference on Parallel Architectures and Compilation Techniques
 , 159–69. 
 Washington, DC, IEEE Computer Society.
  
 5. B. Calder, P. Feller, and A. Eustace. 1997. Value profiling. In
  IEEE/ACM International 
 Symposium on 
  
 Microarchitecture
 , 259–69. Washington, DC, IEEE Computer Society.
  
 6. T. M. Chilimbi. 2001. Efficient representations and abstractions for quantifying and 
 exploiting data reference locality. In
  ACM SIGPLAN Conference on Programming Language 
 Design and Implemen-tation
 , 191–202, Snowbird, UT. New York, NY: ACM Press.
  
 7. T. M. Chilimbi and M. Hirzel. 2002. Dynamic hot data stream prefetching for general-
 purpose programs. In
  ACM SIGPLAN Conference on Programming Language Design and 
 Implementation
 , 199–209. New York, NY: ACM Press.
  
 8. N. Gupta, H. He, X. Zhang, and R. Gupta. 2005. Locating faulty code using failure-inducing chops.
  
 In
  IEEE/ACM International Conference on Automated Software Engineering
 , 263–72, Long 
 Beach, CA. New York, NY: ACM Press.
  
 9. R. Gupta, D. Berson, and J. Z. Fang. 1998. Path profile guided partial redundancy elimination 
 using speculation. In
  IEEE International Conference on Computer Languages
 , 230–39, 
 Chicago, IL. Washington, DC, IEEE Computer Society.
  
 10. R. Hildebrandt and A. Zeller. 2000. Simplifying failure-inducing input. In
  International 
 Symposium 
  
 on Software Testing and Analysis
 , 135–45. New York, NY: ACM Press.
  
 11. Q. Jacobson, E. Rotenberg, and J. E. Smith. 1997. Path-based next trace prediction. In
  
 IEEE/ACM 
  
 International Symposium on Microarchitecture
 , 14–23. Washington, DC, IEEE 
 Computer Society. 12. 
 C.Jaramillo,R.Gupta,andM.L.Soffa.1999.Comparisonchecking:Anapproachtoavoiddebugging 
  
 of 
 optimized code. In
  ACM SIGSOFT 7th Symposium on Foundations of Software Engineering and 
  
 8th European Software Engineering Conference
 .
  LNCS 1687
 , 268–84. Heidelberg, Germany: 
 Springer-
  
 Verlag. New York, NY: ACM Press.",NA
5 ,NA,NA
Optimizations for ,NA,NA
Memory Hierarchies,"5.1 
 5.2
  
 Introduction
  ..........................................
 5
 -
 1 
 Background
 ...........................................
 5
 -
 3
  
 Dependence in 
 Loops
  
 •
  Reuse and Locality
  
 •
  Quantifying
  
 Reuse and Locality
  
 5.3
  
 Loop Interchange
  .....................................
 5
 -
 6
  
 Legality of Loop Interchange
 •
  Dependent Loop 
 Indices
  
 Profitability of Loop Interchange
 •
  A Loop 
 Permutation
  
 •
  
 Algorithm
  
 5.4
  
 5.5
  
 5.6
  
 Loop Blocking
  ........................................
 5
 -
 11
  
 Legality of Strip-Mine and Interchange
 •
  Profitability of
  
 Strip-Mine and Interchange
 •
  Blocking with Skewing
  
 Other Loop Transformations
  ..........................
 5
 -
 15
  
 Loop 
 Fusion
  
 •
  Loop Fission
  
 Data Prefetching
  ......................................
 5
 -
 16
  
 Hardware Support
 •
  Profitability of Prefetching
 •
  Prefetching
  
 Affine Array Accesses
 •
  Prefetching Other Accesses
  
  
 Easwaran Raman 
  
 and 
  
 David I. August 
  
 Department of Computer Science, 
 Princeton University, Princeton, NJ 
 eraman@cs.princeton.edu 
  
 august@cs.princeton.edu",NA
5.1 ,NA,NA
Introduction,"5.7 
  
 5.8
  
 Data Layout Transformations
 ..........................
 5
 -
 22
  
 Field 
 Layout
  
 •
  Data Object Layout
  
 Optimizations for Instruction Caches
  ..................
 5
 -
 24
  
 Code Layout in 
 Procedures
  
 •
  Procedure Splitting
  
 •
  Cache Line
  
 Coloring
  
 5.9 
  
 Summary and Future Directions
  .......................
 5
 -
 27 
 5.10 
 References
  ............................................
 5
 -
 28 
 References
  
 ..................................................
 5
 -
 28
  
 Since the advent of microprocessors, the clock speeds of CPUs have increased at an exponential 
 rate. While the speed at which off-chip memory can be accessed has also increased exponentially, 
 it has not increased at the same rate. A
  memory hierarchy
  is used to bridge this gap between the 
 speeds of the CPU and the memory. In a memory hierarchy, the off-chip main memory is at the 
 bottom. Above it, one or more levels of memory reside. Each level is faster than the level below it 
 but stores a smaller amount of data. Sometimes registers are considered to be the topmost level 
 of this hierarchy.
  
 There are two ways in which the intermediate levels of this hierarchy can be organized.
  
 The most popular approach, used in most general-purpose processors, is to use
  cache memory
 . 
 A cache contains some frequently accessed subset of the data in the main memory. When 
 the processor wants to access a piece of data, it first checks the topmost level of the 
 hierarchy and goes down until
  
 5
 -
 1",NA
5.2,NA,NA
Background,"Code restructuring and data prefetching optimizations typically depend on at least partial 
 regularity in data access patterns. Accesses to arrays within loops are often subscripted by loop 
 indices, resulting in very regular patterns. As a result, most of these techniques operate at the 
 granularity of loops. Hence, we first present some theory on loops that will be used in subsequent 
 sections. A different approach to understanding loop accesses by using the domain of
  Z
 -
 polyhedra to model the loop iteration space is discussed by Rajopadhye et al. [24].
  
 5.2.1 Dependence in Loops
  
 Any compiler transformation has to preserve the semantics of the original code. This is ensured 
 by preserv-ing all true dependences in the original code. Traditional definitions of dependence 
 between instructions are highly imprecise when applied to loops. For example, consider the loop 
 in Figure 5.1 that computes the row sums of a matrix. By the traditional definition of true 
 dependence, the statement
  row sum[i]+= matrix[i][j]
  in the inner loop has a true 
 dependence on itself, but this is an imprecise statement since this is true only for statements 
 within the same inner loop. In other words,
  row sum[i+1]
  does not depend on
  row 
 sum[i]
 , and both can be computed in parallel. This shows the need for a more precise definition 
 of dependence within a loop. While a detailed discussion of loop dependence is beyond the scope 
 of this chapter, we briefly discuss loop dependence theory and refer readers to Kennedy and 
 Allen [13].
  
 An important drawback of the traditional definition of dependences when applied to loops is 
 that they do not have any notion of loop iteration. As the above example suggests, if a statement 
 is qualified by its loop iteration, the dependence definitions will be more precise. The following 
 definitions help precisely specify a loop iteration:
  
 for (i=0; i< m; i++){ 
  
 for(j = 0; j< n; j++){ 
  
  
  
 row_sum[i] += matrix[i][j] 
  
 } 
  
 }",NA
5.3,NA,NA
Loop Interchange,"Loop interchange changes the order of loops in a perfect loop nest to improve spatial locality. 
 Consider the loop in Figure 5.2. This loop adds two
  m
  ×
  n
  matrices. Since multidimensional arrays 
 in C are stored in row-major order, two consecutive accesses to the same array are spaced far 
 apart in memory. If all the three matrices do not completely fit within the cache, every access to
  
 a
 ,
  b
 , and
  c
  will miss in the cache, but a compiler can transform the loop into the following 
 equivalent loop:
  
 for (i=0; i< m; i++){ 
  
 for(j = 0; j< n; j++){ 
  
  
  
 c[i][j] = a[i][j] + b[i][j]; 
  
 } 
  
 }
  
 The above loop is semantically equivalent to the first loop but results in fewer conflict misses.
  
 As this example illustrates, the order of nesting may be unimportant for correctness and yet 
 may have a significant impact on the performance. The goal of loop interchange is to find a 
 suitable nesting order that reduces memory access latency while retaining the semantics of the 
 original loop. Loop interchange comes under a category of optimizations known as
  unimodular 
 transformations
 . A loop transformation is called unimodular if it transforms the dependences in a 
 loop by multiplying it with a matrix, whose deter-minant has a value of either −1 or 1. Other 
 unimodular transformations include loop reversal, skewing, and so on.
  
 5.3.1 Legality of Loop Interchange
  
 It is legal to apply loop interchange on a loop nest if and only if all dependences in the original 
 loop are preserved after the interchange. Whether an interchange preserves dependence can be 
 determined by looking at the direction vectors of the dependences after interchange. Consider a 
 loop nest
  N
  = (
 L
  1
 ,
  L
  2
 ,
  . . . L
  n
 ). Let
  D
 (
 i
 ,
  j
 ) be the direction vector of a dependence in this loop nest. If 
 the order of loops in the loop nest is permuted by some transformation, then the direction vector 
 corresponding to the dependence in the permuted loop nest can be obtained by permuting the 
 entries of
  D
 (
 i
 ,
  j
 ). To understand this, consider the iteration vectors
  i
  and
  j
  corresponding to the 
 source and sink of the dependence. A permutation of the loop nest results in a corresponding 
 permutation of the components of
  i
  and
  j
 . This permutes the distance vector
  d
 (
 i
 ,
  j
 ) and hence the 
 direction vector
  D
 (
 i
 ,
  j
 ). To determine whether
  N
  can be permuted to
  N
 ′
 = (
 L
  i
 1
 ,
  L
  i
 2
 ,
  . . . L
  i
 n
 ), the same 
 permutation is applied to the direction vectors of all dependences, and the resultant direction 
 vectors are checked for validity.
  
 for (j=0; j< n; j++){ 
  
 for(i = 0; i< m; i++){ 
  
  
  
 c[i][j] = a[i][j] + b[i][j]; 
  
 } 
  
 }",NA
5.4,NA,NA
Loop Blocking,"A very common operation in many scientific computations is matrix multiplication. The code 
 shown below is a simple implementation of the basic matrix multiplication algorithm.
  
 double a[m1][n1], b[n1][n2], c[m1][n2]; 
  
 for (i=0; i<m1; i++){ 
  
 for(j=0; j<n2; j++){ 
  
  
  
 c[i][j] = 0; 
  
  
  
 for (k=0; k < n1; k++){ 
  
  
  
  
 c[i][j] += a[i][k]*b[k][j]; 
  
  
  
 } 
  
 } 
  
 }
  
 This code has a lot of data reuse. For instance, every row of matrix
  a
  is used to compute
  n2
  
 different elements of the product matrix
  c
 . However, this reuse does not translate to locality if 
 the matrices do not fit in the cache. If the matrices do not fit in the cache, the loop also suffers 
 from capacity misses that are not eliminated by loop interchange. In this case, the following 
 transformed loop improves locality:
  
 for(i=0; i<m1; i++){ 
  
 for(j=0; j<n2; j++){ 
  
  
  
 c[i][j]=0; 
  
 } 
  
 } 
  
 for(i1=0; i1<m1; i1 += block_size){ 
  
 for(j1=0; j1<n2; j1 += block_size){ 
  
  
  
 for(k1=0; k1<n1; k1 += block_size){ 
  
  
  
  
 for (i=i1; i<min(m1, i1+block_size); i++){ 
  
  
  
  
 for(j=j1; j<min(n2, j1+block_size); j++){ 
  
  
  
  
  
  
 for (k=k1; k < min(n1, k1+block_size); k++){ 
  
  
  
  
  
 c[i][j] += a[i][k]*b[k][j]; 
  
  
  
  
  
  
 } 
  
  
  
  
 } 
  
  
  
  
 } 
  
  
  
 } 
  
 } 
  
 }
  
 First the initialization of the matrix
  c
  is separated out from the rest of the loop. Then, a 
 transformation known as
  blocking
  or
  tiling
  is applied. If the value of
  block size
  is chosen 
 carefully, the reuse in the innermost three loops translates to locality. Figures 5.6 and 5.7 show 
 the iteration space of a two-dimensional loop nest before and after blocking. In this example, the 
 original iteration space has been covered by using four nonoverlapping rectangular tiles or 
 blocks. In general, the tiles can take the shape of a
  parallelepiped
  for an
  n
 -dimensional iteration 
 space. A detailed discussion of tiling shapes can be found in Rajopadhye [24].",NA
5.5,NA,NA
Other Loop Transformations,"5.5.1
  
 Loop Fusion
  
 When there is reuse of data across two independent loops, the loops can be fused together, 
 provided their indices are compatible. For example, the following set of loops:
  
 max = a[0]; 
  
 for(i=1; i<N; i++){ 
  
 if (a[i] > max) 
  
  
  
 max = a[i]; 
  
 } 
  
 min = a[0]; 
  
 for(i=1; i<N; i++){ 
  
 if (a[i] < min) 
  
  
  
 min = a[i]; 
  
 }
  
 can be fused together into a single loop:",NA
5.6,NA,NA
Data Prefetching,"Data prefetching differs from the other loop optimizations discussed above in some significant 
 aspects: 
  
 r
  Instead of transforming the data access pattern of the original code, prefetching 
 introduces additional code that attempts to bring in the cache lines that are likely to be accessed 
 in the
  
 near future.",NA
5.7,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Data Layout Transformations,"The techniques discussed so far transform the code accessing the data so as to decrease the 
 memory access latency. An orthogonal approach to reduce memory access latency is to transform 
 the layout of data in memory. For example, if two pieces of data are always accessed close to each 
 other, the spatial locality can be improved by placing those two pieces of data in the same cache 
 line. Data layout transformations are a class of optimization techniques that optimize the layout 
 of data to improve spatial locality. These transformations can be done either within a single 
 aggregate data type or across data objects.
  
 5.7.1 Field Layout
  
 The fields of a record type can be classified as
  hot
  or
  cold
  depending on their access counts. It is 
 often beneficial to group hot fields together and separate them from cold fields to improve cache 
 utilization.
  
 Consider the following record definition:
  
 struct S1 { 
  
 char hot[4]; 
  
 char cold[60]; 
  
 }; 
  
 S1 s1[512];
  
 Each instance of
  S1
  occupies a single cache line, and the entire array
  s1
  occupies 512 cache 
 lines. The total size of this array is well above the size of L1 cache in most processors, but only 4 
 bytes of the above record type are used frequently. If the
  struct
  consists of only the field
  hot
 , 
 then the entire array fits within an L1 cache.
  
 Structure splitting
  involves separating a set of fields in a record type into a new type and inserting 
 a pointer to this new type in the original record type. Thus, the above
  struct
  can be 
 transformed to:
  
 struct S1_cold { 
  
 char cold[60]; 
  
 };
  
 struct S1 { 
  
 char hot[4]; 
  
 struct S1_cold *cold_link; 
  
 };
  
 S1 s1[512];
  
 After the transformation, the array
  s1
  fits in L1 cache of most processors. While this increases 
 the cost of accessing
  cold
 , as it requires one more indirection, this does not hurt much because 
 it is accessed infrequently. However, even this cost can be eliminated for the
  struct
  defined 
 above, by transforming 
 S1
 , as follows:
  
 struct S1_cold { 
  
 char cold[60]; 
  
 };
  
 struct S1_hot { 
  
 char hot[4]; 
  
 }; 
  
 S1_hot s1_hot[512]; 
  
 S1_cold s1_cold[512];",NA
5.8,NA,NA
Optimizations for Instruction Caches,"Modern processors issue multiple instructions per clock cycle. To efficiently utilize this ability to 
 issue multiple instructions per cycle, the memory system must be able to supply the processor 
 with instructions at a high rate. This requires that the miss rate of instructions in the instruction 
 cache 
 be 
 very 
 low. 
 Several",NA
5.9,NA,NA
Summary and Future Directions,"The various optimizations for memory hierarchies described in this chapter are essential 
 components of optimizing compilers targeting modern architectures. While no single technique 
 is a silver bullet for bridg-ing the processor–memory performance gap, many of these 
 optimizations complement each other, and their combination helps a wide range of applications. 
 Table 5.1 summarizes how each of the optimizations achieve improved memory performance.
  
 While these optimizations were motivated by the widening performance gap between the 
 processor and the main memory, the recent trend of stagnant processor clock frequencies may 
 narrow this gap. However, the stagnation of clock frequencies is accompanied by another trend 
 — the prevalence of
  chip multiprocessors
  (CMPs). CMPs pose a new set of challenges to memory 
 performance and increase the importance of compiler-based memory optimizations. Compiler 
 techniques need to focus on multi-threaded applications, as more applications will become multi-
 threaded to exploit the parallelism offered by CMPs. Compilers also have to efficiently deal with 
 the changes in the memory hierarchy that may have some levels of private caches and some level 
 of caches that are shared among the different cores. The locality in the shared levels of the 
 hierarchy for an application is influenced by applications that are running in the other cores of 
 the CMP.
  
 TABLE 5.1
  
 Classification of optimizations for memory hierarchies
  
 Optimization
  
 Performance improvement
  
 Loop interchange
  
 Fewer conflict misses
  
 Loop blocking
  
 Fewer capacity misses
  
 Loop fusion
  
 Fewer capacity misses
  
 Loop fission
  
 Fewer conflict misses
  
 Data prefetching
  
 Fewer cold, capacity, and conflict misses
  
 Misses partially hidden
  
 Data layout 
  
 Instruction cache 
 optimizations
  
 Fewer conflict misses, improved cache 
 utilization Fewer conflict misses, improved 
 cache utilization",NA
5.10,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
References,"The work of Abu-Sufah et al. [1] was among the first to look at compiler transformations to 
 improve mem-ory locality. Allen and Kennedy [3] proposed the technique of automatic loop 
 interchange. 
 Loop 
 tiling 
 was 
 proposedbyWolfe[32, 
 33],whoalsoproposedloopskewing[31].Severalenhancementstotilingincluding 
 tiling 
 at 
 the 
 register level [11], tiling for imperfectly nested loops [2], and other improvements [12, 27] have 
 been proposed in the literature. Wolf and Lam [29, 30] proposed techniques for combining 
 various unimodular transformations and tiling to improve locality. Detailed discussion of various 
 loop restruc-turing techniques can be found in textbooks written by Kennedy and Allen [13] and 
 Wolfe [34] and in the dissertations of Porterfield [23] and Wolf [28].
  
 Software prefetching was first proposed by Callahan et al. [5] and Mowry et al. [19–21]. 
 Machine-specific enhancements to software prefetching have been proposed by Santhanam et al. 
 [26] and Doshi et al. [8]. Luk and Mowry [15] proposed some compiler techniques to prefetch 
 recursive data structures that may not have a strided access pattern. Saavedra-Barrera et al. [25] 
 discuss the combined effects of unimodular transformations, tiling, and software prefetching. 
 Mcintosh [17] discusses various compiler-based prefetching strategies and evaluates them.
  
 Hundt et al. [10] developed an automatic compiler technique for structure layout 
 optimizations. Calder et al. [4] and Chilimbi et al. [6, 7] proposed techniques for data object 
 layout that require some level of programmer intervention or library support. Mcintosh et al. 
 [18] describe an interprocedural optimiza-tion technique for placement of global values. Lattner 
 and Adve [14] developed compiler analysis and transformation for pool allocation based on the 
 types of data objects.
  
 McFarling [16] first proposed optimizations targeting instruction cache performances. He gave 
 results on optimal performance under certain assumptions. Pettis and Hansen [22] proposed 
 several profile-guided code positioning techniques including basic block ordering, basic block 
 layout, and procedure splitting. Hashemi et al. [9] proposed the coloring-based approach to 
 minimize cache line conflicts in instruction caches.
  
 References
  
 1. W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. 1979. Automatic program transformations for 
 virtual 
  
 memory computers. In
  Proceedings of the National Computer Conference
 , 1979.
  
 2. Nawaaz Ahmed, Nikolay Mateev, and Keshav Pingali. 2000. Tiling imperfectly-nested loop nests.
  
 In
  Supercomputing ’00: Proceedings of the 2000 ACM/IEEE Conference on Supercomputing
  
 31–34. Washington, DC: IEEE Computer Society.
  
 3. John R. Allen and Ken Kennedy. 1984. Automatic loop interchange.
  SIGPLAN Notices
  
 19(6):233–46, 
  
 New York, NY: ACM Press.
  
 4. Brad Calder, Chandra Krintz, Simmi John, and Todd Austin. 1998. Cache-conscious data placement.
  
 In
  ASPLOS-VIII: Proceedings of the Eighth International Conference on Architectural Support 
 for Programming Languages and Operating Systems
 , 139–49. New York: ACM Press.
  
 5. David Callahan, Ken Kennedy, and Allan Porterfield. 1991. Software prefetching.
  SIGARCH 
 Computer Architecture News
  19(2):40–52. In
  ASPLOS-IV: Proceedings of the Fourth 
 International Conference on Architectural Support for Programming Languages and 
 Operating Systems
 , New York, NY: ACM Press.
  
 6. Trishul M.Chilimbi, Bob Davidson, and JamesR. Larus.1999. Cache-conscious structure definition.
  
 In
  PLDI ’99: Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language 
 Design and Implementation
 , Vol. 34, 13–24. New York: ACM Press.
  
 7. Trishul M. Chilimbi, Mark D. Hill, and James R. Larus. 1999. Cache-conscious structure 
 layout. In 
 PLDI ’99: Proceedings of the ACM SIGPLAN 1999 Conference on Programming 
 Language Design and Implementation
 , Vol. 34, 1–12. New York: ACM Press.",NA
6 ,NA,NA
Garbage Collection,NA,NA
Techniques,"6.1
  
 Introduction
  ..........................................
 6
 -
 1 
 The Need for Garbage 
 Collection
 •
  Features of Garbage 
  
 Collection Methods
 •
  Effectiveness of Garbage Collection
  
 •
  The Pragmatics of Garbage Collection
 •
  Locality of Program
  
 Execution
  
 6.2
  
 6.3
  
 Basic Methods of Garbage Collection
  ..................
 6
 -
 6
  
 Reference Counting-Based 
 Collectors
 •
  Mark-Compact 
 Collectors
  
 •
  Mark-Sweep Collectors
  
 Copying Collectors
 ....................................
 6
 -
 12
  
 Cheney’s Copying 
 Collector
  
 •
  Performance of Copying
  
 Collector
 s
  
 •
  Locality Issues
  
  
 Amitabha Sanyal 
  
 and 
  
 Uday P. Khedker 
  
 Department of Computer Sciences & 
 Engineering, 
  
 IIT Bombay, Mumbai, India 
  
 as@cse.iitb.ac.in 
  
 uday@cse.iitb.ac.in
  
 6.4
  
 6.5
  
 Generational Collectors
  ...............................
 6
 -
 16
  
 The Basic Design
 •
  Tenuring Policy
 •
  Recording Ages
  
 of Objects
 •
  Recording of Intergenerational Pointers
  
 Incremental and Concurrent Collectors
  ................
 6
 -
 23
  
 Incremental and Concurrent Non-Moving 
 Collectors
  
 •
  Incremental and Concurrent Moving Collectors
  
 6.6 Conclusions
  ..........................................
 6
 -
 31 
 References
  
 ..................................................
 6
 -
 32
  
 This chapter provides a comprehensive discussion of various garbage collection methods. The 
 goal of this chapter is to highlight fundamental concepts rather than cover all the details. After 
 introducing the underlying issues in garbage collection, the basic methods of garbage collection 
 are described. This is followed by their generational, incremental, and concurrent versions.",NA
6.1,NA,NA
Introduction,"This section examines the need for garbage collection and introduces the basic concepts related 
 to garbage collection. It also establishes several metrics on the basis of which garbage collectors 
 are evaluated and identifies the distinguishing features of garbage collectors.
  
 6.1.1 The Need for Garbage Collection
  
 A program in execution needs memory to store the data manipulated by it. The data is named by 
 variables in the program. Memory is allocated in various ways that differ from each other in their 
 answers to the following questions: At what point of time is a variable bound to a chunk of 
 memory, and how long does the binding last?
  
 6
 -
 1",NA
6.2,NA,NA
Basic Methods of Garbage Collection,"In this section, we describe the early collectors called
  reference counting
  and
  mark-sweep
 . Both 
 these collectors were designed for Lisp implementations around 1960. We also describe the
  
 mark-compact 
 method, which is a later refinement of mark-sweep incorporating compaction.
  
 6.2.1 Reference Counting-Based Collectors
  
 The reference counting method [26, 37] stores in each object a count of the number of pointers 
 pointing to the object. Each pointer assignment also involves manipulation of these counts. When 
 a new object is allocated, the count is initialized to 1. Let pointers p and q hold the addresses of 
 objects A and B before the assignment p = q. When the assignment is executed, both p and q point 
 to B. These assignments are executed through the garbage collector, which decrements the 
 reference count of A by 1 and increments the reference count of B by 1. If the reference count of 
 an object becomes 0, the object is detected as garbage and the space occupied by it is added to a 
 list of free cells. If the reclaimed object contains pointers to other objects, then the reference 
 counts of these objects are also decremented. Clearly, this method is incremental, nontracing, and 
 nonmoving.
  
 This method requires additional space for storing reference counts and additional time for 
 manipulating these counts. In particular, most pointer assignments require two counts to be 
 adjusted rather than one.
  
 The limitations of this method are: 
  
 r
  The objects that are a part of a cyclic data structure continue to have nonzero counts even if 
 they 
 r
  Achainofreclamationsincreasespausetimes,affectingthealmost-real-
 timebehaviorofthemethod.
  
 Several variants of reference counting have been proposed to reduce its overheads and 
 overcome its limitations. The space overheads can be overcome by using reference counts 
 saturating at small values. When the maximum value is reached, the counts are neither 
 decremented nor incremented. Such objects can then be garbage collected by a tracing method 
 and their counts reset to 0. By resorting to a tracing method, this approach also overcomes the 
 limitation of not being able to garbage collect cyclic data structures. In the extreme form of this 
 method, one-bit counts can be used. The bit value, in combination with an incoming pointer, 
 captures three states of an object, as illustrated in Figure 6.2. There are variations of this 
 approach in which the bit is stored in the objects themselves [68], or as a tag within pointers 
 [57]. The latter has the advantage that the cell pointed to by a pointer need not be read during 
 garbage collection. This may improve the cache and paging behavior of the mutator.
  
 While most approaches use mark-sweep collectors for garbage collecting shared objects with 
 saturated counts, some use mark-compact collectors [66] or even copying collectors [67]. A 
 further variation tries to extend the range of single bit counts to accurately determine up to two 
 or more pointers over short program fragments [47].
  
 The time overheads can be reduced by
  deferred reference counting
  [27]. This method avoids 
 adjusting the counts for objects pointed to by parameters and local variables. This is done on the",NA
6.3,NA,NA
Copying Collectors,"Similartomark-compactcollectors,copyingcollectorsarealsotracingcollectorsthatscavengeandcompact
  
 reachable objects. However, instead of using the same space for both allocation and compaction, copying
  
 collectors use a separate empty space for compaction. This makes compaction easy, since the marked data
  
 is just laid contiguously starting from one end of this empty space. Tracing and copying in a copying
  
 collector requires a single traversal over all reachable objects. The earliest copying collector with both
  
 semi-spaces in memory was proposed by Fenichel and Yochelson [29].
 4
 However, their formulation of the
  
 tracing traversal was recursive and thus consumed stack space. The first practical copying collector was
  
 suggested by Cheney [20]. Cheney’s formulation is nonrecursive and uses a part of the space taken by the
  
 copied objects as a replacement for the stack.
  
 6.3.1 Cheney’s Copying Collector
  
 The copying collector divides the heap into two semi-spaces. The mutator uses just one of the two semi-
  
 spaces between consecutive garbage collections. The semi-spaces are traditionally called
  FromSpace
  and
  
 ToSpace
 becausethegarbagecollectorscavengesreachabledatafrom
 FromSpace
 andcopiesthemto
 ToSpace
 .
  
 4
 An earlier copying collector proposed by Minsky [43] had one of the semi-spaces on disk.",NA
6.4,NA,NA
Generational Collectors,"There is a mismatch between the demographic behavior of objects and the strategy employed by 
 tracing collectors. If all objects are collected with the same frequency, objects that survive a 
 number of collections will be repeatedly traced. This represents unremunerative work for the 
 garbage collector. Baker [8] has expressed the cost per allocated object of a garbage collection as 
 the
  mark/cons
 5
 ratio, which should be as low as possible. Generational collectors [40, 43, 59] 
 decrease the
  mark/cons
  ratio by putting objects of different lifetimes in separate collection 
 spaces.
  
 The basis of the generational collectors is the
  weak generational hypothesis
 , which states that 
 younger 
 objectshaveashorterlifeexpectancythanolderobjects.Thishypothesisseemstobetrueinpractice[33,5
 6]. Generationalcollectorstakeadvantageofthisbyhaving 
 morethanonecollectionspace,whicharecollected at varying frequencies. The youngest collection 
 space is collected most often. Objects that survive the youngest collection for a certain amount of 
 time are promoted to an older generation, which is collected less often. The object continues in the 
 older generation for some time before it either dies and is garbage collected or is promoted to the 
 next generation. Thus, to recover the same amount of garbage as a nongenerational garbage 
 collector, the generational collector incurs fewer tracing and copying costs. The central idea of 
 generational garbage collection is to collect different generations at different fre-quencies. The 
 weak generational hypothesis only makes it profitable to collect younger generations more 
 frequently than older generations. As a theoretical exercise, Baker [8] has considered a model of 
 life ex-pectancy of objects in which the fraction of objects expected to survive a period
  t
  is given by 
 2
 −
 t/τ
 , where
 τ
  is a constant. In this exponential decay model, the life expectancy of an object does 
 not depend on its age. Using this model, it has been shown [24, 33, 56] that older generations will 
 have fewer survivors than young ones, so it will be profitable to collect older generations more 
 frequently than younger generations.
  
 In summary, the advantages of the generational scheme are the following: 
  
  
 r
  Because of the decreased
  mark/cons
  ratio, the time taken by the generational collector over 
 the 
  
 r
  Since the younger generation has fewer survivors, its collection is faster and the pauses 
 experienced 
  
 r
  The size of each generation is made considerably smaller than the next. Thus, the 
 size of the youngest generation is quite small. Since most of the activities of the collector are 
 limited to the 
  
  
 youngest generation, the locality of the collector and possibly 
 the mutator is improved. Indeed, 
  
  
 some authors [64] think this is the real reason 
 for the effectiveness of generational collectors.
  
 6.4.1 The Basic Design
  
 We shall consider a generational extension of a copying collector with two generations. 
 Generalization to more than two generations does not pose any additional conceptual problems. 
 Assume that the state of 
 ToSpace
  after two rounds of garbage collection is as shown in Figure 
 6.10. We shall assume for now that each object has an extra field to record its age in terms of the 
 number of collections it has survived.",NA
6.5,NA,NA
Incremental and Concurrent Collectors,"In some applications large pauses cannot be tolerated. Such applications are mostly interactive 
 and in some instances rely on a guarantee of a bound on the length of pauses. While running such 
 ap-plications, it may not be possible to allow the associated garbage collector to run until the end 
 of a round of collection. Hence, one has to use garbage collectors that are incremental, that is, 
 collectors in which a single round of garbage collection is interspersed with mutator execution. 
 The reference count-ing method described earlier is, in its natural form, incremental. However, 
 reference counting has its own limitations of not being able to handle cycles naturally and of 
 uneven distribution of pause times (Section 6.2.1). Therefore, it becomes necessary to think of 
 incremental versions of other kinds of garbage collectors.
  
 A garbage collector is said to be
  incremental
  if garbage collection is interleaved with mutator 
 execution in a single thread of execution. It is said to be
  concurrent
  if the collector and the 
 mutator can run concurrently on multiple threads. In effect, this means the interleaving between 
 the collector and the mutator is well defined in the case of incremental collectors and arbitrary in 
 the case of concurrent collectors. Finally, a garbage collector is said to be
  real-time
  if it can 
 guarantee a bound on the length of pauses.
  
 The central issue with incremental collectors is that because of the interleaving of mutator and 
 collector, their views of the reachability graph may be different. Instead of trying to make the two 
 views identical, the collector’s view of the reachability graph is made an overapproximation of 
 the mutator’s view. Thus,",NA
6.6,NA,NA
Conclusions,"We have reviewed some basic garbage collection strategies along with their generational and 
 incremental versions. Zorn’s [74] empirical studies with the Boehm–Weiser collector have shown 
 that, in some aspects, the performance of programs with even a basic collector such as mark-
 sweep is comparable to that of programs with explicit memory management. For example, the 
 CPU costs of execution under the Boehm–Weiser collector are often comparable to and 
 sometimes better than the costs of execution under explicitly managed memory. However, their 
 memory requirements are high. One of the reasons for this is the conservative nature of the 
 Boehm–Weiser collector. The page and cache faults are also higher compared to programs with 
 explicit memory management.
  
 An important shortcoming of the basic collectors is their large pause times. This is a serious 
 concern 
 in 
 interactiveandmultimediaapplications.Bothincrementalandgenerationaltechniquesreducepauseti
 mes. 
 Generationalcollectorsalsoresultinbetterlocalityofreferences.Zorn[72]hasfoundthetimeoverhead
 sof generational collection over explicit memory management to be within 20%. While 
 generational collectors seem to perform well on the whole, our review suggests that the choice of 
 a collector is highly dependent on the context of its usage.
  
 We now describe the directions of current work on garbage collection. Some of these 
 developments are attempts to improve existing ideas, while others are new developments.
  
 r
  Garbage collection in uncooperative environments:
  Garbage collectors that do not 
 depend on information from the compiler are useful, because they can be made to work 
 with already exist-ing systems without modifications. However, because of incomplete 
 information, these collectors have to be conservative and therefore preserve significant 
 amounts of garbage. Thus, their mem-ory requirement is higher than that of collectors that 
 work in more cooperative environments.",NA
7 ,NA,NA
Energy-Aware ,NA,NA
Compiler ,NA,NA
Optimizations,"7.1 
 7.2
  
 Introduction
  ..........................................
 7
 -
 1 
 Power and Energy 
 Models
  .............................
 7
 -
 2
  
  
 Y. N. Srikant and 
  
 K. Ananda Vardhan 
  
 Department of Computer Science and 
 Automation, 
  
 Indian Institute of Science, 
  
 Bangalore, India 
  
 srikant@csa.iisc.ernet.in 
  
 nandu@csa.iisc.ernet.in",NA
7.1 ,NA,NA
Introduction,"7.3
  
 7.4
  
 Dynamic Voltage Scaling (DVS)
  .......................
 7
 -
 3
  
 Region-Based Compiler Algorithm for DVS
 •
  Finding Regions
 •
  
 Integer Linear Program–Based Compiler Algorithm for DVS
 •
  
 DVS Algorithm for a Dynamic Compiler
  
 Optimizations for Leakage Energy Reduction
  ..........
 7
 -
 13
  
  
 Transition-Aware Scheduling 
  
 7.5 Compile-Time Allocation for Scratchpad Memory
  .....
 7
 -
 21 
  
 The Static Allocation Algorithm 
  
 7.6 Conclusions and Future Directions
  ....................
 7
 -
 23 
 References
  ..................................................
 7
 -
 24
  
 The demands of modern society on computing technology are very severe. Its productivity is 
 closely linked to availability of inexpensive and energy-efficient computing devices. While battery 
 technology is developing at a slow pace, the demands of sophisticated mobile computing systems 
 that require to be powered for several hours by batteries without recharging them require novel 
 techniques to be devised to utilize the available energy efficiently. Similarly, large data centers 
 that support complex Web searches and online purchases house thousands of servers that guzzle 
 power. Any energy saved by the microprocessors will lower electricity bills. Under these 
 circumstances, it is no surprise that modern microprocessor design considers energy efficiency 
 as a first-class design constraint [31].
  
 Apart from the above extremely important reasons, there are other technical reasons for 
 consider-ing power dissipation seriously. According to Pollack [35], the power densities in 
 microprocessors are rapidly approaching that of a nuclear reactor. High power dissipation 
 produces excessive heat and lowers reliability and lifetimes of chips. It increases production cost 
 due to complex cooling and packaging requirements, impacts our environment, and may 
 endanger the human body.
  
 There are many ways to save power and energy in a computing system. Some are hardware-
 based and the others are software-based techniques. The major subsystems of a computing 
 system that can benefit from power- and energy-aware designs are the disk, the memory, the 
 CPU, and the communication subsystems. Energy-saving schemes could be built into the 
 hardware, the operating system, or the device drivers or inserted by the compiler into the 
 application program itself. Various proposals have been made to reduce
  
 7
 -
 1",NA
7.2,NA,NA
Power and Energy Models,"It is essential to understand the models for power and energy consumption in CMOS chips in 
 order to design
  any
  energy-saving optimization in compilers or operating systems. Since energy 
 is the product of power and time, it may seem that optimizing one will automatically optimize the 
 other. This is not true. For example, the power consumed by a computer system can be reduced 
 by halving the clock frequency, but if the computer takes twice as long to complete the same task, 
 the total energy consumed may not be lower. Energy optimization seems more important in 
 mobile systems because it makes the battery run longer. Servers, however, need power",NA
7.3,NA,NA
Dynamic Voltage Scaling (DVS),"It is well known that Dynamic Random Access Memory (DRAM) is slower than the CPU by at least 
 a factor of 3. Caches are used to offset this incompatibility. However, in the case of programs that 
 use memory heavily and whose data do not fit into the cache, the CPU stalls to let memory catch 
 up and to ensure that data dependencies are honored. It is easy to come to the conclusion that 
 the CPU can use a lower clock frequency in certain parts of the program that are memory bound 
 and thereby save power and energy.",NA
7.4,NA,NA
Optimizations for Leakage Energy Reduction,"Power consumption in processor cores has become an important concern in architectural design 
 as well in compiler construction. In older technologies, as indicated in Equation 7.4 and the 
 associated discussion, dynamic energy dissipation was the dominating factor. The leakage energy 
 consumption corresponding to the inactive state of the circuits was negligible. This assumption 
 will no longer hold in the near future.
  
 Static energy dissipation results from leakage current (see Equation 7.4), which in turn is very 
 sensitive to increases in threshold voltage,
  V
 t
 . As integrated circuit technology improves and 
 scales to smaller dimensions,supplyvoltagesarereduced,andthisinturndecreases
  V
 t
 . With the 
 threshold voltages reaching low values, the leakage energy is estimated to be on par with the 
 dynamic switching energy in all the units of the processor, with the forthcoming 70-nm 
 technology. Studies show that with the technology trend, leakage power consumption is going to 
 increase linearly, whereas dynamic power consumption is going to remain almost constant [42], 
 the former thereby constituting a significant portion of the total power consumption. In the 
 literature, the terms
  static energy
  and
  leakage energy
  are used interchangeably, and they will be 
 used similarly in this chapter. Leakage power consumption can be reduced if the number of idle 
 cycles is increased. This can be done automatically by circuit-level switching techniques or by 
 explicitely",NA
7.5,NA,NA
Compile-Time Allocation for Scratchpad Memory,"It is well known that off-chip memory is the slowest and most energy consuming type of memory 
 in the memory hierarchy. Cache memories are much smaller than off-chip main memories and 
 are much faster. They are used to keep frequently used code blocks or variables and thereby 
 reduce the number of main memory accesses. Caches are well known and are used in most 
 processors. 
 Caches 
 have 
 a 
 tag 
 memory 
 to 
 storevalidaddressesandextrahardwareforafastcomparisonofaddresseswiththecontentsoftagmem
 ory. These are needed to detect hits and misses, but they consume energy continuously since 
 comparisons are made during each memory access. Caches integrate very well with software and 
 are transparent to the software in their operation. This has made caches very popular with 
 hardware designers. However, cache hits and misses bring a certain amount of nondeterminism 
 to the execution time and its estimation. Most 
 worst-case execution time
  (WCET) estimation 
 software ignores the presence of caches and regards the time of execution of each instruction 
 assuming a cache miss. WCET estimation in the presence of cache is a hot research topic [30].
  
 Scratchpad memories have been proposed not only to make WCET estimation more accurate, 
 but also to reduce the energy consumption in memory hierarchies. Scratchpad memories are as 
 fast as cache memories but do not have the tag memory and address comparison hardware. 
 Therefore, they consume less energy than caches but require the services of a compiler or the 
 programmer to fill it with code and/or variables. Letting programmers handle scratchpad 
 memories is not desirable since the allocation can become quite complex and may lead to 
 inefficient allocation and possible program errors. Compile-time algorithms based on integer 
 linear programming have been proposed for scratchpad memory allocation.
  
 The discussion in the sections that follow is based on such algorithms described in [2, 41, 44, 49].
  
 7.5.1 The Static Allocation Algorithm
  
 A typical embedded system contains several types of memories, and each type has its own 
 advantages and disadvantages. Cache, on-chip scratchpad Static Random Access Memory (SRAM), 
 off-chip SRAM,",NA
7.6,NA,NA
Conclusions and Future Directions,"This chapter considered a few energy-saving compiler optimizations in detail. Dynamic voltage 
 scaling, which reduces the dynamic energy consumption in processors, was formulated as a 
 minimization prob-lem, and transition-aware scheduling, which reduces leakage energy 
 consumption, was formulated as an instruction scheduling problem. Scratchpad memories, which 
 are an alternative to cache memories in embedded processors, require the compiler to perform 
 the allocation of code and data objects onto them, and this problem was formulated as an integer 
 linear programming problem.
  
 The last word on energy-aware compiler optimizations has hardly been said. The processors 
 used in sensor networks and other embedded systems need aggressive energy optimizations. No 
 professional compilers incorporate all the energy-aware optimizations such as DVS, instruction 
 scheduling, memory bank allocation, scratchpad allocation, cache management, and 
 communication reduction (in clusters), and building such a compiler is an interesting research 
 project. Furthermore, the interaction of various energy-savingoptimizationswithperformance-
 improvingoptimizationshasnotbeenstudiedthoroughly. Integration of optmizations possible at 
 the operating system level into a compiler framework and design of a programming framework 
 to develop power- and energy-aware applications would be other research possibilities. In 
 addition, innovations in architecture are introducing novel features into hardware to",NA
8 ,NA,NA
Statistical and ,NA,NA
Machine Learning ,NA,NA
Techniques in ,NA,NA
Compiler Design,"8.1
  
 Introduction
  ..........................................
 8
 -
 1
  
 Motivation
  
 •
  Classical Solutions
  
  
 P.J. Joseph 
  
 Freescale Semiconductor India Pvt. Ltd., 
 Noida, India 
  
 PJ.Joseph@freescale.com
  
 Matthew T. Jacob, 
  
 Y.N. Srikant, and 
  
 Kapil Vaswani 
  
 Department of Computer Science and 
 Automation, 
  
 Indian Institute of Science, 
  
 Bangalore, India 
  
 mjt@csa.iisc.ernet.in 
  
 srikant@csa.iisc.ernet.in 
  
 kapil@csa.iisc.ernet.in",NA
8.1 ,NA,NA
Introduction,"8.2 
  
 Adaptive Compilers
  ...................................
 8
 -
 7
  
 8.3 
  
 Characterizing Compiler Interactions
  ..................
 8
 -
 8
  
 Empirical Regression Models
  
 8.4 
  
 Heuristic Selection
  ....................................
 8
 -
 14
  
 Deciding Best Loop Unroll Factors
 •
 Learning Priority
  
 Functions
  
 8.5
  
 8.6
  
 Phase Ordering
  .......................................
 8
 -
 21
  
 Characterizing Phase Ordering 
 Space
 •
  Focused Search Techniques
  
 •
  Search Techniques
  
 Optimization Flags and Parameter Selection
  ...........
 8
 -
 27
  
 Optimization Flag Selection Using Inferential Statistics
  
 8.7 Conclusions
  ..........................................
 8
 -
 28
  
 Appendix A: Optimality in Program Compilation
  ............
 8
 -
 29
  
 References
  ..................................................
 8
 -
 30
  
 In computer science, the word
  optimize
  refers to
  the process of modifying to achieve maximum 
 efficiency in storage capacity or time or cost
 . True to their name,
  optimizing compilers
  transform 
 computer programs into semantically equivalent programs with the objective of maximizing 
 performance. The transformation is achieved using a set of optimizations. Each optimization 
 usually consists of an analysis component that identifies specific performance anomalies and a 
 transformation component that eliminates the anomalies via a series of code transformations.
  
 Despite significant advances in compilation techniques and numerous compiler optimizations 
 that have been proposed over the years, the goal of building optimizing compilers that deliver 
 optimal or even near-optimal performance
 1
 is far from being achieved. For instance, it has been 
 observed that performance of compiler-generated code falls far short of hand-optimized code in 
 certain application domains [1]. One might argue that the superiority of hand-optimized code is 
 due to domain knowledge or special insights into the functioning of code that programmers may 
 have, which allows them to make
  
 1
 Refer to Appendix A for a formal description of the notion of optimality in program compilation. 
  
  
 8
 -
 1",NA
8.2,"FIGURE 8.4
  
 The structure of a typical optimizing compiler.",NA
Adaptive Compilers,"Let us first review the structure of existing optimizing compilers and see how they fare in terms 
 of flexibility and adaptability. Figure 8.4 illustrates the architecture of a typical optimizing 
 compiler. The back-end of the compiler consists of a set of optimizations or
  phases
 . These phases 
 are applied to an intermediate representation of the program in a predetermined sequence, 
 irrespective of the program being optimized. Each phase is almost always designed 
 independently, and interactions between optimizations are often ignored. Coarse models of the 
 underlying hardware are sometimes employed to guide optimizations, with the aim of reducing 
 the influence of negative interactions between the optimizations and the hardware. It is easy to 
 see that the structure of a modern optimizing compiler is more or less rigid. This lack of flexibility 
 is one of the key reasons that optimizing compilers fail to deliver optimal performance.
  
 In light of these shortcomings of conventional compilers, many in the compiler community 
 have favored the idea of building compilers that adapt to the program and the hardware [24]. 
 Although the idea of an adaptive compiler sounds compelling, several concerns need to be 
 addressed before an actual implementation can be envisaged. For example, what are the 
 necessary building blocks of an adaptive compiler? How is the compiler structured? What are the 
 compile time requirements of the compiler? What performance guarantees can the compiler 
 make? In the rest of this section, we attempt to answer some of these questions.
  
 Figure 8.5 illustrates the proposed architecture of an adaptive compiler [24]. This architecture 
 dif-fers from the traditional compiler on several counts. An adaptive compiler consists of some 
 form an 
 adaptation unit
 , which controls the entire optimization process. The adaption unit uses a 
 combination of
  
 Program Profile
  
 Source 
  
 Code
  
 Compiler 
 Front-end
  
 IR
  
 Adaptation 
  
 Unit
  
 Binary
  
 Hardware
  
 Architecture 
  
 Description
  
 Compilation Strategies
  
 FIGURE 8.5
  
 The architecture of an adaptive optimizing compiler.",NA
8.3,NA,NA
Characterizing Compiler Interactions,"Wehavealreadyseenthatcompilerinteractionsplayacrucialroleindeterminingtheeventualperforma
 nce of a compiler. Understanding these interactions is critical to problems such as phase ordering 
 and the design of interaction-sensitive compiler optimizations and heuristics. Traditionally, the 
 task identifying and characterizing key interactions has been performed manually by compiler 
 writers and researchers. Given a set of compiler optimizations, compiler writers are expected to 
 analyze the optimizations and determine if (a) two or more optimizations interact and (b) any of 
 the optimizations interact with the architecture. However, the manual nature of this task leaves 
 open 
 the 
 possibility 
 of 
 oversights. 
 It 
 is 
 possible 
 thatthecompilerwritermayhavemisreadorevenoverlookedsomeimportantinteractions.Wehavealr
 eady seen an example of an unexpected and complex interaction register allocation, instruction 
 scheduling, and OoO processors that are usually overlooked by compiler writers [20]. The 
 following example further illustrates this possibility.
  
 Function inlining is a transformation that replaces the call-site with the body of the callee. The 
 trans-formation also replaces the formal arguments in the callee with the actual arguments. This 
 optimization reduces the overheads of the call by eliminating the need for setting up a stack 
 frame and allows the callee to be optimized in the context of the caller. However, inlining leads to 
 an increase in code size and a possible increase in register pressure. It is therefore natural to 
 expect inlining to interact positively with classical optimizations such as common subexpression 
 elimination, PRE, and so on and interact negatively with register allocation and the instruction 
 cache. Hence, virtually all optimizing compilers use heuristics to limit the amount of code growth 
 due to inlining, and some compilers also consider the impact on register pressure before making 
 inlining decisions. However, it was recently shown [4] that on OoO superscalar processors, 
 inlining interacts negatively with the size of the reorder buffer. In other words, the efficacy of 
 inlining is reduced on OoO superscalar processors with large reorder buffers. If this interaction is 
 not considered by the inlining heuristic, excessive inlining may result, which may even hurt 
 performance.",NA
8.4,NA,NA
Heuristic Selection,"It is common for a compiler optimization to find itself in situations where it must choose from 
 one of the several ways it can affect a code transformation (which may include the option of not 
 affecting 
 the 
 transformation).Ideally,wewouldlikeamodelofthesystemthataccuratelycomputesthecostsandben
 efits of applying each transformation. The compiler would then be able to evaluate each of the 
 options and pick the optimal transformation to apply. However, this approach is generally not 
 feasible for two reasons. (a) The number of possible transformations may be too large to 
 enumerate. (b) Building models that accurately compute the costs and benefits of 
 transformations on modern platforms is difficult. For these reasons, compiler optimizations are 
 forced to rely on heuristics to guide decision making. Optimizations may use heuristics to narrow 
 the space of transformations to evaluate and estimate the costs and benefits of each 
 transformation. In essence, heuristics make compiler optimizations tractable at the cost of 
 optimal decision making.
  
 This leads us to the following important question: How do we design good heuristics? This 
 question has plagued compiler writers for several years. The difficulty in designing good 
 heuristics is best illustrated by an example. Consider function inlining and the problem of 
 determining whether to inline a given call-site.
  
 The costs and benefits of inlining depend on a host of factors including: 
  
 r
  Static parameters such as the function sizes (both caller and callee), number and type of 
 arguments, 
 r
  The target platform configuration, which includes parameters such as issue 
 width, cache sizes, reorder buffer size, number of functional units, branch predictor 
 configuration, etc.
  
 r
  Other optimizations in the compilation sequence that may interact with inlining 
  
 Our goal is to derive an inlining heuristic that considers all these factors and determines whether 
 the call-site in question should be inlined. We require that the heuristic make the right decisions 
 across different programs and different target platforms. Designing such a heuristic is a challenge. 
 Not surprisingly, the 
 approachtakenbycompilerwritersisoneof
 trialanderror
 .Compilerwritersareusuallyadeptatidentifyi
 ng program features that should be considered in the heuristic; deriving the heuristic itself is, 
 however, based on experience and repeated experiments.",NA
8.5,NA,NA
Phase Ordering,"In discussions leading up this section, we observed that the nature of interactions between 
 optimizations 
 dependsontheprogrambeingcompiled
  
 and
  
 theorderofapplyingtheoptimizations.Wealsoobservedthat applying optimizations in the
  right
  
 order is likely to yield significant benefits. These observations naturally lead us to the problem of 
 finding the sequence of applying optimizations that results in “optimal” code.
  
 This problem, also known as the
  phase ordering
  problem, has intrigued researchers for several years.
  
 The complex nature of the phase ordering problem has been recognized for several years. 
 Although few in number, attempts have been made to analytically infer phase orderings for 
 specific classes of optimizations. For instance, Whitfield and Soffa [7, 8] proposed a framework 
 for specifying optimizations. The formal specifications are used to automatically identify 
 potential interactions, which in turn could be used to select local phase orderings. Long and 
 Fursin [42] consider the special class of iteration reordering loop transformations and show that 
 the phase ordering problem for this class of optimizations can be transformed to one of searching 
 over a polyhedral space. These methods are unfortunately restricted to a select set of 
 optimizations, restricting their use in practice.
  
 Because of these limitations of analytical methods, researchers have tended to favor empirical 
 techniques such as
  iterative compilation
  [43] for finding effective solutions to the phase ordering 
 problem. Unlike ana-lytical methods, empirical techniques [2, 44–48] evaluate different phase 
 orderings by executing programs on the target platform. Since each phase ordering is evaluated 
 based on its execution time on the target platform, all intricacies of the hardware are 
 automatically accounted for, eliminating the need for complex",NA
8.6,NA,NA
Optimization Flags and Parameter Selection,"The selection of optimization flags is probably the only form of (manual) adaptation built into a 
 conven-
 tionalcompiler.Mostcompilersallowuserstochoosetheirownoptimizationsettingsbyprovidingflags
 that can be used to enable or disable optimizations individually. However, developers and 
 compilation tools rarely exercise this option and rely on the default settings instead (typically the 
 -O
 x
  settings, where
  x
  is the level of optimization). Recent research has shown that the use of 
 default 
 settings 
 often 
 leads 
 to 
 suboptimal 
 performance[3, 
 4].Thisisbecausecompilerwritersusuallychoosethedefaultsettingsbasedontheaverage 
 performance measured over a number of benchmark programs on a limited number of platforms. 
 There-fore, these settings are not guaranteed to deliver the best performance for a given 
 program 
 on 
 a 
 given 
 target 
 platform.Forinstance,itisnotuncommonforsomeofthe
 aggressive
 optimizationslikeloopunrollingto
 be turned off by default [57] because the optimization was found to hurt average performance, 
 probably due to negative interactions and/or ineffective heuristics. These optimizations are not 
 used (unless explicitly turned on by the user), even when the program being compiled may have 
 benefited from the optimizations.
  
 Ideally, we would like the compiler to automatically select the best set of optimization flags for 
 each program, probably by performing a cost-benefit analysis based on an accurate model of all 
 optimizations and the target platform. Such analysis is beyond the capabilities of existing 
 analytical modeling techniques. As an alternate approach, researchers have proposed a number 
 of automatic techniques based on statis-tics and machine learning to address this problem [3, 4, 
 58, 59]. Let us consider the technique based on inferential statistics [3] in some detail.
  
 8.6.1 Optimization Flag Selection Using Inferential Statistics
  
 In this approach, compiler optimizations are considered as
  factors
  that influence the performance 
 of the given program. We design a set of
  experiments
  to test whether each of the factors has a
  
 significant positive 
 effect on performance. Here, each experiment involves measuring the 
 execution time of the program compiled using one of the 2
 k
 possible settings of the optimization 
 flags. For the moment, assume that we are interested in determining whether one of the 
 optimizations
  A
  is effective for the program. We can design a set of experiments
  G
 , which can be 
 split into two equal groups
  G
 0
  and
  G
 1
  with
  N
  experiments each. The group
  G
 0
  consists of 
 experiments in which the optimization
  A
  is turned off, and
  G
  1
  consists of experiments where
  A
  is 
 turned on. All other optimization flags are randomly selected. A statistical test known as the 
 Mann–Whitney test [60] can be used to analyze the data collected by measuring the execution 
 times of the given program for each of these experiments and determine whether the 
 optimization
  A
  is effective. Informally, the test is based on the observation that if optimization
  A
  
 is effective, the performance of experiments in group
  G
 1
  should be better than the performance of 
 experiments in group
  G
  0
 . Note that simply comparing the average execution times of 
 experiments in
  G
  0
  with the average of experiments in 
 G
 1
  may lead to false conclusions because 
 this comparison does not take the variance into account.
  
 The Mann–Whitney test works as follows. The test
  ranks
  each of the experiments in
  G
  based on 
 the measured execution time. The experiment with the smallest execution time is assigned rank 
 1, and the experiment with the highest time is assigned rank 2
 N
 . Then the test computes statistics
  
 S
 0
  and
  S
 1
 , where
  S
 0 
 and
  S
 1
  are the sums of ranks of all experiments in groups
  G
 0
  and
  G
 1
 , 
 respectively. Consider the statistic
  S
 1
 . This statistic can have a minimum value (1+2+· · ·+
  N
 ) and 
 a maximum value ([
 N
  +1]+· · ·+2
 N
 ). The first value occurs when all the experiments in group
  G
  1
  
 have execution times smaller than the experiments in group
  G
 0
 , and the second values occurs in 
 the opposite case. It can be shown that if the optimization 
 A
  is not effective for the program, the 
 statistic
  S
 1
  is normally distributed with mean
  
 μ
  =
 N
 (
 N
  + 1) 
  
 (8.12)
  
 and standard deviation",NA
8.7,NA,NA
Conclusions,"Optimizing compilers were originally conceived as tools that would free programmers from the 
 burden of manually optimizing and tuning programs for performance, allowing them to focus on 
 writing correct and modular code instead. It is therefore ironic that compiler writers find 
 themselves in a similar situation today. Writing high-performance optimizing compilers is 
 becoming an increasingly difficult and tedious task because of the sheer number and nature of 
 compiler optimizations, complex interactions, multiple (often conflicting) objective functions, 
 and the complexity of modern hardware. Therefore, tools and techniques that automatically 
 address these concerns and allow compiler writers to focus on the task of writing correct and 
 retargetable optimizations are desirable.
  
 In this chapter, we observe that the key to high performance lies in flexible and adaptable 
 compilation strategies, a feature missing in conventional compilers. Through a series of 
 examples, we show that effective compilation strategies can be automatically learned using 
 statistical and machine learning techniques. In this model of compilation (illustrated in Figure 
 8.13), the compiler
  learns
  a mapping between different
  
 Program Profile
  
 Source 
  
 Code
  
 Compiler 
 Front-end
  
 IR
  
 Adaptation 
  
 Unit
  
 Binary
  
 Hardware
  
 Compilation Strategies
  
 FIGURE 8.13 
 An adaptive compilation model that uses feedback from the target platform to search and 
 evaluate compilation strategies.",NA
Appendix A: Optimality in Program Compilation,"In the absolute sense, a program
  P
 abs
  is said to be
  optimal
  if there exists no semantically 
 equivalent program 
 P
 ′
 that executes faster than
  P
 abs
  for any input of the program. Therefore, given 
 an arbitrary program, we would ideally like an optimizing compiler to generate the 
 corresponding optimal program. It turns out that this notion of optimality is only of theoretical 
 interest and cannot be realized in practice. In fact, there are simple programs for which an 
 equivalent optimal program does not exist [63]. For instance, consider programs that contain 
 loops with branches conditioned on input variables. The execution paths of these programs 
 clearly depend on the input data. For such programs, one requires the notion of an unbounded 
 speculative window to express or execute an equivalent optimal program.
  
 Sincethisnotionofoptimalityisnotpractical,itisrelaxedbyrestrictingourreasoningtotheperforma
 nce of the program on a given input [49]. Formally, a program
  P
 I
  can be considered optimal if 
 there exists no semantically equivalent program
  P
 ′
 that executes faster for a given input
  I
 . Note 
 that although optimality is defined in terms of the program’s performance over a specific input, 
 the optimal program is still required to execute correctly for all other inputs. It turns out that 
 even this notion of optimality is hard to achieve because it requires reasoning over a large 
 (potentially infinite) number of possible compiler optimizations (those that exist or may exist in 
 the future). Therefore, it makes practical sense to restrict the notion of optimality to a given set of 
 compiler optimizations. Thus, from the perspective of program compilation, a program
  P
 MI
  is 
 considered optimal [49] if there exists no semantically equivalent program that executes faster 
 than
  P
 MI
  for a given input data set
  I
 , and
  P
 MI
  is obtained using compiler optimizations from some 
 finite set
  M
 .",NA
9 ,NA,NA
Type Systems: ,NA,NA
Advances and ,NA,NA
Applications,"9.1
  
 Introduction
  ..........................................
 9
 -
 1
  
  
 Jens Palsberg 
  
 and 
  
 Todd Millstein 
  
 UCLA Computer Science Department, 
 University of California, Los Angeles, CA 
 palsberg@cs.ucla.edu 
  
 todd@cs.ucla.edu",NA
9.1 ,NA,NA
Introduction,"9.2
  
 Types for Confinement
  ................................
 9
 -
 2
  
 Backgroun
 d
  
 •
  Static Analysis
  
 •
  Confined Types
  
 •
  Related
  
 Work on Alias Control
  
 9.3
  
 Type Qualifiers
 ........................................
 9
 -
 12 
 Background
 •
  Static 
 Analysis
 •
  A Type System for Qualifiers
  
 •
  Related Work on Type Refinements
  
 References
  ..................................................
 9
 -
 19
  
 This chapter is about the convergence of type systems and static analysis. Historically, these two 
 approaches to reasoning about programs have had different purposes. Type systems are 
 developed to catch common kinds of programming errors early in the software development 
 cycle. In contrast, static analyses were developed to automatically optimize the code generated 
 by a compiler. The two fields also have different theoretical foundations: type systems are 
 typically formalized as logical inference systems [61], while static analyses are typically 
 formalized as abstract program executions [20, 46].
  
 Recently,however,therehasbeenaconvergenceoftheobjectivesandtechniquesunderlyingtypesys
 tems and static analysis [42, 55, 57, 58]. On the one hand, static analysis is increasingly being 
 used for program understanding and error detection, rather than purely for code optimization. 
 For example, the LCLint tool [30] uses static analysis to detect null-pointer dereferences and 
 other common errors in C programs, and it relies on type-system-like program annotations for 
 efficiency and precision. As another example, the Error Detection via Scalable Program Analysis 
 (ESP) tool [21] uses static analysis to detect violations of Application Programming Interface 
 (API) usage protocols, for example, that a file can only be read or written after it has been 
 opened.
  
 On the other hand, type systems have become a mature and widely accepted technology. 
 Programmers write most new software in languages such as C [45], C++ [29], Java [39], and C# 
 [49], which all feature varying degrees of static type checking. For example, the Java type system 
 guarantees that if a program calls a method on some object, at runtime the object will actually 
 have a method of that name, expecting the proper number and kind of arguments. Types are also 
 used in the intermediate languages of compilers and even in assembly languages [51], such as the 
 typed assembly language for x86 called TALx86 [50].
  
 With this success, researchers have been motivated to explore the potential to extend 
 traditional type systems to detect a variety of interesting classes of program errors. This 
 exploration has shown type systems to be a robust approach to static reasoning about programs 
 and 
 their 
 properties. 
 For 
 example, 
 type 
 systems 
 havebeenusedrecentlytoensurethesafetyofmanualmemorymanagement(e.g.,[40, 
 53, 
 65]),totrackand
  
 9
 -
 1",NA
9.2,NA,NA
Types for Confinement,"Inthissectionwewillusetypestoensurethatanobjectcannotescapethescopeofitsclass.Ourpresentatio
 n is based on results from three papers on
  confined types
  [8, 41, 72].
  
 9.2.1 Background
  
 Object-oriented languages such as Java provide a way of protecting the name of a field but not the 
 contents of a field. Consider the following example.
  
 package p;
  
 public class Table
  { 
  
  
 private Bucket[] buckets;
  
 }
  
 public Object[] get(Object key)
  {
  return buckets;
  }
  
 class Bucket
  { 
  
  
 Bucket next; 
  
  
 Object key, 
 val; 
  
 }",NA
9.3,NA,NA
Type Qualifiers,"In this section we will use types to allow programmers to easily specify and check desired 
 properties of their applications. This is achieved by allowing programmers to introduce new
  
 qualifiers
  that refine existing types. For example, the type
  nonzero int
  is a refinement of the 
 type
  int
  that intuitively denotes the subset of integers other than zero.
  
 9.3.1 Background
  
 Static type systems are useful for catching common programming errors early in the software 
 development cycle. For example, type systems can ensure that an integer is never accidentally 
 used as a string and that a function is always passed the right number and kinds of arguments. 
 Unfortunately, language designers cannot anticipate all of the program errors that programmers 
 will want to statically detect, nor can they anticipate all of the practical ways in which such errors 
 can be detected.
  
 As a simple example, while most type systems in mainstream programming languages can 
 distinguish integers from strings and ensure that each kind of data is used in appropriate ways, 
 these type systems typically cannot distinguish positive from negative integers. Such an ability 
 would enable stronger assur-ances about a program, for example, that it never attempts to take 
 the square root of a negative number. As another example, most type systems cannot distinguish 
 between data that originated from one source and data that originated from a different source 
 within the program. Such an ability could be useful to track a form of
  value flow
 , for example, to 
 ensure that a string that was originally input from the user is treated as
  tainted
  and therefore 
 given restricted capabilities (e.g., such a string should be disallowed as the format-string 
 argument to C’s
  printf
  function, since a bad format string can cause program crashes and 
 worse).
  
 Without static checking for these and other kinds of errors, programmers have little recourse. 
 They can use
  assert
  statements, which catch errors, but only as they occur in a running 
 system. They can specify desired program properties in comments, which are useful 
 documentation but need have no relation to the actual program behavior. In the worst case, 
 programmers simply leave the desired program properties completely implicit, making these 
 properties easy to misunderstand or forget entirely.
  
 9.3.2 Static Analysis
  
 Static analysis could be used to ensure desired program properties and thereby guarantee the 
 absence of classes of program errors. Indeed, generic techniques exist for performing static 
 analyses of programs (e.g., [20, 46]), which could be applied to the properties of interest to 
 programmers. As with confinement, one standard approach is to compute a flow set for each 
 expression
  e
  in the program, which conservatively overapproximates the possible values of
  e
 . 
 However, instead of using class names as the elements of a flow set, each static analysis defines 
 its own domain of
  flow facts
 .
  
 For example, to track positive and negative integers, a static analysis could use a domain of 
 signs [20], consisting of the three elements +,
  0
 , and − with the obvious interpretations. If the 
 flow set computed for an expression
  e
  contains only the element +, we can be sure that
  e
  will 
 evaluate to a positive integer. In our format-string example, a static analysis could use a domain 
 consisting of the elements
  tainted 
 and
  untainted
 , representing, respectively, data that do 
 and do not come from the user. If the flow set computed for an expression
  e
  contains only the 
 element
  untainted
 , we can be sure that
  e
  does not come from the user.
  
 While this approach is general, it suffers from the drawbacks discussed in Section 9.2.2. First, 
 whole-program analysis is typically required for precision, so errors are only caught once the 
 entire program has been implemented. Second, the static analysis is
  descriptive
 , reporting the 
 properties that are true of",NA
10 ,NA,NA
Dynamic Compilation,"10.
 1 
  
 10.
 2 
  
 10.
 3
  
 10.4
  
 Introduction
  .........................................
 10
 -
 1 
 Approaches to 
 Dynamic Compilation
  .................
 10
 -
 3 
 Transparent Binary 
 Dynamic Optimization
  ...........
 10
 -
 4 
 Fragment Selection
 •
  
 Fragment Optimization
 •
  Fragment 
  
 Linking
 •
  Code Cache Management
  
 •
  Handling Exceptions
 •
  Challenges 
  
 Dynamic Binary Translation
  .........................
 10
 -
 14
  
  
 Evelyn Duesterwald 
  
 IBM T.J. Watson Research Center, 
 Yorktown Heights, NY 
  
 duester@us.ibm.com
  
 10.5 
  
 Just-in-Time Compilation
  ............................
 10
 -
 15
  
 10.6 
  
 Nontransparent Approach: Runtime Specialization
  ....
 10
 -
 17
  
 10.7 
  
 Summary
  ............................................
 10
 -
 19
  
 References
  ..................................................
 10
 -
 19",NA
10.1,NA,NA
Introduction,"The term
  dynami
 c
  compilation
  refers to techniques for runtime generation of executable code. 
 The idea of compiling parts or all the application code while the program is executing challenges 
 our intuition about overheads involved in such an endeavor, yet recently a number of approaches 
 have evolved that effectively manage this challenging task.
  
 The ability to dynamically adapt executing code addresses many of the existing problems with 
 tra-ditional static compilation approaches. One such problem is the difficulty for a static compiler 
 to fully exploit the performance potential of advanced architectures. In the drive for greater 
 performance, today’s microprocessors provide capabilities for the compiler to take on a greater 
 role in performance delivery, ranging from predicated and speculative execution (e.g., for the 
 Intel Itanium processor) to various power consumption control models. To exploit these 
 architectural features, the static compiler usually has to rely on profile information about the 
 dynamic execution behavior of a program. However, collecting valid execution profiles ahead of 
 time may not always be feasible or practical. Moreover, the risk of performance degradation that 
 may result from missing or outdated profile information is high.
  
 Current trends in software technology create additional obstacles to static compilation. These 
 are exem-plified by the widespread use of object-oriented programming languages and the trend 
 toward shipping software binaries as collections of dynamically linked libraries instead of 
 monolithic binaries. Unfortu-nately, the increased degree of runtime binding can seriously limit 
 the effectiveness of traditional static compiler optimization, because static compilers operate on 
 the statically bound scope of the program.
  
 Finally,theemergingInternetandmobilecommunicationsmarketplacecreatestheneedforthecom
 piler to produce portable code that can efficiently execute on a variety of machines. In an 
 environment of networked devices, where code can be downloaded and executed on the fly, 
 static compilation at the target device is usually not an option. However, if static compilers can 
 only be used to generate platform-independent intermediate code, their role as a performance 
 delivery vehicle becomes questionable.
  
 10
 -
 1",NA
10.2,NA,NA
Approaches to Dynamic Compilation,"A number of approaches to dynamic compilation have been developed. These approaches differ 
 in several aspects, including the degree of transparency, the extent and scope of dynamic 
 compilation, and the assumed encoding format of the loaded image. On the highest level, dynamic 
 compilation systems can be divided into transparent and nontransparent systems. In a 
 transparent system, the remainder of the compilation pipeline is oblivious to the fact that a 
 dynamic compilation stage has been added. The executable produced by the linker and loader is 
 not specially prepared for dynamic optimization, and it may execute with or without a dynamic 
 compilation stage. Figure 10.2 shows a classification of the various approaches to transparent 
 and nontransparent dynamic compilation.
  
 Transparent dynamic compilation systems can further be divided into systems that operate on 
 binary executable code (binary dynamic compilation) and systems that operate on an 
 intermediate platform-independent encoding (just-in-time [JIT] compilation). A binary dynamic 
 compiler starts out with a loaded fully executable binary. In one scenario, the binary dynamic 
 compiler recompiles the binary code to incorporate native-to-native optimizing transformations. 
 These recompilation systems are also referred to as
  dynami
 c
  optimizer
 s [3, 5, 7, 15, 36]. During 
 recompilation, the binary is optimized by customizing the code with respect to specific runtime 
 control and data flow values. In dynamic binary translation, the
  
 Dynamic  Compilation 
  
 Transparent 
  
 Nontransparent 
  
 “Hardwired” into Executable 
  
 Runtime Specialization
  
 Binary Code 
  
 Native-to-Native 
  
 Dynamic Optimization 
  
 Intermediate Virtual Machine Code 
 Just-in-Time Compilation
  
 Nonnative 
  
 Binary Translation 
  
 FIGURE 10.2",NA
10.3,NA,NA
Transparent Binary Dynamic Optimization,"Anumberofbinarydynamiccompilationsystemshavebeendevelopedthatoperateasanoptionaldyna
 mic stage [3, 5, 7, 15, 35]. An important characteristic of these systems is that they take full 
 control of the execution of the program. Recall that in the transparent approach, the input 
 program is not specially prepared for dynamic compilation. Therefore, if the dynamic compiler 
 does not maintain full control over the execution, the program may escape and simply continue 
 executing natively, effectively bypassing dynamic compilation altogether. The dynamic compiler 
 can afford to relinquish control only if it can guarantee that it will regain control later, for 
 example, via a timer interrupt.
  
 Binary dynamic compilation systems share the general architecture shown in Figure 10.3. 
 Input to the dynamic compiler is the loaded application image as produced by the compiler and 
 linker. Two main components of a dynamic compiler are the compiled code cache that holds the 
 dynamically compiled code fragments and the dynamic compilation engine. At any point in time, 
 execution takes place either in the dynamic compilation engine or in the compiled code cache. 
 Correspondingly, the dynamic compilation engine maintains two distinct execution contexts: the 
 context of the dynamic compilation engine itself and the context of the application code.
  
 Execution of the loaded image starts under control of the dynamic compilation engine. The 
 dynamic compiler determines the address of the next instruction to execute. It then consults a",NA
10.4,NA,NA
Dynamic Binary Translation,"The previous sections have described dynamic compilation in the context of code transformation 
 for performance optimization. Another motivation for employing a dynamic compiler is software 
 migration. In this case, the loaded image is native to a guest architecture that is different from the 
 host architecture, which runs the dynamic compiler. The binary translation model of dynamic 
 compilation is illustrated in Figure 10.7. Caching instruction set simulators [8] and dynamic 
 binary translation systems [19, 35, 39] are examples of systems that use dynamic compilation to 
 translate nonnative guest code to a native host architecture.",NA
10.5,NA,NA
Just-in-Time Compilation,"JIT compilation refers to the runtime compilation of intermediate virtual machine code. Thus, 
 unlike binary dynamic compilation, the process does not start out with already compiled 
 executable code. JIT compilation was introduced for Smalltalk-80 [18] but has recently been 
 widely popularized with the introduction of the Java programming language and its intermediate 
 bytecode format [22].
  
 The virtual machine environment for a loaded intermediate program is illustrated in Figure 
 10.9. As in binary dynamic compilation, the virtual machine includes a compilation module and a 
 compiled code cache. Another core component of the virtual machine is the runtime system that 
 provides various system services that are needed for the execution of the code. The loaded 
 intermediate code image is inherently",NA
10.6,NA,NA
Nontransparent Approach: Runtime Specialization,"A common characteristic among the dynamic compilation systems discussed so far is 
 transparency. The dynamic compiler operates in complete independence from static compilation 
 stages and does not make assumptions about, or require changes to, the static compiler.
  
 A different, nontransparent approach to dynamic compilation has been followed by staged 
 runtime 
 specializationtechniques[9,31,33].Theobjectiveofthesetechniquesistopreparefordynamiccompila
 tion as much as possible at static compilation time. One type of optimization that has been 
 supported in this fashion is value-specific code specialization. Code specialization is an 
 optimization that produces an optimized version by customizing the code to specific values of 
 selected specialization variables.
  
 Consider the code example shown in Figure 10.10. Figure 10.5i shows a dot product function 
 that is called from within a loop in the main program, such that two parameters are fixed (
 n
  = 3 
 and
  row
  = [5, 0, 3]) and only a third parameter (
 col
 ) may still vary. A more efficient 
 implementation can be achieved by specializing the dot function for the two fixed parameters. 
 The resulting function
  spe
 c
  doc
 , which retains only the one varying parameter, is shown in Figure 
 10.10ii.
  
 In principle, functions that are specialized at runtime, such as
  spe
 c
  dot
 , could be produced in a 
 JIT compiler. However, code specialization requires extensive analysis and is too costly to be 
 performed fully at runtime. If the functions and the parameters for specialization are fixed at 
 compile time, the static compiler can prepare the runtime specialization and perform all the 
 required code analyses. Based on the analysis results, the compiler constructs code templates for 
 the specialized procedure. The code templates for
  spec dot
  are shown in Figure 10.11ii in C 
 notation. The templates may be parameterized with respect to missing runtime values. 
 Parameterized templates contain holes that are filled in at runtime with the respective values. 
 For example, template T2 in Figure 10.11ii contains two holes for the runtime parameters 
 row
 [0]. . . row[2] (hole h1) and the values 0,
  . . .
  , (
 n
 −1)(hole h2).
  
 By moving most of the work to static compile time, the runtime overhead is reduced to 
 initialization and linking of the prepared code templates. In the example from Figure 10.10, the 
 program is statically compiled so that in place of the call to routine
  dot
 , a call to a specialized 
 dynamic code generation agent is inserted. The specialized code generation agent for the",NA
10.7,NA,NA
Summary,"Dynamic compilation is a growing research field fueled by the desire to go beyond the traditional 
 com-pilation model that views a compiled binary as a static immutable object. The ability to 
 manipulate and transform code at runtime provides the necessary instruments to implement 
 novel execution services. This chapter discussed the mechanisms of dynamic compilation 
 systems in the context of two applica-tions: dynamic performance optimization and transparent 
 software migration. However, the capabilities of dynamic compilation systems can go further and 
 enable such services as dynamic decompression and decryption or the implementation of 
 security policies and safety checks.
  
 Dynamic compilation should not be viewed as a technique that competes with static 
 compilation. Dynamic compilation complements static compilation, and together they make it 
 possible to move toward a truly write-once-run-anywhere paradigm of software implementation.
  
 Althoughdynamiccompilationresearchhasadvancedsubstantiallyinrecentyears,numerouschalle
 nges remain. Little progress has been made in providing effective development and debugging 
 support for dynamic compilation systems. Developing and debugging a dynamic compilation 
 system is particularly difficult because the source of program bugs may be inside transient 
 dynamically generated code. Break points cannot be placed in code that has not yet materialized, 
 and 
 symbolic 
 debugging 
 of 
 dynamically 
 generatedcodeisnotanoption.Thelackofeffectivedebuggingsupportisoneofthereasonstheengineeri
 ng of dynamic compilation systems is such a difficult task. Another area that needs further 
 attention is code validation. Techniques are needed to assess the correctness of dynamically 
 generated code. Unless dynamic compilation systems can guarantee high levels of robustness, 
 they are not likely to achieve widespread adoption.
  
 This chapter surveys and discusses the major approaches to dynamic compilation with a focus 
 on transparent binary dynamic compilation. For more information on the dynamic compilation 
 systems that have been discussed, we encourage the reader to explore the sources cited in the 
 References section.
  
 References
  
 1. L. Anderson, M. Berc, J. Dean, M. Ghemawat, S. Henzinger, S. Leung, L. Sites, M. 
 Vandervoorde, C. Waldspurger, and W. Weihl. 1977. Continuous profiling: Where have all 
 the cycles gone? In 
 Proceedings of the 16th ACM Symposium of Operating Systems Principles
 , 
 14.
  
 2. D. Bruening and E. Duesterwald. 2000. Exploring optimal compilation unit shapes for an 
 embed-ded just-in-time compiler. In
  Proceedings of the 3rd Workshop on Feedback-Directed 
 and Dynamic Optimization
 .
  
 3. D. Bruening, E. Duesterwald, and S. Amarasinghe. 2001. Design and implementation of a 
 dynamic optimization framework for Windows. In
  Proceedings of the 4th Workshop on 
 Feedback-Directed and Dynamic Optimization
 .",NA
11 ,NA,NA
The Static Single ,NA,NA
Assignment Form: ,NA,NA
Construction and ,NA,NA
Application to ,NA,NA
Program Optimization,"11.1 
  
 11.2
  
 Introduction
  .........................................
 11
 -
 1 
 The Static Single 
 Assignment Form
  ...................
 11
 -
 2
  
 Construction of the SSA 
 Form
  
 •
  A Linear Time Algorithm
  
 for Placing
  φ
 -Nodes
  
 11.3
  
 Variants of the SSA Form
  .............................
 11
 -
 17
  
 Pruned 
 SSA
  
 •
  Semi-Pruned SSA
  
  
 J. Prakash Prabhu
 1
 , 
  
 Priti Shankar, 
  
 and 
  
 Y. N. Srikant 
  
 Department of Computer Science and 
 Automation, 
  
 Indian Institute of Science, 
  
 Bangalore, India 
  
 prakash.prabhu@gmail.com 
  
 priti@csa.iisc.ernet.in 
  
 srikant@csa.iisc.ernet.in
  
 11.4
  
 11.5
  
 Conditional Constant Propagation
 ....................
 11
 -
 20 
 The 
 Lattice of Constants
 •
  The CCP Algorithm: Flow Graph 
  
 Version
 •
  The CCP Algorithm: SSA Version 
  
 Value Numbering
  ....................................
 11
 -
 25
  
 Hashing-Based Value Numbering: Basic Block 
 Version
  
 •
  Value Numbering with Hashing and SSA Forms
  
 11.6 
  
 Partial Redundancy Elimination
  .....................
 11
 -
 36 
  
 Partial 
 Redundancy Elimination on the SSA Form
  
  
 •
  The SSAPRE Algorithm
  
 11.7 
 11.8
  
 Translation from SSA Form to Executable Form
  .......
 11
 -
 47
  
 The Lost Copy 
 Problem
  
 •
  The Swap Problem
  
 Discussion and Conclusions
  ..........................
 11
 -
 50
  
 References
  ..................................................
 11
 -
 51",NA
11.1,NA,NA
Introduction,"The emergence of the
  static single assignment
  (SSA) form as an important intermediate 
 representation for compilers has resulted in considerable research in algorithms to compute this 
 form efficiently. The SSA representation is an example of a
  sparse
  representation where 
 definition sites are directly associated with use sites. Analysis of sparse representations has the 
 advantage of being able to directly access points where relevant data flow information is 
 available. Therefore, one can profitably use this property in im-proving algorithms for 
 optimizations carried out on older, more traditional intermediate representations.",NA
11.2,NA,NA
The Static Single Assignment Form,"A program is in SSA form if each of its variables has exactly one definition, which implies that 
 each use of a variable is reached by exactly one definition. The control flow remains the same as 
 in a traditional (non-SSA) program. A special merge operator, denoted
  φ
 , is used for the selection 
 of values in join nodes. The SSA form is usually augmented with use–definition and/or 
 definition–use chains in its data structure representation to facilitate design of faster algorithms.
  
 Figures 11.1 and 11.4 show two non-SSA form programs, Figures 11.2 and 11.5 show their SSA 
 forms, and Figures 11.3 and 11.6 show the flowcharts of the SSA form.
  
 The program in Figure 11.1 is not in SSA form because there are several assignments to the 
 variable 
 max
 . In the program in Figure 11.2 (with the accompanying flowchart in Figure 11.6), 
 each assignment is made to a different variable,
  max
 i
 . The variable
  max
 5
  is assigned the correct 
 value by the
  φ
 -function, which takes the value
  max
 i
 , if the control reaches it via the
  i
 th incoming 
 branch from left to right.
  
 The
  φ
 -functions in the two blocks
  B
 1 and
  B
 5 in Figure 11.6 are meant to choose the 
 appropriate value based on the control flow. For example, the
  φ
 -assignment to
  RSR
 5
  in the block
  
 B
 5 in Figure 11.6 selects one of
  RSR
 3
 ,
  RSR
 2
 , or
  RSR
 4
  based on the execution following the arc
  B
 2 →
  
 B
 5,
  B
 3 →
  B
 5, or
  B
 4 →
  B
 5, respectively.
  
 Read
  A
 ,
  B
 ,
  C 
  
 if (
 A > B
 ) 
  
 if (
 A > C
 )
  max
 1
  =
  A 
  
 else
  max
 2
  =
  C 
  
 else if (
 B > C
 )
  max
 3
  =
  B
  
 max
 5
  =
  φ
 (
 max
 1
 ,
  max
 2
 ,
  max
 3
 ,
  max
 4
 ) 
 else
  max
 4
  =
  C
  
 Print
  max
 5
  
 FIGURE 11.2
  
 Program of Figure 11.1 in SSA form.",NA
11.4,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Conditional Constant Propagation,"Constant propagation is a well-known compiler optimization during which variables that can be 
 deter-mined to contain only constant values at runtime in all possible executions of the program 
 are discovered. These values are propagated throughout the program, and expressions whose 
 operands can be determined to be only constants are also discovered and evaluated. In effect, 
 there is a symbolic execution of the program with the limited purpose of discovering constant 
 values. The following examples help explain the intricacies of constant propagation.
  
 The simplest case is that of straight line code without any branches, as in a basic block. This re-
 quires only one pass through the code, with forward substitutions and no iteration. However, 
 such a one-pass strategy cannot discover constants in conditionals and loops. For such cases we 
 need to carry out data flow analysis involving work-lists and iterations. A simple algorithm of this 
 kind adds succes-sor nodes to the work-list as symbolic execution proceeds. Nodes are removed 
 one at a time from the work-list and executed. If the new value at a node is different from the old 
 value, all the successors of the node are added to the work-list. The algorithm stops when the 
 work-list becomes empty. Such an algorithm can catch constants in programs such as program A 
 but not in programs such as program B in Figure 11.24.
  
 Conditional constant propagation handles programs such as program B in Figure 11.24 by 
 evaluating all conditional branches with only constant operands. This uses a work-list of edges 
 (instead of nodes) from the flow graph. Furthermore, neither successor edge of a branch node is 
 added to the work-list when the branch condition evaluates to a constant value (true or false); 
 only the relevant successor (true or false) is added.
  
 Conditional constant propagation algorithms on SSA graphs are faster and more efficient than 
 the ones on flow graphs. They find at least as many constants as the algorithms on flow graphs 
 (even with 
 d
 -
 u
  chains, this efficiency cannot be achieved with flow graphs; see [55]). In the 
 following sections, we illustrate the conditional constant propagation algorithms on flow graphs 
 by means of an example and follow it up with a detailed explanation of the algorithm on SSA 
 graphs. Our description is based on the algorithms presented in [55]. We assume that each node 
 contains exactly one instruction and that expressions at nodes can contain at most one operator 
 and two operands (except for
  φ
 -functions). The graphs are supposed to contain
  N
  nodes,
  E
  f
  flow 
 edges,
  E
  s
  SSA edges (corresponding to
  d-u
  information), and
  V
  variables.
  
  
 FIGURE 11.24
  
 Limitations of various CP algorithms.",NA
11.5,NA,NA
Value Numbering,"Value numbering
  is one of the oldest and still a very effective technique that is used for 
 performing several optimizations in compilers [3, 4, 10, 15, 16, 46, 49]. The central idea of this 
 method is to assign numbers (called
  value numbers
 ) to expressions in such a way that two 
 expressions receive the same number if",NA
11.6,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Partial Redundancy Elimination,"Partial redundancy elimination (PRE) is a powerful compiler optimization that subsumes global 
 common subexpression elimination and loop-invariant code motion and can be modified to 
 perform additional code improvements such as strength reduction as well. PRE was originally 
 proposed by Morel and Renvoise [43]. They showed that elimination of redundant computations 
 and the moving of invariant computations out of loops can be combined by solving a more 
 general problem, that is, the elimination of computations performed twice on a given execution 
 path. Such computations were termed partially redundant. PRE per-forms insertions and 
 deletions of computations on a flow graph in such a way that after the transformation, each path, 
 in general, contains fewer occurrences of such computations than before. Most compilers today 
 perform PRE. It is regarded as one of the most important optimizations, and it has generated 
 substantial interest in the research community [13, 20–24, 33, 34, 36–38, 45, 46, 48].
  
 In spite of its benefits, Morel and Renvoise’s algorithm has some shortcomings. It is not 
 optimal in the sense that it does not eliminate all partial redundancies that exist in a program, 
 and it performs redundant code motion. It involves performing bidirectional data flow analysis, 
 which, some claim, is in general more complex than unidirectional analysis [38]. Knoop et al. 
 decomposed the bidirectional structure of the PRE algorithm into a sequence of unidirectional 
 analyses and proposed an optimal solution to the problem with no redundant code motion [37, 
 38].
  
 In this section, we informally describe a simple algorithm for partial redundancy elimination 
 for a program not in the SSA form [48]. It is based on well-known concepts, namely, availability, 
 anticipa-bility, partial availability, and partial anticipability. The algorithm is computationally 
 and lifetime op-timal. Its essential feature is the integration of the notion of safety into the 
 definition of partial avail-ability and partial anticipability. It requires four unidirectional bit 
 vector analyses. A special feature of this algorithm is that it does not require edge-splitting 
 transformation to be done before application of the algorithm.
  
 An informal description of the idea behind the algorithm follows. We say an expression is
  
 available
  at a point if it has been computed along all paths reaching this point with no changes to 
 its operands since the computation. An expression is said to be
  anticipable
  at a point if every path 
 from this point has a computation of that expression with no changes to its operands in between.
  
 Partial availability
  and
  partial anticipability
  are weaker properties with the requirement of a 
 computation along “at least one path” as against “all paths” in the case of availability and 
 anticipability.
  
 We say a point is
  safe
  for an expression if it is either available or anticipable at that point.
  Safe 
 partial availability
  (
 anticipability
 ) at a point differs from partial availability (anticipability) in that 
 it requires all points on the path along which the computation is partially available (anticipable) 
 to be safe. In the example given in Figure 11.48, partial availability at the entry of node 4 is true, 
 but safe partial availability at that point is false, because the entry and exit points of node 3 are 
 not safe. In Figure 11.49, safe partial availability at the entry of node 4 is true. We say a 
 computation is
  safe partially redundant
  in a node if it is locally anticipable and is safe partially 
 available at the entry of the node. In Figure 11.49, the computation in node 4 is safe partially 
 redundant.
  
 The basis of the algorithm is to identify safe partially redundant computations and make them 
 totally redundant by the insertion of new computations at proper points. The totally redundant 
 computations after the insertions are then replaced. If
  a
  +
  b
  is the expression of interest, then by 
 insertion we mean insertion of the computation
  h
  =
  a
  +
  b
 , where
  h
  is a new variable; replacement 
 means substitution of a computation, such as
  x
  =
  a
  +
  b
 , by
  x
  =
  h
 .
  
 Given a control flow graph, we compute availability, anticipability, safety, safe partial 
 availability, and safe partial anticipability at the entry and exit points of all the nodes in the 
 graph. We then mark all points that satisfy both safe partial availability and safe partial 
 anticipability. Now we note that the points of insertion for the transformation are the entry 
 points of all nodes containing the computation whose exit point is marked but whose entry point 
 is not, as well as all edges whose heads are marked but whose tails are not. We also note that 
 replacement points are the nodes containing the computation whose entry or exit point is 
 marked.",NA
11.7,NA,NA
Translation from SSA Form to Executable Form,"Once optimizations have been performed on the SSA form, the SSA form of code has to be 
 converted back to the executable form, replacing the hypothetical
  φ
 -functions by commonly 
 implemented instructions. One way of removing the
  φ
 -functions while still preserving program 
 semantics is to insert a
  copy
  state-ment at the end of each predecessor block of the
  φ
 -node, 
 corresponding to each
  φ
 -node’s argument. Figure 11.59b shows the result of copy insertion 
 applied to the SSA program of Figure 11.59a.
  
 An optimization that is frequently used to reduce the number of SSA variable instances is copy 
 folding. It is usually coupled with the SSA renaming step. At a copy or assign statement
  y
  ←
  x
 , 
 instead of pushing the new version of
  y
  (for instance,
  y
 i
 ) onto the renaming stack of
  y
 , copy 
 folding can be achieved by pushing the current version of
  x
  (for instance,
  x
  j
 ) onto the renaming 
 stack of
  y
 . This step would make sure that subsequent uses of
  y
 i
  are directly replaced by
  x
  j
  since 
 they would see
  x
  j
  on the top of
  y
 ’s stack instead of
  y
 i
 . Figure 11.60 illustrates the effect of copy 
 folding
  y
  with
  x
 .
  
 Copy folding can result in the live ranges of different instances of a variable to overlap with 
 each other. In the example shown in Figure 11.60, the live ranges of
  x
 2
  get extended because of 
 copy folding and now interfere with the live range of
  x
 3
 . Briggs et al. [9] point out two problems 
 that might arise in the back-translated code when the original algorithm of inserting copies for 
 back-translation is performed in the presence of copy folding.
  
 x
 0
  
 ...
  
 x
 2 
 ...
  
 φ
 (
 x
 0
 , x
 1
 ) 
 x
 2
  
 x
 1
  
 ...
  
 (a)
  
 x
 0 
  
 x
 2
  
 ...
  
 x
 0
  
 ...
  
 x
 2
  
 x
 1 
  
 x
 2
  
 ...
  
 x
 1",NA
11.8,NA,NA
Discussion and Conclusions,"In this chapter, we have discussed in detail several algorithms on the traditional flow graph and 
 on the static single assignment form. These algorithms demonstrate how the features of SSA 
 graphs such as
 φ
 -functions, single assignment to variables, and SSA edges (corresponding to
  d-u
  
 information) facilitate faster operation of algorithms for some global optimizations. For example, 
 conditional constant propaga-tion operates much faster on SSA graphs and discovers at least the 
 same set of constants as the algorithm on the flow graph. In the case of value numbering, the SSA 
 version is not only faster and simpler, but also global, because of the single assignment property. 
 For partial redundancy elimination the benefits stem from enabling PRE to be seamlessly 
 integrated into a global optimizer that uses SSA as its internal representation.
  
 As a consequence of its practical advantages, several well-known compilers have adopted the 
 SSA form for code optimizations. The latest version of the GNU compiler collection (GCC) uses a 
 form of SSA",NA
12 ,NA,NA
Shape Analysis,NA,NA
and Applications,NA,NA
1,"12.1
  
 Introduction
  .........................................
 12
 -
 2
  
 Structure of the Chapter
  
 12.2
  
 12.3
  
 12.4
  
 Questions about the Heap Contents
  ..................
 12
 -
 3
  
 Traditional Compiler Analyses
 •
  Analyzing Programs for
  
 Shapes
 •
  Answers as Given by Shape Analysis
  
 Shape Analysis
  .......................................
 12
 -
 9
  
 Summarization
 •
  Parametric Shape 
 Analysis
  
 Functions
 •
  Designing a Shape Abstraction
  
 •
  Abstraction
  
 An Overview of a Shape-Analysis Framework
  .........
 12
 -
 13
  
  
 Thomas Reps
 2 
  
 Computer Sciences Department, 
  
 University of Wisconsin-Madison, WI 
 reps@cs.wisc.edu
  
 Representing Stores via 2-Valued and 3-Valued Logical
  
 Structures
 •
  Extraction of Store Properties
 •
  Expressing the
  
 Semantics of Program Statements
 •
  Abstraction via
  
 Truth-Blurring Embeddings
 •
  Conservative Extraction of Store
  
 Properties
 •
  Abstract Interpretation of Program Statements
  
 Mooly Sagiv
  
 12.5
  
 Applications
  .........................................
 12
 -
 32
  
 Department of Computer Science,
  
 Identifying May- and Must-Aliases
  
 •
  Constructing Program
  
 School of Mathematics and Science,
  
 Dependence
 s
  
 •
  Other Applications
  
  
 Tel Aviv University, Tel Aviv, Israel 
 Sagiv@math.tau.ac.il
  
 Reinhard Wilhelm 
  
 Fachbereich Informatik, 
  
 Universitaet des Saarlandes, 
  
 Saarbruecken, Germany 
  
 Wilhelm@cs.uni-sb.de",NA
Abstract,"12.6
  
 12.7
  
 12.8
  
 Extensions
  ...........................................
 12
 -
 38 
 Interprocedural 
 Analysis
 •
  Computing Intersections of 
  
 Abstractions
 •
  Efficient Heap Abstractions and Representations
  
 •
  Abstracting Numeric Values
 •
  Abstraction Refinement 
  
 Related Work
  ........................................
 12
 -
 40 
 Conclusions
  
 .........................................
 12
 -
 41
  
 References
  ..................................................
 12
 -
 41
  
 A shape-analysis algorithm statically analyzes a program to determine information about the 
 heap-allocated data structures that the program manipulates. The results can be used to 
 understand programs or to verify properties of programs. Shape analysis also recovers 
 information that is valuable for debugging, compile-time garbage collection, instruction 
 scheduling, and parallelization.
  
 1
 Portions of this paper were adapted from [65] (©Springer-Verlag) and excerpted from [58] (©ACM).
  
 2
 SupportedinpartbyNSFGrantsCCR-9619219,CCR-9986308,CCF-0540955,andCCF-
 0524051;byONRGrantsN00014-01-1-0796 and N00014-01-1-0708; by the Alexander von Humboldt Foundation; 
 and by the John Simon Guggenheim Memo-rial Foundation. Address: Comp. Sci. Dept.; Univ. of Wisconsin; 1210 W. 
 Dayton St.; Madison, WI 53706.
  
 3
 Address: School of Comp. Sci.; Tel Aviv Univ.; Tel Aviv 69978; Israel.
  
 4
 Address: Fachrichtung Informatik, Univ. des Saarlandes; 66123 Saarbr¨ucken; Germany.
  
 12
 -
 1",NA
12.1,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Introduction,"Pointers and heap-allocated storage are features of all modern imperative programming 
 languages. How-ever, they are ignored in most formal treatments of the semantics of imperative 
 programming languages because their inclusion complicates the semantics of assignment 
 statements: an assignment through a pointer variable (or through a pointer-valued component of 
 a record) may have far-reaching side effects. Works that have treated the semantics of pointers 
 include [5, 42, 43, 45].
  
 These far-reaching side effects also make program dependence analysis harder, because they 
 make it difficult to compute the aliasing relationships among different pointer expressions in a 
 program. Having less precise program dependence information decreases the opportunities for 
 automatic parallelization and for instruction scheduling.
  
 The usage of pointers is error prone. Dereferencing NULL pointers and accessing previously 
 deallocated storage are two common programming mistakes. The usage of pointers in programs 
 is thus an obstacle for program understanding, debugging, and optimization. These activities 
 need answers to many questions about the structure of the heap contents and the pointer 
 variables pointing into the heap.
  
 By
  shapes
 , we mean descriptors of heap contents.
  Shape analysis
  is a generic term denoting 
 static program-analysis techniques that attempt to determine properties of the heap contents 
 relevant for the applications mentioned above.
  
 12.1.1 
  
 Structure of the Chapter
  
 Section 12.2 lists a number of questions about the contents of the heap. Figure 12.1 presents a 
 program that will be used as a running example, which inserts an element into a singly linked list. 
 Section 12.2.3 shows how shape analysis would answer the questions about the heap contents 
 produced by this program. Section 12.3 then informally presents a parametric shape-analysis 
 framework along the lines of [58], which provides a generative way to design and implement 
 shape-analysis algorithms. The “shape semantics” —plus some additional properties that 
 individual storage elements may or may not possess — are specified in logic, and the shape-
 analysis algorithm is automatically generated from such a specification. Section 12.4 shows how 
 the informal treatment from Section 12.3 can be made precise by basing it on predicate logic. In 
 particular, it is shown how a 2-valued interpretation and a 3-valued interpretation of the same 
 set of
  
 / * insert.c */ 
  
 #include ''list.h'' 
  
 void insert (List x, int d)
  { 
  
  
 List y, t, e;
  
 assert(acyclic
  
 list (x) && x != NULL);
  
   
  
 y = x; 
  
   
  
 while (y->n ! = NULL && ...)
  { 
   
  
 y = y->n; 
  
   
  
 } 
  
   
  
 t = malloc( ); 
  
 /* list.h */ 
  
  
 t->data = d; 
  
 typedef struct node
  { 
  
   
 e = y->n; struct node *n; 
  
  
 t->n = e; 
  
 int data; 
  
  
 y->n = t; 
  
 }
  *List; 
  
 } 
  
 (a) 
  
 (b)
  
 FIGURE 12.1 
 (a) Declaration of a linked-list data type in C. (b) A C function that searches a list pointed to by 
 parameter
  x
 , and splices in a new element.",NA
12.2,NA,NA
Questions about the Heap Contents,"Shape analysis has a somewhat constrained view of programs. It is not concerned with numeric 
 or string values that programs compute, but exclusively with the linked data structures they 
 build in the heap and the pointers into the heap from the stack, from global memory, or from cells 
 in the heap.
 5
 We will therefore use the term
  execution state
  to mean the set of cells in the heap, 
 the connections between them (via pointer components of heap cells), and the values of pointer 
 variables in the store.
  
 12.2.1 
  
 Traditional Compiler Analyses
  
 We list some questions about execution states that a compiler might ask at points in a program, 
 together with (potential) actions enabled by the respective answers:
  
 NULL pointers:
  Does a pointer variable or a pointer component of a heap cell contain NULL at 
 the 
  
 entry to a statement that dereferences the pointer or component?
  
 Yes (for every state):
  Issue an error message.
  
 No (for every state):
  Eliminate a check for NULL. 
  
 Maybe:
  Warn about the potential NULL 
 dereference.
  
 Alias:
  Do two pointer expressions reference the same heap cell?
  
 Yes (for every state):
  Trigger a prefetch to improve cache performance, predict a cache 
 hit to improve cache-behavior prediction, or increase the sets of uses and 
 definitions for an improved liveness analysis.
  
 No (for every state):
  Disambiguate memory references and improve program 
 dependence in-
  
 formation [11, 55].
 6
  
 Sharing:
  Is a heap cell shared?
 7
  
 Yes (for some state):
  Warn about explicit deallocation, because the memory manager 
 may run 
  
 into an inconsistent state.
  
 No (for every state):
  Explicitly deallocate the heap cell when the last pointer to it ceases to exist.
  
 Reachability:
  Is a heap cell reachable from a specific variable or from any pointer variable?
  
 Yes (for every state):
  Use this information for program verification.
  
 No (for every state):
  Insert code at compile time that collects unreachable cells at runtime.
  
 Disjointness:
  Do two data structures pointed to by two distinct pointer variables ever have 
 common 
  
 elements?
  
 No (for every state):
  Distribute disjoint data structures and their computations to 
 different 
  
 processors [24].
  
  
 5
 However, the shape-analysis techniques presented in Sections 12.3 and 12.4 can be extended to account 
 for both numeric values and heap-allocated objects. See Section 12.6.4 and [20, 21, 28].
  
  
 6
 The answer “yes (for some state)” indicates the case of a may-alias. This answer prevents reordering or 
 parallelizing transformations from being applied.
  
 7
 Later in the chapter, the sharing property that is formalized indicates whether a cell is “heap-shared,” 
 that is, pointed to by two or more pointer components of heap cells. Sharing due to two pointer variables or 
 one pointer variable and one heap cell component pointing to the same heap cell is also deducible from the 
 results of shape analysis.",NA
12.3,NA,NA
Shape Analysis,"The example program
  insert
  works for lists of arbitrary lengths. However, as described in the 
 preceding section (at least for one program point), the description of the lists that occur during 
 execution is finite. As shown in Figure 12.3, eight shape graphs are sufficient to describe all of the 
 execution states that occur at the entry of the loop body in
  insert
 . This is a general 
 requirement for shape analysis. Although the data structures that a program builds or 
 manipulates are in general of unbounded size, the shape descriptors, manipulated by a shape-
 analysis algorithm, have to have
  bounded size
 .
  
 This representation of the heap contents has to be
  conservative
  in the sense that whoever asks 
 for properties of the heap contents — for example, a compiler, a debugger, or a program-
 understanding system — receives a reliable answer. The claim that “pointer variable
  p
  or 
 pointer field
  p->c
  never has the value NULL at this program point” may only be made if this is 
 indeed the case for all executions of the program and all program paths leading to the program 
 point. It may still be the case that in no program execution
  p
  (respectively
  p->c
 ) will be NULL 
 at this point but that the analysis will be unable to derive this information. In the field of program 
 analysis, we say that program analysis is allowed to (only) err on the safe side.
  
 In short, shape analysis computes for a given program and each point in the program:
  
 a finite, conservative representation of the heap-allocated data structures that could 
 arise when a path to this program point is executed.
  
 12.3.1 
  
 Summarization
  
 The constraint that we must work with a bounded representation implies a loss of information 
 about the heap contents. Size information, such as the lengths of lists or the depths of trees, will in 
 general be lost. However, structural information may also be lost because of the chosen 
 representation. Thus, a part of the execution state (or some of its properties) is exactly 
 represented, and some part of the execution state (or some of its properties) is only 
 approximately represented. The process leading to the latter is called 
 summarization
 . 
 Summarization intuitively means the following: 
  
  
 r
  Some heap cells will lose their identity, that is, will be represented together with other heap 
 cells by 
  
 r
  The connectivity among those jointly represented heap cells will be represented 
 conservatively; that is, each pointer in the heap will be represented, but several such pointers (or 
 the absence of such 
  
 r
  Properties of these heap cells will also be represented conservatively. 
 This means the following:
  
  
  
 
  A property that holds for all (for none of the) 
 summarized cells will be found to hold (not to 
  
  
  
 hold) for their summary 
 node.
  
 
  A property that holds for some but not all of the summarized cells will have the value 
 “don’t know” for the summary node.
  
 12.3.2 
  
 Parametric Shape Analysis
  
 Shape analysis
  is a generic term representing a whole class of algorithms of varying power and 
 complexity that try to answer questions about the structure of heap-allocated storage. In our 
 setting, a particular shape-analysis algorithm is determined by a set of properties that heap cells 
 may have and by relations that may or may not hold between heap cells.
  
 First, there are the aforementioned
  core properties
 , for example, the “pointed-to-by-
 p
 ” 
 property for each program pointer variable
  p
 , and the property “connected-through-
 c
 ,” which 
 pairs of heap cells (
 l
 1
 ,
 l
 2
 ) possess if the
  c
 -field of
  l
 1
  points to
  l
 2
  (see Table 12.1). These properties 
 are part of any pointer semantics. The core properties in the particular shape analysis of the
  
 insert
  program are",NA
12.4,NA,NA
An Overview of a Shape-Analysis Framework,"This section provides an overview of the formal underpinnings of the shape-analysis framework 
 presented in [58]. The framework is
  parametric
 ; that is, it can be instantiated in different ways to 
 create 
 a 
 variety 
 of 
 specificshape-analysisalgorithms.Theframeworkisbasedon3-
 valuedlogic.Inthispaper,thepresentation is at a semi-technical level; for a more detailed 
 treatment of this material, as well as several elaborations on the ideas covered here, the reader 
 should refer to [58].
  
 To be able to perform shape analysis, the following concepts need to be formalized: 
  
 r
  An encoding (or representation) of stores, so that we can talk precisely about store 
 elements and 
 r
  A way to extract the properties of stores and store elements. 
 r
  A definition of 
 the concrete semantics of the programming language, in particular, one that makes it 
 possible to track how properties change as the execution of a program statement changes 
 the 
 r
  A language in which to state properties that store elements may or may not possess.",NA
12.5,NA,NA
Applications,"The algorithm sketched in Section 12.4 produces a set of 3-valued structures for each program 
 point 
 pt
 . This set provides a conservative representation, that is, it describes a superset of the set 
 of concrete stores that can possibly occur in any execution of the program that ends at
  pt
 . 
 Therefore, questions about the stores at
  pt
  can be answered (conservatively) by posing queries 
 against the set of 3-valued structures that the shape-analysis algorithm associates with
  pt
 . The 
 answers to these questions can be utilized in an optimizing compiler, as explained in Section 
 12.2. Furthermore, the fact that the shape-analysis framework is based on logic allows queries to 
 be specified in a uniform way using logical formulas.
  
 In this section, we discuss several kinds of questions. Section 12.5.1 discusses how 
 instantiations of the parametric shape-analysis framework that have been described in previous 
 sections can be applied to the problem of identifying may- and must-aliases. Section 12.5.2 shows 
 that the shape-analysis framework can be instantiated to produce flow-dependence information 
 for programs that manipulate linked data structures. Finally, Section 12.5.3 sketches some other 
 applications for the results of shape analysis.
  
 12.5.1 
  
 Identifying May- and Must-Aliases
  
 We say that two pointer access paths,
  e
 1
  and
  e
 2
 , are
  may-aliases
  at a program point
  pt
  if there 
 exists an execution sequence ending at
  pt
  that produces a store in which both
  e
 1
  and
  e
 2
  point to 
 the same heap cell. We say that
  e
 1
  and
  e
 2
  are
  must-aliases
  at
  pt
  if, for every execution sequence 
 ending at
  pt
 ,
  e
 1
  and
  e
 2
  point to the same heap cell.
 11
  
 information, we use the formula Consider the access paths
  e
 1
  ≡
  x->
 f
 1
 ->
 · · ·
 ->
 f
 n
  and
  e
 2
  ≡
  x->
 g
 1
 ->
 · · ·
 ->
 g
 m
 . To extract aliasing
  
  
 def
 ∃
 v
 0
 ,
  . . .
  ,
  v
 n
 ,
  w
 0
 ,
  . . .
  ,
  w
 m
  :
  x
 (
 v
 0
 ) ∧
  f
 1
 (
 v
 0
 ,
  v
 1
 ) ∧ · · · ∧
  f
 n
 (
 v
 n
 −1
 ,
  v
 n
 ) 
  
 al
 [
 e
 1
 ,
  e
 2
 ] =
  
  
 ∧
  y
 (
 w
 0
 ) ∧
  g
 1
 (
 w
 0
 ,
  w
 1
 ) ∧ · · · ∧
  g
 m
 (
 w
 m
 −1
 ,
  w
 m
 ) 
  
 (12.23)
  
  
    
 ∧
  v
 n
  =
  w
 m 
  
 If Equation 12.23 evaluates to 0 in every 3-valued structure that the shape-analysis algorithm 
 associates with program point
  pt
 , we know that
  e
 1
  and
  e
 2
  are not may-aliases at
  pt
 . Similarly, when
  
 al
 [
 e
 1
 ,
  e
 2
 ] evaluates to 1 in every such structure, we know that
  e
 1
  and
  e
 2
  are must-aliases at
  pt
 . In all 
 other cases,
  e
 1
  and
  e
 2
  are considered may-aliases.
  
 Notethatinsomecases,
 al
 [
 e
 1
 ,
  e
 2
 ]mayevaluateto1
 /
 2,inwhichcase
 e
 1
  and
 e
 2
  areconsideredmay-aliases; 
 this is a conservative result.
  
 The answer can sometimes be improved by first applying
  focus
  with Equation 12.23. This will 
 produce a set of structures in which
  al
 [
 e
 1
 ,
  e
 2
 ] does not evaluate to an indefinite value. Finally, one 
 can run
  coerce 
 on the 3-valued structures produced by
  focus
  to eliminate infeasible 3-valued 
 structures.
  
 Example 12.17
  
 Consider the 3-valued structure at the bottom right corner of Table 12.3. The formula
  al
 [
 y->n-
 >n
 ,
 e
 ] evaluates to 1 in this structure and in all of the other structures arising after
  y->n = t
 ; 
 thus,
  y->n->n
  
 11
 Variants of these definitions can be defined that account for the case when
  e
 1
  or
  e
 2
  has the value
  NULL
 .",NA
12.6,NA,NA
Extensions,"The approach to shape analysis presented in this chapter has been implemented by T. Lev-Ami in 
 a system called
  TVLA
  (three-valued-logic analyzer) [34, 36]. TVLA provides a language in which 
 the user can specify (a) an operational semantics (via predicates and predicate-update formulas), 
 (b) a control-flow graph for a program, and (c) a set of 3-valued structures that describe the 
 program’s input. Using this specification, TVLA builds the corresponding equation system and 
 finds its least fixed point.
  
 The experience gained from building TVLA led to a number of improvements to, and extensions of, 
 the methods described in this chapter (and in [58]). The enhancements that TVLA incorporates 
 include:
  
 r
  The ability to declare that certain binary predicates specify
  functional properties
 . 
 r
  The 
 ability to specify that structures should be stored only at nodes of the control-flow graph that 
 r
  
 Anenhancedversionof
 coerce
 thatincorporatessomemethodsthataresimilarinflavortorelational-
 r
  An enhanced
  focus
  algorithm that generalizes the methods of Section 12.4.6.2 to handle 
 focusing on arbitrary formulas.
 14
 In addition, this version of
  focus
  takes advantage of the 
 properties of predicates 
 r
  The ability to specify criteria for merging together structures 
 associated with a program point. This feature is motivated by the idea that when the number 
 of structures that arise at a given program point is too large, it may be better to create a 
 smaller number of structures that represent at least the same set of 2-valued structures. In 
 particular, nullary predicates (i.e., predicates of 0-arity) are used to specify which structures 
 are to be merged together. For example, for linked lists, the “
 x
 -is-not-null” predicate, defined 
 by the formula
  nn
 [
 x
 ]() = ∃
 v
  :
  x
 (
 v
 ), discriminates between structures in which
  x
  points to a list 
 element, and structures in which it does not. By using
  nn
 [
 x
 ]() as the criterion for whether to 
 merge structures, the structures in which
  x
  is
  NULL
  are kept separate from those in which
  
 x
  points to an allocated memory cell.
  
 Further details about these features can be found in [34].
  
 The remainder of this section describes several other extensions of our parametric logic-based 
 analysis framework that have been investigated; many of these extensions have also been 
 incorporated into TVLA.
  
 12.6.1 
  
 Interprocedural Analysis
  
 Several papers have investigated interprocedural shape analysis. In [53] procedures are handled 
 by explic-itly representing stacks of activation records as linked lists, allowing rather precise 
 analysis of (possibly recursive) procedures. In [29] procedures are handled by automatically 
 creating summaries of their be-havior. Abstractions of
  two-vocabulary structures
  are used to 
 capture an over-approximation of the relation that describes the transformation effected by a 
 procedure. In [52] a new concrete semantics for programs that manipulate heap-allocated 
 storage is presented, which only passes “local” heaps to procedures. A simplified version of this",NA
12.7,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Related Work,"The shape-analysis problem was first investigated by Reynolds [49], who studied it in the context 
 of a Lisp-like language with no destructive updating. Reynolds treated the problem as one of 
 simplifying a collection of set equations. A similar shape-analysis problem, but for an imperative 
 language support-ing nondestructive manipulation of heap-allocated objects, was formulated 
 independently by Jones and Muchnick, who treated the problem as one of solving (i.e., finding the 
 least fixed point of) a collection of equations using regular tree grammars [30].
  
 Jones and Muchnick [30] also began the study of shape analysis for languages
  with
  destructive 
 updating. To handle such languages, they formulated an analysis method that associates program 
 points with sets of finite shape-graphs.
 15
 To guarantee that the analysis terminates for programs 
 containing loops, the Jones–Muchnick approach limits the length of acyclic selector paths by some 
 chosen parameter
  k
 . All nodes beyond the “
 k
 -horizon” are clustered into a summary node. The 
 Jones–Muchnick formulation has two drawbacks: 
  
   
 r
  The analysis yields poor results for programs that manipulate cons-cells beyond the
  k
 -
 horizon. For example, in the list-reversal program of Figure 12.1, little useful information is 
 obtained. The 
   
  
 analysis algorithm must model what happens when the program 
 is applied to lists of length greater 
  
  
  
 than
  k
 . However, the tail of such a list 
 is treated conservatively, as an arbitrary, and possibly cyclic, 
  
  
 r
  The analysis may 
 be extremely costly because the number of possible shape-graphs is doubly exponential in
  k
 .
  
 In addition to Jones and Muchnick’s work,
  k
 -limiting has also been used in a number of 
 subsequent papers (e.g., Horwitz et al. [25]).
  
 Another well-known shape-analysis algorithm, developed by Chase et al. [8], is based on the 
 following ideas: 
  
   
 r
  Sharing information, in the form of abstract heap reference counts (0, 1, and ∞), is used to 
 characterize shape-graphs that represent list structures.
 16 
  
   
 r
  Several heuristics are introduced to allow several shape-nodes to be maintained for each 
 allocation 
  
  
  
 that will definitely be overwritten, the
  n
 -field of the shape-node 
 that
  x
  points to can be overwritten 
 r
  Foranassignmentto
 x->n
 ,whentheshape-
 nodethat
 x
 pointstorepresentsonlyconcreteelements 
  
  
  
 (a so-called strong 
 update).
  
 The Chase–Wegman–Zadeck algorithm is able to identify list-preservation properties in some 
 cases; for instance, it can determine that a program that appends a list to a list preserves 
 “listness.” However, as noted in [8], allocation-site information alone is insufficient to determine 
 interesting facts in many programs. For example, it cannot determine that “listness” is preserved 
 for either the list-insert program or a list-reversal program that uses destructive-update 
 operations. In particular, in the list-reversal program, the Chase–Wegman–Zadeck algorithm 
 reports that a possibly cyclic structure may arise, and that the two lists used by the program 
 might share cells in common (when in fact the two lists are always disjoint).
  
 The parametric framework presented in this paper can be instantiated to implement the Chase–
 Wegman–Zadeck algorithm, as well as other shape-analysis algorithms. Furthermore, in Section 
 12.5.2, we presented a new algorithm for computing flow dependences using our parametric 
 approach. For additional discussion of related work, the reader is referred to [57, 58].",NA
12.8,NA,NA
Conclusions,"Many of the classical data-flow-analysis algorithms use bit vectors to represent the characteristic 
 functions of set-valued data-flow values. This corresponds to a logical interpretation (in the 
 abstract semantics) that uses two values. It is
  definite
  on one of the bit values and
  conservative
  on 
 the other. That is, either “false”means “false” and “true” means “may be true/may be false,” or 
 “true” means “true” and “false” means“may be true/may be false.” Many other static-analysis 
 algorithms have a similar character.
  
 Moststaticanalyseshavesuchaone-sidedbias;exceptionsincludedata-
 flowanalysesthatsimultaneously track “may” and “must” information, for example, [6, 59].
  
 The material presented in this chapter shows that while
  indefiniteness
  is inherent (i.e., a static 
 analysis is unable, in general, to give a definite answer), one-sidedness is not. By basing the 
 abstract semantics on 3-valued logic, definite truth and definite falseness can both be tracked, 
 with the third value, 1
 /
 2, capturing indefiniteness.
  
 This outlook provides some insight into the true nature of the values that arise in other work on 
 static analysis: 
  
   
 r
  A one-sided analysis that is precise with respect to “false” and conservative with respect to 
 “true”is really a 3-valued analysis over 0, 1, and 1
 /
 2 that conflates 1 and 1
 /
 2 (and uses “true” in 
 place 
    
 is really a 3-valued analysis over 0, 1, and 1
 /
 2 that conflates 0 and 1
 /
 2 (and uses 
 “false” in place 
 r
  Likewise, an analysis that is precise with respect to “true” and conservative with 
 respect to “false”
  
  
  
 of 1
 /
 2).
  
 In contrast, the analyses developed in this chapter are unbiased: They are precise with respect to 
 both 0 and 1 and use 1
 /
 2 to capture indefiniteness. Other work that uses 3-valued logic to 
 develop unbiased analyses includes [26].
  
 We hope the ideas presented in this chapter (and the TVLA system, which embodies these 
 ideas) will help readers implement new static-analysis algorithms that identify interesting 
 properties of programs that make use of heap-allocated data structures.
  
 References
  
 1. L. O. Andersen. 1994. Program analysis and specialization for the C programming language. 
 Ph.D. 
  
 dissertation, DIKU, University of Copenhagen (DIKU report 94/19).
  
 2. G. Arnold, R. Manevich, M. Sagiv, and R. Shaham. 2006. Combining shape analyses by 
 intersecting 
  
 abstractions. In
  Verification, Model Checking, and Abstract Interpretation
 .
  
 3. T. Ball, R. Majumdar, T. Millstein, and S. K. Rajamani. 1998. Automatic predicate abstraction 
 of C 
  
 programs. In
  SIGPLAN Conference on Programming Language Design and 
 Implementation.
  
 4. T. Ball and S. K. Rajamani. 2000. Bebop: A symbolic model checker for Boolean programs. In 
 Proceedingsofthe7thInternationalSPINWorkshoponSPINModelCheckingandSoftwareVerificat
 ion
 , 113–30. London: Springer-Verlag.
  
 5. A. Bijlsma. 1999. A semantics for pointers. Talk at the Dagstuhl-Seminar on Program 
 Analysis. 6. R. Bodik, R. Gupta, and M. L. Soffa. 1998. Complete removal of redundant 
 computations. In 
  
 SIGPLAN Conference on Programming Language Design and 
 Implementation
 .
  
 7. R. E. Bryant. 1986. Graph-based algorithms for Boolean function manipulation. In
  IEEE Trans. 
  
 Comput
 . 6:677–91.
  
 8. D. R. Chase, M. Wegman, and F. Zadeck. 1990. Analysis of pointers and structures. In
  SIGPLAN 
  
 Conference on Programming Language Design and Implementation
 .
  
 9. E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith. 2000. Counterexample-guided 
 abstraction 
  
 refinement. In
  Computer Aided Verification
 .",NA
13 ,NA,NA
Optimizations for ,NA,NA
Object-Oriented ,NA,NA
Languages,"13.1 
  
 13.2
  
 Introduction
  .........................................
 13
 -
 1 
 Object Layout and 
 Method Invocation
  ................
 13
 -
 2 
 Single Inheritance
 •
  Multiple 
 Inheritance
 •
  Bidirectional 
  
 Object Layout
 •
  Dispatch Table Compression
 •
  Java Class 
  
 Layout and Method Invocation
 •
  Dispatch without Virtual
  
 Method Tables
  
 Andreas Krall
  
 13.3
  
 Fast Type Inclusion Tests
  .............................
 13
 -
 10
  
 Binary Matrix Implementation
 •
  Cohen’s Algorithm
 •
  Relative
  
 Numbering
 •
  Hierarchical Encoding
 •
  More Algorithms
  
 •
  Partitioning the Class Hierarchy
 •
  PQ-Encoding
  
  
 Institute fuer Computersprachen, 
 Technische Universit
 ¨
 at Wien, 
  
 Wein, Austria 
  
 andi@complang.tuwien.ac.at
  
 Nigel Horspool 
  
 Department of Computer Science, 
 University of Victoria, 
  
 Victoria, BC, Canada 
  
 nigelh@csr.uvic.ca
  
 13.4
  
 13.5
  
 Devirtualization
  .....................................
 13
 -
 14
  
 Class Hierarchy Analysis
 •
  Rapid Type Analysis
  •
  Other Fast
  
 Precise-Type Analysis Algorithms
 •
  Variable Type Analysis
  
 •
  Cartesian Product Algorithm
 •
  Comparisons and Related
  
 Work
 •
  Inlining and Devirtualization Techniques
  
 Escape Analysis
  ......................................
 13
 -
 20
  
 Escape Analysis by Abstract Interpretation
 •
 Other Approaches
  
 13.6 Conclusion
  ..........................................
 13
 -
 27 
 References
  
 ..................................................
 13
 -
 27",NA
13.1,NA,NA
Introduction,"This chapter introduces optimization techniques appropriate for object-oriented languages. The 
 topics covered include object and class layout, method invocation, efficient runtime-type checks, 
 devirtualization with type analysis techniques, and escape analyses. Object allocation and 
 garbage collection techniques are also very important for the efficiency of object-oriented 
 programming languages. However, because of their complexity and limited space, this important 
 topic must unfortunately be omitted. A good reference is the book by Jones and Lins [18].
  
 Optimization issues relevant to a variety of programming languages, including C++, Java, Eiffel, 
 Smalltalk, and Theta, are discussed. However, to ensure consistent treatment, all examples have 
 been converted to Java syntax. When necessary, some liberties with Java syntax, such as true 
 multiple inheri-tance, have been made.
  
 13
 -
 1",NA
13.2,"y
  
 y
  
 Point
  
 x
  
 ColorPnt
  
 x
  
 FIGURE 13.1
  
 Single inheritance layout.",NA
Object Layout and Method Invocation,"The memory layout of an object and how the layout supports dynamic dispatch are crucial 
 factors in the performance of object-oriented programming languages. For single inheritance 
 there are only a few efficient techniques: dispatch using a virtual dispatch table and direct calling 
 guarded by a type test. For multiple inheritance many techniques with different compromises are 
 available: embedding superclasses, trampolines, and table compression.
  
 13.2.1 
  
 Single Inheritance
  
 In the case of single inheritance, the layout of a superclass is a prefix of the layout of the subclass. 
 Figure 13.1 shows the layouts of an example class and subclass. Access to instance variables 
 requires just one load or store instruction. Adding new instance variables in subclasses is simple.
  
 Invocation of virtual methods can be implemented by a method pointer table (virtual function 
 table, 
 vtbl
 ). Each object contains a pointer to the virtual method table. The
  vtbl
  of a subclass is an 
 extension of the superclass. If the implementation of a method of the superclass is used by the 
 subclass, the pointer in the
  vtbl
  of the subclass is the same as the pointer in the superclass.
  
 Figure 13.2 shows the virtual tables for the classes
  Point
  and
  ColorPnt
  defined as follows:
  
 class Point { 
  
 int x, y; 
  
 void move(int x, int y) {...} 
  
 void draw() {...} 
  
 }
  
 class ColorPnt extends Point { 
  
 int color; 
  
 void draw() {...} 
  
 void setcolor(int c) {...} 
  
 }
  
 Point
  
 ColorPnt
  
 FIGURE 13.2
  
 y
  
 drawptr
  
 draw
  
 x
  
 vtblptr
  
 moveptr
  
 move
  
 color
  
 setcolorptr
  
 setcolor
  
 y
  
 drawptr
  
 draw
  
 x
  
 vtblptr
  
 moveptr
  
 Single inheritance layout with virtual method table.",NA
13.3,NA,NA
Fast Type Inclusion Tests,"In a statically typed language, an assignment may require a runtime test to verify correctness. For 
 example, if class
  B
  is a subclass of
  A
 , the assignment to
  b
  in the following Java code:
  
 A a = new B(); 
  
 ... // intervening code omitted 
  
 B b = a;
  
 needs validation to ensure that
  a
  holds a value of type
  B
  (or some subclass of
  B
 ) instead of type
  
 A
 . Usually that validation can be a runtime test.
  
 Java also has an explicit
  instanceof
  test to check whether an object has the same type or is 
 a subtype of a given type. Other object-oriented languages have similar tests. Static analysis is not 
 very effective in eliminating these tests [13]. Therefore, efficient runtime type checking is very 
 important.
  
 The obvious implementation of a type inclusion test is for a representation of the class 
 hierarchy graph to be held in memory and that graph to be traversed, searching to see whether 
 one node is an ancestor of the other node. The traversal is straightforward to implement for a 
 language with single inheritance and less so for multiple inheritance. However, the defect with",NA
13.4,NA,NA
Devirtualization,"Devirtualization is a technique to reduce the overhead of virtual method invocation in object-
 oriented languages. The aim of this technique is to statically determine which methods can be 
 invoked by virtual method calls. If exactly one method is resolved for a method call, the method 
 can 
 be 
 inlined 
 or 
 the 
 virtual 
 methodcallcanbereplacedbyastaticmethodcall.Theanalysesnecessaryfordevirtualizationalsoimpr
 ove the accuracy of the call graph and the accuracy of subsequent interprocedural analyses. We 
 first discuss different type analysis algorithms, comparing their precision and complexity. Then 
 different solutions for devirtualization of extensible class hierarchies and similar problems are 
 presented.
  
 13.4.1 
  
 Class Hierarchy Analysis
  
 The simplest devirtualization technique is class hierarchy analysis (CHA), which determines the 
 class hierarchy used in a program. A Java class file contains information about all referenced 
 classes. This information can be used to create a conservative approximation of the class 
 hierarchy. The approximation is formed by computing the transitive closure of all classes 
 referenced by the class containing the main method. A more accurate hierarchy can be 
 constructed by computing the call graph [11]. CHA uses the declared types for the receiver of a 
 virtual method call for determining all possible receivers.",NA
13.5,NA,NA
Escape Analysis,"Ingeneral,instancesofclassesaredynamicallyallocated.Storagefortheseinstancesisnormallyallocate
 don the heap. In a language such as C++, where the programmer is responsible for allocating and 
 deallocating",NA
13.6,NA,NA
Conclusion,"We presented the most important optimizations for object-oriented programming languages that 
 should give language implementors good choices for their work. Method invocation can be 
 efficiently solved by different kinds of dispatch tables and inlining. Inlining and specialization 
 greatly 
 improve 
 the 
 performance 
 butneedpreciseandefficienttypeanalysisalgorithms.Escapeanalysiscomputestheinformationneces
 sary to allocate objects on the runtime stack.
  
 References
  
 1. S. Abramsky and C. Hankin. 1987.
  Abstract interpretation of declarative languages
 . New York: 
 Ellis 
  
 Horwood.
  
 2. O. Agesen. 1995. The Cartesian product algorithm: Simple and precise type inference of 
 parametric polymorphism. In
  ECOOP ’95 — Object-Oriented Programming, 9th European 
 Conference
 , ed. W. G. Olthoff, 2–26. Lecture Notes in Computer Science, Vol. 952. New York: 
 Springer-Verlag.
  
 3. H. A¨ıt-Kaci, R. Boyer, P. Lincoln, and R. Nasr. 1989. Efficient implementation of lattice 
 operations. 
  
 ACM Trans. Program. Lang. Syst
 . 11(1):115–46.
  
 4. B. Alpern, A. Cocchi, D. Grove, and D. Lieber. 2001. Efficient dispatch of Java interface methods.
  
 In
  HPCN ’01: Java in High Performance Computing
 , ed. V. Getov and G. K. Thiruvathukal, 621–
 28. Lecture Notes in Computer Science, Vol. 2110. New York: Springer-Verlag.
  
 5. D. F. Bacon. 1997. Fast and effective optimization of statically typed object-oriented 
 languages. 
  
 Ph.D. thesis, University of California, Berkeley.",NA
14,NA,NA
Program Slicing,"14.1 Introduction
  ..........................................
 14
 -
 1 
  
 Static and Dynamic 
 Slicing
 •
  Backward and Forward Slicing
  
  
 •
  Organization of the Chapter 
  
 14.2 Applications of Program Slicing
  .......................
 14
 -
 3 
  
 Debugging
 •
  
 Software Maintenance and Testing
 •
  Program 
  
  
 Integration
 •
  Functional Cohesion-Metric Computation
  
  
 •
  Parallelization
 •
  Other Applications of Program Slicing 
  
 14.3 Some Basic Concepts and Definitions
  ..................
 14
 -
 5
  
 Intermediate Program Representation
  
 •
  Precision and
  
 Correctness of Slices
  
 14.4 Basic Slicing Algorithms: An Overview
  .................
 14
 -
 11
  
 Slicing Using Data Flow Analysis
 •
  Slicing Using
  
 Graph-Reachability Analysis
 •
  Slicing in the Presence of
  
 Composite Data Types and Pointers
 •
  Slicing of Unstructured
  
 Programs
  
 14.5 Slicing of Concurrent and Distributed Programs: An
  
 Overview
  .............................................
 14
 -
 18
  
 Nondeterminism
 •
  Process Interaction
 •
  A Coding View
  
 •
  Interaction via Shared Variables
  •
  Interaction by Message
  
 Passing
 •
  Concurrency at the Operating System Level
 •
  Slicing
  
 G. B. Mund 
  
 Department of Computer Science and 
 Engineering, 
  
 Kalinga Institute of Industrial 
  
 Technology, Bhubaneswar, India, 
  
 mundgb@yahoo.com
  
 Rajib Mall 
  
 Department of Computer Science and 
 Engineering, 
  
 Indian Institute of Technology, 
  
 Kharagpur, India, 
  
 rajib@cse.iitkgp.ernet.in",NA
14.1 Introduction,"Concurrent and Distributed Programs 
  
 14.6 Parallelization of Slicing
  ...............................
 14
 -
 23 
  
 Parallel Slicing 
 of Sequential Programs
 •
  Parallel Slicing of 
  
  
 Concurrent Programs 
  
 14.7 Slicing of Object-Oriented Programs
  ..................
 14
 -
 25 
  
 Static 
 Slicing of Object-Oriented Programs
 •
  Dynamic Slicing 
  
  
 of Object-Oriented Programs
  
 14.8 Slicing of Concurrent and Distributed Object-Oriented 
  
  
 Programs
  .............................................
 14
 -
 27 
  
 Static Slicing
 •
  
 Dynamic Slicing 
  
 14.9 Conclusions
  ..........................................
 14
 -
 28 
 References
  
 ..................................................
 14
 -
 28
  
 Program slicing is a program analysis technique. It can be used to extract the statements of a 
 program that are relevant to a given computation. The concept of program slicing was introduced 
 by Weiser [1–3]. A program can be sliced with respect to a
  slicing criterion
 . A slicing criterion is a 
 pair
  <p
 ,
  V>
 , where
  p
  is a program point of interest and
  V
  is a subset of the program’s variables. If 
 we attach integer labels to all the statements of a program, a program point of interest could be 
 an integer
  i
  representing the label associated with a statement of the program.
  A slice of a 
 program P with respect to a slicing criterion <s
 ,
  V > is the
  
 14
 -
 1",NA
14.2,NA,NA
Applications of Program Slicing,"The program slicing technique was originally developed to realize automated static code 
 decomposi-tion tools. The primary objective of those tools was to aid program debugging [1–3]. 
 From this modest beginning, program slicing techniques have now ramified into a powerful set of 
 tools for use in such diverse applications as program understanding, program verification, 
 debugging, software maintenance and testing, functional cohesion metric computation, dead 
 code elimination, reverse engineering, paral-lelization of sequential programs, software 
 portability analysis, reusable component generation, program integration, tuning of compilers, 
 compiler optimization, determining uninitialized variables, Y2K prob-lems, and so on [7–65]. 
 Excellent surveys of existing slicing algorithms and their applications are reported in [5, 41, 44, 
 63, 64, 66, 67]. In the following, we briefly discuss some of these applications of program slicing.
  
 14.2.1 
  
 Debugging
  
 Realization of automated tools to help effective program debugging was the original motivation 
 for the development of the static slicing technique. In his doctoral thesis, Weiser provided 
 experimental evidence that programmers unconsciously use a mental form of slicing during 
 program debugging [1]. Locating a bug can be a difficult task when one is confronted with a large 
 program. In such cases, program slicing is useful because it can enable one to ignore many 
 statements while attempting to localize the bug. If a program computes an erroneous value for a 
 variable
  x
 , only those statements in its slice would contain the bug; all statements not in the slice 
 can safely be ignored.
  
 The control and data dependences existing in a program are determined during slice 
 computation. A program slicer integrated into a symbolic debugger can help in visualizing 
 control and data dependences. Variants of the basic program slicing technique have been 
 developed to further assist the programmer during debugging;
  program dicing
  [10] identifies 
 statements that are likely to contain bugs by using information that some variables fail some 
 tests while others pass all tests at some program point. Consider a slice with respect to an 
 incorrectly computed variable at a particular statement. Now consider a correctly computed 
 variable at some program point. The bug is likely to be associated with the slice on the 
 incorrectly computed variable minus the slice on the correctly computed variable. This dicing 
 heuristic can be used iteratively to locate a program bug. Several slices may be combined with 
 each other in different ways. The intersection of two slices contains all statements that lead to an 
 error in both test cases. The union of two slices contains all statements that lead to an error in at 
 least one of the test cases. The symmetric difference of two slices contains all statements that 
 lead to an error in exactly one of the test cases.
  
 Another variant of program slicing is
  program chopping
  [68, 69]. It identifies statements that 
 lie between two points
  a
  and
  b
  in the program and are affected by a change at
  a
 . Debugging in 
 such a situation should be focused only on statements between
  a
  and
  b
  that transmit the change 
 of
  a
  to
  b
 .
  
 14.2.2 
  
 Software Maintenance and Testing
  
 Software maintainers often have to perform
  regression testing
 , that is, retesting a software 
 product after any modifications are carried out to ensure that no new bugs have been introduced 
 [59]. Even after a small change, extensive tests may be necessary, requiring running of many test 
 cases. Suppose a program modification requires only changing a statement that defines a variable
  
 x
  at a program point
  p
 . If the forward slice with respect to the slicing criterion
  <p
 ,
  x>
  is disjoint 
 from the coverage of a regression test",NA
14.3,NA,NA
Some Basic Concepts and Definitions,"In this section we present a few basic concepts, notations, and terminologies that are used later 
 in this chapter. The existing program slicing literature shows a wide variation in and 
 disagreement about the notations used in program slicing. We explain our usage here because of 
 this lack of consensus. The usage presented here does not come from any single source but rather 
 is a personal blending of ideas from many sources.
  
 Definition 14.1 Directed graph or graph:
  A
  directed graph
  (or graph) G is a pair
  (
 N
 ,
  E
  )
 , where N 
 is a finite non-empty set of
  nodes
 , and E
  ⊆
  N
  ×
  N is a set of directed edges between the nodes.
  
 Let
  G
  = (
 N
 ,
  E
  ) be a graph. If (
 x
 ,
  y
 ) is an edge of
  G
 , then
  x
  is called a
  predecessor
  of
  y
 , and
  y
  is 
 called a
  successor
  of
  x
 . The number of predecessors of a node is its
  in-degree
 , and the number of 
 successors of the node is its
  out-degree
 . A
  directed path
  (or
  path
 )
  from a node
  x
 1
  to a node
  x
 k
  in a 
 graph
  G
  = (
 N
 ,
  E
  ) is a sequence of nodes (
 x
 1
 ,
  x
 2
 ,
  ...
 ,
  x
 k
 ) such that (
 x
 i
 ,
  x
 i
 +1
 ) ∈
  E
  for every
  i
 , 1 ≤
  i
  ≤
  k
  − 1.
  
 Definition 14.2 
  
 Flow graph:
  A
  flow graph
  is a quadruple (N, E, Start, Stop), where (N, E) is a 
 graph, 
 Start
  ∈
  N is a distinguished node of in-degree 0 called the
  start node
 ,
  Stop
  ∈
  N is a 
 distinguished node of out-degree 0 called the
  stop node
 , there is a path from Start to every other node 
 in the graph, and there is a path from every other node in the graph to Stop.
  
 Definition 14.3 Dominance:
  If x and y are two nodes in a flow graph, then x
  dominates
  y iff every 
 path from
  Start
  to y passes through x. y
  post-dominates
  x iff every path from x to
  Stop
  passes through 
 y.
  
 Let
  x
  and
  y
  be nodes in a flow graph
  G
 . Node
  x
  is said to be the
  immediate post-dominator
  of 
 node 
 y
  iff
  x
  is a post-dominator of
  y
 ,
  x
  ̸=
  y
 , and each post-dominator
  z
  ̸=
  x
  of
  y
  post-dominates
  x
 . 
 The 
 post-dominator tree
  of a flow graph
  G
  is the tree that consists of the nodes of
  G
 , has the root
  
 Stop
 , and has an edge (
 x
 ,
  y
 ) iff
  x
  is the immediate post-dominator of
  y
 .
  
 Consider the flow graph shown in Figure 14.3. In the flow graph, each of the nodes 1, 2, and 3 
 dominates the node 4. Node 5 does not dominate node 7. Node 7 post-dominates nodes 1, 2, 3, 4, 
 5, and 6. Node 6 post-dominates node 5. Node 6 post-dominates none of the nodes 1, 2, 3, 4, 7, 8, 
 and 9. Node 3 is the immediate post-dominator of node 2. Node 7 is the immediate post-
 dominator of node 4.",NA
14.4,NA,NA
Basic Slicing Algorithms: An Overview,"This section presents an overview of the basic program slicing techniques and includes a brief 
 history of their development. We do not aim to give a comprehensive review of the related work. 
 Such an attempt would be extremely difficult because of the numerous publications in this area 
 and the diverse theory and techniques used by researchers over the years. Instead, we briefly 
 review the work relevant to our discussion in this chapter. We start with the original approach of 
 Weiser [1], where slicing is considered as a data flow analysis problem, and then examine the 
 slicing techniques where slicing is seen as a graph-reachability problem.
  
 14.4.1 
  
 Slicing Using Data Flow Analysis
  
 Weiser introduced the concept of program slicing in his doctoral dissertation [1]. His original 
 motivation was to develop the slicing technique as an aid to program debugging. His ideas were 
 inspired by the abstraction mechanisms used by programmers while analyzing existing 
 programs. 
 In 
 his 
 slicing 
 technique, 
 theunitsofabstractionwerecalled
 slices
 .Theslicesabstractaprogrambasedonthebehavioroftheprogr
 am with respect to a specified set of data components, for example, variables.
  
 14.4.1.1 Static Slicing
  
 The first slicing algorithm was presented by Weiser [1]. His static slicing method used a CFG as 
 an intermediate representation of the program to be sliced and was based on iteratively solving 
 data and control flow equations representing interstatement influences. Let
  v
  be a variable and
  n
  
 be a statement (node) of a program
  P
  and
  S
  be the slice with respect to the slicing criterion
  <n
 ,
  v>
 . 
 Consider a node 
 m
  ∈
  S
 . Weiser defined the set of
  relevant variables
  for the node
  m
 ,
  relevant
 (
 m
 ), as 
 the set of variables of the program
  P
  whose values (transitively) affect the computation of the 
 value of the variable
  v
  at the node 
 n
 . Consider the example program shown in Figure 14.1a and 
 the slice with respect to the slicing criterion 
 <
 9,
  prd>
 . For this example,
  relevant
 (8) = {
 prd
 },
  
 relevant
 (7) = {
 prd
 }, and
  relevant
 (6) = {
 prd
 ,
  i
 }. In Weiser’s methodology, computing a slice from a 
 CFG requires computation of the data flow information about the set of
  relevant variables
  at each 
 node. That is, slices can be computed by solving a set of data and control flow equations derived 
 directly from the CFG of the program being sliced.
  
 Weiser’s algorithms [1, 2] for computing static slices did not handle programs with multiple 
 procedures. Later [3], Weiser presented algorithms for interprocedural slicing.
  
 In Weiser’s approaches, every slice is computed from scratch. That is, no information obtained 
 during any previous computation of slices is used. This is a serious disadvantage of his algorithm. 
 It has been shown that computation of static slices using his algorithm requires
  O
 (
 n
 2
 e
 ) time, 
 where
  n
  is the number of nodes and
  e
  is the number of edges in the CFG [1].",NA
14.5,NA,NA
Slicing of Concurrent and Distributed ,NA,NA
Programs: An Overview,"Inthissection,wefirstdiscusssomebasicissuesassociatedwithconcurrentprogramming.Later,wedisc
 uss how these issues have been addressed in computation of slices of concurrent and distributed 
 programs.
  
 The basic “unit” of concurrent programming is the
  process
  (also called
  task
  in the literature). A 
 process is an execution of a program or a section of a program. Multiple processes can execute 
 the same program (or a section of the program) simultaneously. A set of processes can execute 
 on one or more processors. In the limiting case of a single processor, all processes are interleaved 
 or time-shared on this processor. 
 Concurrent program
  is a generic term that is used to describe 
 any program involving potential parallel behavior. Parallel and distributed programs are 
 subclasses of concurrent programs that are designed for execution in specific parallel processing 
 environments.
  
 14.5.1 
  
 Nondeterminism
  
 A sequential program imposes a total ordering on the actions it performs. In a concurrent 
 program, there is an uncertainty over the precise order of occurrence of some events. This 
 property of a concurrent program is referred to as
  nondeterminism
 . A consequence of 
 nondeterminism is that when a concurrent program is executed repeatedly, it may take different 
 execution paths even when operating on the same input data.
  
 14.5.2 
  
 Process Interaction
  
 A concurrent program normally involves process interaction. This occurs for two main reasons: 
 r
  
 Processes compete for exclusive access to shared resources, such as physical devices or data, and 
 therefore need to coordinate access to the resource.
  
 r
  Processes communicate to exchange data.
  
 In both the above cases, it is necessary for the processes concerned to
  synchronize
  their 
 execution, either to avoid conflict, when acquiring resources, or to make contact, when 
 exchanging data. Processes can interact in one of two ways: through
  shared variables
  or by
  
 message passing
 . Process interaction may be 
 explicit
  within a program description or may occur
  
 implicitly
  when the program is executed.
  
 A process needing to use a
  shared resource
  must first
  acquire
  the resource, that is, obtain 
 permission to access it. When the resource is no longer required, it is
  released
 . If a process is 
 unable to acquire a resource, its execution is usually suspended until that resource is available. 
 Resources should be administered so that no process is delayed unduly.
  
 14.5.3
  
 A Coding View",NA
14.6,NA,NA
Parallelization of Slicing,"Parallel algorithms have the potential of being faster than their sequential counterparts since the 
 com-putation work can be shared by many computing agents all executing at the same time. Also, 
 for large programs, sequential algorithms become very slow. Slicing algorithms for concurrent 
 programs are highly compute-intensive, as the graphs required for intermediate representations 
 of the programs often become very large for practical problems. Therefore, parallelization of 
 slicing algorithms seems to be an attractive option to improve efficiency. In the following, we 
 review the research results in parallelization of slicing algorithms for sequential and concurrent 
 programs.
  
 14.6.1 
  
 Parallel Slicing of Sequential Programs
  
 Harman et al. presented a
  parallel slicing algorithm
  to compute intraprocedural slices for 
 sequential pro-grams. In their method, a
  process network
  is constructed from the program to be 
 sliced. A process network is a network of concurrent processes. It is represented as a directed 
 graph in which nodes represent processes and edges represent communication channels among 
 processes.
  
 The process network is constructed using the CFG of the program. The
  reverse controlflow 
 graph
  (RCFG) is constructed by reversing the direction of every edge in the CFG. The topology of 
 the process network is obtained from the RCFG, with one process for each of its nodes and with 
 communication channels corresponding to its edges. The edges entering a node
  i
  represent input 
 to process
  i
 , and the edges leaving node
  i
  represent outputs from process
  i
 .
  
 To compute a slice for the slicing criterion
  <n
 ,
  V>
 , where
  V
  is a set of variables of the program 
 and
  n
  is a node of the CFG of the program, network communication is initiated by outputing the 
 message
  V
  from the process
  n
  of the process network. Messages will then be generated and",NA
14.7,NA,NA
Slicing of Object-Oriented Programs,"Object-oriented programming languages have become very popular during the last decade. The 
 concepts of classes, inheritance, polymorphism, and dynamic binding are the basic strengths of 
 object-oriented programming languages. However, these concepts raise new challenges for 
 program slicing. Intermedi-ate representations for object-oriented programs need to model 
 classes, objects, inheritance, scoping, persistence, polymorphism, and dynamic binding 
 effectively. In the literature, research efforts to slice object-oriented programs are rarely 
 reported. In the following, we briefly review the reported work on static and dynamic slicing of 
 object-oriented programs.
  
 14.7.1 
  
 Static Slicing of Object-Oriented Programs
  
 Several researchers have extended the concepts of intermediate procedural program 
 representation to intermediate object-oriented program representation. Kung et al. [124–126] 
 presented a representation for object-oriented software. Their model consists of an
  object 
 relation diagram
  and a
  block branch diagram
 . The object relation diagram of an object-oriented 
 program provides static structural information on the relationships existing between objects. It 
 models the relationship that exists between classes such as inheritance, aggregation, association, 
 and so on. The block branch diagram of an object-oriented program contains the CFG of each of 
 the class methods and presents a static implementation view of the program. Harrold and 
 Rothermel [127] presented the concept of the
  call graph
 . A call graph provides a static view of the 
 relationship between object classes. A call graph is an interprocedural program representation in 
 which nodes represent individual methods and edges represent call sites. However, a call graph 
 does not represent important object-oriented concepts such as inheritance, polymorphism, and 
 dynamic binding.
  
 Krishnaswamy [128] introduced the concept of the
  object-oriented program dependence graph
  
 (OPDG). The OPDG of an object-oriented program represents control flow, data dependences, and 
 control de-pendences. The OPDG representation of an object-oriented program is constructed in 
 three layers:
  class hierarchy subgrach
  (CHS),
  control dependence subgraph
  (CDS), and
  data 
 dependence subgraph
  (DDS). The CHS represents the inheritance relationship between classes 
 and the composition of methods into a class. A CHS contains a single
  class header node
  and a
  
 method header node
  for each method that is defined in the class. Inheritance relationships are 
 represented by edges connecting class headers. Every method header is connected to the class 
 header by a
  membership edge
 . Subclass representations do not repeat representations of 
 methods that are already defined in the superclasses.
  Inheritance edges
  of a CHS connect the class 
 header node of a derived class to the class header nodes of its superclasses.
  Inherited membership 
 edges
  connect the class header node of the derived class to the method header nodes of the 
 methods that it inherits. A CDS represents the static control dependence relationships that exist 
 within and among the different methods of a class. The DDS represents the data dependence 
 relationship among the statements and predicates of the program. The OPDG of an object-
 oriented program is the union of the three subgraphs: CHS, CDS, and DDS. Slices can be computed 
 using an OPDG as a graph-reachability problem.
  
 The OPDG of an object-oriented program is constructed as the classes are compiled, and hence 
 it captures the complete class representations. The main advantage of OPDG representation over 
 other representations is that the representation has to be generated only once during the entire 
 life of the class. It does not need to be changed as long as the class definition remains unchanged. 
 Figure 14.13b represents the CHS of the example program of Figure 14.13a.
  
 Larsen and Harrold [129] extended the concept of the
  system dependence graph
  (SDG) to 
 represent some of the features of object-oriented programs. They introduced the notions of the
  
 class dependence graph
 , 
 class call graph
 ,
  class control flow graph
 , and
  interclass dependence graph
 .",NA
14.8,NA,NA
Slicing of Concurrent and Distributed ,NA,NA
Object-Oriented Programs,"Thenondeterministicnatureofconcurrentanddistributedprograms,unsynchronizedinteractionsam
 ong objects, lack of global states, and dynamically varying numbers of objects make the 
 understanding and debugging of concurrent and distributed object-oriented programs difficult. 
 An increasing amount of resources are being spent in testing and maintaining these software 
 products. 
 Slicing 
 techniques 
 promise 
 to 
 comeinhandyatthispoint.However,researchreportsdealingwithintermediaterepresentationsandsl
 icing of concurrent and distributed object-oriented programs are scarce in the literature [57, 67, 
 131, 137–148].
  
 14.8.1 
  
 Static Slicing
  
 Zhao et al. [57] introduced an intermediate program representation called
  system dependence net
  
 (SDN) to capture various dependence relations in a concurrent object-oriented program. To 
 represent interprocess communications between different methods in a class of a concurrent 
 object-oriented program, they introduced a new type of dependence edge called an
  external 
 communication dependence edge
 . An SDN captures the object-oriented features as well as the 
 concurrency issues in a concurrent object-oriented program. Using SDN as the intermediate 
 program representation, Zhao et al. [57] presented a static slicing algorithm for concurrent 
 object-oriented programs.
  
 Zhao [138, 139] introduced the concept of a
  multithreaded dependence graph
  (MDG) as an 
 intermediate representation of a concurrent Java program. The MDG consists of a collection of
  
 thread dependence graphs 
 (TDGs). A TDG represents a single thread of the program. Zhao’s 
 algorithm [139] uses an MDG as the intermediate program representation to compute static 
 slices of the concurrent Java program.
  
 Chen and Xu [140] proposed a static slicing algorithm for concurrent Java programs. Their 
 algorithm uses a CPDG as the intermediate program representation and computes more precise 
 slices than Zhao’s algorithm [139].
  
 14.8.2 
  
 Dynamic Slicing
  
 Recently, Mohapatra et al. [136, 146, 147] presented algorithms for dynamic slicing of concurrent 
 Java programs. They use a
  concurrent system dependence graph
  (CSDG) as the intermediate 
 program 
 repre-sentation. 
 A 
 CSDG 
 contains 
 control 
 dependence, 
 data 
 dependence, 
 synchronization dependence, and communication dependence edges. Their edge-marking 
 algorithm is based on marking and unmark-ing the edges of the CSDG as and when the 
 corresponding dependences arise and cease during an ac-tual run of the program. The space",NA
14.9,NA,NA
Conclusions,"We started with a discussion on the basic concepts and terminologies used in the area of 
 program slicing. We also reviewed recent work in the area of program slicing, including slicing of 
 sequential, concurrent, and object-oriented programs. Starting with the basic sequential program 
 constructs, researchers have now started to address various issues of slicing distributed object-
 oriented programs. Also, slicing algo-rithms are being extended to architectural slicing and 
 slicing of low-level programs such as hardware description languages. Since modern software 
 products often require programs with millions of lines of code, development of parallel 
 algorithms for slicing has assumed importance to reduce the slicing time.
  
 References
  
 1. M. Weiser. 1979. Program slices: Formal, psychological, and practical investigations of an 
 automatic 
  
 program abstraction method. PhD thesis, University of Michigan, Ann Arbor.
  
 2. M. Weiser. 1982. Programmers use slices when debugging.
  Communications of the ACM
  
 25(7): 
  
 446–52.
  
 3. M. Weiser. 1984. Program slicing.
  IEEE Transactions on Software Engineering
  10(4):352–57. 
 4. B. Korel and J. Laski. 1988. Dynamic program slicing.
  Information Processing Letters
  
 29(3):155–63. 5. 
 B.KorelandJ.Rilling.1998.Dynamicprogramslicingmethods.
 InformationandSoftwareTechnology 
  
 40:647–49.
  
 6. J.-F. Bergeretti and B. Carr´e. 1985. Information-flow and data-flow analysis of while-
 programs. 
  
 ACM Transactions on Programming Languages and Systems
  7(1):37–61.
  
 7. M.Weiser.1983.Reconstructingsequentialbehaviorfromparallelbehaviorprojections.
  
 Information 
  
 Processing Letters
  17(10):129–35.
  
 8. J. Lyle. Evaluating variations on program slicing for debugging. PhD thesis, University of 
  
 Maryland, College Park.
  
 9. S. Rapps and E. J. Weyuker. 1985. Selecting software test data using data flow information.
  
 IEEE 
  
 Transactions on Software Engineering
  SE-11(4):367–75.
  
 10. J. R. Lyle and M. D. Weiser. 1987. Automatic program bug location by program slicing.
  In 
 Proceedings 
  
 of the Second International Conference on Computers and Applications, Peking, 
 China
 , 877–82.",NA
15 ,NA,NA
Computations on,NA,NA
Iteration Spaces,"Sanjay Rajopadhye, 
  
 Lakshminarayanan 
  
 Renganarayana, 
  
 Gautam, 
  
 and 
  
 Michelle Mills Strout
  
 Department of Computer Science,
  
 Colorado State University,
  
 Fort Collins, CO
  
 Sanjay.Rajopadhye@colostate.edu
  
 ln@cs.colostate.edu
  
 ggupta@cs.colostate.edu
  
 mstrout@cs.colostate.edu
  
 15.1 
 15.2
  
 15.3
  
 Introduction
  .........................................
 15
 -
 1
  
 The
  Z
 -Polyhedral Model and Some Static Analyses
 ....
 15
 -
 1 
 Mathematical Background
  
 •
  Equational Language
 •
  Simplification of Reductions
 •
  The
  Z
 -Polyhedral Model
  
 •
  Scheduling
 •
  Backend
 •
  Bibliographic Notes
  
 Iteration Space Tiling
  ................................
 15
 -
 21
  
 Tiling for Dense Iteration 
 Spaces
 •
  Bibliographic Notes
  
 •
  Tiling Irregular Applications
  
 References
  ..................................................
 15
 -
 46",NA
15.1,NA,NA
Introduction,"This chapter consists of two independent parts. The first deals with programs involving indexed 
 data sets such as dense arrays and indexed computations such as loops. Our position is that high-
 level mathematical equations are the most natural way to express a large class of such 
 computations, and furthermore, such equations are amenable to powerful static analyses that 
 would enable a compiler to derive very efficient code, possibly significantly better than what a 
 human would write. We illustrate this by describing a simple equational language and its 
 semantic foundations and by illustrating the analyses we can perform, including one that allows 
 the compiler to reduce the degree of the polynomial complexity of the algorithm embodied in the 
 program.
  
 The second part of this chapter deals with tiling, an important program reordering 
 transformation 
 applicabletoimperativeloopprograms.Itcanbeusedformanydifferentpurposes.Onsequentialmachi
 nes tiling can improve the locality of programs by exploiting reuse, so that the caches are used 
 more effectively. On parallel machines it can also be used to improve the granularity of programs 
 so that the communication and computation “units” are balanced.
  
 We describe the tiling transformation, an optimization problem for selecting tile sizes, and 
 how to generate tiled code for codes with regular or affine dependences between loop iterations. 
 We also discuss approaches for reordering iterations, parallelizing loops, and tiling sparse 
 computations that have irregular dependences.",NA
15.2,NA,NA
The,NA,NA
 Z,NA,NA
-Polyhedral Model and Some Static Analyses,"It has been widely accepted that the single most important attribute of a programming language 
 is programmer productivity. Moreover, the shift to multi-core consumer systems, with the 
 number of cores expected to double every year, necessitates the shift to parallel programs. This 
 emphasizes the need for productivity even further, since parallel programming is substantially 
 harder than writing unithreaded
  
 15
 -
 1",NA
15.3,NA,NA
Iteration Space Tiling,"This section describes an important class of reordering transformations called tiling. Tiling is 
 crucial to exploit locality on a single proccessor, as well as for adapting the granularity of a 
 parallel 
 program. 
 We 
 first 
 describetilingfordenseiterationspacesanddatasetsandthenconsiderirregulariterationspacesandsp
 arse data sets. Next, we briefly summarize the steps involved in tiling and conclude with 
 bibliographic notes.
  
 15.3.1 
  
 Tiling for Dense Iteration Spaces
  
 Tiling is a loop transformation used for adjusting the granularity of the computation so that its 
 character-istics match those of the execution environment. Intuitively, tiling partitions the 
 iterations of a loop into groups called
  tiles.
  The tile sizes determine the granularity.
  
 In this section, we will study three aspects related to tiling. First, we will introduce tiling as a 
 loop transformation and derive conditions under which it can be applied. Second, we present a",NA
16 ,NA,NA
Architecture ,NA,NA
Description Languages ,NA,NA
for Retargetable ,NA,NA
Compilation,"Wei Qin
  
 16.1 Introduction
  ..........................................
 16
 -
 1
  
  
 Department of Electrical & Computer 
 Engineering, 
  
 Boston University, Boston, MA 
  
 wqin@bu.edu
  
 Sharad Malik 
  
 Department of Electrical Engineering, 
 Princeton University, Princeton, MA 
 sharad@princeton.edu",NA
16.1 Introduction,"16.2 Architecture Description Languages
  ...................
 16
 -
 3 
  
 Structural Languages
 •
  Behavioral Languages
 •
  Mixed 
  
  
 Languages 
  
 16.3 Discussions
  ...........................................
 16
 -
 16 
  
 Summary of ADLs
 •
  
 Essential Elements of an ADL
  
 •
  
 Organization of an ADL
 •
  Challenges
 •
  Future Directions 
  
 16.4 Conclusion
  ...........................................
 16
 -
 24 
 References
  
 ..................................................
 16
 -
 25
  
 Retargetable compilation has posed many challenges to researchers. The ultimate goal of the 
 field is to develop a universal compiler that generates high-quality code for all known 
 architectures. Among all existing compilers, GNU Compiler Collection (GCC) comes closest to this 
 goal by using a myriad of formal algorithms and engineering hacks. Currently, GCC officially 
 supports more than 30 architectures commonly used around the world [12]. Still, for many less 
 known architectures, there is no GCC support. Many recent application-specific instruction set 
 processors (ASIPs) are designed with such specialized architectural features that it is not a 
 straightforward task to create customized compilers for them, let alone retarget GCC. Despite the 
 difficulties, 
 good 
 retargetable 
 compilers 
 are 
 highly 
 desirable 
 for 
 these 
 architectures,especiallyduringtheirdevelopmentstagesinwhichmanycandidatedomain-
 specificfeatures 
 needtobeevaluated.Ifaretargetablecompileriscapableofutilizingthesecandidatefeaturesandgenera
 ting optimized code accordingly, their effectiveness can be quickly evaluated by running the 
 resulting code on an equally retargetable simulator. In this regard, retargetable compilation and 
 retargetable simulation are closely related problems, but retargetable compilation is a much 
 harder one.
  
 The need for retargetability largely comes from the back-ends of compilers. Compiler back-
 ends 
 may 
 be 
 classifiedintothefollowingthreetypes:customized,semi-
 retargetable,andretargetable.Customizedback-ends have little retargetability. They are usually 
 written for high-quality proprietary compilers or developed for special architectures requiring 
 nonstandard code generation flow. In contrast, semi-retargetable and retargetable compilers 
 reuse at least part of the compiler back-end by factoring out target-dependent information into 
 machine description systems. The difference of customization and retargetability is illustrated in 
 Figure 16.1.
  
 16
 -
 1",NA
16.2,NA,NA
Architecture Description Languages,"Traditionally, ADLs are classified into three categories: structural, behavioral, and mixed. This 
 classifica-tion is based on the nature of the information provided by the language. Structural 
 ADLs describe the microarchitecture organization of the processor. Behavioral ADLs focus on the 
 instruction set architecture (ISA) of the processor, and mixed ADLs contain both 
 microarchitecture and architecture information.
  
 16.2.1 
  
 Structural Languages
  
 Structural ADLs are similar to hardware description languages (HDLs). They describe the actual 
 structural organization of microprocessors at the register-transfer level (RT-level). At the RT-
 level, structural ADLs specify data transformation and transfer among storage units at each clock 
 cycle. It is sufficiently general to model the behavior of arbitrary digital logic, but without the 
 tedious details of gates or transistors. The structural ADLs include MIMOLA [33] and UDL/I [4].
  
 It is worth noting that the RT-level abstraction is not the same as the notion of register-
 transfer lists (RT-lists) used by computer scientists, though both are often abbreviated as RTL. 
 The latter is a higher abstraction level used for specifying instruction semantics. The major 
 difference between the two is the notion of time. Cycle time is an essential element in an RT-level 
 description. All RT-level events are associated with a specific time stamp and are fully ordered. In",NA
16.3,NA,NA
Discussions,"16.3.1
  
 Summary of ADLs
  
 ADL as a research area is far from being mature. In contrast to other computer languages such as 
 pro-gramming languages and HDLs, there is no common or de facto standard in the ADL field. 
 New ADLs continue to show up and there seems no sign of convergence.
  
 An obvious reason for this situation is the lack of mathematical formalism for general 
 modeling of computer architecture. Modern computer architectures range from simple 
 DSP/ASIPs 
 with 
 small 
 register 
 filesandlittlecontrollogictocomplexout-of-
 ordersuperscalarmachines[20]withdeepmemoryhierarchy 
 and 
 heavy 
 control 
 and 
 data 
 speculation. As semiconductor processes evolve, more and more transistors are being squeezed 
 into a single chip. The growing thirst for higher performance and lower power led to the birth of 
 even more sophisticated architectures such as multithreaded and multicore processors. Without 
 a solid underlying mathematical model, it is extremely hard for an ADL to simply use description 
 techniques to cover the vast range of computer architectures.
  
 A second reason for the absence of convergence is the diverging purposes of ADLs. Some ADLs 
 were originally designed as hardware description languages, for example, MIMOLA and UDL/I. 
 The main goal of those languages is accurate hardware modeling. Cycle-accurate simulators and 
 hardware synthesizers are natural products of such languages. However, it is nontrivial to extract 
 the ISA from them for use by a compiler. Some other ADLs were initially developed to be high-
 level processor modeling languages, such as nML, LISA, EXPRESSION, and MADL. The main goals 
 of these ADLs are general coverage over a wide architecture range and general support for both 
 compilers and simulators. However, because of the diverging needs of compilers and simulators, 
 these ADLs tend to have notable limitations for their support for one type of tools. Furthermore, 
 many ADLs were developed as configuration systems for their associated tools, such as Maril, 
 HMDES, TDL, and PRMDL. In some sense those languages can be viewed as a by-product of the 
 software tools. They are valuable only in the context of their associated tools.
  
 In summary, ADLs are designed for different purposes, at different abstraction levels, with 
 emphasis on different architectures of interest, and with support for different tools. Table 16.1 
 compares the important features of various ADLs. A few entries in the table were left empty, 
 because information is either not available (no related publication) or not applicable. The 
 parentheses in some entries indicate support with significant limitation. Without a flexible and 
 solid mathematical model for processors, it is hard to unify",NA
16.4,NA,NA
Conclusion,"Architecture Description Languages provide machine models for retargetable software tools 
 including compilers and simulators. They are crucial to the design space exploration of ASIPs. To 
 effectively support an optimizing compiler, an ADL should contain several pieces of information: 
  
  
 r
  Behavioral information in the form of RT-lists. Hierarchical behavioral models based on 
 attribute grammars are common and effective means. For VLIW architectures, instruction formats 
 should 
  
 r
  Structural information in the form of a reservation table or coarse-grained 
 netlists. Essential in-formation provided by this part includes abstracted resources such as 
 pipeline stages and data 
  
 r
  Mapping between behavioral and structural information. Information in 
 this part includes the time when semantic actions take place and the resources used by the 
 actions.
  
 In addition, modeling of irregular ILP constraints is useful for ADLs targeting ASIPs. In summary, 
 the desirable features of an ADL include simplicity for comprehending, conciseness for efficiency,",NA
17 ,NA,NA
Instruction Selection,NA,NA
Using Tree Parsing,"17.1 Introduction and Background
  .......................
  17
 -
 1
  
 Introduction
  
 •
  Dynamic Programming
  
 17.2 Regular Tree Grammars and Tree Parsing
  ............
  17
 -
 5
  
 17.3 A Top-Down Tree-Parsing Approach
 .................
  17
 -
 9
  
 Priti Shankar
  
 17.4 Bottom-Up Tree-Parsing Approaches
  ................
  17
 -
 14
  
 The Iterative Bottom-Up Preprocessing Algorithm
  
 •
  A
  
 Worklist-Based Approach to Bottom-Up Code-Generator
  
 Generators 
  
 Generators
  
 •
  Hard Coded Bottom-Up Code-
 Generator
 •
  The Code Generation Pass
  
 17.5 Techniques Extending LR-Parsers
  ....................
  17
 -
 24
  
 Extension of the
  LR
 (0) Parsing Algorithm
  
 •
  Precomputation of
  
 Tables
  
  
 Department of Computer Science and 
 Automation, 
  
 Indian Institute of Science, 
  
 Bangalore, India 
  
 priti@csa.iisc.ernet.in
  
 17.6 Related Issues
  .......................................
  17
 -
 32 
 17.7 Conclusion 
 and Future Work
  ........................
  17
 -
 33 
 Acknowledgments
  
 ........................................
  17
 -
 34 
 References
  ................................................
  
 17
 -
 34",NA
17.1,NA,NA
Introduction and Background,"17.1.1
  
 Introduction
  
 One of the final phases in a typical compiler is the
  instruction selection
  phase. This traverses an 
 intermediate representation of the source code and selects a sequence of target machine 
 instructions that implement the code. There are two aspects to this task. The first has to do with 
 finding efficient algorithms for generating an optimal instruction sequence with reference to 
 some measure of optimality. The second has to do with the automatic generation of instruction 
 selection programs from precise specifications of machine instructions. Achieving the second aim 
 is a first step toward retargetabiltiy of code generators. We confine our attention to instruction 
 selection for basic blocks. An optimal sequence of instructions for a basic block is called
  locally 
 optimal code
 .
  
 Early techniques in code generation centered around interpretive approaches where code is 
 produced for a virtual machine and then expanded into real machine instructions. The 
 interpretive approach suffers from the drawback of having to change the code generator for each 
 machine. The idea of code generation by tree parsing replaced the strategy of virtual machine 
 interpretation. The intermediate representation (IR) of the source program is in the form of a 
 tree, and the target machine instructions are represented as productions of a
  regular tree 
 grammar
  augmented with semantic actions and costs. The code generator parses the input 
 subject tree, and on each reduction, outputs target code. This is illustrated in Figure 17.1 for a 
 subject tree generated by the grammar of Example 17.1.
  
 17
 -
 1",NA
17.2,NA,NA
Regular Tree Grammars and Tree Parsing,"Let
  A
  be a finite alphabet consisting of a set of operators
  OP
  and a set of terminals
  T
 . Each 
 operator
  op 
 in
  OP
  is associated with an
  arity
 ,
  arity
 (
 op
 ). Elements of
  T
  have arity 0. The set",NA
17.3,NA,NA
A Top-Down Tree-Parsing Approach,"The key idea here is to reduce tree-pattern matching to string-pattern matching. Each root-to-leaf 
 path in a tree is regarded as a string in which the symbols in the alphabet are interleaved with 
 numbers indicating which branch from father to son has been followed. This effectively generates 
 a set of strings. The well-known Aho–Corasick multiple-keyword pattern-matching algorithm [1] 
 is then adapted to generate a top-down tree-pattern-matching algorithm. The Aho–Corasick 
 algorithm converts the set of keywords into a trie; the trie is then converted into a string-pattern-
 matching automaton that performs a parallel search for keywords in the input string. If
  K
  is the 
 set of keywords, then each keyword has a root leaf path in the trie, whose branch labels spell out 
 the keyword. This trie is then converted into a string-pattern-matching automaton as follows. 
 The states of the automaton are the nodes of the trie, with the root being the start state. All states 
 that correspond to complete keywords are final states. The transitions are just the branches of 
 the trie with the labels representing the input symbols on which the transition is made. There is a 
 transition from the start state to itself on every symbol that does not begin a keyword. The 
 pattern-matching automaton has a
  failure function
  for every state other than the start state. For a 
 state reached on input
  w
 , this is a pointer to the state reached on the longest prefix of a keyword 
 that is a proper suffix of
  w
 . The construction of the trie as well as the pattern-matching 
 automaton has complexity linear in the sum of the sizes of the keywords. Moreover, matches of 
 the keywords in an input string
  w
  are found in time linearly proportional to the length of
  w
 . Thus, 
 the string-pattern-matching problem can be solved in time
  O
 (|
 K
  | + |
 w
 |), where
  K
  is the sum of 
 the lengths of the keywords and
  w
  is the length of the input string [1].
  
 Hoffman and O’Donnell generalized this algorithm for tree-pattern matching by noting that a 
 tree can be defined by its root-to-leaf paths. A root-to-leaf path contains, alternately, root labels 
 and branch numbers according to a left-to-right ordering. Consider the tree patterns on the right-
 hand sides of the regular tree grammar in Example 17.4. Arities of various terminals and 
 operators are given in parentheses next to the operators and terminals and rules for computing 
 costs shown along with the productions. Actions to be carried out at each reduction are omitted.
  
 Example 17.4
  
 G
  = ({
 S
 ,
  R
 }, {:= (2), +(2),
  deref
  (1),
  sp
 (0),
  c
 (0)},
  P
 ,
  S
 ), where
  P
  consists of the following 
 productions: 
  
 r
  S
  → := (
 deref
  (
 sp
 ),
  R
 ) 
 r
  R
  →
  deref
  (
 sp
 ) 
 r
  R
  → +(
 R
 ,
  c
 ) 
 r
  R
  → +(
 c
 ,
  R
 ) 
 r
  R
  
 →
  c 
   
 cost
  = 1 +
  cost
 (
 R
 ) 
  
  
  
 cost
  = 1 +
  cost
 (
 R
 )
  
 cost
  = 1 
  
 cost
  = 3 +
  cost
 (
 R
 ) 
  
 cost
  = 2
  
 Thus, the patterns on the right-hand sides of productions in Example 17.4 are associated with the 
 following set of path strings:
  
 1. := 1
  deref
  1
  sp 
  
 2. := 2
  R",NA
17.4,The Compiler Design Handbook: Optimizations and Machine Code Generation,NA
Bottom-Up Tree-Parsing Approaches,"We begin with cost-augmented regular tree grammars in normal form. We first describe a 
 strategy for the generation of tables representing a tree automaton whose states do not encode 
 cost information. Using such an automaton, cost computations for generating locally optimal 
 code will have to be performed at code generation time, that is, dynamically.
  
 Ouraimistofindateachnode
 n
  
 inthesubjecttree,minimalcostrulesforeachnonterminalthatmatches at the node. We call such a set 
 of nonterminals
  matching nonterminals
  at node
  n
 . If the intermediate code tree is in the language 
 generated by the tree grammar, we expect one of the nonterminals that matches at the root to be 
 the start symbol of the regular tree grammar. The information about rules and nonterminals that 
 match as we go up the tree can be computed with the help of a bottom-up tree-pattern-matching 
 automaton built from the specifications. Thus, during the matching or code generation phase, we 
 traverse the intermediate code tree bottom-up, computing states and costs as we go along. For a 
 given nonterminal in a set, we retain only the minimal cost rule associated with that nonterminal. 
 Finally, when we reach the root of the tree, we have associated with the start symbol the minimal 
 cost of deriving the tree from the start nonterminal. Next, in a top-down pass we select the 
 nonterminals that yield the minimal-cost tree and generate code as specified in the translation 
 scheme.
  
 We present an iterative algorithm as well as a worklist-based algorithm drawing from the work of 
 Balachandran et al. [9] and Proebsting [38].
  
 17.4.1 
  
 The Iterative Bottom-Up Preprocessing Algorithm
  
 Let
  G
  = (
 N
 ,
  A
 ,
  P
 ,
  S
 ) be a regular tree grammar in normal form. Before describing the algorithm, we 
 describe some functions that we will be using in the computation. Let
  maxarity
  be the maximum 
 arity of an operator in
  A
 . Let
  I
 l
  be the set of positive integers of magnitude at most
  maxarity
 .
  
 rules
  :
  T
  ∪
  OP
  → 2
 P 
  
 For
 a
  ∈
  Trules
 (
 a
 ) = {
 r
 |
 r
  :
  n
  →
  a
  ∈
  P
 } 
  
 For
 op
  ∈
  OPrules
 (
 op
 ) = {
 r
  : |
 r
  :
  n
  →
  op
 (
 n
 1
 ,
  n
 2
 ,
  . . .
  ,
  n
 k
 ) ∈
  P
 }
  
 The set
  rules
 (
 a
 ) contains all production rules of the grammar whose right-hand side tree patterns 
 are rooted at
  a
 .
  
 child rules
  :
  N
  ×
  I
 l
  → 2
 P 
  
 child rules
 (
 n
 ,
  i
 ) = {
 r
 |
 r
  :
  n
 l
  →
  op
 (
 n
 1
 ,
  n
 2
 ,
  . . .
  ,
  n
 k
 ) and
  n
 i
  =
  n
 }
  
 The set
  child rules
 (
 n
 ,
  i
 ) contains all those productions such that the
  i
 th nonterminal on the right-
 hand side is
  n
 . The function can be extended to a set of nonterminals
  N
 1
  as follows:
  
 child rules
 (
 N
 1
 ,
  i
 ) = ∪
 n
 ∈
 N
 1
 child rules
 (
 n
 ,
  i
 ) 
  
 child NT
  :
  OP
  ×
  I
 l
  → 2
 N 
  
 child NT
 (
 op
 ,
  j
 ) = {
 n
  j
 |
 r
  :
  n
 l
  →
  op n
 1
 ,
  n
 2
 ,
  . . .
  ,
  n
  j
 ,
  . . .
  ,
  n
 k
  ∈
  P
 }
  
 In other words,
  child NT
 (
 op
 ,
  j
 ) is the set of all nonterminals that can appear in the
  j
 th position in 
 the sequence of nonterminals on the right-hand side of a production for operator
  op
 . (If
  j
  exceeds
  
 arity
 [
 op
 ], the function is not defined.)
  
 nt
  :
  P
  → 2
 N 
  
 nt
 (
 r
 ) = {
 n
 |
 r
  :
  n
  →
  α
  ∈
  P
 }",NA
17.5,NA,NA
Techniques Extending LR-Parsers,"The idea of LR-based techniques for table-driven code generation had been proposed earlier by 
 Graham and Glanville [23]. However, their approach cannot be applied in general to the problem 
 of regular tree parsing for ambiguous tree grammars, as it does not carry forward all possible 
 choices 
 in 
 order 
 to 
 be 
 able 
 to 
 reportallmatches.Thetechniquedescribedherecanbeviewedasanextensionofthe
 LR
 (0)parsingstrat
 egy and is based on the work reported in Shankar et al. [40] and Madhavan et al. [34]. Let
  G
 ′
 be 
 the context-free grammar obtained by replacing all right-hand sides of productions of
  G
  by 
 postorder listings of the corresponding trees in
  TREES
 (
 A
  ∪
  N
 ). Note that
  G
  is a regular tree 
 grammar whose associated language contains trees, whereas
  G
 ′
 is a context-free grammar whose 
 language contains strings with symbols from 
 A
 . Of course, these strings are just the linear 
 encodings of trees.
  
 Let
  post
 (
 t
 ) denote the postorder listing of the nodes of a tree
  t
 . The following (rather obvious) 
 claim underlies the algorithm:
  
 procedure
  TopDownTraverse
  (
 node, nonterminal
 ) 
  
 if
  node
  is a leaf
  then 
  
  
 if
  node.state
 [
 nonterminal
 ].
 rule
  is
  r
  :
  X
  →
  Y
 ,
  Y
  ∈
  N
  then 
  
  
 append case number of
  r
  to
  node.caselist 
  
  
 TopDownTravese
 (
 node, Y
 ) 
  
  
 else 
  
  
 if
  node.state
 [
 nonterminal
 ].
 rule
  is
  r
  :
  X
  →
  a
 ,
  a
  ∈
  T
  then 
  
   
 append case number of
  r
  to
  node.caselist 
  
   
 execute actions in
  node.caselist
  in reverse order 
  
  
 end if 
  
  
 end if 
  
 else 
  
  
 if
  node.state
  [
 nonterminal
 ].
 rule
  is
  r
  :
  X
  →
  Y
 ,
  Y
  ∈
  N
  then 
  
  
 append case number of
  r
  to
  node.caselist 
  
  
 TopDownTraverse
  (
 node, Y
 ) 
  
  
 else 
  
  
 if
  node.state
 [
 nonterminal
 ].
 rule
  is
  r
  :
  X
  →
  op
  (
 X
 1
 ,
  X
 2
 , . . .
  X
 k
 )
  then 
  
   
 append case number of
  r
  to
  node.caselist 
  
   
 for
  i
  = 1 to
  k
  do 
  
    
 TopDownTraverse
  (
 child
  (
 i, node
 ),
  X
 i
 ) 
  
   
 end for 
  
  
 end if 
  
  
 execute actions in
  node.caselist
  in reverse order 
  
  
 end if 
  
 end if 
  
 end procedure",NA
17.6,NA,NA
Related Issues,"A question that arises when generating a specification for a particular target architecture is the 
 following: Can a specification for a target machine produce code for every possible intermediate 
 code tree produced by the front-end? (We assume here, of course, that the front-end generates a 
 correct intermediate code tree.) This question has been addressed by Emmelmann [16], who 
 refers to the property that is desired of the specification as the
  completeness
  property. The 
 problem reduces to one of containment of the language 
 L
 (
 T
 ) of all possible intermediate code 
 trees in
  L
 (
 G
 ), the language of all possible trees generated by the regular tree grammar 
 constituting the specification. Thus, the completeness test is the problem of testing the subset 
 property of two regular tree grammars, which is decidable. An algorithm is given in [16].
  
 A second question has to do with whether a code-generator generator designed to compute 
 normalized costs statically terminates on a given input. If the grammar is such that relative costs 
 diverge, the code-generator generator will not halt. A sufficient condition for ensuring that code-
 generator generators based on extensions of LR parsing techniques halt on an input specification 
 is given in [34]. However, it is shown that there are specifications that can be handled by the tool 
 but that fail the test.
  
 An important issue is the generation of code for a directed acyclic graph (DAG) where shared 
 nodes 
 representcommonsubexpressions.TheselectionofoptimalcodeforDAGshasbeenshowntobeintract
 able [5], but there are heuristics that can be employed to generate code [6]. The labeling phase of 
 a bottom-up tree parser can be modified to work with DAGs. One possibility is that the code 
 generator could, in the top-down phase, perform code generation in the normal way but count 
 visits for each node. For the first visit it could evaluate the shared subtree into a register and 
 keep track of the register assigned. On",NA
17.7,NA,NA
Conclusion and Future Work,"We have described various techniques for the generation of instruction selectors from 
 specifications in the form of tree grammars. Top-down, bottom-up, and LR-parser-based 
 techniques have been described in detail. Instruction selection using the techniques described in 
 this chapter is useful and practical, but",NA
Acknowledgments,"The author gratefully acknowledges the assistance of J. Surya Kumari, Balvinder Singh, and V. 
 Suresh in the preparation of the figures.
  
 References
  
 1. A. V. Aho and M. J. Corasick. 1975. Efficient string matching: An aid to bibliographic search. 
  
 Communications of the ACM
  18(6):333–40.
  
 2. A. V. Aho and M. Ganapathi. 1985. Efficient tree pattern matching: An aid to code generation. 
 In 
 Proceedings of the 12th ACM Symposium on Principles of Programming Languages
 , 334–
 40. New York: ACM Press.
  
 3. A. V. Aho, M. Ganapathi, and S. W. Tjiang. 1989. Code generation using tree matching and 
 dynamic 
  
 programming.
  ACM Transactions on Programming Languages and Systems
  
 11(4):491–516.
  
 4. A. V. Aho and S. C. Johnson. 1976. Optimal code generation for expression trees.
  Journal of the 
 ACM 
  
 23(3):146–60.
  
 5. A. V. Aho, S. C. Johnson, and J. D. Ullman. 1977. Code generation for expressions with common 
  
 subexpressions.
  Journal of the ACM
  24(1):21–28.
  
 6. A. V. Aho, R. Sethi, and J. D. Ullman. 1986.
  Compilers: Principles, techniques, and tools
 . Reading 
 MA: 
  
 Addison Wesley.
  
 7. A. W. Appel. 1987. Concise specifications of locally optimal code generators. Technical 
 Report 
  
 CS-TR-080-87, Department of Computer Science, Princeton University, Princeton, 
 NJ.
  
 8. G. Araujo, S. Malik, and M. Lee. 1996. Using register transfer paths in code generation for 
 het-erogenous memory-register architectures. In
  Proceedings of the
  33
 rd Design 
 Automation Conference (DAC)
 : IEEE CS Press.
  
 9. A. Balachandran, D. M. Dhamdhere, and S. Biswas. 1990. Efficient retargetable code 
 generation 
  
 using bottom up tree pattern matching.
  Computer Languages
  3(15):127–40.
  
 10. John Boyland and H. Emmelmann. 1991. Discussion: Code generator specification 
 techniques. In 
 Code-generation—Concepts, tools, techniques
 . Workshops in computing 
 series, ed. R. Giegerich and S. L. Graham, 66–69. Heidelberg, Germany: Springer-Verlag.
  
 11. D. G. Bradlee. 1991. Retargetable instruction scheduling for pipelined processors. PhD thesis, 
 Uni-
  
 versity of Washington, Seattle, WA.
  
 12. R. G. G. Cattell. 1980. Automatic derivation of code generators from machine descriptions.
  
 ACM 
  
 Transactions on Programming Languages and Systems
  2(2):173–90.",NA
18 ,NA,NA
A Retargetable Very ,NA,NA
Long Instruction Word ,NA,NA
Compiler Framework ,NA,NA
for Digital Signal ,NA,NA
Processors,"18.1 Introduction
  ........................................
  18
 -
 1
  
 18.2 Digital Signal Processor Architectures
  ................
  18
 -
 2
  
 Fixed Operation Width Digital Signal Processor Architectures
 •
  
 Variable Operation Width Digital Signal Processor Architectures
  
 18.3 Compilation for Digital Signal Processors
  ............
  18
 -
 5
  
 Operation Instruction Level Parallelism Constraints
  
 •
  Operand Instruction Level Parallelism Constraints
  
 18.4 Retargetable Compilation
  ...........................
  18
 -
 7
  
 Automatic Retargetability
 •
  User Retargetability
 •
  Developer
  
 Subramanian Rajagopalan 
  
 Synopsis (India) EDA Software Pvt. Ltd., 
 Bangalore, India 
  
 Subramanian.Rajagopalan@synopsys.com
  
 Sharad Malik 
  
 Department of Electrical Engineering, 
 Princeton University, Princeton, NJ 
 sharad@ee.princeton.edu",NA
18.1 Introduction,"Retargetability
  
 18.5 Retargetable Digital Signal Processor Compiler 
  
  
 Framework
  .........................................
  18
 -
 10 
  
 Instruction Selection
 •
  
 Offset Assignment Problem
  
  
 •
  Reference Allocation
 •
  Register Allocation for 
  
  
 Irregular Operand Constraints
 •
  Irregular ILP 
  
  
 Constraints
 •
  Array Reference Allocation
 •
  Addressing Modes 
  
 18.6 Summary
  ...........................................
  18
 -
 25 
 References
  
 ................................................
  18
 -
 25
  
 Digitalsignalprocessors(DSPs)areusedinawidevarietyofembeddedsystemsrangingfromsafety-
 critical flight navigation systems to common electronic items such as cameras, printers, and 
 cellular phones. DSPs are also some of the more popular processing element cores available in 
 the market today for use in system-on-a-chip (SOC)-based design for embedded processors. 
 These systems not only have to meet the real-time constraints and power consumption 
 requirements of the application domain, but also need to adapt to the fast-changing applications 
 for which they are used. Thus, it is very important that the target processor be well matched to 
 the particular application to meet the design goals. This in turn requires that DSP compilers 
 produce good-quality code and be highly retargetable to enable a system designer to quickly 
 evaluate different architectures for the application on hand.
  
 18
 -
 1",NA
18.2,NA,NA
Digital Signal Processor Architectures,"A close examination of DSP architectures and the requirements for a DSP compiler suggests that 
 DSPs can be modeled as very long instruction word (VLIW) processors. A VLIW processor is a 
 fully statically scheduled processor capable of issuing multiple operations per cycle. Figure 18.1 
 shows a simplified overview of the organization of instruction set architectures (ISAs) of VLIW 
 processors.
  
 The ISA of a VLIW processor is composed of multiple instruction templates that define the sets 
 of operations that can be issued in parallel. Each slot in an instruction can be filled with one 
 operation from a set of possible operations. Each operation consists of an opcode and a set of 
 operands. The opcode of an operation defines the operation’s resource usage when it executes. 
 An operand in an operation can be a register, a memory address, or an immediate operand. The 
 register operand in an operation can be a single machine word register, a part of a machine word 
 register, or a set of registers. Although the memory address is shown as a single operand in 
 Figure 18.1, in general, the address itself can be composed of multiple operands depending on 
 the addressing mode used in the operation. DSPs have statically determined ILP and thus are 
 specific instances of VLIW compilers, albeit with special constraints.
  
 As compilers for VLIW processors are well studied, they serve as useful starting points for 
 developing compilers for DSP processors. There is one major point of difference, though. 
 Compilers for VLIW architectures need optimizations to exploit the application’s instruction level 
 parallelism (ILP) to primarily
  
 ISA
  
 VLIW Inst. Template 
  
 VLIW Inst. Template VLIW Inst. Template
  
 Operation Operation 
  
 Operation
  
 Opcode 
  
 Operands Opcode 
  
 Operands
  
 Res-table 
  
 Address 
  
 Register",NA
18.3,NA,NA
Compilation for Digital Signal Processors,"In this section, some of the constraints posed by VOW DSP architectures to compilers are 
 described using an example from [49]. Some common DSP ISA constraints are shown in Figure 
 18.3, where the constraints between the different entries are represented in dotted lines. Figure 
 18.3a shows operation ILP constraints in the instruction template that restrict the set of 
 operations that can be performed in parallel.
  
  
 Operation 
  
 VLIW Inst. Template
  
 Operation
  
 Operation
  
 Operation
  
 Opcode
  
 Operands
  
 Register
  
 (a)
  
 Register
  
 Address
  
 (b)",NA
18.4,NA,NA
Retargetable Compilation,"Section 18.2 described the architectural features of DSPs that were designed by hardware 
 architects with the intent of meeting application demands, and Section 18.3 described the 
 constraints posed by the architectures for compiler developers that are in addition to traditional 
 compiler 
 issues. 
 While 
 the 
 compiler 
 developersneedacleanabstractionofthearchitecturethatallowsthemtodevelopreusableorsynthesiz
 able compilers that do not need to know the actual architecture they are compiling for, the 
 hardware architects need a configurable abstraction of the compiler that allows them to quickly 
 evaluate the architectural feature that they have designed without much knowledge of how the 
 feature 
 is 
 supported 
 by 
 the 
 compiler. 
 Traditionally,thecompilerhasbeensplitintotwophases,namely,thefrontendthatconvertstheapplica
 tion program into a semantically equivalent common intermediate representation and the back 
 end that takes the intermediate representation as input and emits the assembly code for the",NA
18.5,"FIGURE 18.6
  
 Retargetable compiler methodology.",NA
Retargetable Digital Signal Processor Compiler Framework,"In this section, the flow of a retargetable DSP compiler for an architecture with various constraints
  
 mentioned in Section 18.3 is described. The DSP code generator flow is shown in Figure 18.7. Because the
  
 compiler flow shown in Figure 18.7 is very similar to a general-purpose compiler flow, only the issue of
  
 C 
  
 Front End
  
 IR
  
 Machine-Independent 
  
 Optimizations
  
 Operation Selection 
  
 Reference Allocation 
  
 Compaction 
  
 Register Allocation 
  
 DSP Optimization 
  
 Assembly Emission 
  
 Back End
  
 Assembly 
  
 FIGURE 18.7 
  
 DSP code generator flow.",NA
18.6,NA,NA
Summary,"DSPs are a significant fraction of the processor market and, in particular, low-cost embedded 
 DSPs are in a wide variety of products and applications of daily use. Hence, it is important not 
 only to design the necessary tools to program these processors but also to develop tools that can 
 help in an automatic cooperative synthesis of both the architecture and the associated software 
 tool-set for a given set of applications. A first step in this direction is the design of a highly 
 reusable and retargetable compiler for DSPs. In this chapter, DSP architectures were first 
 classified into two classes: FOW DSPs and VOW DSPs. Although DSPs in the former category are 
 more regular with orthogonal instruction sets, the ones in the latter category are the low-end 
 processors with highly optimized irregular architectures. VOW DSPs are also more cost sensitive 
 and require special algorithms to exploit their hardware features and thus are the focus of this 
 chapter. Sections 18.2 and 18.3 explain the consequences of a VOW-based design and point out 
 the nature of constraints that a DSP compiler must solve. In Section 18.4, retargetable 
 compilation is classified into three categories: automatic retargetability, user retargetability, and 
 developer retargetability, depending on the level of automation, the level of user interaction in 
 retargeting, and the level of architecture coverage provided by the compiler framework. 
 Retargetability is a key factor in design automation and reduction of time to market. A potential 
 approach for retargetable compiler development is also presented along with some qualitative 
 measures.
  
 Finally, in Section 18.5, some DSP architecture–specific issues that a compiler framework 
 developer must solve to achieve a high level of retargetability and good efficiency were 
 discussed. Some of the common DSP optimizations that have been developed were presented. In 
 addition to the architecture constraints described, new features are available such as media 
 operations that usually pack multiple identical operations into a single operation operating on 
 smaller bit-width operands. Hence, DSP compiler frameworks also need to be extensible in that it 
 should be easy to add new architectural features and new algorithms to exploit such features to 
 the framework.
  
 References
  
 1. A. Aho, M. Ganapathi, and S. Tjiang. 1989. Code generation using tree matching and dynamic 
  
 programming.
  ACM Transactions on Programming Languages and Systems
  11(4):491–516.
  
 2. A. V. Aho, R. Sethi, and J. D. Ullman. 1988.
  Compilers: Principles, techniques, and tools
 . Reading, 
  
 MA: Addison-Wesley.
  
 3. A. W. Appel and L. George. 2001. Optimal spilling for CISC machines with few registers. In
  
 Pro-ceedings of the ACM SIGPLAN Conference on Programming Language Design and 
 Implementation
 , 243–53.
  
 4. G. Araujo. 1997. Code generation algorithms for digital signal processors. PhD thesis, 
 Princeton 
  
 University, Princeton, NJ.
  
 5. G. Araujo and S. Malik. 1970. Code generation for fixed-point DSPs.
  ACM Transactions on 
 Design 
  
 Automation of Electronic Systems
  3(2):136–61.
  
 6. G. Araujo, A. Sudarsanam, and S. Malik. 1996. Instruction set design and optimizations for 
 ad-dress computation in DSP processors. In 9
 th International Symposium on Systems 
 Synthesis
 , 31–37. Washington, DC: IEEE Computer Society Press.
  
 7. D. I. August, D. A. Connors, S. A. Mahlke, J. W. Sias, K. M. Crozier, B.-C. Cheng, P. R. Eaton, Q. B. 
 Olaniran, and W. W. Hwu. 1998. Integrated predicated and speculative execution in the 
 IMPACT EPIC architecture. In
  Proceedings of the 25th International Symposium on Computer 
 Architecture
 . Washington, DC: IEEE Computer Society Press.",NA
19 ,NA,NA
Instruction,NA,NA
Scheduling,"19.1
  
 19.2
  
 19.3
  
 19.4
  
 19.5
  
 19.6
  
 Introduction
  ........................................
  19
 -
 1
  
 Background
 .........................................
  19
 -
 3
  
 Definitions
 •
  Directed Acyclic Graph
  •
  Performance Metrics
  
 for Scheduling Methods
  
 Instruction Scheduling for RISC Architectures
  .......
  19
 -
 7
  
 Architecture Model
 •
  A Simple Instruction Scheduling
  
 Method
 •
  Combined Code Generation and Register
  
 Allocation Method
 •
  Other Pipeline Scheduling Methods
  
 Basic Block Scheduling
  ..............................
  19
 -
 14
  
 Preliminaries
 •
  List Scheduling Method
 •
  Operation
  
 Scheduling Method
 •
  An Optimal Instruction Scheduling
  
 Method
 •
  Resource Usage Models
 •
  Case Study
  
 Global Scheduling
 ...................................
  19
 -
 27
  
 Global Acyclic 
 Scheduling
  
 •
  Cyclic Scheduling
  
 Scheduling and Register Allocation
  ..................
  19
 -
 37
  
 Phase Ordering 
 Issues
  
 •
  Integrated Methods
  
 •
  Phase
  
 Ordering in Out-of-Order Issue Processors
  
 R. Govindarajan 
  
 Supercomputer Education and 
  
 Research Centre, Department of 
  
 Computer Science and Automation, 
 Indian Institute of Science, 
  
 Bangalore, India; 
  
 govind@csa.iisc.ernet.in",NA
19.1 Introduction,"19.7
  
 19.8
  
 Recent Research in Instruction Scheduling
  ...........
  19
 -
 43 
 Instruction Scheduling for Application-Specific Processors
 •
  
 Instruction Scheduling for IA-64
 •
  Instruction Scheduling for 
  
 Spatial Architectures
 •
  Instruction Scheduling for Low Power 
  
 Summary
  ...........................................
  19
 -
 47
  
 Acknowledgments
  .........................................
  19
 -
 48 
 References
  
 .................................................
  19
 -
 48
  
 Ever since the advent of reduced instruction set computers (RISC) [67, 122] and their pipelined 
 execution of instructions, instruction scheduling techniques have gained importance as they 
 rearrange instructions to “cover” the delay or latency that is required between an instruction and 
 its dependent successor. Without such reordering, pipelines would stall, resulting in wasted 
 processor cycles. Pipeline stalls would also occur while executing control transfer instructions, 
 such as branch and jump instructions. In architectures that support
  delayed branching
 , where the 
 control transfers are effected in a
  delayed
  manner [68], instruction reordering is again useful to 
 cover the stall cycles with useful instructions. Instruction scheduling can be limited to a single
  
 basic block
  — a region of straight line code with a single point of entry and a single point of exit, 
 separated by decision points and merge points [3, 73, 107] — or to multiple basic blocks. The 
 former is referred as
  basic block scheduling
 , and the latter as
  global scheduling
  [3, 73, 107].
  
 19
 -
 1",NA
19.2,NA,NA
Background,"In this section we review the relevant background. The following subsection presents a number 
 of defini-tions. In Section 19.2.2 we describe a representation for data dependences, used by 
 instruction scheduling methods, and its construction. Last, we discuss various performance 
 metrics for instruction scheduling methods in Section 19.2.3.
  
 Before we proceed further, let us clarify the use of various notations in the programming 
 examples. We use
  t1
 ,
  t2
 , and so on to represent temporaries or symbolic registers, and
  r1
 ,
  
 r2
 , and so on to represent (logical) registers assigned to temporaries. Symbols, such as
  x
 ,
  y
  and
  
 a
 ,
  b
  represent variables stored in memory locations.
  
 19.2.1 
  
 Definitions
  
 Two instructions
  i1
  and
  i2
  are said to be data dependent on each other if they share a 
 common operand (register or memory operand), and the shared operand appears as a 
 destination operand
 1
 in at least one of the instructions [3, 73, 107]. Consider the following 
 sequence of instructions:
  
 i1
 : 
  
 i2
 : 
  
 i3
 :
  
 r1
  ←
  load (r2) 
  
 r3
  ←
  r1 + 4 
  
 r1
  ←
  r4 - r5
  
 Instruction
  i2
  has r1 as one of its source operands, which is written by
  i1
 . This dependence 
 from
  i1
  to
  i2 
 is said to be a
  flow dependence
  or
  true data dependence
 . Thus, in any legal 
 execution of the above sequence, the operand read of register r1 in instruction
  i2
  must take 
 place after the result value of instruction
  i1
  is written. The dependence between instructions
  
 i2
  and
  i3
  due to register r1 is an
  anti-dependence
 . Here, instruction
  i3
  must write the result 
 value in r1 only after
  i2
  has read its operand from r1. Last, there is an 
 output-dependence
  
 between instructions
  i1
  and
  i3
 , where the order in which they write to the destination must 
 be the same as the program order (i.e.,
  i1
  before
  i3
 ) for correct program behavior.
  
 1
 If the shared operand appears as a source operand in both instructions, then there is an
  input-dependence
  
 between the two instructions. Since an input-dependence does not constrain the execution order, we do not 
 consider this any further in our discussion.",NA
19.3,NA,NA
Instruction Scheduling for RISC Architectures,"In this section we discuss early instruction scheduling methods proposed for handling pipeline 
 stalls. First we present a simple, generic architecture model and the need for instruction 
 scheduling in this architecture model. In Section 19.3.2 we present the instruction scheduling 
 method developed by Gibbons and Muchnick [52] in detail. An alternative approach that 
 combines register allocation and scheduling for pipeline stalls is discussed in Section 19.3.3. Last, 
 a brief review of other instruction schedul-ing methods is presented in Section 19.3.4.
  
 19.3.1 
  
 Architecture Model
  
 In a simple RISC architecture instructions are executed in a pipelined manner. Instruction 
 execution 
 in 
 a 
 simplefive-
 stageRISCpipelineisshowninFigure19.3.Briefly,theinstructionfetch(IF)stageisresponsible 
 for 
 fetching instructions from memory. Instruction decode and operand fetch takes place in the 
 decode (ID) stage. In the execute stage (EX), the specified operation is performed; for memory 
 operations, such as load or store, address calculation takes place in this stage. The memory stage 
 (MEM) is for load and store operations. Finally in the write-back stage (WB), the result of an 
 arithmetic instruction or the value loaded from memory for a load instruction is stored in the 
 destination register.
  
 Let instruction
  (i+1)
  be dependent on instruction
  i
 ; that is,
  (i+1)
  reads the result 
 produced by instruction
  i
 . It can be seen that instruction
  i+1
  may read the operand value (in 
 the 
 ID 
 stage) 
 before 
 instruction
 i
 completes,thatis,before
 i
 finishesthewrite-
 backinthedestinationregister.Thisdependence should cause the execution of instruction
  (i+1)
  
 to stall until instruction
  i
  writes the result, to ensure correct program behavior. This is known 
 as a
  data hazard
  [68].
  
 Another situation that may warrant stalls in the pipeline is due to control hazards [68]. If 
 instruction
  i 
 is a control transfer instruction, such as a conditional branch, unconditional 
 branch, subroutine call, or
  
 IF
  
 ID
  
 EX
  
 MEM
  
 WB
  
 (a) A Simple Pipeline
  
 Time
  
  
  
 t
  
 t
  + 1
  
 t
  + 2
  
 t
  + 3
  
 t
  + 4
  
 t
  + 5
  
 t
  + 6
  
 t
  + 7
  
  
 i 
  
 i+1 
 i+2 
 i+3 
 i+4
  
 IF
  
 ID
  
 EX
  
 MEM
  
 WB
  
 WB
  
 WB
  
 WB
  
 IF
  
 ID
  
 EX
  
 MEM
  
 IF
  
 ID
  
 EX
  
 MEM
  
 IF
  
 ID
  
 EX
  
 MEM
  
 IF
  
 ID
  
 EX
  
 MEM
  
  
  
 (b) Pipelined Execution",NA
19.4,NA,NA
Basic Block Scheduling,"Instruction scheduling can be broadly classified based on whether the scheduling is for a single 
 basic block, multiple basic blocks, or control flow loops involving single or multiple basic blocks 
 [132]. Algorithms that schedule single basic blocks are termed
  local scheduling
  algorithms and 
 are the topic of discussion in this section. Algorithms that deal with multiple basic blocks or basic 
 blocks with cyclic control flow are
  
  
 3
 In this sequence, because of the additional unary store operation, we have
  R
 , instead of (
 R
  − 1), arithmetic 
 operations at the end of the sequence.",NA
19.5,NA,NA
Global Scheduling,"Instruction scheduling within a basic block has limited scope, as the average size of a basic block 
 is quite small, typically in the range of 5 to 20 instructions. Thus, even if the basic block 
 scheduling method produces optimal schedules, the performance, in terms of the exploited ILP, is 
 low. This is especially a serious concern in architectures that support greater ILP, for example, 
 VLIW architectures with several functional units or superscalar architectures that can issue 
 multiple instructions every cycle. The reason for the low ILP, especially near the beginning and 
 end of basic blocks, is that basic block boundaries act like barriers, not allowing the movement of 
 instructions past them.
  
 Global instruction scheduling techniques, in contrast to local scheduling, schedule instructions 
 beyond basic blocks, that is, overlapping the execution of instructions from successive basic 
 blocks. 
 These 
 global 
 schedulingmethodsareeitherforasetofbasicblockswithacycliccontrolflowamongthem[36, 46, 74, 
 102] or for single or multiple basic blocks of a loop [25, 31, 49, 88, 133]. The former case is 
 referred 
 to 
 as
  
 global 
 acyclicscheduling
 andthelatteras
 cyclicscheduling
 .First,wediscussafewglobalacyclicschedulingmeth
 ods. Section 19.5.2 deals with cyclic scheduling.
  
 19.5.1 
  
 Global Acyclic Scheduling
  
 Early global scheduling methods performed local scheduling within each basic block and then 
 tried to move instructions from one block to an empty slot in a neighboring block [25, 162]. 
 However, these methods followed an ad hoc approach in moving instructions. Furthermore, the 
 local compaction or scheduling that took place in each of the blocks resulted in several 
 instruction movements (reorderings) that were done without understanding the opportunities 
 available in neighboring blocks. Hence, some of these reorderings may have to be undone to get 
 better performance. In contrast, global acyclic scheduling methods, such as
  trace scheduling
  [46],
  
 percolation scheduling
  [111],
  superblock scheduling
  [74],
  hyperblock scheduling
  [102], and
  region 
 scheduling
  [63], take a global view in scheduling instructions from different basic blocks. In the 
 following subsections we describe these approaches.
  
 19.5.1.1 Trace Scheduling
  
 Trace scheduling attempts to minimize the overall execution time of a program by identifying 
 frequently executed
  traces
  — acyclic sequences of basic blocks in the control flow graph — and 
 scheduling the instructions in each trace as if they were in a single basic block. The trace 
 scheduling method identifies the most frequently executed trace, a single path in the control flow 
 graph, by identifying the unscheduled basic block that has the highest execution frequency; the 
 trace is then extended forward and backward along the most frequent edges. The frequency of 
 edges and basic blocks are obtained by a linear combination of branch probabilities and loop trip 
 counts obtained either through heuristics or through profiling [13]. Various profiling methods 
 are discussed in greater detail in [60].
  
 The instructions in the selected trace (including branch instructions) are then scheduled using 
 a 
 list 
 schedulingmethod.Theobjectiveoftheschedulingistoreducetheschedulelengthandhencetheexecuti
 on time of the instructions in the trace. During the scheduling, instructions can move above or 
 below a branch instruction. Such movement of instructions may warrant compensation code to 
 be inserted at the beginning or end of the trace. After scheduling the most frequently executed 
 trace, the next trace (involving unscheduled basic blocks) is selected and scheduled. This process 
 continues until all the basic blocks are considered.
  
 Let us illustrate the trace scheduling method with the help of the example code shown in 
 Figure 19.17a, adapted from [73]. The instruction sequence and the control flow graph for the 
 code are shown in Figures 19.17b and 19.17c. Consider a simple two-way issue architecture with 
 two integer units. Let us assume that the latency of an integer instruction, such as an
  add
 ,
  sub
 , 
 or
  mov
  instruction, is one cycle, and that of a
  load
  instruction is two cycles. Thus, there is a",NA
19.6,NA,NA
Scheduling and Register Allocation,NA,NA
19.7,NA,NA
Recent Research in Instruction Scheduling,"In this section we report some of the recent research work on instruction scheduling.
  
 19.7.1 
  
 Instruction Scheduling for Application-Specific Processors
  
 Instruction scheduling methods have been proposed for application-specific processors such as 
 digital signal processing (DSP). Originally, most DSP applications, or their important kernels, 
 were hand-coded in assembly language. The application programmer is required to perform the 
 necessary instruction re-ordering to take full advantage of the parallelism that is available in 
 these processors. With the increasing complexity of the processors and their programmability, 
 programming in higher-level languages and compilation techniques to produce efficient code 
 automatically are becoming increasingly important. A major challenge in applying existing 
 instruction scheduling methods arises from the irregularities of DSP processors [92]. These 
 irregularities include having special-purpose registers in the data path, heteroge-neous registers, 
 dedicated memory address generation units, chained instructions such as multiply-and-
 accumulate, saturation arithmetic, multi-stage functional units, and parallel memory banks. 
 Furthermore, in most cases, code running on a DSP’s processor also has to meet real-time 
 constraints. Thus, instruction scheduling for DSP processors, which needs to take into account 
 the irregularities of the architectures and the real-time constraints in resource usage, poses a 
 major challenge.",NA
19.8,NA,NA
Summary,"Instruction scheduling methods rearrange instructions in a code sequence to expose ILP for 
 multiple instruction issue processors and to reduce the number of stall cycles incurred in single-
 issue, pipelined processors. Simple scheduling methods that cover pipeline stalls use information 
 on the number of stall cycles required between dependent instructions. Basic block instruction 
 scheduling methods are limited to rearranging instructions in a straight line code sequence with 
 a single control flow entry and exit. In",NA
20 ,NA,NA
Advances in Software,NA,NA
Pipelining,"20.1
  
 Introduction
  ........................................
  20
 -
 1
  
 A Historical Perspective
  
 Hongbo Rong
  
 20.2
  
 Software Pipelining for Single Loops
  .................
  20
 -
 4
  
 A Motivating Example
 •
  Background
 •
  A Heuristic Modulo
  
 Scheduling Framework
 •
  An Optimal Modulo Scheduling
  
 Formulation
 •
  Register Allocation and Code Generation
  
 20.3
  
 Advances in Single-Loop Software Pipelining
 .........
  20
 -
 26
  
 Microsoft Corporation,
  
 Redmond, WA
  
 Register-Constrained Software Pipelining
 •
  Modulo Scheduling
  
 and Retiming
 •
  Power-Aware Software Pipelining
 •
  Software
  
 hongbor@microsoft.com
  
  
 R. Govindarajan 
  
 Supercomputer Education and 
  
 Research Centre, Department of 
  
 Computer Science and Automation, 
 Indian Institute of Science, 
  
 Bangalore, India; 
  
 govind@csa.iisc.ernet.in",NA
20.1 Introduction,"Pipelining for New Architectures
  
 20.4
  
 Software Pipelining for Nested Loops
  ................
  20
 -
 41
  
 Basic 
 Concepts
  
 •
  Overview
  
 •
  Single-Dimension Software
  
 Pipelining
  
 20.5 
  
 Summary and Future Directions
  .....................
  20
 -
 64 
 Acknowledgments
  .........................................
  20
 -
 67 
 References
  
 .................................................
  20
 -
 67
  
 Manystate-of-the-
 artprocessorshavemultiplefunctionalunitsandexecuteseveralinstructionssimultane-ously 
 to 
 exploit
  instruction-level parallelism
  (ILP) [97]. The ILP architectures include very long instruction 
 word (VLIW) architectures [27, 43, 88], superscalar processors [56, 97, 98], and explicitly 
 parallel instruc-tion computing (EPIC) architectures [96]. For these architectures, it is important 
 for the compiler to expose parallelism in the application to the underlying hardware. The 
 compiler assists the hardware by statically scheduling independent instructions in the same time 
 step, scheduling dependent instructions apart to satisfy dependences, and mapping the 
 instructions onto appropriate hardware resources. This role is critical for VLIW and EPIC 
 architectures, which fully rely on the compiler to expose parallelism. In these architectures, the 
 compiler identifies and packs a set of independent instructions into a single long word 
 instruction and communicates it to the hardware. The hardware fetches and decodes the long 
 word instruction and executes the instructions in it in parallel, without being required to check 
 the dependences between them. For superscalar processors, although the hardware dynamically 
 identifies independent in-structions and issues them to the resources for execution in parallel, 
 the ability is limited by the size of the instruction window, from which instructions are 
 scheduled, and its hardware complexity. With instruc-tions scheduled according to their 
 dependences and mapped to appropriate resources by the compiler, the chances of finding 
 independent instructions at runtime are increased, while the chances of stalling instruction issue 
 resulting from nonavailability of dependent values and/or resources are decreased.
  
 20
 -
 1",NA
20.2,NA,NA
Software Pipelining for Single Loops,"This section reviews the basic scheduling, register allocation, and code generation techniques for 
 software pipelining of single loops. We focus on modulo scheduling for its simplicity and 
 usefulness. We assume that the loop body does not contain any conditional or function call. While 
 conditionals can be converted into linear code via IF-conversion [10] and predicated execution 
 [88], function calls essentially prevent a loop from being software pipelined.
  
 We first illustrate modulo scheduling with a simple example in Section 20.2.1 and present the 
 required background knowledge in Section 20.2.2. For the resource-constrained modulo 
 scheduling problem, we present a heuristic solution as a generic modulo scheduling framework 
 in Section 20.2.3 and an optimal solution via integer linear programming in Section 20.2.4. For 
 the obtained modulo schedule, the register allocation and code generation techniques, with and 
 without hardware support, are then described in Section 20.2.5.
  
 20.2.1 
  
 A Motivating Example
  
 Figure20.1ashowsasingleloop.ItsintermediaterepresentationisshowninFigure20.1b,wherea
 tempo
 rary name
  (TN) represents a variable. In this example, there are four variables, TN1 to TN4. We 
 use the notation TN
 {d}
  to refer to the TN value defined
  d
  iterations before. For example, TN2
 {
 1
 }
  is 
 the TN2 value defined in the previous loop iteration, and TN2
 {
 2
 }
  is the TN2 value defined in the 
 previous previous loop iteration. 
 ThedatadependencegraphfortheloopisshowninFigure20.1c,whereanoderepresentsaninstruction, 
 and a directed arc between a pair of nodes represents a data dependence between the two",NA
20.3,NA,NA
Advances in Single-Loop Software Pipelining,"Substantial progress has been made in software pipelining in the past decade. In the context of 
 traditional uniprocessor architectures, efforts to construct software pipelined schedules that 
 require no more than the available registers have been made. There has also been an interesting 
 discovery on the relationship between software pipelining and circuit retiming [17], a hardware 
 pipeline design technique.
  
 The past decade has also seen fundamental changes in the direction of architectural design. 
 Power consumption has become a major design constraint for high-performance and embedded 
 systems. As aggressive ILP techniques applied to traditional architectures result in diminishing 
 returns and technology constraints limit the scalability of the large monolithic hardware 
 architecture, newer complexity-effective architectures, for example, clustered VLIW and multi-
 core architectures, have emerged. To meet the demands of these newer architectures and 
 constraints, software pipelining has been adapted appropriately.
  
 Inthissection,wesurveytheserecentadvancesinsoftwarepipelining.First,wediscusstheprogressi
 nthe traditional uniprocessor context, namely register-constrained software pipelining (Section 
 20.3.1), the link between software pipelining and circuit retiming (Section 20.3.2), and power-
 aware software pipelining (Section 20.3.3). Section 20.3.4 deals with extensions to software 
 pipelining for newer architectures.
  
 20.3.1 
  
 Register-Constrained Software Pipelining
  
 Althoughsoftwarepipeliningandregisterallocationarecoupledproblems,theyareusuallyaddressedi
 nde-pendently for simplicity. As a result, though a number of software pipelining methods 
 considered reducing the register requirement as an additional objective [28, 34, 51, 54, 64], they 
 still do not explicitly consider the number of available registers and limit the requirements of the 
 constructed schedule to the available registers. This has resulted in scheduling being performed 
 without considering register constraints, that is, assuming that an infinite number of registers are 
 available, and then register allocation is attempted for the constructed schedule. In case the 
 schedule requires more registers than are available from the target processor, earlier approaches 
 follow one of the two options:
  posteriori spilling
  or
  rescheduling with a larger II
 . Recent advances,",NA
20.4,NA,NA
Software Pipelining for Nested Loops,"In this section, we discuss various approaches for software pipelining of nested loops. Nested 
 loops are important, as they contribute to most of the execution time of scientific applications.
  
 Software pipelining of nested loops is more complicated than that of single loops, as the operation 
 instances in the
  n
 -dimensional iteration space need to be distributed to distinct time steps, such 
 that resources are not overcommitted at any time step, dependences are satisfied, the total 
 number of time steps is minimized, and there is a repeating pattern(s) in the schedule to enable 
 loop rewriting for code generation. A simple approach is to only software pipeline the innermost 
 loop and execute the outer loops sequentially. Unfortunately, this approach may not always result 
 in good performance for one or more of the following reasons: 
  
   
 r
  The innermost loop may have a tight recurrence, due to which the amount of ILP exploited 
 is 
   
 r
  The trip count of the innermost loop is low, due to which the kernel is repeated only a few 
 times 
   
 r
  The data locality exhibited by the innermost loop may not exploit the cache architecture 
 of the system efficiently, resulting in poor overall performance.
  
 Thus, it is important to arrive at other techniques for software pipelining nested loops that can 
 better exploit parallelism and data locality for higher performance.
  
 This section introduces several approaches for such a purpose. We survey some of the 
 traditional approaches, and study two new methods, unroll-and-squash [79], and single-
 dimension software pipelin-ing [92], in more detail. The former combines a loop transformation 
 with software pipelining and employs general techniques to reduce code size in a software 
 pipelined schedule. The latter is interesting because of its generality and efficiency; it subsumes 
 the classical modulo scheduling as a special case and generates a schedule with the shortest 
 computation time compared to traditional modulo scheduling approaches un-der identical 
 conditions. It also extends the traditional hyperplane scheduling [29, 61] to handle resource 
 constraints. The approach has systematically addressed all of the following: scheduling, register 
 allocation, and code generation, for software pipelining of loop nests.
  
 20.4.1 
  
 Basic Concepts
  
 First, let us establish some basic definitions. Formally, a nested loop is referred to as “a loop 
 nest.” An 
 n-deep loop nest
  is composed of
  n
  loops,
  L
  1
 ,
  L
  2
 ,
  . . .
 ,
  L
  n
 , from the outermost to the 
 innermost level, with each level having exactly one loop. Each loop
  L
  x
 (1 ≤
  x
  ≤
  n
 ) has an index 
 variable
  i
 x
  and a
  trip count N
 x
  >
  1. The index is normalized to change from 0 to
  N
 x
  − 1 with a unit",NA
20.5,NA,NA
Summary and Future Directions,"Software pipelining originally emerged as a software approach to exploit ILP available in loops in 
 unipro-cessors. Numerous approaches to software pipelining have been proposed and studied in 
 detail. Both optimal and heuristic approaches have been proposed. The techniques for 
 scheduling, register allocation, and code generation have been extensively investigated. In this 
 chapter, we have described the fundamental techniques in detail and surveyed various research 
 directions.
  
 As traditional architectures cease to scale well, distributed architectures such as clustered 
 VLIW and multi-core processors become prevalent. Power has also become a first-order 
 constraint in architectural design. Consequently, software pipelining has been adapted to these 
 newer architectures, and to save power/energy consumption.
  
 Although software pipelining is powerful in extracting fine-grain parallelism, this is also its 
 limitation. Fine-grain parallelism becomes exploitable usually after the intermediate 
 representation of a program is lowered to machine level during compilation. In such lowering, 
 the high-level context has been largely lost. A comprehensive solution to speed up a loop should",NA
21 ,NA,NA
Advances in Register ,NA,NA
Allocation Techniques,"21.1
  
 Introduction
  ........................................
  21
 -
 1
  
 Classical Approaches
  
 21.2 Language
  ...........................................
  21
 -
 4
  
 21.3 Advances in Graph Coloring Techniques
  .............
  21
 -
 5 
  
  
  
 Chordal Graphs and Impact on Register Allocation
  
 21.4
  
 Register Allocation and Static Single Assignment
  .....
  21
 -
 7
  
 Swapping without Side 
 Effects
  
 •
  Scratch Register
  
 •
  Linear
  
 Scan Register Allocation in the Context of the SSA Form
  
 21.
 5 
  
 21.
 6
  
 Bitwidth-Aware Register Allocation
  ..................
  21
 -
 10
  
 Bitwidth 
 Analysis
  
 •
  Variable Packing
  
 •
  Comments
  
 Extensions to Register Allocation: Code Size 
  
 Reduction
  ..........................................
  21
 -
 15
  
 Implementation
  
 21.
 7 
  
 21.
 8
  
 Register Allocation Super Optimizations
  .............
  21
 -
 17 
 Register Allocation + Stack Location Allocation 
  
 Data Structures
  .....................................
  21
 -
 18
  
 Implementation
  
 21.9
  
 Validating Register Allocation
  .......................
  21
 -
 19
  
 Implementatio
 n
  
 •
  Limitations of Data Flow–Based Validation
  
  
 V. Krishna Nandivada 
 IBM India Research Labs, 
  
 New Delhi, India 
  
 nvkrishna@in.ibm.com
  
 21.10 Other Interesting Ideas
  ..............................
  21
 -
 21 
  
 Live 
 Range Splitting
 •
  Progressive Register Allocation
  
  
 •
  Register Assignment and Hardware Features 
  
 21.11 Common Misconceptions
  ...........................
  21
 -
 23 
 21.12 
 Conclusion and Future Directions
  ...................
  21
 -
 24 
 References
  
 .................................................
  21
 -
 25",NA
21.1,NA,NA
Introduction,"Compilersintroduceanunboundednumberoftemporaryregistersduringdifferentphasesofcompilati
 on; these temporary registers arise from the translation of programmer declared variables, 
 simplification of complex expressions, and introduction of temporaries during different 
 optimization phases. However, the target hardware is constrained by the limited number of 
 actual available registers. The task of the
  register allocator
  is to map these temporary registers to 
 real registers and memory locations. In the text, we shall be abbreviating temporary registers as
  
 pseudos
  and actual registers as
  registers
 .
  
 The register allocation problem has historically been studied under two subproblems: register 
 assign-ment and spilling. Register assignment is the phase of assigning of machine registers to 
 pseudos wherever possible. Spilling is the combined act of storing a currently used pseudo to 
 memory and reloading it for
  
 21
 -
 1",NA
21.2,NA,NA
Language,"Register allocation can be considered as a program translation phase, which takes as input a 
 program with pseudos and returns as output a program where pseudos have been replaced with 
 registers and memory locations. We define a simple three-address code language as our input 
 language. The syntax for operands (pseudos and registers) in this language is given in Figure 
 21.3. The set P represents the set of pseudos, and set R represents the set of registers. The input 
 program can contain statements that use constants and operands shown in Figure 21.3. The 
 compiler during different stages of compilation can generate code containing registers (called 
 precolored registers). Hence, registers are present as an operand in the grammar. The register 
 allocator must respect the data flow constraints between these pre-colored registers in the input 
 program (see Section 21.11).
  
 After the completion of the register allocation phase, the program is still in the same format as 
 the original code, with one difference: the pseudo to register mapping is encoded in the program, 
 along with the new memory instructions added by the register allocator. That is, an operand
  v
  in 
 the register allocated program can be either a register or a pseudo–register pair:
  
 v
  ∈ (P × R) + R
  
 Note that this representation allows a pseudo to be mapped to different registers at different 
 stages of the program. Some of the register allocators insert additional instructions such as move 
 instructions, bit-wise operations, and so on.
  
 The problem of register allocation is to translate an input program with operands in the 
 language shown in Figure 21.3 to a program with the pseudo operands replaced with pseudo–
 register pairs. Figure 21.4 shows an example program and the register allocated program.
  
 FIGURE 21.4
  
 P
  
 ::=
  
 {p
 1
 ,
  p
 2
 , · · · ,
  p
 n
 }
  
 R
  
 ::=
  
 {
 ℜ
 1
 , ℜ
 2
 , · · · , ℜ
 n
 }
  
 pr
  
 ∈ PR
  
 ⊆ (P + R)
  
 FIGURE 21.3
  
 Grammar for the operands in the input program.
  
 p
 1
  := ℜ
 0
  + ℜ
 1 
 p
 2
  :=
  p
 1
  + 2 
  
 p
 3
  :=
  p
 1
  +
  p
 2
  
 (
 p
 1
 , ℜ
 1
 ) := ℜ
 0
  + ℜ
 1 
  
 (
 p
 2
 , ℜ0) := (
 p
 1
 , ℜ
 1
 ) + ℜ
 0
  
 (
 p
 3
 , ℜ
 0
 ) := (
 p
 1
 , ℜ
 1
 ) + (
 p
 2
 , ℜ
 0
 )
  
 (a)
  
 (b)
  
 Register allocation example. (a) Input program. (b) Register allocated program. The input program
  
 contains three pseudos and two pre-colored registers.",NA
21.3,NA,NA
Advances in Graph Coloring Techniques,"An observation that has been successfully used in recent studies is that many of the interference 
 graphs induced by the live ranges of variables in programs are
  1-perfect
  [2]. A graph G is said to 
 be 1-perfect if the chromatic number of G is equal to the size of the maximum clique in G. 
 Andersson [2] shows that all the interference graphs present in the corpus of graphs studied by 
 George and Appel [21] are 1-perfect. However, recognition of 1-perfect graphs is an NP-complete 
 problem [43], so this observation cannot be used directly. A graph G is said to be
  perfect
  if, for 
 each vertex-induced subgraph G
 ′
 of G, the graph G
 ′
 is 1-perfect. A vertex-induced subgraph 
 (sometimes simply called an induced subgraph) consists of a subset of the vertices of a graph 
 together with any edges whose endpoints are both in this subset. Gr¨otschel et al. [23] show that 
 the k-colorability problem can be solved in polynomial time for perfect graphs.
  
 Gavril [20] shows that
  chordal
  graphs come with interesting properties, because of which 
 problems such as minimum coloring, maximum clique, minimum covering by cliques, and 
 maximum independent set, which are NP-complete in general, have polynomial time solutions in 
 the context of chordal graphs. A graph is chordal if every cycle of four or more nodes in the graph 
 has a
  chord
 , an edge joining two nodes that are not adjacent in the cycle (see below for further 
 explanation). Chordal graphs are a subset of perfect graphs, so solving the minimum coloring 
 problem in polynomial time leads to the register allocation problem being solved in polynomial 
 time. This optimal coloring of chordal graphs can be undertaken in time linear in the number of 
 edges and vertices.
  
 Pereira and Palsberg [47] use the features of chordal graphs to do register allocation optimally 
 for a large set of interference graphs. Hack et al. [24] and Brisk et al. [10] have independently 
 proved the chordality of the interference graphs of programs in SSA form.
  
 21.3.1 
  
 Chordal Graphs and Impact on Register Allocation
  
 21.3.1.1 Chordal Graphs and Properties
  
 Figures 21.5a and 21.5b present examples of chordal graphs. Figure 21.5a has no cycle of more 
 than three nodes, and Figure 21.5b has a chord
  bd
  in the only possible cycle
  abcd
 . Figure 21.5c is 
 another example of a chordal graph. However, removal of any of the six dashed edges would 
 make the graph non-chordal.
  
 A useful observation by Pereira and Palsberg [47] is that most of the graphs in the corpus of 
 interference graphs of George and Appel [21] are chordal. It turns out that in a program in SSA 
 form, without unstructured jumps (for example, with no arbitrary goto statements) (called
  strict
  
 programs), it will always generate chordal graphs. Three independent groups [6, 10, 24] came up 
 with this key observation around the same time frame. This observation helps clearly decouple 
 register assignment and spilling. For programs written in a structured language such as Java, 
 where arbitrary goto statements do not appear, the interference graph is guaranteed to be 
 chordal, 
 provided 
 the 
 program 
 is 
 in 
 SSA 
 form. 
 Thus, 
 we 
 can 
 know 
 preciselytheoptimalnumberofcolorsrequiredtocolorthegraphandhencecanproceedtospillingwith
 out going through a coloring phase. In the absence of such decoupling, George and Appel [21] 
 proposed an iterative algorithm to do register allocation, where the register allocator iteratively 
 goes over phases that do coalescing, spilling, and coloring. However, this new optimality result 
 helped Pereira and Palsberg [47] and Hack et al. [24] to design register allocators similar to the 
 one shown in Figure 21.6.
  
 FIGURE 21.5
  
 b
  
 a
  
  
 c
  
 a
  
 b c
  
  
 d
  
 f
  
 a b
  
  
 e d
  
 c
  
 (a)
  
 (b)
  
 (c)",NA
21.4,NA,NA
Register Allocation and Static Single Assignment,"In Section 21.3 we showed that a program in SSA form has interesting properties such that the 
 optimal register assignment can be done in polynomial time. In this section, we show some more 
 issues that are specific to programs in SSA form.
  
 In a program in SSA form,
  φ
  function nodes represent the control-flow-directed value 
 renaming. Unlike regular function nodes, the arguments of a
  φ
  node may be contained in the 
 same register as well. Another interesting point about
  φ
  nodes is that there is no specific 
 ordering among the
  φ
  nodes present at the beginning of a basic block, and semantically, all the
  φ
  
 nodes present (at the beginning of a basic block) can be executed simultaneously.
  
 The
  classical
  way to translate a
  φ
  function (
 removal
  of
  φ
  nodes) is by replacing it with a sequence 
 of copy instructions along different control flow edges. For example, say we have a
  φ
  node in basic 
 block
  b
 0
  
  
   
 ℜ
 1
  =
  φ
 (ℜ
 2
 , ℜ
 3
 ) 
  
 representing the flow of value in ℜ
 2
  across one edge (for instance,
  e
 1
 , connecting a basic block
  b
 1
  
 and
  b
 0
 ) and in ℜ
 3
  across another edge (for instance,
  e
 2
 , connecting a basic block
  b
 2
  and
  b
 0
 ). The
  
 classical
  way to remove the
  φ
  function is to do the following: 
  
  
 r
  Create a new basic block
  b
 01
  and add a copy instruction ℜ
 1
  := ℜ
 2
  in that. Remove the control 
 flow 
  
 r
  Create a new basic block
  b
 02
  and add a copy instruction ℜ
 1
  := ℜ
 3
  in that. Remove the 
 control flow edge between
  b
 0
  and
  b
 2
 . Make
  b
 02
  the successor of
  b
 0
  and
  b
 2
  the successor of
  b
 02
 .
  
 These copy instructions introduce additional interference among the live ranges, and this 
 increases the register pressure further. Pereira and Palsberg [48] prove that register allocation 
 after classical SSA elimi-nation is NP-complete. An interesting observation by Hack et al. [24] is 
 that
  φ
  nodes can be removed in such a way the register demand does not increase; that is, 
 register demand for the
  φ
  node never exceeds the number of variables the
  φ
  nodes define. In the 
 following example,
  
 ℜ
 3
  :=
  φ
 (ℜ
 1
 , ℜ
 2
 )
  
 ℜ
 4
  :=
  φ
 (ℜ
 2
 , ℜ
 1
 )
  
 the
  φ
  nodes pick the values of ℜ
 1
  and ℜ
 2
 , or ℜ
 2
  and ℜ
 1
 , into ℜ
 3
  and ℜ
 4
 . In other words, we choose 
 a permutation of the incoming values.",NA
21.5,NA,NA
Bitwidth-Aware Register Allocation,"Media and network applications have introduced new challenges and problems in research. In 
 the context of register allocation, one of the issues that has added newer dimensions is the 
 extensive use of subword data in these applications. In a classical view, the register allocation 
 process maps pseudos to a complete register, not any subparts in it. An interesting point to note 
 is that in case a pseudo needs only a few of the available bits in a register, then allocating all the 
 bits of the register (full register, that is) to the pseudo can be considered wasteful. To be able to 
 utilize this observation for register allocation and allocate a part of a register for each pseudo, we 
 need a direct way to reference bit sections within the register. Architectures of",NA
21.6,NA,NA
Extensions to Register Allocation: Code Size Reduction,"Register allocation has traditionally been used for code optimization targeted for execution time 
 gains. In the past two decades attempts have been made to focus the goal on code size reduction 
 as well. The reason for the shift of focus is the advent of new embedded processors with very 
 limited memory, and they come with constraints on the code size.
  
 Mostoftheresearchinthisareahasbeentargetarchitecturespecific.Forexample,Liaoetal.[34],Leup
 ers and Marwedel [33], and Rao and Pandey [50] target digital signal processor (DSP) type of 
 processors and the auto-increment, auto-decrement addressing modes. Sudarsanam and Mailk 
 [54] 
 target 
 architectures 
 withtwomemoryunitsandtheparalleldataaccessmodes.Parketal.[46]andNaikandPalsberg[39]targ
 et architectures with more than one register bank and instruction with an RP (register pointer, 
 pointing to the “current” memory bank)–relative addressing mode. Paek et al. [45] target 
 architectures with multiple-load/multiple-store instructions and the generated spill code. In this 
 section, we will present some of the key ideas behind the last two works.
  
 The basic idea behind all of the above work on code size reduction has been modifying the data 
 layout, such that the different instructions can be clubbed together. The clubbed instructions are 
 the ones that access memory or memory addresses. Let us look at the example given below.
  
 BEGIN 
  
 int a, b, c, d, e;
  
 · · ·
  
 L1: d = a + c; 
  
 b = d + e; 
  
 L2: 
  
 END
  
 Say at label L1, all the variables are in memory, and at label L2, all of the values must be back in 
 memory. We first have to access variable a, then c, and then store the sum in d. Then we have to 
 load variable e and then store the sum in b. Some architectures provide efficient mechanisms to 
 access memory addresses next to each other (for example, auto-increment address mode can set 
 up the base address being accessed). Thus, it would be advantageous to have a and c next to each 
 other. That way, we can access variable a from some base address, and then we do not need to set 
 up the base address again; use of auto-increment address mode would set the base address up 
 automatically. Similarly, while storing b and d, if the architecture provides instructions to do 
 multiple stores, it would be efficient (in terms of code size) to store them using one instruction. 
 This will again require that variables b and d be next to each other, so instead of a syntax-driven 
 layout (a, b, c, d, e), an efficient layout for these variables above would be a, c, b, d, e. In other 
 words, depending on the architecture specifications of instructions, the input program changing 
 the layout of the variables can result in efficient access of the variables.
  
 21.6.1 
  
 Implementation
  
 21.6.1.1 Z86E30 with Multiple Register Banks
  
 The Z86E30 [58] architecture does not have explicit memory. All declared variables must be 
 stored in registers. The processor has 16 banks of 16 registers each. Accessing any register 
 requires the use of an RP. If the RP already points to the required bank, it need not be explicitly 
 referenced. Otherwise, either the RP has to be set or the bank number has to be included in the 
 instruction, resulting in
  long
  instructions (each long instruction requires one extra byte). Thus, 
 accessing two variables present in two different banks would require instructions to set the RP 
 or use a long instruction, so it would be advantageous to map variables into banks, depending on 
 the locality. That is, variables that are accessed next to each other should be placed in the same 
 bank, thus avoiding the code space required to access a different bank.",NA
21.7,NA,NA
Register Allocation Super Optimizations,"Register allocation has been long understood to be a key optimization, and its interdependence 
 with many other phases of optimization is explored here. Some researchers have tried to come 
 up with a super optimization that does register allocation and the dependent optimization 
 together. In this section, we will present one such optimization.
  
 21.7.1 
  
 Register Allocation + Stack Location Allocation
  
 To offset the increasing gap between processor and memory speed, some of the modern 
 architectures allow efficient accesses to RAM for some specific types of accesses. For example, in 
 memories like SDRAM, accessing 64 bits of data requires the same amount of time as 32 bits. 
 That 
 is, 
 it 
 is 
 more 
 advantageous 
 to 
 accesstheSDRAMfor64bitsthan32bitsatatime.ArchitectureslikeStrongARM,whicharepartofpopula
 r platforms such as Intel IXPs [26] and Stargate [36], have instructions that allow multiple 
 memory accesses in one instruction (load-multiple/store-multiple). A compiler can take 
 advantage of these architecture and memory features to generate efficient spill code. Nandivada 
 and Palsberg [41] explore this to generate efficient stack accesses for the local variables, by 
 adding a phase after register allocation (SLA, stack location allocation). However, there is a phase 
 ordering issue between SLA and register allocation. A unified method that takes into 
 consideration both problems simultaneously could do better.
  
 An important factor needed for an integrated solution is a unified metric to evaluate the impact 
 of register allocation and SLA. Nandivada and Palsberg [42] present an ILP-based solution (SLA + 
 RA [reg-ister allocation] = SARA) to this problem. Besides the constraints generated for the 
 register allocation problem, they generate the constraints for SLA. They model the two-phase 
 approach of register allocation followed by SLA in one problem. The register allocation tries to 
 generate contiguous memory accesses next to each other wherever possible, and the SLA 
 constraints try to merge these memory accesses into single load-multiple/store-multiple 
 instructions. Each of these load-multiple/store-multiple instructions helps reduce the execution 
 time, compared to the individual load/store instructions. The key point to note here is the unified 
 metric to evaluate the solution; their ILP tries to minimize the memory access time needed 
 because of the spill instructions inserted by the register allocator. The goal of the ILP is to 
 minimize the cost function defined in Figure 21.14. MemAccessCost is the cost of accessing the 
 individual words in memory. MemAccessCostReduction is the savings one gets by using load-
 multiple/store-multiple instructions. The map NumberOfSingleMemAccess(
 i
 ) gives the number 
 of single memory accesses at in-struction
  i
 . The map NumberOfMultpleMemAccess(
 i
 ) gives the 
 number of multiple memory accesses at instruction
  i
 .
  
 Function
  ComputeCost 
  
 BEGIN 
  
  
 Cost = 0 
  
  
 For each Instruction
  i 
  
  
  
 Cost + = MemAccessCost × NumberOfSingleMemAccess(
 i
 ) 
  
  
  
 Cost − = MemAccessCostReduction × NumberOfMultpleMemAccess(
 i
 ).
  
  
 return Cost 
  
 END",NA
21.8,NA,NA
Data Structures,"Traditionally, graph-based register allocator designers have kept interference graphs as the main 
 source of representation of the dependencies among the pseudos. Program semantics and 
 architectures enforce additional constraints among the pseudos and registers. These constraints 
 include requirements such as (a) architectural specification of some of the instructions 
 mandating the use of specific registers (use of ST0 in
  fmul
  operations in x86, etc.), (b) compiler 
 conventions forcing the behavior of some registers (caller save, callee save, return register, etc.), 
 (c) architecture forcing certain structures in the operands of certain instructions (e.g., 
 StrongARM requires that the load-multiple instruction have target registers in increasing order 
 of their number; in IA-64 a coupled load requires a pair of even and odd registers), (d) 
 architectural specification prohibiting the use of certain types of registers (e.g., use of floating 
 point of registers in non-floating-point instructions, etc.), and (e) compiler optimization phases 
 introducing dependencies among pseudos (e.g., while doing whole program register allocation, 
 the compiler could decide on arbitrary registers to pass the function arguments).
  
 Representing these constraints is fairly intuitive when represented as mathematical constraints, 
 but representing and processing them in a graph is hard. The reasons are many: 
  
   
 r
  Representation
 : It is not clear how to represent all the different constraints at the same time. 
 r
  Evaluation
 : Given a graph with multiple constraints, processing them to arrive at a metric that 
 can 
   
 r
  Transitive constraints
 : Graphs are convenient to represent constraints between pairs of 
 nodes. Representing relations that involve multiple nodes (e.g., transitive relations) is not trivial.
  
 There are advantages of including multiple constraints in one graph. It results in a unified 
 analysis that takes into consideration all the factors at the same time. To be able to get a unified 
 picture, one needs a common metric to measure and compare different constraints and then 
 come up with a unified metric.
  
 21.8.1 
  
 Implementation
  
 Koseki et al. [32] present a scheme to represent multiple constraints as part of the interference 
 graph, and they use it for the register assignment process. Their scheme consists of two key data 
 structures: register preference graph and coloring precedence graph. They use these two graphs 
 to do the register assignment.
  
 21.8.1.1 Register Preference Graph
  
 A register preference graph (RPG) is a directed graph with nodes representing live ranges, 
 registers, and register classes and edges representing the binary relation between the two nodes. 
 The preferences can be prioritized according to the benefit the preference would derive. The 
 benefit is calculated as a weighted 
 metric(weightedbytheexecutionfrequencyorprogramstructure)estimatingtheperformancediffere
 ntial between when the preference is honored and when the node is located in memory; this 
 metric gives the 
 strength
  of the preference. One way to calculate the strengths is by estimating 
 the difference in the number of processor cycles required to access a variable when it is in its 
 preferred register and when it is located in memory. Figure 21.15 represents a sample RPG.",NA
21.9,NA,NA
Validating Register Allocation,"Given a register allocation algorithm, it is a nontrivial task to determine if the algorithm does the 
 reg-ister allocation correctly. The problem is undecidable, as the problem in general reduces to 
 program verification. However, given a register allocation (the output of a register allocation 
 algorithm for a given input program), we can conservatively validate the allocation.",NA
21.10,NA,NA
Other Interesting Ideas,"In this section, we present some more interesting ideas that have been proposed and extended in 
 recent years but not categorized under any head in this chapter.",NA
21.11,NA,NA
Common Misconceptions,"In this section, a few of the common misconceptions that are prevalent in the community are 
 addressed. Some of these misconceptions may seem trivial, but the author has found many 
 contrasting experiences during his interactions with different researchers to merit their inclusion: 
  
  
 r
  Register assignment:
  Looking at the vast research work on the register assignment problem, 
 one might get a deceptive notion that register assignment is the most important part of the 
 register 
  
  
 allocation process. However, spill code generation and coalescing, to 
 some extent, have more impact 
   
 on the overall performance. Irrespective of how 
 optimal the register assignment process is, if the 
   
 spill code generation is not done well 
 (for instance, too many spill instructions inside frequently 
   
 executed code) or if too 
 many unnecessary register–register copy instructions remain in the code, 
  
 r
  Registers used:
  
 Another interesting misconception is about the number of used registers. Besides the theoretical 
 importance of finding out the minimum number of required registers, the practical 
  
  
 significance of this result is minimal. In an architecture with
  N
  number of registers, the more",NA
21.12,NA,NA
Conclusion and Future Directions,NA,NA
Index,"A
  
 Adaptive compilers,
  8
 -7–8
  
  
 ABS.
  see
  Abstract buffer state (ABS) 
  
 Absent,
  12
 -8 
  
 Abstract buffer state (ABS),
  1
 -30 
  
 Abstract cache state,
  1
 -28 
  
 Abstract execution,
  1
 -29 
  
 Abstracting numeric values,
  12
 -39 
  
 Abstract interpretation 
  
 analysis of access control,
  2
 -11 
  
 based branch prediction modeling,
  1
 -30 
 escape analysis,
  13
 -21–23 
  
 program statements,
  12
 -19–22 
  
 separated approach,
  1
 -27–30 
  
 Abstraction,
  12
 -12 
  
 computing intersections,
  12
 -39 
  
 functions,
  12
 -10–11 
  
 properties,
  12
 -10 
  
 refinement,
  12
 -39 
  
 truth-blurring embedding,
  12
 -16–17 
 two-vocabulary structures,
  12
 -38 
  
 Abstract pipeline,
  1
 -29 
  
 Abstract semantics,
  12
 -24 
  
 Abstract transformers,
  12
 -23 
  
 Access control 
  
 abstract interpretation,
  2
 -11 
  
 cryptography,
  2
 -14 
  
 policies,
  2
 -9–14 
  
 static analysis,
  2
 -9–14 
  
 Access latency,
  5
 -2 
  
 Accumulation,
  15
 -12 
  
 decomposition,
  15
 -17 
  
 Accuracy,
  5
 -18 
  
 WCET analysis,
  1
 -35 
  
 WCET estimation,
  1
 -40–41 
  
 ACL.
  see
  Affine control loops (ACL) 
  
 Actfor principals,
  2
 -22 
  
 Active values,
  21
 -20 
  
 Actual-in and actual-out nodes,
  14
 -9 
  
 Actual spill,
  21
 -3 
  
 Actual WCET,
  1
 -2 
  
 Acyclic singly linked list,
  12
 -6 
  
 Adaptability,
  8
 -2
  
 Adaptive fragment selection,
  10
 -6–7 
  
 Adaptive optimizing compiler,
  8
 -7 
  
 Adaptive tenuring,
  6
 -18–19 
  
 Addition domain,
  15
 -15 
  
 Addressable memory spaces,
  5
 -2 
  
 Addressing modes,
  18
 -24–25 
  
 Address profiles,
  4
 -1 
  
 Adhesiveness,
  14
 -4 
  
 Adjacency ordering,
  20
 -23 
  
 ADL.
  see
  Architecture Description Language (ADL) 
 Affine control loops (ACL),
  15
 -21 
  
 Affine images,
  15
 -5 
  
 Affine lattices,
  15
 -6–7 
  
 functions,
  15
 -8 
  
 Age of objects,
  6
 -18,
  6
 -19–20 
  
 Aho-Corasick algorithm,
  17
 -10 
  
 Alias,
  12
 -3,
  12
 -32,
  14
 -16 
  
 count,
  9
 -11–12 
  
 flow dependences,
  12
 -33 
  
 memory,
  19
 -4 
  
 Allocate pointer,
  6
 -26 
  
 ALT statement,
  14
 -20 
  
 Ambiguity,
  16
 -21 
  
 Analysis technique,
  1
 -36–37 
  
 Analysis time,
  1
 -35,
  1
 -41 
  
 AND/OR-tree model,
  19
 -25–26 
  
 Anonymity rule,
  9
 -4–5 
  
 ConfinedFJ type system,
  9
 -8–9 
  
 Anonymous method,
  9
 -4 
  
 Anticipable,
  11
 -36 
  
 Antidependence,
  15
 -36,
  19
 -3,
  20
 -7 
  
 Anytime gc,
  6
 -5 
  
 APDG.
  see
  Augmented program dependence graph 
  
 (APDG) 
  
 API 
  
 conformance specifications,
  12
 -37 
  
 uid-setting system calls,
  2
 -10 
  
 Appel-Ellis-Li collector,
  6
 -29 
  
 Appel’s SML/NJ collector,
  6
 -22 
  
 Application specific design constraints,
  3
 -2–3 
  
 Application specific instruction set processors 
 (ASIP), 
  
 16
 -1
  
 I
 -
 1",NA
