Larger Text,Smaller Text,Symbol
Functional Programming in Scala ,Paul Chiusano and Rúnar Bjarnason,NA
Copyright,"For online information and ordering of this and other Manning books, please visit 
 www.manning.com
 . The publisher offers discounts on this book when ordered in quantity. For more 
 information, please contact
  
  Special Sales Department
  
  Manning Publications Co.
  
  20 Baldwin Road
  
  PO Box 761
  
  Shelter Island, NY 11964
  
  Email: 
 orders@manning.com
  
 ©2015 by Manning Publications Co. All rights reserved.
  
 No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in 
 any form or by means electronic, mechanical, photocopying, or otherwise, without prior 
 written permission of the publisher.
  
 Many of the designations used by manufacturers and sellers to distinguish their products are 
 claimed as trademarks. Where those designations appear in the book, and Manning Publications was 
 aware of a trademark claim, the designations have been printed in initial caps or all caps.
  
  Recognizing the importance of preserving what has been written, it is Manning’s policy to have 
 the books we publish printed on acid-free paper, and we exert our best efforts to that end. 
 Recognizing also our responsibility to conserve the resources of our planet, Manning books are 
 printed on paper that is at least 15 percent recycled and processed without the use of elemental 
 chlorine.
  
 Development editor: Jeff Bleiel
  
  
 Manning Publications Co.
  
 Copyeditor: Benjamin Berg
  
 Proofreader: Katie Tennant
  
 20 Baldwin Road
  
 Project editor: Janet Vail
  
 PO Box 261
  
 Typesetter: Dottie Marsico
  
 Shelter Island, NY 11964
  
 Illustrator: Chuck Larson
  
 Cover designer: Irene Scala
  
 ISBN 9781617290657
  
 Printed in the United States of America
  
 1 2 3 4 5 6 7 8 9 10 – EBM – 19 18 17 16 15 14",NA
Brief Table of Contents,"Copyright 
  
 Brief Table of Contents 
  
 Table of Contents 
  
 Foreword 
  
 Preface 
  
 Acknowledgments 
  
 About this Book
  
 1. Introduction to functional programming
  
 Chapter 1. What is functional programming?
  
 Chapter 2. Getting started with functional programming in 
 Scala Chapter 3. Functional data structures 
  
 Chapter 4. Handling errors without exceptions 
  
 Chapter 5. Strictness and laziness 
  
 Chapter 6. Purely functional state
  
 2. Functional design and combinator libraries
  
 Chapter 7. Purely functional 
 parallelism 
  
 Chapter 8. Property-based testing 
  
 Chapter 9. Parser combinators
  
 3. Common structures in functional design
  
 Chapter 10. Monoids 
  
 Chapter 11. Monads 
  
 Chapter 12. Applicative and traversable 
 functors
  
 4. Effects and I/O
  
 Chapter 13. External effects and I/O 
  
 Chapter 14. Local effects and mutable state 
  
 Chapter 15. Stream processing and incremental 
 I/O",NA
Table of Contents,"Copyright 
  
 Brief Table of Contents 
  
 Table of Contents 
  
 Foreword 
  
 Preface 
  
 Acknowledgments 
  
 About this Book
  
 1. Introduction to functional programming
  
 Chapter 1. What is functional programming?
  
 1.1. The benefits of FP: a simple example
  
 1.1.1. A program with side effects 
  
 1.1.2. A functional solution: removing the side 
 effects
  
 1.2. Exactly what is a (pure) function?
  
 1.3. Referential transparency, purity, and the substitution 
 model 1.4. Summary
  
 Chapter 2. Getting started with functional programming in Scala
  
 2.1. Introducing Scala the language: an example 
  
 2.2. Running our program 
  
 2.3. Modules, objects, and namespaces 
  
 2.4. Higher-order functions: passing functions to 
 functions
  
 2.4.1. A short detour: writing loops functionally 
 2.4.2. Writing our first higher-order function
  
 2.5. Polymorphic functions: abstracting over types
  
 2.5.1. An example of a polymorphic function 
 2.5.2. Calling HOFs with anonymous 
 functions",NA
Foreword,"Functional Programming in Scala
  is an intriguing title. After all, Scala is generally called a 
 functional programming language and there are dozens of books about Scala on the market. Are all 
 these other books missing the functional aspects of the language? To answer the question it’s 
 instructive to dig a bit deeper. What is 
 functional programming
 ? For me, it’s simply an alias 
 for“programming with functions,” that is, a programming style that puts the focus on the functions 
 in a program. What are 
 functions
 ? Here, we find a larger spectrum of definitions. While one 
 definition often admits functions that may have side effects in addition to returning a result, pure 
 functional programming restricts functions to be as they are in mathematics: binary relations that 
 map arguments to results.
  
 Scala is an impure functional programming language in that it admits impure as well as pure 
 functions, and in that it does not try to distinguish between these categories by using different 
 syntax or giving them different types. It shares this property with most other functional languages. 
 It would be nice if we could distinguish pure and impure functions in Scala, but I believe we have 
 not yet found a way to do so that is lightweight and flexible enough to be added to Scala without 
 hesitation.
  
 To be sure, Scala programmers are generally encouraged to use pure functions. Side effects such as 
 mutation, I/O, or use of exceptions are not ruled out, and they can indeed come in quite handy 
 sometimes, be it for reasons of interoperability, efficiency, or convenience. But overusing side 
 effects is generally not considered good style by experts. Nevertheless, since impure programs are 
 possible and even convenient to write in Scala, there is a temptation for programmers coming from 
 a more imperative background to keep their style and not make the necessary effort to adapt to the 
 functional mindset. In fact, it’s quite possible to write Scala as if it were Java without the 
 semicolons.
  
 So to properly learn functional programming in Scala, should one make a detour via a pure 
 functional language such as Haskell? Any argument in favor of this approach has been severely 
 weakened by the appearance of 
 Functional Programming in Scala
 .
  
 What Paul and Rúnar do, put simply, is treat Scala as a pure functional programming language. 
 Mutable variables, exceptions, classical input/output, and all other traces of impurity are 
 eliminated.
  
 If you wonder how one can write useful programs without any of these conveniences, you need to 
 read the book. Building up from first principles and extending all the way to incremental input and 
 output, they demonstrate that, indeed, one can express every concept using only pure functions. 
 And they show that it is not only possible, but that it also leads to beautiful code and deep insights 
 into the nature of computation.
  
 The book is challenging, both because it demands attention to detail and because it might 
 challenge the way you think about programming. By reading the book and doing the",NA
Preface,"Writing good software is hard. After years of struggling with other approaches, both of us 
 discovered and fell in love with functional programming (FP). Though the FP approach is different, 
 we came to appreciate how the discipline leads to a coherent, composable, and beautiful way of 
 writing programs.
  
 Both of us participated in the Boston Area Scala Enthusiasts, a group that met regularly in 
 Cambridge.
  
 When the group first started, it mainly consisted of Java programmers who were looking for 
 something better. Many expressed frustration that there wasn’t a clear way to learn how to take 
 advantage of FP in Scala. We could empathize—we had both learned FP somewhat haphazardly, by 
 writing lots of functional code, talking to and learning from other Scala and Haskell programmers, 
 and reading a patchwork of different articles, blog posts, and books. It felt like there should be an 
 easier way. In April 2010 one of the group’s organizers, Nermin Šerifović, suggested that we write 
 a book specifically on the topic of FP in Scala. Based on our learning experiences, we had a clear 
 idea of the kind of book we wanted to write, and we thought it would be quick and easy. More than 
 four years later, we think we have created a good book. It’s the book we wish had existed when we 
 were learning functional programming.
  
 We hope to convey in this book some of the excitement that we felt when we were first 
 discovering FP.",NA
Acknowledgments,"We would like to thank the many people who participated in the creation of this book. To 
 NerminŠerifović, our friend from the Boston Scala group, thank you for first planting the seed of 
 this book in our minds.
  
 To the amazing team at Capital IQ, thank you for your support and for bravely helping beta test 
 the first version of the book’s curriculum.
  
 We would like to especially acknowledge Tony Morris for embarking on this journey with us. 
 His work on the early stages of the book remains invaluable, as does his larger contribution to 
 the practice of functional programming in Scala.
  
 Martin, thank you for your wonderful foreword, and of course for creating this powerful language 
 that is helping to reshape our industry.
  
 During the book-writing process, we were grateful for the encouragement from the enthusiastic 
 community of Scala users interested in functional programming. To our reviewers, MEAP readers, 
 and everyone else who provided feedback or submitted bug reports and pull requests, thank you! 
 This book would not be what it is today without your help.
  
 Special thanks to our development editor Jeff Bleiel, our graphics editor Ben Kovitz, our technical 
 proofreader Tom Lockney, and everyone else at Manning who helped make this a better book, 
 including the following reviewers who read the manuscript at various stages of its development: 
 Ashton Hepburn, Bennett Andrews, Chris Marshall, Chris Nauroth, Cody Koeninger, Dave 
 Cleaver, Douglas Alan, Eric Torreborre, Erich W. Schreiner, Fernando Dobladez, Ionut, G. Stan, 
 Jeton Bacaj, Kai Gellien, Luc Duponcheel, Mark Janssen, Michael Smolyak, Ravindra Jaju, 
 Rintcius Blok, Rod Hilton, Sebastien Nichele, Sukant Hajra, Thad Meyer, Will Hayworth, and 
 William E. Wheeler.
  
 Lastly, Sunita and Allison, thank you so much for your support throughout our multi-year 
 odyssey to make this book a reality.",NA
About this Book,"This is not a book about Scala. This book is an introduction to 
 functional programming
  (FP), a 
 radical, principled approach to writing software. We use Scala as the vehicle to get there, but you 
 can apply the lessons herein to programming in any language. As you work through this book, our 
 goal is for you to gain a firm grasp of functional programming concepts, become comfortable 
 writing purely functional programs, and be able to absorb new material on the subject, beyond what 
 we cover here.",NA
How this book is structured,"The book is organized into four parts. In 
 part 1
 , we talk about exactly what functional programming 
 is and introduce some core concepts. The chapters in 
 part 1
  give an overview of fundamental 
 techniques like how to organize small functional programs, define purely functional data structures, 
 handle errors, and deal with state.
  
 Building on this foundation, 
 part 2
  is a series of tutorials on 
 functional design
 . We work 
 through some examples of practical functional libraries, laying bare the thought process that 
 goes into designing them.
  
 While working through the libraries in 
 part 2
 , it will become clear to you that these libraries follow 
 certain patterns and contain some duplication. This will highlight the need for new and higher 
 abstractions for writing more generalized libraries, and we introduce those abstractions in 
 part 3
 . 
 These are very powerful tools for reasoning about your code. Once you master them, they hold the 
 promise of making you extraordinarily productive as a programmer.
  
 Part 4
  then bridges the remainder of the gap towards writing real-world applications that perform 
 I/O (like working with databases, files, or video displays) and make use of mutable state, all in a 
 purely functional way.
  
 Throughout the book, we rely heavily on programming exercises, carefully sequenced to help you 
 internalize the material. To understand functional programming, it’s not enough to learn the theory 
 abstractly. You have to fire up your text editor and write some code. You have to take the theory 
 that you have learned and put it into practice in your work.
  
 We’ve also provided online notes for all the chapters. Each chapter has a section with discussion 
 related to that chapter, along with links to further material. These chapter notes are meant to be 
 expanded by the community of readers, and are available as an editable wiki at 
  
 https://github.com/fpinscala/fpinscala/wiki
 .",NA
Audience,NA,NA
How to read this book,"Although the book can be read sequentially straight through, the sequencing of the four parts is 
 designed so that you can comfortably break between them, apply what you have learned to your 
 own work, and then come back later for the next part. For example, the material in 
 part 4
  will make 
 the most sense after you have a strong familiarity with the functional style of programming 
 developed in 
 parts 1
 , 
 2
 , and 
 3
 . After 
 part 3
 , it may be a good idea to take a break and try getting 
 more practice writing functional programs beyond the exercises we work on in the chapters. Of 
 course, this is ultimately up to you.
  
 Most chapters in this book have a similar structure. We introduce some new idea or technique, 
 explain it with an example, and then work through a number of exercises. We 
 strongly
  suggest that 
 you download the exercise source code and do the exercises as you go through each chapter. 
 Exercises, hints, and answers are all available at 
 https://github.com/fpinscala/fpinscla
 . We also 
 encourage you to visit the scala-functional Google group 
 (
 https://groups.google.com/forum/#!topic/scala-functional/
 ) and the 
 #fp-in-scala
  IRC channel on 
 irc.freenode.net
  for questions and discussion.
  
 Exercises are marked for both their difficulty and importance. We will mark exercises that we think 
 are 
 hard
  or that we consider to be 
 optional
  to understanding the material. The 
 hard
  designation is 
 our effort to give you some idea of what to expect—it is only our guess and you may find some 
 unmarked questions difficult and some questions marked 
 hard
  to be quite easy. The 
 optional
  
 designation is for exercises that are informative but can be skipped without impeding your ability to 
 follow further material. The exercises have the following icons in front of them to denote whether or 
 not they are optional:",NA
Code conventions and downloads,"All source code in listings or in text is in a 
 fixed-width font like this
  to separate it from ordinary 
 text. Key words in Scala are set in 
 bold fixed-width font like this
 . Code annotations accompany 
 many of the listings, highlighting important concepts.
  
 To download the source code for the examples in the book, the exercise code, and the chapter 
 notes, please go to 
 https://github.com/fpinscala/fpinscala
  or to the publisher’s website at 
  
 www.manning.com/FunctionalProgramminginScala
 .",NA
Setting expectations,"Although functional programming has a profound impact on the way we write software at every 
 level, it takes time to build up to that. It’s an incremental process. Don’t expect to be blown away by 
 how amazing functional programming is right in the first chapter. The principles laid out in the 
 beginning are quite subtle, and may even seem like they’re just common sense. If you think to 
 yourself “that’s something I can already do without knowing FP,” then that’s great! That’s exactly",NA
Author Online,"Purchase of 
 Functional Programming in Scala
  includes free access to a private web forum run by 
 Manning Publications where you can make comments about the book, ask technical questions, and 
 receive help from the authors and other users. To access the forum and subscribe to it, point your 
 web browser to 
 www.manning.com/FunctionalProgramminginScala
 . This Author Online page 
 provides information on how to get on the forum once you’re registered, what kind of help is 
 available, and the rules of conduct on the forum.
  
 Manning’s commitment to our readers is to provide a venue where a meaningful dialog among 
 individual readers and between readers and the authors can take place. It’s not a commitment to 
 any specific amount of participation on the part of the authors, whose contribution to the forum 
 remains voluntary (and unpaid).
  
 The Author Online forum and the archives of previous discussions will be accessible from 
 the publisher’s website as long as the book is in print.",NA
Part 1. Introduction to functional programming,"We begin this book with a radical premise—that we will restrict ourselves to constructing 
 programs using only pure functions with no side effects such as reading from files or mutating 
 memory. This idea, of functional programming, leads to a very different way of writing programs 
 than you may be used to. We therefore start from the very beginning, relearning how to write the 
 simplest of programs in a functional way.
  
 In the first chapter, we’ll explain exactly what functional programming means and give you some 
 idea of its benefits. The rest of the chapters in 
 part 1
  introduce the basic techniques for functional 
 programming in Scala. 
 Chapter 2
  introduces Scala the language and covers fundamentals like how 
 to write loops functionally and manipulate functions as ordinary values. 
 Chapter 3
  deals with in-
 memory data structures that may change over time. 
 Chapter 4
  talks about handling errors in pure 
 functions, and 
 chapter 5
  introduces the notion of non-strictness, which can be used to improve the 
 efficiency and modularity of functional code. Finally, 
 chapter 6
  introduces modeling stateful 
 programs using pure functions.
  
 The intent of this first part of the book is to get you thinking about programs purely in terms of 
 functions from inputs to outputs, and to teach you the techniques you’ll need in 
 part 2
 , when we 
 start writing some practical code.",NA
Chapter 1. What is functional programming?,"Functional programming (FP) is based on a simple premise with far-reaching implications: we 
 construct our programs using only 
 pure functions
 —in other words, functions that have no 
 side 
 effects
 .
  
 What are side effects? A function has a side effect if it does something other than simply return 
 a result, for example:
  
  
 Modifying a variable
  
  
 Modifying a data structure in place
  
  
 Setting a field on an object
  
  
 Throwing an exception or halting with an error
  
  
 Printing to the console or reading user input
  
  
 Reading from or writing to a file
  
  
 Drawing on the screen
  
 We’ll provide a more precise definition of side effects later in this chapter, but consider what 
 programming would be like without the ability to do these things, or with significant restrictions on 
 when and how these actions can occur. It may be difficult to imagine. How is it even possible to 
 write useful programs at all? If we can’t reassign variables, how do we write simple programs like 
 loops? What about working with data that changes, or handling errors without throwing exceptions? 
 How can we write programs that must perform I/O, like drawing to the screen or reading from a 
 file?
  
 The answer is that functional programming is a restriction on 
 how
  we write programs, but not on 
 what 
 programs we can express. Over the course of this book, we’ll learn how to express 
 all
  of our 
 programs without side effects, and that includes programs that perform I/O, handle errors, and 
 modify data. We’ll learn how following the discipline of FP is tremendously beneficial because of 
 the increase in 
 modularity
  that we gain from programming with pure functions. Because of their 
 modularity, pure functions are easier to test, reuse, parallelize, generalize, and reason about. 
 Furthermore, pure functions are much less prone to bugs.
  
 In this chapter, we’ll look at a simple program with side effects and demonstrate some of the 
 benefits of FP by removing these side effects. We’ll also discuss the benefits of FP more generally 
 and define two important concepts—
 referential transparency
  and the 
 substitution model
 .",NA
1.1. The benefits of FP: a simple example,"Let’s look at an example that demonstrates some of the benefits of programming with pure 
 functions. The point here is just to illustrate some basic ideas that we’ll return to throughout this 
 book. This will also be your first exposure to Scala’s syntax. We’ll talk through Scala’s syntax 
 much more in the next chapter, so don’t worry too much about following every detail. As long as 
 you have a basic idea of what the code is doing, that’s what’s important.",NA
1.2. Exactly what is a (pure) function?,"We said earlier that FP means programming with pure functions, and a pure function is one that 
 lacks side effects. In our discussion of the coffee shop example, we worked off an informal notion 
 of side effects and purity. Here we’ll formalize this notion, to pinpoint more precisely what it 
 means to program functionally. This will also give us additional insight into one of the benefits of 
 functional programming: pure functions are easier to reason about.
  
 A function 
 f
  with input type 
 A
  and output type 
 B
  (written in Scala as a single type: 
 A => B
 , 
 pronounced “A to B” or “A arrow B”) is a computation that relates every value 
 a
  of type 
 A
  to 
 exactly one value 
 b
  of type 
 B
  such that 
 b
  is determined solely by the value of 
 a
 . Any changing state 
 of an internal or external process is irrelevant to computing the result 
 f(a)
 . For example, a function 
 intToString
  having type 
 Int => String
  will take every integer to a corresponding string. 
 Furthermore, if it really is a 
 function
 , it will do nothing else.
  
 In other words, a function has no observable effect on the execution of the program other than 
 to compute a result given its inputs; we say that it has no side effects. We sometimes qualify 
 such functions as 
 pure
  functions to make this more explicit, but this is somewhat redundant. 
 Unless we state otherwise, we’ll often use 
 function
  to imply no side effects.
 [
 2
 ]
  
 2
 Procedure
  is often used to refer to some parameterized chunk of code that may have side effects.
  
 You should be familiar with a lot of pure functions already. Consider the addition (
 +
 ) function on 
 integers. It takes two integer values and returns an integer value. For any two given integer values, 
 it will 
 always return the same integer value
 . Another example is the 
 length
  function of a 
 String 
 in 
 Java, Scala, and many other languages where strings can’t be modified (are immutable). For any 
 given string, the same length is always returned and nothing else occurs.
  
 We can formalize this idea of pure functions using the concept of 
 referential transparency (RT)
 . 
 This is a property of 
 expressions
  in general and not just functions. For the purposes of our 
 discussion, consider an expression to be any part of a program that can be evaluated to a result—
 anything that you could type into the Scala interpreter and get an answer. For example, 
 2 + 3
  is an",NA
"1.3. Referential transparency, purity, and the substitution model","Let’s see how the definition of RT applies to our original 
 buyCoffee
  example:
  
 def
  buyCoffee(cc: CreditCard): Coffee = { 
  
   
 val
  cup = 
 new
  Coffee()
  
  
  cc.charge(cup.price)
  
  
  cup 
  
 }
  
 Whatever the return type of 
 cc.charge(cup.price)
  (perhaps it’s 
 Unit
 , Scala’s equivalent of 
 void
  in 
 other languages), it’s discarded by 
 buyCoffee
 . Thus, the result of evaluating 
  
 buyCoffee(aliceCreditCard)
  will be merely 
 cup
 , which is equivalent to a 
 new 
  
 Coffee()
 . For 
 buyCoffee
  to be pure, by our definition of RT, it must be the case that 
  
 p(buyCoffee(aliceCreditCard))
  behaves the same as 
 p(new Coffee())
 , for 
 any
 p
 .
  
 This clearly doesn’t hold—the program 
 new Coffee()
  doesn’t do anything, whereas 
  
 buyCoffee(aliceCreditCard)
  will contact the credit card company and authorize a charge. Already 
 we have an observable difference between the two programs.
  
 Referential transparency forces the invariant that everything a function 
 does
  is represented by the
  
 value
  that it returns, according to the result type of the function. This constraint enables a simple 
 and
  
 natural mode of reasoning about program evaluation called the 
 substitution model
 . When 
 expressions
  
 are referentially transparent, we can imagine that computation proceeds much like we’d solve an",NA
1.4. Summary,"In this chapter, we introduced functional programming and explained exactly what FP is and why 
 you might use it. Though the full benefits of the functional style will become more clear over the 
 course of this book, we illustrated some of the benefits of FP using a simple example. We also",NA
Chapter 2. Getting started with functional programming in Scala,"Now that we have committed to using only pure functions, a question naturally emerges: how do 
 we write even the simplest of programs? Most of us are used to thinking of programs as sequences 
 of instructions that are executed in order, where each instruction has some kind of effect. In this 
 chapter, we’ll begin learning how to write programs in the Scala language just by combining pure 
 functions.
  
 This chapter is mainly intended for those readers who are new to Scala, to functional programming, 
 or both. Immersion is an effective method for learning a foreign language, so we’ll just dive in. The 
 only way for Scala code to look familiar and not foreign is if we look at a lot of Scala code. We’ve 
 already seen some in the first chapter. In this chapter, we’ll start by looking at a small but complete 
 program. We’ll then break it down piece by piece to examine what it does in some detail, in order 
 to understand the basics of the Scala language and its syntax. Our goal in this book is to teach 
 functional programming, but we’ll use Scala as our vehicle, and need to know enough of the Scala 
 language and its syntax to get going.
  
 Once we’ve covered some of the basic elements of the Scala language, we’ll then introduce some 
 of the basic techniques for how to write functional programs. We’ll discuss how to write loops 
 using 
 tail recursive functions
 , and we’ll introduce 
 higher-order functions (HOFs)
 . HOFs are 
 functions that take other functions as arguments and may themselves return functions as their 
 output. We’ll also look at some examples of 
 polymorphic
  HOFs where we use types to guide us 
 toward an 
  
 implementation.
  
 There’s a lot of new material in this chapter. Some of the material related to HOFs may be brain-
 bending if you have a lot of experience programming in a language 
 without
  the ability to pass 
 functions around like that. Remember, it’s not crucial that you internalize every single concept in 
 this chapter, or solve every exercise. We’ll come back to these concepts again from different 
 angles throughout the book, and our goal here is just to give you some initial exposure.",NA
2.1. Introducing Scala the language: an example,"The following is a complete program listing in Scala, which we’ll talk through. We aren’t 
 introducing any new concepts of functional programming here. Our goal is just to introduce the 
 Scala language and its syntax.
  
 Listing 2.1. A simple Scala program",NA
2.2. Running our program,"This section discusses the simplest possible way of running your Scala programs, suitable for short
  
 examples. More typically, you’ll build and run your Scala code using 
 sbt
 , the build tool for 
 Scala, and/or an IDE like IntelliJ or Eclipse. See the book’s source code repo on GitHub
  
 (
 https://github.com/fpinscala/fpinscala
 ) for more information on getting set up with 
 sbt
 .
  
 The simplest way we can run this Scala program (
 MyModule
 ) is from the command line, 
 by invoking the Scala compiler directly ourselves. We start by putting the code in a file 
 called
  
 MyModule.scala
  or something similar. We can then compile it to Java bytecode using the 
 scalac
  compiler:
  
 > scalac MyModule.scala
  
 This will generate some files ending with the 
 .class
  suffix. These files contain compiled code that 
 can be run with the Java Virtual Machine (JVM). The code can be executed using the 
 scala 
 command-line tool:
  
 > scala MyModule 
  
 The absolute value of -42 is 42.
  
 Actually, it’s not strictly necessary to compile the code first with 
 scalac
 . A simple program like the 
 one we’ve written here can be run using just the Scala interpreter by passing it to the 
 scala 
 command-line tool directly:",NA
"2.3. Modules, objects, and namespaces","In this section, we’ll discuss some additional aspects of Scala’s syntax related to modules, objects, 
 and namespaces. In the preceding REPL session, note that in order to refer to our 
 abs
  method, we 
 had to say 
 MyModule.abs
  because 
 abs
  was defined in the 
 MyModule
  object. We say that 
 MyModule
  is its 
 namespace
 . Aside from some technicalities, every value in Scala is what’s called 
 an 
 object
 ,
 [
 1
 ]
  and each object may have zero or more 
 members
 . An object whose primary purpose is 
 giving its members a namespace is sometimes called a 
 module
 . A member can be a method 
 declared with the 
 def
  keyword, or it can be another object declared with 
 val
  or 
 object
 . Objects can 
 also have other kinds of members that we’ll ignore for now.
  
 1
  Unlike Java, values of primitive types like 
 Int
  are also considered objects for the purposes of this discussion.
  
 We access the members of objects with the typical object-oriented dot notation, which is a 
 namespace (the name that refers to the object) followed by a dot (the period character), followed by 
 the name of the member, as in 
 MyModule.abs(-42)
 . To use the 
 toString
  member on the object 
 42
 , 
 we’d use 
 42.toString
 . The implementations of members within an object can refer to each other 
 unqualified (without prefixing the object name), but if needed they have access to their enclosing 
 object using a special name: 
 this
 .
 [
 2
 ]",NA
2.4. Higher-order functions: passing functions to functions,"Now that we’ve covered the basics of Scala’s syntax, we’ll move on to covering some of the 
 basics of writing functional programs. The first new idea is this: 
 functions are values
 . And just 
 like values of other types—such as integers, strings, and lists—functions can be assigned to 
 variables, stored in data structures, and passed as arguments to functions.
  
 When writing purely functional programs, we’ll often find it useful to write a function that accepts 
 other functions as arguments. This is called a 
 higher-order function (HOF)
 , and we’ll look next at 
 some simple examples to illustrate. In later chapters, we’ll see how useful this capability really is, 
 and how it permeates the functional programming style. But to start, suppose we wanted to adapt 
 our program to print out both the absolute value of a number 
 and
  the factorial of another number. 
 Here’s a sample run of such a program:
  
 The absolute value of -42 is 42 
  
 The factorial of 7 is 5040
  
 2.4.1. A short detour: writing loops functionally
  
 First, let’s write 
 factorial
 :",NA
2.5. Polymorphic functions: abstracting over types,"So far we’ve defined only 
 monomorphic
  functions, or functions that operate on only one type of 
 data.
  
 For example, 
 abs
  and 
 factorial
  are specific to arguments of type 
 Int
 , and the higher-order 
 function 
 formatResult
  is also fixed to operate on functions that take arguments of type 
 Int
 . 
 Often, and especially when writing HOFs, we want to write code that works for 
 any
  type it’s 
 given.
  
 These are called 
 polymorphic functions
 ,
 [
 5
 ]
  and in the chapters ahead, you’ll get plenty of experience
  
 writing such functions. Here we’ll just introduce the idea.",NA
2.6. Following types to implementations,"As you might have seen when writing 
 isSorted
 , the universe of possible implementations is 
 significantly reduced when implementing a polymorphic function. If a function is polymorphic 
 in
  
 some type 
 A
 , the only operations that can be performed on that 
 A
  are those passed into the function 
 as arguments (or that can be defined in terms of these given operations).
 [
 6
 ]
  In some cases, you’ll 
 find that
  
 the universe of possibilities for a given polymorphic type is constrained such that only one
  
 implementation is possible!
  
 6
  Technically, all values in Scala can be compared for equality (using 
 ==
 ), and turned into strings with 
 toString
  and integers with 
 hashCode
 . But this is something of a wart inherited from Java.
  
 Let’s look at an example of a function signature that can only be implemented in one way. It’s a
  
 higher-order function for performing what’s called 
 partial application
 . This function, 
 partial1
 , 
 takes a value and a function of two arguments, and returns a function of one argument as its result. 
 The
  
 name comes from the fact that the function is being applied to some but not all of the arguments it
  
 requires:
  
 def partial1[A,B,C](a: A, f: (A,B) => C): B => C",NA
2.7. Summary,"In this chapter, we learned enough of the Scala language to get going, and some preliminary 
 functional programming concepts. We learned how to define simple functions and programs, 
 including how we can express loops using recursion; then we introduced the idea of higher-order 
 functions, and we got some practice writing polymorphic functions in Scala. We saw how the 
 implementations of 
  
 polymorphic functions are often significantly constrained, such that we can often simply “follow 
 the types” to the correct implementation. This is something we’ll see a lot of in the chapters ahead.
  
 Although we haven’t yet written any large or complex programs, the principles we’ve discussed 
 here are scalable and apply equally well to programming in the large as they do to programming in 
 the small.
  
 Next we’ll look at using pure functions to manipulate data.",NA
Chapter 3. Functional data structures,"We said in the introduction that functional programs don’t update variables or modify mutable data 
 structures. This raises pressing questions: what sort of data structures 
 can
  we use in functional 
 programming, how do we define them in Scala, and how do we operate on them? In this chapter, 
 we’ll learn the concept of 
 functional data structures
  and how to work with them. We’ll use this as 
 an opportunity to introduce how data types are defined in functional programming, learn about the 
 related technique of 
 pattern matching
 , and get practice writing and generalizing pure functions.
  
 This chapter has a lot of exercises, particularly to help with this last point—writing and 
 generalizing pure functions. Some of these exercises may be challenging. As always, consult the 
 hints or the answers at our GitHub site (
 https://github.com/fpinscala/fpinscala
 ; see the preface), or 
 ask for help online if you need to.",NA
3.1. Defining functional data structures,"A functional data structure is (not surprisingly) operated on using only pure functions. Remember, a 
 pure function must not change data in place or perform other side effects. 
 Therefore, functional data 
 structures are by definition immutable.
  For example, the empty list (written 
 List()
  or 
 Nil
  in Scala) is 
 as eternal and immutable as the integer values 
 3
  or 
 4
 . And just as evaluating 
 3 + 4
  results in a new 
 number 
 7
  without modifying either 
 3
  or 
 4
 , concatenating two lists together (the syntax for this is 
 a 
 ++ b
  for two lists 
 a
  and 
 b
 ) yields a new list and leaves the two inputs unmodified.
  
 Doesn’t this mean we end up doing a lot of extra copying of the data? Perhaps surprisingly, the 
 answer is no, and we’ll talk about exactly why that is. But first let’s examine what’s probably 
 the most ubiquitous functional data structure, the singly linked list. The definition here is 
 identical in spirit to (though simpler than) the 
 List
  data type defined in Scala’s standard library. 
 This code listing introduces a lot of new syntax and concepts that we’ll talk through in detail.
  
 Listing 3.1. Singly linked lists",NA
3.2. Pattern matching,"Let’s look in detail at the functions 
 sum
  and 
 product
 , which we place in the 
 object List
 , 
 sometimes called the 
 companion object
  to 
 List
  (see sidebar). Both these definitions make use of 
 pattern matching:
  
 def
  sum(ints: List[Int]): Int = ints 
 match
  { 
  
  
 case
  Nil => 0 
  
  
 case
  Cons(x,xs) => x + sum(xs) 
  
 }
  
 def
  product(ds: List[Double]): Double = ds 
 match
  { 
  
 case
  Nil => 1.0 
  
  
 case
  Cons(0.0, _) => 0.0 
  
  
 case
  Cons(x,xs) => x * product(xs) 
  
 }
  
 As you might expect, the 
 sum
  function states that the sum of an empty list is 
 0
 , and the sum of a 
 nonempty list is the first element, 
 x
 , plus the sum of the remaining elements, 
 xs
 .
 [
 3
 ]
  Likewise the 
 product
  definition states that the product of an empty list is 
 1.0
 , the product of any list starting with 
 0.0
  is 
 0.0
 , and the product of any other nonempty list is the first element multiplied by the product 
 of the remaining elements. Note that these are recursive definitions, which are common when
  
 writing functions that operate over recursive data types like 
 List
  (which refers to itself recursively 
 in its 
 Cons
  data constructor).
  
 3
  We could call 
 x
  and 
 xs
  anything there, but it’s a common convention to use 
 xs
 , 
 ys
 , 
 as
 , or 
 bs
  as variable names for a sequence 
 of some sort, and 
 x
 , 
 y
 , 
 z
 , 
 a
 , or 
 b
  as the name for a single element of a sequence. Another common naming convention is 
 h
  for 
 the first element of a list (the 
 head
  of the list), 
 t
  for the remaining elements (the 
 tail
 ), and 
 l
  for an entire list.
  
 Pattern matching works a bit like a fancy 
 switch
  statement that may descend into the structure of 
 the expression it examines and extract subexpressions of that structure. It’s introduced with an 
 expression
  
 (the 
 target
  or 
 scrutinee
 ) like 
 ds
 , followed by the keyword 
 match
 , and a 
 {}
 -wrapped sequence of 
 cases
 . Each case in the match consists of a 
 pattern
  (like 
 Cons(x,xs)
 ) to the left of the 
 =>
  and a 
 result
  (like 
 x * product(xs)
 ) to the right of the 
 =>
 . If the target 
 matches
  the pattern in a case 
 (discussed next), the result of that case becomes the result of the entire match expression. If 
 multiple
  
 patterns match the target, Scala chooses the first matching case.
  
  
 Companion objects in Scala
  
 We’ll often declare a 
 companion object
  in addition to our data type and its data constructors. This is",NA
3.3. Data sharing in functional data structures,"When data is immutable, how do we write functions that, for example, add or remove elements 
 from a list? The answer is simple. When we add an element 
 1
  to the front of an existing list, say 
 xs
 , 
 we return a new list, in this case 
 Cons(1,xs)
 . Since lists are immutable, we don’t need to actually 
 copy 
 xs
 ; we can just reuse it. This is called 
 data sharing
 . Sharing of immutable data often lets us",NA
3.4. Recursion over lists and generalizing to higher-order functions,"Let’s look again at the implementations of 
 sum
  and 
 product
 . We’ve simplified the 
 product 
 implementation slightly, so as not to include the “short-circuiting” logic of checking for 
 0.0
 :
  
 def
  sum(ints: List[Int]): Int = ints 
 match
  { 
  
  
 case
  Nil => 0 
  
  
 case
  Cons(x,xs) => x + sum(xs) 
  
 }
  
 def
  product(ds: List[Double]): Double = ds 
 match
  { 
  
 case
  Nil => 1.0 
  
  
 case
  Cons(x, xs) => x * product(xs) 
  
 }
  
 Note how similar these two definitions are. They’re operating on different types (
 List[Int] 
 versus 
 List[Double]
 ), but aside from this, the only differences are the value to return in the case that the 
 list is empty (
 0
  in the case of 
 sum
 , 
 1.0
  in the case of 
 product
 ), and the operation to combine 
 results (
 +
  in the case of 
 sum
 , 
 *
  in the case of 
 product
 ). Whenever you encounter duplication like 
 this, you can generalize it away by pulling subexpressions out into function arguments. If a 
 subexpression refers to any local variables (the 
 +
  operation refers to the local variables 
 x
  and 
 xs
  
 introduced by the pattern, similarly for 
 product
 ), turn the subexpression into a function that 
 accepts these variables as arguments. Let’s do that now. Our function will take as arguments the 
 value to return in the case of the empty list, and the function to add an element to the result in the 
 case of a nonempty list.
 [
 9
 ]
  
 9
  In the Scala standard library, 
 foldRight
  is a method on 
 List
  and its arguments are curried similarly for better type inference.
  
 Listing 3.2. Right folds and simple uses",NA
3.5. Trees,NA,NA
3.6. Summary,"In this chapter, we covered a number of important concepts. We introduced algebraic data types and 
 pattern matching, and showed how to implement purely functional data structures, including the 
 singly linked list. We hope that, through the exercises in this chapter, you got more comfortable 
 writing pure functions and generalizing them. We’ll continue to develop these skills in the chapters 
 ahead.",NA
Chapter 4. Handling errors without exceptions,"We noted briefly in 
 chapter 1
  that throwing exceptions is a side effect. If exceptions aren’t used in 
 functional code, what 
 is
  used instead? In this chapter, we’ll learn the basic principles for raising 
 and handling errors functionally. The big idea is that we can represent failures and exceptions with 
 ordinary values, and we can write higher-order functions that abstract out common patterns of 
 error handling and recovery. The functional solution, of returning errors as values, is safer and 
 retains referential transparency, and through the use of higher-order functions, we can preserve the 
 primary benefit of exceptions—
 consolidation of error-handling logic.
  We’ll see how this works 
 over the course of this chapter, after we take a closer look at exceptions and discuss some of their 
 problems.
  
 For the same reason that we created our own 
 List
  data type in the previous chapter, we’ll re-create 
 in this chapter two Scala standard library types: 
 Option
  and 
 Either
 . The purpose is to enhance your 
 understanding of how these types can be used for handling errors. After completing this chapter, 
 you should feel free to use the Scala standard library version of 
 Option
  and 
 Either
  (though you’ll 
 notice that the standard library versions of both types are missing some of the useful functions we 
 define in this chapter).",NA
4.1. The good and bad aspects of exceptions,"Why do exceptions break referential transparency, and why is that a problem? Let’s look at a 
 simple example. We’ll define a function that throws an exception and call it.
  
 Listing 4.1. Throwing and catching an exception
  
  
 Calling 
 failingFn
  from the REPL gives the expected error:
  
 scala> failingFn(12) 
  
 java.lang.Exception: fail!
  
  
  at .failingFn(<console>:8)
  
  ...
  
 We can prove that 
 y
  is not referentially transparent. Recall that any RT expression may be 
 substituted with the value it refers to, and this substitution should preserve program meaning. If we",NA
4.2. Possible alternatives to exceptions,"Let’s now consider a realistic situation where we might use an exception and look at different 
 approaches we could use instead. Here’s an implementation of a function that computes the mean of 
 a list, which is undefined if the list is empty:
  
  
  
 The 
 mean
  function is an example of what’s called a 
 partial function
 : it’s not defined for some 
 inputs. A function is typically partial because it makes some assumptions about its inputs that aren’t 
 implied by the input types.
 [
 1
 ]
  You may be used to throwing exceptions in this case, but we have a 
 few other options. Let’s look at these for our 
 mean
  example.
  
 1
  A function may also be partial if it doesn’t terminate for some inputs. We won’t discuss this form of partiality here, since it’s 
 not a recoverable error so there’s no question of how best to handle it. See the chapter notes for more about partiality.
  
 The first possibility is to return some sort of bogus value of type 
 Double
 . We could simply return 
 xs.sum / xs.length
  in all cases, and have it result in 
 0.0/0.0
  when the input is empty, which is 
 Double.NaN
 ; or we could return some other sentinel value. In other situations, we might return 
 null
  instead of a value of the needed type. This general class of approaches is how error handling 
 is often done in languages without exceptions, and we reject this solution for a few reasons:
  
  
 It allows errors to silently propagate—the caller can forget to check this condition and won’t 
 be 
  
 alerted by the compiler, which might result in subsequent code not working properly. Often 
 the 
  
 error won’t be detected until much later in the code.
  
  
 Besides being error-prone, it results in a fair amount of boilerplate code at call sites, with 
  
 explicit 
 if
  statements to check whether the caller has received a “real” result. This boilerplate 
  
 is magnified if you happen to be calling several functions, each of which uses error codes that 
  
 must be checked and aggregated in some way.
  
  
 It’s not applicable to polymorphic code. For some output types, we might not even 
 have
  a",NA
4.3. The Option data type,"The solution is to represent explicitly in the return type that a function may not always have an 
 answer. We can think of this as deferring to the caller for the error-handling strategy. We introduce 
 a new type, 
 Option
 . As we mentioned earlier, this type also exists in the Scala standard library, but 
 we’re re-creating it here for pedagogical purposes:
  
 sealed trait
  Option[+A] 
  
 case class
  Some[+A](get: A) 
 extends
  Option[A] 
 case object
  
 None 
 extends
  Option[Nothing]
  
 Option
  has two cases: it can be defined, in which case it will be a 
 Some
 , or it can be undefined, in 
 which case it will be 
 None
 .
  
 We can use 
 Option
  for our definition of 
 mean
  like so:
  
 def
  mean(xs: Seq[Double]): Option[Double] = 
  
 if
  (xs.isEmpty) None 
  
 else
  Some(xs.sum / xs.length)",NA
4.4. The Either data type,"The big idea in this chapter is that we can represent failures and exceptions with ordinary values, 
 and write functions that abstract out common patterns of error handling and recovery. 
 Option
  isn’t 
 the only data type we could use for this purpose, and although it gets used frequently, it’s rather 
  
 simplistic. One thing you may have noticed with 
 Option
  is that it doesn’t tell us anything about 
 what went wrong in the case of an exceptional condition. All it can do is give us 
 None
 , indicating 
 that there’s no value to be had. But sometimes we want to know more. For example, we might want 
 a 
 String
  that gives more information, or if an exception was raised, we might want to know what 
 that error actually was.
  
 We can craft a data type that encodes whatever information we want about failures. Sometimes 
 just knowing whether a failure occurred is sufficient, in which case we can use 
 Option
 ; other times 
 we want more information. In this section, we’ll walk through a simple extension to 
 Option
 , the",NA
4.5. Summary,"In this chapter, we noted some of the problems with using exceptions and introduced the basic 
 principles of purely functional error handling. Although we focused on the algebraic data types 
 Option
  and 
 Either
 , the bigger idea is that we can represent exceptions as ordinary values and use 
 higher-order functions to encapsulate common patterns of handling and propagating errors. This 
 general idea, of representing effects as values, is something we’ll see again and again throughout 
 this book in various guises.
  
 We don’t expect you to be fluent with all the higher-order functions we wrote in this chapter, but 
 you should now have enough familiarity to get started writing your own functional code complete 
 with error handling. With these new tools in hand, exceptions should be reserved only for truly 
  
 unrecoverable conditions.
  
 Lastly, in this chapter we touched briefly on the notion of a 
 non-strict
  function (recall the functions 
 orElse
 , 
 getOrElse
 , and 
 Try
 ). In the next chapter, we’ll look more closely at why non-strictness is 
 important and how it can buy us greater modularity and efficiency in our functional programs.",NA
Chapter 5. Strictness and laziness,"In 
 chapter 3
  we talked about purely functional data structures, using singly linked lists as an 
 example.
  
 We covered a number of bulk operations on lists—
 map
 , 
 filter
 , 
 foldLeft
 , 
 foldRight
 , 
 zipWith
 , and 
 so on. We noted that each of these operations makes its own pass over the input and constructs a 
 fresh list for the output.
  
 Imagine if you had a deck of cards and you were asked to remove the odd-numbered cards and then
  
 flip over all the queens. Ideally, you’d make a single pass through the deck, looking for queens and
  
 odd-numbered cards at the same time. This is more efficient than removing the odd cards and then
  
 looking for queens in the remainder. And yet the latter is what Scala is doing in the following 
 code:
 [
 1
 ]
  
 1
  We’re now using the Scala standard library’s 
 List
  type here, where 
 map
  and 
 filter
  are methods on 
 List
  rather than 
 standalone functions like the ones we wrote in 
 chapter 3
 .
  
 scala> List(1,2,3,4).map(_ + 10).filter(_ % 2 == 0).map(_ * 3) List(36,42)
  
 In this expression, 
 map(_ + 10)
  will produce an intermediate list that then gets passed to 
 filter(_ 
 % 2 == 0)
 , which in turn constructs a list that gets passed to 
 map(_ * 3)
 , which then produces the 
 final list. In other words, each transformation will produce a temporary list that
  
 only ever gets used as input to the next transformation and is then immediately discarded.
  
 Think about how this program will be evaluated. If we manually produce a trace of its evaluation, 
 the
  
 steps would look something like this.
  
 Listing 5.1. Program trace for 
 List
  
 List(1,2,3,4).map(_ + 10).filter(_ % 2 == 0).map(_ * 3)
  
 List(11,12,13,14).filter(_ % 2 == 0).map(_ * 3)
  
 List(12,14).map(_ * 3)
  
 List(36,42)
  
 Here we’re showing the result of each substitution performed to evaluate our expression. For
  
 example, to go from the first line to the second, we’ve replaced 
 List(1,2,3,4).map(_ + 10)
  with 
 List(11,12,13,14)
 , based on the definition of 
 map
 .
 [
 2
 ]
  This view makes it clear how the calls to 
 map
  and 
 filter
  each perform their own traversal of the input and allocate lists for the output. 
 Wouldn’t it be nice if we could somehow fuse sequences of transformations like this into a
  
 single pass and avoid creating temporary data structures? We could rewrite the code into a 
 while 
 loop by hand, but ideally we’d like to have this done automatically while retaining the same 
 high-",NA
5.1. Strict and non-strict functions,"Before we get to our example of lazy lists, we need to cover some basics. What are strictness 
 and non-strictness, and how are these concepts expressed in Scala?
  
 Non-strictness is a property of a function. To say a function is non-strict just means that the 
 function may choose 
 not
  to evaluate one or more of its arguments. In contrast, a 
 strict
  function 
 always evaluates its arguments. Strict functions are the norm in most programming languages, 
 and indeed most languages only support functions that expect their arguments fully evaluated. 
 Unless we tell it otherwise, any function definition in Scala will be strict (and all the functions 
 we’ve defined so far have been strict). As an example, consider the following function:
  
 def
  square(x: Double): Double = x * x
  
 When you invoke 
 square(41.0 + 1.0)
 , the function 
 square
  will receive the evaluated value of 
 42.0
  because it’s strict. If you invoke 
 square(sys.error(""failure""))
 , you’ll get an exception before 
 square
  has a chance to do anything, since the 
 sys.error (""failure"") 
 expression will be evaluated 
 before entering the body of 
 square
 .
  
 Although we haven’t yet learned the syntax for indicating non-strictness in Scala, you’re almost 
 certainly familiar with the concept. For example, the short-circuiting Boolean functions 
 &&
  and 
 ||
 , 
 found in many programming languages including Scala, are non-strict. You may be used to thinking 
 of 
 &&
  and 
 ||
  as built-in syntax, part of the language, but you can also think of them as functions that 
 may choose not to evaluate their arguments. The function 
 &&
  takes two 
 Boolean
  arguments, but 
 only evaluates the second argument if the first is 
 true
 :
  
 scala> false && { println(""!!""); true } // does not print anything res0: Boolean = false
  
 And 
 ||
  only evaluates its second argument if the first is 
 false
 :
  
 scala> true || { println(""!!""); false } // doesn't print anything either res1: Boolean = true
  
 Another example of non-strictness is the 
 if
  control construct in Scala:
  
 val
  result = 
 if
  (input.isEmpty) sys.error(""empty input"") 
 else
  input",NA
5.2. An extended example: lazy lists,"Let’s now return to the problem posed at the beginning of this chapter. We’ll explore how laziness 
 can be used to improve the efficiency and modularity of functional programs using 
 lazy lists
 , or 
 streams
 , as an example. We’ll see how chains of transformations on streams are fused into a single 
 pass through the use of laziness. Here’s a simple 
 Stream
  definition. There are a few new things here 
 we’ll discuss next.
  
 Listing 5.2. Simple definition for 
 Stream",NA
5.3. Separating program description from evaluation,"A major theme in functional programming is 
 separation of concerns
 . We want to separate the
  
 description of computations from actually running them. We’ve already touched on this theme in
  
 previous chapters in different ways. For example, first-class functions capture some computation in
  
 their bodies but only execute it once they receive their arguments. And we used 
 Option
  to capture 
 the fact that an error occurred, where the decision of what to do about it became a separate 
 concern.
  
 With 
 Stream
 , we’re able to build up a computation that produces a sequence of elements 
 without running the steps of that computation until we actually need those elements.
  
 More generally speaking, laziness lets us separate the description of an expression from the
  
 evaluation of that expression. This gives us a powerful ability—we may choose to describe a
  
 “larger” expression than we need, and then evaluate only a portion of it. As an example, let’s look at
  
 the function 
 exists
  that checks whether an element matching a 
 Boolean
  function exists in this 
 Stream
 :
  
 def
  exists(p: A => Boolean): Boolean = 
 this match
  { 
  
 case
  Cons(h, 
 t) => p(h()) || t().exists(p) 
  
  
 case
  _ => 
 false 
  
 }
  
 Note that 
 ||
  is non-strict in its second argument. If 
 p(h())
  returns 
 true
 , then 
 exists
  terminates the 
 traversal early and returns 
 true
  as well. Remember also that the tail of the stream is a 
 lazy val
 . So 
 not only does the traversal terminate early, the tail of the stream is never evaluated at all! So 
 whatever code would have generated the tail is never actually executed.
  
 The 
 exists
  function here is implemented using explicit recursion. But remember that with 
 List
  in 
 chapter 3
 , we could implement a general recursion in the form of 
 foldRight
 . We can do the same 
 thing for 
 Stream
 , but lazily:",NA
5.4. Infinite streams and corecursion,"Because they’re incremental, the functions we’ve written also work for 
 infinite streams
 . Here’s an
  
 example of an infinite 
 Stream
  of 
 1
 s:
  
 val
  ones: Stream[Int] = Stream.cons(1, ones)
  
 Although 
 ones
  is infinite, the functions we’ve written so far only inspect the portion of the 
 stream needed to generate the demanded output. For example:
  
 scala> ones.take(5).toList 
  
 res0: List[Int] = List(1, 1, 1, 1, 1)
  
 scala> ones.exists(_ % 2 != 0) 
  
 res1: Boolean = true
  
 Try playing with a few other examples:
  
  
 ones.map(_ + 1).exists(_ % 2 == 0)
  
  
 ones.takeWhile(_ == 1)
  
  
 ones.forAll(_ != 1)
  
 In each case, we get back a result immediately. Be careful though, since it’s easy to write 
 expressions
  
 that never terminate or aren’t stack-safe. For example, 
 ones.forAll(_ == 1)
  will forever need to 
 inspect more of the series since it’ll never encounter an element that allows it to terminate with a
  
 definite answer (this will manifest as a stack overflow rather than an infinite loop).
 [
 6
 ]
  
 6
  It’s possible to define a stack-safe version of 
 forAll
  using an ordinary recursive loop.",NA
5.5. Summary,"In this chapter, we introduced non-strictness as a fundamental technique for implementing efficient 
 and modular functional programs. Non-strictness can be thought of as a technique for recovering 
 some efficiency when writing functional code, but it’s also a much bigger idea—non-strictness can 
 improve modularity by separating the description of an expression from the how-and-when of its 
 evaluation. Keeping these concerns separate lets us reuse a description in multiple contexts, 
 evaluating different portions of our expression to obtain different results. We weren’t able to do that 
 when description and evaluation were intertwined as they are in strict code. We saw a number of 
 examples of this principle in action over the course of the chapter, and we’ll see many more in the 
 remainder of the book.
  
 We’ll switch gears in the next chapter and talk about purely functional approaches to 
 state
 . This is 
 the last building block needed before we begin exploring the process of functional design.",NA
Chapter 6. Purely functional state,"In this chapter, we’ll see how to write purely functional programs that manipulate state, using the 
 simple domain of 
 random number generation
  as the example. Although by itself it’s not the most 
 compelling use case for the techniques in this chapter, the simplicity of random number generation 
 makes it a good first example. We’ll see more compelling use cases in 
 parts 3
  and 
 4
  of the book, 
 especially 
 part 4
 , where we’ll say a lot more about dealing with state and effects. The goal here is 
 to give you the basic pattern for how to make 
 any
  stateful API purely functional. As you start 
 writing your own functional APIs, you’ll likely run into many of the same questions that we’ll 
 explore here.",NA
6.1. Generating random numbers using side effects,"If you need to generate random
 [
 1
 ]
  numbers in Scala, there’s a class in the standard library, 
  
 scala.util.Random
 ,
 [
 2
 ]
  with a pretty typical imperative API that relies on side effects. Here’s an 
 example of its use.
  
 1
  Actually, pseudo-random, but we’ll ignore this distinction.
  
 2
  Scala API link: 
 http://mng.bz/3DP7
 .
  
 Listing 6.1. Using 
 scala.util.Random
  to generate random numbers
  
  
 Even if we didn’t know anything about what happens inside 
 scala.util.Random
 , we can assume that 
 the object 
 rng
  has some internal state that gets updated after each invocation, since we’d otherwise 
 get the same value each time we called 
 nextInt
  or 
 nextDouble
 . Because the state updates are 
 performed as a side effect, these methods aren’t referentially transparent. And as we know, this 
 implies that they aren’t as testable, composable, modular, and easily parallelized as they could be.
  
 Let’s just take testability as an example. If we want to write a method that makes use of 
 randomness, we need tests to be reproducible. Let’s say we had the following side-effecting method, 
 intended to simulate the rolling of a single six-sided die, which 
 should
  return a value between 1 and 
 6, inclusive:",NA
6.2. Purely functional random number generation,"The key to recovering referential transparency is to make the state updates 
 explicit
 . Don’t update 
 the state as a side effect, but simply return the new state along with the value that we’re generating.
  
 Here’s one possible interface to a random number generator:
  
 trait
  RNG { 
  
  
 def
  nextInt: (Int, RNG) 
  
 }
  
 This method should generate a random 
 Int
 . We’ll later define other functions in terms of 
 nextInt
 . 
 Rather than returning only the generated random number (as is done in 
 scala.util.Random
 ) and 
 updating some internal state by 
 mutating
  it in place, we return the random number and the new 
 state, leaving the old state unmodified.
 [
 3
 ]
  In effect, we separate the concern of 
 computing
  what the 
 next state is from the concern of 
 communicating
  the new state to the rest of the program. No global 
 mutable memory is being used—we simply return the next state back to the caller. This leaves the 
 caller of 
 nextInt
  in complete control of what to do with the new state. Note that we’re still",NA
6.3. Making stateful APIs pure,"This problem of making seemingly stateful APIs pure and its solution (having the API 
 compute
  the 
 next state rather than actually mutate anything) aren’t unique to random number generation. It 
 comes up frequently, and we can always deal with it in this same way.
 [
 4
 ]
  
 4
  An efficiency loss comes with computing next states using pure functions, because it means we can’t actually mutate the data 
 in place. (Here, it’s not really a problem since the state is just a single 
 Long
  that must be copied.) This loss of efficiency can be 
 mitigated by using efficient purely functional data structures. It’s also possible in some cases to mutate the data in place without 
 breaking referential transparency, which we’ll talk about in 
 part 4
 .
  
 For instance, suppose you have a class like this:
  
 class
  Foo { 
  
  
 private var
  s: FooState = ...
  
 def
  bar: Bar 
  
 def
  baz: Int 
  
 }
  
 Suppose 
 bar
  and 
 baz
  each mutate 
 s
  in some way. We can mechanically translate this to the 
 purely functional API by making explicit the transition from one state to the next:
  
 trait
  Foo { 
  
  
 def
  bar: (Bar, Foo) 
  
  
 def
  baz: (Int, Foo) 
  
 }
  
 Whenever we use this pattern, we make the caller responsible for passing the computed next state 
 through the rest of the program. For the pure 
 RNG
  interface just shown, if we reuse a previous 
 RNG
 , 
 it will always generate the same value it generated before. For instance:
  
 def
  randomPair(rng: RNG): (Int,Int) = { 
  
   
 val
  (i1,_) = rng.nextInt 
  
   
 val
  (i2,_) = rng.nextInt
  
  
  (i1,i2) 
  
 }",NA
6.4. A better API for state actions,"Looking back at our implementations, we’ll notice a common pattern: each of our functions has a 
 type
  
 of the form 
 RNG => (A, RNG)
  for some type 
 A
 . Functions of this type are called 
 state actions
  or 
 state transitions
  because they transform 
 RNG
  states from one to the next. These state actions can be 
 combined using 
 combinators
 , which are higher-order functions that we’ll define in this section. 
 Since
  
 it’s pretty tedious and repetitive to pass the state along ourselves, we want our combinators to pass
  
 the state from one action to the next automatically.
  
 To make the type of actions convenient to talk about, and to simplify our thinking about them, let’s
  
 make a type alias for the 
 RNG
  state action data type:
  
 type
  Rand[+A] = RNG => (A, RNG)
  
 We can think of a value of type 
 Rand[A]
  as “a randomly generated 
 A
 ,” although that’s not really 
 precise. It’s really a state action—a 
 program
  that depends on some 
 RNG
 , uses it to generate an 
 A
 , 
 and also transitions the 
 RNG
  to a new state that can be used by another action later.",NA
6.5. A general state action data type,"The functions we’ve just written—
 unit
 , 
 map
 , 
 map2
 , 
 flatMap
 , and 
 sequence
 —aren’t really specific 
 to random number generation at all. They’re general-purpose functions for working with state
  
 actions, and don’t care about the type of the state. Note that, for instance, 
 map
  doesn’t care that 
 it’s dealing with 
 RNG
  state actions, and we can give it a more general signature:
  
 def
  map[S,A,B](a: S => (A,S))(f: A => B): S => (B,S)
  
 Changing this signature doesn’t require modifying the implementation of 
 map
 ! The more 
 general signature was there all along; we just didn’t see it.
  
 We should then come up with a more general type than 
 Rand
 , for handling any type of state:
  
 type
  State[S,+A] = S => (A,S)
  
 Here 
 State
  is short for 
 computation that carries some state along
 , or 
 state action
 , 
 state 
 transition
 , or even 
 statement
  (see the next section). We might want to write it as its own class,
  
 wrapping the underlying function like this:
  
 case class
  State[S,+A](run: S => (A,S))",NA
6.6. Purely functional imperative programming,"In the preceding sections, we were writing functions that followed a definite pattern. We’d run a 
 state action, assign its result to a 
 val
 , then run another state action that used that 
 val
 , assign its result 
 to another 
 val
 , and so on. It looks a lot like 
 imperative
  programming.
  
 In the imperative programming paradigm, a program is a sequence of statements where each 
 statement may modify the program state. That’s exactly what we’ve been doing, except that our 
 “statements” are really 
 State
  actions, which are really functions. As functions, they read the current 
 program state simply by receiving it in their argument, and they write to the program state simply 
 by returning a value.
  
  
 Aren’t imperative and functional programming opposites?
  
 Absolutely not. Remember, functional programming is simply programming without side effects. 
 Imperative programming is about programming with statements that modify some program state, 
 and as we’ve seen, it’s entirely reasonable to maintain state without side effects.
  
 Functional programming has excellent support for writing imperative programs, with the added 
 benefit that such programs can be reasoned about equationally because they’re referentially 
 transparent. We’ll have much more to say about equational reasoning about programs in 
 part 2
 , 
 and imperative programs in particular in 
 parts 3
  and 
 4
 .",NA
6.7. Summary,"In this chapter, we touched on the subject of how to write purely functional programs that have 
 state.
  
 We used random number generation as the motivating example, but the overall pattern comes up in 
 many different domains. The idea is simple: use a pure function that accepts a state as its argument,",NA
Part 2. Functional design and combinator libraries,"We said in 
 chapter 1
  that functional programming is a radical premise that affects how we write and 
 organize programs at every level. In 
 part 1
 , we covered the fundamentals of FP and saw how the 
 commitment to using only pure functions affects the basic building blocks of programs: loops, data 
 structures, exceptions, and so on. In 
 part 2
 , we’ll see how the assumptions of functional 
 programming affect 
 library design
 .
  
 We’ll create three useful libraries in 
 part 2
 —one for parallel and asynchronous computation, 
 another for testing programs, and a third for parsing text. There won’t be much in the way of new 
 syntax or language features, but we’ll make heavy use of the material already covered. Our primary 
 goal isn’t to teach you about parallelism, testing, and parsing. The primary goal is 
 to help you 
 develop skill in designing functional libraries
 , even for domains that look nothing like the ones 
 here.
  
 This part of the book will be a somewhat meandering journey. Functional design can be a messy, 
 iterative process. We hope to show at least a stylized view of how functional design proceeds in the 
 real world. Don’t worry if you don’t follow every bit of discussion. These chapters should be like 
 peering over the shoulder of someone as they think through possible designs. And because no two 
 people approach this process the same way, the particular path we walk in each case might not 
 strike you as the most natural one—perhaps it considers issues in what seems like an odd order, 
 skips too fast, or goes too slow. Keep in mind that when you design your own functional libraries, 
 you get to do it at your own pace, take whatever path you want, and, whenever questions come up 
 about design choices, you get to think through the consequences in whatever way makes sense for 
 you, which could include running little experiments, creating prototypes, and so on.
  
 There are no right answers in functional library design. Instead, we have a collection of 
 design 
 choices
 , each with different trade-offs. Our goal is that you start to understand these trade-offs and 
 what different choices mean. Sometimes, when designing a library, we’ll come to a fork in the 
 road.
  
 In this text we may, for pedagogical purposes, deliberately make a choice with undesirable 
  
 consequences that we’ll uncover later. We want you to see this process first-hand, because it’s part 
 of what actually occurs when designing functional programs. We’re less interested in the particular 
 libraries covered here in 
 part 2
 , and more interested in giving you insight into how functional 
 design proceeds and how to navigate situations that you will likely encounter. Library design is not 
 something that only a select few people get to do; it’s part of the day-today work of ordinary 
 functional programming. In these chapters and beyond, you should absolutely feel free to 
 experiment, play with different design choices, and develop your own aesthetic.
  
 One final note: as you work through 
 part 2
 , you may notice repeated patterns of similar-looking 
 code.",NA
Chapter 7. Purely functional parallelism,"Because modern computers have multiple cores per CPU, and often multiple CPUs, it’s more 
 important than ever to design programs in such a way that they can take advantage of this 
 parallel processing power. But the interaction of programs that run with parallelism is complex, 
 and the traditional mechanism for communication among execution threads—shared mutable 
 memory—is notoriously difficult to reason about. This can all too easily result in programs that 
 have race conditions and deadlocks, aren’t readily testable, and don’t scale well.
  
 In this chapter, we’ll build a purely functional library for creating parallel and asynchronous 
 computations. We’ll rein in the complexity inherent in parallel programs by describing them using 
 only pure functions. This will let us use the substitution model to simplify our reasoning and 
 hopefully make working with concurrent computations both easy and enjoyable.
  
 What you should take away from this chapter is not only how to write a library for purely functional 
 parallelism, but 
 how to approach the problem of designing a purely functional library
 . Our main 
 concern will be to make our library highly composable and modular. To this end, we’ll keep with 
 our theme of separating the concern of 
 describing
  a computation from actually 
 running
  it. We want 
 to allow users of our library to write programs at a very high level, insulating them from the nitty-
 gritty of how their programs will be executed. For example, towards the end of the chapter we’ll 
 develop a combinator, 
 parMap
 , that will let us easily apply a function 
 f
  to every element in a 
 collection simultaneously:
  
 val
  outputList = parMap(inputList)(f)
  
 To get there, we’ll work iteratively. We’ll begin with a simple use case that we’d like our library 
 to handle, and then develop an interface that facilitates this use case. Only then will we consider 
 what our implementation of this interface should be. As we keep refining our design, we’ll 
 oscillate between the interface and implementation as we gain a better understanding of the 
 domain and the design space through progressively more complex use cases. We’ll emphasize 
 algebraic reasoning 
 and introduce the idea that an API can be described by 
 an algebra
  that obeys 
 specific 
 laws
 .
  
 Why design our own library? Why not just use the concurrency primitives that come with Scala’s 
 standard library in the 
 scala.concurrent
  package? This is partially for pedagogical purposes—we 
 want to show you how easy it is to design your own practical libraries. But there’s another reason: 
 we want to encourage the view that no existing library is authoritative or beyond 
  
 reexamination, even if designed by experts and labeled “standard.” There’s a certain safety in doing 
 what everybody else does, but what’s conventional isn’t necessarily the most practical. Most 
 libraries contain a lot of arbitrary design choices, many made unintentionally. When you start from 
 scratch, you get to revisit all the fundamental assumptions that went into designing the library, take 
 a different path, and discover things about the problem space that others may not have even 
 considered. As a result, you might arrive at your own design that suits your purposes better. In this",NA
7.1. Choosing data types and functions,"When you begin designing a functional library, you usually have some general ideas about what 
 you want to be able to do, and the difficulty in the design process is in refining these ideas and 
 finding a data type that enables the functionality you want. In our case, we’d like to be able to 
 “create parallel computations,” but what does that mean exactly? Let’s try to refine this into 
 something we can implement by examining a simple, parallelizable computation—summing a list 
 of integers. The usual left fold for this would be as follows:
  
 def
  sum(ints: Seq[Int]): Int =
  
  
  ints.foldLeft(0)((a,b) => a + b)
  
 Here 
 Seq
  is a superclass of lists and other sequences in the standard library. Importantly, it has 
 a 
 foldLeft
  method.
  
 Instead of folding sequentially, we could use a divide-and-conquer algorithm; see the 
 following listing.
  
 Listing 7.1. Summing a list using a divide-and-conquer algorithm
  
 We divide the sequence in half using the 
 splitAt
  function, recursively sum both halves, and then 
 combine their results. And unlike the 
 foldLeft
 -based implementation, this implementation can be 
 parallelized—the two halves can be summed in parallel.
  
  
 The importance of simple examples
  
 Summing integers is in practice probably so fast that parallelization imposes more overhead than it 
 saves. But simple examples like this are exactly the kind that are most helpful to consider when 
 designing a functional library. Complicated examples include all sorts of incidental structure and 
 extraneous detail that can confuse the initial design process. We’re trying to explain the essence of 
 the problem domain, and a good way to do this is to start with trivial examples, factor out common 
 concerns across these examples, and gradually add complexity. In functional design, our goal is to",NA
7.2. Picking a representation,"Just by exploring this simple example and thinking through the consequences of different choices,
  
 we’ve sketched out the following API.
  
 Listing 7.4. Basic sketch for an API for 
 Par
  
 We’ve also loosely assigned meaning to these various functions:
  
  
  
  
  
  
 unit
  promotes a constant value to a parallel computation.
  
 map2
  combines the results of two parallel computations with a binary function.
  
 fork
  marks a computation for concurrent evaluation. The evaluation won’t actually occur until 
 forced by 
 run
 .
  
 lazyUnit
  wraps its unevaluated argument in a 
 Par
  and marks it for concurrent evaluation. 
 run
  extracts a value from a 
 Par
  by actually performing the computation.
  
 At any point while sketching out an API, you can start thinking about possible 
 representations
  for 
 the
  
 abstract types that appear.",NA
7.3. Refining the API,"The way we’ve worked so far is a bit artificial. In practice, there aren’t such clear boundaries 
 between designing the API and choosing a representation, and one doesn’t necessarily precede the 
 other. Ideas for a representation can inform the API, the API can inform the choice of 
 representation, and it’s natural to shift fluidly between these two perspectives, run experiments as 
 questions arise, build prototypes, and so on.
  
 We’ll devote this section to exploring our API. Though we got a lot of mileage out of considering a 
 simple example, before we add any new primitive operations, let’s try to learn more about what’s 
 expressible using those we already have. With our primitives and choices of meaning for them, 
 we’ve carved out a little universe for ourselves. We now get to discover what ideas are expressible 
 in this universe. This can and should be a fluid process—we can change the rules of our universe at 
 any time, make a fundamental change to our representation or introduce a new primitive, and 
 explore how our creation then behaves.
  
 Let’s begin by implementing the functions of the API that we’ve developed so far. Now that we 
 have a representation for 
 Par
 , a first crack at it should be straightforward. What follows is a 
 simplistic implementation using the representation of 
 Par
  that we’ve chosen.
  
 Listing 7.5. Basic implementation for 
 Par",NA
7.4. The algebra of an API,"As the previous section demonstrates, we often get far just by writing down the type signature for 
 an operation we want, and then “following the types” to an implementation. When working this 
 way, we can almost forget the concrete domain (for instance, when we implemented 
 map
  in terms 
 of 
 map2 
 and 
 unit
 ) and just focus on lining up types. This isn’t cheating; it’s a natural style of 
 reasoning, analogous to the reasoning one does when simplifying an algebraic equation. We’re 
 treating the API as an 
 algebra
 ,
 [
 6
 ]
  or an abstract set of operations along with a set of 
 laws
  or 
 properties we assume to be true, and simply doing formal symbol manipulation following the rules 
 of the game specified by this algebra.
  
 6
  We do mean algebra in the mathematical sense of one or more sets, together with a collection of functions operating on objects 
 of these sets, and a set of 
 axioms
 . Axioms are statements assumed true from which we can derive other 
 theorems
  that must also 
 be true. In our case, the sets are particular types like 
 Par[A]
  and 
 List[Par[A]]
 , and the functions are operations like 
 map2
 , 
 unit
 , 
 and 
 sequence
 .
  
 Up until now, we’ve been reasoning somewhat informally about our API. There’s nothing wrong 
 with this, but it can be helpful to take a step back and formalize what laws you expect to hold (or 
 would like to hold) for your API.
 [
 7
 ]
  Without realizing it, you’ve probably mentally built up a model 
 of what properties or laws you expect. Actually writing these down and making them precise can 
 highlight design choices that wouldn’t be otherwise apparent when reasoning informally.",NA
7.5. Refining combinators to their most general form,"Functional design is an iterative process. After you write down your API and have at least a 
 prototype implementation, try using it for progressively more complex or realistic 
 scenarios.
  
 Sometimes you’ll find that these scenarios require new combinators. But before jumping right to 
 implementation, it’s a good idea to see if you can refine the combinator you need to 
 its most 
 general form
 . It may be that what you need is just a specific case of some more general 
 combinator.
  
  
 About the exercises in this section
  
 The exercises and answers in this section use our original simpler (blocking) representation of 
 Par[A]
 . If you’d like to work through the exercises and answers using the non-blocking 
  
 implementation we developed in the previous section instead, see the file 
 Nonblocking.scala 
 in 
 both the 
 exercises
  and 
 answers
  projects.",NA
7.6. Summary,"We’ve now completed the design of a library for defining parallel and asynchronous computations 
 in a purely functional way. Although this domain is interesting, the primary goal of this chapter was 
 to give you a window into the process of functional design, a sense of the sorts of issues you’re 
 likely to encounter, and ideas for how you can handle those issues.
  
 Chapters 4
  through 
 6
  had a strong theme of 
 separation of concerns
 : specifically, the idea of 
 separating the description of a computation from the interpreter that then runs it. In this chapter, we 
 saw that principle in action in the design of a library that describes parallel computations as values 
 of a data type 
 Par
 , with a separate interpreter 
 run
  to actually spawn the threads to execute them.
  
 In the next chapter, we’ll look at a completely different domain, take another meandering 
 journey toward an API for that domain, and draw further lessons about functional design.",NA
Chapter 8. Property-based testing,"In 
 chapter 7
  we worked through the design of a functional library for expressing parallel 
  
 computations. There we introduced the idea that an API should form an 
 algebra
 —that is, a 
 collection of data types, functions over these data types, and importantly, 
 laws
  or 
 properties
  that 
 express relationships between these functions. We also hinted at the idea that it might be possible to 
 somehow check these laws automatically.
  
 This chapter will take us toward a simple but powerful library for 
 property-based testing
 . The 
 general idea of such a library is to decouple the specification of program behavior from the 
 creation of test cases. The programmer focuses on specifying the behavior of programs and giving 
 high-level constraints on the test cases; the framework then automatically generates test cases that 
 satisfy these constraints, and runs tests to ensure that programs behave as specified.
  
 Although a library for testing has a very different purpose than a library for parallel 
 computations, we’ll discover that these libraries have a lot of surprisingly similar combinators. 
 This similarity is something we’ll return to in 
 part 3
 .",NA
8.1. A brief tour of property-based testing,"As an example, in ScalaCheck (
 http://mng.bz/n2j9
 ), a property-based testing library for Scala, 
 a property looks something like this.
  
 Listing 8.1. ScalaCheck properties
  
  
 And we can check properties like so:
  
 scala> prop.check 
  
 + OK, passed 100 tests.
  
 scala> failingProp.check 
  
 ! Falsified after 6 passed tests.
  
 > ARG_0: List(0, 1)
  
 Here, 
 intList
  is not a 
 List[Int]
 , but a 
 Gen[List[Int]]
 , which is something that knows how to 
 generate test data of type 
 List[Int]
 . We can 
 sample
  from this generator, and it will produce lists of 
 different lengths, filled with random numbers between 0 and 100. Generators in a property-based 
 testing library have a rich API. We can combine and compose generators in different",NA
8.2. Choosing data types and functions,"This section will be another messy and iterative process of discovering data types and functions for 
 our library. This time around, we’re designing a library for property-based testing. As before, this is 
 a chance to peer over the shoulder of someone working through possible designs. The particular 
 path we take and the library we arrive at isn’t necessarily the same as what you would come up with",NA
8.3. Test case minimization,"Earlier, we mentioned the idea of test case minimization. That is, ideally we’d like our framework 
 to find the smallest or simplest failing test case, to better illustrate the problem and facilitate 
 debugging.
  
 Let’s see if we can tweak our representations to support this outcome. There are two 
 general approaches we could take:
  
  
 Shrinking
 —After we’ve found a failing test case, we can run a separate procedure to minimize 
  
 the test case by successively decreasing its “size” until it no longer fails. This is called 
  
 shrinking
 , and it usually requires us to write separate code for each data type to implement this 
  
 minimization process.
  
  
 Sized generation
 —Rather than shrinking test cases after the fact, we simply generate our test 
  
 cases in order of increasing size and complexity. So we start small and increase the size until 
 we 
  
 find a failure. This idea can be extended in various ways to allow the test runner to 
 make larger 
  
 jumps in the space of possible sizes while still making it possible to find the 
 smallest failing 
  
 test.
  
 ScalaCheck, incidentally, takes the first approach: shrinking. There’s nothing wrong with this 
 approach (it’s also used by the Haskell library QuickCheck that ScalaCheck is based on: 
  
 http://mng.bz/E24n
 ), but we’ll see what we can do with sized generation. It’s a bit simpler and in 
 some ways more modular, because our generators only need to know how to generate a test case of 
 a given size. They don’t need to be aware of the “schedule” used to search the space of test cases, 
 and the function that runs the tests therefore has the freedom to choose this schedule. We’ll see 
 how this plays out shortly.
  
 Instead of modifying our 
 Gen
  data type, for which we’ve already written a number of useful 
 combinators, let’s introduce sized generation as a separate layer in our library. A simple 
 representation of a sized generator is just a function that takes a size and produces a 
 generator:
  
 case class
  SGen[+A](forSize: Int => Gen[A])
  
  
 Exercise 8.10",NA
8.4. Using the library and improving its usability,"We’ve converged on what seems like a reasonable API. We could keep tinkering with it, but at this 
 point let’s try 
 using
  the library to construct tests and see if we notice any deficiencies, either in 
 what it can express or in its general usability. Usability is somewhat subjective, but we generally 
 like to have convenient syntax and appropriate helper functions for common usage patterns. We 
 aren’t necessarily aiming to make the library more expressive, but we want to make it pleasant to 
 use.
  
 8.4.1. Some simple examples
  
 Let’s revisit an example that we mentioned at the start of this chapter—specifying the behavior of 
 the function 
 max
 , available as a method on 
 List
  (API docs link: 
 http://mng.bz/Pz86
 ). The maximum 
 of a list should be greater than or equal to every other element in the list. Let’s specify this:
  
  
 At this point, calling 
 run
  directly on a 
 Prop
  is rather cumbersome. We can introduce a helper 
 function for running our 
 Prop
  values and printing their result to the console in a useful format. 
 Let’s make this a method on the 
 Prop
  companion object.
  
 Listing 8.5. A 
 run
  helper function for 
 Prop",NA
8.5. Testing higher-order functions and future directions,"So far, our library seems quite expressive, but there’s one area where it’s lacking: we don’t 
 currently
  
 have a good way to test higher-order functions. While we have lots of ways of generating 
 data
  
 using
  
 our generators, we don’t really have a good way of generating 
 functions
 .
  
 For instance, let’s consider the 
 takeWhile
  function defined for 
 List
  and 
 Stream
 . Recall that this 
 function returns the longest prefix of its input whose elements all satisfy a predicate. For instance,
  
 List(1,2,3).takeWhile(_ < 3)
  results in 
 List(1,2)
 . A simple property we’d like to check is that for 
 any list, 
 s: List[A]
 , and any 
 f: A => Boolean
 , the expression 
  
 s.takeWhile(f).forall(f)
  evaluates to 
 true
 . That is, every element in the returned list satisfies the 
 predicate.
 [
 11
 ]
  
 11
  In the Scala standard library, 
 forall
  is a method on 
 List
  and 
 Stream
  with the signature 
 def forall[A] (f: A => Boolean): 
 Boolean
 .
  
  
 Exercise 8.18
  
  
 Come up with some other properties that 
 takeWhile
  should satisfy. Can you think of a good 
 property expressing the relationship between 
 takeWhile
  and 
 dropWhile
 ?
  
  
 We could certainly take the approach of only examining 
 particular
  arguments when testing higher-
  
 order functions. For instance, here’s a more specific property for 
 takeWhile
 :
  
 val
  isEven = (i: Int) => i%2 == 0 
  
 val
  takeWhileProp =
  
  
  Prop.forAll(Gen.listOf(int))(ns => ns.takeWhile(isEven).forall(isEven))
  
 This works, but is there a way we could let the testing framework handle generating functions to use
  
 with 
 takeWhile
 ?
 [
 12
 ]
  Let’s consider our options. To make this concrete, let’s suppose we have a 
 Gen[Int]
  and would like to produce a 
 Gen[String => Int]
 . What are some ways we could do that?",NA
8.6. The laws of generators,NA,NA
8.7. Summary,"In this chapter, we worked through another extended exercise in functional library design, using 
 the domain of property-based testing as inspiration.
  
 We reiterate that our goal was not necessarily to learn about property-based testing as such, but to 
 highlight particular aspects of functional design. First, we saw that oscillating between the 
 abstract algebra and the concrete representation lets the two inform each other. This avoids 
 overfitting the library to a particular representation, and also avoids ending up with a floating 
 abstraction disconnected from the end goal.
  
 Second, we noticed that this domain led us to discover many of the same combinators we’ve now 
 seen a few times before: 
 map
 , 
 flatMap
 , and so on. Not only are the signatures of these functions 
 analogous, the 
 laws
  satisfied by the implementations are analogous too. There are a great many 
 seemingly distinct 
 problems
  being solved in the world of software, yet the space of functional 
 solutions
  is much smaller. Many libraries are just simple combinations of certain fundamental 
 structures that appear over and over again across a variety of different domains. This is an 
  
 opportunity for code reuse that we’ll exploit in 
 part 3
 , when we learn both the names of some of 
 these structures and how to spot more general abstractions.
  
 In the next and final chapter of 
 part 2
 , we’ll look at another domain, 
 parsing
 , with its own unique 
 challenges. We’ll take a slightly different approach in that chapter, but once again familiar 
 patterns will emerge.",NA
Chapter 9. Parser combinators,"In this chapter, we’ll work through the design of a combinator library for creating 
 parsers
 . We’ll 
 use JSON parsing (
 http://mng.bz/DpNA
 ) as a motivating use case. Like 
 chapters 7
  and 
 8
 , this 
 chapter is not so much about parsing as it is about providing further insight into the process of 
 functional design.
  
  
 What is a parser?
  
 A parser is a specialized program that takes unstructured data (such as text, or any kind of stream 
 of symbols, numbers, or tokens) as input, and outputs a structured representation of that data. For 
 example, we can write a parser to turn a comma-separated file into a list of lists, where the 
 elements of the outer list represent the records, and the elements of each inner list represent the 
 comma-separated fields of each record. Another example is a parser that takes an XML or JSON 
 document and turns it into a tree-like data structure.
  
 In a parser combinator library, like the one we’ll build in this chapter, a parser doesn’t have to be 
 anything quite that complicated, and it doesn’t have to parse entire documents. It can do something 
 as elementary as recognizing a single character in the input. We then use combinators to assemble 
 composite parsers from elementary ones, and still more complex parsers from those.
  
  
 This chapter will introduce a design approach that we’ll call 
 algebraic design
 . This is just a natural 
 evolution of what we’ve already been doing to different degrees in past chapters—designing our 
 interface first, along with associated laws, and letting this guide our choice of data type 
  
 representations.
  
 At a few key points during this chapter, we’ll give more open-ended exercises, intended to mimic 
 the scenarios you might encounter when writing your own libraries from scratch. You’ll get the 
 most out of this chapter if you use these opportunities to put the book down and spend some time 
 investigating possible approaches. When you design your own libraries, you won’t be handed a 
 nicely chosen sequence of type signatures to fill in with implementations. You’ll have to make the 
 decisions about what types and combinators you need, and a goal in 
 part 2
  of this book has been to 
 prepare you for doing this on your own. As always, if you get stuck on one of the exercises or want 
 some more ideas, you can keep reading or consult the answers. It may also be a good idea to do 
 these exercises with another person, or compare notes with other readers online.
  
  
 Parser combinators versus parser generators
  
 You might be familiar with 
 parser generator
  libraries like Yacc (
 http://mng.bz/w3zZ
 ) or similar 
 libraries in other languages (for instance, ANTLR in Java: 
 http://mng.bz/aj8K
 ). These libraries 
 generate
  code for a parser based on a specification of the grammar. This approach works fine and",NA
"9.1. Designing an algebra, first","Recall that we defined 
 algebra
  to mean a collection of functions operating over some data type(s), 
 along with a set of laws
  specifying relationships between these functions. In past chapters, we 
 moved rather fluidly between inventing functions in our algebra, refining the set of functions, and 
 tweaking our data type representations. Laws were somewhat of an afterthought—we worked out 
 the laws only after we had a representation and an API fleshed out. There’s nothing wrong with this 
 style of design, 
 [
 1
 ]
  but here we’ll take a different approach. We’ll 
 start
  with the algebra (including 
 its laws) and decide on a representation later. This approach—let’s call it 
 algebraic design
 —can be 
 used for any design problem but works particularly well for parsing, because it’s easy to imagine 
 what 
  
 combinators are required for parsing different kinds of inputs.
 [
 2
 ]
  This lets us keep an eye on the 
 concrete goal even as we defer deciding on a representation.
  
 1
  For more about different functional design approaches, see the chapter notes for this chapter.
  
 2
  As we’ll see, there’s a connection between algebras for parsers and the classes of languages (regular, context-free, 
 context-sensitive) studied by computer science.
  
 There are many different kinds of parsing libraries.
 [
 3
 ]
  Ours will be designed for expressiveness 
 (we’d like to be able to parse arbitrary grammars), speed, and good error reporting. This last point is 
 important. Whenever we run a parser on input that it doesn’t expect—which can happen if the input 
 is malformed—it should generate a parse error. If there are parse errors, we want to be able to point 
 out exactly where the error is in the input and accurately indicate its cause. Error reporting is often 
 an afterthought in parsing libraries, but we’ll make sure we give careful attention to it.
  
 3
  There’s even a parser combinator library in Scala’s standard libraries. As in the previous chapter, we’re deriving our own 
 library from first principles partially for pedagogical purposes, and to further encourage the idea that no library is 
 authoritative. The standard library’s parser combinators don’t really satisfy our goals of providing speed and good error 
 reporting (see the chapter notes for some additional discussion).
  
 OK, let’s begin. For simplicity and for speed, our library will create parsers that operate on strings 
 as input.
 [
 4
 ]
  We need to pick some parsing tasks to help us discover a good algebra for our parsers. 
 What should we look at first? Something practical like parsing an email address, JSON, or HTML? 
 No! These tasks can come later. A good and simple domain to start with is parsing various 
 combinations of repeated letters and gibberish words like 
 ""abracadabra""
  and 
 ""abba""
 . As silly as",NA
9.2. A possible algebra,NA,NA
9.3. Handling context sensitivity,"Let’s take a step back and look at the primitives we have so far:
  
  
  
  
  
  
  
 string(s)
 —Recognizes and returns a single 
 String 
  
 slice(p)
 —Returns the portion of input inspected by 
 p
  if successful 
  
 succeed(a)
 —Always succeeds with the value 
 a 
  
 map(p)(f)
 —Applies the function 
 f
  to the result of 
 p
 , if successful 
  
 product(p1,p2)
 —Sequences two parsers, running 
 p1
  and then 
 p2
 , and returns the pair of 
 their results if both succeed
  
 or(p1,p2)
 —Chooses between two parsers, first attempting 
 p1
 , and then 
 p2
  if 
 p1
  fails
  
 Using these primitives, we can express repetition and nonempty repetition (
 many
 , 
 listOfN
 , and 
 many1
 ) as well as combinators like 
 char
  and 
 map2
 . Would it surprise you if these primitives were 
 sufficient for parsing 
 any
  context-free grammar, including JSON? Well, they are! We’ll get to 
 writing that JSON parser soon, but what 
 can’t
  we express yet?
  
 Suppose we want to parse a single digit, like 
 '4'
 , followed by 
 that many
 'a'
  characters (this sort of 
 problem should feel familiar from previous chapters). Examples of valid input are 
 ""0""
 , 
 ""1a""
 , 
 ""2aa""
 , 
 ""4aaaa""
 , and so on. This is an example of a context-sensitive grammar. It can’t be 
 expressed with 
 product
  because our choice of the second parser 
 depends on
  the result of the first",NA
9.4. Writing a JSON parser,"Let’s write that JSON parser now, shall we? We don’t have an implementation of our algebra yet, 
 and we’ve yet to add any combinators for good error reporting, but we can deal with these things 
 later. Our JSON parser doesn’t need to know the internal details of how parsers are represented. We 
 can simply write a function that produces a JSON parser using only the set of primitives we’ve 
 defined and any derived combinators.
  
 That is, for some JSON parse result type (we’ll explain the JSON format and the parse result 
 type shortly), we’ll write a function like this:
  
  
 This might seem like a peculiar thing to do, since we won’t actually be able to run our parser until 
 we have a concrete implementation of the 
 Parsers
  interface. But we’ll proceed, because in FP, it’s 
 common to define an algebra and explore its expressiveness without having a concrete 
  
 implementation. A concrete implementation can tie us down and makes changes to the API more 
 difficult. Especially during the design phase of a library, it can be much easier to refine an algebra 
 without
  having to commit to any particular implementation, and part of our goal here is to get you 
 comfortable with this style of working.
  
 After this section, we’ll return to the question of adding better error reporting to our parsing API. 
 We can do this without disturbing the overall structure of the API or changing our JSON parser 
 very much. And we’ll also come up with a concrete, runnable representation of our 
 Parser
  type. 
 Importantly, the JSON parser we’ll implement in this next section will be completely independent 
 of that representation.
  
 9.4.1. The JSON format
  
 If you aren’t already familiar with the JSON format, you may want to read Wikipedia’s 
 description (
 http://mng.bz/DpNA
 ) and the grammar specification (
 http://json.org
 ). Here’s an 
 example JSON document:
  
 {
  
  ""Company name"" : ""Microsoft Corporation"",
  
  ""Ticker"" : ""MSFT"",
  
  ""Active"" : true,
  
  ""Price""   : 30.66,",NA
9.5. Error reporting,"So far we haven’t discussed error reporting at all. We’ve focused exclusively on discovering a set of 
 primitives that let us express parsers for different grammars. But besides just parsing a grammar, we 
 want to be able to determine how the parser should respond when given unexpected input.
  
 Even without knowing what an implementation of 
 Parsers
  will look like, we can reason abstractly 
 about what information is being specified by a set of combinators. None of the combinators we’ve 
 introduced so far say anything about 
 what error message
  should be reported in the event of failure 
 or what other information a 
 ParseError
  should contain. Our existing combinators only specify what 
 the grammar is and what to do with the result if successful. If we were to declare ourselves done 
 and move to implementation at this point, we’d have to make some arbitrary decisions about error 
 reporting and error messages that are unlikely to be universally appropriate.
  
  
 Exercise 9.10
  
  
 Hard:
  If you haven’t already done so, spend some time discovering a nice set of combinators for 
 expressing what errors get reported by a 
 Parser
 . For each combinator, try to come up with laws 
 specifying what its behavior should be. This is a very open-ended design task. Here are some 
 guiding questions:
  
  
  
 Given the parser 
 ""abra"".**("" "".many).**(""cadabra"")
 , what sort of error would you like to 
 report given the input 
 ""abra cAdabra""
  (note the capital 
 'A'
 )? Only something like 
 Expected 
 'a'
 ? Or 
 Expected ""cadabra""
 ? What if you wanted to choose a different error message, like 
 ""Magic word incorrect, try again!""
 ?
  
 Given 
 a or b
 , if 
 a
  fails on the input, do we 
 always
  want to run 
 b
 , or are there cases where we 
 might not want to? If there are such cases, can you think of additional combinators that would",NA
9.6. Implementing the algebra,"By this point, we’ve fleshed out our algebra and defined a 
 Parser[JSON]
  in terms of it.
 [
 14
 ]
  Aren’t 
 you curious to try running it?
  
 14
  You may want to revisit your parser to make use of some of the error-reporting combinators we just discussed in the 
 previous section.
  
 Let’s again recall our set of primitives:
  
  
  
  
  
  
  
  
  
 string(s)
 —Recognizes and returns a single 
 String 
  
 regex(s)
 —Recognizes a regular expression 
 s 
  
 slice(p)
 —Returns the portion of input inspected by 
 p
  if successful 
  
 label(e)(p)
 —In the event of failure, replaces the assigned message with 
 e 
  
 scope(e)(p)
 —In the event of failure, adds 
 e
  to the error stack returned by 
 p 
  
 flatMap(p)(f)
 —Runs a parser, and then uses its result to select a second parser to run in 
 sequence
  
 attempt(p)
 —Delays committing to 
 p
  until after it succeeds 
  
 or(p1,p2)
 —Chooses between two parsers, first attempting 
 p1
 , and then 
 p2
  if 
 p1
  fails in an 
 uncommitted state on the input
  
  
 Exercise 9.12
  
  
 Hard:
  In the next section, we’ll work through a representation for 
 Parser
  and implement the 
 Parsers
  interface using this representation. But before we do that, try to come up with some ideas 
 on your own. This is a very open-ended design task, but the algebra we’ve designed places strong",NA
9.7. Summary,"In this chapter, we introduced 
 algebraic design
 , an approach to writing combinator libraries, and we 
 used it to design a parser library and to implement a JSON parser. Along the way, we discovered a 
 number of combinators similar to what we saw in previous chapters, and these were again related by 
 familiar laws. In 
 part 3
 , we’ll finally understand the nature of the connection between these libraries 
 and learn how to abstract over their common structure.
  
 This is the final chapter in 
 part 2
 . We hope you’ve come away from these chapters with a basic 
 sense of how functional design can proceed, and more importantly, we hope these chapters have 
 motivated you to try your hand at designing your own functional libraries, for whatever domains 
 interest 
 you
 . Functional design isn’t something reserved only for experts—it should be part of the 
 day-to-day work done by functional programmers at all levels of experience. Before you start on 
 part 3
 , we encourage you to venture beyond this book, write some more functional code, and design 
 some of your own libraries. Have fun, enjoy struggling with design problems that come up, and see 
 what you discover. When you come back, a universe of patterns and abstractions awaits in 
 part 3
 .",NA
Part 3. Common structures in functional design,"We’ve now written a number of libraries using the principles of functional design. In 
 part 2
 , we saw 
 these principles applied to a few concrete problem domains. By now you should have a good grasp 
 of how to approach a programming problem in your own work while striving for compositionality 
 and algebraic reasoning.
  
 Part 3
  takes a much wider perspective. We’ll look at the common patterns that arise in functional 
 programming. In 
 part 2
 , we experimented with various libraries that provided concrete solutions 
 to real-world problems, and now we want to integrate what we’ve learned from our experiments 
 into abstract theories that describe the common structure among those libraries.
  
 This kind of abstraction has a direct practical benefit: the elimination of duplicate code. We can 
 capture abstractions as classes, interfaces, and functions that we can refer to in our actual 
 programs.
  
 But the primary benefit is 
 conceptual integration
 . When we recognize common structure among 
 different solutions in different contexts, we unite all of those instances of the structure under a 
 single definition and give it a 
 name
 . As you gain experience with this, you can look at the general 
 shape of a problem and say, for example: “That looks like a 
 monad
 !” You’re then already far along 
 in finding the shape of the solution. A secondary benefit is that if other people have developed the 
 same kind of vocabulary, you can communicate your designs to them with extraordinary efficiency.
  
 Part 3
  won’t be a sequence of meandering journeys in the style of 
 part 2
 . Instead, we’ll begin each 
 chapter by introducing an abstract concept, give its definition, and then tie it back to what we’ve 
 seen already. The primary goal will be to train you in recognizing patterns when designing your 
 own libraries, and to write code that takes advantage of such patterns.",NA
Chapter 10. Monoids,"By the end of 
 part 2
 , we were getting comfortable with considering data types in terms of their 
 algebras
 —that is, the operations they support and the laws that govern those operations. 
 Hopefully you will have noticed that the algebras of very different data types tend to share certain 
 patterns in common. In this chapter, we’ll begin identifying these patterns and taking advantage 
 of them.
  
 This chapter will be our first introduction to 
 purely algebraic
  structures. We’ll consider a simple 
 structure, the 
 monoid
 ,
 [
 1
 ]
  which is defined 
 only by its algebra
 . Other than satisfying the same laws, 
 instances of the monoid interface may have little or nothing to do with one another. Nonetheless, 
 we’ll see how this algebraic structure is often all we need to write useful, polymorphic functions.
  
 1
  The name 
 monoid
  comes from mathematics. In category theory, it means a category with one object. This 
 mathematical connection isn’t important for our purposes in this chapter, but see the chapter notes for more information.
  
 We choose to start with monoids because they’re simple, ubiquitous, and useful. Monoids come up 
 all the time in everyday programming, whether we’re aware of them or not. Working with lists, 
  
 concatenating strings, or accumulating the results of a loop can often be phrased in terms of 
 monoids.
  
 We’ll see how monoids are useful in two ways: they facilitate parallel computation by giving us the 
 freedom to break our problem into chunks that can be computed in parallel; and they can be 
 composed to assemble complex calculations from simpler pieces.",NA
10.1. What is a monoid?,"Let’s consider the algebra of string concatenation. We can add 
 ""foo"" + ""bar""
  to get 
 ""foobar""
 , and 
 the empty string is an 
 identity element
  for that operation. That is, if we say 
 (s + """")
  or 
 ("""" + s)
 , the 
 result is always 
 s
 . Furthermore, if we combine three strings by saying 
 (r + s + t)
 , the operation is 
 associative
 —it doesn’t matter whether we parenthesize it: 
 ((r + s) + t)
  or 
 (r + (s + t))
 .
  
 The exact same rules govern integer addition. It’s associative, since 
 (x + y) + z
  is always equal to 
 x 
 + (y + z)
 , and it has an identity element, 
 0
 , which “does nothing” when added to another integer. 
 Ditto for multiplication, whose identity element is 
 1
 .
  
 The Boolean operators 
 &&
  and 
 ||
  are likewise associative, and they have identity elements 
 true 
 and 
 false
 , respectively.
  
 These are just a few simple examples, but algebras like this are virtually everywhere. The term 
 for this kind of algebra is 
 monoid
 . The laws of associativity and identity are collectively called 
 the 
 monoid laws
 . A monoid consists of the following:",NA
10.2. Folding lists with monoids,"Monoids have an intimate connection with lists. If you look at the signatures of 
 fold-Left
  and 
 foldRight
  on 
 List
 , you might notice something about the argument types:
  
 def
  foldRight[B](z: B)(f: (A, B) => B): B 
  
 def
  foldLeft[B](z: B)(f: (B, A) => B): B
  
 What happens when 
 A
  and 
 B
  are the same type?
  
 def
  foldRight(z: A)(f: (A, A) => A): A 
  
 def
  foldLeft(z: A)(f: (A, A) => A): A
  
 The components of a monoid fit these argument types like a glove. So if we had a list of 
 String
 s, 
 we could simply pass the 
 op
  and 
 zero
  of the 
 stringMonoid
  in order to reduce the list with the 
 monoid and concatenate all the strings:
  
 scala> val words = List(""Hic"", ""Est"", ""Index"") words: List[String] = 
 List(Hic, Est, Index)
  
 scala> val s = words.foldRight(stringMonoid.zero)(stringMonoid.op) s: String = ""HicEstIndex""
  
 scala> val t = words.foldLeft(stringMonoid.zero)(stringMonoid.op) t: String = ""HicEstIndex""
  
 Note that it doesn’t matter if we choose 
 foldLeft
  or 
 foldRight
  when folding with a monoid;
 [
 3
 ] 
 we 
 should get the same result. This is precisely because the laws of associativity and identity hold. A 
 left fold associates operations to the left, whereas a right fold associates to the right, with the 
 identity element on the left and right respectively:
  
 3
  Given that both 
 foldLeft
  and 
 foldRight
  have tail-recursive implementations.
  
 words.foldLeft("""")(_ + _) == (("""" + ""Hic"") + ""Est"") + ""Index""
  
 words.foldRight("""")(_ + _) == ""Hic"" + (""Est"" + (""Index"" + """"))
  
 We can write a general function 
 concatenate
  that folds a list with a monoid:
  
 def
  concatenate[A](as: List[A], m: Monoid[A]): A =
  
  
 as.foldLeft(m.zero)(m.op)
  
 But what if our list has an element type that doesn’t have a 
 Monoid
  instance? Well, we can always 
 map
  over the list to turn it into a type that does:",NA
10.3. Associativity and parallelism ,"The fact that a monoid’s operation is associative means we can choose how we fold a data structure 
 like a list. We’ve already seen that operations can be associated to the left or right to reduce a list 
 sequentially with 
 foldLeft
  or 
 foldRight
 . But if we have a monoid, we can reduce a list using a 
 balanced fold
 , which can be more efficient for some operations and also allows for parallelism. As 
 an example, suppose we have a sequence 
 a
 , 
 b
 , 
 c
 , 
 d
  that we’d like to reduce using some monoid.
  
 Folding to the right, the combination of 
 a
 , 
 b
 , 
 c
 , and 
 d
  would look like this: 
  
 op(a, op(b, op(c, d))) 
  
 Folding to the left would look like this: 
  
 op(op(op(a, b), c), d) 
  
 But a balanced fold looks like this: 
  
 op(op(a, b), op(c, d)) 
  
 Note that the balanced fold allows for parallelism, because the two inner 
 op
  calls are independent 
 and can be run simultaneously. But beyond that, the more balanced tree structure can be more 
 efficient in cases where the cost of each 
 op
  is proportional to the size of its arguments. For instance, 
 consider the runtime performance of this expression:",NA
10.4. Example: Parallel parsing,"As a nontrivial use case, let’s say that we wanted to count the number of words in a 
 String
 . This is a 
 fairly simple parsing problem. We could scan the string character by character, looking for 
 whitespace and counting up the number of runs of consecutive nonwhitespace characters. Parsing 
 sequentially like that, the parser state could be as simple as tracking whether the last character seen 
 was a whitespace.
  
 But imagine doing this not for just a short string, but an enormous text file, possibly too big to fit in 
 memory on a single machine. It would be nice if we could work with chunks of the file in parallel. 
 The strategy would be to split the file into manageable chunks, process several chunks in parallel, 
 and then combine the results. In that case, the parser state needs to be slightly more complicated, 
 and we need to be able to combine intermediate results regardless of whether the section we’re 
 looking at is at the beginning, middle, or end of the file. In other words, we want the combining 
 operation to be associative.
  
 To keep things simple and concrete, let’s consider a short string and pretend it’s a large file:
  
 ""lorem ipsum dolor sit amet, ""
  
 If we split this string roughly in half, we might split it in the middle of a word. In the case of our 
 string, that would yield 
 ""lorem ipsum do""
  and 
 ""lor sit amet, ""
 . When we add up the results of 
 counting the words in these strings, we want to avoid double-counting the word 
 dolor
 . Clearly, 
 just counting the words as an 
 Int
  isn’t sufficient. We need to find a data structure that can handle 
 partial results like the half words 
 do
  and 
 lor
 , and can track the complete words seen so far, like 
 ipsum
 , 
 sit
 , and 
 amet
 .
  
 The partial result of the word count could be represented by an algebraic data type:
  
 sealed trait
  WC 
  
 case class
  Stub(chars: String) 
 extends
  WC 
  
 case class
  Part(lStub: String, words: Int, rStub: String) 
 extends
  WC
  
 A 
 Stub
  is the simplest case, where we haven’t seen any complete words yet. But a 
 Part
  keeps the 
 number of complete words we’ve seen so far, in 
 words
 . The value 
 lStub
  holds any partial word 
 we’ve seen to the left of those words, and 
 rStub
  holds any partial word on the right.
  
 For example, counting over the string 
 ""lorem ipsum do""
  would result in 
 Part (""lorem"",",NA
10.5. Foldable data structures,"In 
 chapter 3
 , we implemented the data structures 
 List
  and 
 Tree
 , both of which could be folded. In 
 chapter 5
 , we wrote 
 Stream
 , a lazy structure that also can be folded much like a 
 List
  can, and now 
 we’ve just written a fold for 
 IndexedSeq
 .
  
 When we’re writing code that needs to process data contained in one of these structures, we often 
 don’t care about the shape of the structure (whether it’s a tree or a list), or whether it’s lazy or not, 
 or provides efficient random access, and so forth.
  
 For example, if we have a structure full of integers and want to calculate their sum, we can 
 use 
 foldRight
 :
  
 ints.foldRight(0)(_ + _)
  
 Looking at just this code snippet, we shouldn’t have to care about the type of 
 ints
 . It could be a 
 Vector
 , a 
 Stream
 , or a 
 List
 , or anything at all with a 
 foldRight
  method. We can capture this 
 commonality in a 
 trait
 :
  
 trait
  Foldable[F[_]] { 
  
  
 def
  foldRight[A,B](as: F[A])(z: B)(f: (A,B) => B): B 
  
 def
  foldLeft[A,B](as: 
 F[A])(z: B)(f: (B,A) => B): B 
  
 def
  foldMap[A,B](as: F[A])(f: A => B)(mb: 
 Monoid[B]): B 
  
 def
  concatenate[A](as: F[A])(m: Monoid[A]): A =
  
  
  
  foldLeft(as)(m.zero)(m.op) 
  
 }
  
 Here we’re abstracting over a type constructor 
 F
 , much like we did with the 
 Parser
  type in the 
 previous chapter. We write it as 
 F[_]
 , where the underscore indicates that 
 F
  is not a type but a 
 type 
 constructor
  that takes one type argument. Just like functions that take other functions as arguments 
 are called higher-order functions, something like 
 Foldable
  is a 
 higher-order type constructor
  or a 
 higher-kinded type
 .
 [
 7
 ]
  
 7
  Just like values and functions have types, types and type constructors have 
 kinds
 . Scala uses kinds to track how many type
  
 arguments a type constructor takes, whether it’s co- or contravariant in those arguments, and what the kinds of those arguments
  
 are.",NA
10.6. Composing monoids,"The 
 Monoid
  abstraction in itself is not all that compelling, and with the generalized 
 foldMap
  it’s 
 only slightly more interesting. The real power of monoids comes from the fact that they 
 compose
 .
  
 This means, for example, that if types 
 A
  and 
 B
  are monoids, then the tuple type 
 (A, B)
  is also a 
 monoid (called their 
 product
 ).
  
  
 Exercise 10.16",NA
10.7. Summary,"Our goal in 
 part 3
  is to get you accustomed to working with more abstract structures, and to 
 develop the ability to recognize them. In this chapter, we introduced one of the simplest purely 
 algebraic abstractions, the monoid. When you start looking for it, you’ll find ample opportunity to 
 exploit the monoidal structure of your own libraries. The associative property enables folding any 
 Foldable 
 data type and gives the flexibility of doing so in parallel. Monoids are also 
 compositional, and you can use them to assemble folds in a declarative and reusable way.",NA
Chapter 11. Monads,"In the previous chapter, we introduced a simple algebraic structure, the monoid. This was our first 
 instance of a completely abstract, purely algebraic interface, and it led us to think about interfaces 
 in a new way. A useful interface may be defined only by a collection of operations related by laws.
  
 In this chapter, we’ll continue this mode of thinking and apply it to the problem of factoring out 
 code duplication across some of the libraries we wrote in 
 parts 1
  and 
 2
 . We’ll discover two new 
 abstract interfaces, 
 Functor
  and 
 Monad
 , and get more general experience with spotting these sorts 
 of abstract structures in our code.
 [
 1
 ]
  
 1
  The names 
 functor
  and 
 monad
  come from the branch of mathematics called 
 category theory
 , but it isn’t necessary to have any 
 category theory background to follow the content in this chapter. You may be interested in following some of the references in 
 the chapter notes for more information.",NA
11.1. Functors: generalizing the map function,"In 
 parts 1
  and 
 2
 , we implemented several different combinator libraries. In each case, we proceeded 
 by writing a small set of primitives and then a number of combinators defined purely in terms of 
 those primitives. We noted some similarities between derived combinators across the libraries we 
 wrote. For instance, we implemented a 
 map
  function for each data type, to lift a function taking one 
 argument“into the context of” some data type. For 
 Gen
 , 
 Parser
 , and 
 Option
 , the type signatures 
 were as follows:
  
 def
  map[A,B](ga: Gen[A])(f: A => B): Gen[B]
  
 def
  map[A,B](pa: Parser[A])(f: A => B): Parser[B]
  
 def
  map[A,B](oa: Option[A])(f: A => B): Option[A]
  
 These type signatures differ only in the concrete data type (
 Gen
 , 
 Parser
 , or 
 Option
 ). We can 
 capture as a Scala trait the idea of “a data type that implements 
 map
 ”:
  
 trait
  Functor[F[_]] { 
  
  
 def
  map[A,B](fa: F[A])(f: A => B): F[B] 
  
 }
  
 Here we’ve parameterized 
 map
  on the type constructor, 
 F[_]
 , much like we did with 
 Foldable
  in the 
 previous chapter.
 [
 2
 ]
  Instead of picking a particular 
 F[_]
 , like 
 Gen
  or 
 Parser
 , the 
 Functor 
 trait is 
 parametric in the choice of 
 F
 . Here’s an instance for 
 List
 :
  
 2
  Recall that a 
 type constructor
  is applied to a type to produce a type. For example, 
 List
  is a type constructor, not a type. There 
 are no values of type 
 List
 , but we can apply it to the type 
 Int
  to produce the type 
 List[Int]
 . Likewise, 
 Parser
  can be applied to 
 String
  to yield 
 Parser[String]
 .
  
 val
  listFunctor = 
 new
  Functor[List] { 
  
  
 def
  map[A,B](as: List[A])(f: A => B): List[B] = as map f",NA
11.2. Monads: generalizing the flatMap and unit functions,NA,NA
11.3. Monadic combinators,"Now that we have our primitives for monads, we can look back at previous chapters and see if there
  
 were some other functions that we implemented for each of our monadic data types. Many of them 
 can
  
 be implemented once for all monads, so let’s do that now.
  
  
 Exercise 11.3
  
  
 The 
 sequence
  and 
 traverse
  combinators should be pretty familiar to you by now, and your 
 implementations of them from various prior chapters are probably all very similar. Implement 
 them
  
 once and for all on 
 Monad[F]
 .
  
 def
  sequence[A](lma: List[F[A]]): F[List[A]] 
  
 def
  traverse[A,B](la: List[A])(f: A => F[B]): F[List[B]]
  
  
 One combinator we saw for 
 Gen
  and 
 Parser
  was 
 listOfN
 , which allowed us to replicate a parser 
 or generator 
 n
  times to get a parser or generator of lists of that length. We can implement this 
 combinator for all monads 
 F
  by adding it to our 
 Monad
  trait. We should also give it a more 
 generic name such as 
 replicateM
  (meaning “replicate in a monad”).
  
  
 Exercise 11.4
  
  
 Implement 
 replicateM
 .
  
 def
  replicateM[A](n: Int, ma: F[A]): F[List[A]]
  
  
  
 Exercise 11.5",NA
11.4. Monad laws,"In this section, we’ll introduce laws to govern our 
 Monad
  interface.
 [
 6
 ]
  Certainly we’d expect the 
 functor laws to also hold for 
 Monad
 , since a 
 Monad[F]
 is
  a 
 Functor[F]
 , but what else do we 
 expect? What laws should constrain 
 flatMap
  and 
 unit
 ?
  
 6
  These laws, once again, come from the concept of monads from category theory, but a background in category theory isn’t
  
 necessary to understand this section.
  
 11.4.1. The associative law
  
 For example, if we wanted to combine three monadic values into one, which two should we 
 combine
  
 first? Should it matter? To answer this question, let’s for a moment take a step down from the 
 abstract
  
 level and look at a simple concrete example using the 
 Gen
  monad.",NA
11.5. Just what is a monad?,"Let’s now take a wider perspective. There’s something unusual about the 
 Monad
  interface. The 
 data types for which we’ve given monad instances don’t seem to have much to do with each other. 
 Yes, 
 Monad
  factors out code duplication among them, but what 
 is
  a monad exactly? What does 
 “monad”
 mean
 ?
  
 You may be used to thinking of interfaces as providing a relatively complete API for an abstract 
 data type, merely abstracting over the specific representation. After all, a singly linked list and an 
 array-based list may be implemented differently behind the scenes, but they’ll share a common 
 interface in terms of which a lot of useful and concrete application code can be written. 
 Monad
 , like 
 Monoid
 , is a more abstract, purely algebraic interface. The 
 Monad
  combinators are often just a 
 small fragment of the full API for a given data type that happens to be a monad. So 
 Monad
  doesn’t 
 generalize one type or another; rather, many vastly different data types can satisfy the 
 Monad
  
 interface and laws.
  
 We’ve seen three minimal sets of primitive 
 Monad
  combinators, and instances of 
 Monad
  will 
 have to provide implementations of one of these sets:
  
  
  
  
 unit
  
 and 
 flatMap 
  
 unit
  
 and 
 compose 
  
 unit
 , 
 map
 , and 
 join
  
 And we know that there are two monad laws to be satisfied, associativity and identity, that can 
 be formulated in various ways. So we can state plainly what a monad 
 is
 :
  
 A monad is an implementation of one of the minimal sets of monadic combinators, satisfying 
 the laws of associativity and identity.
  
 That’s a perfectly respectable, precise, and terse definition. And if we’re being precise, this is 
 the 
 only
  correct definition. A monad is precisely defined by its operations and laws; no more, no 
 less.",NA
11.6. Summary,"In this chapter, we took a pattern that we’ve seen repeated throughout the book and we unified it 
 under a single concept: monad. This allowed us to write a number of combinators once and for all, 
 for many different data types that at first glance don’t seem to have anything in common. We 
 discussed laws that they all satisfy, the monad laws, from various perspectives, and we developed 
 some insight into what it all means.
  
 An abstract topic like this can’t be fully understood all at once. It requires an iterative approach 
 where you keep revisiting the topic from different perspectives. When you discover new monads or 
 new applications of them, or see them appear in a new context, you’ll inevitably gain new insight. 
 And each time it happens, you might think to yourself, “OK, I thought I understood monads before, 
 but now I 
 really
  get it.”",NA
Chapter 12. Applicative and traversable functors,"In the previous chapter on monads, we saw how a lot of the functions we’ve been writing for 
 different combinator libraries can be expressed in terms of a single interface, 
 Monad
 . Monads 
 provide a powerful interface, as evidenced by the fact that we can use 
 flatMap
  to essentially write 
  
 imperative programs in a purely functional way.
  
 In this chapter, we’ll learn about a related abstraction, 
 applicative functors
 , which are less powerful 
 than monads, but more general (and hence more common). The process of arriving at applicative 
 functors will also provide some insight into 
 how to discover such abstractions
 , and we’ll use some 
 of these ideas to uncover another useful abstraction, 
 traversable functors
 . It may take some time for 
 the full significance and usefulness of these abstractions to sink in, but you’ll see them popping up 
 again and again in your daily work with FP if you pay attention.",NA
12.1. Generalizing monads,"By now we’ve seen various operations, like 
 sequence
  and 
 traverse
 , implemented many times for 
 different monads, and in the last chapter we generalized the implementations to work for 
 any 
 monad 
 F
 :
  
 def
  sequence[A](lfa: List[F[A]]): F[List[A]]
  
  
  traverse(lfa)(fa => fa)
  
 def
  traverse[A,B](as: List[A])(f: A => F[B]): F[List[B]]
  
  
  as.foldRight(unit(List[B]()))((a, mbs) => map2(f(a), mbs)(_ :: _))
  
 Here, the implementation of 
 traverse
  is using 
 map2
  and 
 unit
 , and we’ve seen that 
 map2
  can be 
 implemented in terms of 
 flatMap
 :
  
 def
  map2[A,B,C](ma: F[A], mb: F[B])(f: (A,B) => C): F[C] =
  
  flatMap(ma)(a => 
 map(mb)(b => f(a,b)))
  
 What you may not have noticed is that a large number of the useful combinators on 
 Monad
  can be 
 defined using only 
 unit
  and 
 map2
 . The 
 traverse
  combinator is one example—it doesn’t call 
 flatMap
  directly and is therefore agnostic to whether 
 map2
  is primitive or derived. Furthermore, 
 for many data types, 
 map2
  can be implemented directly, without using 
 flatMap
 .
  
 All this suggests a variation on 
 Monad
 —the 
 Monad
  interface has 
 flatMap
  and 
 unit
  as primitives, 
 and derives 
 map2
 , but we can obtain a 
 different
  abstraction by letting 
 unit
  and 
 map2 
 be the 
 primitives. We’ll see that this new abstraction, called an 
 applicative functor
 , is less powerful than a 
 monad, but we’ll also see that limitations come with benefits.",NA
12.2. The Applicative trait,NA,NA
12.3. The difference between monads and applicative functors,"In the last chapter, we noted there were several minimal sets of operations that defined a 
 Monad
 :
  
  
  
  
 unit
  
 and 
 flatMap 
  
 unit
  
 and 
 compose 
  
 unit
 , 
 map
 , and 
 join
  
 Are the 
 Applicative
  operations 
 unit
  and 
 map2
  yet another minimal set of operations for monads? 
 No. There are monadic combinators such as 
 join
  and 
 flatMap
  that can’t be implemented with just 
 map2
  and 
 unit
 . To see convincing proof of this, take a look at 
 join
 :
  
 def
  join[A](f: F[F[A]]): F[A]
  
 Just reasoning algebraically, we can see that 
 unit
  and 
 map2
  have no hope of implementing this 
 function. The 
 join
  function “removes a layer” of 
 F
 . But the 
 unit
  function only lets us 
 add
  an 
 F 
 layer, and 
 map2
  lets us apply a function 
 within
 F
  but does no flattening of layers. By the same 
 argument, we can see that 
 Applicative
  has no means of implementing 
 flatMap
  either.
  
 So 
 Monad
  is clearly adding some extra capabilities beyond 
 Applicative
 . But what exactly? 
 Let’s look at some concrete examples.
  
 12.3.1. The Option applicative versus the Option monad
  
 Suppose we’re using 
 Option
  to work with the results of lookups in two 
 Map
  objects. If we simply 
 need to combine the results from two (independent) lookups, 
 map2
  is fine.
  
 Listing 12.3. Combining results with the 
 Option
  applicative
  
 Here we’re doing two lookups, but they’re independent and we merely want to combine their 
 results
  
 within the 
 Option
  context. If we want 
 the result of one lookup to affect what lookup we do next
 , 
 then we need 
 flatMap
  or 
 join
 , as the following listing shows.
  
 Listing 12.4. Combining results with the Option monad",NA
12.4. The advantages of applicative functors,"The 
 Applicative
  interface is important for a few reasons:
  
  
 In general, it’s preferable to implement combinators like 
 traverse
  using as few assumptions",NA
12.5. The applicative laws,"This section walks through the laws for applicative functors.
 [
 4
 ]
  For each of these laws, you may 
 want to verify that they’re satisfied by some of the data types we’ve been working with so far (an 
 easy one to verify is 
 Option
 ).
  
 4
  There are various other ways of presenting the laws for 
 Applicative
 . See the chapter notes for more information.
  
 12.5.1. Left and right identity
  
 What sort of laws should we expect applicative functors to obey? Well, we should definitely 
 expect them to obey the functor laws:
  
 map(v)(id) == v 
  
 map(map(v)(g))(f) == map(v)(f compose g)
  
 This implies some other laws for applicative functors because of how we’ve implemented 
 map
  
 in terms of 
 map2
  and 
 unit
 . Recall the definition of 
 map
 :
  
 def
  map[B](fa: F[A])(f: A => B): F[B] = 
  
 map2(fa, unit(()))((a, _) => f(a))
  
 Of course, there’s something rather arbitrary about this definition—we could have just as easily 
 put the 
 unit
  on the 
 left
  side of the call to 
 map2
 :
  
 def
  map[B](fa: F[A])(f: A => B): F[B] = 
  
 map2(unit(()), fa)((_, a) => f(a))
  
 The first two laws for 
 Applicative
  might be summarized by saying that 
 both
  these 
  
 implementations of 
 map
  respect the functor laws. In other words, 
 map2
  of some 
 fa: F[A]
  with 
 unit
  preserves the structure of 
 fa
 . We’ll call these the left and right identity laws (shown here in 
 the first and second lines of code, respectively):
  
 map2(unit(()), fa)((_,a) => a) == fa 
  
 map2(fa, unit(()))((a,_) => a) == fa
  
 12.5.2. Associativity
  
 To see the next law, 
 associativity
 , let’s look at the signature of 
 map3
 :
  
 def
  
 map3[A,B,C,D](fa: 
 F[A],
  
  fb: 
 F[B],
  
  fc: F[C])(f: (A, B, C) => D): F[D]",NA
12.6. Traversable functors,NA,NA
12.7. Uses of Traverse,"Let’s now explore the large set of operations that can be implemented quite generally using
  
 Traverse
 . We’ll only scratch the surface here. If you’re interested, follow some of the references 
 in the chapter notes to learn more, and do some exploring on your own.
  
  
 Exercise 12.14
  
  
 Hard:
  Implement 
 map
  in terms of 
 traverse
  as a method on 
 Traverse[F]
 . This establishes that 
 Traverse
  is an extension of 
 Functor
  and that the 
 traverse
  function is a generalization of 
 map 
 (for 
 this reason we sometimes call these 
 traversable functors
 ). Note that in implementing 
 map
 , you can 
 call 
 traverse
  with 
 your choice
  of 
 Applicative[G]
 .
  
 trait
  Traverse[F[_]] 
 extends
  Functor[F] {
  
 def
  traverse[G[_],A,B](fa: F[A])(f: A => G[B])( 
  
   
 implicit
  G: Applicative[G]): G[F[B]] =
  
  
 sequence(map(fa)(f))",NA
12.8. Summary,"In this chapter, we discovered two new useful abstractions, 
 Applicative
  and 
 Traverse
 , simply by 
 playing with the signatures of our existing 
 Monad
  interface. Applicative functors are a less",NA
Part 4. Effects and I/O,"Functional programming is a complete programming paradigm. All programs that we can imagine 
 can be expressed functionally, including those that mutate data in place and interact with the 
 external world by writing to files or reading from databases. In this part, we’ll apply what we 
 covered in 
 parts 1
 –
 3
  of this book to show how FP can express these effectful programs.
  
 We’ll begin in the next chapter by examining the most straightforward handling of external 
 effects, using an I/O monad. This is a simplistic embedding of an imperative programming 
 language into a functional language. The same general approach can be used for handling local 
 effects and mutation, which we’ll introduce in 
 chapter 14
 . Both of these chapters will motivate the 
 development of more composable ways to deal with effects. In 
 chapter 15
 , our final chapter, we’ll 
 develop a library for streaming I/O, and discuss how to write compositional and modular 
 programs that incrementally process I/O streams.
  
 Our goal in this part of the book is not to cover absolutely every technique relevant to handling 
 I/O and mutation, but to introduce the essential ideas and equip you with a conceptual framework 
 for future learning. You’ll undoubtedly encounter problems that don’t look exactly like those 
 discussed here. But along with 
 parts 1
 –
 3
 , after finishing this part you’ll be in good position to 
 apply FP to whatever programming tasks you may face.",NA
Chapter 13. External effects and I/O,"In this chapter, we’ll take what we’ve learned so far about monads and algebraic data types and 
 extend it to handle 
 external effects
  like reading from databases and writing to files. We’ll develop 
 a monad for I/O, aptly called 
 IO
 , that will allow us to handle such external effects in a purely 
 functional way.
  
 We’ll make an important distinction in this chapter between 
 effects
  and 
 side effects
 . The 
 IO
  
 monad provides a straightforward way of embedding 
 imperative programming with I/O effects
  in 
 a pure program while preserving referential transparency. It clearly separates 
 effectful
  code—
 code that needs to have some effect on the outside world—from the rest of our program.
  
 This will also illustrate a key technique for dealing with external effects—using pure functions to 
 compute a 
 description
  of an effectful computation, which is then executed by a separate 
 interpreter 
 that actually performs those effects. Essentially we’re crafting an embedded domain-specific 
 language (EDSL) for imperative programming. This is a powerful technique that we’ll use 
 throughout the rest of 
 part 4
 . Our goal is to equip you with the skills needed to craft your own 
 EDSLs for describing effectful programs.",NA
13.1. Factoring effects,"We’ll work our way up to the 
 IO
  monad by first considering a simple example of a program with 
 side effects.
  
 Listing 13.1. Program with side effects
  
 case class
  Player(name: String, score: Int)
  
 def
  contest(p1: Player, p2: Player): Unit = 
  
  
 if
  (p1.score > p2.score)
  
  
  println(s""${p1.name} is the winner!"") 
  
 else if
  (p2.score > p1.score)
  
  
  println(s""${p2.name} is the winner!"") 
  
 else
  
  
  println(""It's a draw."")
  
 The 
 contest
  function couples the I/O code for displaying the result to the pure logic for computing 
 the winner. We can factor the logic into its own pure function, 
 winner
 :",NA
13.2. A simple IO type,"It turns out that even procedures like 
 println
  are doing more than one thing. And they can be 
 factored in much the same way, by introducing a new data type that we’ll call 
 IO
 :",NA
13.3. Avoiding the StackOverflowError,"To better understand the 
 StackOverflowError
 , consider this very simple program that 
 demonstrates the problem:
  
 val
  p = IO.forever(PrintLine(""Still going...""))
  
 If we evaluate 
 p.run
 , it will crash with a 
 StackOverflowError
  after printing a few thousand lines. 
 If you look at the stack trace, you’ll see that 
 run
  is calling itself over and over. The problem is in 
 the definition of 
 flatMap
 :
  
 def
  flatMap[B](f: A => IO[B]): IO[B] = 
  
  
 new
  IO[B] { 
 def
  run = f(self.run).run }
  
 This method creates a new 
 IO
  object whose 
 run
  definition calls 
 run
  again 
 before
  calling 
 f
 . This 
 will keep building up nested 
 run
  calls on the stack and eventually overflow it. What can be done 
 about this?
  
 13.3.1. Reifying control flow as data constructors
  
 The answer is surprisingly simple. Instead of letting program control just flow through with 
 function
  
 calls, we explicitly bake into our data type the control flow that we want to support. For example,
  
 instead of making 
 flatMap
  a method that constructs a new 
 IO
  in terms of 
 run
 , we can just make it a 
 data constructor of the 
 IO
  data type. Then the interpreter can be a tail-recursive loop. Whenever it 
 encounters a constructor like 
 FlatMap(x,k)
 , it will simply interpret 
 x
  and then call 
 k
  on the result. 
 Here’s a new 
 IO
  type that implements that idea.
  
 Listing 13.4. Creating a new 
 IO
  type",NA
13.4. A more nuanced IO type,"If we use 
 TrailRec
  as our 
 IO
  type, this solves the stack overflow problem, but the other two 
 problems with the monad still remain—it’s inexplicit about what kinds of effects may occur, 
 and it has no concurrency mechanism or means of performing I/O without blocking the current 
 thread of execution.
  
 During execution, the 
 run
  interpreter will look at a 
 TailRec
  program such as 
  
 FlatMap(Suspend(s),k)
 , in which case the next thing to do is to call 
 s()
 . The program is returning 
 control to 
 run
 , requesting that it execute some effect 
 s
 , wait for the result, and respond by passing 
 the resulting value to 
 k
  (which may subsequently return a further request). At the moment, the 
 interpreter can’t know anything about 
 what kind of effect
 s
  the program is going to have. It’s 
 completely opaque. So the only thing it can do is call 
 s()
 . Not only can that have an arbitrary and 
 unknowable side effect, there’s no way that the interpreter could allow asynchronous calls if it 
 wanted to. Since the suspension is a 
 Function0
 , all we can do is call it and wait for it to complete.
  
 What if we used 
 Par
  from 
 chapter 7
  for the suspension instead of 
 Function0
 ? Let’s call this type 
 Async
 , since the interpreter can now support asynchronous execution.
  
 Listing 13.5. Defining our 
 Async
  type
  
 sealed trait
  Async[A] { 
  
  
 def
  flatMap[B](f: A => Async[B]): Async[B] =
  
  
  
  FlatMap(
 this
 , f) 
  
  
 def
  map[B](f: A => B): Async[B] =
  
  
  
  flatMap(f andThen (Return(_))) 
  
 } 
  
 case class
  Return[A](a: A) 
 extends
  Async[A] 
  
 case class
  Suspend[A](resume: Par[A]) 
 extends
  Async[A] 
 case class
  
 FlatMap[A,B](sub: Async[A],
  
  
   
  k: A => Async[B]) 
 extends
  Async[B]
  
 Note that the 
 resume
  argument to 
 Suspend
  is now a 
 Par[A]
  rather than a 
 () => A
  (or a 
 Function0[A]
 ). The implementation of 
 run
  changes accordingly—it now returns a 
 Par[A] 
 rather 
 than an 
 A
 , and we rely on a separate tail-recursive 
 step
  function to reassociate the 
 FlatMap 
 constructors:
  
 @annotation.tailrec 
  
 def
  step[A](async: Async[A]): Async[A] = async 
 match
  { 
  
  
 case
  FlatMap(FlatMap(x,f), g) => step(x flatMap (a => f(a) flatMap g))",NA
13.5. Non-blocking and asynchronous I/O,"Let’s turn our attention now to the last remaining problem with our original 
 IO
  monad—that of 
 performing non-blocking or asynchronous I/O. When performing I/O, we frequently need to 
 invoke operations that take a long time to complete and don’t occupy the CPU. These include 
 accepting a network connection from a server socket, reading a chunk of bytes from an input 
 stream, writing a large number of bytes to a file, and so on. Let’s think about what this means in 
 terms of the implementation of our 
 Free
  interpreter.
  
 When 
 runConsole
 , for example, encounters a 
 Suspend(s)
 , 
 s
  will be of type 
 Console
  and we’ll 
 have a translation 
 f
  from 
 Console
  to the target monad. To allow for non-blocking asynchronous 
 I/O, we simply change the target monad from 
 Function0
  to 
 Par
  or another concurrency monad 
 such as 
 scala.concurrent.Future
 . So just like we were able to write both pure and effectful 
 interpreters for 
 Console
 , we can write both blocking and non-blocking interpreters as well, just 
 by varying the target monad.
 [
 14
 ]
  
 14
  Our 
 Par
  monad from 
 chapter 7
  doesn’t do any handling of exceptions. See the file 
 Task.scala
  in the answer source code 
 accompanying this chapter for an example of an asynchronous I/O monad with proper exception handling.
  
 Let’s look at an example. Here, 
 runConsolePar
  will turn the 
 Console
  requests into 
 Par 
 actions 
 and then combine them all into one 
 Par[A]
 . We can think of it as a kind of compilation—we’re 
 replacing the abstract 
 Console
  requests with more concrete 
 Par
  requests that will actually read 
 from and write to the standard input and output streams when the resulting 
 Par
  value is run:
  
 scala> def p: ConsoleIO[Unit] = for {
  
  
  |   _ <- printLn(""What's your name?"")
  
  
  |   n <- readLn
  
  
  |   _ <- n match {
  
  
  |     case Some(n) => printLn(s""Hello, $n!"")
  
  
  |     case None => printLn(s""Fine, be that way."")
  
  |   }
  
  
  | } yield () 
  
 p: ConsoleIO[Unit] = 
  
 FlatMap(Suspend(PrintLine(What's your name?)),<function1>)
  
 scala> val q = runConsolePar(p) 
  
 q: Par[Unit] = <function1>
  
 Although this simple example runs in 
 Par
 , which in principle permits asynchronous actions, it 
 doesn’t make use of any asynchronous actions—
 readLine
  and 
 println
  are both 
 blocking
  I/O 
 operations. But there are I/O libraries that support 
 non-blocking
  I/O directly and 
 Par
  will let us bind 
 to such libraries. The details of these libraries vary, but to give the general idea, a non-blocking 
 source of bytes might have an interface like this:
  
 trait
  Source { 
  
  
 def
  readBytes(
  
  
  
  numBytes: Int,
  
  
  
  callback: Either[Throwable, Array[Byte]] => Unit): Unit }",NA
13.6. A general-purpose IO type,"We can now formulate a general methodology of writing programs that perform I/O. For any given 
 set
  
 of I/O operations that we want to support, we can write an algebraic data type whose case classes
  
 represent the individual operations. For example, we could have a 
 Files
  data type for file I/O, a 
 DB 
 data type for database access, and use something like 
 Console
  for interacting with standard input 
 and output. For any such data type 
 F
 , we can generate a free monad 
 Free[F,A]
  in which to write our 
 programs. These can be tested individually and then finally “compiled” down to a lower-level
  
 I/O type, what we earlier called 
 Async
 :
  
 type
  IO[A] = Free[Par, A]
  
 This 
 IO
  type supports both trampolined sequential execution (because of 
 Free
 ) and asynchronous 
 execution (because of 
 Par
 ). In our main program, we bring all of the individual effect types 
 together under this most general type. All we need is a translation from any given 
 F
  to 
 Par
 .
  
 13.6.1. The main program at the end of the universe
  
 When the JVM calls into our main program, it expects a 
 main
  method with a specific signature. 
 The return type of this method is 
 Unit
 , meaning that it’s expected to have some side effects. But 
 we can delegate to a 
 pureMain
  program that’s entirely pure! The only thing the 
 main
  method does 
 in that case is interpret our pure program, actually performing the effects.
  
 Listing 13.8. Turning side effects into just effects",NA
13.7. Why the IO type is insufficient for streaming I/O,"Despite the flexibility of the 
 IO
  monad and the advantage of having I/O actions as first-class 
 values, the 
 IO
  type fundamentally provides us with the same level of abstraction as ordinary 
 imperative programming. This means that writing efficient, streaming I/O will generally involve 
 monolithic loops.
  
 Let’s look at an example. Suppose we wanted to write a program to convert a file, 
  
 fahrenheit.txt
 , containing a sequence of temperatures in degrees Fahrenheit, separated by line 
 breaks, to a new file, 
 celsius.txt
 , containing the same temperatures in degrees Celsius. An algebra 
 for this might look something like this:
 [
 18
 ]
  
 18
  We’re ignoring exception handling in this API.
  
 trait
  Files[A] 
  
 case class
  ReadLines(file: String) 
 extends
  Files[List[String]] 
 case class
  
 WriteLines(file: String, lines: List[String]) 
  
 extends
  Files[Unit]
  
 Using this as our 
 F
  type in 
 Free[F,A]
 , we might try to write the program we want in the following 
 way:
  
 val
  p: Free[Files,Unit] = 
 for
  {
  
  
  lines <- Suspend { (ReadLines(""fahrenheit.txt"")) }
  
  
  cs = lines.map(s => fahrenheitToCelsius(s.toDouble).toString)
  
  _ <- Suspend { 
 WriteLines(""celsius.txt"", cs) } 
  
 } 
 yield
  ()
  
 This works, although it requires loading the contents of 
 fahrenheit.txt
  entirely into memory to 
 work on it, which could be problematic if the file is very large. We’d prefer to perform this task 
 using roughly constant memory—read a line or a fixed-size buffer full of lines from 
 farenheit.txt
 ,",NA
13.8. Summary,"This chapter introduced a simple model for handling external effects and I/O in a purely functional 
 way. We began with a discussion of factoring effects and demonstrated how effects can be moved 
 to the outer layers of a program. We generalized this to an 
 IO
  data type that lets us describe 
 interactions with the outside world without resorting to side effects.
  
 We discovered that monads in Scala suffer from a stack overflow problem, and we solved it in a 
 general way using the technique of 
 trampolining
 . This led us to the even more general idea of 
 free monads
 , which we employed to write a very capable 
 IO
  monad with an interpreter that used 
 non-blocking asynchronous I/O internally.
  
 The 
 IO
  monad is not the final word in writing effectful programs. It’s important because it 
 represents a kind of lowest common denominator when interacting with the external world. But in 
 practice, we want to use 
 IO
  directly as little as possible because 
 IO
  programs tend to be monolithic 
 and have limited reuse. In 
 chapter 15
 , we’ll discuss how to build nicer, more composable, more 
 reusable abstractions using essentially the same technique that we used here.
  
 Before getting to that, we’ll apply what we’ve learned so far to fill in the other missing piece of the 
 puzzle: 
 local effects
 . At various places throughout this book, we’ve made use of local mutation 
 rather casually, with the assumption that these effects weren’t 
 observable
 . In the next chapter, we’ll 
 explore what this means in more detail, see more examples of using local effects, and show how 
 effect scoping
  can be enforced by the type system.",NA
Chapter 14. Local effects and mutable state,"In the first chapter of this book, we introduced the concept of referential transparency, setting the 
 premise for purely functional programming. We declared that pure functions can’t mutate data in 
 place or interact with the external world. In 
 chapter 13
 , we learned that this isn’t exactly true. We 
 can
  write purely functional and compositional programs that describe interactions with the outside 
 world.
  
 These programs are unaware that they can be interpreted with an evaluator that has an effect on 
 the world.
  
 In this chapter we’ll develop a more mature concept of referential transparency. We’ll consider the 
 idea that effects can occur 
 locally
  inside an expression, and that we can guarantee that no other part 
 of the larger program can observe these effects occurring.
  
 We’ll also introduce the idea that expressions can be referentially transparent 
 with regard to
  
 some programs and not others.",NA
14.1. Purely functional mutable state,"Up until this point, you may have had the impression that in purely functional programming, you’re 
 not allowed to use mutable state. But if we look carefully, there’s nothing about the definitions of 
 referential transparency and purity that disallows mutation of 
 local
  state. Let’s refer to our 
 definitions from 
 chapter 1
 :
  
 Definition of referential transparency and purity
  
 An expression 
 e
  is referentially transparent if for all programs 
 p
  all occurrences of 
 e
  in 
 p
  can be 
 replaced by the result of evaluating 
 e
  without affecting the meaning of 
 p
 .
  
 A function 
 f
  is pure if the expression 
 f(x)
  is referentially transparent for all referentially 
 transparent 
 x
 .
  
 By that definition, the following function is pure, even though it uses a 
 while
  loop, an 
 updatable 
 var
 , and a mutable array.
  
 Listing 14.1. In-place 
 quicksort
  with a mutable array",NA
14.2. A data type to enforce scoping of side effects,"The preceding section makes it clear that pure functions may have side effects with respect to data 
 that’s 
 locally scoped
 . The 
 quicksort
  function may mutate the array because it allocated that array, 
 it’s locally scoped, and it’s not possible for any outside code to observe the mutation. If, on the 
 other hand, 
 quicksort
  somehow mutated its input list directly (as is common in mutable collection 
 APIs), that side effect would be observable to all callers of 
 quicksort
 .
  
 There’s nothing wrong with doing this sort of loose reasoning to determine the scoping of side 
 effects, but it’s sometimes desirable to 
 enforce
  effect scoping using Scala’s type system. The 
 constituent parts of 
 quicksort
  would have direct side effects if used on their own, and with the 
 types we’re using, we get no help from the compiler in controlling the scope of these side effects. 
 Nor are we alerted if we accidentally leak side effects or mutable state to a broader scope than 
 intended. In this section, we’ll develop a data type that uses Scala’s type system to enforce scoping 
 of mutations.
 [
 1
 ]
  
 1
  There’s a cost in terms of efficiency and notational convenience, so think of this as another technique you have at your 
 disposal, not something that must be employed every time you make use of local mutation.
  
 Note that we could just work in 
 IO
 , but that’s really not appropriate for local mutable state. If 
 quicksort
  returned 
 IO[List[Int]]
 , then it would be an 
 IO
  action that’s perfectly safe to run and 
 would have no side effects, which isn’t the case in general for arbitrary 
 IO
  actions. We want to be 
 able to distinguish between effects that are safe to run (like locally mutable state) and external 
 effects like I/O. So a new data type is in order.
  
 14.2.1. A little language for scoped mutation
  
 The most natural approach is to make a little language for talking about mutable state. Writing and 
 reading a state is something we can already do with the 
 State[S,A]
  monad, which you’ll recall is 
 just a function of type 
 S => (A, S)
  that takes an input state and produces a result and an output state. 
 But when we’re talking about mutating the state 
 in place
 , we’re not really passing it from one 
 action to the next. What we’ll pass instead is a kind of token marked with the type 
 S
 . A function 
 called with the token then has the authority to mutate data that’s tagged with the same type 
 S
 .
  
 This new data type will employ Scala’s type system to gain two static guarantees. We want our 
 code to 
 not compile
  if it violates these invariants:",NA
14.3. Purity is contextual,NA,NA
14.4. Summary,"In this chapter, we discussed two different implications of referential transparency.
  
 We saw that we can get away with mutating data that never escapes a local scope. At first blush it 
 may seem that mutating state can’t be compatible with pure functions. But as we’ve seen, we can 
 write components that have a pure interface and mutate local state behind the scenes, and we can 
 use Scala’s type system to guarantee purity.",NA
Chapter 15. Stream processing and incremental I/O,"We said in the introduction to 
 part 4
  that functional programming is a 
 complete
  paradigm. Every 
 imaginable program can be expressed functionally, including programs that interact with the 
 external world. But it would be disappointing if the 
 IO
  type were the only way to construct such 
 programs. 
 IO 
 and 
 ST
  work by simply embedding an imperative programming language into the 
 purely functional subset of Scala. While programming within the 
 IO
  monad, we have to reason 
 about our programs much like we would in ordinary imperative programming.
  
 We can do better. In this chapter, we’ll show how to recover the high-level compositional style 
 developed in 
 parts 1
 –
 3
  of this book, even for programs that interact with the outside world. The 
 design space in this area is enormous, and our goal here is not to explore it completely, but just 
 to convey ideas and give a sense of what’s possible.",NA
15.1. Problems with imperative I/O: an example,"We’ll start by considering a simple concrete usage scenario that we’ll use to highlight some of the 
 problems with imperative I/O embedded in the 
 IO
  monad. Our first easy challenge in this chapter 
 is to write a program that checks whether the number of lines in a file is greater than 40,000.
  
 This is a deliberately simple task that illustrates the essence of the problem that our library is 
 intended to solve. We could certainly accomplish this task with ordinary imperative code, inside 
 the 
 IO
  monad. Let’s look at that first.
  
 Listing 15.1. Counting line numbers in imperative style
  
  
 We can then 
 run
  this 
 IO
  action with 
 unsafePerformIO(linesGt40k(""lines.txt""))
 , where 
 unsafePerformIO
  is a side-effecting method that takes 
 IO[A]
 , returning 
 A
  and actually 
 performing the desired effects (see 
 section 13.6.1
 ).
  
 Although this code uses low-level primitives like a 
 while
  loop, a mutable 
 Iterator
 , and a 
 var
 , there 
 are some 
 good
  things about it. First, it’s 
 incremental
 —the entire file isn’t loaded into memory up 
 front. Instead, lines are fetched from the file only when needed. If we didn’t buffer the input, we",NA
15.2. Simple stream transducers,"Our first step toward recovering the high-level style we’re accustomed to from 
 Stream
  and 
 List 
 while doing I/O is to introduce the notion of 
 stream transducers
  or 
 stream processors
 . A stream 
 transducer specifies a transformation from one stream to another. We’re using the term 
 stream
  
 quite generally here to refer to a sequence, possibly lazily generated or supplied by an external 
 source.
  
 This could be a stream of lines from a file, a stream of HTTP requests, a stream of mouse click 
 positions, or anything else. Let’s consider a simple data type, 
 Process
 , that lets us express stream 
 transformations.
 [
 3
 ]
  
 3
  We’ve chosen to omit variance annotations in this chapter for simplicity, but it’s possible to write this as 
 Process[-I,+O]
 . 
 We’re also omitting some trampolining that would prevent stack overflows in certain circumstances. See the chapter notes 
 for discussion of more robust representations.
  
 Listing 15.2. The 
 Process
  data type
  
 A 
 Process[I,O]
  can be used to transform a stream containing 
 I
  values to a stream of 
 O
  values.
  
 But 
 Process[I,O]
  isn’t a typical function 
 Stream[I] => Stream[O]
 , which could consume the input 
 stream and construct the output stream. Instead, we have a state machine that must be driven 
 forward with a 
 driver
 , a function that simultaneously consumes both our 
 Process
  and the input 
 stream. A 
 Process
  can be in one of three states, each of which signals something to the driver:
  
  
  
  
 Emit(head,tail)
  indicates to the driver that the 
 head
  value should be emitted to the 
 output stream, and the machine should then be transitioned to the 
 tail
  state.
  
 Await(recv)
  requests a value from the input stream. The driver should pass the next 
 available value to the 
 recv
  function, or 
 None
  if the input has no more elements.
  
 Halt
  indicates to the driver that no more elements should be read from the input or emitted 
 to the output.
  
 Let’s look at a sample driver that will actually interpret these requests. Here’s one that transforms 
 a 
 Stream
 . We can implement this as a method on 
 Process
 :",NA
15.3. An extensible process type,"Our existing 
 Process
  type implicitly assumes an 
 environment
  or 
 context
  containing a single stream 
 of values. Furthermore, the 
 protocol
  for communicating with the driver is also fixed. A 
 Process 
 can 
 only issue three instructions to the driver—
 Halt
 , 
 Emit
 , and 
 Await
 —and there’s no way to extend 
 this protocol short of defining a completely new type. In order to make 
 Process
  extensible, we’ll 
 parameterize on the protocol used for issuing requests of the driver. This works in much the
  
 same way as the 
 Free
  type we covered in 
 chapter 13
 .
  
 Listing 15.4. An extensible 
 Process
  type",NA
15.4. Applications,"The ideas presented in this chapter are widely applicable. A surprising number of programs can 
 be cast in terms of stream processing—once you’re aware of the abstraction, you begin seeing it 
 everywhere. Let’s look at some domains where it’s applicable:
  
  
 File I/O
 —We’ve already demonstrated how to use stream processing for file I/O. Although
  
 we’ve focused on line-by-line reading and writing for the examples here, we can also use the",NA
15.5. Summary,"We began this book with a simple premise: that we assemble our programs using only pure 
 functions. From this sole premise and its consequences, we were led to explore a completely new 
 approach to programming that’s both coherent and principled. In this final chapter, we constructed a 
 library for stream processing and incremental I/O, demonstrating that we can retain the 
 compositional style developed throughout this book even for programs that interact with the outside 
 world. Our story, of how to use FP to architect programs both large and small, is now complete.
  
 FP is a deep subject, and we’ve only scratched the surface. By now you should have everything 
 you need to continue the journey on your own, to make functional programming a part of your 
 own work.
  
 Though good design is always difficult, expressing your code functionally will become 
 effortless over time. As you apply FP to more problems, you’ll discover new patterns and more 
 powerful abstractions.
  
 Enjoy the journey, keep learning, and good luck!",NA
Index,"[
 SYMBOL
 ][
 A
 ][
 B
 ][
 C
 ][
 D
 ][
 E
 ][
 F
 ][
 G
 ][
 H
 ][
 I
 ][
 J
 ][
 K
 ][
 L
 ][
 M
 ][
 N
 ][
 O
 ][
 P
 ][
 Q
 ][
 R
 ][
 S
 ][
 T
 ][
 U
 ][
 V
 ][
 W
 ][
 Y
 ]
  
 SYMBOL
  
 : (colon) 
  
 & (ampersand) 
  
 == operator 
  
 => arrow 
  
 | (pipe symbol)
 , 
 2
 nd
  
 A
  
 ADTs (algebraic data types) 
  
 algebra 
  
  
 algebraic reasoning and 
  
  
 expressiveness 
  
  
 parallel computations 
  
  
  
 avoiding deadlock 
  
  
  
 law of forking 
  
  
  
 law of mapping 
  
  
  
 overview 
  
  
 for parser combinators 
  
  
  
 context-sensitivity 
  
  
  
 failover 
  
  
  
 implementing 
  
  
  
 labeling parsers 
  
  
  
 overview 
  
  
  
 sequencing parsers 
  
 algebraic data types
 . 
  
 See
  ADTs. 
  
 algebraic design
 , 
 2
 nd 
  
 algebraic reasoning
 , 
 2
 nd 
  
 ampersand ( & ) 
  
 ANTLR library 
  
 applicative associativity law 
  
 applicative effects 
  
 arrays, mutable 
  
 associativity
 , 
 2
 nd 
  
 asynchronous I/O 
  
 awkwardness in functional 
 programming
  
 B
  
 backtracking 
  
 balanced 
 fold 
  
 block",NA
List of Listings,"Chapter 1. What is functional programming?
  
 Listing 1.1. A Scala program with side effects 
 Listing 1.2. Adding a payments object 
  
 Listing 1.3. Buying multiple cups with 
 buyCoffees
  
 Chapter 2. Getting started with functional programming in Scala
  
 Listing 2.1. A simple Scala program 
  
 Listing 2.2. A simple program including the factorial function 
 Listing 2.3. Monomorphic function to find a String in an array 
 Listing 2.4. Polymorphic function to find an element in an 
 array
  
 Chapter 3. Functional data structures
  
 Listing 3.1. Singly linked lists 
  
 Listing 3.2. Right folds and simple 
 uses
  
 Chapter 4. Handling errors without exceptions
  
 Listing 4.1. Throwing and catching an 
 exception Listing 4.2. The Option data type 
  
 Listing 4.3. Using Option 
  
 Listing 4.4. Using Either to validate data
  
 Chapter 5. Strictness and laziness
  
 Listing 5.1. Program trace for List 
  
 Listing 5.2. Simple definition for 
 Stream 
  
 Listing 5.3. Program trace for Stream
  
 Chapter 6. Purely functional state",NA
