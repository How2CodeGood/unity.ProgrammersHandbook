Larger Text,Smaller Text,Symbol
Data Structures ,NA,NA
And ,NA,NA
Algorithmic ,NA,NA
Thinking With ,NA,NA
Go ,NA,NA
Narasimha Karumanchi,NA,NA
Concepts ,NA,NA
 Problems ,NA,NA
 Interview ,NA,NA
Questions ,NA,NA
Copyright,©,NA
2021 by ,NA,NA
. ,NA,NA
All rights reserved. ,NA,NA
Designed by ,NA,NA
ℎ ℎ,"Copyright
 © 
  2021 CareerMonk Publications. All rights reserved. 
  
 All rights reserved. No part of this book may be reproduced in any form or by any electronic or mechanical 
 means, including information storage and retrieval systems, without written permission from the publisher or 
 author. 
  
 This book has been published with all efforts taken to make the material error-free after the consent of the 
 author. However, the author and the publisher do not assume and hereby disclaim any liability to any party for 
 any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from 
 negligence, accident, or any other cause. 
  
 While every effort has been made to avoid any mistake or omission, this publication is being sold on the 
 condition and understanding that neither the author nor the publishers or printers would be liable in any 
 manner to any person by reason of any mistake or omission in this publication or for any action taken or 
 omitted to be taken or advice rendered or accepted on the basis of this work. For any defect in printing or 
 binding the publishers will be liable only to replace the defective copy by another copy of this work then 
 available.",NA
Acknowledgements ,"This book would not have been possible without the help of many people. I would like to express my gratitude to 
 all of the people who provided support, talked things over, read, wrote, offered comments, allowed me to quote 
 their remarks and assisted in the editing, proofreading and design. In particular, I would like to thank the 
 following individuals: 
  
 ▪
  
 ℎ
 , IIT Bombay, Architect, dataRPM Pvt. Ltd. 
  
 −ℎ ℎ
  
  M-Tech, 
  
 ▪
  
 , Senior Consultant, Juniper Networks Inc. 
  
  Founder,",NA
Preface ,"Please hold on
 !I know many people typically do not read the Preface of a book. But I strongly recommend that 
 you read this particular Preface. 
  
 The study of algorithms and data structures is central to understanding what computer science is all about. 
 Learning computer science is not unlike learning any other type of difficult subject matter. The only way to be 
 successful is through deliberate and incremental exposure to the fundamental ideas. A beginning computer 
 scientist needs practice so that there is a thorough understanding before continuing on to the more complex 
 parts of the curriculum. In addition, a beginner needs to be given the opportunity to be successful and gain 
 confidence. This textbook is designed to serve as a text for a first course on data structures and algorithms. In 
 this book, we cover abstract data types and data structures, writing algorithms, and solving problems. We look 
 at a number of data structures and solve classic problems that arise. The tools and techniques that you learn 
 here will be applied over and over as you continue your study of computer science. 
  
 It is not the main objective of this book to present you with the theorems and proofs on  and 
 ℎ
 . I have followed a 
 pattern of improving the problem solutions with different complexities (for each problem, you will find multiple 
 solutions with different, and reduced, complexities). Basically, it’s an enumeration of possible solutions. With 
 this approach, even if you get a new question, it will show you a way to 
 think 
 about the possible solutions. You 
 will find this book useful for interview preparation, competitive exams preparation, and campus interview 
 preparations. 
  
 As a 
  ,
  if you read the complete book, I am sure you will be able to challenge the interviewers. If you read it as an 
 , it will help you to deliver lectures with an approach that is easy to follow, and as a result your students will 
 appreciate the fact that they have opted for Computer Science / Information Technology as their degree. 
  
 This book is also useful for  and  during their academic preparations. In all the chapters you will see that there 
 is more emphasis on problems and their analysis rather than on theory. In each chapter, you will first read 
 about the basic required theory, which is then followed by a section on problem sets. 
  
 If you read the book as a  preparing for competitive exams for Computer Science / Information Technology, the 
 content covers 
 thetopics
  in full detail. While writing this book, my main focus was to help students who are 
 preparing for these exams. 
  
 In all the chapters you will see more emphasis on problems and analysis rather than on theory. In each chapter, 
 you will first see the basic required theory followed by various problems. 
  
 For many problems,  solutions are provided with different levels of complexity. We start with the solution and 
 slowly move toward the  possible for that problem. For each problem, we endeavor to understand how much 
 time the algorithm takes and how much memory the algorithm uses. 
  
 It is 
 recommended
  that the reader does at least one 
 complete
  reading of this book to gain a full understanding of 
 all the topics that are covered. Then, in subsequent readings you can skip directly to any chapter to refer to a 
 specific topic. Even though many readings have been done for the purpose of correcting errors, there could still 
 be some minor typos in the book. If any are found, they will be updated at
   . . 
 . You can monitor this site for any 
 corrections and also for new problems and solutions. If any are found, they will be updated at
   . 
 . You can 
 monitor this site for any corrections and also for new problems and solutions. Also, please provide your valuable 
 suggestions at:
 @. 
 . 
  
 I wish you all the best and I am confident that you will find this book useful. 
  
  
 −ℎ ℎ
  
  M-Tech, 
  
  Founder, 
  
 :
  
 https://github.com/careermonk/data-structures-and-algorithmic-thinking-with-go.git",NA
Other Books by Narasimha Karumanchi ,NA,NA
 Data Structures and Algorithms Made Easy,NA,NA
 IT Interview Questions ,NA,NA
Data Structures and Algorithms for GATE ,NA,NA
Data Structures and Algorithms Made Easy in Java ,NA,NA
Coding Interview Questions ,NA,NA
Peeling Design Patterns ,NA,NA
Elements of Computer Networking ,NA,NA
Data Structures and Algorithmic Thinking with Python ,NA,NA
Algorithm Design Techniques ,NA,NA
Table of Contents ,"Organization of Chapters ---------------------------------------------------------------------- 15 
 0.1 What Is This Book About? ------------------------------------------------------------------------------- 15 
 0.2 Should I Buy This Book? -------------------------------------------------------------------------------- 15 0.3 
 Organization of Chapters -------------------------------------------------------------------------------- 15 0.4 
 Some Prerequisites --------------------------------------------------------------------------------------- 18 0.5 
 GoLang Cheat Sheet -------------------------------------------------------------------------------------- 18 
 1.Introduction --------------------------------------------------------------------------------- 27 
 1.1 
 Variables --------------------------------------------------------------------------------------------------- 27 1.2 
 Data Types ------------------------------------------------------------------------------------------------- 27 1.3 
 Data Structures ------------------------------------------------------------------------------------------- 28 1.4 
 Abstract Data Types (ADTs) ----------------------------------------------------------------------------- 28 1.5 
 What is an Algorithm? ----------------------------------------------------------------------------------- 28 1.6 
 Why the Analysis of Algorithms? ----------------------------------------------------------------------- 29 1.7 
 Goal of the Analysis of Algorithms --------------------------------------------------------------------- 29 1.8 
 What is Running Time Analysis? ----------------------------------------------------------------------- 29 1.9 
 How to Compare Algorithms ---------------------------------------------------------------------------- 29 1.10 
 What is Rate of Growth? ------------------------------------------------------------------------------- 29 1.11 
 Commonly Used Rates of Growth --------------------------------------------------------------------- 30 1.12 
 Types of Analysis ---------------------------------------------------------------------------------------- 31 1.13 
 Asymptotic Notation ------------------------------------------------------------------------------------ 31 1.14 
 Big-O Notation ------------------------------------------------------------------------------------------- 31 1.15 
 Omega-Ω Notation [Lower Bounding Function] ----------------------------------------------------- 32 1.16 
 Theta-
 
  Notation ---------------------------------------------------------------------------------------- 33 1.17 
 Why is it called Asymptotic Analysis? ---------------------------------------------------------------- 34 1.18 
 Guidelines for Asymptotic Analysis ------------------------------------------------------------------- 34 1.20 
 Simplifying properties of asymptotic notations ----------------------------------------------------- 35 1.21 
 Commonly used Logarithms and Summations ----------------------------------------------------- 35 1.22 
 Master Theorem for Divide and Conquer Recurrences -------------------------------------------- 36 1.23 
 Divide and Conquer Master Theorem: Problems & Solutions ------------------------------------ 36 1.24 
 Master Theorem for Subtract and Conquer Recurrences ----------------------------------------- 37 1.25 
 Variant of Subtraction and Conquer Master Theorem --------------------------------------------- 37 1.26 
 Method of Guessing and Confirming ----------------------------------------------------------------- 37 1.27 
 Amortized Analysis -------------------------------------------------------------------------------------- 39 1.28 
 Algorithms Analysis: Problems & Solutions --------------------------------------------------------- 39 
 2.Recursion and Backtracking -------------------------------------------------------------- 50 
 2.1 Introduction ------------------------------------------------------------------------------------------------ 50 
 2.2 What is Recursion? --------------------------------------------------------------------------------------- 50 
 2.3 Why Recursion? ------------------------------------------------------------------------------------------- 50 
 2.4 Format of a Recursive Function ------------------------------------------------------------------------ 50 2.5 
 Recursion and Memory (Visualization) ---------------------------------------------------------------- 51",NA
Data Structure Operations Cheat Sheet ,"Data 
  
 Structure 
 Name 
  
 Average Case Time Complexity 
  
 Worst Case Time Complexity 
  
 Space 
  
 Complexity 
  
 Accessing 
 element 
  
 Search 
  
 Insertion 
  
 Deletion 
  
 Accessing 
  
 element 
  
 Search 
  
 Insertion 
  
 Deletion 
  
 Worst Case 
  
 Arrays 
  
 O(
 1
 ) 
  
 O() 
  
 O() 
  
 O() 
  
 O(
 1
 ) 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Stacks 
  
 O() 
  
 O() 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 O() 
  
 O() 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 O() 
  
 Queues 
  
 O() 
  
 O() 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 O() 
  
 O() 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 O() 
  
 Binary 
 Trees 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Binary 
 Search 
 Trees 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Balanced 
 Binary 
  
 Search 
  
 Trees 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Hash 
  
 Tables 
  
 N/A 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 O(
 1
 ) 
  
 N/A 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Note
 : For best case operations, the time complexities are O(1).",NA
Sorting Algorithms Cheat Sheet ,"Sorting 
  
 Algorithm 
 Name 
  
 Time Complexity 
  
 Space 
  
 Complexity 
  
 Is 
  
 Stable? 
  
 Sorting 
  
 Class Type 
  
 Remarks 
  
 Best 
 Case 
  
 Average 
 Case 
  
 Worst 
 Case 
  
 Worst Case 
  
 Bubble Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O(1) 
  
 Yes 
  
 Comparison 
  
 Not a preferred sorting 
 algorithm. 
  
 Insertion 
 Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O(1) 
  
 Yes 
  
 Comparison 
  
 In the best case (already 
 sorted), every insert 
  
 requires constant time 
  
 Selection 
 Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O(1) 
  
 Yes 
  
 Comparison 
  
 Even a perfectly sorted 
 array requires scanning 
 the entire array 
  
 Merge Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Yes 
  
 Comparison 
  
 On arrays, it requires O() 
 space; and on linked lists, 
 it requires constant space 
  
 Heap Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O(1) 
  
 No 
  
 Comparison 
  
 By using input array as 
 storage for the heap, it is 
 possible to achieve 
  
 constant space 
  
 Quick Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 No 
  
 Comparison 
  
 Randomly picking a pivot 
 value can help avoid worst 
 case scenarios such as a 
 perfectly sorted array. 
  
 Tree Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O() 
  
 Yes 
  
 Comparison 
  
 Performing inorder 
  
 traversal on the balanced 
 binary search tree. 
  
 Counting 
 Sort 
  
 O(
  + 
 ) 
  
 O(
  + 
 ) 
  
 O(
  + 
 ) 
  
 O() 
  
 Yes 
  
 Linear 
  
 Where  is the range of the 
 non-negative key values. 
  
 Bucket Sort 
  
 O(
  + 
 ) 
  
 O(
  + 
 ) 
  
 O() 
  
 O() 
  
 Yes 
  
 Linear 
  
 Bucket sort is stable, if the 
 underlying sorting 
  
 algorithm is stable. 
  
 Radix Sort 
  
 O() 
  
 O() 
  
 O() 
  
 O(
  + 
 ) 
  
 Yes 
  
 Linear 
  
 Radix sort is stable, if the 
 underlying sorting 
  
 algorithm is stable.",NA
Chapter,NA,NA
Organization of ,NA,NA
0 ,NA,NA
Chapters ,NA,NA
0.1 What Is This Book About? ,"This book is about the fundamentals of data structures and algorithms – the basic elements from which large 
 and complex software projects are built. To develop a good understanding of a data structure requires three 
 things: first, you must learn how the information is arranged in the memory of the computer; second, you must 
 become familiar with the algorithms for manipulating the information contained in the data structure; and 
 third, you must understand the performance characteristics of the data structure so that when called upon to 
 select a suitable data structure for a particular application, you are able to make an appropriate decision. 
  
 The algorithms and data structures in this book are presented in the  programming language. A unique feature 
 of this book, when compared to the available books on the subject, is that it offers a balance of theory, practical 
 concepts, problem solving, and interview questions. 
  
  +  + 
  
 The book deals with some of the most important and challenging areas of programming and computer science 
 in a highly readable manner. It covers both algorithmic theory and programming practice, demonstrating how 
 theory is reflected in real  programs. Well-known algorithms and data structures that are built into the  
 language is explained, and the user is shown how to implement and evaluate others. 
  
 The book offers a large number of questions, with detailed answers, so you can practice and assess your 
 knowledge before you take the exam or are interviewed. 
  
 Salient features of the book are: 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Basic principles of algorithm design 
  
 How to represent well-known data structures in  language 
  
 How to implement well-known algorithms in  language 
  
 How to transform new problems into well-known algorithmic problems with efficient solutions 
  
 How to analyze algorithms and  programs using both mathematical tools and basic experiments and 
 benchmarks 
  
 How to understand several classical algorithms and data structures in depth, and be able to implement 
 these efficiently in",NA
0.2 Should I Buy This Book? ,"The book is intended for Go programmers who need to learn about algorithmic problem-solving or who need a 
 refresher. However, others will also find it useful, including data and computational scientists employed to do 
 cloud computing; big data analytic analysis; game programmers and financial analysts/engineers; and students 
 of computer science or programming-related subjects such as bioinformatics. 
  
 Although this book is more precise and analytical than many other data structure and algorithm books, it rarely 
 uses mathematical concepts that are more advanced than those taught in high school. I have made an effort to 
 avoid using any advanced calculus, probability, or stochastic process concepts. The book is therefore 
 appropriate for undergraduate students preparing for interviews.",NA
0.3 Organization of Chapters ,"Data structures and algorithms are important aspects of computer science as they form the fundamental 
 building blocks of developing logical solutions to problems, as well as creating efficient programs that perform 
 tasks optimally. This book covers the topics required for a thorough understanding of the subjects such 
 concepts as Linked Lists, Stacks, Queues, Trees, Priority Queues, Searching, Sorting, Hashing, Algorithm 
 Design Techniques, Greedy, Divide and Conquer, Dynamic Programming and Symbol Tables.",NA
0.4 Some Prerequisites ,"This book is intended for two groups of people:  programmers who want to beef up their algorithmics, and 
 students taking algorithm courses who want a supplement to their algorithm’s textbook. Even if you belong to 
 the latter group, I’m assuming you have a familiarity with programming in general and with  in particular. If 
 you don’t, the  web site also has a lot of useful material.  is a really easy language to learn. There is some math 
 in the pages ahead, but you don’t have to be a math prodigy to follow the text. We’ll be dealing with some simple 
 sums and nifty concepts such as polynomials, exponentials, and logarithms, but I’ll explain it all as we go 
 along.",NA
0.5 GoLang Cheat Sheet ,"Go (also referred to as GoLang) is an open source and lower level progra-mming language designed and created 
 at Google in 2009 by Robert Griesemer, Rob Pike and Ken Thompson, to enable users to easily write simple, 
 reliable, and highly efficient computer programs. Golang is better known for its built-in concurrency and 
 garbage collection features. Also, GoLang is a statically typed language. 
  
 0.5.1 Go in a Nutshell 
  
  
 •
  
  
 •
  
 Go is a statically typed language 
  
  
 •
  
 Syntax similar to Java/C/C++, but less parentheses and no semicolons 
  
 •
  
 Compiles to native code (no JVM) 
  
  
 •
  
 Go does not have classes, but it has structs with methods 
  
  
 •
  
 Go has interfaces 
  
  
 •
  
 No implementation inheritance. There's type embedding, though. 
  
 •
  
 Functions are first class citizens",NA
Introduction ,NA,NA
Chapter,NA,NA
1 ,"The objective of this chapter is to explain the importance of the analysis of algorithms, their notations, 
 relationships and solving as many problems as possible. Let us first focus on understanding the basic elements 
 of algorithms, the importance of algorithm analysis, and then slowly move toward the other topics as mentioned 
 above. After completing this chapter, you should be able to find the complexity of any given algorithm (especially 
 recursive functions).",NA
1.1 Variables ,"Before going to the definition of variables, let us relate them to old mathematical equations. All of us have solved 
 many mathematical equations since childhood. As an example, consider the below equation: 
  
 + 2 − 2 = 1
  
 We don’t have to worry about the use of this equation. The important thing that we need to understand is that 
 the equation has names ( and ), which hold values (data). That means the  ( and ) are placeholders for 
 representing data. Similarly, in computer science programming we need something for holding data, and is the 
 way to do that.",NA
1.2 Data Types ,"In the above-mentioned equation, the variables  and  can take any values such as integral numbers (
 10
 , 
 20
 ), 
 real numbers (
 0.23, 5.5
 ), or just 
 0
  and 
 1
 . To solve the equation, we need to relate them to the kind of values they 
 can take, and  is the name used in computer science programming for this purpose. A  in a programming 
 language is a set of data with predefined values. Examples of data types are: integer, floating point, unit 
 number, character, string, etc. 
  
 Computer memory is all filled with zeros and ones. If we have a problem and we want to code it, it’s very 
 difficult to provide the solution in terms of zeros and ones. To help users, programming languages and 
 compilers provide us with data types. For example,  takes 
 2
  bytes (actual value depends on compiler),  takes 
 4
  
 bytes, etc. This says that in memory we are combining 
 2
  bytes (
 16
  bits) and calling it an . Similarly, combining 
 4
  
 bytes (
 32
  bits) and calling it a . A data type reduces the coding effort. At the top level, there are two types of data 
 types: 
  
 •
  
 •
  
 System-defined data types (also called  data types) User-
 defined data types 
  
 System-defined data types (Primitive data types) 
  
 Data types that are defined by system are called  data types. The primitive data types provided by programming 
 language are: int (platform dependent, int32, int64,float32, float64, byte, bool, etc. The number of bits allocated 
 for each primitive data type depends on the programming languages, the compiler and the operating system. For 
 the same primitive data type, different languages may use different sizes. Depending on the size of the data 
 types, the total available values (domain) will also change. 
  
 The int, uint, and uintptr types are usually 32 bits wide on 32-bit systems and 64 bits wide on 64-bit systems. 
 When you need an integer value you should use int unless you have a specific reason to use a sized or unsigned 
 integer type. For example, “” may take 
 4
  bytes or 
 8
  bytes. If it takes 
 4
  bytes (
 32
  bits), then the total possible 
 values are from -
 2  2
 -1. If it takes 
 8
  bytes (
 64
  bits), then the possible values are between -
 2  2
 -1. The same is the 
 case with other data types. 
  
 1.1 Variables 
  
 27",NA
1.3 Data Structures ,"Based on the discussion above, once we have data in variables, we need some mechanism for manipulating that 
 data to solve problems.  is a particular way of storing and organizing data in a computer so that it can be used 
 efficiently. A  is a special format for organizing and storing data. General data structure types include arrays, 
 files, linked lists, stacks, queues, trees,graphsand so on. 
  
 Depending on the organization of the elements, data structures are classified into two types: 
  
 1) 
  
 2)
  
  
  :
  Elements are accessed in a sequential order but it is not compulsory to store all elements 
 sequentially.  : Linked Lists, Stacks and Queues. 
  
  −   
 : Elements of this data structure are stored/accessed in a non-linear order. : Trees and graphs.",NA
1.4 Abstract Data Types (ADTs) ,"Before defining abstract data types, let us consider the different view of system-defined data types. We all know 
 that, by default, all primitive data types (int, float, etc.) support basic operations such as addition and 
 subtraction. The system provides the implementations for the primitive data types. For user-defined data types 
 we also need to define operations. The implementation for these operations can be done when we want to 
 actually use them. That means, in general, user defined data types are defined along with their operations. 
  
 To simplify the process of solving problems, we combine the data structures with their operations and we call 
 this 
  
  (ADTs). An ADT consists of 
 parts
 : 
  
 1.Declaration of data 
  
 2.Declaration of operations 
  
 Commonly used ADTs : Linked Lists, Stacks, Queues, Priority Queues, Binary Trees, Dictionaries, Disjoint Sets 
 (Union and Find), Hash Tables, Graphs, and many others. For example, stack uses LIFO (Last-In-First-Out) 
 mechanism while storing the data in data structures. The last element inserted into the stack is the first 
 element that gets deleted. Common operations of it are: creating the stack, pushing an element onto the stack, 
 popping an element from stack, finding the current top of the stack, finding number of elements in the stack, 
 etc. 
  
 While defining the ADTs do not worry about the implementation details. They come into the picture only when 
 we want to use them. Different kinds of ADTs are suited to different kinds of applications, and some are highly 
 specialized to specific tasks. By the end of this book, we will go through many of them and you will be in a 
 position to relate the data structures to the kind of problems they solve.",NA
1.5 What is an Algorithm? ,"Let us consider the problem of preparing an . To prepare an omelette, we follow the steps given below: 
  
 1) 
  
 2)
  
 3)
  
 Get the frying pan. 
  
 Get the oil. 
  
 a.
  
 Do we have oil? 
  
 i.If yes, put it in the pan. 
  
 ii.If no, do we want to buy oil? 
  
 1.If yes, then go out and buy. 
 2.If no, we can terminate. 
  
 Turn on the stove, etc... 
  
 What we are doing is, for a given problem (preparing an omelet), we are providing a step-by-step procedure for 
 solving it. The formal definition of an algorithm can be stated as: 
  
 1.3 Data Structures 
  
 28",NA
1.6 Why the Analysis of Algorithms? ,"To go from city 
 “”
  to city 
 “”,
  there can be many ways of accomplishing this: by flight, by bus, by train and also by 
 bicycle. Depending on the availability and convenience, we choose the one that suits us. Similarly, in computer 
 science, multiple algorithms are available for solving the same problem (for example, a sorting problem has 
 many algorithms, like insertion sort, selection sort, quick sort and many more). Algorithm analysis helps us to 
 determine which algorithm is most efficient in terms of time and space consumed.",NA
1.7 Goal of the Analysis of Algorithms ,"The analysis of an algorithm can help us understand it better and can suggest informed improvements. The 
 main and important role of analysis of algorithm is to predict the performance of different algorithms in order to 
 guide design decisions. The goal of the 
   ℎ 
 is to compare algorithms (or solutions) mainly in terms of running 
 time but also in terms of other factors (e.g., memory, developer effort, etc.). 
  
 In theoretical analysis of algorithms, it is common to estimate their complexity in the asymptotic sense, i.e., to 
 estimate the complexity function for arbitrarily large input. The term ""analysis of algorithms"" was coined by 
 Donald Knuth. 
  
 Algorithm analysis is an important part of computational complexity theory, which provides theoretical 
 estimation for the required resources of an algorithm to solve a specific computational problem. Most 
 algorithms are designed to work with inputs of arbitrary length. Analysis of algorithms is the determination of 
 the amount of time and space resources required to execute it. 
  
 Usually, the efficiency or running time of an algorithm is stated as a function relating the input length to the 
 number of steps, known as time complexity, or volume of memory, known as space complexity.",NA
1.8 What is Running Time Analysis? ,"It is the process of determining how processing time increases as the size of the problem (input size) increases. 
 Input size is the number of elements in the input, and depending on the problem type, the input may be of 
 different types. The following are the common types of inputs. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Size of an array 
  
 Polynomial degree 
  
 Number of elements in a matrix 
  
 Number of bits in the  binary representation of the input 
 Vertices and edges in a graph.",NA
1.9 How to Compare Algorithms ,"To compare algorithms, let us define a few: 
  
 Execution times?  
 as execution times are specific to a particular computer. 
  
 Number of statements executed? 
 , since the number of statements varies with the programming language as well 
 as the style of the individual programmer. 
  
 Ideal solution
 ? 
 Let us assume that we express the running time of a given algorithm as a function of the input 
 size  (i.e., 
 ()
 )and compare these different functions corresponding to running times. This kind of comparison is 
 independent of machine time, programming style, etc.",NA
1.10 What is Rate of Growth? ,"The rate at which the running time increases as a function of input is called 
   ℎ
 . Let us assume that you go to a 
 shop to buy a car and a bicycle. If your friend sees you there and asks what you are buying, then in general you 
 say 
   .
 This is because the cost of the car is high compared to the cost of the bicycle (approximating the cost of 
 the bicycle to the cost of the car). 
  
  =  __ +  __
  
  
 1.6 Why the Analysis of Algorithms? 
  
  ≈  __ ()
  
 29",NA
1.11 Commonly Used Rates of Growth ,"Below is the list of growth rates you will come across in the following chapters. 
  
 Time 
  
 Complexity 
  
 Name 
  
 Description 
  
 1
  
 Constant 
  
 Whatever is the input size , these functions take a constant amount of 
 time. 
  
  
 Logarithmic 
  
 These are slower growing than even linear functions. 
  
  
 Linear 
  
 These functions grow linearly with the input size . 
  
  
 Linear Logarithmic 
  
 Faster growing than linear but slower than quadratic. 
  
  
 Quadratic 
  
 These functions grow faster than the linear logarithmic functions. 
  
  
 Cubic 
  
 Faster growing than quadratic but slower than exponential. 
  
 2
  
 Exponential 
  
 Faster than all of the functions mentioned here except the factorial 
 functions. 
  
 !
  
 Factorial 
  
 Fastest growing than all these functions mentioned here. 
  
 The diagram below shows the relationship between different rates of growth. 
  
 2
  
 !
  
  log 
  
 4
  
 log (!)
  
 h 
  
 a 
  
 s 
  
 t 
  
 2
  
 2
  
 log log 
  
 1
  
 1.11 Commonly Used Rates of Growth 
  
 30",NA
1.12 Types of Analysis,"To analyze the given algorithm, we need to know with which inputs the algorithm takes less time (performing 
 well) and with which inputs the algorithm takes a long time. We have already seen that an algorithm can be 
 represented in the form of an expression. That means we represent the algorithm with multiple expressions: one 
 for the case where it takes less time and another for the case where it takes more time. 
  
 In general, the first case is called the  and the second case is called the  for the algorithm. To analyze an 
 algorithm, we need some kind of syntax, and that forms the base for asymptotic analysis/notation. 
  
 There are three types of analysis: 
  
 •
  
 •
  
 •
  
 Worst case 
  
 o
  
 Defines the input for which the algorithm takes a long time (slowest time to complete). 
  
 o
  
 Input is the one for which the algorithm runs the slowest. 
  
 Best case 
  
 o
  
 Defines the input for which the algorithm takes the least time (fastest time to complete). 
  
 o
  
 Input is the one for which the algorithm runs the fastest. 
  
 Average case 
  
 o
  
 Provides a prediction about the running time of the algorithm. 
  
 o
  
 Run the algorithm many times, using many different inputs that come from some 
 distribution 
  
 that generates these inputs, compute the total running time (by adding the individual 
 times), and divide by the number of trials. 
  
 o
  
 Assumes that the input is random. 
  
  
  <=    <=    
  
 For a given algorithm, we can represent the best, worst and average cases in the form of expressions. As an 
 example, let 
 ()
  be the function which represents the given algorithm. 
  
 () =  +  500
 , for worst case 
  
 () =    +  100 +  500,
  for best case 
  
 Similarly, for the average case. The expression defines the inputs with which the algorithm takes the average 
 running time (or memory).",NA
1.13 Asymptotic Notation,"Having the expressions for the best, average and worst cases, for all three cases we need to identify the upper and lower bounds. 
 To represent 
  
 these upper and lower bounds, we need some kind of syntax, and that is the subject of the following discussion. Let us assume 
 that the given 
  
 algorithm is represented in the form of function
  ()
 .",NA
1.14 Big-O Notation,"This notation gives the 
 ℎ
  upper bound of the given function. Generally, it is represented as
  () = 
 O
 (())
 . That 
 means, at larger values of 
 , 
 the upper bound of 
 ()
  is
  ()
 . For example, if 
 () =  +  100 +  10 +  50 
 is the given 
 algorithm, then is
  ()
 . That means 
 ()
  gives the maximum rate of growth for 
 ()
  at larger values of. 
  
 () 
  
 () 
  
 Rate of 
  
 growth 
  
  
  
 Input size, 
  
 Let us see the O
 −
 notation with a little more detail. O
 −
 notation defined as O
 (()) = {():
  there exist positive 
 constants  and  such that 
 0 ≤  () ≤  ()
  for all
   ≥ }
 . 
 ()
  is an asymptotic tight upper bound for 
 ()
 . Our objective is to 
 give the smallest rate of growth 
 ()
  which is greater than or equal to the given algorithms’ rate of growth
  ()
 . 
  
 Generally, we discard lower values of. That means the rate of growth at lower values of  is not important. In the 
 figure,  is the point from which we need to consider the rate of growth for a given algorithm. Below , the rate of 
 growth could be different.  is called threshold for the given function. 
  
 1.12 Types of Analysis 
  
 31",NA
1.15 Omega-Ω Notation [Lower Bounding Function] ,"Similar to the O discussion, this notation gives the tighter lower bound of the given algorithm and we represent 
 it as
  () =  
 
 (())
 . That means, at larger values of 
 , 
 the tighter lower bound of 
 ()
  is 
 ().
  For example, if 
 () = 100 +  10 +  
 50
 , 
 ()
  is 
 
 ().
  
 The
 Ω
 notation can be defined as
 Ω
 (g(n)) = {f(n):
 there exist positive constants
 c
 and such that
 0 ≤  ()  ≤ ()
 for all 
  n ≥  
 }.()
  is an asymptotic tight lower bound for 
 ().  
 Our objective is to give the largest rate of growth 
 ()
  which is less 
 than or equal to the given algorithm’s rate of growth
  ()
 .
  
 1.15 Omega-Ω Notation [Lower Bounding Function] 
  
 32",NA
1.16 Theta-,NA,NA
,NA,NA
 Notation,"This notation decides whether the upper and lower bounds of a given function (algorithm) are the same. The 
 average running time of an algorithm is always between the lower bound and the upper bound. If the upper 
 bound (O) and lower bound (
 
 ) give the same result, then the 
 
  notation will also have the same rate of growth. 
 As an example, let us assume that 
 () = 10 + 
  is the expression. Then, its tight upper bound 
 ()
  is O
 ()
 . The rate of 
 growth in the best case is
  () =
  O
 ()
 . 
  
 In this case, the rates of growth in the best case and worst case are the same. As a result, the average case will 
 also be the same. For a given function (algorithm), if the rates of growth (bounds) for O and 
 
  are not the same, 
 then the rate of growth for the 
 
  case may not be the same. In this case, we need to consider all possible time 
 complexities and take the average of those (for example, for a quick sort average case, refer to the  chapter). 
  
 c()
  
 () 
  
 Rate of growth 
  
  
  
 c() 
  
 Input size, 
  
 Now consider the definition
  of 
 
 notation
 . 
 It is defined as
 
 (()) = {():
 there existpositive constants
 , 
 and such that
 0 ≤ ()  
 ≤   () ≤ ()
 for all
    ≥ }
 . 
 ()
  is an asymptotic tight bound for 
 ().  
 
 (())
  is the set of functions with the same order of 
 growth as 
 ().
  
 
  Examples 
  
 Example 1
  Find 
 
  bound for 
 () =
  
 −
  
 Solution: 
  
 ≤
  
 −
  
 ≤ 
 , for all, 
  ≥  2
  
 ∴
  
 −
  
 = 
 
 ()
  with 
   =  1/5, = 1
  and = 2 
  
 Example 2
  Prove 
  ≠  
 
 () 
  
 1.16 Theta- Notation 
  
 33",NA
1.17 Why is it called Asymptotic Analysis?,"From the discussion above (for all three notations: worst case, best case, and average case), we can easily 
 understand that, in every case for a given function 
 ()
  we are trying to find another function 
 ()
  which 
 approximates 
 ()
  at higher values of . That means 
 ()
  is also a curve which approximates 
 ()
  at higher values of . 
  
 In mathematics we call such a curve an . In other terms, 
 ()
  is the asymptotic curve for 
 ().
  For this reason, we 
 call algorithm analysis .",NA
1.18 Guidelines for Asymptotic Analysis ,"There are some general rules to help us determine the running time of an algorithm. 
  
 1)
  
 Loops
 :  
 The running time of a loop is, at most, the running time of the statements inside the loop 
 (including tests) multiplied by the number of iterations.
  
  
 for i := 1; i <= n; i++ { 
  
 // executes  times 
  
  m = m + 2   
  
 // constant time, c 
  
 } 
  
 Total time = a constant 
   ×   =    = 
 O
 ().
  
 2)
  
 Nested loops:
 Analyze from the inside out. Total running time is the product of the sizes of all the loops.
  
  
 for i := 1; i <= n; i++ { 
  
 // outer loop executed n times 
  
  for j := 1; j <= n; j++ {   
  
 // inner loop executes n times 
  
  k = k+1   
  
 // constant time 
  
 } 
  
  } 
  
  
 Total time 
 =   ×   ×   =   = 
 O
 ().
  
 3)
  
 Consecutive statements:
 Add the time complexities of each statement.
  
  
 x = x +1; //constant time 
  
 for i := 1; i <= n; i++ { 
  
 // executes n times 
  
  m = m + 2   
  
 // constant time 
  
 } 
  
 for i := 1; i <= n; i++ { 
  
 // outer loop executes n times 
  
 } 
  
  for j := 1; j <= n; j++ {       // inner loop executed n times  
 k = k+1 //constant time 
  
  } 
  
  
 Total time 
 =   +  +  = 
 O
 ().
  
 4)
  
 If-then-else statements:
 Worst-case running time: the test, plus 
 ℎ
  the 
 ℎ
  part  the  part (whichever 
  
 is the larger). 
  
 if length( ) == 0  { 
  
 //constant time 
  
  return false   
  
 //then part: constant 
  
 } else { 
  
 // else part: (constant + constant) * n 
  
 1.17 Why is it called Asymptotic Analysis? 
  
 34",NA
1.20 Simplifying properties of asymptotic notations ,"•
  
 Transitivity:
 () = 
 
 (())
   and 
 () =
  
 (ℎ()) 
 
  () =
  
 (ℎ()).
  Valid for O and 
 
  as well. 
 •
  
 Reflexivity:
 () =
  
 (()).
  Valid for O and 
 
 . 
  
 •
  
 Symmetry:
  () =
  
 (()) 
 if and only if 
 () = 
 
 (()).
  
 •
  
 Transpose symmetry:
  () = 
 O
 (())
  if and only if 
 () =
  
 (()).
  
 •
  
 If () is in O(()) for any constant 
  >  0
 , then () is in O(()). 
  
 •
  
 If () is in O(()) and () is in O(()), then ( + )() is in O(max((), ())). 
 •
  
 If () is in O(()) and 
 () is in O(()) then ()() is in O(()()).",NA
1.21 Commonly used Logarithms and Summations ,"Logarithms
  
 =                 = 
  
  =   +                = ()
  
  =  () = 
  
 =   – 
  
  = 
  
  
 Arithmetic series
  
  = 1 + 2 + ⋯+  = ( + 1) 
  
  
 2
  
 Geometric series
  
  
 = 1 +  + … + =  − 1 − 1
  
 ( ≠  1)
  
  
 1.20 Simplifying properties of asymptotic notations 
  
 35",NA
1.22 Master Theorem for Divide and Conquer Recurrences,"All divide and conquer algorithms (also discussed in detail in the  chapter) divide the problem into sub-problems, each of 
 which is part of the original problem, and then perform some additional work to compute the final answer. As an example, 
 a merge sort algorithm [for details, refer to  chapter] operates on two sub-problems, each of which is half the size of the 
 original, and then performs O
 ()
  additional work for merging. This gives the running time equation: 
  
 T
 () =  2 
  
  +
  O
 ()
  
 The following theorem can be used to determine the running time of divide and conquer algorithms. For a given 
 program (algorithm), first we try to find the recurrence relation for the problem. If the recurrence is of the below 
  
 form then we can directly give the answer without fully solving it. If the recurrence is of the form T
 () = (
  
 ) +
  
 
 (),
  where 
  ≥ 1, >  1, ≥ 0
  and  is a real number, then: 
  
 1) 
  
 If  
 > 
 , then 
 () = 
 Θ
  
 2) 
  
 If  = 
  
 a. 
  
 If 
  > −1
 , then 
 () = 
 Θ
  
 b.If 
  = −1
 , then 
 () = 
 Θ
  
 3)
  
 c.
  
 If 
  < −1
 , then 
 () = 
 Θ
  
 If  < 
  
 If 
  ≥ 0
 , then 
 () = 
 Θ
 ()
  
 a.
  
 b.If 
  < 0
 , then 
 () = 
 O
 ()",NA
1.23 ,NA,NA
Divide and Conquer Master Theorem: Problems & Solutions,"For each of the following recurrences, give an expression for the runtime 
 ()
  if the recurrence can be solved with 
 the Master Theorem. Otherwise, indicate that the Master Theorem does not apply. 
  
 Problem-1
 () = 3 (/2) + 
  
 Solution
 :
   () = 3 (/2) +  =>   () =
 Θ
 ()
  (Master Theorem Case 3.a) 
  
 Problem-2
 () = 4 (/2) + 
  
 Solution
 :
   () =  4 (/2) +  =>   () = 
 Θ
 ()
  (Master Theorem Case 2.a) 
  
 Problem-3
 () = (/2) + 
  
 Solution
 :
   () =  (/2) +  => 
 Θ
 () 
 (Master Theorem Case 3.a) 
  
 Problem-4
 () = 2(/2) + 
  
 Solution
 :
   () = 2(/2) + 
  => Does not apply ( is not constant) 
  
 Problem-5
 () =  16(/4) + 
  
 Solution
 :
   () =  16 (/4) +   =>  () = 
 Θ
 ()
  (Master Theorem Case 1) 
  
 Problem-6
 () =  2(/2) + 
  
 Solution
 :
   () =  2(/2) +   =>  () = 
 Θ
 ()
  (Master Theorem Case 2.a) 
  
 Problem-7
 () =  2(/2) +  / 
  
 Solution
 :
   () =  2(/2) +  / =>() = 
 Θ
 ()
  (Master Theorem Case 2.b) 
  
 Problem-8
 () =  2 (/4) + 
 .
  
 Solution
 :
   () =  2(/4) + 
 .
 =>   () = 
 Θ
 (
 .
 )
  (Master Theorem Case 3.b) 
  
 Problem-9
 () =  0.5(/2) +  1/ 
  
 1.22 Master Theorem for Divide and Conquer Recurrences 
  
 36",NA
1.24 Master Theorem for Subtract and Conquer Recurrences ,"Let 
 ()
  be a function defined on positive, and having the property 
  
 () =  ,                                       if  ≤ 1 
  
  
 ( − ) + (), if  > 1
  
 for some constants 
 ,  > 0,  > 0, ≥ 0
 , and function 
 (). 
 If
  ()
  is in O
 (),
  then 
  
  () = 
  
 O(),                if a < 1 
  
 O(),            if a = 1
  
  
  
  
 O 
  
 ,          if a > 1",NA
1.25 Variant of Subtraction and Conquer Master Theorem,"The solution to the equation 
 () = ( ) + ((1 − )) + 
 , where 
 0 <  < 1
  and 
  > 0
  are constants, is O
 ().",NA
1.26 Method of Guessing and Confirming ,"Now, let us discuss a method which can be used to solve any recurrence. The basic idea behind this method is: 
  
  the answer; and then  it correct by induction. 
  
 In other words, it addresses the question: What if the given recurrence doesn’t seem to match with any of these 
  
 (master theorem) methods? If we guess a solution and then try to verify our guess inductively, usually either the 
  
 proof will succeed (in which case we are done), or the proof will fail (in which case the failure will help us refine 
  
 our guess). 
  
 As an example, consider the recurrence T() = 
 √
  T(
 √
 ) + . This doesn’t fit into the form required by the Master 
  
 Theorems. Carefully observing the recurrence gives us the impression that it is similar to the divide and conquer 
  
 method (dividing the problem into 
 √
  subproblems each with size 
 √
 ). As we can see, the size of the subproblems 
  
 at the first level of recursion is . So, let us guess that T() = O(), and then try to prove that our guess is 
  
 correct. 
  
 Let’s start by trying to prove an  bound T() 
 ≤ 
 : 
  
 1.24 Master Theorem for Subtract and Conquer Recurrences 
  
 37",NA
1.27 Amortized Analysis ,"Amortized analysis refers to determining the time-averaged running time for a sequence of operations. It is 
 different from average case analysis, because amortized analysis does not make any assumption about the 
 distribution of the data values, whereas average case analysis assumes the data are not ""bad"" (e.g., some 
 sorting algorithms do well 
 on 
  over all input orderings but very badly on certain input orderings). That is, 
 amortized analysis is a worst-case analysis, but for a sequence of operations rather than for individual 
 operations. 
  
 The motivation for amortized analysis is to better understand the running time of certain techniques, where 
 standard worst-case analysis provides an overly pessimistic bound. Amortized analysis generally applies to a 
 method that consists of a sequence of operations, where the vast majority of the operations are cheap, but some 
 of the operations are expensive. If we can show that the expensive operations are particularly rare, we can 
 ℎ ℎ
  to 
 the cheap operations, and only bound the cheap operations. 
  
 The general approach is to assign an artificial cost to each operation in the sequence, such that the total of the 
 artificial costs for the sequence of operations bounds the total of the real costs for the sequence. This artificial 
 cost is called the amortized cost of an operation. To analyze the running time, the amortized cost thus is a 
 correct way of understanding the overall running time — but note that particular operations can still take 
 longer so it is not a way of bounding the running time of any individual operation in the sequence. 
  
 When one event in a sequence affects the cost of later events: 
  
 •
  
 •
  
 One particular task may be expensive. 
  
 But it may leave data structure in a state that the next few operations become easier. 
  
 Example
 : Let us consider an array of elements from which we want to find the  smallest element. We can solve 
 this problem using sorting. After sorting the given array, we just need to return the  element from it. The cost of 
 performing the sort (assuming comparison-based sorting algorithm) is O(). If we perform  such selections then 
 the average cost of each selection is O(
 /) =
  O(). This clearly indicates that sorting once is reducing the 
 complexity of subsequent operations.",NA
1.28 Algorithms Analysis: Problems & Solutions,"Note: 
 From the following problems, try to understand the cases which have different complexities (O
 (), 
 O
 (), 
 O
 ()
  
 etc.). 
  
 Problem-21 
 Find the complexity of the below recurrence: 
  
  
 () = 3( − 1),   > 0, 
  
  
  
 1,              
 ℎ
 Solution:
  Let us try solving this function with substitution.
  
 () = 3( − 1)
  
 () = 33( − 2) = 3( − 2)
  
 () = 3(3( − 3)) 
  
 .
  
 .
  
  
 () = 3( − ) = 3(0) = 3 
  
 This clearly shows that the complexity of this function is O(
 3)
 . 
  
 Note:
  We can use the  master theorem for this problem.
  
 Problem-22 
 Find the complexity of the below recurrence: 
  
  
  
 () = 2( − 1) − 1,  > 0, 
  
  
  
 1,                      ℎ
 Solution:
  Let 
 us try solving this function with substitution.
  
 () = 2( − 1) − 1
  
 () = 2(2( − 2) − 1) − 1 = 2( − 2) − 2 − 1
  
 () = 2(2( − 3) − 2 − 1) − 1 = 2( − 4) − 2− 2− 2
  
 () = 2( − ) − 2− 2− 2… . 2− 2− 2
  
 () = 2− 2− 2− 2… . 2− 2− 2
  
 () = 2− (2− 1) [: 2+ 2+ ⋯ + 2= 2]
  
 () = 1
  
 ∴
  Time Complexity is O
 (1).
  Note that while the recurrence relation looks exponential, the solution to the 
 recurrence relation here gives a different result. 
  
 Problem-23
  
 What is the running time of the following function? 
  
  
 func function(n int) { 
  
  i := 1 
  
  s := 1 
  
  for s <= n { 
  
 i++ 
  
 s = s + i 
  
 1.27 Amortized Analysis 
  
 39",NA
Recursion and ,NA,NA
Backtracking ,NA,NA
Chapter,NA,NA
2 ,NA,NA
2.1 Introduction ,"In this chapter, we will look at one of the important topics, “”, which will be used in almost every chapter, and 
 also its relative “”.",NA
2.2 What is Recursion? ,"Any function which calls itself is called . A recursive method solves a problem by calling a copy of itself to work 
 on a smaller problem. This is called the recursion step. The recursion step can result in many more such 
 recursive calls. 
  
 It is important to ensure that the recursion terminates. Each time the function calls itself with a slightly simpler 
 version of the original problem. The sequence of smaller problems must eventually converge on the base case.",NA
2.3 Why Recursion? ,"Recursion is a useful technique borrowed from mathematics. Recursive code is generally shorter and easier to 
 write than iterative code. Generally, loops are turned into recursive functions when they are compiled or 
 interpreted. 
  
 Recursion is most useful for tasks that can be defined in terms of similar subtasks. For example, sort, search, 
 and traversal problems often have simple recursive solutions.",NA
2.4 Format of a Recursive Function ,"A recursive function performs a task in part by calling itself to perform the subtasks. At some point, the function encounters a 
 subtask that it 
  
 can perform without calling itself. This case, where the function does not recur, is called the 
  . 
 The former, where the function calls 
  
 itself to perform a subtask, is referred to as the 
 . 
  We can write all recursive functions using the format: 
  
 if(test for the base case) { 
  
  return some base case value 
  
 } 
  
 else if(test for another base case) { 
  
  return some other base case value 
  
 } 
  
 // the recursive case 
  
 return (some work and then a recursive call) 
  
 As an example, consider the factorial function: 
 !
 is the product of all integers between and 
 1
 . The definition of recursive factorial 
 looks 
  
 like: 
  
 ! =  1
 , 
  
 if 
  =  0
  
  
 ! =   ∗ ( − 1)!
   if 
  >  0 
  
 This definition can easily be converted to recursive implementation. Here the problem is determining the value 
 of 
 !,
 and the subproblem is determining the value of 
 ( − )!
 . 
 In the recursive case, when is greater than 
 1
 , the 
 function calls itself to determine the value of
  ( − )! 
 and multiplies that with . In the base case, when is 0 or 
 1
 , the 
 function simply returns 
 1
 . This looks like the following: 
  
 var factVal uint64 = 1 // uint64 is the set of all unsigned 64-bit integers. 
  
  
 // calculates factorial of a positive integer 
  
 func factorial(n int) int {",NA
2.5 Recursion and Memory (Visualization) ,"Each recursive call makes a new copy of that method (actually only the variables) in memory. Once a method 
 ends (that is, returns some data), the copy of that returning method is removed from memory. The recursive 
 solutions look simple but visualization and tracing takes time. For better understanding, let us consider the 
 following example. 
  
 package main 
  
 import ""fmt"" 
  
 // print numbers 1 to n backward 
  
 func print(n int) int { 
  
  if n == 0 { 
  
  return 0   
  
 // this is the terminating base case 
  
 } 
  
  } 
  
  fmt.Println(n) 
  
  return print(n - 1)         // recursive call to itself again 
  
  
  
 func main() { 
  
  fmt.Println(print(5)) 
  
 } 
  
 For this example, if we call the print function with n=4, visually our memory assignments may look like: 
  
 print(4) 
  
 print (3) 
  
 Returns 0 
  
 Returns 0 
  
 print (2) 
  
 print (1) 
  
 print (0) 
  
 Returns 0 to main function 
  
 Returns 0 
  
 Returns 
 0 
  
 Now, let us consider our factorial function. The visualization of factorial function with n=4 will look like: 
  
 4! 
  
 4* 3! 
  
 4*6=24 
  
 is 
  
 3*2! 
  
 is 
  
 2*1! 
  
 1 
  
  
 returned 
  
 is 
  
 3*2=6 
  
 Returns 24 to main function 
  
 2*1=2 
  
 Returns 1 
  
 returned 
  
 2.5 Recursion and Memory (Visualization) 
  
  
  
  
  
 51",NA
2.6 Recursion versus Iteration ,"While discussing recursion, the basic question that comes to mind is: which way is better? – iteration or 
 recursion? The answer to this question depends on what we are trying to do. A recursive approach mirrors the 
 problem that we are trying to solve. A recursive approach makes it simpler to solve a problem that may not have 
 the most obvious of answers. But recursion adds overhead for each recursive call (needs space on the stack 
 frame). 
  
 Recursion 
  
 •
  
 •
  
 •
  
 •
  
 Terminates when a base case is reached. 
  
 Each recursive call requires extra space on the stack frame (memory). 
  
 If we get infinite recursion, the program may run out of memory and result in stack overflow. 
 Solutions to some problems are easier to formulate recursively. 
  
 Iteration 
  
 •
  
 •
  
 •
  
 •
  
 Terminates when a condition is proven to be false. 
  
 Each iteration does not require extra space. 
  
 An infinite loop could loop forever since there is no extra memory being created. 
 Iterative solutions to a problem may not always be as obvious as a recursive solution.",NA
2.7 Notes on Recursion ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Recursive algorithms have two types of cases, recursive cases and base cases. 
  
 Every recursive function case must terminate at a base case. 
  
 Generally, iterative solutions are more efficient than recursive solutions [due to the overhead of function 
 calls]. 
  
 A recursive algorithm can be implemented without recursive function calls using a stack, but it’s 
 usually more trouble than its worth. That means any problem that can be solved recursively can also be 
 solved iteratively. 
  
 For some problems, there are no obvious iterative algorithms. 
  
 Some problems are best suited for recursive solutions while others are not.",NA
2.8 Example Algorithms of Recursion ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Fibonacci Series, Factorial Finding 
  
 Merge Sort, Quick Sort 
  
 Binary Search 
  
 Tree Traversals and many Tree Problems: InOrder, PreOrder PostOrder 
 Graph Traversals: DFS [Depth First Search] and BFS [Breadth First Search] 
 Dynamic Programming Examples 
  
 Divide and Conquer Algorithms 
  
 Towers of Hanoi 
  
 Backtracking Algorithms [we will discuss in next section]",NA
2.9 Recursion: Problems & Solutions ,"In this chapter we cover a few problems with recursion and we will discuss the rest in other chapters. By the 
 time you complete reading the entire book, you will encounter many recursion problems. 
  
 Problem-1
  Discuss Towers of Hanoi puzzle. 
  
 Solution:
  The Towers of Hanoi is a mathematical puzzle. It consists of three rods (or pegs or towers), and a 
 number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks on one rod in 
 ascending order of size, the smallest at the top, thus making a conical shape. The objective of the puzzle is to 
 move the entire stack to another rod, satisfying the following rules: 
  
 •
  
 •
  
 •
  
 Only one disk may be moved at a time. 
  
 Each move consists of taking the upper disk from one of the rods and sliding it onto another rod, on top 
 of the other disks that may already be present on that rod. 
  
 No disk may be placed on top of a smaller disk. 
  
 Algorithm: 
  
 •
  
 Move the top 
  − 1
  disks from  to  tower, 
  
  
 •
  
 Move the  disk from  to  tower, 
  
 •
  
 Move the 
  − 1
 disks from  tower to  tower. 
  
 2.6 Recursion versus Iteration 
  
 52",NA
2.10 What is Backtracking? ,"Backtracking is an improvement of the brute force approach. It systematically searches for a solution to a 
 problem among all available options. In backtracking, we start with one possible option out of many available 
 options and try to solve the problem if we are able to solve the problem with the selected move then we will print 
 the solution else we will backtrack and select some other option and try to solve it. If none if the options work 
 out, we will claim that there is no solution for the problem. 
  
 Backtracking is a form of recursion. The usual scenario is that you are faced with a number of options, and you 
 must choose one of these. After you make your choice you will get a new set of options; just what set of options 
 you get depends on what choice you made. This procedure is repeated over and over until you reach a final 
 state. If you made a good sequence of choices, your final state is a goal state; if you didn't, it isn't. 
  
 2.10 What is Backtracking? 
  
 53",NA
2.11 Example Algorithms of Backtracking ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Binary Strings: generating all binary strings 
 Generating 
  −
 ary Strings 
  
 N-Queens Problem 
  
 The Knapsack Problem 
  
 Generalized Strings 
  
 Hamiltonian Cycles [refer to 
 ℎ
  chapter] 
 Graph Coloring Problem",NA
2.12 Backtracking: ,NA,NA
Problems & Solutions,"Problem-3
   Generate all the strings of  bits. Assume 
 [0. . − 1]
  is an array of size 
 .
  
 Solution: 
  
 func printResult(A []int, n int) { 
  
  
 var i int 
  
  
 for ; i < n; i++ { 
  
  
 // Function to print the output 
  
  
 fmt.Print(A[i]) 
  
  
 } 
  
  
 fmt.Printf(""\n"") 
  
 } 
  
  
 // Function to generate all binary strings 
  
 func generateBinaryStrings(n int, A []int, i int) { 
  
  
 if i == n { 
  
  
 printResult(A, n) 
  
  
 return 
  
  
 } 
  
  
 // assign ""0"" at ith position and try for all other permutations for remaining positions 
  
 A[i] = 0 
  
  
 generateBinaryStrings(n, A, i+1) 
  
  
 // assign ""1"" at ith position and try for all other permutations for remaining positions 
  
 A[i] = 1 
  
  
 generateBinaryStrings(n, A, i+1) 
  
 } 
  
 Let 
 ()
  be the running time of 
 ().
  Assume function  takes time O
 (1).
  
 () = ,                              if  < 0 
  
  
 2( − 1) + , otherwise
  
 Using Subtraction and Conquer Master theorem we get: 
 () =
  O
 (2)
 . This means the algorithm for generating bit-
 strings is optimal. 
  
 Problem-4
 Generate all the strings of length  drawn from 
 0.. .  −  1.
  
 Solution:
   Let us assume we keep current k-ary string in an array 
 [0. .  − 1]
 . Call function -(n, k): 
  
 2.11 Example Algorithms of Backtracking 
  
 54",NA
Linked Lists ,NA,NA
Chapter,NA,NA
3 ,NA,NA
3.1 What is a Linked List? ,"One disadvantage of using arrays to store data is that arrays are static structures and therefore cannot be 
 easily extended or reduced to fit the data set. Arrays are also expensive to maintain new insertions and 
 deletions. In this chapter we consider another data structure called Linked Lists that addresses some of the 
 limitations of arrays. A linked list is a data structure used for storing collections of data. A linked list has the 
 following properties. A linked list is a linear dynamic data structure. The number of nodes in a list is not fixed 
 and can grow and shrink on demand. Each node of a linked list is made up of two items - the data and a 
 reference to the next node. The last node has a reference to . The entry point into a linked list is called the head 
 of the list. It should be noted that head is not a separate node, but the reference to the first node. If the list is 
 empty then the head is a null reference. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Successive elements are connected by pointers. 
  
 The last element points to . 
  
 Can grow or shrink in size during execution of a program. 
  
 Can be made just as long as required (until systems memory exhausts). 
  
 Does not waste memory space (but takes some extra memory for pointers). It 
 allocates memory as list 
 grows.
  
  4 
  
  15 
  
  7 
  
  40 
  
  
 head",NA
3.2 Linked Lists ADT ,"The following operations make linked lists an ADT: 
  
 Main Linked Lists Operations 
  
 •
  
 •
  
 Insert: inserts an element into the list 
  
 Delete: removes and returns the specified position element from the list 
  
 Auxiliary Linked Lists Operations 
  
 •
  
 •
  
 •
  
 Delete List: removes all elements of the list (disposes the list) 
 Count: returns the number of elements in the list 
  
 Find  node from the end of the list",NA
3.3 Why Linked Lists? ,"There are many other data structures that do the same thing as linked lists. Before discussing linked lists, it is 
 important to understand the difference between linked lists and arrays. Both linked lists and arrays are used to 
 store collections of data, and since both are used for the same purpose, we need to differentiate their usage. 
 That means in which cases  are suitable and in which cases  are suitable.",NA
3.4 Arrays Overview,"One memory block is allocated for the entire array to hold the elements of the array. The array elements can be 
 accessed in constant time by using the index of the particular element as the subscript. 
  
  3             2             1             2           2           3 
  
 Index 
  
 0 
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
  
 3.1 What is a Linked List? 
  
  
  
  
  
  
  
 57",NA
3.5 Comparison of Linked Lists with Arrays and Dynamic Arrays ,"As with most choices in computer programming and design, no method is well suited to all circumstances. A 
 linked list data structure might work well in one case, but cause problems in another. This is a list of some of 
 the common tradeoffs involving linked list structures. In general, if you have a dynamic collection, where 
 elements are frequently being added and deleted, and the location of new elements added to the list is 
 significant, then benefits of a linked list increase. 
  
 3.5 Comparison of Linked Lists with Arrays and Dynamic Arrays 
  
 58",NA
3.6 Singly Linked Lists ,"The linked list consists of a series of structures called nodes. We can think of each node as a record. The first 
 part of the record is a field that stores the data, and the second part of the record is a field that stores a pointer 
 to a node. So, each node contains two fields: a  field and a  field which is a pointer used to link one node to the 
 next node. Generally, ""linked list"" means a singly linked list. This list consists of a number of nodes in which 
 each node has a pointer to the following element. The link of the last node in the list is , which indicates the end 
 of the list. 
  
 Each node is allocated in the heap with a call to (), so the node memory continues to exist until it is explicitly 
 deallocated with a call to (). The node called a 
 ℎ
  is the first node in the list. The last node's next pointer points to  
 value. 
  
  4 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 head 
  
 A linked list node contains a pointer to the next node as well the data. This generic behavior is achieved by 
 marking the data field as type interface{}: 
  
 type ListNode struct  {             // defines a ListNode in a singly linked list  
 data interface{}                    // the datum 
  
  next *ListNode                    // pointer to the next ListNode  
  
 } 
  
 A linked list contains pointer to first node in the list and its size. The  field in the linked list structure stores the 
 length of the linked list. The 
 ℎ
  field of ListNode type stores the memory address of the head or the first node of 
 the linked list. 
  
 type LinkedList struct { 
  
  
 head *ListNode 
  
  
 size  int 
  
 }",NA
Basic Operations on a List ,"•
  
 •
  
 •
  
 Traversing the list 
  
 Inserting an item in the list 
  
 Deleting an item from the list",NA
Traversing the Linked List,"Let us assume that the 
 ℎ
  points to the first node of the list. To traverse the list, we do the following. 
  
 •
  
 Follow the pointers. 
  
 nil 
  
 •
  
 Display the contents of the nodes (or count) as they are traversed. 
  
 •
  
 Stop when the next pointer points to . 
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 3.6 Singly Linked Lists 
  
 59",NA
Singly Linked List Insertion ,"Insertion into a singly-linked list has three cases: 
  
 •
  
 •
  
 •
  
 Inserting a new node before the head (at the beginning) 
  
 Inserting a new node after the tail (at the end of the list) 
  
 Inserting a new node at the middle of the list (random location) 
  
 Note:
   To insert an element in the linked list at some position , assume that after inserting the element the 
 position of this new node is . 
  
 Inserting a Node at the Beginning
  
 In this case, a new node is inserted before the current head node.  needs to be modified (new node’s next pointer) and i
 t can be 
 done in two steps: 
  
  
 •
  
 Update the next pointer of new node, to point to the current head. 
  
 new node 
  
  data 
  
  15 
  
  7 
  
  40 
  
 nil 
  
  
 head 
  
 •
  
 Update head pointer to point to the new node.
  
 new node 
  
  data 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 head 
  
 func (ll *LinkedList) InsertBeginning(data interface{}) {  
 node := &ListNode{ 
  
  
 3.6 Singly Linked Lists 
  
 60",NA
Inserting a Node at the Given Position ,"Let us assume that we are given a position where we want to insert the new node. In this case also, we need to 
 modify two next pointers. 
  
 •
  
 To add an element at position 
 3
  then we stop at position 
 2
 . That means, traverse 
 2
  nodes and insert the 
 new node. For simplicity let us assume that  will point to the predecessor of new node. The next 
  
 pointer of new node points to the next node of the  node.  
 node 
  
  4 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 head 
  
 new node 
  
  data 
  
 •
  
  node’s next pointer now points to the new node. 
  
  
 3.6 Singly Linked Lists 
  
 61",NA
Singly Linked List Deletion ,"Similar to insertion, here also have three cases. 
  
 •
  
 •
  
 •
  
 Deleting the first node 
  
 Deleting the last node 
  
 Deleting an intermediate node 
  
 Deleting the First Node 
  
 First node (current head node) is removed from the list. It can be done in two steps: 
  
 •
  
 Create a temporary node which will point to the same node as that of head. 
  
  4  15 
  
  7  40 
  
 nil 
  
 head 
  
 temp 
  
 •
  
 Now, move the head nodes pointer to the next node and dispose of the temporary node. 
  
  4 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 temp 
  
 head 
  
 func (ll *LinkedList) DeleteFirst() (interface{}, error) { 
  
  
 3.6 Singly Linked Lists 
  
 62",NA
3.7 Doubly Linked Lists ,"A doubly linked list is a linked list in which a node contains a pointer to the previous as well as the next node in 
 the sequence. Therefore, in a doubly linked list (also called 
  −   
 ), a node consists of three parts: data, a pointer 
 to the next node in sequence (next pointer), a pointer to the previous node (previous pointer). Each node has 
 two links: one point to the previous node, or points to a  value or empty list if it is the first node, and one point 
 to the next, or points to a  value or empty list if it is the final node. 
  
 The  of a doubly linked list is that given a node in the list, we can navigate in both directions. A node in a singly 
 linked list cannot be removed unless we have the pointer to its predecessor. But in a doubly linked list, we can 
 delete a node even if we don’t have the previous node’s address (since each node has a left pointer pointing to 
 the previous node and can move backward). 
  
 The primary  of doubly linked lists are: 
  
 •
  
 •
  
 Each node requires an extra pointer, requiring more space. 
  
 The insertion or deletion of a node takes a bit longer (more pointer operations). 
  
 Similar to a singly linked list, let us implement the operations of a doubly linked list. If you understand the 
 singly linked list operations, then doubly linked list operations are obvious. Following is a type declaration for a 
 doubly linked list node: 
  
 type DLLNode struct { // defines a DLLNode in a doubly linked list 
  
  
 3.7 Doubly Linked Lists 
  
 64",NA
3.8 Circular Linked Lists ,"In a singly-circularly-linked list, each node has one link, similar to an ordinary singly-linked list, except that the 
 next link of the last node points back to the first node. In singly linked lists and doubly linked lists, the end of 
 lists are indicated with nil value. But, circular linked lists do not have ends. While traversing the circular linked 
 lists we should be careful; otherwise we will be traversing the list indefinitely. 
  
 In circular linked lists, each node has a successor. In a circular linked list, every node point to its next node in 
 the sequence but the last node points to the first node in the list. 
  
  4 
  
  15 
  
  7 
  
  40 
  
  
 In some situations, circular linked lists are useful. For example, when several processes are using the same 
 computer resource (say, CPU) for the same amount of time, we have to assure that no process accesses the 
 resource before all other processes do (round robin algorithm). The following is a type declaration for a circular 
 linked list of integers: 
  
 // CLLNode ... newNode of circular linked list 
  
  
 type CLLNode struct { 
  
  
 data int 
  
  
 next *CLLNode 
  
 } 
  
 In a circular linked list, we access the elements using the
 ℎ  
 node (similar to 
 ℎ
  node in singly linked list and 
 doubly linked lists). 
  
 // CLL ... A linked list with len, start/head 
  
 type CLL struct { 
  
  
 size  int 
  
  
 head *CLLNode 
  
 }",NA
Counting Nodes in a Circular Linked List,"4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 The circular list is accessible through the node marked 
 ℎ
 . To count the nodes, the list has to be traversed from 
 the node marked 
 ℎ
 , with the help of a dummy node 
 ,
 and stop the counting when  reaches the starting node 
 ℎ
 .  If 
 the list is empty, 
 ℎ
  will be nil, and in that case set 
  =  0
 . Otherwise, set the current pointer to the first node, and 
 keep on counting till the current pointer reaches the starting node. 
  
 // Since we have size property in CLL, we can access the length directly, but following is an alternative 
 func (cll *CLL) Length() int { 
  
  
 current := cll.head 
  
  
 count := 1 
  
  
 if current == nil { 
  
  
 return 0 
  
  
 } 
  
  
 current = current.next 
  
  
 for current != cll.start { 
  
  
 current = current.next 
  
 3.8 Circular Linked Lists 
  
 70",NA
Printing the Contents of a Circular Linked List ,"We assume here that the list is being accessed by its 
 ℎ
  node. Since all the nodes are arranged in a circular 
 fashion, the  node of the list will be the node previous to the 
 ℎ
  node. Let us assume we want to print the 
 contents of the nodes starting with the 
 ℎ
  node. Print its contents, move to the next node and continue printing 
 till we reach the 
 ℎ
  node again. 
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 func (cll *CLL) Display() { 
  
  
 head := cll.head 
  
  
 for i := 0; i < cll.size; i++ { 
  
 fmt.Print(head.data) 
  
 fmt.Print(""-->"") 
  
  
 head = head.next 
  
  
 } 
  
  
 fmt.Println() 
  
 } 
  
 // Display ... Print Circular list 
  
 Time Complexity
 : 
 O
 (), 
 for scanning the complete list of size . Space Complexity: O
 (1), 
 for the temporary variable.",NA
Inserting a Node at the Front ,"The only difference between inserting a node at the beginning and at the end is that, after inserting the new 
 node, we just need to update the pointer. The steps for doing this are given below: 
  
 •
  
 •
  
 Create a new node and initially keep its next pointer pointing to itself. 
  
 data 
  
  4 
  
  15 
  
  7 
  
  40 
  
 new node 
  
 head 
  
 Update the next pointer of the new node with the head node and also traverse the list until the tail node 
 (node with data 40). 
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 •
  
 data 
  
 new node 
  
  7 
  
  40 
  
 Update the tail node in the list to point to the new node. 
  
  4 
  
  15 
  
  7 
  
 •
  
 head 
  
 data 
  
 new node 
  
 40 
  
 Make the new node as the head. 
  
  data 
  
  4 
  
 15 
  
 head 
  
 3.8 Circular Linked Lists 
  
 71",NA
Inserting a Node at the End ,"Let us add a node containing , at the end of a list (circular list) headed by 
 ℎ
 . The new node will be placed just 
 after the tail node (which is the last node of the list), which means it will have to be inserted in between the tail 
 node and the first node. 
  
 •
  
 •
  
 Create a new node and initially keep its next pointer pointing to itself. 
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 new node 
  
 data 
  
 Update the next pointer of the new node with the head node and also traverse the list to the tail. That 
 means, in a circular list we should stop at the node whose next node is head. 
  
 tail node 
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 new node 
  
 data 
  
 •
  
 Update the next pointer of the tail node to point to the new node and we get the list as shown below. 
  
  4 
  
  15 
  
  17 
  
  40 
  
 data 
  
 head 
  
 3.8 Circular Linked Lists 
  
 72",NA
Inserting a Node at the given Position ,"As discussed in singly and doubly linked lists, traverse the list to the position node and insert the new node. 
  
 •
  
  ′
  right pointer points to the next node of the  where we want to insert the new node. Also, 
  ′
  left pointer 
 points to the 
  node
 . 
  
 Position node 
  
 •
  
  4 
  
  15 
  
  7 
  
  40 
  
 head 
  
 new node 
  
 data 
  
 Point the position node’s next pointer to the new node. 
  
 Position node 
  
  4 
  
  15 
  
 data 
  
  7 
  
  40 
  
 head 
  
 new node 
  
 Now, let us write the code for all of these three cases. We must update the first element pointer in the calling 
 function, not just in the called function. The following code inserts a node in the doubly linked list. 
  
 // Insert ... Insert in the list on a specific location 
  
 func (cll *CLL) Insert(data int, pos int) { 
  
  
 if !(cll.CheckIfEmptyAndAdd(data)) { 
  
  
 current := cll.head 
  
  
 count := 1 
  
  
 if pos == 1 { 
  
  
  
  
 cll.InsertBeginning(data) 
  
  
  
  
 return 
  
  
 } 
  
  
 newNode := &CLLNode{ 
  
  
  
  
 data: data, 
  
  
  
 next: nil, 
  
 } 
  
 for { 
  
  
 if current.next == nil && pos-1 > count { 
  
  
  
 break 
  
  
 } 
  
  
 if count == pos-1 { 
  
  
  
 newNode.next = current.next 
  
  
 3.8 Circular Linked Lists 
  
 73",NA
Deleting the First Node ,"The first node can be deleted by simply replacing the next field of the tail node with the next field of the first 
 node. 
  
 •
  
 •
  
 Find the tail node of the linked list by traversing the list. Tail node is the previous node to the head 
 node which we want to delete. 
  
  60 
  
  4 
  
 15 
  
  7 
  
 40 
  
 head 
  
 Node to be deleted 
  
 Previous node to deleting node 
  
 Create a temporary node which will point to the head. Also, update the tail nodes next pointer to point 
 to next node of head (as shown below). 
  
 temp 
  
  60 
  
  4 
  
 15 
  
  7 
  
 40 
  
 head 
  
 Node to be deleted 
  
 Previous node to deleting node 
  
 •
  
 Now, move the head pointer to next node and dispose the temporary node (as shown below). 
  
 temp 
  
  60 
  
  4 
  
 15 
  
  7 
  
 40 
  
 head 
  
 // CheckIfEmpty ... check if empty 
  
 func (cll *CLL) CheckIfEmpty() bool { 
  
  
 if cll.size == 0 { 
  
  
 return true 
  
  
 } 
  
  
 return false 
  
 } 
  
 // DeleteBeginning ... Delete from the head of the list 
 func (cll *CLL) DeleteBeginning() int { 
  
  
 if !(cll.CheckIfEmpty()) { 
  
 //check if list if empty 
  
 current := cll.head 
  
 deletedElem := current.data 
  
 if cll.size == 1 { 
  
  
 cll.head = nil 
  
  
 cll.size-- 
  
  
 return deletedElem 
  
 } 
  
 prevStart := cll.head 
  
 cll.head = current.next 
  
 for { 
  
 // traverse till end and update last node's next to updated head 
  
 } 
  
 if current.next == prevStart { 
  
 break 
  
 } 
  
 current = current.next 
  
  
 3.8 Circular Linked Lists 
  
 74",NA
Deleting the Last Node ,"The list has to be traversed to reach the last but one node. This has to be named as the tail node, and its next 
 field has to point to the first node. Consider the following list.  To delete the last node 
 40
 , the list has to be 
 traversed till you reach 
 7
 . The next field of 7 has to be changed to point to 
 60
 , and this node must be renamed . 
  
 •
  
 Traverse the list and find the tail node and its previous node. 
  
 40 
  
  60 
  
  4 
  
 15 
  
  7 
  
 •
  
 head 
  
 Previous node to deleting node 
  
 Node to be deleted 
  
 Update the next pointer of tail node’s previous node to point to head. 
  
 •
  
  60 
  
  4 
  
  15 
  
  7 
  
 40 
  
 head 
  
 Previous node to deleting node 
  
 Node to be deleted 
  
 Disposethe tail node.
  
 40 
  
  60 
  
  4 
  
  15 
  
  7 
  
 head 
  
 Previous node to deleting node 
  
 Node to be deleted 
  
  
 // DeleteEnd ... Delete the last element from circular list 
 func (cll *CLL) DeleteEnd() int { 
  
  
 if !(cll.CheckIfEmpty()) { 
  
  
 current := cll.head 
  
  
 deletedEle := current.data 
  
  
 if cll.size == 1 { 
  
  
  
  
 // delete from beginning 
  
  
  
  
 deletedEle = cll.DeleteBeginning() 
  
  
  
  
 return deletedEle 
  
  
 } 
  
  
 //traverse till end 
  
  
 for { 
  
  
  
  
 if current.next.next == cll.head { 
  
  
  
  
  
 deletedEle = current.next.data 
  
  
  
  
  
 break 
  
  
  
  
 } 
  
  
  
  
 current = current.next 
  
  
 } 
  
  
 // update last element's next pointer 
  
  
 current.next = cll.head 
  
  
 cll.size-- 
  
  
 return deletedEle 
  
  
 } 
  
  
 return -1 
  
 } 
  
  
 3.8 Circular Linked Lists 
  
 75",NA
Applications of Circular List,"Circular linked lists are used in managing the computing resources of a computer. We can use circular lists for 
 implementing stacks and queues.",NA
3.9 A Memory-efficient Doubly Linked Lists (XOR Linked Lists) ,"In conventional implementation, we need to keep a forward pointer to the next item on the list and a backward 
 pointer to the previous item. That means elements in doubly linked list implementations consist of data, a 
 pointer to the next node and a pointer to the previous node in the list as shown below.
  
 3.9 A Memory-efficient Doubly Linked Lists (XOR Linked Lists) 
  
 76",NA
3.10 Unrolled Linked Lists ,"One of the biggest advantages of linked lists over arrays is that inserting an element at any location takes only 
 O(1) time. However, it takes O() to search for an element in a linked list. There is a simple variation of the singly 
 linked list called . 
  
 An unrolled linked list stores multiple elements in each node (let us call it a block for our convenience). In each 
 block, a circular linked list is used to connect all nodes. 
  
 3.10 Unrolled Linked Lists 
  
 77",NA
Searching for an element in Unrolled Linked Lists ,"In unrolled linked lists, we can find the  element in O(
 √
 ): 
  
 1.Traverse the  to the one that contains the  node, i.e., the 
 √
 th
  block. It takes O(
 √
 ) since 
  
 we may find it 
 by going through no more than 
 √
  blocks. 
  
 2.Find the ( mod 
 √
 )
 th
  node in the circular linked list of this block. It also takes O(
 √
 ) since there are no 
  
 more than 
 √
  nodes in a single block. 
  
 List Head 
  
 10 
  
 blockHead 
  
 30 
  
 6 
  
 70 
  
 blockHead 
  
 45 
  
 2 
  
 91 
  
 blockHead 
  
 4 
  
 17 
  
 / 
  
 19 
  
 1 
  
 3",NA
Inserting an element in Unrolled Linked Lists ,"When inserting a node, we have to re-arrange the nodes in the unrolled linked list to maintain the properties 
 previously mentioned, that each block contains 
 √
  nodes. Suppose that we insert a node  after the  node, and  
 should be placed in the  block. Nodes in the  block and in the blocks after the  block have to be shifted toward 
 the tail of the list so that each of them still have 
 √
  nodes. In addition, a new block needs to be added to the tail 
 if the last block of the list is out of space, i.e., it has more than 
 √
  nodes. 
  
 List Head 
  
 blockHead 
  
 22 
  
 30 
  
 6 
  
 blockHead 
  
 45 
  
 2 
  
 blockHead 
  
 17 
  
 / 
  
 / 
  
 10 
  
 1 
  
 70 
  
 3 
  
 19 
  
 4 
  
 List Head 
  
 22 
  
 30 
  
 Shifting 
  
 45 
  
 Shifting 
  
 17 
  
 element 
  
 element 
  
 blockHead 
  
 blockHead 
  
 blockHead 
  
 10 
  
 1 
  
 6 
  
 70 
  
 3 
  
 2 
  
 19 
  
 4",NA
Performing Shift Operation ,"Note that each 
 ℎ
  operation, which includes removing a node from the tail of the circular linked list in a block 
 and inserting a node to the head of the circular linked list in the block after, takes only O(1). The total time 
 complexity of an insertion operation for unrolled linked lists is therefore O(
 √
 ); there are at most O(
 √
 ) blocks and 
 therefore at most O(
 √
 ) shift operations. 
  
 1.A temporary pointer is needed to store the tail of . 
  
 A 
  
 3 
  
 45 
  
 temp 
  
 B 
  
 4 
  
 17 
  
  
 70 
  
 19 
  
 3.10 Unrolled Linked Lists 
  
  
  
  
  
  
  
 78",NA
Performance ,"With unrolled linked lists, there are a couple of advantages, one in speed and one in space. First, if the number 
 of elements in each block is appropriately sized (e.g., at most the size of one cache line), we get noticeably better 
 cache performance from the improved memory locality. Second, since we have O(/) links, where  is the number 
 of elements in the unrolled linked list and  is the number of elements we can store in any block, we can also 
 save an appreciable amount of space, which is particularly noticeable if each element is small.",NA
Comparing Linked Lists and Unrolled Linked Lists ,"To compare the overhead for an unrolled list, elements in doubly linked list implementations consist of data, a 
 pointer to the next node, and a pointer to the previous node in the list, as shown below. 
  
 type ListNode struct { 
  
  
 data int 
  
 next *ListNode 
  
  
 } 
  
 Assuming we have 4-byte pointers, each node is going to take 8 bytes. But the allocation overhead for the node 
 could be anywhere between 8 and 16 bytes. Let’s go with the best case and assume it will be 8 bytes. So, if we 
 want to store 1K items in this list, we are going to have 16KB of overhead. 
  
 Now, let’s think about an unrolled linked list node (let us call it ). It will look something like this: 
  
 type LinkedBlock struct { 
  
  
 nodeCount int 
  
  
 next      *LinkedBlock 
  
  
 head      *ListNode 
  
  
 3.10 Unrolled Linked Lists 
  
 79",NA
Implementation ,"package main 
  
 import ""fmt"" 
  
 var blockSize int //max number of nodes in a block 
  
 type ListNode struct { 
  
  
 data int 
  
  
 next *ListNode 
  
 } 
  
  
 type LinkedBlock struct { 
  
  
 nodeCount int 
  
  
 next      *LinkedBlock 
  
  
 head      *ListNode 
  
 } 
  
 var blockHead *LinkedBlock 
  
 // create an empty block 
  
 func newLinkedBlock() *LinkedBlock { 
  
 block := &LinkedBlock{} 
  
  
 block.next = nil 
  
  
 block.head = nil 
  
  
 block.nodeCount = 0 
  
  
 return block 
  
 } 
  
  
 // create a node 
  
 func newListNode(data int) *ListNode { 
  
 newNode := &ListNode{} 
  
  
 newNode.next = nil 
  
  
 newNode.data = data 
  
  
 return newNode 
  
 } 
  
  
 func searchElement(k int, fLinkedBlock **LinkedBlock, fListNode **ListNode) { 
  
 // find the block 
  
  
 j := (k + blockSize - 1) / blockSize // k-th node is in the j-th block 
  
  
 p := blockHead 
  
  
 j = j - 1 
  
  
 for j > 0 { 
  
  
 p = p.next 
  
  
 j = j - 1 
  
  
 } 
  
  
 *fLinkedBlock = p 
  
 } 
  
 // find the node 
  
 q := p.head 
  
 k = k % blockSize 
  
 if k == 0 { 
  
 k = blockSize 
  
 } 
  
 k = p.nodeCount + 1 - k 
  
 for k > 0 { 
  
 k = k - 1 
  
 q = q.next 
  
 } 
  
 *fListNode = q 
  
  
  
 //start shift operation from block *p 
  
  
 3.10 Unrolled Linked Lists 
  
 80",NA
3.11 Skip Lists ,"Binary trees can be used for representing abstract data types such as dictionaries and ordered lists. They work 
 well when the elements are inserted in a random order. Some sequences of operations, such as inserting the 
 elements in order, produce degenerate data structures that give very poor performance. If it were possible to 
 randomly permute the list of items to be inserted, trees would work well with high probability for any input 
 sequence. In most cases queries must be answered on-line, so randomly permuting the input is impractical. 
 Balanced tree algorithms re-arrange the tree as operations are performed to maintain certain balance conditions 
 and assure good performance. 
  
 Skip lists are a probabilistic alternative to balanced trees. Skip list is a data structure that can be used as an 
 alternative to balanced binary trees (refer to  chapter). As compared to a binary tree, skip lists allow quick 
 search, insertion and deletion of elements. This is achieved by using probabilistic balancing rather than strictly 
 enforce balancing. It is basically a linked list with additional pointers such that intermediate nodes can be 
 skipped. It uses a random number generator to make some decisions. 
  
 In an ordinary sorted linked list, search, insert, and delete are in O() because the list must be scanned node-by-
 node from the head to find the relevant node. If somehow we could scan down the list in bigger steps (skip 
 down, as it were), we would reduce the cost of scanning. This is the fundamental idea behind Skip Lists. 
  
 Skip Lists with One Level 
  
 3 
  
 7 
  
 12 
  
 19 
  
 23 
  
 29 
  
 43 
  
 70 
  
 Skip Lists with Two Levels 
  
 3 
  
 7 
  
 12 
  
 19 
  
 23 
  
 29 
  
 43 
  
 70 
  
 Skip Lists with Three Levels 
  
 3 
  
 7 
  
 12 
  
 19 
  
 23 
  
 29 
  
 43 
  
 70",NA
Performance ,"In a simple linked list that consists of  elements, to perform a search  comparisons are required in the worst 
 case. If a second pointer pointing two nodes ahead is added to every node, the number of comparisons goes 
 down to 
 /2 + 1
  in the worst case. 
  
 Adding one more pointer to every fourth node and making them point to the fourth node ahead reduces the 
 number of comparisons to 
 ⌈/4⌉
  + 2. If this strategy is continued so that every node with  pointers points to 
 2 ∗ − 
 1
  nodes ahead, O() performance is obtained and the number of pointers has only doubled ( + 
 /2
  + 
 /4
  + 
 /8
  + 
 /16
  + 
 ....  = 
 2
 ). 
  
 The find, insert, and remove operations on ordinary binary search trees are efficient, O(), when the input data is 
 random; but less efficient, O(), when the input data is ordered. Skip List performance for these same operations 
 and for any data set is about as good as that of randomly-built binary search trees - namely O(). 
  
 3.11 Skip Lists 
  
 82",NA
Comparing Skip Lists and Unrolled Linked Lists ,"In simple terms, Skip Lists are sorted linked lists with two differences: 
  
 •
  
 •
  
 The nodes in an ordinary list have one next reference. The nodes in a Skip List have many  references 
 (also called  references). 
  
 The number of  references for a given node is determined probabilistically. 
  
 We speak of a Skip List node having levels, one level per forward reference. The number of levels in a node is 
 called the  of the node. In an ordinary sorted list, insert, remove, and find operations require sequential 
 traversal of the list. This results in O() performance per operation. Skip Lists allow intermediate nodes in the list 
 to be skipped during a traversal - resulting in an expected performance of O() per operation.",NA
Implementation ,"package main 
  
 import ( 
  
  
 ""fmt"" 
  
  
 ""math/rand"" 
  
  
 ""time"" 
  
 ) 
  
 const MAX_LEVEL = 32 
  
 type SkiplistNode struct { 
  
  
 data  int 
  
  
 level []*SkiplistNode 
  
 } 
  
 type Skiplist struct { 
  
  
 head            *SkiplistNode 
  
  
 currentMaxLevel int 
  
  
 random          *rand.Rand 
  
 } 
  
  
 func createSkipList() Skiplist { 
  
  
 skl := Skiplist{ 
  
  
 head:            new(SkiplistNode), 
  
 currentMaxLevel: 0, 
  
  
 } 
  
  
 } 
  
 skl.head.level = make([]*SkiplistNode, MAX_LEVEL) 
 source := rand.NewSource(time.Now().UnixNano()) 
 skl.random = rand.New(source) 
  
 return skl 
  
  
 func (this *Skiplist) search(target int) bool { 
  
 p 
 := this.head 
  
  
 for i := this.currentMaxLevel - 1; i >= 0; i-- { 
  
 for p.level[i] != nil { 
  
  
  
  
 if p.level[i].data == target { 
  
  
  
  
  
 return true 
  
  
  
  
 } 
  
  
  
  
 if p.level[i].data > target { 
  
  
  
  
  
 break 
  
  
  
  
 } 
  
  
  
  
 p = p.level[i] 
  
 } 
  
 } 
  
 } 
  
 return false 
  
  
  
 func (this *Skiplist) add(num int) { 
  
  
 update := make([]*SkiplistNode, MAX_LEVEL) 
  
 p := this.head 
  
  
 for i := this.currentMaxLevel - 1; i >= 0; i-- { 
  
 for p.level[i] != nil && p.level[i].data < num { 
  
  
  
 p = p.level[i] 
  
  
 3.11 Skip Lists 
  
 83",NA
3.12 Linked Lists,NA,NA
: Problems & Solutions,"Problem-1
 Implement Stack using Linked List.
  
 Solution:
  Refer to  chapter.
  
 Problem-2
 Find node from the end of a Linked List.
  
 Solution:Brute-Force Method: 
 Start with the first node and count the number of nodes present after that 
 node. If the number of nodes is 
 <   − 1
  then return saying “fewer number of nodes in the list”. If the number of 
 nodes is 
 >   − 1 
 then go to next node. Continue this until the numbers of nodes after current node are 
  − 1.
  
 Time Complexity: O
 (),
  for scanning the remaining list (from current node) for each node. Space Complexity: O
 (1). 
  
 Problem-3
 Can we improve the complexity of Problem-2? 
  
 Solution:Yes
 , using hash table. As an example, consider the following list. 
  
  5 
  
  1 
  
  17 
  
  4 
  
 nil 
  
 Head 
  
 In this approach, create a hash table whose entries are
  <   
 , 
   >
 . That means, key is the position of the node in 
 the list and value is the address of that node. 
  
 Position in List 
  
 Address of Node 
  
 1
  
 Address of 5 node
  
 2
  
 Address of 1 node
  
 3
  
 Address of 17 node
  
 4
  
 Address of 4 node
  
 By the time we traverse the complete list (for creating the hash table), we can find the list length. Let us say the 
 list length is . To find  from the end of linked list, we can convert this to -
   + 1
 from the beginning. Since we 
 already know the length of the list, it is just a matter of returning -
   + 1
 key value from the hash table.
  
 Time Complexity: Time for creating the hash table, 
 () = 
 O
 ().
  Space Complexity
 : 
 Since we need to create a hash 
 table of size 
 , 
 O
 ().
  
 Problem-4
 Can we use the Problem-3 approach for solving Problem-2 without creating the hash table? 
  
 Solution:Yes
 . If we observe the Problem-3solution, what we are actually doing is finding the size of the linked 
 list. That means we are using the hash table to find the size of the linked list. We can find the length of the 
 linked list just by starting at the head node and traversing the list. So, we can find the length of the list without 
 creating the hash table. After finding the length, compute 
  −  + 1
  and with one more scan we can get the 
 ( −   + 1) 
 node from the beginning.  This solution needs two scans: one for finding the length of the list and the other for 
 finding 
 ( −   + 1)
  node from the beginning. 
  
 Time Complexity: Time for finding the length 
 +
  Time for finding the 
 ( −   + 1)
  node from the beginning. Therefore, 
 () = 
 O
 () +
  O
 () ≈ 
 O
 ().
  Space Complexity: O
 (1).  
 Hence, no need to create the hash table. 
  
 Problem-5
 Can we solve Problem-2 in one scan? 
  
 Solution:Yes
 . 
  
 Efficient Approach
 : The above algorithm could be optimized to one pass. Instead of one pointer, we could use 
 two pointers. The  pointer advances the list by 
  + 1
  steps from the beginning, while the  pointer starts 
  
 3.12 Linked Lists: Problems & Solutions 
  
 85",NA
Stacks ,NA,NA
4.1 ,NA,NA
What is a Stack? ,NA,NA
Chapter,NA,NA
4 ,"A stack is a simple data structure used for storing data (similar to Linked Lists). In a stack, the order in which 
 the data arrives is important. A pile of plates in a cafeteria is a good example of a stack. The plates are added to 
 the stack as they are cleaned and they are placed on the top. When a plate, is required it is taken from the top 
 of the stack. The first plate placed on the stack is the last one to be used. 
  
 Definition:
  A is an ordered list in which insertion and deletion are done at one end, called . The last element 
 inserted is the first one to be deleted. Hence, it is called the Last in First out (LIFO) or First in Last out (FILO) 
 list. 
  
 Special names are given to the two changes that can be made to a stack. When an element is inserted in a 
 stack, the concept is called 
 ℎ
 , and when an element is removed from the stack, the concept is called . Trying to 
 pop out an empty stack is called  and trying to push an element in a full stack is called . Generally, we treat 
 them as exceptions. As an example, consider the snapshots of the stack. 
  
 top
  
 C 
  
 Push D 
  
 D 
  
 top 
  
 Pop D 
  
 C 
  
 top 
  
 C 
  
 B 
  
 B 
  
 B 
  
 A
  
 A
  
 A",NA
4.2 ,NA,NA
How Stacks are used,"Consider a working day in the office. Let us assume a developer is working on a long-term project. The manager 
 then gives the developer a new task which is more important. The developer puts the long-term project aside 
 and begins work on the new task. The phone rings, and this is the highest priority as it must be answered 
 immediately. The developer pushes the present task into the pending tray and answers the phone. 
  
 When the call is complete the task that was abandoned to answer the phone is retrieved from the pending tray 
 and work progresses. To take another call, it may have to be handled in the same manner, but eventually the 
 new task will be finished, and the developer can draw the long-term project from the pending tray and continue 
 with that.",NA
4.3 ,NA,NA
Stack ADT,"The following operations make a stack an ADT. For simplicity, assume the data is an integer type.",NA
Main stack operations ,"•
  
 •
  
 func Push(data interface{}): Inserts  onto stack. 
  
 func Pop() data interface{}: Removes and returns the last inserted element from the stack.",NA
Auxiliary stack operations ,NA,NA
Exceptions ,"Attempting the execution of an operation may sometimes cause an error condition, called an exception. 
 Exceptions are said to be “thrown” by an operation that cannot be executed. In the Stack ADT, operations pop 
 and top cannot be performed if the stack is empty. Attempting the execution of pop (top) on an empty stack 
 throws an exception. Trying to push an element in a full stack throws an exception.",NA
4.4 Applications ,Following are some of the applications in which stacks play an important role.,NA
Direct applications ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Balancing of symbols 
  
 Infix-to-postfix conversion 
  
 Evaluation of postfix expression 
  
 Implementing function calls (including recursion) 
  
 Finding of spans (finding spans in stock markets, refer to  section) Page-
 visited history in a Web browser [Back Buttons] 
  
 Undo sequence in a text editor 
  
 Matching Tags in HTML and XML",NA
Indirect applications ,"•
  
 •
  
 Auxiliary data structure for other algorithms (Example: Tree traversal algorithms) 
 Component of other data structures (Example: Simulating queues, refer  chapter)",NA
4.5 ,NA,NA
Implementation ,"There are many ways of implementing stack ADT; below are the commonly used methods. 
  
 •
  
 •
  
 •
  
 Simple array-based implementation 
 Dynamic array-based implementation 
 Linked lists implementation 
  
 Simple Array Implementation 
  
 This implementation of stack ADT uses an array. In the array, we add elements from left to right and use a 
 variable to keep track of the index of the top element. 
  
 S 
  
 …….. 
  
 top 
  
 The array storing the stack elements may become full. A push operation will then throw a . Similarly, if we try 
 deleting an element from an empty stack it will throw . 
  
 package main 
  
 import ( 
  
  
 ""errors"" 
  
  
 ""fmt"" 
  
 ) 
  
 type Stack struct { 
  
  
 top      int 
  
  
 capacity uint 
  
  
 array    []interface{} 
  
 } 
  
  
 // Returns an initialized stack 
  
  
 func (stack *Stack) Init(capacity uint) *Stack { 
  
 stack.top = -1 
  
 stack.capacity = capacity 
  
 stack.array = make([]interface{}, capacity) 
  
  
 4.4 Applications 
  
 112",NA
Linked List Implementation,"4 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 top 
  
 The other way of implementing stacks is by using Linked lists. Push operation is implemented by inserting 
 element at the beginning of the list. Pop operation is implemented by deleting the node from the beginning (the 
 header/top node).
  
 package main 
  
 import ""fmt"" 
  
  
 4.5 Implementation 
  
 116",NA
4.6 Comparison of Implementations ,"Comparing Incremental Strategy and Doubling Strategy 
  
 We compare the incremental strategy and doubling strategy by analyzing the total time 
 ()
  needed to perform a 
  
 series of push operations. We start with an empty stack represented by an array of size 
 1.
  We call  time 
  
  
 () 
 of 
 a push operation is the average time taken by a push over the series of operations, that is, 
  
 .
  
 Incremental Strategy
  
 The amortized time (average time per operation) of a push operation isO
 ()
  [O
 ()/ 
 ]. 
  
 Doubling Strategy 
  
 In this method, the amortized time of a push operation is O
 (1)
  [O
 ()/
 ]. 
  
 Note
 : For analysis, refer to the  section.",NA
Comparing Array Implementation and Linked List Implementation,"Array Implementation 
  
 •
  
 •
  
 •
  
 Operations take constant time. 
  
 Expensive doubling operation every once in a while. 
  
 Any sequence of  operations (starting from empty stack) – """" bound takes time proportional to . 
  
 Linked List Implementation 
  
 •
  
 •
  
 •
  
 Grows and shrinks gracefully. 
  
 Every operation takes constant time O
 (1).
  
 Every operation uses extra space and time to deal with references.",NA
4.7 Stacks: Problems & Solutions ,"Problem-1
 Discuss how stacks can be used for checking balancing of symbols/parentheses. 
  
 Solution:
  Stacks can be used to check whether the given expression has balanced symbols. This algorithm is 
 very 
  
 useful in compilers. Each time the parser reads one character at a time. If the character is an opening delimiter 
  
 such as (, {, or [- then it is written to the stack. When a closing delimiter is encountered like ), }, or ]- the stack is 
  
 popped. The opening and closing delimiters are then compared. If they match, the parsing of the string 
 continues. 
  
 If they do not match, the parser indicates that there is an error on the line. A linear-time O
 ()
  algorithm based on 
  
 stack can be given as: 
  
 Algorithm: 
  
 a) 
  
 Create a stack. 
  
 b) 
  
 while (end of input is not reached) { 
  
 1) 
  
 If the character read is not a symbol to be balanced, ignore it. 
  
 2) 
  
 If the character is an opening symbol like (, [, {, push it onto the stack 
  
 3) 
  
 If it is a closing symbol like ),],}, then if the stack is empty report an error. Otherwise pop the 
  
 stack. 
  
 4) 
  
 If the symbol popped is not the corresponding opening symbol, report an error. 
  
 } 
  
 c) 
  
 At end of input, if the stack is not empty report an error 
  
 Examples:
  
 Example 
  
 Valid? 
  
 Description 
  
 (A+B)+(C-D) 
  
 Yes 
  
 The expression has a balanced symbol 
  
  
 4.6 Comparison of Implementations 
  
 118",NA
Queues ,NA,NA
5.1 What is a Queue? ,NA,NA
Chapter,NA,NA
5 ,"A queue is a data structure used for storing data (similar to Linked Lists and Stacks). In queue, the order in 
 which data arrives is important. In general, a queue is a line of people or things waiting to be served in 
 sequential order starting at the beginning of the line or sequence. 
  
 Definition
 :
  A is an ordered list in which insertions are done at one end () and deletions are done at other end (). 
 The first element to be inserted is the first one to be deleted. Hence, it is called First in First out (FIFO) or Last 
 in Last out (LILO) list. 
  
 Similar to , special names are given to the two changes that can be made to a queue. When an element is 
 inserted in a queue, the concept is called , and when an element is removed from the queue, the concept is 
 called . 
  
  
  an empty queue is called  and  an element in a full queue is called . Generally, we treat them as 
 exceptions. As an example, consider the snapshot of the queue. 
  
 Elements ready 
  
 front 
  
  
 rear 
  
 New elements ready 
  
 to be served 
  
 to enter Queue 
  
 (DeQueue) 
  
 (EnQueue)",NA
5.2 How are Queues Used? ,"The concept of a queue can be explained by observing a line at a reservation counter. When we enter the line we 
 stand at the end of the line and the person who is at the front of the line is the one who will be served next. He 
 will exit the queue and be served. 
  
 As this happens, the next person will come at the head of the line, will exit the queue and will be served. As 
 each person at the head of the line keeps exiting the queue, we move towards the head of the line. Finally we 
 will reach the head of the line and we will exit the queue and be served. This behavior is very useful in cases 
 where there is a need to maintain the order of arrival.",NA
5.3 Queue ADT ,"The following operations make a queue an ADT. Insertions and deletions in the queue must follow the FIFO 
 scheme. For simplicity we assume the elements are integers. 
  
 Main Queue Operations 
  
 •
  
 •
  
 func EnQueue(data interface{}): Inserts an element (data) at the end of the queue (at rear) 
  
 func DeQueue() (data interface{}): Removes and returns the element (data) from the front of the 
 queue 
  
 Auxiliary Queue Operations 
  
 •
  
 •
  
 •
  
 •
  
 func Front() (data interface{}): Returns the element at the front without removing it 
 func Rear() (data interface{}): Returns the element at the rear without removing it 
 func Size() int: Returns the number of elements stored in the queue 
  
 func IsEmpty() bool: Indicates whether no elements are stored in the queue or not",NA
5.4 Exceptions ,NA,NA
5.5 Applications ,"Following are some of the applications that use queues. 
  
 Direct Applications 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Operating systems schedule jobs (with equal priority) in the order of arrival (e.g., a print queue). 
 Simulation of real-world queues such as lines at a ticket counter or any other first-come first-served 
 scenario requires a queue. 
  
 Multiprogramming. 
  
 Asynchronous data transfer (file IO, pipes, sockets). 
  
 Waiting times of customers at call center. 
  
 Determining number of cashiers to have at a supermarket. 
  
 Indirect Applications 
  
 •
  
 •
  
 Auxiliary data structure for algorithms 
 Component of other data structures",NA
5.6 Implementation ,"There are many ways (similar to Stacks) of implementing queue operations and some of the commonly used 
 methods are listed below. 
  
 •
  
 •
  
 •
  
 Simple circular array-based implementation 
 Dynamic circular array-based implementation 
 Linked list implementation 
  
 Why Circular Arrays? 
  
 First, let us see whether we can use simple arrays for implementing queues as we have done for stacks. We 
 know that, in queues, the insertions are performed at one end and deletions are performed at the other end. 
 After performing some insertions and deletions the process becomes easy to understand. 
  
 In the example shown below, it can be seen clearly that the initial slots of the array are getting wasted. So, 
 simple array implementation for queue is not efficient. To solve this problem we assume the arrays as circular 
 arrays. That means, we treat the last element and the first array elements as contiguous. With this 
 representation, if there are any free slots at the beginning, the rear pointer can easily go to its next free slot. 
  
 New elements ready 
  
 to enter Queue 
  
 front 
 (EnQueue) 
  
 rear 
  
 Note: 
 The simple circular array and dynamic circular array implementations are very similar to stack array 
 implementations. Refer to  chapter for analysis of these implementations. 
  
 Simple Circular Array Implementation 
  
  rear 
  
  front 
  
  
 Fixed size array 
  
 This simple implementation of Queue ADT uses an array. In the array, we add elements circularly and use two 
 variables to keep track of the start element and end element. Generally,  is used to indicate the start element 
 and is used to indicate the end element in the queue. The array storing the queue elements may become full. An  
 operation will then throw a . Similarly, if we try deleting an element from an empty queue it will throw . 
  
 5.5 Applications 
  
 140",NA
Linked List Implementation,"Another way of implementing queues is by using Linked lists.  operation is implemented by inserting an element 
 at the end of the list.  operation is implemented by deleting an element from the beginning of the list. 
  
 5.6 Implementation 
  
  4 
  
  15 
  
  7 
  
  40 
  
 nil 
  
 145 
  
 front 
  
 rear",NA
5.7 ,NA,NA
Queues: Problems & Solutions,"Problem-1
 Give an algorithm for reversing a queue . To access the queue, we are only allowed to use the methods 
 of queue ADT. 
  
 Solution: 
 To solve this question will take the help of an auxiliary stack. The steps involved will be:- 
  
 •
  
 •
  
 •
  
 •
  
 Create an auxiliary stack S. 
  
 Until the queue Q is not empty, deQueue all the elements of the queue Q and push on to the stack 
 S. Now we have a stack in which the last element of the queue Q is at the top in the stack S. 
  
 Until the stack is empty pop(S), pop all  the elements of the stack S and enQueue to the queue Q. 
  
  
 void reverseQueue(struct Queue *Q) { 
  
  struct Stack *S = createStack(5); 
  
  while (!isEmptyQueue(Q)) 
  
   
  push(S, deQueue(Q)); 
  
  while (!isEmptyStack(S)) 
  
   
  enQueue(Q, pop(S)); 
  
 } 
  
 Time Complexity:O
 ().
  
 Problem-2
 How can you implement a queue using two stacks?
  
 Solution
 : Let S1 and S2 be the two stacks to be used in the implementation of queue. All we have to do is to 
 define the enQueue and deQueue operations for the queue. 
  
 EnQueue Algorithm 
  
 •
  
 Just push on to stack S1
  
 Time Complexity: O
 (1)
 .
  
 DeQueue Algorithm 
  
 •
  
 •
  
 •
  
 If stack S2 is not empty then pop from S2 and return that element.
  
 If stack is empty, then transfer all elements from S1 to S2 and pop the top element from S2 and return 
 that popped element [we can optimize the code a little by transferring only 
  − 1
  elements from S1 to S2 
 and pop the  element from S1 and return that popped element].
  
 If stack S1 is also empty then throw error.
  
  
 package main 
  
 import ( 
  
  
 ""fmt"" 
  
 ) 
  
  
 type Queue struct { 
  
  
 stack1 []int 
  
  
 stack2 []int 
  
 } 
  
  
 func NewQueue() Queue { 
  
  
 return Queue{} 
  
 } 
  
  
 // Push element data to the back of queue. 
  
 func (q *Queue) EnQueue(data int) { 
  
  
 q.stack1 = append(q.stack1, data) 
  
 } 
  
  
 // Removes the element from in front of queue and returns that element. 
  
 func (q *Queue) DeQueue() int { 
  
  
 if len(q.stack2) == 0 { 
  
  
 for len(q.stack1) > 0 { 
  
  
  
  
 item := q.stack1[len(q.stack1)-1] 
  
  
  
  
 q.stack1 = q.stack1[:len(q.stack1)-1] 
  
  
  
  
 q.stack2 = append(q.stack2, item) 
  
 } 
  
 } 
  
  
  
 item := q.stack2[len(q.stack2)-1] 
  
 q.stack2 = q.stack2[:len(q.stack2)-1] 
  
  
 5.7 Queues: Problems & Solutions 
  
 148",NA
Trees ,NA,NA
6.1 ,NA,NA
What is a Tree?,NA,NA
Chapter,NA,NA
6 ,"A  is a data structure similar to a linked list but instead of each node pointing simply to the next node in a 
  
 linear fashion, each node points to a number of nodes. Tree is an example of a non-linear data structure. A 
  
 structure is a way of representing the hierarchical nature of a structure in a graphical form. 
  
 In trees ADT (Abstract Data Type), the order of the elements is not important. If we need ordering information, 
  
 linear data structures like linked lists, stacks, queues, etc. can be used.",NA
6.2 Glossary ,"•
  
 root 
  
 A
  
 B 
  
 C
  
 D
  
 E
  
 F 
  
 G 
  
 H
  
 I
  
 J 
  
 K 
  
 The of a tree is the node with no parents. There can be at most one root node in a tree (node  in the 
  
 above example). 
  
 •
  
 An  refers to the link from parent to child (all links in the figure). 
  
 •
  
 A node with no children is called node (
 , , , 
  and ). 
  
 •
  
 Children of same parent are called  (
 , , 
  are siblings of 
 ,
  and 
 , 
  are the siblings of ). 
  
 •
  
 A node  is an  of node  if there exists a path from  to  and  appears on the path. The node 
  
 is called a  of . For example, 
 , 
  and  are the ancestors of . 
  
 •
  
 The set of all nodes at a given depth is called the  of the tree (
 ,  
 and  are the same level). The root node 
  
 is at level zero. 
  
 root 1 Level-0 
  
 2 
  
 3 Level-1 
  
 6 7 Level-2 
  
 •
  
 The 
 ℎ
  of a node is the length of the path from the root to the node (depth of  is 
 2, −  − 
 ). 
  
 •
  
 The 
 ℎℎ
  of a node is the length of the path from that node to the deepest node. The height of a tree is the 
  
 length of the path from the root to the deepest node in the tree. A (rooted) tree with only one node (the root) 
  
 has a height of zero. In the previous example, the height of  is 2 (
  −  − 
 ). 
  
 •
  
 ℎ  ℎ 
  is the maximum height among all the nodes in the tree and 
 ℎ  ℎ 
  is the maximum 
  
 depth among all the nodes in the tree. For a given tree,  depth and height returns the same value. But for 
  
 individual nodes we may get different results. 
  
 6.1 What is a Tree? 
  
 157",NA
6.3 Binary Trees ,"A tree is called  if each node has zero child, one child or two children. Empty tree is also a valid binary tree. We 
 can visualize a binary tree as consisting of a root and two disjoint binary trees, called the left and right subtrees 
 of the root. 
  
 Generic Binary Tree 
  
 Left 
  
 root 
  
 Example 
  
 root 
  
 5 
  
 1
  
 6 
  
 3 
  
 7 
  
 Right 
  
 2
  
 Subtree 
  
 4 
  
 Subtree",NA
6.4 ,NA,NA
Types of Binary Trees,"Strict Binary Tree:
  A binary tree is called  if each node has exactly two children or no children. 
  
 root 
  
 2
  
 1
  
 4
  
 3
  
 5 
  
 Full Binary Tree:
  A binary tree is called  if each node has exactly two children and all leaf nodes are at the 
 same level. 
  
 root 
  
 5 
  
 1
  
 6 
  
 3
  
 7 
  
 2
  
 4 
  
 Complete Binary Tree:
  Before defining the 
   ,
  let us assume that the height of the binary tree is 
 ℎ
 . In complete 
 binary trees, if we give numbering for the nodes by starting at the root (let us say the root node has 
 1
 ) then we 
 get a complete sequence from 
 1
  to the number of nodes in the tree. While traversing we should give numbering 
 for nil pointers also. A binary tree is called  if all leaf nodes are at height 
 ℎ
  or 
 ℎ − 1 
 and also without any missing 
 number in the sequence. 
  
 6.3 Binary Trees 
  
 root 
  
 5
  
 1 
  
 6 
  
 3 
  
 158 
  
 2
  
 4",NA
6.5 ,NA,NA
Properties of Binary Trees,"For the following properties, let us assume that the height of the tree is 
 ℎ
 . Also, assume that root node is at 
 height zero. 
  
 Height Number of nodes at level 
  
 root 
  
 1 
  
  ℎ = 0 
  
  
  
  2= 1
  
 root 
  
 1
  
  
  
 ℎ = 1
  
  2= 2
  
 2 
  
 3 
  
 root 
  
 1 
  
 2 3 
 ℎ = 2 
  
  2= 4
  
 4 
  
 5 
  
 6 
  
 From the diagram we can infer the following properties: 
  
 •
  
 •
  
 •
  
 •
  
 The number of nodes  in a full binary tree is
  2− 1
 . Since, there are 
 ℎ 
 levels we need to add all nodes at 
 each level 
 [2+  2+ 2+ ⋯+  2=  2− 1]
 . 
  
 The number of nodes in a complete binary tree is between 
 2
  (minimum) and 
 2− 1
  (maximum). For more 
 information on this, refer to chapter. 
  
 The number of leaf nodes in a full binary tree is
   2
 . 
  
 The number of None links (wasted pointers) in a complete binary tree of  nodes is
   + 1
 . 
  
 Structure of Binary Trees 
  
 Now let us define structure of the binary tree. For simplicity, assume that the data of the nodes are integers. 
 One way to represent a node (which contains data) is to have two links which point to left and right children 
 along with data fields as shown below: 
  
 data 
  
 Or 
  
 data 
  
  
 // A BinaryTreeNode is a binary tree with integer values. 
 type BinaryTreeNode struct { 
  
  
 left  *BinaryTreeNode 
  
  
 data int 
  
  
 right *BinaryTreeNode 
  
 } 
  
 Note: 
 In trees, the default flow is from parent to children and it is not mandatory to show directed branches. 
 For our discussion, we assume both the representations shown below are the same. 
  
 data 
  
 data 
  
 Operations on Binary Trees 
  
 Basic Operations 
  
 •
  
 •
  
 •
  
 •
  
 Inserting an element into a tree 
  
 Deleting an element from a tree 
  
 Searching for an element 
  
 Traversing the tree 
  
 Auxiliary Operations 
  
 •
  
 Finding the size of the tree
  
  
 •
  
 Finding the height of the tree
  
 •
  
 Finding the level which has maximum sum
  
 •
  
 Finding the least common ancestor (LCA) for a given pair of nodes, and many more.
  
 6.5 Properties of Binary Trees 
  
 159",NA
6.6 Binary Tree Traversals ,"In order to process trees, we need a mechanism for traversing them, and that forms the subject of this section. 
 The process of visiting all nodes of a tree is called 
  .
   Each node is processed only once but it may be visited 
 more than once. As we have already seen in linear data structures (like linked lists, stacks, queues, etc.), the 
 elements are visited in sequential order. But, in tree structures there are many different ways. 
  
 Tree traversal is like searching the tree, except that in traversal the goal is to move through the tree in a 
 particular order.  In addition, all nodes are processed in the 
   ℎ
  stops when the required node is found. 
  
 Traversal Possibilities 
  
 Starting at the root of a binary tree, there are three main steps that can be performed and the order in which 
 they are performed defines the traversal type. These steps are: performing an action on the current node 
 (referred to as ""visiting"" the node and denoted with “”), traversing to the left child node (denoted with “”), and 
 traversing to the right child node (denoted with “”). This process can be easily described through recursion. 
 Based on the above definition there are 
 6
  possibilities: 
  
 1.: Process left subtree, process the current node data and then process right subtree 
  
 2.: Process left subtree, process right subtree and then process the current node data 
  
 3.: Process the current node data, process left subtree and then process right subtree 
  
 4.: Process the current node data, process right subtree and then process left subtree 
  
 5.: Process right subtree, process the current node data and then process left subtree 
  
 6.: Process right subtree, process left subtree and then process the current node data 
  
 Classifying the Traversals 
  
 The sequence in which these entities (nodes) are processed defines a particular traversal method. The 
 classification is based on the order in which current node is processed. That means, if we are classifying based 
 on current node () and if  comes in the middle then it does not matter whether  is on left side of  or  is on left 
 side of . 
  
 Similarly, it does not matter whether  is on right side of  or  is on right side of . Due to this, the total 6 
 possibilities are reduced to 3 and these are: 
  
 •
  
 •
  
 •
  
 PreOrder () Traversal 
  
 InOrder () Traversal 
  
 PostOrder () Traversal 
  
 There is another traversal method which does not depend on the above orders and it is: 
  
 •
  
 Level Order Traversal: This method is inspired from Breadth First Traversal (BFS of Graph algorithms). 
  
 Let us use the diagram below for the remaining discussion. 
  
 root 
  
 5 
  
 1
  
 6 
  
 3
  
 7
  
 2
  
 4 
  
 PreOrder Traversal 
  
 In preorder traversal, each node is processed before (pre) either of its subtrees. This is the simplest traversal to 
 understand.  However, even though each node is processed before the subtrees, it still requires that some 
 information must be maintained while moving down the tree.  In the example above, 
 1
  is processed first, then 
 the left subtree, and this is followed by the right subtree. 
  
 Therefore, processing must return to the right subtree after finishing the processing of the left subtree. To move 
 to the right subtree after processing the left subtree, we must maintain the root information. The obvious ADT 
 for",NA
6.7 Generic Trees (N-ary Trees) ,"In the previous section we discussed binary trees where each node can have a maximum of two children and 
 these are represented easily with two pointers. But suppose if we have a tree with many children at every node 
 and also if we do not know how many children a node can have, how do we represent them? 
  
 For example, consider the tree shown below. 
  
 A 
  
 B 
  
 C 
  
 H 
  
 D 
  
 I 
  
 P 
  
 E 
  
 K 
  
 F 
  
 M 
  
 G 
  
 L 
  
 N 
  
 J 
  
 Q 
  
 How do we represent the tree? 
  
 In the above tree, there are nodes with 
 6
  children, with 
 3
  children, 
 with 2
  children, with 
 1
  child, and with zero 
 children (leaves). To present this tree we have to consider the worst case (
 6
  children) and allocate that many 
 child pointers for each node. Based on this, the node representation can be given as: 
  
 struct TreeNode{ 
  
  int data; 
  
  struct TreeNode *firstChild; 
  
  
 6.7 Generic Trees (N-ary Trees) 
  
 187",NA
Representation of Generic Trees,"Since our objective is to reach all nodes of the tree, a possible solution to this is as follows: 
  
 •
  
 •
  
 At each node link children of same parent (siblings) from left to right. 
 Remove the links from parent to all children except the first child. 
  
 A 
  
 B 
  
 C 
  
 D 
  
 E 
  
 K 
  
 F 
  
 L 
  
 G 
  
 N 
  
 H 
  
 I 
  
 J 
  
 M 
  
 P 
  
 Q 
  
 What these above statements say is if we have a link between children then we do not need extra links from 
 parent to all children. This is because we can traverse all the elements by starting at the first child of the 
 parent. So if we have a link between parent and first child and also links between all children of same parent 
 then it solves our problem. 
  
 This representation is sometimes called first child/next sibling representation. First child/next sibling 
 representation of the generic tree is shown above. The actual representation for this tree is: 
  
 A 
  
 Element 
  
  First Child 
  
 B 
  
 None 
  
 Next 
  
 …….. 
  
 A 
  
 Sibling 
  
 A 
  
 None 
  
 None 
  
 A 
  
 None 
  
 None 
  
 Based on this discussion, the tree node declaration for general tree can be given as:
  
 type TreeNode struct { 
  
  
  data int 
  
  
  firstChild *TreeNode 
  
  
  nextSibling *TreeNode 
  
 } 
  
 Note:
  Since we are able to convert any generic tree to binary representation; in practice we use binary trees. We 
 can treat all generic trees with a first child/next sibling representation as binary trees. 
  
 Generic Trees: Problems & Solutions 
  
 Problem-37 
 Can you implement a simple N-ary tree? 
  
 Solution
 : Trie is one of the most frequently used N-ary trees. Refer 
  ℎ
 s chapter for more details. 
  
 package main 
  
 import ( 
  
  
 ""fmt"" 
  
  
 6.7 Generic Trees (N-ary Trees) 
  
 188",NA
6.8 Threaded Binary Tree Traversals (Stack or Queue-less Traversals) ,"In earlier sections we have seen that, 
 ,   
  binary tree traversals used stacks and  traversals used queues as an 
 auxiliary data structure. In this section we will discuss new traversal algorithms which do not need both stacks 
 and queues. Such traversal algorithms are called 
 ℎ   
  or 
 / −   
 . 
  
 Issues with Regular Binary Tree Traversals 
  
 1
  
 19 
  
 10 
  
 32 
  
 20 
  
 13 
  
  2 
  
 11 
  
 •
  
 The storage space required for the stack and queue is large. 
  
 •
  
 The majority of pointers in any binary tree are nil. For example, a binary tree with  nodes has 
  +  1
  nil 
  
 pointers and these were wasted. 
  
 •
  
 It is difficult to find successor node (preorder, inorder and postorder successors) for a given node. 
  
 Motivation for Threaded Binary Trees 
  
 To solve these problems, one idea is to store some useful information in nil pointers. If we observe the previous 
 traversals carefully, stack/queue is required because we have to record the current position in order to move to 
 the right subtree after processing the left subtree. If we store the useful information in nil pointers, then we 
 don’t have to store such information in stack/queue. 
  
 The binary trees which store such information in nil pointers are called 
 ℎ  .
  From the above discussion, let us 
 assume that we want to store some useful information in nil pointers. The next question is what to store? 
  
 The common convention is to put predecessor/successor information. That means, if we are dealing with 
 preorder traversals, then for a given node, nil left pointer will contain preorder predecessor information and nil 
 right pointer will contain preorder successor information. These special pointers are called 
 ℎ
 . 
  
 Classifying Threaded Binary Trees 
  
 The classification is based on whether we are storing useful information in both nil pointers or only in one of 
 them. 
  
 •
  
 If we store predecessor information in nil left pointers only, then we can call such binary trees 
  
  ℎ  .
  
 •
  
 If we store successor information in nil right pointers only, then we can call such binary trees 
  
 ℎ ℎ  .
  
 •
  
 If we store predecessor information in nil left pointers and successor information in nil right pointers, 
  
 then we can call such binary trees 
  ℎ  
  or simply 
 ℎ  .
  
 Note: 
 For the remaining discussion we consider only 
 ()ℎ  
 . 
  
 Types of Threaded Binary Trees 
  
 Based on above discussion we get three representations for threaded binary trees. 
  
 •
  
  ℎ  :
  nil left pointer will contain PreOrder predecessor information and nil right 
  
 pointer will contain PreOrder successor information. 
  
 •
  
  ℎ  :
  nil left pointer will contain InOrder predecessor information and nil right 
  
 pointer will contain InOrder successor information. 
  
 •
  
  ℎ  :
  nil left pointer will contain PostOrder predecessor information and nil 
  
 right pointer will contain PostOrder successor information. 
  
 Note: 
 As the representations are similar, for the remaining discussion we will use InOrder threaded binary 
 trees. 
  
 6.8 Threaded Binary Tree Traversals (Stack or Queue-less Traversals) 
  
 194",NA
6.9 Expression Trees ,"A tree representing an expression is called an . In expression trees, leaf nodes are operands and non-leaf nodes 
 are operators. That means, an expression tree is a binary tree where internal nodes are operators 
  
 6.9 Expression Trees 
  
 198",NA
6.10 XOR Trees ,"This concept is similar to  of  chapter. Also, like threaded binary trees this representation does not need stacks 
 or queues for traversing the trees. This representation is used for traversing back (to parent) and forth (to 
 children) using 
 ⊕ 
 operation. To represent the same in XOR trees,  for each node below are the rules used for 
 representation: 
  
 •
  
 Each nodes left will have the 
 ⊕ 
 of its parent and its left children. 
  
 •
  
 Each nodes right will have the 
 ⊕ 
 of its parent and its right children. 
  
 •
  
 The root nodes parent is nil and also leaf nodes children are nil nodes. 
  
  A 
  
  D 
  
  B 
  
  E 
  
  F 
  
  C 
  
  G 
  
 Based on the above rules and discussion, the tree can be represented as: 
  
 B
 ⊕nil
  
 A
 ⊕D
  
 nil
 ⊕B
  
 A 
  
 nil
 ⊕C
  
 B
 ⊕nil
  
 C 
  
 A
 ⊕nil
  
 A
 ⊕E
  
 A
 ⊕F
  
 D 
  
 B
 ⊕nil
  
 B
 ⊕G
  
 E 
  
 F 
  
 C
 ⊕nil
  
 C
 ⊕nil
  
 E
 ⊕nil
  
 G 
  
 E
 ⊕nil
  
 The major objective of this presentation is the ability to move to parent as well to children. Now, let us see how 
 to use this representation for traversing the tree. For example, if we are at node B and want to move to its 
 parent node A, then we just need to perform 
 ⊕ 
 on its left content with its left child address (we can use right 
 child also for going to parent node). Similarly, if we want to move to its child (say, left child D) then we have to 
 perform 
 ⊕ 
 on its left content with its parent node address. One important point that we need to understand 
 about this representation is: When we are at node B, how do we know the address of its children D? Since the 
 traversal starts at node root node, we can apply 
 ⊕ 
 on root’s left content with nil. As a result we get its left child, 
 B. When we are at B, we can apply 
 ⊕ 
 on its left content with A address.",NA
6.11 Binary Search Trees (BSTs) ,"Why Binary Search Trees? 
  
 In previous sections we have discussed different tree representations and in all of them we did not impose any 
 restriction on the nodes data. As a result, to search for an element we need to check both in left subtree and in 
 right subtree. Due to this, the worst-case complexity of search operation is O
 ().
  In this section, we will discuss 
 another variant of binary trees: Binary Search Trees (BSTs). As the name suggests, the main use of this 
  
 6.10 XOR Trees 
  
 200",NA
6.12 Balanced Binary Search Trees ,"In earlier sections we have seen different trees whose worst-case complexity is O
 (),
  where  is the number of 
 nodes in the tree. This happens when the trees are skew trees. In this section we will try to reduce this worst-
 case complexity to O
 ()
  by imposing restrictions on the heights. 
  
 In general, the height balanced trees are represented with 
 (),
  where  is the difference between left subtree height 
 and right subtree height. Sometimes is called balance factor.",NA
Full Balanced Binary Search Trees ,"In 
 (),
  if 
  =  0
  (if balance factor is zero), then we call such binary search trees as  balanced binary search trees. 
 That means, in 
 (0)
  binary search tree, the difference between left subtree height and right subtree height should 
 be at most zero. This ensures that the tree is a full binary tree. For example, 
  
 4 
  
 1 
  
 2 
  
 3
  
 5 
  
 6 
  
 7 
  
 Note:
  For constructing 
 (0) 
 tree refer to  section.",NA
6.13 AVL (Adelson-Velskii and Landis) Trees ,"In 
 (), 
 if 
  =  1
  (if balance factor is one), such a binary search tree is called an . That means an AVL tree is a binary 
 search tree with a  condition: the difference between left subtree height and right subtree height is at most 
 1.
  
 6.12 Balanced Binary Search Trees 
  
 215",NA
Properties of AVL Trees ,"A binary tree is said to be an AVL tree, if: 
  
 •
  
 •
  
 It is a binary search tree, and 
  
 For any node , the height of left subtree of  and height of right subtree of  differ by at most 
 1
 . 
  
 8 
  
 8
  
 6 
  
 12
  
 6 
  
 10
  
 3
  
 10
  
 3
  
 9 
  
 12 
  
 9 
  
 As an example, among the above binary search trees, the left one is not an AVL tree, whereas the right binary 
 search tree is an AVL tree. 
  
 Minimum/Maximum Number of Nodes in AVL Tree 
  
 For simplicity let us assume that the height of an AVL tree is 
 ℎ
  and 
 (ℎ)
  indicates the number of nodes in AVL 
 tree with height 
 ℎ.
  To get the minimum number of nodes with height 
 ℎ,
  we should fill the tree with the minimum 
 number of nodes possible. That means if we fill the left subtree with height 
 ℎ − 1
  then we should fill the right 
 subtree with height 
 ℎ − 2
 . As a result, the minimum number of nodes with height 
 ℎ
  is: 
  
 (ℎ) = (ℎ − 1) + (ℎ − 2) + 1
  
 In the above equation: 
  
 •
  
 •
  
 •
  
 (ℎ − 1)
  indicates the minimum number of nodes with height 
 ℎ − 1. 
 (ℎ − 2)
  indicates the minimum number of nodes with height 
 ℎ − 2. 
 In the above expression, “1” indicates the current node. 
  
 We can give 
 (ℎ − 1)
  either for left subtree or right subtree. Solving the above recurrence gives: 
  
  (ℎ) =
  O
 (1.618) ⟹ ℎ = 1.44 ≈
  O
 ()
  
 Where  is the number of nodes in AVL tree. Also, the above derivation says that the maximum height in AVL 
 trees is O
 ().
  Similarly, to get maximum number of nodes, we need to fill both left and right subtrees with height 
 ℎ − 1. 
 As a result, we get: 
  
 (ℎ) = (ℎ − 1) + (ℎ − 1) + 1 = 2(ℎ − 1) + 1
  
 The above expression defines the case of full binary tree. Solving the recurrence we get: 
  
 (ℎ) =
  O
 (2) ⟹ ℎ =  ≈
 O
 ()
  
 ∴
  In both the cases, AVL tree property is ensuring that the height of an AVL tree with  nodes is O
 ().
  
 root 
  
 ℎ 
  
 ℎ − 1 
  
 (ℎ − 1) 
  
 (ℎ − 2) 
  
 ℎ − 2 
  
 AVL Tree Declaration 
  
 Since AVL tree is a BST, the declaration of AVL is similar to that of BST. But just to simplify the operations, we 
 also include the height as part of the declaration. 
  
 type AVLTreeNode struct { 
  
  
 data   int 
  
 left   *AVLTreeNode 
  
  
 right  *AVLTreeNode 
  
 height int 
  
  
 } 
  
  
 6.13 AVL (Adelson-Velskii and Landis) Trees 
  
 216",NA
6.14 Other Variations on Trees ,"In this section, let us enumerate the other possible representations of trees. In the earlier sections, we have 
 looked at AVL trees, which is a binary search tree (BST) with balancing property. Now, let us look at a few more 
 balanced binary search trees: Red-black Trees and Splay Trees. 
  
 6.14.1 Red-Black Trees 
  
 In Red-black trees each node is associated with an extra attribute: the color, which is either red or black. To get 
 logarithmic complexity we impose the following restrictions. 
  
 Definition: 
 A Red-black tree is a binary search tree that satisfies the following properties: 
  
 •
  
 •
  
 •
  
 •
  
 Root Property: the root is black 
  
 External Property: every leaf is black 
  
 Internal Property: the children of a red node are black 
 Depth Property: all the leaves have the same black 
  
 Similar to AVL trees, if the Red-black tree becomes imbalanced, then we perform rotations to reinforce the 
 balancing property. With Red-black trees, we can perform the following operations in O
 ()
  in worst case, where is 
 the number of nodes in the trees. 
  
 •
  
 •
  
 •
  
 Insertion, Deletion 
  
 Finding predecessor, successor 
  
 Finding minimum, maximum 
  
 6.14.2 Splay Trees 
  
 Splay-trees are BSTs with a self-adjusting property. Another interesting property of splay-trees is: starting with 
 an empty tree, any sequence of  operations with maximum of  nodes takes O
 ()
  time complexity in worst case. 
 Splay trees are easier to program and also ensure faster access to recently accessed items. Similar to and Red-
 Black trees, at any point that the splay tree becomes imbalanced, we can perform rotations to reinforce the 
 balancing property. 
  
 6.14 Other Variations on Trees 
  
 230",NA
6.15 Supplementary Questions ,"Problem-92
   A binary tree is univalued if every node in the tree has the same value. Given an algorithm to check 
 whether the given binary tree is univalued or not. 
  
 9 
  
 9
  
 9
  
 5 
  
 9
  
 9 
  
 9
  
 9 
  
 9 
  
 6
  
  
  
 Solution
 : A tree is univalued if both its children are univalued, plus the root node has the same value as the 
 child nodes. We can write our function recursively.  will represent that the left subtree is correct: ie., that it is 
 univalued, and the root value is equal to the left child's value. 
 ℎ
  will represent the same thing for the right 
 subtree. We need both of these properties to be true.",NA
Priority Queues ,NA,NA
and Heaps ,NA,NA
Chapter,NA,NA
7 ,NA,NA
7.1 What is a Priority Queue? ,"In some situations we may need to find the minimum/maximum element among a collection of elements. We 
 can do this with the help of Priority Queue ADT. A priority queue ADT is a data structure that supports the 
 operations  and  (which returns and removes the minimum element) or  (which returns and removes the 
 maximum element). 
  
 These operations are equivalent to  and  operations of a queue. The difference is that, in priority queues, the 
 order in which the elements enter the queue may not be the same in which they were processed. An example 
 application of a priority queue is job scheduling, which is prioritized instead of serving in first come first serve. 
  
 Insert 
  
 Priority Queue 
  
 DeleteMax 
  
 A priority queue is called an 
  − 
  queue, if the item with the smallest key has the highest priority (that means, 
 delete the smallest element always). Similarly, a priority queue is said to be a 
  − 
 queue if the item with the 
 largest key has the highest priority (delete the maximum element always). Since these two types are symmetric, 
 we will be concentrating on one of them: ascending-priority queue.",NA
7.2 Priority Queue ADT ,"The following operations make priority queues an ADT. 
  
 Main Priority Queues Operations 
  
 A priority queue is a container of elements, each having an associated key. 
  
 •
  
 •
  
 •
  
 insert (key, data): Inserts data with  to the priority queue. Elements are ordered based on key. 
 deleteMin/deleteMax: Remove and return the element with the smallest/largest key. 
  
 GetMinimum/getMaximum: Return the element with the smallest/largest key without deleting it. 
  
 Auxiliary Priority Queues Operations 
  
 •
  
 •
  
 •
  
 − 
 Smallest/
 − 
 Largest: Returns the 
 −
 Smallest/
 −
 Largest key in priority queue. Size: 
 Returns number of elements in priority queue. 
  
 Heap Sort: Sorts the elements in the priority queue based on priority (key).",NA
7.3 Priority Queue Applications ,"Priority queues have many applications – a  few of them are listed below: 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Data compression: Huffman Coding algorithm 
  
 Shortest path algorithms: Dijkstra's algorithm 
  
 Minimum spanning tree algorithms: Prim's algorithm 
 Event-driven simulation: customers in a line 
  
 Selection problem: Finding - smallest element",NA
7.4 Priority Queue Implementations ,"Before discussing the actual implementation, let us enumerate the possible options. 
  
 7.1 What is a Priority Queue? 
  
 235",NA
7.5 Heaps and Binary Heaps ,"What is a Heap? 
  
 A heap is a tree with some special properties. The basic requirement of a heap is that the value of a node must 
 be 
 ≥
  (or 
 ≤)
  than the values of its children. This is called 
 ℎ
 . A heap also has the additional property that all 
 leaves should be at 
 ℎ
  or 
 ℎ −  1
  levels (where 
 ℎ
  is the height of the tree) for some 
 ℎ >  0
  (). That means heap should 
 form a (as shown below). 
  
 4
  
 2
  
 5 
  
 6 
  
 3 
  
 In the examples below, the left tree is a heap (each element is greater than its children) and the right tree is not 
 a heap (since 
 11
  is greater than 
 2
 ). 
  
 7.5 Heaps and Binary Heaps 
  
 236",NA
7.6 Binary Heaps ,"In binary heap each node may have up to two children. In practice, binary heaps are enough and we concentrate 
  
 on binary min heaps and binary max heaps for the remaining discussion. 
  
 Representing Heaps: 
 Before looking at heap operations, let us see how heaps can be represented. One 
 possibility 
  
 is using arrays. Since heaps are forming complete binary trees, there will not be any wastage of locations. For 
 the 
  
 discussion below let us assume that elements are stored in arrays, which starts at index 
 0
 . The previous max 
  
 heap can be represented as: 
  
 0 
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 Note: 
 For the remaining discussion let us assume that we are doing manipulations in max heap. 
  
 Declaration of Heap 
  
 // Item - defines the interface for an element to be held by a Heap instance 
 type Item interface { 
  
  
 Less(item Item) bool 
  
 } 
  
 // Heap - binary heap with support for min heap operations 
  
 type Heap struct { 
  
  
 size int 
  
  
 data []Item 
  
 } 
  
 Creating Heap 
  
 // New - returns a pointer to an empty min-heap 
  
 func New() *Heap { 
  
  
 return &Heap{} 
  
 } 
  
 Time Complexity: O
 (1).
  
 Parent of a Node 
  
 For a node at  location, its parent is at 
  
  location. In the previous example, the element 
 6
  is at second 
 location 
  
  
 func parent(i int) int { 
  
  
 return int(math.Floor(float64(i-1) / 2.0)) 
  
 }",NA
7.7 Heapsort ,"One main application of heap ADT is sorting (heap sort). The heap sort algorithm inserts all elements (from an 
 unsorted array) into a heap, then removes them from the root of a heap until the heap is empty. Note that heap 
 sort can be done in place with the array to be sorted. Instead of deleting an element, exchange the first element 
 (maximum) with the last element and reduce the heap size (array size). Then, we heapify the first element. 
 Continue this process until the number of remaining elements is one. 
  
 func HeapSort(data []Item) []Item { 
  
  
 hp := Heapify(data) 
  
  
 size := len(hp.data) 
  
  
 for i := size - 1; i > 0; i-- { 
  
  
 // Move current root to end 
  
  
 swap(hp, 0, i) 
  
  
 hp.size-- 
  
  
 hp.percolateDown(0)  // heapify the root as it might not satisfy the heap property 
  
 } 
  
  
 hp.size = size 
  
  
 return hp.data 
  
 } 
  
  
 func main() {  // Test code, since the heap is a min-heap, it gives the elements in the descending 
 order 
  
 items := randomPerm(30) 
  
  
 for i := 0; i < len(items); i++ { 
  
  
 fmt.Print(items[i].(Int), "" "") 
  
  
 } 
  
  
 items = HeapSort(items) 
  
  
 fmt.Print(""\n"") 
  
  
 for i := 0; i < len(items); i++ { 
  
  
 fmt.Print(items[i].(Int), "" "") 
  
  
 } 
  
 } 
  
 Time complexity: As we remove the elements from the heap, the values become sorted (since maximum elements 
 are always  only). Since the time complexity of both the insertion algorithm and deletion algorithm is O
 () 
 (where  
 is the number of items in the heap), the time complexity of the heap sort algorithm is O
 ().",NA
7.8 Priority Queues [Heaps]: Problems & Solutions ,"Problem-1
 What are the minimum and maximum number of elements in a heap of height 
 ℎ
 ? 
  
 Solution: 
 Since heap is a complete binary tree (all levels contain full nodes except possibly the lowest level), it 
 has at most 
 2− 1
  elements (if it is complete). This is because, to get maximum nodes, we need to fill all the 
 ℎ
  
 levels completely and the maximum number of nodes is nothing but the sum of all nodes at all 
 ℎ
  levels. 
  
 To get minimum nodes, we should fill the 
 ℎ − 1
  levels fully and the last level with only one element. As a result, 
 the minimum number of nodes is nothing but the sum of all nodes from 
 ℎ − 1
  levels plus 
 1 
 (for the last level) and 
 we get 
 2 − 1 + 1 = 2
  elements (if the lowest level has just 
 1
  element and all the other levels are complete). 
  
 Problem-2
 Is there a min-heap with seven distinct elements so that the preorder traversal of it gives the elements 
 in sorted order? 
  
 Solution
 : 
 Yes
 . For the tree below, preorder traversal produces ascending order. 
  
 3 
  
 2
  
 4 
  
 6 
  
 5 
  
 7 
  
 Problem-3
 Is there a max-heap with seven distinct elements so that the preorder traversal of it gives the elements 
 in sorted order? 
  
 Solution: Yes
 . For the tree below, preorder traversal produces descending order. 
  
 7.7 Heapsort 
  
 5 
  
 6
  
 4 
  
 2 
  
 3 
  
 1 
  
 243",NA
Disjoint Sets ,NA,NA
ADT ,NA,NA
Chapter,NA,NA
8 ,NA,NA
8.1 Introduction ,"In this chapter, we will represent an important mathematics concept: . This means how to represent a group of 
 elements which do not need any order. The disjoint sets ADT is the one used for this purpose. It is used for 
 solving the equivalence problem. It is very simple to implement. A simple array can be used for the 
 implementation and each function takes only a few lines of code. Disjoint sets ADT acts as an auxiliary data 
 structure for many other algorithms (for example, 
 ’
  algorithm in graph theory). Before starting our discussion on 
 disjoint sets ADT, let us look at some basic properties of sets.",NA
8.2 ,NA,NA
Equivalence Relations and Equivalence Classes ,"For the discussion below let us assume that  is a set containing the elements and a relation is defined on it
 . 
 That 
 means for every pair of elements in 
 , 
 ,  is either true or false. If is true, then we say is related to , otherwise is 
 not related to 
 .
  A relation is called an  if it satisfies the following properties: 
  
 •
  
 •
  
 •
  
 : For every element 
  
  ,   
  is true. 
  
 : For any two elements 
 ,  ∈ ,
  if  is true then  is true. 
  
 : For any three elements a, b, c 
 ∈
  S, if a R b and  are true then  is true. 
  
 As an example, relations 
 ≤
  (less than or equal to) and 
 ≥
  (greater than or equal to) on a set of integers are not 
 equivalence relations. They are reflexive (since 
  ≤  )
  and transitive (
  ≤  
  and 
  ≤  
  implies 
  ≤  ) 
 but not symmetric (
  ≤  
 does not imply 
  ≤  )
 . 
  
 Similarly,  is an equivalence relation. This relation is reflexive because any location is connected to itself. If 
 there is connectivity from city to city , then city also has connectivity to city , so the relation is symmetric. 
 Finally, if city is connected to city and city is connected to city , then city is also connected to city . 
  
 The  of an element 
  ∈ 
  is a subset of that contains all the elements that are related to 
 . 
 Equivalence classes 
 create a  of 
 .
  Every member of appears in exactly one equivalence class. To decide if , we just need to check 
 whether and are in the same equivalence class (group) or not. In the above example, two cities will be in same 
 equivalence class if they have rail connectivity. If they do not have connectivity then they will be part of different 
 equivalence classes. 
  
 Since the intersection of any two equivalence classes is empty (), the equivalence classes are sometimes called . 
 Disjoint-set data structure also called a union–find data structure or merge–find set. It maintains a 
  
 collection 
  = {,,. . . , }
  of disjoint dynamic sets. We identify each set by a representative, which is some member of 
 the set. 
  
  −  
  are data structures where each set is represented by a tree data structure, in which each node holds a 
 reference to its parent node. In a disjoint-set forest, the representative of each set is the root of that set's tree. 
 Find follows parent nodes until it reaches the root. Union combines two trees into one by attaching the root of 
 one to the root of the other. 
  
 In the subsequent sections, we will try to see the operations that can be performed on equivalence classes. The 
 possible operations are: 
  
 •
  
 •
  
 •
  
 Creating an equivalence class (making a set) 
  
 Finding the equivalence class name (set name or representative name) (Find) 
 Combining the equivalence classes (Union)",NA
8.3 Disjoint Sets ADT ,"To manipulate the set elements we need basic operations defined on sets. In this chapter, we concentrate on the 
 following set operations: 
  
 8.1 Introduction 
  
 255",NA
8.4 Applications ,"Disjoint sets ADT have many applications and a few of them are: 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 To represent network connectivity 
  
 Image processing 
  
 To find least common ancestor 
  
 To define equivalence of finite state automata 
  
 Kruskal's minimum spanning tree algorithm (graph theory) 
 In game algorithms",NA
8.5 Tradeoffs in Implementing Disjoint Sets ADT ,"Let us see the possibilities for implementing disjoint set operations. Initially, assume the input elements area 
 collection of  sets, each with one element. That means, initial representation assumes all relations (except 
 reflexive relations) are false. Each set has a different element, so that 
 ∩
 = ф. This makes the sets . 
  
 To add the relation  (UNION), we first need to check whether  and are already related or not. This can be verified 
 by performing FINDs on both  and and checking whether they are in the same equivalence class (set) or not. 
  
 If they are not, then we apply UNION. This operation merges the two equivalence classes containing and  into 
 anew equivalence class by creating a new set  = 
 ∪ 
  and deletes  and. Basically there are two ways to implement 
 the above FIND/UNION operations: 
  
 •
  
 •
  
 Fast FIND implementation (also called Quick FIND) 
  
 Fast UNION operation implementation (also called Quick UNION) 
  
 8.6 Fast FIND Implementation (Quick FIND) 
  
 In this method, we use an array. As an example, in the representation below the array contains the set name for 
 each element. For simplicity, let us assume that all the elements are numbered sequentially from 
 0
  to 
  − 1
 . 
 In 
 the example below, element
  0
  has the set name 
 3,
  element 
 1
  has the set name 
 5,
  and so on. With this 
 representation FIND takes only O
 (1)
  since for any element we can find the set name by accessing its array 
 location in constant time. 
  
 Set Name 
  
  3                 5                             ……….                    2                          3 
  
 0 
  
 1 
  
  ………. 
  
 -2 
  
 -1 
  
 In this representation, to perform UNION(
 , 
 ) [assuming that is in set nd is in set ] we need to scan the complete 
 array and change all 
 ’
  to . This takes O
 ().
  
 A sequence of 
  −  1
  unions take O
 ()
  time in the worst case. If there are O
 ()
  FIND operations, this performance is 
 fine, as the average time complexity is O
 (1)
  for each UNION or FIND operation. If there are fewer FINDs, this 
 complexity is not acceptable. 
  
 8.7 Fast UNION Implementation (Quick UNION) 
  
 In this and subsequent sections, we will discuss the faster implementations and its variants. There are different 
 ways of implementing this approach and the following is a list of a few of them. 
  
 •
  
 Fast UNION implementations (Slow FIND)
  
 •
  
 Fast UNION implementations (Quick FIND)
  
 •
  
 Fast UNION implementations with path compression",NA
8.8 Fast UNION Implementation (Slow FIND),"As we have discussed, FIND operation returns the same answer (set name) if and only if they are in the same 
 set. In representing disjoint sets, our main objective is to give a different set name for each group. In general we 
 do not care about the name of the set. One possibility for implementing the set is  as each element has only one  
 and we can use it as the set name. 
  
 8.4 Applications 
  
 256",NA
8.9 Fast UNION Implementations (Quick FIND) ,"The main problem with the previous approach is that, in the worst case we are getting the skew trees and as a 
 result the FIND operation is taking O
 ()
  time complexity. There are two ways to improve it: 
  
 •
  
 •
  
 UNION by Size (also called UNION by Weight): Make the smaller tree a subtree of the larger tree UNION 
 by Height (also called UNION by Rank): Make the tree with less height a subtree of the tree with more 
 height 
  
 UNION by Size 
  
 In the earlier representation, for each element  we have stored  (in the parent array) for the root element and for 
 other elements we have stored the parent of . But, in this approach, we add extra property to indicate the size of 
 the tree. 
  
 type Element struct { 
  
 parent *Element 
  
 size   int 
  
  
 Data   interface{} } 
  
 // Parent element 
  
 // Size of the subtree with this element as root 
 // Arbitrary user-provided payload 
  
 MAKESET 
  
 // MakeSet creates a singleton set and returns its sole element. 
 func MakeSet(Data interface{}) *Element { 
  
  
 s := &Element{} 
  
  
 s.parent = s 
  
 s.size = 1 
  
 }
  
 s.Data = Data 
  
 return s 
  
  
 FIND 
  
 // No change in the find operation 
  
 UNION by Size 
  
 For union operation, the element with higher size would become the root for the new set after the union. 
  
 func Union(e1, e2 *Element) { 
  
  
 // Ensure the two Elements aren't already part of the same union. 
  
  
 8.9 Fast UNION Implementations (Quick FIND) 
  
 259",NA
8.10 ,NA,NA
Summary,"Performing  union-find operations on a set of  objects. 
  
 Algorithm 
  
 Worst-case time 
  
 Quick-Find 
  
  
 Quick-Union 
  
  
 Quick-Union by Size/Height 
  
  + 
  
 Path compression 
  
  + 
  
 Quick-Union by Size/Height + Path Compression 
  
 ( +  )",NA
8.11 ,NA,NA
Disjoint Sets: Problems & Solutions,"Problem-1
 Consider a list of cities, 
 ,
 …,. Assume that we have a relation  such that, for any 
 , , (, )
  is 1 if cities  and  
 are in the same state, and 0 otherwise. If  is stored as a table, how much space does it require? 
  
 8.10 Summary 
  
 261",NA
Graph ,NA,NA
Algorithms ,NA,NA
9.1 Introduction,NA,NA
Chapter,NA,NA
9 ,"In the real world, many problems are represented in terms of objects and connections between them. For 
 example, in an airline route map, we might be interested in questions like: “What’s the fastest way to go from 
 Hyderabad to New York?”  “What is the cheapest way to go from Hyderabad to New York?” To answer these 
 questions we need information about connections (airline routes) between objects (towns). Graphs are data 
 structures used for solving these kinds of problems. 
  
 As part of this chapter, you will learn several ways to traverse graphs and how you can do useful things while 
 traversing the graph in some order. We will also talk about shortest paths algorithms. We will finish with 
 minimum spanning trees, which are used to plan road, telephone and computer networks and also find 
 applications in clustering and approximate algorithms.",NA
9.2 Glossary ,"Graph:
 A graph G is simply a way of encoding pairwise relationships among a set of objects: it consists of a 
 collection V of nodes and a collection E of edges, each of which “joins” two of the nodes. We thus represent an 
 edge e in E as a two-element subset of V: e = {u, v} for some u, v in V, where we call u and v the ends of e. 
  
 Edges in a graph indicate a symmetric relationship between their ends. Often we want to encode asymmetric 
 relationships, and for this, we use the closely related notion of a directed graph. A directed graph G’ consists of 
 a set of nodes V and a set of directed edges E’. Each e’ in E’ is an ordered pair (u, v); in other words, the roles of 
 u and v are not interchangeable, and we call u the tail of the edge and v the head. We will also say that edge e’ 
 leaves node u and enters node v. 
  
 When we want to emphasize that the graph we are considering is , we will call it an 
  ℎ
 ; by default, however, the 
 term “graph” will mean an undirected graph. It is also worth mentioning two warnings in our use of graph 
 terminology. First, although an edge e in an undirected graph should properly be written as a set of nodes {u, 
 u}, one will more often see it written in the notation used for ordered pairs: e = (u, v). Second, a node in a graph 
 is also frequently called a vertex; in this context, the two words have exactly the same meaning. 
  
 •
  
  and  are positions and store elements 
  
  
 •
  
 Definitions that we use: 
  
 o
  
  : 
  
 ▪
  
 Ordered pair of vertices 
 (, )
  
 ▪
  
 First vertex  is the origin 
  
 ▪
  
 Second vertex  is the destination 
  
 ▪
  
 Example: one-way road traffic 
  
 o
  
  : 
  
 ▪
  
 Unordered pair of vertices (
 , )
  
 ▪
  
 Example: railway lines 
  
 o
  
  ℎ: 
  
 ▪
  
 All the edges are directed 
  
 ▪
  
 Example: route network
  
 A 
  
 B 
  
 D 
  
 C",NA
9.3 Applications of Graphs,"||(||  )
  
  (in undirected graph). This is because each node can connect to 
  
 •
  
 Representing relationships between components in electronic circuits
  
 •
  
 Transportation networks: Highway network, Flight network 
  
 •
  
 Computer networks: Local area network, Internet, Web 
  
 •
  
 Databases: For representing ER (Entity Relationship) diagrams in databases, for representing 
 dependency 
  
 of tables in databases 
  
 9.3 Applications of Graphs 
  
 265",NA
9.4 Graph Representation,"As in other ADTs, to manipulate graphs we need to represent them in some useful form. There are several ways 
 to represent graphs, each with its advantages and disadvantages. Some situations, or algorithms that we want 
 to run with graphs as input, call for one representation, and others call for a different representation. Here, we'll 
 see three ways to represent graphs.
  
 •
  
 •
  
 •
  
 Adjacency Matrix 
  
 Adjacency List 
  
 Adjacency Set
  
 Adjacency Matrix 
  
 Graph Declaration for Adjacency Matrix 
  
 First, let us look at the components of the graph data structure. To represent graphs, we need the number of 
 vertices, the number of edges and also their interconnections. So, the graph can be declared as: 
  
 type AdjacencyMatrix struct { 
  
  
 Vertices  int 
  
  
 Edges     int 
  
  
 GraphType GraphType 
  
  
 AdjMatrix [][]int 
  
 } 
  
 Description 
  
 The adjacency matrix of a graph is a square matrix of size 
  × 
 . The  is the number of vertices of the graph G. The 
 values of matrix are boolean. Let us assume the matrix is 
 .
  The value 
 [, ] 
 is set to 
 1
  if there is an edge from vertex 
 u
 to vertex 
 v
 and 
 0
  otherwise. In the matrix, each edge is represented by two bits for undirected graphs. That 
 means, an edge from 
 u
  to 
 v
  is represented by 
 1
  value in both 
 [u, v]
  and 
 [, ]
 . To save time, we can process only half 
 of this symmetric matrix. Also, we can assume that there is an “edge” from each vertex to itself. So, 
 [u,u]
  is set to 
 1
 for all 
 vertices
 . 
  
 If the graph is a directed graph then we need to mark only one entry in the adjacency matrix. As an example, 
 consider the directed graph below. 
  
 A 
  
 B 
  
 D 
  
 C 
  
 The adjacency matrix for this graph can be given as: 
  
  
 A 
  
 B 
  
 C 
  
 D 
  
 A 
  
 0 
  
 1 
  
 0 
  
 1 
  
 B 
  
 0 
  
 0 
  
 1 
  
 0 
  
 C 
  
 1 
  
 0 
  
 0 
  
 1 
  
 D 
  
 0 
  
 0 
  
 0 
  
 0 
  
 Now, let us concentrate on the implementation. To read a graph, one way is to first read the vertex names and 
 then read pairs of vertex names (edges). The code below reads an undirected graph. 
  
 type GraphType string 
  
  
 const ( 
  
  
 DIRECTED   GraphType = ""DIRECTED"" 
  
  
 UNDIRECTED GraphType = ""UNDIRECTED"" 
 ) 
  
  
 type Graph interface { 
  
  
 Init() 
  
  
 AddEdge(vertexOne int, vertexTwo int) error 
  
  
 AddEdgeWithWeight(vertexOne int, vertexTwo int, weight int) error 
  
  
 RemoveEdge(vertexOne int, vertexTwo int) error 
  
  
 HasEdge(vertexOne int, vertexTwo int) bool 
  
  
 GetGraphType() GraphType 
  
  
 GetAdjacentNodesForVertex(vertex int) map[int]bool 
  
  
 9.4 Graph Representation 
  
 266",NA
9.5 Graph Traversals,"To solve problems on graphs, we need a mechanism for traversing the graphs. Graph traversal algorithms are 
 also called 
 ℎ ℎ
  algorithms. Like trees traversal algorithms (Inorder, Preorder, Postorder and Level-Order 
 traversals), graph search algorithms can be thought of as starting at some source vertex in a graph and 
 ""searching"" the graph by going through the edges and marking the vertices. Now, we will discuss two such 
 algorithms for traversing the graphs. 
  
 •
  
 Depth First Search [DFS]
  
 •
  
 Breadth First Search [BFS]
  
 A graph can contain cycles, which may bring you to the same node again while traversing the graph. To avoid 
 processing of same node again, use a boolean array which marks the node after it is processed. While visiting 
 the nodes in the layer of a graph, store them in a manner such that you can traverse the corresponding child 
 nodes in a similar order. 
  
 Depth First Search [DFS]
  
 Depth-first search (DFS) is a method for exploring a tree or graph. In a DFS, you go as deep as possible down 
 one path before backing up and trying a different one. DFS algorithm works in a manner similar to 
 preorder
  
 traversal of the trees. Like 
 preorder
  traversal, internally this algorithm also uses stack. Let us consider the 
 following example. Suppose a person is trapped inside a maze. To come out from that maze, the person visits 
 each path and each intersection (in the worst case). Let us say the person uses two colors of paint to mark the 
 intersections already passed. When discovering a new intersection, it is marked grey, and he continues to go 
 deeper. 
  
 After reaching a “dead end” the person knows that there is no more unexplored path from the grey intersection, 
 which now is completed, and he marks it with black. This “dead end” is either an intersection which has already 
 been marked grey or black, or simply a path that does not lead to an intersection. 
  
 The intersections of the maze are the vertices and the paths between the intersections are the edges of the 
 graph. The process of returning from the “dead end” is called . We are trying to go away from the starting vertex 
 into the graph as deep as possible, until we have to backtrack to the preceding grey vertex. In DFS algorithm, 
 we encounter the following types of edges. 
  
  :
  encounter new vertex
  
  : 
 from descendent to ancestor
  
 :
  from ancestor to descendent
  
  :
  between a tree or subtrees
  
 For most algorithms boolean classification, unvisited/visited is enough (for three color implementation refer to 
 problems section). That means, for some problems we need to use three colors, but for our discussion two 
 colors are enough. 
  
  false 
  
  
 Vertex is unvisited 
  
 true 
  
  
 Vertex is visited 
  
 Initially all vertices are marked unvisited (false). The DFS algorithm starts at a vertex  in the graph. By starting 
 at vertex  it considers the edges from  to other vertices. If the edge leads to an already visited vertex, then 
 backtrack to current vertex . If an edge leads to an unvisited vertex, then go to that vertex and start processing 
 from that vertex. That means the new vertex becomes the current vertex. Follow this process until we reach the 
 dead-end. At this point start . The process terminates when backtracking leads back to the start vertex. 
  
 As an example, consider the following graph. We can see that sometimes an edge leads to an already discovered 
 vertex. These edges are called 
  ,
  and the other edges are called  because deleting the back edges from the graph 
 generates a tree. 
  
 The final generated tree is called the 
 DFS tree
  and the order in which the vertices are processed is called  of the 
 vertices. In the graph below, the gray color indicates that the vertex is visited (there is no other 
  
 significance). We need to see when the 
 visited
  table is being updated. In the following example, DFS algorithm 
 traverses from A to B to C to D first, then to E, F, and G and lastly to H. It employs the following rules. 
  
 9.5 Graph Traversals 
  
 273",NA
9.6 ,NA,NA
Topological Sort ,"Assume that we need to schedule a series of tasks, such as classes or construction jobs, where we cannot start 
 one task until after its prerequisites are completed. We wish to organize the tasks into a linear order that allows 
 us to complete them one at a time without violating any prerequisites. We can model the problem using a DAG. 
 The graph is directed because one task is a prerequisite of another -- the vertices have a directed relationship. It 
 is acyclic because a cycle would indicate a conflicting series of prerequisites that could not be completed 
 without violating at least one prerequisite. The process of laying out the vertices of a DAG in a linear order to 
 meet the prerequisite rules is called a topological sort.",NA
9.7 Shortest Path Algorithms ,"Shortest path algorithms are a family of algorithms designed to solve the shortest path problem. The shortest 
 path problem is something most people have some intuitive familiarity with: given two points, A and B, what is 
 the shortest path between them? Given a graph 
  = (, )
  and a distinguished vertex s, we need to find the shortest 
 path from  to every other vertex in . There are variations in the shortest path algorithms which depends on the 
 type of the input graph and are given below.
  
 9.7 Shortest Path Algorithms 
  
 281",NA
Algorithms,"If the edges have weights, the graph is called a weighted graph. Sometimes these edges are bidirectional and the 
 graph is called undirected. Sometimes there can be even be cycles in the graph. Each of these subtle differences 
 are what makes one algorithm work better than another for certain graph type. 
  
 There are also different types of shortest path algorithms. Maybe you need to find the shortest path between 
 point A and B, but maybe you need to shortest path between point A and all other points in the graph. 
  
 Shortest path in unweighted graph 
  
 Shortest path in weighted graph 
  
 Shortest path in weighted graph with negative edges 
  
 Applications of Shortest Path Algorithms 
  
 Shortest path algorithms have many applications. As noted earlier, mapping software like Google or Apple maps 
 makes use of shortest path algorithms. They are also important for road network, operations, and logistics 
 research. Shortest path algorithms are also very important for computer networks, like the Internet. 
  
 Any software that helps you choose a route uses some form of a shortest path algorithm. Google Maps, for 
 instance, has you put in a starting point and an ending point and will solve the shortest path problem for you. 
  
 •
  
 Finding fastest way to go from one place to another 
  
 •
  
 Finding cheapest way to fly/send data from one city to another 
  
 Types of Shortest Path Algorithms 
  
 •
  
 •
  
 •
  
 •
  
  ℎ ℎ 
 :  In a Single Source Shortest Paths Problem, we are given a Graph G = (V, E), we want to find the 
 shortest path from a given source vertex s 
 ∈
  V to every vertex v 
 ∈
  V. 
  
  ℎ ℎ 
 : Find the shortest path to a given destination vertex t from every vertex v. By shift 
 the direction of each edge in the graph, we can shorten this problem to a single - source problem. 
  
  ℎ ℎ 
 : Find the shortest path from u to v for given vertices u and v. If we determine the single - 
 source problem with source vertex u, we clarify this problem also. Furthermore, no algorithms for this 
 problem are known that run asymptotically faster than the best single - source algorithms in the worst 
 case. 
  
  ℎ ℎ 
 : Find the shortest path from u to v for every pair of vertices u and v. Running a single - 
 source algorithm once from each vertex can clarify this problem; but it can generally be solved faster, 
 and its structure is of interest in the own right. 
  
 All these types have algorithms that perform best in their own way. All-pairs algorithms take longer to run 
 because of the added complexity. All shortest path algorithms return values that can be used to find the 
 shortest path, even if those return values vary in type or form from algorithm to algorithm. The most common 
 algorithm for the all-pairs problem is the floyd-warshall algorithm. 
  
 Shortest Path in Unweighted Graph 
  
 Let  be the input vertex from which we want to find the shortest path to all other vertices. Unweighted graph is 
 a special case of the weighted shortest-path problem, with all edges a weight of 
 1
 . The algorithm is similar to 
 BFS and we need to use the following data structures: 
  
 •
  
 A distance table with three columns (each row corresponds to a vertex): 
  
 o 
  
 Distance from source vertex. 
  
 o 
  
 Path - contains the name of the vertex through which we get the shortest distance. 
  
 •
  
 A queue is used to implement breadth-first search. It contains vertices whose distance from the source 
  
 node has been computed and their adjacent vertices are to be examined. 
  
 As an example, consider the following graph and its adjacency list representation. 
  
 A
  
 D 
  
 B
  
 E
  
 F
  
 G
  
 The adjacency list for this graph is: 
  
 :  → 
  
 9.7 Shortest Path Algorithms 
  
 282",NA
9.8 Minimal Spanning Tree,"The  of a graph is a subgraph that contains all the vertices and is also a tree. A graph may have many spanning 
 trees. As an example, consider a graph with 4 vertices as shown below. Let us assume that the corners of the 
 graph are vertices. 
  
 Vertices 
 Edges
  
 For this simple graph, we can have multiple spanning trees as shown below. 
  
  
  
  
  
 The cost of the spanning tree is the sum of the weights of all the edges in the tree. There can be many spanning 
 trees. Minimum spanning tree is the spanning tree where the cost is minimum among all the spanning trees. 
 There also can be many minimum spanning trees. 
  
 The algorithm we will discuss now is  in an undirected graph. We assume that the given graphs are weighted 
 graphs. If the graphs are unweighted graphs then we can still use the weighted graph algorithms by treating all 
 weights as equal. A of an undirected graph  is a tree formed from graph edges that connect all the vertices of  
 with minimum total cost (weights). A minimum spanning tree exists only if thegraph is connected. There are two 
 famous algorithms for this problem: 
  
 •
  
 •
  
 ′ 
 Algorithm 
  
 ′ 
 Algorithm 
  
 Prim's Algorithm 
  
 Prim’s algorithm also use  approach to find the minimum spanning tree. Prim's algorithm shares a similarity 
 with the shortest path first algorithms. In Prim’s algorithm we grow the spanning tree from a starting position. 
 Prim's algorithm is almost same as Dijkstra's algorithm. Like in Dijkstra's algorithm, in Prim's algorithm also we 
 keep values  and 
 ℎ
 in distance table. The only exception is that since the definition of  is different and as a result 
 the updating statement also changes little. The update statement is simpler than before. 
  
 In Prim’s algorithm, we will start with an arbitrary node (it doesn’t matter which one) and mark it. In each 
 iteration we will mark a new vertex that is adjacent to the one that we have already marked. As a greedy 
 algorithm, Prim’s algorithm will select the cheapest edge and mark the vertex. 
  
 Algorithm 
  
 •
  
 •
  
 •
  
 Maintain two disjoint sets of vertices. One containing vertices that are in the growing spanning tree and 
 other that are not in the growing spanning tree. 
  
 Select the cheapest vertex that is connected to the growing spanning tree and is not in the growing 
 spanning tree and add it into the growing spanning tree. This can be done using priority queues. Insert 
 the vertices, that are connected to growing spanning tree, into the priority queue. 
  
 Check for cycles. To do that, mark the nodes which have been already selected and insert only those 
 nodes in the priority queue that are not marked. 
  
  
 func Prims(g *Graph) (*Tree, error) { 
  
  
 9.8 Minimal Spanning Tree 
  
 290",NA
9.9 Graph Algorithms,NA,NA
: Problems & Solutions,"Problem-1
 In an undirected simple graph with  vertices, what is the maximum number of edges? Self-loops are 
  
 not allowed. 
  
 Solution:
  Since every node can connect to all other nodes, the first node can connect to 
  − 1
  nodes. The second 
 node can connect to 
  − 2
  nodes [since one edge is already there from the first node]. The total number of edges 
  
 is: 
 1 +  2 +  3 + ⋯+  − 1 =
  
 ()
  
  
  edges. 
  
 Problem-2
 How many different adjacency matrices does a graph with  vertices and  edges have? 
  
 Solution:
  It's equal to the number of permutations of  elements. i.e., 
 !
 . 
  
 Problem-3
 How many different adjacency lists does a graph with  vertices have? 
  
 Solution:
  It's equal to the number of permutations of edges. i.e., 
 !
 . 
  
 Problem-4
 Which undirected graph representation is most appropriate for determining whether or not a vertex is 
  
 isolated (is not connected to any other vertex)? 
  
 Solution:Adjacency List
 . If we use the adjacency matrix, then we need to check the complete row to determine 
 whether that vertex has edges or not. By using the adjacency list, it is very easy to check, and it can be done 
 just by checking whether that vertex has  for next pointer or not [nil indicates that the vertex is not connected 
 to any other vertex]. 
  
 Problem-5
 For checking whether there is a path from source  to target , which one is best between disjoint sets 
  
 and DFS? 
  
 Solution:
  The table below shows the comparison between disjoint sets and DFS. The entries in the table 
 represent the case for any pair of nodes (for  and ). 
  
 Method 
  
 Processing Time 
  
 Query Time 
  
 Space 
  
 Union-Find 
  
  + 
  
  
  
 DFS 
  
  + 
  
 1
  
  + 
  
  
 9.9 Graph Algorithms: Problems & Solutions 
  
 294",NA
Sorting ,NA,NA
10.1 What is Sorting? ,NA,NA
Chapter ,NA,NA
10 ,"is an algorithm that arranges the elements of a list in a certain order [either  or ]. The output is a permutation 
 or reordering of the input.",NA
10.2 Why is SortingNecessary? ,"Sorting is one of the important categories of algorithms in computer science and a lot of research has gone into 
 this category. Sorting can significantly reduce the complexity of a problem, and is often used for database 
 algorithms and searches.",NA
10.3 Classification of Sorting Algorithms ,"Sorting algorithms are generally categorized based on the following parameters. 
  
 By Number of Comparisons 
  
 In this method, sorting algorithms are classified based on the number of comparisons. For comparison based 
 sorting algorithms, best case behavior is O
 ()
  and worst case behavior is O
 (). 
  Comparison-based sorting 
 algorithms evaluate the elements of the list by key comparison operation and need at least O
 ()
  comparisons for 
 most inputs. 
  
 Later in this chapter we will discuss a few 
  − 
  () sorting algorithms like Counting sort, Bucket sort, Radix sort, etc. 
 Linear Sorting algorithms impose few restrictions on the inputs to improve the complexity. 
  
 By Number of Swaps 
  
 In
 this method, sorting algorithms are categorized by the numberof  (also called ). 
  
 By Memory Usage 
  
 Some sorting algorithms are """" and they need O
 (1)
  or O
 ()
  memory to create auxiliary locations for sorting the 
 data temporarily. 
  
 By Recursion 
  
 Sorting algorithms are either recursive [quick sort] or non-recursive [selection sort, and insertion sort], and 
 there are some algorithms which use both (merge sort). 
  
 By Stability 
  
 Sorting algorithm is  if for all indices  and  such that the key 
 []
  equals key 
 []
 , if record 
 []
  precedes record 
 []
  in the original file, 
 record 
 []
  precedes record 
 []
  in the sorted list. Few sorting algorithms 
 maintain the relative order of elements with equal 
 keys (equivalent elements retain their relative positions even after sorting).",NA
By Adaptability,"With a few sorting algorithms, the complexity changes based on pre-sortedness [quick sort]: pre-sortedness of 
 the input affects the running time. Algorithms that take this into account are known to be adaptive.",NA
10.4 Other Classifications ,"Another method of classifying sorting algorithms is: 
  
 •
  
 •
  
 Internal Sort 
  
 External Sort 
  
 Internal Sort 
  
 Sort algorithms that use main memory exclusively during the sort are called  sorting algorithms. This kind of 
 algorithm assumes high-speed random access to all memory. 
  
 External Sort 
  
 Sorting algorithms that use external memory, such as tape or disk, during the sort come under this category.",NA
10.5 Bubble Sort ,"Bubble sort is the simplest sorting algorithm. Bubble sort, sometimes referred to as sinking sort, is a simple 
 sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and 
 swaps them if they are in the wrong order. 
  
 In the bubble sorting, two adjacent elements of a list are first checked and then swapped. In case the adjacent 
 elements are in the incorrect order then the process keeps on repeating until a fully sorted list is obtained. Each 
 pass that goes through the list will place the next largest element value in its proper place. So, in effect, every 
 item bubbles up with an intent of reaching the location wherein it rightfully belongs. 
  
 The only significant advantage that bubble sort has over other implementations is that it can detect whether the 
 input list is already sorted or not. 
  
 Following table shows the first pass of a bubble sort. The shaded items are being compared to see if they are out 
 of order. If there are  items in the list, then there are 
 −
 1 pairs of items that need to be compared on the first 
 pass. It is important to note that once the largest value in the list is part of a pair, it will continually be moved 
 along until the pass is complete. 
  
 First pass 
  
 Remarks 
  
  
 10 
  
 4 
  
 43 
  
 5 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 Swap 
  
 4 
  
 10 
  
 43 
  
 5 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 No swap 
  
 4 
  
 10 
  
 43 
  
 5 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 Swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 No swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 No swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 Swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 45 
  
 91 
  
 9 
  
 7 
  
 Swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 45 
  
 9 
  
 91 
  
 7 
  
 Swap 
  
 4 
  
 10 
  
 5 
  
 43 
  
 57 
  
 45 
  
 9 
  
 7 
  
 91 
  
 At the end of first pass, 91 is in correct place 
  
 At the start of the first pass, the largest value is now in place. There are 
 −
 1 items left to sort, meaning that there 
 will be 
 −
 2 pairs. Since each pass places the next largest value in place, the total number of passes necessary 
 will be 
 −
 1. After completing the 
 −
 1 passes, the smallest item must be in the correct position with no further 
 processing required.",NA
Implementation ,"func BubbleSort(A []int) []int { 
  
  
 n := len(A) 
  
  
 for i := 0; i < n-1; i++ { 
  
  
 for j := 0; j < n-i-1; j++ { 
  
  
  
  
 if A[j] > A[j+1] { 
  
  
  
  
 A[j], A[j+1] = A[j+1], A[j] 
  
  
  
  
 } 
  
  
 } 
  
  
 } 
  
 } 
  
 return A 
  
  
  
 func main() { 
  
  
 A := []int{3, 4, 5, 2, 1} 
  
  
 10.4 Other Classifications 
  
 314",NA
Performance ,"Worst case complexity 
  
 O
 ()
  
 Best case complexity (Improved 
 version) 
  
 O
 ()
  
 Average case complexity (Basic 
 version) 
  
 O
 ()
  
 Worst case space complexity 
  
 O
 (1)
  auxiliary",NA
10.6 Selection Sort ,"In selection sort, the smallest element is exchanged with the first element of the unsorted list of elements (the 
 exchanged element takes the place where smallest element is initially placed). Then the second smallest element 
 is exchanged with the second element of the unsorted list of elements and so on until all the elements are 
 sorted. 
  
 The selection sort improves on the bubble sort by making only one exchange for every pass through the list. In 
 order to do this, a selection sort looks for the smallest (or largest) value as it makes a pass and, after completing 
 the pass, places it in the proper location. As with a bubble sort, after the first pass, the largest item is in the 
 correct place. After the second pass, the next largest is in place. This process continues and requires 
  − 1
  passes 
 to sort  items, since the final item must be in place after the 
 ( − 1)
 pass. 
  
 Selection sort is an in-place sorting algorithm. Selection sort works well for small files. It is used for sorting the 
 files with very large values and small keys. This is because selection is made based on keys and swaps are 
 made only when required. 
  
 Following table shows the entire sorting process. On each pass, the largest remaining item is selected and then 
 placed in its proper location. The first pass places 91, the second pass places 57, the third places 45, and so on. 
  
  
 Remarks: Select largest in each pass 
  
 10 
  
 4 
  
 43 
  
 5 
  
 57 
  
 91 
  
 45 
  
 9 
  
 7 
  
 Swap 91 and 7 
  
 10 
  
 4 
  
 43 
  
 5 
  
 57 
  
 7 
  
 45 
  
 9 
  
 91 
  
 Swap 57 and 9 
  
 10 
  
 4 
  
 43 
  
 5 
  
 9 
  
 7 
  
 45 
  
 57 
  
 91 
  
 45 is the next largest, skip 
  
 10 
  
 4 
  
 43 
  
 5 
  
 9 
  
 7 
  
 45 
  
 57 
  
 91 
  
 Swap 43 and 7 
  
 10 
  
 4 
  
 7 
  
 5 
  
 9 
  
 43 
  
 45 
  
 57 
  
 91 
  
 Swap 10 amd 9 
  
 9 
  
 4 
  
 7 
  
 5 
  
 10 
  
 43 
  
 45 
  
 57 
  
 91 
  
 Swap 9 amd 5 
  
  
 10.6 Selection Sort 
  
 315",NA
10.7 Insertion Sort ,"Insertion sort algorithm picks elements one by one and places it to the right position where it belongs in the 
 sorted list of elements.  Insertion sort is a simple and efficient comparison sort. In this algorithm, each iteration 
 removes an element from the input list and inserts it into the sorted sublist. The choice of the element being 
 removed from the input list is random and this process is repeated until all input elements have gone through. 
  
 It always maintains a sorted sublist in the lower positions of the list. Each new item is then “inserted” back into 
 the previous sublist such that the sorted sublist is one item larger. 
  
 10.7 Insertion Sort 
  
 316",NA
Analysis ,"The implementation of insertionSort shows that there are again 
  − 1
  passes to sort  items. The iteration starts at 
 position 1 and moves through position 
  − 1
 , as these are the items that need to be inserted back into the sorted 
 sublists. Notice that this is not a complete swap as was performed in the previous algorithms. 
  
 The maximum number of comparisons for an insertion sort is the sum of the first 
  − 1
  integers. Again, this is 
 O(). However, in the best case, only one comparison needs to be done on each pass. This would be the case for 
 an already sorted list. 
  
 One note about shifting versus exchanging is also important. In general, a shift operation requires 
 approximately a third of the processing work of an exchange since only one assignment is performed. In 
 benchmark studies, insertion sort will show very good performance. 
  
 Worst case analysis 
  
 Worst case occurs when for every  the inner loop has to move all elements 
 [1],
  . . . , 
 [ − 1]
  (which happens when 
 [] =
  key is smaller than all of them), that takes  
 
 ( − 1)
  time. 
  
  () = 
 
 (1) + 
 
 (2) + 
 Θ
 (2) + … …+  
 
 ( − 1) 
  
  = 
 
 (1 +  2 +  3 +  … . . +  − 1) =  
 
  
 ()
  
  ≈  
 
 (n) 
  
 Average case analysis 
  
 For the average case, the inner loop will insert 
 []
 in the middle of 
 [1],
  . . . , 
 [ − 1]
 . This takes 
 
  
  time. 
  
 () = 
  
 (/2)
  ≈  
 
 (
 2
 )",NA
Performance ,"=1
  
 If every element is greater than or equal to every element to its left, the running time of insertion sort is 
 
 (). This 
 situation occurs if the array starts out already sorted, and so an already-sorted array is the best case for 
 insertion sort. 
  
 Worst case complexity 
  
 O
 ()
  
 Best case complexity (Improved version) 
  
 O
 ()
  
 Average case complexity (Basic version) 
  
 O
 ()
  
 Worst case space complexity 
  
 O
 () 
 total, O
 (1)
  auxiliary",NA
Comparisons to Other Sorting Algorithms,"Insertion sort is one of the elementary sorting algorithms with O
 ()
  worst-case time. Insertion sort is used when 
 the data is nearly sorted (due to its adaptiveness) or when the input size is small (due to its low overhead). For 
 these reasons and due to its stability, insertion sort is used as the recursive base case (when the problem size is 
 small) for higher overhead divide-and-conquer sorting algorithms, such as merge sort or quick sort. 
  
 Notes: 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Bubble sort takes 
  
  comparisons and 
  
  swaps (inversions) in both average case and in worst case. 
  
 Selection sort takes 
  
  comparisons and  swaps. 
  
 Insertion sort takes 
  
  comparisons and 
  
  swaps in average case and in the worst case they are double. 
  
 Insertion sort is almost linear for partially sorted input. 
  
 Selection sort is best suits for elements with bigger values and small keys.",NA
10.8 Shell Sort ,"and uses various gaps between adjacent elements (ending with the gap of 1 or classical insertion sort). 
  
 algorithm which got less than quadratic complexity among comparison sort algorithms. 
  
 10.8 Shell Sort 
  
 318 
  
 generalization of insertion sort. Insertion sort works efficiently on input that is already almost sorted. Shell sort 
 is also known as -gap insertion sort. Instead of comparing only the adjacent pair, shell sort makes several 
 passes 
  
 In insertion sort, comparisons are made between the adjacent elements. At most 
 1
  inversion is eliminated for 
 each comparison done with insertion sort. The variation used in shell sort is to avoid comparing adjacent 
 elements until the last step of the algorithm. So, the last step of shell sort is effectively the insertion sort 
 algorithm. It improves insertion sort by allowing the comparison and exchange of elements that are far away. 
 This is the first 
 ℎ 
 (also called 
 ℎ  
 ) was invented by 
 ℎ
 . This sorting algorithm is a",NA
Implementation ,"func ShellSort(A []int) { 
  
  
 n := len(A) 
  
  
 h := 1 
  
  
 for h < n/3 { 
  
  
 h = 3*h + 1 
  
  
 } 
  
  
 for h >= 1 { 
  
  
 for i := h; i < n; i++ { 
  
  
  
  
 for j := i; j >= h && A[j] < A[j-h]; j -= h { 
  
  
  
  
  
 A[j], A[j-h] = A[j-h], A[j] 
  
  
  
  
 } 
  
  
 } 
  
  
 h /= 3 
  
  
 } 
  
 } 
  
 Note that when 
 ℎ ==  1, 
 the algorithm makes a pass over the entire list, comparing adjacent elements, but doing 
 very few element exchanges. For 
 ℎ ==  1,
  shell sort works just like insertion sort, except the number of 
 inversions that have to be eliminated is greatly reduced by the previous steps of the algorithm with 
 ℎ >  1
 . 
  
 Analysis 
  
 The worse-case time complexity of shell sort depends on the increment sequence. For the increments 1 4 13 40 
 121…, which is what is used here, the time complexity is O(
 /
 ). For other increments, time complexity is known 
 to be O(
 /
 ) and even O(
  · ()
 ). Neither tight upper bounds on time complexity nor the best increment sequence are 
 known. 
  
 Shell sort is efficient for medium size lists. For bigger lists, the algorithm is not the best choice. It is the fastest 
 of all O
 () 
 sorting algorithms. 
  
 The disadvantage of Shell sort is that it is a complex algorithm and not nearly as efficient as the merge, heap, 
 and quick sorts. Shell sort is significantly slower than the merge, heap, and quick sorts, but is a relatively 
 simple algorithm, which makes it a good choice for sorting lists of less than 
 5000
  items unless speed is 
 important. It is also a good choice for repetitive sorting of smaller lists. 
  
 The best case in Shell sort is when the array is already sorted in the right order. The number of comparisons is 
 less. The running time of Shell sort depends on the choice of increment sequence. 
  
 Because shell sort is based on insertion sort, shell sort inherits insertion sort’s adaptive properties. The 
 adapation is not as dramatic because shell sort requires one pass through the data for each increment, but it is 
 significant. For the increment sequence shown above, there are 
 ()
  increments, so the time complexity for nearly 
 sorted data is O(
  · ()
 ). Because of its low overhead, relatively simple implementation, adaptive properties, and 
 sub-quadratic time complexity, shell sort may be a viable alternative to the O(
  · ()
 ) sorting algorithms for some 
 applications when the data to be sorted is not very large. 
  
 10.8 Shell Sort 
  
 319",NA
10.9 Merge Sort ,"Merge sort is an example of the divide and conquer strategy. Merge sort first divides the array into equal halves 
 and then combines them in a sorted manner. It is a recursive algorithm that continually splits an array in half. 
 If the array is empty or has one element, it is sorted by definition (the base case). If the array has more than one 
 element, we split the array and recursively invoke a merge sort on both halves. Once the two halves are sorted, 
 the fundamental operation, called a merge, is performed. Merging is the process of taking two smaller sorted 
 arrays and combining them together into a single, sorted, new array. 
  
 Algorithm 
  
 Because we're using divide-and-conquer to sort, we need to decide what our subproblems are going to look like. 
 The full problem is to sort an entire array. Let's say that a subproblem is to sort a subarray. In particular, we'll 
 think of a subproblem as sorting the subarray starting at index  and going through index 
 ℎ
 . It will be convenient 
 to have a notation for a subarray, so let's say that 
 [. . ℎ]
  denotes this subarray of array . In terms of our notation, 
 for an array of  elements, we can say that the original problem is to sort A[0..n-1]. 
  
 Algorithm Merge-sort(A): 
  
 •
  
  by finding the number  of the position midway between  and 
 ℎ
 . Do this step the same way we found the 
 midpoint in binary search: 
  
  
  =  + 
  
 ()
  
 . 
  
  
  
 •
  
 •
  
  
  by recursively sorting the subarrays in each of the two subproblems created by the divide step. 
 That is, recursively sort the subarray 
 [. . ]
  and recursively sort the subarray 
 [ + 1. . ℎ]
 . 
  
  
  by 
 merging the two sorted subarrays back into the single sorted subarray 
 [.. ℎ]
 . 
  
 We need a base case. The base case is a subarray containing fewer than two elements, that is, when 
 ≥ℎ
 , since a 
 subarray with no elements or just one element is already sorted. So we'll divide-conquer-combine only when 
  <  
 ℎ
 . 
  
 Example 
  
 To understand merge sort, let us walk through an example: 
  
 54 
  
 26 
  
 93 
  
 17 
  
 77 
  
 31 
  
 44 
  
 55 
  
 We know that merge sort first divides the whole array iteratively into equal halves unless the atomic values are 
 achieved. We see here that an array of 8 items is divided into two arrays of size 4. 
  
  
 54 
  
 26 
  
 93 
  
 17 
  
  
  
 77 
  
 31 
  
 44 
  
 55 
  
  
 This does not change the sequence of appearance of items in the original. Now we divide these two arrays into 
 halves. 
   
 54 
  
 26 
  
  
  
 93 
  
 17 
  
  
  
 77 
  
 31 
  
  
  
 44 
  
 55 
  
  
 We further divide these arrays and we achieve atomic value which can no more be divided. 
  
  
 54 
  
  
  
 26 
  
  
  
 93 
  
  
  
 17 
  
  
  
 77 
  
  
  
 31 
  
  
  
 44 
  
  
  
 55 
  
  
 Now, we combine them in exactly the same manner as they were broken down. 
  
 We first compare the element for each array and then combine them into another array in a sorted manner. We 
 see that 54 and 26 and in the target array of 2 values we put 26 first, followed by 54. 
  
 Similarly, we compare 93 and 17 and in the target array of 2 values we put 17 first, followed by 93. On the 
 similar lines, we change the order of 77 and 31 whereas 44 and 55 are placed sequentially. 
  
  
 26 
  
 54 
  
  
  
 17 
  
 93 
  
  
  
 31 
  
 77 
  
  
  
 44 
  
 55 
  
  
 In the next iteration of the combining phase, we compare lists of two data values, and merge them into an array 
 of found data values placing all in a sorted order. 
  
  
 17 
  
 26 
  
 54 
  
 93 
  
  
  
 31 
  
 44 
  
 55 
  
 77 
  
  
 After the final merging, the array should look like this: 
  
 10.9 Merge Sort 
  
 320",NA
10.10 Heap Sort ,"Heapsort is a comparison-based sorting algorithm and is part of the selection sort family. Although somewhat 
 slower in practice on most machines than a good implementation of Quick sort, it has the advantage of a more 
 favorable worst-case Θ
 ()
  runtime. Heapsort is an in-place algorithm but is not a stable sort.",NA
Performance ,"Worst case performance 
  
 
 ()
  
 Best case performance 
  
 
 ()
  
 Average case performance 
  
 
 ()
  
 Worst case space complexity 
  
 
 ()
  total, 
 
 (1)
  auxiliary space 
  
 For other details on Heapsort refer to the  chapter.",NA
10.11 Quick Sort ,"Quick sort is the famous algorithm among comparison-based sorting algorithms. Like merge sort, quick sort 
 uses divide-and-conquer technique, and so it's a recursive algorithm. The way that quick sort uses divide-and-
 conquer is a little different from how merge sort does. The quick sort uses divide and conquer technique to gain 
 the same advantages as the merge sort, while not using additional storage. As a trade-off, however, it is possible 
 that the list may not be divided into half. When this happens, we will see that the performance is diminished. 
  
 It sorts in place and no additional storage is required as well. The slight disadvantage of quick sort is that its 
 worst-case performance is similar to the average performances of the bubble, insertion or selection sorts (i.e., 
 O
 ()
 ). 
  
 Divide and conquer strategy 
  
 A quick sort first selects an element from the given list, which is called the  value. Although there are many 
 different ways to choose the pivot value, we will simply use the  item in the list. The role of the pivot value is to 
 assist with splitting the list into two sublists. The actual position where the pivot value belongs in the final 
 sorted list, commonly called the  point, will be used to divide the list for subsequent calls to the quick sort. 
  
 All elements in the first sublist are arranged to be smaller than the , while all elements in the second sublist are 
 arranged to be larger than the . The same partitioning and arranging process is performed repeatedly on the 
 resulting sublists until the whole list of items are sorted. 
  
 Let us assume that array  is the list of elements to be sorted, and has the lower and upper bounds  and 
 ℎℎ
 respectively. With this information, we can define the divide and conquer strategy as follows: 
  
 : The list 
 [ … ℎℎ]
  is partitioned into two non-empty sublists 
 [ … ]
  and 
 [ + 1 …  ℎℎ]
 , such that each element of 
 [ … ]
  is less than or equal to each element of 
 [ + 1 … ℎℎ]
 . The index  is computed as part of partitioning procedure 
 with the first element as . 
  
 : The two sublists 
 [ …] 
 and 
 [ + 1 … ℎℎ]
  are sorted by recursive calls to quick sort. 
  
 10.10 Heap Sort 
  
 322",NA
10.12 Tree Sort ,"Tree sort uses a binary search tree. It involves scanning each element of the input and placing it into its proper 
 position in a binary search tree. This has two phases: 
  
 •
  
 •
  
 First phase is creating a binary search tree using the given array elements. 
  
 Second phase is traversing the given binary search tree in inorder, thus resulting in a sorted array. 
  
 Performance 
  
 The average number of comparisons for this method is O
 ()
 . But in worst case, the number of comparisons is 
 reduced by O
 (),
  a case which arises when the sort tree is skew tree.",NA
10.13 ,NA,NA
Comparison of Sorting Algorithms,"Name 
  
 Average Case 
  
 Worst 
 Case 
  
 Auxiliary Memory 
  
 Is Stable? 
  
 Other Notes 
  
 Bubble 
  
 O
 ()
  
 O
 ()
  
 1 
  
 yes 
  
 Small code 
  
 Selection 
  
 O
 ()
  
 O
 ()
  
 1 
  
 no 
  
 Stability depends on the implementation. 
  
  
 10.12 Tree Sort 
  
 326",NA
10.14 Linear Sorting Algorithms ,"In earlier sections, we have seen many examples of comparison-based sorting algorithms. Among them, the best 
 comparison-based sorting has the complexityO
 ()
 . In this section, we will discuss other types of algorithms: 
 Linear Sorting Algorithms. To improve the time complexity of sorting these algorithms, we make some 
 assumptions about the input. A few examples of Linear Sorting Algorithms are: 
  
 •
  
 •
  
 •
  
 Counting Sort 
  
 Bucket Sort 
  
 Radix Sort",NA
10.15 Counting Sort ,"Counting sort is not a comparison sort algorithm and gives O
 ()
  complexity for sorting. To achieve O
 ()
  
 complexity, 
  
  
  sort assumes that each of the elements is an integer in the range 1 to , for some integer 
 . When
   = 
 O
 ()
 , the  sort runs in O
 ()
  time. The basic idea of Counting sort is to determine, for each input element 
  
 , the number of elements less than . This information can be used to place it directly into its correct position. 
 For example, if 
 10
  elements are less than , then  belongs to position 
 11
  in the output. 
  
 In the code below, 
 [0 . .  − 1]
  is the input array with length . In Counting sort we need two more arrays: let us 
 assume array 
 [0 ..  − 1]
  contains the sorted output and the array 
 [0 . .  − 1]
  provides temporary storage. 
  
 func CountingSort(A []int, K int) []int { 
  
 bucketLen := K + 1 
  
 C := make([]int, bucketLen) 
  
  
 sortedIndex := 0 
  
 length := len(A) 
  
  
 } 
  
 for i := 0; i < length; i++ { 
  
 C[A[i]] += 1 
  
 } 
  
 for j := 0; j < bucketLen; j++ { 
  
 for C[j] > 0 { 
  
  
 A[sortedIndex] = j 
  
  
 sortedIndex += 1 
  
  
 C[j] -= 1 
  
 } 
  
 } 
  
 return A 
  
 Total Complexity: O() + O() + O() + O() = O() if 
   =
 O
 ().
  Space Complexity: O
 ()
   if 
   =
 O
 ().
  
 Note
 : Counting works well if
   =
 O
 ()
 . Otherwise, the complexity will be greater.",NA
10.16 Bucket Sort (or Bin Sort) ,"Like  sort,  sort also imposes restrictions on the input to improve the performance. In other words, Bucket sort 
 works well if the input is drawn from fixed set.  sort is the generalization of  Sort. For example, assume that all 
 the input elements from {
 0
 , 
 1
 , . . . , 
  − 1
 }, i.e., the set of integers in the interval [
 0
 , 
  − 1
 ]. That means,  is the 
 number of distant elements in the input.  sort uses  counters. The  counter keeps track of the number of 
 occurrences of the  element. Bucket sort with two buckets is effectively a version of Quick sort with two 
 buckets. 
  
 10.14 Linear Sorting Algorithms 
  
 327",NA
10.17 Radix Sort ,"Similar to  sort and  sort, this sorting algorithm also assumes some kind of information about the input 
 elements. Suppose that the input values to be sorted are from base . That means all numbers are -digit 
 numbers. 
  
 In Radix sort, first sort the elements based on the last digit [the least significant digit]. These results are again 
 sorted by second digit [the next to least significant digit]. Continue this process for all digits until we reach the 
 most significant digits. Use some stable sort to sort them by last digit. Then stable sort them by the second least 
 significant digit, then by the third, etc. If we use Counting sort as the stable sort, the total time is O
 () ≈
 O
 ()
 . 
  
 Algorithm
 : 
  
 1)
  
 Take the least significant digit of each element. 
  
  
 10.17 Radix Sort 
  
 328",NA
10.18 Topological Sort ,"Refer to 
 ℎ ℎ
  Chapter.",NA
10.19 External Sorting ,"External sorting is a generic term for a class of sorting algorithms that can handle massive amounts of data. 
 These external 
 sorting algorithms are useful when the files are too big and cannot fit into main memory. 
  
 As with internal sorting algorithms, there are a number of algorithms for external sorting. One such algorithm is 
 External Mergesort. In practice, these external sorting algorithms are being supplemented by internal sorts. 
  
 Simple External Mergesort 
  
 10.18 Topological Sort 
  
 329",NA
10.20 Sorting,NA,NA
: Problems & Solutions,"Problem-1
 Given an array 
 [0 …  − 1]
  of  numbers containing the repetition of some number. Give an algorithm for 
 checking whether there are repeated elements or not. Assume that we are not allowed to use additional 
 space (i.e., we can use a few temporary variables, O
 (1)
  storage). 
  
 Solution
 : Since we are not allowed to use extra space, one simple way is to scan the elements one-by-one and 
 for each element check whether that element appears in the remaining elements. If we find a match we return 
 true. 
  
 func CheckDuplicatesInArray(A []int) bool { 
  
  
 for i := 0; i < len(A); i++ { 
  
  
 // Scan slice for a previous element of the same value. 
  
  
 for v := 0; v < i; v++ { 
  
  
  
  
 if A[v] == A[i] { 
  
  
  
  
 return true 
  
  
  
  
 break 
  
  
  
  
 } 
  
  
 } 
  
  
 } 
  
  
 return false 
  
 }
  
 Each iteration of the inner, -indexed loop uses O
 (1)
  space, and for a fixed value of 
 ,
  theloop executes 
  − 
  times. 
 The outer loop executes 
  − 1
 times, so the entire function uses time proportional to 
  
  ∑
  
  − 
  
 = ( − 1) − ∑
  
 = ( − 1) −
  
 ()
  
 =
  
 ()
  
 =
 O
 ()",NA
Searching ,NA,NA
11.1 ,NA,NA
What is Searching?,NA,NA
Chapter ,NA,NA
11 ,"In computer science, 
 ℎ
  is the process of finding an item with specified properties from a collection of items. The 
 items may be stored as records in a database, simple data elements in arrays, text in files, nodes in trees, 
 vertices and edges in graphs, or they may be elements of other search spaces.",NA
11.2 ,NA,NA
Why do we need Searching?,"ℎ
  is one of the core computer science algorithms. We know that today’s computers store a lot of information. To 
 retrieve this information proficiently we need very efficient searching algorithms. There are certain ways of 
 organizing the data that improves the searching process. That means, if we keep the data in proper order, it is 
 easy to search the required element. Sorting is one of the techniques for making the elements ordered. In this 
 chapter we will see different searching algorithms.",NA
11.3 Types of Searching ,"Following are the types of searches which we will be discussing in this book. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Unordered Linear Search 
  
 Sorted/Ordered Linear Search 
  
 Binary Search 
  
 Interpolation search 
  
 Binary Search Trees (operates on trees and refer  chapter) Symbol 
 Tables and Hashing 
  
 String Searching Algorithms: Tries, Ternary Search and Suffix Trees",NA
11.4 Unordered Linear Search ,"Let us assume we are given an array where the order of the elements is not known. That means the elements of 
 the array are not sorted. In this case, to search for an element we have to scan the complete array and see if the 
 element is there in the given list or not. 
  
 package main 
  
 import ""fmt"" 
  
 func linearSearch(A []int, data int) int { 
  
  
  for i, item:= range A { 
  
   
  if(item == data) { 
  
     
  return i+1 
  
    
  } 
  
  
  } 
  
  
  return 0 
  
 } 
  
  
 func main() { 
  
  
  A := []int {67,68,16,8,5,86,29,21,50} 
  
  
  a := linearSearch(A, 5) 
  
  
  fmt.Printf(""The Source Array : %v\n"", A) 
  
  
  fmt.Printf(""The element %v is found at %v location"", 5, a) 
 } 
  
 Time complexity: O
 (),
  in the worst case we need to scan the complete array. Space complexity: O
 (1).",NA
11.5 Sorted/Ordered Linear Search ,"If the elements of the array are already sorted, then in many cases we don’t have to scan the complete array to 
 see if the element is there in the given array or not. In the algorithm below, it can be seen that, at any point if 
 the value at 
 []
  is greater than the  to be searched, then we just return 
 −1
  without searching the remaining 
 array. 
  
 package main 
  
 import ""fmt"" 
  
 func  orderedLinearSearch(A []int, data int) int { 
  
  for i, item:= range A { 
  
  
  if(item == data) { 
  
   
  return i+1 
  
  
  } else if (A[i] > data) { 
  
   
  return 0 
  
  
  } 
  
  } 
  
  return 0 
  
 } 
  
  
 func main() { 
  
  A := []int {67,68,16,8,5,86,29,21,50} 
  
  a := orderedLinearSearch(A, 5) 
  
  fmt.Printf(""The Source Array : %v\n"", A) 
  
  fmt.Printf(""The element %v is found at %v location"", 5, a) }
  
 Time complexity of this algorithm is O
 (). 
 This is because in the worst case we need to scan the complete array. 
 But in the average case it reduces the complexity even though the growth rate is the same. 
  
 Space complexity: O
 (1).
  
 Note: 
 For the above algorithm we can make further improvement by incrementing the index at a faster rate (say, 
 2
 ). This will reduce the number of comparisons for searching in the sorted list.",NA
11.6 Binary Search ,"Let us consider the problem of searching a word in a dictionary. Typically, we directly go to some approximate 
 page [say, middle page] and start searching from that point. If the  that we are searching is the same then the 
 search is complete. If the page is before the selected pages then apply the same process for the first half; 
 otherwise apply the same process to the second half. Binary search also works in the same way. The algorithm 
 applying such a strategy is referred to as 
  ℎ
 algorithm. 
  
  
 to be searched
  
 ℎℎ
  
  
  
  =  + 
  
 ()
  
  
 // Iterative Binary Search Algorithm 
  
 package main 
  
 import ""fmt"" 
  
  
 func binarySearch(data int, A []int) bool { 
  
  low, high := 0, len(A) - 1 
  
  for low <= high{ 
  
   
  mid := (low + high) / 2 
  
   
  if A[mid] < data { 
  
   
  
  low = mid + 1 
  
   
  }else{ 
  
   
  
  high = mid - 1 
  
   
  } 
  
  } 
  
  if low == len(A) || A[low] != data { 
  
   
  return false 
  
  } 
  
  return true 
  
 } 
  
  
 11.5 Sorted/Ordered Linear Search 
  
 342",NA
11.7 Interpolation Search ,"Undoubtedly binary search is a great algorithm for searching with average running time complexity of . It 
 always chooses the middle of the remaining search space, discarding one half or the other, again depending on 
 the comparison between the key value found at the estimated (middle) position and the key value sought. The 
 remaining search space is reduced to the part before or after the estimated position. 
  
 In themathematics,interpolationis a process of constructing new data points within the range of adiscrete setof 
 known data points. Incomputerscience, one often has a number of data points which represent the values of a 
 function for a limited number of values of the independent variable. It is often required tointerpolate(i.e. 
 estimate) the value of that function for an intermediate value of the independent variable.
  
 For example, suppose we have a table like this, which gives some values of an unknown function
 f
 . Interpolation 
 provides a means of estimating the function at intermediate points, such as
 x
  = 5.5. 
  
   
 ()
  
  
 1
  
  
 10 
  
  
 2
  
  
 20 
  
  
 3
  
  
 30 
  
  
 4
  
  
 40 
  
  
 5
  
  
 50 
  
  
 6
  
  
 60 
  
  
 7
  
  
 70 
  
 There are many different interpolation methods, and one of the simplest methods is linear interpolation. 
 Consider the above example of estimating 
 f
 (5.5)
 . Since 5.5 is midway between 5 and 6, it is reasonable to take 
 (5.5) midway 
  
 between (5) = 50 and (6) = 60, which yields 55 ((50+60)/2) 
 Linear interpolation takes two data points, say (,) and (
  
 . 
  
 ,), and the interpolant is given by: 
  
 can lead us closer to the searched item. 
 11.7 Interpolation Search 
  
 343",NA
11.8 Comparing Basic Searching Algorithms ,"Implementation 
  
 Search-Worst Case 
  
 Search-Average Case 
  
 Unordered Array 
  
  
 /2
  
 Ordered Array (Binary Search) 
  
  
  
 Unordered List 
  
  
 /2
  
 Ordered List 
  
  
 /2
  
 Binary Search Trees (for skew trees) 
   
  
 Interpolation search 
  
  
 ()
  
 Note: 
 For discussion on binary search trees refer to  chapter.",NA
11.9 Symbol Tables and Hashing ,"Refer to  and 
 ℎ
  chapters. 
  
 11.8 Comparing Basic Searching Algorithms 
  
 344",NA
11.10 String Searching Algorithms ,"Refer to 
  ℎ 
 chapter.",NA
11.11 ,NA,NA
Searching: Problems & Solutions ,"Problem-1
 Given an array of  numbers, give an algorithm for checking whether there are any duplicate elements 
  
 in the array or not? 
  
 Solution:
  This is one of the simplest problems. One obvious answer to this is exhaustively searching for 
 duplicates in the array. That means, for each input element check whether there is any element with the same 
 value. This we can solve just by using two simple  loops. The code for this solution can be given as:
  
 func checkDuplicatesInArray(A []int) bool { 
  
  
 for i := 0; i < len(A); i++ { 
  
 for v := 0; v < i; v++ { 
  
 // Scan slice for a previous element of the same value. 
  
 } 
  
  
 if A[v] == A[i] { 
  
  
  
  
 return true 
  
  
 } 
  
 } 
  
 } 
  
 return false 
  
  
 Time Complexity: O
 ()
 , for two nested  loops. Space Complexity: O
 (1). 
  
 Problem-2
 Can we improve the complexity of Problem-1’solution? 
  
 Solution:Yes.
  Sort the given array. After sorting, all the elements with equal values will be adjacent. Now, do 
 another scan on this sorted array and see if there are elements with the same value and adjacent. 
  
 func checkDuplicatesInArray(A []int) bool { 
  
  
 sort.Ints(A) 
  
  
 for i := 0; i < len(A)-1; i++ { 
  
  
 if A[i] == A[i+1] { 
  
  
  
  
 return true 
  
  
 } 
  
  
 } 
  
  
 return false 
  
 } 
  
 Time Complexity: O
 (),
  for sorting (assuming  sorting algorithm). Space Complexity: O
 (1).
  
 Problem-3
 Is there any alternative way of solving 
 Problem-1
 ? 
  
 Solution:Yes
 , using hash table. Hash tables are a simple and effective method used to implement dictionaries. 
  
  time to search for an element is O
 (1),
  while worst-case time is O
 ().
  Refer to 
 ℎ
  chapter for more details on 
 hashing algorithms. As an example, consider the array, 
  = {3,2, 1, 2, 2,3}
 . 
  
 Scan the input array and insert the elements into the hash. For each inserted element, keep the  as 
 1 
 (assume 
 initially all entires are filled with zeros). This indicates that the corresponding element has occurred already. For 
 the given array, the hash table will look like (after inserting the first three elements 
 3, 2
  and 
 1
 ): 
  
  
 1 
  
 2 
  
 3 
  
  
 →
  
 1 
  
 →
  
 1 
  
 →
  
 1 
  
 Now if we try inserting 
 2
 , since the counter value of 
 2
  is already 
 1
 , we can say the element has appeared twice. 
  
 func checkDuplicatesInArray(A []int) bool { 
  
  
 var HT = map[int]bool{} 
  
  
 for _, num := range A { 
  
  
 if _, ok := HT[num]; ok { 
  
  
  
  
 return true 
  
  
 } 
  
  
 HT[num] = true 
  
  
 } 
  
  
 return false 
  
 } 
  
 Time Complexity: O
 ().
  Space Complexity: O
 ().
  
 11.10 String Searching Algorithms 
  
 345",NA
Selection ,NA,NA
Algorithms ,NA,NA
[Medians] ,NA,NA
Chapter ,NA,NA
12 ,NA,NA
12.1 What are Selection Algorithms? ,"ℎ
  is an algorithm for finding the  smallest/largest number in a list (also called as  order statistic). This includes 
 finding the minimum, maximum, and median elements. For finding the   order statistic, there are multiple 
 solutions which provide different complexities, and in this chapter we will enumerate those possibilities.",NA
12.2 Selection by Sorting ,"A selection problem can be converted to a sorting problem. In this method, we first sort the input elements and 
 then get the desired element. It is efficient if we want to perform many selections. 
  
 For example, let us say we want to get the minimum element. After sorting the input elements we can simply 
 return the first element (assuming the array is sorted in ascending order). Now, if we want to find the second 
 smallest element, we can simply return the second element from the sorted list. 
  
 That means, for the second smallest element we are not performing the sorting again. The same is also the case 
 with subsequent queries. Even if we want to get  smallest element, just one scan of the sorted list is enough to 
 find the element (or we can return the -indexed value if the elements are in the array). 
  
 From the above discussion what we can say is, with the initial sorting we can answer any query in one scan, O
 (). 
 In general, this method requires O
 ()
  time (for ), where  is the length of the input list. Suppose we are 
  
 performing  queries, then the average cost per operation is just
  
 ≈
 O
 ()
 . This kind of analysis is called 
  
  analysis.",NA
12.3 Partition-based Selection Algorithm ,"For the algorithm check Problem-7. This algorithm is similar to Quick 
 sort.",NA
12.4 Linear Selection Algorithm - Median of Medians Algorithm ,"Worst-case performance 
  
 O
 ()
  
 Best-case performance 
  
 O
 ()
  
 Worst-case space 
 complexity 
  
 O
 (1)
  auxiliary
  
 Refer to Problem-14.",NA
12.5 Finding the K Smallest Elements in Sorted Order ,For the algorithm check Problem-7. This algorithm is similar to Quick sort.,NA
12.6 Selection Algorithms,NA,NA
: Problems & Solutions ,"Problem-1
 Find the largest element in an array A of size . 
  
 Solution:
   Scan the complete array and return the largest element. 
  
 func findMax(A []int) (max int) { 
  
  
 max = math.MinInt32 
  
  
 12.1 What are Selection Algorithms? 
  
 374",NA
Symbol Tables ,NA,NA
Chapter ,NA,NA
13 ,NA,NA
13.1 Introduction ,"Since childhood, we all have used a dictionary, and many of us have a word processor (say, Microsoft Word) 
 which comes with a spell checker. The spell checker is also a dictionary but limited in scope. There are many 
 real time examples for dictionaries and a few of them are: 
  
 •
  
 •
  
 •
  
 •
  
 Spell checker 
  
 The data dictionary found in database management applications 
 Symbol tables generated by loaders, assemblers, and compilers 
 Routing tables in networking components (DNS lookup) 
  
 In computer science, we generally use the term ‘symbol table’ rather than ‘dictionary’ when referring to the 
 abstract data type (ADT).",NA
13.2 What are Symbol Tables? ,"We can define the as a data structure that associates a  with a . It supports the following operations: 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Search whether a particular name is in the table 
 Get the attributes of that name 
  
 Modify the attributes of that name 
  
 Insert a new name and its attributes 
  
 delete a name and its attributes 
  
 There are only three basic operations on symbol tables: searching, inserting, and deleting. 
  
 Example
 :
  DNS lookup. Let us assume that the key in this case is the URL and the value is an IP address. 
  
 •
  
 •
  
 Insert URL with specified IP address 
  
 Given URL, find corresponding IP address 
  
  
 Key[Website] 
  
 Value [IP Address] 
  
 www.CareerMonks.com 
  
 128.112.136.11 
  
 www.AuthorsInn.com 
  
 128.112.128.15 
  
 www.AuthInn.com 
  
 130.132.143.21 
  
 www.klm.com 
  
 128.103.060.55 
  
 www.CareerMonk.com 
  
 209.052.165.60",NA
13.3 Symbol Table Implementations ,"Before implementing symbol tables, let us enumerate the possible implementations. Symbol tables can be 
 implemented in many ways and some of them are listed below. 
  
 Unordered Array Implementation 
  
 With this method, just maintaining an array is enough. It needs O
 ()
  time for searching, insertion and deletion in 
 the worst case. 
  
 Ordered [Sorted] Array Implementation 
  
 In this we maintain a sorted array of keys and values. 
  
 13.1 Introduction 
  
 384",NA
13.4 Comparison Table of Symbols for Implementations ,"Let us consider the following comparison table for all the implementations. 
  
 Implementation 
  
 Search 
  
 Insert 
  
 Delete 
  
 Unordered Array 
  
  
  
  
 Ordered Array (can be implemented with array binary search) 
   
  
  
 Unordered List 
  
  
  
  
 Ordered List 
  
  
  
  
 Binary Search Trees (O
 () 
 on average) 
  
  
  
  
 Balanced Binary Search Trees (O
 () 
 in worst case) 
  
  
  
  
 Ternary Search (only change is in logarithms base) 
  
  
  
  
 Hashing (O
 (1)
  on average) 
  
 1 
  
 1 
  
 1 
  
 Notes: 
  
 •
  
 In the above table, s the input size. 
  
 •
  
 Table indicates the possible implementations discussed in this book. But, there could be other 
  
 implementations.
  
 13.4 Comparison Table of Symbols for Implementations 
  
 385",NA
Hashing ,NA,NA
14.1 What is Hashing? ,NA,NA
Chapter ,NA,NA
14 ,"In this chapter we introduce so-called , that is, data structures that are similar to arrays but are not indexed by 
 integers, but other forms of data such as strings. One popular data structures for the implementation of 
 associative arrays are hash tables. To analyze the asymptotic efficiency of hash tables we have to explore a new 
 point of view, that of average case complexity. Hashing is a technique used for storing and retrieving 
 information as quickly as possible. It is used to perform optimal searches and is useful in implementing symbol 
 tables.",NA
14.2 Why Hashing? ,"In the  chapter we saw that balanced binary search trees support operations such as ,  and 
 ℎ 
 in O
 ()
  time. In 
 applications, if we need these operations inO
 (1)
 , then hashing provides a way. Remember that worst case 
 complexity of hashing is still O
 (),
  but it gives O
 (1)
  on the average.",NA
14.3 ,NA,NA
Hash Table ADT ,"The hash table structure is an unordered collection of associations between a key and a data value. The keys in 
 a hash table are all unique so that there is a one-to-one relationship between a key and a value. The operations 
 are given below. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 HashTable: Creates a new hash table 
  
 Get: Searches the hash table with key and return the value if it finds the element with the given key 
 Put: Inserts a new key-value pair into hash table 
  
 Delete: Deletes a key-value pair from hash table 
  
 DeleteHashTable: Deletes the hash table",NA
14.4 Understanding Hashing ,"In simple terms we can treat  as a hash table. For understanding the use of hash tables, let us consider the 
 following example: Give an algorithm for printing the first repeated character if there are duplicated elements in 
 it. Let us think about the possible solutions. 
  
 The simple and brute force way of solving is: given a string, for each character check whether that character is 
 repeated or not. The time complexity of this approach is O
 ()
  with O
 (1) 
 space complexity.
  
 Now, let us find a better solution for this problem. Since our objective is to find the first repeated character, 
 what if we remember the previous characters in some array?
  
 We know that the number of possible characters is 
 256
  (for simplicity assume  characters only). Create an array 
 of size 
 256
  and initialize it with all zeros. For each of the input characters go to the corresponding position and 
 increment its count. Since we are using arrays, it takes constant time for reaching any location. While scanning 
 the input, if we get a character whose counter is already 
 1
  then we can say that the character is the one which 
 is repeating for the first time. 
  
 func firstRepeatedChar(str string) byte { 
  
 n := len(str) 
  
 counters := [256]int{}",NA
14.5 Components of Hashing ,"Hashing has four key components: 
  
 1)
  
 Hash Table 
  
  
 2)
  
 Hash Functions 
  
 3)
  
 Collisions 
  
 4)
  
 Collision Resolution Techniques",NA
14.6 Hash Table ,"Hash table is a generalization of array. With an array, we store the element whose key is  at a position  of the 
 array. That means, given a key , we find the element whose key is  by just looking in the  position of the array. 
 This is called . 
  
 Direct addressing is applicable when we can afford to allocate an array with one position for every possible key. 
 But if we do not have enough space to allocate a location for each possible key, then we need a mechanism to 
 handle this case. Another way of defining the scenario is: if we have less locations and more possible keys, then 
 simple array implementation is not enough. 
  
 In these cases one option is to use hash tables. Hash table or hash map is a data structure that stores the keys 
 and their associated values, and hash table uses a hash function to map keys to their associated values. The 
 general convention is that we use a hash table when the number of keys actually stored is small relative to the 
 number of possible keys. 
  
 A hash table is a collection of items which are stored in such a way as to make it easy to find them later. Each 
 position of the hash table, often called a  (or a ), can hold an item and is named by an integer value starting at 
 0. 
  
 For example, we will have a slot named 0, a slot named 1, a slot named 2, and so on. Initially, the hash table 
 contains no items so every slot is empty. We can implement a hash table by using a list with each element 
 initialized to the special NULL. 
  
 600 
  
 5 
  
 Universe of possible 
  
 Slots 
  
 or 
  
 0 
  
 Buckets 
  
 keys 
  
 10 
  
 1 
  
 6 
  
 11 
  
 2 
  
 5
  
 20
  
 3
  
 3 
  
 0 
  
 3 
  
 1 
  
 4 
  
 5 
  
 6 
  
 Used keys",NA
14.7 Hash Function ,"The first idea behind hash tables is to exploit the efficiency of arrays. So: to map a key to an entry, we first map 
 a key to an integer and then use the integer to index an array A. The first map is called a 
 ℎℎ 
 . The hash function 
 is used to transform the key into the slot index (or bucket index). Ideally, the hash function should map each 
 possible key to a unique slot index, but it is difficult to achieve in practice. 
  
 Given a collection of elements, a hash function that maps each item into a unique slot is referred to as a 
 ℎℎ
 . If 
 we know the elements and the collection will never change, then it is possible to construct a perfect hash 
 function. Unfortunately, given an arbitrary collection of elements, there is no systematic way to construct a 
 perfect hash function. Luckily, we do not need the hash function to be perfect to still gain performance 
 efficiency. 
  
 One way to always have a perfect hash function is to increase the size of the hash table so that each possible 
 value in the element range can be accommodated. This guarantees that each element will have a unique slot. 
 Although this is practical for small numbers of elements, it is not feasible when the number of possible 
 elements is large. For example, if the elements were nine-digit Social Security numbers, this method would 
 require almost one billion slots. If we only want to store data for a class of 25 students, we will be wasting an 
 enormous amount of memory. 
  
 Our goal is to create a hash function that minimizes the number of collisions, is easy to compute, and evenly 
 distributes the elements in the hash table. There are a number of common ways to extend the simple remainder 
 method. We will consider a few of them here. 
  
 14.6 Hash Table 
  
 388",NA
14.8 Load Factor ,"The load factor of a non-empty hash table is the number of items stored in the table divided by the size of the 
 table. This is the decision parameter used when we want to rehash  expand the existing hash table entries. This 
 also helps us in determining the efficiency of the hashing function. That means, it tells whether the hash 
 function is distributing the keys uniformly or not. 
  
  =     ℎℎ 
  
  
 ℎ",NA
14.9 Collisions ,"Hash functions are used to map each key to a different address space, but practically it is not possible to create 
 such a hash function and the problem is called . Collision is the condition where two keys are hashed to the 
 same slot.",NA
14.10 Collision Resolution Techniques ,"Fortunately, there are effective techniques for resolving the conflict created by collisions. The process of finding 
 an alternate location for a key in the case of a collision is called . Even though hash tables have collision 
 problems, they are more efficient in many cases compared to all other data structures, like search trees. There 
 are a number of collision resolution techniques, and the most popular are direct chaining and open addressing. 
  
 •
  
 Direct Chaining (or Closed Addressing)
 :
  An array of linked list application 
  
 o 
  
 Separate chaining (linear chaining) 
  
 •
  
 Open Addressing
 :
  Array-based implementation 
  
 o 
  
 Linear probing (linear search) 
  
 o 
  
 Quadratic probing (nonlinear search) 
  
 o 
  
 Double hashing (use multiple hash functions) 
  
 Of course, the ideal solution would be to avoid collisions altogether. We might try to achieve this goal by 
 choosing a suitable hash function.",NA
14.11 Separate Chaining ,"A first idea to explore is to implement the associative array as a linked list, called a chain or a linked list. 
 Separate chaining is one of the most commonly used collision resolution techniques. It is usually implemented 
 using linked 
  
 14.8 Load Factor 
  
 389",NA
14.12 ,NA,NA
Open Addressing,"In open addressing all keys are stored in the hash table itself. This approach is also known as 
  ℎℎ
 . This 
 procedure is based on probing. A collision is resolved by probing. 
  
 Linear Probing
  
 The interval between probes is fixed at 
 1
 . In linear probing, we search the hash table sequentially. starting from 
 the original hash location. If a location is occupied, we check the next location. We wrap around from the last 
 table location to the first table location if necessary. The function for rehashing is the following: 
  
 ℎℎ() = ( + 1)% 
  
 One of the problems with linear probing is that table items tend to cluster together in the hash table. This 
 means that the table contains groups of consecutively occupied locations that are called . 
  
 Clusters can get close to one another, and merge into a larger cluster. Thus, the one part of the table might be 
 quite dense, even though another part has relatively few items. Clustering causes long probe searches and 
 therefore decreases the overall efficiency. 
  
 14.12 Open Addressing 
  
 390",NA
14.13 Comparison of Collision Resolution Techniques ,"Comparisons: Linear Probing vs. Double Hashing 
  
 The choice between linear probing and double hashing depends on the cost of computing the hash function and 
 on the load factor [number of elements per slot] of the table. Both use few probes but double hashing take more 
 time because it hashes to compare two hash functions for long keys. 
  
 Comparisons: Open Addressing vs.Separate Chaining 
  
 It is somewhat complicated because we have to account for the memory usage. Separate chaining uses extra 
 memory for links. Open addressing needs extra memory implicitly within the table to terminate the probe 
 sequence. Open-addressed hash tables cannot be used if the data does not have unique keys. An alternative is 
 to use separate chained hash tables.",NA
14.14 How Hashing Gets O(1) Complexity ,"We stated earlier that in the best case hashing would provide a O(1), constant time search technique. However, 
 due to collisions, the number of comparisons is typically not so simple. Even though a complete analysis of 
 hashing is beyond the scope of this text, we can state some well-known results that approximate the number of 
 comparisons necessary to search for an item. From the previous discussion, one doubts how hashing gets O
 (1)
  
 if multiple elements map to the same location. 
  
 The answer to this problem is simple. By using the load factor we make sure that each block (for example, 
 linked list in separate chaining approach) on the average stores the maximum number of elements less than the 
 . Also, in practice this load factor is a constant (generally, 
 10
  or 
 20
 ). As a result, searching in 
 20 
 elements or 
 10
  
 elements becomes constant. 
  
 If the average number of elements in a block is greater than the load factor, we rehash the elements with a 
 bigger hash table size. One thing we should remember is that we consider average occupancy (total number of 
 elements in the hash table divided by table size) when deciding the rehash. 
  
 The access time of the table depends on the load factor which in turn depends on the hash function. This is 
 because hash function distributes the elements to the hash table. For this reason, we say hash table gives O
 (1) 
 complexity on average. Also, we generally use hash tables in cases where searches are more than insertion and 
 deletion operations.",NA
14.15 Hashing Techniques ,"There are two types of hashing techniques: static hashing and dynamic hashing 
  
 Static Hashing
  
 If the data is fixed then static hashing is useful. In static hashing, the set of keys is kept fixed and given in 
 advance, and the number of primary pages in the directory are kept fixed. 
  
 Dynamic Hashing 
  
 If the data is not fixed, static hashing can give bad performance, in which case dynamic hashing is the 
 alternative, in which case the set of keys can change dynamically.",NA
14.16 Problems for which Hash Tables are not suitable ,"•
  
 •
  
 •
  
 •
  
 •
  
 Problems for which data ordering is required 
  
 Problems having multidimensional data 
  
 Prefix searching, especially if the keys are long and of variable-lengths 
 Problems that have dynamic data 
  
 Problems in which the data does not have unique keys.",NA
14.17 Bloom Filters ,"A Bloom filter is a probabilistic data structure which was designed to check whether an element is present in a 
 set with memory and time efficiency. It tells us that the element either definitely is  in the set or 
 may
  be in the 
 set. The base data structure of a Bloom filter is a . The algorithm was invented in 1970 by Burton Bloom and it 
 relies on the use of a number of different hash functions.",NA
How it works? ,"A Bloom filter starts off with a bit array initialized to zero. To store a data value, we simply apply  different hash 
 functions and treat the resulting  values as indices in the array, and we set each of the  array elements to 1. We 
 repeat this for every element that we encounter. 
  
 14.14 How Hashing Gets O(1) Complexity 
  
 392",NA
14.18 ,NA,NA
Hashing: Problems & Solutions,"Problem-1
 Implement a separate chaining collision resolution technique. Also, discuss time complexities of each 
  
 function. 
  
 Solution:
  To create a hash table of given size, say , we allocate an array of 
 /  
 (whose value is usually between 
 5
  
 and 
 20
 ) pointers to list, initialized to nil. To perform 
 // 
 operations, we first compute the index of the table from 
 the given key by using 
 ℎℎ
  and then do the corresponding operation in the linear list maintained at that location. 
 To get uniform distribution of keys over a hash table, maintain table size as the prime number. 
  
 package main 
  
 import ""fmt"" 
  
 var ( 
  
  
 minLoadFactor    = 0.25 
  
  
 maxLoadFactor    = 0.75 
  
  
 defaultTableSize = 3 
  
 ) 
  
  
 type Element struct { 
  
  
 key   int 
  
  
 value int 
  
  
 next  *Element 
  
 } 
  
  
 type Hash struct { 
  
  
 buckets []*Element 
  
 } 
  
  
 type HashTable struct { 
  
  
 table *Hash 
  
  
 size  *int 
  
 } 
  
  
 // createHashTable: Called by checkLoadFactorAndUpdate when creating a new hash 
 func createHashTable(tableSize int) HashTable { 
  
  
 num := 0 
  
  
 hash := Hash{make([]*Element, tableSize)} 
  
  
 return HashTable{table: &hash, size: &num} 
  
 } 
  
  
 // CreateHashTable: Called by the user to create a hashtable. 
  
 func CreateHashTable() HashTable { 
  
 num := 0 
  
 hash := Hash{make([]*Element, defaultTableSize)} 
  
  
 14.18 Hashing: Problems & Solutions 
  
 394",NA
String ,NA,NA
Algorithms ,NA,NA
Chapter ,NA,NA
15 ,NA,NA
15.1 Introduction ,"To understand the importance of string algorithms let us consider the case of entering the URL (Uniform 
 Resource Locator) in any browser (say, Internet Explorer, Firefox, or Google Chrome). You will observe that after 
 typing the prefix of the URL, a list of all possible URLs is displayed. That means, the browsers are doing some 
 internal processing and giving us the list of matching URLs. This technique is sometimes called 
  − 
 . 
  
 Similarly, consider the case of entering the directory name in the command line interface (in both  and ). After 
 typing the prefix of the directory name, if we press the  button, we get a list of all matched directory names 
 available. This is another example of auto completion. 
  
 In order to support these kinds of operations, we need a data structure which stores the string data efficiently. 
 In this chapter, we will look at the data structures that are useful for implementing string algorithms. 
  
 We start our discussion with the basic problem of strings: given a string, how do we search a substring 
 (pattern)
 ? 
 This is called a 
  ℎ
  problem. After discussing various string matching algorithms, we will look at 
 different data structures for storing strings.",NA
15.2 String Matching Algorithms ,"In this section, we concentrate on checking whether a pattern  is a substring of another string ( stands for text) 
 or not. Since we are trying to check a fixed string, sometimes these algorithms are called 
  
  ℎ
  algorithms. 
 To simplify our discussion, let us assume that the length of given text is  and the length of the pattern  which we 
 are trying to match has the length. That means,  has the characters from
  0 
 to 
  − 1
  (
 [0 …  − 1]
 ) and  has the 
 characters from 
 0
  to 
  − 1
  (
 [0 … − 1]
 ). This algorithm is implemented in 
  + +
  as (). 
  
 In the subsequent sections, we start with the brute force method and gradually move towards better algorithms. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Brute Force Method 
  
 Rabin-Karp String Matching Algorithm 
 String Matching with Finite Automata 
 KMP Algorithm 
  
 Boyer-Moore Algorithm 
  
 Suffix Trees",NA
15.3 Brute Force Method ,"In this method, for each possible position in the text  we check whether the pattern  matches or not. Since the 
 length of is, we have 
  −  + 1
  possible choices for comparisons. This is because we do not need to check the last 
  
 − 1
  locations of as the pattern length is . The following algorithm searches for the first occurrence of a pattern 
 string in a text string 
 .
  
 Algorithm 
  
 func strStr(T, P string) int { 
  
  
 if len(P) == 0 { 
  
  
 return 0 
  
  
 } 
  
  
 i, j := 0, 0 
  
  
 for i < len(T) { 
  
  
 if T[i] == P[0] { 
  
  
  
  
 for j = 1; j < len(P); j++ { 
  
  
 15.1 Introduction 
  
 407",NA
15.4 Rabin-Karp String Matching Algorithm ,"Rabin-Karp Algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin that 
 uses hashing to find any one of a set of pattern strings in a text. In this method, we will use the hashing 
 technique and instead of checking for each possible position in 
 ,
  we check only if the hashing of  and the 
 hashing of characters of give the same result. 
  
 Initially, apply the hash function to the first  characters of  and check whether this result and’s hashing result 
 is the same or not. If they are not the same, then go to the next character of and again apply the hash function 
 to  characters (by starting at the second character). If they are the same then we compare those characters of  
 with. 
  
 Selecting Hash Function 
  
 At each step, since we are finding the hash of  characters of 
 ,
  we need an efficient hash function. If the hash 
 function takes O
 ()
  complexity in every step, then the total complexity is O
 ( × )
 . This is worse than the brute 
 force method because first we are applying the hash function and also comparing. 
  
 Our objective is to select a hash function which takes O
 (1)
  complexity for finding the hash of characters of every 
 time. Only then can we reduce the total complexity of the algorithm. If the hash function is not good (worst 
 case), the complexity of the Rabin-Karp algorithm is O
 ( −  + 1) × ) ≈
 O
 ( × ).
  If we select a good hash function, the 
 complexity of the Rabin-Karp algorithm complexity is O
 ( + ).
  Now let us see how to select a hash function which 
 can compute the hash of  characters of  at each step in O
 (1).
  
 For simplicity, let's assume that the characters used in string  are only integers. That means, all characters in 
  ∈ 
 {0, 1,2, . . . , 9 }.
  Since all of them are integers, we can view a string of  consecutive characters as decimal numbers. 
 For example, string 
 ′61815′
  corresponds to the number 
 61815
 . With the above assumption, the pattern  is also a 
 decimal value, and let us assume that the decimal value of  is . For the given text 
 [0..  − 1], 
 let
  () 
 denote the 
 decimal value of length
 −
  substring 
 [. .  +  − 1]
  for 
  =  0,1,. . . , −  − 1
 . So, 
 () ==  
  if and only if 
 [. .  +  − 1]
  == 
 [0. .  − 1]
 . 
  
 We can compute  in O
 () 
 time using Horner's Rule as
 : 
  
  =  [ − 1] +  10([ − 2] +  10([ − 3] + . . . + 10 ([1] +  10 [0]). . . )) 
  
 The code for the above assumption is: 
  
 func hash(s string) uint32 { 
  
  
 var h uint32 
  
  
 for i := 0; i < len(s); i++ { 
  
  
 h = (h*base + uint32(s[i])) 
  
  
 } 
  
  
 15.4 Rabin-Karp String Matching Algorithm 
  
 408",NA
15.5 String Matching with Finite Automata ,"In this method we use the finite automata which is the concept of the Theory of Computation (ToC). Before 
 looking at the algorithm, first let us look at the definition of finite automata. 
  
 Finite Automata 
  
 A finite automaton F is a 5-tuple (
 , , , ∑,
 ), where 
  
 •
  
 •
  
 •
  
 •
  
 •
  
   
  is a finite set of states 
  
 q∈  
  is the start state 
  
  ⊆   
  is a set of accepting states 
  
 ∑
  is a finite input alphabet 
  
  is the transition function that gives the next state for a given current state and input 
  
 How does Finite Automata Work? 
  
 •
  
 The finite automaton  begins in state 
  
  
 •
  
 Reads characters from 
 ∑
  one at a time 
  
 •
  
 If  is in state  and reads input character ,   moves to state 
 (,)
  
 •
  
 At the end, if its state is in 
 , 
  then we say,  accepted the input string read so far 
  
 •
  
 If the input string is not accepted it is called the rejected string 
  
 15.5 String Matching with Finite Automata 
  
 409",NA
15.6 KMP Algorithm ,"As before, let us assume that  is the string to be searched and  is the pattern to be matched. This algorithm was 
 presented by Knuth, Morris and Pratt. It takes O
 () 
 time complexity for searching a pattern. To get O
 ()
  time 
 complexity, it avoids the comparisons with elements of  that were previously involved in comparison with some 
 element of the pattern . 
  
 The algorithm uses a table and in general we call it  or  or 
   F
 .  First we will see how to fill this table and later 
 how to search for a pattern using this table. The prefix function
  F
  for a pattern stores the knowledge about how 
 the pattern matches against shifts of itself. This information can be used to avoid useless shifts of the pattern 
 .
  
 It means that this table can be used for avoiding backtracking on the string 
  .
  
 Prefix Table 
  
 // Table building algorithm. Takes pattern P to be analyzed and return prefix table. 
 func KMP_PrefixTable(P string) (F []int) { 
  
  
 F = make([]int, len(P)) 
  
  
 pos, cnd := 2, 0 
  
  
 F[0], F[1] = -1, 0 
  
  
 15.6 KMP Algorithm 
  
 410",NA
15.7 Boyer-Moore Algorithm ,"Like the KMP algorithm, this also does some pre-processing and we call it 
 . 
 The algorithm scans the characters 
 of the pattern from right to left beginning with the rightmost character. During the testing of a possible 
 placement of pattern  in , a mismatch is handled as follows: Let us assume that the current character being 
 matched is 
 []  =  
  and the corresponding pattern character is 
 []
 . If  is not contained anywhere in 
 ,
  then shift the 
 pattern  completely past 
 []. 
 Otherwise, shift P until an occurrence of character  ingets aligned with 
 []. 
 This 
 technique avoids needless comparisons by shifting the pattern relative to the text. 
  
 The  function takes O
 ( + |∑|)
  time and the actual search takes O
 ()
  time. Therefore the worst case running time 
 of the Boyer-Moore algorithm is O
 ( + |∑|).
  This indicates that the worst-case running time is quadratic, in the 
 case of 
  == 
 , the same as the brute force algorithm. 
  
 •
  
 •
  
 •
  
 •
  
 The Boyer-Moore algorithm is very fast on the large alphabet (relative to the length of the 
 pattern). For the small alphabet, Boer-Moore is not preferable. 
  
 For binary strings, the KMP algorithm is recommended. 
  
 For the very shortest patterns, the brute force algorithm is better.",NA
15.8 Data Structures for Storing Strings ,"If we have a set of strings (for example, all the words in the dictionary) and a word which we want to search in 
 that set,  in order to perform the search operation faster, we need an efficient way of storing the strings. To 
 store sets of strings we can use any of the following data structures. 
  
 •
  
 •
  
 •
  
 •
  
 Hashing Tables 
  
 Binary Search Trees 
  
 Tries 
  
 Ternary Search Trees",NA
15.9 Hash Tables for Strings ,"As seen in the 
 ℎ
  chapter, we can use hash tables for storing the integers or strings. In this case, the keys are 
 nothing but the strings. The problem with hash table implementation is that we lose the ordering information – 
 after applying the hash function, we do not know where it will map to. 
  
 As a result, some queries take more time. For example, to find all the words starting with the letter 
 “”,
  with hash 
 table representation we need to scan the complete hash table. This is because the hash function takes the 
 complete key, performs hash on it, and we do not know the location of each word.",NA
15.10 Binary Search Trees for Strings ,"In this representation, every node is used for sorting the strings alphabetically. This is possible because the 
 strings have a natural ordering:  comes before , which comes before , and so on. This is because words can be 
 ordered and we can use a Binary Search Tree (BST) to store and retrieve them. 
  
 is 
  
 a 
  
 caree
  
 monk 
  
 string 
  
 this 
  
  
 15.7 Boyer-Moore Algorithm 
  
  
  
  
  
 413",NA
15.11 Tries ,"Now, let us see the alternative representation that reduces the time complexity of the search operation. The 
 name  is taken from the word re”trie”. 
  
 What is a Trie? 
  
 A  is a tree data structure and each node in it contains a number of pointers equal to the number of characters 
 of the alphabet. For example, if we assume that all the strings are formed with English alphabet characters 
 “”
  to 
 “”
  then each node of the trie contains 26 pointers. A trie (also known as a digital tree) and sometimes even radix 
 tree or prefix tree (as they can be searched by prefixes) is an ordered tree structure, which takes advantage of 
 the keys that it stores – usually strings. Suppose we want to store the strings 
 “”, “”, “”
 , and
  “”
 ”:  for these 
  
 strings will look like: 
  
 ‘a’ 
  
 1 
  
 … 
  
 … 
  
 ‘a’ 
  
 1 
  
 … 
  
 ‘s’ 
  
 Non 
  
 e 
  
 None
  
 ‘s’ 
  
 1 
  
 … 
  
 … 
  
 None None
  
 None
  
 None
  
 None 
  
 1 
  
 … 
  
 ‘1’ 
  
 1 
  
 … 
  
 26-Pointers for each possible character 
  
 None 
  
 None None
  
 None
  
 None
  
 None 
  
 There may be cases when a trie is a binary search tree, but in general, these are different. Both binary search 
 trees and tries are trees, but each node in binary search trees always has two children, whereas tries' nodes, on 
 the other hand, can have more. In a trie, every node (except the root node) stores one character or a digit. By 
 traversing the trie down from the root node to a particular node , a common prefix of characters or digits can be 
 formed which is shared by other branches of the trie as well. 
  
 Why Tries? 
  
 The tries can insert and find strings in O
 ()
  time (where  represents the length of a single word). This is much 
 faster than hash table and binary search tree representations. 
  
 Trie Declaration 
  
 The structure of the  node has letter (char), isLeaf (boolean), and has a collection of child nodes (Collection of 
 Trie nodes). The basic element – Trie node of a trie data structure looks like this: 
  
 // Trie node 
  
 type Trie struct { 
  
 letter   rune 
  
 // Contains the current node character 
  
 children []*Trie 
  
 // Pointers to other tri nodes 
  
 meta     map[string]interface{} // meta data for the given word 
  
 isLeaf   bool 
  
 // Indicates whether the string formed from root to current node is a string or not 
  
 } 
  
 Now that we have defined our Trie node, let’s go ahead and look at the other operations of trie. Fortunately, the 
 trie data structure is simple to implement since it has three major methods: insert, search and delete. Let’s look 
 at the elementary implementation of these methods. 
  
 Inserting a String in Trie 
  
 To insert a string, we just need to start at the root node and follow the corresponding path (path from root 
 indicates the prefix of the given string). Before we start the implementation, it's important to understand the 
 algorithm: 
  
 15.11 Tries 
  
 414",NA
15.12 Ternary Search Trees ,"In computer science, a ternary search tree is a type of trie (sometimes called a prefix tree) where nodes are 
 arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's 
 limit of two. This representation was initially provided by Jon Bentley and Sedgewick. A ternary search tree 
 takes the advantages of binary search trees (BSTs) and tries. That means, it combines the memory efficiency of 
 BSTs and the time efficiency of tries. 
  
 Ternary Search Trees Declaration 
  
 type ( 
  
  
 TSTNode struct { 
  
  
 key             byte 
  
  
 value           interface{} 
  
 left, eq, right *TSTNode 
  
 } 
  
 ) 
  
 TernarySearchTree struct { 
 length int 
  
 root   *TSTNode 
  
 } 
  
  
 // Value for the key, if it is the end of the string 
  
 Each node of a ternary search tree stores a single character, and pointers to its three children conventionally 
 named equal child, left child and right child. A node may also have a pointer to its parent node as well as an 
 indicator as to whether or not the node marks the end of a word. The left child pointer must point to a node 
 whose character value is less than the current node. The right child pointer must point to a node whose 
 character is greater than the current node. The equal child points to the next character in the word. 
  
 Inserting strings in the Ternary Search Tree 
  
 Inserting a value into a ternary search can be defined recursively much as lookups are defined. This recursive 
 method is continually called on nodes of the tree given a key which gets progressively shorter by pruning 
 characters off the front of the key. If this method reaches a node that has not been created, it creates the node 
 and assigns it the character value of the first character in the key. Whether a new node is created or not, the 
 method checks to see if the first character in the string is greater than or less than the character value in the 
 node and makes a recursive call on the appropriate node as in the lookup operation. If, however, the key's first 
 character is equal to the node's value then the insertion procedure is called on the  child and the key's first 
 character is pruned away. Like binary search trees and other data structures, ternary search trees can become 
 degenerate depending on the order of the keys. 
  
 ‘b’ 
  
 0 
  
 ‘o’ 
  
 0 
  
 nil 
  
 nil 
  
 nil
  
 nil
  
 nil 
  
  
 nil 
  
 nil 
  
 ‘a’ 
  
 0 
  
 nil
  
 ‘t’ 
  
 0 
  
 1 
  
 nil 
  
 ‘s’ 
  
 nil 
  
 nil 
  
 15.12 Ternary Search Trees 
  
  
  
  
  
  
  
 417",NA
"15.13 Comparing BSTs, Tries and TSTs ","•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Hash table and BST implementation stores complete the string at each node. As a result they take more 
 time for searching. But they are memory efficient. 
  
 TSTs can grow and shrink dynamically but hash tables resize only based on load factor. 
  
 TSTs allow partial search whereas BSTs and hash tables do not support it. 
  
 TSTs can display the words in sorted order, but in hash tables we cannot get the sorted order. 
  
 Tries perform search operations very fast but they take huge memory for storing the string. 
  
 TSTs combine the advantages of BSTs and Tries. That means they combine the memory efficiency of 
 BSTs and the time efficiency of tries.",NA
15.14 Suffix Trees ,"Suffix trees are an important data structure for strings. With suffix trees we can answer the queries very fast. 
 But this requires some preprocessing and construction of a suffix tree. Even though the construction of a suffix 
 tree is complicated, it solves many other string-related problems in linear time. 
  
 Note: 
 Suffix trees use a tree (suffix tree) for one string, whereas Hash tables, BSTs, Tries and TSTs store a set of 
 strings. That means, a suffix tree answers the queries related to one string.
  
 Let us see the terminology we use for this representation. 
  
 15.13 Comparing BSTs, Tries and TSTs 
  
 420",NA
Observation ,"From the above example, we can easily see that for a given text  and pattern 
 ,
  the exact string matching problem 
 can also be defined as: 
  
 •
  
 •
  
 Find a suffix of such that  is a prefix of this suffix Find 
 a prefix of   such that  is a suffix of this prefix.
  
 Example: 
 Let the text to be searched be 
  = 
  and the pattern be 
  = .
   For this example,  is a prefix of the suffix  
 and also a suffix of the prefix .",NA
What is a Suffix Tree? ,"In simple terms, the suffix tree for text  is a Trie-like data structure that represents the suffixes of 
 . 
  The 
 definition of suffix trees can be given as: A suffix tree for a  character string 
 [1 … ]
  is a rooted tree with the 
 following properties. 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 A suffix tree will contain leaves which are numbered from 
 1 
 to Each 
 internal node (except root) should have at least 
 2
  children Each edge 
 in a tree is labeled by a nonempty substring of 
  
 No two edges of a node (children edges) begin with the same character 
 The paths from the root to the leaves represent all the suffixes of",NA
The Construction of Suffix Trees ,"Algorithm 
  
 1.Let  be the set of all suffixes of . Append $ to each of the suffixes. 
  
 2.Sort the suffixes in S based on their first character. 
  
 3.For each group 
 S( ∈ ∑): 
  
  
 (i) 
  
 If 
 S
  group has only one element, then create a leaf node. 
  
 (ii) 
  
 Otherwise, find the longest common prefix of the suffixes in 
 S
  group, create an internal 
 node, 
  
 and recursively continue with Step 2, S being the set of remaining suffixes from 
 S
  after 
  
 splitting off the longest common prefix. 
  
 For better understanding, let us go through an example. Let the given text be 
  = 
 . For this string, give a number 
 to each of the suffixes. 
  
 Index 
  
 Suffix 
  
 1
  
 $
  
 2
  
 $
  
 3
  
 $
  
 4
  
 $
  
 5
  
 $
  
 6
  
 $
  
 Now, sort the suffixes based on their initial characters. 
  
 Index 
  
 Suffix 
  
 Group  based on 
 a
  
 1
  
 $
  
  3
  
 $
  
 Group  based on 
 a
  
 5
  
 $
  
 2
  
 $
  
 Group  based on 
 t
  
 4
  
 $
  
 6
  
 $
  
 In the three groups, the first group has only one element. So, as per the algorithm, create a leaf node for it, as 
 shown below. 
  
 $ 
  
 15.14 Suffix Trees 
  
 421",NA
Applications of Suffix Trees ,"All the problems below (but not limited to these) on strings can be solved with suffix trees very efficiently (for 
 algorithms refer to  section). 
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Exact String Matching:
  Given a text  and a pattern , how do we check whether  appears in  or not? 
 Longest Repeated Substring:
  Given a text  how do we find the substring of  that is the maximum 
 repeated substring? 
  
 Longest Palindrome:
  Given a text  how do we find the substring of  that is the longest palindrome of 
  
 ? 
  
 Longest Common Substring:
  Given two strings, how do we find the longest common substring? 
 Longest Common Prefix: 
 Given two strings 
 [ …]
  and 
 [ … ], 
 how do we find the longest common prefix? 
  
 How do we search for a regular expression in given text ? 
  
 Given a text  and a pattern , how do we find the first occurrence of  in ?",NA
15.15 String Algorithms,NA,NA
: Problems & Solutions,"Problem-1
 Given a paragraph of words, give an algorithm for finding the word which appears the maximum 
 number of times. If the paragraph is scrolled down (some words disappear from the first frame, some words 
 still appear, and some are new words), give the maximum occurring word. Thus, it should be dynamic. 
  
 Solution:
  For this problem we can use a combination of priority queues and tries. We start by creating a trie in 
 which we insert a word as it appears, and at every leaf of trie. Its node contains that word along with a pointer 
 that points to the node in the heap [priority queue] which we also create. This heap contains nodes whose 
 structure contains a 
 .
  This is its frequency and also a pointer to that leaf of trie, which contains that word so 
 that there is no need to store the word twice. 
  
 Whenever a new word comes up, we find it in trie. If it is already there, we increase the frequency of that node 
 in the heap corresponding to that word, and we call it heapify.  This is done so that at any point of time we can 
 get the word of maximum frequency. While scrolling, when a word goes out of scope, we decrement the counter 
 in heap. If the new frequency is still greater than zero, heapify the heap to incorporate the modification. If the 
 new frequency is zero, delete the node from heap and delete it from trie. 
  
 Problem-2
 Given two strings, how can we find the longest common substring? 
  
 Solution: 
 Let us assume that the given two strings are  and .The longest common substring of two strings,  and 
 , can be found by building a generalized suffix tree for  and . That means we need to build a single suffix tree for 
 both the strings. Each node is marked to indicate if it represents a suffix of  or  or both. This indicates that we 
 need to use different marker symbols for both the strings (for example, we can use $ for the first string and # for 
 the second symbol). After constructing the common suffix tree, the deepest node marked for both  and  
 represents the longest common substring. 
  
 Another way of doing this is:
  We can build a suffix tree for the string $
 #
 . This is equivalent to building a 
 common suffix tree for both the strings. 
  
 Time Complexity: O
 ( + ),
  where  and  are the lengths of input strings  and . 
  
 Problem-3
 Longest Palindrome: 
 Given a text  how do we find the substring of  which is the longest palindrome 
  
 of ? 
  
 Solution: 
 The longest palindrome of 
 [1.. ]
  can be found in O
 ()
  time. The algorithm is: first build a suffix tree for 
 $()#
  or build a generalized suffix tree for  and 
 ().
  After building the suffix tree, find the deepest node marked 
 with both 
 $
  and 
 #
 . Basically it means find the longest common substring. 
  
 Problem-4
 Given a string (word), give an algorithm for finding the next word in the dictionary. 
  
 Solution:
  Let us assume that we are using Trie for storing the dictionary words. To find the next word in Tries 
 we can follow a simple approach as shown below. Starting from the rightmost character, increment the 
 characters one by one. Once we reach , move to the next character on the left side. 
  
 15.15 String Algorithms: Problems & Solutions 
  
 423",NA
Algorithms ,NA,NA
Design ,NA,NA
Techniques ,NA,NA
Chapter ,NA,NA
16 ,NA,NA
16.1 Introduction ,"In the previous chapters, we have seen many algorithms for solving different kinds of problems. Before solving a 
 new problem, the general tendency is to look for the similarity of the current problem to other problems for 
 which we have solutions. This helps us in getting the solution easily. 
  
 In this chapter, we will see different ways of classifying the algorithms and in subsequent chapters we will focus 
 on a few of them (Greedy, Divide and Conquer, Dynamic Programming).",NA
16.2 Classification ,"There are many ways of classifying algorithms and a few of them are shown below: 
  
 •
  
 Implementation Method 
  
 •
  
 Design Method 
  
 •
  
 Other Classifications",NA
16.3 Classification by Implementation Method ,"Recursion or Iteration
  
 A  algorithm is one that calls itself repeatedly until a base condition is satisfied. It is a common method used in 
 functional programming languages like 
 , + +,
  etc.
  
  
  algorithms use constructs like loops and sometimes other data structures like stacks and queues to 
 solve the problems. 
  
 Some problems are suited for recursive and others are suited for iterative. For example, the 
  
 problem can be easily understood in recursive implementation. Every recursive version has an iterative version, 
 and vice versa. 
  
 Procedural or Declarative (Non-Procedural) 
  
 In  programming languages, we say what we want without having to say how to do it. With programming, we 
 have to specify the exact steps to get the result. For example, SQL is more declarative than procedural, because 
 the queries don't specify the steps to produce the result. Examples of procedural languages include: C, PHP, and 
 PERL. 
  
 Serial or Parallel or Distributed
  
 In general, while discussing the algorithms we assume that computers execute one instruction at a time. These 
  
 are called  algorithms. 
  
  algorithms take advantage of computer architectures to process several instructions at a time. They 
 divide the problem into subproblems and serve them to several processors or threads. Iterative algorithms are 
 generally parallelizable. 
  
 If the parallel algorithms are distributed on to different machines then we call such algorithms 
 algorithms
 .
  
 16.1 Introduction 
  
 432",NA
16.4 Classification by Design Method ,"Another way of classifying algorithms is by their design method. 
  
 Greedy Method
  
  algorithms work in stages. In each stage, a decision is made that is good at that point, without bothering 
 about the future consequences. Generally, this means that some  is chosen. It assumes that the local best 
 selection also makes for the  optimal solution. 
  
 Divide and Conquer
  
 The 
 D & C
  strategy solves a problem by: 
  
 1) 
  
 2) 
  
 3)
  
 Divide: Breaking the problem into sub problems that are themselves smaller instances of the same type 
 of problem. 
  
 Recursion: Recursively solving these sub problems. 
  
 Conquer: Appropriately combining their answers. 
  
 Examples: merge sort and binary search algorithms. 
  
 Dynamic Programming 
  
 Dynamic programming (DP) and memoization work together. The difference between DP and divide and conquer 
 is that in the case of the latter  there is no dependency among the sub problems, whereas in DP there will be an 
 overlap of sub-problems. By using memoization [maintaining a table for already solved sub problems], DP 
 reduces the exponential complexity to polynomial complexity (O
 ()
 , O
 ()
 , etc.) for many problems. 
  
 The difference between dynamic programming and recursion is in the memoization of recursive calls. When sub 
 problems are independent and if there is no repetition, memoization does not help, hence dynamic 
 programming is not a solution for all problems. 
  
 By using memoization [maintaining a table of sub problems already solved], dynamic programming reduces the 
 complexity from exponential to polynomial. 
  
 Linear Programming 
  
 Linear programming is not a programming language like C++, Java, or Visual Basic. Linear programming can be 
 defined as: 
  
 A method to allocate scarce resources to competing activities in an optimal manner when the problem can be 
 expressed using a linear objective function and linear inequality constraints. 
  
 A linear program consists of a set of variables, a linear objective function indicating the contribution of each 
 variable to the desired outcome, and a set of linear constraints describing the limits on the values of the 
 variables. The  to a linear program is a set of values for the problem variables that results in the best --  -- value 
 of the objective function and yet is consistent with all the constraints. Formulation is 
  
 the process of translating a real-world problem into a linear program. 
  
 Once a problem has been formulated as a linear program, a computer program can be used to solve the 
 problem. In this regard, solving a linear program is relatively easy. The hardest part about applying linear 
 programming is formulating the problem and interpreting the solution. In linear programming, there are 
 inequalities in terms of inputs and  (or ) some linear function of the inputs. Many problems (example: maximum 
 flow for directed graphs) can be discussed using linear programming.",NA
Reduction [Transform and Conquer] ,"In this method we solve a difficult problem by transforming it into a known problem for which we have 
 asymptotically optimal algorithms. In this method, the goal is to find a reducing algorithm whose complexity is 
  
 16.4 Classification by Design Method 
  
 433",NA
16.5 Other Classifications ,NA,NA
Classification by Research Area ,"In computer science each field has its own problems and needs efficient algorithms. Examples: search 
 algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, 
 geometric algorithms, combinatorial algorithms, machine learning, cryptography, parallel algorithms, data 
 compression algorithms, parsing techniques, and more. 
  
 Classification by Complexity 
  
 In this classification, algorithms are classified by the time they take to find a solution based on their input size. 
 Some algorithms take linear time complexity (O
 ()
 ) and others take exponential time, and some never halt.  Note 
 that some problems may have multiple algorithms with different complexities. 
  
 Randomized Algorithms
  
 A few algorithms make choices randomly. For some problems, the fastest solutions must involve randomness. 
 Example: Quick Sort.",NA
Branch and Bound Enumeration and Backtracking ,"These were used in Artificial Intelligence and we do not need to explore these fully. For the Backtracking method 
 refer to the  chapter. 
  
 Note:
  In the next few chapters we discuss the Greedy, Divide and Conquer, and Dynamic Programming] design 
 methods. These methods are emphasized because they are used more often than other methods to solve 
 problems.
  
 16.5 Other Classifications 
  
 434",NA
Greedy ,NA,NA
Algorithms ,NA,NA
17.1 Introduction ,NA,NA
Chapter ,NA,NA
17 ,"Let us start our discussion with simple theory that will give us an understanding of the Greedy technique. In 
 the game of
  ℎ
 , every time we make a decision about a move, we have to also think about the future 
 consequences. Whereas, in the game of   (or ), our action is based on the immediate situation. 
  
 This means that in some cases making a decision that looks right at that moment gives the best solution (), but 
 in other cases it doesn’t. The Greedy technique is best suited for looking at the immediate situation.",NA
17.2 Greedy Strategy ,"Greedy algorithms work in stages. In each stage, a decision is made that is good at that point, without 
 bothering about the future. This means that some  is chosen. It assumes that a local good selection makes for a 
 global optimal solution.",NA
17.3 Elements of Greedy Algorithms ,"The two basic properties of optimal Greedy algorithms are: 
  
 1) 
  
 2)
  
 Greedy choice property 
  
 Optimal substructure 
  
 Greedy choice property 
  
 This property says that the globally optimal solution can be obtained by making a locally optimal solution 
 (Greedy). The choice made by a Greedy algorithm may depend on earlier choices but not on the future. It 
 iteratively makes one Greedy choice after another and reduces the given problem to a smaller one. 
  
 Optimal substructure
  
 A problem exhibits optimal substructure if an optimal solution to the problem contains optimal solutions to the 
 subproblems. That means we can solve subproblems and build up the solutions to solve larger problems.",NA
17.4 Does Greedy Always Work? ,"Making locally optimal choices does not always work. Hence, Greedy algorithms will not always give the best 
 solutions. We will see particular examples in the section and in the  chapter.",NA
17.5 Advantages and Disadvantages of Greedy Method ,"The main advantage of the Greedy method is that it is straightforward, easy to understand and easy to code. In 
 Greedy algorithms, once we make a decision, we do not have to spend time re-examining the already computed 
 values. Its main disadvantage is that for many problems there is no greedy algorithm. That means, in many 
 cases there is no guarantee that making locally optimal improvements in a locally optimal solution gives the 
 optimal global solution.",NA
17.6 Greedy Applications ,"•
  
 Sorting: Selection sort, Topological sort 
  
  
 •
  
 Priority Queues: Heap sort 
  
 •
  
 Huffman coding compression algorithm 
  
 •
  
 Prim’s and Kruskal’s algorithms 
  
 •
  
 Shortest path in Weighted Graph without Negative Edge Weights [Dijkstra’s Algorithm]",NA
17.7 Understanding Greedy Technique ,For better understanding let us go through an example.,NA
Huffman Coding Algorithm ,"Definition 
  
 Given a set of  characters from the alphabet 
 A
  [each character 
 c ∈  A
 ]
  and their associated frequency 
 (c)
 ,
  find a 
 binary code for each character
  c ∈  A
 ,
  such that 
 ∑
  ∈ 
 freq(c)|binarycode(c)| 
  is minimum, where 
 |
  binarycode
 (
 c
 )| 
 represents the length of 
 binary code
 of character
 c
 . 
 That means the sum of the lengths of all character codes should 
 be minimum [the sum of each character’s frequency multiplied by the number of bits in the representation].
  
 The basic idea behind the Huffman coding algorithm is to use fewer bits for more frequently occurring 
 characters. The Huffman coding algorithm compresses the storage of data using variable length codes.We know 
 that each character takes 8 bits for representation. But in general, we do not use all of them. Also, we use some 
 characters more frequently than others. 
 When reading a file, the system generally reads 8 bits at a time to read a single 
 character. But this coding 
  
 scheme is inefficient. The reason for this is that some characters are more frequently used than other characters. Let's say that 
 the character 
  
 ′′
  is used 
 10
  times more frequently than the character 
 ′′
 . It would then be advantageous for us to instead use a 
 7
  bit code for e and 
 a 
 9
  bit 
  
 code for  because that could reduce our overall message length.
  
 On average, using Huffman coding on standard files can reduce them anywhere from 
 10%
  to 
 30%
  depending on the character 
 frequencies
 . 
  
 The idea behind the character coding is to give longer binary codes for less frequent characters and groups of characters. Also, the 
 character 
  
 coding is constructed in such a way that no two character codes are prefixes of each other. 
  
 An Example 
  
 Let's assume that after scanning a file we find the following character frequencies: 
  
 Character 
  
 Frequency 
  
  
 12
  
  
 2
  
  
 7
  
  
 13
  
  
 14
  
  
 85
  
 Given this, create a binary tree for each character that also stores the frequency with which it occurs (as shown below). 
  
  
 b-2 
  
 c-7 
  
 a-12 
  
 d-13 
  
 e-14 
  
 f-85 
  
  
  
  
 The algorithm works as follows: In the list, find the two binary 
 trees t
  
 nodes connected below it. So our picture looks like this: 
  
 9 
  
 Repeat this process until only one tree is left: 
  
 21 
  
 hat store 
 m
  
 inim
 u
  
 27 
  
 m 
 frequenc
  
 ies at their 
 no
  
 des.",NA
17.8 Greedy Algorithms: Problems & Solutions,"Problem-1
 Given an array  with size . Assume the array content 
 [] 
 indicates the length of the file and we want to 
 merge all these files into one single file. Check whether the following algorithm gives the best solution for 
 this problem or not? 
  
 Algorithm: 
 Merge the files contiguously. That means select the first two files and merge them. Then select 
 the output of the previous merge and merge with the third file, and keep going… 
  
 Note:
  Given two files  and  with sizes  and , the complexity of merging is O
 ( + ).
  
 Solution: 
 This algorithm will not produce the optimal solution. For a counter example, let us consider the 
 following file sizes array. 
  
  = {10,5,100,50,20,15}
  
 As per the above algorithm, we need to merge the first two files (
 10
  and 
 5
  size files), and as a result we get the 
 following list of files. In the list below, 
 15
  indicates the cost of merging two files with sizes 
 10
  and 
 5
 . 
  
 {15,100,50,20,15}
  
 Similarly, merging 
 15
  with the next file 
 100
  produces:  
 {115,50,20,15}.
  For the subsequent steps the list becomes 
  
  {165,20,15},{185,15}
  
 Finally, 
  {200}
  
 The total cost of merging 
 =
  Cost of all merging operations 
 = 15 + 115 + 165 + 185 + 200 = 680.
  
 To see whether the above result is optimal or not, consider the order: 
 {5,10, 15,20, 50,100}
 . For this example, 
 following the same approach, the total cost of merging 
 = 15 + 30 + 50 + 100 + 200 = 395.
  So, the given algorithm is 
 not giving the best (optimal) solution. 
  
 Problem-2
 Similar to Problem-1, does the following algorithm give the optimal  solution? 
  
 Algorithm: 
 Merge the files in pairs. That means after the first step, the algorithm produces the 
 /2 
 intermediate files. For the next step, we need to consider these intermediate files and merge them in pairs 
 and keep going. 
  
 Note: 
 Sometimes this algorithm is called 
 2
 -way merging. Instead of two files at a time, if we merge  files at a 
 time then we call it -way merging. 
  
 Solution:
  This algorithm will not produce the optimal solution and consider the previous example for a counter 
 example. As per the above algorithm, we need to merge the first pair of files (10 and 5 size files), the second pair 
 of files (
 100
  and 
 50
 ) and the third pair of files (
 20
  and 
 15
 ). As a result we get the following list of files. 
  
 17.8 Greedy Algorithms: Problems & Solutions 
  
 439",NA
Divide and ,NA,NA
Conquer ,NA,NA
Algorithms ,NA,NA
Chapter ,NA,NA
18 ,NA,NA
18.1 Introduction ,"In the chapter, we have seen that for many problems the 
 Greedy
  strategy failed to provide optimal solutions. Among those 
 problems, there are some that can be easily solved by using the  (
 D & 
 ) technique.  Divide and Conquer is an important algorithm 
 design technique based on recursion. 
  
 The 
  & 
  algorithm works by recursively breaking down a problem into two or more sub problems of the same type, until they 
 become simple enough to be solved directly. The solutions to the sub problems are then combined to give a solution to the 
 original problem.",NA
18.2 What is the Divide and Conquer Strategy? ,"The D & C strategy solves a problem by: 
  
 1) 
  
 2) 
  
 3)
  
 : Breaking the problem into subproblems that are themselves smaller instances of the same type of 
 problem. 
  
 : Conquer the subproblems by solving them recursively. 
  
 :
  Combine the solutions to the subproblems into the solution for the original given problem.",NA
18.3 Does Divide and Conquer Always Work?,"It’s not possible to solve all the problems with the Divide & Conquer technique. As per the definition of D & C, 
 the recursion solves the subproblems which are of the same type. For all problems it is not possible to find the 
 subproblems which are the same size and 
  & 
  is not a choice for all problems.",NA
18.4 Divide and Conquer Visualization ,"For better understanding, consider the following visualization. Assume that  is the size of the original problem. 
 As described above, we can see that the problem is divided into sub problems with each of size 
 /
  (for some 
 constant ). We solve the sub problems recursively and combine their solutions to get the solution for the original 
 problem. 
  
 a problem of size 
  
 a subproblem of size 
  
 Subproblems 
  
 a subproblem of size 
  
 ………….. 
  
 /
  
 /
  
 Solution to subproblem 
 /
  
 Solution to subproblem 
 /
  
 Combine sub-solutions for solution to 
  
  
 problem 
  
 func DivideAndConquer ( P )  { 
  
  
  if( small ( P ) )  
  
  
  // P is very small so that a solution is obvious 
  
  
  return solution ( n ) 
  
  
  divide the problem P into k sub problems P1, P2, ..., Pk 
  
  
 18.1 Introduction 
  
 446",NA
18.5 Understanding Divide and Conquer ,"For a clear understanding of 
 D & 
 , let us consider a story. There was an old man who was a rich farmer and had 
 seven sons. He was afraid that when he died, his land and his possessions would be divided among his seven 
 sons, and that they would quarrel with one another. 
  
 So he gathered them together and showed them seven sticks that he had tied together and told them that 
 anyone who could break the bundle would inherit everything. They all tried, but no one could break the bundle. 
 Then the old man untied the bundle and broke the sticks one by one. The brothers decided that they should 
 stay together and work together and succeed together. The moral for problem solvers is different. If we can't 
 solve the problem, divide it into parts, and solve one part at a time. 
  
 In earlier chapters we have already solved many problems based on 
  & 
  strategy: like Binary Search, Merge Sort, 
 Quick Sort, etc.... Refer to those topics to get an idea of how 
  & 
  works. Below are a few other real-time problems 
 which can easily be solved with 
  & 
  strategy. For all these problems we can find the subproblems which are 
 similar to the original problem. 
  
 •
  
 •
  
 •
  
 •
  
 Looking for a name in a phone book: We have a phone book with names in alphabetical order. Given a 
 name, how do we find whether that name is there in the phone book or not? 
  
 Breaking a stone into dust: We want to convert a stone into dust (very small stones). 
  
 Finding the exit in a hotel: We are at the end of a very long hotel lobby with a long series of doors, with 
 one door next to us. We are looking for the door that leads to the exit. 
  
 Finding our car in a parking lot.",NA
18.6 Advantages ofDivide and Conquer ,"Solving difficult problems: 
  & 
  is a powerful method for solving difficult problems. As an example, consider the 
 Tower of Hanoi problem. This requires breaking the problem into subproblems, solving the trivial cases and 
 combining the subproblems to solve the original problem. Dividing the problem into subproblems so that 
 subproblems can be combined again is a major difficulty in designing a new algorithm. For many such 
 problems D & C provides a simple solution. 
  
 Parallelism: 
 Since 
  & 
  allows us to solve the subproblems independently, this allows for execution in multi-
 processor machines, especially shared-memory systems where the communication of data between processors 
 does not need to be planned in advance, because different subproblems can be executed on different processors. 
  
 Memory access: 
  & 
  algorithms naturally tend to make efficient use of memory caches. This is because once a 
 subproblem is small, all its subproblems can be solved within the cache, without accessing the slower main 
 memory.",NA
18.7 Disadvantages ofDivide and Conquer ,"One disadvantage of the 
  & 
  approach is that recursion is slow. This is because of the overhead of the repeated 
 subproblem calls. Also, 
 the  & 
  approach needs stack for storing the calls (the state at each point in the 
 recursion). Actually this depends upon the implementation style. With large enough recursive base cases, the 
 overhead of recursion can become negligible for many problems. 
  
 Another problem with 
  & 
  is that, for some problems, it may be more complicated than an iterative approach. For 
 example, to add  numbers, a simple loop to add them up in sequence is much easier than a 
 D & C 
 approach 
 that breaks the set of numbers into two halves, adds them recursively, and then adds the sums.",NA
18.8 Master Theorem ,"As stated above, in the 
  & 
  method, we solve the sub problems recursively. All problems are generally defined in 
 terms of recursive definitions. These recursive problems can easily be solved using Master theorem. For details 
 on Master theorem, refer to the 
 ℎ
  chapter. Just for continuity, let us reconsider the Master theorem. 
  
 18.5 Understanding Divide and Conquer 
  
 447",NA
18.9 Divide and Conquer Applications ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Binary Search 
  
 Merge Sort and Quick 
 Sort 
  
 Median Finding 
  
 Min and Max Finding 
  
 Matrix Multiplication 
  
 Closest Pair problem",NA
18.10 Divide and Conquer,NA,NA
: Problems & Solutions,"Problem-1
 Let us consider an algorithm  which solves problems by dividing them into five subproblems of half 
 the size, recursively solving each subproblem, and then combining the solutions in linear time. What is the 
 complexity of this algorithm?
  
 Solution:
  Let us assume that the input size is 
 n
  and 
 ()
  defines the solution to the given problem. As per the 
  
 description, the algorithm divides the problem into 
 5
  sub problems with each of size
  
 . So we need to solve 
 5T(
  
 )
  
 subproblems. After solving these sub problems, the given array (linear time) is scanned to combine these 
 solutions. 
  
 The total recurrence algorithm for this problem can be given as: 
 () = 5 
 D 
 & C), we get the complexity asO
 ( 
  
 ) ≈
  O
 () ≈
  O
 ().
  
  +
 O
 ()
 . Using the Master theorem (of 
  
 Problem-2
 Similar to Problem-1,an algorithm  solves problems of size  by recursively solving two subproblems 
  
 of size 
  −  1
  and then combining the solutions in constant time. What is the complexity of this algorithm? 
 Solution:
  Let us assume that the input size is 
 n
  and 
 ()
  defines the solution to the given problem. As per the 
 description of algorithm we divide the problem into 
 2
  sub problems with each of size
  n − 1
 . So we have to solve 
 2T( − 1)
  sub problems. After solving these sub problems, the algorithm takes only a constant time to combine 
 these solutions. The total recurrence algorithm for this problem can be given as: 
  
  
  
 () = 2( − 1) +
  O
 (1)
  
 Using Master theorem (of ), we get the complexity asO
 2
  
 =O
 (2)
 . (Refer to 
  
 chapter for more details). 
  
 Problem-3
 Again similar to Problem-1,another algorithm  solves problems of size  by dividing them into nine 
  
 subproblems of size
  
 , recursively solving each subproblem, and then combining the solutions in O
 ()
  time. 
  
 What is the complexity of this algorithm?
  
 Solution: 
 Let us assume that input size is 
 n
  and 
 ()
  defines the solution to the given problem. As per the 
  
 description of algorithm we divide the problem into 
 9
  sub problems with each of size
  
 . So we need to solve 
 9(
  
 )
  
 sub problems. After solving the sub problems, the algorithm takes quadratic time to combine these solutions. 
 The 
  
 total recurrence algorithm for this problem can be given as:
  () = 9 
 get 
 the complexity as O
 ()
 .
  
 Problem-4
 Write a recurrence and solve it. 
  
  +
  O
 ()
 . Using 
  & 
  Master theorem, we 
  
 func function(n int) { 
  
  
 if n > 1 { 
  
  
 fmt.Println(""*"") 
  
  
 function(n / 2) 
  
  
 function(n / 2) 
  
  
 } 
  
 }
  
  
 18.9 Divide and Conquer Applications 
  
 448",NA
Dynamic ,NA,NA
Programming ,NA,NA
Chapter ,NA,NA
19 ,NA,NA
19.1 Introduction ,"In this chapter, we will try to solve few of the problems for which we failed to get the optimal solutions using 
 other techniques (say,  and  &  approaches). Dynamic Programming is a simple technique but it can be difficult 
 to master. Being able to tackle problems of this type would greatly increase your skill. 
  
 Dynamic programming (usually referred to as DP) is a very powerful technique to solve a particular class of 
 problems. It demands very elegant formulation of the approach and simple thinking and the coding part is very 
 easy. The idea is very simple, if you have solved a problem with the given input, then save the result for future 
 reference, so as to avoid solving the same problem again. Simply, we need to remember the past. 
  
 One easy way to identify and master DP problems is by solving as many problems as possible. The term DP is 
 not related to coding, but it is from literature, and means filling tables.",NA
19.2 What is Dynamic Programming Strategy? ,"Dynamic programming is typically applied to . In such problems there can be many possible solutions. Each 
 solution has a value, and we wish to find a solution with the optimal (minimum or maximum) value. We call 
 such a solution an optimal solution to the problem, as opposed to the optimal solution, since there may be 
 several solutions that achieve the optimal value. 
  
 The development of a dynamic-programming algorithm can be broken into a sequence of four steps. 
  
 1.Characterize the structure of an optimal solution. 
  
 2.Recursively define the value of an optimal solution. 
  
 3.Compute the value of an optimal solution in a bottom-up fashion. 
  
 4.Construct an optimal solution from computed information. 
  
 Steps 1-3 form the basis of a dynamic-programming solution to a problem. Step 4 can be omitted if only the 
 value of an optimal solution is required. When we do perform step 4, we sometimes maintain additional 
 information during the computation in step 3 to ease the construction of an optimal solution. 
  
 If the given problem can be broken up into smaller sub-problems and these smaller subproblems are in turn 
 divided into still-smaller ones, and in this process, if you observe some over-lapping subproblems, then it's a big 
 hint for DP. Also, the optimal solutions to the subproblems contribute to the optimal solution of the given 
 problem.",NA
19.3 Properties of Dynamic Programming Strategy ,"The two dynamic programming properties which can tell whether it can solve the given problem or not are: 
  
 •
  
 many times. 
  
 : An optimal solution to a problem contains optimal solutions to sub problems. 
  
 •
  
 : A recursive solution contains a small number of distinct sub problems 
 repeated",NA
19.4 Greedy vs Divide and Conquer vs DP ,"All algorithmic techniques construct an optimal solution of a subproblem based on optimal solutions of smaller 
 subproblems. 
  
 Greedy algorithms are one which finds optimal solution at each and every stage with the hope of finding global 
 optimum at the end. The main difference between DP and greedy is that, the choice made by a greedy algorithm 
 may depend on choices made so far but not on future choices or all the solutions to the sub problem. It 
 iteratively makes one greedy choice after another, reducing each given problem into a smaller one. 
  
 19.1 Introduction 
  
 464",NA
19.5 Can DP solve all problems? ,"Like greedy and divide and conquer techniques, DP cannot solve every problem. There are problems which 
 cannot be solved by any algorithmic technique [greedy, divide and conquer and DP]. The difference between DP 
 and straightforward recursion is in memoization of recursive calls. If the sub problems are independent and 
 there is no repetition then DP does not help. So, dynamic programming is not a solution for all problems.",NA
19.6 Dynamic Programming Approaches ,"Dynamic programming is all about ordering computations in a way that we avoid recalculating duplicate work. 
 In dynamic programming, we have a main problem, and subproblems (subtrees). The subproblems typically 
 repeat and overlap. The major components of DP are: 
  
 •
  
 Overlapping subproblems:
  Solves sub problems recursively. 
  
 •
  
 Storage:
  Store the computed values to avoid recalculating already solved subproblems. 
  
 By using extra storage, DP reduces the exponential complexity to polynomial complexity (O
 ()
 , O
 ()
 , etc.) for many 
 problems. 
  
 Basically, there are two approaches for solving DP problems: 
  
 •
  
 •
  
 Top-down approach [Memoization] 
  
 Bottom-up approach [Tabulation] 
  
 These approaches were classified based on the way we fill the storage and reuse them. 
  
  =    + Memoization or",NA
19.7 Understanding DP Approaches ,"Top-down Approach [Memoization] 
  
 In this method, the problem is broken into sub problems; each of these subproblems is solved; and the 
 solutions remembered, in case they need to be solved. Also, we save each computed value as the final action of 
 the recursive function, and as the first action we check if pre-computed value exists. 
  
 Bottom-up Approach [Tabulation] 
  
 In this method, we evaluate the function starting with the smallest possible input argument value and then we 
 step through possible values, slowly increasing the input argument value. While computing the values we store 
 all computed values in a table (memory). As larger arguments are evaluated, pre-computed values for smaller 
 arguments can be used. 
  
 Example: Fibonacci Series 
  
 Let us understand how DP works through an example; Fibonacci series. 
  
 In Fibonacci series, the current number is the sum of previous two numbers. The Fibonacci series is defined as 
 follows: 
  
  (n) =  0,                                                =  0 
  
  
  =  1,                                                =  1 
  
  
  =  (n − 1) +  (n − 2),  >  1 
  
 Calling (5) produces a call tree that calls the function on the same value many times: 
  
  (5) 
  
  (4) +  (3) 
  
  ((3) +  (2)) + ((2) +  (1))
  
  (((2) +  (1)) + ((1) +  (0))) + (((1) +  (0)) +  (1))
  
  ((((1) +  (0)) +  (1)) + ((1) +  (0))) + (((1) +  (0)) +  (1))
  
 19.5 Can DP solve all problems? 
  
 465",NA
19.8 Examples of DP Algorithms ,"•
  
 •
  
 •
  
 •
  
 •
  
 •
  
 Many string algorithms including longest common subsequence, longest increasing subsequence, 
 longest common substring, edit distance. 
  
 Algorithms on graphs can be solved efficiently: Bellman-Ford algorithm for finding the shortest distance 
 in a graph, Floyd's All-Pairs shortest path algorithm, etc. 
  
 Chain matrix multiplication 
  
 Subset Sum 
  
 0/1 Knapsack 
  
 Travelling salesman problem, and many more",NA
19.9 Longest Common Subsequence ,"Given two strings: string of length  [
 (1. . 
 )], and string  of length  [
 (1.. )
 ], find the longest common subsequence: 
 the longest sequence of characters that appear left-to-right (but not necessarily in a contiguous block) in both 
 strings. For example, if  = ""ABCBDAB"" and  = ""BDCABA"", the (X, Y) = {""BCBA"", ""BDAB"", ""BCAB""}. We can see 
 there are several optimal solutions. 
  
 Brute Force Approach: 
 One simple idea is to check every subsequence of 
 [1.. ]
  ( is the length of sequence ) to 
 see if it is also a subsequence of
  [1. . ]
  ( is the length of sequence ). Checking takes O
 ()
  time, and there are 
 2
  
 subsequences of . The running time thus is exponential O(
 .
  2
 ) and is not good for large sequences. 
  
 Recursive Solution: 
 Before going to DP solution, let us form the recursive solution for this and later we can 
 add memoization to reduce the complexity. Let's start with some simple observations about the LCS problem. If 
 we have two strings, say ""ABCBDAB"" and ""BDCABA"", and if we draw lines from the letters in the first string to 
 the corresponding letters in the second, no two lines cross: 
  
 A    B      C     B D A B 
  
  
  B D  C A  B     A 
  
 From the above observation, we can see that the current characters of  and  may or may not match. That 
 means, suppose that the two first characters differ. Then it is not possible for both of them to be part of a 
 common subsequence - one or the other (or maybe both) will have to be removed. Finally, observe that once we 
 have decided what to do with the first characters of the strings, the remaining sub problem is again a  problem, 
 on two shorter strings. Therefore we can solve it recursively. 
  
 The solution to  should find two sequences in  and  and let us say the starting index of sequence in  is 
  
 and the 
 starting index of sequence in  is . Also, assume that 
 [ …
 ] is a substring of starting at character and going until 
 the end of 
 ,
  and that 
 [ …]
  is a substring of  starting at character  and going until the end of 
  
  
 . 
  
 Based on the above discussion, here we get the possibilities as described below: 
  
 1) 
  
 If 
 [] ==  []
  : 
 1 +  ( + 1,  + 1) 
  
 2) 
  
 If 
 [] ≠  []: (,  + 1)
  // skipping  character of 3) 
  
 If 
 [] 
 ≠  []: ( + 1,)
  // skipping  character of 
  
 In the first case, if 
 []
  is equal to 
 []
 , we get a matching pair and can count it towards the total length of the . 
 Otherwise, we need to skip either  character of  or  character of  and find the longest common subsequence. 
  
 Now, 
 (,)
  can be defined as: 
  
 (, ) = 
  
  0,                                                                        =    = {(,  + 1), ( 
 + 1,  )},                 X[i] ≠  Y[j] 
  
  
  1 +  [ +  1,  +  1],                                   X[i] ==  Y[j]
  
 LCS has many applications. In web searching, if we find the smallest number of changes that are needed to 
 change one word into another. A 
 ℎ
  here is an insertion, deletion or replacement of a single character. 
  
 // Initial Call: LCSLengthRecursive(X, 0, m-1, Y, 0, n-1); 
  
 func LCSLengthRecursive(X string, i, m int, Y string, j, n int) int { 
  
  
 if i == m || j == n { 
  
  
 return 0 
  
  
 } else if X[i] == Y[j] { 
  
  
 return 1 + LCSLengthRecursive(X, i+1, m, Y, j+1, n) 
  
  
 } else { 
  
  
 return max(LCSLengthRecursive(X, i+1, m, Y, j, n), LCSLengthRecursive(X, i, m, Y, j+1, n)) 
  
 } 
  
 } 
  
  
 19.8 Examples of DP Algorithms 
  
 469",NA
19.10  Dynamic Programming,NA,NA
: Problems & Solutions,"Problem-1
 A child is climbing a stair case. It takes n steps to reach to the top. Each time child can either climb 1 
  
 or 2 steps. In how many distinct ways can the child climb to the top? 
  
 Solution
 : This problem is exactly same as that of Fibonacci series. If you see carefully, the answer for  = 
 1,2,3,4,5 … form a pattern. 
  
  
 Number of ways to climb  step 
  
 1 
  
 1 
  
 2 
  
 2 
  
 3 
  
 3 
  
 4 
  
 5 
  
 5 
  
 8 
  
 6 
  
 13 
  
 This clearly forms a Fibonacci sequence. It is not just a coincidence that the answers to the climbing stair 
 problem form a Fibonacci sequence. Why? 
  
 If you want to reach the  step in the staircase, what will be your last second step? If would be either the 
  − 1 
 step or the 
  − 2
  step, because you can jump only 1 or 2 steps at a time. 
  
 Number of ways to reach stair = Number of ways to reach 
  − 1
  stair + Number of ways to reach 
  − 2
  stair 
  
 climbStairs(n) = climbStairs(n-1) + climbStairs(n-2) 
  
 This is also the relation followed by the Fibonacci sequence. now that we know that our solution follows a 
 Fibonacci sequence and we have a defined recursive relation, we just need to figure out the termination case or 
 base case, i.e., when will the recursion end? The recursion can end if  becomes 0 or 1, the answer in which case 
 will be 1. 
  
 func climbStairs(n int) int { 
  
  
 if n < 3 { 
  
  
 return n 
  
  
 } 
  
  
 cache := make([]int, n) 
  
  
 cache[0], cache[1] = 1, 2 
  
  
 for i := 2; i < n; i++ { 
  
 cache[i] = cache[i-1] + cache[i-2] 
  
 } 
  
 return cache[n-1] 
  
 } 
  
 Problem-2
 A child is climbing up a staircase with  steps, and can hop either 1 step, 2 steps, or 3 steps at a time. 
  
 Implement a method to count how many possible ways the child can jump up the stairs. 
  
 Solution:
  The solution to this problem is very much similar to the previous problem. If you see carefully, the 
 answer for  = 1,2,4,7,13 … form a pattern. If you want to reach the  step in the staircase, what will be your last 
 second step? If would be either the 
  − 1
  step, 
  − 2
  step or the 
  − 3
  step, because you can jump only 1, 2 or 3 
 steps at a time. 
  
 Number of ways to reach stair = Number of ways to reach 
  − 1
  stair + 
  
  
  Number of ways to reach 
  − 2
  stair +  
  
  
  Number of ways to reach 
  − 3
  stair 
  
  
 climbStairs(n) = climbStairs(n-1) + climbStairs(n-2) + climbStairs(n-3) 
  
 The recursion can end if  becomes 0, 1 or 2, the answer in which case will be 1, 1, and 2. 
  
 Dynamic Programming: Problems & Solutions 
  
 471",NA
Complexity ,NA,NA
Classes ,NA,NA
20.1 Introduction ,NA,NA
Chapter ,NA,NA
20 ,"In the previous chapters we have solved problems of different complexities. Some algorithms have lower rates of 
 growth while others have higher rates of growth. The problems with lower rates of growth are called  problems 
 (or ) and the problems with higher rates of growth are called 
 ℎ
  problems (or 
 ℎ
  
 ). This classification is done 
 based on the running time (or memory) that an algorithm takes for solving the problem. 
  
 Time Complexity 
  
 Name 
  
 Example 
  
 Problems 
  
 O
 (1)
  
 Constant 
  
 Adding an element to the front of a linked list 
  
 Easy solved problems 
  
 O
 ()
  
 Logarithmic 
  
 Finding an element in a binary search tree 
  
 O
 ()
  
 Linear 
  
 Finding an element in an unsorted array 
  
 O
 ()
  
 Linear Logarithmic 
  
 Merge sort 
  
 O
 ()
  
 Quadratic 
  
 Shortest path between two nodes in a graph 
  
 O
 ()
  
 Cubic 
  
 Matrix Multiplication 
  
 O
 (2)
  
 Exponential 
  
 The Towers of Hanoi problem 
  
 Hard solved problems 
  
 O
 (!)
  
 Factorial 
  
 Permutations of a string 
  
 There are lots of problems for which we do not know the solutions. All the problems we have seen so far are the 
 ones which can be solved by computer in deterministic time. Before starting our discussion let us look at the 
 basic terminology we use in this chapter.",NA
20.2 Polynomial/Exponential Time ,"Exponential time means, in essence, trying every possibility (for example, backtracking algorithms) and they are 
 very slow in nature. Polynomial time means having some clever algorithm to solve a problem, and we don't try 
 every possibility. Mathematically, we can represent these as: 
  
 •
  
 •
  
 Polynomial time is O
 (
 ), for some 
 . 
 Exponential time is O
 (),
  for some 
 .",NA
20.3 What is a Decision Problem? ,"A decision problem is a question with a
  / 
 answer and the answer depends on the values of input. For example, 
 the problem “Given an array of  numbers, check whether there are any duplicates or not?” is a decision 
 problem. The answer for this problem can be either  or  depending on the values of the input array. 
  
  
 Input Algorithm 
  
 No",NA
20.4 Decision Procedure ,"For a given decision problem let us assume we have given some algorithm for solving it. The process of solving a 
 given decision problem in the form of an algorithm is called a  for that problem.",NA
20.5 What is a Complexity Class? ,"In computer science, in order to understand the problems for which solutions are not there, the problems are 
 divided into classes and we call them as complexity classes. In complexity theory, a  is a set of",NA
20.6 Types of Complexity Classes ,"P Class 
  
 The complexity class  is the set of decision problems that can be solved by a deterministic machine in 
 polynomial 
  
 time ( stands for polynomial time).  problems are a set of problems whose solutions are easy to find. 
  
 NP Class 
  
 The complexity class ( stands for non-deterministic polynomial time) is the set of decision problems that can be 
 solved by a non-deterministic machine in polynomial time.  class problems refer to a set of problems whose 
 solutions are hard to find, but easy to verify. 
  
 For better understanding let us consider a college which has 
 500
  students on its roll. Also, assume that there 
 are 
 100
  rooms available for students. A selection of 
 100
  students must be paired together in rooms, but the dean 
 of students has a list of pairings of certain students who cannot room together for some reason. 
  
 The total possible number of pairings is too large. But the solutions (the list of pairings) provided to the dean, 
 are easy to check for errors.  If one of the prohibited pairs is on the list, that's an error. In this problem, we can 
 see that checking every possibility is very difficult, but the result is easy to validate. 
  
 That means, if someone gives us a solution to the problem, we can tell them whether it is right or not in 
 polynomial time. Based on the above discussion, for  class problems if the answer is , then there is a proof of 
 this fact, which can be verified in polynomial time. 
  
 Co-NP Class 
  
  − 
  is the opposite of  (complement of ). If the answer to a problem in 
  − 
  is , then there is a proof of this fact that 
 can be checked in polynomial time. 
  
  
 Solvable in polynomial time 
  
  
  answers can be checked in polynomial time 
  
  − 
  
  answers can be checked in polynomial time 
  
 Relationship between P, NP and Co-NP 
  
 Every decision problem in  is also in . If a problem is in , we can verify YES answers in polynomial time. 
 Similarly, any problem in P is also in 
  − 
 . 
  
 Co-NP 
  
 NP 
  
 P 
  
 One of the important open questions in theoretical computer science is whether or not 
  =
 . Nobody knows. 
 Intuitively, it should be obvious that 
  ≠  ,
  but nobody knows how to prove it. 
  
 Another open question is whether  and 
  − 
  are different. Even if we can verify every YES answer quickly, there’s 
 no reason to think that we can also verify NO answers quickly. 
  
 It is generally believed that 
  ≠   − 
 , but again nobody knows how to prove it. 
  
 NP-hard Class 
  
 It is a class of problems such that every problem in  reduces to it. All -hard problems are not in , so it takes a 
 long time to even check them. That means, if someone gives us a solution for -hard problem, it takes a long time 
 for us to check whether it is right or not. 
  
 A problem  is -hard indicates that if a polynomial-time algorithm (solution) exists for  then a polynomial-time 
 algorithm for every problem is . Thus: 
  
 20.6 Types of Complexity Classes 
  
 504",NA
20.7 Reductions ,"Before discussing reductions, let us consider the following scenario. Assume that we want to solve problem  but 
 feel it’s very complicated. In this case what do we do? 
  
 The first thing that comes to mind is, if we have a similar problem to that of  (let us say ), then we try to map 
 to  and use 
 ’
  solution to solve  also. This process is called reduction. 
  
 Instance of 
  
 Algorithm for 
  
 Solution to 
  
 Input (for ) 
  
 I 
  
 Algorithm for 
  
 In order to map problem  to problem, we need some algorithm and that may take linear time or more. Based on 
 this discussion the cost of solving problem  can be given as: 
  
 20.7 Reductions 
  
 505",NA
20.8 Complexity Classes,NA,NA
: Problems & Solutions,"Problem-1
 What is a quick algorithm? 
  
 Solution: 
 A quick algorithm (solution) means not trial-and-error solution. It could take a billion years, but as 
 long 
  
 as we do not use trial and error, it is efficient. Future computers will change those billion years to a few 
 minutes. 
  
 Problem-2
 What is an efficient algorithm? 
  
 Solution: 
 An algorithm is said to be efficient if it satisfies the following properties: 
  
 •
  
 •
  
 •
  
 Scale with input size. 
  
 Don’t care about constants. 
  
 Asymptotic running time: polynomial time. 
  
 Problem-3
 Can we solve all problems in polynomial time? 
  
 Solution
 : 
 No
 . The answer is trivial because we have seen lots of problems which take more than polynomial 
 time. 
  
 Problem-4
 Are there any problems which are -hard? 
  
 Solution: 
 By definition, -hard implies that it is very hard. That means it is very hard to prove and to verify that 
  
 it is hard. Cook’s Theorem proves that Circuit satisfiability problem is -hard. 
  
 Problem-5
 For 
 2
 -SAT problem, which of the following are applicable? 
  
 (a) 
  
 (b) 
  
 (c)  (d) -Hard 
  
 (e) -Hard 
  
 (f) -Complete 
  
 (g) -Complete 
  
 Solution
 :  2-SAT is solvable in poly-time. So it is , , and . 
  
 Problem-6
 For 3-SAT problem, which of the following are applicable? 
  
 (a) 
  
 (b) 
  
 (c)  (d) -Hard 
  
 (e) -Hard 
  
 (f) -Complete 
  
 (g) -Complete 
  
 Solution
 : 
 3
 -SAT is NP-complete. So it is NP, NP-Hard, and NP-complete. 
  
 Problem-7
 For 
 2
 -Clique problem, which of the following are applicable? 
  
 (a) 
  
 (b) 
  
 (c)   (d) -Hard 
  
 (e) -Hard 
  
 (f) -Complete 
  
 (g) -Complete 
  
 Solution
 : 2-Clique is solvable in poly-time (check for an edge between all vertex-pairs in O(
 n
 ) time). So it is 
 , 
 , 
  
 and . 
  
 Problem-8
 For 
 3
 -Clique problem, which of the following are applicable? 
  
 (a) 
  
 (b) 
  
 (c)   (d) -Hard 
  
 (e) -Hard 
  
 (f) -Complete 
  
 (g) -Complete 
  
 Solution
 : 
 3
 -Clique is solvable in poly-time (check for a triangle between all vertex-triplets in O
 ()
  time). So it is 
  
 , , and . 
  
 20.8 Complexity Classes: Problems & Solutions 
  
 507",NA
Miscellaneous ,NA,NA
Concepts ,NA,NA
Chapter ,NA,NA
21,NA,NA
21.1 Introduction ,In this chapter we will cover the topics which are useful for interviews and exams.,NA
21.2 Hacks on Bit-wise Programming,"In  and 
  + + 
 we can work with bits effectively. First let us see the definitions of each bit operation and then move 
 onto different techniques for solving the problems. Basically, there are six operators that  and 
  + + 
 support for 
 bit manipulation: 
  
 Symbol 
  
 Operation 
  
 & 
  
 Bitwise AND 
  
 | 
  
 Bitwise OR 
  
 ^ 
  
 Bitwise Exclusive-OR 
  
 ≪
  
 Bitwise left shift 
  
 ≫
  
 Bitwise right shift 
  
 ~ 
  
 Bitwise complement 
  
 21.2.1 Bitwise AND 
  
 The bitwise AND tests two binary numbers and returns bit values of 
 1
  for positions where both numbers had a 
 one, and bit values of 
 0
  where both numbers did not have one: 
  
  
  
 01001011 
  
  
 & 
  
 00010101 
  
  
  
  
 ---------- 
  
  
  
 00000001 
  
 21.2.2Bitwise OR 
  
 The bitwise OR tests two binary numbers and returns bit values of 
 1
  for positions where either bit or both bits 
 are one, the result of 
 0
  only happens when both bits are 
 0
 : 
  
 01001011 
  
 | 
  
 00010101 
  
 ---------- 
  
 01011111 
  
 21.2.3 Bitwise Exclusive-OR 
  
 The bitwise Exclusive-OR tests two binary numbers and returns bit values of 
 1
  for positions where both bits are 
 different; if they are the same then the result is 
 0
 : 
  
  
 01001011 
  
 ^ 00010101 
  
  
  
  
 ---------- 
  
  
 01011110 
  
 21.2.4Bitwise Left Shift 
  
 The bitwise left shift moves all bits in the number to the left and fills vacated bit positions with 
 0
 . 
  
 << 2 
  
 01001011 
  
 -------- 
  
 00101100 
  
 21.1 Introduction 
  
 509",NA
21.3 Other Programming Questions ,"Problem-1
 Give an algorithm for printing the matrix elements in spiral order. 
  
 Solution
 : Non-recursive solution involves directions right, left, up, down, and dealing their corresponding 
 indices. Once the first row is printed, direction changes (from right) to down, the row is discarded by 
 incrementing the upper limit. Once the last column is printed, direction changes to left, the column is discarded 
 by decrementing the right hand limit. 
  
 func spiral(A [][]int) { 
  
  
 rowStart, rowEnd, columnStart, columnEnd := 0, len(A)-1, 0, len(A[0])-1 
  
 for { 
  
  
 // Right 
  
  
 for i := columnStart; i <= columnEnd; i++ { 
  
  
  
  
 fmt.Printf(""%d "", A[rowStart][i]) 
  
  
 } 
  
  
 rowStart++ 
  
 if rowStart > rowEnd { 
  
  
 break 
  
 } 
  
  
 // Down 
  
 for i := rowStart; i <= rowEnd; i++ { 
  
  
 fmt.Printf(""%d "", A[i][columnEnd]) 
  
 } 
  
 columnEnd-- 
  
 if columnStart > columnEnd { 
  
  
 break 
  
 } 
  
  
 // Left 
  
 for i := columnEnd; i >= columnStart; i-- { 
  
  
 fmt.Printf(""%d "", A[rowEnd][i]) 
  
 } 
  
 rowEnd-- 
  
 if rowStart > rowEnd { 
  
  
 break 
  
 } 
  
  
 // Right 
  
  
 for i := rowEnd; i >= rowStart; i-- { 
  
  
  
  
 fmt.Printf(""%d "", A[i][columnStart]) 
  
  
 } 
  
  
 columnStart++ 
  
  
 if columnStart > columnEnd { 
  
  
  
  
 break 
  
  
 } 
  
  
 } 
  
 } 
  
 func main() { 
  
  
 A := [][]int{ 
  
  
 {0, 1, 2, 3}, 
  
 /*  initializers for row indexed by 0 */ 
  
 {4, 5, 6, 7}, 
  
 /*  initializers for row indexed by 1 */ 
  
 {8, 9, 10, 11}, 
  
 /*  initializers for row indexed by 2 */ 
  
 21.3 Other Programming Questions 
  
 513",NA
References ,"[1] 
  
 [2] 
  
 [3] 
  
 [4]
  
 [5] 
  
 [6]
  
 [7] 
  
 [8] 
  
 [9] 
  
 [10] 
 [11] 
 [12]
  
 [13] 
 [14] 
 [15]
  
 [16] 
 [17]
  
 [18] 
 [19] 
 [20] 
 [21] 
 [22] 
 [23]
  
 [24] 
 [25]
  
 [26] 
 [27] 
 [28] 
 [29] 
 [30] 
 [31] 
 [32] 
 [33] 
 [34] 
 [35] 
 [36]
  
 Golang tour: https://tour.golang.org/ 
  
 Alfred V.Aho,J. E. (1983). Data Structures and Algorithms. Addison-Wesley. 
  
 Algorithms.Retrieved from cs.princeton.edu/algs4/home 
  
 Anderson., 
  
 S. 
  
 E. 
  
 Bit 
  
 Twiddling 
  
 Hacks. 
  
 Retrieved 
  
 2010, 
  
 from 
  
 Bit 
  
 Twiddling 
  
 Hacks: 
  
 graphics.stanford.edu 
  
 Bentley, J. AT&T Bell Laboratories. Retrieved from AT&T Bell Laboratories. 
  
 Bondalapati, K. Interview Question Bank. Retrieved 2010, from Interview Question Bank: 
 halcyon.usc.edu/~kiran/msqs.html 
  
 Chen. Algorithms hawaii.edu/~chenx. 
  
 Database, P.Problem Database. Retrieved 2010, from Problem Database: datastructures.net 
 Drozdek, A. (1996). Data Structures and Algorithms in C++. 
  
 Ellis Horowitz, S. S. Fundamentals of Data Structures. 
  
 Gilles Brassard, P. B. (1996). Fundamentals of Algorithmics. 
  
 Hunter., J. Introduction to Data Structures and Algorithms. Retrieved 2010, from Introduction to 
 Data Structures and Algorithms. 
  
 James F. Korsh, L. J. Data Structures, Algorithms and Program Style Using C. 
  
 John Mongan, N. S. (2002). Programming Interviews Exposed. Wiley-India. . 
  
 Judges. 
  
 Comments 
  
 on 
  
 Problems 
  
 and 
  
 Solutions. 
  
 http://www.informatik.uni-
  
 ulm.de/acm/Locals/2003/html/judge.html. 
  
 Kalid. P, NP, and NP-Complete. Retrieved from P, NP, and NP-Complete.: cs.princeton.edu/~kazad 
 Knuth., D. E. (1973). Fundamental Algorithms, volume 1 of The Art of Computer Programming. 
 Addison-Wesley. 
  
 Leon, J. S. Computer Algorithms. Retrieved 2010, from Computer Algorithms : math.uic.edu/~leon 
 Leon., J. S. Computer Algorithms. math.uic.edu/~leon/cs-mcs401-s08. 
  
 OCF. Algorithms. Retrieved 2010, from Algorithms: ocf.berkeley.edu 
  
 Parlante., N. Binary Trees. Retrieved 2010, from cslibrary.stanford.edu: cslibrary.stanford.edu 
 Patil., V. Fundamentals of data structures. Nirali Prakashan. 
  
 Poundstone., W. HOW WOULD YOU MOVE MOUNT FUJI? New York Boston.: Little, Brown and 
 Company. 
  
 Pryor, M. Tech Interview. Retrieved 2010, from Tech Interview: techinterview.org 
  
 Questions, A. C. A Collection of Technical Interview Questions. Retrieved 2010, from A Collection of 
 Technical Interview Questions 
  
 S. Dasgupta, C. P. Algorithms cs.berkeley.edu/~vazirani. 
  
 Sedgewick., R. (1988). Algorithms. Addison-Wesley. 
  
 Sells, C. (2010). Interviewing at Microsoft. Retrieved 2010, from Interviewing at Microsoft 
 Shene, C.-K. Linked Lists Merge Sort Implementation. 
  
 Sinha, P. Linux Journal. Retrieved 2010, from: linuxjournal.com/article/6828. 
  
 Structures., d. D. www.math-cs.gordon.edu. Retrieved 2010, from www.math-cs.gordon.edu 
 T. H. Cormen, C. E. (1997). Introduction to Algorithms. Cambridge: The MIT press. 
  
 Tsiombikas, J. Pointers Explained. nuclear.sdf-eu.org. 
  
 Warren., H. S. (2003). Hackers Delight. Addison-Wesley. 
  
 Weiss., M. A. (1992). Data Structures and Algorithm Analysis in C. 
  
 SANDRASI http://sandrasi-sw.blogspot.in/",NA
