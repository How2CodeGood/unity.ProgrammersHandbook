Larger Text,Smaller Text,Symbol
Mastering Social Media Mining ,NA,NA
with R,NA,NA
Extract valuable data from social media sites and make ,NA,NA
better business decisions using R,NA,NA
Sharan Kumar Ravindran,NA,NA
Vikram Garg,BIRMINGHAM - MUMBAI,NA
Mastering Social Media Mining with R,"Copyright © 2015 Packt Publishing
  
 All rights reserved. No part of this book may be reproduced, stored in a retrieval 
 system, or transmitted in any form or by any means, without the prior written 
 permission of the publisher, except in the case of brief quotations embedded in 
 critical articles or reviews.
  
 Every effort has been made in the preparation of this book to ensure the accuracy of 
 the information presented. However, the information contained in this book is sold 
 without warranty, either express or implied. Neither the authors nor Packt 
 Publishing, and its dealers and distributors will be held liable for any damages 
 caused or alleged to be caused directly or indirectly by this book.
  
 Packt Publishing has endeavored to provide trademark information about all of the 
 companies and products mentioned in this book by the appropriate use of capitals. 
  
 However, Packt Publishing cannot guarantee the accuracy of this information.
  
 First published: September 2015
  
 Production reference: 1180915
  
 Published by Packt Publishing Ltd.
  
 Livery Place 
  
 35 Livery Street 
  
 Birmingham B3 2PB, UK.
  
 ISBN 978-1-78439-631-2
  
 www.packtpub.com",NA
Credits,"Authors 
  
 Sharan Kumar Ravindran
  
 Vikram Garg
  
 Reviewers 
  
 Richard Iannone
  
 Hasan Kurban
  
 Mahbubul Majumder
  
 Haichuan Wang
  
 Commissioning Editor 
 Pramila Balan
  
 Acquisition Editor 
 Rahul Nair
  
 Content Development Editor 
 Susmita Sabat
  
 Technical Editor 
  
 Manali Gonsalves
  
 Copy Editor 
  
 Roshni Banerjee
  
 Project Coordinator
  
 Milton Dsouza
  
 Proofreader
  
 Safis Editing
  
 Indexer
  
 Priya Sane
  
 Graphics
  
 Sheetal Aute
  
 Disha Haria
  
 Production Coordinator
  
 Shantanu N. Zagade
  
 Cover Work
  
 Shantanu N. Zagade",NA
About the Authors,"Sharan Kumar Ravindran
  is a data scientist with over five years of experience. 
 He is currently working for a leading e-commerce company in India. His primary 
 interests lie in statistics and machine learning, and he has worked with customers 
 from Europe and the U.S. in the e-commerce and IoT domains.
  
 He holds an MBA degree with specialization in marketing and business analysis. He 
 conducts workshops for Anna University to train their staff, research scholars, and 
 volunteers in analytics.
  
 In addition to coauthoring 
 Social Media Mining with R
 , he has also reviewed 
 R Data 
 Visualization Cookbook
 . He maintains a website, 
 www.rsharankumar.com
 , with links 
 to his social profiles and blog.
  
 I would like to thank the R community for their generous 
 contributions.
  
 I am grateful to Mr. Derick Jose for the inspiration and opportunities 
 given to me.
  
 I would like to thank all my friends, colleagues, and family 
 members, without whom I wouldn't have learned as much.
  
 I would like to thank my dad and brother-in-law for all their support and 
 also helping me in proofreading and testing.
  
 I would like to thank my wife, Aishwarya, and my sister, Saranya, for 
 the constant motivation, and also my son, Rithik, and niece, Shravani, 
 who make every day of mine joyful and fulfilling.
  
 Most of all, I would like to thank my mother for always believing in 
 me.",NA
About the Reviewers,"Richard Iannone
  is an R enthusiast and a very simple person. Those who know him 
 (and know him well) know that this is indeed true. He has authored many R packages 
 that have achieved great success. Those who have reviewed the code know that it 
 possesses a 
 je ne sais quoi
  essence to it. In any case, the code coverage is quite adequate 
 (thanks to the many ""test parties"" he held), and he often offers builds that pass muster 
 according to Travis CI.
  
 Although he has a tendency toward modesty, others have remarked that he's just a 
 straight shooter with upper management written all over him. You know what, we 
 couldn't agree more. We bet you'll hear a 
 lot more
  about him in the near future.
  
 Hasan Kurban
  is a PhD candidate from the School of Informatics and Computing at 
 Indiana University, Bloomington. He is majoring in Computer Science and minoring in 
 Statistics. His main fields of interest are Data Mining, Machine Learning, Data Science, and 
 Statistics. He also received his master's degree in Computer Science from Indiana 
 University, Bloomington, in 2012. You can contact him at 
 hakurban@indiana.edu
 .
  
 Mahbubul Majumder
  is an assistant professor of statistics in the Department of 
 Mathematics, the University of Nebraska at Omaha (UNO). He earned his PhD in statistics 
 with specialization in data visualization and visual statistical inference from Iowa State 
 University. He had the opportunity to work with some industries dealing with data and 
 creating data products. His research interests include exploratory data analysis, data 
 visualization, and statistical modeling. He teaches data science and he is currently 
 developing a data science program for UNO.",NA
www.PacktPub.com,NA,NA
"Support files, eBooks, discount offers, and more","For support files and downloads related to your book, please visit 
 www.PacktPub.com
 .
  
 Did you know that Packt offers eBook versions of every book published, with PDF and 
 ePub files available? You can upgrade to the eBook version at 
 www.PacktPub.com 
 and 
 as a print book customer, you are entitled to a discount on the eBook copy. Get in touch 
 with us at 
 service@packtpub.com
  for more details.
  
 At 
 www.PacktPub.com
 , you can also read a collection of free technical articles, sign up for 
 a range of free newsletters and receive exclusive discounts and offers on Packt books and 
 eBooks.
  
 TM
  
 https://www2.packtpub.com/books/subscription/packtlib
  
 Do you need instant solutions to your IT questions? PacktLib is Packt's online digital 
 book library. Here, you can search, access, and read Packt's entire library of books.",NA
Why subscribe?,"• 
  
 Fully searchable across every book published by Packt
  
 • 
  
 Copy and paste, print, and bookmark content
  
 • 
  
 On demand and accessible via a web browser",NA
Free access for Packt account holders,"If you have an account with Packt at 
 www.PacktPub.com
 , you can use this to access 
 PacktLib today and view 9 entirely free books. Simply use your login credentials for 
 immediate access.",NA
Table of Contents,"Preface 
  
 v
  
 Chapter 1: Fundamentals of Mining 
  
 1
  
 Social media and its importance 
  
 Various social media platforms 
  
 Social media mining 
  
 Challenges for social media mining 
  
 Social media mining techniques 
  
 Graph mining 
  
 Text mining 
  
 The generic process of social media mining 
  
 Getting authentication from the social website – OAuth 2.0 
  
 Differences between OAuth and OAuth 2.0 
  
 Data visualization R packages 
  
  
 The simple word cloud 
  
  
 Sentiment analysis Wordcloud 
  
 Preprocessing and cleaning in R 
  
 Data modeling – the application of mining algorithms 
 Opinion mining (sentiment analysis) 
  
 Steps for sentiment analysis 
  
  
 Community detection via clustering 
  
 Result visualization 
  
 An example of social media mining 
  
 Summary 
  
 1 
  
 3 
  
 4 
  
 4 
  
 6 
  
 6 
  
 7 
  
 7 
  
 8 
  
 10 
  
 10 
  
 11 
  
 12 
  
 14 
  
 14 
  
 14 
  
 15 
  
 18 
  
 19 
  
 19 
  
 20
  
 Chapter 2: Mining Opinions, Exploring Trends, and More 
  
 with Twitter 
  
 21
  
 Twitter and its importance 
  
 Understanding Twitter's APIs 
 Twitter vocabulary 
  
 2
 1 
  
 2
 3 
  
 2
 3
  
 [
  i 
 ]",NA
Preface,"In recent times, the popularity of social media has grown exponentially and is 
 increasingly being used as a channel for mass communication, such that the brands 
 consider it as a medium of promotion and people largely use it for content sharing. 
 With the increase in the number of users online, the data generated has increased 
 many folds, bringing in the huge scope for gaining insights into the untapped gold 
 mine, the social media data.
  
 Mastering Social Media Mining with R
  will provide you with a detailed step-by-step guide 
 to access the data using R and the APIs of various social media sites, such as Twitter, 
 Facebook, Instagram, GitHub, Foursquare, LinkedIn, Blogger, and a few more networks. 
 Most importantly, this book will provide you detailed explanations of implementation of 
 various use cases using R programming; and by reading this book, you will be ready to 
 embark your journey as an independent social media analyst. This book is structured in 
 such a way that people new to the field of data mining or a seasoned professional can 
 learn to solve powerful business cases with the application of machine learning 
 techniques on the social media data.",NA
What this book covers,"Chapter 1
 , 
 Fundaments of Mining
 , introduces you to the concepts of social media 
 mining, various social media platforms, generic processes involved in accessing and 
 processing the data, and techniques that can be implemented, as well as the 
 importance, challenges, and applications of social media mining.
  
 Chapter 2
 , 
 Mining Opinions, Exploring Trends, and More with Twitter
 , focuses on steps 
 involved in collecting tweets using the Twitter API and solve business cases, such as 
 identifying the trending topics, searching tweets, collecting tweets, processing them, 
 performing sentiment analysis, exploring few business cases based on sentiment 
 analysis, and visualizing the sentiments in the form of word clouds.
  
 [
  v 
 ]",NA
What you need for this book,"In order to make your learning efficient, you need to have a computer with either 
 Windows, Mac, or Ubuntu.
  
 You need to download R to execute the codes mentioned in this book. You can 
 download and install R using the CRAN website available at 
 http://cran.r-
 project.org/
 . All the codes are written using RStudio. RStudio is an integrated 
 development environment  for R and can be downloaded from 
 http://www. 
 rstudio.com/products/rstudio/
 .
  
 In order to access the APIs of the social media, it will be necessary to create an app 
 and follow certain instructions. All of these procedures are explained in their 
 respective chapters.
  
 [
  vi 
 ]",NA
Who this book is for,"Mastering Social Media Mining with R
  is intended for those who have basic knowledge of R 
 in terms of its libraries and are aware of different machine learning techniques, or if you 
 are a data analyst and interested in mining social media data; however, there is no need to 
 have any prior knowledge of the usage of APIs of social media websites. This book will 
 make you master in getting the required social media data and transforming them into 
 actions resulting in improved business values.",NA
Conventions,"In this book, you will find a number of text styles that distinguish between different 
 kinds of information. Here are some examples of these styles and an explanation of their 
 meaning.
  
 Code words in text, database table names, folder names, filenames, file extensions, 
 pathnames, dummy URLs, user input, and Twitter handles are shown as follows: ""We 
 can include other contexts through the use of the 
 include
  directive.""
  
 A block of code is set as follows:
  
 [default]
  
 post_id<- head(page$id, n = 100)
  
 head(post_id, n=10)
  
 post_id<- as.matrix(post_id)
  
 Any command-line input or output is written as follows:
  
 # Location (Country)
  
 New terms
  and 
 important words
  are shown in bold. Words that you see on the 
 screen, for example, in menus or dialog boxes, appear in the text like this: 
 ""Clicking the 
 Next
  button moves you to the next screen.""
  
  
 Exercise to be tried by the readers and notes appear in a box 
  
  
 like this.
  
 [
  vii 
 ]",NA
Reader feedback,"Feedback from our readers is always welcome. Let us know what you think about this 
 book—what you liked or disliked. Reader feedback is important for us as it helps us 
 develop titles that you will really get the most out of.
  
 To send us general feedback, simply e-mail 
 feedback@packtpub.com
 , and mention the 
 book's title in the subject of your message.
  
 If there is a topic that you have expertise in and you are interested in either writing or 
 contributing to a book, see our author guide at 
 www.packtpub.com/authors
 .",NA
Customer support,"Now that you are the proud owner of a Packt book, we have a number of things to help 
 you to get the most from your purchase.",NA
Downloading the example code,"You can download the example code files from your account at 
 http://www.
  
 packtpub.com
  for all the Packt Publishing books you have purchased. If you 
 purchased this book elsewhere, you can visit 
 http://www.packtpub.com/support 
 and register to have the files e-mailed directly to you.",NA
Errata,"Although we have taken every care to ensure the accuracy of our content, mistakes do 
 happen. If you find a mistake in one of our books—maybe a mistake in the text or the 
 code—we would be grateful if you could report this to us. By doing so, you can save 
 other readers from frustration and help us improve subsequent versions of this book. If 
 you find any errata, please report them by visiting 
 http://www.packtpub. 
 com/submit-errata
 , selecting your book, clicking on the 
 Errata Submission Form 
 link, and entering the details of your errata. Once your errata are verified, your 
 submission will be accepted and the errata will be uploaded to our website or added to 
 any list of existing errata under the Errata section of that title.
  
 To view the previously submitted errata, go to 
 https://www.packtpub.com/books/ 
 content/support
  and enter the name of the book in the search field. The required 
 information will appear under the 
 Errata
  section.
  
 [
  viii 
 ]",NA
Piracy,"Piracy of copyrighted material on the Internet is an ongoing problem across all media. At 
 Packt, we take the protection of our copyright and licenses very seriously. If you come 
 across any illegal copies of our works in any form on the Internet, please provide us with 
 the location address or website name immediately so that we can pursue a remedy.
  
 Please contact us at 
 copyright@packtpub.com
  with a link to the suspected 
 pirated material.
  
 We appreciate your help in protecting our authors and our ability to bring you 
 valuable content.",NA
Questions,"If you have a problem with any aspect of this book, you can contact us at 
 questions@packtpub.com
 , and we will do our best to address the problem.
  
 [
  ix 
 ]",NA
Fundamentals of Mining,"Our approach in this book will be to use statistics and social science theory to mine 
 social media and we'll use R as our base programming language. We will walk you 
 through many important and recent developments in the field of social media. We'll 
 cover advanced topics such as 
 Open Authorization
  (
 OAuth
 ), Twitter's OAuth API, 
 Facebook's graph API, and so on, along with some interesting references and resources. 
 It is assumed that the target audience has a basic understanding of R, along with basic 
 concepts of social sciences.
  
 In this chapter, we will cover the following topics:
  
 • 
  
 Importance of social media mining
  
 • 
  
 Basics of social media mining
  
 • 
  
 Social media mining techniques 
  
 • 
  
 Basic data mining algorithms 
  
 • 
  
 Opinion mining
  
 • 
  
 Social recommendations",NA
Social media and its importance,"In simple terms, social media is a way of communication using online tools such as 
 Twitter, Facebook, LinkedIn, and so on. Andreas Kaplan and Michael Haenlein define 
 social media as follows:
  
 ""A group of Internet-based applications that build on the ideological and 
 technological foundations of Web 2.0 and that allow the creation and 
 exchange of user-generated content"".
  
 [
  1 
 ]",NA
Various social media platforms,"Social media is not restricted to email or chat or media sharing; it is collection of a 
 larger group of content generating platforms such as:
  
 • 
  
 Blogs
  
 • 
  
 Micro blogs
  
 • 
  
 Social news
  
 • 
  
 Social bookmarking
  
 • 
  
 Professional groups
  
 • 
  
 Community-based questions and answers
  
 • 
  
 Wikis
  
 [
  3 
 ]",NA
Social media mining,"In simple terms, social media mining is a 
 systematic analysis of information generated 
 from social media
 . It becomes necessary to tap into this enormous social media data with 
 the help of today's technology, which is not without its challenges. Data stream is a 
 prime example of Big Data. Dealing with data sets measured in petabytes is challenging, 
 and things like signal-to-noise ratio need to be taken into consideration. 
  
 It is estimated that around 20 percent of such social media data streams contain 
 relevant information.
  
 The set of tools and techniques, which are used to mine such information, are collectively 
 called Data mining technique and in the context of social media it's called 
 social media 
 mining
  (
 SMM
 ). SMM can generate insights about how much someone is influencing 
 others on the Web. SMM can help businesses identify the pain points of its customer in 
 real time. In turn, this can be used for proactive planning. Identification of potential 
 customers is a very important problem every business has been trying to solve for ages. 
 SMMs can help us identify the potential customers based on their online activities and 
 based on their friend's online activities. 
  
 There has been a lot of research in multiple disciplines of social media:
  
 • 
  
 Why does social media mining matter?
  
 • 
  
 If you can measure it, you can improve it
  
 • 
  
 Modeling behavior
  
 • 
  
 Predictive analysis
  
 • 
  
 Recommending content",NA
Challenges for social media mining,"Social media mining is currently in a stage of infancy, and its practitioners are learning 
 and developing new approaches. Social media mining draws its roots from many fields, 
 such as statistics, machine learning, information retrieval, pattern recognition, and 
 bioinformatics. The parent fields themselves are not without their challenges. The sheer 
 amount of data being generated daily is staggering, but current techniques allow for novel 
 data mining solutions and scalable computational models with help from the fundamental 
 concepts and theories and algorithms.
  
 [
  4 
 ]",NA
Social media mining techniques,"We'll go through a few of the standard social media mining techniques available. We 
 will consider examples with Facebook and Twitter as our data sources.",NA
Graph mining,"Network graphs make up the dominant data structure and appear, essentially, in all 
 forms of social media data/information. Typically, user communities constitute a group 
 of nodes in such graphs where nodes within the same community or cluster tend to 
 share common features.
  
 Graph mining can be described as 
 the process of extracting useful knowledge (patterns, 
 outliers and so on.) from a social relationship between the community members can be 
 represented as a graph
 . The most influential example of graph mining is 
 Facebook 
 Graph Search
 .
  
  
 [
  6 
 ]",NA
Text mining,"Extraction of meaning from unstructured text data present in social media is 
  
 described as text mining. The primary targets of this type of mining are blogs and micro 
 blogs such as Twitter. It's applicable to other social networks such as Facebook that 
 contain links to posts, blogs, and other news articles.",NA
The generic process of social media ,NA,NA
mining,"Any data mining activity follows some generic steps to gain some useful insights from 
 the data. Since social media is the central theme of this book, let's discuss these steps by 
 taking example data from Twitter:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Getting authentication from the social website 
  
 Data visualization 
  
 Cleaning and preprocessing 
  
 Data modeling using standard algorithms such as opinion mining, clustering, 
 anomaly/spam detection, correlations and segmentations, recommendations 
 Result visualization
  
 [
  7 
 ]",NA
Getting authentication from the social website ,NA,NA
– OAuth 2.0,"Most social media websites provide API access to their data. To do the mining, we (as a 
 third-party) would need some mechanism to get access to users' data, available on these 
 websites. But the problem is that a user will not share their credentials with anyone due 
 to obvious security reasons. This is where OAuth comes in the picture. According to its 
 home page (
 http://oauth.net/
 ), OAuth can be defined as follows:
  
 An open protocol to allow secure authorization in a simple and standard 
 method from web, mobile and desktop applications.
  
 To understand it better, let's take an example of Instagram where a user can 
 allow a printing service access to his/her private photographs stored on 
 Instagram's server, without sharing her credentials with the printing service. 
  
 Instead, they authenticate directly with Instagram, which issues the printing service 
 delegation-specific permissions. The user here is the primary owner of the resource and 
 the printing service is the third-party client. Social media websites such as Instagram, 
 Twitter, and Facebook allow various applications to access user data for various 
 advertisements or recommendations. Almost all cab service applications access user 
 location.
  
 Here's a diagram illustrating the concept:
  
  
 [
  8 
 ]",NA
Differences between OAuth and OAuth 2.0,"Here are some of the major differences:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 More flows in OAuth 2.0 to permit improved support for non-browser 
 based apps
  
 OAuth 2.0 does not need the client app to have cryptography
  
 OAuth 2.0 offers much less complicated signatures
  
 OAuth 2.0 generates 
 short-lived
  access tokens, hence it is more secure
  
 OAuth 2.0 has a clearer segregation of roles concerning the server responsible for 
 handling user authorization and the server handling OAuth requests",NA
Data visualization R packages,"A number of visualization R packages for text data are available as R package. 
  
 These libraries, based on available data and objective, provide various options 
 varying from simple clusters of words to the one inline with semantic analysis or 
 topic modeling of the corpus. These libraries provide means to better understand 
 text data. In this book, we'll use the following libraries:
  
 [
  10 
 ]",NA
The simple word cloud,"One of the simplest and most frequently used visualization libraries is the 
 simple 
 word cloud
 . The basic intent to using word cloud is to visualize the weights of the 
 words present. The ""wordcloud"" R library helps the user get an understanding of 
 weights of a word/term with respect to the tf-idf matrix. The weights are 
 proportional to the size and color of the word you see in the plot. Here's an example 
 of one such simple word cloud based on the corpus created from tweets:
  
  
 [
  11 
 ]",NA
Sentiment analysis Wordcloud,"There are R packages that can generate a word cloud similar to the preceding figure, 
 along with the sentiments each word is representing. Such plots are one step ahead of 
 the basic word cloud because they let the user get an understanding of what kind of 
 sentiments are present and why the particular documents (collection of tweets) are of a 
 particular nature (joy, sadness, disgust, love, and so on.). Timothy Jurka developed one 
 such package, which we are going to use. The two main functions of this package are as 
 follows:
  
 • 
  
 Classify_emotion
 : As the name suggests, the procedure helps the user 
 understand the type of sentiment that is present. This procedure also clusters 
 the words present in the query based on the sentiment and level of emotions 
 that particular word present. A voting-based classification is one the algorithms 
 used in this particular procedure. The Naive Bayes algorithm is also used for 
 more enhanced results. The training dataset used on the above algorithms is 
 from Carlo Strapparava and Alessandro Valitutti. 
  
 Here's a sample output:
  
  
 [
  12 
 ]",NA
Preprocessing and cleaning in R,"Preprocessing and cleaning are the very basic and first steps in any data-mining 
 problem. A learning algorithm on a unified and cleaned dataset cannot only run very 
 fast, but can also produce more accurate results. The first steps involve the annotation 
 of target data, in the case of classification problems and understating the feature vector 
 space, to apply an appropriate distance measure for clustering problems. Identification 
 of noise samples and their clean up is a very tricky task but the better it's done, the more 
 accuracy one can expect in the results. As mentioned previously, you need to be careful 
 in cleaning tasks as this can lead to a rejection of good samples. Furthermore, the 
 preprocessing steps need to be a reversible process because at the end of the exercise, 
 the results need to be processed back to the original sample space for it to make sense.",NA
Data modeling – the application of mining ,NA,NA
algorithms,Let's look at some of the standard mining algorithms.,NA
Opinion mining (sentiment analysis),"In simple words, opinion mining or sentiment analysis is the method in which we try to 
 assess the opinion/sentiment present in the given phrase. The phrase could be any 
 sentence. Though our examples would be English, the sentiment analysis is not limited 
 to any language. Also, the sentence could come from any source—it could be a 140-
 character tweet, Facebook post/chats, SMSs, and so on. Consider the following 
 examples:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Visiting to the wonderful places in Europe. Feeling real happy—Positive. I 
 love little sunshine in winters, make me feel live—Positive.
  
 I am stuck in a same place, feeling sad—Negative.
  
 The cab driver was a nice person. Think many of them are actually good 
 people—Positive.
  
 [
  14 
 ]",NA
Steps for sentiment analysis,"A belief or an opinion or sentiment to a computer can be described as a 
 quintuple
 ; that is an object in a five dimensional space, where each axis 
 represents the following:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 O
 j
 : This is the objective (that is, product). It is realized via named 
 entity extraction.
  
 f
 jk
 : This is a feature of O
 j
 . It is assessed using information mining theory 
 SO
 ijkl
 :This is the sentiment value of the opinion of the opinion holder hi on 
 feature f
 jk
  of object o
 j
  at time t
 l 
  
 h
 i
 : This is the information miner 
  
 T
 i
 : This is for data extraction
  
 Perform the following steps to get the sentiment value SO
 ijkl
 :
  
 1. 
 Part-of-speech tagging
  (
 pos
 ) means the term in the text (or the sentence) 
 that are marked using a pos-tagger so that it allocates a label to each term, 
 allowing the system to do something with it.
  
 2. We look at 
 sentiment orientation
  (
 SO
 ) of the patterns we mined. For 
 example, we may have extracted 
 Remarkable + Handset
 , which is, 
 [JJ] + 
 [NN]
  (or adjective trailed by noun). The opposite might be ""Awful"" for 
 instance. In this phase, the system attempts to position the terms on an 
 emotive scale.
  
 3. The average sentiment orientation of all the terms we gathered is computed. 
  
 This allows the system to say something like:
  
 °
  
 °
  
 ""Usually individuals like the fresh 
 Handset
 ."" They recommend it 
 ""Usually individuals hate the fresh 
 Handset
 ."" They don't 
  
 recommend it
  
 [
  15 
 ]",NA
Community detection via clustering,"In graph analogy, a community is a set of nodes between which the 
  
 communications/ interactions are rather more frequent than with those outside the set. 
 From a marketing point of view, community detection become very crucial and has been 
 proven to be very rewarding in terms of 
 return-of-investments
  (
 ROIs
 ). For example, 
 travel enthusiasts can be identified on various social media websites based on their 
 visited places, posts, comments, tweets, and so on. If such segmentation can be done, 
 then selling them some product related to travel (such as a handheld compass, travel 
 pillow, global alarm clock, binoculars, slim digital camera, 
  
 noise-cancelling headphones, and so on) would stand a higher chance of 
  
 purchase. Hence, with a focused marketing effort, the business can get 
  
 more ROIs.
  
 While spam detection is a supervised machine-learning task, community detection or 
 clustering falls under the class of unsupervised learning algorithms. Social media offers 
 two types of communities. Some are explicitly created groups with people of common 
 location, hobbies, or occupation. There are several other people who might not be 
 connected to such groups. Identification of these people is a clustering task. This is 
 performed based on their interaction (for example, they mentioned a common thing in 
 their comments/posts/tweets) as features sets (x
 i
 's) and without label information (as 
 in the case of supervised machine learning algorithms). These features are passed to 
 various unsupervised machine learning algorithms to find the commonalities and hence 
 the communities. Many algorithms also provide the extent/degree/affinity score with 
 which a particular person belongs to a specific community.
  
 There are many algorithms and techniques proposed in academia that we'll discuss in 
 detail in the following chapters. Basically, these methods are based on calculation of the 
 influence on the link between various edges (people, locations, and other such entities). 
 Similar people are likely to be linked, and edges between these links indicate that linked 
 users will influence each other and become more similar, two users in the same group 
 or community if they have higher similarity.
  
 [
  18 
 ]",NA
Result visualization,"Visualization helps one understand more about the data in hand. 
 A picture is worth a 
 thousand words
 . We get a better understanding of the feature space by representing data 
 on a graphical platform. Trends, anomalies, relationships, and other similar patterns 
 help us think more about the possible algorithm and heuristics to use on the given data 
 for a given problem. There can be various levels of abstraction and granularities present 
 in the data. Here's a list of a few standard methods used to visualize data:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Boxplots 
  
 Scatter plots 
  
 Word clouds 
  
 Decision trees 
  
 Various social networks analysis tools such as Igraph, MuxViz, NetworkX, and 
 so on
  
 In the next chapters, we'll show you how these help us understand the results better. 
 How to interpret the results is a crucial part of the mining process.",NA
An example of social media mining,"Let's look at a few examples of well-known social media sites:
  
 Twitter
  
 • 
  
 What are people talking about right now?
  
 • 
  
 Mining entities from user's tweets
  
 • 
  
 Sentiment analysis
  
 Facebook
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Gender analysis of Facebook post likes 
  
 Analysis of Facebook friends network 
  
 Inferring community behavior dynamically 
  
 Fraud prevention 
  
 Questions such as ""Who influences whom?"" 
  
 Getting peoples' interest based on their chat history, such as with whom they are 
 chatting, what they are chatting, where they are chatting, and so on.
  
 [
  19 
 ]",NA
Summary,"In this chapter, we tried to familiarize the user with the concept of social media and 
 mining.
  
 We discussed the OAuth API, which offers a technique for clients to allow 
 third-party entry to their resources without sharing their credentials. It also 
 offers a way to grant controlled access in terms of scope and duration.
  
 We saw examples of various R packages available to visualize the text data. 
  
 We discussed innovative ways to analyze and study the text data via plots. The 
 application of sentiment analysis along with topic mining was also discussed in the 
 same sections. To many, it's a new way to look at these kinds of data. Historically, 
 people have used plots to plot numerical data, but plotting words on 2D graphs is very 
 new. People have made more advances than 2D plots. With Facebook and LinkedIn, 
 the Gephi library allows visualizing the social networks in 3D.
  
 Next, you learned the basic steps of any data-mining problem along with various 
 machine learning algorithms. We'll see the applications of many of these algorithms in 
 the coming chapters. We briefly talked about sentiment analysis, anomaly detection, and 
 various community detection algorithms. So far, we have not gone deep into any of the 
 algorithms, but will dive into them in the later chapters.
  
 In the next chapter, we will apply the knowledge gained so far to mine Twitter and 
 give detailed information of the methods and techniques used there.
  
 [
  20 
 ]",NA
"Mining Opinions, Exploring ",NA,NA
"Trends, and More with Twitter","Our approach in this book is to use statistics and social science theory to mine social 
 media, and we'll use R as our base programming language.
  
 In this chapter, we will cover the following:
  
 • 
  
 • 
  
 • 
  
 Twitter and its importance 
  
 Getting hands-on with Twitter's data and using various Twitter APIs Use of 
 data to solve business problems—comparison of various businesses based on 
 tweets",NA
Twitter and its importance,"Twitter can be considered an extension of the short messages service, or SMS, but on an 
 Internet-based platform. In the words of Jack Dorsey, co-founder and co-creator of 
 Twitter:
  
 ""...We came across the word 'twitter', and it was just perfect. The definition was 
 'a short burst of inconsequential information,' and 'chirps from birds'. And that's 
 exactly what the product was.""
  
 [
  21 
 ]",NA
Understanding Twitter's APIs,"Twitter APIs provide a means to access the Twitter data; that is, tweets sent by its 
 millions of users. Let's get to know these APIs a bit better.",NA
Twitter vocabulary,"As described earlier, Twitter is a microblogging service with a social aspect. It allows its 
 users to express their views/sentiments through an Internet SMS, called ""tweets"" in the 
 context of Twitter. These tweets are entities formed of maximum of 140 characters. The 
 content of these tweets can be anything ranging from a person's mood to person's 
 location to a person's curiosity. The platform on which these tweets are posted is called a 
 Timeline
 . To use Twitter's APIs, one must understand the basic terminology.
  
 Tweets are the crux of Twitter. Theoretically, a tweet is just 140 characters of text 
 content tweeted by a user, but there is more to it than just that. There is more 
 metadata associated with the same tweet, which are classified by Twitter as entities 
 and places:
  
 • 
  
 • 
  
 The entities consist of hashtags, URLs, and other media data that users have 
 included in their tweets.
  
 The places are nothing but the locations from which the tweets originate. It is 
 possible the place is the real world location from which the tweet was sent, or it 
 could be a location mentioned in the text of the tweet.
  
 Take the following tweet as an example:
  
 Learn how to consume millions of tweets with @twitterapi at #TDC2014 in São 
 Paulo #bigdata tomorrow at 2:10pm http://t.co/pTBlWzTvVd
  
 The preceding tweet was tweeted by 
 @TwitterDev
  and it's about 132 characters long. 
 The following are the entities mentioned in this tweet:
  
 • 
  
 Handle
 : 
 @twitterapi
  
 • 
  
 Hashtags
 : 
 #TDC2014
 , 
 #bigdata
  
 • 
  
 URL
 : 
 http://t.co/pTBlWzTvVd
  
 [
  23 
 ]",NA
Creating a Twitter API connection,"We need to have an app created at 
 https://dev.twitter.com/apps
  before making any 
 API requests to Twitter. It's a standard method for developers to gain API access, and, 
 more importantly, it helps Twitter to observe and restricts developer from making high 
 load API requests.
  
 The 
 ROAuth
  package is the one we are going to use in our experiments. Recall that in 
 Chapter 1
 , 
 Fundamentals of Mining
 , we discussed a lot about the 
 OAuth
  protocol to for 
 obtaining tokens. These tokens allow users to authorize third-party apps to access the 
 data from any user account without the need to have their passwords (or other sensitive 
 information). 
 ROAuth
  basically facilitates the same thing.
  
 [
  24 
 ]",NA
Creating a new app,"The first step toward getting any kind of token access from Twitter is to create an 
 app on it. You have to go to 
 https://dev.twitter.com/
  and log in with your 
 Twitter credentials. Having logged in using your credentials, the step for creating an 
 app are as follows:
  
 1. Go to 
 https://apps.twitter.com/app/new
 .
  
 2. Put the name of your application in the 
 Name
  field. This name can be 
  
 anything you like.
  
 3. Similarly, enter the description in the 
 Description
  field.
  
 4. The 
 Website
  field needs to be filled with a valid URL, but, again, 
  
 that 
 can be any random URL.
  
 5. You can leave the 
 Callback URL
  field blank.
  
  
 [
  25 
 ]",NA
Finding trending topics ,"Now that you understand how to create API connections to Twitter and fetch data using 
 it, we will look at how to get answer to 
 what is trending on Twitter
  to list what topic 
 (worldwide or local) is being talked about the most right now. Using the same API, we 
 can easily access the trending information:
  
 #return data frame with name, country & woeid.
  
 Locs <- availableTrendLocations() 
  
 # Where woeid is a numerical identification code describing a location 
 ID
  
 # Filter the data frame for Delhi (India) and extract the woeid of the 
 same 
  
 LocsIndia = subset(Locs, country == ""India"") 
  
 woeidDelhi = subset(LocsIndia, name == ""Delhi"")$woeid
  
 # getTrends takes a specified woeid and returns the trending topics 
 associated with that woeid 
  
 trends = getTrends(woeid=woeidDelhi)
  
 The function 
 availableTrendLocations()
  returns R dataframe containing the 
 name
 , 
 country
 , and 
 woeid
  parameters. We then filter this data frame for a location of our 
 choosing; in this example, its 
 Delhi, India
 . The function 
 getTrends()
  fetches the top 10 
 trends in the location determined by the 
 woeid
 .
  
 Here are the top four trending hashtags in the region defined by 
 woeid = 20070458
 , that 
 is, 
 Delhi, India
 :
  
 head(trends) 
  
 name         url                   query                woeid 
  
 1 #AntiHinduNGOsExposed 
 http://twitter.com/search?q=%23AntiHinduNGOsEx posed 
 %23AntiHinduNGOsExposed 20070458 
  
 2           #KhaasAadmi           
 http://twitter.com/search?q=%23Khaas Aadmi           %23KhaasAadmi 
  
 20070458 
  
 3            #WinGOSF14            
 http://twitter.com/search?q=%23WinG OSF14            %23WinGOSF14 
  
 20070458 
  
 4     #ItsForRealONeBay     http://twitter.com/search?q=%23ItsForReal 
 ONeBay 
  
 %23ItsForRealONeBay 20070458
  
 [
  28 
 ]",NA
Searching tweets,"Similar to trends, there is one more important function that comes with the 
 TwitteR 
 package: 
 searchTwitter()
 . This function will return tweets containing the searched 
 string, along with the other constraints. Some of the constraints that can be imposed are 
 as follows:
  
 • 
  
 • 
  
 • 
  
 lang
 : This constraints the tweets of given language
  
 since/until
 : This constraints the tweets to be 
 since
  the given date or 
 until 
 the given date
  
 geocode
 : This constraints tweets to be from only those users who are located 
 within a certain distance from the given latitude/longitude
  
 For example, extracting tweets about the cricketer 
 Sachin Tendulkar
  in the month of 
 November 2014 returns the following:
  
 head(searchTwitter('Sachin Tendulkar', since='2014-11-01', 
 until= '2014-11-30'))
  
 [[1]]
  
 [1] ""TendulkarFC: RT @Moulinparikh: Sachin Tendulkar had a long 
 session with the Mumbai Ranji Trophy team after today's loss.""
  
 [[2]]
  
 [1] ""tyagi_niharika: @WahidRuba @Anuj_dvn @Neel_D_ @alishatariq3 
 @VWellwishers @Meenal_Rathore oh... Yaadaaya....hmaraesachuuu 
 sir\xed\xa0\xbd\xed\xb8\x8d..i mean sachin Tendulkar""
  
 [[3]]
  
 [1] ""Meenal_Rathore: @WahidRuba @Anuj_dvn @tyagi_niharika @Neel_D_ 
 @alishatariq3 @AliaaFcc @VWellwishers .. Sachin Tendulkar 
  
 \xed\xa0\xbd\xed\xb8\x8a
 ☺️
 ""
  
 [[4]]
  
 [1] ""MishraVidyanand: Vidyanand Mishra is following the Interest 
 \""The Living Legend SachinTendu...\"" on http://t.co/tveHXMB4BM - 
 http://t.co/CocNMcxFge""
  
 [[5]]
  
 [1] ""CSKalwaysWin: I have never tried to compare myself to anyone 
 else.\n - Sachin Tendulkar""
  
 [
  29 
 ]",NA
Twitter sentiment analysis,"Depending on the objective, and based on the functionality to search any type of tweets 
 from the public timeline, one can always collect the required corpus. For example, you 
 may want to learn about customer satisfaction levels with various cab services, which 
 are up and coming in the Indian market. These start-ups are offering various discounts 
 and coupons to attract customers, but at the end of the day, the service quality 
 determines the business of any organization. These start-ups are constantly promoting 
 themselves on various social media websites. Customers are showing various 
 sentiments on the same platform.
  
 Let's target the following:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Meru Cabs
 : A radio cabs service based in Mumbai, India, launched in 2007
  
 Ola Cabs
 : A taxi aggregator company based in Bangalore, India, launched in 
 2011
  
 TaxiForSure
 : A taxi aggregator company based in Bangalore, India, launched in 
 2011
  
 Uber India
 : A taxi aggregator company headquartered in San Francisco, 
 California, launched in India in 2014
  
 Let's make it our goal to get the general sentiments about each of the preceding service 
 providers based on the customer sentiments present in the tweets on Twitter.",NA
Collecting tweets as a corpus,"We'll start with the 
 searchTwitter()
 function (discussed previously) on the 
 TwitteR
  package to gather the tweets for each of the preceding organizations.
  
 Now, in order to avoid writing the same code again and again, we push the 
 following authorization code in the file called 
 authenticate.R
 :
  
 library(twitteR)
  
 api_key<- ""xx""
  
 api_secret<- ""xx""
  
 access_token<- ""xx""
  
 access_token_secret<- ""xx""
  
 setup_twitter_oauth(api_key,api_secret,access_token, 
 access_token_secret)
  
 [
  30 
 ]",NA
Cleaning the corpus,"Before applying any intelligent algorithms to gather more insights from the tweets 
 collected so far, let's first clean the corpus. In order to clean up, we need to 
 understand what the list of tweets looks like:
  
 head(Meru_tweets)
  
 [[1]]
  
 [1] ""MeruCares: @KapilTwitts 2&gt;...and other details at 
 feedback@merucabs.com We'll check back and reach out soon.""
  
 [[2]]
  
 [1] ""vikasraidhan: @MeruCabs really disappointed with @GenieCabs. 
 Cab is never assigned on time. Driver calls after 30 minutes. Why 
 would I ride with Meru?""
  
 [[3]]
  
 [1] ""shiprachowdhary: fallback of #ubershame , #MERUCABS taking 
 customers for a ride""
  
 [[4]]
  
 [1] ""shiprachowdhary: They book Genie, but JIT inform of 
 cancellation &amp; send full fare #MERUCABS . Very 
  
 disappointed.Always used these guys 4 and recommend 
 them.""
  
 [[5]]
  
 [1] ""shiprachowdhary: No choice bt to take the #merucabs premium 
 service. Driver told me that this happens a lot with #merucabs.""
  
 [
  32 
 ]",NA
Estimating sentiment (A),"There are many sophisticated resources available for estimating sentiments. 
  
 Many research papers and software packages are available open source, and they 
 implement very complex algorithms for sentiment analysis. After getting the 
 cleaned Twitter data, we are going to use few such R packages to assess the 
 sentiments in the tweets.
  
 It's worth mentioning here that not all the tweets represent sentiments. A few 
 tweets can be just information/facts, while others can be customer care 
 responses. Ideally, they should not be used to assess the customer sentiment 
 about a particular organization.
  
 As a first step, we'll use a Naïve algorithm, which gives a score based on the number of 
 times a positive or a negative word occurred in the given sentence (and, in our case, in a 
 tweet).
  
 Please download the positive and negative opinion/sentiment (nearly 68, 000) words 
 from English language. This 
 opinion lexicon
  will be used as a first example in our 
 sentiment analysis experiment. The good thing about this approach is that we are relying 
 on highly researched, and at the same time customizable, input parameters. 
  
 Here are a few examples of existing positive and negative sentiment words:
  
 • 
  
 Positive
 : Love, best, cool, great, good, and amazing
  
 • 
  
 Negative
 : Hate, worst, sucks, awful, and nightmare
  
 >opinion.lexicon.pos = 
  
 scan('opinion-lexicon-English/positive-words.txt', 
 what='character', comment.char=';')
  
 >opinion.lexicon.neg = 
  
 scan('opinion-lexicon-English/negative-words.txt', 
 what='character', comment.char=';')
  
 > head(opinion.lexicon.neg)
  
 [1] ""2-faced""    ""2-faces""    ""abnormal""   ""abolish"" 
 ""abominable"" ""abominably""
  
 > head(opinion.lexicon.pos)
  
 [1] ""a+""         ""abound""     ""abounds""    ""abundance""  ""abundant"" 
 ""accessable""
  
 We'll add a few industry-specific and/or especially emphatic terms based on our 
 requirements:
  
 pos.words = c(opinion.lexicon.pos,'upgrade')
  
 neg.words = c(opinion.lexicon.neg,'wait', 
  
 'waiting', 'wtf', 'cancellation')
  
 [
  35 
 ]",NA
Estimating sentiment (B),"Let's now move one step further. Instead of using simple matching of opinion lexicon, 
 we'll use something called 
 Naive Bayes
  to decide on the emotion present in any tweet. 
 We will require packages called 
 Rstem
  and 
 sentiment
  to assist with this. It's 
 important to mention here that both these packages are no longer available in CRAN, 
 so we have to provide the repository location as a parameter 
 install.
  
 package()
  function. Here's the R script to install the required packages:
  
 install.packages(""Rstem"", 
  
 repos = ""http://www.omegahat.org/R"", type=""source"")
  
 require(devtools)
  
 install_url(""http://cran.r-project.org/src/contrib/Archive/sentiment/ 
 sentiment_0.2.tar.gz"") require(sentiment)
  
 ls(""package:sentiment"")
  
 Now that we have the 
 sentiment
  and 
 Rstem
  packages installed in our R workspace, we 
 can build the 
 bayes
  classifier for sentiment analysis:
  
 library(sentiment)
  
 # classify_emotion function returns an object of class data frame # 
 with seven columns (anger, disgust, fear, joy, sadness, surprise,  # 
 # best_fit) and one row for each document:
  
 MeruTweetsClassEmo = classify_emotion(MeruTweetsCleaned, 
 algorithm=""bayes"", prior=1.0)
  
 OlaTweetsClassEmo = classify_emotion(OlaTweetsCleaned, 
 algorithm=""bayes"", prior=1.0)
  
 TaxiForSureTweetsClassEmo = 
  
 classify_emotion(TaxiForSureTweetsCleaned, algorithm=""bayes"", 
 prior=1.0)
  
 UberTweetsClassEmo = classify_emotion(UberTweetsCleaned, 
 algorithm=""bayes"", prior=1.0)
  
 [
  39 
 ]",NA
Summary,"In this chapter, you gained knowledge of the various Twitter APIs. We discussed how to 
 create a connection with Twitter, and we saw how to retrieve tweets with various 
 attributes. We saw the power of Twitter in helping us determine customers' attitudes 
 toward today's various businesses. The activity can be done on a weekly basis, and one 
 easily get the monthly, quarterly, or yearly changes in customer sentiment. This can not 
 only help the customer decide the trending business, but also the business itself can get a 
 well-defined metric of its own performance. It can use such scores/graphs to improve. We 
 also discussed various methods of sentiment analysis, varying from basic word-matching 
 the advanced Bayesian algorithms. In the next chapter, we will apply a similar analysis to 
 Facebook.
  
 [
  54 
 ]",NA
Find Friends on Facebook,"There is no need for an introduction about Facebook. It's a huge source of 
  
 information, we are connected to a lot of people, and we keep following the things that 
 happen in our network. We get to know not only about the people in our network but 
 also about places, movies, companies, and so on. With over 1.44 billion active users 
 monthly, Facebook is also used as a medium to make promotions. It is being used by 
 individuals, companies, news channels, and so on. Let's see what we can do using the 
 Facebook Graph API.
  
 In this chapter, we will see how to use the R package 
 Rfacebook
 , which provides access 
 to the Facebook Graph API from R. It includes a series of functions that allow us to 
 extract various data about our network such as friends, likes, comments, followers, 
 newsfeeds, and much more.
  
 The idea behind the chapter is to learn how to pull the data from our network and use 
 suitable techniques to convert that data into valuable information that can be used to 
 solve a problem or a business case. We will discuss how to visualize our Facebook 
 network and we will see some methodologies to make use of the available data to 
 implement business cases, such as identifying the influential persons in a network, 
 methods to detect a spam post, and finally recommend your friends what they might be 
 interested in based on your network information.
  
 In this chapter, we will cover the following topics:
  
 • 
  
 Creating an app on the Facebook platform
  
 • 
  
 Installation and authentication of the 
 Rfacebook
  package
  
 • 
  
 Basic analysis of your network
  
 [
  55 
 ]",NA
Creating an app on the Facebook ,NA,NA
platform,"In this section, we will cover the steps involved in creating a Facebook app to 
 connect to the Facebook Graph API. Version 2 of the API deprecated a lot of 
 features compared to Version 1, but still Facebook provides access to a lot of 
 public data that can be used to solve various use cases.
  
 In order to create a Facebook app, go to 
 https://developers.facebook.com/
 , 
 register as a Facebook developer, click on the 
 My Apps
  option, and then choose 
 Add a New App
 .
  
 [
  56 
 ]",NA
Rfacebook package installation and ,NA,NA
authentication,"The 
 Rfacebook
  package is authored and maintained by Pablo Barbera and Michael 
 Piccirilli. It provides an interface to the Facebook API. It needs Version 2.12.0 or later of 
 R and it is dependent on a few other packages, such as 
 httr
 , 
 rjson
 , and 
 httpuv
 . 
  
 Before starting, make sure those packages are installed. It is preferred to have 
 Version 0.6 of the 
 httr
  package installed.",NA
Installation,"We will now install the 
 Rfacebook
  packages. We can download and install the latest 
 package from GitHub using the following code and load the package using the 
 library
  function. On the other hand, we can also install the 
 Rfacebook 
 package 
 from the CRAN network. One prerequisite for installing the package using the 
 function 
 install_github
  is to have the package 
 devtools
  loaded into the R 
 environment. The code is as follows:
  
 install.packages(""devtools"") 
  
 library(devtools) 
  
 install_github(""Rfacebook"", ""pablobarbera"", subdir=""Rfacebook"") 
 library(Rfacebook)
  
 After installing the 
 Rfacebook
  package for connecting to the API, make an 
  
 authentication request. This can be done via two different methods. The first method is 
 by using the access token generated for the app, which is short-lived (valid for two 
 hours); on the other hand, we can create a long-lasting token using the 
 OAuth
  function.
  
 Let's first create a temporary token. Go to 
 https://developers.facebook.com/ 
 tools/explorer
 , click on 
 Get Token
 , and select the required user data permissions.
  
  
 [
  58 
 ]",NA
A closer look at how the package works,"The 
 getUsers
  function using the token will hit the Facebook Graph API. Facebook will 
 be able to uniquely identify the users as well as the permissions to access information. 
 If all the check conditions are satisfied, we will be able to get the required data.
  
 Copy the token from the mentioned URL and paste it within the double quotes. 
  
 Remember that the token generated will be active only for two hours. Use the 
 getUsers
  
 function to get the details of the user. Earlier, the 
 getUsers
  function used to work based 
 on the Facebook friend's name as well as ID; in API Version 2.0, we cannot access the 
 data using the name. Consider the following code for example:
  
 token<- ""XXXXXXXXX""
  
 me<- getUsers(""778278022196130"", token, private_info = TRUE)
  
 Then, the details of the user, such as name and hometown, can be retrieved using the 
 following code:
  
 me$name
  
 The output is also mentioned for your reference:
  
 [1] ""Sharan Kumar R""
  
 For the following code:
  
 me$hometown
  
 [
  59 
 ]",NA
A basic analysis of your network,"In this section, we will discuss how to extract Facebook network of friends and 
 some more information about the people in our network.
  
 After completing the app creation and authentication steps, let's move forward and 
 learn to pull some basic network data from Facebook. First, let's find out which 
 friends we have access to, using the following command in R. Let's use the 
 temporary token for accessing the data:
  
 token<- ""XXXXXXXXX""
  
 friends<- getFriends(token, simplify = TRUE)
  
 head(friends) # To see few of your friends
  
 The preceding function will return all our Facebook friends whose data is accessible. 
 Version 1 of the API would allow us to download all the friends' data by default. But in 
 the new version, we have limited access. Since we have set 
 simplify
  as 
 TRUE
 , we will 
 pull only the username and their Facebook ID. By setting the same parameter to 
 FALSE
 , 
 we will be able to access additional data such as gender, location, hometown, profile 
 picture, relationship status, and full name.
  
 We can use the function 
 getUsers
  to get additional information about a particular user. 
 The following information is available by default: gender, location, and language. 
  
 We can, however, get some additional information such as relationship status, 
 birthday, and the current location by setting the parameter 
 private_info
  to 
 TRUE
 :
  
 friends_data<- getUsers(friends$id, token, private_info = TRUE)
  
 table(friends_data$gender)
  
 The output is as follows:
  
 female   male
  
  5     21
  
 We can also find out the language, location, and relationship status. The 
 commands to generate the details as well as the respective outputs are 
 given here for your reference:
  
 #Language
  
 table(substr(friends_data$locale, 1, 2))
  
 The output is as follows:
  
 en
  
 26
  
 [
  62 
 ]",NA
Network analysis and visualization,"So far, we used a few functions to get the details about our Facebook profile as well as 
 friends' data. Let's see how to get to know more about our network. Before learning to 
 get the network data, let's understand what a network is as well as a few important 
 concepts about the network.
  
 Anything connected to a few other things could be a network. Everything in real life 
 is connected to each other, for example, people, machines, events, and so on. It would 
 make a lot of sense if we analyzed them as a network. Let's consider a network of 
 people; here, people will be the nodes in the network and the relationship between 
 them would be the edges (lines connecting them).",NA
Social network analysis,"The technique to study/analyze the network is called social network analysis. We 
 will see how to create a simple plot of friends in our network in this section.
  
 To understand the nodes (people/places/etc) in a network in social network analysis, 
 we need to evaluate the position of the nodes. We can evaluate the nodes using 
 centrality. Centrality can be measured using different methods like degree, 
 betweenness, and closeness. Let's first get our Facebook network and then get to know 
 the centrality measures in detail.
  
 [
  64 
 ]",NA
Degree,"The number of direct connections a node has in a network is called the 
 degree
 . A higher 
 degree means that the node is connected to a lot of other nodes, which makes the nodes 
 with a higher degree very important for various business cases. Degree can be further 
 classified into in-degree and out-degree. To know more about the concept of networks, 
 refer to the course 
 Social Network Analysis
  by Lada Adamic available at coursera, which 
 can be found at 
 https://www.coursera.org/course/sna
 . 
  
 Consider the following code to measure the degree of a network:
  
 # MeasuringDegree for a network
  
 degree(social_graph, v=V(social_graph), mode = 
  
 c(""all"", ""out"", ""in"", ""total""),loops = TRUE, normalized = FALSE)
  
 degree.distribution(social_graph, cumulative = FALSE)
  
 [
  66 
 ]",NA
Betweenness,"Betweenness is also a concept of centrality. It is calculated based on how many pairs of 
 individuals (other nodes in the network) would have to go through you (node for which 
 it is calculated) in order to reach one another in the minimum number of hops. 
  
 The node with higher Betweenness will have a greater influence in the flow of the 
 information. Consider the following code to measure Betweenness:
  
 #Measuring Betweenness
  
 betweenness(social_graph, v=V(social_graph), directed = TRUE, 
 weights = NULL, nobigint = TRUE, normalized = FALSE)
  
 If our network is a directed one, then we need to set the parameter 
 directed
  as 
 TRUE
 . The preceding function measures the Betweenness for all the nodes in the 
 network. If you would like to measure the Betweenness manually, it can be done 
 using this formula:
  
  
 The output to the preceding function will also be very similar to the 
 degree
  function, but 
 here we will see the Betweenness value for each of our friends in our network.
  
 [
  67 
 ]",NA
Closeness,"Closeness
  is also a measure of centrality; how central you are (node for which it is 
 calculated) depends on the length of the average shortest path between the measuring 
 node and all other nodes in the network. The nodes with high closeness are very 
 important because they are in an excellent position to monitor what's happening in the 
 network, that is, nodes with highest visibility. This measure might not be of much use 
 when our network has many disconnected components. Consider the following code to 
 measure closeness:
  
 # Measuring Closeness
  
 closeness(social_graph, vids=V(social_graph), mode = c(""out"", 
 ""in"", ""all"", ""total""), weights = NULL, normalized = FALSE)
  
 The preceding function measures the closeness for all the nodes in the network. 
 You can measure the closeness manually as well using the following formula:
  
  
 The output to the preceding function will also be very similar to the degree function, but 
 here we will see the closeness score for each of our friends in our network.",NA
Cluster,"Cluster is a measure of extent to which the nodes in the network tend to cluster 
 with each other. We can see how many clusters there are in our network using the 
 following function:
  
 # Cluster in network
  
 is.connected(social_graph, mode=c(""weak"", ""strong""))
  
 The first function 
 is.connected
  is used to check whether the network is strongly 
 clustered or not. The second function actually returns us the number of the clusters 
 and the size of the clusters in the network.
  
 The 
 clusters
  function is used to identify the total number of clusters in the 
 network. It computes the number of elements in each of the clusters, that is, the 
 size of the cluster as well as the total number of clusters.
  
 [
  68 
 ]",NA
Communities,"After checking the number of clusters in the network, let's check how these clusters are 
 spread in the network. We can use the 
 walktrap.community
  function. This function will 
 identify the communities in the network, and we can see the communities using the 
 plot
  
 function. From the output, we can clearly see the five different groups in the network, 
 which match with the output of the cluster function. We can also check the strength of the 
 division of network into subgroups. Networks with high modularity will have dense 
 connections between the nodes in the subgroup. Consider the 
  
 following code to plot communities:
  
 # Plotting Community
  
 network_Community<- walktrap.community(social_graph)
  
 The 
 modularity
  function is used to detect the communities in the network. It measures 
 how modular a given division of a network graph into subgraphs is, that is, how strong a 
 division within a network is. Networks with high a modularity score have strong 
 connections between the nodes within their cluster (group/ community). Consider the 
 following code to find the modularity:
  
 modularity(network_Community)
  
 plot(network_Community, social_graph, vertex.size=10, 
 vertex.label.cex=0.5, vertex.label=NA, edge.arrow.size=0, 
 edge.curved=TRUE,layout=layout.fruchterman.reingold)
  
 [
  69 
 ]",NA
Getting Facebook page data,"Facebook is not only used by individuals but also by many businesses. Most of the 
 businesses create a Facebook page to advertise about their company and to display 
 their products, offers, and related content. Hence, it becomes really important to check 
 what kinds of posts are liked by the user, perform experimentation with different 
 content, and identify the content that results in a follower's engagement.
  
 Let's see how to get the contents of a page. First, we need to get the name of the 
 page. We can get this from the URL of the page. Let's take TED as an example; from 
 the URL, we know that the name of the page is TED.
  
  
 [
  71 
 ]",NA
Trending topics,"The concept of trending topics is quite popular. We can see the trending topics in news 
 websites, Twitter, and so on. But how can we identify the trending topic for a particular 
 Facebook page or a group of Facebook pages? Let's see how it can be done in detail.",NA
Trend analysis,"Now, we will see how to learn which posts are doing well in recent times. After 
 selecting the page that we are planning to do some analysis for, we will filter the 
 posts' data based on a time range. Let's consider the same TED page and filter the 
 recent data and see the posts that were popular:
  
 # Most trending posts
  
 page<- getPage(""TED"", token, n = 500)
  
 head(page, n=20)
  
 We pull the interactions, that is, messages posted in a page using the 
 getPage
  function. In 
 the following code, we are filtering the data. We are pulling the data that was posted after 
 April 1, 2015. Then, we order the post based on the number of likes, and we use the 
 head
  
 function to display the top posts and their details. The code is as follows:
  
 pageRecent<- page[which(page$created_time> ""2015-04-01""), ]
  
 top<- pageRecent[order(- pageRecent$likes),]
  
 head(top, n=10)
  
 Here's the output:
  
  
  
  
  
  
  
  
  
  
 [
  73 
 ]",NA
Influencers,"Having seen the details of the post, let's see how to learn about the people who comment 
 and like these posts and to check if there is anyone who is more influential. For doing such 
 an analysis, first we need to pull the data about the user interaction in a particular post.",NA
Based on a single post,"Let's take the most recent post and pull all the user comments using the function 
 getPost
 . For each of those comments, let's see how many people liked it using the 
 following code:
  
 post_id<- head(page$id, n = 1)  ## ID of most recent post
  
 post<- getPost(post_id, token, n = 1000, likes = TRUE, 
 comments = TRUE)
  
 head(post$comments, n=2)
  
 [
  74 
 ]",NA
Based on multiple posts ,"Based on a single post or a few comments, can we come to a conclusion on if a 
 particular user is influential or not? In order to identify the overall favorite user, we 
 will first download the comments posted in all the posts. Combine them using the 
 rbind
  function, and finally write a SQL query using the 
 sqldf
  function with 
 groupby""user name""
 .
  
 For the Facebook page TED, let's check the most influential person. We will first 
 download the top 100 posts from the page. Convert them into a matrix so that it 
 becomes easy to access the comments based on their position, and initialize the 
 data frame 
 allcomments
  as null. The code is as follows:
  
 post_id<- head(page$id, n = 100) 
  
 head(post_id, n=10) 
  
 post_id<- as.matrix(post_id) 
  
 allcomments<- """"
  
 In the 
 for
  loop, we will traverse post by post and append all the comments of the posts 
 to the data frame 
 allcomments
 . The following 
 for
  loop might take some time because 
 we are consolidating thousands of comments. Finally, we will sort the users based on 
 the number of likes they got for their comments. Hence, we get to know the most 
 influential person in the page. The code is as follows:
  
 # Collecting all the commments from all the 100 posts 
 for (i in 1:nrow(post_id)) 
  
 {
  
  
  # Get upto 1000 comments for each post 
  
 post<- getPost(post_id[i,], token, n = 1000, 
  
 likes = TRUE, comments = TRUE) 
  
 comments<- post$comments
  
  
  # Append the comments to a single data frame 
  
 allcomments<- rbind(allcomments, comments) 
  
 }
  
 Once we have consolidated all the comments, we use the 
 sqldf
  function to 
 aggregate the likes based on user. To know how many users have commented in 
 the posts and in total how many liked their comments, the code is as follows:
  
 # Consolidating the like for each user.
  
 influentialusers<- sqldf(""select from_name, sum(likes_count) as 
 totlikes from allcomments group by from_name"") 
  
 influentialusers$totlikes<- as.numeric(influentialusers$totlikes) 
 top<- influentialusers[order(- influentialusers$totlikes),] 
 head(top, n=20)
  
 [
  76 
 ]",NA
Measuring CTR performance for a page,"The performance of a page can be measured by the user activity in the page and the 
 user's interaction in the posts published in the page. Let's measure the performance for 
 a page. When we say measuring the performance, it is by means of counting the user 
 interaction through likes, comments, and shares.
  
 In order to come up with a trend, we will need the timestamp data. Then, we need to 
 consolidate the data on a monthly basis so that we can draw the time-series 
 performance chart.
  
 [
  77 
 ]",NA
Spam detection,"Spam detection is an important use case to deal with. With the growing number of 
 users, the number of spam comments/messages is also increasing. Hence, it is 
 important to build a model or a rule engine which would be capable of identifying the 
 fraudulent user, posting some random message.
  
 The implementation of this algorithm would be slightly difficult because there is no 
 direct mechanism to tag a post as spam. In this section, we will teach you to build a basic 
 model based on certain parameters as well as users' inputs to identify a spam post. This 
 will definitely help you to understand the concept. Any such algorithm implemented 
 would require a constant update since the spammers too, would change their strategy.",NA
Implementing a spam detection algorithm,"The following is a simple implementation of a spam detection algorithm using 
 logistic regression. Let's see in detail what the code does to predict the spam 
 messages as comments in the Facebook page posts:
  
 page<- getPage(""beach4all"", token, n = 500)
  
 post_id<- head(page$id, n = 100)
  
 head(post_id, n=10)
  
 post_id<- as.matrix(post_id)
  
 allcomments<- """"
  
 for (i in 1:nrow(post_id))
  
 {
  
 post<- getPost(post_id[i,], token, n = 1000, 
 likes = TRUE, comments = TRUE)
  
 [
  80 
 ]",NA
The order of stories on a user's home ,NA,NA
page ,"In Facebook, when we open the home page we see multiple newsfeeds. These newsfeed 
 are updated continuously, let's try to imitate the same in R. The following code will sort 
 the newsfeeds in an order based on the interactions, as well as the recency of publishing. 
 If you face any problems here, check the version of the API and retry with the API of 
 Version 2.3. The code is as follows:
  
 newsfeed<- getNewsfeed(token, n = 200) 
  
 head(newsfeed, 20) 
  
 newsfeed$datetime<- format.facebook.date(newsfeed$created_time) 
 currdate<- Sys.time() 
  
 maxdiff<- 
  
 max(difftime(currdate, newsfeed$datetime, units=""hours"")) 
 newsfeed$priority<- 
  
 maxdiff - difftime(currdate, newsfeed$datetime, units=""hours"") 
 newsfeed$priority<- as.numeric(newsfeed$priority) 
  
 fnpriority<- function(x){(x-min(x))/(max(x)-min(x))} 
  
 newsfeed$priority<- fnpriority(newsfeed$priority) *100 
  
 newsfeed$plikes_count<- fnpriority(newsfeed$likes_count) *100 
 newsfeed$pcomments_count<- 
  
 fnpriority(newsfeed$comments_count) *100 
  
 newsfeed$pshares_count<- fnpriority(newsfeed$shares_count) *100 
 newsfeed$score<- newsfeed$plikes_count + 
  
 newsfeed$pcomments_count + newsfeed$pshares_count + 
  
 newsfeed$priority 
  
 newsfeed<- newsfeed[order(-newsfeed$score),]
  
 [
  84 
 ]",NA
Recommendations to friends ,"The objective of this chapter is to recommend to your friend, pages that they might like. 
 We will build this recommendation using the Apriori algorithm. The following code will 
 be useful for building the recommendation model. It makes use of the Apriori algorithm 
 to build, generate the rules, and hence extract the patterns in the data that can be used 
 as recommendation. Let's understand the code in detail:
  
 friends<- getFriends(token, simplify = TRUE) 
  
 head(friends, 26) 
  
 friend1<- getLikes(""500637447"", n = 100, token) 
  
 friend2<- getLikes(""505108142"", n = 100, token) 
  
 friend1$user <- ""friend1"" 
  
 friend2$user <- ""friend2"" 
  
 friendlikedata<- rbind(friend1, friend2) 
  
 head(friendlikedata) 
  
 forRecc<- friendlikedata[,c(""user"", ""id"")] 
  
 write.csv(forRecc,""C:/Users/Sharan/Desktop/Chapter 3/forRecc.csv"", 
 row.names = FALSE, col.names = NA) 
  
 library(arules) 
  
 data = read.transactions(file=""C:/Users/Sharan/Desktop/Chapter 
 3/forRecc.csv"", rm.duplicates= FALSE, 
  
 format=""single"",sep="","",cols =c(1,2)); 
  
 head(data, 10) 
  
 nrow(data) 
  
 inspect(data) 
  
 rules<- apriori(data,parameter = list(sup = 0.2, conf = 0.001, 
 target=""rules"", minlen=3, maxlen=5)); 
  
 inspect(rules); 
  
 itemFrequencyPlot(data) 
  
 image(data)
  
 [
  87 
 ]",NA
Reading the output,"The output produced by the Apriori algorithm might not be self-explanatory. Let's see 
 what it means. Those people who like the pages in the column 
 lhs
  are most likely to like 
 the page in the column 
 rhs
 . We also get to know the support, as well as the confidence, for 
 the rules generated. The lift ratio is the confidence of the rule divided by the confidence, 
 assuming the independence of the consequent from the antecedent. A lift ratio higher 
 than 1 suggests that there is a strong association, which means the rule generated is 
 useful. For recommendations to the people, first, filter the users based on the pages in the 
 column 
 lhs
  and recommend them the pages in the column 
 rhs
 .
  
 [
  89 
 ]",NA
Other business cases,"So far, we have implemented some of the business cases. Let's see some of the other 
 possible business cases that could be solved using the Facebook data:
  
 1. A well-established company can use Facebook data to select the people whom 
 they can use for the social media campaign, such as providing offers so that 
 they could reach these people faster.
  
 2. Identify how the reviews of the product are across different zones among 
 people speaking different languages, people belonging to different social 
 groups, and so on.
  
 3. How we can merge two different communities in our network who do not 
  
 have 
 any single attribute in common.
  
 4. What is the time when the interactions in your network are high? Is there any 
 difference in behavior between gender, location, and qualification of the 
 people in the network?",NA
Summary,"In this chapter, we covered the sequential steps involved in the creation of a Facebook 
 app and used the authentication details to connect to the Facebook Graph API. We also 
 discussed how to use the various functions implemented in the 
 Rfacebook
  package.
  
 This chapter covers the important techniques that helps in performing vital network 
 analysis and also enlightens us about the wide range of business problems that could be 
 addressed with the Facebook data. It gives us a glimpse of the great potential for 
 implementation of various analyses.
  
 [
  90 
 ]",NA
Finding Popular Photos ,NA,NA
on Instagram,"Instagram is not just a platform for sharing photos and videos; with more than 300 
 million active users, it has become a popular platform for marketing. It becomes 
 absolutely necessary for the brands and corporations to track the performance of 
 various activities and users on Instagram to keep them ahead of the competition.
  
 In this chapter, we will explore ways to get some interesting stats from the Instagram 
 platform. Using the package 
 instaR
  v0.1.4, we will pull the data and use the 
  
 analytics capabilities in R, to explore and answer interesting questions. Some of the data 
 that we will be extracting in this chapter will be public media from a specific hashtag, 
 location, or user, and we will also get user profile information, followers, and following 
 details. We can also get some of the picture details such as the likes, comments, or 
 captions used while posting; picture type, and much more.
  
 The objective of this chapter is to use the aforementioned data and get some really 
 interesting metrics on users, brands, and location data, based on their activities on 
 Instagram which solves some business use cases such as identifying popular 
 personalities, identifying popular destinations, and providing recommendations to 
 celebrities on the users they might be interested in following.
  
 In this chapter, we will cover the following topics:
  
 • 
  
 Creating an app on the Instagram platform
  
 • 
  
 Installation and authentication of the instaR package
  
 • 
  
 Accessing data from R
  
 • 
  
 Building a dataset
  
 • 
  
 Popular personalities
  
 • 
  
 Finding the most popular destination
  
 [
  93 
 ]",NA
Creating an app on the Instagram ,NA,NA
platform,"We need to register our application with Instagram in order to access the data. In 
 order to register an application, we need to create an Instagram account, which 
 can be created only from a mobile device. Here are the steps involved in creating 
 an application on Instagram.
  
 After creating an Instagram account, open the URL 
 https://instagram.com/ 
 developer/
 :
  
  
 Click on the 
 Register Your Application
  button to create a new client and fill in the 
 following details. In the 
 Redirect URL
  textbox, type in 
 http://localhost:1410/
 ; this 
 is the call back URL that Instagram will return to after successful authentication. After 
 filling in all the relevant details, click on the 
 Register
  button at the bottom of the page:
  
 [
  94 
 ]",NA
Installation and authentication of the ,NA,NA
instaR package,"The R package 
 instaR
  is authored and maintained by Pablo Barbera and it helps R 
 users to access the Instagram API through R. This package provides a series of 
 functions to access information from Instagram.
  
 We can install the latest 
 instaR
  package directly from the GitHub repository using the 
 following code and load the package into R using the 
 library
  function. 
  
 The package 
 devtools
  is required in order to install directly from GitHub:
  
 library(devtools)
  
 install_github(""pablobarbera/instaR/instaR"")
  
 library(instaR)
  
 After installing the required package for enabling the access to the Instagram API, we 
 will proceed to make the authentication process from R. In the following code, the 
 variable 
 app_id
  holds the actual 
 Client ID
  of your app and the variable 
 app_secret 
 holds the 
 Client Secret
 . Using the function 
 instaOAuth
 , we generate the access token 
 that makes it possible to make an authenticated call to the Instagram API. The token can 
 be saved locally in the system so that it can be reused in the future:
  
 app_id<- ""<<paste your Client ID here>>""
  
 app_secret<- ""<<paste your key here>>""
  
 token<- instaOAuth(app_id, app_secret)
  
 On executing the preceding statement, you will find the following message in the R 
 console:
  
 Copy and paste into 'OAuthredirect_uri' on Instagram App Settings: 
 http://localhost:1410/
  
 When done, press any key to continue...
  
 If the redirect URL is already specified while creating the app in Instagram, you can 
 continue by pressing any key; the authentication would happen in the browser and on 
 successful authentication, the following message will be shown:
  
 Waiting for authentication in browser...
  
 Press Esc/Ctrl + C to abort
  
 Authentication complete.
  
 Authentication successful.
  
 [
  96 
 ]",NA
Accessing data from R,"The Instagram API provides access to some amazing content published on Instagram. 
 It uses the OAuth 2.0 protocol for authentication and authorization as explained in 
 the previous section. Let's see the functions present in the package 
 instaR
 , which 
 enables us to download data from R.",NA
Searching public media for a specific hashtag,"The function 
 searchInstagram
  allows the users to download the public media 
 posted on Instagram with a specific hashtag:
  
 MachuPicchu<- searchInstagram(""MachuPicchu"", token, 
 n=10, folder=""MachuPicchu"")
  
 The preceding code will return the recent public media posts on Instagram with the 
 hashtag 
 MachuPicchu
 , and the media files will be downloaded in a folder named 
 MachuPicchu
  (as specified in the preceding code) in the current working directory of R. 
 The working directory of R can be changed using the function 
 setwd()
 . You can explore 
 the content downloaded using the below code. The function 
 names
  will give the various 
 columns present in the data:
  
 names(MachuPicchu)
  
 The output is as follows:
  
 [1] ""type""           ""longitude""      ""latitude""       ""location_name""
  
  [5] ""location_id""    ""comments_count"" ""filter""         ""created_time""
  
  [9] ""link""           ""likes_count""    ""image_url""      ""caption""
  
 [13] ""username""       ""user_id""        ""user_fullname""  ""id""
  
 The function 
 head
  shows the snapshot of the output:
  
 head(MachuPicchu,2)
  
 [
  97 
 ]",NA
Searching public media from a specific ,NA,NA
location,"We can also download the public media with a specific hashtag from a particular location. 
 To the preceding code, we will add the location filter to make sure that the public media 
 was posted from that particular location. This can be achieved using the function 
 searchInstagram
  along with the location parameters. We can get the latitude and 
 longitude of a location from Google maps by zooming in to the location, right-clicking, and 
 selecting the option 
 What's here?
 . The code is as follows:
  
 MachuPicchu<- searchInstagram(""MachuPicchu"", token, n=10, lat= 
 13.1633, lng= 72.5456, distance=1000, folder=""MachuPicchu"")
  
 In the preceding code, the latitude and the longitude specified correspond, to the 
 location of 
 MachuPicchu
 . The parameter 
 distance
  allows us to extract the media 
 content posted within a radius of 1000 meters, which can be increased up to a 
 distance of 5000 meters.
  
 The function 
 searchInstagram
  returns the content that is up to 7 days old only, and 
 it allows us to filter based on 
 Hashtag
  or 
 Location
 , or both.
  
 [
  98 
 ]",NA
Extracting public media of a user,"Using the function 
 getUserMedia
 , we can download the public media of a particular user. 
 We will use the function to download the public media contents posted by the user 
 instagram
 . Using the following code, we download the latest 100 public posts by the user 
 instagram
 . Alternatively, we can also download the content by using the user's 
 Instagram ID. The code is as follows:
  
 instag <- getUserMedia(""instagram"", token, n=100, 
 folder=""instagram"")
  
 The preceding function will download the content to a folder 
 instagram
  in the current 
 working directory of R. The following code is used to get an idea about the data:
  
 names(instag)
  
 head(instag)
  
 The structure of the data downloaded will be similar to what we saw in the 
 previous section.",NA
Extracting user profile,"In order to extract the basic user profile, we can use the function 
 getUser
 . We need to 
 pass the username and the token as parameters to this function. This function 
 provides us with the basic profile details such as username, basic biodata, website (if 
 available), link to the profile picture, complete username, media published, and 
 followed by, as well as the number of people the user follows. Using the following 
 function, we get the profile information of 
 Barack Obama
 :
  
 usr<- getUser(""barackobama"", token)
  
 head(usr)
  
 The output is as follows:
  
  
 [
  99 
 ]",NA
Getting followers,"To know about the followers of a particular profile, we can use the function 
 getFollowers
 . Similar to the 
 getUser
  function, we need to pass the user's 
 Instagram account name and the token for authentication. The code is as follows:
  
 instaf<- getFollowers(""instagram"", token)
  
 This function not only extracts the name of the followers, but also the profile 
 information of the followers. We use the function 
 names
  to get the details on the 
 various variables extracted using 
 getFollowers
 . This function might not extract the 
 complete list of the followers if the number of followers is huge, but it will extract the 
 recent followers:
  
 names(instaf)
  
 The output is as follows:
  
  
  
  
 This code might run for few minutes as it is downloading details 
  
  
 of a large number of followers.
  
 The 
 getFollowers
  function extracts the data in the data frame format. We can check 
 the data using the function 
 head
 . We have extracted the followers of the account 
 instagram
 . We can use the function 
 nrow
  to get to know about the number of follower's 
 details downloaded by the function 
 getFollowers
 . In the previous case, we extracted 
 the basic profile details of about 521,851 followers of the account 
 instagram
 , whose 
 details were available as public. The code is as follows:
  
 head(instaf,2)
  
 The output is as follows:
  
  
  
  
  
  
  
 [
  100 
 ]",NA
Who does the user follow?,"We have already seen how to extract details about the followers. Now, we will see how 
 to extract the details about the people/accounts that the users follow. This can be 
 achieved using the function 
 getFollows
  and the same two parameters. This function 
 will extract the data in the data frame format. We can check a sample of the data using 
 the function 
 head
 .
  
 The following function extracts the basic details of the people/accounts who are 
 followed by the account 
 instagram
 . We can use the function 
 nrow
  to know about the 
 number of people whose information was extracted. In our case, we were able to 
 extract basic profile details of 429 people. The code is as follows: 
  
  
 instaff<- getFollows(""instagram"", token) 
  
  
 head(instaff,3) 
  
 The output is as follows:
  
  
 For the following code: 
  
  
 nrow(instaff) 
  
 The output is as follows:
  
  
  
  [1] 429
  
 [
  101 
 ]",NA
Getting comments,"We can get the comments posted on public media using the function 
 getComments
 . This 
 function will provide us with the recent comments posted on the specified media, and it 
 can extract a maximum of 150 comments from a post. This function also extracts the 
 details such as the text of the comments and details of the user who posted it, such as 
 the name, ID, profile picture, and the comments ID. The code is as follows:
  
 comm<- getComments(""1027502496068994465_25025320"", token)
  
 The output is as follows:
  
  150 comments
  
 For the following code:
  
 names(comm)
  
 The output is as follows:
  
  
 In our example, we downloaded the most recent 150 comments, and all the details 
 regarding the comments are stored in the data frame 
 comm
 . Let's see what the 
 snapshot of the output looks like using the following command:
  
 tail(comm)
  
 [
  102 
 ]",NA
Number of times hashtag is used,"The function 
 getTagCount
  helps us know the usage of the specified hashtag in the 
 comments for the media post. We can use this function to get the occurrence of any 
 hashtag. We will use this function to get the number of times the tags 
 greece
  and 
 obama
  are used:
  
 Tag1<- getTagCount(""greece"", token)
  
 Tag1
  
 The output is as follows:
  
  [1] 7805055
  
 For 
 obama
 :
  
 Tag2<- getTagCount(""obama"", token)
  
 Tag2
  
 The output is as follows:
  
  [1] 2416024
  
 We can use this function to get the popularity of brands, users, and so on. We can 
 also get trends by executing the function at a fixed time interval.
  
  
 Exercise
 :
  
  
 • 
  
 Do a word cloud analysis on the recent comments posted in the 
  
 account 
 Instagram
 .
  
 • 
  
 Do an hourly trend analysis for a week on the number of times 
  
 the following tags are used:
  
 • 
  
 #love
  
 • 
  
 #instagood
  
 • 
  
 #me
  
 • 
  
 #tbt
  
 • 
  
 #followme
  
 • 
  
 #photooftheday
  
 • 
  
 #happy
  
 • 
  
 #tagforlikes
  
 • 
  
 #selfie
  
 [
  104 
 ]",NA
Building a dataset,"In this section, we will create multiple datasets using a specific set of users as well as the 
 hashtags that will be used for further analysis, so we can answer some interesting 
 questions. We have created a list of popular users as well as some popular hashtags that 
 are commonly used while sharing media related to travelling. All the users and the 
 hashtags used for the analysis will be provided. The name of the CSV file is 
 UsersAndHashtags
 , this CSV file will have two columns: one with the popular users and 
 the other with the hashtags.
  
 Place the aforementioned CSV file in the current working directory. You can get the 
 current working directory using the function 
 getwd()
 ; alternatively, you can also 
 change the working directory using the function 
 setwd()
 . After placing the file in the 
 current working directory, execute the following commands:
  
 userAndTags<- read.csv(""UsersAndHashtags.csv"")
  
 names(userAndTags)
  
 head(userAndTags)
  
 The output is as follows:
  
  
  
  
 This is the list of celebrity profiles as well as travel-related hashtags that will help us 
 extract important information.
  
 [
  105 
 ]",NA
User profile ,"First, we will get the complete profile information about these celebrity user profiles. We 
 can get the profile information using the function 
 getUser
 . We will loop through the 
 preceding table to get the profile information about all the users. Since data extraction 
 involves multiple API calls and, in some cases, might take more time, we will store the 
 dataset in the local system using the function 
 write.csv
 . In the following case, we have a 
 counter on the number of users whose data has been processed so far; it will be printed 
 on the screen. If the code breaks in between, it can be continued based on the counter. 
 The code is as follows:
  
 users<- userAndTags$Users 
  
 users<- as.matrix(users) 
  
 userprofiles = data.frame(matrix("""", ncol = 8, nrow = 0)) 
 for (i in 1:nrow(users)) 
  
 {
  
  
  #uf<- getFollows(users[i,1], token) 
  
 usrp<- getUser(users[i,1], token) 
  
 userprofiles<- rbind(userprofiles, usrp) 
  
 print(i)
  
 } 
  
 write.csv(userprofiles, ""userprofiles.csv"") 
  
 head(userprofiles, 3)
  
 The output is as follows:
  
  
 [
  106 
 ]",NA
User media,"Let's extract all the user-specific data. In order to do so, we need to get all the usernames 
 and convert them into a matrix so that we can easily refer to the positions of the 
 usernames. Then, we can create an empty data frame of the same dimensions as the 
 produced output. The code is as follows:
  
 users<- userAndTags$Users
  
 users<- as.matrix(users)
  
 usermedia <- data.frame(matrix("""", ncol = 16, nrow = 0))
  
 We are using a 
 for
  loop to extract all the user data. In the following code, we are 
 extracting just the 20 most recent posts from all the users using the function 
 getUserMedia
 . We will keep appending the data to the existing dataset using the 
 function 
 rbind
 . The following code will not only produce a large dataset with the 20 
 most recent posts from all the users provided as input, but also download their media 
 files to the folder named 
 users
  in the current working directory. The code is as 
 follows:
  
 for (i in 7:nrow(users))
  
 {
  
 um<- getUserMedia(users[i,1], token, n=20, folder=""users"")
  
 usermedia<- rbind(usermedia, um)
  
 print(i)
  
 }
  
 head(usermedia)
  
 The output is as follows:
  
  
 [
  107 
 ]",NA
Travel-related media ,"After extracting the user data, we will now focus on the popular hashtags commonly used 
 with regard to travel and sightseeing. The second column of the dataset provided has the 
 list of hashtags. Similar to extracting the user data, we use the 
 for
  loop to extract travel-
 related posts. In the following code, for each of the hashtags, we download 100 recent 
 posts. Finally, we combine the data using the function 
 rbind
 . 
  
 The code is as follows:
  
 tags<- userAndTags$Hashtags 
  
 tags<- as.matrix(tags) 
  
 for (i in 1:nrow(tags)) 
  
 { 
  
 hm<- getUserMedia(tags[i,1], token, n=100, folder=""tags"") 
 hashmedia<- rbind(hashmedia, hm) 
  
 print(i) 
  
 } 
  
 head(hashmedia)
  
 The output is as follows:
  
  
 [
  108 
 ]",NA
Who do they follow?,"In addition to the preceding datasets, we will also create a dataset on whom the 
 celebrity users follow on Instagram. This information will be very useful to provide 
 recommendations to various celebrity users whom they would like to follow based on 
 similar user behavior. We will be discussing these recommendations in detail later in 
 this chapter.
  
 We will use the following code to read the list of users from our dataset 
  
 userAndTags
  and convert them to matrix format using the function 
 as.matrix
  and 
 finally use a 
 for
  loop on the users to extract basic details of the accounts they follow.
  
 users<- userAndTags$Users 
  
 users<- as.matrix(users) 
  
 userfollows = data.frame(matrix("""", ncol = 7, nrow = 0)) 
 for (i in 1:nrow(users)) 
  
 { 
  
 uf<- getFollows(users[i,1], token) 
  
 auf<- cbind(users[i,1], uf) 
  
 userfollows<- rbind(userfollows, auf) 
  
 print(i)
  
 } 
  
 nrow(userfollows) 
  
 write.csv(userfollows, ""userfollows.csv"") 
  
 head(userfollows)
  
  
 Note that the 
 for
  loop in the preceding code will break if any of 
  
  
 the users have zero follows. Hence we have the counter in place, 
  
 so that we can change the starting pointer in the 
 for
  loop, and 
  
 continue to pull the data until the list is completed.
  
 [
  109 
 ]",NA
Popular personalities,"From the dataset we built, we will work on identifying the most popular users using 
 different aspects. Let's see those in detail.",NA
Who has the most followers?,"We can get the users with most number of followers from the dataset 
 userprofiles 
 by sorting the data using the column 
 followed_by
  and using the function 
 order
 . The 
 following code will return the dataset by sorting the data in the descending order based 
 on the column 
 followed_by
 . The code is as follows:
  
 mostfollowed<- userprofiles[with(userprofiles, 
 order(-followed_by)), ]
  
 head(mostfollowed$full_name, 15)
  
 [
  110 
 ]",NA
Who follows more people?,"To know the user who follows the most number of people, we use the same dataset 
 userprofiles
 . Everything is similar to the previous one, but we need to use the column 
 follows
  instead of 
 followed_by
 . The code is as follows:
  
 mostfollows<- userprofiles[with(userprofiles, order(-follows)), ]
  
 head(mostfollows$full_name, 15)
  
 The output is as follows:
  
  
 The preceding output shows the users who follow the most number of people on 
 Instagram.",NA
Who shared most media?,"Now, let's see who are the most active users and who have shared the most number of 
 pictures/videos on Instagram. We use the dataset 
 userprofiles
 :
  
 mostmedia <- userprofiles[with(userprofiles, 
 order(-media_count)), ]
  
 head(mostmedia$full_name, 15)
  
 The output is as follows:
  
  
 [
  111 
 ]",NA
Overall top users,"So far, we have explored the popular users based on the factors 
 followed_by
 , 
 follows
 , and 
 media_count
  from the dataset 
 userprofiles
 . In the following 
 code, we will get the overall active users by using all of the aforementioned 
 factors. The code is as follows:
  
 userprofiles$overallmetric<- 
  
 ((userprofiles$media_count/max(userprofiles$media_count)) + 
 (userprofiles$followed_by/max(userprofiles$followed_by)) 
 +(userprofiles$followed_by/max(userprofiles$followed_by)))*10
 0
  
 In the preceding code, we normalize the values of each of the factors to the range of 0-
 100. Then, we add them up to come up with the final score. The final score can have a 
 maximum value of 300. The code is as follows:
  
 overallmet<- userprofiles[with(userprofiles, 
 order(-overallmetric)), ]
  
 head(overallmet$full_name, 15)
  
 The output is as follows:
  
  
 These are the most active users based on all of the previously mentioned factors. 
 These are the various analyses to get us the most popular users on Instagram.",NA
Most viral media,"We can find out about the media which went most viral. To get that information, we will 
 use the dataset 
 alldata
 , and the columns 
 comments_count
  and 
 likes_count
 . The 
 following code will help us to identify the media that had the most number of comments 
 as well as the one with the most number of likes:
  
 mostcomm<- alldata[with(alldata, order(-comments_count)), ]
  
 head(mostcomm, 1)
  
 [
  112 
 ]",NA
Finding the most popular destination,"We explored some of the user metrics as well as the media metrics. Now, we will explore 
 the geography of the media posts. We can get the geo-location if it has been enabled by 
 the user. We will perform this analysis on the dataset 
 alldata
 .
  
 [
  113 
 ]",NA
Locations,"In the following code, we are just getting the unique locations found in the dataset 
 collected by us. Since we are getting the data with the help of a query, we need to load 
 the 
 sqldf
  package in the R console and use the function 
 na.omit
  to remove the posts 
 without any location details. The following code consolidates the locations and finally, 
 using the function 
 nrow
 , we get to know about the unique number of locations from 
 where the posts were made.
  
 library(sqldf)
  
 names(alldata)
  
 allloc<- sqldf(""select distinct location_name from alldata"")
  
 allloc<- na.omit(allloc)
  
 nrow(allloc)
  
 The output is as follows:
  
  [1] 432
  
 There are posts from 432 different location in the dataset collected by us. Let's get a 
 snapshot of some of the locations:
  
 head(allloc, 20)
  
 The output is as follows:
  
  
 [
  114 
 ]",NA
Locations with most likes ,"We have seen all the locations from where the posts were made. Now, we will see 
 which location, the posts with the maximum number of likes were made from. We can 
 get this data by using the 
 group by
  function in SQL, and then sorting the data based 
 on the descending order of the total likes received. The code is as follows:
  
 loclikes<- sqldf(""select location_name, sum(likes_count) as 
 totlikes from alldata group by location_name"") 
  
 loc<- loclikes[with(loclikes, order(-totlikes)), ] 
  
 loc<- na.omit(loc) 
  
 head(loc, 25)
  
 The output is as follows:",NA
Locations most talked about ,"We assume the total comments made in the post as a proxy for the location most 
 talked about, though the comments might not really be about the place. This can be 
 implemented in a way similar to the likes for a location by using the column 
 comments_count
  instead of 
 likes_count
 . The code is as follows:
  
 loccomments<- sqldf(""select location_name, sum(comments_count) as 
 totcomm from alldata group by location_name"") 
  
 loccomm<- loccomments[with(loccomments, order(-totcomm)), ] 
  
 loccomm<- na.omit(loccomm)
  
 [
  115 
 ]",NA
What are people saying about these ,NA,NA
locations?,"Having seen some of the quantitative measure of the locations, we will now see what 
 people are saying about those locations, or in those posts made by them. For this 
 analysis, we use the text and tags found under the caption. After collecting all the 
 caption text, we perform the text analysis.
  
 To perform text analysis, we need to load some packages that enable text analysis. For 
 this section, we need to load the package 
 wordcloud
  for plotting the word cloud and 
 tm
  
 to perform some processing on the text data and make it usable for the text analysis. The 
 code is as follows:
  
 library(wordcloud)
  
 library(tm)
  
 We first break the sentences into words using the function 
 strsplit
 . Then, we 
 perform a series of steps to remove the special characters from the dataset, convert 
 them into lower case words, and finally remove the standard stop words. The code is 
 as follows:
  
 words<- strsplit(as.character(alldata$caption), "" "")
  
 [
  116 
 ]",NA
Most repeating locations ,"Having seen some of the metrics on location, we will now look at the repetition of posts 
 made from a particular location. The following query will get us the required 
 information and sort the locations that were repeated the most. Since the dataset used 
 in our case is not huge, it is possible that a location could have come out on top because 
 of repeated posts from a single user. The code is as follows:
  
 locations<- sqldf(""select location_name, count(location_id) as 
 locid from alldata group by location_name"")
  
 [
  117 
 ]",NA
Clustering the pictures,"Clustering is an example of unsupervised learning as there is no prior knowledge of the 
 groups present in the dataset. It is a method of dividing the dataset into different groups 
 based on various parameters of the dataset. Each group is called a cluster, and the 
 various objects present in a group will be share some similarities as well as 
 dissimilarities when compared with the objects outside the group. We will cover the 
 clustering algorithm in this section.
  
 One of the greatest examples of the clustering algorithm would be the search engine; 
 where the pages that are closely related to each other are shown together, and the pages 
 that are different are kept away as far as possible. The most important factor here is the 
 factor that we consider to measure the similarity or the dissimilarity between the 
 objects.
  
 In order to implement the clustering algorithms in R, we need to load the package 
 fpc
  
 into the R environment. The package 
 fpc
 , a flexible procedure for clustering, has multiple 
 functions to implement various kinds of clustering techniques. The code is as follows:
  
 library(fpc)
  
 data<- alldata
  
 cdata<- subset(data, select= c(type, comments_count, 
 likes_count, filter)
  
 [
  118 
 ]",NA
Recommendations to the users,"Recommendation has become very common nowadays. Many online companies like 
 Amazon, Facebook, LinkedIn, and so on provide recommendations. These 
 recommendations are produced by the recommendation system that is nothing but an 
 algorithm that uses some of the historic data to predict what the user would like. The 
 recommendation can be implemented using the collaborative filtering algorithm, 
 which can be implemented using either user-based or item-based methodology. In this 
 section, we will see in detail the implementation of 
  
 the algorithm using user-based filtering.",NA
How to do it,"We will use the information of the users whose details we have downloaded, and the 
 users whom they follow, and we will build the recommendation engine based on this 
 data. Since we have already downloaded the data, we will read the data using the 
 function 
 read.csv
 :
  
 userfollows<- read.csv(""userfollows.csv"")
  
 names(userfollows)
  
 The output is as follows:
  
  
 [
  123 
 ]",NA
Top three recommendations,"In the preceding recommendation list, we find that the first recommendation is the 
 same as that of the original user. This is mostly because of the self-similarity 
 computation and it has to be removed. The preceding data frame view is not very clear, 
 so there is an Excel view below, where the first column is the celebrity users, who are 
 the subject of our analysis, and the next three columns are the users who are similar to 
 them. Here are some of the celebrity users and their top three similar users.
  
 Celebrity Users
  
 Recommendation1
  
 Recommendation2
  
 Recommendation3
  
 aliciakeys
  
 johnlegend
  
 ijessewilliams
  
 ellenpompeo
  
 angelcandices
  
 caradelevingne
  
 krisjenner
  
 gisele
  
 aw
  
 mindykaling
  
 lenadunham
  
 reesewitherspoon
  
 brooklynbeckham
  
 itsashbenzo
  
 khloekardashian
  
 krisjenner
  
 busyphilipps
  
 mindykaling
  
 lenadunham
  
 reesewitherspoon
  
 camerondiaz
  
 reesewitherspoon
  
 msleamichele
  
 mindykaling
  
 caradelevingne
  
 angelcandices
  
 popsugar
  
 emmaroberts
  
 champagnepapi
  
 iamdiddy
  
 ijessewilliams
  
 khloekardashian
  
 charlizeafrica
  
 randyjackson
  
 ellenpompeo
  
 aw
  
 chrissyteigen
  
 johnlegend
  
 khloekardashian
  
 popsugar
  
 derekhough
  
 juleshough
  
 glassofwhiskey
  
 msleamichele
  
 dianekrugerperso
  
 katebosworth
  
 jaime_king
  
 reesewitherspoon
  
 drewbarrymore
  
 reesewitherspoon
  
 minkak
  
 jaime_king
  
 ellenpompeo
  
 reesewitherspoon
  
 jessicaalba
  
 minkak
  
 emmaroberts
  
 itsashbenzo
  
 jaime_king
  
 jessicaalba
  
 emmyrossum
  
 jessicaalba
  
 jaime_king
  
 reesewitherspoon
  
 fergie
  
 joshduhamel
  
 mileycyrus
  
 popsugar
  
 gisele
  
 angelcandices
  
 ellenpompeo
  
 jaime_king
  
 glassofwhiskey
  
 juleshough
  
 msleamichele
  
 emmaroberts
  
 iamdiddy
  
 champagnepapi
  
 ijessewilliams
  
 randyjackson
  
 iansomerhalder
  
 khloekardashian
  
 normancook
  
 msleamichele
  
 ijessewilliams
  
 iamdiddy
  
 champagnepapi
  
 aliciakeys
  
 instagranph
  
 popsugar
  
 mindykaling
  
 reesewitherspoon
  
 itsashbenzo
  
 emmaroberts
  
 jaime_king
  
 jessicaalba
  
 jaime_king
  
 jessicaalba
  
 emmaroberts
  
 itsashbenzo
  
 [
  130 
 ]",NA
Improvements to the recommendation system,"We can compute the similarity based on the use of multiple methods and finally 
 combine them to form an ensemble algorithm. We can also implement the hybrid 
 methodology, which combines the user-based method as well as the item-based 
 methods.
  
 We will also introduce the concept of classification, that is, dividing the dataset into 
 different groups and build recommendation engines for each of them. Generally, this 
 improves the accuracy, as the recommendations are customized to the groups.
  
  
 Exercise
 :
  
  
 We have seen the implementation of the recommendation engine. Now, 
  
 try the following exercises:
  
 • 
  
 Compute the similarity between the users using the Pearson 
  
 methodology and give 50 percent weightage to both the 
  
 methods.
  
 • 
  
 For the preceding dataset, try to implement the item-based 
  
 recommendation. Instead of identifying the similarity between 
  
 the users, identify the similarity between the items (in this case, 
  
 it is the user accounts followed by our celebrity users).
  
 [
  131 
 ]",NA
Business case,"Some of the business cases that can be implemented using the Instagram data are 
  
 as follows:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Provide recommendations to users on places to travel 
  
 Determine visitor trends to various tourist locations 
  
 Find out what people say about various locations 
  
 Determine the performance of various media posts and create a time 
 series trend for them 
  
 Find out the popularity trend of various celebrity users and brands on 
 Instagram 
  
 Compare Instagram popularity scores of brands with that of 
  
 their competitors",NA
Reference,"• 
  
 Amazon's 
 Item-to-Item Collaborative Filtering.
  
 http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf
  
 • 
  
 More about clustering analysis
  
 http://www.statmethods.net/advstats/cluster.html
  
 • 
  
 About the package 
 instaR
  
 https://github.com/pablobarbera/instaR/tree/master/instaR
  
 • 
  
 Online courses
  
 https://www.coursera.org/course/clusteranalysis
  
 https://class.coursera.org/ml-003/lecture/100
  
 • 
  
 Celebrity users and the location hashtags were obtained from here:
  
 https://socialblade.com/instagram/top/100/followers
  
 http://www.popsugar.com/celebrity/Celebrities-Using-Instagram-
  
 21244293?stream_view=1#photo-23154687
  
 http://top-hashtags.com/hashtag/monument/
  
 [
  132 
 ]",NA
Summary,"In this chapter, we covered the procedure involved in creation of an app on the 
 Instagram platform. We covered the sequential steps for authentication and accessing 
 the data from R using the package 
 instaR
 . We also acquired competency to build a 
 dataset of users and location from the Instagram platform.
  
 We discussed the skill of using the collected data to solve critical business problems. 
  
 Some of the problems that we have solved include identifying the popular users based 
 on multiple metrics, exploring the destinations which people talk about the most, 
 dividing the dataset into different groups by applying a clustering algorithm, building a 
 recommendation system using the collaborative filtering algorithm on who the users 
 might be interested to follow based on the behavior of similar users, and finally a quick 
 brief about the various other business cases that could be solved using the Instagram 
 data.
  
 In the next chapter we will be learning about the implementation of some of the 
 graphical and non-graphical EDA techniques such as histogram, pie chart, box plot, 
 correlations and much more by using a heterogeneous dataset created by way of the 
 GitHub API from R.
  
 [
  133 
 ]",NA
Let's Build Software ,NA,NA
with GitHub,"GitHub is a Web-based Git repository hosting service and it offers distributed revision 
 control, source code management functionality of Git, and much more. GitHub supports 
 both private and public repositories. There are a vast number of public projects in 
 GitHub to which contributions come from multiple people around the world. GitHub 
 provides an API to access their data; the public data can be accessed by anyone whereas 
 the private data can be accessed only by authorized users.
  
 In this chapter, we will see how to access the public data of GitHub using its API from 
 R, and we will see how to perform 
 Exploratory Data Analysis
  (
 EDA
 ) and mine 
 significant patterns from the GitHub data extracted by us. As a part of this chapter, 
 we will cover different methods to extract the data from GitHub and various 
 graphical and non-graphical EDA techniques.
  
 The objectives of this chapter are to show how to extract the public data from GitHub 
 and focus on getting a better understanding of various EDA techniques, detecting 
 anomalies, and extracting patterns from the data. We will start from the basics of 
 EDA and discuss a few advanced visualizations that will be useful for getting answers 
 to some interesting questions such as getting to know the most popular language, 
 user comparison with other users, trends on updates made to the public repositories, 
 different programming languages and their affinity towards one another, and much 
 more.
  
 The topics that will be covered in this chapter are as follows:
  
 • 
  
 Creating an app on GitHub
  
 • 
  
 GitHub package installation and authentication
  
 • 
  
 Accessing GitHub data from R
  
 [
  135 
 ]",NA
Creating an app on GitHub,"We need to register a new application on GitHub in order to access the public GitHub 
 data from R with authentication. In Facebook, creating an app to access the data is not 
 mandatory and we can generate a temporary token to access most of the data. 
  
 In GitHub, we can access data without authentication but with a limitation on the 
 number of calls that can be made, whereas authentication provides the access we need.
  
 In order to create a GitHub app, we need to log in as a developer in GitHub. Go to 
 https://developer.github.com/program/
  and log in as a developer:
  
  
 Click on the 
 Register now
  button in the page and you can choose to pay and buy a 
 developer login. The free account user cannot register as a developer. Alternatively, you 
 can use an existing account and login.
  
 [
  136 
 ]",NA
GitHub package installation and ,NA,NA
authentication,"We will use GitHub API Version 3 for accessing the data. All the API access that 
 happens is over HTTPS and it is accessed from the domain 
 api.github.com
  or 
 yourdomain.com/api/v3
  for the enterprise accounts. All the data received will be 
 in the JSON format.
  
 Now that you know how to create a GitHub app, we will see how to proceed further 
 towards accessing the data from R. In order to connect to the GitHub app, we need to 
 install the 
 rgithub
  package and other dependent packages like 
 devtools
 . The 
 devtools
  package is required so that we can install the latest 
 rgithub
  package directly 
 from the source.
  
 require(devtools)
  
 install_github(""cscheid/rgithub"")
  
 library(github)
  
 [
  139 
 ]",NA
Accessing GitHub data from R,"Accessing the GitHub data from R is simple. It can be accessed using the package 
 rgithub
  developed by Carlos Scheidegger, which provides the binding for the 
 GitHub web services API. We can also use the API URL directly in the function 
 fromJSON
 , which will extract the JSON data in data frame format.
  
 Previously, we saw how to authenticate using the package 
 rgithub
 ; now let's use 
 some of the functions available in the package to pull data from GitHub.
  
 First, let's pull our GitHub account data using the function 
 get.myself
  and pass the 
 variable 
 ctx
  as a parameter, which is a GitHub context object holding the 
 authentication results. This function will provide basic details about our account such 
 as date created, last updated, location, e-mail, number of public repositories 
 contributed, following and followers, and also about the number of API calls we have 
 made in the current session. Let's execute the function 
 get.myself
  and check the 
 output.
  
 get.myself(ctx)
  
 We get the following output:
  
  
 [
  141 
 ]",NA
Building a heterogeneous dataset using ,NA,NA
the most active users,"Let's build a heterogeneous dataset based on the public repositories created by the most 
 active users of GitHub. As we know how to extract data using the package 
 rgithub
 , we 
 will explore the other method too, for example, directly using the API's URL. In this 
 method, we need to pass the API URL to the function 
 fromJSON
 , which has a 
 dependency on the package 
 jsonlite
 . The API URL will also work from the browser 
 and will be checked for accuracy of the data. Those URLs will return data in JSON format.
  
 The most active users of GitHub will be obtained through the following URL, or you can 
 use the CSV file named 
 TopUsers.csv
 , which also holds the data of users who were 
 active as of July 2015. We will make use of the username to pull the additional data 
 about the users.
  
 [
  142 
 ]",NA
Data processing,"Though we have extracted the active user's data, it is not ready for usage. We need to 
 perform a few data processing operations to make it ready to use for the analysis. Use 
 the following code to read the master copy of the data which was saved by us:
  
 activeusers<- read.csv(""ActiveUsers.csv"")
  
 We need to change the date format so that it will be supported by R. This can be 
 accomplished using the following function. It is better to use a function in this case 
 because there are multiple date columns that need to be changed; hence, this function 
 will reduce code redundancy. Since the time zone of the data is in 
 GMT
 , we set the 
 parameter 
 tz
  to GMT. In your case, if the time zone corresponds to your present time 
 zone, it should be set as empty double quotes:
  
 # Change date to format supported by R
  
 format.git.date<- function(datestring) {
  
 date<- as.POSIXct(datestring, format = ""%Y-%m-%dT%H:%M:%SZ"", 
 tz = ""GMT"")
  
 }
  
 The preceding function is required to convert the date format of three columns. 
 Execute the following code to make the changes to the date format of the columns 
 created_at
 , 
 updated_at
  and 
 pushed_at
 , which holds the information of the date on 
 which the repository was created, last updated, as well as last pushed:
  
 # Updating the column with new date format
  
 activeusers$created_at<- format.git.date(activeusers$created_at)
  
 activeusers$updated_at<- format.git.date(activeusers$updated_at)
  
 activeusers$pushed_at<- format.git.date(activeusers$pushed_at)
  
 The data frame 
 activeusers
  holds additional URL data which is not required for the 
 analysis that will be performed as a part of this chapter. Hence, we will select only the 
 required columns through the column selection method. First, we use the function 
 colnames
  to learn all the column names as well as their position, and then we use the 
 respective column number to subset the required data:
  
 # Subsetting required data
  
 # To check which columns to select
  
 colnames(activeusers)
  
 # Selecting the required data
  
 [
  144 
 ]",NA
Building additional metrics ,"We completed the data formatting part, and processed the data so that it can be used 
 for our analysis. Before going to the analysis bit, let's see how to construct a few 
 metrics, which will become a derived column in our dataset. Let's write code to 
 create the following metrics:
  
 1. Identify if there is a web page associated with the repository.
  
 2. Count the number of characters in the description.
  
 3. Identify how long it had been since the repository was created, updated, 
  
 and 
 pushed.
  
 [
  145 
 ]",NA
Exploratory data analysis,"EDA techniques are used for discovering patterns in the data, summarization, as 
 well as for visualization of the data. It is an essential step in the data analysis 
 process, which helps to formulate various hypotheses about the data.
  
 The EDA techniques shall be broadly classified into three types: 
 univariate
 , 
 bivariate
 , 
 and 
 multivariate
  analysis. Let's implement a few of the EDA techniques on our dataset.
  
 First, let's see what kind of data we are analyzing. Using the function 
 sapply
 , we 
 determine the various columns present in the dataset and the datatype of those 
 columns:
  
 sapply(ausersubset, class)
  
 We get the following output:
  
  
  
  
  
  
  
  
  
 Note that the preceding screenshot is just a part of the output.
  
  
 In order to get a basic understanding of the whole dataset, such as the distribution of the 
 values of the columns, we can use the 
 summary
  function to get the highlights of the 
 dataset. For example, we will get the minimum, mean, median, maximum, and quartile 
 values for each column:
  
 summary(ausersubset)
  
 [
  148 
 ]",NA
EDA – graphical analysis,"""A picture is worth a thousand words.""
  
 Graphical analysis is quite popular, as it helps people grasp the content faster. The 
 existence of so many dashboard tools in the market is also proof of this. With the 
 recent innovation in the field of visualization, it is certainly one of the best mediums 
 of communication.
  
 In this section, let's explore a few graphical EDA. The graphical EDA techniques will help 
 us get a more penetrative understanding of the data and also help in presenting 
 complicated statistical analysis in a more understandable format. We will use some of 
 the visualization packages in R that will help in making the output look better.",NA
Which language is most popular among the ,NA,NA
active GitHub users?,"We have the data at the repository level. Each repository is a project that could have 
 been implemented in any language. Let's present the language data in a graphical format 
 and understand the popularity. First, we will use the function 
 table
  to see how many 
 languages are used and how many times they are being used:
  
 table(ausersubset$language)
  
 [
  150 
 ]",NA
"What is the distribution of watchers, forks, ",NA,NA
and issues in GitHub?,"In the introduction to the EDA section, we already saw the distribution of all the 
 columns in the dataset. Now, we will see how it would be to view it in a graphical 
 representation. We will use the box plot to explore the distribution of the dataset.
  
 [
  153 
 ]",NA
How many repositories had issues?,"A pie chart is a data presentation technique where the data is represented in the form 
 of circle, where the circle will be in turn divided into multiple segments. Each segment 
 represents a certain proportion or percentage of the total. Though a pie chart is a 
 popular graphical representation among sales teams as well as print and digital media, 
 it has some limitations:
  
 • 
  
 It can represent only one continuous variable.
  
 • 
  
 It occupies too much space.
  
 • 
  
 It is difficult to compare and interpret multiple pie charts
  
 Let's use the pie chart to understand how many of the repositories had issues. In 
 the following code, we have used the 
 ggplot
  function to draw the pie chart:
  
 # Pie chart: Issues in the repositary
  
 pie<- ggplot(ausersubset, aes(x = factor(1), fill = 
 factor(ausersubset$has_issues))) + geom_bar(width = 
 1)
  
 pie + coord_polar(theta = ""y"") =
  
 ggsave(file=""C:/Users/Sharan/Desktop/SMM/Chapter 5/ 
 Pics/pie-chart.png"", dpi=500)
  
 We get the following output:
  
  
 [
  156 
 ]",NA
What is the trend on updating repositories? ,"Trend analysis is a method of analysis that helps us in understanding the patterns in a 
 parameter across time. We can get a clear view on the trend of the dataset. Let's 
 perform trend analysis through a line chart.
  
 We will consider only the columns 
 updated_at
  and 
 id
  for this analysis. After 
 updating the new data frame with the following values, we will convert the date time 
 into date format using the function 
 as.POSIXct
  and we will consolidate the data to a 
 daily level using the function 
 table
 :
  
 library(data.table) 
  
 trenddata<- ausersubset[c(""updated_at"", ""id"")] 
  
 trenddata$updated_at<- 
  
 as.POSIXct(strptime(trenddata$updated_at, ""%Y-%m-
 %d"")) tdata<- table(trenddata$updated_at)
  
 The data needs to be converted to the data frame format and then we can rename the 
 columns for ease of readability. For this analysis, let's consider the data around a 
 recent time period; let's consider only the last 75 days, data:
  
 tdata<- as.data.frame(tdata) 
  
 colnames(tdata) <- c(""Date"",""Repositories"") 
  
 tdata$Date <- as.Date(tdata$Date) 
  
 tdata1 <- tail(tdata, 75)
  
 Finally, we will plot the trend chart using the function 
 ggplot
  and with the help of 
 additional parameters such as 
 geom_line
  and 
 geom_point
 , we can make the chart 
 better looking as well as more communicative:
  
 q <- ggplot(data=tdata1, aes(x=Date, y=Repositories, group=1)) + 
 geom_line() + 
  
 geom_point() 
  
 q + theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
 ggsave(file=""C:/Users/Sharan/Desktop/SMM/Chapter 5/Pics/ 
  
 line-chart.png"", dpi=500)
  
 [
  157 
 ]",NA
Compare users through heat map,"Heat map is a popular visualization tool. It is one of the best tools for 
 multivariate 
 and 
 timeseries
  analysis and helps us in comparing the multiple variables visually. We will 
 learn to use the heat map to study the GitHub users. First, we will aggregate some 
 information at the user level and then compare it across multiple users. We will 
 consider the following variables to perform this analysis at the user level and perform 
 aggregation based on the variable at the user level (
 full_name
 ).
  
 Variable
  
 Aggregation
  
 Details
  
 full_name
  
 N.A.
  
 User's name
  
 id
  
 Count
  
 Number of repositories
  
 Size
  
 Average
  
 Average size of repositories
  
 Watchers_count
  
 Sum
  
 Total Watchers to user's repositories
  
 forks_count
  
 Sum
  
 Total Forks to his repositories
  
 open_issues_count
  
 Sum
  
 Total Issues in his repositories
  
 desclen
  
 Average
  
 Average length of title description
  
 dayscreated
  
 Average
  
 Average age of user's repositories
  
 [
  158 
 ]",NA
EDA – correlation analysis,"Correlation analysis measures the statistical relationship between two different 
 variables. The result will show how the change in one parameter would impact the 
 other parameter. Correlation analysis is a very important concept, popular in the field 
 of predictive analytics. Also, it is mandatory to complete the correlations analysis 
 before building the model and before arriving at a conclusion about variable 
 relationships. Though correlation analysis helps us in understanding the association 
 between two variables in a dataset, it can't explain, or measure, the cause.
  
 So far, we haven't explored the relationship between different parameters. In this 
 section, we will focus on the 
 bivariate
  and 
 multivariate
  analysis of the GitHub dataset.
  
 We will use the dataset that was created for plotting the heat map to perform the 
 correlation analysis. The following code will get us the required dataset:
  
 cordata<- ausersubset[c(""id"",""full_name"",""size"",""watchers_count"", 
 ""forks_count"", ""open_issues_count"", ""desclen"", ""dayscreated"", 
 ""daysupdated"", ""dayspushed"")]
  
 cdata<- sqldf(""select full_name, count(id), avg(size), 
  
 sum(watchers_count), sum(forks_count), sum(open_issues_count), 
 avg(desclen), avg(dayscreated), avg(daysupdated), 
  
 avg(dayspushed) from cordata group by full_name"")
  
 colnames(cdata) <- c(""Name"", ""Repositories"", ""AverageSize"", 
 ""Watchers"", ""Forks"", ""Issues"", ""Avg_desc_length"", 
  
 ""Avg_days_since_created"", ""Avg_days_since_updated"", 
  
 ""Avg_days_since_pushed"")
  
 First let's check the relationship between the parameters 
 Watchers
  and 
 Forks
 . We 
 can identify the relationship between these parameters using the function 
 cor
 :
  
 cor(cdata$Forks, cdata$Watchers)
  
 We get the following output:
  
 [1] 0.8934664
  
 [
  161 
 ]",NA
How Watchers is related to Forks,"The correlation can also be represented in a graphical format using the scatter plot. 
 We will plot the scatter plot using 
 ggplot
 . The following code will generate the scatter 
 plot where the two variables passed as inputs will be considered as two axes and the 
 values will be plotted.
  
 ggplot(cdata, aes(x=Forks, y=Watchers)) +
  
 geom_point(shape=1) +
  
 geom_smooth(method=lm)
  
 ggsave(file=""C:/Users/Sharan/Desktop/SMM/Chapter 5/Pics/ 
 scatter-plot.png"", dpi=500)
  
 We get the following output:
  
  
 [
  162 
 ]",NA
Correlation with regression line,"We have already seen in the heat map that the parameters 
 Avg_days_since_created 
 and 
 Avg_days_since_pushed
  seem to have a very good positive relationship. We can 
 test out the same using the correlation function as well as the scatter plot:
  
 cor(cdata$Avg_days_since_created, cdata$Avg_days_since_pushed)
  
 We get the following output:
  
 [1] 0.7899095
  
 For generating the scatter plot, use the following code:
  
 ggplot(cdata, aes(x=Avg_days_since_created, 
 y=Avg_days_since_pushed)) +
  
 geom_point(shape=1) +
  
 geom_smooth(method=lm)
  
 ggsave(file=""C:/Users/Sharan/Desktop/SMM/Chapter 5/Pics/ 
 scatter-plot2.png"", dpi=500)
  
 We get the following output:
  
  
 [
  163 
 ]",NA
Correlation with local regression curve,"We have successfully embedded the linear regression line over the scatter plots but 
 let's try to replace it with the 
 Loess smoothed fit curve
 . This will give us a detailed view 
 and show us how the relationship is at different ranges of values. Let's repeat the 
 preceding plot using this method. In order to bring in the smoothed curve, we use the 
 function 
 geom_smooth
 , the default smoothed curve will be plotted based on the 
 method 
 loess
 . We can change it using the parameter 
 method
 . The code is as follows:
  
 ggplot(cdata, aes(x=Avg_days_since_created, 
 y=Avg_days_since_pushed)) +
  
 geom_point(shape=1) +    # Use hollow circles
  
 geom_smooth()
  
 ggsave(file=""C:/Users/Sharan/Desktop/SMM/Chapter 5/Pics/ 
 scatter-plot3.png"", dpi=500)
  
 We get the following output:
  
  
 [
  164 
 ]",NA
Correlation on segmented data,"Now, let's go to the previous scatter plot add a third variable. We will create an 
 issues 
 flag, where the users who had less than 10 issues in all the repositories considered 
 together will be flagged as 
 0
 , whereas the other users will be flagged as 
 1
 . This can be 
 implemented through the following code:
  
 cordata<- ausersubset[c(""id"",""full_name"",""size"",""watchers_count"", 
 ""forks_count"", ""open_issues_count"", ""desclen"", ""dayscreated"", 
 ""daysupdated"", ""dayspushed"", ""has_issues"")]
  
 cdata<- sqldf(""select full_name, count(id), avg(size), 
  
 sum(watchers_count), sum(forks_count), sum(open_issues_count), 
 avg(desclen), avg(dayscreated), avg(daysupdated), avg(dayspushed), 
 sum(has_issues) from cordata group by full_name"")
  
 colnames(cdata) <- c(""Name"", ""Repositories"", ""AverageSize"", 
 ""Watchers"", ""Forks"", ""Issues"", ""Avg_desc_length"", 
  
 ""Avg_days_since_created"", ""Avg_days_since_updated"", 
  
 ""Avg_days_since_pushed"", ""IssuesF"")
  
 cdata$IssuesF<- as.factor(cdata$IssuesF)
  
 cdata$IssuesF[cdata$Issues< 10]  = 0
  
 cdata$IssuesF[cdata$Issues>= 10]  = 1
  
 cdata$IssuesF<- as.factor(cdata$IssuesF)
  
 After getting the desired data, we will convert the 
 flag
  column to a factor using the 
 function 
 as.factor
 . Now, we will use the plotting function as before, but we will add 
 one more parameter, 
 color
 , to differentiate the different categories. Hence, the first line 
 of the code would be something like this:
  
 ggplot(cdata, aes(x=Avg_days_since_created, 
 y=Avg_days_since_pushed, color=IssuesF)) +
  
 geom_point(shape=1) +    # Use hollow circles
  
 geom_smooth()
  
 ggsave(file=""scatter-plot3.png"", dpi=500)
  
 [
  165 
 ]",NA
Correlation between the languages that user's ,NA,NA
use to code,"In this section, we will answer just one question: 
 given a programmer programs in one 
 language, what would be the other languages that they might know to code?
  
 [
  166 
 ]",NA
How to get the trend of correlation?,"So far, we have explored the correlation between different parameters and plotted the 
 same in the scatter plot to visualize the relationship. Now, let's explore a methodology 
 to find the trend in the correlation. We will check how the correlation between 
 different parameters is affected by time. The rolling correlation will help us 
 understand the volatility in the relationship between the parameters.
  
 To solve this problem, let's consider the two parameters 
 watchers_count
  and 
 forks_count
 . First, we need to extract the data from the master dataset. We will use 
 the following code to select those columns by their names:
  
 mdata<- ausersubset[c(""created_at"", ""watchers_count"", 
 ""forks_count"")]
  
 After extracting the data, we need to convert the column 
 created_at
 , which is in the 
 date-time format, to the date format supported by R using the function 
 as.POSIXct
 . 
  
 Also, we need to convert the other two columns to the numeric format so that we can 
 perform aggregation at a daily level based on the date column and then find 
 correlation between the two numeric columns. The code is as follows:
  
 mdata$created_at<- as.POSIXct(strptime(mdata$created_at, 
 ""%Y-%m-%d""))
  
 [
  168 
 ]",NA
Reference,"• 
  
 For improvisation of the chart appearance, you can refer to the 
 following links:
  
 °
  
 http://www.cookbook-r.com/Graphs/
  
 °
  
 http://www.statmethods.net/advgraphs/ggplot2.html
  
 °
  
 http://cran.r-project.org/web/packages/ggplot2/index.
  
 html
  
 • 
  
 • 
  
 °
  
 http://www.r-bloggers.com/search/ggplot2
  
 For more information on the GitHub package in R and the GitHub API, visit 
 the following links:
  
 °
  
 https://github.com/cscheid/rgithub
  
 °
  
 https://developer.github.com/v3/
  
 Online courses on EDA can be found at:
  
 °
  
 https://www.coursera.org/course/exdata
  
 °
  
 https://www.udacity.com/course/data-analysis-with-r--
  
 ud651
  
 [
  171 
 ]",NA
Business cases,"Some of the business cases that can be implemented using the GitHub data are as 
 follows:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Identify the popular programmers in different languages.
  
 The online courses' websites can target potential subscribers based 
 on suitable patterns in behavior.
  
 Provide recommendations to users on which user to follow as well as 
 which repository to look out for.
  
 Identify the best people to target for a new open source project 
 development in Java.
  
 Plot the trend on languages; here, the data point will be the number of 
 repositories in a language. In order to get the trend, we need to pull the data 
 on a daily basis to know how many new repositories are created and which 
 language is used to develop them. This will help in understanding the lifecycle 
 of language popularity.
  
 Build a regression model to predict the number of watchers a repository 
 would get within a month of launch.
  
 [
  172 
 ]",NA
Summary,"In this chapter, we covered the steps involved in the creation of the app on GitHub as 
 well as the procedure for the installation and authentication using the GitHub package 
 for R. We also discussed the public data that can be accessed using the GitHub API 
 from R, implementation of some graphical and nongraphical EDA techniques on the 
 GitHub data, and how to perform, as well as, interpret the correlation analysis.
  
 By implementing the various EDA techniques and exploring the questions that were 
 answered using it, we get a better understanding of when to use what kind of 
 techniques for easier communication.
  
 In the next chapter we will explore APIs of a few more social media sites such as 
 LinkedIn, Tumblr, Wikipedia, Google Maps, Blogger, Foursquare and Quora. We will 
 also cover use-cases that can be implemented.
  
 [
  173 
 ]",NA
More Social Media Websites,"So far, we have discussed how to use the APIs of Twitter, Facebook, Instagram, and 
 GitHub to make use of vital concepts and some machine learning techniques/ algorithms 
 to answer critical business questions. In this chapter, we will see APIs of other social 
 media websites, the methodology involved to pull data, the analysis that can be 
 implemented, and cover some critical problems that can be solved.
  
 Social media data is generally massive, noisy, and dynamic in nature; hence, taming 
 data and performing the data analysis becomes challenging, but with a good grasp 
 on the concepts it will be an amazing journey. With such huge data, they become 
 rich sources of information that can help in various research fields and the business 
 world.
  
 The objective of this chapter is to understand the methodology involved in accessing 
 data from social media websites, understanding the huge scope that social data analysis 
 uncovers, as well as highlights on business cases that could be solved and the limitations 
 involved.
  
 The topics that will be covered in this chapter are as follows:
  
 • 
  
 Searching on social media
  
 • 
  
 Accessing product reviews from sites
  
 • 
  
 Retrieving data from Wikipedia
  
 • 
  
 Using the Tumblr API
  
 • 
  
 Accessing data from Quora
  
 • 
  
 Mapping solutions using Google Maps
  
 • 
  
 Professional network data using LinkedIn
  
 • 
  
 Getting Blogger data
  
 • 
  
 Retrieving venue data from Foursquare
  
 • 
  
 Yelp and other networks
  
 [
  175 
 ]",NA
Searching on social media,"We will discuss how to use the package 
 SocialMediaMineR
  in R, which would allow us to 
 consider a few URLs and learn about the reach of those URLs in various social media 
 websites. This package can get us details such as number of likes, shares, and comments 
 on social media websites such as Facebook, Twitter, LinkedIn, Pinterest, reddit, and a few 
 others. This package also has functions for pulling data specific to individual social 
 networking sites.
  
 We need to install the package and load it to the R environment, which can be done 
 using the following code:
  
 install.packages(""SocialMediaMineR"")
  
 library(SocialMediaMineR)
  
 After loading the package using the function 
 library
 , we will proceed with using the 
 various functions of the package. Let's start with the function 
 get_facebook
 . 
  
 This function will search for the mentioned URL in the social networking site 
 Facebook
 , and it will return the mentioned URL; the normalized URL; and the 
 number of Facebook shares, likes, comments, total hits, and clicks. This function 
 can take input as any URL, normalize the URL, and then retrieve the details. 
  
 The code is as follows:
  
 get_facebook(""http://www.bbc.com/"")
  
 We get the following output:
  
  
 The 
 get_pinterest
  function will give us the number of pins on Pinterest for the 
 mentioned URL. This function can take any URL as input—blogs, YouTube videos 
 shared, marketing campaigns, and so on. The code is as follows:
  
 get_pinterest(""http://www.bbc.com/"")
  
 We get the following output:
  
  url count
  
  1 http://www.bbc.com/  1281
  
 We can perform analysis such as correlation on performance across different sites, 
 trend analysis to decode the seasonality, reach to the audience, and much more on the 
 preceding dataset.
  
 [
  176 
 ]",NA
Accessing product reviews from sites,"Online product reviews are a very good source of information. They can be used to 
 judge a brand or a product. It becomes very difficult to read all the reviews, so we can 
 write a program to get the product reviews. Let's see one of the ways to extract the 
 customer review data from Amazon. For example, let's consider the movie 
 Transformers – Age of Extinction
  and see the customer reviews:
  
 urll<- 
  
 'http://www.amazon.com/gp/video/detail/B00L83TQR6?ie=UTF8&redirect 
 =true&ref_=s9_nwrsa_gw_g318_i1'
  
 First, we get the relevant URL and store it in a variable so that it can be used in the 
 functions. Then, we need to parse the HTML content of the page and save it to the 
 variable 
 doc
 . In order to do so, we need to import the package 
 XML
 . Now, the parsed 
 HTML is stored in the variable 
 doc
 . Please follow the link for more details on the HTML 
 DOM: 
 http://www.w3schools.com/jsref/dom_obj_document.asp
 . The code is as 
 follows:
  
 library(XML)
  
 doc<- htmlParse(urll)
  
 From this parsed data, we can get the required information by specifying the class in 
 which the content is present, using the function 
 xpathSApply
 . In our case, the 
 customer reviews are present in the tag 
 div
  with the class name 
 a-section
 . The 
 following code will extract the customer reviews.
  
 review<- xpathSApply(doc,'//div[@class=""a-section""]',xmlValue)
  
 data.frame(review)
  
  
 Note that the websites, in the preceding case Amazon, might 
  
  
 change the DOM structure. Hence, verify the DOM structure before 
  
 executing the preceding code. Also, note that this data would require 
  
 processing before consumption in the analysis.
  
 [
  180 
 ]",NA
Retrieving data from Wikipedia,"Wikipedia is an open source encyclopedia project developed collaboratively by multiple 
 people across the world. This is a rich source of information, and we can find content 
 about anything in this world. In this section, we are going to check out the ways to 
 extract the content from Wikipedia for our analysis. We will concentrate only on the 
 tabular content.
  
 [
  181 
 ]",NA
Using the Tumblr API,"Tumblr is a microblogging and social networking platform. Tumblr, as of August 2015, 
 hosts around 248 million blogs and about 117 billion posts, and over 75 million posts 
 are created on a daily basis. In this section, we will see how to access the data from 
 Tumblr and, in the process, understand how the steps involved are different from the 
 methodology discussed so far.
  
 [
  190 
 ]",NA
Accessing data from Quora,"Quora is a popular question and answer website where questions are asked, answered, 
 and managed by the community members and the entire operations is 
 gamified
 . The 
 interesting answers can get some points and the users could also shell out some points 
 to get some of their questions answered by certain people.
  
 There are ways to get the data from Quora, but there is an unofficial API which returns 
 the data in the JSON format. It is actually a set of URLs that will provide us with the 
 required information. These URLs will also work in browsers. Once we get the data in 
 the JSON format, we can convert it into the data frame format later.
  
 Let's see some of the URLs and ways to use it in R. The API's base URL is 
 http://quora.christopher.su
 . To the base URL, we need to add the 
 following to get the relevant data:
  
 • 
  
 /users/<user>/activity/answers
  
 • 
  
 /users/<user>/activity/user_follows
  
 • 
  
 /users/<user>/activity/want_answers
  
 • 
  
 /users/<user>/activity/upvotes
  
 • 
  
 /users/<user>/activity/review_requests
  
 First, we will try to get the user's profile details. The following code will provide us 
 with the details of the specified user in the JSON format. It provides some basic 
 information like the followers, following, number of posts and the total number of 
 edits made, questions asked, and the answers made:
  
 udata<- fromJSON(""http://quora.christopher.su/users/ 
 Sharan-Kumar-R"")
  
 [
  196 
 ]",NA
Mapping solutions using Google Maps,"Google Maps provides mapping solutions. By using the R package 
 RgoogleMaps
 , we can 
 get the static images from Google Maps using the name of the place or using the latitude 
 and longitude of that place. We can also use the map as the background and plot 
 location-specific charts.
  
 In this section, we will see how to access the Google Maps API from R. We need to first 
 install the package 
 RgoogleMaps
 :
  
 install.packages(""RgoogleMaps"")
  
 library(RgoogleMaps)
  
 [
  198 
 ]",NA
Professional network data from LinkedIn,"LinkedIn is a social networking website for people in professional jobs. It has over 
 364 million users across the world. Many companies have started using LinkedIn 
 to publish their job requirements and as a medium to showcase their skillset to 
 the world.
  
 In order to access the LinkedIn data, we need to use the package 
 Rlinkedin
 . 
 Get the latest package from the GitHub repository using the following code:
  
 install_github(""mpiccirilli/Rlinkedin"")
  
 library(Rlinkedin)
  
 For authentication, we need to have a LinkedIn app. It can be registered at 
 https://www.linkedin.com/developer/apps
 . Similar to the way we had created 
 an app in Facebook, Instagram, and GitHub, we need to create an app at LinkedIn as 
 well. Once the app is created, it will appear in your developers login. 
  
 [
  203 
 ]",NA
Getting Blogger data,"Blogger is a blog publishing service that was bought by Google in the year 2003 and it 
 has more than 500,000 blogs. Blogger provides numerous APIs to access the data, 
 provided you have the API key and know some required data such as the blog ID, user 
 ID, and post ID based on the data that you are trying to access.
  
 In order to get the API key, one has to create a project in the Google developer's 
 console. Here's the screenshot of the Google developer's console:
  
  
 On opening the console, create a new project, open it and click on the link 
 APIs &auth
 , 
 select the credential, look to see if any client IDs and client keys are available, and if not 
 then create one. On creation, you will be provided with a client ID and an API key that can 
 be used to access the data.
  
  
 [
  208 
 ]",NA
Retrieving venue data from Foursquare,"Foursquare is a local search services company that offers its service on the mobile 
 platform as well as a website. Log in to Foursquare as a developer in order to get the 
 idea of the API request and response patterns. On registering, a unique OAuth token is 
 generated.
  
 [
  211 
 ]",NA
Use cases,"This enormous amount of data could be very powerful. Some of the use cases possible 
 include details such as the items that are most liked by the user based on mining the 
 user's tips, performing correlations between different venues and getting to know more 
 about the most similar venues, and, based on the likes data, we can also provide 
 recommendations using collaborative filtering. Based on the venue data, we can also 
 compute the clusters.
  
 There are few more use cases that would help the venues, such as finding out the 
 performance of a venue in terms of likes and positive tips from user, and comparing it 
 with other nearby venues in the same category. Mining the text data would help the 
 venues in knowing the areas of improvement. With exploration of the data, we can also 
 get to know about the services that are most valued by the customers. This would help 
 the venues in improving their rating.",NA
Yelp and other networks,"Yelp is a crowdsourced local business review and social networking site. Over 31 million 
 people access Yelp's website each month. Getting the data from Yelp is quite similar to 
 how we get it from the other social networks. The steps are as follows:
  
 1. First, log in as a developer.
  
 2. Then, register and get the authentication credentials.
  
 3. Get the standard API request URL.
  
 4. Pass the URL along with the authentication credentials to either the function 
  
 fromJSON
  or 
 GET
 .
  
 5. Data will be retrieved in the JSON format.
  
 6. Read the required data and convert it to data frame for further analysis.
  
 To know about the various API services offered by Yelp, visit 
 https://www.yelp. 
 com/developers/documentation/v2/overview
 .
  
 Websites such as Glassdoor and Indeed provide API access on request. The process 
 involved in working with those APIs would be similar to those we have covered so far.
  
 [
  218 
 ]",NA
Limitations,"The only limitation in performing social media mining is that the APIs consistently 
 undergo changes with respect to the accessibility of the data and also in the way in which 
 they work. Using the LinkedIn API, users were able to download the complete information 
 about their network, but later it was made 
 on request
 . Similarly, the Facebook API went 
 through lot of changes too. When the data is accessed through the R package, the user 
 needs to update. Alternatively, when accessed using the URL in the function 
 fromJSON
 , 
 then the API needs to be updated.
  
 The other limitation is on the quality of the data. Since all this data is created by 
 people online, there is always a possibility for skewness in the data. Therefore, 
 measures should be taken to keep a check on the quality.",NA
Summary,"In this chapter, we saw how to access many of the social media websites and also 
 discussed the various use cases that could be implemented. The methodology involved 
 in accessing data through the APIs are similar to one another; while most APIs require 
 authentication, some APIs can be accessed without authentication even in a browser. 
 Most APIs provide the data in the JSON format, but for some popular sites there are 
 packages built in R that can convert the data to a data frame while retrieving. This helps 
 in speeding up the analysis. These APIs provide us the data in a variety of formats: 
 structured in some cases, but unstructured in most cases. With a higher limit on the API 
 requests that can be called, the volume at which we can generate data is also quite high.
  
 In this book, we covered the methodologies to access the data from R using the APIs of 
 various social media sites such as Twitter, Facebook, Instagram, GitHub, Foursquare, 
 LinkedIn, Blogger, and a few more networks. This book also provided details on the 
 implementation of various use cases using R programming. Now, you should be 
 completely equipped to embark on your journey as a social media analyst.
  
 [
  219 
 ]",NA
Index,NA,NA
A,"active users 
  
  
 used, for building heterogeneous 
  
   
 dataset  142, 143 
  
 additional metrics 
  
  
 building  145-147 
  
 app 
  
  
 creating, on Facebook platform  56, 57 
  
 creating, on GitHub  136-138 
  
 avatar  193",NA
B,"Betweenness  67 
  
 Blogger  208 
  
 Blogger API usage 
  
  
 URL  211 
  
 Blogger data 
  
  
 obtaining  208-
 211 
  
  
 URL  209 
  
 business cases 
  
  
 defining  132, 172 
  
  
 implementing  90",NA
C,"celebrity users and location hashtags 
  
 reference  132 
  
 challenges, social media mining 
  
  
 Big Data  5 
  
  
 evaluation dilemma  6 
  
  
 noise removal error  5 
  
  
 sufficiency  5 
  
 Click-through rate (CTR)  56 
  
 closeness  68
  
 cluster  68 
  
 clustering  118 
  
 clustering analysis 
  
  
 reference  132 
  
 community detection 
  
  
 via clustering  18 
  
 correlation analysis, EDA 
  
  
 correlation, between languages  166-168 
  
 correlation, on segmented data  165, 166 
  
 correlation, with local regression 
  
   
 curve  164, 165 
  
  
 correlation, with regression line  163 
  
 defining  161, 162 
  
  
 references  171, 172 
  
  
 trend of correlation, obtaining  168-170 
  
 watchers, relating to forks  162, 163 
  
 CTR performance 
  
  
 measuring, for page  77-80 
  
 customer relationship 
  
   
 management (CRM)  3",NA
D,"data 
  
  
 accessing, from Quora  196-198 
  
  
 accessing, from R  97 
  
  
 retrieving, from Wikipedia  181-190 
 data access 
  
  
 comments, obtaining  102, 103 
  
  
 followers, obtaining  100 
  
  
 hashtag, using  104 
  
  
 public media, searching from specific 
  
  
 location  98 
  
  
 public media, searching for 
  
   
 specific hashtag  97
  
 [
  221 
 ]",NA
E,"EDA techniques 
  
  
 bivariate  148 
  
  
 multivariate  148 
  
  
 univariate  148 
  
 emotions, sentiment 
 package 
  
 anger  40 
  
  
 disgust  40 
  
  
 fear  40 
  
  
 joy  40 
  
  
 sadness  40 
  
  
 surprise  40 
  
 entities, tweet  23 
  
 exploratory data analysis 
  
  
 defining  148-150",NA
F,"Facebook app 
  
  
 about  56 
  
  
 URL  56 
  
 Facebook Graph API  56 
 Facebook Graph Search  6 
 Facebook page data 
  
  
 obtaining  71, 72 
  
 Facebook platform 
  
  
 app, creating  56, 57 
  
 Foursquare 
  
  
 about  211
  
 references  216, 217 
  
 use cases  218 
  
 venue data, retrieving from  211-217",NA
G,"GitHub 
  
  
 about  135 
  
  
 app, creating  136-138 
  
  
 data, accessing from R  141, 142 
  
  
 URL  142 
  
 GitHub package 
  
  
 authentication  139, 140 
  
  
 installing  139, 140 
  
 Google Maps 
  
  
 URL  203 
  
  
 used, for mapping solutions  198-203 
  
 graphical analysis, EDA 
  
  
 defining  150 
  
  
 distribution of forks, in GitHub  153-155 
  
 distribution of issues, in GitHub  153-155 
  
 distribution of watchers, in GitHub  153-155 
  
 language, for active GitHub users  150-152 
  
 repositories, updating  157, 158 
  
  
 repositories, with issues  156, 157 
  
  
 users, comparing through 
  
   
 heat map  158-161 
  
 graph mining  6",NA
H,"heterogeneous dataset 
  
  
 building, active users used  142, 143 
 HTML DOM 
  
  
 URL  180",NA
I,"igraph  65 
  
 influencers 
  
  
 about  74 
  
  
 based, on multiple posts  76, 77 
  
 based, on single post  74, 75 
  
 Instagram 
  
  
 about  93 
  
  
 account, creating  94 
  
  
 app, creating  94, 95
  
 [
  222 
 ]",NA
L,"LinkedIn 
  
  
 about  203 
  
  
 professional network data, 
   
 defining from  203-207 
 LinkedIn app 
  
  
 URL  203",NA
M,"methods 
  
  
 used, for visualizing data  19 
  
 mining algorithms 
  
  
 defining  14 
  
  
 opinion mining (sentiment analysis)  14 
 most popular destination 
  
  
 finding  113 
  
  
 locations  114 
  
  
 locations, most talked about  115 
  
  
 locations, with most likes  115 
  
  
 people, talking about locations  116, 117 
  
 repeating locations  117, 118 
  
 multiple newsfeeds 
  
  
 updating  84-86",NA
N,"Naive Bayes  39 
  
 network analysis 
  
  
 Betweenness  67 
  
  
 closeness  68 
  
  
 cluster  68 
  
  
 communities  69, 70 
  
  
 defining  62-64 
  
  
 degree  66 
  
  
 social network analysis  64-66 
 network visualization 
  
  
 defining  64 
  
 Neural Networks (NN)  16 
  
 new app, Twitter 
  
  
 URL  25",NA
O,"Oauth 2.0 
  
  
 and OAuth, comparing  10 
  
 defining  8-10 
  
 Open Authorization (OAuth) 
  
 about  1 
  
  
 URL  8",NA
P,"partnership program 
  
  
 URL  207 
  
 part-of-speech tagging (pos)  15 
  
 pictures 
  
  
 clustering  118-123 
  
 pie chart  156 
  
 popular personalities 
  
  
 active users  111 
  
  
 defining  110 
  
  
 overall top users  112 
  
  
 users, with most number of followers  110 
  
 user, who follows most number 
  
   
 of people  111 
  
  
 viral media, finding  112 
  
 product reviews 
  
  
 accessing, from sites  180, 181",NA
Q,"quintuple  15 
  
 Quora 
  
  
 about  196 
  
  
 data, accessing from  196-198 
  
 references  196-198",NA
R,"R 
  
  
 cleaning  14 
  
  
 data, accessing from  97 
  
  
 GitHub data, accessing from  141, 142 
  
 preprocessing  14 
  
 rbind function 
  
  
 URL  143
  
 [
  223 
 ]",NA
S,"sentiment analysis 
  
  
 steps  15-18 
  
 sentiment analysis Wordcloud 
  
  
 Classify_emotion  12 
  
  
 Classify_polarity  13 
  
 sentiment orientation (SO)  15 
  
 sentiment package  40 
  
 sites 
  
  
 product reviews, accessing from  180, 181 
 social media 
  
  
 defining  1-3 
  
  
 platforms  3 
  
  
 searching on  176-179 
  
 social media data 
  
  
 about  175 
  
  
 references  13 
  
 social media mining 
  
  
 about  4 
  
  
 authentication, obtaining from social 
  
  
 website  8-10
  
  
 challenges  4, 5 
  
  
 data visualization R packages  10 
  
  
 example  19 
  
  
 process  7 
  
  
 techniques  6 
  
 social network analysis 
  
  
 using  70 
  
 solutions 
  
  
 mapping, Google Maps used  198-203 
  
 spam detection  80 
  
 spam detection algorithm 
  
  
 implementing  80-83 
  
 supervised machine learning algorithms  17 
 Support Vector Machine (SVM)  16",NA
T,"techniques, social media mining 
  
 graph mining  6 
  
  
 text mining  7 
  
 temporary token 
  
  
 URL  58 
  
 text mining  7 
  
 timeline  24 
  
 trending topics 
  
  
 defining  73 
  
  
 trend analysis  73, 74 
  
 Tumblr 
  
  
 references  194-196 
  
 Tumblr API 
  
  
 URL  191 
  
  
 using  190-196 
  
 tweets 
  
  
 about  23 
  
  
 constraints  29 
  
 Twitter 
  
  
 about  23 
  
  
 defining  21, 22 
  
  
 URL  25 
  
 Twitter API connection 
  
  
 creating  24 
  
  
 new app, creating  25-27 
  
  
 trending topics, finding  28 
  
  
 tweets, searching  29
  
 [
  224 
 ]",NA
W,"Wikipedia 
  
  
 data, retrieving from  181-190",NA
Y,"Yelp 
  
 about  218 
  
 data, obtaining from  218 
  
 limitations  219 
  
 URL  218
  
 [
  225 
 ]",NA
Thank you for buying ,NA,NA
Mastering Social Media Mining with R,NA,NA
About Packt Publishing,"Packt, pronounced 'packed', published its first book, 
 Mastering phpMyAdmin for Effective MySQL 
 Management
 , in April 2004, and subsequently continued to specialize in publishing highly 
 focused books on specific technologies and solutions.
  
 Our books and publications share the experiences of your fellow IT professionals in adapting and 
 customizing today's systems, applications, and frameworks. Our solution-based books give you the 
 knowledge and power to customize the software and technologies you're using to get the job done. 
 Packt books are more specific and less general than the IT books you have seen in the past. Our 
 unique business model allows us to bring you more focused information, giving you more of what 
 you need to know, and less of what you don't.
  
 Packt is a modern yet unique publishing company that focuses on producing quality, 
 cutting-edge books for communities of developers, administrators, and newbies alike. For 
 more information, please visit our website at 
 www.packtpub.com
 .",NA
About Packt Open Source,"In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to 
 continue its focus on specialization. This book is part of the Packt Open Source brand, home to 
 books published on software built around open source licenses, and offering information to 
 anybody from advanced developers to budding web designers. The Open Source brand also runs 
 Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each open source project 
 about whose software a book is sold.",NA
Writing for Packt,"We welcome all inquiries from people who are interested in authoring. Book proposals should be 
 sent to 
 author@packtpub.com
 . If your book idea is still at an early stage and you would like to 
 discuss it first before writing a formal book proposal, then please contact us; one of our 
 commissioning editors will get in touch with you. 
  
 We're not just looking for published authors; if you have strong technical skills but no writing 
 experience, our experienced editors can help you develop a writing career, or simply get some 
 additional reward for your expertise.",NA
Social Media Mining with R ,"ISBN: 978-1-78328-177-0               Paperback: 122 pages
  
 Deploy cutting-edge sentiment analysis techniques to 
 real-world social media data using R
  
 1. 
  
 2. 
  
 3. 
  
 Learn how to face the challenges of analyzing 
 social media data.
  
 Get hands-on experience with the most 
  
 common, up-to-date sentiment analysis tools 
 and apply them to data collected from social 
 media websites through a series of in-depth 
 case studies, which includes how to mine 
 Twitter data.
  
 A focused guide to help you achieve practical 
 results when interpreting social media data.",NA
Learning Data Mining with R ,"ISBN: 978-1-78398-210-3              Paperback: 314 pages
  
 Develop key skills and techniques with R to create 
 and customize data mining algorithms
  
 1. 
  
 2. 
  
 3. 
  
 Develop a sound strategy for solving predictive 
 modeling problems using the most popular data 
 mining algorithms.
  
 Gain understanding of the major methods of 
 predictive modeling.
  
 Packed with practical advice and tips to help 
 you get to grips with data mining.
  
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
R for Data Science ,"ISBN: 978-1-78439-086-0             Paperback: 364 pages
  
 Learn and explore the fundamentals of data science 
 with R
  
 1. 
  
 2. 
  
 3. 
  
 Familiarize yourself with R programming 
 packages and learn how to utilize them 
 effectively.
  
 Learn how to detect different types of data 
 mining sequences.
  
 A step-by-step guide to understanding R scripts 
 and the ramifications of your changes.",NA
Instant Social Media Marketing ,NA,NA
with HootSuite,"ISBN: 978-1-84969-666-1             Paperback: 60 pages
  
 Manage and enhance your social media marketing 
 with HootSuite
  
 1. 
  
 2. 
  
 3. 
  
 4. 
  
 Learn something new in an Instant! A 
 short, fast, focused guide delivering 
 immediate results.
  
 Presents you with an insight into your 
 organization’s social assets.
  
 Packed with useful tips to automate your social 
 media sharing and tracking.
  
 Analyze social media traffic and generate 
 reports using HootSuite.
  
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
