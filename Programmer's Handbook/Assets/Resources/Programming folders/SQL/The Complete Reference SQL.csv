Larger Text,Smaller Text,Symbol
SQL: The Complete Reference,"James R. Groff 
  
 Paul N. Weinberg
  
 Publisher 
  
 Brandon A Nordin
  
 Associate Publisher and Editor-in-Chief 
  
 Scott Rogers
  
 Senior Acquisitions Editor 
  
 Wendy Rinaldi
  
 Acquisitions Editor 
  
 Jane K. Brownlow
  
 Project Editor 
  
 Heidi Poulin
  
 Editorial Assistant 
  
 Monica Faltiss
  
 Copy Editor 
  
 Nancy Crumpton
  
 Proofreader 
  
 Rhonda Holmes
  
 Indexer 
  
 Valerie Robbins
  
 Computer Designer 
  
 Jani Beckwith 
  
 Michelle Galicia
  
 Illustrators 
  
 Robert Hansen 
  
 Brian Wells 
  
 Beth Young
  
 - 4 -",NA
Acknowledgments,"Special thanks to Matan Arazi for doing such an exceptional job assembling the Bonus 
 CD-ROM. He pulled off a real miracle to squeeze all five SQL, DBMS products onto a 
 single CD, a technical feat that would not have been possible without his diligent tenacity
  
 Thanks also to everyone at Osborne for pulling it all together, including Jane Brownlow and 
 Wendy Rinaldi for doing tag-team duty as our acquisitions editors, and to Heidi Poulin for 
 her meticulous attention to detail.
  
 - 5 -",NA
Preface,NA,NA
Overview,"SQL: The Complete Reference
  provides a comprehensive, in-depth treatment of the SQL 
 language for both technical and non-technical users, programmers, data processing 
 professionals, and managers who want to understand the impact of SQL in the computer 
 market. This book offers a conceptual framework for understanding and using SQL, 
 describes the history of SQL and SQL standards, and explains the role of SQL in the 
 computer industry today. It will show you, step-by-step, how to use SQL features, with 
 many illustrations and realistic examples to clarify SQL concepts. The book also 
  
 compares SQL products from leading DBMS vendors  describing their advantages, 
 benefits, and trade-offs  to help you select the right product for your application. The 
 accompanying CD contains actual trial versions of five leading SQL databases, so you can 
 try them for yourself and gain actual experience in using major database products from 
 Oracle, Microsoft, Sybase, Informix, an IBM.
  
 In some of the chapters in this book, the subject matter is explored at two different levels—a 
 fundamental description of the topic, and an advanced discussion intended for computer 
 professionals who need to understand some of the ""internals"" behind SQL. The more 
 advanced information is covered in sections marked with an asterisk (*). You do not need to 
 read these sections to obtain an understanding of what SQL is and what it does.",NA
How this Book Is Organized,"The book is divided into six parts that cover various aspects of the SQL language:
  
 •
 Part One, ""An Overview of SQL,""
  provides an introduction to SQL and a market 
 perspective of its role as a database language. Its four chapters describe the history of 
 SQL, the evolution of SQL standards, and how SQL relates to the relational data model 
 and to earlier database technologies. Part One also contains a quick tour of SQL that 
 briefly illustrates its most important features and provides you with an overview of the 
 entire language early in the book.
  
 •
 Part Two, ""Retrieving Data,""
  describes the features of SQL that allow you to perform 
 database queries. The first chapter in this part describes the basic structure of the SQL 
 language. The next four chapters start with the simplest SQL queries, and progressively 
 build to more complex queries, including multi-table queries, summary queries, and 
 queries that use subqueries.
  
 •
 Part Three, ""Updating Data,""
  shows how you can use SQL to add new data to a 
 database, delete data from a database, and modify existing database data. It also 
 describes the database integrity issues that arise when data is updated, and how SQL 
 addresses these issues. The last of the three chapters in this part discusses the SQL 
 transaction concept and SQL support for multi-user transaction processing.
  
 •
 Part Four, ""Database Structure,""
  deals with creating and administering a SQL-based 
 database. Its four chapters tell you how to create the tables, views, and indexes that form 
 the structure of a relational database. It also describes the SQL security scheme that 
 prevents unauthorized access to data, and the SQL system catalog that describes the 
 structure of a database. This part also discusses the significant differences between the 
 database structures supported by various SQL-based DBMS products.
  
 •
 Part Five, ""Programming with SQL,""
  describes how application programs use SQL for 
 database access. It discusses the embedded SQL specified by the ANSI standard and 
 used by IBM, Oracle, Ingres, Informix, and most other SQL-based DBMS products. It also 
 describes the dynamic SQL interface that is used to build general-purpose database tools, 
 such as report writers and database browsing programs. Finally, this 
  
 - 6 -",NA
Conventions Used in this Book,"SQL: The Complete Reference
  describes the SQL features and functions that are 
 available in the most popular SQL-based DBMS products and those that are described in 
 the ANSI/ISO SQL standards. Whenever possible, the SQL statement syntax described in 
 the book and used in the examples applies to all dialects of SQL. When the dialects differ, 
 the differences are pointed out in the text, and the examples follow the most common 
 practice. In these cases, you may have to modify the SQL statements in the examples 
 slightly to suit your particular brand of DBMS.
  
 Throughout the book, technical terms appear in italics the first time that they are used and 
 defined. SQL language elements, including SQL keywords, table and column names, and 
 sample SQL statements appear in an uppercase monospace font. SQL API function names 
 appear in a lowercase monospace font. Program listings also appear in monospace font, 
 and use the normal case conventions for the particular programming language (uppercase 
 for COBOL and FORTRAN, lowercase for C). Note that these conventions are used solely 
 to improve readability; most SQL implementations will accept either uppercase or 
  
 lowercase statements. Many of the SQL examples include query results, which appear 
 immediately following the SQL statement as they would in an interactive SQL session. In 
 some cases, long query results are truncated after a few rows; this is indicated by a vertical 
 ellipsis (. . .) following the last row of query results.",NA
Why this Book Is for You,"SQL: The Complete Reference
  is the right book for anyone who wants to understand and 
 learn SQL, including database users, data processing professionals, programmers, 
 students, and managers. It describes—in simple, understandable language liberally 
 illustrated with figures and examples—what SQL is, why it is important, and how you use 
 it. This book is not specific to one particular brand or dialect of SQL. Rather, it describes 
 the standard, central core of the SQL language and then goes on to describe the 
 differences among the most popular SQL products, including Oracle, Microsoft SQL 
 Server, IBM's DB2, Informix Universal Server, Sybase Adaptive Server, and others. It also 
 explains the importance of SQL-based standards, such as ODBC and the ANSI/ISO SQL2 
 and evolving SQL3 standards.
  
 If you are a new user of SQL, this book offers comprehensive, step-by-step treatment of 
 the language, building from simple queries to more advanced concepts. The structure of 
 the book will allow you to quickly start using SQL, but the book will continue to be 
 valuable as you begin to use more complex features of the language. You can use the 
 SQL software on the companion CD to try out the examples and build your SQL skills.
  
 If you are a data processing professional or a manager, this book will give you a 
 perspective on the impact that SQL is having in every segment of the computer market—
 from personal computers, to mainframes, to online transaction processing systems and 
 data warehousing applications. The early chapters describe the history of SQL, its role in 
 the market, and its evolution from earlier database technologies. The final chapters 
 describe the future of SQL and the development of new database technologies such as 
 distributed databases, business intelligence databases, and object-relational database 
 capabilities.
  
 - 7 -",NA
Part I:,NA,NA
 An Overview of SQL,NA,NA
Chapter List,"Chapter Introduction 
  
 1: 
  
 Chapter A Quick Tour of SQL 
  
 2: 
  
 Chapter SQL In Perspective 
  
 3: 
  
 Chapter Relational Databases 
  
 4:",NA
Chapter 1: ,NA,NA
Introduction,NA,NA
Overview,"The SQL language and relational database systems based on it are one of the most 
 important foundation technologies in the computer industry today. Over the last decade, 
 the popularity of SQL has exploded, and it stands today as 
 the
  standard computer 
 database language. Literally hundreds of database products now support SQL, running on 
 computer systems from mainframes to personal computers and even handheld devices. 
 An official international SQL standard has been adopted and expanded twice. Virtually 
 every major enterprise software product relies on SQL for its data management, and SQL 
 is at the core of the database products from Microsoft and Oracle, two of the largest 
 software companies in the world. From its obscure beginnings as an IBM research project, 
 SQL has leaped to prominence as both an important computer technology and a powerful 
 market force.
  
 What, exactly, is SQL? Why is it important? What can it do, and how does it work? If SQL is 
 really a standard, why are there so many different versions and dialects? How do popular 
 SQL products like SQL Server, Oracle, Informix, Sybase, and DB2 compare? How 
  
 - 8 -",NA
The SQL Language,"SQL is a tool for organizing, managing, and retrieving data stored by a computer 
 database. The name ""SQL"" is an abbreviation for 
 Structured Query Language
 . For 
 historical reasons, SQL is usually pronounced ""sequel,"" but the alternate pronunciation 
 ""S.Q.L."" is also used. As the name implies, SQL is a computer 
 language
  that you use to 
 interact with a database. In fact, SQL works with one specific type of database, called a 
 relational database
 .
  
 Figure 1-1 shows how SQL works. The computer system in the figure has a 
 database 
 that 
 stores important information. If the computer system is in a business, the database might 
 store inventory, production, sales, or payroll data. On a personal computer, the database 
 might store data about the checks you have written, lists of people and their phone 
 numbers, or data extracted from a larger computer system. The computer program that 
 controls the database is called a 
 database management system
 , or DBMS.
  
  
 Figure 1-1:
  Using SQL for database access
  
 When you need to retrieve data from a database, you use the SQL language to make the 
 request. The DBMS processes the SQL request, retrieves the requested data, and returns 
 it to you. This process of requesting data from a database and receiving back the results is 
 called a database 
 query
 —hence the name Structured 
 Query
  Language.
  
 The name Structured Query Language is actually somewhat of a misnomer. First of all, 
 SQL is far more than a query tool, although that was its original purpose and retrieving 
 data is still one of its most important functions. SQL is used to control all of the functions 
 that a DBMS provides for its users, including:
  
 •
 Data definition
 . SQL lets a user define the structure and organization of the stored 
 data and relationships among the stored data items.
  
 •
 Data retrieval
 . SQL allows a user or an application program to retrieve stored data 
 from the database and use it.
  
 •
 Data manipulation
 . SQL allows a user or an application program to update the 
 database by adding new data, removing old data, and modifying previously stored 
 data.
  
 •
 Access control
 . SQL can be used to restrict a user's ability to retrieve, add, and modify 
 data, protecting stored data against unauthorized access.
  
 •
 Data sharing
 . SQL is used to coordinate data sharing by concurrent users, ensuring 
 that they do not interfere with one another.
  
 - 9 -",NA
The Role of SQL,"SQL is not itself a database management system, nor is it a stand-alone product. You 
 cannot go into a computer store and ""buy SQL."" Instead, SQL is an integral part of a 
 database management system, a language and a tool for communicating with the DBMS. 
 Figure 1-2 shows some of the components of a typical DBMS, and how SQL acts as the 
 ""glue"" that links them together.
  
  
 Figure 1-2:
  Components of a typical database management system
  
 The 
 database engine
  is the heart of the DBMS, responsible for actually structuring, 
 storing, and retrieving the data in the database. It accepts SQL requests from other 
 DBMS components, such as a forms facility, report writer, or interactive query facility, 
 from user-written application programs, and even from other computer systems. As the 
  
 - 10 -",NA
SQL Features and Benefits,"SQL is both an easy-to-understand language and a comprehensive tool for managing 
  
 data. Here are some of the major features of SQL and the market forces that have made 
  
 it successful:
  
 •Vendor independence
  
 •Portability across computer systems
  
 •SQL standards
  
 •IBM endorsement (DB2)
  
 •Microsoft commitment (ODBC and ADO)
  
 •Relational foundation
  
 •High-level, English-like structure
  
 •Interactive, ad hoc queries
  
 - 11 -",NA
Vendor Independence,"SQL is offered by all of the leading DBMS vendors, and no new database product over the 
 last decade has been highly successful without SQL support. A SQL-based database and 
 the programs that use it can be moved from one DBMS to another vendor's DBMS with 
 minimal conversion effort and little retraining of personnel. PC database tools, such as 
 query tools, report writers, and application generators, work with many different brands of 
 SQL databases. The vendor independence thus provided by SQL was one of the most 
 important reasons for its early popularity and remains an important feature today.",NA
Portability Across Computer Systems,"SQL-based database products run on computer systems ranging from mainframes and 
 midrange systems to personal computers, workstations, and even handheld devices. 
  
 They operate on stand-alone computer systems, in departmental local area networks, 
 and in enterprise-wide or Internet-wide networks. SQL-based applications that begin on 
 single-user systems can be moved to larger server systems as they grow. Data from 
 corporate SQL-based databases can be extracted and downloaded into departmental or 
 personal databases. Finally, economical personal computers can be used to prototype a 
 SQL-based database application before moving it to an expensive multi-user system.",NA
SQL Standards,"An official standard for SQL was initially published by the American National Standards 
 Institute (ANSI) and the International Standards Organization (ISO) in 1986, and was 
 expanded in 1989 and again in 1992. SQL is also a U.S. Federal Information Processing 
 Standard (FIPS), making it a key requirement for large government computer contracts. 
 Over the years, other international, government, and vendor groups have pioneered the 
 standardization of new SQL capabilities, such as call-level interfaces or object-based 
 extensions. Many of these new initiatives have been incorporated into the ANSI/ISO 
 standard over time. The evolving standards serve as an official stamp of approval for SQL 
 and have speeded its market acceptance.",NA
IBM Endorsement (DB2),- 12 -,NA
Microsoft Commitment (ODBC and ADO),"Microsoft has long considered database access a key part of its Windows personal 
 computer software architecture. Both desktop and server versions of Windows provide 
 standardized relational database access through Open Database Connectivity (ODBC), a 
 SQL-based call-level API. Leading Windows software applications (spreadsheets, word 
 processors, databases, etc.) from Microsoft and other vendors support ODBC, and all 
 leading SQL databases provide ODBC access. Microsoft has enhanced ODBC support 
 with higher-level, more object-oriented database access layers as part of its Object Linking 
 and Embedding technology (OLE DB), and more recently as part of Active/X (Active/X 
 Data Objects, or ADO).",NA
Relational Foundation,"SQL is a language for relational databases, and it has become popular along with the 
 relational database model. The tabular, row/column structure of a relational database is 
 intuitive to users, keeping the SQL language simple and easy to understand. The 
 relational model also has a strong theoretical foundation that has guided the evolution 
 and implementation of relational databases. Riding a wave of acceptance brought about 
 by the success of the relational model, SQL has become 
 the
  database language for 
 relational databases.",NA
"High-Level, English-Like Structure","SQL statements look like simple English sentences, making SQL easy to learn and 
 understand. This is in part because SQL statements describe the 
 data
  to be retrieved, 
 rather than specifying 
 how
  to find the data. Tables and columns in a SQL database can 
 have long, descriptive names. As a result, most SQL statements ""say what they mean"" 
 and can be read as clear, natural sentences.",NA
"Interactive, Ad Hoc Queries","SQL is an interactive query language that gives users ad hoc access to stored data. Using 
 SQL interactively, a user can get answers even to complex questions in minutes or 
 seconds, in sharp contrast to the days or weeks it would take for a programmer to write a 
 custom report program. Because of SQL's ad hoc query power, data is more accessible 
 and can be used to help an organization make better, more informed decisions. SQL's ad 
 hoc query capability was an important advantage over nonrelational databases early in its 
 evolution and more recently has continued as a key advantage over pure object-based 
 databases.",NA
Programmatic Database Access,"SQL is also a database language used by programmers to write applications that access a 
 database. The same SQL statements are used for both interactive and programmatic 
 access, so the database access parts of a program can be tested first with interactive SQL 
 and then embedded into the program. In contrast, traditional databases provided one set 
 of tools for programmatic access and a separate query facility for ad hoc requests, without 
 any synergy between the two modes of access.",NA
Multiple Views of Data,- 13 -,NA
Complete Database Language,"SQL was first developed as an ad hoc query language, but its powers now go far beyond 
 data retrieval. SQL provides a complete, consistent language for creating a database, 
 managing its security, updating its contents, retrieving data, and sharing data among 
 many concurrent users. SQL concepts that are learned in one part of the language can be 
 applied to other SQL commands, making users more productive.",NA
Dynamic Data Definition,"Using SQL, the structure of a database can be changed and expanded dynamically, even 
 while users are accessing database contents. This is a major advance over static data 
 definition languages, which prevented access to the database while its structure was 
 being changed. SQL thus provides maximum flexibility, allowing a database to adapt to 
 changing requirements while on-line applications continue uninterrupted.",NA
Client/Server Architecture,"SQL is a natural vehicle for implementing applications using a distributed, client/server 
 architecture. In this role, SQL serves as the link between ""front-end"" computer systems 
 optimized for user interaction and ""back-end"" systems specialized for database 
 management, allowing each system to do what it does best. SQL also allows personal 
 computers to function as front-ends to network servers or to larger minicomputer and 
 mainframe databases, providing access to corporate data from personal computer 
 applications.",NA
Extensibility and Object Technology,"The major challenge to SQL's continued dominance as a database standard has come 
 from the emergence of object-based programming, and the introduction of object-based 
 databases as an extension of the broad market trend toward object-based technology. 
 SQL-based database vendors have responded to this challenge by slowly expanding and 
 enhancing SQL to include object features. These ""object/relational"" databases, which 
 continue to be based on SQL, have emerged as a more popular alternative to ""pure 
 object"" databases and may insure SQL's continuing dominance for the next decade.",NA
Internet Database Access,"With the exploding popularity of the Internet and the World Wide Web, and their 
  
 standards-based foundation, SQL found a new role in the late 1990s as an Internet data 
 access standard. Early in the development of the Web, developers needed a way to 
 retrieve and present database information on web pages and used SQL as a common 
 language for database gateways. More recently, the emergence of three-tiered Internet 
 architectures with distinct thin client, application server and database server layers, have 
 established SQL as the standard link between the application and database tiers.",NA
Java Integration (JDBC),"One of the major new areas of SQL development is the integration of SQL with Java. 
 Seeing the need to link the Java language to existing relational databases, Sun 
  
 - 14 -",NA
Chapter 2:,NA,NA
 A Quick Tour of SQL,NA,NA
Overview,"Before diving into the details of SQL, it's a good idea to develop an overall perspective on 
 the language and how it works. This chapter contains a quick tour of SQL that illustrates its 
 major features and functions. The goal of the quick tour is not to make you proficient in 
 writing SQL statements; that is the goal of 
 Part II
  of this book. Rather, by the time you've 
 finished this chapter, you will have a basic familiarity with the SQL language and an 
 overview of its capabilities.",NA
A Simple Database,"The examples in the quick tour are based on a simple relational database for a small 
 distribution company. The database, shown in Figure 2-1, stores the information needed 
 to implement a small order processing application. Specifically, it stores the following 
 information:
  
  
 Figure 2-1: 
 A simple relational database
  
 •the 
 customers
  who buy the company's products,
  
 - 15 -",NA
Retrieving Data,"First, let's list the sales offices, showing the city where each one is located and its year-
 to-date sales. The SQL statement that retrieves data from the database is called 
 SELECT
 . This SQL statement retrieves the data you want:
  
 SELECT CITY, OFFICE, SALES
  
  
  FROM OFFICES
  
 CITY          OFFICE        
 SALES
  
 ------------  ------  ----------
 -
  
 Denver            22  
 $186,042.00 
  
 New York          11  
 $692,637.00 
  
 Chicago           12  
 $735,042.00 
  
 Atlanta           13  
 $367,911.00 
  
 Los 
 Angeles 
  
  
  
  
  
  
 21  
 $835,915.00
  
 The 
 SELECT
  statement asks for three pieces of data—the city, the office number, and the 
 sales—for each office. It also specifies that the data comes from the 
 OFFICES
  table, 
 which stores data about sales offices. The results of the query appear, in tabular form, 
 immediately after the request.
  
 The 
 SELECT
  statement is used for all SQL queries. For example, here is a query that lists 
 the names and year-to-date sales for each salesperson in the database. It also shows the 
 quota (sales target) and the office number where each person works. In this case, the data 
 comes from 
 SALESREPS
  table:
  
 SELECT NAME, REP_OFFICE, SALES, QUOTA
  
  
  FROM SALESREPS
  
 NAME            REP_OFFICE        SALES        
 QUOTA--------------  ----------  -----------  -----
 ------Bill Adams              13  $367,911.00  
 $350,000.00 Mary Jones              11  $392,725.00  
 $300,000.00 Sue Smith               21  $474,050.00  
 $350,000.00 Sam Clark               11  $299,912.00  
 $275,000.00 Bob Smith               12  $142,594.00  
 $200,000.00 Dan Roberts             12  $305,673.00  
 $300,000.00 Tom Snyder            NULL   $75,985.00         
 NULL Larry Fitch             21  $361,865.00",NA
Summarizing Data,- 17 -,NA
Adding Data to the Database,"SQL is also used to add new data to the database. For example, suppose you just opened 
 a new Western region sales office in Dallas, with target sales of $275,000. Here's the 
 INSERT
  statement that adds the new office to the database, as office number 23:
  
 - 18 -",NA
Deleting Data,"Just as the SQL 
 INSERT
  statement adds new data to the database, the SQL 
 DELETE 
 statement removes data from the database. If Acme Industries decides a few days later 
 to switch to a competitor, you can delete them from the database with this statement:
  
 DELETE FROM CUSTOMERS 
  
 WHERE COMPANY = 'Acme Industries'
  
 1 row deleted.
  
 And if you decide to terminate all salespeople whose sales are less than their quotas, you 
 can remove them from the database with this 
 DELETE
  statement:
  
 DELETE FROM SALESREPS 
  
 WHERE SALES < QT<R
  
 2 rows deleted.",NA
Updating the Database,"The SQL language is also used to modify data that is already stored in the database. For 
 example, to increase the credit limit for First Corp. to $75,000, you would use the SQL 
 UPDATE
  statement:
  
 UPDATE CUSTOMERS
  
  
  SET CREDIT_LIMIT = 75000.00 
  
 WHERE COMPANY = 'First Corp.'
  
 1 row updated.
  
 The 
 UPDATE
  statement can also make many changes in the database at once. For 
 example, this 
 UPDATE
  statement raises the quota for all salespeople by $15,000:
  
 UPDATE SALESREPS
  
  
  SET QUOTA = QUOTA + 15000.00
  
 8 rows updated.",NA
Protecting Data,- 19 -,NA
Creating a Database,"Before you can store data in a database, you must first define the structure of the data. 
 Suppose you want to expand the sample database by adding a table of data about the 
 products sold by your company. For each product, the data to be stored includes:
  
 •a three-character manufacturer ID code,
  
 •a five-character product ID code,
  
 •a description of up to thirty characters,
  
 •the price of the product, and
  
 •the quantity currently on hand.
  
 - 20 -",NA
Summary,"This quick tour of SQL showed you what SQL can do and illustrated the style of the SQL 
 language, using eight of the most commonly used SQL statements. To summarize:
  
 •
  
 SQL is used to 
 retrieve
  data from the database, using the 
 SELECT
  statement. You can 
 retrieve all or part of the stored data, sort it, and ask SQL to summarize the data, using 
  
 totals and averages.
  
 •SQL is used to 
 update
  the database, by adding new data with the 
 INSERT 
 statement, 
 deleting data with the 
 DELETE
  statement, and modifying existing data with the 
 UPDATE 
 statement.
  
 •SQL is used to 
 control access
  to the database, by granting and revoking specific 
 privileges for specific users with the 
 GRANT
  and 
 REVOKE
  statements.
  
 •SQL is used to 
 create
  the database by defining the structure of new tables and dropping 
 tables when they are no longer needed, using the 
 CREATE
  and 
 DROP
  statements.",NA
Chapter 3: ,NA,NA
SQL In Perspective,- 21 -,NA
Overview,"SQL is both a 
 de facto
  and an official standard language for database management. What 
 does it mean for SQL to be a standard? What role does SQL play as a database 
  
 language? How did SQL become a standard, and what impact is the SQL standard having 
 on personal computers, local area networks, minicomputers, and mainframes? To answer 
 these questions, this chapter traces the history of SQL and describes its current role in the 
 computer market.",NA
SQL and Database Management,"One of the major tasks of a computer system is to store and manage data. To handle this 
 task, specialized computer programs known as 
 database management systems
  began to 
 appear in the late 1960s and early 1970s. A database management system, or DBMS, 
 helped computer users to organize and structure their data and allowed the computer 
 system to play a more active role in managing the data. Although database management 
 systems were first developed on large mainframe systems, their popularity has spread to 
 minicomputers, personal computers, workstations, and specialized server computers.
  
 Database management also plays a key role in the explosion of computer networking and 
 the Internet. Early database systems ran on laarge, monolithic computer systems, where 
 the data, the database management software, and the user or application program 
 accessing the database all operated on the same system. The 1980s and 1990s saw the 
 explosion of a new, client/server model for database access, in which a user on a personal 
 computer or an application program accessed a database on a separate computer system 
 using a network. In the late 1990s, the increasing popularity of the Internet and the World 
 Wide Web intertwined the worlds of networking and data 
  
 management even further. Now users require little more than a web browser to access 
 and interact with databases, not only within their own organizations, but around the world.
  
 Today, database management is very big business. Independent software companies and 
 computer vendors ship billions of dollars worth of database management products every 
 year. Computer industry experts say that mainframe and minicomputer database products 
 each account for about 10 to 20 percent of the database market, and personal computer 
 and server-based database products account for 50 percent or more. Database servers 
 are one of the fastest-growing segments of the computer systems market, driven by 
 database installations on Unix and Windows NT-based servers. Database 
  
 management thus touches every segment of the computer market.
  
 Since the late 1980s a specific type of DBMS, called a 
 relational
  database management 
 system (RDBMS), has become so popular that it is 
 the
  standard database form. Relational 
 databases organize data in a simple, tabular form and provide many advantages over 
 earlier types of databases. SQL is specifically a relational database language used to work 
 with relational databases.",NA
A Brief History of SQL,"The history of the SQL language is intimately intertwined with the development of 
 relational databases. Table 3-1 shows some of the milestones in its 30-year history. The 
 relational database concept was originally developed by Dr. E.F. ""Ted"" Codd, an IBM 
 researcher. In June 1970 Dr. Codd published an article entitled ""A Relational Model of 
 Data for Large Shared Data Banks"" that outlined a mathematical theory of how data 
 could be stored and manipulated using a tabular structure. Relational databases and 
 SQL trace their origins to this article, which appeared in the 
 Communications of the 
 Association for Computing Machinery
 .
  
 Table 3-1: Milestones in the Development of SQL
  
 - 22 -",NA
The Early Years,"Codd's article triggered a flurry of relational database research, including a major 
 research project within IBM. The goal of the project, called System/R, was to prove the 
 workability of the relational concept and to provide some experience in actually 
 implementing a relational DBMS. Work on System/R began in the mid-1970s at IBM's 
 Santa Teresa laboratories in San Jose, California.
  
 In 1974 and 1975 the first phase of the System/R project produced a minimal prototype of 
 a relational DBMS. In addition to the DBMS itself, the System/R project included work on 
 database query languages. One of these languages was called SEQUEL, an acronym for 
 Structured English Query Language. In 1976 and 1977 the System/R research prototype 
 was rewritten from scratch. The new implementation supported multi-table queries and 
 allowed several users to share access to the data.
  
 The System/R implementation was distributed to a number of IBM customer sites for 
 evaluation in 1978 and 1979. These early customer sites provided some actual user 
 experience with System/R and its database language, which, for legal reasons, had been 
 renamed SQL, or Structured Query Language. Despite the name change, the SEQUEL 
 pronunciation remained and continues to this day. In 1979 the System/R research project 
 came to an end, with IBM concluding that relational databases were not only feasible, but 
 could be the basis for a useful commercial product.",NA
Early Relational Products,"The System/R project and its SQL database language were well-chronicled in technical 
 journals during the 1970s. Seminars on database technology featured debates on the 
 merits of the new and ""heretical"" relational model. By 1976 it was apparent that IBM was 
 becoming enthusiastic about relational database technology and that it was making a 
 major commitment to the SQL language.
  
 The publicity about System/R attracted the attention of a group of engineers in Menlo 
 Park, California, who decided that IBM's research foreshadowed a commercial market for 
 relational databases. In 1977 they formed a company, Relational Software, Inc., to build a 
 relational DBMS based on SQL. The product, named Oracle, shipped in 1979 and became 
 the first commercially available relational DBMS. Oracle beat IBM's first product to market 
 by a full two years and ran on Digital's VAX minicomputers, which were less expensive 
 than IBM mainframes. Today the company, renamed Oracle Corporation, is a leading 
 vendor of relational database management systems, with annual sales of many billions of 
 dollars.
  
 Professors at the University of California's Berkeley computer laboratories were also 
 researching relational databases in the mid-1970s. Like the IBM research team, they built 
 a prototype of a relational DBMS and called their system Ingres. The Ingres project 
 included a query language named QUEL that, although more ""structured"" than SQL, was 
 less English-like. Many of today's database experts trace their involvement with relational 
  
 - 24 -",NA
IBM Products,"While Oracle and Ingres raced to become commercial products, IBM's System/R project 
 had also turned into an effort to build a commercial product, named SQL/Data System 
 (SQL/DS). IBM announced SQL/DS in 1981 and began shipping the product in 1982. In 
 1983 IBM announced a version of SQL/DS for VM/CMS, an operating system that is 
 frequently used on IBM mainframes in corporate ""information center"" applications.
  
 In 1983 IBM also introduced Database 2 (DB2), another relational DBMS for its 
  
 mainframe systems. DB2 operated under IBM's MVS operating system, the workhorse 
 operating system used in large mainframe data centers. The first release of DB2 began 
 shipping in 1985, and IBM officials hailed it as a strategic piece of IBM software 
  
 technology. DB2 has since become IBM's flagship relational DBMS, and with IBM's weight 
 behind it, DB2's SQL language became the 
 de facto
  standard database language. DB2 
 technology has now migrated across all IBM product lines, from personal computers to 
 network servers to mainframes. In 1997, IBM took the DB2 cross-platform strategy even 
 farther, by announcing DB2 versions for computer systems made by Sun 
  
 Microsystems, Hewlett-Packard, and other IBM hardware competitors.",NA
Commercial Acceptance,"During the first half of the 1980s, the relational database vendors struggled for 
  
 commercial acceptance of their products. The relational products had several 
  
 disadvantages when compared to the traditional database architectures. The 
  
 performance of relational databases was seriously inferior to that of traditional databases. 
 Except for the IBM products, the relational databases came from small ""upstart"" vendors. 
  
 And, except for the IBM products, the relational databases tended to run on 
 minicomputers rather than on IBM mainframes.
  
 The relational products did have one major advantage, however. Their relational query 
 languages (SQL, QUEL, and others) allowed users to pose 
 ad hoc
  queries to the 
 database— and get immediate answers—without writing programs. As a result, relational 
 databases began slowly turning up in information center applications as decision-support 
 tools. By May 1985 Oracle proudly claimed to have ""over 1,000"" installations. Ingres was 
 installed in a comparable number of sites. DB2 and SQL/DS were also being slowly 
 accepted and counted their combined installations at slightly over 1,000 sites.
  
 During the last half of the 1980s, SQL and relational databases were rapidly accepted as 
 the database technology of the future. The performance of the relational database 
 products improved dramatically. Ingres and Oracle, in particular, leapfrogged with each 
 new version claiming superiority over the competitor and two or three times the 
  
 performance of the previous release. Improvements in the processing power of the 
 underlying computer hardware also helped to boost performance.
  
 Market forces also boosted the popularity of SQL in the late 1980s. IBM stepped up its 
 evangelism of SQL, positioning DB2 as the data management solution for the 1990s. 
 Publication of the ANSI/ISO standard for SQL in 1986 gave SQL ""official"" status as a 
  
 - 25 -",NA
SQL Standards,"One of the most important developments in the market acceptance of SQL is the 
 emergence of SQL standards. References to ""the SQL standard"" usually mean the 
 official standard adopted by the American National Standards Institute (ANSI) and the 
 International Standards Organization (ISO). However, there are other important SQL 
 standards, including the 
 de facto
  standard SQL defined by IBM's DB2 product family.",NA
The ANSI/ISO Standards,"Work on the official SQL standard began in 1982, when ANSI charged its X3H2 
 committee with defining a standard relational database language. At first the committee 
 debated the merits of various proposed database languages. However, as IBM's 
 commitment to SQL increased and SQL emerged as a 
 de facto
  standard in the market, 
 the committee selected SQL as their relational database language and turned their 
 attention to standardizing it.
  
 The resulting ANSI standard for SQL is largely based on DB2 SQL, although it contains 
 some major differences from DB2. After several revisions, the standard was officially 
 adopted as ANSI standard X3.135 in 1986, and as an ISO standard in 1987. The 
 ANSI/ISO standard has since been adopted as a Federal Information Processing 
 Standard (FIPS) by the U.S. government. This standard, slightly revised and expanded in 
 1989, is usually called the ""SQL-89"" or ""SQL1"" standard.
  
 - 26 -",NA
Other SQL Standards,"Although it is the most widely recognized, the ANSI/ISO standard is not the only standard 
 for SQL. X/OPEN, a European vendor group, has also adopted SQL as part of its suite of 
 standards for a ""portable application environment"" based on Unix. The X/OPEN 
  
 - 27 -",NA
ODBC and the SQL Access Group,"An important area of database technology not addressed by official standards is 
  
 database interoperability
 —the methods by which data can be exchanged among different 
 databases, usually over a network. In 1989, a group of vendors formed the SQL Access 
 Group to address this problem. The resulting SQL Access Group specification for Remote 
 Database Access (RDA) was published in 1991. Unfortunately, the RDA specification is 
 closely tied to the OSI protocols, which have not been widely accepted, so it has had little 
 impact. Transparent interoperability among different vendors' databases remains an 
 elusive goal.
  
 A second standard from the SQL Access Group has had far more market impact. At 
 Microsoft's urging and insistence, SQL Access Group expanded its focus to include a 
 call-level interface for SQL. Based on a draft from Microsoft, the resulting Call-Level 
 Interface (CLI) specification was published in 1992. Microsoft's own Open Database 
 Connectivity (ODBC) specification, based on the CLI standard, was published the same 
 year. With the market power of Microsoft behind it, and the ""open standards"" blessing of 
 SQL Access Group, ODBC has emerged as the 
 de facto
  standard interface for PC 
 access to SQL databases. Apple and Microsoft announced an agreement to support 
 ODBC on Macintosh and Windows in the spring of 1993, giving ODBC ""standard"" status 
 in both popular graphical user interface environments. ODBC implementations for Unix-
 based systems soon followed.
  
 Today, ODBC is in its fourth major revision as a cross-platform database access standard. 
 ODBC support is available for all major DBMS brands. Most packaged application 
 programs that have database access as an important part of their capabilities support 
 ODBC, range from multi-million dollar enterprise class applications like 
  
 Enterprise Resource Planning (ERP) and Supply Chain Management (SCM) to PC 
 applications such as spreadsheets, query tools, and reporting programs. Microsoft's focus 
 has moved beyond ODBC to higher-level interfaces (such as OLE/DB) and more recently 
 to ADO (Active Data Objects), but these new interfaces are layered on top of ODBC for 
 relational database access, and it remains a key cross-platform database access 
 technology.",NA
The Portability Myth,"The existence of published SQL standards has spawned quite a few exaggerated claims 
 about SQL and applications portability. Diagrams such as the one in Figure 3-1 are 
 frequently drawn to show how an application using SQL can work interchangeably with 
 any SQL-based database management system. In fact, the holes in the SQL-89 standard 
 and the current differences between SQL dialects are significant enough that an 
  
 application must 
 always
  be modified when moved from one SQL database to another. 
 These differences, many of which were eliminated by the SQL2 standard but have not yet 
 implemented in commercial products, include:
  
 - 28 -",NA
SQL and Networking,"The dramatic growth of computer networking over the past decade has had a major 
 impact on database management and given SQL a new prominence. As networks became 
 more common, applications that traditionally ran on a central minicomputer or mainframe 
 moved to local area networks of desktop workstations and servers. In these networks SQL 
 plays a crucial role as the link between an application running on a desktop workstation 
 with a graphical user interface and the DBMS that manages shared data on a cost-
 effective server. More recently, the exploding popularity of the Internet and the World Wide 
 Web has reinforced the network role for SQL. In the emerging ""three-tier"" Internet 
 architecture, SQL once again provides the link between the application logic (now running 
 in the ""middle tier,"" on an application server or web server) and the database residing in 
 the ""back-end"" tier. The next few sections in this chapter discuss the evolution of database 
 network architectures and the role of SQL in each one.",NA
Centralized Architecture,"The traditional database architecture used by DB2, SQL/DS, and the original 
  
 minicomputer databases such as Oracle and Ingres is shown in Figure 3-2. In this 
 architecture the DBMS and the physical data both reside on a central minicomputer or 
 mainframe system, along with the application program that accepts input from the user's 
 terminal and displays data on the user's screen. The application program communicates 
 with the DBMS using SQL.
  
  
 Figure 3-2: 
 Database management in a centralized architecture
  
 Suppose that the user types a query that requires a sequential search of a database, 
 such as a request to find the average amount of merchandise of all orders. The DBMS 
 receives the query, scans through the database fetching each record of data from the 
 disk, calculates the average, and displays the result on the terminal screen. Both the 
 application processing and the database processing occur on the central computer, so 
 execution of this type of query (and in fact, all kinds of queries) is very efficient.
  
 The disadvantage of the centralized architecture is scalability. As more and more users 
 are added, each of them adds application processing workload to the system. Because 
 the system is shared, each user experiences degraded performance as the system 
 becomes more heavily loaded.
  
 - 30 -",NA
File Server Architecture,"The introduction of personal computers and local area networks led to the development of 
 the 
 file server
  architecture, shown in Figure 3-3. In this architecture, an application running 
 on a personal computer can transparently access data located on a file server, which 
 stores shared files. When a PC application requests data from a shared file, the 
 networking software automatically retrieves the requested block of the file from the server. 
 Early PC databases, such as dBASE and later Microsoft's Access, supported this file 
 server approach, with each personal computer running its own copy of the DBMS 
 software.
  
  
 Figure 3-3: 
 Database management in a file server architecture
  
 For typical queries that retrieve only one row or a few rows from the database, this 
 architecture provides excellent performance, because each user has the full power of a 
 personal computer running its own copy of the DBMS. However, consider the query made 
 in the previous example. Because the query requires a sequential scan of the database, 
 the DBMS repeatedly requests blocks of data from the database, which is physically 
 located across the network on the server. Eventually 
 every
  block of the file will be 
 requested and sent across the network. Obviously this architecture produces very heavy 
 network traffic and slow performance for queries of this type.",NA
Client/Server Architecture,"Figure 3-4 shows the next stage of network database evolution—the 
 client/server 
 database architecture. In this scheme, personal computers are combined in a local area 
 network with a 
 database server
  that stores shared databases. The functions of the DBMS 
 are split into two parts. Database ""front-ends,"" such as interactive query tools, report 
 writers, and application programs, run on the personal computer. The back-end database 
 engine that stores and manages the data runs on the server. As the client/server 
 architecture grew in popularity during the 1990s, SQL became the standard database 
 language for communication between the front-end tools and the back-end engine in this 
 architecture.
  
  
 Figure 3-4: 
 Database management in a client/server architecture
  
 Consider once more the query requesting the average order size. In the client/server 
 architecture, the query travels across the network to the database server as a SQL 
  
 - 31 -",NA
Multi-Tier Architecture,"With the emergence of the Internet and especially the World Wide Web, network 
  
 database architecture has taken another step. At first, the Web was used to access 
 (""browse"") static documents and evolved outside of the database world. But as the use of 
 web browsers became widespread, it wasn't long before companies thought about using 
 them as a simple way to provide access to corporate databases as well. For example, 
 suppose a company starts using the Web to provide product information to its customers, 
 by making product descriptions and graphics available on its web site. A natural next step 
 is to give customers access to current product availability information through the same 
 web browser interface. This requires linking the web server to the database system that 
 stores the (constantly changing) current product inventory levels.
  
 The methods used to link web servers and DBMS systems have evolved rapidly over the 
 last several years and have converged on the three-tier network architecture shown in 
 Figure 3-5. The user interface is a web browser running on a PC or some other ""thin 
 client"" device in the ""front"" tier. It communicates with a web server in the ""middle tier."" 
 When the user request is for something more complex than a simple web page, the web 
 server passes the request to an 
 application server
  whose role is to handle the business 
 logic required to process the request. Often the request will involve access to an existing 
 (""legacy"") application running on a mainframe system or to a corporate database. These 
 systems run in the ""back"" tier of the architecture. As with the client/server architecture, 
 SQL is solidly entrenched as the standard database language for communicating between 
 the application server and back-end databases. All of the packaged application server 
 products provide a SQL-based callable API for database access.
  
 - 32 -",NA
The Proliferation of SQL,"As the standard for relational database access, SQL has had a major impact on all parts 
 of the computer market. IBM has adopted SQL as a unifying database technology for its 
 product line. SQL-based databases dominate the market for Unix-based computer 
 systems. In the PC market, SQL databases on Windows NT are mounting a serious 
 challenge to the dominance of Unix as a database processing platform, especially for 
 departmental applications. SQL is accepted as a technology for online transaction 
 processing, fully refuting the conventional wisdom of the 1980s that relational databases 
 would never offer performance good enough for transaction processing applications. 
  
 SQL-based data warehousing and data mining applications are helping companies to 
 discover customer purchase patterns and offer better products and services. On the 
 Internet, SQL-based databases are the foundation of more personalized products, 
 services, and information services that are a key benefit of electronic commerce.",NA
SQL and IBM's Unified Database Strategy,"SQL plays a key role as the database access language that unifies IBM's multiple 
 incompatible computer families. Originally, this role was part of IBM's Systems 
  
 Application Architecture (SAA) strategy, announced in March 1987. Although IBM's grand 
 goals for SAA were not achieved, the unifying role of SQL has grown even more 
  
 important over time. The DB2 database system, IBM's flagship SQL-based DBMS, now 
 runs on a broad range of IBM and non-IBM computer systems, including:
  
 •
 Mainframes.
  DB2 started as the SQL standard-bearer for IBM mainframes running 
 MVS and has now replaced SQL/DS as the relational system for the VM and VSE 
 mainframe operating systems.
  
 •
 AS/400.
  This SQL implementation runs on IBM's family of midrange business systems, 
 targeted at small- and medium-sized businesses and server applications.
  
 •
 RS/6000.
  DB2 runs under the Unix operating system on IBM's family of RISC-based 
 workstations and servers, for engineering and scientific applications and as IBM's own 
 Unix database server platform.
  
 •
 Other Unix platforms.
  IBM supports DB2 on Unix-based server platforms from Sun 
 Microsystems and Hewlett-Packard, the two largest Unix system vendors, and on 
 Unix-based workstations from Silicon Graphics.
  
 •
 OS/2.
  A smaller-scale version of DB2 runs on this IBM-proprietary operating system 
 for Intel-based personal computers
  
 - 33 -",NA
SQL on Minicomputers,"Minicomputers were one of the most fertile early markets for SQL-based database 
 systems. Oracle and Ingres were both originally marketed on Digital's VAX/VMS 
 minicomputer systems. Both products have since been ported to many other platforms. 
  
 Sybase, a later database system specialized for online transaction processing, also 
 targeted the VAX as one of its primary platforms.
  
 Through the 1980s, the minicomputer vendors also developed their own proprietary 
 relational databases featuring SQL. Digital considered relational databases so important 
 that it bundled a run-time version of its Rdb/VMS database with every VAX/VMS system. 
 Hewlett-Packard offered Allbase, a database that supported both its HPSQL dialect and a 
 nonrelational interface. Data General's DG/SQL database replaced its older nonrelational 
 databases as DG's strategic data management tool. In addition, many of the 
  
 minicomputer vendors resold relational databases from the independent database 
 software vendors. These efforts helped to establish SQL as an important technology for 
 midrange computer systems.
  
 Today, the minicomputer vendors' SQL products have largely disappeared, beaten in the 
 marketplace by multi-platform software from Oracle, Informix, Sybase, and others. 
 Accompanying this trend, the importance of proprietary minicomputer operating systems 
 has faded as well, replaced by widespread use of Unix on midrange systems. 
  
 Yesterday's minicomputer SQL market has effectively become today's market for Unix-
 based database servers based on SQL.",NA
SQL on Unix-Based Systems,"SQL has firmly established itself as the data management solution of choice for Unix-
 based computer systems. Originally developed at Bell Laboratories, Unix became very 
 popular in the 1980s as a vendor-independent, standard operating system. It runs on a 
 wide range of computer systems, from workstations to mainframes, and has become the 
 standard operating system for scientific and engineering applications.
  
 In the early 1980s four major databases were already available for Unix systems. Two of 
 them, Ingres and Oracle, were Unix versions of the products that ran on DEC's 
  
 proprietary minicomputers. The other two, Informix and Unify, were written specifically for 
 Unix. Neither of them originally offered SQL support, but by 1985 Unify offered a SQL 
 query language, and Informix had been rewritten as Informix-SQL, with full SQL support.
  
 Today, Oracle, Informix, and Sybase dominate the Unix-based database market and are 
 available on all of the leading Unix systems. Unix-based database servers are a 
 mainstream building block for both client/server and three-tier Internet architectures. The 
 constant search for higher SQL database performance has driven some of the most 
 important trends in Unix system hardware. These include the emergence of symmetric 
 multiprocessing (SMP) as a mainstream server architecture, and the use of RAID 
 (Redundant Array of Independent Disk) technology to boost I/O performance.",NA
SQL on Personal Computers,"Databases have been popular on personal computers since the early days of the IBM PC. 
 Ashton-Tate's dBASE product reached an installed base of over one million MS-DOS-
 based PCs. Although these early PC databases often presented data in tabular form, they 
 lacked the full power of a relational DBMS and a relational database language such as 
 SQL. The first SQL-based PC databases were versions of popular minicomputer products 
 that barely fit on personal computers. For example, Professional Oracle for the IBM PC, 
 introduced in 1984, required two megabytes of memory—well above the typical 
  
 - 34 -",NA
SQL and Transaction Processing,"SQL and relational databases originally had very little impact in online transaction 
 processing (OLTP) applications. With their emphasis on queries, relational databases 
 were confined to decision support and low volume online applications, where their slower 
 performance was not a disadvantage. For OLTP applications, where hundreds of users 
 needed online access to data and subsecond response times, IBM's nonrelational 
 Information Management System (IMS) reigned as the dominant DBMS.
  
 In 1986 a new DBMS vendor, Sybase, introduced a new SQL-based database especially 
 designed for OLTP applications. The Sybase DBMS ran on VAX/VMS minicomputers and 
 Sun workstations and focused on maximum online performance. Oracle Corporation and 
 Relational Technology followed shortly with announcements that they, too, would offer 
 OLTP versions of their popular Oracle and Ingres database systems. In the Unix market, 
 Informix announced an OLTP version of its DBMS, named Informix-Turbo.
  
 In 1988 IBM jumped on the relational OLTP bandwagon with DB2 Version 2, with 
 benchmarks showing the new version operating at over 250 transactions per second on 
  
 - 35 -",NA
SQL and Workgroup Databases,"The dramatic growth of PC LANs through the 1980s and 1990s created a new 
  
 opportunity for departmental or ""workgroup"" database management. The original database 
 systems focused on this market segment ran on IBM's OS/2 operating system. In fact, 
 SQL Server, now a key part of Microsoft's Windows strategy, originally made its debut as 
 an OS/2 database product. In the mid-1990s, Novell also made a concentrated effort to 
 make its NetWare operating system an attractive workgroup database server platform. 
 From the earliest days of PC LANs, NetWare had become established as the dominant 
 network operating system for file and print servers. Through deals with Oracle and others, 
 Novell sought to extend this leadership to workgroup database servers as well.
  
 The arrival of Windows NT on the workgroup computing scene was the catalyst that 
 caused the workgroup database market to really take off. While NetWare offered a clear 
 performance advantage over NT as a workgroup file server, NT had a more robust, 
 general-purpose architecture, more like the minicomputer operating systems. Microsoft 
 successfully positioned NT as a more attractive platform for running workgroup 
  
 applications (as an ""application server"") and workgroup databases. Microsoft's own SQL 
 Server product was marketed (and often bundled) with NT as a tightly integrated 
 workgroup database platform. Corporate information systems departments were at first 
 very cautious about using relatively new and unproven technology, but the NT/SQL 
 Server combination allowed departments and non-IS executives to undertake smaller-
 scale, workgroup-level projects on their own, without corporate IS help. This 
  
 phenomenon, like the grass roots support for personal computers a decade earlier, 
 fueled the early growth of the workgroup database segment.
  
 Today, SQL is well established as a workgroup database standard. Microsoft's SQL 
 Server has been joined by Oracle, Informix, Sybase, DB2, and many other DBMS brands 
 running on the Windows NT/Windows 2000 platform. Windows-based SQL databases are 
 the second largest segment of the DBMS market and are the fastest growing. From this 
 solid dominance in the workgroup segment, Windows-based server systems are mounting 
 a continued assault on enterprise-class database applications, slowly but surely eating 
 into low-end Unix-based database deployments.",NA
SQL and Data Warehousing,"For several years, the effort to make SQL a viable technology for OLTP applications 
 shifted the focus away from the original relational database strengths of query processing 
 and decision making. Performance benchmarks and competition among the major DBMS 
  
 - 36 -",NA
Summary,"This chapter described the development of SQL and its role as a standard language for 
 relational database management:
  
 •SQL was originally developed by IBM researchers, and IBM's strong support of SQL is a 
 key reason for its success.
  
 •There are official ANSI/ISO SQL standards and several other SQL standards, each 
 slightly different from the ANSI/ISO standards.
  
 •Despite the existence of standards, there are many small variations among 
 commercial SQL dialects; no two SQLs are exactly the same.
  
 •SQL has become the standard database management language across a broad range of 
 computer systems and applications areas, including mainframes, workstations, personal 
 computers, OLTP systems, client/server systems, data warehousing, and the Internet.",NA
Chapter 4: ,NA,NA
Relational Databases,NA,NA
Overview,"Database management systems organize and structure data so that it can be retrieved 
 and manipulated by users and application programs. The data structures and access 
 techniques provided by a particular DBMS are called its 
 data model
 . A data model 
 determines both the ""personality"" of a DBMS and the applications for which it is 
 particularly well suited.
  
 SQL is a database language for relational databases that uses the 
 relational data model
 . 
 What exactly is a relational database? How is data stored in a relational database? How do 
 relational databases compare to earlier technologies, such as hierarchical and network 
 databases? What are the advantages and disadvantages of the relational model? This 
 chapter describes the relational data model supported by SQL and compares it to earlier 
 strategies for database organization.",NA
Early Data Models,"As database management became popular during the 1970s and 1980s, a handful of 
 popular data models emerged. Each of these early data models had advantages and 
 disadvantages that played key roles in the development of the relational data model. In 
 many ways the relational data model represented an attempt to streamline and simplify 
 the earlier data models. In order to understand the role and contribution of SQL and the 
 relational model, it is useful to briefly examine some data models that preceded the 
 development of SQL.
  
 - 38 -",NA
File Management Systems,"Before the introduction of database management systems, all data permanently stored 
 on a computer system, such as payroll and accounting records, was stored in individual 
 files. A 
 file management system
 , usually provided by the computer manufacturer as part 
 of the computer's operating system, kept track of the names and locations of the files. 
  
 The file management system basically had no data model; it knew nothing about the 
 internal contents of files. To the file management system, a file containing a word 
 processing document and a file containing payroll data appeared the same.
  
 Knowledge about the contents of a file—what data it contained and how the data was 
 organized—was embedded in the application programs that used the file, as shown in 
 Figure 4-1. In this payroll application, each of the COBOL programs that processed the 
 employee master file contained a 
 file description
  (FD) that described the layout of the 
 data in the file. If the structure of the data changed—for example, if an additional item of 
 data was to be stored for each employee—every program that accessed the file had to 
 be modified. As the number of files and programs grew over time, more and more of a 
 data processing department's effort went into maintaining existing applications rather 
 than developing new ones.
  
  
 Figure 4-1: 
 A payroll application using a file management system
  
 The problems of maintaining large file-based systems led in the late 1960s to the 
 development of database management systems. The idea behind these systems was 
 simple: take the definition of a file's content and structure out of the individual programs, 
 and store it, together with the data, in a database. Using the information in the database, 
 the DBMS that controlled it could take a much more active role in managing both the data 
 and changes to the database structure.",NA
Hierarchical Databases,"One of the most important applications for the earliest database management systems 
 was production planning for manufacturing companies. If an automobile manufacturer 
 decided to produce 10,000 units of one car model and 5,000 units of another model, it 
 needed to know how many parts to order from its suppliers. To answer the question, the 
 product (a car) had to be decomposed into assemblies (engine, body, chassis), which 
 were decomposed into subassemblies (valves, cylinders, spark plugs), and then into sub-
 subassemblies, and so on. Handling this list of parts, known as a 
 bill of materials
 , was a 
 job tailor-made for computers.
  
 The bill of materials for a product has a natural hierarchical structure. To store this data, 
  
 - 39 -",NA
Network Databases,"The simple structure of a hierarchical database became a disadvantage when the data 
 had a more complex structure. In an order-processing database, for example, a single 
  
 - 40 -",NA
The Relational Data Model,"The relational model proposed by Dr. Codd was an attempt to simplify database 
 structure. It eliminated the explicit parent/child structures from the database, and instead 
 represented all data in the database as simple row/column tables of data values. Figure 
 4-5 shows a relational version of the network order-processing database in 
 Figure 4-4
 .
  
  
 Figure 4-5: 
 A relational order-processing database
  
 Unfortunately, the practical definition of ""What is a relational database?"" became much 
 less clear-cut than the precise, mathematical definition in Codd's 1970 paper. Early 
 relational database management systems failed to implement some key parts of Codd's 
 model, which are only now finding their way into commercial products. As the relational 
 concept grew in popularity, many databases that were called ""relational"" in fact were not.
  
 - 42 -",NA
The Sample Database,"Figure 4-6 shows a small relational database for an order-processing application. This 
 sample database is used throughout this book and provides the basis for most of the 
 examples. 
 Appendix A
  contains a complete description of the database structure and its 
 contents.
  
  
 Figure 4-6: 
 The sample database
  
 The sample database contains five tables. Each table stores information about one 
 particular 
 kind
  of entity:
  
 •The 
 CUSTOMERS
  table stores data about each customer, such as the company name, 
 credit limit, and the salesperson who calls on the customer.
  
 •The 
 SALESREPS
  table stores the employee number, name, age, year-to-date sales, 
 and other data about each salesperson.
  
 •The 
 OFFICES
  table stores data about each of the five sales offices, including the city 
 where the office is located, the sales region to which it belongs, and so on.
  
 •
  
 The 
 ORDERS
  table keeps track of every order placed by a customer, identifying the 
 salesperson who took the order, the product ordered, the quantity and amount of the 
  
 order, and so on. For simplicity, each order is for only one product.
  
 •The 
 PRODUCTS
  table stores data about each product available for sale, such as the 
 manufacturer, product number, description, and price.
  
 - 43 -",NA
Tables,"The organizing principle in a relational database is the 
 table,
  a rectangular, row/column 
 arrangement of data values. Each table in a database has a unique 
 table name
  that 
 identifies its contents. (Actually, each user can choose their own table names without 
 worrying about the names chosen by other users, as explained in 
 Chapter 5
 .)
  
 The row/column structure of a table is shown more clearly in Figure 4-7, which is an 
 enlarged view of the 
 OFFICES
  table. Each horizontal 
 row 
 of the 
 OFFICES
  table 
  
 represents a single physical entity—a single sales office. Together the five rows of the 
 table represent all five of the company's sales offices. All of the data in a particular row of 
 the table applies to the office represented by that row.
  
  
 Figure 4-7:. 
 The row/column structure of a relational table
  
 Each vertical 
 column
  of the 
 OFFICES
  table represents one item of data that is stored in 
 the database for each office. For example, the 
 CITY
  column holds the location of each 
 office. The 
 SALES
  column contains each office's year-to-date sales total. The 
 MGR
  column 
 shows the employee number of the person who manages the office.
  
 Each row of a table contains exactly one data value in each column. In the row 
  
 representing the New York office, for example, the 
 CITY
  column contains the value ""New 
 York."" The 
 SALES
  column contains the value ""$692,637.00,"" which is the year-to-date 
 sales total for the New York office.
  
 For each column of a table, all of the data values in that column hold the same type of 
 data. For example, all of the 
 CITY
  column values are words, all of the 
 SALES
  values are 
 money amounts, and all of the 
 MGR
  values are integers (representing employee 
  
 numbers). The set of data values that a column can contain is called the 
 domain
  of the 
 column. The domain of the 
 CITY
  column is the set of all names of cities. The domain of 
 the 
 SALES
  column is any money amount. The domain of the 
 REGION
  column is just two 
 data values, ""Eastern"" and ""Western,"" because those are the only two sales regions the 
 company has!
  
 - 44 -",NA
Primary Keys,"Because the rows of a relational table are unordered, you cannot select a specific row by 
 its position in the table. There is no ""first row,"" ""last row,"" or ""thirteenth row"" of a table. 
 How then can you specify a particular row, such as the row for the Denver sales office?
  
 In a well-designed relational database every table has some column or combination of 
 columns whose values uniquely identify each row in the table. This column (or columns) is 
 called the 
 primary key
  of the table. Look once again at the 
 OFFICES
  table in
 Figure 4-7
 . At 
 first glance, either the 
 OFFICE
  column or the 
 CITY
  column could serve as a primary key 
 for the table. But if the company expands and opens two sales offices in the same city, the 
 CITY
  column could no longer serve as the primary key. In practice, 
 ""ID 
  
 numbers,"" such as an office number (
 OFFICE
  in the 
 OFFICES
  table), an employee 
 number (
 EMPL_NUM
  in the 
 SALESREPS
  table), and customer numbers (
 CUST_NUM
  in the 
 CUSTOMERS
  table), are often chosen as primary keys. In the case of the 
 ORDERS
  table 
 there is no choice—the only thing that uniquely identifies an order is its order number 
 (
 ORDER_NUM
 ).
  
 The 
 PRODUCTS
  table, part of which is shown in Figure 4-8, is an example of a table where 
 the primary key must be a 
 combination
  of columns. The 
 MFR_ID
  column identifies the 
 manufacturer of each product in the table, and the 
 PRODUCT_ID
  column specifies the 
 manufacturer's product number. The 
 PRODUCT_ID
  column might make a good primary 
 key, but there's nothing to prevent two different manufacturers from using the same 
 number for their products. Therefore, a combination of the 
 MFR_ID 
 and 
 PRODUCT_ID 
 columns must be used as the primary key of the 
 PRODUCTS
  table. Every product in the 
 table is guaranteed to have a unique combination of data values in these two columns.
  
 - 45 -",NA
Relationships,"One of the major differences between the relational model and earlier data models is that 
 explicit pointers, such as the parent/child relationships of a hierarchical database, are 
 banned from relational databases. Yet obviously these relationships exist in a relational 
 database. For example, in the sample database, each of the salespeople is assigned to a 
 particular sales office, so there is an obvious relationship between the rows of the 
 OFFICES
  table and the rows of the 
 SALESREPS
  table. Doesn't the relational model ""lose 
 information"" by banning these relationships from the database?
  
 As shown in Figure 4-9, the answer to the question is ""no."" The figure shows a close-up 
 of a few rows of the 
 OFFICES
  and 
 SALESREPS
  tables. Note that the 
 REP_OFFICE 
 column of the 
 SALESREPS
  table contains the office number of the sales office where 
 each salesperson works. The domain of this column (the set of legal values it may 
 contain) is 
 precisely
  the set of office numbers found in the 
 OFFICE
  column of the 
 OFFICES
  table. In fact, you can find the sales office where Mary Jones works by finding 
 the value in Mary's 
 REP_OFFICE
  column (11) and finding the row of the 
 OFFICES
  table 
 that has a matching value in the 
 OFFICE
  column (in the row for the New York office). 
 Similarly, to find all the salespeople who work in New York, you could note the 
 OFFICE 
 value for the New York row (11) and then scan down the 
 REP_OFFICE
  column of the 
 SALESREPS
  table looking for matching values (in the rows for Mary Jones and Sam 
 Clark).
  
 - 46 -",NA
Foreign Keys,"A column in one table whose value matches the primary key in some other table is called 
 a 
 foreign key.
  In Figure 4-9 the 
 REP_OFFICE
  column is a foreign key for the 
 OFFICES 
 table. Although 
 REP_OFFICE
  is a column in the 
 SALESREPS
  table, the values that this 
 column contains are office numbers. They match values in the 
 OFFICE 
 column, which is 
 the primary key for the 
 OFFICES
  table. Together, a primary key and a foreign key create a 
 parent/child relationship between the tables that contain them, just like the parent/child 
 relationships in a hierarchical database.
  
 Just as a combination of columns can serve as the primary key of a table, a foreign key 
 can also be a combination of columns. In fact, the foreign key will 
 always
  be a compound 
 (multi-column) key when it references a table with a compound primary key. Obviously, 
 the number of columns and the data types of the columns in the foreign key and the 
 primary key must be identical to one another.
  
 A table can contain more than one foreign key if it is related to more than one other table. 
 Figure 4-10 shows the three foreign keys in the 
 ORDERS
  table of the sample database:
  
 - 47 -",NA
Codd's Twelve Rules *,"In his 1985 
 Computerworld
  article, Ted Codd presented 12 rules that a database must 
 obey if it is to be considered truly relational. Codd's 12 rules, shown in the following list, 
 have since become a semi-official definition of a relational database. The rules come out 
 of Codd's theoretical work on the relational model and actually represent more of an ideal 
 goal than a definition of a relational database.
  
 1.
 The information rule.
  All information in a relational database is represented explicitly at 
 the logical level and in exactly one way—by values in tables.
  
 2. 
 Guaranteed access rule.
  Each and every datum (atomic value) in a relational 
 database is guaranteed to be logically accessible by resorting to a combination of table 
 name, primary key value, and column name.
  
 3.
 Systematic treatment of null values
 . Null values (distinct from an empty character 
 string or a string of blank characters and distinct from zero or any other number) are 
  
 - 48 -",NA
Summary,"SQL is based on the relational data model that organizes the data in a database as a 
 collection of tables:
  
 •Each table has a table name that uniquely identifies it.
  
 •Each table has one or more named columns, which are arranged in a specific, left-to-
 right order.
  
 •Each table has zero or more rows, each containing a single data value in each 
 column. The rows are unordered.
  
 - 50 -",NA
Part II:,NA,NA
 Retrieving Data,NA,NA
Chapter List,"Chapter SQL Basics 
  
 5: 
  
 Chapter Simple Queries 
  
 6: 
  
 Chapter Multi-Table Queries (Joins) 
  
 7: 
  
 Chapter Summary Queries 
  
 8: 
  
 Chapter Subqueries and Query Expressions 9:",NA
Chapter 5:,NA,NA
 SQL Basics,NA,NA
Overview,"This chapter begins a detailed description of the features of SQL. It describes the basic 
 structure of a SQL statement and the basic elements of the language, such as keywords, 
 data types, and expressions. The way that SQL handles missing data through 
 NULL 
 values is also described. Although these are basic features of SQL, there are some subtle 
 differences in the way they are implemented by various popular SQL products, and in 
 many cases the SQL products provide significant extensions to the capabilities specified in 
 the ANSI/ISO SQL standard. These differences and extensions are also described in this 
 chapter.",NA
Statements,"The main body of the SQL language consists of about 40 statements, which are 
 summarized in Table 5-1. Each statement requests a specific action from the DBMS, 
 such as creating a new table, retrieving data, or inserting new data into the database. All 
 SQL statements have the same basic form, illustrated in Figure 5-1.
  
 - 51 -",NA
Names,"The objects in a SQL-based database are identified by assigning them unique names. 
  
 Names are used in SQL statements to identify the database object on which the 
  
 statement should act. The most fundamental named objects in a relational database are 
 table names (which identify tables), column names (which identify columns), and user 
 names (which identify users of the database); conventions for naming these objects were 
 specified in the original SQL1 standard. The ANSI/ISO SQL2 standard significantly 
 expanded the list of named entities, to include schemas (collections of tables), 
  
 constraints (restrictions on the contents of tables and their relationships), domains (sets of 
 legal values that may be assigned to a column), and several other types of objects. Many 
 SQL implementations support additional named objects, such as stored procedures 
 (Sybase and SQL Server), primary key/foreign key relationships (DB2), and data entry 
 forms (Ingres).
  
 The original ANSI/ISO standard specified that SQL names must contain 1 to 18 
 characters, must begin with a letter, and may not contain any spaces or special 
 punctuation characters. The SQL2 standard increased the maximum to 128 characters. 
 In practice the names supported by SQL-based DBMS products vary significantly. DB2, 
 for example, restricts user names to 8 characters but allows longer table and column 
 names. The various products also differ in the special characters they permit in table 
 names. For portability it's best to keep names relatively short and to avoid the use of 
 special characters.",NA
Table Names,"When you specify a table name in a SQL statement, SQL assumes that you are referring 
 to one of your own tables (that is, a table that you created). Usually, you will want to 
 choose table names that are short but descriptive. The table names in the sample 
 database (
 ORDERS
 , 
 CUSTOMERS
 , 
 OFFICES
 , 
 SALESREPS
 ) are good examples. In a 
 personal or departmental database, the choice of table names is usually up to the 
 database developer or designer.
  
 In a larger, shared-use corporate database, there may be corporate standards for naming 
 tables, to insure that table names do not conflict. In addition, most DBMS brands allow 
 different users to create tables with the same name (that is, both Joe and Sam can create 
 a table named 
 BIRTHDAYS
 ). The DBMS uses the appropriate table, depending on which 
 user is requesting data. With the proper permission, you can also refer to tables owned by 
 other users, by using a 
 qualified table name.
  A qualified table name specifies both the 
 name of the table's owner and the name of the table, separated by a period (.). For 
 example, Joe could access the 
 BIRTHDAYS
  table owned by Sam by using the qualified 
 table name:
  
 SAM.BIRTHDAYS
  
 A qualified table name generally can be used in a SQL statement wherever a table name 
 can appear.
  
 The ANSI/ISO SQL2 standard generalizes the notion of a qualified table name even 
 further. It allows you to create a named collection of tables, called a 
 schema
 . You can 
 refer to a table in a specific schema using a qualified table name. For example, the 
 BIRTHDAYS
  table in the 
 EMPLOYEEINFO
  schema would be referenced as: 
  
 EMPLOYEEINFO.BIRTHDAYS
  
 Chapter 13
  provides more information about schemas, users, and other aspects of SQL 
 database structure.
  
 - 57 -",NA
Column Names,"When you specify a column name in a SQL statement, SQL can normally determine from 
 the context which column you intend. However, if the statement involves two columns with 
 the same name from two different tables, you must use a 
 qualified column name
  to 
 unambiguously identify the column you intend. A qualified column name specifies both the 
 name of the table containing the column and the name of the column, separated by a 
 period (.). For example, the column named 
 SALES
  in the 
 SALESREPS 
 table has the 
 qualified column name:
  
 SALESREPS.SALES
  
 If the column comes from a table owned by another user, a qualified table name is used 
 in the qualified column name. For example, the 
 BIRTHDATE
  column in the 
 BIRTHDAYS 
 table owned by the user 
 SAM
  is specified by the fully qualified column name:
  
 SAM.BIRTHDAYS.BIRTH_DATE
  
 Qualified column names can generally be used in a SQL statement wherever a simple 
 (unqualified) column name can appear; exceptions are noted in the descriptions of the 
 individual SQL statements.",NA
Data Types,"The ANSI/ISO SQL standard specifies the various types of data that can be stored in a 
 SQL-based database and manipulated by the SQL language. The original SQL1 standard 
 specified only a minimal set of data types. The SQL2 standard expanded this list to 
 include variable-length character strings, date and time data, bit strings, and other types. 
 Today's commercial DBMS products can process a rich variety of different kinds of data, 
 and there is considerable diversity in the particular data types supported across different 
 DBMS brands. Typical data types include:
  
 •
 Integers
 . Columns holding this type of data typically store counts, quantities, ages, and so 
 on. Integer columns are also frequently used to contain I.D. numbers, such as customer, 
 employee, and order numbers.
  
 •
 Decimal numbers
 . Columns with this data type store numbers that have fractional 
 parts and must be calculated exactly, such as rates and percentages. They are also 
 frequently used to store money amounts.
  
 •
 Floating point numbers
 . Columns with this data type are used to store scientific 
 numbers that can be calculated approximately, such as weights and distances. Floating 
 point numbers can represent a larger range of values than decimal numbers but can 
 produce round-off errors in computations.
  
 •
 Fixed-lengthcharacter strings
 . Columns holding this type of data typically store names of 
 people and companies, addresses, descriptions, and so on.
  
 •
 Variable-length character strings
 . This data type allows a column to store character 
 strings that vary in length from row to row, up to some maximum length. (The SQL1 
 standard permitted only fixed-length character strings, which are easier for the DBMS to 
 process but can waste considerable space.)
  
 •
  
 Money amounts
 . Many SQL products support a 
 MONEY
  or 
 CURRENCY
  type, which is 
 usually stored as a decimal or floating point number. Having a distinct money type 
  
 allows the DBMS to properly format money amounts when they are displayed.
  
 - 58 -",NA
Constants,"In some SQL statements a numeric, character, or date data value must be expressed in 
 text form. For example, in this 
 INSERT
  statement, which adds a salesperson to the 
 database:
  
 INSERT INTO SALESREPS (EMPL_NUM, NAME, QUOTA, HIRE_DATE, SALES)
  
  VALUES (115, 'Dennis Irving', 175000.00, '21-JUN-90', 0.00)
  
 the value for each column in the newly inserted row is specified in the 
 VALUES
  clause. 
 Constant data values are also used in expressions, such as in this 
 SELECT
  statement:
  
 SELECT CITY
  
  FROM OFFICES
  
 WHERE TARGET > (1.1 * SALES) + 10000.00
  
 The ANSI/ISO SQL standard specifies the format of numeric and string constants, or 
 literals,
  which represent specific data values. These conventions are followed by most 
 SQL implementations.",NA
Numeric Constants,"Integer and decimal constants (also called 
 exact numeric literals
 ) are written as ordinary 
 decimal numbers in SQL statements, with an optional leading plus or minus sign.
  
 21  -375  2000.00  +497500.8778
  
 - 62 -",NA
String Constants,"The ANSI/ISO standard specifies that SQL constants for character data be enclosed in 
 single quotes ('. . .'), as in these examples:
  
 'Jones, John J.'  'New York'  'Western'
  
 If a single quote is to be included in the constant text, it is written within the constant as 
 two consecutive single quote characters. Thus this constant value:
  
 'I can''t'
  
 becomes the seven-character string ""I can't"".
  
 Some SQL implementations, such as SQL Server and Informix, accept string constants 
 enclosed in double quotes ("". . .""):
  
 ""Jones, John J.""  ""New York""  ""Western""
  
 Unfortunately, the double quotes pose portability problems with other SQL products, 
 including some unique portability problems with SQL/DS. SQL/DS allows column names 
 containing blanks and other special characters (in violation of the ANSI/ISO standard). 
 When these characters appear as names in a SQL statement, they must be enclosed in 
 double quotes. For example, if the 
 NAME
  column of the 
 SALESREPS
  table were called 
 ""FULL NAME
 "" in a SQL/DS database, this 
 SELECT
  statement would be valid:
  
 SELECT ""FULL NAME"", SALES, QUOTA
  
  FROM SALESREPS
  
 WHERE ""FULL NAME"" = 'Jones, John J.'
  
 The SQL2 standard provides the additional capability to specify string constants from a 
 specific national character set (for example, French or German) or from a user-defined 
 character set. These capabilities have not yet found their way into mainstream SQL 
 implementations.",NA
Date and Time Constants,"In SQL products that support date/time data, constant values for dates, times, and time 
 intervals are specified as string constants. The format of these constants varies from one 
 DBMS to the next. Even more variation is introduced by the differences in the way dates 
 and times are written in different countries.
  
 - 63 -",NA
Symbolic Constants,"In addition to user-supplied constants, the SQL language includes special symbolic 
 constants that return data values maintained by the DBMS itself. For example, in some 
 DBMS brands the symbolic constant 
 CURRENT_DATE
  yields the value of the current date 
 and can be used in queries such as the following, which lists the salespeople whose hire 
 date is still in the future.
  
 SELECT NAME, HIRE_DATE
  
  FROM SALESREPS
  
 WHERE HIRE_DATE > CURRENT_DATE
  
 The SQL1 standard specified only a single symbolic constant (the 
 USER
  constant 
 described in 
 Chapter 15
 ), but most SQL products provide many more. Generally, a 
 symbolic constant can appear in a SQL statement anywhere that an ordinary constant of 
 the same data type could appear. The SQL2 standard adopted the most useful symbolic 
 constants from current SQL implementations and provides for 
 CURRENT_DATE
 , 
  
 CURRENT_TIME
 , and 
 CURRENT_TIMESTAMP
  (note the underscores!) as well as 
 USER
 , 
 SESSION_USER
 , and 
 SYSTEM_USER
 .
  
 Some SQL products, including SQL Server, provide access to system values through 
 built-in functions rather than symbolic constants. The SQL Server version of the 
 preceding query is:
  
 SELECT NAME, HIRE_DATE
  
  FROM SALESREPS
  
 WHERE HIRE_DATE > GETDATE()
  
 Built-in functions are described later in this chapter.",NA
Expressions,"Expressions are used in the SQL language to calculate values that are retrieved from a 
 database and to calculate values used in searching the database. For example, this 
 query calculates the sales of each office as a percentage of its target:
  
 SELECT CITY, TARGET, SALES, (SALES/TARGET) * 100
  
  FROM OFFICES
  
 and this query lists the offices whose sales are more than $50,000 over target:
  
 SELECT CITY
  
  FROM OFFICES
  
 WHERE SALES > TARGET + 50000.00
  
 The ANSI/ISO SQL standard specifies four arithmetic operations that can be used in 
 expressions: addition (X + Y), subtraction (X – Y), multiplication (X * Y), and division (X / 
 Y). Parentheses can also be used to form more complicated expressions, like this one:
  
 - 65 -",NA
Built-in Functions,"Although the SQL1 standard doesn't specify them, most SQL implementations include a 
 number of useful 
 built-in functions.
  These facilities often provide data type conversion 
 facilities. For example, DB2's built-in 
 MONTH()
  and 
 YEAR()
  functions take a 
 DATE
  or 
 TIMESTAMP
  value as their input and return an integer that is the month or year portion of 
 the value. This query lists the name and month of hire for each salesperson in the sample 
 database:
  
 SELECT NAME, MONTH(HIRE_DATE)
  
  FROM SALESREPS
  
 and this one lists all salespeople hired in 1988:
  
 SELECT NAME, MONTH(HIRE_DATE)
  
  FROM SALESREPS
  
 WHERE YEAR(HIRE_DATE) = 1988
  
 Built-in functions also are often used for data reformatting. Oracle's built-in 
 TO_CHAR() 
 function, for example, takes a 
 DATE
  data type and a format specification as its arguments 
 and returns a string containing a formatted version of the date. In the results produced by 
 this query:
  
 SELECT NAME, TO_CHAR(HIRE_DATE,'DAY MONTH DD, YYYY')
  
  FROM SALESREPS
  
 the hire dates will all have the format ""Wednesday June 14, 1989"" because of the built-in 
 function.
  
 In general, a built-in function can be specified in a SQL expression anywhere that a 
 constant of the same data type can be specified. The built-in functions supported by 
 popular SQL dialects are too numerous to list here. The IBM SQL dialects include about 
 two dozen built-in functions, Oracle supports a different set of about two dozen built-in 
  
 - 66 -",NA
Missing Data (,NA,NA
NULL,NA,NA
 Values),"Because a database is usually a model of a real-world situation, certain pieces of data are 
 inevitably missing, unknown, or don't apply. In the sample database, for example, the 
 QUOTA
  column in the 
 SALESREPS
  table contains the sales goal for each salesperson. 
 However, the newest salesperson has not yet been assigned a quota; this data is missing 
 for that row of the table. You might be tempted to put a zero in the column for this 
 salesperson, but that would not be an accurate reflection of the situation. The 
  
 salesperson does not have a zero quota; the quota is just ""not yet known.""
  
 Similarly, the 
 MANAGER
  column in the 
 SALESREPS
  table contains the employee number of 
 each salesperson's manager. But Sam Clark, the Vice President of Sales, has no 
 manager in the sales organization. This column does not apply to Sam. Again, you might 
 think about entering a zero, or a 9999 in the column, but neither of these values would 
 really be the employee number of Sam's boss. No data value is applicable to this row.
  
 SQL supports missing, unknown, or inapplicable data explicitly, through the concept of a 
 null value.
  A null value is an 
 indicator
  that tells SQL (and the user) that the data is missing 
 or not applicable. As a convenience, a missing piece of data is often said to have the 
 value 
 NULL
 . But the 
 NULL
  value is not a real data value like 0, 473.83, or ""Sam Clark."" 
 Instead, it's a signal, or a reminder, that the data value is missing or unknown. 
  
 Figure 5-3 shows the contents of the 
 SALESREPS
  table. Note that the 
 QUOTA 
 and 
 REP_OFFICE
  values for Tom Snyder's row and the 
 MANAGER
  value for Sam Clark's row 
 of the table all contain 
 NULL
  values.
  
  
 Figure 5-3:
 NULL
  values in the 
 SALEREPS
  table
  
 In many situations 
 NULL
  values require special handling by the DBMS. For example, if the 
 user requests the sum of the 
 QUOTA
  column, how should the DBMS handle the missing 
 data when computing the sum? The answer is given by a set of special rules that govern 
 NULL
  value handling in various SQL statements and clauses. Because of these rules, 
 some leading database authorities feel strongly that 
 NULL
  values should not be used. 
 Others, including Dr. Codd, have advocated the use of multiple 
 NULL
  values, with distinct 
 indicators for ""unknown"" and ""not applicable"" data.
  
 Regardless of the academic debates, 
 NULL
  values are a well-entrenched part of the 
 ANSI/ISO SQL standard and are supported in virtually all commercial SQL products. They 
 also play an important, practical role in production SQL databases. The special rules that 
 apply to 
 NULL
  values (and the cases where 
 NULL
  values are handled inconsistently by 
 various SQL products) are pointed out in the relevant sections of this book.",NA
Summary,"This chapter described the basic elements of the SQL language. The basic structure of 
 SQL can be summarized as follows:
  
 •The SQL language that is in common use includes about 30 statements, each 
 consisting of a verb and one or more clauses. Each statement performs a single, 
  
 - 68 -",NA
Chapter 6: ,NA,NA
Simple Queries,NA,NA
Overview,"In many ways, queries are the heart of the SQL language. The 
 SELECT
  statement, which is 
 used to express SQL queries, is the most powerful and complex of the SQL statements. 
 Despite the many options afforded by the 
 SELECT
  statement, it's possible to start simply 
 and then work up to more complex queries. This chapter discusses the simplest SQL 
 queries—those that retrieve data from a single table in the database.",NA
The ,NA,NA
SELECT,NA,NA
 Statement,"The 
 SELECT
  statement retrieves data from a database and returns it to you in the form of 
 query results. You have already seen many examples of the 
 SELECT
  statement in the 
 quick tour presented in 
 Chapter 2
 . Here are several more sample queries that retrieve 
 information about sales offices:
  
 List the sales offices with their targets and actual sales.
  
 SELECT CITY, TARGET, SALES
  
  
  FROM OFFICES
  
 CITY               TARGET        SALES-
 ------------  ----------   -----------
 Denver        $300,000.00   $186,042.00 
 New York      $575,000.00   $692,637.00 
 Chicago       $800,000.00   $735,042.00 
 Atlanta       $350,000.00   $367,911.00 
 Los Angeles   $725,000.00   $835,915.00
  
 List the Eastern region sales offices with their targets and sales.
  
 SELECT CITY, TARGET, SALES
  
  
  FROM OFFICES 
  
 WHERE REGION = 'Eastern'
  
 CITY                TARGET         
 SALES-------------  -----------  ------
 ------New York       $575,000.00   
 $692,637.00 Chicago        $800,000.00   
 $735,042.00
  
 - 69 -",NA
The ,NA,NA
SELECT,NA,NA
 Clause,"The 
 SELECT
  clause that begins each 
 SELECT
  statement specifies the data items to be 
 retrieved by the query. The items are usually specified by a 
 select list,
  a list of 
 select 
 items
  separated by commas. Each select item in the list generates a single column of 
 query results, in left-to-right order. A select item can be:
  
 •
  
 a 
 column name
 , identifying a column from the table(s) named in the 
 FROM 
 clause. 
 When a column name appears as a select item, SQL simply takes the value of that 
  
 column from each row of the database table and places it in the corresponding row of 
 query results.
  
 •a 
 constant
 , specifying that the same constant value is to appear in every row of the 
 query results.
  
 •a 
 SQL expression
 , indicating that SQL must calculate the value to be placed into the 
 query results, in the style specified by the expression.
  
 Each type of select item is described later in this chapter.",NA
The ,NA,NA
FROM,NA,NA
 Clause,"The 
 FROM
  clause consists of the keyword 
 FROM
 , followed by a list of table specifications 
 separated by commas. Each table specification identifies a table containing data to be 
 retrieved by the query. These tables are called the 
 source tables
  of the query (and of the 
 SELECT
  statement) because they are the source of all of the data in the query results. All of 
 the queries in this chapter have a single source table, and every 
 FROM
  clause contains a 
 single table name.",NA
Query Results,- 71 -,NA
Simple Queries,"The simplest SQL queries request columns of data from a single table in the database. 
 For example, this query requests three columns from the 
 OFFICES
  table:
  
 List the location, region, and sales of each sales office.
  
 SELECT CITY, REGION, SALES
  
  
  FROM OFFICES
  
 CITY             REGION          
 SALES
  
 --------------  -------   -----------
 -
  
 Denver 
  
  
  
  
  
  
  
  
  
 Western    
 $186,042.00 
  
 New 
 York 
  
  
  
  
  
  
  
 Eastern    
 $692,637.00 
  
 Chicago 
  
  
  
  
  
  
  
  
 Eastern    
 $735,042.00 
  
 Atlanta 
  
  
  
  
  
  
  
  
 Eastern    
 $367,911.00 
  
 Los 
 Angeles 
  
  
  
  
 Western    
 $835,915.00
  
 The 
 SELECT
  statement for simple queries like this one includes only the two required 
 clauses. The 
 SELECT
  clause names the requested columns; the 
 FROM
  clause names the 
 table that contains them.
  
 Conceptually, SQL processes the query by going through the table named in the 
 FROM 
 clause, one row at a time, as shown in Figure 6-3. For each row, SQL takes the values of 
 the columns requested in the select list and produces a single row of query results. The 
 query results thus contain one row of data for each row in the table.
  
  
 Figure 6-3: 
 Simple query processing (no 
 WHERE
  clause)",NA
Calculated Columns,NA,NA
Selecting All Columns (,NA,NA
SELECT *,NA,NA
),"Sometimes it's convenient to display the contents of all the columns of a table. This can 
 be particularly useful when you first encounter a new database and want to get a quick 
 understanding of its structure and the data it contains. As a convenience, SQL lets you 
 use an asterisk (*) in place of the select list as an abbreviation for ""all columns"":
  
 Show me all the data in the 
 OFFICES
  table.
  
 SELECT *
  
  
  FROM OFFICES
  
 OFFICE CITY         REGION    MGR       TARGET        SALES-
 ------------------  --------  ---  -----------  -----------
  
  
 22 Denver       Western   108  $300,000.00  $186,042.00
  
  
 11 New York     Eastern   106  $575,000.00  $692,637.00
  
  
 12 Chicago      Eastern   104  $800,000.00  $735,042.00
  
  
 13 Atlanta      Eastern   105  $350,000.00  $367,911.00
  
  
 21 Los Angeles  Western   108  $725,000.00  $835,915.00
  
 The query results contain all six columns of the 
 OFFICES
  table, in the same left-to-right 
 order as in the table itself.
  
 The ANSI/ISO SQL standard specifies that a 
 SELECT
  statement can have either an all-
 column selection or a select list, but not both, as shown in 
 Figure 6-1
 . However, many 
 SQL implementations treat the asterisk (*) as just another element of the select list. Thus 
 the query:
  
 SELECT *, (SALES - TARGET)
  
  
  FROM OFFICES
  
 is legal in most commercial SQL dialects (for example in DB2, Oracle, and SQL Server), 
 but it is not permitted by the ANSI/ISO standard.
  
 The all-columns selection is most appropriate when you are using interactive SQL 
 casually. It should be avoided in programmatic SQL, because changes in the database 
 structure can cause a program to fail. For example, suppose the 
 OFFICES
  table were 
 dropped from the database and then re-created with its columns rearranged and a new 
 seventh column added. SQL automatically takes care of the database-related details of 
 such changes, but it cannot modify your application program for you. If your program 
 expects a 
 SELECT * FROM OFFICES
  query to return six columns of query results with 
 certain data types, it will almost certainly stop working when the columns are rearranged 
 and a new one is added.
  
 These difficulties can be avoided if you write the program to request the columns it needs 
 by name. For example, the following query produces the same results as 
 SELECT * 
 FROM OFFICES
 . It is also immune to changes in the database structure, as long as the 
 named columns continue to exist in the 
 OFFICES
  table:
  
 SELECT OFFICE, CITY, REGION, MGR, TARGET, SALES 
 FROM OFFICES
  
 - 77 -",NA
Duplicate Rows (,NA,NA
DISTINCT,NA,NA
),"If a query includes the primary key of a table in its select list, then every row of query 
 results will be unique (because the primary key has a different value in each row). If the 
 primary key is not included in the query results, duplicate rows can occur. For example, 
 suppose you made this request:
  
 List the employee numbers of all sales office managers.
  
 SELECT MGR
  
  FROM OFFICES
  
 MGR
  
 ----
  
 108
  
 106
  
 104
  
 105
  
 108
  
 The query results have five rows (one for each office), but two of them are exact 
  
 duplicates of one another. Why? Because Larry Fitch manages both the Los Angeles and 
 Denver offices, and his employee number (108) appears in both rows of the 
 OFFICES 
 table. These query results are probably not exactly what you had in mind. If there are four 
 different managers, you might have expected only four employee numbers in the query 
 results.
  
 You can eliminate duplicate rows of query results by inserting the keyword 
 DISTINCT
  in 
 the 
 SELECT
  statement just before the select list. Here is a version of the previous query 
 that produces the results you want:
  
 List the employee numbers of all sales office managers.
  
 SELECT DISTINCT MGR
  
  FROM OFFICES
  
 MGR
  
 ----
  
 104
  
 105
  
 106
  
 108
  
 Conceptually, SQL carries out this query by first generating a full set of query results (five 
 rows) and then eliminating rows that are exact duplicates of one another to form the final 
 query results. The 
 DISTINCT
  keyword can be specified regardless of the contents of the 
 SELECT
  list (with certain restrictions for summary queries, as described in 
 Chapter 8
 ).
  
 If the 
 DISTINCT
  keyword is omitted, SQL does not eliminate duplicate rows. You can also 
 specify the keyword 
 ALL
  to explicitly indicate that duplicate rows are to be retained, but it is 
 unnecessary since this is the default behavior.",NA
Row Selection (,NA,NA
WHERE,NA,NA
 Clause),- 78 -,NA
Search Conditions,"SQL offers a rich set of search conditions that allow you to specify many different kinds of 
 queries efficiently and naturally. Five basic search conditions (called 
 predicates
  in the 
 ANSI/ISO standard) are summarized here and are described in the sections that follow:
  
 •
 Comparison test
 . Compares the value of one expression to the value of another 
 expression. Use this test to select offices in the Eastern region, or salespeople whose 
 sales are above their quotas.
  
 •
 Range test
 . Tests whether the value of an expression falls within a specified range of 
 values. Use this test to find salespeople whose sales are between $100,000 and 
 $500,000.
  
 •
 Set membership test.
  Checks whether the value of an expression matches one of a 
 set of values. Use this test to select offices located in New York, Chicago, or Los 
 Angeles.
  
 - 80 -",NA
Comparison Test (,NA,NA
=,NA,NA
", ",NA,NA
<>,NA,NA
", ",NA,NA
<,NA,NA
", ",NA,NA
<=,NA,NA
", ",NA,NA
>,NA,NA
", ",NA,NA
>=,NA,NA
),"The most common search condition used in a SQL query is a comparison test. In a 
 comparison test, SQL computes and compares the values of two SQL expressions for 
 each row of data. The expressions can be as simple as a column name or a constant, or 
 they can be more complex arithmetic expressions. SQL offers six different ways of 
 comparing the two expressions, as shown in Figure 6-7. Here are some examples of 
 typical comparison tests:
  
  
 Figure 6-7: 
 Comparison test syntax diagram
  
 Find salespeople hired before 1988.
  
 SELECT NAME
  
  
  FROM SALESREPS 
  
 WHERE HIRE_DATE < '01-JAN-88'
  
 NAME
  
 ---------
  
 Sue Smith 
  
 Bob Smith 
  
 Dan Roberts 
  
 Paul Cruz
  
 List the offices whose sales fall below 80 percent of target.
  
 SELECT CITY, SALES, TARGET
  
  
  FROM OFFICES 
  
 WHERE SALES < (.8 * TARGET)
  
 CITY           SALES       
 TARGET
  
 -------  -----------  ----------
 -
  
 Denver 
  
  
 $186,042.00  
 $300,000.00
  
 List the offices not managed by employee number 108.
  
 SELECT CITY, MGR
  
 - 81 -",NA
Range Test (,NA,NA
BETWEEN,NA,NA
),"SQL provides a different form of search condition with the range test (
 BETWEEN
 ) shown in 
 Figure 6-8. The range test checks whether a data value lies between two specified values. 
 It involves three SQL expressions. The first expression defines the value to be tested; the 
 second and third expressions define the low and high ends of the range to be checked. 
 The data types of the three expressions must be comparable. 
  
  
 Figure 6-8: 
 Range test 
 (BETWEEN)
  syntax diagram
  
 This example shows a typical range test:
  
 - 83 -",NA
Set Membership Test (,NA,NA
IN,NA,NA
),"Another common search condition is the set membership test (
 IN
 ), shown in Figure 6-9. 
 It tests whether a data value matches one of a list of target values. Here are several 
 queries that use the set membership test:
  
  
 Figure 6-9: 
 Set membership  test 
 (IN)
 syntax diagram
  
 - 85 -",NA
Pattern Matching Test (,NA,NA
LIKE,NA,NA
),"A simple comparison test can be used to retrieve rows where the contents of a text 
 column match some particular text. For example, this query retrieves a row of the 
 CUSTOMERS
  table by name:
  
 Show the credit limit for Smithson Corp.
  
 SELECT COMPANY, CREDIT_LIMIT
  
  FROM CUSTOMERS
  
 WHERE COMPANY = 'Smithson Corp.'
  
 However, you might easily forget whether the company's name was ""Smith,"" ""Smithson,"" 
 or ""Smithsonian."" SQL's pattern matching test can be used to retrieve the data based on a 
 partial match of the customer's name.
  
 The pattern matching test (
 LIKE
 ), shown in Figure 6-10, checks to see whether the data 
 value in a column matches a specified 
 pattern.
  The pattern is a string that may include 
 one or more 
 wildcard
  characters. These characters are interpreted in a special way.
  
  
 Figure 6-10: 
 Pattern matching test 
 (LIKE)
  syntax diagram
  
 Wildcard Characters
  
 The percent sign (%) wildcard character matches any sequence of zero or more 
  
 characters. Here's a modified version of the previous query that uses the percent sign for 
 pattern matching:
  
 SELECT COMPANY, CREDIT_LIMIT
  
  FROM CUSTOMERS
  
 WHERE COMPANY LIKE 'Smith% Corp.'
  
 - 87 -",NA
Null Value Test (,NA,NA
IS NULL,NA,NA
),"NULL
  values create a three-valued logic for SQL search conditions. For any given row, 
 the result of a search condition may be 
 TRUE
  or 
 FALSE
 , or it may be 
 NULL
  because one 
 of the columns used in evaluating the search condition contains a 
 NULL
  value. 
  
 Sometimes it's useful to check explicitly for 
 NULL
  values in a search condition and handle 
 them directly. SQL provides a special 
 NULL
  value test (
 IS NULL
 ), shown in Figure 6-11, 
 to handle this task.
  
  
 Figure 6-11: 
 NULL
  value test 
 (IS NULL)
  syntax diagram
  
 This query uses the 
 NULL
  value test to find the salesperson in the sample database who 
 has not yet been assigned to an office:
  
 Find the salesperson not yet assigned to an office.
  
 SELECT NAME
  
  FROM SALESREPS
  
 WHERE REP_OFFICE IS NULL
  
 NAME
  
 -----------
  
 Tom Snyder
  
 The negated form of the 
 NULL
  value test (
 IS NOT NULL
 ) finds rows that do not contain a 
 NULL
  value:
  
 - 89 -",NA
Compound Search Conditions (,NA,NA
AND,NA,NA
", ",NA,NA
OR,NA,NA
", and ",NA,NA
NOT,NA,NA
),"The simple search conditions described in the preceding sections return a value of 
 TRUE
 , 
 FALSE
 , or 
 NULL
  when applied to a row of data. Using the rules of logic, you can combine 
 these simple SQL search conditions to form more complex ones, as shown in Figure 6-12. 
 Note that the search conditions combined with 
 AND
 , 
 OR
 , and 
 NOT
  may themselves be 
 compound search conditions.
  
 - 90 -",NA
Sorting Query Results (,NA,NA
ORDER BY,NA,NA
 Clause),"Like the rows of a table in the database, the rows of query results are not arranged in any 
 particular order. You can ask SQL to sort the results of a query by including the 
 ORDER 
 BY
  clause in the 
 SELECT
  statement. The 
 ORDER BY
  clause, shown in Figure 6-14, 
 consists of the keywords 
 ORDER BY
 , followed by a list of sort specifications separated by 
  
 - 93 -",NA
Rules for Single-Table Query Processing,"Single-table queries are generally simple, and it's usually easy to understand the 
 meaning of a query just by reading the 
 SELECT
  statement. As queries become more",NA
Combining Query Results (,NA,NA
UNION,NA,NA
) *,"Occasionally, it's convenient to combine the results of two or more queries into a single 
 table of query results. SQL supports this capability through the 
 UNION
  feature of the 
 SELECT
  statement. Figure 6-15 illustrates how the 
 UNION
  operation can be used to 
 satisfy the following request:
  
  
 Figure 6-15: 
 Using 
 UNION
  to combine query results
  
 List all the products where the price of the product exceeds $2,000 or where more than 
 $30,000 of the product has been ordered in a single order.
  
 The first part of the request can be satisfied with the top query in the figure:
  
 List all the products whose price exceeds $2,000.
  
 - 96 -",NA
Unions and Duplicate Rows *,"Because the 
 UNION
  operation combines the rows from two sets of query results, it would 
 tend to produce query results containing duplicate rows. For example, in the query of 
 Figure 6-15
 , product REI-2A44L sells for $4,500.00, so it appears in the top set of query 
 results. There is also an order for $31,500.00 worth of this product in the 
 ORDERS
  table, so 
 it also appears in the bottom set of query results. By default, the 
 UNION
  operation 
 eliminates
  duplicate rows as part of its processing. Thus, the combined set of query 
 results contains only 
 one
  row for product REI-2A44L.
  
 If you want to retain duplicate rows in a 
 UNION
  operation, you can specify the 
 ALL 
 keyword immediately following the word 
 ""UNION
 ."" This form of the query produces two 
 duplicate rows for product REI-2A44L:
  
 List all the products where the price of the product exceeds $2,000 or where more than 
 $30,000 of the product has been ordered in a single order.
  
 SELECT MFR_ID, PRODUCT_ID
  
  
  FROM PRODUCTS 
  
 WHERE PRICE > 2000.00 
  
 UNION ALL 
  
 SELECT DISTINCT MFR, PRODUCT
  
  
  FROM ORDERS 
  
 WHERE AMOUNT > 30000.00
  
 ACI    4100Y 
  
 REI    2A44L 
  
 ACI    4100Z 
  
 REI    2A44R 
  
 IMM    775C 
  
 REI    2A44L 
  
 REI    2A44R
  
 Note that the default duplicate row handling for the 
 UNION
  operation and for the simple 
 SELECT
  statement is exactly opposite. For the 
 SELECT
  statement, 
 SELECT ALL 
 (duplicates retained) is the default. To eliminate duplicate rows, you must explicitly 
  
 - 98 -",NA
Unions and Sorting *,"The 
 ORDER BY
  clause cannot appear in either of the two 
 SELECT
  statements combined 
 by a 
 UNION
  operation. It wouldn't make much sense to sort the two sets of query results 
 anyway, because they are fed directly into the 
 UNION
  operation and are never visible to 
 the user. However, the 
 combined
  set of query results produced by the 
 UNION 
 operation 
 can be sorted by specifying an 
 ORDER BY
  clause after the second 
 SELECT 
 statement. 
  
 Since the columns produced by the 
 UNION
  operation are not named, the 
 ORDER BY 
 clause must specify the columns by column number.
  
 Here is the same products query as that shown in 
 Figure 6-15
 , with the query results 
 sorted by manufacturer and product number:
  
 List all the products where the price of the product exceeds $2,000 or where more than 
 $30,000 of the product has been ordered in a single order, sorted by manufacturer and 
 product number.
  
 SELECT MFR_ID, PRODUCT_ID
  
  FROM PRODUCTS
  
 WHERE PRICE > 2000.00
  
 UNION
  
 SELECT DISTINCT MFR, PRODUCT
  
  FROM ORDERS
  
 WHERE AMOUNT > 30000.00
  
 ORDER BY 1, 2
  
 ACI    4100Y
  
 ACI    4100Z
  
 IMM    775C
  
 REI    2A44L
  
 REI    2A44R",NA
Multiple ,NA,NA
UNIONs ,NA,NA
*,"The 
 UNION
  operation can be used repeatedly to combine three or more sets of query 
 results, as shown in Figure 6-16. The union of Table B and Table C in the figure 
 produces a single, combined table. This table is then combined with Table A in another 
  
 - 99 -",NA
Summary,"This chapter is the first of four chapters about SQL queries. It described the following 
 query features:
  
 •
  
 The 
 SELECT
  statement is used to express a SQL query. Every 
 SELECT
  statement 
 produces a table of query results containing one or more columns and zero or more 
  
 rows.
  
 •The 
 FROM
  clause specifies the table(s) containing the data to be retrieved by a query.
  
 •The 
 SELECT
  clause specifies the column(s) of data to be included in the query results, 
 which can be columns of data from the database, or calculated columns.
  
 •The 
 WHERE
  clause selects the rows to be included in the query results by applying a 
 search condition to rows of the database.
  
 •A search condition can select rows by comparing values, by checking a value against a 
 range or set of values, by matching a string pattern, and by checking for 
 NULL 
 values.
  
 •Simple search conditions can be combined with 
 AND
 , 
 OR
 , and 
 NOT
  to form more 
 complex search conditions.
  
 •The 
 ORDER BY
  clause specifies that the query results should be sorted in ascending or 
 descending order, based on the values of one or more columns.
  
 •The 
 UNION
  operation can be used within a 
 SELECT
  statement to combine two or more 
 sets of query results into a single set.",NA
A Two-Table Query Example,"The best way to understand the facilities that SQL provides for multi-table queries is to 
 start with a simple request that combines data from two different tables:
  
 ""List all orders, showing the order number and amount, and the name and credit limit of 
 the customer who placed it.""
  
 The four specific data items requested are clearly stored in two different tables, as shown 
 in Figure 7-1.
  
 - 101 -",NA
Simple Joins (Equi-Joins),"The process of forming pairs of rows by matching the contents of related columns is called 
 joining
  the tables. The resulting table (containing data from both of the original tables) is 
 called a 
 join
  between the two tables. (A join based on an exact match between two 
 columns is more precisely called an 
 equi-join
 . Joins can also be based on other kinds of 
 column comparisons, as described later in this chapter.)
  
 Joins are the foundation of multi-table query processing in SQL. All of the data in a 
 relational database is stored in its columns as explicit data values, so all possible 
 relationships between tables can be formed by matching the contents of related columns. 
 Joins thus provide a powerful facility for 
 exercising
  the data relationships in a database. 
  
 In fact, because relational databases do not contain pointers or other mechanisms for 
 relating rows to one another, joins are the 
 only
  mechanism for exercising cross-table data 
 relationships.
  
 Because SQL handles multi-table queries by matching columns, it should come as no 
 surprise that the 
 SELECT
  statement for a multi-table query must contain a search 
 condition that specifies the column match. Here is the 
 SELECT
  statement for the query 
 that was performed manually in 
 Figure 7-2
 :
  
 List all orders showing order number, amount, customer name, and the customer's credit 
 limit.
  
 SELECT ORDER_NUM, AMOUNT, COMPANY, CREDIT_LIMIT
  
  FROM ORDERS, CUSTOMERS
  
 WHERE CUST = CUST_NUM
  
 ORDER_NUM      AMOUNT COMPANY             CREDIT_LIMIT
  
 ----------   --------------------------    ------------
  
  112989   $1,458.00 Jones Mfg.            $65,000.00
  
  112968   $3,978.00 First Corp.           $65,000.00
  
  112963   $3,276.00 Acme Mfg.             $50,000.00
  
  112987  $27,500.00 Acme Mfg.             $50,000.00
  
  112983     $702.00 Acme Mfg.             $50,000.00
  
  113027   $4,104.00 Acme Mfg.             $50,000.00
  
  112993   $1,896.00 Fred Lewis Corp.      $65,000.00
  
  113065   $2,130.00 Fred Lewis Corp.      $65,000.00
  
  113036  $22,500.00 Ace International     $35,000.00
  
  113034     $632.00 Ace International     $35,000.00
  
 - 103 -",NA
Parent/Child Queries,"The most common multi-table queries involve two tables that have a natural parent/child 
 relationship. The query about orders and customers in the preceding section is an 
 example of such a query. Each order (child) has an associated customer (parent), and 
 each customer (parent) can have many associated orders (children). The pairs of rows 
 that generate the query results are parent/child row combinations.
  
 You may recall from 
 Chapter 4
  that foreign keys and primary keys create the parent/child 
 relationship in a SQL database. The table containing the foreign key is the child in the 
 relationship; the table with the primary key is the parent. To exercise the parent/child 
 relationship in a query, you must specify a search condition that compares the foreign key 
 and the primary key. Here is another example of a query that exercises a parent/child 
 relationship, shown in Figure 7-3:
  
  
 Figure 7-3: 
 A parent/child query with 
 OFFICES
  and 
 SALESREPS
  
 - 104 -",NA
Joins with Row Selection Criteria,"The search condition that specifies the matching columns in a multi-table query can be 
 combined with other search conditions to further restrict the contents of the query results. 
  
 Suppose you want to rerun the preceding query, showing only offices with large sales 
 targets:
  
 List the offices with a target over $600,000.
  
 SELECT CITY, NAME, TITLE
  
  FROM OFFICES, SALESREPS
  
 WHERE MGR = EMPL_NUM
  
  AND TARGET > 600000.00
  
 CITY         NAME         TITLE
  
 -----------  -----------  ---------
  
 Chicago      Bob Smith    Sales Mgr
  
 Los Angeles  Larry Fitch  Sales Mgr
  
 With the additional search condition, the rows that appear in the query results are further 
 restricted. The first test (
 MGR=EMPL_NUM
 ) selects only pairs of 
 OFFICES
  and 
 SALESREPS 
 rows that have the proper parent/child relationship; the second test further selects only 
 those pairs of rows where the office is above target.",NA
Multiple Matching Columns,"The 
 ORDERS
  table and the 
 PRODUCTS
  table in the sample database are related by a 
 composite foreign key/primary key pair. The 
 MFR
  and 
 PRODUCT
  columns of the 
 ORDERS 
 table together form a foreign key for the 
 PRODUCTS
  table, matching its 
 MFR_ID
  and 
 PRODUCT_ID
  columns, respectively. To join the tables based on this parent/child 
 relationship, you must specify 
 both
  pairs of matching columns, as shown in this example:
  
 List all the orders, showing amounts and product descriptions.
  
 SELECT ORDER_NUM, AMOUNT, DESCRIPTION
  
  FROM ORDERS, PRODUCTS
  
 WHERE MFR = MFR_ID
  
  AND PRODUCT = PRODUCT_ID
  
 ORDER_NUM     AMOUNT DESCRIPTION
  
 ----------  --------- --------------
  
  113027  $4,104.00 Size 2 Widget
  
 - 106 -",NA
Queries with Three or More Tables,"SQL can combine data from three or more tables using the same basic techniques used 
 for two-table queries. Here is a simple example of a three-table join:
  
 List orders over $25,000, including the name of the salesperson who took the order and 
 the name of the customer who placed it.
  
 SELECT ORDER_NUM, AMOUNT, COMPANY, NAME
  
  FROM ORDERS, CUSTOMERS, SALESREPS 
  
 WHERE CUST = CUST_NUM
  
   
  AND REP = EMPL_NUM
  
   
  AND AMOUNT > 25000.00
  
 ORDER_NUM      AMOUNT  COMPANY          NAME
  
 ----------  ----------  ---------------  -------------
  
  112987  $27,500.00  Acme Mfg.        Bill Adams
  
  
 113069  $31,350.00  Chen Associates  Nancy Angelli
  
  
 113045  $45,000.00  Zetacorp         Larry Fitch
  
  
 112961  $31,500.00  J.P. Sinclair    Sam Clark
  
 This query uses two foreign keys in the 
 ORDERS
  table, as shown in Figure 7-5. The 
 CUST 
 column is a foreign key for the 
 CUSTOMERS
  table, linking each order to the customer who 
 placed it. The 
 REP
  column is a foreign key for the 
 SALESREPS
  table, linking each order to 
 the salesperson who took it. Informally speaking, the query links each order to its 
 associated customer and salesperson.
  
  
 - 107 -",NA
Other Equi-Joins,"The vast majority of multi-table queries are based on parent/child relationships, but SQL 
 does not require that the matching columns be related as a foreign key and primary key. 
 Any pair of columns from two tables can serve as matching columns, provided they have 
 comparable data types. The next example demonstrates a query that uses a pair of dates 
 as matching columns.
  
 Find all orders received on days when a new salesperson was hired.
  
 SELECT ORDER_NUM, AMOUNT, ORDER_DATE, NAME
  
  FROM ORDERS, SALESREPS 
  
 WHERE ORDER_DATE = HIRE_DATE
  
 ORDER_NUM      AMOUNT  ORDER_DATE  NAME
  
 ----------  ----------  ---------   -----------
  
  112968   $3,978.00  12-OCT-89   Mary Jones
  
  112979  $15,000.00  12-OCT-89   Mary Jones
  
  112975   $2,100.00  12-OCT-89   Mary Jones
  
  112968   $3,978.00  12-OCT-89   Larry Fitch
  
  112979  $15,000.00  12-OCT-89   Larry Fitch
  
  112975   $2,100.00  12-OCT-89   Larry Fitch
  
 - 109 -",NA
Non-Equi Joins,"The term 
 join
  applies to any query that combines data from two tables by comparing the 
 values in a pair of columns from the tables. Although joins based on equality between 
 matching columns (equi-joins) are by far the most common joins, SQL also allows you to 
 join tables based on other comparison operators. Here's an example where a greater than 
 (>) comparison test is used as the basis for a join:
  
 List all combinations of salespeople and offices where the salesperson's quota is more 
 than the office's target.
  
 SELECT NAME, QUOTA, CITY, TARGET
  
  FROM SALESREPS, OFFICES
  
 - 110 -",NA
SQL Considerations for Multi-Table Queries,"The multi-table queries described thus far have not required any special SQL syntax or 
 language features beyond those described for single-table queries. However, some multi-
 table queries cannot be expressed without the additional SQL language features 
 described in the following sections. Specifically:
  
 •
 Qualified column names
  are sometimes needed in multi-table queries to eliminate 
 ambiguous column references.
  
 •
 All-column selections
  (
 SELECT *
 ) have a special meaning for multi-table queries.
  
 •
 Self-joins
  can be used to create a multi-table query that relates a table to itself.
  
 •
 Table aliases
  can be used in the 
 FROM
  clause to simplify qualified column names and 
 allow unambiguous column references in self-joins.",NA
Qualified Column Names,"The sample database includes several instances where two tables contain columns with 
 the same name. The 
 OFFICES
  table and the 
 SALESREPS
  table, for example, both have a 
 column named 
 SALES
 . The column in the 
 OFFICES
  table contains year-to-date sales for 
 each office; the one in the 
 SALESREPS
  table contains year-to-date sales for each 
 salesperson. Normally, there is no confusion between the two columns, because the 
 FROM
  
 clause determines which of them is appropriate in any given query, as in these examples:
  
 Show the cities where sales exceed target.
  
 SELECT CITY, SALES
  
  FROM OFFICES
  
 WHERE SALES > TARGET
  
 Show all salespeople with sales over $350,000.
  
 - 111 -",NA
All-Column Selections,- 112 -,NA
Self-Joins,"Some multi-table queries involve a relationship that a table has with itself. For example, 
 suppose you want to list the names of all salespeople and their managers. Each 
 salesperson appears as a row in the 
 SALESREPS
  table, and the 
 MANAGER
  column 
 contains the employee number of the salesperson's manager. It would appear that the 
 MANAGER
  column should be a foreign key for the table that holds data about managers. 
 In fact it is—it's a foreign key for the 
 SALESREPS
  table itself!
  
 If you tried to express this query like any other two-table query involving a foreign 
 key/primary key match, it would look like this:
  
 SELECT NAME, NAME
  
  FROM SALESREPS, SALESREPS
  
 WHERE MANAGER = EMPL_NUM
  
 This 
 SELECT
  statement is illegal because of the duplicate reference to the 
 SALESREPS 
 table in the 
 FROM
  clause. You might also try eliminating the second reference to the 
 SALESREPS
  table:
  
 SELECT NAME, NAME
  
  FROM SALESREPS
  
 - 113 -",NA
Table Aliases,"As described in the previous section, table aliases are required in queries involving self-
 joins. However, you can use an alias in any query. For example, if a query refers to 
 another user's table, or if the name of a table is very long, the table name can become 
 tedious to type as a column qualifier. This query, which references the 
 BIRTHDAYS
  table 
 owned by the user named 
 SAM
 :
  
 List names, quotas, and birthdays of salespeople.
  
 SELECT SALESREPS.NAME, QUOTA, SAM.BIRTHDAYS.BIRTH_DATE
  
  FROM SALESREPS, BIRTHDAYS
  
 WHERE SALESREPS.NAME = SAM.BIRTHDAYS.NAME
  
 becomes easier to read and type when the aliases 
 S
  and 
 B
  are used for the two tables:
  
 List names, quotas, and birthdays of salespeople.
  
 SELECT S.NAME, S.QUOTA, B.BIRTH_DATE
  
  FROM SALESREPS S, SAM.BIRTHDAYS B
  
 WHERE S.NAME = B.NAME
  
 Figure 7-10 shows the basic form of the 
 FROM
  clause for a multi-table 
 SELECT
  statement, 
 complete with table aliases. The clause has two important functions:
  
  
 Figure 7-10: 
 FROM
  clause syntax diagram
  
 •The 
 FROM
  clause identifies all of the tables that contribute data to the query results. Any 
 columns referenced in the 
 SELECT
  statement must come from one of the tables named 
 in the 
 FROM
  clause. (There is an exception for 
 outer
  references contained in a subquery, 
 as described in 
 Chapter 9
 .)
  
 •The 
 FROM
  clause specifies the 
 tag
  that is used to identify the table in qualified column 
 references within the 
 SELECT
  statement. If a table alias is specified, it becomes the table 
 tag; otherwise, the table's name, exactly as it appears in the 
 FROM
  clause, becomes the 
 tag.
  
 The only requirement for table tags in the 
 FROM
  clause is that all of the table tags in a given 
 FROM
  clause must be distinct from each other. The SQL2 specification optionally allows the 
 keyword 
 AS
  to appear between a table name and table alias. While this makes the 
 FROM 
 clause easier to read, it may not yet be supported in your specific SQL implementation. 
 (Note that the SQL2 specification uses the term 
 correlation name
  to refer to what we have 
 called a 
 table alias
 . The function and meaning of a correlation name are exactly as 
 described here; many SQL products use the term 
 alias
 , and it is more descriptive of the 
 function that a table alias performs. The SQL2 standard specifies a similar technique for 
 designating alternate 
 column
  names, and in that situation the 
 column alias
  name is actually 
 called an 
 alias
  in the standard.)
  
 - 116 -",NA
Multi-Table Query Performance,"As the number of tables in a query grows, the amount of effort required to carry it out 
 increases rapidly. The SQL language itself places no limit on the number of tables joined 
 by a query. Some SQL products do limit the number of tables, with a limit of about eight 
 tables being fairly common. The high processing cost of queries that join many tables 
 imposes an even lower practical limit in many applications.
  
 In online transaction processing (OLTP) applications, it's common for a query to involve 
 only one or two tables. In these applications, response time is critical—the user typically 
 enters one or two items of data and needs a response from the database within a second 
 or two. Here are some typical OLTP queries for the sample database:
  
 •The user enters a customer number into a form, and the DBMS retrieves the 
 customer's credit limit, account balance, and other data (a single-table query).
  
 •A cash register scans a product number from a package and retrieves the product's 
 name and price from the database (a single-table query).
  
 •The user enters a salesperson's name, and the program lists the current orders for 
 that salesperson (a two-table inquiry).
  
 In decision-support applications, by contrast, it's common for a query to involve many 
 different tables and exercise complex relationships in the database. In these applications, 
 the query results are often used to help make expensive decisions, so a query that 
 requires several minutes or even several hours to complete is perfectly acceptable. Here 
 are some typical decision-support queries for the sample database:
  
 •The user enters an office name, and the program lists the 25 largest orders taken by 
 salespeople in that office (a three-table query).
  
 •A report summarizes sales by product type for each salesperson, showing which 
 salespeople are selling which products (a three-table query).
  
 •A manager considers opening a new Seattle sales office and runs a query analyzing the 
 impact on orders, products, customers, and the salespeople who call on them (a four-table 
 query).",NA
The Structure of a Join,"For simple joins, it's fairly easy to write the correct 
 SELECT
  statement based on an 
 English-language request or to look at a 
 SELECT
  statement and figure out what it does. 
 When many tables are joined or when the search conditions become complex, however, it 
 becomes very difficult just to look at a 
 SELECT
  statement and figure out what it means. 
 For this reason, it's important to define more carefully and just a bit more formally what a 
 join is, what query results are produced by a given 
 SELECT
  statement, and just a little bit 
 of the theory of relational database operation that underlies joins.",NA
Table Multiplication,"A join is a special case of a more general combination of data from two tables, known as 
 the 
 Cartesian product
  (or just the 
 product
 ) of two tables. The product of two tables is 
 another table (the 
 product table
 ), which consists of all possible pairs of rows from the two 
 tables. The columns of the product table are all the columns of the first table, followed by 
 all the columns of the second table. Figure 7-11 shows two small sample tables and their 
 product.
  
 - 117 -",NA
Rules for Multi-Table Query Processing,"The steps following the code below restate the rules for SQL query processing originally 
 introduced in Figure 6-14 and expands them to include multi-table queries. The rules 
 define the meaning of any multi-table 
 SELECT
  statement by specifying a procedure that 
 always generates the correct set of query results. To see how the procedure works, 
 consider this query:
  
 List the company name and all orders for customer number 2103.
  
 SELECT COMPANY, ORDER_NUM, AMOUNT
  
  
  FROM CUSTOMERS, ORDERS 
  
 WHERE CUST_NUM = CUST
  
   
  AND CUST_NUM = 2103 
  
 ORDER BY ORDER_NUM
  
 - 118 -",NA
Outer Joins *,"The SQL join operation combines information from two tables by forming 
 pairs
  of related 
 rows from the two tables. The row pairs that make up the joined table are those where 
 the matching columns in each of the two tables have the same value. If one of the rows 
 of a table is unmatched in this process, the join can produce unexpected results, as 
 illustrated by these queries:
  
 List the salespeople and the offices where they work.
  
 SELECT NAME, REP_OFFICE
  
  
  FROM SALESREPS
  
 NAME            
 REP_OFFICE
  
 --------------  ---------
 -
  
 Bill 
 Adams              
 13 
  
 Mary 
 Jones              
 11 
  
 Sue 
 Smith               
 21 
  
 Sam 
 Clark               
 11 
  
 Bob 
 Smith               
 12 
  
 Dan 
 Roberts             
 12 
  
 Tom 
 Snyder            
 NULL 
  
 Larry 
 Fitch             
 21 
  
 Paul 
 Cruz               
 12 
  
 Nancy 
 Angelli           
 22
  
 List the salespeople and the cities where they work.
  
 SELECT NAME, CITY
  
  
  FROM SALESREPS, OFFICES 
  
 WHERE REP_OFFICE = OFFICE
  
 NAME           CITY
  
 -------------  --------
  
 Mary Jones     New York 
  
 Sam Clark      New York 
  
 Bob Smith      Chicago 
  
 Paul Cruz      Chicago 
  
 Dan Roberts    Chicago 
  
 Bill Adams     Atlanta 
  
 Sue Smith      Los Angeles 
  
 Larry Fitch    Los Angeles 
  
 Nancy Angelli  Denver
  
 Based on the English-language descriptions of these two queries, you would probably 
 expect them to produce the same number of rows. But the first query includes a row for 
 each of the ten salespeople, while the second query produces only nine. Why? Because 
 Tom Snyder is currently unassigned and has a 
 NULL
  value in the 
 REP_OFFICE
  column 
 (which is the matching column for the join). This 
 NULL
  value doesn't match any of the",NA
Left and Right Outer Joins *,"Technically, the outer join produced by the previous query is called the 
 full outer join
  of 
 the two tables. Both tables are treated symmetrically in the full outer join. Two other well-
 defined outer joins do not treat the two tables symmetrically.
  
 The 
 left outer join
  between two tables is produced by following Step 1 and Step 2 in the 
 previous numbered list but omitting Step 3. The left outer join thus includes 
 NULL
 -
 extended copies of the unmatched rows from the first (left) table but does not include any 
 unmatched rows from the second (right) table. Here is a left outer join between the 
 GIRLS
  
 and 
 BOYS
  tables:
  
 List girls and boys in the same city and any unmatched girls.
  
 SELECT *
  
  
  FROM GIRLS, BOYS 
  
 WHERE GIRLS.CITY *= BOYS.CITY
  
 GIRLS.NAME  GIRLS.CITY  BOYS.NAME  BOYS.CITY-
 ---------  ----------  ---------  ---------
 Mary        Boston      John       Boston 
 Mary        Boston      Henry      Boston 
 Susan       Chicago     Sam        Chicago 
 Betty       Chicago     Sam        Chicago 
 Anne        Denver      NULL       NULL 
  
 Nancy       NULL        NULL       NULL
  
 The query produces six rows of query results, showing the matched girl/boy pairs and the 
 unmatched girls. The unmatched boys are missing from the results.
  
 Similarly, the 
 right outer join
  between two tables is produced by following Step 1 and Step 
 3 in the previous numbered list but omitting Step 2. The right outer join thus includes 
 NULL
 -extended copies of the unmatched rows from the second (right) table but does not 
 include the unmatched rows of the first (left) table. Here is a right outer join between the 
 GIRLS
  and 
 BOYS
  tables:
  
 List girls and boys in the same city and any unmatched boys.
  
 SELECT *
  
 - 123 -",NA
Outer Join Notation *,"Because the outer join was not part of the SQL1 standard and was not implemented in 
 early IBM SQL products, the DBMS vendors who support the outer join have used various 
 notations in their SQL dialects. The 
 ""*=*
 "" notation used in the earlier examples of this 
 section is used by SQL Server. This notation indicates an outer join by appending an 
 asterisk (*) to the comparison test in the 
 WHERE
  clause that defines the join condition. To 
 indicate the full outer join between two tables, 
 TBL1
  and 
 TBL2
 , on columns 
 COL1
  and 
 COL2
 , an asterisk (*) is placed before and after the standard join operator. The resulting 
 full outer join comparison test looks like this:
  
 WHERE COL1 *=* COL2
  
 To indicate a left outer join of 
 TBL1
  to 
 TBL2
 , only the leading asterisk is specified, giving 
 a comparison test like this:
  
 WHERE COL1 *= COL2
  
 To indicate a right outer join of 
 TBL1
  to 
 TBL2
 , only the trailing asterisk is specified, giving 
 a comparison test like this:
  
 WHERE COL1 =* COL2
  
 An outer join can be used with any of the comparison operators using the same notation. 
 For example, a left outer join of 
 TBL1
  to 
 TBL2 
 using a greater than or equal (>=) 
 comparison would produce a comparison test like this:
  
 WHERE COL1 *>= COL2
  
 Oracle also supports the outer join operation but uses a different notation. This notation 
 indicates the outer join in the 
 WHERE
  clause by including a parenthesized plus sign 
 following the column 
 whose table is to have the imaginary
 NULL
 row added
  (that is, the 
 minor table in the outer join)
 .
  The left outer join of 
 TBL1
  to 
 TBL2
  produces a search 
 condition that looks like this:
  
 WHERE COL1 = COL2 (+)
  
 and the right outer join of 
 TBL1
  to 
 TBL2
  produces a search condition that looks like this:
  
 WHERE COL1 (+) = COL2
  
 Note that the plus sign appears on the 
 opposite
  side of the comparison from where the 
 asterisk appears in the SQL Server notation. Oracle does not support a full outer join, but 
 as indicated earlier, this does not diminish the practical usefulness of the Oracle outer join 
 capability.
  
 Although both of these outer join notations are relatively convenient, they're also 
 somewhat deceiving. Recall that the rules for multi-table SQL query processing begin by 
 examining the 
 FROM
  clause of a query and conceptually building the product of the two 
 (or more) tables. Only after the product table is constructed does the DBMS start 
 eliminating rows that do not meet the 
 WHERE
  clause search condition. But with the SQL 
  
 - 126 -",NA
Joins and the SQL2 Standard,"Outer joins posed a problem for the writers of the SQL2 standard. Because outer joins 
 are the only way to represent some extremely useful queries, it was important that the 
 SQL2 standard include support for outer joins. In addition, outer joins were supported in 
 many commercial SQL products and were becoming a more important part of the SQL 
 language. However, the methods used to represent outer joins varied widely among the 
 different SQL products, as shown in the preceding sections. Furthermore, the methods 
 used to denote outer joins in commercial products all had deficiencies and had been 
 chosen more because of their minor impact on the SQL language than because of their 
 clarity or correctness.
  
 Against this background, the SQL2 standard specified a brand new method for 
  
 supporting outer joins, which was not based on the established notation of a popular SQL 
 product. The SQL2 specification puts the support for outer joins into the 
 FROM
  clause, with 
 an elaborate syntax that allows the user to specify exactly how the source tables for a 
 query are to be joined together. The outer join support in the SQL2 standard has two 
 distinct advantages. First, the SQL2 standard can express even the most complex of joins. 
 Second, existing database products can support the SQL2 extensions to SQL1 and retain 
 support for their own proprietary outer join syntax without conflict. IBM's DB2 relational 
 database, for example, has added support for most, but not all, of the new SQL2 join 
 syntax at this writing. It's reasonable to expect that most of the major DBMS brands will 
 follow, and that the SQL2-style join features will become a part of the SQL mainstream 
 over the next several years.
  
 The advantages of the SQL2 expanded join support come at the expense of some 
 significant added complexity for what had previously been one of the simpler parts of the 
 SQL language. In fact, the expanded join support is part of a much larger expansion of 
 query capabilities in SQL2 which add even more capability and complexity. The other 
 expanded features include set operations on query results (union, intersection, and 
 differences of tables) and much richer query expressions that manipulate rows and tables 
 and allow them to be used in subqueries. The expanded join-related capabilities are 
 described in this section. The other expanded capabilities are described in the 
 next 
  
 - 127 -",NA
Inner Joins in SQL2 *,"Figure 7-13 shows a simplified form of the extended SQL2 syntax for the 
 FROM
  clause. It's 
 easiest to understand all of the options provided by considering each type of join, one by 
 one, starting with the basic inner join and then moving to the various forms of outer join. 
 The standard inner join of the 
 GIRLS
  and 
 BOYS
  tables can be expressed in SQL1 
 language:
  
  
 Figure 7-13: 
 Extended 
 FROM
  clause in the SQL2 standard
  
 SELECT *
  
  
  FROM GIRLS, BOYS 
  
 WHERE GIRLS.CITY = BOYS.CITY
  
 This is still an acceptable statement in SQL2. The writers of the SQL2 standard really 
 couldn't have made it illegal without ""breaking"" all of the millions of multi-table SQL 
 queries that had already been written by the early 1990s. But the SQL2 standard 
 specifies an alternative way of expressing the same query:
  
 SELECT *
  
  
  FROM GIRLS INNER JOIN BOYS
  
   
  ON GIRLS.CITY = BOYS.CITY
  
 Note that the two tables to be joined are explicitly connected by a 
 JOIN
  operation, and 
  
 - 128 -",NA
Outer Joins in SQL2 *,"The SQL2 standard provides complete support for outer joins using the same clauses 
 described in the preceding section for inner joins and additional keywords. For example, 
 the full outer join of the 
 GIRLS
  and 
 BOYS
  tables (without the 
 AGE
  columns) is generated 
 by this query:
  
 SELECT *
  
  FROM GIRLS FULL OUTER JOIN BOYS
  
  ON GIRLS.CITY = BOYS.CITY
  
 As explained earlier in this chapter, the query results will contain a row for each matched 
 girl/boy pair, as well as one row for each unmatched boy, extended with 
 NULL
  values in 
 the columns from the other, unmatched table. SQL2 allows the same variations for outer 
 joins as for inner joins; the query could also have been written:
  
 SELECT *
  
  FROM GIRLS NATURAL FULL OUTER JOIN BOYS
  
 or
  
 SELECT *
  
  FROM GIRLS FULL OUTER JOIN BOYS
  
 USING (CITY)
  
 Just as the keyword 
 INNER
  is optional in the SQL2 language, the SQL2 standard also 
 allows you to omit the keyword 
 OUTER
 . The preceding query could also have been 
 written:
  
 SELECT *
  
  FROM GIRLS FULL JOIN BOYS
  
 USING (CITY)
  
 The DBMS can infer from the word 
 FULL
  that an outer join is required.
  
 By specifying 
 LEFT
  or 
 RIGHT
  instead of 
 FULL
 , the SQL2 language extends quite 
 naturally to left or right outer joins. Here is the left outer join version of the same query:
  
 SELECT *
  
  FROM GIRLS LEFT OUTER JOIN BOYS
  
 USING (CITY)
  
 As described earlier in the chapter, the query results will include matched girl/boy pairs 
 and 
 NULL
 -extended rows for each unmatched row in the 
 GIRLS
  table (the ""left"" table in 
 the join), but the results do not include unmatched rows from the 
 BOYS
  table. Conversely, 
 the right outer join version of the same query, specified like this:
  
 SELECT *
  
  FROM GIRLS RIGHT OUTER JOIN BOYS
  
 USING (CITY)
  
 - 130 -",NA
Cross Joins and Union Joins in SQL2 *,"The SQL2 support for extended joins includes two other methods for combining data from 
 two tables. A 
 cross join
  is another name for the Cartesian product of two tables, as 
 described earlier in this chapter. A 
 union join
  is closely related to the full outer join; its 
 query results are a subset of those generated by the full outer join.
  
 Here is a SQL2 query that generates the complete product of the 
 GIRLS
  and 
 BOYS 
 tables:
  
 SELECT *
  
  FROM GIRLS CROSS JOIN BOYS
  
 By definition, the Cartesian product (also sometimes called the 
 cross product
 , hence the 
 name 
 ""CROSS JOIN
 "") contains every possible pair of rows from the two tables. It 
 ""multiplies"" the two tables, turning tables of, for example, three girls and two boys into a 
 table of six (3 x 2 = 6) boy/girl pairs. No ""matching columns"" or ""selection criteria"" are 
 associated with the cross products, so the 
 ON
  clause and the 
 USING
  clause are not 
 allowed. Note that the cross join really doesn't add any new capabilities to the SQL 
 language. Exactly the same query results can be generated with an inner join that 
 specifies no matching columns. So the preceding query could just as well have been 
 written as: 
  
 SELECT *
  
  FROM GIRLS, BOYS
  
 The use of the keywords 
 CROSS JOIN
  in the 
 FROM
  clause simply makes the cross join 
 more explicit. In most databases, the cross join of two tables by itself is of very little 
 practical use. Its usefulness really comes as a building block for more complex query 
 expressions that start with the cross product of two tables and then use SQL2 summary 
 query capabilities (described in the 
 next chapter
 ) or SQL2 set operations to further 
 manipulate the results.
  
 The union join in SQL2 combines some of the features of the 
 UNION
  operation (described 
 in the previous chapter) with some of the features of the join operations described in this 
 chapter. Recall that the 
 UNION
  operation effectively combines the rows of two tables, 
 which must have the same number of columns and the same data types for each 
 corresponding column. This query, which uses a simple 
 UNION
  operation:
  
 SELECT *
  
  FROM GIRLS
  
 UNION ALL
  
 SELECT *
  
  FROM BOYS
  
 when applied to a three-row table of girls and a two-row table of boys yields a five-row 
 table of query results. Each row of query results corresponds precisely to either a row of 
 the 
 GIRLS
  table or a row of the 
 BOYS
  table from which it was derived. The query results 
 have two columns, 
 NAME
  and 
 CITY
 , because the 
 GIRLS
  and 
 BOYS
  tables each have 
 these two columns.
  
 The union join of the 
 GIRLS
  and 
 BOYS
  tables is specified by this SQL2 query: 
  
 - 131 -",NA
Multi-Table Joins in SQL2,"An important advantage of the SQL2 notation is that it allows very clear specification of 
 three-table or four-table joins. To build these more complex joins, any of the join 
 expressions shown in Figure 7-13 and described in the preceding sections can be 
 enclosed in parentheses. The resulting join expression can itself be used in another join 
 expression, as if it were a simple table. Just as SQL allows you to combine mathematical 
 operations (+, 
 −
 , *, and /) with parentheses and build more complex expressions, the 
 SQL2 standard allows you to build more complex join expressions in the same way.
  
 To illustrate multi-table joins, assume that a new 
 PARENTS
  table has been added to the 
 database containing the 
 GIRLS
  and 
 BOYS
  example we have been using. The 
 PARENTS 
 table has three columns:
  
 CHILD 
  
 Matches the 
 NAME
  column in 
 GIRLS
  or 
 BOYS
  table
  
 TYPE 
  
 Specifies 
 FATHER
  or 
 MOTHER
  
 PNAME 
  
 First name of the parent
  
 A row in the 
 GIRLS
  or 
 BOYS
  table can have two matching rows in the 
 PARENTS
  table, 
 one specifying a 
 MOTHER
  and one a 
 FATHER
 , or it can have only one of these rows, or it 
 can have no matching rows if no data on the child's parents is available. The 
 GIRLS
 , 
 BOYS
 , and 
 PARENTS
  tables together provide a rich set of data for some multi-table join 
 examples.
  
 For example, suppose you wanted to make a list of all of the girls, along with the names 
 of their mothers and the names of the boys who live in the same city. Here is one query 
 that produces the list:
  
 SELECT GIRLS.NAME, PNAME, BOYS.NAME
  
  FROM ((GIRLS JOIN PARENTS 
  
  ON PARENT.CHILD = NAME)
  
  JOIN (BOYS
  
 - 133 -",NA
Summary,"This chapter described how SQL handles queries that combine data from two or more 
 tables:
  
 •In a multi-table query (a 
 join
 ), the tables containing the data are named in the 
 FROM 
 clause.
  
 •Each row of query results is a combination of data from a single row in each of the 
 tables, and it is the 
 only
  row that draws its data from that particular combination.
  
 •The most common multi-table queries use the parent/child relationships created by 
 primary keys and foreign keys.
  
 •In general, joins can be built by comparing 
 any
  pair(s) of columns from the two joined 
 tables, using either a test for equality or any other comparison test.
  
 •A join can be thought of as the product of two tables from which some of the rows 
 have been removed.
  
 •A table can be joined to itself; self-joins require the use of a table alias.
  
 •Outer joins extend the standard (inner) join by retaining unmatched rows of one or both 
 of the joined tables in the query results, and using 
 NULL
  values for data from the other 
 table.
  
 •The SQL2 standard provides comprehensive support for inner and outer joins, and for 
 combining the results of joins with other multi-table operations such as unions, 
 intersections, and differences.
  
 - 135 -",NA
Chapter 8: ,NA,NA
Summary Queries,NA,NA
Overview,"Many requests for information don't require the level of detail provided by the SQL 
 queries described in the last two chapters. For example, each of the following requests 
 asks for a single value or a small number of values that summarize the contents of the 
 database:
  
 •What is the total quota for all salespeople?
  
 •What are the smallest and largest assigned quotas?
  
 •How many salespeople have exceeded their quota?
  
 •What is the size of the average order?
  
 •What is the size of the average order for each sales office?
  
 •How many salespeople are assigned to each sales office?
  
 SQL supports these requests for summary data through column functions and the 
 GROUP 
 BY
  and 
 HAVING
  clauses of the 
 SELECT
  statement, which are described in this chapter.",NA
Column Functions,"SQL lets you summarize data from the database through a set of 
 column functions.
  A 
 SQL column function takes an entire column of data as its argument and produces a 
 single data item that summarizes the column. For example, the 
 AVG()
  column function 
 takes a column of data and computes its average. Here is a query that uses the 
 AVG() 
 column function to compute the average value of two columns from the 
 SALESREPS 
 table:
  
 What are the average quota and average sales of our salespeople?
  
 SELECT AVG(QUOTA), AVG(SALES)
  
  FROM SALESREPS
  
  AVG(QUOTA)   AVG(SALES)
  
 ------------  -----------
  
 $300,000.00  $289,353.20
  
 Figure 8-1 graphically shows how the query results are produced. The first column 
 function in the query takes values in the 
 QUOTA
  column and computes their average; the 
 second one averages the values in the 
 SALES
  column. The query produces a single row 
 of query results summarizing the data in the 
 SALESREPS
  table.
  
 - 136 -",NA
Computing a Column Total (,NA,NA
SUM,NA,NA
),"The 
 SUM()
  column function computes the sum of a column of data values. The data in the 
 column must have a numeric type (integer, decimal, floating point, or money). The result of 
 the 
 SUM()
  function has the same basic data type as the data in the column, but the result 
 may have a higher precision. For example, if you apply the 
 SUM()
  function to a column of 
 16-bit integers, it may produce a 32-bit integer as its result.
  
 Here are some examples that use the 
 SUM()
  column function:
  
 What are the total quotas and sales for all salespeople?
  
 SELECT SUM(QUOTA), SUM(SALES)
  
  
  FROM SALESREPS
  
   
  SUM(QUOTA)     SUM(SALES)
  
 --------------  -------------
  
 $2,700,000.00  $2,893,532.00
  
 What is the total of the orders taken by Bill Adams?
  
 SELECT SUM(AMOUNT)
  
  
  FROM ORDERS, SALESREPS 
  
 WHERE NAME = 'Bill Adams'
  
   
  AND REP = EMPL_NUM
  
 SUM(AMOUNT)
  
 ------------
  
  
  $39,327.00",NA
Computing a Column Average (,NA,NA
AVG,NA,NA
),"The 
 AVG()
  column function computes the average of a column of data values. As with the 
 SUM()
  function, the data in the column must have a numeric type. Because the 
 AVG()
  
 function adds the values in the column and then divides by the number of values, its result 
 may have a different data type than that of the values in the column. For example, if you 
 apply the 
 AVG()
  function to a column of integers, the result will be either a decimal or a 
 floating point number, depending on the brand of DBMS you are using.
  
 Here are some examples of the 
 AVG()
  column function:
  
 Calculate the average price of products from manufacturer ACI.
  
 SELECT AVG(PRICE)
  
 - 138 -",NA
Finding Extreme Values (,NA,NA
MIN,NA,NA
 and ,NA,NA
MAX,NA,NA
),"The 
 MIN()
  and 
 MAX()
  column functions find the smallest and largest values in a column, 
 respectively. The data in the column can contain numeric, string, or date/time information. 
 The result of the 
 MIN()
  or 
 MAX()
  function has exactly the same data type as the data in 
 the column.
  
 Here are some examples that show the use of these column functions:
  
 What are the smallest and largest assigned quotas?
  
 SELECT MIN(QUOTA), MAX(QUOTA)
  
  
  FROM SALESREPS
  
  MIN(QUOTA)   
 MAX(QUOTA)
  
 ------------  ----------
 -
  
 $200,000.00  $350,000.00
  
 What is the earliest order date in the database?
  
 SELECT MIN(ORDER_DATE)
  
  
  FROM ORDERS
  
 MIN(ORDER_DATE
 )
  
 --------------
 -
  
 04-JAN-89
  
 What is the best sales performance of any salesperson?
  
 SELECT MAX(100 * (SALES/QUOTA))
  
  
  FROM SALESREPS 
  
 MAX(100*(SALES/QUOTA))
  
 -----------------------
  
 135.44
  
 When the 
 MIN()
  and 
 MAX()
  column functions are applied to numeric data, SQL 
  
 - 139 -",NA
Counting Data Values (,NA,NA
COUNT,NA,NA
),"The 
 COUNT()
  column function counts the number of data values in a column. The data in 
 the column can be of any type. The 
 COUNT()
  function always returns an integer, 
 regardless of the data type of the column. Here are some examples of queries that use the 
 COUNT()
  column function:
  
 How many customers are there?
  
 SELECT COUNT(CUST_NUM)
  
  FROM CUSTOMERS
  
 - 140 -",NA
Column Functions in the Select List,"Simple queries with a column function in their select list are fairly easy to understand. 
 However, when the select list includes several column functions, or when the argument to 
 a column function is a complex expression, the query can be harder to read and 
  
 understand. The following steps show the rules for SQL query processing expanded once 
 more to describe how column functions are handled. As before, the rules are intended to 
 provide a precise definition of what a query means, not a description of how the DBMS 
 actually goes about producing the query results.
  
 To generate the query results for a 
 SELECT
  statement:
  
 1.If the statement is a 
 UNION
  of 
 SELECT
  statements, apply Steps 2 through 5 to each of 
 the statements to generate their individual query results.
  
 2.Form the product of the tables named in the 
 FROM
  clause. If the 
 FROM
  clause names a 
 single table, the product is that table.
  
 3. If there is a 
 WHERE
  clause, apply its search condition to each row of the product table, 
 retaining those rows for which the search condition is 
 TRUE
  (and discarding those for 
 which it is 
 FALSE
  or 
 NULL
 ).
  
 4. For each remaining row, calculate the value of each item in the select list to produce a 
 single row of query results. For a simple column reference, use the value of the column 
 in the current row. For a column function, use the entire set of rows as its argument.
  
 5.If 
 SELECT DISTINCT
  is specified, eliminate any duplicate rows of query results that 
 were produced.
  
 6.
  
 If the statement is a 
 UNION
  of 
 SELECT
  statements, merge the query results for the 
 individual statements into a single table of query results. Eliminate duplicate rows 
  
 unless 
 UNION ALL
  is specified.
  
 7.If there is an 
 ORDER BY
  clause, sort the query results as specified.
  
 The rows generated by this procedure comprise the query results.
  
 One of the best ways to think about summary queries and column functions is to imagine 
 the query processing broken down into two steps. First, you should imagine how the query 
 would work 
 without
  the column functions, producing many rows of detailed query results. 
 Then you should imagine SQL applying the column functions to the detailed query results, 
 producing a single summary row. For example, consider the following complex query:
  
 Find the average order amount, total order amount, average order amount as a 
 percentage of the customer's credit limit, and average order amount as a percentage of 
 the salesperson's quota.
  
 SELECT AVG(AMOUNT), SUM(AMOUNT), (100 * 
 AVG(AMOUNT/CREDIT_LIMIT)),
  
  
  (100 * AVG(AMOUNT/QUOTA))
  
  FROM ORDERS, CUSTOMERS, SALESREPS
  
 WHERE CUST = CUST_NUM
  
  AND REP = EMPL_NUM
  
 - 142 -",NA
NULL,NA,NA
 Values and Column Functions,"The 
 SUM()
 , 
 AVG()
 , 
 MIN()
 , 
 MAX()
 , and 
 COUNT()
  column functions each take a column 
 of data values as their argument and produce a single data value as a result. What 
 happens if one or more of the data values in the column is a 
 NULL
  value? The ANSI/ISO 
 SQL standard specifies that 
 NULL
  values in the column are 
 ignored
  by the column 
 functions.
  
 This query shows how the 
 COUNT()
  column function ignores any 
 NULL
  values in a 
 column:
  
 SELECT COUNT(*), COUNT(SALES), COUNT(QUOTA)
  
  FROM SALESREPS
  
 COUNT(*)  COUNT(SALES)  COUNT(QUOTA)
  
 ---------  ------------  ------------
  
 - 143 -",NA
Duplicate Row Elimination (,NA,NA
DISTINCT,NA,NA
),"Recall from 
 Chapter 6
  that you can specify the 
 DISTINCT
  keyword at the beginning of the 
 select list to eliminate duplicate rows of query results. You can also ask SQL to eliminate 
 duplicate values from a column before applying a column function to it. To eliminate 
 duplicate values, the keyword 
 DISTINCT
  is included before the column function 
 argument, immediately after the opening parenthesis.
  
 Here are two queries that illustrate duplicate row elimination for column functions:
  
 How many different titles are held by salespeople?
  
 SELECT COUNT(DISTINCT TITLE)
  
  
  FROM SALESREPS
  
 COUNT(DISTINCT TITLE)
  
 ----------------------
  
  
  3
  
 How many sales offices have salespeople who are over quota?
  
 SELECT COUNT(DISTINCT REP_OFFICE)
  
  
  FROM SALESREPS 
  
 WHERE SALES > QUOTA
  
 COUNT(DISTINCT REP_OFFICE)
  
 ---------------------------
  
  
  4
  
 The SQL1 standard specified that when the 
 DISTINCT
  keyword is used, the argument to 
 the column function must be a simple column name; it cannot be an expression. The 
 standard allows the 
 DISTINCT
  keyword for the 
 SUM()
  and 
 AVG()
  column functions. The 
 standard does not permit use of the 
 DISTINCT
  keyword with the 
 MIN()
  and 
 MAX() 
 column functions because it has no impact on their results, but many SQL 
  
 - 145 -",NA
Grouped Queries (,NA,NA
GROUPBY,NA,NA
 Clause),"The summary queries described thus far are like the totals at the bottom of a report. They 
 condense all of the detailed data in the report into a single, summary row of data. Just as 
 subtotals are useful in printed reports, it's often convenient to summarize query results at 
 a ""subtotal"" level. The 
 GROUP BY
  clause of the 
 SELECT
  statement provides this 
  
 capability.
  
 The function of the 
 GROUP BY
  clause is most easily understood by example. Consider 
 these two queries:
  
 What is the average order size?
  
 SELECT AVG(AMOUNT)
  
  
  FROM ORDERS
  
 AVG(AMOUNT)
  
 ------------
  
  
  $8,256.37
  
 What is the average order size for each salesperson?
  
 SELECT REP, AVG(AMOUNT)
  
  
  FROM ORDERS 
  
 GROUP BY REP
  
 REP  AVG(AMOUNT)
  
 ----  -----------
  
 101    $8,876.00 
  
 102    $5,694.00 
  
 103    $1,350.00 
  
 105    $7,865.40 
  
 106   $16,479.00 
  
 107   $11,477.33 
  
 108    $8,376.14 
  
 109    $3,552.50 
  
 110   $11,566.00
  
 The first query is a simple summary query like the previous examples in this chapter. The 
 second query produces several summary rows—one row for each group, summarizing the 
 orders taken by a single salesperson. Figure 8-3 shows how the second query works. 
  
 - 146 -",NA
Multiple Grouping Columns,"SQL can group query results based on the contents of two or more columns. For 
 example, suppose you want to group the orders by salesperson and by customer. This 
 query groups the data based on both criteria:
  
 Calculate the total orders for each customer of each salesperson.
  
 SELECT REP, CUST, SUM(AMOUNT)
  
  
  FROM ORDERS 
  
 GROUP BY REP, CUST
  
 REP  CUST  SUM(AMOUNT)
  
 ----  ----  -----------
  
 101  2102    $3,978.00 
  
 101  2108      $150.00 
  
 101  2113   $22,500.00 
  
 102  2106    $4,026.00 
  
 102  2114   $15,000.00 
  
 102  2120    $3,750.00 
  
 103  2111    $2,700.00 
  
 105  2103   $35,582.00 
  
 105  2111    $3,745.00
  
  
  .
  
  .
  
  .
  
 Even with multiple grouping columns, SQL provides only a single level of grouping. The 
 query produces a separate summary row for each salesperson/customer pair. It's 
 impossible to create groups and subgroups with two levels of subtotals in SQL. The best 
 you can do is sort the data so that the rows of query results appear in the appropriate 
 order. In many SQL implementations, the 
 GROUP BY
  clause will automatically have the 
 side effect of sorting the data, but you can override this sort with an 
 ORDER BY
  clause, as 
 shown here:
  
 Calculate the total orders for each customer of each salesperson, sorted by customer, 
 and within each customer by salesperson.
  
 SELECT CUST, REP, SUM(AMOUNT)
  
  
  FROM ORDERS 
  
 GROUP BY CUST, REP 
  
 ORDER BY CUST, REP
  
 - 149 -",NA
Restrictions on Grouped Queries,"Grouped queries are subject to some rather strict limitations. The grouping columns must 
  
 - 151 -",NA
NULL,NA,NA
 Values in Grouping Columns,"A 
 NULL
  value poses a special problem when it occurs in a grouping column. If the value of 
 the column is unknown, which group should the row be placed into? In the 
 WHERE 
 clause, 
 when two different 
 NULL
  values are compared, the result is 
 NULL
  (not 
 TRUE
 ), that is, the 
 two 
 NULL
  values are 
 not
  considered to be equal. Applying the same convention to the 
 GROUP BY
  clause would force SQL to place each row with a 
 NULL
  grouping column into a 
 separate group by itself.
  
 In practice this rule proves too unwieldy. Instead, the ANSI/ISO SQL standard considers 
 two 
 NULL
  values to be equal for purposes of the 
 GROUP BY
  clause. If two rows have 
 NULLs
  in the same grouping columns and identical values in all of their non-
 NULL 
 grouping columns, they are grouped together into the same row group. The small sample 
 table in Figure 8-4 illustrates the ANSI/ISO handling of 
 NULL
  values by the 
 GROUP BY 
 clause, as shown in this query:",NA
Group Search Conditions (,NA,NA
HAVING,NA,NA
 Clause),"Just as the 
 WHERE
  clause can be used to select and reject the individual rows that 
 participate in a query, the 
 HAVING
  clause can be used to select and reject row groups. 
 The format of the 
 HAVING
  clause parallels that of the 
 WHERE
  clause, consisting of the 
 keyword 
 HAVING
  followed by a search condition. The 
 HAVING
  clause thus specifies a 
 search condition for groups.
  
 An example provides the best way to understand the role of the 
 HAVING
  clause. 
 Consider this query:
  
 What is the average order size for each salesperson whose orders total more than 
 $30,000?
  
 SELECT REP, AVG(AMOUNT)
  
  
  FROM ORDERS 
  
 GROUP BY REP 
  
 HAVING SUM(AMOUNT) > 30000.00",NA
Restrictions on Group Search Conditions,"The 
 HAVING
  clause is used to include or exclude row groups from the query results, so 
 the search condition it specifies must be one that applies to the group as a whole rather 
 than to individual rows. This means that an item appearing within the search condition in 
 a 
 HAVING
  clause can be:
  
 •a constant,
  
 •a column function, which produces a single value summarizing the rows in the group,
  
 •a grouping column, which by definition has the same value in every row of the group, or
  
 •an expression involving combinations of the above.
  
 In practice, the search condition in the 
 HAVING
  clause will always include at least one 
 column function. If it did not, the search condition could be moved to the 
 WHERE
  clause 
 and applied to individual rows. The easiest way to figure out whether a search condition 
 belongs in the 
 WHERE
  clause or in the 
 HAVING
  clause is to remember how the two 
 clauses are applied:
  
 - 157 -",NA
NULL,NA,NA
 Values and Group Search Conditions,"Like the search condition in the 
 WHERE
  clause, the 
 HAVING
  clause search condition can 
 produce one of three results:
  
 •If the search condition is 
 TRUE
 , the row group is retained, and it contributes a 
 summary row to the query results.
  
 •If the search condition is 
 FALSE
 , the row group is discarded, and it does not contribute a 
 summary row to the query results.
  
 •If the search condition is 
 NULL
 , the row group is discarded, and it does not contribute a 
 summary row to the query results.
  
 The anomalies that can occur with 
 NULL
  values in the search condition are the same as 
 those for the 
 WHERE
  clause and have been described in 
 Chapter 6
 .",NA
HAVING Without GROUP BY,"The 
 HAVING
  clause is almost always used in conjunction with the 
 GROUP BY
  clause, but 
 the syntax of the 
 SELECT
  statement does not require it. If a 
 HAVING
  clause appears without 
 a 
 GROUP BY
  clause, SQL considers the entire set of detailed query results to be a single 
 group. In other words, the column functions in the 
 HAVING
  clause are applied to one and 
 only one group to determine whether the group is included or excluded from the query 
 results, and that group consists of all the rows. The use of a 
 HAVING
  clause without a 
 corresponding 
 GROUP BY
  clause is seldom seen in practice.",NA
Summary,"This chapter described summary queries, which summarize data from the database:
  
 •Summary queries use SQL column functions to collapse a column of data values into a 
 single value that summarizes the column.
  
 •Column functions can compute the average, sum, minimum, and maximum values of a 
 column, count the number of data values in a column, or count the number of rows of 
 query results.
  
 •A summary query without a 
 GROUP BY
  clause generates a single row of query results, 
 summarizing all the rows of a table or a joined set of tables.
  
 •A summary query with a 
 GROUP BY
  clause generates multiple rows of query results, 
 each summarizing the rows in a particular group.
  
 •The 
 HAVING
  clause acts as a 
 WHERE
  clause for groups, selecting the row groups that 
 contribute to the summary query results.",NA
Chapter 9: ,NA,NA
Subqueries and Query Expressions,- 158 -,NA
Overview,"The SQL subquery feature lets you use the results of one query as part of another query. 
 The ability to use a query within a query was the original reason for the word ""structured"" 
 in the name Structured Query Language. The subquery feature is less well known than 
 SQL's join feature, but it plays an important role in SQL for three reasons:
  
 •A SQL statement with a subquery is often the most natural way to express a query, 
 because it most closely parallels the English-language description of the query.
  
 •
  
 Subqueries make it easier to write 
 SELECT
  statements, because they let you ""break a 
 query down into pieces"" (the query and its subqueries) and then ""put the pieces 
  
 together.""
  
 •Some queries cannot be expressed in the SQL language without using a subquery.
  
 The first several sections of this chapter describe subqueries and show how they are used 
 in the 
 WHERE
  and 
 HAVING
  clauses of a SQL statement. The later sections of this chapter 
 describe the advanced query expression capabilities that have been added to the SQL2 
 standard, which substantially expands the power of SQL to perform even the most complex 
 of database operations.",NA
Using Subqueries,"A 
 subquery
  is a query-within-a-query. The results of the subquery are used by the DBMS 
 to determine the results of the higher-level query that contains the subquery. In the 
 simplest forms of a subquery, the subquery appears within the 
 WHERE
  or 
 HAVING 
 clause 
 of another SQL statement. Subqueries provide an efficient, natural way to handle query 
 requests that are themselves expressed in terms of the results of other queries. Here is an 
 example of such a request:
  
 List the offices where the sales target for the office exceeds the sum of the individual 
 salespeople's quotas.
  
 The request asks for a list of offices from the 
 OFFICES
  table, where the value of the 
 TARGET
  column meets some condition. It seems reasonable that the 
 SELECT
  statement 
 that expresses the query should look something like this:
  
 SELECT CITY
  
  FROM OFFICES
  
 WHERE TARGET > ???
  
 The value 
 ""???
 "" needs to be filled in and should be equal to ""the sum of the quotas of the 
 salespeople assigned to the office in question."" How can you specify that value in the 
 query? From 
 Chapter 8
 , you know that the sum of the quotas for a specific office (say, 
 office number 21) can be obtained with this query:
  
 SELECT SUM(QUOTA)
  
  FROM SALESREPS
  
 WHERE REP_OFFICE = 21
  
 But how can you put the results of this query into the earlier query in place of the 
 question marks? It would seem reasonable to start with the first query and replace the 
 ""???
 "" with the second query, as follows:
  
 SELECT CITY
  
  FROM OFFICES
  
 - 159 -",NA
What Is a Subquery?,"Figure 9-1 shows the form of a SQL subquery. The subquery is enclosed in parentheses, 
 but otherwise it has the familiar form of a 
 SELECT
  statement, with a 
 FROM
  clause and 
 optional 
 WHERE
 , 
 GROUP BY
 , and 
 HAVING
  clauses. The form of these clauses in a 
 subquery is identical to that in a 
 SELECT
  statement, and they perform their normal 
 functions when used within a subquery. There are, however, a few differences between a 
 subquery and an actual 
 SELECT
  statement:
  
  
 Figure 9-1: 
 Basic subquery syntax diagram
  
 •In the most common uses, a subquery must produce a single column of data as its query 
 results. This means that a subquery almost always has a single select item in its 
 SELECT
  
 clause.
  
 •
  
 The 
 ORDER BY
  clause cannot be specified in a subquery. The subquery results are 
 used internally by the main query and are never visible to the user, so it makes little 
  
 sense to sort them anyway.
  
 •Column names appearing in a subquery may refer to columns of tables in the main 
 query. These 
 outer references
  are described in detail later in this chapter.
  
 •In most implementations, a subquery cannot be the 
 UNION
  of several different 
 SELECT 
 statements; only a single 
 SELECT
  is allowed. (The SQL2 standard allows much more 
 powerful query expressions and relaxes this restriction, as described later in the chapter.)
  
 - 160 -",NA
Subqueries in the ,NA,NA
WHERE,NA,NA
Clause,"Subqueries are most frequently used in the 
 WHERE
  clause of a SQL statement. When a 
 subquery appears in the 
 WHERE
  clause, it works as part of the row selection process. The 
 very simplest subqueries appear within a search condition and produce a value that is 
 used to test the search condition. Here is an example of a simple subquery:
  
 List the salespeople whose quota is less than 10% of the company-wide sales target.
  
 SELECT NAME
  
  FROM SALESREPS
  
 WHERE QUOTA < (.1 * (SELECT SUM(TARGET) FROM OFFICES))
  
 NAME
  
 ---------
  
 Bob Smith
  
 In this case, the subquery calculates the sum of the sales targets for all of the offices to 
 determine the company-wide target, which is multiplied by 10 percent to determine the 
 ""cutoff"" sales quota for the query. That value is then used in the search condition to check 
 each row of the 
 SALESREPS
  table and find the requested names. In this simple case, the 
 subquery produces the same value for every row of the 
 SALESREPS
  table; the 
 QUOTA
  
 value for each salesperson is compared to the same company-wide number. In fact, you 
 could carry out this query by first performing the subquery, to calculate the cutoff quota 
 amount ($275,000 in the sample database), and then carry out the main query using this 
 number in a simple 
 WHERE
  clause: 
  
 WHERE QUOTA < 275000
  
 It's more convenient to use the subquery, but it's not essential. Usually subqueries are 
 not this simple. For example, consider once again the query from the previous section:
  
 List the offices where the sales target for the office exceeds the sum of the individual 
 salespeople's quotas.
  
 SELECT CITY
  
  FROM OFFICES
  
 WHERE TARGET > (SELECT SUM(QUOTA)
  
  FROM SALESREPS
  
  WHERE REP_OFFICE = OFFICE)
  
 CITY
  
 -----------
  
 Chicago
  
 Los Angeles
  
 In this (more typical) case, the subquery cannot be calculated once for the entire query. 
  
 The subquery produces a 
 different
  value for each office, based on the quotas of the 
 salespeople in that particular office. Figure 9-2 shows conceptually how SQL carries out 
 the query. The main query draws its data from the 
 OFFICES
  table, and the 
 WHERE 
 clause 
 selects which offices will be included in the query results. SQL goes through the rows of 
 the 
 OFFICES
  table one-by-one, applying the test stated in the 
 WHERE
  clause. The 
 WHERE
  
 clause compares the value of the 
 TARGET
  column in the current row to the value produced 
 by the subquery. To test the 
 TARGET
  value, SQL carries out the subquery, finding the sum 
 of the quotas for salespeople in the ""current"" office. The subquery produces a single 
 number, and the 
 WHERE
  clause compares the number to the 
 TARGET
  
 - 161 -",NA
Outer References,"Within the body of a subquery, it's often necessary to refer to the value of a column in the 
 ""current"" row of the main query. Consider once again the query from the previous 
 sections:
  
 List the offices where the sales target for the office exceeds the sum of the individual 
 salespeople's quotas.
  
 SELECT CITY
  
  FROM OFFICES
  
 WHERE TARGET > (SELECT SUM(QUOTA)
  
  FROM SALESREPS
  
  WHERE REP_OFFICE = OFFICE)
  
 The role of the subquery in this 
 SELECT
  statement is to calculate the total quota for those 
 salespeople who work in a particular office—specifically, the office currently being tested 
 by the 
 WHERE
  clause of the main query. The subquery does this by scanning the 
  
 SALESREPS
  table. But notice that the 
 OFFICE
  column in the 
 WHERE
  clause of the 
 subquery doesn't refer to a column of the 
 SALESREPS
  table; it refers to a column of the 
 OFFICES
  table, which is a part of the main query. As SQL moves through each row of the 
 OFFICES
  table, it uses the 
 OFFICE
  value from the current row when it carries out the 
 subquery.
  
 The 
 OFFICE
  column in this subquery is an example of an 
 outer reference.
  An outer 
 reference is a column name that does not refer to any of the tables named in the 
 FROM 
 clause of the subquery in which the column name appears. Instead, the column name refers 
 to a column of a table specified in the 
 FROM
  clause of the main query. As the previous 
 example shows, when the DBMS examines the search condition in the subquery, the value 
 of the column in an outer reference is taken from the row currently being tested by the main 
 query.",NA
Subquery Search Conditions,"A subquery usually appears as part of a search condition in the 
 WHERE
  or 
 HAVING
  
 - 162 -",NA
Subquery Comparison Test ,NA,NA
(=,NA,NA
", ",NA,NA
<>,NA,NA
", ",NA,NA
<,NA,NA
", ",NA,NA
<=,NA,NA
", ",NA,NA
>,NA,NA
", ",NA,NA
>=),"The subquery comparison test is a modified form of the simple comparison test, as shown 
 in Figure 9-3. It compares the value of an expression to the value produced by a subquery 
 and returns a 
 TRUE
  result if the comparison is true. You use this test to compare a value 
 from the row being tested to a 
 single
  value produced by a subquery, as in this example:
  
  
 Figure 9-3: 
 Subquery comparison test syntax diagram
  
 List the salespeople whose quotas are equal to or higher than the target of the Atlanta 
 sales office.
  
 SELECT NAME
  
  
  FROM SALESREPS 
  
 WHERE QUOTA >= (SELECT TARGET
  
   
  
  FROM OFFICES
  
   
  WHERE CITY = 'Atlanta')
  
 NAME
  
 ----------
  
 Bill Adams 
  
 Sue Smith 
  
 Larry Fitch
  
 The subquery in the example retrieves the sales target of the Atlanta office. The value is 
 then used to select the salespeople whose quotas are higher than the retrieved target.
  
 The subquery comparison test offers the same six comparison operators (=, <>, <, <=, >, 
 >=) available with the simple comparison test. The subquery specified in this test must 
  
 - 163 -",NA
Set Membership Test ,NA,NA
(IN),"The subquery set membership test (
 IN
 ) is a modified form of the simple set membership 
 test, as shown in Figure 9-4. It compares a single data value to a column of data values 
 produced by a subquery and returns a 
 TRUE
  result if the data value matches one of the 
 values in the column. You use this test when you need to compare a value from the row 
 being tested to a 
 set
  of values produced by a subquery. Here is a simple example:
  
  
 Figure 9-4: 
 Subquery set membership test 
 (IN)
  syntax diagram
  
 List the salespeople who work in offices that are over target.
  
 SELECT NAME
  
  
  FROM SALESREPS 
  
 WHERE REP_OFFICE IN (SELECT OFFICE
  
   
  
  FROM OFFICES
  
   
  WHERE SALES > TARGET)
  
 NAME
  
 ----------
  
 Mary Jones 
  
 Sam Clark 
  
 Bill Adams 
  
 Sue Smith 
  
 Larry Fitch
  
 The subquery produces a set of office numbers where the sales are above target (in the 
 sample database, there are three such offices, numbered 11, 13, and 21). The main query 
 then checks each row of the 
 SALESREPS
  table to determine whether that particular 
 salesperson works in an office with one of these numbers. Here are some other 
  
 examples of subqueries that test set membership:
  
 List the salespeople who do not work in offices managed by Larry Fitch (employee 108).
  
 SELECT NAME
  
  
  FROM SALESREPS 
  
 WHERE REP_OFFICE NOT IN (SELECT OFFICE
  
   
  
  FROM OFFICES
  
  
  WHERE MGR = 108)
  
 NAME
  
 ----------
  
 Bill Adams 
  
 Mary Jones
  
 - 165 -",NA
Existence Test ,NA,NA
(EXISTS),"The existence test (
 EXISTS
 ) checks whether a subquery produces any rows of query 
 results, as shown in Figure 9-5. There is no simple comparison test that resembles the 
 existence test; it is used only with subqueries.
  
  
 Figure 9-5: 
 Existence test 
 (EXISTS)
  syntax diagram
  
 Here is an example of a request that can be expressed naturally using an existence test:
  
 List the products for which an order of $25,000 or more has been received.
  
 The request could easily be rephrased as:
  
 List the products for which there exists at least one order in the 
 ORDERS
  table (a) that is 
 for the product in question and (b) has an amount of at least $25,000.
  
 The 
 SELECT
  statement used to retrieve the requested list of products closely resembles 
 the rephrased request:
  
 - 166 -",NA
Quantified Tests ,NA,NA
(ANY and ALL) *,"The subquery version of the 
 IN
  test checks whether a data value is equal to some value in 
 a column of subquery results. SQL provides two 
 quantified tests,
 ANY
  and 
 ALL,
 that extend 
 this notion to other comparison operators, such as greater than (>) and less than (<). Both 
 of these tests compare a data value to the column of data values produced by a subquery, 
 as shown in Figure 9-6.
  
  
 Figure 9-6: 
 Quantified comparison tests (
 ANY
  and 
 ALL
 ) syntax diagram
  
 The 
 ANY
  Test *
  
 The 
 ANY
  test is used in conjunction with one of the six SQL comparison operators (=, <>, 
 <, <=, >, >=) to compare a single test value to a column of data values produced by a 
 subquery. To perform the test, SQL uses the specified comparison operator to compare 
 the test value to 
 each
  data value in the column, one at a time. If 
 any
  of the individual 
 comparisons yield a 
 TRUE
  result, the 
 ANY
  test returns a 
 TRUE
  result.
  
 - 168 -",NA
Subqueries and Joins,"You may have noticed as you read through this chapter that many of the queries that 
 were written using subqueries could also have been written as multi-table queries, or 
 joins. This is often the case, and SQL allows you to write the query either way. This 
  
 - 172 -",NA
Nested Subqueries,"All of the queries described thus far in this chapter have been ""two-level"" queries, 
 involving a main query and a subquery. Just as you can use a subquery ""inside"" a main 
 query, you can use a subquery inside another subquery. Here is an example of a request 
 that is naturally represented as a three-level query, with a main query, a subquery, and a 
 subsubquery:
  
 List the customers whose salespeople are assigned to offices in the Eastern sales region.
  
 SELECT COMPANY
  
  
  FROM CUSTOMERS 
  
 WHERE CUST_REP IN (SELECT EMPL_NUM
  
   
  
  FROM SALESREPS
  
   
  WHERE REP_OFFICE IN (SELECT OFFICE
   
   
  FROM OFFICES
  
 'Eastern'))
  
  WHERE REGION = 
  
 COMPANY
  
 ---------------
  
 First Corp.
  
 Smithson Corp. 
  
 AAA Investments 
  
 JCP Inc.
  
 Chen Associates 
  
 QMA Assoc.
  
 Ian & Schmidt 
  
 Acme Mfg.
  
  .
  
  .
  
  .
  
 In this example, the innermost subquery:
  
 SELECT OFFICE
  
  
  FROM OFFICES 
  
 WHERE REGION = 'Eastern'
  
 produces a column containing the office numbers of the offices in the Eastern region. The 
 next subquery:
  
 - 174 -",NA
Correlated Subqueries *,"In concept, SQL performs a subquery over and over again—once for each row of the 
 main query. For many subqueries, however, the subquery produces the 
 same
  results for 
 every row or row group. Here is an example:
  
 List the sales offices whose sales are below the average target.
  
 SELECT CITY
  
  
  FROM OFFICES 
  
 WHERE SALES < (SELECT AVG(TARGET)
  
   
  FROM OFFICES)
  
 CITY
  
 -------
  
 Denver 
  
 Atlanta
  
 In this query, it would be silly to perform the subquery five times (once for each office). 
  
 The average target doesn't change with each office; it's completely independent of the 
 office currently being tested. As a result, SQL can handle the query by first performing the 
 subquery, yielding the average target ($550,000), and then converting the main query into:
  
 SELECT CITY
  
  
  FROM OFFICES 
  
 WHERE SALES < 550000.00
  
 Commercial SQL implementations automatically detect this situation and use this 
 shortcut whenever possible to reduce the amount of processing required by a subquery. 
 However, the shortcut cannot be used if the subquery contains an outer reference, as in 
 this example:
  
 List all of the offices whose targets exceed the sum of the quotas of the salespeople who 
 work in them:
  
 - 175 -",NA
Subqueries in the ,NA,NA
HAVING,NA,NA
 Clause *,"Although subqueries are most often found in the 
 WHERE
  clause, they can also be used in 
 the 
 HAVING
  clause of a query. When a subquery appears in the 
 HAVING
  clause, it works 
 as part of the row group selection performed by the 
 HAVING
  clause. Consider this query 
 with a subquery:
  
 List the salespeople whose average order size for products manufactured by ACI is 
 higher than overall average order size.
  
 SELECT NAME, AVG(AMOUNT)
  
  
  FROM SALESREPS, ORDERS 
  
 WHERE EMPL_NUM = REP
  
   
  AND MFR = 'ACI' 
  
 GROUP BY NAME 
  
 HAVING AVG(AMOUNT) > (SELECT AVG(AMOUNT)
    
  FROM ORDERS)
  
 NAME         
 AVG(AMOUNT)
  
 -----------  ----------
 -
  
 Sue 
 Smith     
 $15,000.00 
  
 Tom 
 Snyder    
 $22,500.00
  
 Figure 9-7 shows conceptually how this query works. The subquery calculates the ""overall 
 average order size."" It is a simple subquery and contains no outer references, so SQL can 
 calculate the average once and then use it repeatedly in the 
 HAVING
  clause. The main 
 query goes through the 
 ORDERS
  table, finding all orders for ACI products, and 
  
 - 177 -",NA
Subquery Summary,"This chapter has described subqueries, which allow you to use the results of one query to 
 help define another query:
  
 •A subquery is a ""query within a query."" Subqueries appear within one of the subquery 
 search conditions in the 
 WHERE
  or 
 HAVING
  clause.
  
 •When a subquery appears in the 
 WHERE
  clause, the results of the subquery are used to 
 select the individual rows that contribute data to the query results.
  
 •When a subquery appears in the 
 HAVING
  clause, the results of the subquery are used to 
 select the row groups that contribute data to the query results.
  
 •Subqueries can be nested within other subqueries.
  
 •The subquery form of the comparison test uses one of the simple comparison 
 operators to compare a test value to the single value returned by a subquery.
  
 •The subquery form of the set membership test (
 IN
 ) matches a test value to the set of 
 values returned by a subquery.
  
 •The existence test (
 EXISTS
 ) checks whether a subquery returns any values.
  
 •
  
 The quantified tests (
 ANY
  and 
 ALL
 ) use one of the simple comparison operators to 
 compare a test value to all of the values returned by a subquery, checking to see 
  
 whether the comparison holds for some or all of the values.
  
 •A subquery may include an 
 outer reference
  to a table in any of the queries that contain it, 
 linking the subquery to the ""current"" row of that query.",NA
Advanced Queries in SQL2 *,"The SQL queries described thus far in Chapters 6–9 are the mainstream capabilities 
 provided by most SQL implementations today. The combination of features they 
 represent—column selection in the 
 SELECT
  clause, row selection criteria in the 
 WHERE 
 clause, multi-table joins in the 
 FROM
  clause, summary queries in the 
 GROUP BY
  and 
 HAVING
  clauses, and subqueries for more complex requests—give the user a powerful 
 set of data retrieval and data analysis capabilities. However, database experts have 
 pointed out many limitations of these mainstream query capabilities, including these:
  
 •
 No decision-making within queries. 
 Suppose you wanted to generate a two-column report 
 from the sample database showing the name of each sales office and either its annual 
 sales target or its year-to-date sales, whichever is larger. With standard SQL query 
 features, this is hard to do. Or suppose you had a database that kept track of sales by 
 quarter (four columns of data for each office) and wanted to write a program that displayed 
 offices and their sales for a specific (user-supplied) quarter. Again, this program is more 
 difficult to write using standard SQL queries. You must include four separate SQL queries 
 (one for each quarter), and the program logic must select which query to run, based on 
 user input. This simple case isn't too difficult, but in a more general case, the program 
 could become much more complex.
  
 •
 Limited use of subqueries.
  The simplest example of this limitation is the SQL1 restriction 
 that a subquery can appear only on the right side of a comparison test in a 
 WHERE
  
 clause. The database request ""list the offices where the sum of the 
  
 salesperson's quotas is greater than the office target"" is most directly expressed as this 
 query:
  
 - 179 -",NA
Scalar-Valued Expressions (SQL2),"The simplest extended query capabilities in SQL2 are those that provide more data 
 manipulation and calculation power involving individual data values (called 
 scalars
  in the 
 SQL2 standard). Within the SQL language, individual data values tend to have three 
 sources:
  
 •The value of an individual column within an individual row of a table
  
 •A literal value, such as 125.7 or ""ABC""
  
 •A user-supplied data value, entered into a program
  
 In this SQL query:
  
 SELECT NAME, EMPL_NUM, HIRE_DATE, (QUOTA * .9)
  
  FROM SALESREPS 
  
 WHERE (REP_OFFICE = 13) OR TITLE = 'VP SALES'
  
 the column names 
 NAME
 , 
 EMPL_NUM
 , 
 HIRE_DATE
 , and 
 QUOTA
  generate individual data 
 values for each row of query results, as do the column names 
 REP_OFFICE
  and 
 TITLE 
 in the 
 WHERE
  clause. The numbers .9 and 13 and the character string ""VP SALES"" 
 similarly generate individual data values. If this SQL statement appeared within an 
 embedded SQL program (described in 
 Chapter 17
 ), the program variable 
 office_num 
 might contain an individual ata value, and the query might appear as:
  
 SELECT NAME, EMPL_NUM, HIRE_DATE, (QUOTA * .9)
  
  FROM SALESREPS 
  
 WHERE (REP_OFFICE = :office_num) OR TITLE = 'VP SALES'
  
 As this query and many previous examples have shown, individual data values can be 
 combined in simple expressions, like the calculated value 
 QUOTA * .9
 . To these basic 
 SQL1 expressions, SQL2 adds the 
 CAST
  operator for explicit data type conversion, the 
 CASE
  operator for decision-making, the 
 NULLIF
  operation for conditionally creating a 
 NULL
  value, and the 
 COALESCE
  operator for conditionally creating non-
 NULL
  values.
  
 The 
 CAST
  Expression (SQL2)
  
 The SQL standard has fairly restrictive rules about combining data of different types in 
 expressions. It specifies that the DBMS shall automatically convert among very similar 
 data types, such as 2-byte integers and 4-byte integers. However, if you try to compare 
 numbers and character data, for example, the standard says that the DBMS should 
 generate an error. The standard considers this an error condition even if the character 
 string contains numeric data. You can, however, 
 explicitly
  ask the DBMS to convert 
 among data types using the 
 CAST
  expression, whose syntax is shown in Figure 9-8.
  
 - 181 -",NA
Row Value Expressions (SQL2),"Although columns and the scalar data values they contain are the atomic building blocks 
 of a relational database, the structuring of columns into rows that represent ""real-world"" 
 entities, such as individual offices or customers or orders, is one of the most important 
 features of the relational model. The SQL1 standard, and most mainstream commercial 
 database products, certainly reflect this row/column structure, but they provide very limited 
 capability to actually manipulate rows and groups of rows. Basically, SQL1 operations 
 allowed you to insert a row into a table, or to retrieve, update or delete groups of rows 
 from a database (using the 
 SELECT
 , 
 UPDATE
  or 
 DELETE
  statements).
  
 The SQL2 standard goes well beyond these capabilities, allowing you to generally use 
 rows in SQL expressions in much the same way that you can use scalar values. It 
 provides a syntax for constructing rows of data. It allows row-valued subqueries. And it 
 defines row-valued meanings for the SQL comparison operators and other SQL 
  
 - 185 -",NA
Table Value Expressions (SQL2),"In addition to its extended capabilities for expressions involving simple scalar data values 
 and row values, the SQL2 standard dramatically extended the SQL capabilities for table 
 processing. It provides a mechanism for constructing a table of data values""in place"" 
 within a SQL statement. It allows table value subqueries and extends the subquery tests 
 described earlier in this chapter to handle them. It also allows subqueries to appear in 
 many more places within a SQL statement—for example, a subquery can appear in the 
 FROM
  clause of a 
 SELECT
  statement as of its ""source tables."" Finally, it provides 
  
 expanded capabilities for combining tables, including the 
 UNION
 , 
 INTERSECTION
 , and 
 DIFFERENCE
  operations.
  
 Table Value Constructor (SQL2)
  
 - 188 -",NA
Query Expressions (SQL2),"The SQL2 standard defines a 
 query expression
  as the full, general-purpose way that you 
 can specify a table of query results in the SQL2 language. The basic building blocks you 
 can use to build a query expression are:
  
 •A query specification, as described in the preceding section (
 SELECT
 …
 FROM
 …). Its 
 value is a table of query results.
  
 •A table value constructor, as previous described (
 VALUES
  …). Its value is a table of 
 constructed values.
  
 •An explicit table reference (
 TABLE
 tblname
 ). Its value is the contents of the named 
 table.
  
 Using these building blocks, SQL2 lets you combine their table values using the following 
 operations:
  
 •
  
 JOIN
 . SQL2 provides explicit support for full cross-product (cross-join), natural join, 
 inner joins, and all types of outer joins (left, right, and full), as described in 
 Chapter 6
 . 
  
 A join operation takes two tables as its input, and produces a table of combined query 
 results according to the join specification.
  
 •
  
 UNION
 . The SQL2 
 UNION
  operation provides explicit support for merging the rows of 
 two compatible tables (that is, two tables having the same number of columns and with 
  
 corresponding columns having the same data types). The union operation takes two 
 tables as its input and produces a single ""merged table"" of query results.
  
 •
  
 DIFFERENCE
 . The SQL2 
 EXCEPT
  operation takes two tables as its input and produces 
 as its output a table containing the rows that appear in the first table but do not appear 
  
 in another table—that is, the rows that are ""missing"" from the second table. 
  
 Conceptually, the 
 EXCEPT
  operation is like ""table subtraction."" The rows of the second 
 table are ""taken away"" from the rows of the first table, and the answer is the remaining 
 rows of the first table.
  
 - 191 -",NA
SQL Queries—A Final Summary,"This concludes the discussion of the SQL queries and the 
 SELECT
  statement that began 
 in 
 Chapter 6
 . As described in the last three chapters, the clauses of the 
 SELECT 
  
 statement provide a powerful, flexible set of capabilities for retrieving data from the 
 database. Each clause plays a specific role in data retrieval:
  
 •The 
 FROM
  clause specifies the source tables that contribute data to the query results. 
 Every column name in the body of the 
 SELECT
  statement must unambiguously identify a 
 column from one of these tables, or it must be an outer reference to a column from a 
 source table of an outer query.
  
 - 194 -",NA
Part III: ,NA,NA
Updating Data,NA,NA
Chapter List,"Chapter Database Updates 
  
 10: 
  
 Chapter Data Integrity 
  
 11: 
  
 Chapter Transaction Processing 
  
 12:",NA
Chapter 10: ,NA,NA
Database Updates,NA,NA
Overview,"SQL is a complete data manipulation language that is used not only for database queries, 
 but also to modify and update data in the database. Compared to the complexity of the 
 SELECT
  statement, which supports SQL queries, the SQL statements that modify 
 database contents are extremely simple. However, database updates pose some 
 challenges for a DBMS beyond those presented by database queries. The DBMS must 
 protect the integrity of stored data during changes, ensuring that only valid data is 
 introduced into the database, and that the database remains self-consistent, even in the 
 event of system failures. The DBMS must also coordinate simultaneous updates by 
 multiple users, ensuring that the users and their changes do not interfere with one 
 another.
  
 This chapter describes the three SQL statements that are used to modify the contents of 
 a database:
  
 •
 INSERT
 , which adds new rows of data to a table,
  
 •
 DELETE
 , which removes rows of data from a table, and
  
 - 196 -",NA
Adding Data to the Database,"A new row of data is typically added to a relational database when a new entity 
 represented by the row ""appears in the outside world."" For example, in the sample 
 database:
  
 •When you hire a new salesperson, a new row must be added to the 
 SALESREPS
  table to 
 store the salesperson's data.
  
 •When a salesperson signs a new customer, a new row must be added to the 
 CUSTOMERS
  table, representing the new customer.
  
 •When a customer places an order, a new row must be added to the 
 ORDERS
  table to 
 contain the order data.
  
 In each case, the new row is added to maintain the database as an accurate model of the 
 real world. The smallest unit of data that can be added to a relational database is a single 
 row. In general, a SQL-based DBMS provides three ways to add new rows of data to a 
 database:
  
 •A 
 single-row
 INSERT
  statement adds a single new row of data to a table. It is 
 commonly used in daily applications—for example, data entry programs.
  
 •
  
 A 
 multi-row
 INSERT
  statement extracts rows of data from another part of the database 
 and adds them to a table. It is commonly used in end-of-month or end-of-year 
  
 processing when ""old"" rows of a table are moved to an inactive table.
  
 •A 
 bulk load
  utility adds data to a table from a file that is outside of the database. It is 
 commonly used to initially load the database or to incorporate data downloaded from 
 another computer system or collected from many sites.",NA
The Single-Row ,NA,NA
INSERT,NA,NA
 Statement,"The single-row 
 INSERT
  statement, shown in Figure 10-1, adds a new row to a table. The 
 INTO
  clause specifies the table that receives the new row (the 
 target
  table), and the 
 VALUES
  clause specifies the data values that the new row will contain. The column list 
 indicates which data value goes into which column of the new row
  
  
 Figure 10-1: 
 Single-row 
 INSERT
  statement syntax diagram
  
 Suppose you just hired a new salesperson, Henry Jacobsen, with the following personal 
 data:
  
 - 197 -",NA
The Multi-Row ,NA,NA
INSERT,NA,NA
 Statement,"The second form of the 
 INSERT
  statement, shown in Figure 10-3, adds multiple rows of 
 data to its target table. In this form of the 
 INSERT
  statement, the data values for the new 
 rows are not explicitly specified within the statement text. Instead, the source of new rows 
 is a database query, specified in the statement.
  
  
 Figure 10-3: 
 Multi-row 
 INSERT
  statement syntax diagram
  
 Adding rows whose values come from within the database itself may seem strange at first, 
 but it's very useful in some special situations. For example, suppose that you want to copy 
 the order number, date, and amount of all orders placed before January 1, 1990, from the 
 ORDERS
  table into another table, called 
 OLDORDERS
 . The multi-row 
 INSERT 
 statement 
 provides a compact, efficient way to copy the data:
  
 Copy old orders into the 
 OLDORDERS
  table.
  
 INSERT INTO OLDORDERS (ORDER_NUM, ORDER_DATE, AMOUNT)
  
  SELECT ORDER_NUM, ORDER_DATE, AMOUNT
  
 - 200 -",NA
Bulk Load Utilities,"Data to be inserted into a database is often downloaded from another computer system 
 or collected from other sites and stored in a sequential file. To load the data into a table, 
 you could write a program with a loop that reads each record of the file and uses the 
 single-row 
 INSERT
  statement to add the row to the table. However, the overhead of 
 having the DBMS repeatedly execute single-row 
 INSERT
  statements may be quite high. 
  
 - 202 -",NA
Deleting Data from the Database,"A row of data is typically deleted from a database when the entity represented by the row 
 ""disappears from the outside world."" For example, in the sample database:
  
 •When a customer cancels an order, the corresponding row of the 
 ORDERS
  table must be 
 deleted.
  
 •When a salesperson leaves the company, the corresponding row of the 
 SALESREPS 
 table must be deleted.
  
 •
  
 When a sales office is closed, the corresponding row of the 
 OFFICES
  table must be 
 deleted. If the salespeople in the office are terminated, their rows should be deleted 
  
 from the 
 SALESREPS
  table as well. If they are reassigned, their 
 REP_OFFICE
  columns 
 must be updated.
  
 In each case, the row is deleted to maintain the database as an accurate model of the 
 real world. The smallest unit of data that can be deleted from a relational database is a 
 single row.",NA
The ,NA,NA
DELETE,NA,NA
 Statement,"The 
 DELETE
  statement, shown in Figure 10-5, removes selected rows of data from a 
 single table. The 
 FROM
  clause specifies the target table containing the rows. The 
 WHERE 
 clause specifies which rows of the table are to be deleted.
  
  
 Figure 10-5: 
 DELETE
  statement syntax diagram
  
 Suppose that Henry Jacobsen, the new salesperson hired earlier in this chapter, has just 
 decided to leave the company. Here is the 
 DELETE
  statement that removes his row from 
 the 
 SALESREPS
  table:
  
 Remove Henry Jacobsen from the database.
  
 DELETE FROM SALESREPS
  
 WHERE NAME = 'Henry Jacobsen'
  
 1 row deleted.
  
 The 
 WHERE
  clause in this example identifies a single row of the 
 SALESREPS
  table, which 
 SQL removes from the table. The 
 WHERE
  clause should have a familiar appearance—it's 
 exactly the same 
 WHERE
  clause that you would specify in a 
 SELECT
  statement to 
 retrieve
  
 - 203 -",NA
Deleting All Rows,- 204 -,NA
DELETE,NA,NA
 with Subquery *,"DELETE
  statements with simple search conditions, such as those in the previous 
 examples, select rows for deletion based solely on the contents of the rows themselves. 
  
 Sometimes the selection of rows must be made based on data from other tables. For 
 example, suppose you want to delete all orders taken by Sue Smith. Without knowing her 
 employee number, you can't find the orders by consulting the 
 ORDERS
  table alone. To find 
 the orders, you could use a two-table query:
  
 Find the orders taken by Sue Smith.
  
 SELECT ORDER_NUM, AMOUNT
  
  
  FROM ORDERS, SALESREPS 
  
 WHERE REP = EMPL_NUM
  
   
  AND NAME = 'Sue Smith'
  
 ORDER_NUM      AMOUNT
  
 ----------  ----------
  
  
  112979  $15,000.00
  
  
  113065   $2,130.00
  
  
  112993   $1,896.00
  
  
  113048   $3,750.00
  
 But you can't use a join in a 
 DELETE
  statement. The parallel 
 DELETE
  statement is illegal:
  
 DELETE FROM ORDERS, SALESREPS 
  
 WHERE REP = EMPL_NUM
  
  
  AND NAME = 'Sue Smith'
  
 Error: More than one table specified in FROM clause
  
 The way to handle the request is with one of the 
 subquery
  search conditions. Here is a 
 valid form of the 
 DELETE
  statement that handles the request:
  
 - 205 -",NA
Modifying Data in the Database,"Typically, the values of data items stored in a database are modified when corresponding 
 changes occur in the outside world. For example, in the sample database:
  
 •When a customer calls to change the quantity on an order, the 
 QTY
  column in the 
 appropriate row of the 
 ORDERS
  table must be modified.
  
 •When a manager moves from one office to another, the 
 MGR
  column in the 
 OFFICES 
 table and the 
 REP_OFFICE
  column in the 
 SALESREPS
  table must be changed to reflect 
 the new assignment.
  
 •When sales quotas are raised by 5 percent in the New York sales office, the 
 QUOTA 
 column of the appropriate rows in the 
 SALESREPS
  table must be modified.
  
 In each case, data values in the database are updated to maintain the database as an 
 accurate model of the real world. The smallest unit of data that can be modified in a 
 database is a single column of a single row.",NA
The ,NA,NA
UPDATE,NA,NA
 Statement,"The 
 UPDATE
  statement, shown in Figure 10-6, modifies the values of one or more 
 columns in selected rows of a single table. The target table to be updated is named in the 
 statement, and you must have the required permission to update the table as well as each 
 of the individual columns that will be modified. The 
 WHERE
  clause selects the rows of the 
 table to be modified. The 
 SET
  clause specifies which columns are to be updated and 
 calculates the new values for them.
  
  
 Figure 10-6: 
 UPDATE
  statement syntax diagram
  
 Here is a simple 
 UPDATE
  statement that changes the credit limit and salesperson for a 
 customer:
  
 - 207 -",NA
Updating All Rows,"The 
 WHERE
  clause in the 
 UPDATE
  statement is optional. If the 
 WHERE
  clause is omitted, 
 then 
 all
  rows of the target table are updated, as in this example:
  
 Raise all quotas by 5 percent.
  
 UPDATE SALESREPS
  
  SET QUOTA = 1.05 * QUOTA
  
 10 rows updated.
  
 Unlike the 
 DELETE
  statement, in which the 
 WHERE
  clause is almost never omitted, the 
 UPDATE
  statement without a 
 WHERE
  clause performs a useful function. It basically 
 performs a bulk update of the entire table, as demonstrated in the preceding example.",NA
UPDATE,NA,NA
 with Subquery *,"As with the 
 DELETE
  statement, subqueries can play an important role in the 
 UPDATE 
 statement because they let you select rows to update based on information contained in 
 other tables. Here are several examples of 
 UPDATE
  statements that use subqueries:
  
 Raise by $5,000 the credit limit of any customer who has placed an order for more than 
 $25,000.
  
 - 209 -",NA
Summary,"This chapter described the SQL statements that are used to modify the contents of a 
 database:
  
 •The single-row 
 INSERT
  statement adds one row of data to a table. The values for the 
 new row are specified in the statement as constants.
  
 - 210 -",NA
Chapter 11: ,NA,NA
Data Integrity,NA,NA
Overview,"The term 
 data integrity
  refers to the correctness and completeness of the data in a 
 database. When the contents of a database are modified with the 
 INSERT, DELETE
 , or 
 UPDATE
  statements, the integrity of the stored data can be lost in many different ways. 
  
 For example:
  
 •Invalid data may be added to the database, such as an order that specifies a 
 nonexistent product.
  
 •Existing data may be modified to an incorrect value, such as reassigning a 
 salesperson to a nonexistent office.
  
 •Changes to the database may be lost due to a system error or power failure.
  
 •Changes may be partially applied, such as adding an order for a product without 
 adjusting the quantity available for sale.
  
 One of the important roles of a relational DBMS is to preserve the integrity of its stored data 
 to the greatest extent possible. This chapter describes the SQL language features that 
 assist the DBMS in this task.",NA
What Is Data Integrity?,"To preserve the consistency and correctness of its stored data, a relational DBMS 
 typically imposes one or more 
 dataintegrity constraints.
  These constraints restrict the 
 data values that can be inserted into the database or created by a database update. 
 Several different types of data integrity constraints are commonly found in relational 
 databases, including:
  
 •
 Required data.
  Some columns in a database must contain a valid data value in every 
 row; they are not allowed to contain missing or 
 NULL
  values. In the sample database, 
 every order must have an associated customer who placed the order. Therefore, the 
 CUST
  column in the 
 ORDERS
  table is a 
 required column.
  The DBMS can be asked to 
 prevent 
 NULL
  values in this column.
  
 •
 Validity checking.
  Every column in a database has a 
 domain,
  a set of data values that 
 are legal for that column. The sample database uses order numbers that begin at 
  
 - 211 -",NA
Required Data,"The simplest data integrity constraint requires that a column contain a non-
 NULL
  value. 
  
 The ANSI/ISO standard and most commercial SQL products support this constraint by 
 allowing you to declare that a column is 
 NOT NULL
  when the table containing the column 
 is first created. The 
 NOT NULL
  constraint is specified as part of the 
 CREATE TABLE
  
 - 212 -",NA
Simple Validity Checking,"The SQL1 standard provides limited support for restricting the legal values that can 
 appear in a column. When a table is created, each column in the table is assigned a data 
 type, and the DBMS ensures that only data of the specified type is introduced into the 
 column. For example, the 
 EMPL_NUM
  column in the 
 SALESREPS
  table is defined as an 
 INTEGER
 , and the DBMS will produce an error if an 
 INSERT
  or 
 UPDATE
  statement tries to 
 store a character string or a decimal number in the column.
  
 However, the SQL1 standard and many commercial SQL products do not provide a way to 
 restrict a column to certain specific data values. The DBMS will happily insert a 
 SALESREPS
  row with an employee number of 12345, even though employee numbers in 
 the sample database have three digits by convention. A hire date of December 25 would 
 also be accepted, even though the company is closed on Christmas day.
  
 Some commercial SQL implementations provide extended features to check for legal data 
 values. In DB2, for example, each table in the database can be assigned a 
  
 corresponding 
 validation procedure
 , a user-written program to check for valid data values. 
 DB2 invokes the validation procedure each time a SQL statement tries to change or insert 
 a row of the table, and gives the validation procedure the ""proposed"" column values for 
 the row. The validation procedure checks the data and indicates by its return value 
 whether the data is acceptable. The validation procedure is a conventional program 
 (written in S/370 assembler or PL/I, for example), so it can perform whatever data value 
 checks are required, including range checks and internal consistency checks within the 
 row. However, the validation procedure 
 cannot
  access the database, so it cannot be used 
 to check for unique values or foreign key/primary key relationships.
  
 SQL Server also provides a data validation capability by allowing you to create a 
 rule
  that 
 determines what data can be entered into a particular column. SQL Server checks the rule 
 each time an 
 INSERT
  or 
 UPDATE
  statement is attempted for the table that contains the 
 column. Unlike DB2's validation procedures, SQL Server rules are written in the Transact-
 SQL dialect that is used by SQL Server. For example, here is a Transact-SQL 
  
 - 213 -",NA
Column Check Constraints (SQL2),"A SQL2 
 check constraint 
 is a search condition, like the search condition in a 
 WHERE 
 clause, that produces a true/false value. When a check constraint is specified for a 
 column, the DBMS automatically checks the value of that column each time a new row is 
 inserted or a row is updated to insure that the search condition is true. If not, the 
 INSERT 
 or 
 UPDATE
  statement fails. A column check constraint is specified as part of the column 
 definition within the 
 CREATE TABLE
  statement, described in 
 Chapter 13
 .
  
 Consider this excerpt from a 
 CREATE TABLE
  statement, modified from the definition of 
 the demo database to include three check constraints:
  
 CREATE TABLE SALESREPS
  
  
  (EMPL_NUM INTEGER NOT NULL
  
  
  
  
  CHECK (EMPL_NUM BETWEEN 101 AND 199),
  
  
  AGE INTEGER
  
  
  
  
  CHECK (AGE >= 21),
  
  
  
  .
  
  .
  
  .
  
  QUOTA MONEY
  
  
  
  CHECK (MONEY >= 0.0)
  
  
  .
  
  .
  
  .
  
 The first constraint (on the 
 EMPL_NUM
  column) requires that valid employee numbers be 
 three-digit numbers between 101 and 199. The second constraint (on the 
 AGE
  column) 
 similarly prevents hiring of minors. The third constraint (on the 
 QUOTA
  column) prevents a 
 salesperson from having a quota target less than $0.00.
  
 All three of these column check constraints are very simple examples of the capability 
 specified by the SQL2 standard. In general, the parentheses following the keyword 
 CHECK
  can contain any valid search condition that makes sense in the context of a 
 column definition. With this flexibility, a check constraint can compare values from two 
 different columns of the table, or even compare a proposed data value against other 
 values from the database. These capabilities are more fully described later in this 
 chapter.
  
 - 214 -",NA
Domains (SQL2),"A SQL2 
 domain 
 generalizes the check-constraint concept and allows you to easily apply 
 the same check constraint to many different columns within a database. A domain is a 
 collection of legal data values. You specify a domain and assign it a domain name using 
 the SQL2 
 CREATE DOMAIN
  statement, described in 
 Chapter 13
 . As with the check-
 constraint definition, a search condition is used to define the range of legal data values. 
  
 For example, here is a SQL2 
 CREATE DOMAIN
  statement to create the domain 
 VALID_EMPLOYEE_ID
 , which includes all legal employee numbers:
  
 CREATE DOMAIN VALID_EMPLOYEE_ID INTEGER 
 CHECK (VALUE BETWEEN 101 AND 199)
  
 After the 
 VALID_EMPLOYEE_ID
  domain has been defined, it may be used to define 
 columns in database tables instead of a data type. Using this capability, the example 
 CREATE TABLE
  statement for the 
 SALESREPS
  table would appear as:
  
 CREATE TABLE SALESREPS
  
  
  (EMPL_NUM VALID_EMPLOYEE_ID,
  
  
  
  AGE INTEGER
  
  
  
  
  CHECK (AGE >= 21),
  
  
  
  .
  
  .
  
  .
  
  QUOTA MONEY
  
  
  
  CHECK (MONEY >= 0.0)
  
  
  .
  
  .
  
  .
  
 The advantage of using the domain is that if other columns in other tables also contain 
 employee numbers, the domain name can be used repeatedly, simplifying the table 
 definitions. The 
 OFFICES
  table contains such a column: 
  
 CREATE TABLE OFFICES
  
  
  (OFFICE INTEGER NOT NULL,
  
  
    
  CITY VARCHAR(15) NOT NULL,
  
  
  
  REGION VARCHAR(10) NOT NULL,
  
  
     
  MGR VALID_EMPLOYEE_ID,
  
  
  
  TARGET MONEY,
  
  
   
  SALES MONEY NOT NULL
  
  
     
  .
  
  .
  
  .
  
 Another advantage of domains is that the definition of ""valid data"" (such as valid employee 
 numbers in this example) is stored in one, central place within the database. If the definition 
 changes later (for example, if the company grows and employee numbers in the range 200-
 299 must be allowed), it is much easier to change one domain definition than to change 
 many column constraints scattered throughout the database.",NA
Entity Integrity,"A table's primary key must have a unique value for each row of the table, or the database 
 will lose its integrity as a model of the outside world. For example, if two rows of the 
  
 - 215 -",NA
Other Uniqueness Constraints,"It is sometimes appropriate to require a column that is not the primary key of a table to 
 contain a unique value in every row. For example, suppose you wanted to restrict the 
 data in the 
 SALESREPS
  table so that no two salespeople could have exactly the same 
 name in the table. You could achieve this goal by imposing a 
 uniqueness
  constraint on 
 the 
 NAME
  column. The DBMS enforces a uniqueness constraint in the same way that it 
 enforces the primary key constraint. Any attempt to insert or update a row in the table 
 that violates the uniqueness constraint will fail.
  
 The ANSI/ISO SQL standard uses the 
 CREATE TABLE
  statement to specify uniqueness 
 constraints for columns or combinations of columns. However, uniqueness constraints 
 were implemented in DB2 long before the publication of the 
 ANSI/ISO 
 standard, and 
 DB2 made them a part of its 
 CREATE INDEX
  statement. This statement is one of the SQL 
 database administration statements that deals with physical storage of the database on 
 the disk. Normally the SQL user doesn't have to worry about these statements at all; they 
 are used only by the database administrator. 
  
 Many commercial SQL products followed the original DB2 practice rather than the 
 ANSI/ISO standard for uniqueness constraints and required the use of the a 
 CREATE 
 INDEX
  statement. Subsequent versions of DB2 added a uniqueness constraint to the 
 CREATE TABLE
  statement. Most of the other commercial vendors already support or will 
 support the ANSI/ISO syntax for the uniqueness constraint as they add support for SQL2 
 features.",NA
Uniqueness and ,NA,NA
NULL,NA,NA
 Values,"NULL
  values pose a problem when they occur in the primary key of a table or in a column 
 that is specified in a uniqueness constraint. Suppose you tried to insert a row with a 
 primary key that was 
 NULL
  (or partially 
 NULL
 , if the primary key is composed of more than 
 one column). Because of the 
 NULL
  value, the DBMS cannot conclusively decide whether 
 the primary key does or does not duplicate one that is already in the table. The answer 
 must be ""maybe,"" depending on the ""real"" value of the missing (
 NULL
 ) data.
  
 For this reason, the SQL standard requires that every column that is part of a primary key 
 must be declared 
 NOT NULL
 . The same restriction applies for every column that is named 
 in a uniqueness constraint. Together, these restrictions ensure that columns that are 
 ""supposed to"" contain unique data values in each row of a table actually do contain unique 
 values.
  
 - 216 -",NA
Referential Integrity,"Chapter 4
  discussed primary keys, foreign keys, and the parent/child relationships that 
 they create between tables. Figure 11-1 shows the 
 SALESREPS
  and 
 OFFICES
  tables and 
 illustrates once again how foreign keys and primary keys work. The 
 OFFICE
  column is the 
 primary key for the 
 OFFICES
  table, and it uniquely identifies each row. The 
  
 REP_OFFICE
  column, in the 
 SALESREPS
  table, is a foreign key for the 
 OFFICES
  table. It 
 identifies the office where each salesperson is assigned.
  
  
 Figure 11-1: 
 A foreign key/primary key reference
  
 The 
 REP_OFFICE
  and 
 OFFICE
  columns create a parent/child relationship between the 
 OFFICES
  and 
 SALESREPS
  rows. Each 
 OFFICES
  (parent) row has zero or more 
 SALESREPS
  (child) rows with matching office numbers. Similarly, each 
 SALESREPS 
 (child) row has exactly one 
 OFFICES
  (parent) row with a matching office number.
  
 Suppose you tried to insert a new row into the 
 SALESREPS
  table that contained an invalid 
 office number, as in this example:
  
 INSERT INTO SALESREPS (EMPL_NUM, NAME, REP_OFFICE, AGE,
  
  HIRE_DATE, SALES)
  
  VALUES (115, 'George Smith', 31, 37, '01-APR-90', 0.00)
  
 On the surface, there's nothing wrong with this 
 INSERT
  statement. In fact, many SQL 
 implementations will successfully add the row. The database will show that George Smith 
 works in office number 31, even though no office number 31 is listed in the 
 OFFICES 
 table. The newly inserted row clearly ""breaks"" the parent/child relationship between the 
 OFFICES
  and 
 SALESREPS
  tables. In fact, the office number in the 
 INSERT
  statement is 
 probably an error—the user may have intended office number 11, 21, or 13.
  
 It seems clear that every legal value in the 
 REP_OFFICE
  column should be forced to 
 match some value that appears in the 
 OFFICE
  column. This rule is known as a 
 referential 
 integrity
  constraint. It ensures the integrity of the parent/child relationships created by 
 foreign keys and primary keys.
  
 Referential integrity has been a key part of the relational model since it was first proposed 
 by Codd. However, referential integrity constraints were not included in IBM's prototype 
 System/R DBMS, nor in early releases of DB2 or SQL/DS. IBM added referential integrity 
 support to DB2 in 1989, and referential integrity was added to the SQL1 standard after its 
 initial release. Most DBMS vendors now have either implemented referential integrity or 
 indicated plans to include referential integrity support in future releases of their products.",NA
Referential Integrity Problems,- 217 -,NA
Delete and Update Rules *,"For each parent/child relationship created by a foreign key in a database, the SQL2 
 standard allows you to specify an associated delete rule and an associated update rule. 
 The delete rule tells the DBMS what to do when a user tries to delete a row of the parent 
 table. These four delete rules can be specified:
  
 •The 
 RESTRICT
  delete rule prevents you from deleting a row from the parent table if the 
 row has any children. A 
 DELETE
  statement that attempts to delete such a parent row is 
 rejected with an error message. Deletions from the parent table are thus restricted to 
 rows without any children. Applied to 
 Figure 11-1
 , this rule can be summarized as ""You 
 can't delete an office if any salespeople are assigned to it."" 
  
 •
  
 The 
 CASCADE
  delete rule tells the DBMS that when a parent row is deleted, all of its 
 child rows should 
 also
  automatically be deleted from the child table. For 
 Figure 11-1
 , 
  
 this rule can be summarized as ""Deleting an office automatically deletes all the 
 salespeople assigned to that office.""
  
 •The 
 SET NULL
  delete rule tells the DBMS that when a parent row is deleted, the foreign 
 key values in all of its child rows should automatically be set to 
 NULL
 . Deletions from the 
 parent table thus cause a ""set to 
 NULL
 "" update on selected columns of the child table. For 
 the tables in 
 Figure 11-1
 , this rule can be summarized as ""If an office is deleted, indicate 
 that the current office assignment of its salespeople is unknown.""
  
 - 219 -",NA
Cascaded Deletes and Updates *,"The 
 RESTRICT
  rule for deletes and updates is a ""single-level"" rule—it affects only the 
 parent table in a relationship. The 
 CASCADE
  rule, on the other hand, can be a ""multi-level"" 
 rule, as shown in Figure 11-3.
  
 - 221 -",NA
Referential Cycles *,"In the sample database, the 
 SALESREPS
  table contains the 
 REP_OFFICE
  column, a 
 foreign key for the 
 OFFICES
  table. The 
 OFFICES
  table contains the 
 MGR
  column, a 
 foreign key for the 
 SALESREPS
  table. As shown in Figure 11-5, these two relationships 
 form a 
 referential cycle.
  Any given row of the 
 SALESREPS
  table refers to a row of the 
 OFFICES
  table, which refers to a row of the 
 SALESREPS
  table, and so on. This cycle 
 includes only two tables, but it's also possible to construct cycles of three or more tables.
  
  
 Figure 11-5: 
 A referential cycle
  
 Regardless of the number of tables that they involve, referential cycles pose special 
 problems for referential integrity constraints. For example, suppose that 
 NULL
  values were 
 not allowed in the primary or foreign keys of the two tables in Figure 11-5. (This is not, in 
 fact, the way the sample database is actually defined, for reasons that will become 
 obvious in a moment.) Consider this database update request and the 
 INSERT 
  
 statements that attempt to implement it:
  
 You have just hired a new salesperson, Ben Adams (employee number 115), who is the 
 manager of a new sales office in Detroit (office number 14).
  
 - 223 -",NA
Foreign Keys and ,NA,NA
NULL,NA,NA
 Values *,"Unlike primary keys, foreign keys in a relational database are allowed to contain 
 NULL 
 values. In the sample database the foreign key 
 REP_OFFICE
 , in the 
 SALESREPS
  table, 
 permits 
 NULL
  values. In fact, this column does contain a 
 NULL
  value in Tom Snyder's 
  
 - 225 -",NA
Advanced Constraint Capabilities (SQL2),"Primary key and foreign key constraints, uniqueness constraints, and restrictions on 
 missing (
 NULL
 ) values all provide data integrity checking for very specific structures and 
 situations within a database. The SQL2 standard goes beyond these capabilities to 
 include a much more general capability for specifying and enforcing data integrity 
  
 - 226 -",NA
Assertions,"Examples of the first three types of constraints have previously appeared in earlier 
 sections of this chapter. An 
 assertion
  is specified using the SQL2 
 CREATE ASSERTION 
 statement. Here is an assertion that might be useful in the demo database:
  
 Insure that an office's quota target does not exceed the sum of the quotas for its 
 salespeople:
  
 CREATE ASSERTION quota_valid
  
 CHECK ((OFFICES.QUOTA <= SUM(SALESREPS.QUOTA)) AND
  
  (SALESREPS.REP_OFFICE = OFFICES.OFFICE))
  
 Because it is an object in the database (like a table or a column), the assertion must be 
 given a name (in this case, it's 
 ""quota_valid
 ""). The name is used in error messages 
 produced by the DBMS when the assertion is violated. The assertion causing an error 
 may be obvious in a small demo database, but in a large database that might contain 
 dozens or hundreds of assertions, it's critical to know which of the assertions was 
 violated.
  
 - 227 -",NA
SQL2 Constraint Types,"The types of constraints that can be specified in SQL2, and the role played by each, can 
 be summarized as follows:
  
 •The 
 NOT NULL
 constraint can appear only as a column constraint. It prevents the 
 column from being assigned a 
 NULL
  value.
  
 •
  
 A 
 PRIMARY KEY
  constraint can appear as a column constraint or a table constraint. If 
 the primary key consists of a single column, the column constraint may be more 
  
 convenient. If it consists of multiple columns, it should be specified as a table 
 constraint.
  
 •
  
 A 
 UNIQUE
  constraint can appear as a column constraint or a table constraint. If the 
 unique values restriction is being enforced for a single column only, the column 
  
 constraint is the easiest way to specify it. If the unique values restriction applies to a 
 set of two or more columns (that is, the 
 combination
  of values for those columns must 
 be unique for all rows in the table), then the table constraint form should be used.
  
 •
  
 A referential (
 FOREIGNKEY
 ) constraint can appear as a column constraint or a table 
 constraint. If the foreign key consists of a single column, the column constraint may be 
  
 more convenient. If it consists of multiple columns, it should be specified as a table 
 constraint. If a table has many foreign key relationships to other tables, it may be most 
 convenient to gather 
 all
  of its foreign key constraints together at one place in the table 
 definition, rather than having them scattered throughout the column definitions.
  
 •
  
 A 
 CHECK
  constraint can appear as a column constraint or a table constraint. It is also 
 the 
 only 
 kind of constraint that forms part of the definition of a domain or an assertion. 
  
 The check constraint is specified as a search condition, like the search condition that 
 appears in the 
 WHERE
  clause of a database query. The constraint is satisfied if the 
 search condition has a 
 TRUE
  value.
  
 - 228 -",NA
Deferred Constraint Checking,"In their simplest form, the various constraints that are specified within a database are 
 checked every time an attempt is made to change the database contents—that is, during 
 the execution of every attempted 
 INSERT
 , 
 UPDATE
 , or 
 DELETE
  statement. For database 
 systems claiming only Intermediate level or Entry level conformance to the SQL2 
 standard, this is the only mode of operation allowed for database constraints. The Full 
 level SQL2 standard specifies an additional capability for 
 deferred 
 constraint checking.
  
 When constraint checking is deferred, the constraints are not checked for each individual 
 SQL statement. Instead, constraint checking is held in abeyance until the end of a SQL 
 transaction. (Transaction processing and the associated SQL statements are described in 
 detail in the 
 next chapter
 .) When the completion of the transaction is signaled by the SQL 
 COMMIT
  statement, the DBMS checks the deferred constraints. If all of the 
  
 constraints are satisfied, then the 
 COMMIT
  statement can proceed, and the transaction can 
 complete normally. At this point, any changes made to the database during the transaction 
 become permanent. If, however, one or more of the constraints would be violated by the 
 proposed transaction, then the 
 COMMIT
  statement fails, and the 
  
 transaction is ""rolled back""—that is, all of the proposed changes to the database are 
 reversed, and the database goes back to its state before the transaction began.
  
 Deferred constraint checking can be very important when several updates to a database 
 must all be made ""at once"" to keep the database in a consistent state. For example, 
 suppose the demo database contained this assertion: 
  
 Insure that an office's quota target is exactly equal to the sum of the quotas for its 
 salespeople.
  
 CREATE ASSERTION quota_totals
  
 CHECK ((OFFICES.QUOTA = SUM(SALESREPS.QUOTA)) AND
  
  (SALESREPS.REP_OFFICE = OFFICES.OFFICE))
  
 Without the deferred constraint checking, this constraint would effectively prevent you from 
 ever adding a salesperson to the database. Why? Because to keep the office quota and 
 the salespersons' quotas in the right relationship, you must 
 both
  add a new 
  
 salesperson row with the appropriate quota (using an 
 INSERT
  statement) 
 and
  increase 
 the quota for the appropriate office by the same amount (using an 
 UPDATE
  statement). If 
 you try to perform the 
 INSERT
  statement on the 
 SALESREPS
  table first, the 
 OFFICES 
 table will not yet have been updated, the assertion will not be true, and the statement will 
 fail.Similarly, if you try to perform the 
 UPDATE
  statement on the 
 OFFICES
  table first, the 
 SALESREPS
  table will not yet have been updated, the assertion will not be true, and the 
 statement will fail. The only solution to this dilemma is to defer constraint checking until 
 both
  statements have completed, and 
 then
  check to make sure that both operations, taken 
 together, have left the database in a valid state.
  
 The SQL2 deferred constraint mechanism provides for this capability, and much more. 
 Each individual constraint (of all types) within the database can be identified as either 
 DEFERRABLE
  or 
 NOT DEFERRABLE
  when it is first created or defined:
  
 - 229 -",NA
Business Rules,"Many of the data integrity issues in the real world have to do with the rules and 
  
 procedures of an organization. For example, the company that is modeled by the sample 
 database might have rules like these:
  
 •No customer is allowed to place orders that would exceed the customer's credit limit.
  
 •The sales vice president must be notified whenever any customer is assigned a credit 
 limit higher than $50,000.
  
 •Orders may remain on the books only for six months; orders older than six months 
 must be canceled and reentered.
  
 In addition, there are often ""accounting rules"" that must be followed to maintain the 
 integrity of totals, counts, and other amounts stored in a database. For the sample 
 database, these rules probably make sense:
  
 •
  
 Whenever a new order is taken, the 
 SALES
  column for the salesperson who took the 
 order and for the office where that salesperson works should be increased by the 
  
 order amount. Deleting an order or changing the order amount should also cause the 
 SALES
  columns to be adjusted.
  
 •
  
 Whenever a new order is taken, the 
 QTY_ON_HAND
  column for the product being 
 ordered should be decreased by the quantity of products ordered. Deleting an order, 
  
 changing the quantity, or changing the product ordered should also cause 
 corresponding adjustments to the 
 QTY_ON_HAND
  column.
  
 These rules fall outside the realm of the SQL language as defined by the SQL1 standard 
 and as implemented by many SQL-based DBMS products today. The DBMS takes 
 responsibility for storing and organizing data and ensuring its basic integrity, but enforcing 
 the business rules is the responsibility of the application programs that access the 
 database.
  
 - 231 -",NA
What Is a Trigger?,"The concept of a trigger is relatively straightforward. For any event that causes a change 
 in the contents of a table, a user can specify an associated action that the DBMS should 
 carry out. The three events that can trigger an action are attempts to 
 INSERT
 , 
 DELETE
 , or 
 UPDATE
  rows of the table. The action triggered by an event is specified by a sequence of 
 SQL statements.
  
 To understand how a trigger works, let's examine a concrete example. When a new order 
 is added to the 
 ORDERS
  table, these two changes to the database should also take place:
  
 •The 
 SALES
  column for the salesperson who took the order should be increased by the 
 amount of the order.
  
 •The 
 QTY_ON_HAND
  amount for the product being ordered should be decreased by the 
 quantity ordered.
  
 This Transact-SQL statement defines a SQL Server trigger, named 
 NEWORDER
 , that 
 causes these database updates to happen automatically:
  
 CREATE TRIGGER NEWORDER
  
  ON ORDERS
  
  FOR INSERT
  
  AS UPDATE SALESREPS
  
  SET SALES = SALES + INSERTED.AMOUNT
  
  FROM SALESREPS, INSERTED
  
  WHERE SALESREPS.EMPL_NUM = INSERTED.REP
  
  UPDATE PRODUCTS
  
  SET QTY_ON_HAND = QTY_ON_HAND - INSERTED.QTY
  
 - 232 -",NA
Triggers and Referential Integrity,"Triggers provide an alternative way to implement the referential integrity constraints 
 provided by foreign keys and primary keys. In fact, advocates of the trigger feature point 
 out that the trigger mechanism is more flexible than the strict referential integrity provided 
 by DB2 and the ANSI/ISO standard. For example, here is a trigger that enforces 
  
 referential integrity for the 
 OFFICES
 /
 SALESREPS
  relationship and displays a message 
 when an attempted update fails:
  
 CREATE TRIGGER REP_UPDATE
  
  
  
  ON SALESREPS
  
  
  FOR INSERT, UPDATE
  
  
  
  AS IF ((SELECT COUNT(*)
  
  
   
  
  
  
  FROM OFFICES, INSERTED
  
  
   
  
  
  WHERE OFFICES.OFFICE = INSERTED.REP_OFFICE) = 0)
  
   
  BEGIN
  
  
   
  
  PRINT ""Invalid office number specified.""
  
  
   
  
  ROLLBACK TRANSACTION
  
  
   
  END
  
 Triggers can also be used to provide extended forms of referential integrity. For example, 
 DB2 initially provided cascaded deletes through its 
 CASCADE
  delete rule but did not 
 support ""cascaded updates"" if a primary key value is changed. This limitation need not 
 apply to triggers, however. The following SQL Server trigger cascades any update of the 
 OFFICE
  column in the 
 OFFICES
  table down into the 
 REP_OFFICE
  column of the 
 SALESREPS
  table:
  
 CREATE TRIGGER CHANGE_REP_OFFICE
  
  
  
  ON OFFICES
  
  
  FOR UPDATE
  
  
  
  AS IF UPDATE (OFFICE)
  
  
   
  BEGIN
  
  
   
  
  UPDATE SALESREPS
  
  
   
  
    
  SET SALESREPS.REP_OFFICE = INSERTED.OFFICE
  
   
  
   
  FROM SALESREPS, INSERTED, DELETED
  
  
   
  
  
  WHERE SALESREPS.REP_OFFICE = DELETED.OFFICE
  
   
  END
  
 - 234 -",NA
Trigger Advantages and Disadvantages,"A complete discussion of triggers is beyond the scope of this book, but even these simple 
 examples shows the power of the trigger mechanism. The major advantage of triggers is 
 that business rules can be stored in the database and enforced consistently with each 
 update to the database. This can dramatically reduce the complexity of application 
 programs that access the database. Triggers also have some disadvantages, including 
 these:
  
 •
 Database complexity.
  When the rules are moved into the database, setting up the 
 database becomes a more complex task. Users who could reasonably be expected to 
 create small, 
 ad hoc
  applications with SQL will find that the programming logic of triggers 
 makes the task much more difficult.
  
 •
 Hidden rules.
  With the rules hidden away inside the database, programs that appear to 
 perform straightforward database updates may, in fact, generate an enormous amount of 
 database activity. The programmer no longer has total control over what happens to the 
 database. Instead, a program-initiated database action may cause other, hidden actions.
  
 •
 Hidden performance implications.
  With triggers stored inside the database, the 
 consequences of executing a SQL statement are no longer completely visible to the 
 programmer. In particular, an apparently simple SQL statement could, in concept, trigger a 
 process that involves a sequential scan of a very large database table, which would take a 
 long time to complete. These performance implications of any given SQL statement are 
 invisible to the programmer.",NA
Triggers and the SQL Standard,"Triggers were one of the most widely praised and publicized features of Sybase SQL 
 Server when it was first introduced, and they have since found their way into many 
 commercial SQL products. Although the SQL2 standard provided an opportunity to 
 standardize the DBMS implementation of triggers, the standards committee included 
 check constraints instead. As the trigger and check-constraint examples in the preceding 
 sections show, check constraints can be effectively used to limit the data that can be 
 added to a table or modified in a table. However, unlike triggers, they lack the ability to 
 cause an independent action in the database, such as adding a row or changing a data 
 item in another table.
  
 The extra capability provided by triggers has led several industry experts to advocate that 
 they be included in a future SQL3 standard. Other experts have argued that triggers are a 
 pollution of the data management function of a database, and that the functions performed 
 by triggers belong in fourth generation languages (4GLs) and other database tools, rather 
 than in the DBMS itself. While the debate continues, DBMS products have experimented 
 with new trigger capabilities that extend beyond the database itself. These ""extended 
 trigger"" capabilities allow modifications to data in a database to automatically cause actions 
 such as sending mail, alerting a user, or launching another program to perform a task. This 
 makes triggers even more useful and will add to the debate over including them in future 
 official SQL standards. Regardless of the official stance, it appears that triggers will become 
 a more important part of the SQL language over the next several years.",NA
Summary,- 235 -,NA
Chapter 12: ,NA,NA
Transaction Processing,NA,NA
Overview,"Database updates are usually triggered by real-world events, such as the receipt of a 
 new order from a customer. In fact, receiving a new order would generate not just one, 
 but this series of 
 four
  updates to the sample database:
  
 •Add the new order to the 
 ORDERS
  table.
  
 •Update the sales total for the salesperson who took the order.
  
 •Update the sales total for the salesperson's office.
  
 •Update the quantity-on-hand total for the ordered product.
  
 To leave the database in a self-consistent state, all four updates must occur as a unit. If a 
 system failure or another error creates a situation where some of the updates are 
  
 processed and others are not, the integrity of the database will be lost. Similarly, if another 
 user calculates totals or ratios part way through the sequence of updates, the calculations 
 will be incorrect. The sequence of updates must thus be an ""all-or-nothing"" proposition in 
 the database. SQL provides precisely this capability through its transaction processing 
 features, which are described in this chapter.",NA
What Is a Transaction?,"A 
 transaction
  is a sequence of one or more SQL statements that together form a logical 
 unit of work. The SQL statements that form the transaction are typically closely related 
 and perform interdependent actions. Each statement in the transaction performs some 
 part of a task, but all of them are required to complete the task. Grouping the statements 
 as a single transaction tells the DBMS that the entire statement sequence should be 
  
 - 236 -",NA
COMMIT,NA,NA
 and ,NA,NA
ROLLBACK,"SQL supports database transactions through two SQL transaction processing 
 statements, shown in Figure 12-2:
  
  
 Figure 12-2: 
 COMMIT
  and 
 ROLLBACK
  statement syntax diagrams
  
 •
  
 The 
 COMMIT
  statement signals the successful end of a transaction. It tells the DBMS 
 that the transaction is now complete; all of the statements that comprise the 
  
 transaction have been executed, and the database is self-consistent.
  
 •
  
 The 
 ROLLBACK
  statement signals the unsuccessful end of a transaction. It tells the 
 DBMS that the user does not want to complete the transaction; instead, the DBMS 
  
 should 
 back out
  any changes made to the database during the transaction. In effect, 
 the DBMS restores the database to its state before the transaction began.
  
 The 
 COMMIT
  and 
 ROLLBACK
  statements are executable SQL statements, just like 
 SELECT
 , 
 INSERT
 , and 
 UPDATE
 . Here is an example of a successful update transaction 
 that changes the quantity and amount of an order and adjusts the totals for the product, 
 salesperson, and office associated with the order. A change like this would typically be 
 handled by a forms-based ""change order"" program, which would use programmatic SQL 
 to execute the statements shown on the next page of text.
  
 Change the quantity on order number 113051 from 4 to 10, which raises its amount from 
 $1,458 to $3,550. The order is for QSA-XK47 Reducers and was placed with Larry Fitch 
 (employee number 108) who works in Los Angeles (office number 21).
  
 UPDATE ORDERS
  
  
  SET QTY = 10, AMOUNT = 3550.00 
  
 WHERE ORDER_NR = 113051
  
 UPDATE SALESREPS
  
  
  SET SALES = SALES - 1458.00 + 3550.00 
 WHERE EMPL_NUM = 108
  
 UPDATE OFFICES
  
  
  SET SALES = SALES - 1458.00 + 3550.00 
 WHERE OFFICE = 21
  
 UPDATE PRODUCTS
  
  
  SET QTY_ON_HAND = QTY_ON_HAND + 4 - 10 
 WHERE MFR_ID = 'QSA'
  
  
  AND PRODUCT_ID = 'XK47'
  
 - 238 -",NA
The ANSI/ISO Transaction Model,"The ANSI/ISO SQL standard defines a SQL 
 transactionmodel
  and the roles of the 
 COMMIT
  
 and 
 ROLLBACK
  statements. Most, but not all, commercial SQL products use this 
 transaction model, which is based on the transaction support in the early releases of DB2. 
 The standard specifies that a SQL transaction 
 automatically
  begins with the first SQL 
 statement executed by a user or a program. The transaction continues through 
 subsequent SQL statements until it ends in one of four ways:
  
 •A 
 COMMIT
  statement ends the transaction successfully, making its database changes 
 permanent. A new transaction begins immediately after the 
 COMMIT
  statement.
  
 •A 
 ROLLBACK
  statement aborts the transaction, backing out its database changes. A 
 new transaction begins immediately after the 
 ROLLBACK
  statement.
  
 •Successful program termination (for programmatic SQL) also ends the transaction 
 successfully, just as if a 
 COMMIT
  statement had been executed. Because the program is 
 finished, there is no new transaction to begin.
  
 •Abnormal program termination (for programmatic SQL) also aborts the transaction, just 
 as if a 
 ROLLBACK
  statement had been executed. Because the program is finished, there is 
 no new transaction to begin.
  
 - 239 -",NA
Other Transaction Models,"A few commercial SQL products depart from the ANSI/ISO and DB2 transaction model to 
 provide additional transaction processing capability for their users. The Sybase DBMS, 
 which is designed for online transaction processing applications, is one example. SQL 
 Server, which was derived from the Sybase product, also uses the Sybase transaction 
 model.
  
 The Transact-SQL dialect used by Sybase includes four transaction processing 
 statements:
  
 •
  
 The 
 BEGIN TRANSACTION
  statement signals the beginning of a transaction. Unlike 
 the ANSI/ISO transaction model, which implicitly begins a new transaction when the 
  
 previous one ends, Sybase requires an explicit statement to start a transaction.
  
 •The 
 COMMIT TRANSACTION
  statement signals the successful end of a transaction. As 
  
 - 240 -",NA
Transactions: Behind the Scenes *,"The ""all-or-nothing"" commitment that a DBMS makes for the statements in a transaction 
 seems almost like magic to a new SQL user. How can the DBMS possibly back out the 
 changes made to a database, especially if a system failure occurs during the middle of a 
 transaction? The actual techniques used by brands of DBMS vary, but almost all of them 
 are based on a 
 transaction log
 , as shown in Figure 12-5.
  
  
 Figure 12-5: 
 The transaction log
  
 Here is how the transaction log works, in simplified form. When a user executes a SQL 
 statement that modifies the database, the DBMS automatically writes a record in the 
 transaction log showing two copies of each row affected by the statement. One copy 
 shows the row 
 before
  the change, and the other copy shows the row 
 after
  the change. 
  
 Only after the log is written does the DBMS actually modify the row on the disk. If the user 
 subsequently executes a 
 COMMIT
  statement, the end-of-transaction is noted in the 
 transaction log. If the user executes a 
 ROLLBACK
  statement, the DBMS examines the log 
 to find the ""before"" images of the rows that have been modified since the transaction 
 began. Using these images, the DBMS restores the rows to their earlier state, effectively 
 backing out all changes to the database that were made during the transaction.
  
 If a system failure occurs, the system operator typically recovers the database by running 
 a special recovery utility supplied with the DBMS. The recovery utility examines the end of 
 the transaction log, looking for transactions that were not committed before the failure. 
  
 The utility rolls back each of these incomplete transactions, so that only committed 
 transactions are reflected in the database; transactions in process at the time of the 
 failure have been rolled back.
  
 The use of a transaction log obviously imposes an overhead on updates to the database. 
  
 In practice, the mainstream commercial DBMS products use much more sophisticated 
 logging techniques than the simple scheme described here to minimize this overhead. In 
 addition, the transaction log is usually stored on a fast disk drive, different from the one that 
  
 - 242 -",NA
Transactions and Multi-User Processing,"When two or more users concurrently access a database, transaction processing takes on 
 a new dimension. Now the DBMS must not only recover properly from system failures or 
 errors, it must also ensure that the users' actions do not interfere with one another. 
  
 Ideally, each user should be able to access the database as if he or she had exclusive 
 access to it, without worrying about the actions of other users. The SQL transaction 
 model allows a SQL-based DBMS to insulate users from one another in this way.
  
 The best way to understand how SQL handles concurrent transactions is to look at the 
 problems that result if transactions are not handled properly. Although they can show up in 
 many different ways, four fundamental problems can occur. The next four sections give a 
 simple example of each problem.",NA
The Lost Update Problem,"Figure 12-6 shows a simple application where two users accept telephone orders from 
 customers. The order entry program checks the 
 PRODUCTS
  file for adequate inventory 
 before accepting the customer's order. In the figure, Joe starts entering an order for 100 
 ACI-41004 Widgets from his customer. At the same time, Mary starts entering her 
 customer's order for 125 ACI-41004 Widgets. Each order entry program does a query on 
 the 
 PRODUCTS
  file, and each finds that 139 Widgets are in stock—more than enough to 
 cover the customer's request. Joe asks his customer to confirm the order, and his copy of 
 the order entry program updates the 
 PRODUCTS
  file to show (139 – 100) = 39 Widgets 
 remaining for sale and inserts a new order for 100 Widgets into the 
 ORDERS
  table. A few 
 seconds later, Mary asks her customer to confirm their order. Her copy of the order entry 
 program updates the 
 PRODUCTS
  file to show (139 – 125) = 14 Widgets remaining in stock 
 and inserts a new order for 125 Widgets into the 
 ORDERS
  table.
  
  
 Figure 12-6: 
 The lost update problem
  
 The handling of the two orders has obviously left the database in an inconsistent state. 
  
 The first of the two updates to the 
 PRODUCTS
  file has been lost! Both customers' orders 
 have been accepted, but not enough Widgets are in inventory to satisfy both orders. 
  
 Further, the database shows that there are still 14 Widgets remaining for sale! This 
 example illustrates the ""lost update"" problem that can occur whenever two programs read 
 the same data from the database, use the data as the basis for a calculation, and then try 
 to update the data.
  
 - 243 -",NA
The Uncommitted Data Problem,"Figure 12-7 shows the same order-processing application as Figure 12-6. Joe again 
 begins taking an order for 100 ACI-41004 Widgets from his customer. This time, Joe's 
 copy of the order processing program queries the 
 PRODUCTS
  table, finds 139 Widgets 
 available, and updates the 
 PRODUCTS
  table to show 39 Widgets remaining after the 
 customer's order. Then Joe begins to discuss with the customer the relative merits of the 
 ACI-41004 and ACI-41005 Widgets. In the meantime, Mary's customer tries to order 125 
 ACI-41004 Widgets. Mary's copy of the order processing program queries the 
 PRODUCTS 
 table, finds only 39 Widgets available, and refuses the order. It also generates a notice 
 telling the purchasing manager to buy more ACI-41004 Widgets, which are in great 
 demand. Now Joe's customer decides that they don't want the size 4 Widgets after all, and 
 Joe's order entry program does a 
 ROLLBACK
  to abort its transaction.
  
  
 Figure 12-7: 
 The uncommitted data problem
  
 Because Mary's order-processing program was allowed to see the uncommitted update of 
 Joe's program, the order from Mary's customer was refused, and the purchasing manager 
 will order more Widgets, even though 139 of them are still in stock. The situation would 
 have been even worse if Mary's customer had decided to settle for the 39 available 
 Widgets. In this case, Mary's program would have updated the 
 PRODUCTS
  table to show 
 zero units available. But when the 
 ROLLBACK
  of Joe's transaction occurred, the DBMS 
 would have set the available inventory back to 139 Widgets, even though 39 of them are 
 committed to Mary's customer. The problem in this example is that Mary's program has 
 been allowed to see the uncommitted updates from Joe's program and has acted upon 
 them, producing the erroneous results. The SQL2 standard refers to this as problem ""P1,"" 
 also known as the ""dirty read"" problem. In the parlance of the standard, the data that 
 Mary's program has seen is ""dirty"" because it has not been committed by Joe's program.",NA
The Inconsistent Data Problem,"Figure 12-8 shows the order-processing application once more. Again, Joe begins taking 
 an order for 100 ACI-41004 Widgets from his customer. A short time later, Mary also 
 begins talking to her customer about the same Widgets, and her program does a single-
 row query to find out how many are available. This time Mary's customer inquires about 
 the ACI-41005 Widgets as an alternative, and Mary's program does a single-row query on 
 that row. Meanwhile, Joe's customer decides to order the Widgets, so his program 
 updates that row of the database and does a 
 COMMIT
  to finalize the order in the 
  
 database. After considering the ACI-41005 Widgets as an alternative, Mary's customer 
 decides to order the ACI-41004 Widgets that Mary originally proposed. Her program does 
 a new single-row query to get the information for the ACI-41004 Widgets again. But 
 instead of finding the 139 Widgets that were in stock just a moment ago, the new query 
  
 - 244 -",NA
The Phantom Insert Problem,"Figure 12-9 shows an order-processing application once more. This time, the sales 
 manager runs a report program that scans the 
 ORDERS
  table, printing a list of the orders 
 from customers of Bill Adams and computing their total. In the meantime, a customer 
 calls Bill to place an additional order for $5000. The order is inserted into the database, 
 and the transaction is committed. A short time later, the sales manager's program again 
 scans the 
 ORDERS
  table, running the very same query. This time, there is an additional 
 order, and the total is $5000 higher than for the first query.
  
 - 245 -",NA
Concurrent Transactions,"As the three multi-user update examples show, when users share access to a database 
 and one or more users is updating data, there is a potential for database corruption. SQL 
 uses its transaction mechanism to eliminate this source of database corruption. In addition 
 to the ""all-or-nothing"" commitment for the statements in a transaction, a SQL-based DBMS 
 makes this commitment about transactions:
  
 During a transaction, the user will see a completely consistent view of the database. 
  
 The user will never see the uncommitted changes of other users, and even 
 committed changes made by others will not affect data seen by the user in mid-
 transaction.
  
 Transactions are thus the key to both recovery and concurrency control in a SQL 
 database. The previous commitment can be restated explicitly in terms of concurrent 
 transaction execution:
  
 If two transactions, A and B, are executing concurrently, the DBMS ensures that the 
 results will be the same as they would be if 
 either
  (a) Transaction A were executed 
 first, followed by Transaction B, 
 or
  (b) Transaction B were executed first, followed by 
 Transaction A.
  
 This concept is known as the 
 serializability
  of transactions. Effectively, it means that each 
 database user can access the database as if no other users were concurrently accessing 
 the database.
  
 The fact that SQL insulates you from the actions of other concurrent users doesn't mean, 
 however, that you can forget all about the other users. In fact, the situation is quite the 
 opposite. Because other users want to concurrently update the database, you should 
  
 - 246 -",NA
Locking *,"Virtually all major DBMS products use sophisticated locking techniques to handle 
 concurrent SQL transactions for many simultaneous users. However, the basic concepts 
 behind locking and transactions are very simple. Figure 12-10 shows a simple locking 
 scheme and how it handles contention between two concurrent transactions.
  
 - 247 -",NA
Locking Levels,"Locking can be implemented at various levels of the database. In its crudest form, the 
 DBMS could lock the entire database for each transaction. This locking strategy would be 
 simple to implement, but it would allow processing of only one transaction at a time. If the 
 transaction included any ""think time"" at all (such as time to discuss an order with a 
 customer), all other access to the database would be blocked during that time, leading to 
 unacceptably slow performance.
  
 An improved form of locking is 
 table-level
  locking. In this scheme, the DBMS locks only 
 the tables accessed by a transaction. Other transactions can concurrently access other 
 tables. This technique permits more parallel processing, but still leads to unacceptably 
 slow performance in applications such as order entry, where many users must share 
 access to the same table or tables.
  
 Many DBMS products implement locking at the 
 page level.
  In this scheme, the DBMS 
 locks individual blocks of data (""pages"") from the disk as they are accessed by a 
  
 transaction. Other transactions are prevented from accessing the locked pages but may 
 access (and lock for themselves) other pages of data. Page sizes of 2KB, 4KB, and 16KB 
 are commonly used. Since a large table will be spread out over hundreds or thousands of 
 pages, two transactions trying to access two different rows of a table will usually be 
 accessing two different pages, allowing the two transactions to proceed in parallel.
  
 - 248 -",NA
Shared and Exclusive Locks,"To increase concurrent access to a database, most commercial DBMS products use a 
 locking scheme with more than one type of lock. A scheme using shared and exclusive 
 locks is quite common:
  
 •A 
 shared
  lock is used by the DBMS when a transaction wants to read data from the 
 database. Another concurrent transaction can also acquire a shared lock on the same 
 data, allowing the other transaction to also read the data.
  
 •An 
 exclusive
  lock is used by the DBMS when a transaction wants to update data in the 
 database. When a transaction has an exclusive lock on some data, other transactions 
 cannot acquire any type of lock (shared or exclusive) on the data.
  
 Figure 12-11 shows the rules for this locking scheme and the permitted combinations of 
 locks that can be held by two concurrent transactions. Note that a transaction can acquire 
 an exclusive lock only if no other transaction currently has a shared or an exclusive lock 
 on the data. If a transaction tries to acquire a lock not permitted by the rules in Figure 12-
 11, it is blocked until other transactions unlock the data that it requires.
  
  
 Figure 12-11: 
 Rules for shared and exclusive locks
  
 Figure 12-12 shows the same transactions shown in Figure 12-10, this time using shared 
  
 - 249 -",NA
Deadlocks *,"Unfortunately, the use of any locking scheme to support concurrent SQL transactions 
 leads to a problem called a 
 deadlock.
  Figure 12-13 illustrates a deadlock situation. 
 Program A updates the 
 ORDERS
  table, thereby locking part of it. Meanwhile, Program B 
 updates the 
 PRODUCTS
  table, locking part of it. Now Program A tries to update the 
 PRODUCTS
  table and Program B tries to update the 
 ORDERS
  table, in each case trying to 
 update a part of the table that has been previously locked by the other program. Without 
 outside intervention, each program will wait forever for the other program to commit its 
 transaction and unlock the data. The situation in the figure is a simple deadlock between 
 two programs, but more complex situations can occur where three, four, or more programs 
 are in a ""cycle"" of locks, each waiting for data that is locked by one of the other programs.
  
  
 Figure 12-13: 
 A transaction deadlock
  
 - 250 -",NA
Advanced Locking Techniques *,"Many commercial database products offer advanced locking facilities that go well beyond 
 those provided by standard SQL transactions. These include:
  
 •
 Explicit locking
 . A program can explicitly lock an entire table or some other part of the 
 database if it will be repeatedly accessed by the program.
  
 •
 Isolation levels
 . You can tell the DBMS that a specific program will not re-retrieve data 
 during a transaction, allowing the DBMS to release locks before the transaction ends.
  
 •
 Locking parameters
 . The database administrator can manually adjust the size of the 
 ""lockable piece"" of the database and other locking parameters to tune locking 
 performance.
  
 These facilities tend to be nonstandard and product specific. However, several of them, 
 particularly those available in DB2, have been implemented in several commercial SQL 
 products and have achieved the status of common, if not standard, features. In fact, the 
 Isolation Level capabilities introduced in DB2 have found their way into the SQL2 
 standard.
  
 Explicit Locking *
  
 If a transaction repeatedly accesses a table, the overhead of acquiring small locks on 
 many parts of the table can be very substantial. A bulk update program that walks 
 through every row of a table, for example, will lock the entire table, piece by piece, as it 
 proceeds. For this type of transaction, the program should explicitly lock the entire table, 
 process the updates, and then unlock the table. Locking the entire table has three 
 advantages:
  
 - 251 -",NA
Summary,"This chapter described the transaction mechanism provided by the SQL language:
  
 •A transaction is a logical unit of work in a SQL-based database. It consists of a 
 sequence of SQL statements that are effectively executed as a single unit by the 
 DBMS.
  
 •The 
 COMMIT
  statement signals successful completion of a transaction, making all of its 
 database modifications permanent.
  
 •The 
 ROLLBACK
  statement asks the DBMS to abort a transaction, backing out all of its 
 database modifications.
  
 •Transactions are the key to recovering a database after a system failure; only 
 transactions that were committed at the time of failure remain in the recovered 
 database.
  
 •Transactions are the key to concurrent access in a multi-user database. A user or 
 program is guaranteed that its transaction will not be interfered with by other 
 concurrent transactions.
  
 •Occasionally a conflict with another concurrently executing transaction may cause the 
 DBMS to roll back a transaction through no fault of its own. An application program that 
 uses SQL must be prepared to deal with this situation if it occurs.",NA
Part IV: ,NA,NA
Database Structure,NA,NA
Chapter List,"Chapter Creating a Database 
  
 13: 
  
 - 255 -",NA
Chapter 13: ,NA,NA
Creating a Database,NA,NA
Overview,"Many SQL users don't have to worry about creating a database; they use interactive or 
 programmatic SQL to access a database of corporate information or to access some other 
 database that has been created by someone else. In a typical corporate database, for 
 example, the database administrator may give you permission to retrieve and perhaps to 
 update the stored data. However, the administrator will not allow you to create new 
 databases or to modify the structure of the existing tables.
  
 As you grow more comfortable with SQL, you will probably want to start creating your 
 own private tables to store personal data such as engineering test results or sales 
 forecasts. If you are using a multi-user database, you may want to create tables or even 
 entire databases that will be shared with other users. If you are using a personal 
 computer database, you will certainly want to create your own tables and databases to 
 support your personal applications.
  
 This chapter describes the SQL language features that let you create databases and tables 
 and define their structure.",NA
The Data Definition Language,"The 
 SELECT
 , 
 INSERT
 , 
 DELETE
 , 
 UPDATE
 , 
 COMMIT
 , and 
 ROLLBACK
  statements described 
 in Parts II and III of this book are all concerned with manipulating the data in a database. 
 These statements collectively are called the SQL 
 Data Manipulation Language,
  or DML. 
 The DML statements can modify the data stored in a database, but they cannot change its 
 structure. None of these statements creates or deletes tables or columns, for example.
  
 Changes to the structure of a database are handled by a different set of SQL statements, 
 usually called the SQL 
 Data Definition Language,
  or DDL. Using DDL statements, you 
 can:
  
 •Define and create a new table
  
 •Remove a table that's no longer needed
  
 •Change the definition of an existing table
  
 •Define a virtual table (or view) of data
  
 •Establish security controls for a database
  
 •Build an index to make table access faster
  
 •Control the physical storage of data by the DBMS
  
 - 256 -",NA
Creating a Database,"In a large mainframe or enterprise-level network DBMS installation, the corporate 
 database administrator is solely responsible for creating new databases. On smaller 
 workgroup DBMS installations, individual users may be allowed to create their own 
 personal databases, but it's much more common for databases to be created centrally 
 and then accessed by individual users. If you are using a personal computer DBMS, you 
 are probably both the database administrator and the user, and you will have to create 
 the database(s) that you use personally.
  
 - 257 -",NA
Table Definitions,"The most important structure in a relational database is the table. In a multi-user 
  
 production database, the major tables are typically created once by the database 
 administrator and then used day after day. As you use the database you will often find it 
 convenient to define your own tables to store personal data or data extracted from other 
 tables. These tables may be temporary, lasting only for a single interactive SQL session, 
 or more permanent, lasting weeks or months. In a personal computer database, the table 
 structure is even more fluid. Because you are both the user and the database 
  
 administrator, you can create and destroy tables to suit your own needs, without worrying 
 about other users.
  
 - 258 -",NA
Creating a Table (,NA,NA
CREATE TABLE,NA,NA
),"The 
 CREATE TABLE
  statement, shown in Figure 13-1, defines a new table in the 
 database and prepares it to accept data. The various clauses of the statement specify the 
 elements of the table definition. The syntax diagram for the statement appears complex 
 because there are so many parts of the definition to be specified and so many options for 
 each element. In addition, some of the options are available in some DBMS brands or in 
 the SQL2 standard, but not in other brands. In practice, creating a new table is relatively 
 straightforward.
  
  
 Figure 13-1: 
 Basic 
 CREATE TABLE
 syntax diagram
  
 When you execute a 
 CREATE TABLE
  statement, you become the owner of the newly 
 created table, which is given the name specified in the statement. The table name must be 
 a legal SQL name, and it must not conflict with the name of one of your existing tables. 
 The newly created table is empty, but the DBMS prepares it to accept data added with the 
 INSERT
  statement.
  
 Column Definitions
  
 The columns of the newly created table are defined in the body of the 
 CREATE TABLE 
 statement. The column definitions appear in a comma-separated list enclosed in 
 parentheses. The order of the column definitions determines the left-to-right order of the 
 columns in the table. In the 
 CREATE TABLE
  statements supported by the major DBMS 
 brands, each column definition specifies:
  
 •The 
 column name,
  which is used to refer to the column in SQL statements. Every 
  
 - 259 -",NA
Removing a Table (,NA,NA
DROP TABLE,NA,NA
),"Over time the structure of a database grows and changes. New tables are created to 
 represent new entities, and some old tables are no longer needed. You can remove an 
 unneeded table from the database with the 
 DROP TABLE
  statement, shown in Figure 13-
 3.
  
 - 266 -",NA
Changing a Table Definition (,NA,NA
ALTER TABLE,NA,NA
),"After a table has been in use for some time, users often discover that they want to store 
 additional information about the entities represented in the table. In the sample database, 
 for example, you might want to:
  
 •Add the name and phone number of a key contact person to each row of the 
 CUSTOMERS
  table, as you begin to use it for contacting customers.
  
 •Add a minimum inventory level column to the 
 PRODUCTS
  table, so the database can 
 automatically alert you when stock of a particular product is low.
  
 •Make the 
 REGION
  column in the 
 OFFICES
  table a foreign key for a newly created 
 REGIONS
  table, whose primary key is the region name.
  
 •Drop the foreign key definition linking the 
 CUST
  column in the 
 ORDERS
  table to the 
 CUSTOMERS
  table, replacing it with two foreign key definitions linking the 
 CUST
  column to 
 the newly created 
 CUST_INFO
  and 
 ACCOUNT_INFO
  tables.
  
 Each of these changes, and some others, can be handled with the 
 ALTER TABLE 
 statement, shown in Figure 13-4. As with the 
 DROP TABLE
  statement, you will normally 
 use the 
 ALTER TABLE
  statement on one of your own tables. With proper permission, 
 however, you can specify a qualified table name and alter the definition of another user's 
 table. As shown in the figure, the 
 ALTER TABLE
  statement can:
  
 - 267 -",NA
Constraint Definitions,"The tables in a database define its basic structure, and in most early commercial SQL 
 products, the table definitions were the only specification of database structure. With the 
 advent of primary key/foreign key support in DB2 and in the SQL2 standard, the definition 
 of database structure was expanded to include the 
 relationships
  among the tables in a 
 database. More recently, through the SQL2 standard and the evolution of commercial 
 products, the definition of database structure has expanded to include a new area—
 database constraints that restrict the data that can be entered into the database. The 
 types of constraints, and the role that they play in maintaining database integrity, are 
 described in 
 Chapter 11
 .
  
 Four types of database constraints (uniqueness constraints, primary and foreign key 
 constraints, and check constraints) are closely associated with a single database table. 
  
 They are specified as part of the 
 CREATE TABLE
  statement and can be modified or 
 dropped using the 
 ALTER TABLE
  statement. The other two types of database integrity 
 constraints, assertions and domains, are created as independent ""objects"" within a 
 database, independent of any individual table definition.",NA
Assertions,"An 
 assertion
  is a database constraint that restricts the contents of the database as a 
 whole. Like a check constraint, an assertion is specified as a search condition. But unlike 
 a check constraint, the search condition in an assertion can restrict the contents of 
 multiple tables and the data relationships among them. For that reason, an assertion is 
 specified as part of the overall database definition, via a SQL2 
 CREATE ASSERTION 
 statement. Suppose you wanted to restrict the contents of the sample database so that the 
 total orders for any given customer may not exceed that customer's credit limit. You can 
 implement that restriction with the statement:
  
 CREATE ASSERTION CREDLIMIT
  
  CHECK ((CUSTOMERS.CUST_NUM = ORDERS.CUST) AND
  
  (SUM (AMOUNT) <= CREDIT_LIMIT))
  
 With the assertion named 
 CREDLIMIT
  as part of the database definition, the DBMS is 
 required to check that the assertion remains true each time a SQL statement attempts to 
 modify the 
 CUSTOMER
  or 
 ORDERS
  tables. If you later determine that the assertion is no 
 longer needed, you can drop it using the 
 DROP ASSERTION
  statement: 
  
 DROP ASSERTION CREDLIMIT
  
 There is no SQL2 
 ALTER ASSERTION
  statement. To change an assertion definition, you 
 must drop the old definition and then specify the new one with a new 
 CREATE 
  
 ASSERTION
  statement.",NA
Domains,"The SQL2 standard implements the formal concept of a domain as a part of a database 
 definition. A domain is a named collection of data values that effectively functions as an 
 additional data type, for use in database definitions. A domain is created with a 
 CREATE 
 DOMAIN
  statement. Once created, the domain can be referenced as if it were a data type 
 within a column definition. Here is a 
 CREATE DOMAIN
  statement to define a domain 
 named 
 VALID_EMPL_IDS
 , which consists of valid employee identification numbers in the 
 sample database. These numbers are three-digit integers in the range 101 to 999, 
 inclusive: 
  
 CREATE DOMAIN VALID_EMPL_IDS INTEGER
  
 - 271 -",NA
Aliases and Synonyms (,NA,NA
CREATE/DROP ALIAS,NA,NA
),"Production databases are often organized like the copy of the sample database shown in 
 Figure 13-5, with all of their major tables collected together and owned by the database 
 administrator. The database administrator gives other users permission to access the 
 tables, using the SQL security scheme described in 
 Chapter 15
 . Recall, however, that you 
 must use qualified table names to refer to another user's tables. In practice, this means 
 that 
 every
  query against the major tables in Figure 13-5 must use qualified table names, 
 which makes queries like the following one long and tedious to type:
  
  
 Figure 13-5: 
 Typical organization of a production database
  
 List the name, sales, office, and office sales for everyone.
  
 SELECT NAME, OP_ADMIN.SALESREPS.SALES, OFFICE, 
 OP_ADMIN.OFFICES.SALES
  
  
  FROM OP_ADMIN.SALESREPS, OP_ADMIN.OFFICES
  
 To address this problem, many SQL DBMS products provide an 
 alias
  or 
 synonym 
 capability. A synonym is a name that you define that stands for the name of some other 
 table. In DB2, you create an alias using the 
 CREATE ALIAS
  statement. (Older versions 
 of DB2 actually used a 
 CREATE SYNONYM
  statement, and Oracle still uses this form of 
 the statement, but it has the same effect as the 
 CREATE ALIAS
  statement.) If you were 
  
 - 272 -",NA
Indexes (,NA,NA
CREATE/DROP INDEX,NA,NA
),"One of the physical storage structures that is provided by most SQL-based database 
 management systems is an 
 index.
  An index is a structure that provides rapid access to 
 the rows of a table based on the values of one or more columns. Figure 13-6 shows the 
 PRODUCTS
  table and two indexes that have been created for it. One of the indexes 
 provides access based on the 
 DESCRIPTION
  column. The other provides access based 
 on the primary key of the table, which is a combination of the MFR_ID and 
 PRODUCT_ID 
 columns.
  
 - 273 -",NA
Managing Other Database Objects,"The 
 CREATE
 , 
 DROP
 , and 
 ALTER
  verbs form the cornerstone of the SQL Data Definition 
 Language. Statements based on these verbs are used in all SQL implementations to 
 manipulate tables, indexes, and views (described in 
 Chapter 14
 ). Most of the popular 
 SQL-based DBMS products also use these verbs to form additional DDL statements that 
 create, destroy, and modify other database objects unique to that particular brand of 
 DBMS.
  
 The Sybase DBMS, for example, pioneered the use of triggers and stored procedures, 
 which are treated as ""objects"" within a SQL database, along with its tables, assertions, 
 indexes, and other structures. Sybase added the 
 CREATE TRIGGER
  and 
 CREATE 
 PROCEDURE
  statements to its SQL dialect to define these new database structures, and 
 the corresponding 
 DROP
  statements to delete them when no longer needed. As these 
 features became popular, other DBMS products added the capabilities, along with their 
 own variants of the 
 CREATE TRIGGER
  and 
 CREATE PROCEDURE
  statements.
  
 The common conventions across DBMS brands is (a) the use of the 
 CREATE
 / 
  
 DROP
 /
 ALTER
  verbs, (b) the next word in the statement is the type of object being 
 managed, and (c) the third word is the name of the object, which must obey SQL naming 
 conventions. Beyond the first three words, the statements become very DBMS-specific 
 and nonstandard. Nonetheless, this commonality gives a uniform feel to the various SQL 
 dialects. At the very least, it tells you where to look in the reference manual for a 
 description of a new capability. If you encounter a new SQL-based DBMS and know that it 
 supports an object known as a 
 BLOB
 , the odds are that it uses 
 CREATE BLOB
 , 
 DROP 
 BLOB
 , and 
 ALTER BLOB
  statements. Table 13-1 shows how some of the popular SQL 
 products use the 
 CREATE
 , 
 DROP
 , and 
 ALTER
  verbs in their expanded DDL. The SQL2 
 standard adopts this same convention to deal with the creation, destruction, and 
 modification of all ""objects"" in an SQL2 database.
  
 Table 13-1: DDL Statements in Popular SQL-Based Products
  
 SQL DDL Statements
  
 Managed Object
  
 Supported by almost all DBMS brands
  
 CREATE/DROP/ALTER TABLE 
  
 Table
  
 CREATE/DROP/ALTER VIEW 
  
 View
  
 - 276 -",NA
Database Structure,"Conversion between character sets
  
 The SQL1 standard specified a simple structure for the contents of a database, shown in 
  
 - 279 -",NA
Single-Database Architecture,"Figure 13-9 shows a single-database architecture where the DBMS supports one system-
 wide database. Mainframe and minicomputer databases (such as the mainframe version 
 of DB2 and Oracle) have historically tended to use this approach. Order processing, 
 accounting, and payroll data are all stored in tables within the database. The major tables 
 for each application are gathered together and owned by a single user, who is probably 
 the person in charge of that application on this computer.
  
  
 Figure 13-9: 
 A single-database architecture
  
 An advantage of this architecture is that the tables in the various applications can easily 
 reference one another. The 
 TIMECARDS
  table of the payroll application, for example, can 
 contain a foreign key that references the 
 OFFICES
  table, and the applications can use that 
 relationship to calculate commissions. With proper permission, users can run queries that 
 combine data from the various applications.
  
 - 280 -",NA
Multi-Database Architecture,"Figure 13-10 shows a multi-database architecture where each database is assigned a 
 unique name. Sybase Adaptive Server, Microsoft SQL Server, Ingres, and others use this 
 scheme. As shown in the figure, each of the databases in this architecture is usually 
 dedicated to a particular application. When you add a new application, you will probably 
 create a new database.
  
  
 Figure 13-10: 
 A multi-database architecture
  
 The main advantage of the multi-database architecture over the single-database 
 architecture is that it divides the data management tasks into smaller, more manageable 
 pieces. Each person responsible for an application can now be the database 
  
 administrator of their own database, with less worry about overall coordination. When it's 
 time to add a new application, it can be developed in its own database, without disturbing 
 the existing databases. It's also more likely that users and programmers can remember 
 the overall structure of their own databases.
  
 The main disadvantage of the multi-database architecture is that the individual databases 
 may become ""islands"" of information, unconnected to one another. Typically a table in one 
 database cannot contain a foreign key reference to a table in a different database. 
  
 Often the DBMS does not support queries across database boundaries, making it 
  
 - 281 -",NA
Multi-Location Architecture,"Figure 13-11 shows a multi-location architecture that supports multiple databases and 
 uses the computer system's directory structure to organize them. Several of the earlier 
 minicomputer databases (including Rdb/VMS and Informix) used this scheme for 
 supporting multiple databases. As with the multi-database architecture, each application 
 is typically assigned to its own database. As the figure shows, each database has a 
 name, but it's possible for two different databases in two different directories to have the 
 same name.
  
 - 282 -",NA
Database Structure and the ANSI/ISO Standard,"The ANSI/ISO SQL1 standard made a very strong distinction between the SQL Data 
 Manipulation Language and Data Definition Language, defining them effectively as two 
 separate languages. The standard did not require that the DDL statements be accepted by 
 the DBMS during its normal operation. One of the advantages of this separation of the 
 DML and DDL was that the standard permitted a static database structure like that used 
 by older hierarchical and network DBMS products, as shown in Figure 13-12.
  
  
 Figure 13-12: 
 A DBMS with static DDL
  
 The database structure specified by the SQL1 standard was fairly straightforward. 
 Collections of tables were defined in a 
 database schema
 , associated with a specific user. 
 In Figure 13-12, the simple database has two schemas. One schema is associated with 
 (the common terminology is ""owned by"") a user named Joe, and the other is owned by 
 Mary. Joe's schema contains two tables, named 
 PEOPLE
  and 
 PLACES
 . Mary's schema 
 also contains two tables, named 
 THINGS
  and 
 PLACES
 . Although the database contains 
 two tables named 
 PLACES
 , it's possible to tell them apart because they have different 
 owners.
  
 The SQL2 standard significantly extended the SQL1 notion of database definition and 
 database schemas. As previously noted, the SQL2 standard requires that data definition 
 statements be executable by an interactive SQL user or by a SQL program. With this 
 capability, changes to the database structure can be made at any time, not just when the 
 database is created. In addition, the SQL1 concepts of schemas and users (officially 
 called ""authorization-ids"" in the standard) is significantly expanded. Figure 13-13 shows 
 the high-level database structure specified by the SQL2 standard.
  
 - 284 -",NA
SQL2 Catalogs,"Within a SQL-environment, the database structure is defined by one or more named 
 catalogs
 . The word ""catalog"" in this case is used in the same way that it has historically 
 been used on mainframe systems—to describe a collection of objects (usually files). On 
 minicomputer and personal computer systems, the concept is roughly analogous to a 
 ""directory."" In the case of a SQL2 database, the catalog is a collection of named 
 database schemas. The catalog also contains a set of system tables (confusingly, often 
 called the ""system catalog"") that describe the structure of the database. The catalog is 
 thus a self-describing entity within the database. This characteristic of SQL2 catalogs 
 (which is provided by all major SQL products) is described in detail in 
 Chapter 16
 .
  
 The SQL2 standard describes the role of the catalog and specifies that a SQL-
  
 environment may contain one or more (actually zero or more) catalogs, each of which 
 must have a distinct name. It explicitly says that the mechanism for creating and 
  
 destroying catalogs is implementation-defined. The standard also says that the extent to 
 which a DBMS allows access ""across catalogs"" is implementation-defined. Specifically, 
 whether a single SQL statement can access data from multiple catalogs, whether a single 
 SQL transaction can span multiple catalogs, or even whether a single user session with 
  
 - 285 -",NA
SQL2 Schemas,"The SQL2 
 schema
  is the key high-level ""container"" for objects in a SQL2 database 
 structure. A schema is a named entity within the database and includes the definitions 
 for:
  
 •
 Tables
 , as described earlier in this chapter, along with their associated structures 
 (columns, primary and foreign keys, table constraints, and so on).
  
 •
 Views
 , which are ""virtual tables"" derived from the ""real tables"" in the database, as 
 described in the 
 next chapter
 .
  
 •
 Domains
 , which function like extended data types for defining columns within the 
 tables of the schema, as described earlier in this chapter.
  
 •
 Assertions
 , which are database integrity constraints that restrict the data relationships 
 across tables within the schema, as described earlier in this chapter.
  
 •
 Privileges
 , which control the capabilities that are given to various users to access and 
 update data in the database and to modify the database structure. The SQL security 
 scheme created by these privileges is described in the 
 next chapter
 .
  
 •
 Character sets
 , which are database structures used to support international languages 
 and manage the representation of non-Roman characters in those character sets (for 
 example, the diacritical ""accent"" marks used by many European languages or the two-byte 
 representations of the word-symbols used in many Asian languages).
  
 •
 Collations
 , which define the sorting sequence for a character set.
  
 •
 Translations
 , which control how text data is converted from one character set to 
 another and how comparisons are made of text data from different character sets.
  
 A schema is created with the 
 CREATE SCHEMA
  statement, shown in Figure 13-14. Here 
 is a simple SQL2 schema definition for the simple two-table schema for the user Joe 
 shown in 
 Figure 13-12
 :
  
  
 Figure 13-14: 
 CREATE SCHEMA
  statement syntax diagram
  
 - 286 -",NA
Summary,"This chapter described the SQL Data Definition Language features that define and 
 change the structure of a database:
  
 •The 
 CREATE TABLE
  statement creates a table and defines its columns, primary key, 
 and foreign keys.
  
 •The 
 DROP TABLE
  statement removes a previously created table from the database.
  
 •The 
 ALTER TABLE
  statement can be used to add a column to an existing table and to 
 change primary key and foreign key definitions.
  
 •The 
 CREATE INDEX
  and 
 DROP INDEX
  statements define indexes, which speed 
 database queries but add overhead to database updates.
  
 •Most DBMS brands support other 
 CREATE
 , 
 DROP
 , and 
 ALTER
  statements used with 
 DBMS-specific objects.
  
 •The SQL2 standard specifies a database schema containing a collection of tables, and 
 the database schema is manipulated with 
 CREATE SCHEMA
  and 
 DROP SCHEMA 
 statements 
  
 - 288 -",NA
Chapter 14: ,NA,NA
Views,NA,NA
Overview,"The tables of a database define the structure and organization of its data. However, SQL 
 also lets you look at the stored data in other ways by defining alternative views of the data. 
 A 
 view
  is a SQL query that is permanently stored in the database and assigned a name. 
 The results of the stored query are ""visible"" through the view, and SQL lets you access 
 these query results as if they were, in fact, a ""real"" table in the database.
  
 Views are an important part of SQL, for several reasons:
  
 •Views let you tailor the appearance of a database so that different users see it from 
 different perspectives.
  
 •Views let you restrict access to data, allowing different users to see only certain rows or 
 certain columns of a table.
  
 •Views simplify database access by presenting the structure of the stored data in the 
 way that is most natural for each user.
  
 This chapter describes how to create views and how to use views to simplify processing 
 and enhance the security of a database.",NA
What Is a View?,"A 
 view
  is a ""virtual table"" in the database whose contents are defined by a query, as 
 shown in Figure 14-1. To the database user, the view appears just like a real table, with a 
 set of named columns and rows of data. But unlike a real table, a view does not exist in 
 the database as a stored set of data values. Instead, the rows and columns of data visible 
 through the view are the query results produced by the query that defines the view. SQL 
 creates the illusion of the view by giving the view a name like a table name and storing the 
 definition of the view in the database.
  
  
 Figure 14-1: 
 A typical view with two source tables
  
 The view shown in Figure 14-1 is typical. It has been given the name 
 REPDATA
  and is 
 defined by this two-table query:
  
 SELECT NAME, CITY, REGION, QUOTA, SALESREPS.SALES
  
  FROM SALESREPS, OFFICES
  
 WHERE REP_OFFICE = OFFICE
  
 - 290 -",NA
How the DBMS Handles Views,"When the DBMS encounters a reference to a view in a SQL statement, it finds the 
 definition of the view stored in the database. Then the DBMS translates the request that 
 references the view into an 
 equivalent
  request against the source tables of the view and 
 carries out the equivalent request. In this way the DBMS maintains the illusion of the view 
 while maintaining the integrity of the source tables.
  
 For simple views, the DBMS may construct each row of the view ""on the fly,"" drawing the 
 data for the row from the source table(s). For more complex views, the DBMS must 
 actually 
 materialize
  the view; that is, the DBMS must actually carry out the query that 
 defines the view and store its results in a temporary table. The DBMS fills your requests 
 for view access from this temporary table and discards the table when it is no longer 
 needed. Regardless of how the DBMS actually handles a particular view, the result is the 
 same for the user—the view can be referenced in SQL statements exactly as if it were a 
 real table in the database.",NA
Advantages of Views,"Views provide a variety of benefits and can be useful in many different types of 
  
 databases. In a personal computer database, views are usually a convenience, defined to 
 simplify database requests. In a production database installation, views play a central role 
 in defining the structure of the database for its users and enforcing its security. Views 
  
 - 291 -",NA
Disadvantages of Views,"While views provide substantial advantages, there are also two major disadvantages to 
 using a view instead of a real table:
  
 •
 Performance
 . Views create the 
 appearance
  of a table, but the DBMS must still translate 
 queries against the view into queries against the underlying source tables. If the view is 
 defined by a complex, multi-table query, then even a simple query against the view 
 becomes a complicated join, and it may take a long time to complete.
  
 •
 Updaterestrictions
 . When a user tries to update rows of a view, the DBMS must 
 translate the request into an update on rows of the underlying source tables. This is 
 possible for simple views, but more complex views cannot be updated; they are ""read-
 only.""
  
 These disadvantages mean that you cannot indiscriminately define views and use them 
 instead of the source tables. Instead, you must in each case consider the advantages 
 provided by using a view and weigh them against the disadvantages.",NA
Creating a View (,NA,NA
CREATE VIEW,NA,NA
),"The 
 CREATE VIEW
  statement, shown in Figure 14-2, is used to create a view. The 
 statement assigns a name to the view and specifies the query that defines the view. To 
 create the view successfully, you must have permission to access all of the tables 
 referenced in the query.
  
  
 Figure 14-2: 
 CREATE VIEW
  statement syntax diagram
  
 The 
 CREATE VIEW
  statement can optionally assign a name to each column in the newly 
 created view. If a list of column names is specified, it must have the same number of items 
 as the number of columns produced by the query. Note that only the column names are 
 specified; the data type, length, and other characteristics of each column are derived from 
 the definition of the columns in the source tables. If the list of column names is omitted 
 from the 
 CREATE VIEW
  statement, each column in the view takes the name of 
  
 - 292 -",NA
Horizontal Views,"A common use of views is to restrict a user's access to only selected rows of a table. For 
 example, in the sample database, you may want to let a sales manager see only the 
 SALESREPS
  rows for salespeople in the manager's own region. To accomplish this, you 
 can define two views, as follows:
  
 Create a view showing Eastern region salespeople.
  
 CREATE VIEW EASTREPS AS
  
  
  SELECT *
  
  
   
  FROM SALESREPS
  
  
  
  WHERE REP_OFFICE IN (11, 12, 13)
  
 Create a view showing Western region salespeople.
  
 CREATE VIEW WESTREPS AS
  
  
  SELECT *
  
  
   
  FROM SALESREPS
  
  
  
  WHERE REP_OFFICE IN (21, 22)
  
 Now you can give each sales manager permission to access either the 
 EASTREPS
  or the 
 WESTREPS
  view, denying them permission to access the other view and the 
 SALESREPS 
 table itself. This effectively gives the sales manager a customized view of the 
  
 SALESREPS
  table, showing only salespeople in the appropriate region.
  
 A view like 
 EASTREPS
  or 
 WESTREPS
  is often called a 
 horizontal view.
  As shown in Figure 
 14-3, a horizontal view ""slices"" the source table horizontally to create the view. All of the 
 columns of the source table participate in the view, but only some of its rows are visible 
 through the view. Horizontal views are appropriate when the source table contains data 
 that relates to various organizations or users. They provide a ""private table"" for each user, 
 composed only of the rows needed by that user.
  
  
 Figure 14-3: 
 Two horizontal views of the 
 SALESREPS
  table
  
 Here are some more examples of horizontal views:
  
 Define a view containing only Eastern region offices.
  
 - 293 -",NA
Vertical Views,"Another common use of views is to restrict a user's access to only certain columns of a 
 table. For example, in the sample database, the order processing department may need 
 access to the employee number, name, and office assignment of each salesperson, 
 because this information may be needed to process an order correctly. However, there is 
 no need for the order processing staff to see the salesperson's year-to-date sales or 
 quota. This selective view of the 
 SALESREPS
  table can be constructed with the following 
 view:
  
 Create a view showing selected salesperson information.
  
 CREATE VIEW REPINFO AS
  
  
  SELECT EMPL_NUM, NAME, REP_OFFICE
  
  
  
  FROM SALESREPS
  
 By giving the order processing staff access to this view and denying access to the 
 SALESREPS
  table itself, access to sensitive sales and quota data is effectively restricted.
  
 A view like the 
 REPINFO
  view is often called a 
 verticalview.
  As shown in Figure 14-4, a 
 vertical view ""slices"" the source table vertically to create the view. Vertical views are 
 commonly found where the data stored in a table is used by various users or groups of 
 users. They provide a ""private table"" for each user, composed only of the columns 
 needed by that user.
  
 - 294 -",NA
Row/Column Subset Views,"When you define a view, SQL does not restrict you to purely horizontal or vertical slices of 
 a table. In fact, the SQL language does not include the notion of horizontal and vertical 
 views. These concepts merely help you to visualize how the view presents the 
  
 information from the source table. It's quite common to define a view that slices a source 
 table in 
 both
  the horizontal and vertical dimensions, as in this example:
  
 Define a view that contains the customer number, company name, and credit limit of all 
 customers assigned to Bill Adams (employee number 105).
  
 CREATE VIEW BILLCUST AS
  
  SELECT CUST_NUM, COMPANY, CREDIT_LIMIT
  
  FROM CUSTOMERS
  
  WHERE CUST_REP = 105
  
 The data visible through this view is a row/column subset of the 
 CUSTOMERS
  table. Only 
 the columns explicitly named in the select list of the view and the rows that meet the 
 search condition are visible through the view.
  
 - 295 -",NA
Grouped Views,"The query specified in a view definition may include a 
 GROUP BY
  clause. This type of view 
 is called a 
 grouped view,
  because the data visible through the view is the result of a 
 grouped query. Grouped views perform the same function as grouped queries; they group 
 related rows of data and produce one row of query results for each group, summarizing 
 the data in that group. A grouped view makes these grouped query results into a virtual 
 table, allowing you to perform further queries on them.
  
 Here is an example of a grouped view:
  
 Define a view that contains summary order data for each salesperson.
  
 CREATE VIEW ORD_BY_REP (WHO, HOW_MANY, TOTAL, LOW, HIGH, AVERAGE) 
 AS
  
  
  SELECT REP, COUNT(*), SUM(AMOUNT), MIN(AMOUNT), MAX(AMOUNT),
  
  AVG(AMOUNT)
  
  FROM ORDERS
  
  GROUP BY REP
  
 As this example shows, the definition of a grouped view always includes a column name 
 list. The list assigns names to the columns in the grouped view, which are derived from 
 column functions such as 
 SUM()
  and 
 MIN()
 . It may also specify a modified name for a 
 grouping column. In this example, the 
 REP
  column of the 
 ORDERS
  table becomes the 
 WHO 
 column in the 
 ORD_BY_REP
  view.
  
 Once this grouped view is defined, it can be used to simplify queries. For example, this 
 query generates a simple report that summarizes the orders for each salesperson:
  
 Show the name, number of orders, total order amount, and average order size for each 
 salesperson.
  
 SELECT NAME, HOW_MANY, TOTAL, AVERAGE
  
  FROM SALESREPS, ORD_BY_REP
  
 WHERE WHO = EMPL_NUM
  
 ORDER BY TOTAL DESC
  
 NAME            HOW_MANY       TOTAL     AVERAGE
  
 --------------  --------  ----------  ----------
  
 Larry Fitch            7  $58,633.00   $8,376.14
  
 Bill Adams             5  $39,327.00   $7,865.40
  
 Nancy Angelli          3  $34,432.00  $11,477.33
  
 Sam Clark              2  $32,958.00  $16,479.00
  
 Dan Roberts            3  $26,628.00   $8,876.00
  
 Tom Snyder             2  $23,132.00  $11,566.00
  
 Sue Smith              4  $22,776.00   $5,694.00
  
 Mary Jones             2   $7,105.00   $3,552.50
  
 Paul Cruz              2   $2,700.00   $1,350.00
  
 Unlike a horizontal or vertical view, the rows in a grouped view do not have a one-to-one 
 correspondence with the rows in the source table. A grouped view is not just a filter on its 
 source table that screens out certain rows and columns. It is a summary of the source 
 tables, and therefore a substantial amount of DBMS processing is required to maintain the 
 illusion of a virtual table for grouped views.
  
 Grouped views can be used in queries just like other, simpler views. A grouped view 
  
 - 296 -",NA
Joined Views,"One of the most frequent reasons for using views is to simplify multi-table queries. By 
 specifying a two-table or three-table query in the view definition, you can create a 
 joined 
 view
  that draws its data from two or three different tables and presents the query results 
 as a single virtual table. Once the view is defined, you can often use a simple, single-table 
 query against the view for requests that would otherwise each require a two-table or three-
 table join.
  
 For example, suppose that Sam Clark, the vice president of sales, often runs queries 
 against the 
 ORDERS
  table in the sample database. However, Sam doesn't like to work 
 with customer and employee numbers. Instead, he'd like to be able to use a version of 
  
 - 297 -",NA
Updating a View,"What does it mean to insert a row of data into a view, delete a row from a view, or update 
 a row of a view? For some views these operations can obviously be translated into 
 equivalent operations against the source table(s) of the view. For example, consider once 
 again the 
 EASTREPS
  view, defined earlier in this chapter:
  
 Create a view showing Eastern region salespeople.
  
 CREATE VIEW EASTREPS AS
  
  
  SELECT *
  
  
   
  FROM SALESREPS
  
  
  
  WHERE REP_OFFICE IN (11, 12, 13)
  
 This is a straightforward horizontal view, derived from a single source table. As shown in 
 Figure 14-5, it makes sense to talk about inserting a row into this view; it means the new 
 row should be inserted into the underlying 
 SALESREPS
  table from which the view is 
 derived. It also makes sense to delete a row from the 
 EASTREPS
  view; this would delete 
 the corresponding row from the 
 SALESREPS
  table. Finally, updating a row of the 
 EASTREPS
  view makes sense; this would update the corresponding row of the 
  
 SALESREPS
  table. In each case the action can be carried out against the corresponding 
 row of the source table, preserving the integrity of both the source table and the view.
  
  
 Figure 14-5: 
 Updating data through a view
  
 However, consider the 
 ORDS_BY_REP
  grouped view, also defined earlier in this chapter:
  
 - 299 -",NA
View Updates and the ANSI/ISO Standard,"The ANSI/ISO SQL1 standard specifies the views that must be updateable in a database 
 that claims conformance to the standard. Under the standard, a view can be updated if the 
 query that defines the view meets all of these restrictions:
  
 •
 DISTINCT
  must not be specified; that is, duplicate rows must 
 not
  be eliminated from 
 the query results.
  
 •
  
 The 
 FROM
  clause must specify only one updateable table; that is, the view must have a 
 single source table for which the user has the required privileges. If the source table is 
  
 itself a view, then that view must meet these criteria.
  
 •Each select item must be a simple column reference; the select list cannot contain 
 expressions, calculated columns, or column functions.
  
 •The 
 WHERE
  clause must not include a subquery; only simple row-by-row search 
 conditions may appear.
  
 •The query must not include a 
 GROUP BY
  or a 
 HAVING
  clause.
  
 The basic concept behind the restrictions is easier to remember than the rules 
 themselves:
  
 For a view to be updateable, the DBMS must be able to trace any row of the view 
 back to its source row in the source table. Similarly, the DBMS must be able to trace 
 each individual column to be updated back to its source column in the source table.
  
 If the view meets this test, then it's possible to define meaningful 
 INSERT
 , 
 DELETE
 , and 
 UPDATE
  operations for the view in terms of the source table(s).",NA
View Updates in Commercial SQL Products,"The SQL1 standard rules on view updates are very restrictive. Many views can be 
 theoretically updated but do not satisfy all of the restrictions. In addition, some views can 
  
 - 300 -",NA
Checking View Updates (,NA,NA
CHECK OPTION,NA,NA
),"If a view is defined by a query that includes a 
 WHERE
  clause, only rows that meet the 
 search condition are visible in the view. Other rows may be present in the source table(s) 
 from which the view is derived, but they are not visible through the view. For example, the 
 EASTREPS
  view described earlier in this chapter contains only those rows of the 
  
 SALESREPS
  table with specific values in the 
 REP_OFFICE
  column:
  
 Create a view showing Eastern region salespeople.
  
 CREATE VIEW EASTREPS AS
  
  SELECT *
  
  FROM SALESREPS
  
  WHERE REP_OFFICE IN (11, 12, 13)
  
 This is an updateable view for most commercial SQL implementations. You can add a 
 new salesperson with this 
 INSERT
  statement:
  
 INSERT INTO EASTREPS (EMPL_NUM, NAME, REP_OFFICE, AGE, SALES)
  
  VALUES (113, 'Jake Kimball', 11, 43, 0.00)
  
 The DBMS will add the new row to the underlying 
 SALESREPS
  table, and the row will be 
 visible through the 
 EASTREPS
  view. But consider what happens when you add a new 
 salesperson with this 
 INSERT
  statement:
  
 INSERT INTO EASTREPS (EMPL_NUM, NAME, REP_OFFICE, AGE, SALES)
  
 - 301 -",NA
Dropping a View (,NA,NA
DROP VIEW,NA,NA
),"Recall that the SQL1 standard treated the SQL Data Definition Language (DDL) as a static 
 specification of the structure of a database, including its tables and views. For this reason, 
 the SQL1 standard did not provide the ability to drop a view when it was no longer 
 needed. However, all major DBMS brands have provided this capability for some time. 
 Because views behave like tables and a view cannot have the same name as a table, 
 some DBMS brands used the 
 DROP TABLE
  statement to drop views as well. Other SQL 
 implementations provided a separate 
 DROP VIEW
  statement. 
  
 The SQL2 standard formalized support for dropping views through a 
 DROP VIEW 
 statement. It also provides for detailed control over what happens when a user attempts 
 to drop a view when the definition of another view depends on it. For example, suppose 
 two views on the 
 SALESREPS
  table have been created by these two 
 CREATE VIEW 
 statements:
  
 CREATE VIEW EASTREPS AS
  
  SELECT *
  
  FROM SALESREPS
  
  WHERE REP_OFFICE IN (11, 12, 13)
  
 CREATE VIEW NYREPS AS
  
 - 303 -",NA
Summary,"Views allow you to redefine the structure of a database, giving each user a personalized 
 view of the database structure and contents:
  
 •A view is a virtual table defined by a query. The view appears to contain rows and 
 columns of data, just like a ""real"" table, but the data visible through the view is, in fact, the 
 results of the query.
  
 •A view can be a simple row/column subset of a single table, it can summarize a table (a 
 grouped view), or it can draw its data from two or more tables (a joined view).
  
 •
  
 A view can be referenced like a real table in a 
 SELECT
 , 
 INSERT
 , 
 DELETE
 , or 
 UPDATE 
 statement. However, more complex views cannot be updated; they are read-only 
  
 views.
  
 •Views are commonly used to simplify the apparent structure of a database, to simplify 
 queries, and to protect certain rows and/or columns from unauthorized access.",NA
Chapter 15: ,NA,NA
SQL Security,NA,NA
Overview,"When you entrust your data to a database management system, the security of the 
 stored data is a major concern. Security is especially important in a SQL-based DBMS 
 because interactive SQL makes database access very easy. The security requirements 
 of a typical production database are many and varied:
  
 •The data in any given table should be accessible to some users, but access by other 
 users should be prevented.
  
 •Some users should be allowed to update data in a particular table; others should only 
  
 - 304 -",NA
SQL Security Concepts,"Implementing a security scheme and enforcing security restrictions are the responsibility 
 of the DBMS software. The SQL language defines an overall framework for database 
 security, and SQL statements are used to specify security restrictions. The SQL security 
 scheme is based on three central concepts:
  
 •
 Users
  are the actors in the database. Each time the DBMS retrieves, inserts, deletes, or 
 updates data, it does so on behalf of some user. The DBMS permits or prohibits the action 
 depending on which user is making the request.
  
 •
 Databaseobjects
  are the items to which SQL security protection can be applied. 
  
 Security is usually applied to tables and views, but other objects such as forms, 
 application programs, and entire databases can also be protected. Most users will 
 have permission to use certain database objects but will be prohibited from using 
 others.
  
 •
 Privileges
  are the actions that a user is permitted to carry out for a given database 
 object. A user may have permission to 
 SELECT
  and 
 INSERT
  rows in a certain table, for 
 example, but may lack permission to 
 DELETE
  or 
 UPDATE
  rows of the table. A different 
 user may have a different set of privileges.
  
 Figure 15-1 shows how these security concepts might be used in a security scheme for 
 the sample database.
  
  
 Figure 15-1: 
 A security scheme for the sample database
  
 To establish a security scheme for a database, you use the SQL 
 GRANT
  statement to 
 specify which users have which privileges on which database objects. For example, here 
 is a 
 GRANT
  statement that lets Sam Clark retrieve and insert data in the 
 OFFICES
  table of 
 the sample database:
  
 - 305 -",NA
User-Ids,"Each user of a SQL-based database is typically assigned a 
 user-id
 , a short name that 
 identifies the user to the DBMS software. The user-id is at the heart of SQL security. Every 
 SQL statement executed by the DBMS is carried out on behalf of a specific user-id. 
  
 The user-id determines whether the statement will be permitted or pro-hibited by the 
 DBMS. In a production database, user-ids are assigned by the database administrator. A 
 personal computer database may have only a single user-id, identifying the user who 
 created and who owns the database. In special purpose databases (for example, those 
 designed to be embedded within an appli-cation or a special purpose system), there may 
 be no need for the additionaloverhead associated with SQL security. These databases 
 typically operate as if there were a single user-id.
  
 In practice, the restrictions on the names that can be chosen as user-ids vary from 
 implementation to implementation. The SQL1 standard permitted user-ids of up to 18 
 characters and required them to be valid SQL names. In some mainframe DBMS systems, 
 user-ids may have no more than eight characters. In Sybase and SQL Server, user-ids 
 may have up to 30 characters. If portability is a concern, it's best to limit user-ids to eight 
 or fewer characters. Figure 15-2 shows various users who need access to the sample 
 database and typical user-ids assigned to them. Note that all of the users in the order 
 processing department can be assigned the same user-id because they are to have 
 identical privileges in the database.
  
  
 Figure 15-2: 
 User-id assignments for the sample database
  
 The ANSI/ISO SQL standard uses the term 
 authorization-id
  instead of user-id, and you 
  
 - 306 -",NA
Security Objects,"SQL security protections apply to specific 
 objects
  contained in a database. The SQL1 
 standard specified two types of security objects—tables and views. Thus each table and 
 view can be individually protected. Access to a table or view can be permitted for certain 
 user-ids and prohibited for other user-ids. The SQL2 standard expanded security 
 protections to include other objects, including domains and user-defined character sets, 
 and added a new type of protection for table or view access.
  
 Most commercial SQL products support additional security objects. In a SQL Server 
 database, for example, a stored procedure is an important database object. The SQL 
 security scheme determines which users can create and drop stored procedures and 
 which users are allowed to execute them. In IBM's DB2, the physical tablespaces where 
 tables are stored are treated as security objects. The database administrator can give 
 some user-ids permission to create new tables in a particular tablespace and deny that 
 permission to other user-ids. Other SQL implementations support other security objects. 
 However, the underlying SQL security scheme—of specific privileges applied to specific 
 objects, granted or revoked through the same SQL statements—is almost universally 
 applied.",NA
Privileges,"The set of actions that a user can carry out against a database object are called the 
 privileges
  for the object. The SQL1 standard specified four basic privileges for tables and 
 views:
  
 •The 
 SELECT
  privilege allows you to retrieve data from a table or view. With this 
 privilege, you can specify the table or view in the 
 FROM
  clause of a 
 SELECT
  statement or 
 subquery.
  
 •The 
 INSERT
  privilege allows you to insert new rows into a table or view. With this 
 privilege, you can specify the table or view in the 
 INTO
  clause of an 
 INSERT 
 statement.
  
 •The 
 DELETE
  privilege allows you to delete rows of data from a table or view. With this 
 privilege, you can specify the table or view in the 
 FROM
  clause of a 
 DELETE
  statement.
  
 •The 
 UPDATE
  privilege allows you to modify rows of data in a table or view. With this 
 privilege, you can specify the table or view as the target table in an 
 UPDATE
  statement. 
  
 The 
 UPDATE
  privilege can be restricted to specific columns of the table or view, 
 allowing updates to these columns but disallowing updates to any other columns.
  
 These four privileges are supported by virtually all commercial SQL products.
  
 SQL2 Extended Privileges
  
 The SQL2 standard expanded the basic SQL1 privileges in several dimensions. It added 
 new capabilities to the SQL1 
 INSERT
  and 
 UPDATE
  privileges. It added a new 
  
 REFERENCES
  privilege that restricts a user's ability to create a reference to a table from a 
 foreign key in another table. It also added a new 
 USAGE
  privilege that controls access to 
 the new SQL2 database structures of domains, character sets, collation sequences, and 
 translations.
  
 - 309 -",NA
Views and SQL Security,"In addition to the restrictions on table access provided by the SQL privileges, views also 
 play a key role in SQL security. By carefully defining a view and giving a user permission 
 to access the view but not its source tables, you can effectively restrict the user's access 
 to only selected columns and rows. Views thus offer a way to exercise very precise 
  
 - 311 -",NA
Granting Privileges (,NA,NA
GRANT,NA,NA
),"The basic 
 GRANT
  statement, shown in Figure 15-5, is used to grant security privileges on 
 database objects to specific users. Normally the 
 GRANT
  statement is used by the owner of 
 a table or view to give other users access to the data. As shown in the figure, the 
 GRANT
  
 statement includes a specific list of the privileges to be granted, the name of the table to 
 which the privileges apply, and the user-id to which the privileges are granted.
  
  
 Figure 15-5: 
 GRANT
  statement syntax diagram
  
 The 
 GRANT
  statement shown in the syntax diagram conforms to the ANSI/ISO SQL 
 standard. Many DBMS brands follow the DB2 
 GRANT
  statement syntax, which is more 
 flexible. The DB2 syntax allows you to specify a list of user-ids and a list of tables, 
 making it simpler to grant many privileges at once. Here are some examples of simple 
 GRANT
  statements for the sample database:
  
 Give order processing users full access to the 
 ORDERS
  table.
  
 GRANT SELECT, INSERT, DELETE, UPDATE
  
  ON ORDERS
  
  TO OPUSER
  
 Let accounts receivable users retrieve customer data and add new customers to the 
 CUSTOMERS
  table, but give order processing users read-only access.
  
 GRANT SELECT, INSERT
  
 - 313 -",NA
Column Privileges,"The SQL1 standard allowed you to grant the 
 UPDATE
  privilege for individual columns of a 
 table or view, and the SQL2 standard allows a column list for 
 INSERT
  and 
 REFERENCES 
 privileges as well. The columns are listed after the 
 UPDATE
 , 
 INSERT
 , or 
 REFERENCES 
 keyword and enclosed in parentheses. Here is a 
 GRANT
  statement that allows the order 
 processing department to update only the company name and assigned salesperson 
 columns of the 
 CUSTOMERS
  table:
  
 Let order processing users change company names and salesperson assignments.
  
 GRANT UPDATE (COMPANY, CUST_REP)
  
  ON CUSTOMERS
  
  TO OPUSER
  
 - 314 -",NA
Passing Privileges (,NA,NA
GRANT OPTION,NA,NA
),"When you create a database object and become its owner, you are the only person who 
 can grant privileges to use the object. When you grant privileges to other users, they are 
 allowed to use the object, but they cannot pass those privileges on to other users. In this 
 way, the owner of an object maintains very tight control both over who has permission to 
 use the object and over what forms of access are allowed.
  
 Occasionally you may want to allow other users to grant privileges on an object that you 
 own. For example, consider again the 
 EASTREPS
  and 
 WESTREPS
  views in the sample 
 database. Sam Clark, the vice president of sales, created these views and owns them. 
  
 He can give the Los Angeles office manager, Larry Fitch, permission to use the 
 WESTREPS
  view with this 
 GRANT
  statement:
  
 GRANT SELECT
  
  ON WESTREPS
  
  TO LARRY
  
 What happens if Larry wants to give Sue Smith (user-id 
 SUE
 ) permission to access the 
 WESTREPS
  data because she is doing some sales forecasting for the Los Angeles office? 
  
 With the preceding 
 GRANT
  statement, he cannot give her the required privilege. Only 
 Sam Clark can grant the privilege, because he owns the view.
  
 If Sam wants to give Larry discretion over who may use the 
 WESTREPS
  view, he can use 
 this variation of the previous 
 GRANT
  statement:
  
 - 315 -",NA
Revoking Privileges (,NA,NA
REVOKE,NA,NA
),"In most SQL-based databases, the privileges that you have granted with the 
 GRANT 
 statement can be taken away with the 
 REVOKE
  statement, shown in Figure 15-7. The 
 REVOKE
  statement has a structure that closely parallels the 
 GRANT
  statement, specifying a 
 specific set of privileges to be taken away, for a specific database object, from one or 
 more user-ids.
  
  
 Figure 15-7: 
 REVOKE
  statement syntax diagram
  
 A 
 REVOKE
  statement may take away all or some of the privileges that you previously 
 granted to a user-id. For example, consider this statement sequence:
  
 Grant and then revoke some 
 SALESREPS
  table privileges.
  
 GRANT SELECT, INSERT, UPDATE
  
  ON SALESREPS
  
  TO ARUSER, OPUSER
  
 REVOKE INSERT, UPDATE
  
   
  ON SALESREPS
  
  
  FROM OPUSER
  
 The 
 INSERT
  and 
 UPDATE
  privileges on the 
 SALESREPS
  table are first given to the two 
 users and then revoked from one of them. However, the 
 SELECT
  privilege remains for 
 both user-ids. Here are some other examples of the 
 REVOKE
  statement:
  
 - 317 -",NA
REVOKE,NA,NA
 and the ,NA,NA
GRANT OPTION,"When you grant privileges with the 
 GRANT OPTION
  and later revoke these privileges, 
 most DBMS brands will 
 automatically
  revoke all privileges derived from the original grant. 
  
 Consider again the chain of privileges in 
 Figure 15-6
 , from Sam Clark, the sales vice 
 president, to Larry Fitch, the Los Angeles office manager, and then to Sue Smith. If Sam 
 now revokes Larry's privileges for the 
 WESTREPS
  view, Sue's privilege is automatically 
 revoked as well.
  
 The situation gets more complicated if two or more users have granted privileges and one 
 of them later revokes the privileges. Consider Figure 15-8, a slight variation on the last 
 example. Here Larry receives the 
 SELECT
  privilege with the 
 GRANT OPTION
  from both 
 Sam (the sales vice president) and George (the marketing vice president) and then grants 
 privileges to Sue. This time when Sam revokes Larry's privileges, the grant of privileges 
 from George remains. Furthermore, Sue's privileges also remain because they can be 
 derived from George's grant.
  
  
 Figure 15-8: 
 Revoking privileges granted by two users
  
 However, consider another variation on the chain of privileges, with the events slightly 
 rearranged, as shown in Figure 15-9. Here Larry receives the privilege with the 
 GRANT 
 OPTION
  from Sam, grants the privilege to Sue, and 
 then
  receives the grant, with the 
 GRANT OPTION
 , from George. This time when Sam revokes Larry's privileges, the results 
 are slightly different, and they may vary from one DBMS to another. As in Figure 15-8, 
 Larry retains the 
 SELECT
  privilege on the 
 WESTREPS
  view because the grant from George 
 is still intact. But in a DB2 or SQL/DS database, Sue automatically loses her 
 SELECT
  
 privilege on the table. Why? Because the grant from Larry to Sue was clearly derived from 
 the grant from Sam to Larry, which has just been revoked. It could not have been derived 
 from George's grant to Larry because that grant had not yet taken place when the grant 
 from Larry to Sue was made.
  
 - 319 -",NA
REVOKE,NA,NA
 and the ANSI/ISO Standard,"The SQL1 standard specified the 
 GRANT
  statement as part of the SQL Data Definition 
 Language (DDL). Recall from 
 Chapter 13
  that the SQL1 standard treated the DDL as a 
 separate, static definition of a database and did not require that the DBMS permit 
 dynamic changes to database structure. This approach applies to database security as 
 well. Under the SQL1 standard, accessibility to tables and views in the database is 
 determined by a series of 
 GRANT
  statements included in the database schema. There is 
 no mechanism for changing the security scheme once the database structure is defined. 
 The 
 REVOKE
  statement is therefore absent from the SQL1 standard, just as the DROP 
 TABLE statement is missing from the standard.
  
 Despite its absence from the SQL1 standard, the 
 REVOKE
  statement was provided by 
 virtually all commercial SQL-based DBMS products since their earliest versions. As with 
 the 
 DROP
  and 
 ALTER
  statements, the DB2 dialect of SQL has effectively set the standard 
 for the 
 REVOKE
  statement. The SQL2 standard includes a specification for the 
 REVOKE 
 statement based on the DB2 statement with some extensions. One of the extensions 
 gives the user more explicit control over how privileges are revoked when the privileges 
 have, in turn, been granted to others. The other provides a way to revoke the 
 GRANT 
 OPTION
  without revoking the privileges themselves.
  
 To specify how the DBMS should handle the revoking of privileges that have been in turn 
 granted to others, the SQL2 standard requires that a 
 CASCADE
  or 
 RESTRICT
  option be 
 specified in a 
 REVOKE
  statement. (A similar requirement applies to many of the DROP 
 statements in the SQL2 standard, as described in 
 Chapter 13
 .) Suppose that 
 SELECT 
 and 
 UPDATE
  privileges have previously been granted to Larry on the 
 ORDERS
  table, with 
 the 
 GRANT OPTION
 , and that Larry has further granted these options to Bill. Then this 
 REVOKE
  statement:
  
 REVOKE SELECT, UPDATE
  
  ON ORDERS
  
  FROM LARRY CASCADE
  
 revokes not only Larry's privileges, but Bill's as well. The effect of the 
 REVOKE
  statement 
  
 - 320 -",NA
Summary,"The SQL language is used to specify the security restrictions for a SQL-based database:
  
 •The SQL security scheme is built around privileges (permitted actions) that can be 
 granted on specific database objects (such as tables and views) to specific user-ids 
 (users or groups of users).
  
 •Views also play a key role in SQL security because they can be used to restrict access to 
 specific rows or specific columns of a table.
  
 •The 
 GRANT
  statement is used to grant privileges; privileges that you grant to a user 
 with the 
 GRANT OPTION
  can in turn be granted by that user to others.
  
 •The 
 REVOKE
  statement is used to revoke privileges previously granted with the 
 GRANT 
 statement.",NA
Chapter 16: ,NA,NA
The System Catalog,NA,NA
Overview,- 321 -,NA
What Is the System Catalog?,"The system catalog is a collection of special tables in a database that are owned, 
 created, and maintained by the DBMS itself. These 
 system tables
  contain data that 
 describes the structure of the database. The tables in the system catalog are 
  
 automatically created when the database is created. They are usually gathered under a 
 special ""system user-id"" with a name like 
 SYSTEM, SYSIBM, MASTER, 
 or
  DBA.
  
 The DBMS constantly refers to the data in the system catalog while processing SQL 
 statements. For example, to process a two-table 
 SELECT
  statement, the DBMS must:
  
 •Verify that the two named tables actually exist.
  
 •Ensure that the user has permission to access them.
  
 •Check whether the columns referenced in the query exist.
  
 •Resolve any unqualified column names to one of the tables.
  
 •Determine the data type of each column.
  
 By storing structural information in system tables, the DBMS can use its own access 
 methods and logic to rapidly and efficiently retrieve the information it needs to perform 
 these tasks.
  
 If the system tables were only used internally to the DBMS, they would be of little interest 
 to database users. However, the DBMS generally makes the system tables available for 
 user access as well. If the system tables themselves are not made available, the DBMS 
 generally provides views based on the system tables that offer a set of user-retrievable 
 catalog information. User queries against the system catalogs or views are almost always 
 permitted by personal computer and minicomputer databases. These queries are also 
 supported by mainframe DBMS products, but the database administrator may restrict 
 system catalog access to provide an additional measure of database security. By querying 
 the system catalogs, you can discover information about the structure of a database, even 
 if you have never used it before.
  
 User access to the system catalog is read-only. The DBMS prevents users from directly 
 updating or modifying the system tables because such modifications would destroy the 
 integrity of the database. Instead, the DBMS itself takes care of inserting, deleting, and 
  
 - 322 -",NA
The Catalog and Query Tools,"One of the most important benefits of the system catalog is that it makes possible user-
 friendly query tools, as shown in Figure 16-1. The objective of such a tool is to let users 
 simply and transparently access the database without learning the SQL language. 
  
 Typically, the tool leads the user through a series of steps like this one:
  
 1.The user gives a name and password for database access.
  
 2.The query tool displays a list of available tables.
  
 3.The user chooses a table, causing the query tool to display a list of the columns it 
 contains.
  
 4.The user chooses columns of interest, perhaps by clicking on their names as they 
 appear on a PC screen.
  
 5.The user chooses columns from other tables or restricts the data to be retrieved with a 
 search condition.
  
 6.The query tool retrieves the requested data and displays it on the user's screen.
  
 A general-purpose query tool like the one in Figure 16-1 will be used by many different 
 users, and it will be used to access many different databases. The tool cannot possibly 
 know in advance the structure of the database that it will access during any given 
 session. Thus, it must be able to dynamically learn about the tables and columns of a 
 database. The tool uses system catalog queries for this purpose.
  
  
 Figure 16-1: 
 A user-friendly query tool",NA
The Catalog and the ANSI/ISO Standard,"The ANSI/ISO SQL1 standard did not specify the structure and contents of the system 
 catalog. In fact, the SQL1 standard does not require a system catalog at all. However, all 
 of the major SQL-based DBMS products provide a system catalog in one form or another. 
 The structure of the catalog and the tables it contains vary considerably from one brand of 
 DBMS to another.
  
 Because of the growing importance of general-purpose database tools that must access 
  
 - 323 -",NA
Catalog Contents,"Each table in the system catalog contains information about a single kind of structural 
 element in the database. Although the details vary, almost all commercial SQL products 
 include system tables that describe each of these five entities:
  
 •
 Tables.
  The catalog describes each table in the database, identifying its table name, its 
 owner, the number of columns it contains, its size, and so on.
  
 •
 Columns.
  The catalog describes each column in the database, giving the column's name, 
 the table to which it belongs, its data type, its size, whether 
 NULLs
  are allowed, and so on.
  
 •
 Users.
  The catalog describes each authorized database user, including the user's 
 name, an encrypted form of the user's password, and other data.
  
 •
 Views.
  The catalog describes each view defined in the database, including its name, the 
 name of its owner, the query that defines the view, and so on.
  
 •
 Privileges.
  The catalog describes each set of privileges granted in the database, 
 including the names of the grantor and grantee, the privileges granted, the object on 
 which the privileges have been granted, and so on.
  
 Table 16-1 shows the names of the system tables that provide this information in each of 
 the major SQL-based DBMS products. The remainder of this chapter describes some 
 typical system tables in more detail and gives examples of system catalog access. 
  
 Because of the wide variations among the system catalogs among DBMS brands, a 
 complete description of the system catalogs and complete examples for all of the major 
 DBMS brands is beyond the scope of this book. With the information provided here, you 
 should be able to consult the system documentation for your DBMS brand and construct 
 the appropriate system catalog queries.
  
 Table 16-1: Selected System Tables in Popular SQL-Based Products
  
 DBMS
  
 Tables
  
 Columns
  
 Users
  
 Views
  
 Privileges
  
 DB2
 1
  
 SCHEMATA
  
 COLUMNS
  
 DBAUTH
  
 VIEWS
  
 SCHEMAAUTH
  
 TABLES
  
 VIEWDEP
  
 TABAUTH
  
 REFERENCES
  
 COLAUTH
  
 KEYCOLUSE
  
 Oracle
  
 USER_CATALOG
  
 USER_TAB_
  
 ALL_USERS
  
 USER_VIEWS
  
 USER_TAB_
  
 USER_TABLES
  
 COLUMNS
  
 ALL_VIEWS
  
 PRIVSUSER_COL_
  
 ALL_TABLES
  
 ALL_TAB_
  
 PRIVSUSER_
  
 USER_SYNONYMS
  
 COLUMNS
  
 SYS_PRIVS
  
 - 324 -",NA
Table Information,"Each of the major SQL products has a system table or view that keeps track of the tables 
 in the database. In DB2, for example, this information is provided by a system catalog 
 view named 
 SYSCAT.TABLES
 . (All of the DB2 system catalog views are part of a schema 
 named 
 SYSCAT
 , so they all have qualified table/view names of the form 
 SYSCAT.
 xxx
 .)
  
 Table 16-2 shows some of the columns of the 
 SYSCAT.TABLES
  view. It contains one row 
 for each table, view, or alias defined in the database. The information in this view is typical 
 of that provided by the corresponding views in other major DBMS products.
  
 Table 16-2: The 
 SYSCAT.TABLES
  view (DB2)
  
 Column Name
  
 Data Type
  
 Information
  
 TABSCHEMA
  
 CHAR(8)
  
 Schema containing the table, view or alias
  
 TABNAME
  
 VARCHAR(18)
  
 Name of the table, view or alias
  
 DEFINER
  
 CHAR(8)
  
 User-id of table/view/alias creator
  
 TYPE 
  
 CHAR(1)
  
 T=table / V=view / A=alias
  
 STATUS
  
 CHAR(1)
  
 Status of object (system use)
  
 BASE_TABSCHEMA
  
 CHAR(8)
  
 Schema of ""base"" table for an alias
  
 BASE_TABNAME
  
 VARCHAR(18)
  
 Name of ""base"" table for an alias
  
 CREATE_TIME
  
 TIMESTAMP
  
 Time of object creation
  
 STATS_TIME
  
 TIMESTAMP
  
 Time when last statistics computed
  
 COLCOUNT
  
 SMALLINT
  
 Number of columns in table
  
 - 325 -",NA
Column Information,"All of the major SQL products have a system table that keeps track of the columns in the 
 database. There is one row in this table for each column in each table or view in the 
 database. Most DBMS brands restrict access to this base system table, but provide user 
 column information through a view that shows only columns in tables owned by, or 
 accessible to, the current user. In an Oracle8 database, two system catalog views provide 
 this information—
 USER_TAB_COLUMNS
 , which includes one row for each column in each 
 table owned by the current user, and 
 ALL_TAB_COLUMNS
 , which contains one row for 
 each column in each table accessible to the current user.
  
 Most of the information in the ""system columns"" table or view stores the definition of a 
 column—its name, its data type, its length, whether it can take 
 NULL
  values, and so on. In 
 addition, the table sometimes includes information about the distribution of data values 
 found in each column. This statistical information helps the DBMS decide how to carry out 
 a query in the optimal way.
  
 Here is a typical query you could use to find out about the columns in an Oracle8 
 database:
  
 List the names and data types of the columns in my 
 OFFICES
  table.
  
 SELECT COLUMN_NAME, DATA_TYPE
  
  FROM USER_TAB_COLUMNS
  
 WHERE TABLE_NAME = 'OFFICES'
  
 - 328 -",NA
View Information,"The definitions of the views in a database are usually stored by the DBMS in the system 
 catalog. The DB2 catalog contains two system tables that keep track of views. The 
 SYSCAT.VIEWS
  table, described in Table 16-6, contains the SQL text definition of each 
 view. If the definition exceeds 3,600 characters, it is stored in multiple rows, with 
 sequence numbers 1, 2, 3, and so on.
  
 Table 16-6: The 
 SYSCAT.VIEWS
  view (DB2)
  
 Column Name
  
 Data Type
  
 Information
  
 VIEWSCHEMA
  
 CHAR(8)
  
 Schema containing the view
  
 VIEWNAME
  
 VARCHAR(18)
  
 Name of the view
  
 DEFINER
  
 CHAR(8)
  
 User-id who created the view
  
 SEQNO
  
 SMALLINT
  
 Sequence number for this row of SQL text
  
 VIEWCHECK
  
 CHAR(1)
  
 Type of view checking
  
 READONLY
  
 CHAR(1)
  
 Is view read-only? (Y/N)
  
 VALID
  
 CHAR(1)
  
 Is view definition valid? (Y/N)
  
 FUNC_PATH
  
 VARCHAR(254)
  
 Path for resolving function calls in view
  
 - 331 -",NA
Remarks,"IBM's DB2 products allow you to associate up to 254 characters of 
 remarks
  with each 
 table, view, and column defined in the database. The remarks allow you to store a brief 
 description of the table or data item in the system catalog. The remarks are stored in the 
 SYSCAT.TABLES and SYSCAT.COLUMNS
  system tables of the system catalog. Unlike 
 the other elements of table and column definitions, the remarks and labels are not 
 specified by the 
 CREATE TABLE
  statement. Instead, the 
 COMMENT
  statement is used. Its 
 syntax is shown in Figure 16-2. Here are some examples:
  
  
 Figure 16-2: 
 DB2 
 COMMENT
  statement syntax diagram 
  
 Define remarks for the 
 OFFICES
  table.
  
 COMMENT ON TABLE OFFICES
  
 - 333 -",NA
Relationship Information,"With the introduction of referential integrity into the major enterprise DBMS products 
 during the mid-1990s, system catalogs were expanded to describe primary keys, foreign 
 keys, and the parent/child relationships that they create. In DB2, which was among the 
 first to support referential integrity, the description is provided by the 
  
 SYSCAT.REFERENCES
  system catalog table, described in Table 16-8. Every parent/child 
 relationship between two tables in the database is represented by a single row in the 
 SYSCAT.REFERENCES
  table. The row identifies the names of the parent and child tables, 
 the name of the relationship, and the delete and update rules for the relationship. You can 
 query it to find out about the relationships in the database:
  
 Table 16-8: The 
 SYSCAT.REFERENCES
  view (DB2)
  
 Column 
  
 Data Type
  
 Information
  
 Name
  
 CONSTNAME
  
 VARCHAR(18)
  
 Name of constraint described by this row
  
 TABSCHEMA
  
 CHAR(8)
  
 Schema containing the constraint
  
 TABNAME
  
 VARCHAR(18)
  
 Table to which constraint applies
  
 DEFINER
  
 CHAR(8)
  
 Creator of table to which constraint applies
  
 REFKEYNAME
  
 VARCHAR(18)
  
 Name of parent key
  
 REFTABSCHEMA
  
 CHAR(8)
  
 Schema containing parent table
  
 REFTABNAME
  
 VARCHAR(18)
  
 Name of parent table
  
 COLCOUNT
  
 SMALLINT
  
 Number of columns in the foreign key
  
 DELETERULE
  
 CHAR(1)
  
 Delete rule for foreign key constraint(A=no action, 
  
 - 334 -",NA
User Information,- 336 -,NA
Privileges Information,"In addition to storing database structure information, the system catalog generally stores 
 the information required by the DBMS to enforce database security. As described in 
  
 - 337 -",NA
The SQL2 Information Schema,"The SQL2 standard does not directly specify the structure of a system catalog that must 
 be supported by DBMS implementations. In practice, given the widely differing features 
 supported by different DBMS brands and the major differences in the system catalogs that 
 were already being used by commercial SQL products when the SQL2 standard was 
 adopted, it would have been impossible to reach an agreement on a standard catalog 
 definition. Instead, the writers of the SQL2 standard defined an ""idealized"" system catalog 
 that a DBMS vendor might design if they were building a DBMS to support the SQL2 
 standard ""from scratch."" The tables in this idealized system catalog (called the 
 definition 
 schema
  in the standard) are summarized in Table 16-13.
  
 Table.16-13: Idealized System Catalog Used by the SQL2 Standard
  
 System Table
  
 Contents
  
 USERS
  
 One row for each user (""authorization-id"") in the catalog 
  
 cluster
  
 SCHEMATA 
  
 On row for each schema in the catalog cluster
  
 DATA_TYPE_DESCRIPTOR 
 One row for each domain or column defined with a data 
 type
  
 DOMAINS 
  
 One row for each domain
  
 DOMAIN_CONSTRAINTS 
  
 One row for each domain constraint
  
 TABLES 
  
 One row for each table or view
  
 VIEWS 
  
 One row for each table or view
  
 COLUMNS 
  
 One row for each column in each table or view definition
  
 VIEW_TABLE_USAGE 
  
 One row for each table referenced in each view definition (if 
  
 a view is defined by a query on multiple tables, there will be 
  
 a row for each table)
  
 VIEW_COLUMN_USAGE 
  
 One row for each column referenced by a view
  
 TABLE_CONSTRAINTS 
  
 One row for each table constraint specified in a table 
  
 definition
  
 - 339 -",NA
Other Catalog Information,"The system catalog is a reflection of the capabilities and features of the DBMS that uses 
 it. Because of the many SQL extensions and additional features offered by popular 
 DBMS products, their system catalogs always contain several tables unique to the 
 DBMS. Here are just a few examples:
  
 •DB2 and Oracle support aliases and synonyms (alternate names for tables). DB2 
 stores alias information with other table information in the 
 SYSCAT.TABLES
  system 
 table. Oracle makes synonym information available through its 
 USER_SYNONYMNS 
 system view.
  
 •SQL Server supports multiple named databases. It has a system table called 
 SYSDATABASES
  that identifies the databases managed by a single server.
  
 •Many DBMS brands now support stored procedures, and the catalog contains one or 
 more tables that describes them. Sybase stores information about stored procedures in 
 its 
 SYSPROCEDURES
  system table.
  
 •Ingres supports tables that are distributed across several disk volumes. Its 
 IIMULTI_LOCATIONS
  system table keeps track of the locations of multi-volume 
 tables.
  
 - 343 -",NA
Summary,"The system catalog is a collection of system tables that describe the structure of a 
 relational database:
  
 •The DBMS maintains the data in the system tables, updating it as the structure of the 
 database changes.
  
 •A user can query the system tables to find out information about tables, columns, and 
 privileges in the database.
  
 •Front-end query tools use the system tables to help users navigate their way through 
 the database in a user-friendly way.
  
 •The names and organization of the system tables differ widely from one brand of DBMS 
 to another; and there are even differences among different DBMS products from the same 
 vendor, reflecting the different internal structures and capabilities of the products.
  
 •The SQL2 standard does not require that a DBMS actually have a set of system catalog 
 tables, but it does define a set of standard catalog views for products that claim higher 
 levels of SQL2 conformance.",NA
Part V:,NA,NA
 Programming with SQL,NA,NA
Chapter List,"Chapter Embedded SQL 
  
 17: 
  
 Chapter Dynamic SQL* 
  
 18: 
  
 Chapter SQL API
 s 
  
 19:",NA
Chapter 17: ,NA,NA
Embedded SQL,NA,NA
Overview,"SQL is a 
 dual-mode language
 . It is both an interactive database language used for 
 ad 
 hoc
  queries and updates and a programmatic database language used by application 
 programs for database access. For the most part, the SQL language is identical in both 
 modes. The dual-mode nature of SQL has several advantages:
  
 •It is relatively easy for programmers to learn how to write programs that access the 
 database.
  
 •Capabilities available through the interactive query language are also automatically 
 available to application programs.
  
 •The SQL statements to be used in a program can be tried out first using interactive 
 SQL and then coded into the program.
  
 •Programs can work with tables of data and query results instead of navigating their 
  
 - 344 -",NA
Programmatic SQL Techniques,"SQL is a language and can be used programmatically, but it would be incorrect to call 
 SQL a programming language. SQL lacks even the most primitive features of ""real"" 
 programming languages. It has no provision for declaring variables, no 
 GOTO
  statement, 
 no 
 IF
  statement for testing conditions, no 
 FOR
 , 
 DO
 , or 
 WHILE
  statements to construct 
 loops, no block structure, and so on. SQL is a database 
 sublanguage
  that handles 
 special purpose database management tasks. To write a program that accesses a 
 database, you must start with a conventional programming language, such as COBOL, 
 PL/I, FORTRAN, Pascal, or C and then ""add SQL to the program.""
  
 The initial ANSI/ISO SQL standard was concerned exclusively with this programmatic use 
 of SQL. In fact, the standard did not even include the interactive 
 SELECT
  statement 
 described in Chapters 6 through 9. It only specified the programmatic 
 SELECT
  statement 
 described later in this chapter. The SQL2 standard, published in 1992, expanded its focus 
 to include interactive SQL (called ""direct invocation of SQL"" in the standard) and more 
 advanced forms of programmatic SQL (the 
 dynamic SQL 
 capabilities described in 
 Chapter 
 20
 ).
  
 Commercial SQL database vendors offer two basic techniques for using SQL within an 
 application program:
  
 •
 Embedded SQL
 . In this approach, SQL statements are embedded directly into the 
 program's source code, intermixed with the other programming language statements. 
  
 Special embedded SQL statements are used to retrieve data into the program. A 
 special SQL precompiler accepts the combined source code and, along with other 
 programming tools, converts it into an executable program.
  
 •
 Application program interface
 . In this approach, the program communicates with the 
 DBMS through a set of function calls called an 
 application program interface
 , or API. The 
 program passes SQL statements to the DBMS through the API calls and uses API calls to 
 retrieve query results. This approach does not require a special precompiler.
  
 The initial IBM SQL products used an embedded SQL approach, and it was adopted by 
 most commercial SQL products in the 1980s. The original ANSI/ISO SQL standard 
 specified only an awkward ""module language"" for programmatic SQL, but commercial 
 SQL products continued to follow the IBM 
 de facto 
 standard. In 1989, the ANSI/ISO 
 standard was extended to include a definition of how to embed SQL statements within 
 the Ada, C, COBOL, FORTRAN, Pascal, and PL/I programming languages, this time 
 following the IBM approach. The SQL2 standard continued this specification.
  
 In parallel with this evolution of embedded SQL, several DBMS vendors who were focused 
 on minicomputer systems introduced callable database APIs in the 1980s. When the 
 Sybase DBMS was introduced, it offered 
 only
  a callable API. Microsoft's SQL Server, 
 derived from the Sybase DBMS, also used the API approach exclusively. Soon after the 
 debut of SQL Server, Microsoft introduced Open Database Connectivity (ODBC), another 
 callable API. ODBC is roughly based on the SQL Server API, but with the additional goals 
 of being database independent and permitting concurrent access to two or more different 
 DBMS brands through a common API. More recently, Java Database 
  
 Connectivity (JDBC) has emerged as an important API for accessing a relational 
  
 database from within programs written in Java. With the growing popularity of callable 
 APIs, the callable and embedded approaches are both in active use today. The following 
 table summarizes the programmatic interfaces offered by some of the leading SQL-based 
 DBMS products.
  
 - 345 -",NA
DBMS Statement Processing,"To understand any of the programmatic SQL techniques, it helps to understand a little bit 
 more about how the DBMS processes SQL statements. To process a SQL statement, the 
 DBMS goes through a series of five steps, shown in Figure 17-1:
  
  
 Figure 17-1: 
 How the DBMS processes a SQL statement
  
 1. The DBMS begins by 
 parsing
  the SQL statement. It breaks the statement up into 
 individual words, makes sure that the statement has a valid verb, legal clauses, and so 
 on. Syntax errors and misspellings can be detected in this step.
  
 2. The DBMS 
 validates
  the statement. It checks the statement against the system 
 catalog. Do all the tables named in the statement exist in the database? Do all of the 
 columns exist, and are the column names unambiguous? Does the user have the 
 required privileges to execute the statement? Semantic errors are detected in this step.
  
 3. The DBMS 
 optimizes
  the statement. It explores various ways to carry out the statement. 
 Can an index be used to speed a search? Should the DBMS first apply a search condition 
 to Table A and then join it to Table B, or should it begin with the join 
  
 - 346 -",NA
Embedded SQL Concepts,"The central idea of embedded SQL is to blend SQL language statements directly into a 
 program written in a ""host"" programming language, such as C, Pascal, COBOL, 
 FORTRAN, PL/I, or Assembler. Embedded SQL uses the following techniques to embed 
 the SQL statements:
  
 •SQL statements are intermixed with statements of the host language in the source 
 program. This ""embedded SQL source program"" is submitted to a SQL precompiler, 
 which processes the SQL statements.
  
 •Variables of the host programming language can be referenced in the embedded SQL 
 statements, allowing values calculated by the program to be used by the SQL statements.
  
 •Program language variables also are used by the embedded SQL statements to 
 receive the results of SQL queries, allowing the program to use and process the 
 retrieved values.
  
 •Special program variables are used to assign 
 NULL
  values to database columns and to 
 support the retrieval of 
 NULL
  values from the database.
  
 •Several new SQL statements that are unique to embedded SQL are added to the 
 interactive SQL language, to provide for row-by-row processing of query results.
  
 Figure 17-2 shows a simple embedded SQL program, written in C. The program illustrates 
 many, but not all, of the embedded SQL techniques. The program prompts the user for an 
 office number, retrieves the city, region, sales, and target for the office, and 
  
 - 347 -",NA
Developing an Embedded SQL Program,"An embedded SQL program contains a mix of SQL and programming language 
  
 statements, so it can't be submitted directly to a compiler for the programming language. 
 Instead, it moves through a multi-step development process, shown in Figure 17-3. The 
 steps in the figure are actually those used by the IBM mainframe databases (DB2, 
 SQL/DS), but all products that support embedded SQL use a similar process:
  
  
 Figure 17-3: 
 The embedded SQL development process
  
 1. The embedded SQL source program is submitted to the SQL precompiler, a 
 programming tool. The precompiler scans the program, finds the embedded SQL 
 statements, and processes them. A different precompiler is required for each 
 programming language supported by the DBMS. Commercial SQL products typically 
 offer precompilers for one or more languages, including C, Pascal, COBOL, FORTRAN, 
 Ada, PL/I, RPG, and various assembly languages.
  
 2. The precompiler produces two files as its output. The first file is the source program, 
 stripped of its embedded SQL statements. In their place, the precompiler substitutes calls 
 to the ""private"" DBMS routines that provide the run-time link between the program and the 
 DBMS. Typically, the names and calling sequences of these routines are known only to 
 the precompiler and the DBMS; they are not a public interface to the DBMS. The second 
 file is a copy of all the embedded SQL statements used in the program. This file is 
 sometimes called a 
 database request module
 , or DBRM.
  
 3. The source file output from the precompiler is submitted to the standard compiler for 
 the host programming language (such as a C or COBOL compiler). The compiler 
 processes the source code and produces object code as its output. Note that this step 
 has nothing in particular to do with the DBMS or with SQL.
  
 4.The 
 linker
  accepts the object modules generated by the compiler, links them with 
  
 - 349 -",NA
Running an Embedded SQL Program,"Recall from 
 Figure 17-3
  that the embedded SQL development process produces two 
 executable components, the executable program itself and the program's application 
 plan, stored in the database. When you run an embedded SQL program, these two 
 components are brought together to do the work of the application:
  
 - 350 -",NA
Simple Embedded SQL Statements,"The simplest SQL statements to embed in a program are those that are self-contained 
 and do not produce any query results. For example, consider this interactive SQL 
 statement:
  
 Delete all salespeople with sales under $150,000.
  
 DELETE FROM SALESREPS
  
 WHERE SALES < 150000.00
  
 Figures 17-4, 
 17-5
 , and 
 17-6
  show three programs that perform the same task as this 
 interactive SQL statement, using embedded SQL. The program in Figure 17-4 is written 
 in C, the program in 
 Figure 17-5
  is written in COBOL, and the program in 
 Figure 17-6
  is 
 written in FORTRAN. Although the programs are extremely simple, they illustrate the 
 most basic features of embedded SQL:
  
 main()
  
 {
  
  exec sql include sqlca;
  
  exec sql declare salesreps table
  
  (empl_num integer not null,
  
  name varchar(15) not null,
  
  age integer
  
  rep_office integer,
  
  title varchar(10),
  
  hire_date date not null,
  
  manager integer,
  
  quota money,
  
 - 352 -",NA
Declaring Tables,- 355 -,NA
Error Handling,"When you type an interactive SQL statement that causes an error, the interactive SQL 
 program displays an error message, aborts the statement, and prompts you to type a new 
 statement. In embedded SQL, error handling becomes the responsibility of the application 
 program. Actually, embedded SQL statements can produce two distinct types of errors:
  
 •
 Compile time errors
 . Misplaced commas, misspelled SQL keywords, and similar errors in 
 embedded SQL statements are detected by the SQL precompiler and reported to the 
 programmer. The programmer can fix the errors and recompile the application program.
  
 •
 Run-time errors
 . An attempt to insert an invalid data value or lack of permission to 
 update a table can be detected only at run-time. Errors such as these must be 
 detected and handled by the application program.
  
 In embedded SQL programs, the DBMS reports run-time errors to the application program 
 through a returned error code. If an error is detected, a further description of the error, and 
 other information about the statement just executed is available through additional 
 diagnostic information. The earliest IBM embedded SQL implementations defined an 
 error-reporting mechanism that was adopted, with variations, by most of the major DBMS 
 vendors. The central part of this scheme—an error status variable named 
 SQLCODE
 —was 
 also defined in the original ANSI/ISO SQL standard. The SQL2 standard, published in 
 1992, defined an entirely new, parallel error-reporting mechanism, built around an error 
 status variable named 
 SQLSTATE
 . These mechanisms are described in the next two 
 sections.
  
 Error Handling with 
 SQLCODE
  
 Under this scheme, pioneered by the earliest IBM products, the DBMS communicates 
 status information to the embedded SQL program through an area of program storage 
 called the 
 SQL Communications Area
 , or 
 SQLCA
 . The 
 SQLCA
  is a data structure that 
  
 - 356 -",NA
Using Host Variables,"The embedded SQL programs in the previous figures don't provide any real interaction 
 between the programming statements and the embedded SQL statements. In most 
 applications, you will want to use the value of one or more program variables in the 
 embedded SQL statements. For example, suppose you wanted to write a program to 
 adjust all sales quotas up or down by some dollar amount. The program should prompt 
 the user for the amount and then use an embedded 
 UPDATE
  statement to change the 
 QUOTA
  column in the 
 SALESREPS
  table.
  
 Embedded SQL supports this capability through the use of 
 host variables
 . A host variable 
 is a program variable declared in the host language (for example, a COBOL or C 
  
 - 363 -",NA
Data Retrieval in Embedded SQL,"Using the embedded SQL features described thus far, you can embed any interactive 
 SQL statement 
 except
  the 
 SELECT
  statement in an application program. Retrieving data 
 with an embedded SQL program requires some special extensions to the 
 SELECT 
 statement. The reason for these extensions is that there is a fundamental mismatch 
 between the SQL language and programming languages such as
  C 
 and 
 COBOL
 : a SQL 
 query produces an entire table of query results, but most programming languages can 
 only manipulate individual data items or individual records (rows) of data.
  
 Embedded SQL must build a ""bridge"" between the table-level logic of the SQL 
 SELECT 
 statement and the row-by-row processing of 
 C
 , 
 COBOL
 , and other host programming 
 languages. For this reason, embedded SQL divides SQL queries into two groups:
  
 •
 Single-row queries
 , where you expect the query results to contain a single row of data. 
 Looking up a customer's credit limit or retrieving the sales and quota for a particular 
 salesperson are examples of this type of query.
  
 •
 Multi-row queries
 , where you expect that the query results may contain zero, one, or 
 many rows of data. Listing the orders with amounts over $20,000 or retrieving the names 
 of all salespeople who are over quota are examples of this type of query.
  
 Interactive SQL does not distinguish between these two types of queries; the same 
 interactive 
 SELECT
  statement handles them both. In embedded SQL, however, the two 
 types of queries are handled very differently. Single-row queries are simpler to handle 
 and are discussed in the next section. Multi-row queries are discussed later in this 
 chapter.",NA
Single-Row Queries,"Many useful SQL queries return a single row of query results. Single-row queries are 
 especially common in transaction processing programs, where a user enters a customer 
 number or an order number and the program retrieves relevant data about the customer or 
 order. In embedded SQL, single-row queries are handled by the 
 singleton
 SELECT 
 statement, shown in Figure 17-19. The singleton 
 SELECT
  statement has a syntax much 
 like that of the interactive 
 SELECT
  statement. It has a 
 SELECT
  clause, a 
 FROM
  clause, and 
 an optional 
 WHERE
  clause. Because the singleton 
 SELECT 
 statement returns a single row 
 of data, there is no need for a
  GROUP BY,HAVING
  or 
 ORDER BY
  clause. The 
 INTO
  
 clause specifies the host variables that are to receive the data retrieved by the statement.
  
  
 Figure 17-19: 
 Singleton 
 SELECT
  statement syntax diagram
  
 - 370 -",NA
Multi-Row Queries,"When a query produces an entire table of query results, embedded SQL must provide a 
 way for the application program to process the query results one row at a time. 
  
 Embedded SQL supports this capability by defining a new SQL concept, called a 
 cursor
 , 
 and adding several statements to the interactive SQL language. Here is an overview of 
 embedded SQL technique for multi-row query processing and the new statements it 
 requires:
  
 1.The 
 DECLARE CURSOR
  statement specifies the query to be performed and 
 associates a 
 cursor
  name with the query.
  
 2.The 
 OPEN
  statement asks the 
 DBMS
  to start executing the query and generating query 
 results. It positions the cursor before the first row of query results.
  
 3.
  
 The 
 FETCH
  statement advances the cursor to the first row of query results and 
 retrieves its data into host variables for use by the application program. Subsequent 
  
 FETCH
  statements move through the query results row by row, advancing the cursor 
 to the next row of query results and retrieving its data into the host variables.
  
 4.The 
 CLOSE
  statement ends access to the query results and breaks the association 
 between the cursor and the query results.
  
 Figure 17-23 shows a program that uses embedded SQL to perform a simple multi-row 
 query. The numbered callouts in the figure correspond to the numbers in the preceding 
 steps. The program retrieves and displays, in alphabetical order, the name, quota, and 
 year-to-date sales of each salesperson whose sales exceed quota. The interactive SQL 
  
 - 376 -",NA
Cursor-Based Deletes and Updates,"Application programs often use cursors to allow the user to browse through a table of data 
 row by row. For example, the user may ask to see all of the orders placed by a particular 
 customer. The program declares a cursor for a query of the 
 ORDERS
  table and displays 
 each order on the screen, possibly in a computer-generated form, waiting for a signal from 
 the user to advance to the next row. Browsing continues in this fashion until the user 
 reaches the end of the query results. The cursor serves as a pointer to the current row of 
 query results. If the query draws its data from a single table, and it is not a summary 
 query, as in this example, the cursor implicitly points to a row of a database table, because 
 each row of query results is drawn from a single row of the table.
  
 While browsing the data, the user may spot data that should be changed. For example, 
 the order quantity in one of the orders may be incorrect, or the customer may want to 
 delete one of the orders. In this situation, the user wants to update or delete ""this"" order. 
 The row is not identified by the usual SQL search condition; rather, the program uses the 
 cursor as a pointer to indicate which particular row is to be updated or deleted.
  
 Embedded SQL supports this capability through special versions of the 
 DELETE
  and 
 UPDATE
  statements, called the 
 positioned
 DELETE
  and 
 positioned
 UPDATE 
 statements, 
 respectively.
  
 The positioned 
 DELETE
  statement, shown in Figure 17-30, deletes a single row from a 
 table. The deleted row is the current row of a cursor that references the table. To process 
 the statement, the DBMS locates the row of the base table that corresponds to the current 
 row of the cursor and deletes that row from the base table. After the row is deleted, the 
 cursor has no current row. Instead, the cursor is effectively positioned in the ""empty 
 space"" left by the deleted row, waiting to be advanced to the next row by a subsequent 
 FETCH
  statement.
  
  
 Figure 17-30: 
 Positioned 
 DELETE
  statement syntax diagram
  
 The positioned 
 UPDATE
  statement, shown in Figure 17-31, updates a single row of a table. 
 The updated row is the current row of a cursor that references the table. To process the 
 statement, the DBMS locates the row of the base table that corresponds to the current row 
 of the cursor and updates that row as specified in the 
 SET
  clause. After the row is 
 updated, it remains the current row of the cursor. Figure 17-32 shows an order browsing 
 program that uses the positioned 
 UPDATE
  and 
 DELETE
  statements:
  
  
 Figure 17-31: 
 Positioned 
 UPDATE
  statement syntax diagram
  
 - 383 -",NA
Cursors and Transaction Processing,"The way that your program handles its cursors can have a major impact on database 
 performance. Recall from 
 Chapter 12
  that the SQL transaction model guarantees the 
 consistency of data during a transaction. In cursor terms, this means that your program 
 can declare a cursor, open it, fetch the query results, close it, reopen it, and fetch the 
 query results again—and be guaranteed that the query results will be identical both 
 times. The program can also fetch the same row through two different cursors and be 
 guaranteed that the results will be identical. In fact, the data is guaranteed to remain 
 consistent until your program issues a 
 COMMIT
  or 
 ROLLBACK
  to end the transaction. 
 Because the consistency is not guaranteed across transactions, both the 
 COMMIT
  and 
 ROLLBACK
  statements automatically close all open cursors.
  
 Behind the scenes, the DBMS provides this consistency guarantee by locking all of the 
 rows of query results, preventing other users from modifying them. If the query produces 
 many rows of data, a major portion of a table may be locked by the cursor. Furthermore, if 
 your program waits for user input after fetching each row (for example, to let the user 
 verify data displayed on the screen), parts of the database may be locked for a very long 
 time. In an extreme case, the user might leave for lunch in mid-transaction, locking out 
 other users for an hour or more!
  
 To minimize the amount of locking required, you should follow these guidelines when 
 writing interactive query programs:
  
 •Keep transactions as short as possible.
  
 •Issue a 
 COMMIT
  statement immediately after every query and as soon as possible 
 after your program has completed an update.
  
 •Avoid programs that require a great deal of user interaction or that browse through 
  
 - 386 -",NA
Summary,"In addition to its role as an interactive database language, SQL is used for programmatic 
 access to relational databases:
  
 •The most common technique for programmatic use of SQL is embedded SQL, where 
 SQL statements are embedded into the application program, intermixed with the 
 statements of a host programming language such as C or COBOL.
  
 •Embedded SQL statements are processed by a special SQL precompiler. They begin 
 with a special introducer (usually 
 EXEC SQL
 ) and end with a terminator, which varies 
 from one host language to another.
  
 •Variables from the application program, called host variables, can be used in 
 embedded SQL statements wherever a constant can appear. These input host 
 variables tailor the embedded SQL statement to the particular situation.
  
 •Host variables are also used to receive the results of database queries. The values of 
 these output host variables can then be processed by the application program.
  
 •
  
 Queries that produce a single row of data are handled with the singleton 
 SELECT 
 statement of embedded SQL, which specifies both the query and the host variables to 
  
 receive the retrieved data.
  
 •Queries that produce multiple rows of query results are handled with cursors in 
 embedded SQL. The 
 DECLARE CURSOR
  statement defines the query, the 
 OPEN 
 statement begins query processing, the 
 FETCH
  statement retrieves successive rows of 
 query results, and the 
 CLOSE
  statement ends query processing.
  
 •The positioned UPDATE and DELETE statements can be used to update or delete the 
 row currently selected by a cursor.",NA
Chapter 18: ,NA,NA
Dynamic SQL*,NA,NA
Overview,"The embedded SQL programming features described in the preceding chapter are 
 collectively known as 
 staticSQL.
  Static SQL is adequate for writing all of the programs 
 typically required in a data processing application. For example, in the order processing 
 application of the sample database, you can use static SQL to write programs that handle 
 order entry, order updates, order inquiries, customer inquiries, customer file 
  
 maintenance, and programs that produce all types of reports. In every one of these 
 programs, the pattern of database access is decided by the programmer and ""hard-coded"" 
 into the program as a series of embedded SQL statements.
  
 There is an important class of applications, however, where the pattern of database access 
  
 - 387 -",NA
Limitations of Static SQL,"As the name ""static SQL"" implies, a program built using the embedded SQL features 
 described in Chapter 17 (host variables, cursors, and the 
 DECLARE CURSOR
 , 
 OPEN
 , 
 FETCH
 , and 
 CLOSE
  statements) has a predetermined, fixed pattern of database access. 
 For each embedded SQL statement in the program, the tables and columns referenced by 
 that statement are determined in advance by the programmer and hard-coded into the 
 embedded SQL statement. Input host variables provide some flexibility in static SQL, but 
 they don't fundamentally alter its static nature. Recall that a host variable can appear 
 anywhere a constant is allowed in a SQL statement. You can use a host variable to alter a 
 search condition:
  
 exec sql select name, quota, sales
  
  from salesreps
  
  where quota > :cutoff_amount;
  
 You can also use a host variable to change the data inserted or updated in a database:
  
 exec sql update salesreps
  
  set quota = quota + :increase
  
  where quota >:cutoff_amount;
  
 However, you cannot use a host variable in place of a table name or a column reference. 
 The attempted use of the host variables 
 which_table
  and 
 which_column
  in these 
 statements is illegal:
  
 exec sql update :which_table
  
  set :which_column = 0;
  
 exec sql declare cursor cursor7 for
  
  select *
  
  from :which_table;
  
 Even if you could use a host variable in this way (and you cannot), another problem 
 would immediately arise. The number of columns produced by the query in the second 
 statement would vary, depending on which table was specified by the host variable. For 
 the 
 OFFICES
  table, the query results would have six columns; for the 
 SALESREPS
  table, 
 they would have nine columns. Furthermore, the data types of the columns would be 
 different for the two tables. But to write a 
 FETCH
  statement for the query, you must know 
 in advance how many columns of query results there will be and their data types, 
 because you must specify a host variable to receive each column:
  
 exec sql fetch cursor7
  
  into :var1, :var2, :var3;
  
 As this discussion illustrates, if a program must be able to determine at run-time which 
 SQL statements it will use, or which tables and columns it will reference, static SQL is 
 inadequate for the task. Dynamic SQL overcomes these limitations.
  
 - 388 -",NA
Dynamic SQL Concepts,"The central concept of dynamic SQL is simple: don't hard-code an embedded SQL 
 statement into the program's source code. Instead, let the program build the text of a SQL 
 statement in one of its data areas at runtime. The program then passes the 
  
 statement text to the DBMS for execution ""on the fly."" Although the details get quite 
 complex, all of dynamic SQL is built on this simple concept, and it's a good idea to keep it 
 in mind.
  
 To understand dynamic SQL and how it compares with static SQL, it's useful to consider 
 once again the process the DBMS goes through to execute a SQL statement, originally 
 shown in 
 Figure 17-1
  and repeated here in Figure 18-1. Recall from 
 Chapter 17
  that a 
 static SQL statement goes through the first four steps of the process at compile-time. The 
 BIND
  utility (or the equivalent part of the DBMS run-time system) analyzes the SQL 
 statement, determines the best way to carry it out, and stores the application plan for the 
 statement in the database as part of the program development process. When the static 
 SQL statement is executed at run-time, the DBMS simply executes the stored application 
 plan.
  
  
 - 389 -",NA
Dynamic Statement Execution (,NA,NA
EXECUTE IMMEDIATE,NA,NA
),"The simplest form of dynamic SQL is provided by the 
 EXECUTE IMMEDIATE
  statement, 
 shown in Figure 18-2. This statement passes the text of a dynamic SQL statement to the 
 DBMS and asks the DBMS to execute the dynamic statement immediately. To use this 
 statement, your program goes through the following steps:
  
  
 Figure 18-2: 
 EXECUTE IMMEDIATE
  statement syntax diagram
  
 1. The program constructs a SQL statement as a string of text in one of its data areas 
 (usually called a 
 buffer
 ). The statement can be almost any SQL statement that does not 
 retrieve data.
  
 2.The program passes the SQL statement to the DBMS with the 
 EXECUTE IMMEDIATE 
 statement.
  
 3.
  
 The DBMS executes the statement and sets the 
 SQLCODE/SQLSTATE
  values to 
 indicate the completion status, exactly as if the statement had been hard-coded using 
  
 static SQL.
  
 - 390 -",NA
Two-Step Dynamic Execution,"The 
 EXECUTE IMMEDIATE
  statement provides one-step support for dynamic statement 
 execution. As described previously, the DBMS goes through all five steps of 
 Figure 18-1 
 for the dynamically executed statement. The overhead of this process can be very 
 significant if your program executes many dynamic statements, and it's wasteful if the 
 statements to be executed are identical or very similar. In practice, the 
 EXECUTE 
 IMMEDIATE
  statement should only be used for ""one-time"" statements that will be 
 executed once by a program and then never executed again.
  
 To deal with the large overhead of the one-step approach, dynamic SQL offers an 
 alternative, two-step method for executing SQL statements dynamically. In practice, this 
 two-step approach is used for 
 all
  SQL statements in a program that are executed more 
 than once, and especially for those that are executed repeatedly, hundreds or thousands 
 of times, in response to user interaction. Here is an overview of the two-step technique:
  
 1. The program constructs a SQL statement string in a buffer, just as it does for the 
 EXECUTE IMMEDIATE
  statement. A question mark (?) can be substituted for a constant 
 anywhere in the statement text to indicate that a value for the constant will be supplied 
 later. The question mark is called a 
 parametermarker
 .
  
 2.
  
 The 
 PREPARE
  statement asks the DBMS to parse, validate, and optimize the 
 statement and to generate an application plan for it. The DBMS sets the 
  
 SQLCODE/SQLSTATE
  values to indicate any errors found in the statement and retains 
 the application plan for later execution. Note that the DBMS does 
 not
  execute the plan 
 in response to the 
 PREPARE
  statement.
  
 3. When the program wants to execute the previously prepared statement, it uses the 
 EXECUTE
  statement and passes a value for each parameter marker to the DBMS. 
  
 The DBMS substitutes the parameter values, executes the previously generated 
 application plan, and sets the 
 SQLCODE/SQLSTATE
  values to indicate its completion 
 status.
  
 4.The program can use the 
 EXECUTE
  statement repeatedly, supplying different 
 parameter values each time the dynamic statement is executed.
  
 - 392 -",NA
The ,NA,NA
PREPARE,NA,NA
 Statement,"The 
 PREPARE
  statement, shown in Figure 18-5, is unique to dynamic SQL. It accepts a 
 host variable containing a SQL statement string and passes the statement to the DBMS. 
  
 The DBMS compiles the statement text and prepares it for execution by generating an 
 application plan. The DBMS sets the 
 SQLCODE/SQLSTATE
  variables to indicate any errors 
 detected in the statement text. As described previously, the statement string can contain a 
 parameter marker, indicated by a question mark, anywhere that a constant can appear. 
 The parameter marker signals the DBMS that a value for the parameter will be supplied 
 later, when the statement is actually executed.
  
  
 Figure 18-5: 
 PREPARE
  statement syntax diagram
  
 As a result of the 
 PREPARE
  statement, the DBMS assigns the specified 
 statementname 
 to the prepared statement. The statement name is a SQL identifier, like a cursor name. 
  
 You specify the statement name in subsequent 
 EXECUTE
  statements when you want to 
 execute the statement. DBMS brands differ in how long they retain the prepared 
  
 statement and the associated statement name. For some brands, the prepared statement 
 can only be reexecuted until the end of the current transaction (that is, until the next 
 COMMIT
  or 
 ROLLBACK
  statement). If you want to execute the same dynamic statement 
 later during another transaction, you must prepare it again. Other brands relax this 
 restriction and retain the prepared statement throughout the currentsession with the 
 DBMS. The ANSI/ISO SQL2 standard acknowledges these differences and explicitly says 
 that the validity of a prepared statement outside of the current transaction is 
 implementation dependent.
  
 The 
 PREPARE
  statement can be used to prepare almost any executable DML or DDL 
 statement, including the 
 SELECT
  statement. Embedded SQL statements that are actually 
 precompiler directives (such as the 
 WHENEVER
  or 
 DECLARE CURSOR 
 statements) cannot 
 be prepared, of course, because they are not executable.",NA
The ,NA,NA
EXECUTE,NA,NA
 Statement,"The 
 EXECUTE
  statement, shown in Figure 18-6, is unique to dynamic SQL. It asks the 
  
 - 395 -",NA
Dynamic Queries,"The 
 EXECUTE IMMEDIATE
 , 
 PREPARE
 , and 
 EXECUTE
  statements as described thus far 
 support dynamic execution of most SQL statements. However, they can't support dynamic 
 queries because they lack a mechanism for retrieving the query results. To support 
 dynamic queries, SQL 
 combines
  the dynamic SQL features of the 
 PREPARE
  and 
 EXECUTE
  
 statements with extensions to the static SQL query processing statements, and adds a 
 new statement. Here is an overview of how a program performs a dynamic query:
  
 1. A dynamic version of the 
 DECLARE CURSOR
  statement declares a cursor for the 
 query. Unlike the static 
 DECLARE CURSOR
  statement, which includes a hard-coded 
 SELECT
  statement, the dynamic form of the 
 DECLARE CURSOR
  statement specifies the 
 statement name that will be associated with the dynamic 
 SELECT
  statement.
  
 2. The program constructs a valid 
 SELECT
  statement in a buffer, just as it would 
 construct a dynamic 
 UPDATE
  or 
 DELETE
  statement. The 
 SELECT
  statement may 
 contain parameter markers like those used in other dynamic SQL statements.
  
 3.
  
 The program uses the 
 PREPARE
  statement to pass the statement string to the DBMS, 
 which parses, validates, and optimizes the statement and generates an application 
  
 plan. This is identical to the 
 PREPARE
  processing used for other dynamic SQL 
 statements.
  
 4.
  
 The program uses the 
 DESCRIBE
  statement to request a description of the query 
 results that will be produced by the query. The DBMS returns a column-by-column 
  
 description of the query results in a SQL Data Area (
 SQLDA
 ) supplied by the program, 
 telling the program how many columns of query results there are, and the name, data 
 type, and length of each column. The 
 DESCRIBE
  statement is used exclusively for 
 dynamic queries.
  
 5.
  
 The program uses the column descriptions in the 
 SQLDA
  to allocate a block of 
 memory to receive each column of query results. The program may also allocate 
  
 space for an indicator variable for the column. The program places the address of the 
 data area and the address of the indicator variable into the 
 SQLDA
  to tell the DBMS 
 where to return the query results.
  
 6. A dynamic version of the 
 OPEN
  statement asks the DBMS to start executing the query 
 and passes values for the parameters specified in the dynamic 
 SELECT
  statement. 
  
 The 
 OPEN
  statement positions the cursor before the first row of query results.
  
 7.
  
 A dynamic version of the 
 FETCH
  statement advances the cursor to the first row of 
 query results and retrieves the data into the program's data areas and indicator 
  
 variables. Unlike the static 
 FETCH
  statement, which specifies a list of host variables to 
 receive the data, the dynamic 
 FETCH
  statement uses the 
 SQLDA
  to tell the DBMS 
 where to return the data. Subsequent 
 FETCH
  statements move through the query 
 results row by row, advancing the cursor to the next row of query results and 
  
 retrieving its data into the program's data areas.
  
 8. The 
 CLOSE
  statement ends access to the query results and breaks the association 
 between the cursor and the query results. This 
 CLOSE
  statement is identical to the 
 static SQL 
 CLOSE
  statement; no extensions are required for dynamic queries.
  
 The programming required to perform a dynamic query is more extensive than the 
  
 - 403 -",NA
The ,NA,NA
DESCRIBE,NA,NA
 Statement,"The 
 DESCRIBE
  statement, shown in Figure 18-10, is unique to dynamic queries. It is 
 used to request a description of a dynamic query from the DBMS. The 
 DESCRIBE 
 statement is used after the dynamic query has been compiled with the 
 PREPARE
  
 - 407 -",NA
The ,NA,NA
DECLARE CURSOR,NA,NA
 Statement,"The dynamic 
 DECLARE CURSOR
  statement, shown in Figure 18-11, is a variation of the 
 static 
 DECLARE CURSOR
  statement. Recall from 
 Chapter 17
  that the static 
 DECLARE 
 CURSOR
  statement literally specifies a query by including the 
 SELECT
  statement as one of 
 its clauses. By contrast, the dynamic 
 DECLARE CURSOR
  statement specifies the query 
  
 - 409 -",NA
The Dynamic ,NA,NA
OPEN,NA,NA
 Statement,"The dynamic 
 OPEN
  statement, shown in Figure 18-12, is a variation of the static 
 OPEN 
 statement. It causes the DBMS to begin executing a query and positions the associated 
 cursor just before the first row of query results. When the 
 OPEN
  statement completes 
 successfully, the cursor is in an open state and is ready to be used in a 
 FETCH
  statement.
  
  
 Figure 18-12: 
 Dynamic 
 OPEN
  statement syntax diagram
  
 The role of the 
 OPEN
  statement for dynamic queries parallels the role of the 
 EXECUTE 
 statement for other dynamic SQL statements. Both the 
 EXECUTE
  and the 
 OPEN 
  
 statements cause the DBMS to execute a statement previously compiled by the 
 PREPARE
  statement. If the dynamic query text includes one or more parameter markers, 
 then the 
 OPEN
  statement, like the 
 EXECUTE
  statement, must supply values for these 
 parameters. The 
 USING
  clause is used to specify parameter values, and it has an 
 identical format in both the 
 EXECUTE
  and 
 OPEN
  statements.
  
 If the number of parameters that will appear in a dynamic query is known in advance, the 
 program can pass the parameter values to the DBMS through a list of host variables in the 
 USING
  clause of the 
 OPEN
  statement. As in the 
 EXECUTE
  statement, the number of host 
 variables must match the number of parameters, the data type of each host variable must 
 be compatible with the type required by the corresponding parameter, and an indicator 
 variable can be specified for each host variable, if necessary. Figure 18-13 shows a 
 program excerpt where the dynamic query has three parameters whose values are 
 specified by host variables.
  
  .
  
  .
  
  .
  
 /* Program has previously generated and prepared a SELECT
  
  statement like this one:
  
  SELECT A, B, C ... 
  
 - 410 -",NA
The Dynamic ,NA,NA
FETCH,NA,NA
 Statement,"The dynamic 
 FETCH
  statement, shown in Figure 18-15, is a variation of the static 
 FETCH 
 statement. It advances the cursor to the next available row of query results and retrieves 
 the values of its columns into the program's data areas. Recall from 
 Chapter 17
  that the 
 static 
 FETCH
  statement includes an 
 INTO
  clause with a list of host variables that receive 
 the retrieved column values. In the dynamic 
 FETCH
  statement, the list of host variables is 
 replaced by a 
 SQLDA
 .
  
  
 Figure 18-15: 
 Dynamic 
 FETCH 
 statement syntax diagram
  
 - 412 -",NA
The Dynamic ,NA,NA
CLOSE,NA,NA
 Statement,"The dynamic form of the 
 CLOSE
  statement is identical in syntax and function to the static 
 CLOSE
  statement shown in 
 Figure 17-25
 . In both cases, the 
 CLOSE
  statement ends 
 access to the query results. When a program closes a cursor for a dynamic query, the 
 program normally should also deallocate the resources associated with the dynamic 
 query, including:
  
 •The 
 SQLDA
  allocated for the dynamic query and used in the 
 DESCRIBE
  and 
 FETCH 
 statements
  
 •A possible second 
 SQLDA
 , used to pass parameter values to the 
 OPEN
  statement
  
 •The data areas allocated to receive each column of query results retrieved by a 
 FETCH 
 statement
  
 •The data areas allocated as indicator variables for the columns of query results
  
 It may not be necessary to deallocate these data areas if the program will terminate 
 immediately after the 
 CLOSE
  statement.",NA
Dynamic SQL Dialects,"Like the other parts of the SQL language, dynamic SQL varies from one brand of DBMS to 
 another. In fact, the differences in dynamic SQL support are more serious than for static 
 SQL, because dynamic SQL exposes more of the ""nuts and bolts"" of the underlying 
 DBMS—data types, data formats, and so on. As a practical matter, these differences 
 make it impossible to write a single, general-purpose database front-end that is portable 
 across different DBMS brands using dynamic SQL. Instead, database front-end programs 
 must include a ""translation layer,"" often called a 
 driver
 , for each brand of DBMS that they 
 support to accommodate the differences.
  
 The early front-end products usually shipped with a separate driver for each of the 
  
 - 413 -",NA
Dynamic SQL in SQL/DS,"SQL/DS, for many years IBM's flagship relational database for IBM's mainframe VM 
 operating system, provides the same basic dynamic SQL support as DB2. It also supports 
 a feature called 
 extended dynamic SQL.
  With extended dynamic SQL, you can write a 
 program that prepares a statement string and permanently stores the compiled statement 
 in the database. The compiled statement can then be executed very efficiently, either by 
 the same program or by a different program, without having to be prepared again. Thus 
 extended dynamic SQL provides some of the performance advantages of static SQL in a 
 dynamic SQL context.
  
 The prepared statements in a SQL/DS database are stored in an 
 access module,
  which 
 is a named collection of compiled statements. SQL/DS users may have their own sets of 
 access modules, protected by SQL/DS privileges. To create an empty access module, 
 you use the SQL/DS 
 CREATE PROGRAM
  statement, specifying a name of up to eight 
 characters:
  
 CREATE PROGRAM OPSTMTS
  
 You can later remove the access module from the database with the 
 DROP PROGRAM 
 statement:
  
 DROP PROGRAM OPSTMTS
  
 Note that although the statements are called 
 CREATE PROGRAM
  and 
 DROP PROGRAM
 , 
 they actually operate on access modules. Often, however, the set of compiled statements 
 stored in an access module are, in fact, the set of statements used by a single program.
  
 Once an access module has been created, a program can store compiled statements in it 
 and execute those compiled statements. Special extended versions of the dynamic SQL 
 PREPARE
 , 
 DROP
 , 
 DESCRIBE
 , 
 EXECUTE
 , 
 DECLARE CURSOR
 , 
 OPEN
 , 
 FETCH
 , and 
 CLOSE 
 statements, shown in Figure 18-16, are used for this purpose. These statements are 
 supported by the SQL/DS precompiler for use in host programs written in IBM S/370 
 assembly language.
  
 - 414 -",NA
Dynamic SQL in Oracle *,"The Oracle DBMS preceded DB2 into the market and based its dynamic SQL support 
 upon IBM's System/R prototype. For this reason, the Oracle support for dynamic SQL 
 differs somewhat from the IBM SQL standard. Although Oracle and DB2 are broadly 
 compatible, they differ substantially at the detail level. These differences include Oracle's 
 use of parameter markers, its use of the 
 SQLDA
 , the format of its 
 SQLDA
 , and its support 
 for data type conversion. The Oracle differences from DB2 are similar to those you may 
 encounter in other DBMS brands. For that reason it is instructive to briefly examine 
 Oracle's dynamic SQL support and its points of difference from DB2.
  
 Named Parameters
  
 Recall that DB2 does not allow host variable references in a dynamically prepared 
 statement. Instead, parameters in the statement are identified by question marks 
 (parameter markers), and values for the parameters are specified in the 
 EXECUTE
  or 
 OPEN
  statement. Oracle allows you to specify parameters in a dynamically prepared 
 statement using the syntax for host variables. For example, this sequence of embedded 
 SQL statements is legal for Oracle:
  
 exec sql begin declare section;
  
  char  stmtbuf[1001];
  
  int   employee_number;
  
 exec sql end declare section;
  
  .
  
  .
  
  .
  
 strcpy(stmtbuf, ""delete from salesreps
  
  where empl_num = :rep_number;"");
  
 exec sql prepare delstmt from :stmtbuf;
  
 exec sql execute delstmt using :employee_number;
  
 Although 
 rep_number
  appears to be a host variable in the dynamic 
 DELETE
  statement, it 
 is in fact a 
 namedparameter
 . As shown in the example, the named parameter behaves 
 exactly like the parameter markers in DB2. A value for the parameter is supplied from a 
 ""real"" host variable in the 
 EXECUTE
  statement. Named parameters are a real 
  
 - 416 -",NA
Dynamic SQL and the SQL2 Standard,"The SQL1 standard did not address dynamic SQL, so the 
 de facto
  standard for dynamic 
 SQL, as described in the preceding sections, was set by IBM's implementation in DB2. 
  
 The SQL2 standard explicitly included a standard for dynamic SQL, specified in a 
 separate chapter of the standard that is nearly 50 pages long. In the simplest areas of 
 dynamic SQL, the new SQL2 standard closely follows the dynamic SQL currently used by 
 commercial DBMS products. But in other areas, including even the most basic dynamic 
 SQL queries, the new standard introduces incompatibilities with existing DBMS products, 
 which will require the rewriting of applications. The next several sections describe the 
 SQL2 standard for dynamic SQL in detail, with an emphasis on the differences from the 
 DB2-style dynamic SQL described in the preceding sections.
  
 In practice, support for SQL2-style dynamic SQL is appearing slowly in commercial DBMS 
 products, and most dynamic SQL programming still requires the use of the ""old,"" DB2-
 style dynamic SQL. Even when a new version of a DBMS product supports the new SQL2 
 statements, the DBMS vendor always provides a precompiler option that accepts the ""old"" 
 dynamic SQL structure used by the particular DBMS. Often, this is the default option for 
 the precompiler, because with thousands and thousands of SQL programs already in 
 existence, the DBMS vendor has an absolute requirement that new DBMS versions do not 
 ""break"" old programs. Thus, the migration to portions of SQL2 that represent 
 incompatibilities with current practice will be a slow and evolutionary one.",NA
Basic Dynamic SQL2 Statements,"The SQL2 statements that implement basic dynamic SQL statement execution (that is, 
 dynamic SQL that does not involve database queries) are shown in Figure 18-18. These 
 statements closely follow the DB2 structure and language. This includes the single-step 
 and two-step methods of executing dynamic SQL statements.
  
  
 Figure 18-18: 
 SQL2 dynamic SQL statements
  
 The SQL2 
 EXECUTE IMMEDIATE
  statement has an identical syntax and operation to that 
  
 - 419 -",NA
SQL2 and the ,NA,NA
SQLDA,- 420 -,NA
SQL2 and Dynamic SQL Queries,"In the dynamic SQL statements of the preceding sections, the SQL2 descriptor, like the 
 SQLDA
  it replaces, is used to pass parameter information from the host program to the 
 DBMS, for use in dynamic statement execution. The SQL2 standard also uses the SQL 
 descriptor in dynamic query statements where, like the 
 SQLDA
  it replaces, it controls the 
 passing of query result from the DBMS back to the host program. 
 Figure 18-9
  lists a DB2-
 style dynamic SQL query program. It's useful to examine how the program in 
 Figure 18-9 
 would change to conform to the SQL2 standard. Again the flow of the program remains 
 identical under SQL2, but the specifics change quite a lot. The SQL2 forms of the dynamic 
 SQL query-processing statements are shown in Figure 18-21.
  
  
 Figure 18-21: 
 SQL2 dynamic query processing statements
  
 The declaration of the cursor for the dynamic query, in callout 1 of 
 Figure 18-9
 , remains 
  
 - 426 -",NA
Summary,"This chapter described dynamic SQL, an advanced form of embedded SQL. Dynamic 
 SQL is rarely needed to write simple data processing applications, but it is crucial for 
 building general-purpose database front-ends. Static SQL and dynamic SQL present a 
 classic trade-off between efficiency and flexibility, which can be summarized as follows:
  
 •
 Simplicity.
  Static SQL is relatively simple; even its most complex feature, cursors, can be 
 easily understood in terms of familiar file input/output concepts. Dynamic SQL is complex, 
 requiring dynamic statement generation, variable-length data structures, and memory 
 management, with memory allocation/deallocation, data type alignment, pointer 
 management, and associated issues.
  
 •
 Performance.
  Static SQL is compiled into an application plan at compile-time; dynamic 
 SQL must be compiled at run-time. As a result, static SQL performance is generally much 
 better than that of dynamic SQL. The performance of dynamic SQL is 
  
 dramatically impacted by the quality of the application design; a design that minimizes the 
 amount of compilation overhead can approach static SQL performance.
  
 •
 Flexibility.
  Dynamic SQL allows a program to decide at run-time what specific SQL 
 statements it will execute. Static SQL requires that all SQL statements be coded in 
 advance, when the program is written, limiting the flexibility of the program.
  
 Dynamic SQL uses a set of extended embedded SQL statements to support its dynamic 
 features:
  
 •The 
 EXECUTE IMMEDIATE
  statement passes the text of a dynamic SQL statement to the 
 DBMS, which executes it immediately.
  
 •
  
 The 
 PREPARE
  statement passes the text of a dynamic SQL statement to the DBMS, 
 which compiles it into an application plan but does not execute it. The dynamic 
  
 statement may include parameter markers whose values are specified when the 
 statement is executed.
  
 •The 
 EXECUTE
  statement asks the DBMS to execute a dynamic statement previously 
 compiled by a 
 PREPARE
  statement. It also supplies parameter values for the statement 
 that is to be executed.
  
 - 429 -",NA
Chapter 19: ,NA,NA
SQL APIs,NA,NA
Overview,"The embedded SQL technique for programmatic access to SQL-based databases was 
 pioneered by the early IBM relational database prototypes and was widely adopted by 
 mainstream SQL products. However, several major DBMS products, led by Sybase's first 
 SQL Server implementation, took a very different approach. Instead of trying to blend SQL 
 with another programming language, these products provide a library of function calls as 
 an application programming interface (API) for the DBMS. To pass SQL 
  
 statements to the DBMS, an application program calls functions in the API, and it calls 
 other functions to retrieve query results and status information from the DBMS.
  
 For many programmers, a SQL API is a very straightforward way to use SQL. Most 
 programmers have some experience in using function libraries for other purposes, such 
 as string manipulation, mathematical functions, file input/output, and screen forms 
 management. Modern operating systems, such as Unix and Windows, extensively use 
 API suites to extend the core capabilities provided by the OS itself. The SQL API thus 
 becomes ""just another library"" for the programmer to learn.
  
 Over the last several years, SQL APIs have become very popular, equaling if not 
  
 surpassing the popularity of the embedded SQL approach for new applications 
  
 development. This chapter describes the general concepts used by all SQL API interfaces. 
 It also describes specific features of some of the APIs used by popular SQL-based DBMS 
 systems, and Microsoft's ODBC API that has become a 
 de facto
  SQL API standard. Finally, 
 it covers the ANSI/ISO SQL Call Level Interface standard, which is based on the core of the 
 ODBC interface.",NA
API Concepts,"When a DBMS supports a function call interface, an application program communicates 
 with the DBMS exclusively through a set of calls that are collectively known as an 
 application program interface,
  or API. The basic operation of a typical DBMS API is 
 illustrated in Figure 19-1:
  
 - 430 -",NA
The ,NA,NA
dblib,NA,NA
 API (SQL Server),"The first major DBMS product to emphasize its callable API was Sybase's and Microsoft's 
 SQL Server. For many years, the SQL Server callable API was the 
 only
  interface offered 
 by these products. Both Microsoft and Sybase now offer embedded SQL capabilities and 
 have added newer or higher-level callable APIs, but the original SQL Server API remains 
 a very popular way to access these DBMS brands. The SQL Server API also provided the 
 model for much of Microsoft's ODBC API. SQL Server and its API are also an excellent 
 example of a DBMS designed from the ground up around a client/server architecture. For 
 all of these reasons, it's useful to begin our exploration of SQL APIs by examining the 
 basic SQL Server API.
  
 The original SQL Server API, which is called the 
 database library
  or 
 dblib
 , consists of 
 about 100 functions available to an application program. The API is very comprehensive, 
 but a typical program uses only about a dozen of the function calls, which are 
  
 summarized in Table 19-1. The other calls provide advanced features, alternative methods 
 of interacting with the DBMS, or single-call versions of features that otherwise would 
 require multiple calls.
  
 Table 19-1: Basic 
 dblib
  API Functions
  
 Function
  
 Description
  
 Database 
  
 connection/disconnection
  
 dblogin() 
  
 Provides a data structure for login information
  
 - 432 -",NA
Basic SQL Server Techniques ,"A simple SQL Server program that updates a database can use a very small set of 
 dblib
  calls to do its work. The program in Figure 19-3 implements a simple quota 
 update application for the 
 SALESREPS
  table in the sample database. It is identical to the 
 program in 
 Figure 17-17
 , but uses the SQL Server API instead of embedded SQL. The 
 figure illustrates the basic interaction between a program and SQL Server:
  
 - 433 -",NA
SQL Server Queries,"The SQL Server technique for handling programmatic queries is very similar to its 
 technique for handling other SQL statements. To perform a query, a program sends a 
 SELECT
  statement to SQL Server and uses 
 dblib
  to retrieve the query results row by 
 row. The program in Figure 19-7 illustrates the SQL Server query-processing technique:
  
 main()
  
 {
  
  LOGINREC  *loginrec;           /* data structure for login 
  
 - 439 -",NA
Positioned Updates,"In an embedded SQL program, a cursor provides a direct, intimate link between the 
 program and the DBMS query processing. The program communicates with the DBMS 
 row by row as it uses the 
 FETCH
  statement to retrieve query results. If the query is a 
 simple, single-table query, the DBMS can maintain a direct correspondence between the 
 current row of query results and the corresponding row within the database. Using this 
 correspondence, the program can use the positioned update statements (
 UPDATE
 …
  
 - 444 -",NA
Dynamic Queries,"In the program examples thus far in this chapter, the queries to be performed were known 
 in advance. The columns of query results could be bound to program variables by explicit 
 dbbind()
  calls hard-coded in the program. Most programs that use SQL Server can be 
 written using this technique. (This static column binding corresponds to the fixed list of 
 host variables used in the static SQL 
 FETCH
  statement in standard embedded SQL, 
 described in 
 Chapter 17
 .)
  
 If the query to be carried out by a program is not known at the time the program is 
 written, the program cannot include hard-coded 
 dbbind()
  calls. Instead, the program 
 must ask 
 dblib
  for a description of each column of query results, using special API 
 functions. The program can then bind the columns ""on the fly"" to data areas that it 
 allocates at run-time. (This dynamic column binding corresponds to the use of the 
 dynamic SQL 
 DBNUMCOLS()
  statement and 
 SQLDA
 , in dynamic embedded SQL, as 
 described in 
 Chapter 18
 .)
  
 Figure 19-10 shows an interactive query program that illustrates the 
 dblib
  technique for 
 handling dynamic queries. The program accepts a table name entered by the user and 
 then prompts the user to choose which columns are to be retrieved from the table. As the 
 user selects the columns, the program constructs a 
 SELECT
  statement and then uses 
 these steps to execute the 
 SELECT
  statement and display the data from the selected 
 columns:
  
 main()
  
 {
  
  /* This is a simple general-purpose query program. It prompts
  
  
  the user for a table name and then asks the user which 
 columns
  
  
  of the table are to be included in the query. After the 
 user's
  
  
  selections are complete, the program runs the requested 
 query and
  
  
  displays the results.
  
  */
  
 - 445 -",NA
ODBC and the SQL/CLI Standard,"Open Database Connectivity (ODBC) is a database-independent callable API suite 
 originally developed by Microsoft. Although Microsoft plays an important role as a 
 database software vendor, its development of ODBC was motivated even more by its role 
 as a major operating system developer. Microsoft wanted to make it easier for developers 
 of Windows applications to incorporate database access. But the large differences 
 between the various database systems and their APIs made this very difficult. If an 
 application developer wanted a program to work with several different DBMS brands, it 
 had to provide a separate, specially written database interface module (usually called a 
 ""driver"") for each one. Each application program that wanted to provide access to multiple 
 databases had to provide a set of drivers.
  
 Microsoft's solution to this problem was to create ODBC as a uniform, standardized 
 database access interface, and incorporate it into the Windows operating system. For 
  
 - 449 -",NA
Call-Level Interface Standardization,"ODBC would have been important even as a Microsoft-only standard. However, Microsoft 
 worked to make it a vendor-independent standard as well. A database vendor association 
 called the SQL Access Group was working on standardizing client/server protocols for 
 remote database access at about the same time as Microsoft's original development of 
 ODBC. Microsoft persuaded the SQL Access Group to expand their focus and adopt 
 ODBC as their standard for vendor-independent database access. Management of the 
 SQL Access Group standard was eventually turned over to the European X/Open 
 consortium, another standards organization, as part of its overall standards for a 
  
 Common Application Environment (CAE).
  
 With the growing popularity of call-level APIs for database access, the official SQL 
 standards groups eventually turned their attention to standardization of this aspect of 
 SQL. The X/Open standard (based on Microsoft's earlier ODBC work) was taken as a 
 starting point and slightly modified to create an official ANSI/ISO standard. The resulting 
 SQL/Call Level Interface (SQL/CLI) standard was published in 1995 as ANSI/ISO/IEC 
 9075-3-1995. It is officially 
 Part 3
  of a contemplated multi-part standard that will be an 
 evolution of the SQL2 standard published in 1992.
  
 Microsoft has since evolved ODBC to conform to the official SQL/CLI standard. The CLI 
 standard roughly forms the core level of Microsoft's ODBC 3 revision. Other, higher-level 
 capabilities of ODBC 3 go beyond the CLI specification to provide more API functionality 
 and to deal with the specific problems of managing ODBC as part of the Windows 
 operating system. In practice, the core-level ODBC capabilities and the SQL/CLI 
 specification form the effective ""callable API standard.""
  
 Because of its substantial advantages for both application developers and database 
 vendors, ODBC/CLI has become a very widely supported standard. Virtually all SQL-
 based database systems provide an ODBC/CLI interface as one of their supported 
 interfaces. Some DBMS brands have even adopted ODBC/CLI as their standard database 
 API. Thousands of application programs support ODBC/CLI, including all of the leading 
 programming tools packages, query- and forms-processing tools and report writers, and 
 popular productivity software such as spreadsheets and graphics programs.
  
 The SQL/CLI standard includes about forty different API calls, summarized in Table 19-2. 
  
 The calls provide a comprehensive facility for establishing connections to a database 
 server, executing SQL statements, retrieving and processing query results, and handling 
 errors in database processing. They provide all of the capabilities available through the 
 standard's embedded SQL interface, including both static SQL and dynamic SQL 
 capabilities.
  
 Table 19-2: SQL/CLI API Functions
  
 Function
  
 Description
  
 Resource and connection 
  
 management
  
 SQLAllocHandle() 
  
 Allocates resources for environment, connection, 
  
 descriptor, or statement
  
 - 450 -",NA
CLI Structures,"The CLI manages interactions between an application program and a supported 
 database through a hierarchy of concepts, reflected in a hierarchy of CLI data structures:
  
 •
 SQL-environment.
  The highest-level ""environment"" within which database access 
 takes place. The CLI uses the data structure associated with a SQL-environment to 
 keep track of the various application programs that are using it.
  
 •
 SQL-connection.
  A logical ""connection"" to a specific database server. Conceptually, the 
 CLI allows a single application program to connect to several different database servers 
 concurrently. Each connection has its own data structure, which the CLI uses to track 
 connection status.
  
 •
 SQL-statement.
  An individual SQL statement to be processed by a database server. A 
 statement may move through several stages of processing, as the DBMS prepares 
 (compiles) it, executes it, processes any errors, and in the case of queries, returns the 
 results to the application program. Conceptually, an application program may have 
 multiple SQL statements moving through these processing stages in parallel. Each 
 statement has its own data structure, which the CLI uses to track its progress.
  
 The CLI uses a technique commonly used by modern operating systems and library 
 packages to manage these conceptual entities. A symbolic pointer called a 
 handle
  is 
 associated with the overall SQL environment, with a SQL connection to a specific 
 database server, and with the execution of a SQL statement. The handle identifies an area 
 of storage managed by the CLI itself. Some type of handle is passed as one of the 
 parameters in every CLI call. The CLI routines that manage handles are shown in Figure 
 19-12.
  
 /* Allocate a handle for use in subsequent CLI calls */
  
 short SQLAllocHandle (
  
  
  short  HdlType,               /* IN:  integer handle type code 
 */
  
  
  long   inHdl,                 /* IN:  environment or conn 
 handle */
  
  
  long  *rtnHdl)                /* OUT: returned handle */
  
 /* Free a handle previously allocated by SQLAllocHandle() */
  
 short SQLFreeHandle (
  
  
  short  HdlType,               /* IN:  integer handle type code 
 */
  
  
  long   inHdl)                 /* IN:  handle to be freed */
  
 /* Allocate a handle for a new SQL-environment */
  
 short SQLAllocEnv (
  
  
  long  *envHdl)                /* OUT: returned environment 
 handle */
  
 - 455 -",NA
CLI Statement Processing,"The CLI processes SQL statements using a technique very similar to that described for 
 dynamic embedded SQL in the previous chapter. The SQL statement is passed to the 
 CLI in text form, as a character string. It can be executed in a one-step or two-step 
 process.
  
 Figure 19-14 shows the basic SQL statement-processing calls. The application program 
 must first call 
 SQLAllocHandle()
 to obtain a statement handle, which identifies the 
 statement to the program and the CLI. All subsequent 
 SQLExecDirect()
 , 
  
 SQLPrepare()
 , and 
 SQLExecute()
  calls reference this statement handle. When the 
 handle is no longer needed, it is freed with a 
 SQLFreeHandle()
  call.
  
 /* Directly execute a SQL statement */ 
  
 short SQLExecDirect (
  
  
  long   stmtHdl,         /* IN:  statement handle */
  
  char  *stmttext,        /* IN:  SQL statement text */
  
  short  textlen)         /* IN:  statement text length */
  
 /* Prepare a SQL statement */ 
  
 short SQLPrepare (
  
  
  long   stmtHdl,         /* IN:  statement handle */
  
  char  *stmttext,        /* IN:  SQL statement text */
  
  short  textlen)         /* IN:  statement text length */
  
 /* Execute a previously-prepared SQL statement */ 
 short SQLExecute (
  
  
  long   stmtHdl)         /* IN:  statement handle */
  
 /* Bind a SQL statement parameter to a program data area */ short 
 SQLBindParam (
  
  
  long   stmtHdl,         /* IN:  statement handle */
  
  
  short  parmnr,          /* IN:  parameter number (1,2,3...) */
  
  short  valtype,         /* IN:  data type of value supplied */
  
  short  parmtype,        /* IN:  data type of parameter */
  
  short  colsize,         /* IN:  column size */
  
  
  short  decdigits,       /* IN:  number of decimal digits */
  
 - 458 -",NA
CLI Errors and Diagnostic Information,"Each CLI function returns a short integer value that indicates its completion status. If the 
 completion status indicates an error, the error-handling CLI calls shown in Figure 19-21 
 can be used to obtain more information about the error and diagnose it. The most basic 
 error-handling call is 
 SQLError()
 . The application program passes the environment, 
 connection, and statement handles and is returned the SQL2 
 SQLSTATE
  result code, the 
 native error code of the subsystem producing the error, and an error message in text form.
  
 /* Retrieve error information associated with a previous CLI call 
 */ 
  
 short SQLError (
  
  long   envHdl,            /* IN:  environment handle */
  
  long   connHdl,           /* IN:  connection handle */
  
  long   stmtHdl,           /* IN:  statement handle */
  
  
  char  *sqlstate,          /* OUT: five-character SQLSTATE 
 value */
  
  
  long  *nativeerr,         /* OUT: returned native error code 
 */
  
  
  char  *msgbuf,            /* OUT: buffer for err message text 
 */
  
  
  short  buflen,            /* IN:  length of err msg text 
 buffer */
  
  
  short *msglen)            /* OUT: returned actual msg length 
 */
  
 - 475 -",NA
CLI Attributes,"The CLI provides a number of options that control some of the details of its processing. 
 Some of these control relatively minor but critical details, such as whether or not the CLI 
 should automatically assume that parameters passed as string values are null-
  
 terminated. Others control broader aspects of CLI operation, such as the scrollability of 
 cursors.
  
 The CLI gives application programs the ability to control these processing options through 
 a set of CLI 
 attributes
 . The attributes are structured in a hierarchy, paralleling the 
 environment / connection / statement hierarchy of the CLI handle structure. Environment 
 attributes control overall operational options. Connection options apply to a particular 
 connection created by the 
 SQLConnect()
  call but may vary from one connection to 
 another. Statement attributes apply to the processing of an individual statement, 
  
 identified by a CLI statement handle.
  
 A set of CLI calls, shown in Figure 19-22, are used by an application program to control 
 attributes. The ""get"" calls (
 SQLGetEnvAttr()
 , 
 SQLGetConnectAttr()
 , and 
 SQLGetStmtAttr()
 ) obtain current attribute values. The ""set"" calls 
  
 (
 SQLSetEnvAttr()
 , 
 SQLSetConnectAttr()
 , and 
 SQLSetStmtAttr()
 ) modify the 
 current attribute values. In all of the calls, the particular attribute being processed is 
 indicated by a code value.
  
 /* Obtain the value of a SQL-environment attribute */ 
  
 short SQLGetEnvAttr(
  
  
  long   envHdl,            /* IN:  environment handle */
  
  
 long   attrCode,          /* IN:  integer attribute code */
  
  
 void  *rtnVal,            /* OUT: return value */
  
  
  long   bufLen,            /* IN:  length of rtnVal buffer */
  
  long  *strLen)            /* OUT: length of actual data */
  
 /* Set the value of a SQL-environment attribute */ 
  
 short SQLSetEnvAttr(
  
  
  long   envHdl,            /* IN:  environment handle */
  
  
 long   attrCode,          /* IN:  integer attribute code */
  
  
 void  *attrVal,           /* IN:  new attribute value */
  
  
 long  *strLen)            /* IN:  length of data */
  
 /* Obtain the value of a SQL-connection attribute */ 
  
 short SQLGetConnectAttr(
  
  
  long   connHdl,           /* IN:  connection handle */
  
  
 long   attrCode,          /* IN:  integer attribute code */
  
  
 void  *rtnVal,            /* OUT: return value */
  
  
  long   bufLen,            /* IN:  length of rtnVal buffer */
  
  long  *strLen)            /* OUT: length of actual data */
  
 /* Set the value of a SQL-connection attribute */ 
  
 short SQLSetConnectAttr(
  
  
  long   connHdl,           /* IN:  connection handle */
  
  
 long   attrCode,          /* IN:  integer attribute code */
  
  
 void  *attrVal,           /* IN:  new attribute value */
  
  
 long  *strLen)            /* IN:  length of data */
  
 - 477 -",NA
CLI Information Calls,"The CLI includes three specific calls that can be used to obtain information about the 
 particular CLI implementation. In general, these calls will not be used by an application 
 program written for a specific purpose. They are needed by general-purpose programs 
 (such as a query or report writing program) that need to determine the specific 
 characteristics of the CLI they are using. The calls are shown in Figure 19-23.
  
 /* Retrieve detailed info about capabilities of a CLI 
 implementation */ 
  
 short SQLGetInfo (
  
  long   connHdl,           /* IN:  connection handle */
  
  short  infotype,          /* IN:  type of info requested */
  
 - 478 -",NA
The ODBC API,"As described earlier in this chapter, Microsoft originally developed the Open Database 
 Connectivity (ODBC) API to provide a database-brand-independent API for database 
 access on its Windows operating systems. The early ODBC API became the foundation 
 for the SQL/CLI standard, which is now the official ANSI/ISO standard for a SQL call-
 level interface. The original ODBC API was extended and modified during the 
  
 standardization process to create the SQL/CLI specification. With the introduction of 
 ODBC release 3.0, Microsoft brought ODBC into conformance with the SQL/CLI 
 standard. With this revision, ODBC becomes a superset of the SQL/CLI specification.
  
 ODBC goes beyond the SQL/CLI capabilities in several areas, in part because 
  
 Microsoft's goals for ODBC were broader than simply creating a standardized database 
 access API. Microsoft also wanted to allow a single Windows application program to be 
 able to concurrently access several different databases using the ODBC API. It also 
 wanted to provide a structure where database vendors could support ODBC without 
 giving up their proprietary APIs, and where the software that provided ODBC support for 
 a particular brand of DBMS could be distributed by the database vendor and installed on 
 Windows-based client systems as needed. The layered structure of ODBC and special 
 ODBC management calls provide these capabilities.
  
 - 479 -",NA
The Structure of ODBC,"The structure of ODBC as it is provided on Windows-based or other operating systems 
 as shown in Figure 19-24. There are three basic layers to the ODBC software:
  
  
 Figure 19-24: 
 ODBC architecture
  
 •
 Callable API
 . At the top layer, ODBC provides a single, callable database access API 
 that can be used by all application programs. The API is packaged as a dynamic-linked 
 library (DLL), which is an integral part of the various Windows operating systems.
  
 •
 ODBC drivers
 . At the bottom layer of the ODBC structure is a collection of ODBC drivers. 
 There is a separate driver for each of the DBMS brands. The purpose of the driver is to 
 translate the standardized ODBC calls into the appropriate call or calls for the specific 
 DBMS that it supports. Each driver can be independently installed on a particular 
 computer system. This allows the DBMS vendors to provide an ODBC driver for their 
 particular brand of DBMS and distribute the driver independent of the 
  
 Windows operating system software. If the database resides on the same system as the 
 ODBC driver, the driver is usually linked directly to the database's native API code. 
  
 If the database is to be accessed over a network, the driver may call a native DBMS 
 client to handle the client/server connection, or the driver might handle the network 
 connection itself.
  
 •
 Driver manager
 . In the middle layer of the ODBC structure is the ODBC driver manager. 
 Its role is to load and unload the various ODBC drivers, on request from application 
 programs. The driver manager is also responsible for routing the API calls made by 
 application programs to the appropriate driver for execution.
  
 When an application program wants to access a database via ODBC, it goes through the 
 same initiation sequence specified by the SQL/CLI standard. The program allocates an 
 environment handle, and then a connection handle, and then calls 
 SQLConnect()
 , 
 specifying the particular data source to be accessed. When it receives the 
  
 SQLConnect()
  call, the ODBC driver manager examines the connection information 
 provided and determines the appropriate ODBC driver that is needed. The driver manager 
 loads the driver into memory if it's not already being used by another application program. 
 Subsequent calls by the application program on this particular CLI/ODBC connection are 
 routed to this driver. The application program can, if appropriate, make other 
 SQLConnect()
  calls for other data sources that will cause the driver manager to 
 concurrently load other drivers for other DBMS brands. The application program can then 
 use ODBC to communicate with two or more different databases, of different brands, 
 using a uniform API.",NA
ODBC and DBMS Independence,- 480 -,NA
ODBC Catalog Functions,"One of the areas where ODBC offers capability beyond the SQL/CLI standard is the 
 retrieval of information about the structure of a database from its system catalog. As a part 
 of the ANSI/ISO SQL standard, the CLI assumes that this information (about tables, 
 columns, privileges, and so forth) is available through the SQL2 Information Schema, as 
 described in 
 Chapter 16
 . ODBC doesn't assume the presence of an Information Schema. 
  
 Instead, it provides a set of specialized functions, shown in Table 19-4, that provide 
 information about the structure of a data source. By calling these functions and 
  
 processing their results, an application program can determine, at run-time, information 
 about the tables, columns, privileges, primary keys, foreign keys, and stored procedures 
 that form the structure of a data source.
  
 Table 19-4: ODBC Catalog Functions
  
 Function
  
 Description
  
 SQLTables()
  
 Returns a list of tables within specified catalog(s) and 
  
 schema(s)
  
 SQLTablePrivileges() 
 Returns a list of privileges for a table or tables
  
 SQLColumns() 
  
 Returns a list of the column names in a specified table or 
  
 tables
  
 SQLColumnPrivileges() 
 Returns a list of columns and their privileges for a 
  
 particular table
  
 SQLPrimaryKeys() 
  
 Returns a list of the column names that make up the 
  
 primary key for a table
  
 SQLForeignKeys() 
  
 Returns a list of foreign keys in a specified table and a list 
  
 of foreign keys in other tables that refer to the specified 
  
 table
  
 SQLSpecialColumns() 
  
 Returns a list of the columns that uniquely identify rows 
 in 
  
 a table or columns that are automatically updated when a 
  
 row is updated
  
 - 481 -",NA
Extended ODBC Capabilities,"ODBC provides a set of extended capabilities beyond those specified in the SQL/CLI 
 standard. Many of the capabilities are designed to improve the performance of ODBC-
 based applications by minimizing the number of ODBC function calls an application 
 program must make and/or the amount of network traffic generated by the ODBC calls. 
 Other capabilities provide useful features for maintaining database independence or aid 
 an application program in the database connection process. Some of the capabilities are 
 provided through the additional set of ODBC function calls shown in Table 19-5. Others 
 are provided through statement or connection attributes. Many of these additional 
 capabilities were introduced in the 3.0 revision of ODBC and are not yet supported by 
 most ODBC drivers or ODBC-based applications.
  
 Table 19-5: Additional ODBC Functions
  
 Function
  
 Description
  
 SQLBrowseConnect()
  
 Supplies information about available ODBC data sources and 
  
 the attributes required to connect to each
  
 SQLDrivers() 
  
 Returns a list of the available drivers and driver attribute 
  
 names
  
 SQLDriverConnect()
  
 SQLNumParams()
  
 Extended form of the 
 SQLConnect()
  call for passing 
 additional connection information
  
 Returns the number of parameters in a previously prepared 
  
 - 482 -",NA
The Oracle Call Interface (OCI),"The most popular programmatic interface to Oracle is embedded SQL. However, Oracle 
 also provides an alternative callable API, known as the 
 Oracle Call Interface
 , or OCI. OCI 
 has been available for many years and remained fairly stable through a number of major 
 Oracle upgrade cycles, including all of the Oracle7 versions. With the introduction of 
 Oracle8, OCI underwent a major revision, and many of the original OCI calls were 
 replaced by new, improved versions. However, the original OCI calls are still supported, 
 and tens of thousands of applications depend on them and thousands of programmers are 
 familiar with them.",NA
Legacy OCI,"The original OCI API includes about twenty calls, summarized in Table 19-6. The OCI 
 functions use the term ""cursor"" to refer to a connection to the Oracle database. A 
 program uses the 
 olon()
  call to logon to the Oracle database, but it must use the 
 oopen()
  call to open a cursor through which SQL statements can be executed. By 
 issuing multiple 
 oopen()
  calls, the application program can establish multiple cursors 
 (connections) and execute statements in parallel. For example, a program might be 
 retrieving query results on one of its connections and use a different connection to issue 
 UPDATE
  statements.
  
 Table 19-6: Oracle Call Interface Functions
  
 Function
  
 Description
  
 - 485 -",NA
OCI and Oracle8,"With the introduction of Oracle8, the Oracle Call Interface was effectively replaced with a 
 newer, more modern, and far more complex OCI. The ""new"" OCI uses many of the same 
 concepts as the SQL/CLI standard and ODBC, including the use of handles to identify 
 interface ""objects."" Several hundred routines are defined in the API, and a complete 
 description of them is beyond the scope of this book. The following sections identify the 
 major routines that will be used by most application programs and their functions.
  
 OCI Handles
  
 The new OCI uses a hierarchy of handles to manage interaction with an Oracle 
 database, like the handle hierarchy of the SQL/CLI described earlier in this chapter. The 
 handles are:
  
 - 488 -",NA
Summary,- 494 -,NA
Part VI:,NA,NA
 SQL Today and Tomorrow,NA,NA
Chapter List,"Chapter Database Processing and Stored Procedures 
 20: 
  
 Chapter SQL and Data Warehousing 
  
 21: 
  
 Chapter SQL Networking and Distributed Databases 
 22: 
  
 Chapter SQL and Objects 
  
 23: 
  
 Chapter The Future of SQL 
  
 24:",NA
Chapter 20:,NA,NA
 Database Processing and Stored ,NA,NA
Procedures,NA,NA
Overview,"The long-term trend in the database market is for databases to take on a progressively 
  
 - 495 -",NA
Stored Procedure Concepts,"In its original form, SQL was not envisioned as a complete programming language. It was 
 designed and implemented as a language for expressing database operations—creating 
 database structures, entering data into the database, updating database data—and 
 especially for expressing database queries and retrieving the answers. SQL could be used 
 interactively by typing SQL statements at a keyboard, one by one. In this case, the 
 sequence of database operations was determined by the human user. SQL could also be 
 embedded within another programming language, such as COBOL or C. In this case, the 
 sequence of database operations was determined by the flow of control within the COBOL 
 or C program.
  
 With stored procedures, several capabilities normally associated with programming 
 languages are ""grafted onto"" the SQL language. Sequences of ""extended SQL"" 
 statements are grouped together to form SQL programs or procedures. The specifics 
 vary from one implementation to another, but generally these capabilities are provided:
  
 •
 Conditional execution
 . An 
 IF…THEN…ELSE
  structure allows a SQL procedure to test a 
 condition and carry out different operations depending on the result.
  
 •
  
 Looping
 . A 
 WHILE
  or 
 FOR
  loop or similar structure allows a sequence of SQL 
  
 operations to be performed repeatedly, until some terminating condition is met. Some 
  
 implementations provide a special cursor-based looping structure to process each row 
 of query results.
  
 •
 Block structure
 . A sequence of SQL statements can be grouped into a single block 
 and used in other flow-of-control constructs as if the statement block were a single 
 statement.
  
 •
 Named variables
 . A SQL procedure may store a value that it has calculated, retrieved 
 from the database, or derived in some other way into a program variable, and later 
 retrieve the stored value for use in subsequent calculations.
  
 •
 Named procedures
 . A sequence of SQL statements may be grouped together, given a 
 name, and assigned formal input and output parameters, like a subroutine or function 
  
 - 496 -",NA
A Basic Example,"It's easiest to explain the basics of stored procedures through an example. Consider the 
 process of adding a customer to the sample database. Here are the steps that may be 
 involved:
  
 1.Obtain the customer number, name, credit limit, and target sales amount for the 
 customer, as well as the assigned salesperson and office.
  
 2.Add a row to the customer table containing the customer's data.
  
 3.Update the row for the assigned salesperson, raising the quota target by the specified 
 amount.
  
 4.Update the row for the office, raising the sales target by the specified amount.
  
 5.Commit the changes to the database, if all were successful.
  
 Without a stored procedure capability, here is a SQL statement sequence that does this 
 work for XYZ Corporation, new customer number 2137, with a credit limit of $30,000 and 
 first-year target sales of $50,000 to be assigned to Paul Cruz (employee #103) of the 
 Chicago office:
  
 INSERT INTO CUSTOMERS (CUST_NUM, COMPANY, CUST_REP, CREDIT_LIMIT)
  
  VALUES (2137, 'XYZ Corporation', 30000.00);
  
  
  UPDATE SALESREPS
  
  
   
  SET QUOTA = QUOTA + 50000.00
  
  
  
  WHERE EMPL_NUM = 103;
  
  
  UPDATE OFFICES
  
  
   
  SET TARGET = TARGET + 50000.00
  
  
  
  WHERE CITY = 'Chicago';
  
  
  COMMIT;
  
 With a stored procedure, all of this work can be embedded into a single defined SQL 
 routine. Figure 20-1 shows a stored procedure for this task, expressed in Oracle's PL/SQL 
 stored procedure dialect. The procedure is named 
 ADD_CUST
 , and it accepts six 
 parameters—the customer name, number, credit limit, and target sales, the employee 
 number of the assigned salesperson, and the city where the assigned sales office is 
 located.
  
 /* Add a customer procedure */ 
  
 create procedure add_cust (
  
  
  c_name   in varchar(20),            /* input customer name */
  
 - 497 -",NA
Using Stored Procedures,"The procedure defined in Figure 20-1 illustrates several of the basic structures common 
 to all SPL dialects. Nearly all dialects use a 
 CREATE PROCEDURE
  statement to initially 
 define a stored procedure. A corresponding 
 DROP PROCEDURE
  statement is used to 
 discard procedures that are no longer needed. The 
 CREATE PROCEDURE
  statement 
 defines the following.
  
 •The name of the stored procedure
  
 •The number and data types of its parameters
  
 - 498 -",NA
Creating a Stored Procedure,"In many common SPL dialects, the 
 CREATE PROCEDURE
  statement is used to create a 
 stored procedure and specify how it operates. The 
 CREATE PROCEDURE
  statement 
 assigns the newly defined procedure a name, which is used to call it. The name must 
 typically follow the rules for SQL identifiers (the procedure in Figure 20-1 is named 
 ADD_CUST)
 . A stored procedure accepts zero or more parameters as its arguments (this 
 one has six parameters, 
 C_NAME
 , 
 C_NUM
 , 
 CRED_LIMI
 , 
 TGT_SLS
 , 
 C_REP
 , and 
 C_OFFC
 ). 
  
 In all of the common SPL dialects, the values for the parameters appear in a comma-
 separated list, enclosed in parentheses, following the procedure name when the 
 procedure is called. The header of the stored procedure definition specifies the names of 
 the parameters and their data types. The same SQL data types supported by the DBMS 
 for columns within the database can be used as parameter data types.
  
 In Figure 20-1, all of the parameters are 
 input
  parameters (signified by the 
 IN
  keyword in 
 the procedure header in the Oracle PL/SQL dialect). When the procedure is called, the 
 parameters are assigned the values specified in the procedure call, and the statements in 
 the procedure body begin to execute. The parameter names may appear within the 
 procedure body (and particularly within standard SQL statements in the procedure body) 
 anywhere that a constant may appear. When a parameter name appears, the DBMS uses 
 its current value. In Figure 20-1, the parameters are used in the 
 INSERT
  statement and 
 the 
 UPDATE
  statement, both as data values to be used in column calculations and search 
 conditions.
  
 In addition to input parameters, some SPL dialects also support 
 output
  parameters. 
  
 These allow a stored procedure to ""pass back"" values that it calculates during its 
 execution. Output parameters aren't useful for stored procedures invoked from interactive 
 SQL, but they provide an important capability for passing back information from one stored 
 procedure to another stored procedure that calls it. Some SPL dialects support 
 parameters that operate as 
 both
  input and output parameters. In this case, the parameter 
 passes a value to the stored procedure, and any changes to the value during the 
 procedure execution are reflected in the calling procedure.
  
 Figure 20-2 shows the same 
 ADD_CUST
  procedure definition, expressed in the Sybase 
 Transact-SQL dialect. (The Transact-SQL dialect is also used by Microsoft SQL Server; its 
 basics are largely unchanged since the original Sybase SQL Server version, which was 
 the foundation for both the Microsoft and Sybase product lines.) Note the differences from 
 the Oracle dialect:
  
 /* Add a customer procedure */
  
 create proc add_cust
  
  @c_name   varchar(20),              /* input customer name */
  
  
  @c_num    integer,                  /* input customer number 
 */
  
  
  @cred_lim money,                    /* input credit limit */
  
  @tgt_sls  money,                    /* input target sales */
  
  @c_rep    integer,                  /* input salesrep emp # */
  
  @c_offc   varchar(15)               /* input office city */
  
 as
  
 - 499 -",NA
Calling a Stored Procedure,"Once defined by the 
 CREATE PROCEDURE
  statement, a stored procedure can be used. 
  
 An application program may request execution of the stored procedure, using the 
 appropriate SQL statement. Another stored procedure may call it to perform a specific 
 function. The stored procedure may also be invoked through an interactive SQL interface.
  
 The various SQL dialects differ in the specific syntax used to call a stored procedure. 
 Here is a call to the 
 ADD_CUST
  procedure in the PL/SQL dialect:
  
 EXECUTE ADD_CUST('XYZ 
  
 Corporation',2137,30000.00,50000.00,103,'Chicago')
  
 The values to be used for the procedure's parameters are specified, in order, in a list that 
 is enclosed by parentheses. When called from within another procedure or a trigger, the 
 EXECUTE
  statement may be omitted, and the call becomes simply:
  
 ADD_CUST('XYZ Corporation',2137,30000.00,50000.00,103,'Chicago')
  
 In the Transact-SQL dialect, the call to the stored procedure becomes:
  
 EXECUTE ADD_CUST 'XYZ 
  
 Corporation',2137,30000.00,50000.00,103,'Chicago'
  
 - 501 -",NA
Stored Procedure Variables,"In addition to the parameters passed into a stored procedure, it's often convenient or 
 necessary to define other variables to hold intermediate values during the procedure's 
 execution. All stored procedure dialects provide this capability. Usually the variables are 
 declared at the beginning of the procedure body, just after the procedure header and 
 before the list of SQL statements. The data types of the variables can be any of the SQL 
 data types supported as column data types by the DBMS.
  
 Figure 20-4 shows a simple Transact-SQL stored procedure fragment that computes the 
 total outstanding order amount for a specific customer number, and sets up one of two 
 messages depending on whether the total order amount is over or under $30,000. Note 
 that Transact-SQL local variable names, like parameter names, begin with an at sign (
 @
 ). 
  
 The 
 DECLARE
  statement declares the local variables for this procedure. In this case, 
 there are two variables, one with the 
 MONEY
  data type and one 
 VARCHAR
 .
  
 /* Check order total for a customer */
  
 create proc chk_tot 
  
  @c_num integer      /* one input parameter */
  
 as
  
  /* Declare two local variables */
  
  declare @tot_ord money, @msg_text varchar(30)
  
  begin
  
  /* Calculate total orders for customer */
  
  select @tot_ord = sum(amount)
  
  from orders
  
 - 502 -",NA
Statement Blocks,"In all but the very simplest stored procedures, it is often necessary to group a sequence of 
 SQL statements together so that they will be treated as if they were a single statement. 
  
 For example, in the 
 IF
 …
 THEN
 …
 ELSE
  structure typically used to control the flow of 
 execution within a stored procedure, most stored procedure dialects expect a single 
 statement following the 
 THEN
  keyword. If a procedure needs to perform a sequence of 
 several SQL statements when the tested condition is true, it must group the statements 
 together as a statement block, and this block will appear after 
 THEN
 .
  
 In Transact-SQL, a statement block has this simple structure:
  
 /* Transact-SQL block of statements */
  
 begin
  
  /* Sequence of SQL statements appears here */
  
  . . .
  
 end
  
 The sole function of the 
 BEGIN
 …
 END
  pair is to create a statement block; they do not 
 impact the scope of local variables or other database objects. The Transact-SQL 
 procedure definition, conditional execution and looping constructs, and others, are all 
 designed to operate with single SQL statements, so statement blocks are frequently used 
 in each of these contexts to group statements together as a single unit.
  
 In Informix SPL, a statement block includes not only a statement sequence but may 
 optionally declare local variables for use within the block and exception handlers to 
 handle errors that may occur within the block. Here is the structure of an Informix SQL 
 statement block:
  
 /* Informix SPL block of statements */
  
 /* Declaration of any local variables */
  
 define . . .
  
 /* Declare handling for exceptions */
  
 on exception . . .
  
 /* Define the sequence of SQL statements */
  
 begin. . .
  
 end
  
 - 505 -",NA
Returning a Value,"In addition to stored procedures, most SPL dialects support a stored 
 function
  capability. 
 The distinction is that a stored function returns a value while a stored procedure does not. 
  
 Here's a simple example of a stored function. Assume you want to define a stored 
 procedure that, given a customer number, calculates the total current order amount for 
 that customer. If you define the procedure as a function, the total amount can be returned 
 as its value.
  
 Figure 20-7 shows an Oracle stored function that calculates the total amount of current 
 orders for a customer, given the customer number. Note the 
 RETURNS
  clause in the 
 procedure definition, which tells the DBMS the data type of the value being returned. In 
 most DBMS products, if you enter a function call via the interactive SQL capability, the 
 function value is displayed in response. Within a stored procedure, you can call a stored 
 function and use its return value in calculations or store it in a variable.
  
 /* Return total order amount for a customer */
  
 create function get_tot_ords(c_num in integer)
  
 - 506 -",NA
Returning Values via Parameters,"The stored function capability provides only the ability to return a single value from a 
 stored routine. Several stored procedure dialects provide a method for returning more 
 than one value, by passing the values back to the calling routine through 
 output 
 parameters
 . The output parameters are listed in the stored procedure's parameter list, 
 just like the input parameters seen in the previous examples. However, instead of being 
 used to pass data values 
 into
  the stored procedure when it is called, the output 
  
 parameters are used to pass data 
 back out of
  the stored procedure to the calling 
 procedure.
  
 Figure 20-9 shows a PL/SQL stored procedure to retrieve the name of a customer, their 
 salesperson, and the sales office to which the customer is assigned, given a supplied 
 customer number. The procedure has four parameters. The first one, 
 CNUM
 , is an input 
 parameter and supplies the requested customer number. The other three parameters are 
 output parameters, used to pass the retrieved data values back to the calling procedure. In 
 this simple example, the 
 SELECTINTO
  form of the query places the returned variables 
 directly into the output parameters. In a more complex stored procedure, the returned 
 values might be calculated and placed into the output parameters with a PL/SQL 
 assignment statement.
  
 /* Get customer name, sales rep and office */
  
 create procedure get_cust_info(c_num  in  integer,
  
  c_name out varchar(20),
  
  r_name out varchar(15),
  
  c_offc out varchar(15))
  
 as
  
 begin
  
  /* Simple single-row query to get info */
  
 - 508 -",NA
Conditional Execution,"One of the most basic features of stored procedures is an 
 IF
 …
 THEN
 …
 ELSE
  construct for 
 decision-making within the procedure. Look back at the original 
 ADD_CUST
  procedure 
 defined in Figure 20-1 for adding a new customer. Suppose that the rules for adding new 
 customers are modified so that there is a cap on the amount by which a salesperson's 
 quota should be increased for a new customer. If the customer's anticipated first year 
 orders are $20,000 or less, that amount should be added to the quota, but if they are more 
 than $20,000, the quota should be increased only by $20,000. Figure 20-12 shows a 
 modified procedure that implements this new policy. The 
 IF
 …
 THEN
 …
 ELSE
  logic operates 
 exactly as it does in any conventional programming language.
  
 /* Add a customer procedure */ 
  
 create procedure add_cust (
  
  
  c_name   in varchar(20),            /* input customer name */
  
  c_num    in integer,                /* input customer number 
 */
  
  
  cred_lim in number(16,2),           /* input credit limit */
  
  tgt_sls  in number(16,2),           /* input target sales */
  
  c_rep    in integer,                /* input salesrep empl # 
 */
  
  
  c_offc   in varchar(15))            /* input office city */ 
 as 
  
 begin
  
  
  /* Insert new row of CUSTOMERS table */
  
  
  insert into customers (cust_num, company, cust_rep, 
  
 credit_limit)
  
  
  
  values (c_num, c_name, c_rep, cred_lim);
  
  if tgt_sales < 20000.00
  
 - 511 -",NA
Repeated Execution,"Another feature common to almost all stored procedure dialects is a construct for 
 repeated execution of a group of statements (looping). Depending on the dialect, there 
  
 - 512 -",NA
Other Flow-of-Control Constructs,"Some stored procedure dialects provide statements to control looping and alter the flow of 
 control. In Informix, for example, the 
 EXIT
  statement interrupts the normal flow within a 
 loop and causes execution to resume with the next statement 
 following
  the loop itself. The 
 CONTINUE
  statement interrupts the normal flow within the loop but causes execution to 
 resume 
 with the next loop iteration
 . Both of these statements have three forms, depending 
 on the type of loop being interrupted:
  
 exit for; 
  
 continue for;
  
 - 514 -",NA
Cursor-Based Repetition ,"One common need for repetition of statements within a stored procedure is when the 
 procedure executes a query and needs to process the query results, row by row. All of the 
 major dialects provide a structure for this type of processing. Conceptually, the structures 
 parallel the 
 DECLARE CURSOR
 , 
 OPEN CURSOR
 , 
 FETCH
 , and 
 CLOSE CURSOR 
 statements 
 in embedded SQL or the corresponding SQL API calls. However, instead of fetching the 
 query results into the application program, in this case they are being fetched into the 
 stored procedure, which is executing within the DBMS itself. Instead of retrieving the query 
 results into application program variables (host variables), the stored procedure retrieves 
 them into local stored procedure variables.
  
 To illustrate this capability, assume that you want to populate two tables with data from the 
 ORDERS
  table. One table, named 
 BIGORDERS
 , should contain customer name and order 
 size for any orders over $10,000. The other, 
 SMALLORDERS
 , should contain the 
 salesperson's name and order size for any orders under $1000. The best and most 
 efficient way to do this would actually be with two separate SQL 
 INSERT
  statements with 
 subqueries, but for purposes of illustration, consider this method instead:
  
 1.Execute a query to retrieve the order amount, customer name, and salesperson 
 name for each order.
  
 2.For each row of query results, check the order amount to see whether it falls into the 
 proper range for including in the 
 BIGORDERS
  or 
 SMALLORDERS
  tables.
  
 3.Depending on the amount, 
 INSERT
  the appropriate row into the 
 BIGORDERS
  or 
 SMALLORDERS
  table.
  
 4.Repeat Steps 2 and 3 until all rows of query results are exhausted.
  
 5.Commit the updates to the database.
  
 Figure 20-13 shows an Oracle stored procedure that carries out this method. The cursor 
 that defines the query is defined in the declare section of the procedure and assigned the 
 name 
 O_CURSOR
 . The variable 
 CURS_ROW
 , defined in the same section, is defined as an 
 Oracle ""row type."" It is a structured Oracle ""row variable"" with individual components (like 
 a C-language structure). By declaring it as having the same 
 ROWTYPE
  as the cursor, the 
 individual components of 
 CURS_ROW
  have the same data types and names as the cursor's 
 query results columns.
  
 - 515 -",NA
Handling Error Conditions,- 518 -,NA
Advantages of Stored Procedures,"Stored procedures offer several advantages, both for database users and database 
 administrators, including:
  
 •
 Run-time performance
 . Many DBMS brands compile stored procedures (either 
 automatically or at the user's request) into an internal representation that can be 
 executed very efficiently by the DBMS at run-time. Executing a precompiled stored 
 procedure can be much faster than running the equivalent SQL statements through the 
 PREPARE/EXECUTE
  process.
  
 •
 Reusability
 . Once a stored procedure has been defined for a specific function, that 
 procedure may be called from many different application programs that need to perform 
 the function, permitting very easy reuse of application logic and reducing the risk of 
 application programmer error.
  
 •
 Reduced network traffic
 . In a client/server configuration, sending a stored procedure call 
 across the network and receiving the results in a reply message generates much less 
 network traffic than using a network round-trip for each individual SQL statement. 
  
 This can improve overall system performance considerably in a network with heavy 
 traffic or one that has lower speed connections.
  
 •
 Security
 . In most DBMS brands, the stored procedure is treated as a ""trusted"" entity 
 within the database and executes with its own privileges. The user executing the stored 
 procedure needs to have only permission to execute it, not permission on the underlying 
 tables that the stored procedure may access or modify. Thus, the stored procedure 
 allows the database administrator to maintain tighter security on the underlying data, 
 while still giving individual users the specific data update or data access capabilities they 
 require.
  
 •
 Encapsulation
 . Stored procedures are a way to achieve one of the core objectives of 
 object-oriented programming—the ""encapsulation"" of data values, structures, and access 
 within a set of very limited, well-defined external interfaces. In object 
  
 terminology, stored procedures can be the ""methods"" through which the objects in the 
 underlying RDBMS are exclusively manipulated. To fully attain the object-oriented 
 approach, all direct access to the underlying data via SQL must be disallowed through the 
 RDBMS security system, leaving 
 only
  the stored procedures for database access. In 
 practice, few if any production relational databases operate in this restricted way.",NA
Stored Procedure Performance,"Different DBMS brands vary in the way they actually implement stored procedures. In 
 several brands, the stored procedure text is stored within the database and is interpreted 
 when the procedure is executed. This has the advantage of creating a very flexible stored 
 procedure language, but it creates significant run-time overhead for complex stored 
 procedures. The DBMS must read the statements that make up the stored procedure at 
 run-time, parse and analyze them, and determine what to do on the fly.
  
 Because of the overhead in the interpreted approach, some DBMS brands compile stored 
 procedures into an intermediate form that is much more efficient to execute. Compilation 
 may be automatic when the stored procedure is created, or the DBMS may provide the 
 ability for the user to request stored procedure compilation. The disadvantage of compiled 
 stored procedures is that the exact technique used to carry out the stored procedure is 
 fixed when the procedure is compiled. Suppose, for example, that a stored procedure is 
 created and compiled soon after a database is first created, and later some useful indexes 
 are defined on the data. The compiled queries in the stored procedure won't take 
 advantage of these indexes, and as a result they may run much more slowly 
  
 - 521 -",NA
System-Defined Stored Procedures,"DBMS brands that support stored procedures sometimes provide built-in, system-defined 
 stored procedures to automate database processing or management functions. Sybase 
 SQL Server pioneered this use of system stored procedures. Today hundreds of Transact-
 SQL system stored procedures provide functions such as managing users, database 
 roles, job execution, distributed servers, replication, and others. Most Transact-SQL 
 system procedures follow this naming convention:
  
 •
 SP_ADD_
 something
 . Add a new object (user, server, replica, and so on)
  
 •
 SP_DROP_
 something
 . Drop an existing object
  
 •
 SP_HELP_
 something
 . Get information about an object or objects
  
 For example, the 
 SP_HELPUSER
  procedure returns information about the valid users of the 
 current database.",NA
System-Defined Stored Procedures,"DBMS brands that support stored procedures sometimes provide built-in, system-defined 
 stored procedures to automate database processing or management functions. Sybase 
 SQL Server pioneered this use of system stored procedures. Today hundreds of Transact-
 SQL system stored procedures provide functions such as managing users, database 
 roles, job execution, distributed servers, replication, and others. Most Transact-SQL 
 system procedures follow this naming convention:
  
 •
 SP_ADD_
 something
 . Add a new object (user, server, replica, and so on)
  
 •
 SP_DROP_
 something
 . Drop an existing object
  
 •
 SP_HELP_
 something
 . Get information about an object or objects
  
 For example, the 
 SP_HELPUSER
  procedure returns information about the valid users of the 
 current database.",NA
Triggers,"A trigger is a special set of stored procedure code whose activation is caused by 
 modifications to the database contents. Unlike stored procedures created with a 
 CREATE 
  
 - 522 -",NA
Advantages and Disadvantages of Triggers,"Triggers can be extremely useful as an integral part of a database definition. Triggers can 
 be used for a variety of different functions, including:
  
 •
 Auditing changes
 . A trigger can detect and disallow specific updates and changes that 
 should not be permitted in the database.
  
 •
 Cascaded operations
 . A trigger can detect an operation within the database (such as 
 deletion of a customer or salesperson) and automatically cascade the impact throughout 
 the database (such as adjusting account balances or sales targets).
  
 •
 Enforce interrelationships
 . A trigger can enforce more complex interrelationships among 
 the data in a database than those that can be expressed by simple referential integrity 
 constraints or check constraints, such as those that require a sequence of SQL 
 statements or 
 IF…THEN…ELSE 
 processing.
  
 •
 Stored procedure invocation
 . A trigger can call one or more stored procedures or even 
 invoke actions outside the DBMS itself through external procedure calls in response to 
 database updates.
  
 In each of these cases, a trigger embodies a set of business rules that govern the data in 
 the database and modifications to that data. The rules are embedded in a single place in 
 the database (the trigger definition). As a result, they are uniformly enforced across all 
 applications that access the database. When they need to be changed, they can be 
 changed once with the assurance that the change will be applied uniformly.
  
 The major disadvantage of triggers is their potential performance impact. If a trigger is set 
  
 - 523 -",NA
Triggers in Transact-SQL,"Transact-SQL provides triggers through a 
 CREATE TRIGGER
  statement in both its 
 Microsoft SQL Server and Sybase Adaptive Server dialects. Here is a Transact-SQL 
 trigger definition for the sample database, using the example from earlier in this chapter:
  
 create trigger upd_tgt
  
  /* Insert trigger for SALESREPS */
  
  on salesreps
  
  for insert
  
  as
  
  if (@@rowcount = 1)
  
  begin
  
  update offices
  
  set target = target + inserted.quota
  
  from offices, inserted
  
  where offices.office = inserted.rep_office;
  
  end
  
  else
  
  raiserror 23456
  
 The first clause names the trigger (
 UPD_TGT
 ). The second clause is required and 
 identifies the table to which the trigger applies. The third clause is also required and tells 
 which database update operations cause the trigger to be fired. In this case, only an 
 INSERT
  statement causes the trigger to fire. You can also specify 
 UPDATE
  or 
 DELETE 
 operations, or a combination of two or three of these operations in a comma-separated 
 list. Transact-SQL restricts triggers so that only one trigger may be defined on a particular 
 table for each of the three data modification operations. The body of the trigger follows the 
 AS keyword. To understand the body of a trigger like this one, you need to 
  
 understand how Transact-SQL treats the rows in the target table during database 
 modification operations. 
  
 For purposes of trigger operation, Transact-SQL defines two ""virtual tables"" whose column 
 structure is identical to the ""target"" table on which the trigger is defined. One of these 
 virtual tables is named 
 DELETED
 , and the other is named 
 INSERTED
 . These virtual tables 
 are populated with rows from the target table, depending on the data modification 
 statement that caused the trigger to fire, as follows:
  
 •
 For a 
 DELETE
  statement
 . Each row of the target table that is deleted by the 
 DELETE 
 statement is placed into the 
 DELETED
  table. The 
 INSERTED
  table is empty.
  
 •
 For an 
 INSERT
  statement
 . Each row of the target table that is added by the 
 INSERT 
 statement is also placed into the 
 INSERTED
  table. The 
 DELETED
  table is empty.
  
 •
 For an 
 UPDATE
  statement
 . For each row of the target table that is changed by the 
 UPDATE
  statement, a copy of the row 
 before
  any modifications is placed into the 
 DELETED
  table. A copy of the row 
 after
  all modifications is placed into the 
 INSERTED 
 table.
  
 These two virtual tables can be referenced within the body of the trigger, and the data in 
  
 - 524 -",NA
Triggers in Informix SPL,"Informix also supports triggers through a 
 CREATE TRIGGER
  statement. As in the 
 Transact-SQL dialect, the beginning of the 
 CREATE TRIGGER
  statement defines the 
 trigger name, the table on which the trigger is being defined, and the triggering actions. 
  
 Here are statement fragments that show the syntax:
  
 create trigger new_sls
  
  
  insert on salesreps . . .
  
 create trigger del_cus_chk
  
  
  delete on customers . . .
  
 - 525 -",NA
Triggers in Oracle PL/SQL,"Oracle provides a more complex trigger facility than either the Informix or Transact-SQL 
 facility described in the preceding sections. It uses a 
 CREATE TRIGGER
  statement to 
 specify triggered actions. As in the Informix facility, a trigger can be specified to fire at 
 specific times during specific update operations:
  
 •A 
 statement-level trigger 
 fires once for each data modification statement. It can be 
 specified to fire either before the statement is executed or after the statement has 
 completed its action.
  
 •A 
 row-level trigger 
 fires once for each row being modified by a statement. In Oracle's 
 structure, this type of trigger may also fire either 
 before
  the row is modified or 
 after
  it is 
 modified.
  
 •An 
 instead-of trigger 
 takes the place of an attempted data modification statement. It 
 provides a way to detect an attempted update, insert or delete operation by a user or 
 procedure, and substitute other processing instead. You can specify a trigger should be 
 executed instead of a statement or that it should be executed instead of each attempted 
 modification of a row.
  
 These trigger structures and their options provide 14 different valid Oracle trigger types 
 (12 resulting from a choice of 
 INSERT/DELETE/UPDATE
  triggers for 
 BEFORE
  or 
 AFTER 
 processing at the 
 ROW
  or 
 STATEMENT
  level (3 x 2 x 2), and 2 more from 
 INSTEAD OF
  
 - 527 -",NA
Other Trigger Considerations,"Triggers pose some of the same issues for DBMS processing that 
 UPDATE
  and 
 DELETE 
 rules present. For example, triggers can cause a cascaded series of actions. A user's 
 attempt to update a table may cause a trigger to fire. Within the body of that trigger is an 
 UPDATE
  statement for another table. A trigger on that table causes the 
 UPDATE
  of still 
  
 - 528 -",NA
Stored Procedures and the SQL Standard,"The development of DBMS stored procedures has been largely driven by DBMS vendors 
 and the competitive dynamics of the database industry. Sybase's initial introduction of 
 stored procedures and triggers in SQL Server triggered a competitive response, and by 
 the mid-1990s many of the enterprise-class systems had added their own, proprietary 
 procedural extensions to SQL. Stored procedures were not a focus of the SQL2 
  
 standards efforts but became a part of the standardization agenda after the 1992 
 publication of the SQL2 standard. The work on stored procedure standards was split off 
 from the broader object-oriented extensions that were proposed for SQL3, and was 
 focused on a set of procedural extensions to the SQL2 language.
  
 The result was a new part of the SQL standard, published in 1996 as SQL/Persistent 
 Stored Modules (SQL/PSM), International Standard ISO/IEC 9075-4. The actual form of 
 the standard specification is a collection of additions, edits, new paragraphs, and 
 replacement paragraphs to the 1992 SQL2 standard (ISO/IEC 9075:1992). SQL/PSM is 
 actually Part 4 of an expected multi-part structure for the ISO SQL standard. The SQL 
 Call-Level Interface (CLI) standard, described in 
 Chapter 19
 , is being treated the same 
 way; it is 
 Part 3
  of the eventual standard.
  
 The SQL/PSM standard addresses only stored procedures and stored functions. It 
 explicitly does 
 not
  provide a specification of a trigger facility for the ISO SQL standard. The 
 standardization of trigger functions is closely tied to other object-oriented extensions 
 proposed for SQL3 and must await the resolution of the larger issues involved with those 
 object features.",NA
Core Capabilities,"The capabilities specified in the SQL/PSM standard parallel the core features of the 
 proprietary stored procedure capabilities of today's DBMS systems. They include SQL 
 language constructs to:
  
 •Define and name procedures and functions written in the extended SQL language
  
 •Invoke (call) a previously defined procedure or function
  
 •Pass parameters to a called procedure or function, and obtain the results of its 
 execution
  
 - 529 -",NA
Creating a SQL Routine,"Following the practice of most DBMS brands, the SQL/PSM standard uses the 
 CREATE 
 PROCEDURE
  and 
 CREATE FUNCTION
  statements to specify the definitions of stored 
 procedures and functions. Figure 20-18 shows a simplified syntax for each of these 
 statements. In addition to the capabilities shown in the figure, the standard provides a 
 capability to define external stored procedures, specifying the language they are written 
 in, whether they can or cannot read or modify data in the database, their calling 
 conventions, and other characteristics.
  
  
 Figure 20-18: 
 SQL/PSM 
 CREATE PROCEDURE
  syntax diagram",NA
Flow-of-Control Statements,"The SQL/PSM standard specifies the common programming structures to control the flow 
 of execution that are found in most stored procedure dialects. Figure 20-19 shows the 
 conditional branching and looping syntax. Note that the SQL statement lists specified for 
 each structure consist of a sequence of SQL statements, each ending with a semicolon. 
  
 - 530 -",NA
Cursor Operations ,"The SQL/PSM standard extends the cursor manipulation capabilities specified in the 
 SQL2 standard for embedded SQL into SQL routines. The 
 DECLARE CURSOR
 , 
 OPEN
 , 
 FETCH
 , and 
 CLOSE
  statements retain their roles and functions. Instead of using 
 application program host variables to supply parameter values and to receive retrieved 
 data, SQL routine parameters and variables can be used for these functions.
  
 The SQL/PSM standard does introduce one new cursor-controlled looping structure, 
 shown in Figure 20-20. Like the similar structures in the Oracle and Informix dialects 
 described earlier in this chapter, it combines the cursor definition, 
 OPEN
 , 
 FETCH
 , and 
 CLOSE
  statement in a single loop definition that also specifies the processing to be 
 performed for each row of retrieved query results.
  
  
 Figure 20-20: 
 SQL/PSM cursor-controlled loop syntax diagram",NA
Block Structure ,"Figure 20-21 shows the block structure specified by the SQL/PSM standard. It is a quite 
 comprehensive structure, providing the following capabilities:
  
 - 531 -",NA
Error Handling,"The block structure specified by the SQL/PSM standard provides fairly comprehensive 
 support for error handling. The standard specifies predefined ""conditions"" that can be 
 detected and handled, including:
  
 •
 SQLWARNING
 . One of the warning conditions specified in the SQL2 standard
  
 •
 NOT FOUND
 . The condition that normally occurs when the end of a set of query results is 
 reached with a 
 FETCH
  statement
  
 •
 SQLSTATE
  values
 . A test for specific 
 SQLSTATE
  error codes
  
 •
 User-defined conditions
 . A condition named by the stored procedure
  
 Conditions are typically defined in terms of 
 SQLSTATE
  values. Rather than using 
 numerical 
 SQLSTATE
  codes, you can assign the condition a symbolic name. You can 
 also specify your own user-defined condition:
  
 declare bad_err condition for sqlstate '12345';
  
 declare my_err condition;
  
 Once the condition has been defined, you can force the condition to occur through the 
 execution of a SQL routine with the 
 SIGNAL
  statement: 
  
 signal bad_err;
  
 signal sqlstate '12345';
  
 To handle error conditions that may arise, SQL/PSM allows you to declare a 
 condition-
 handler
 . The declaration specifies the list of conditions that are to be handled and the 
 action to be taken. It also specifies one of four types of condition handling. The four types 
 differ in what happens to the flow of control after the handler is finished with its work:
  
 •
  
 CONTINUE
  type
 . After the condition handler completes its work, control returns to the 
 next statement following the one that caused the condition. That is, execution 
  
 continues
  with the next statement.
  
 •
  
 EXIT
  type
 . After the condition handler completes its work, control returns to the 
 end
  of 
 the satement block containing the statement that caused the condition. That is, 
  
 execution effectively 
 exits
  the block.
  
 •
  
 UNDO
  type
 . After the condition handler completes its work, all modifications to data in 
 the database caused by statements within the same statement block as the statement 
  
 causing the error are undone. The effect is the same as if a transaction had been 
 initiated at the beginning of the statement block and was being rolled back.
  
 Here are some examples that show the structure of the handler definition:
  
 /* Handle SQL warnings here, then continue */
  
 declare continue handler for sqlwarning
  
  call my_warn_routine();
  
 /* Handle severe errors by undoing effects */
  
 declare undo handler for user_disaster
  
  begin
  
  /* Do disaster cleanup here */
  
 - 533 -",NA
Routine Name Overloading,"The SQL/PSM standard permits ""overloading"" of stored procedure and function names. 
  
 Overloading is a common attribute in object-oriented systems and is a way to make stored 
 routines more flexible in handling a wide variety of data types and situations. Using the 
 overloading capability, several different routines can be given the 
 same
  routine name. The 
 multiple routines defined with the same name must differ from one another in the number 
 of parameters that they accept or in the data types of the individual 
  
 parameters. For example, you might define these three stored functions:
  
 create function combo(a, b)
  
  a integer;
  
  b integer;
  
  returns integer;
  
  as return (a+b)
  
 create function combo(a, b, c)
  
  a integer;
  
  b integer;
  
  c integer;
  
  returns integer;
  
  as return (a+b+c)
  
 create procedure combo(a, b)
  
  a varchar(255);
  
  b varchar(255);
  
  returns varchar(255);
  
  as return (a || b)
  
 The first 
 COMBO
  function ""combines"" two integers by adding them and returns the sum. 
  
 The second 
 COMBO
  function ""combines"" three integers the same way. The third 
 COMBO 
 function ""combines"" two character strings by concatenating them. The standard allows 
 both of these functions named 
 COMBO
  to be defined at the same time within the database. 
 When the DBMS encounters a reference to the 
 COMBO
  function, it examines the number of 
 arguments in the reference and their data types, and determines which version of the 
 COMBO
  function to call. Thus, the overloading capability allows a SQL programmer to 
 create a family of routines that logically perform the same function and have the same 
 name, even though the specifics of their usage for different data types is different.
  
 To simplify the management of a family of routines that share an overloaded name, the 
 SQL/PSM standard has the concept of a 
 specific
  name. A specific name is a second name 
 that is assigned to the routine that 
 is
  unique within the database schema or module. It 
 uniquely identifies a specific routine. The specific name 
 is
  used to drop the routine, and it 
 is reflected in the Information Schema views that describe stored routines. The specific 
 name is not used to call the routine; that would defeat the primary purpose of the 
 overloaded routine name. Support for specific names is beginning to appear in commercial 
 relational databases that support object-oriented features, such as Informix 
  
 - 534 -",NA
Other Stored Procedure Considerations,"The SQL/PSM standard adds one additional privilege to the set specified by the SQL2 
 standard. The 
 EXECUTE
  privilege gives a user the ability to execute a stored procedure. It 
 is managed by the 
 GRANT
  and 
 REVOKE
  statements in the same manner as other 
  
 database privileges.
  
 Because the stored routines defined by SQL/PSM are defined within SQL2 schemas, 
 many routines can be defined in many different schemas throughout the database. When 
 calling a stored routine, the routine name can be fully qualified to uniquely identify the 
 routine within the database. The SQL/PSM standard provides an alternative method of 
 searching for the definition of unqualified routine names through a new 
 PATH
  concept.
  
 The 
 PATH
  is the sequence of schema names that should be searched to resolve a routine 
 reference. A default 
 PATH
  can be specified as part of the schema header in the 
 CREATE 
 SCHEMA
  statement. The 
 PATH
  can also be dynamically modified during a SQL session 
 through the 
 SETPATH
  statement.",NA
Summary,"Stored procedures and triggers are two very useful capabilities for SQL databases used 
 in transaction processing applications:
  
 •Stored procedures allow you to predefine common database operations, and invoke 
 them simply by calling the stored procedure, for improved efficiency and less chance of 
 error.
  
 •Extensions to the basic SQL language give stored procedures the features normally 
 found in programming languages. These features include local variables, conditional 
 processing, branching, and special statements for row-by-row query results 
  
 processing.
  
 •Stored functions are a special form of stored procedures that return a value.
  
 •Triggers are procedures whose execution is automatically initiated based on attempted 
 modifications to a table. A trigger can be fired by an 
 INSERT
 , 
 DELETE
 , or 
 UPDATE 
 statement for the table.
  
 •There is wide variation in the specific SQL dialects used by the major DBMS brands to 
 support stored procedures and triggers
  
 •There is now an international standard for stored procedures (but not triggers); as one of 
 the newer standards, it has not yet had a major impact on the actual implementation by 
 leading DBMS vendors.",NA
Chapter 21: ,NA,NA
SQL and Data Warehousing,NA,NA
Overview,"One of the most important forces shaping relational database technology and the SQL 
 language today is the rapidly growing area of data warehousing and business 
 intelligence. The focus of data warehousing is to use accumulated data to provide 
 information and insights for decision making. The rhetoric of data warehousing talks 
 about an organization ""treating its data as a valuable asset."" The process of ""data 
 mining"" involves in-depth analysis of historical and trend data to find ""nuggets"" of 
  
 - 535 -",NA
Data Warehousing Concepts,"One of the foundations of data warehousing is the notion that databases for transaction 
 processing and databases for business analysis serve very different needs. The core 
 focus of an OLTP (online transaction processing) database is to support the basic day-to-
 day functions of an organization. In a manufacturing company, OLTP databases support 
 the taking of customer orders, ordering of raw materials, management of inventory, billing 
 of customers, and similar functions. Their heaviest users are the applications used by 
 order processing clerks, production workers, warehouse staff, and the like. By contrast, 
 the core focus of a business intelligence (BI) database is to support business decision 
 making through data analysis and reporting. Its heaviest users are typically product 
 managers, production planners, and marketing professionals.
  
 Table 21-1 highlights the significant differences in OLTP and business intelligence 
 application profiles and the database workloads they produce. A typical OLTP transaction 
 processing a customer's order might involve these database accesses:
  
 Table 21-1: OLTP versus Data Warehouse Database Attributes
  
 Database Characteristic
  
 OLTP Database
  
 Data Warehouse 
  
 Database
  
 Data contents
  
 Current data
  
 Historical data
  
 Data structure
  
 Tables organized to align 
  
 Tables organized to be 
  
  
 with transaction structure
  
 easy to understand and 
 query
  
 Typical table size
  
 Thousands of rows
  
 Millions of rows
  
 Access patterns
  
 Predetermined for each 
  
 Ad hoc
 , depending on the 
  
 Rows accessed per 
  
 type of transaction to be 
  
 particular decision to be 
  
 processed
  
 made
  
 Tens
  
 Thousands to millions
  
 ""request""
  
 Row coverage per access
  
 Individual rows
  
 Groups (summary queries)
  
 - 536 -",NA
Components of a Data Warehouse,"Figure 21-1 shows the architecture of a data warehousing environment. There are three 
 key components:
  
  
 Figure 21-1: 
 Data warehousing components
  
 •
 Warehouse loading tools
 . Typically a suite of programs that extract data from corporate 
 transaction processing systems (relational databases, mainframe and minicomputer files, 
 legacy databases), process it, and load it into the warehouse. This process typically 
 involves substantial ""cleanup"" of the transaction data, filtering it, reformatting it, and 
 loading it on a bulk basis into the warehouse.
  
 •
 A warehouse database
 . Typically a relational database optimized for storing vast 
 quantities of data, bulk loading data at high speeds, and supporting complex business 
 analysis queries.
  
 •
 Data analysis tools
 . Typically a suite of programs for performing statistical and time 
 series analysis, doing ""what if"" analysis, and presenting the results in graphical form.
  
 Vendors in the data warehousing market have tended to concentrate in one of these 
 component areas. Several vendors build product suites that focus on the warehouse 
 loading process and challenges. A different group of vendors have focused on data 
 analysis. There has been some vendor consolidation in each of these areas, but both 
 remain areas of focus for individual independent software companies, including several 
 whose revenues are in the $100 million range.
  
 Specialized warehouse databases were also the target of several startup companies 
 early in the data warehousing market. Over time, the major enterprise DBMS vendors 
 also moved to address this area. Some developed their own specialized warehouse 
 databases; others added warehouse databases to their product line by acquiring smaller 
 companies that produced them. Today the database component in the figure is almost 
 always a specialized, SQL-based warehouse DBMS supplied by one of the major 
 enterprise database vendors.",NA
The Evolution of Data Warehousing,- 538 -,NA
Database Architecture for Warehousing,"The structure (schema) of a warehouse database is typically designed to make the 
 information easy to analyze, since that is the major focus of its use. The structure must 
 make it easy to ""slice and dice"" the data along various dimensions. For example, one day 
 a business analyst may want to look at sales by product category by region, to compare 
 the performance of different products in different areas of the country. The next day, the 
 same analyst may want to look at sales trends over time by region, to see which regions 
 are growing and which are not. The structure of the database must lend itself to this type 
 of analysis along several different dimensions.",NA
Fact Cubes,"In most cases, the data stored in a warehouse can be accurately modeled as an N-
 dimensional cube (""N-cube"") of historical business facts. A simple, three- dimensional 
 cube of sales data is shown in Figure 21-2 to illustrate the structure. The ""fact"" in each 
 cell of the cube is a dollar sales amount. Along one edge of the cube, one of the 
  
 - 539 -",NA
Star Schemas,"In most data warehouses, the most effective way to model the N-dimensional fact cube is 
 with a star schema. A star schema for the distributor warehouse in the previous example 
 is shown in Figure 21-3. Each dimension of the cube is represented by a dimension table. 
 There are five of them in the figure, named CATEGORIES, SUPPLIERS, CUSTOMERS, 
 REGIONS, and MONTHS. There is one row in each dimension table for each possible 
 value of that dimension. The MONTHS table has 36 rows, one for each month of sales 
 history being stored. Three regions produce a three-row REGIONS table.
  
  
 Figure 21-3: 
 Star schema for distributor warehouse
  
 Dimension tables in a star schema often contain columns with descriptive text information 
 or other attributes associated with that dimension (such as the name of the buyer for a 
 customer, or the customer's address and phone number, or the purchasing terms for a 
 supplier). These columns may be displayed in reports generated from the database. A 
 dimension table always has a primary key that contains the value of the dimension. If the 
 ""values"" of a dimension are numbers (such as a clothing size) or short text strings (such 
 as a city name), the primary key may be this dimension value itself. It's more common for 
 dimension values to be expressed in some type of ""code-value."" Three-letter airport codes 
 and customer numbers are typical examples. In the sample warehouse of Figure 21-3, we 
 assume that actual values are used as primary keys for 
 REGIONS
  (East, West, and so on), 
 CATEGORIES
  (Clothing, Shoes, and so on), and 
 MONTHS
 . The other two dimensions use 
 coded values (
 CUST_CODE
  for 
 CUSTOMERS
 , 
 SUPP_CODE
  for 
  
 SUPPLIERS
 ).
  
 The largest table in the database is the 
 fact table
  in the center of the schema. This table is 
 named 
 SALES
  in 
 Figure 21-3
 . The fact table contains a column with the data values that 
 appear in the cells of the N-cube in 
 Figure 21-2
 . In addition, the fact table contains a 
 column (or columns) that forms a foreign key for each of the dimension tables. In this 
 example, there are five such foreign-key columns. With this structure, each row 
  
 represents the data for one cell of the N-cube. The foreign keys link the row to the 
 corresponding dimension table rows for its position in the cube.
  
 The fact table typically contains only a few columns, but many rows—hundreds of 
 thousands or even millions of rows are not unusual in a production data warehouse. The 
 ""fact"" column almost always contains numeric values, such as currency amounts, units 
 shipped or received, or pounds processed. Virtually all reports from the warehouse 
 involve summary data—totals, averages, high or low values, percentages—based on 
 arithmetic computations on this numeric value.
  
 - 541 -",NA
Multi-Level Dimensions,"In the star schema structure of 
 Figure 21-3
 , each of the dimensions has only one level. In 
 practice, multi-level dimensions are quite common. For example:
  
 •Sales data may in fact be accumulated for each sales office. Each office is a part of a 
 sales district, and each district is a part of a sales region.
  
 •Sales data is accumulated by month, but it may also be useful to look at quarterly 
 sales results. Each month is a part of a particular quarter.
  
 •Sales data may be accumulated for individual products ordered, and the products are 
 associated with a particular supplier.
  
 Multi-level dimensions such as these complicate the basic star schema, and in practice 
 there are several ways to deal with them:
  
 •
 Additional data in the dimension tables
 . The geographic dimension table might contain 
 information about individual offices, but also include columns indicating the district and 
 region that the office belongs to. Aggregate data for these higher levels of the geographic 
 dimension can then be obtained by summary queries that join the fact table to the 
 dimension table and select based on the district or region columns. This approach is 
 conceptually simple, but it means that all aggregate (summary) data must be calculated 
 query-by-query. This likely produces unacceptably poor performance.
  
 •
 Multiple levels within the dimension tables
 . The geographic dimension table might be 
 extended to include rows for offices, districts, and regions. Rows containing summary 
 (total) data for these higher-level dimensions are added to the fact table when it is 
 updated. This solves the run-time query performance problem by precalculating 
 aggregate (summary) data. However, it complicates the queries considerably. Since 
 every sale is now included in three separate fact table rows (one each for office, district, 
 and region), any totals must be computed very carefully. Specifically, the fact 
  
 - 542 -",NA
SQL Extensions for Data Warehousing,"With a star schema structure, a relational database conceptually provides a good 
 foundation for managing data for business analysis. The ability to freely relate information 
 within the database based solely on data values is a good match for the 
 ad hoc
 , 
  
 unstructured queries that typify business intelligence applications. But there are some 
 serious mismatches between typical business intelligence queries and the capabilities of 
 the core SQL language. For example:
  
 •
 Data ordering
 . Many business intelligence queries deal explicitly or implicitly with data 
 ordering—they pose questions like ""what is the top 10 percent,"" ""what are the top 10,"" or 
 ""which are the worst-performing."" As a set-oriented language, SQL manipulates unordered 
 sets of rows. The only support for sorting and ordering data within standard SQL is the 
 ORDERBY
  clause in the 
 SELECT
  statement, which is applied only at the end of all other set-
 oriented processing.
  
 •
 Time series
 . Many business intelligence queries compare values based on time—
 contrasting this year's results to last year's, or this month's results to the same month 
 last year, or computing year-over-year growth rates, for example. It is very hard, and 
 sometimes impossible, to get ""side-by-side"" comparisons of data from different time 
 periods within a single row of standard SQL query results, depending on the structure of 
 the underlying database.
  
 •
 Comparison to aggregate values
 . Many business intelligence queries compare values for 
 individual entities (for example, office sales results) to an overall total, or to subtotals (such 
 as regional results). These comparisons are difficult to express in standard SQL. A report 
 format showing line-item detail, subtotals, and totals is impossible to generate directly from 
 SQL, since all rows of query results must have the same column structure.
  
 To deal with these issues, DBMS products on data warehousing have tended to extend 
 the core SQL language. For example, the DBMS from Red Brick, one of the data 
 warehousing pioneers and now a part of Informix's product line, features these 
  
 - 543 -",NA
Warehouse Performance,"The performance of a data warehouse is one of the keys to its usefulness. If business 
 analysis queries take too long, people tend not to use the warehouse on an ad hoc basis 
 for decision making. If it takes too long to load data into the warehouse, the corporate IS 
 organization will probably resist frequent updates, and stale data may make the 
  
 warehouse less useful. Achieving a good balance between load performance and run-time 
 performance is one of the keys to successful warehouse deployment.",NA
Load Performance,"The process of loading a warehouse can be very time-consuming. It's not uncommon for 
 warehouse data loads to take hours or even days for very large warehouses. Load 
 processing typically involves these operations:
  
 •
 Data extraction
 . The data to be loaded into the warehouse database typically comes 
 from several different operational data sources. Some may be relational databases that 
 support OLTP applications. 
  
 •
 Data cleansing
 . Operational data tends to be ""dirty"" in the sense that it contains 
 significant errors. For example, older transaction processing systems may not have 
 strong integrity checks, permitting the entry of incorrect customer numbers or product 
 numbers. The warehouse loading process typically includes data integrity and ""data 
 sanity"" checks.
  
 •
 Data cross-checking
 . In many companies, the data processing systems that support 
 various business operations have been developed at different times and are not 
 integrated. Changes that are processed by one system (for example, adding new product 
 numbers to an order processing application) may not automatically be reflected in other 
 systems (for example, the inventory control system), or there may be delays in 
 propagating changes. When data from these nonintegrated systems arrives at the 
 warehouse, it must be checked for internal consistency.
  
 •
 Data reformatting
 . Data formats in the operational data stores may differ considerably 
  
 - 544 -",NA
Query Performance,"Database vendors focused on warehousing have invested considerable energy in 
 optimizing their DBMS products to maximize query performance. As a result, 
  
 warehousing performance has improved dramatically over the last several years. The 
 growth in the size and complexity of warehouses has prevented some of this 
  
 performance gain from actually being translated into perceived end-user benefit.
  
 Several different techniques have evolved to maximize the performance of business 
 analysis queries in a warehouse, including:
  
 •
 Specialized indexing schemes
 . Typical business analysis queries involve a subset of the 
 data in the warehouse, selected on the basis of dimension values. For example, a 
 comparison of this month's and last month's results involves only two of the 36 months of 
 data in the example warehouse. Specialized indexing schemes have been 
  
 developed to allow very rapid selection of the appropriate rows from the fact table and 
 joining to the dimension tables. Several of these involve bitmap techniques, where the 
 individual possible values for a dimension (or a combination of dimensions) are each 
 assigned a single bit in an index value. Rows meeting a selection criteria can be very 
 rapidly identified by bitwise logical operations, which a computer system can perform more 
 rapidly than value comparisons.
  
 •
 Parallel processing techniques
 . Business analysis queries can often be broken up into 
 parts that can be carried out in parallel, to reduce the overall time required to produce the 
 final results. In a query joining four warehouse tables, for example, the DBMS might take 
 advantage of a two-processor system by joining two of the tables in one process and two 
 others in another. The results of these intermediate joins are then combined. Alternatively, 
 the workload of processing a single table in the query might be split and carried out in 
 parallel – for example, assigning rows for specific month-ranges to specific processes. 
 The use of multiprocessor systems in these cases is 
  
 - 545 -",NA
Summary,"Data warehousing is a rapidly growing part of the market for SQL-based relational 
 databases and is one with a set of specialized requirements:
  
 •Warehouse databases are optimized for the workload of typical business analysis 
 queries, which is quite different from OLTP workloads.
  
 •Specialized utility programs provide high-performance loading of the warehouse and 
 analysis tools for taking advantage of warehoused data.
  
 •Specialized database schema structures, such as the star schema, are typically used in 
 warehouse applications to support typical business analysis queries and optimize 
 performance.
  
 •SQL extensions are frequently used to support typical business analysis queries 
 involving time series and trend analysis, rank orderings, and time-based comparisons.
  
 •Careful design of a large warehouse is required to provide the correct balance between 
 load-time performance and run-time performance.",NA
Chapter  22: ,NA,NA
SQL Networking and Distributed ,NA,NA
Databases,NA,NA
Overview,"One of the major computing trends through the late 1980s and late 1990s has been the 
 move from large centralized computers to distributed networks of computer systems. 
 With the advent of minicomputers, data processing tasks such as inventory control and 
 order processing moved from corporate mainframes to smaller departmental systems. 
  
 The explosive increase in the popularity of the personal computer brought computer 
 power directly onto the desktops of millions of people.
  
 With the widespread adoption of personal computers, organizations moved to connect 
 them into local area networks (LANs), powered by hardware and software from 
  
 companies like 3COM and Novell. Later, corporate IS organizations focused on 
  
 interconnecting departmental LANs into large, enterprise-wide data networks, linked by 
 routers and other network equipment from companies like Cisco Systems. The exploding 
 popularity of the Internet added a new chapter, and a new era of growth, to computer 
 networking. Today, the Internet creates a global, interconnected data network that is 
 capable of linking computers and people around the world. The focus of computer 
  
 - 546 -",NA
The Challenge of Distributed Data Management,"When the foundations of relational database management and the SQL language were 
 being laid in the 1970s, almost all commercial data processing happened on large, 
 centralized computer systems. The company's data was stored on mass storage 
 attached to the central system. The business programs that processed transactions and 
 generated reports ran on the central system and accessed the data. Much of the 
 workload of the central system was batch processing. Online users accessed the central 
 system through ""dumb"" computer terminals with no processing power of their own. The 
 central system formatted information to be displayed for the online user and accepted 
 data typed by the user for processing.
  
 In this environment, the roles of a relational database system and its SQL language were 
 clear and well contained. The DBMS had responsibility for accepting, storing, and 
 retrieving data based on requests expressed in the SQL language. The business 
 processing logic resided 
 outside
  the database and was the responsibility of the business 
 programs developed and maintained by the information systems staff. The programs and 
 the DBMS software executed on the same centralized system where the data was stored, 
 so the performance of the system was not affected by external factors like network traffic 
 or outside system failures.
  
 Commercial data processing in a modern corporation has evolved a long way from the 
 centralized environment of the 1970s. Figure 22-1 shows a portion of a computer network 
 that you might find in a manufacturing company, a financial services firm, or a distribution 
 company today. Data is stored on a variety of computer systems in the network:
  
  
 - 547 -",NA
Distributing Data—Practical Approaches,"Because of the formidable obstacles to realizing the ""ideal"" distributed database, DBMS 
 vendors are taking a step-by-step approach to databases and networking. They have 
 focused on specific forms of network database access, data distribution, and distributed 
 data management that are appropriate for particular application scenarios. For example, a 
 DBMS vendor may first provide tools to rapidly extract subset data from a ""master"" 
 database and send it across a network for loading into a ""slave"" database. Later the 
 vendor may enhance the tool to track updates to the master database since the last 
 extract, and to extract and transmit only the changes to the master database. A 
  
 subsequent version of the tool may automate the entire process, providing a graphical 
 user interface for specifying the data to be extracted and scripts to automate the periodic 
 extract process. Similarly, a DBMS may provide initial support for distributed queries by 
 allowing a user on one system to query a database located on another system. In 
 subsequent releases, the DBMS may allow the remote query as a subquery within a query 
 that accesses local database tables. Still later, the DBMS may allow distributed queries 
 that more freely intermix data from local and remote databases.",NA
Remote Database Access,"One of the simplest approaches to managing data stored in multiple locations is remote 
 data access. With this capability, a user of one database is given the ability to ""reach out"" 
 across a network and retrieve information from a different database. In its simplest form, 
 this may involve carrying out a single query against the remote database, as shown in 
 Figure 22-2. It may also involve performing an 
 INSERT
 , 
 UPDATE
 , or 
 DELETE
  statement to 
 modify the remote database contents. This type of requirement often arises when the local 
 database is a ""satellite"" database (such as a database in a local sales office or distribution 
 center) and the remote database is a central, corporate database.
  
  
 Figure 22-2: 
 A remote database access request
  
 In addition to the remote data access request, Figure 22-2 also shows a client/server 
 request to the remote database from a (different) PC user. Note that, from the standpoint 
 of the remote database, there is very little difference between processing the request from 
 the PC client and processing the remote database access request. In both cases, a SQL 
 request arrives across the network and the remote database determines that the user 
 making the request has appropriate privileges and then carries it out. And, in both cases, 
 the status of the SQL processing is reported back across the network.
  
 The local database in 
 Figure 22-2
  must do some very different work than the process it 
 normally uses to process local database requests, however. There are several 
 complications for the local DBMS:
  
 - 551 -",NA
Remote Data Transparency,"With any of the remote database-naming conventions that extend the usual SQL table and 
 view names, the additional qualifiers can quickly become annoying or confusing. For 
 example, if two tables in the remote database have columns with the same names, any 
 query involving both tables must use qualified column names—and the ""tablename"" 
 qualifiers now have the remote database qualification as well. Here's a qualified Informix 
 column name for the 
 NAME
  column in the remote 
 SALESREPS
  table owned by the user 
 JOE
  in a remote database named 
 SAMPLE
  on the remote Informix server 
 CENTRALHOST
 :
  
 SAMPLE@CENTRALHOST.JOE.SALESREPS.NAME
  
 A single column reference has grown to half a line of SQL text! For this reason, table 
 aliases are frequently used in SQL statements involving remote database access.
  
 Synonyms and aliases (previously described in 
 Chapter 16
 ) are also very useful for 
 providing more transparent access to remote databases. Here's an Informix synonym 
 definition that could be established by a user or a database administrator:
  
 CREATE SYNONYM REMOTE_REPS FOR SAMPLE@CENTRALHOST.JOE.SALESREPS
  
 The equivalent Oracle synonym definition is: 
  
 CREATE SYNONYM REMOTE_REPS FOR JOE.SALESREPS@CENTRALHOST
  
 With this synonym in place, the preceding qualified column name becomes simply:
  
 REMOTE_REPS.NAME
  
 Any query referencing the 
 REMOTE_REPS
  ""table"" and its columns is actually a remote 
 database query, but that fact is transparent to the user. In practice, most database 
  
 - 554 -",NA
Table Extracts,"Remote database access is very convenient for small remote queries and occasional 
 remote database access. If an application requires heavy and frequent access to a 
 remote database, however, the communications overhead of remote database access 
 can become large. Once remote access grows beyond a certain point, it is often more 
 efficient to maintain a local 
 copy
  of the remote data in the local database. Many of the 
  
 - 555 -",NA
Table Replication,"Several DBMS vendors have moved beyond their extract and load utility programs to offer 
 support for table extraction within the DBMS itself. Oracle8, for example, offers a 
 ""snapshot"" facility to automatically create a local copy of a remote table. In its simplest 
 form, the local table is a read-only replica of the remote ""master"" table, which is 
  
 automatically refreshed by the Oracle DBMS on a periodic basis. Here is an Oracle SQL 
 statement to create a local copy of product pricing data, assuming that the remote master 
 database includes a 
 PRODUCTS
  table like the one in the sample database:
  
 Create a local replica of pricing information from the remote 
 PRODUCTS
  table.
  
 CREATE SNAPSHOT PRODPRICE
  
  AS SELECT MFR_ID, PRODUCT_ID, PRICE
  
  FROM PRODUCTS@REMOTE_LINK
  
 This statement effectively creates a local Oracle table named 
 PRODPRICE
 . It contains 
 three columns, specified by the 
 SELECT
  statement against the remote (master) database. 
  
 The at sign and name 
 REMOTE_LINK
  in the statement tell Oracle that the 
 PRODUCTS 
 table from which the data is to be replicated is a remote table, accessible via the Oracle 
 database link named 
 REMOTE_LINK
 . The Oracle database administrator sets up these 
 remote database links as part of the distributed Oracle capabilities that are required to 
 use the snapshot feature. Finally, the 
 CREATE SNAPSHOT
  statement will actually cause 
 the local 
 PRODPRICE
  snapshot table to be populated with data from the remote 
 PRODUCTS
  table.
  
 With this type of read-only snapshot, users are not allowed to change the snapshot table 
 with 
 INSERT
 , 
 UPDATE
 , or 
 DELETE
  statements. All database updates occur in the master 
 (remote) table and are propagated to the replicated (snapshot) table by Oracle. The 
 database administrator can manually refresh the snapshot table as desired. The 
 CREATE 
 SNAPSHOT
  statement also includes rather comprehensive facilities for specifying 
  
 automatic refreshes. Here are some examples: 
  
 Create a local replica of pricing information from the remote 
 PRODUCTS
  table. Refresh the 
 data once per week, with a complete reload of the data.
  
 CREATE SNAPSHOT PRODPRICE
  
  REFRESH COMPLETE START WITH SYSDATE NEXT SYSDATE+7
  
  AS SELECT MFR_ID, PRODUCT_ID, PRICE
  
  FROM PRODUCTS@REMOTE_LINK
  
 Create a local replica of pricing information from the remote 
 PRODUCTS
  table. Refresh the 
 data once per day, sending only changes from the master table.
  
 CREATE SNAPSHOT PRODPRICE
  
  REFRESH FAST START WITH SYSDATE NEXT SYSDATE+1
  
  AS SELECT MFR_ID, PRODUCT_ID, PRICE
  
  FROM PRODUCTS@REMOTE_LINK
  
 In the latter example, the snapshot is refreshed by transmitting only changes from the 
 remote 
 PRODUCTS
  table. Oracle implements this capability by maintaining a log of 
 changes (a ""snapshot log"") on the remote system and updating the log every time an 
 update to the 
 PRODUCTS
  table would effect the snapshot replica. When the time for a 
  
 - 557 -",NA
Updateable Replicas,"In the simplest implementations, a table and its replicas have a strict master/slave 
 relationship, as shown in Figure 22-3. The central/master copy contains the ""real"" data. It 
 is always up to date, and all updates to the table must occur on this copy of the table. 
  
 The other slave copies are populated by periodic updates, managed by the DBMS. 
 Between updates, they may become slightly out of date, but if the database is configured 
 in this way, then it is an acceptable ""price to pay"" for the advantage of having a local copy 
 of the data. Updates to the slave copies are not permitted. If attempted, the DBMS returns 
 an error condition.
  
 By default, the Oracle 
 CREATE SNAPSHOT
  statement creates this type of ""slave"" replica of 
 a table. The master/slave relationship is implicit in the Microsoft SQL Server structure for 
 replication. The SQL server architecture defines the master as the ""publisher"" of the data 
 and the slaves as ""subscribers"" to the data. In the default configuration, there is a single 
 (updateable) publisher, and there may be multiple (read-only) subscribers. The SQL 
 Server architecture carries this analogy one step further, supporting both the notion of 
 ""push"" updates (the publisher actively sends the update data to the subscribers) and ""pull"" 
 updates (where the subscribers have primary responsibility for getting updates from the 
 publisher).
  
 There are some applications for which table replication is an excellent technique, but 
 where the master/slave relationship does not apply. For example, applications that 
 demand high availability use replicated tables to maintain identical copies of data on two 
 different computer systems. If one system fails, the other contains current data and can 
 carry on processing. An Internet application may demand very high database access 
 rates, and achieve this scalability by replicating a table many times on different computer 
 systems and then spreading out the workload across the systems. A sales force 
  
 automation application will probably contain one central 
 CUSTOMER
  table and hundreds of 
 replicas on laptop systems, and individual salespeople should be able to enter new 
 customers or change customer contact information on the laptop replicas. In these 
 configurations (and others), the most efficient use of the computer resources is achieved if 
 all
  of the replicas can accept updates to the table, as shown in Figure 22-4.
  
  
 Figure 22-4: 
 Replicas with multiple update sites
  
 A replicated table where multiple copies can accept updates creates a new set of data 
 integrity issues. What happens if the same row of the table is updated in one or more 
 replicas? When the DBMS tries to synchronize the replicas, which of the two updates 
 should apply, or should neither apply or both? What happens if a row is deleted from one 
 copy of the table, but it is updated in another copy of the table?
  
 - 559 -",NA
Replication Tradeoffs,"Practical replication strategies always involve a tradeoff between the desire to keep data 
 as current as possible and the desire to keep network traffic down to a practical level and 
 provide adequate performance. These tradeoffs usually involve not just technical 
 considerations, but business practices and policies as well. For example, consider an 
 order processing application using the sample database, and assume that order 
  
 processing is distributed across five different call centers that are geographically 
 distributed around the world. Each call center has its own computer system and 
  
 database. Incoming orders are checked against the 
 PRODUCTS
  table to be certain that 
 enough inventory is on hand to fill the order. The 
 PRODUCTS
  table keeps track of product-
 on-hand quantities for all of the company's warehouses, worldwide.
  
 Suppose the company's policy is that the order processing clerk must be able to 
  
 absolutely guarantee a customer that products can be shipped within 24 hours of the time 
 an order is accepted. In this case, the 
 PRODUCTS
  table must contain absolutely up-to-the-
 minute data, reflecting the inventory impact of orders taken just seconds earlier. There are 
 only two possible designs for the database in this case. There could be a single, central 
 copy of the 
 PRODUCTS
  table, shared by all users at all five order processing sites. 
  
 Alternatively, there could be a fully mirrored copy of the 
 PRODUCTS
  table at each of the 
 five sites. The fully mirrored solution is almost certainly impractical because the frequent 
 updates to the 
 PRODUCTS
  table as each order is taken will cause excessive network 
 traffic to keep the five copies of the table in perfect synchronization.
  
 But suppose the company believes it can still maintain adequate customer satisfaction 
 with a policy that is slightly less strict—for example, it promises to notify any customer 
 within 24 hours if their order cannot be filled immediately and give the customer an 
 opportunity to cancel the order. In this case, a replicated 
 PRODUCTS
  table becomes an 
 excellent solution. Once a day, updates to the 
 PRODUCTS
  table can be downloaded to the 
 replicated copy at each of the five sites. During the day, orders are verified against the 
 local copy
  of the 
 PRODUCTS
  table, but only the local 
 PRODUCTS
  table is updated. This 
 prevents the company from taking an order for which there was not adequate stock on 
 hand at the beginning of the day, but it does not prevent orders taken at two or three 
 different sites from exceeding the available stock. The next night, when data 
  
 communications costs are lower than they are during the day, the orders from each site 
 are transmitted to a central site, which processes them against a central copy of the 
 PRODUCTS
  table. Orders that cannot be filled from inventory are flagged, and a report of 
 them is generated. When processing is complete, the updated 
 PRODUCTS
  table, along 
 with the ""problem orders report,"" is transmitted back to each of the five sites to prepare for 
 the next day's processing.
  
 Which is the ""correct"" architecture for supporting the operation of this global business? 
 As the example shows, it is not so much a database architecture question as a business 
 policy question. The interdependence of computer systems architectures and business 
 operations is one of the reasons why decisions about replication and data distribution 
 inevitably make certain types of business operations easier and others harder.",NA
Typical Replication Architectures,"In many cases, it's possible to structure an application that involves replicated data so 
  
 - 560 -",NA
Distributed Database Access,"Over the last several years, research into fully distributed database access has slowly but 
 surely found its way into commercial products. Today many of the mainstream enterprise 
 database products offer at least some level of transparent distributed database access. 
  
 As noted earlier in the chapter, the performance implications of distributed database 
 access and updates can be very substantial. Two very similar-looking queries can create 
 massively different amounts of network traffic and overhead. A single query, carried out in 
 a ""brute force"" method or an ""optimized"" method can create the same differences, 
 depending on the quality of the optimization done by the DBMS.
  
 Because of these challenges, all of the vendors have taken a step-by-step approach to 
 delivering distributed database access. Several years ago, IBM announced its blueprint 
 for its SQL products and has been steadily implementing it. IBM was not the first vendor 
 to offer distributed data access, and it is not the vendor with the most advanced 
 distributed DBMS capability today, but IBM's four stages, shown in Table 22-1, provide 
 an excellent framework for understanding distributed data management capabilities and 
 their implications.
  
 - 563 -",NA
Remote Requests,"The first stage of distributed data access, as defined by IBM, is a 
 remote request
 , shown 
 in Figure 22-9. In this stage, the PC user may issue a SQL statement that queries or 
 updates data in a single remote database. Each individual SQL statement operates as its 
 own transaction, similar to the ""auto-commit"" mode provided by many interactive SQL 
 programs. The user can issue a sequence of SQL statements for various databases, but 
 the DBMS doesn't support multi-statement transactions.
  
  
 Figure 22-9: 
 Distributed data access: remote requests
  
 - 564 -",NA
Remote Transactions,"The second stage of distributed data access, as defined by IBM, is a 
 remote transaction 
 (called a ""remote unit of work"" by IBM), shown in Figure 22-10. Remote transactions 
 extend the remote request stage to include multi-statement transaction support. The PC 
 user can issue a series of SQL statements that query or update data in a remote 
 database and then commit or roll back the entire series of statements as a single 
 transaction. The DBMS guarantees that the entire transaction will succeed or fail as a 
 unit, as it does for transactions on a local database. However, all of the SQL statements 
 that make up the transaction must reference a single remote database.
  
  
 Figure 22-10: 
 Distributed data access: remote transactions
  
 Remote transactions open the door for distributed transaction processing applications. 
 For example, in an order processing application, a PC-based order entry program can 
 now perform a sequence of queries, updates, and inserts in the inventory database to 
 process a new order. The program ends the statement sequence with a 
 COMMIT
  or 
 ROLLBACK
  for the transaction.
  
 Remote transaction capability typically requires a DBMS (or at least transaction 
  
 processing logic) on the PC as well as the system where the database is located. The 
 transaction logic of the DBMS must be extended across the network to ensure that the 
 local and remote systems always have the same opinion about whether a transaction has 
 been committed. However, the actual responsibility for maintaining database integrity 
 remains with the remote DBMS.
  
 Remote transaction capability is often the highest level of distributed database access 
 provided by database gateways that link one vendor's DBMS to other DBMS brands. For 
 example, most of the independent enterprise database vendors (Sybase, Oracle, Informix) 
 provide gateways from their Unix-based DBMS systems to IBM's mainframe DB2 
 implementation. Some gateway products go beyond the bounds of remote 
  
 transactions, allowing a user to join, in a single query, tables from a local database with 
  
 - 565 -",NA
Distributed Transactions,"The third stage of distributed data access, as defined by IBM, is a 
 distributed transaction 
 (a ""distributed unit of work"" in IBM parlance), shown in Figure 22-11. At this stage, each 
 individual SQL statement still queries or updates a single database on a single remote 
 computer system. However, the sequence of SQL statements within a transaction may 
 access two or more databases located on different systems. When the transaction is 
 committed or rolled back, the DBMS guarantees that all parts of the transaction on all of 
 the systems involved in the transaction, will be committed or rolled back. The DBMS 
 specifically guarantees that there will not be a ""partial transaction,"" where the transaction 
 is committed on one system and rolled back on another.
  
  
 Figure 22-11: 
 Distributed data access: distributed transactions
  
 Distributed transactions support the development of very sophisticated transaction 
 processing applications. For example, in the corporate network of 
 Figure 22-1
 , a PC order 
 processing application can query the inventory databases on two or three different 
 distribution center servers to check the inventory of a scarce product and then update the 
 databases to commit inventory from multiple locations to a customer's order. The DBMS 
 ensures that other concurrent orders do not interfere with the remote access of the first 
 transaction.
  
 Distributed transactions are much more difficult to provide than the first two stages of 
 distributed data access. It's impossible to provide distributed transactions without the 
 active cooperation of the individual DBMS systems involved in the transaction. For this 
 reason, the DBMS brands that support distributed transactions almost always support 
 them only for a homogeneous network of databases, all managed by the same DBMS 
 brand (that is, an all-Oracle or all-Sybase network). A special transaction protocol, called 
 the 
 two-phase commit
  protocol, is used to implement distributed transactions and insure 
 that they provide the ""all-or-nothing"" requirement of a SQL transaction. The details of this 
 protocol are described later in this chapter.",NA
Distributed Requests,"The final stage of distributed data access in the IBM model is a distributed request, shown 
 in Figure 22-12. At this stage, a single SQL statement may reference tables from two or 
 more databases located on different computer systems. The DBMS is responsible for 
 automatically carrying out the statement across the network. A sequence of 
  
 distributed request statements can be grouped together as a transaction. As in the 
 previous distributed transaction stage, the DBMS must guarantee the integrity of the 
 distributed transaction on all systems that are involved.
  
 - 566 -",NA
The Two-Phase Commit Protocol *,"A distributed DBMS must preserve the ""all or nothing"" quality of a SQL transaction if it is to 
 provide distributed transactions. The user of the distributed DBMS expects that a 
 committed transaction will be committed on all of the systems where data resides, and that 
 a rolled back transaction will be rolled back on all of the systems as well. Further, failures 
 in a network connection or in one of the systems should cause the DBMS to abort a 
 transaction and roll it back, rather than leaving the transaction in a partially committed 
 state.
  
 All commercial DBMS systems that support or plan to support distributed transactions 
 use a technique called 
 two-phase commit
  to provide that support. You don't have to 
 understand the two-phase commit scheme to use distributed transactions. In fact, the 
 whole point of the scheme is to support distributed transactions without your knowing it. 
  
 However, understanding the mechanics of a two-phase commit can help you plan 
 efficient database access.
  
 To understand why a special two-phase commit protocol is needed, consider the database 
 in Figure 22-13. The user, located on System A, has updated a table on System B and a 
 table on System C and now wants to commit the transaction. Suppose that the DBMS 
 software on System A tried to commit the transaction by simply sending a 
 COMMIT
  
 message to System B and System C, and then waiting for their affirmative replies. This 
 strategy works so long as Systems B and C can both successfully commit their part of the 
 transaction. But what happens if a problem such as a disk failure or a deadlock condition 
 prevents System C from committing as requested? System B will commit its part of the 
 transaction and send back an acknowledgment, System C will roll back its part of the 
 transaction because of the error and send back an error message, and the user ends up 
 with a partially committed, partially rolled back transaction. Note that System A can't 
 ""change its mind"" at this point and ask System B to roll back the transaction. The 
 transaction on System B has been committed, and other users may already have modified 
 the data on System B based on the changes made by the transaction.
  
  
 Figure 22-13: 
 Problems with a ""broadcast"" commit scheme
  
 The two-phase commit protocol eliminates the problems of the simple strategy shown in 
 Figure 22-13. Figure 22-14 illustrates the steps involved in a two-phase commit:
  
 - 568 -",NA
Network Applications and Database Architecture,"Innovations in computer networking have been closely linked to many of the innovations in 
 relational database architectures and SQL over the years. Powerful minicomputers with 
 mainframe network connections (such as Digital's VAX family) were the first popular 
 platform for SQL-based databases. They offered a platform for decision support, based on 
 data offloaded from mainframe systems. They also supported local data processing 
 applications, for capturing business data and uploading it to corporate mainframe 
 applications.
  
 Unix-based servers and powerful local area networks (such as Sun's server products) 
 drove another wave of DBMS growth and innovation. This era of databases and networks 
 gave birth to the client/server architecture that dominated enterprise data processing in the 
 late 1980s and 1990s. Later, the rise of enterprise-wide networks and applications (such 
 as ERP) created a need for a new level of database scalability and distributed database 
 capability. Today, the exploding popularity of the Internet is driving still another wave of 
 innovation, as very high peak-load transaction rates and personalized user interaction 
 drive database caching and main-memory database technologies.",NA
Client/Server Applications and Database Architecture,"When SQL-based databases were first deployed on minicomputer systems, the database 
 and application architecture was very simple—all of the processing, from screen display 
 (""presentation"") to calculation and data processing (""business logic"") to database access 
 occurred on the minicomputer's CPU. The advent of powerful personal computers and 
 server platforms drove a major change in that architecture, for several reasons.
  
 The graphical user interface (GUI) of popular PC office automation software 
  
 (spreadsheets, word processors, and so on ) set a new standard for ease of use, and 
 companies demanded the same style of interface from corporate applications. Supporting 
 a GUI is processor-intensive and demands a high-bandwidth path from the processor to 
 the display memory that holds the screen image. While some protocols emerged for 
 running a GUI over the LAN (the X-windows protocol), the best place to run a production 
 application's presentation-layer code was clearly on the PC itself.
  
 Economics was also a factor. Personal computer systems were much cheaper, on a cost-
 per-processing-power basis, than minicomputers or Unix-based servers. If more of the 
  
 - 570 -",NA
Client/Server Applications with Stored Procedures,"Whenever an application is split across two or more networked computer systems, as in 
 Figure 22-15, one of the major issues is the interface between the two halves of the split 
 application. Each interaction across this interface now generates network traffic, and the 
 network is always the slowest part of the overall system, both in its data transmission 
 capacity (bandwidth) and in round-trip messaging delays (latency). With the architecture 
 shown in Figure 22-15, each database access (that is, each SQL statement) generates at 
 least one round-trip across the network.
  
 In an OLTP application, typical transactions may require as many as a dozen individual 
 SQL statements. For example, to take a customer's order for a single product in the 
 simple structure of the sample database, the order processing application might:
  
 •Retrieve the customer number based on the customer name (single-row 
 SELECT
 )
  
 •Retrieve the customer's credit limit to verify credit-worthiness (single-row 
 SELECT
 )
  
 •Retrieve product information, such as price and quantity available (single-row 
 SELECT
 )
  
 •Add a row to the 
 ORDERS
  table for the new order (
 INSERT
 )
  
 •Update the product information to reflect the lower quantity available (
 UPDATE
 )
  
 •Update the customer's credit limit, reducing the available credit (
 UPDATE
 )
  
 •Commit the entire transaction (
 COMMIT
 )
  
 for a total of seven round-trips between the application and the database. In a real-world 
 application, the number of database accesses might be two or three times this amount. 
  
 As transaction volumes grow, the amount of network traffic can be very significant.
  
 - 571 -",NA
Enterprise Applications and Data Caching,"Today, major applications from the large packaged enterprise software vendors are all 
 based on SQL and relational databases. Examples include large enterprise resource 
 planning (ERP), supply chain management (SCM), human resources management 
 (HRM), financial management, and other packages from vendors such as SAP, BAAN, 
 PeopleSoft, Vantive, Clarify, Siebel Systems, I2 Technologies, Manugistics, and others. 
  
 These large-scale applications typically run on large Unix-based server systems and 
 place a heavy workload on the supporting DBMS. To isolate the applications and DBMS 
 processing, and apply more total processing power to the application, they often use a 
 three-tier architecture shown in Figure 22-17.
  
  
 - 572 -",NA
High-Volume Internet Data Management,"High-volume Internet applications are also driving the trend to database caching and 
 replication in networked database architectures. For example, financial services firms are 
 competing for online brokerage clients by offering more and more advanced real-time 
 stock reporting and analysis capabilities. The data management to support this 
  
 application involve real-time data feeds (to insure that pricing and volume information in 
 the database is current) and peak-load database inquiries of tens of thousands of 
 transactions per second. Similar volume demands are found in applications for managing 
 and monitoring high-volume Internet sites. The trend to personalize Web sites 
  
 (determining ""on the fly"" what banner ads to display, what products to feature, and so on) 
 and measure the effectiveness of such personalization is another trend driving peak-load 
 data access and data capture rates.
  
 The Web has already shown an effective architecture for dealing with these types of 
 peak-load Internet volume demands—through Web site caching. Copies of heavily-
 accessed Web pages are ""pulled forward"" in the network and replicated. As a result, the 
  
 - 573 -",NA
Summary,"This chapter described the distributed data management capabilities offered by various 
 DBMS products and the tradeoffs involved in providing access to remote data:
  
 •A distributed database is implemented by a network of computer systems, each running 
 its own copy of the DBMS software and operating autonomously for local data access. 
 The copies of the DBMS cooperate to provide remote data access when required.
  
 •The ""ideal"" distributed database is one in which the user doesn't know and doesn't 
 care that the data is distributed; to the user, all of the relevant data appears as if it 
 were on the local system.
  
 •Because this ideal distributed DBMS is very difficult to provide and involves too many 
 performance tradeoffs, commercial DBMS products are providing distributed database 
 capability in phases.
  
 •Remote database access can be useful in situations where the remote access is a small 
 part of total database activity; in this case, it's more practical to leave the data in the 
 remote location and incur the network overhead for each database access.
  
 •Database replication is very useful in situations where there is relatively heavy access to 
 data in multiple locations; it brings the data closer to the point-of-access, but at the 
  
 - 574 -",NA
Chapter 23: ,NA,NA
SQL and Objects,NA,NA
Overview,"The only serious challenge to the dominance of SQL and relational database 
  
 management over the last few years has come from the emergence of an equally 
 significant trend—the growing popularity of object-oriented technologies. Object-oriented 
 programming languages (such as C++ and Java), object-oriented development tools, and 
 object-oriented networking (including object request brokers) have emerged as 
  
 foundation technologies for modern software development. Object technologies gained 
 much of their initial popularity for building personal computer applications with graphical 
 user interfaces. But their impact has grown, and they are being used today to build (and 
 more importantly, to link together) enterprise-wide network-based applications for large 
 corporations.
  
 In the early 1990s, a group of venture-backed ""object-oriented database"" companies was 
 formed with the goal of applying object-oriented principles to database management. 
  
 These companies believed that their object-oriented databases would supplant the 
 ""outdated"" relational databases as surely as the relational model had supplanted earlier 
 data models. However, they met with limited marketplace success in the face of 
  
 entrenched relational technologies and SQL. In response to the object challenge, many 
 relational database vendors moved aggressively to graft object technologies onto their 
 relational systems, creating hybrid ""object-relational"" models. This chapter describes the 
 object database challenge to SQL and the resulting object-relational features provided by 
 some major DBMS vendors.",NA
Object-Oriented Databases,"Considerable academic research on database technology over the past decade has been 
 focused on new, ""post-relational"" data models. Just as the relational model provided clear-
 cut advantages over the earlier hierarchical and network models, the goal of this research 
 is to develop new data models that will overcome some of the disadvantages of the 
 relational model. Much of this research has focused on how to merge the principles of 
 object-oriented programming and design with traditional database characteristics, such as 
 persistent storage and transaction management.
  
 In addition to the academic research, in the early and mid-1990s some large venture 
 capital investments flowed into a group of startup software companies whose goal was to 
 build a new generation of data management technologies. These companies typically 
 started with the object data structures used by an object-oriented program to manage its 
 in-memory data, and extended them for disk-based storage and multi-user access. 
  
 Enthusiastic supporters of these ""object-oriented databases"" (OODBs) firmly believed 
 that they would mount a serious challenge to the relational model and become the 
 dominant database architecture by the end of the decade. That scenario proved far off 
 the mark, but the object database vendors have had a significant impact on their 
 relational rivals.
  
 - 575 -",NA
Object-Oriented Database Characteristics,"Unlike the relational data model, where Codd's 1970 paper provided a clear, 
  
 mathematical definition of a relational database, there is no single definition of an object-
 oriented database. However, the core principles embodied in most object-oriented 
 databases include:
  
 •
 Objects
 . In an object-oriented database, everything is an object and is manipulated as an 
 object. The tabular, row/column organization of a relational database is replaced by the 
 notion of collections of objects. Generally, a collection of objects is itself an object and can 
 be manipulated in the same way that other objects are manipulated.
  
 •
 Classes
 . Object-oriented databases replace the relational notion of atomic data types with 
 a hierarchical notion of classes and subclasses. For example, 
 VEHICLES
  might be a class 
 of object, and individual members (""instances"") of that class would include a car, a bicycle, 
 a train, or a boat. The 
 VEHICLES
  class might include subclasses called 
 CARS
  and 
 BOATS
 , 
 representing a more specialized form of vehicle. Similarly, the 
 CARS 
 class might include a 
 subclass called 
 CONVERTIBLES
 , and so on.
  
 •
 Inheritance
 . Objects inherit characteristics from their class and from all of the higher-level 
 classes to which they belong. For example, one of the characteristics of a vehicle might be 
 ""number of passengers."" All members of the 
 CARS
 , 
 BOATS
 , and 
  
 CONVERTIBLES
  classes also have this attribute, because they are subclasses of 
 VEHICLES
 . The 
 CARS
  class might also have the attribute ""number of doors,"" and the 
 CONVERTIBLES
  class would inherit this attribute. However, the 
 BOATS
  class would not 
 inherit the attribute.
  
 •
 Attributes
 . The characteristics that an object possesses are modeled by its attributes. 
 Examples include the color of an object, or the number of doors that it has, and its 
 English-language name. The attributes are related to the object they describe in roughly 
 the same way that the columns of a table relate to its rows.
  
 •
 Messagesandmethods
 . Objects communicate with one another by sending and 
 receiving 
 messages.
  When it receives a message, an object responds by executing a 
 method,
  a program stored within the object that determines how it processes the 
 message. Thus an object includes a set of behaviors described by its methods. Usually 
 an object shares many of the same methods with other objects in higher-level classes.
  
 •
 Encapsulation
 . The internal structure and data of objects is hidden from the outside 
 world (""encapsulated"") behind a limited set of well-defined interfaces. The only way to 
 find out about an object, or to act on it, is through its methods, whose functions and 
 behaviors are clearly specified. This makes the object more predictable and limits the 
 opportunities for accidental data corruption.
  
 •
 Object identity
 . Objects can be distinguished from one another through unique object 
 identifiers, usually implemented as an abstract pointer known as an object 
 handle
 . 
 Handles are frequently use to represent relationships among objects; an object ""points to"" 
 a related object by storing the object's handle as one of its data items (attributes).
  
 These principles and techniques make object-oriented databases well suited to 
  
 applications involving complex data types, such as computer-aided design or compound 
 documents that combine text, graphics, and spreadsheets. The database provides a 
 natural way to represent the hierarchies that occur in complex data. For example, an 
 entire document can be represented as a single object, composed of smaller objects 
 (sections), composed of still smaller objects (paragraphs, graphs, and so on). The class 
 hierarchy allows the database to track the ""type"" of each object in the document 
  
 (paragraphs, charts, illustrations, titles, footnotes, and so on). Finally, the message 
 mechanism offers natural support for a graphical user interface. The application program 
 can send a ""draw yourself"" message to each part of the document, asking it to draw itself 
  
 - 576 -",NA
Pros and Cons of Object-Oriented Databases,"Object-oriented databases have stirred up a storm of controversy in the database 
 community. Proponents claim that object databases are essential to create a proper match 
 between the programming and database data models. They claim that the rigid, fixed, 
 row/column structure of relational tables is a holdover from the punch-card era of data 
 processing with its fixed data fields and ""record"" orientation. A more flexible model, where 
 classes of objects can be similar to one another (that is, share certain attributes) but also 
 different from one another is essential, they claim, to effectively model real-world 
 situations. Another claim is that the multi-table joins that are an integral part of the 
 relational data model inherently create database overhead and make relational 
  
 technology unsuitable for the ever-increasing performance demands of today's 
  
 applications. Finally, since objects are well-established as the in-memory data model for 
 modern programs, the proponents claim that the only ""natural"" data model is one that 
 transparently extends the in-memory model to permanent, shared, disk-based, multi-user 
 storage.
  
 Opponents of object-oriented databases are just as adamant in their claims that object-
 oriented databases are unnecessary and offer no real, substantive advantages over the 
 relational model. They claim that the ""handles"" of object-oriented databases are nothing 
 more than the embedded database pointers of pre-relational, hierarchical, and network 
 databases, recycled with different names. They point out that, like these earlier database 
 technologies, the object-oriented databases lack the strong underlying mathematical 
 theory that forms the basis of relational databases. The lack of object database standards 
 and the absence of a standardized query language like SQL are reflections of this 
 deficiency, and have prevented the development of vendor- independent tools and 
 applications that have been essential to the development of the database industry. In 
 response to claims of inferior performance, they point to the use of relational technology in 
 some of the most performance-demanding enterprise applications. They are also careful 
 to draw a distinction between the 
 external
  relational model of data and the underlying 
 implementation
 , which may well contain embedded pointers for performance acceleration. 
 Finally, they claim that any mismatch between object-oriented programming and relational 
 databases can be addressed by technologies like JDBC and other object-to-relational 
 interfaces.",NA
Objects and the Database Market,"In the marketplace, pure object-oriented databases have gained some success in 
 applications with very complex data models and those where the model of classes and 
 inheritance closely parallels the real world. However, the object database companies 
 have had real difficulty breaking through into the mainstream. Most are still far from 
 breaking through the $100 million annual revenue mark, and many are not yet profitable 
 and have been through several generations of management. In contrast, the largest 
 relational database vendors have continued to experience steady growth. The largest 
 have annual revenues in hundreds of millions or billions of dollars per year. Relational 
 database technology clearly continues to dominate the database market today.
  
 Not surprisingly, the object-oriented and relational camps have had a substantial impact 
 on one another. With the slow marketplace acceptance of object-oriented technology, the 
 object-oriented vendors have focused on some of the factors that created the success of 
 the relational generation two decades ago. They have formed standards groups, such as 
 the Object Data Management Group (ODMG), to standardize object-oriented database 
 technology. Several have added relational adapters, with standard interfaces such as 
 ODBC and SQL, as optional layers for relational access to their databases. Several have 
 focused on the international standards process and have worked to put strong object-
 oriented capabilities into the SQL3 standard. The net result has been a trend toward 
  
 - 577 -",NA
Object-Relational Databases,"Object-relational databases typically begin with a relational database foundation, and add 
 selected features that provide object-oriented capabilities. This approach simplifies the 
 addition of object capabilities for the major RDBMS vendors, whose enterprise-class 
 RDBMS products have been developed over the course of 15 or more years and would be 
 tremendously costly to reproduce from scratch. It also recognizes the large installed base 
 of relational systems and gives those customers a smoother upgrade path (not to mention 
 an upgrade revenue stream for the vendors).
  
 The object extensions that are commonly found in object-relational databases are:
  
 •
 Large data objects
 . Traditional relational data types are small in size—integers, dates, 
 short character strings; large data objects can store documents, audio and video clips, 
 Web pages, and other ""new media"" data types.
  
 •
 Structured/abstract data types
 . Relational data types are atomic and indivisible; 
 structured data types allow groups of individual data items to be grouped into higher-
 level structures that can be treated as entities of their own. 
  
 •
 User-defined data types
 . Relational databases typically provide a limited range of built-in 
 data types; object-oriented systems and databases emphasize the user's ability to define 
 their own, new data types.
  
 •
 Tables-within-tables
 . Relational database columns store individual data items; object-
 relational databases allow columns to contain complex data items, such as structured 
 types or even entire tables. This can be used to represent object hierarchies.
  
 •
 Sequences, sets, and arrays
 . In a traditional relational database, sets of data are 
 represented by rows in their own table, linked to an ""owning"" entity by a foreign key; 
 object-relational databases may allow the direct storage of collections of data items 
 (sequences, sets, arrays) within a single column.
  
 •
 Stored procedures
 . Traditional relational databases provide set-based interfaces, such as 
 SQL, for storing, selecting, and retrieving data; object-relational databases provide 
 procedural interfaces, such as stored procedures, that encapsulate the data and provide 
 strictly defined interactions.
  
 - 578 -",NA
Large Object Support,"Relational databases have traditionally focused on business data processing, storing and 
 manipulating data items that represent money amounts, names, addresses, unit 
  
 quantities, dates, times, and the like. These data types are relatively simple and require 
 small amounts of storage space, from a few bytes for an integer that holds order or 
 inventory quantities to a few dozen bytes for a customer name, employee address, or 
 product description. Relational databases have been optimized to manage rows 
  
 containing up to a few dozen columns of this type of data. The techniques they use to 
 manage disk storage and to index data assume that data rows will occupy a few hundred 
 to a few thousand bytes. The programs that store and retrieve data can easily hold dozens 
 or hundreds of these types of data items in memory, and can easily store and retrieve 
 entire rows of data at a time through reasonably sized memory buffers. The row-at-a-time 
 processing techniques for relational query results work well.
  
 Many ""modern"" types of data have quite different characteristics from traditional business 
 data. A single high-resolution graphical image to be displayed on a PC screen can require 
 hundreds of thousands of bytes of storage or more. A word processing document, such as 
 a contract or the text of this book, can take even more storage. The HTML text that 
 defines Web pages and the PostScript files that define printed images are other examples 
 of larger, document-oriented data items. Even a relatively short high-quality audio track 
 can occupy millions of bytes, and video clips can run to hundreds of 
  
 megabytes or even gigabytes of data. As multimedia applications have become more 
 important, users have wanted to manage these types of data along with the other data in 
 their databases. The ability to efficiently manage ""large objects,"" often called ""binary large 
 objects"" or 
 BLOB
 s, was one of the earliest advantages claimed for object-oriented 
 databases.",NA
BLOB,NA,NA
s in the Relational Model,"The first approach to supporting 
 BLOB
 s in relational databases was through the 
  
 underlying operating system and its file system. Each individual 
 BLOB
  data item was 
 stored in its own operating system file. The name of the file was placed in a character-
 valued column within a table, as a pointer to the file. The table's other columns could be 
 searched to find rows that met certain criteria. When an application needed to manipulate 
 the 
 BLOB
  content associated with one of the rows, it read the name of the file and 
 retrieved the 
 BLOB
  data from it. Management of the file input/output was the responsibility 
 of the application program. This approach worked, but it was error-prone and required that 
 a programmer understand 
 both
  the RDBMS and the file system interfaces. The lack of 
 integration between the 
 BLOB 
 contents and the database was readily apparent. For 
 example, you couldn't ask the database to compare two 
 BLOB
  data items to see if they 
 were the same, and the database couldn't provide even basic text searching capability for 
 BLOB
  contents.
  
 Today, most major enterprise-class DBMS systems provide direct support for one or 
 more types of 
 BLOB
  data. You can define a column as containing one of these 
 BLOB 
 data types and use it in certain situations in SQL statements. There are typically 
 substantial restrictions on the 
 BLOB
  data, such as not allowing its use in a join condition 
 or a 
 GROUPBY
  clause.
  
 Sybase Adaptive Server provides two large object data types. Its 
 TEXT
  data type can store 
 up to two billion bytes of variable-length text data. You can use a limited set of SQL 
 capabilities (such as the 
 LIKE
  text-search operator) to search the contents of 
 TEXT 
 columns. A companion 
 IMAGE
  data type can store up to two billion bytes of variable-length 
 binary data. Microsoft SQL Server supports these types, plus an 
 NTEXT
  data type 
  
 - 579 -",NA
Specialized ,NA,NA
BLOB,NA,NA
 Processing,"Because 
 BLOB
 s can be very large in size compared to the data items typically handled by 
 RDBMS systems, they pose special problems in several areas:
  
 •
  
 Data storage and optimization
 . Storing a 
 BLOB
  item ""in-line"" with the other contents of 
 a table's row would destroy the optimization that the DBMS performs to fit database 
  
 data neatly into pages that match the size of disk pages. For this reason, 
 BLOB
  data is 
 always stored ""out-of-line"" in separate storage areas. Most DBMS brands that support 
 BLOBs
  provide special 
 BLOB
  storage options, including named storage spaces that are 
  
 - 580 -",NA
Abstract (Structured) Data Types,"The data types envisioned by the relational data model are simple, indivisible ""atomic"" 
 data values. If a data item such as an address is actually composed of a street address, 
 city, state, and postal code, you as a database designer have two choices. You can treat 
  
 - 582 -",NA
Defining Abstract Data Types,"With the Informix row data type capabilities illustrated so far, each individual structured 
 column is defined in isolation. If two tables need to use the same row data type structure, 
 it is defined within each table. This violates one of the key principles of object-oriented 
 design, which is reusability. Instead of having each ""object"" (the two columns in the two 
 different tables) have its own definition, the row data type should be defined once and then 
 reused for the two columns. Informix Universal Server provides this capability through its 
 named row type
  feature. (The row data types shown in previous examples are 
 unnamed
  
 row data types.)
  
 You create an Informix named row type with the 
 CREATE ROW TYPE
  statement. Here are 
 examples for the 
 PERSONNEL
  table:
  
 CREATE ROW TYPE NAME_TYPE (
  
  F_NAME VARCHAR(15),
  
  M_INIT CHAR(1),
  
  L_NAME VARCHAR(20));
  
 CREATE ROW TYPE POST_TYPE (
  
  
  MAIN INTEGER,
  
  
  
  SFX INTEGER);
  
 CREATE ROW TYPE ADDR_TYPE (
  
   
  STREET VARCHAR(35),
  
   
   
  CITY VARCHAR(15),
  
   
  
  STATE CHAR(2),
  
  
  POSTCODE POST_TYPE);
  
 Note that the definition of a named row type can depend on other, previously created 
 named row types, as shown by the 
 ADDR_TYPE
  and 
 POST_TYPE
  definitions. With these 
 row data types defined, the name and address columns in the 
 PERSONNEL
  table (and any 
 other columns holding name or address data in other tables of the database) can be 
  
 - 584 -",NA
Manipulating Abstract Data Types,"Unfortunately, structured data types create new complexity for database update 
  
 statements that must insert or modify their structured data values. Informix Universal 
 Server is fairly liberal in its data type conversion requirements for unnamed row types. The 
 data you assign into a row-type column must simply have the same number of fields, of 
 the same data types. The 
 ROW
  constructor is used, as shown in previous examples, to 
 assemble individual data items into a row-type value for inserting or updating data.
  
 For named row types, the requirement is more stringent; the data you assign into a 
 named row-type column must actually have the same named row type. You can achieve 
 this in the 
 INSERT
  statement by 
 explicitly casting
  the constructed row-value to have the 
 NAME_TYPE
  data type: 
  
 INSERT INTO PERSONNEL
  
  VALUES (1234,
  
  ROW('John','J','Jones')::NAME_TYPE,
  
  ROW('197 Rose St.','Chicago','IL',
  
  ROW(12345,6789)));
  
 The double-colon operator casts the constructed three-field row as a 
 NAME_TYPE
  row and 
 makes the 
 VALUES
  clause compatible with the data types of the columns in the table.
  
 Oracle uses a slightly different approach to constructing structured data items and 
 inserting them into columns that have abstract data types. When you create an Oracle 
 abstract data type (using the 
 CREATE TYPE
  statement), Oracle automatically defines a 
 constructor method
  for the type. You can think of the constructor method as a function that 
 takes as its arguments the individual components of the abstract data type and returns an 
 abstract data type value, with the individual components all packaged together. The 
 constructor is used in the 
 VALUES
  clause of the 
 INSERT
  statement to ""glue together"" the 
 individual data item values into a structured data value that matches the column definition. 
 Here is an 
 INSERT
  statement for the 
 PERSONNEL
  table: 
  
 INSERT INTO PERSONNEL
  
  VALUES (1234,
  
 - 586 -",NA
Inheritance,"Support for abstract data types gives the relational data model a foundation for object-
 based capabilities. The abstract data type can embody the representation of an ""object,"" 
 and the values of its individual fields or subcolumns are its attributes. Another important 
 feature of the object-oriented model is 
 inheritance
 . With inheritance, new objects can be 
 defined as being a ""particular type of"" an existing object type (""class"") and inherit the 
 predefined attributes and behaviors of that type.
  
 Figure 23-2 shows an example of how inheritance might work in a model of a company's 
 employee data. All employees are members of the class 
 PERSONNEL
 , and they all have 
 the attributes associated with being an employee (employee number, name, and 
  
 address). Some employees are salespeople, and they have additional attributes (such as 
 a sales quota and the identity of their sales manager). Other employees are engineers, 
 with a different set of attributes (such as the academic degrees they hold or the current 
 project to which they are assigned). Each of these employee types has its own class, 
 which is a ""subclass"" of 
 PERSONNEL
 . The subclass inherits all of the characteristics of the 
 class above it in the hierarchy (we want to track all of the core personnel data for 
 engineers and salespeople, too). However, the subclasses have additional information 
 that is unique to their type of object. In Figure 23-2, the class hierarchy goes down to a 
 third layer for engineers, differentiating between technicians, developers, and managers.
  
  
 Figure 23-2: 
 Natural class hierarchy for a personnel application
  
 Informix Universal Server's abstract data type inheritance mechanism provides an easy 
 way to define abstract data types (Informix row types) that correspond to the natural 
 hierarchy in Figure 23-2. Assume that the Informix 
 PERS_TYPE
  row type has already been 
 created, as defined earlier in this chapter, and a typed table named 
 PERSONNEL 
 has been 
 created based on this row type. Using the Informix inheritance capabilities, here are 
 someCREATEROWTYPE
  statements for other types in the hierarchy:
  
 CREATE ROW TYPE SALES_TYPE (
  
  
  SLS_MGR INTEGER,                    /* employee number of 
 sales mgr */
  
  
  
  SALARY MONEY(9,2),                 /* annual salary */
  
  QUOTA MONEY(9,2))
  
  UNDER PERS_TYPE;
  
 CREATE ROW TYPE ENGR_TYPE (
  
  SALARY MONEY(9,2),                 /* annual salary */
  
 - 587 -",NA
Table Inheritance—Implementing Object Classes,"Informix Universal Server provides a capability called 
 table inheritance
  that moves the 
 table structure of a database away from the traditional relational model and makes it much 
 closer to the concept of an object class. Using table inheritance, it's possible to create a 
 hierarchy of typed tables (""classes""), such as the one shown in Figure 23-3. The 
  
 - 588 -",NA
"Sets, Arrays, and Collections","In a relational database, tables are the 
 only
  database structure used to represent a ""set of 
 objects."" For example, the set of engineers in our personnel database is represented by 
 the rows in the 
 ENGINEERS
  table. Suppose each engineer has a set of academic degrees 
 (a B.S. in science from MIT, a Ph.D. in electrical engineering from Michigan, and so on) 
 that are to be stored in the database. The number of degrees for each engineer will vary—
 from none for some engineers to perhaps half a dozen for others. In a pure relational 
 database, there is only one ""correct"" way to add this information to the data model. A new 
 table, 
 DEGREES
 , must be created, as shown in Figure 23-5. Each row in the 
 DEGREES
  
 table represents one individual academic degree held by one of the engineers. A column 
 in the 
 DEGREES
  table holds the employee number of the engineer holding the degree 
 described by that particular row, and serves as a foreign key to the 
 ENGINEERS
  table, 
 linking the two tables in a parent/child relationship. The other columns in the 
 DEGREES
  
 table describe the particulars of the degree.
  
  
 Figure 23-5: 
 A relational modeling of engineers and their degrees
  
 You have seen the type of parent/child relational table structure shown in Figure 23-5 
 many times in the earlier chapters of this book, and it has been a basic construct of 
 relational databases since the beginning. However, there are some disadvantages to 
 having this be the 
 only
  way in which sets of data attributes can be modeled. First, the 
 database tends to have a great many tables and foreign key relationships and becomes 
 hard to understand. Second, many common queries need to join three, four, or more 
 tables to get the required answers. Third, with the implementations of relational joins 
 provided by most DBMS systems, the performance of queries will deteriorate as they 
 involve more and more joins.
  
 An object-oriented model of the engineers and their degrees would tend to reject the 
  
 - 591 -",NA
Defining Collections,"Informix Universal Server supports collections of attributes through its 
 collection data 
 types
 . Three different collection data types are supported:
  
 •A 
 list
  is an ordered collection of data items, all of which have the same type. Within a 
 list, there is the concept of a ""first"" item, a ""last"" item, and the 
 n
 -th item. The items in the 
 list are 
 not
  required to be unique. For example, a list of the first names of the employees 
 hired in the last year, in order of hire, might be {'Jim', 'Mary', 'Sam', 'Jim', 'John'}.
  
 •A 
 multiset
  is an unordered collection of data items, all of which have the same type. 
  
 There is no concept of a sequencing to the items in a multiset; its items have no 
 implied ordering. The items are 
 not
  required to be unique. The list of employee first 
 names could be considered a multiset if you didn't care about the order of hire: {'Jim', 
 'Sam', 'John', 'Jim', 'Mary'}.
  
 •A 
 set
  is an unordered collection of unique data items, all of which have the same type. As 
 in a multiset, there is no concept of ""first"" or ""last""; the set has no implied ordering. 
  
 The items 
 must
  have unique values. The first names in the previous examples 
 wouldn't qualify, but the last names might: {'Johnson', 'Samuels', 'Wright', 'Jones', 
 'Smith'}.
  
 To illustrate the concept of collection data, we will expand the tables in our example 
 object-relational database as follows:
  
 •
  
 The 
 REPS
  table will include sales targets for each of the first, second, third, and fourth 
 quarters. The quarterly targets can naturally be represented as a list column added to 
  
 the 
 REPS
  table. The quarters have a natural ordering (first through fourth), the quota 
 for each quarter has the same data type (money), and the values are not necessarily 
 unique (that is, the quotas for the first and second quarters might be the same).
  
 •
  
 The 
 ENGINEERS
  table will include information about the academic degrees that each 
 engineer holds. Two items of data will actually be stored about each degree—the 
  
 actual degree (B.S., Ph.D., MBA, and so on) and the school. This data will be stored as 
 a multiset column added to the 
 ENGINEERS
  table, because it's possible to have two 
 identical entries—for example, an engineer may have an B.S. degree in engineering 
 and an B.S. degree in business from the same school.
  
 •
  
 The 
 TECHNICIANS
  table will include information about the projects to which each 
 technician is assigned. Each technician may be assigned to two or more projects, but 
  
 each project has a unique name. This data will be stored as a set column added to the 
 TECHNICIANS
  table. The data values must be unique, but no particular order is 
 associated with them.
  
 Here are some Informix 
 ALTER TABLE
  statements that implement these changes to the 
 previously defined tables:
  
 - 592 -",NA
Querying Collection Data,"Collection-valued columns complicate the process of querying the tables that contain 
 them. In the 
 SELECT
  item list, they generate multiple data values for each row of query 
 results. In search conditions, they don't contain individual data items, but it's sometimes 
 convenient to treat them as sets of data. The object-relational databases typically provide 
 a limited set of SQL extensions or extend existing SQL concepts to provide simple queries 
 involving collection data. For more advanced queries, they require you to write stored 
 procedure language programs with loop structures that process the collection data items 
 one-by-one.
  
 For query purposes, Informix treats the collection types as if they were a set of data 
 values, like the values that might be returned by a subquery. You can match individual 
  
 - 594 -",NA
Manipulating Collection Data,"Extensions to standard SQL syntax are used to insert new rows into a table containing 
 collection-valued columns. Informix provides a trio of ""constructors""—the 
 SET 
  
 constructor, 
 MULTISET
  constructor, and 
 LIST
  constructor—for this purpose. They 
 transform a list of data items into the corresponding collections to be inserted. Here is a 
 pair of 
 INSERT
  statements that illustrates their use with the tables in 
 Figure 23-6
 :
  
 INSERT INTO TECHNICIANS
  
  
  VALUES (1279,
  
  
  
  ROW('Sam','R','Jones'),
  
  
  
  ROW('164 Elm St.','Highland','IL',ROW(12345,6789)),
  
  
  ""SET{'atlas','checkmate','bingo'}"");
  
 INSERT INTO ENGINEERS
  
  
  VALUES (1281,
  
 - 595 -",NA
Collections and Stored Procedures,"Collections pose special problems for stored procedures that are retrieving and 
 manipulating data in tables that contain them. Both Oracle and Informix provide special 
 stored procedure language facilities for this purpose. In Informix, special SPL collection 
 variables must be used. Here is an SPL stored procedure fragment that handles the 
 PROJECTS
  collection column from the 
 TECHNICIANS
  table:
  
 define proj_coll collection;           /* holds project 
  
 collection */ 
  
 define a_project varchar(15);          /* holds individual 
 project */ 
  
 define proj_cnt  integer;              /* number of projects */ 
 define empl_name name_type;            /* buffer for tech name */
  
 /* Check how many projects the technician is supporting */ 
 select cardinality(projects) into proj_cnt
  
  
  from technicians 
  
 where empl_num = 1234;
  
 /* If too many projects, then refuse to add a new one */ 
 if (proj_cnt > 6) then . . .
  
 - 596 -",NA
User-Defined Data Types,"Object-relational data management systems generally provide a mechanism through 
 which a user can extend the built-in data types provided by the DBMS with additional, 
 user-defined data types. For example, a mapping application might need to operate on a 
 LOCATION
  data type that consists of a pair of latitude and longitude measurements, each 
 consisting of hours, minutes, and seconds. To effectively process location data, the 
 application may need to define special functions, such as a 
 DISTANCE(X,Y)
  function that 
 computes the distance between two locations. The meanings of some built-in operations, 
 such as a test for equality (
 =
 ) will need to be redefined for location type data.
  
 One way that Informix Universal Server supports user-defined data types is through its 
 OPAQUE
  data type. An 
 OPAQUE
  data type is (not surprisingly) ""opaque"" to the DBMS. The 
 DBMS can store and retrieve data with this type, but it has no knowledge of the internal 
 workings of the type. In object-oriented terms, the data is completely encapsulated. The 
 user must explicitly provide (in external routines, written in C or some similar 
  
 programming language) the data structure for the type, code to implement the functions or 
 operations that can be performed on the type (such as comparing two data items of the 
 type for equality), and code to convert the opaque type between internal and external 
 representations. Thus, 
 OPAQUE
  data types represent a low-level capability to extend the 
 core functionality of the DBMS with data types that appear as if they were built-in.
  
 A more basic user-defined data type capability is provided by the implementation of 
 DISTINCT
  data types within Informix. A 
 DISTINCT
  type is useful to distinguish among 
 different types of data, all of which use one of the DBMS built-in data types. For example, 
 the city and company name data items in a database might both be defined with the data 
 type 
 VARCHAR(20)
 . Even though they share the same underlying DBMS data type, these 
 data items really represent quite different types of data. You would never normally 
 compare a city value to a company name, and yet the DBMS will let you do this because 
 the two 
 VARCHAR(20)
  columns are directly comparable.
  
 To maintain a higher level of database integrity, you could define each of these three 
 data items as having a 
 DISTINCT
  data type:
  
 CREATE DISTINCT TYPE CITY_TYPE AS VARCHAR(20);
  
 CREATE DISTINCT TYPE CO_NAME_TYPE AS VARCHAR(20);
  
 Now tables can be created containing city and customer name data items in terms of the 
 CITY_TYPE
  and 
 CO_NAME_TYPE
  data types. If you try to compare columns with these two 
 different data types, the DBMS automatically detects the situation and generates an error. 
 You 
 can
  compare them, but only by explicitly casting the data type of one item to match the 
 data type of the other. As a result, the distinct data types assigned to the different columns 
  
 - 598 -",NA
Methods and Stored Procedures,"In object-oriented languages, objects encapsulate both the data and programming code 
 that they contain; the details of the data structures within an object and the programming 
 instructions that manipulate those data structures are explicitly hidden from view. The only 
 way to manipulate the object and obtain information about it is through 
 methods
 , which are 
 explicitly defined procedures associated with the object (or more accurately with the object 
 class). For example, one method associated with a customer object might obtain the 
 customer's current credit limit. Another method might provide the ability to change the 
 credit limit. The credit limit data itself is encapsulated, hidden within the customer object. 
  
 The data within the tables of a relational database is inherently 
 not
  encapsulated. The 
 data and its structure are directly visible to ""outside"" users. In fact, one of the main 
 advantages of a relational database is that SQL can be used to carry out 
 ad hoc
  queries 
 against the database. When the system catalog of a relational database is considered, 
 the contrast with the object-oriented ideal is even more extreme. With the catalog, the 
 database is self-describing, so that even applications that don't know the internal 
 structure of the database in advance can use SQL queries to find out what it is!
  
 Stored procedures provide a way for relational databases to offer capabilities that 
 resemble those of object-oriented methods. At the extreme, all users of a relational 
 database could be granted permission only to execute a limited set of stored procedures, 
 and no underlying data access permissions on the base tables at all. In this case, the 
 users' access would approach the encapsulation of the object-oriented ideal. In practice, 
 stored procedures are often used to provide application designers with the limited 
 database access that they need. However, the 
 ad hoc
  capabilities of the database are 
 almost always exploited by query tools or reporting programs.
  
 Oracle formalizes the linkage between object methods and database stored procedures by 
 allowing you to explicitly define a stored procedure as a 
 member function
  of an abstract 
 data type. Once defined in this way, the member function can be used in queries involving 
 the abstract data type, just as if it were a built-in function of the DBMS designed to work 
 on that type. Here is a redefinition of the 
 ADDR_TYPE 
 abstract data type that is used to 
 store addresses, with a relatively simple member function, named 
  
 GET_FULL_POST
 . The function takes the postal-code part of the address, which stores 
 both a five-digit main postal code and a four-digit suffix as two separate numbers, and 
 combines them into one nine-digit number, which it returns:
  
 CREATE TYPE ADDR_TYPE AS OBJECT (
  
  STREET VARCHAR(35),
  
  CITY VARCHAR(15),
  
  STATE CHAR(2),
  
  POSTCODE POST_TYPE,
  
  MEMBER FUNCTION GET_FULL_POST(POSTCODE IN POST_TYPE)
  
  RETURN NUMBER,
  
  PRAGMA RESTRICT_REFERENCES(GET_FULL_POST, WNDS));
  
 CREATE TYPE BODY ADDR_TYPE AS
  
  MEMBER FUNCTION GET_FULL_POST(POSTCODE POST_TYPE)
  
  RETURN NUMBER IS
  
  BEGIN
  
  RETURN((POSTCODE.MAIN * 10000) + POSTCODE.SFX);
  
  END;
  
  . . .
  
 - 599 -",NA
Summary,"Object-oriented databases will likely play an increasing role in specialized market 
 segments, such as engineering design, compound document processing, and graphical 
 user interfaces. They are not being widely adopted for mainstream enterprise data 
 processing applications. However, hybrid object-relational databases are being offered by 
 some of the leading enterprise DBMS vendors: 
  
 •The object-relational databases significantly extend the SQL and stored procedure 
 languages with object-oriented statements, structures, and capabilities.
  
 •Common object-relational structures include abstract/structured data types, tables-
 within-tables, and explicit support for object identifiers. These capabilities stretch the 
 simple relational model a great deal and tend to add complexity for casual or 
 ad hoc 
 users.
  
 •The object-relational extensions added by the various DBMS vendors are highly 
 proprietary. There are significant conceptual differences in the approaches as well as 
 differences in implementation approach.
  
 •Object-relational capabilities are particularly well suited for more complex data models, 
 where the overall design of the database may be simpler, even though individual 
  
 - 601 -",NA
Chapter 24: ,NA,NA
The Future of SQL,NA,NA
Overview,"SQL is one of the most important foundation technologies underpinning the computer 
 market today. From its first commercial implementation about two decades ago, SQL has 
 grown to become 
 the
  standard database language. In its first decade, the backing of IBM, 
 the blessing of standards bodies, and the enthusiastic support of DBMS vendors made 
 SQL a dominant standard for enterprise-class data management. In its second decade, 
 the dominance of SQL extended to personal computer and workgroup environments and 
 to new, database-driven market segments, such as data warehousing. Today's evidence 
 clearly shows the importance of SQL:
  
 •The world's second-largest software company, Oracle, has been built on the success of 
 SQL-based relational data management, through both its flagship database servers and 
 tools and SQL-based enterprise applications.
  
 •IBM, the world's largest computer company, offers its SQL-based DB2 product line as a 
 common foundation across all of its product lines and for use on competitor's systems as 
 well.
  
 •Microsoft, the world's largest software company, uses SQL Server as a critical part of its 
 strategy to penetrate the enterprise computing market with its Windows NT and 
 Windows 2000 platforms.
  
 •Every significant database company offers either a SQL-based relational database 
 product or SQL-based access to its non-relational products.
  
 •All of the major packaged enterprise applications (enterprise resource planning, supply 
 chain management, financial reporting, sales force automation, customer service 
 management, etc.) are built on SQL-based databases.
  
 •SQL is emerging as a standard for specialized databases in applications ranging from 
 data warehousing to mobile laptop databases to embedded applications in telecomm and 
 data communications networks.
  
 •SQL-based access to databases is an integral feature of Windows, available on the vast 
 majority of personal computer systems, and it is a built-in capability of popular PC 
 software products such as spreadsheets and report writers.
  
 This chapter describes some of the most important current trends and developments in the 
 database market, and projects the major forces acting on SQL and database management 
 over the next five to ten years.",NA
Database Market Trends,"Today's market for database management products exceeds $10 billion per year in 
 products and services revenues, up from about $3 billion per year a decade ago. On 
 several occasions over the last decade, lower year-over-year growth in the quarterly 
 revenues of the major database vendors have led analysts to talk about a maturing 
 database market. Each time, a wave of new products or new data management 
 applications has returned the market to double-digit growth. If the history of the 1990s is 
 any indication, database technology will continue to find new applications and generate 
 increasing revenues for years to come. The trends shaping the market bode well for its 
  
 - 602 -",NA
Enterprise Database Market Maturity,"Relational database technology has become accepted as a core enterprise data 
  
 processing technology and relational databases have been deployed by virtually all large 
 corporations. Because of the importance of corporate databases and years of experience 
 in using relational technology, many, if not most, large corporations have selected a single 
 DBMS brand as an enterprise-wide database standard. Once such a standard has been 
 established and widely deployed within a company, there is strong resistance to switching 
 brands. Even though an alternative DBMS product may offer advantages for a particular 
 application or may pioneer a new, useful feature, an announcement by the ""standard"" 
 vendor that such features are planned for a future release will often forestall the loss of a 
 customer by the established vendor. The trend to corporate database standards has 
 tended to reinforce and strengthen the market positions of the established major DBMS 
 vendors. The existence of large direct sales forces, established customer support 
 relationships, and multi-year volume purchase agreements has become as important as, 
 or more important than, technology advantage. With this market dynamic, the large, 
 established players tend to concentrate on growing their business within their existing 
 installed base instead of attempting to take customers away from competitors. In the late 
 1990s, industry analysts saw and predicted this tendency at both Informix and Sybase. 
 Oracle, with a much larger share of the market, was forced to aggressively compete for 
 new accounts in its attempt to maintain its database license revenue growth. 
  
 Microsoft, as the ""upstart"" in the enterprise database market, was cast in the role of 
 challenger, attempting to leverage its position in workgroup databases into enterprise-
 level prototypes and pilot projects as a way to pry enterprise business away from the 
 established players.
  
 One important impact of the trend to corporate DBMS vendor standardization has been a 
 consolidation in the database industry. New startup database vendors tend to pioneer new 
 database technology and grow by selling it to early adopters. These early adopters have 
 helped to shape the technology and identified the solution areas where it can deliver real 
 benefits. After a few years, when the advantages of the new technology have been 
 demonstrated, the startup vendors are acquired by large, established players. 
  
 These vendors can bring the new technology into their installed base, and bring their 
 marketing and sales muscle to bear in an attempt to win business in their competitor's 
 accounts. The early 1990s saw this cycle play out with database vendor acquisitions of 
 database tools vendors. In the late 1990s, the same cycle applied to mergers and 
 acquisitions of database vendors. Informix's purchase of Illustra (a pioneering object-
 relational vendor) and Red Brick (a pioneering data warehousing vendor) are two 
 examples of the pattern.",NA
Market Diversity and Segmentation,"Despite the maturing of some parts of the database market (especially the market for 
 corporate enterprise-class database systems), it continues to develop new segments and 
 niches that appear and then grow rapidly. For much of the 1990s, the most useful way to 
 segment the database market has been based on database size and scale—there were 
 PC databases, minicomputer databases, mainframe databases, and later workgroup 
 databases. Today's database market is much more diverse and is more accurately 
 segmented based on target application and specialized database capabilities to address 
 unique application requirements. Market segments that have appeared and are 
  
 experiencing high growth include:
  
 •Data warehousing databases, focused on managing thousands of gigabytes of data, 
 such as historical retail purchase data.
  
 •OLAP and ROLAP databases, focused on carrying out complex analyses of data to 
 discover underlying trends (""data mining""), allowing organizations to make better 
  
 - 603 -",NA
Packaged Enterprise Applications,"A decade or two ago, the vast majority of corporate applications were developed in-house 
 by the company's information systems department. Decisions about database technology 
 and vendor standardization were part of the company's IS architecture planning function. 
 Leading-edge companies sometimes took a risk on new, relatively unproven database 
 technologies in the belief that they could gain competitive advantage by using them. 
 Sybase's rise to prominence in the financial services sector during the late 1980s and 
 early 1990s is an example.
  
 Today, most corporations have shifted from ""make"" to ""buy"" strategies for major 
  
 enterprise-wide applications. Examples include Enterprise Resource Planning (ERP) 
 applications, Supply Chain Management (SCM) applications, Human Resource 
  
 Management (HRM) applications, Sales Force Automation (SFA) applications, customer 
 support applications, and others. All of these areas are now supplied as enterprise-class 
 packaged applications, along with consulting, customization and installation services, by 
 groups of software vendors. Several of these vendors have reached multi-hundred-million 
 dollar annual revenues. All of these packages are built on a foundation of SQL-based 
 relational databases.
  
 The emergence of dominant purchased enterprise applications has had a significant effect 
 on the dynamics of the database market. The major enterprise software package vendors 
 have tended to support DBMS products from only two or three of the major DBMS 
 vendors. For example, if a customer chooses to deploy SAP as its enterprise-wide ERP 
 application, the underlying database is restricted to those supported by the SAP 
 packages. This has tended to reinforce the dominant position of the current ""top-tier"" 
 enterprise database players and make it more difficult for newer database vendors. It has 
 also tended to lower average database prices, as the DBMS is viewed more as a 
 component part of an application-driven decision rather than a strategic decision in its own 
 right.
  
 The emergence of packaged enterprise software has also shifted the relative power of 
 corporate IS organizations and the packaged software vendors. The DBMS vendors today 
 have marketing and business development teams focused on the major enterprise 
 application vendors to insure that the latest versions of the applications support their 
 DBMS and to support performance tuning and other activities. The largest independent 
 DBMS vendor, Oracle Corporation, is playing 
 both
  roles, supplying both DBMS software 
 and major enterprise applications (that run on the Oracle DBMS, of course).",NA
Hardware Performance Gains,"One of the most important contributors to the rise of SQL has been a dramatic increase in 
 the performance of relational databases. Part of this performance increase was due to 
  
 - 604 -",NA
Benchmark Wars,"As SQL-based relational databases have moved into the mainstream of enterprise data 
 processing, database performance has become a critical factor in DBMS selection. User 
 focus on database performance, coupled with the DBMS vendors' interest in selling high-
 priced, high-margin, high-end DBMS configurations, has produced a series of 
  
 ""benchmark wars"" among DBMS vendors. Virtually all of the DBMS vendors have joined 
 the fray at some point over the last decade. Some have focused on maximum absolute 
 database performance. Others emphasize price/performance and the cost-effectiveness of 
 their DBMS solution. Still others emphasize performance for specific types of database 
 processing, such as OLTP or OLAP. In every case, the vendors tout benchmarks that 
 show the superior performance of their products while trying to discredit the benchmarks 
 of competitors.
  
 The early benchmark claims focused on vendor-proprietary tests, and then on two early 
 vendor-independent benchmarks that emerged. The Debit/Credit benchmark simulated 
 simple accounting transactions. The TP1 benchmark, first defined by Tandem, measured 
 basic OLTP performance. These simple standardized benchmarks were still easy for the 
 vendors to manipulate to produce results that cast them in the most favorable light.
  
 In an attempt to bring more stability and meaning to the benchmark data, several vendors 
 and database consultants banded together to produce standardized database 
  
 benchmarks that would allow meaningful comparisons among various DBMS products. 
 This group, called the Transaction Processing Council, defined a series of ""official"" OLTP 
 benchmarks, known as TPC-A, TPC-B, and TPC-C. The Council has also assumed a role 
 as a clearinghouse for validating and publishing the results of benchmarks run on various 
 brands of DBMS and computer systems. The results of TPC benchmarks are usually 
 expressed in transactions per minute (e.g., tpmC), but it's common to hear the results 
 referred to simply by the benchmark name (e.g.,""DBMS Brand X on hardware Y delivered 
 10,000 TPC-Cs"").
  
 The most recent TCP OLTP benchmark, TPC-C, attempts to measure not just raw 
 database server performance but the overall performance of a client/server configuration. 
  
 Modern multiprocessor workgroup-level servers are delivering thousands or tens of 
 thousands of transactions per minute on the TPC-C test. Enterprise-class Unix-based 
 SMP servers are delivering multiple tens of thousands of tpmC. The maximum results on 
 typical commercially-available systems (a multi-million dollar 64-bit Alpha processor 
 cluster) exceed 100,000 tpmC.
  
 The Transaction Processing Council has branched out beyond OLTP to develop 
  
 benchmarks for other areas of database performance. The TPC-D benchmark focuses on 
 data warehousing applications. The suite of tests that comprise TPC-D are based on a 
 database schema typical of warehousing environments, and they include more complex 
 data analysis queries, rather than the simple database operations more typical of OLTP 
 environments. As of this writing, development is underway on a third type of benchmark, 
  
 - 606 -",NA
SQL Standardization,"The adoption of an official ANSI/ISO SQL standard was one of the major factors that 
 secured SQL's place as the standard relational database language in the 1980s. 
  
 Compliance with the ANSI/ISO standard has become a checkoff item for evaluating DBMS 
 products, so each DBMS vendor claims that its product is ""compatible with"" or ""based on"" 
 the ANSI/ISO standard. Through the late 1980s and early 1990s, all of the popular DBMS 
 products evolved to conform to the parts of the standard that represented common usage. 
 Other parts, such as the module language, were effectively ignored. This produced slow 
 convergence around a core SQL language in popular DBMS products.
  
 As discussed in 
 Chapter 3
 , the SQL1 standard was relatively weak, with many omissions 
 and areas that are left as implementation choices. For several years, the standards 
 committee worked on an expanded SQL2 standard that remedies these weaknesses and 
 significantly extends the SQL language. Unlike the first SQL standard, which specified 
 features that were already available in most SQL products, the SQL2 standard, when it 
 was published in 1992, was an attempt to lead rather than follow the market. It specified 
 features and functions that were not yet widely implemented in current DBMS products, 
 such as scroll cursors, standardized system catalogs, much broader use of subqueries, 
 and a new error message scheme. DBMS vendors are still in the process of evolving their 
 products to support the full features of SQL2. In practice, proprietary extensions (such as 
 enhanced support for multimedia data or stored procedures or object 
  
 extensions) have often been more important to a DBMS vendor's success than higher 
 levels of SQL2 compliance.
  
 The work of the SQL standards groups continues, but it appears unlikely to produce a 
 single ""SQL3"" standard as a large step forward. Work on ""SQL3"" was divided fairly early 
 into separate, parallel efforts, focused on the core of the language, a call-level interface, 
 persistent stored modules (stored procedures), distributed transaction capabilities, time-
 based data, etc. Some of these efforts have already surfaced as standards in their own 
 right, or as enhancements to the 1992 SQL2 standard. For example, a SQL2-compatible 
  
 - 607 -",NA
SQL in the Next Decade,"Predicting the path of the database market and SQL over the next ten years is a risky 
 proposition. The computer market is in the early stages of a major transition into a new, 
 Internet-driven era whose impact is not yet fully understood. The emergence of the PC 
 and its creation of the client/server era of the 1980s and 1990s illustrates how shifts in the 
 underlying computer systems market can produce major changes in data 
  
 management architectures. It's likely that the Internet will have at least as large, if not a 
 larger, impact on the data management architectures of the next ten years. Nonetheless, 
 several trends appear to be safe predictions for the future evolution of database 
  
 management. They are discussed in the final sections of this chapter.",NA
Distributed Databases,"As more and more applications are used on an enterprise-wide basis or beyond, the ability 
 of a single, centralized database to support dozens of major applications and thousands of 
 concurrent users will continue to erode. Instead, major corporate databases will become 
 more and more distributed, with dedicated databases supporting the major applications 
 and functional areas of the corporation. To meet the higher service levels required of 
 enterprise-wide or Internet-based applications, data must be distributed; but to insure the 
 integrity of business decisions and operations, the operation of these distributed 
 databases must be tightly coordinated.
  
 - 608 -",NA
Massive Data Warehousing,"The last few years have demonstrated that companies that use database technology 
 aggressively and treat their data as a valuable corporate asset can gain tremendous 
 competitive advantage. The competitive success of WalMart, for example, is widely 
 attributed to its use of information technology (led by database technology) to track its 
 inventory and sales on a daily basis, based on cash register transaction data. This allowed 
 the company to minimize its inventory levels and closely manage its supplier relationships. 
 Data mining techniques have allowed companies to discover unexpected trends and 
 relationships based on their accumulated data—including the legendary discovery by one 
 retailer that late-night sales of diapers were highly correlated with sales of beer.
  
 It seems clear that companies will continue to accumulate as much information as they 
 can on their customers, sales, inventories, prices, and other business factors. The 
 databases to manage these massive quantities of data will need to support multi-level 
 storage systems. They will need to rapidly import vast quantities of new data, and rapidly 
 peel off large data subsets for analysis. Despite the high failure rate of data warehousing 
 projects, the large potential payoffs in reduced operating costs andmore ""on-target"" 
 marketing and sales activities will continue to drive data warehousing growth.
  
 Beyond the collection and warehousing of data, pressure will build to perform business 
 analyses in ""real-time."" One IS consulting group has already coined the term ""zero-
 latency enterprise"" to describe an architecture in which customer purchases translate 
 directly into changes in business plans with zero or very little delay. To meet this 
 challenge, database systems will continue to take advantage of processor speed 
 advances and multiprocessing technologies.",NA
Ultra-High-Performance Databases,"The emergence of an Internet-centric architecture is exposing enterprise data processing 
 infrastructures to new peak-load demands that dwarf the workloads of just a few years 
 ago. When databases primarily supported in-house applications used by a few dozen 
 employees at a time, database performance issues may have produced employee 
 frustration, but they did not really impact customers. The advent of call centers and other 
 customer support applications produced a closer coupling between data management and 
 customer satisfaction, but applications were still limited to at most hundreds of concurrent 
 users (the people manning the phones in the call center).
  
 With the Internet, the connection between a customer and the company's databases 
  
 - 609 -",NA
Internet and Network Services Integration,"In the Internet era, database management will increasingly become just one more 
 network service, and one that must be tightly integrated with other services, such as 
 messaging, transaction services, and network management. In some of these areas, 
 standards have emerged, such as the XA standard for distributed transaction 
 management. In others, standards have not yet emerged, making integration a more 
 difficult problem.
  
 The multi-tier architecture that is emerging for Internet-centric applications also poses new 
 questions about what roles should be played by the database manager and by other 
 components of the overall information system. For example, when network transactions 
 are viewed from the point of distributed databases, a two-phase commit protocol, 
 implemented in a proprietary way by a DBMS vendor, may provide a solution. When 
 network transactions involve a combination of legacy applications (e.g., mainframe CICS 
 transactions), relational database updates, and inter-application messages, the 
  
 transaction management problem moves outside the database and external mechanisms 
 are required.
  
 A similar tradeoff is being created by the emergence of application servers as a middle-tier 
 platform for executing business logic. Stored procedures have emerged as the DBMS 
 technique for embedding business logic within the database itself. Application servers are 
 creating an alternative platform for business logic, external to the database. It's not yet 
 clear how these two trends will be rationalized, and whether business logic will continue its 
 migration into the database or will settle in an application server layer. Whichever trend 
 predominates, tighter integration between database servers and application servers will be 
 required. Several of the DBMS vendors now produce their own application servers, and it 
 seems likely that they will provide the best integration within their own product lines. 
 Whether this approach will prevail against a ""best-of-breed"" approach remains another 
 open question.",NA
Embedded Databases,"Relational database technology has reached into many parts of the computer industry, 
 from small handheld devices to large mainframes. Databases underlie nearly all 
 enterprise-class applications as the foundation for storing and managing their 
  
 information. Lightweight database technology underlies an even broader range of 
 applications. Directory services, a foundation technology for the new era of value-added 
 data communications network services, are a specialized form of database technology. 
  
 - 610 -",NA
Object Integration,"The most significant unknown in the future evolution of SQL is how it will integrate with 
 object-oriented technologies. The center of gravity of application development has clearly 
 shifted to object-oriented techniques and tools. C++ and Java are growing in popularity, 
 not only for client-side interaction, but for server-side business logic as well. The core 
 row/column principles of the relational data model and SQL, however, are rooted in a 
 much earlier COBOL era of records and fields, not objects and methods.
  
 The object database vendors solution to the relational/object mismatch has been the 
 wholesale discarding of the relational model in favor of pure object database structures. 
  
 But the lack of standards, steep learning curve, lack of simple query facilities and other 
 disadvantages have prevented pure object databases from having any significant market 
 success to date. The relational database vendors have responded to the object database 
 challenge by embracing object-oriented features, but the result has been a proliferation of 
 non-standard, proprietary database features and SQL extensions.
  
 It's clear that relational database technology and object technology must be more tightly 
 integrated if relational databases are to remain an integral part of the next generation of 
 applications. Several trends are visible today:
  
 •Java-based interfaces to RDBMSs, such as JDBC and embedded SQL for Java, and 
 perhaps additional interfaces more like those presented by the OODBMSs.
  
 •Java as a standardized stored procedure language for implementing business logic 
 within a RDBMS. Virtually all of the major DBMS vendors have announced plans to 
 support Java as an alternative to their proprietary stored procedure languages.
  
 •Abstract, complex data types that exhibit object-oriented capabilities such as 
 encapsulation and inheritance. Beyond high-level agreement on the need to store 
 ""objects"" within a row/column structure, the specifics (nested tables, arrays, complex 
 columns) vary dramatically.
  
 •Extensions to standard SQL constructs to deal with complex data structures, including the 
 extensions in the object-oriented parts of the proposed SQL3 standard. The diversity in 
 SQL extensions matches the diversity in the way objects are being integrated into the 
 relational model.
  
 •Message-oriented interfaces, including database triggers that produce messages 
 external to the DBMS for integration with other applications.
  
 Whether these extensions to SQL and the relational model can successfully integrate the 
 worlds of RDBMS and objects remains to be seen. The object-oriented database vendors 
  
 - 611 -",NA
Part VII:,NA,NA
 Appendices,NA,NA
Appendix List,"Appendix The Sample Database 
  
 A: 
  
 Appendix Database Vendor Profiles 
  
 B: 
  
 Appendix Company and Product List 
  
 C: 
  
 Appendix SQL Syntax Reference 
  
 D: 
  
 Appendix SQL Call Level Interface 
  
 E: 
  
 Appendix SQL Information Schema Standard 
 F: 
  
 Appendix CD-ROM Installation Guide 
  
 G:",NA
Appendix A:,NA,NA
 The Sample Database,NA,NA
Overview,"Most of the examples in this book are based on the sample database described in this 
 appendix. The sample database contains data that supports a simple order processing 
 application for a small distribution company. It consists of five tables:
  
 •
 CUSTOMERS
 , which contains one row for each of the company's customers.
  
 •
 SALESREPS
 , which contains one row for each of the company's ten salespeople.
  
 •
 OFFICES
 , which contains one row for each of the company's five sales offices where 
 the salespeople work.
  
 •
 PRODUCTS
 , which contains one row for each type of product that is available for sale.
  
 •
 ORDERS
 , which contains one row for each order placed by a customer. For simplicity, 
 each order is assumed to be for a single product.
  
 - 612 -",NA
Appendix B:,NA,NA
 Database Vendor Profiles ,NA,NA
Overview ,"The database systems vendors profiled in this appendix have been selected because of 
  
 their unique positions within the broader database industry. They include the providers of 
  
 the leading enterprise-class DBMS products, some smaller companies that are leaders in 
  
 new technology areas, pioneers in newer segments of the database market, and vendors 
  
 that focus on embeddable database technology. Any compilation like this cannot possibly 
  
 be exhaustive, and the omission of a company does not mean that its products or 
  
 capabilities are inferior to those of the vendors profiled here. Collectively, these 
  
  
 companies and their profiles as presented, illustrate the landscape of today's multi-billion-
  
 dollar database software and services market. The vendors are:
  
  
 •A2i, Inc.
  
 •Angara Database Systems
  
 •Arbor Software (now Hyperion Solutions 
 Corporation)•Ardent Software
  
 •Centura Software (SQLBase)
  
 •Cloudscape, Inc.
  
 •Computer Associates (Jasmine, Ingres II)
  
 •Computer Corporation of America (Model 204)
  
 •Empress Software
  
 •IBM Corporation (DB2)
  
 •Informix Software
  
 •Microsoft Corporation (SQL Server)
  
  
 - 616 -",NA
"A2i, Inc. (www.a2i.com) ","Founded in 1993, A2i develops and markets an integrated, database-driven, cross media 
  
 catalog publishing system that centralizes the management of catalog data, simplifies the 
  
 catalog production process, and completely automates the catalog production workflow. 
  
 The system includes tools for creating, designing, and publishing both printed and 
 electronic catalogs; supports simultaneous publishing to paper, CD-ROM and the Web 
 from a single data source; and efficiently manages catalogs containing from hundreds to 
 millions of items.
  
 All of A2i's software products layer on top of a SQL-based DBMS. They include 
  
 performance accelerators that improve catalog access by a factor of 10 to 1000 times that 
 of SQL alone, and feature additional catalog-specific functionality that supports interactive 
 browsing and sorting of large databases in ways that would otherwise be impossible using a 
 traditional SQL-based DBMS alone. A2i's parametric search technology—an alternative to 
 DBMS-style query forms that is intuitive, easy to use, and very, very fast—allows a user to 
 search an entire catalog and locate any item or group of items in a matter of seconds, 
 narrowing down from thousands or millions of items to one or several with just a few mouse 
 clicks.",NA
Angara Database Systems (www.angara.com) ,"Angara Database Systems is focused on the emerging market for in-memory database 
 systems. The Angara main-memory data manager is planned to offer both a SQL-level 
 interface as well as a lower-level C language API for direct access to the storage manager. 
 For data sets that can be completely contained in a computer system's main memory, the 
 company claims database performance of up to 40 times the speed of a 
   
 - 617 -",NA
Arbor Software (www.hyperion.com),"Arbor was one of the early leaders in the development of Online Analytic Processing 
 (OLAP) databases and tools. Arbor's flagship Essbase OLAP server was first introduced 
 in 1992 and pioneered many of the capabilities that have now become commonplace in 
 analytic systems. Large corporations typically use the Essbase product suite to create 
 integrated reporting, analysis, and planning systems.
  
 Current versions of the Essbase product support both client/server and Web-based 
 analytic processing and reporting. They support both pre-calculated data (a hallmark of 
 most OLAP systems) and dynamic, ""on the fly"" calculations. Another major enhancement 
 of the Essbase product is ""distributed OLAP"" capability, which allows OLAP databases to 
 be partitioned across computer networks. Essbase supports both its own proprietary 
 multidimensional database formats and integrates with conventional relational databases. 
  
 It runs on Windows-based systems, OS/2, the leading Unix systems, and IBM's AS/400 
 mid-range systems.
  
 In 1998, Arbor merged with Hyperion Solutions Corporation to create a $400 million 
 company (annual revenue) focused on business reporting and analysis. The product line 
 has grown to include integration products and customization services. It spans applications 
 from single-user analysis on Windows workstations to enterprise-wide Web-based OLAP 
 deployments for hundreds of users.",NA
Ardent Software (www.ardentsoftware.com),"Ardent Software, headquartered in Westboro, Massachusetts, offers a family of database 
 products and tools. Ardent's UniVerse relational database system is a SQL-based RDBMS 
 with ODBC and ActiveX interfaces. It offers entry-level compliance with the SQL2 
 standard, with extensions including national language support, stored procedures, 
 triggers, and distributed database capability. A' separate data manager, UniData, offers 
 support for complex data management applications with a ""nested relational"" capability 
 (tables-within-tables).
  
 Other products in the Ardent suite include development tools, integration tools, and system 
 administration tools. The DataStage product suite handles data reformatting, 
  
 transformation, and cleansing tasks for creating data warehouses. The JRB (Java 
  
 Relational Binding) toolkit bridges the object-oriented world of Java applications with the 
 row/column structure of an RDBMS. Other tools extend and enhance the capabilities of the 
 O2 object database.",NA
Centura Software (www.centurasoft.com),"Centura Software was founded as Gupta Technologies, by a former manager of Oracle's 
 microcomputer division. The company's initial focus was a DBMS and database 
  
 development tools for PCs and PC-local area networks. Renamed Centura Software, the 
 company now focuses on embedded database applications, primarily targeting 
  
 independent software vendors and value-added resellers.
  
 SQLBase, the company's flagship DBMS product, has evolved considerably since its 
 origins as a standalone and client/server database for IBM PCs under MS-DOS. It has 
 grown to support Windows NT and Netware as database servers. Centura currently 
 targets SQLBase for applications on PCs and sub-PC devices such as  handheld PCs, 
 RISC-based information appliances (e.g., smart phones), and even smart cards. It 
  
 - 618 -",NA
"Cloudscape, Inc. (www.cloudscape.com)","Cloudscape is a venture-backed, privately held database company founded by some of 
 the principal architects of Sybase's database systems. The founders formed Cloudscape 
 to build a 100 percent pure Java implementation of a SQL-based relational database. 
 Because the Cloudscape DBMS itself is a set of Java routines, it can run on virtually any 
 computer system that has a Java virtual machine. This includes devices ranging from 
 ""information appliances"" such as enhanced telephones and network computers to 
 mainframe-class systems. It also means the Cloudscape DBMS can be integrated 
 relatively easily as an embedded DBMS within a Web browser. New or updated 
  
 components of the DBMS can be transmitted over a network in the same way that Java 
 applets are transmitted to a Web browser.
  
 Cloudscape is targeting its DBMS to mobile computing applications running on notebook 
 computer systems. In this configuration, Cloudscape can run as a ""disconnected"" DBMS to 
 support local laptop applications. Later, when the notebook computer is connected to a 
 central server database, the Java-based capabilities of Cloudscape make activities like data 
 and application synchronization easier. At this writing, the Cloudscape DBMS has just 
 recently begun to ship.",NA
Computer Associates (www.cai.com),"Computer Associates (CA) is one of the world's largest independent software companies. 
 Initially focused on mainframe software, the company has steadily expanded its focus to 
 provide an extensive line of software products and services for enterprise data 
  
 processing. Computer Associates has been built largely through acquisition, taking 
 advantage of its large direct sales force and well-established relationships with senior 
 Fortune 500 information systems executives. Through its acquisitions, it has steadily 
 added more products to its portfolio.
  
 Ingres, one of the earliest relational database systems to appear on the market, is now a 
 product of Computer Associates. It was originally developed at the University of California 
 at Berkeley as a research project under the direction of Professor Michael Stonebreaker. 
  
 The research project became the foundation of an independent company, which 
 eventually changed its name to Ingres Corporation in the 1980s. Ingres and its native 
 QUEL query language were an early competitor to SQL, which was backed by rival Oracle 
 Corporation. Although most analysts gave Ingres clear claim to technical 
  
 leadership, Oracle's aggressive marketing and sales efforts, coupled with IBM's backing of 
 SQL, eventually led to SQL dominance in the market. Eventually, Ingres was adapted to 
 support SQL, which emerged as the dominant standard. In the 1990s, Ingres was sold to 
 the ASK Group, and eventually to Computer Associates.
  
 The current version of the product, Ingres II, is a comprehensive relational database 
 management product suite. The core Ingres/DBMS is augmented by Ingres/ICE (Internet 
 Commerce Enabled), a capability that links the DBMS to the Web. Networking support and 
 standards-based ODBC access are supported by the Ingres/Net product. Distributed 
 database support is available through Ingres/Star (a sophisticated distributed data 
 manager) and Ingres/Replicator, which provides transparent replication. Computer 
 Associates' OpenROAD product provides a layered development environment for Ingres 
 with a three-layer development framework, encompassing Presentation, Business Object, 
 and DBMS layers.
  
 - 619 -",NA
Computer Corporation of America (www.cca-int.com),"Computer Corporation of America (CCA) is one of the pioneering software companies, 
 and has been involved in data management since its founding in 1965. It develops and 
 sells one of the earliest DBMS systems: Model 204. The product has been substantially 
 enhanced over the years, but the focus continues to be on mainframe systems.
  
 Model 204 now features an ANSI-compliant SQL interface, even though the underlying 
 structure is a network database architecture. The network structure is manifested in 
 Model 204's embedded table capability—essentially a table-within-a-table structure. 
 Although network databases fell out of favor with the advent of SQL and the relational 
 model, some of the same capabilities provided by the network systems are now 
 appearing in highly touted new object-relational systems. The nested table structure 
 offered by Model 204 is an example of such a capability, which appears in object-
 relational systems from Informix and in Oracle's flagship Oracle 8 object-oriented 
 extensions.
  
 The current version of Model 204 includes multiprocessing and parallel query options for 
 data warehousing applications. Over the years its indexing structures have become quite 
 sophisticated and now include bit-map, hashing, b-tree, and record list schemes. Another 
 unique feature of Model 204 is support for iterative queries—queries that are carried out 
 against the results of previous queries. SQL-based access to mainframe Model 204 
 databases is available through CCA's Connect* product, which offers ODBC and OLE-DB 
 APIs for remote database access from Windows and Unix-based client workstations.",NA
Empress Software (www.empress.com),"Empress Software produces an ANSI SQL relational database system for embedded 
 applications. The company was founded in 1979 and is headquartered in Toronto, 
 Canada. The Empress DBMS offers both an ODBC callable API and Embedded SQL 
 interfaces. It also offers a low-level set of database access calls that come in ""below"" the 
 SQL access layer. These calls provide direct access to the Empress storage manager 
 layer for very high performance record insert, update, delete, and retrieve operations.
  
 The Empress DBMS runs on many different Unix-based systems, including several Unix 
 operating system variants that run on Intel processor-based systems. It also supports 
 Windows, Windows NT, and a range of real-time operating systems typically used for 
 embedded applications. It offers a rich collection of data types, plus user-definable 
 functions and procedures. For Internet-based applications, Empress also offers script 
 language interfaces for the popular Perl and Tcl/Tk scripting languages.",NA
IBM Corporation (www.ibm.com),- 620 -,NA
Informix Software (www.informix.com),"Informix was one of the original leaders in the Unix-based relational database market. 
  
 The company's first relational DBMS was implemented on Unix-based microcomputer 
 systems in the early 1980s, and was known for its efficiency and compactness. In 1985, 
 Informix was rewritten as a SQL-based DBMS and introduced as Informix-SQL. It was 
 subsequently ported to a wide range of systems, from IBM PCs under MS-DOS to 
 Amdahl mainframes running Unix. Informix was also one of the first database vendors to 
 expand its product offerings beyond the core database engine to include development 
 tools. Its Informix-4GL product family supports the development of forms-based 
  
 interactive applications.
  
 In the early 1990s, Informix expanded its product line into the office automation area, 
 including among other products, a database-integrated spreadsheet named Wingz. This 
 effort was not very successful against Microsoft's office suite juggernaut, and Informix 
 refocused on its core database capabilities. One of its flagship products during the mid-
 1990s was Informix Parallel Server, the technology leader in so-called parallel query 
 technology. Parallel Server splits the processing of a single complex query into multiple, 
 parallel operations, which can take advantage of symmetric multiprocessing (SMP) 
 servers. Later, Informix established a leadership position in object-relational technology 
 through the acquisition of Illustra. Illustra was a venture-backed database software firm, 
 led by Michael Stonebreaker (the same Berkeley professor who had led the development 
 of Ingres years before). A side-effect of the Illustra acquisition was a proliferation of 
 product lines and development teams within Informix, adding to some confusion among 
 Informix customers.
  
 Today, Informix is a multi-hundred-million dollar database company. It has merged its 
 distinct product lines into a unified product line based on Informix Dynamic Server, a 
 multithreaded database server for Unix and Windows NT-based systems. What were 
 formerly separate product lines are now modular optional additions to the core Dynamic 
 Server architecture. The Universal Data option adds object-relational capabilities, including 
 the capability to develop customized plug-in ""data blades"" that support new data types and 
 methods for handling them. The Advanced Decision Support Option provides capabilities for 
 complex analytic processing, and a MetaCube ROLAP option supports multidimensional
  
 - 621 -",NA
Microsoft Corporation (www.microsoft.com),"Microsoft Corporation, the world's largest personal computer software company, is also a 
 major vendor in the SQL-based database market. Microsoft's first foray into database 
 products came in 1987 and began as a defensive move. With the announcement of OS/2 
 Extended Edition, IBM tried to establish built-in database management and data 
  
 communications as key components of an enterprise-class PC operating system. In 1988, 
 Microsoft responded with SQL Server, a version of the Sybase DBMS ported to OS/2. 
 Although Microsoft later abandoned OS/2 in favor of its own Windows NT 
  
 operating system, SQL Server continued as its flagship DBMS. Today SQL Server is a 
 major product in the workgroup database segment, and Microsoft is aggressively moving 
 to establish it as an enterprise-class DBMS competing with Oracle and DB2.
  
 Expanding on its early experience with SQL Server, Microsoft moved on several other 
 fronts to expand its role as a database vendor. In the early 1990s, Microsoft acquired 
 Foxbase Corporation, developer of the Foxbase DBMS. Foxbase had established itself 
 as a very successful ""clone"" of dBASE, the most popular and widely used PC database 
 product. Through the acquisition, Microsoft moved to challenge Borland International, 
 which had acquired the rights to dBASE shortly before.
  
 While the Foxbase acquisition was focused more on the PC installed base and the 
 relatively mature market for character-based, flat file PC databases, Microsoft's internal 
 development focused on the new, growing market for graphical lightweight relational PC 
 databases. After several false starts and abandoned development prototypes, the result 
 product, Microsoft Access, was introduced. Microsoft Access continues today as both a 
 standalone lightweight database product, and a front-end for SQL-based production 
 databases.
  
 Microsoft also moved aggressively to enable Windows as a database access and 
 database development platform. Its first major move in this area was the introduction of 
 Open DataBase Connectivity (ODBC), a SQL-based API for database access. Microsoft 
 built ODBC capability into Windows and successfully lobbied the SQL Access Group, a 
 database vendor association, to adopt it as a callable database API standard. This early 
 version of ODBC eventually made its way into the formal ISO standards as the SQL Call 
 Level Interface (CLI). Microsoft has continued to evolve ODBC and expand its 
  
 capabilities.
  
 Microsoft has also layered other database access APIs on top of ODBC. The first such 
 step was to incorporate database access into Microsoft's Object Linking and Embedding 
 (OLE) framework for linking applications together. The OLE/DB portion of the OLE suite 
 provided source-independent data access, and relied on ODBC as its underlying 
 architecture for working with relational databases. Later, with the recasting of OLE into the 
 ActiveX component framework, another layer was added to the database access 
 hierarchy. The Active Data Objects (ADO) set of components provide data access within 
 Microsoft's Component Object Model (COM) architecture. Again, the ADO capabilities are 
 layered on top of ODBC for relational database access.
  
 Parallelling the evolution of the Windows database access capability, Microsoft has steadily 
 expanded and enhanced the capabilities of SQL Server. SQL Server 7, introduced in 1998, 
 represented a major step forward. Among its major features was an integrated OLAP server 
 and Data Transformation Services, putting Microsoft squarely into competition with the data 
 warehousing vendors and the warehouse-oriented database engine of the major database 
 vendors. The high-end Enterprise Edition package provided fail-over clustering, 
 multiprocessing support for up to 8-way SMP systems and much more extensive replication 
 services for both online and offline distributed databases. The major enterprise database 
 vendors maintain that SQL Server is still not an enterprise-scale DBMS, but in typical 
  
 - 622 -",NA
Object Design (www.odi.com),"Object Design was one of the early object database vendors. The company, 
  
 headquartered in Burlington, Massachusetts, was founded in 1988. The initial version of 
 its ObjectStore object database system was shipped in 1990. Object Design is still firmly 
 focused on a pure object-oriented database approach. It does not offer SQL-based 
 access to ObjectStore. However, it does position ObjectStore as a front-end database 
 technology that can access ""legacy"" SQL-based relational databases, such as Oracle, 
 DB2, Sybase, and Informix, through its ObjectStore DBConnect product.
  
 The current ObjectStore product is offered in two packages addressing two different target 
 markets. ObjectStore Persistent Storage Engine (PSE) is a small-footprint persistent object 
 store for Java, C++, and ActiveX applications. It is focused on embedded database 
 applications, where the DBMS is hidden within the applications. The most recent release of 
 ObjectStore PSE is a pure Java database for embedded use. The full-blown ObjectStore 
 OODBMS is focused on more conventional database applications, with features such as 
 distributed database support and replication. It also focuses on delivering improved 
 OODBMS performance through intelligent object caching, using a so-called cache-forward 
 architecture.",NA
Objectivity (www.objectivity.com),"Objectivity was one of the early object-oriented database vendors, and has steadily 
 enhanced its Objectivity OODBMS over the years. It has added fault-tolerant and data 
 replication capabilities to its core object database engine. Access to the Objectivity 
 OODBMS is provided from C++, Java, and Smalltalk.
  
 Although Objectivity remains firmly focused on an object-oriented architecture, it has moved 
 to provide SQL-based access to its object database engine. The Objectivity/SQL++ product 
 provides both an ODBC interface and a proprietary Objectivity C++ API, and an Interactive 
 SQL++ capability. The SQL language used through these interfaces contains many 
 extensions to accommodate access to object database structures. Unique object-ids within 
 the Objectivity database are automatically mapped to row-ids available via the SQL 
 interface. Object ""associations"" within the OODB are available for use as SQL join criteria. 
 Stored procedures and triggers are presented via extended SQL features. Extended SQL 
 syntax is also provided to access elements of arrays and nested object structures, which 
 appear as ""complex columns"" to the SQL user. These capabilities provide the advantages 
 of ""SQL-based"" access to many of Objectivity's object-oriented capabilities, but at the 
 expense of very non-standard SQL syntax.",NA
Oracle Corporation (www.oracle.com),"Oracle Corporation was the first DBMS vendor to offer a commercial SQL product, 
 preceding IBM's own announcement by almost two years. During the 1980s, Oracle grew 
 to become the largest independent DBMS vendor. Today it is the dominant enterprise 
 DBMS competitor, selling its products through an aggressive direct sales force and 
 through a variety of other channels.
  
 The Oracle DBMS was originally implemented on Digital minicomputers, but the center of 
 gravity of Oracle system sales shifted firmly to Unix-based minicomputers and servers in 
 the 1990s. One of the major advantages of Oracle is its portability. It is available on 
 dozens of different computer systems, from Windows-based laptop computers through 
 Sun, HP, and IBM Unix-based systems to IBM mainframes. Using Oracle's SQL*Net 
 networking software, many of these Oracle implementations can participate in a 
  
 distributed network of Oracle systems. With these capabilities, Oracle has targeted 
 enterprise-wide database deployments and has been effective in leveraging its market 
 leadership into a position as an IS-imposed corporate-wide database standard in many 
 organizations.
  
 - 623 -",NA
Persistence Software (www.persistence.com),"Persistence Software was initially focused on software that bridged the gap between 
 object-oriented development and messaging technologies (including object request 
 brokers) and relational database technology. Its middleware products supported object-
 based data management structures and requests, and mapped them into relational 
 databases stored in the major RDBMS systems. One of the primary target markets for 
 Persistence products has been the financial services market.
  
 More recently, Persistence has enhanced its products and repositioned them as a 
 transactional application server. The company's PowerTier server family includes versions 
 designed to support C++ development or Java (via Enterprise Java Beans). One of the 
 major features of the PowerTier servers is in-memory caching of objects, which 
  
 Persistence describes as in-memory caching of ""current business state."" Other capabilities 
 of the servers include object transaction isolation and object triggers. The servers continue 
 to offer database independence, integrating with the mainstream enterprise database 
 engines of Oracle, Informix, Sybase, and Microsoft. Application development in C++, Java, 
 and Visual Basic is supported.",NA
Persistence Software (www.persistence.com),- 624 -,NA
Pervasive Software (www.pervasive.com),"Pervasive Software is one of the newer RDBMS companies, but it traces its roots back to 
 the earliest days of personal computer databases. The storage manager that underlies the 
 Pervasive products, Btrieve, was initially developed as a PC-based database for MS-DOS 
 systems in the early 1980s. SoftCraft, the company that developed Btrieve, was acquired 
 in 1987 by Novell Netware, the vendor of the industry's leading network operating system. 
 As a result, Btrieve became a more tightly integrated part of the Netware OS. Layered 
 capabilities, including Netware SQL, were developed as layers on top of the Btrieve 
 storage manager.
  
 In 1994, Novell decided to refocus on its core network operating system capabilities, and 
 its database technologies were spun out into a new company, which was renamed 
 Pervasive Software in 1996. Pervasive's focus is on cost-effective SQL-based databases 
 for use by independent software vendors (ISVs) and value-added resellers (VARs). 
  
 Packaged software for accounting, inventory control, order processing, and similar 
 functions use it as an underlying, bundled database manager. These products are 
 typically sold to small and medium-sized businesses, and to departments of big 
 companies.
  
 Pervasive's current product, Pervasive SQL, combines their Scalable SQL and Btrieve 
 products. The emphasis is on features important to the small/medium business market. 
 These include low database administration, scalability to support business volumes, a 
 small DBMS footprint, and the ability to handle reasonable data volumes at low cost. 
 Overwhelmingly, Pervasive SQL is used by an ISV or VAR and delivered as a bundled 
 component of their software product, often invisible to the end user.",NA
Quadbase Systems (www.quadbase.com),"Quadbase is a SQL-based client/server database system for IBM-compatible PCs. It was 
 originally offered in the early 1990s as a DOS/Windows database with a file-server 
 architecture. It has since evolved into a client/server database, with support for Netware, 
 Windows, and Windows NT-based servers. The Quadbase SQL implementation is ANSI 
 SQL-92 compliant at the Entry Level. It provides both Embedded SQL interfaces (for C, 
 C++, and SmallTalk) and an ODBC callable API.
  
 Quadbase supports a number of advanced SQL features including updateable scroll 
 cursors and views. Its multi-user concurrency control offers the flexibility of multiple 
 isolation levels for balancing database integrity requirements with performance concerns. 
 Quadbase also supports read-only schemas that allow it to be used to create and access 
 read-only databases on CD-ROMs.
  
 - 625 -",NA
Raima Corporation (www.raima.com),"Raima Corporation, founded in 1982, was an early database vendor focused on the IBM 
 PC database market. Its initial db_VISTA product was first released in 1984. It has been 
 steadily enhanced over the years and combined with an object manager to create the 
 current Raima Database Manager++ (RDM++) product.
  
 A newer Raima product, the Velocis Database Server, was first shipped in 1993. Velocis is 
 a SQL-based relational database system with an ODBC interface. It is designed as an 
 embeddable database, and the company targets it to professional application developers 
 (ISVs and VARs) who use it as a bundled database foundation. Velocis runs on 
  
 Windows, Windows NT, OS/2, and many Unix-based operating system variants.
  
 A distinctive feature of the Velocis server is its explicit support for network data model's 
 embedded pointers within a SQL-based database. A 
 CREATE JOIN
  statement specifies an 
 explicit relationship, implemented with network database-style pointers, which are stored 
 within the database structure. These can then be exploited with SQL syntax, delivering very 
 fast performance. Velocis supports C/C++, Java, Visual Basic, Delphi, and Perl language 
 interfaces as well as the industry-standard ODBC interface.",NA
Red Brick Systems (www.redbrick.com),"Red Brick (named after the red brick building where the company was founded in Los 
 Gatos, California) was an early pioneer in the data warehousing market. Its founder, 
 Ralph Kimball, remains a recognized expert in data warehousing. The company's core 
 offering is a SQL-based DBMS which is heavily optimized for data warehousing 
 applications.
  
 Optimizations in the Red Brick system include high-performance data loading, with a 
 parallel loader capability for exploiting SMP systems and high-performance data 
 transformation, cleansing, and integrity checking. The Red Brick software also allows 
 automatic pre-calculation of aggregate data values (sums, averages, minimum, and 
 maximum values) during the table loading process.
  
 The Red Brick DBMS also focused on a high-performance implementation of the ""star 
 schema"" structure often found in data warehousing applications. Its STARindex 
  
 technology and associated STARjoin capability implement support for star schemas within 
 the database structure itself. The DBMS also features adaptive bitmap indexing for rapid 
 data selection from very large tables. SQL extensions within the RISQL language handle 
 typical decision support query structures, such as selecting the ""top 3"" or the ""95
  
 Despite its early lead in the data warehousing market and several early customer 
 successes, Red Brick found its early momentum hard to sustain. Other, much larger 
 database vendors, including Oracle Corporation, Sybase, IBM, and eventually Microsoft, 
 saw data warehousing as a major market opportunity and announced (sometimes with 
 much-delayed shipment) data warehousing capabilities for their product lines. Although 
 its products retained acknowledged technical advantages, Red Brick saw customers 
 decide to wait for their current DBMS vendor. The company was sold to Informix 
 Corporation in 1998, and the Red Brick data warehousing engine will be integrated into 
 the Informix product line.",NA
Rogue Wave Software (www.roguewave.com),"Rogue Wave Software, founded in 1989, is a provider of object-oriented software 
 components. The company's products include component object parts that can be 
 combined and reused to build enterprise-class applications. Other products are 
 development tools for building user interfaces and other application elements using 
 object-oriented techniques.
  
 - 626 -",NA
"Sybase, Inc. (www.sybase.com)","Sybase was a hot mid-1980s DBMS startup company, funded by tens of millions of dollars 
 in venture capital. The company's founding team and many of its early employees were 
 alumni of other DBMS vendors, and for most of them, Sybase represented the second or 
 third relational DBMS they had built. Sybase quite effectively positioned its product as ""the 
 relational DBMS for on-line applications,"" and stressed the technical and architectural 
 features that distinguished it from contemporary SQL-based DBMS 
  
 products. These features included the following:
  
 •A client/server architecture, with client software running on Sun and VAX workstations 
 and IBM PCs and the server running on VAX/VMS or Sun systems
  
 •A multi-threaded server that handled its own task management and input/output for 
 maximum efficiency
  
 •A programmatic API, instead of the embedded SQL interface used by most other 
 DBMS vendors at the time
  
 •Stored procedures, triggers, and a Transact-SQL dialect that extended SQL into a 
 complete programming language for building substantial parts of an application within 
 the database itself
  
 Aggressive marketing and a first-class roster of venture capital backers gained Sybase the 
 attention of industry analysts, but it was a subsequent OEM deal with Microsoft (the 
 leading PC software vendor) and Ashton-Tate (the leading PC database vendor) that 
 positioned the company as an up-and-coming DBMS vendor. Renamed SQL Server, the 
 Sybase DBMS was ported to OS/2 (at the time, both IBM's and Microsoft's strategic future 
 PC operating system) to be marketed to computer systems vendors by Microsoft and 
 through retail computer channels by Ashton-Tate. Sales from the alliance never met early 
 expectations, but it propelled Sybase into the DBMS market as a serious player. 
  
 Today, SQL Server (several generations later) continues to be Microsoft's strategic 
 DBMS for Windows NT; Microsoft has split from Sybase, pursuing its own development 
 path. Sybase remains a major DBMS vendor, but the positive impact of its formative 
 alliance with Microsoft has long since passed.
  
 The innovations that made the Sybase product unique in the late 1980s were eventually 
 copied by the other DBMS vendors. Sybase's early lead cemented its leadership position 
 in market segments that demanded high-performance OLTP, including especially financial 
 services applications—these niches remain Sybase strongholds today. During the 1990s, 
 Sybase expanded its product line to include development tools through a merger with 
 PowerSoft, one of the leading DBMS tools vendors. Other mergers and acquisitions 
 brought consulting services and other data management technologies.
  
 Sybase's current product line has three distinct database engines, focused on three 
 different segments of the database market:
  
 •Sybase Adaptive Server IQ is focused on data warehousing. It features complex query 
 optimization techniques that are claimed to improve performance by 100 times over 
 conventional RDBMSs.
  
 •Sybase Adaptive Server Anywhere is focused on mobile computing. It features a small 
 footprint and integrated support for Java classes and objects as well as Java stored 
 procedures.
  
 - 627 -",NA
Tache Group (www.tachegroup.com),"The Tache Group is the vendor of CQL++, a SQL and B-tree/ISAM data management 
 package. CQL++ offers both single-user and client/server operation. It is a layered product, 
 providing database access both at the SQL level and at the lower, ISAM record-oriented 
 level. CQL++ is designed for embedded applications. A unique feature is that it includes 
 complete C++ source code for the DBMS, which allows the user to extend the core 
 database and ISAM capabilities. CQL++ is available on Linux, HP, Silicon Graphics, and 
 Solaris Unix-based systems, and on Windows and Windows NT platforms.",NA
Tandem Computers (www.tandem.com),"Tandem was an early leader in the market for fault-tolerant minicomputer systems and 
 remains a major competitor in this market. Many Tandem systems are sold to financial 
 services and transportation companies for use in online transaction processing 
  
 applications that demand 24 hours/day, 7 days/week non-stop operation. Tandem's older 
 systems run the proprietary TXP operating system, and fault-tolerant applications are 
 generally written in the proprietary Tandem Application Language (TAL). More recent 
 Tandem systems are based on Unix operating systems. In 1997, Tandem was acquired by 
 Compaq Computer Corporation, a leading vendor of personal computer systems and 
 workgroup servers, as part of its move to become a major enterprise computer systems 
 vendor. Tandem has since announced that its future fault-tolerant systems will be based 
 on the Digital Alpha 64-bit processor and Digital Unix. (Digital Computer, once a leading 
 independent minicomputer vendor, was itself acquired by Compaq in 1998, continuing 
 Compaq's push into enterprise data processing.)
  
 Database management for non-stop applications on Tandem systems has been provided 
 for many years by a SQL-based Tandem-developed RDBMS called Non-Stop SQL. 
  
 Because of Tandem's heavy OLTP emphasis, Non-Stop SQL has pioneered several 
 special techniques, such as disk mirroring. It also takes advantage of the inherent Tandem 
 multi-processor architecture and provides distributed database capabilities. The 
 programmatic interface to Non-Stop SQL is through embedded SQL.
  
 During the 1980s and early 1990s, virtually every minicomputer vendor had its own 
 proprietary SQL-based implementation (Digital with Rdb/VMS, Hewlett-Packard with 
 Allbase/SQL, Data General with DG-SQL, etc.). Over the years, all of the other systems 
 vendors have concluded that the high cost of maintaining their own RDBMS with 
 competitive features was prohibitive. They also had difficulty managing the dual roles of 
 competing with the independent DBMS vendors (such as Oracle) and also working with 
 them as ISV partners on their platforms. As a result, Tandem is the only remaining major 
 system vendor (except for IBM) with its own proprietary SQL-based RDBMS.",NA
TimesTen Performance Software (www.timesten.com),"TimesTen is a venture-backed database company focused on delivering ultra-high-
 performance main-memory database systems. The company was formed as a spinoff of 
 a main-memory database project at Hewlett-Packard, and its underlying technology has 
 been shipping as an embedded component of HP telecommunications systems since 
 1996. TimesTen's version of the technology began shipments in early 1998. It features 
 an ODBC API and industry-standard SQL, and runs on Windows NT and Unix-based 
 servers from HP, Sun Microsystems, and IBM.
  
 - 628 -",NA
Versant Corporation (www.versant.com),"Versant was one of the early object database vendors. Its first OODBMS product shipped 
 in September 1990. The current version of its database product offers Java, C++, and 
 Smalltalk interfaces. The object database engine is multi-session and multi-threaded and 
 runs on Windows NT and Unix platforms. One of its distinguishing characteristics is fault-
 tolerant capability with automatic failover.
  
 Like all of the pure object database vendors, Versant initially presented itself as a next 
 generation DBMS system, rejecting the relational vendors and their systems as 
  
 ""yesterday's technology."" More recently, the company has opened its OODBMS to the 
 relational world through the Versant SQL suite, providing SQL access and an ODBC API. 
  
 The SQL facility, and a corresponding Interactive SQL utility, are available for Versant 
 servers on Solaris, AIX, HP-UX, and Windows NT platforms
  
 The philosophy of the Versant SQL suite is to automatically present as much of the 
 OODBMS capabilities in a relational model as possible. It automatically maps the Versant 
 database's object schema to a corresponding SQL schema: for example, it transforms two 
 object classes with many-to-many relationship into two base tables and intersection table to 
 represent relationships. SQL schema information is available through virtual 
  
 SYSTABLES
 , 
 SYSCOLUMNS
 , and 
 SYSINDEXES
  catalog views. Embedded pointers within the 
 object schema are exploited transparently to enhance query performance. In addition to the 
 programmatic (ODBC) and interactive SQL interfaces, the SQL suite includes data loading 
 and extraction tools to move information between the Versant OODBMS and conventional 
 RDBMS systems.",NA
Appendix C:,NA,NA
 Company and Product List,NA,NA
Overview,"This appendix contains a list of companies and products in the DBMS marketplace, most 
 of which are mentioned in this book. The majority of the products listed are SQL-based 
 database management systems or database tools. The companies appear in alphabetical 
 order. Key products for each company appear in italics.
  
 A2i, Inc.
  
 1925 Century Park East, Suite 255
  
 Los Angeles, CA 90067
  
 phone:  310-286-2220
  
 fax:       310-286-2221
  
 e-mail: info@a2i.com
  
 - 629 -",NA
Appendix D:,NA,NA
 SQL Syntax Reference,NA,NA
Overview,"The ANSI/ISO SQL standard specifies the syntax of the SQL language using a formal 
 BNF notation. Unfortunately, the standard is difficult to read and understand for several 
 reasons. First, the standard specifies the language bottom-up rather than top-down, 
 making it difficult to get the ""big picture"" of a SQL statement. Second, the standard uses 
 unfamiliar terms (such as 
 table-expression
  and 
 predicate
 ). Finally, the BNF in the 
 standard is many layers deep, providing a very precise specification but masking the 
 relatively simple structure of the SQL language.
  
 This appendix presents a complete, simplified BNF for ""standard"" SQL as it is commonly 
 implemented in the products of most DBMS vendors. Specifically: 
  
 •The language described generally conforms to that required for entry-level 
 conformance to the SQL2 standard, plus those intermediate-level and full-level 
 conformance features that are commonly found in the major DBMS products.
  
 •The module language is omitted because it is replaced in virtually all SQL 
 implementations by embedded SQL or a SQL API.
  
 •Components of the language are referred to by the common names generally used in 
 DBMS vendor documentation, rather than by the technical names used in the standard.
  
 The BNF in this appendix uses the following conventions:
  
 •SQL keywords appear in all 
 UPPERCASE MONOSPACE
  characters.
  
 •Syntax elements are specified in 
 italics
 .
  
 •The notation 
 element-list 
 indicates an 
 element
  or a list of 
 elements
  separated by 
  
 - 634 -",NA
Appendix E:,NA,NA
 SQL Call Level Interface,NA,NA
Overview,"This appendix describes the collection of routines that comprise the ISO/IEC standard 
 SQL Call Level Interface (CLI). The routines are presented here in their C-language 
 forms. The names of the routines presented are identical to the names used in the 
 standard. They should be used in exactly this form to call the routines in a CLI-compliant 
 library. 
  
 For clarity, the routines are presented here with two differences from the standard. The 
 names of the parameters of the routines are abbreviated in this appendix to make the 
 routine headers easier to read, and in some cases, to clarify their function. In actual calls 
 to the routines from an application program, you use the names of the application program 
 variables to be used as input and output parameters instead of the parameter names. Also 
 for clarity, the data types of the parameters are stated here in terms of the actual C-
 language data types (e.g., 
 long
 , 
 short
 , 
 *char
 ). The standard defines the parameters 
 using defined symbolic constants (
 #define
 's in the C language) to represent these data 
 types.
  
 Appendix A.1 of the standard (ISO/IEC 9075-3:1995) is a C-language header file that 
 defines symbolic constants for all of the constants and codes specified in the standard, 
 and uses the full parameter variable names specified in the standard. The following is a 
 summary of the routines, organized by function:
  
 AllocHandle() 
  
 Allocates resources for environment, connection, descriptor, or 
  
 statement
  
 FreeHandle() 
  
 Frees previously allocated resources
  
 AllocConnect() 
  
 Allocates resources for a database connection
  
 FreeConnect() 
  
 Frees resources for a database connection
  
 Connect() 
  
 Establishes a database connection
  
 Disconnect() 
  
 Ends an established database connection
  
 DataSources() 
  
 Gets a list of available SQL servers to which connection may be 
  
 made
  
 AllocEnv() 
  
 Allocates resources for a SQL environment
  
 FreeEnv() 
  
 Frees resources for a SQL environment
  
 SetEnvAttr() 
  
 Set attribute value for a SQL environment
  
 - 635 -",NA
CLI Return Values ,"Every CLI routine returns a 
 short
  value with one of the following values and meanings: 
  
 CLI Return Value 
  
 Meaning 
  
  
 0 
  
 Statement completed successfully 
  
  
 1 
  
 Successful completion with warning 
  
     
 No data found (when retrieving query results) 
  
  
 99 
  
 Data needed (required dynamic parameter missing)
  
  
 -1 
  
 Error during SQL statement execution 
  
     
 Error—invalid handle supplied in call
  
 -2",NA
General Handle Management Routines ,"These routines are used to allocate a handle for use by the CLI, and to free a previously-
 allocated handle that is no longer needed. The allocation routine accepts an argument 
 indicating what type of handle is to be allocated. In general, it may be preferable to use the 
 routines that create and free the specific types of handles, described in their respective 
 sections. These routines must be used to allocate and free application program descriptor 
 handles.
  
 /* Allocate a handle for use in subsequent CLI calls */ 
  
 short  SQLAllocHandle (
  
  
  short    hdlType,    /* IN:  integer handle type code */
  
  long     inHdl,      /* IN:  env or conn handle */
  
  
  long     *rtnHdl)    /* OUT: returned handle */ 
  
 /* Free a handle previously allocated by SQLAllocHandle() */ 
 short  SQLFreeHandle (
  
  
  
  short    hdlType,    /* IN:  integer handle type code */
  
   
 - 637 -",NA
SQL Environment Management Routines,"These routines are used to allocate a handle a new SQL-environment, to free an 
 environment handle when it is no longer needed, and to retrieve and set the value of 
 attributes associated with the SQL-environment.
  
 /* Allocate a handle for a new SQL-environment */ 
  
 short  SQLAllocEnv (
  
  
  long    *envHdl)    /* OUT: returned env handle */
  
 /* Free an environment handle previously allocated */ 
 short    SQLFreeEnv (
  
  
  long    envHdl)    /* IN:  environment handle */
  
 /* Obtain the value of a SQL-environment attribute */ 
  
 short  SQLGetEnvAttr(
  
  
  long    envHdl,      /* IN:  environment handle */
  
  
 long    AttrCode,    /* IN:  integer attribute code*/
  
  
 void    *rtnVal,     /* OUT: return value */
  
  
  long    bufLen,      /* IN:  length of rtnVal buffer */
  
  long    *strLen)     /* OUT: length of actual data */
  
 /* Set the value of a SQL-environment attribute */ 
  
 short  SQLSetEnvAttr(
  
  
  long    envHdl,     /* IN:  environment handle */
  
  
 long    AttrCode,   /* IN:  integer attribute code*/
  
  void    
 *attrVal,   /* IN:  new attribute value */
  
  long    
 *strLen)    /* IN:  length of data */",NA
SQL Connection Management Routines,"These routines are used to create, terminate, and manage a connection to a SQL-server. 
  
 They allocate and free the handles used to maintain connection status, setup and 
 terminate connections, manage the attributes associated with a connection, and obtain a 
 list of the SQL-servers available for connection.
  
 /* Allocate a handle for a new SQL-connection */ 
  
 short  SQLAllocConnect (
  
  
  long    envHdl,     /* IN:  environment handle */
  
  
  long    *connHdl)   /* OUT: returned connection handle */
  
 /* Free a connection handle previously allocated */ 
 short    SQLFreeConnect (
  
  
  long    connHdl)    /* IN:  connection handle */
  
 /* Initiate a connection to a SQL-server */ 
  
 short  SQLConnect(
  
  
  long    connHdl,     /* IN:  connection handle */
  
  
  char    *svrName,    /* IN:  name of target SQL-server */
  
  short    svrnamlen,  /* IN:  length of SQL-server name */
  
  char    *userName,   /* IN:  user name for connection */
  
 - 638 -",NA
SQL Statement Management Routines,"These routines are used to allocate and free the handle associated with a SQL 
 statement, to pass SQL statement text for execution, and to request preparation and 
 actual execution of the statement via the CLI.
  
 /* Allocate a handle to manage processing of SQL statement(s) */ 
 short  SQLAllocStmt (
  
  
  long    envHdl,    /* IN:  environment handle */
  
  
  long    *stmtHdl)  /* OUT: statement handle */
  
 /* Free a statement handle previously allocated */ 
  
 short  SQLFreeStmt (
  
  
  long    stmtHdl,   /* IN:  statement handle */
  
  
  long    option)    /* IN:  cursor & unbind options */
  
 - 639 -",NA
SQL Statement Execution Routines,"These routines are used to pass SQL statement text to the CLI and to request SQL 
 statement execution, either immediately or after being prepared. They also control the 
 execution of SQL transactions and the cancellation of currently operating statements.
  
 /* Pass SQL statement text and request its execution */ short  
 SQLExecDirect (
  
  
  long    stmtHdl,      /* IN:  statement handle */
  
  
 char    *stmttext,    /* IN:  SQL statement text */
  
  short   
 textlen)      /* IN:  statement text length */
  
 /* Prepare a SQL statement, passing it in SQL text form */ 
 short  SQLPrepare (
  
  
  long    stmtHdl,      /* IN:  statement handle */
  
  
 char    *stmttext,    /* IN:  SQL statement text */
  
  short   
 textlen)      /* IN:  statement text length */
  
 /* Execute a previously-prepared SQL statement */ 
 short    SQLExecute (
  
  
  long    stmtHdl)    /* IN:  statement handle */
  
 /* COMMIT or ROLLBACK a SQL transaction */ 
 short  SQLEndTran (
  
 - 640 -",NA
Query Results Processing Routines,"These routines are used to retrieve rows of query results and to specify the application 
 program data areas that are to receive the returned query results.
  
 /* Advance the cursor to the next row of query results */ 
 short  QLFetch (
  
  
  long    stmtHdl)    /* IN:  statement handle */
  
 /* Scroll the cursor up or down through the query results */ 
 short  SQLFetchScroll (
  
  
  long    stmtHdl,     /* IN:  statement handle */
  
 */
  
  short   fetchdir,    /* IN:  direction (first/next/prev)  
 long    offset)      /* IN:  offset (number of rows) */
  
 /* Get the data for a single column of query results */ 
 short  SQLGetData (
  
  
  long    stmtHdl,    /* IN:  statement handle */
  
 *
 / 
  
 *
 / 
  
 *
 /
  
 */
  
  short   colnr,      /* IN:  column number to be retrieved  
 short   tgttype,    /* IN:  data type to return to program  
 void    *value,     /* IN:  ptr to buffer for column data  
 long    buflen,     /* IN:  length of program buffer */ long    
 *lenind)    /* OUT: actual length and/or NULL ind 
  
 /* Close a cursor to end access to query results */ 
 short  SQLCloseCursor (
  
  
  long    stmtHdl)    /* IN:  statement handle */
  
 /* Establish a cursor name for an open cursor */ 
  
 short    SQLSetCursorName (
  
  
  long    stmtHdl,     /* IN:  statement handle */
  
  
 char    cursname,    /* IN:  name for cursor */
  
  
  short   namelen)     /* IN:  length of cursor name */
  
 /* Retrieve the name of an open cursor */ 
  
 short  SQLGetCursorName (
  
  
  long     stmtHdl,     /* IN:  statement handle */
  
  
  char     cursname,    /* OUT: buffer for returned name */
  
  short    buflen,      /* IN:  length of buffer */
  
  
  short    *namlen)     /* OUT: actual length of returned 
 name */
  
 - 641 -",NA
Query Results Description Routines,"These routines are used to obtain a description of the results of a query, including the 
 number of columns of query results, the data type, and other attributes of each column.
  
 /* Determine the number of result columns in a query */ 
 short  SQLNumResultCols (
  
  
  long    stmtHdl,      /* IN:  statement handle */
  
 */
  
  short   *colcount)    /* OUT: returned number of columns 
  
 /* Determine the characteristics of a column of query results */ 
 short  SQLDescribeCol (
  
  
  long    stmtHdl,    /* IN:  statement handle */
  
 *
 / 
  
 *
 / 
  
 *
 /
  
 */
  
  short   colnr,      /* IN:  number of column to describe  
 char    *colname,   /* OUT: name of query results column  
 short   buflen,     /* IN:  length of column name buffer  
 short   *namlen,    /* OUT: actual column name length */ 
 short   *coltype,   /* OUT: returned column data type code  
 short   *colsize,   /* OUT: returned column data length */
  
  short   *decdigits, /* OUT: returned # digits in column */ 
 short   *nullable)  /* OUT: can column have NULL values */
  
 /* Obtain detailed info about a column of query results */ 
 short  SQLColAttribute (
  
  
  long    stmtHdl,    /* IN:  statement handle */
  
 */ 
  
 */
  
 */ 
  
 */
  
  short   colnr,      /* IN:  number of column to describe  
 short   attrcode,   /* IN:  code of attribute to retrieve  
 char    *attrinfo,  /* OUT: buffer for attribute info */ 
 short   buflen,     /* IN:  length of col attribute buffer  
 short   *actlen)    /* OUT: actual attribute info length",NA
Query Results Descriptor Management Routines,"These routines are used to obtain a description of the results of a query using the CLI 
 descriptor mechanism, and to manipulate the descriptors to manage the return of query 
  
 - 642 -",NA
Deferred Dynamic Parameter Processing Routines,"These routines are used to process deferred parameters when their values are requested 
 by the CLI during execution of a SQL statement containing them.
  
 /* Get param-tag for next required dynamic parameter */ 
 short  SQLParamData (
  
 */ 
  
 */
  
  long    stmtHdl,    /* IN:  stmt handle w/ dynamic params  
 void    *prmtag)    /* OUT: buffer for rtn param-tag value 
  
 /* Obtain detailed info for an item described by a CLI descriptor 
 */ 
  
 short  SQLPutData (
  
 */
  
  long    stmtHdl,     /* IN:  stmt handle w/ dynamic params  
 void    *prmdata,    /* IN:  buffer with data for param */
  
  short   prmlenind)   /* IN:  param length or NULL ind */",NA
"Error, Status, and Diagnostic Routines","These routines are used to determine the reason for an error condition returned by the 
 CLI, to determine the number of rows affected by successful statement execution, and to 
 obtain detailed diagnostic information about error conditions.
  
 /* Retrieve error information associated with a previous CLI call 
 */ 
  
 short  SQLError (
  
  
  long    envHdl,     /* IN:  environment handle */
  
  
  long    connHdl,    /* IN:  connection handle */
  
  
  long    stmtHdl,    /* IN:  statement handle */
  
 */
  
  char    *sqlstate,  /* OUT: five-character SQLSTATE value  
 long    *nativeerr, /* OUT: returned native error code */
  
  char    *msgbuf,    /* OUT: buffer for err message text */
  
 */
  
  short    buflen,    /* IN:  length of err msg text buffer  
 short   *msglen)    /* OUT: returned actual msg length */
  
 /* Determine number of rows affected by previous SQL statement */ 
 short  SQLRowCount (
  
  
  long    stmtHdl,    /* IN:  statement handle */
  
  
  long    *rowcnt)    /* OUT: number of rows */
  
 /* Retrieve info from one of the CLI diagnostic error records */ 
 short  QLGetDiagRec (
  
  
  short   hdltype,    /* IN:  handle type code */
  
  
  long    inHdl,      /* IN:  CLI handle */
  
 - 644 -",NA
CLI Implementation Information Routines,"These routines return information about the specific CLI implementation, including the 
 CLI calls, statements, and data types that it supports.
  
 /* Retrieve info about capabilities of a CLI implementation */ 
 short  SQLGetInfo (
  
  
  long    connHdl,     /* IN:  connection handle */
  
  
  short   infotype,    /* IN:  type of info requested */
  
  void    *infoval,    /* OUT: buffer for retrieved info */
  
  short    buflen,     /* IN:  length of info buffer */
  
 */
  
  short   *infolen)    /* OUT: returned info actual length 
  
 /* Determine number of rows affected by previous SQL statement */ 
 short  SQLGetFunctions (
  
  
  long     connHdl,     /* IN:  connection handle */
  
  
  short    functid,     /* IN:  function id code */
  
 */
  
  short    *supported)  /* OUT: whether function supported 
  
 /* Determine information about supported data types */ 
  
 short  SQLGetTypeInfo (
  
  
  long     stmtHdl,     /* IN:  statement handle */
  
  
  short    datatype)    /* IN:  ALL TYPES or type requested */",NA
CLI Parameter Value Codes,"These codes are passed to or returned by the CLI as parameter values, to indicate 
 handle types, data types, statement types, etc.
  
 Code 
  
 Value
  
 - 645 -",NA
Appendix F:,NA,NA
 SQL Information Schema ,NA,NA
Standard,NA,NA
Overview,"This appendix describes the Information Schema views specified by the SQL2 standard. 
 These views must be supported by any database system claiming Intermediate-Level or 
 Full conformance to the standard; they are not required for Entry-Level conformance. The 
 views make a SQL2-compliant database self-describing. By querying them, a user can 
 determine relevant information about all of the database objects (schemas, tables, 
 columns, views, constraints, domains, character sets, etc.) accessible to him or her.
  
 Information about schemas, tables, and columns:
  
 SCHEMATA 
  
 Describes all schemas owned by the current user
  
 - 651 -",NA
SCHEMATA,NA,NA
 View,"The 
 SCHEMATA
  view contains one row for each schema that is owned by the current 
 user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CATALOG_NAME
  
 VARCHAR(
 len
 )
  
 Name of catalog containing this 
  
 schema
  
 SCHEMA_NAME
  
 VARCHAR(
 len
 )
  
 Name of schema described by 
  
 this row
  
 SCHEMA_OWNER 
  
 VARCHAR(
 len
 ) 
 Name of schema's creator
  
 DEFAULT_CHARACTER_SET_CATALOG VARCHAR(
 len
 ) 
  
 Catalog of default 
 character set 
  
  
 for this schema
  
 DEFAULT_CHARACTER_SET_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema of default character set 
  
 for this schema
  
 DEFAULT_CHARACTER_SET_NAME
  
 VARCHAR(
 len
 )
  
 Name of default character set for 
  
 this schema
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.",NA
TABLES,NA,NA
 View,"The 
 TABLES
  view contains one row for each table defined in the current catalog that is 
 accessible to the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description 
  
 Name of catalog containing this table definition 
 Name of schema containing this table definition 
 Name of the table 
  
  
 Type of table
 (BASE TABLE / VIEW / 
  
  
 GLOBAL TEMPORARY / LOCAL TEMPORARY)
  
 TABLE_CATALO
 G
  
 VARCHAR(
 len
 )
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 TABLE_TYPE
  
 VARCHAR(
 maxlen
 )
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
 by the SQL implementation.",NA
COLUMNS,NA,NA
 View,- 653 -,NA
VIEWS,NA,NA
 View,"The 
 VIEWS
  view contains one row for each view defined in the current catalog that is 
 accessible to the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 TABLE_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing this view 
  
 definition
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing this view 
  
 definition
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 Name of the view
  
 VIEW_DEFINITION
  
 VARCHAR(
 maxlen
 )
  
 Text of the SQL 
 SELECT statement defining 
  
 the view
  
 CHECK_OPTION
  
 VARCHAR(
 maxlen
 )
  
 Check option for this view (
 CASCADED / 
  
 IS_UPDATABLE
  
 VARCHAR(
 maxlen
 )
  
 LOCAL / NONE)
  
 Whether the view is updateable (
 YES / 
  
 NO)
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
 by the SQL implementation.",NA
VIEW_TABLE_USAGE,NA,NA
 View,"The 
 VIEW_TABLE_USAGE
  view contains one row for each table on which a view defined 
 in the current catalog by the current user depends. Its structure is shown in the following 
 table:
  
 - 655 -",NA
VIEW_COLUMN_USAGE,NA,NA
 View,"The 
 VIEW_COLUMN_USAGE
  view contains one row for each column on which a view 
 defined in the current catalog by the current user depends. Its structure is shown in the 
 following table:
  
 Column Name
  
 Data Type
  
 Description
  
 VIEW_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the view definition
  
 VIEW_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the view definition
  
 VIEW_NAME
  
 VARCHAR(
 len
 )
  
 Name of the view
  
 TABLE_CATALOG
  
 VARCHAR(
 len
 )
  
 Catalog containing the definition of the column on 
  
 which the view depends
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the definition of the column on 
  
 which the view depends
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 Name of the table containing the column on which 
  
 the view depends
  
 COLUMN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the column on which the view depends
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.",NA
TABLE_CONSTRAINTS,NA,NA
 View,"The 
 TABLE_CONSTRAINTS
  view contains one row for each table constraint defined for 
 tables in the current catalog owned by the current user. Its structure is shown in the 
 following table:
  
 Column Name
  
 Data Type
  
 Description
  
 - 656 -",NA
REFERENTIAL_CONSTRAINTS,NA,NA
 View,"The 
 REFERENTIAL_CONSTRAINTS
  view contains one row for each referential constraint 
 (foreign key / primary key relationship) defined for tables in the current catalog owned by 
 the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the 
  
 constraint definition
  
 CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the 
  
 constraint definition
  
 CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the constraint
  
 UNIQUE_CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the 
  
 unique or primary key constraint 
  
 definition for the ""parent"" table
  
 UNIQUE_CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the 
  
 unique or primary key constraint 
  
 definition for the ""parent"" table
  
 UNIQUE_CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the unique or primary key 
  
 constraint definition for the ""parent"" 
  
 table
  
 - 657 -",NA
CHECK_CONSTRAINTS,NA,NA
 View,"The 
 CHECK_CONSTRAINTS
  view contains one row for each check constraint (check 
 constraint, domain check constraint, or assertion definition) defined in the current catalog 
 that is owned by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the constraint 
  
 definition
  
 CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the constraint 
  
 definition
  
 CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the constraint
  
 CHECK_CLAUSE
  
 VARCHAR(
 maxlen
 )
  
 Text of the SQL search condition that 
  
 defines the check constraint
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
 by the SQL implementation.",NA
KEY_COLUMN_USAGE,NA,NA
 View,"The 
 KEY_COLUMN_USAGE
  view contains one row for each column that participates in a 
 key defined in the current catalog by the current user. Its structure is shown in the 
 following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the key 
  
 constraint definition
  
 - 658 -",NA
ASSERTIONS,NA,NA
 View,"The 
 ASSERTIONS
  view contains one row for each assertion defined in the current catalog 
 that are owned by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the assertion 
  
 definition
  
 CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the 
  
 assertion definition
  
 CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the assertion
  
 IS_DEFERRABLE
  
 VARCHAR(
 maxlen
 )
  
 Is assertion deferrable? (
 YES / NO)
  
 INITIALLY_DEFERRE
 D
  
 VARCHAR(
 maxlen
 )
  
 Is assertion initially deferred? (
 YES / 
  
 NO)
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
 by the SQL implementation.",NA
CONSTRAINT_TABLE_USAGE,NA,NA
 View,"The 
 CONSTRAINT_TABLE_USAGE
  view contains one row for each table used by a 
 constraint (referential constraint, unique constraint, check constraint, or assertion) 
 defined in the current catalog by the current user. Its structure is shown in the following 
 table:
  
 Column Name
  
 Data Type
  
 Description
  
 - 659 -",NA
CONSTRAINT_COLUMN_USAGE,NA,NA
 View,"The 
 CONSTRAINT_COLUMN_USAGE
  view contains one row for each column used by a 
 constraint (referential constraint, unique constraint, check constraint or assertion) defined 
 in the current catalog by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 TABLE_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the column 
  
 definition
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the column 
  
 definition
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 Name of the table containing the column
  
 COLUMN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the column
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Catalog containing the definition of the 
  
 constraint
  
 CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the definition of the 
  
 constraint
  
 CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the constraint
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.",NA
TABLE_PRIVILEGES,NA,NA
 View,"The 
 TABLE_PRIVILEGES
  view contains one row for each privilege on tables defined in 
 the current catalog that has been granted to the current user, been granted to all users, 
 or has been granted by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 - 660 -",NA
COLUMN_PRIVILEGES,NA,NA
 View,"The 
 COLUMN_PRIVILEGES
  view contains one row for each privilege on columns defined 
 in the current catalog that has been granted to the current user, been granted to all users, 
 or has been granted by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 GRANTOR
  
 VARCHAR(
 len
 )
  
 Authorization-id of user granting the privilege
  
 GRANTEE
  
 VARCHAR(
 len
 )
  
 Authorization-id of user being granted the 
  
 privilege
  
 TABLE_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the table 
  
 definition containing this column
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the table 
  
 definition containing this column
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 Name of the table containing this column
  
 COLUMN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the column
  
 PRIVILEGE_TYPE
  
 VARCHAR(
 maxlen
 )
  
 Type of privilege
 (SELECT / INSERT / 
  
 IS_GRANTABLE
  
 VARCHAR(
 maxlen
 )
  
 DELETE / UPDATE / REFERENCES)
  
 Is privilege granted
 WITH GRANT OPTION? 
  
 (YES / NO)
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
  
 - 661 -",NA
USAGE_PRIVILEGES,NA,NA
 View,"The 
 USAGE_PRIVILEGES
  view contains one row for each privilege on objects defined in 
 the current catalog that has been granted to the current user, been granted to all users, or 
 has been granted by the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 GRANTOR
  
 VARCHAR(
 len
 )
  
 Authorization-id of user granting the privilege
  
 GRANTEE
  
 VARCHAR(
 len
 )
  
 Authorization-id of user being granted the 
  
 privilege
  
 OBJECT_CATALO
 G
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the object definition 
 Name of schema containing the object definition 
 Name of the object 
  
  
 Type of object
 (DOMAIN / CHARACTER SET 
  
 / COLLATION / TRANSLATION) 
  
  
 Type of privilege (always the literal 
 USAGE) 
  
 Is 
 privilege granted
 WITH GRANT OPTION? 
  
 (YES / NO)
  
 OBJECT_SCHEMA
  
 VARCHAR(
 len
 )
  
 OBJECT_NAME
  
 VARCHAR(
 len
 )
  
 OBJECT_TYPE
  
 VARCHAR(
 maxlen
 )
  
 PRIVILEGE_TYPE
  
 VARCHAR(
 maxlen
 )
  
 IS_GRANTABLE
  
 VARCHAR(
 maxlen
 )
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.
  
 VARCHAR(
 maxlen
 )
  is a 
 VARCHAR
  data type with the largest maximum length permitted 
 by the SQL implementation.",NA
DOMAINS,NA,NA
 View,"The 
 DOMAINS
  view contains one row for each domain defined in the current catalog that 
 is accessible to the current user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 DOMAIN_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing this 
  
 domain definition
  
 DOMAIN_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing this 
  
 domain definition
  
 DOMAIN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the domain
  
 DATA_TYPE
  
 VARCHAR(
 maxlen
 )
  
 SQL2 data type on which the 
  
 domain definition is based (text 
  
 representation)
  
 CHARACTER_MAXIMUM_LENGTH
  
 INTEGER > 0
  
 Maximum length, in characters, for 
  
 variable-length character types
  
 - 662 -",NA
DOMAIN_CONSTRAINTS,NA,NA
 View,"The 
 DOMAIN_CONSTRAINTS
  view contains one row for each domain constraint for a 
 domain defined in the current catalog that is accessible to the current user. Its structure is 
 shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CONSTRAINT_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing this constraint 
  
 definition
  
 CONSTRAINT_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing this constraint 
  
 definition
  
 CONSTRAINT_NAME
  
 VARCHAR(
 len
 )
  
 Name of the constraint
  
 - 663 -",NA
DOMAIN_COLUMN_USAGE,NA,NA
 View,"The 
 DOMAIN_COLUMN_USAGE
  view contains one row for each column used by a domain 
 defined in the current catalog by the current user. Its structure is shown in the following 
 table:
  
 Column Name
  
 Data Type
  
 Description
  
 DOMAIN_CATALOG
  
 VARCHAR(
 len
 )
  
 Catalog containing the definition of the domain
  
 DOMAIN_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the definition of the domain
  
 DOMAIN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the domain
  
 TABLE_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the column definition
  
 TABLE_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the column definition
  
 TABLE_NAME
  
 VARCHAR(
 len
 )
  
 Name of the table containing the column
  
 COLUMN_NAME
  
 VARCHAR(
 len
 )
  
 Name of the column
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.",NA
CHARACTER_SETS ,NA,NA
View,"The 
 CHARACTER_SETS
  view contains one row for each character set defined in the 
 current catalog that is accessible to the current user. Its structure is shown in the 
 following table:
  
 Column Name
  
 Data Type
  
 Description
  
 CHARACTER_SET_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the 
  
 character set definition
  
 - 664 -",NA
COLLATIONS ,NA,NA
View,"The 
 COLLATIONS
  view contains one row for each collation (sorting sequence) defined in 
 the current catalog that is accessible to the current user. Its structure is shown in the 
 following table:
  
 Column Name
  
 Data Type
  
 Description
  
 COLLATION_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the collation 
  
 definition
  
 COLLATION_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the collation 
  
 definition
  
 COLLATION_NAME 
  
 VARCHAR(
 len
 ) 
  
 Name of the collation
  
 CHARACTER_SET_CATALOG VARCHAR(
 len
 ) 
  
 Catalog containing the character set 
  
  
 definition on which the collating 
  
  
 sequence is defined
  
 CHARACTER_SET_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the character set 
  
 definition on which the collating 
  
 sequence is defined
  
 CHARACTER_SET_NAME
  
 VARCHAR(
 len
 )
  
 Name of the character set on which the 
  
 collating sequence is defined
  
 PAD_ATTRIBUTE
  
 VARCHAR(
 maxlen
 )
  
 Character padding (
 PAD SPACE / NO 
  
 PAD)
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
  
 - 665 -",NA
TRANSLATIONS ,NA,NA
View,"The 
 TRANSLATIONS
  view contains one row for each translation (conversion from one 
 character set to another) defined in the current catalog that is accessible to the current 
 user. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 TRANSLATION_CATALOG
  
 VARCHAR(
 len
 )
  
 Name of catalog containing the 
  
 translation definition
  
 TRANSLATION_SCHEMA
  
 VARCHAR(
 len
 )
  
 Name of schema containing the 
  
 translation definition
  
 TRANSLATION_NAME 
  
 VARCHAR(
 len
 ) 
  
 Name of the translation
  
 SOURCE_CHARACTER_SET_CATALOG VARCHAR(
 len
 ) 
  
 Catalog containing the 
 character 
  
  
 set definition from which the 
  
  
 translation occurs
  
 SOURCE_CHARACTER_SET_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the character 
  
 set definition from which the 
  
 translation occurs
  
 CHARACTER_SET_NAME
  
 VARCHAR(
 len
 )
  
 Name of the character set from 
  
 which translation occurs
  
 TARGET_CHARACTER_SET_CATALOG
  
 VARCHAR(
 len
 )
  
 Catalog containing the character 
  
 set definition to which the 
  
 translation occurs
  
 TARGET_CHARACTER_SET_SCHEMA
  
 VARCHAR(
 len
 )
  
 Schema containing the character 
  
 set definition to which the 
  
 translation occurs
  
 TARGET_CHARACTER_SET_NAME
  
 VARCHAR(
 len
 )
  
 Name of the character set to which 
  
 translation occurs
  
 VARCHAR(
 len
 )
  is the data type for SQL identifiers; 
 len
  is the maximum length defined 
 by the SQL implementation.",NA
SQL_LANGUAGES ,NA,NA
View,"The 
 SQL_LANGUAGES
  view contains one row for each ANSI-standard language 
 supported by this SQL implementation. Its structure is shown in the following table:
  
 Column Name
  
 Data Type
  
 Description
  
 SQL_LANGUAGE_SOURECE
  
 VARCHAR(
 maxlen
 )
  
 Text identifying source of 
  
 language standard
 (e.g., ISO 
  
 9075)
  
 - 666 -",NA
Appendix G: ,NA,NA
CD-ROM Installation Guide ,NA,NA
Overview ,"The Bonus CD that accompanies this book includes full-function Windows 95/NT 
 versions of all five of the leading brands of SQL-based DBMS:
  
 •Microsoft SQL Server 7
  
 •Oracle8
  
 •Informix
  
 •Sybase
  
 •IBM DB2 
  
 These are 
 not 
 incomplete ""demo"" versions of the DBMS products. Rather, they are full-
 capability 
 evaluation 
 versions* of the latest software from these five leading database 
 vendors, allowing you to learn SQL using a live DBMS, test out and compare each 
 product, and find a SQL DBMS that best suits your specific needs.
  
 The CD also includes data files that you can use to populate the DBMS products with the 
 five tables of the sample database so that you can easily run the example queries in the 
 book. The files reside in the root directory of the CD and are named:
  
 •CUSTOMERS.DAT
  
 •OFFICES.DAT
  
 •ORDERS.DAT
  
 •PRODUCTS.DAT
  
 - 667 -",NA
Installing the SQL DBMS Software ,"When you insert the Bonus CD into your CD-ROM drive, Windows automatically 
 launches the DBMS installation program on the CD. The program prompts you to accept 
 the Osborne/McGraw-Hill license agreement and then allows you to select a DBMS for 
 installation as shown here:
  
  
 The sections that follow contain detailed instructions for installing each brand of SQL 
 DBMS.",NA
Microsoft SQL Server 7 ,"The following table lists details and facts about the product and the company.
  
 Product name 
  
 SQL Server 7.0 
  
 First shipment 
  
 1988 
  
 Platform 
  
 Windows 95/NT 
  
 DBMS version 
  
 7.0 
  
 Installation program 
  
 \SQLSERVER7\AUTORUN.EXE 
 Software limitations 
  
 Expires 120 days after installation 
 Vendor 
  
 Microsoft Corporation
  
 - 668 -",NA
Hardware and Software Requirements,"Microsoft SQL Server requires the following hardware and software:
  
 Category 
  
 Requirements
  
 Computer 
  
 Intel or compatible (Pentium 166MHz or higher, Pentium PRO, or 
  
 Pentium II)
  
 Memory (RAM) 
  
 32MB minimum
  
 Hard disk space
 1
  
 SQL Server 
  
 180MB (full), 170MB (typical), 65MB (minimum), 90MB 
  
 (management tools only)
  
 OLAP 
  
 Services
 2
  
 English Query
  
 Operating system
  
 Internet software
  
 Network software
  
 50MB 
  
 12MB
  
 Windows 95/98 or Windows NT Workstation 4.0 or later
 3
  with 
 SP4 or later
 3 
  
 Microsoft Internet Explorer version 4.01 with SP1 or later
 4 
 Microsoft Windows NT or Windows 95/98 built-in network 
  
 software. Additional network software is not required unless you 
  
 are using Banyan VINES or AppleTalk ADSP. Novell NetWare 
  
 client support is provided by NWLink.
  
 Clients supported
  
 Windows 95/98, Windows NT Workstation, Unix
 5,4
 Macintosh
 5,4 
 and OS/2
 5
  
 1These figures are the maximum hard disk space required. Setup installs a number of 
  
 components that can be shared by other applications and may already exist on the 
  
 computer.
  
 2 Enter the 10-digit CD Key ""111-1111111"" when installing OLAP Services.
  
 3 Windows NT SP4 is included on the CD.
  
 4 Internet Explorer is required for Microsoft Management Console (MMC) and HTML 
  
 Help. A minimal installation is sufficient, and your default browser need not be Internet 
  
 Explorer. Internet Explorer 4.01 with SP1 is included on the CD.
  
 5 Requires ODBC client software from a third-party vendor.",NA
SQL Server Services User Accounts,"Under Windows NT, you must assign a user account for each of the Microsoft SQL 
  
 - 669 -",NA
SQL Server 7.0 Installation,"Perform the following steps to install SQL Server 7.0:
  
 1.Under Windows NT, log on to the system as a member of the Administrators group.
  
 2. If you are installing SQL Server 7.0 on the same computer alongside SQL Server 6.x, 
 back up your Microsoft SQL Server 6.x installation, and do 
 not 
 install SQL Server 7.0 in 
 the same directory as SQL Server 6.x.
  
 3.If necessary, shut down all services dependent on SQL Server. This includes any 
 service that is using ODBC, such as Microsoft Internet Information Services (IIS).
  
 4.If necessary, shut down Microsoft Windows NT Event Viewer and REGEDT32.EXE.
  
 5.Insert the Bonus CD into your CD-ROM drive.
  
 6. Windows automatically launches the DBMS installation program on the CD. If the 
 installation program does not launch automatically, double-click
  SQLINSTALL.EXE
  in the 
 root directory of the CD to launch it manually.
  
 7. The installation program displays a dialog prompting you to accept the 
  
 Osborne/McGraw-Hill license agreement. Indicate your acceptance of the agreement, and 
 click 
 Next
 .
  
 8. The installation program displays the DBMS selection dialog. Select 
 Microsoft SQL 
 Server 7 
 from the list of DBMS choices, and click 
 Next
 . This invokes the SQL Server 
 Autorun application as shown here:
  
  
 9.Autorun displays the entry screen. Click 
 Install SQL Server 7.0 Components
 .
  
 10. Autorun displays the next set of choices. Click 
 Database Server – Standard Edition 
 or Database Server – Desktop Edition
 . Under Windows 95/98 or Windows NT 
 Workstation, you must select 
 Desktop Edition
 .
  
 11.Under Windows NT, the Setup Install Method dialog appears. Select
  Local Install
 , 
  
 - 670 -",NA
Starting SQL Server 7.0,"After you install SQL Server for the first time, you must reboot your computer before you 
 can begin to use the software.
  
 Under Windows 95, after you reboot, double-click on the 
 Service Manager 
 icon in the 
 Windows system tray, and then click on the green 
 Start/Continue 
 icon to start the 
 MSSQLServer service. Also check the Auto
 -
 startservicewhenOSstartsbox so that SQL 
 Server is started automatically each time you subsequently reboot your computer.
  
 Under Windows NT, SQL server is started automatically after you reboot and each time 
 you start your computer after that.",NA
Stopping SQL Server 7.0,"Perform the following steps to shut down SQL Server 7.0:
  
 1.Stop all SQL Server services (if any are running). To do this…
  
 Under Windows 95:
  
 •Double-click on the 
 Service Manager 
 icon in the Windows system tray.
  
 - 671 -",NA
Uninstalling SQL Server 7.0 ,"Perform the following steps to uninstall SQL Server 7.0: 
  
 1.Stop all SQL Server services, and close all SQL Server applications.
  
 2.Choose 
 Start | Programs | Microsoft SQL Server 7.0 | Uninstall SQL Server 7.0 
 from the Windows taskbar.
  
 3.The Confirm File Deletion dialog appears and asks you to confirm that you want to 
 uninstall the program. Click 
 Yes
  to continue.
  
 4. The Remove Shared Files? dialog appears and asks you to confirm that you want to 
 remove shared files that are no longer being used. Click 
 Yes to All 
 to remove these 
 files, and click 
 Yes 
 when the second confirmation dialog appears. The Uninstall program 
 removes all SQL Server 7.0 programs from your system.",NA
Oracle8 ,"The following table provides a variety of miscellaneous details about the product and the 
 company: 
  
 Product name 
 Oracle8 Personal Edition 
  
 First shipment 
 1979 
  
 Platform 
  
 Windows 95/NT 
  
 8.0.4 
 DBMS version 
  
 Installation program 
 \TEMP\ORACLE8\SETUP.EXE (after decompression) 
  
 Software limitations 
 None (subject to 30-day trial license) 
  
 Vendor 
 Oracle Corporation
  
 - 672 -",NA
Products Included on the CD,"You'll find the following products on the CD-ROM included with this book:
  
 Product 
  
 Version
  
 Assistant Common Files 
  
 1.0.1.0.0
  
 Java Runtime Environment 
  
 1.1.1.0.0
  
 Oracle Call Interface 
  
 8.0.4.0.0
  
 Oracle Data Migration Assistant 
  
 8.0.4.0.0
  
 Oracle Database Assistant (for Windows NT only) 
  
 2.0.0.0.1
  
 Oracle Documentation 
  
 8.0.4.0.0
  
 Oracle Installer 
  
 3.3.0.1.3
  
 Oracle Migration Assistant for Microsoft Access 
  
 8.0.4.0.0
  
 Oracle Net8 Add-on 
  
 8.0.4.0.0
  
 Oracle Net8 Assistant 
  
 8.0.4.0.3
  
 Oracle Net8 Client 
  
 8.0.4.0.2c
  
 Oracle LU6.2 Protocol Adapter (for Windows NT only) 
  
 8.0.4.0.0
  
 Oracle Named Pipes Protocol Adapter 
  
 8.0.4.0.0
  
 Oracle SPX Protocol Adapter 
  
 8.0.4.0.0
  
 Oracle TCP/IP Protocol Adapter 
  
 8.0.4.0.2
  
 SQL*Net Add-on (Patch) (for Windows 95 only) 
  
 2.2.2.1.1
  
 Oracle LU6.2 Protocol Adapter (for Windows NT only) 
  
 2.3.4.0.0
  
 Oracle Named Pipes Protocol Adapter 
  
 2.3.4.0.0
  
 Oracle SPX Protocol Adapter 
  
 2.3.4.0.0
  
 Oracle TCP/IP Protocol Adapter 
  
 2.3.4.0.0
  
 SQL*Net Add-on 
  
 2.3.4.0.2
  
 - 673 -",NA
Hardware and Software Requirements,"Oracle8 Personal Edition requires the following hardware and software:
  
 Category 
  
 Requirements
  
 Computer 
  
 Pentium-based personal computer
  
 Memory (RAM) 
  
 32MB for standard starter database
  
 48MB for replication starter database
  
 Hard disk space 
  
 134MB (Application Developer with standard database)
  
 149MB (Application Developer with replication database)
 1
  
 100MB (Runtime with standard database)
  
 115MB (Runtime with replication database)
  
 163MB (Custom installation)
 2
  
 Operating system 
  
 Windows 95 or Windows NT 4.0
  
 - 674 -",NA
Products Available for Installation,"When you run the Oracle installation program, adialog box displays several product 
  
 - 675 -",NA
Oracle8 Personal Edition Installation,"Perform the following steps to install Oracle8 Personal Edition:
  
 1.Under Windows NT, log on to the system as a member of the Administrators group.
  
 2.Insert the Bonus CD into your CD-ROM drive.
  
 3. Windows automatically launches the DBMS installation program on the CD. If the 
 installation program does not launch automatically, double-click 
 SQLINSTALL.EXE
  in the 
 root directory of the CD to launch it manually.
  
 4. The installation program displays a dialog prompting you to accept the 
  
 Osborne/McGraw-Hill license agreement. Indicate your acceptance of the agreement, and 
 click 
 Next
 .
  
 5.The installation program displays the DBMS selection dialog. Select 
 Oracle8 
 Personal Edition
  from the list of DBMS choices, and click 
 Next
 .
  
 6. Due to the size of the Oracle distribution, it must first be decompressed into a 
 temporary directory on your hard disk before installation begins. The DBMS installation 
 program prompts you for the name of the temporary directory. Verify the name of the 
 directory, and click 
 Next
 .
  
 7. The Oracle files are decompressed onto the temporary directory. This may take ten or 
 fifteen minutes. When decompression is complete, the installation program invokes the 
 Oracle Setup application as shown here:
  
  
 8. Setup displays the Oracle Installation Settings dialog and indicates the default values 
 for Company Name, Oracle Home Name, Oracle Home Location, and Oracle Home 
 Language. Accept the default values or specify different values, and click 
 OK
 .
  
 9.The Select Installation Options dialog appears. Select 
 Application Developer 
 (Complete)
 , and click 
 OK
 .
  
 10.The Select a Starter Database Configuration dialog appears. Select 
 Standard
 ,and 
  
 click 
 OK
 .
  
 11. The Installing Oracle Documentation dialog appears. Select 
 Hard Drive 
 if you would 
 like to install the Oracle HTML documentation on your hard drive. Click 
 OK
  to 
 continue.
  
 - 677 -",NA
Starting Oracle8 Personal Edition,"After you install Oracle8 Personal Edition, you must start Oracle8 before you can begin to 
 use the software.
  
 Under Windows 95, choose 
 Start | Programs | Oracle8 Personal Edition | Start 
 Database
  from the Windows taskbar. Rebooting is not necessary.
  
 Under Windows NT, Oracle8 is started automatically after you reboot the machine and 
 each time you start your computer after that.",NA
Stopping Oracle8 Personal Edition,"Perform the following steps to shut down Oracle8 Personal Edition:
  
 1.Choose Start | Programs | Oracle8 Personal Edition | Stop Database from the 
 Windows taskbar.
  
 2.Under Windows NT, stop all Oracle8 services (if any are running). To do this:
  
 •Choose Start | Settings | Control Panel from the Windows taskbar.
  
 •Double-click on the 
 Services 
 application.
  
 •Click on the 
 Stop 
 icon to shut down any Oracle service that has the status Started.
  
 •Click 
 Close 
 to exit theServices application.",NA
Uninstalling Oracle8 Personal Edition,"Perform the following steps to uninstall Oracle8 Personal Edition:
  
 1.Stop all Oracle services and close all Oracle applications.
  
 2.Choose 
 Start | Programs | Oracle for Windows 
 XX
  | Oracle Installer
  from the 
 Windows taskbar, where 
 XX
  is either 95 or NT.
  
 3.The Software Asset Manager dialog appears. Select the product(s) you want to 
 remove from the Installed Products list on the right side, and click 
 Remove
 .
  
 4.The Confirmation dialog appears. Click 
 Yes 
 to continue.
  
 5.The Dependencies dialog appears warning you that other products are dependent on 
 Required Support Files. Click 
 Yes 
 to continue.
  
 6.The Remove Database dialog appears asking if you want to remove the Oracle 
 database. Click 
 Yes 
 to continue.
  
 7.The Database Files dialog appears informing you that the additional databases you 
 created will not be removed automatically by deinstallation. Click 
 OK 
 to continue. 
  
 - 678 -",NA
Informix,"The following table provides a variety of miscellaneous details about the product and the 
  
 company:
  
 Product name 
  
 Informix Dynamic Server Personal Edition
  
 First shipment 
  
 1981
  
 Platform 
  
 Windows 95/NT
  
 DBMS version 
  
 7.22.TC2
  
 Installation program 
  
 \INFORMIX\SETUP.EXE
  
 License information
  
 Serial number 
  
 AAC#A524494
  
 Key 
  
 BDQGIP
  
 Software limitations 
  
 None
  
 Vendor 
  
 Informix Software, Inc.
  
 Founded 
  
 1980
  
 Annual sales 
  
 $700 million",NA
Hardware and Software Requirements,"Informix Personal Edition requires the following hardware and software:
  
 Category Requirements
  
 Computer Pentium-based personal computer
  
 Memory (RAM) 16MB required, 32MB recommended
  
 Hard disk space 72MB
  
 Operating system Windows 95 or Windows NT 4.0 with SP3 or earlier
  
 Network software TCP/IP (Microsoft version)
  
 User account Under Windows NT, you must be a member of the Windows 
  
 Administrators group to install or upgrade Informix Personal 
  
 Edition and the Administration tools on your system. Use the 
  
 - 679 -",NA
Informix User Accounts,"Under Windows NT, the installation program creates a user account called informix and 
 an administrative group called Informix-Admin. Users in the Informix-Admin group can 
 administer, back up and restore, modify tables, and modify dbspaces for the database 
 server.
  
 The installation program automatically assigns the current user and user informix 
 accounts to the Informix-Admin group. Informix Personal Edition runs under the user 
 informix account. The user informix owns and is the default user for this database server.",NA
Informix Personal Edition Installation,"Perform the following steps to install Informix Personal Edition:
  
 1.Under Windows NT, log on to the system as a member of the Administrators group.
  
 2.Insert the Bonus CD into your CD-ROM drive.
  
 3. Windows automatically launches the DBMS installation program on the CD. If the 
 installation program does not launch automatically, double-click 
 SQLINSTALL.EXE 
 in the 
 root directory of the CD to launch it manually.
  
 4. The installation program displays a dialog prompting you to accept the 
  
 Osborne/McGraw-Hill license agreement. Indicate your acceptance of the agreement, and 
 click 
 Next
 .
  
 5.The installation program displays the DBMS selection dialog. Select 
 Informix 
 Dynamic Server Personal Edition
  from the list of DBMS choices, and click 
 Next
 .
  
 6. The installation program displays a dialog prompting you to accept the Informix 
 Software license agreement. Indicate your acceptance of the agreement, and click 
 Next
 . This invokes the Informix Setup application as shown here:
  
  
 7.Setup displays the Welcome dialog. Click 
 Next
  to continue.
  
 8.The Select Installation Preference dialog appears. Select 
 Express Installation
 ,and 
 click 
 Next
 .
  
 - 680 -",NA
Starting Informix Personal Edition,"After you install Informix Personal Edition, you must reboot your computer before you can 
 begin to use the software. After you reboot, log on as user informix to the Informix DBMS 
 (under Windows 95) or to the system (under Windows NT), and choose 
 Start | Programs 
 | Informix Dynamic Server - Personal Edition | IDS-PE Control Panel 
 from the 
 Windows taskbar. Informix Personal Edition is initialized, and the IDS-PE Control Panel 
 appears. Click 
 OK 
 to continue and 
 Hide 
 to minimize the IDS-PE control panel.",NA
Stopping Informix Personal Edition,"Perform the following steps to shut down Informix Personal Edition:
  
 1.Log on as user informix to the Informix DBMS (under Windows 95) or to the system 
 (under Windows NT).
  
 2.Maximize the IDS-PE control panel. To do so, either:
  
 •Double-click on 
 IDS-PE control panel
  on the Windows taskbar, or
  
 •Choose 
 Start | Programs | Informix Dynamic Server - Personal Edition | IDS-PE 
 Control Panel
  from the Windows taskbar.
  
 3.Click 
 Shut Down
 , and then click 
 OK 
 to confirm.
  
 - 681 -",NA
Troubleshooting Installation Problems ,"Installation problems that you might encounter with the Informix Personal Edition 
 installation are listed here: 
  
 Problem 
  
 Solution
  
 You install Informix Personal Edition under 
 Windows NT and the installation program 
 displays the following error: Install failed to 
  
 Make sure that you are a local 
  
 Administrator on your Windows NT 
 computer when you run the installation 
  
 add OnLine service 
  
 program. You must also be a Windows 
  
 NT administrator. To assign yourself 
  
 Administrator privileges, use the Windows 
  
 User Manager.
  
 You attempt to install Informix Personal 
 Edition. The IDS-PE control panel and the 
 online.log display the following error 
  
 Make sure that your TCP/IP setup is 
  
 correct. Try to ping machine_name where 
 machine_name is the name of your 
  
 message: -930 Cannot connect to 
  
 computer.
  
 database server servername
  
 When you install Informix Personal Edition, 
 you are unable to install the product in a 
 directory such as: C:\Program 
  
 Your destination directory contains a 
  
 space. Your destination directory name 
 should only contain letters, numbers, and 
  
 Files\Informix. 
  
 underscores.",NA
Uninstalling Informix Personal Edition ,"Perform the following steps to uninstall Informix Personal Edition: 
 1.Log on to the system as user informix.
  
 2.Stop all Informix Personal Edition services, and close all 
 Informix Personal Edition 
 applications.
  
 3.Choose 
 Start | Programs | Informix Dynamic Server - Personal Edition | 
 Uninstall
  from the Windows taskbar.
  
 4.Select the items to uninstall, and click 
 OK
 .
  
 5.The program asks you to confirm that you want to uninstall the specified items. Click 
 Yes
  to continue. Uninstall removes the specified Informix items from your system.",NA
Sybase,- 682 -,NA
Hardware and Software Requirements,"Sybase Adaptive Server Anywhere requires the following hardware and software:
  
 Category 
  
 Requirements
  
 Computer 
  
 Pentium-based personal computer
  
 Memory (RAM) 
  
 16MB required, 32MB recommended. Adaptive Server Anywhere 
  
 can run in as little as 3MB of memory. If you use Java in the 
  
 database, Adaptive Server Anywhere requires an additional 8MB 
  
 of memory. Your computer must have this much memory in 
  
 addition to the requirements for the operating system.
  
 Hard disk space 
  
 40MB
  
 Operating system 
  
 Windows 95 or Windows NT 4.0
  
 Network software 
  
 If you are running a Sybase Adaptive Server Anywhere network 
  
 server, you must have appropriate networking software installed 
  
 and running. The Sybase Adaptive Server Anywhere network 
  
 server is available for Windows 95, Windows NT, Novell 
  
 NetWare, OS/2, and Unix operating systems. It is not available 
  
 for Windows 3.x. Sybase Adaptive Server Anywhere supports the 
  
 following network protocols: TCP/IP, IPX (not Unix operating 
  
 systems), and NetBIOS (not Unix operating systems)",NA
Sybase Adaptive Server Anywhere Installation,"Perform the following steps to install Sybase Adaptive Server Anywhere:
  
 1.Insert the Bonus CD into your CD-ROM drive.
  
 2.Windows automatically launches the DBMS installation program on the CD. If the 
  
 - 683 -",NA
Starting Sybase Adaptive Server Anywhere,"After you install Sybase Adaptive Server Anywhere, choose
  Start | Programs | Sybase | 
 Adaptive Server Anywhere 6.0 | Personal Server Sample
  from the Windows taskbar. 
  
 This starts a personal server running the sample database. The server displays as an 
 icon in the Windows system tray at the opposite end of the taskbar from the Start button.",NA
Stopping Sybase Adaptive Server Anywhere,"To shut down the Sybase Adaptive Server Anywhere server (if it is running), double-click 
 on the 
 Server 
 icon in the Windows system tray, and click 
 Shutdown
 . The window closes, 
 and the Sybase server stops.
  
 - 684 -",NA
Uninstalling Sybase Adaptive Server Anywhere,"Perform the following steps to uninstall Sybase Adaptive Server Anywhere:
  
 1.Close all Sybase Adaptive Server Anywhere applications.
  
 2.Choose Start | Settings | Control Panel from the Windows taskbar.
  
 3.Double-click on the 
 Add/Remove Programs 
 application.
  
 4.Select Adaptive Server Anywhere 6.0 from the list, and click Add/Remove.
  
 5.Windows asks you to confirm that you want to uninstall the program. Click 
 Yes
  to 
  
 continue.
  
 6. The Remove Shared Files? dialog appears and asks you to confirm that you want to 
  
 remove shared files that are no longer being used. Click 
 Yes to All 
 to remove these 
  
 files, and click 
 Yes 
 when the second confirmation dialog appears. Windows removes all 
  
 Sybase Adaptive Server Anywhere programs from your system.",NA
IBM DB2,"The following provides a variety of miscellaneous details about the product and the 
  
 company:
  
 Product name 
 DB2 Universal Database Personal Edition
  
 First shipment 
 1985
  
 Platform 
 Windows 95/NT
  
 DBMS version 
 5.2
  
 Installation program 
 \IBMDB2\DB2\WINNT95\EN\DB2INST.EXE
  
 Software limitations 
 Expires 60 days after installation
  
 Vendor 
 IBM Corporation
  
 Founded 
 1911
  
 Annual sales 
 $80.3 billion",NA
Hardware and Software Requirements,"IBM DB2 Personal Edition requires the following hardware and software:
  
 Category 
  
 Requirements
  
 Computer 
  
 Pentium-based personal computer
  
 Memory (RAM) 
  
 32MB—the minimum required to accommodate an average 
  
 size database while using the full set of graphical tools. The 
  
 - 685 -",NA
DB2 User Accounts,"During installation, you will be asked to provide a user name and password that will be 
 used by the Administration Server to log on to the system and to start itself as a service.
  
 Under Windows 95, the user name must simply be a valid DB2 user name (eight 
 characters or less and compliant with DB2's naming rules).
  
 Under Windows NT, the user name must belong to the Administrators group and also be a 
 valid DB2 user name, or have the ""Act as part of the operating system"" advanced user 
 right. If this user name does not comply with DB2's naming rules but has the Act as part of 
 the operating system advanced user right, the Setup program will create the user name 
 db2admin to perform the installation. This user name will be removed from the system 
 when the installation is complete unless it will be used by the Administration Server.
  
 By default, the Setup program will fill the Username, Password, and Confirm Password 
 fields with db2admin. You can accept these default values or provide your own. Setup will 
 check to see if the user name specified for the Administration Server exists. If it does not 
 exist, it will be created. If it does exist, Setup will verify that the user name is a member of 
 the Administrators group and also that the password is valid, provided that the user name 
 used to install DB2 has the Act as part of the operating system advanced user right.
  
 When Setup creates the db2admin user name, it also makes it a member of the 
 Administrators group. If you did not change the default password for this user name, you 
 should change its password using the User Manager function of Administration Tools; 
 you should also change the password for the DB2-DB2DAS00 service to match the new 
 password that you specified for the db2admin user name.",NA
DB2 Personal Edition Installation,"Perform the following steps to install DB2 Universal Database Personal Edition:
  
 1.Under Windows NT, log on to the system as a user that meets the requirements for 
  
 - 686 -",NA
Starting DB2 Personal Edition,"After you install DB2 Personal Edition, you must reboot your computer before you can 
 begin to use the software. DB2 is started automatically after you reboot and each time 
 you start your computer after that.
  
 The DB2 First Steps application executes automatically the first time you reboot after 
 installing the software. You can use First Steps to create the sample database. First 
 Steps can be invoked at any time by choosing 
 Start | Programs | DB2 for Windows 
 XX 
 | First Steps
  from the Windows taskbar, where 
 XX
  is either 95 or NT.",NA
Stopping DB2 Personal Edition,"Perform the following steps to shut down DB2 Personal Edition:
  
 1.If necessary, exit the DB2 First Steps application.
  
 2.If necessary, exit the DB2 Control Center.
  
 3.Under Windows NT, stop all DB2 services (if any are running). To do this:
  
 •Choose 
 Start | Settings | Control Panel
  from the Windows taskbar.
  
 •Double-click on the 
 Services
  application.
  
 •Click on the 
 Stop 
 icon to shut down any DB2 service that has the status Started.
  
 •Click 
 Close 
 to exit theServices application.",NA
Verifying the Installation,"You can verify that DB2 is installed correctly by creating the DB2 sample database on 
 your system and accessing data from it by performing the following steps:
  
 1.Log on to the system with a valid DB2 username.
  
 2.Create the sample database by clicking on 
 Create the SAMPLE Database 
 in the 
 main panel of First Steps.
  
 3. Once the database is created, click on 
 View the SAMPLE Database 
 on the main panel 
 of First Steps to select data from the SAMPLE database. This starts the Command Center 
 allowing you to use the supplied script that shows some of the data from the database. 
 Click on the 
 Execute 
 icon to begin the query.
  
 4. Click on 
 Work with the SAMPLE Database 
 on the main panel of First Steps to start 
 the Control Center. This allows you to see the tables that are in the SAMPLE database 
 and enables you to perform actions on them.
  
 After you have verified the installation, you can remove the sample database to free up 
 disk space. Issue the 
 drop database sample 
 command from the command-line 
 processor to remove the sample database.",NA
Troubleshooting Installation Problems,- 688 -,NA
Uninstalling DB2 Personal Edition,"Perform the following steps to uninstall DB2 Personal Edition:
  
 1.Stop all IBM DB2 services, and close all IBM DB2 applications.
  
 2.Choose 
 Start | Programs | DB2 for Windows 
 XX
  | Uninstall 
 from the Windows 
 taskbar, where 
 XX
  is either 95 or NT.
  
 3. The Confirm DB2 Deletion dialog appears and asks you to confirm that you want to 
 uninstall the program. Click 
 Yes
  to continue. The Uninstall program removes all IBM 
 DB2 Personal Edition programs from your system.
  
 - 689 -",NA
