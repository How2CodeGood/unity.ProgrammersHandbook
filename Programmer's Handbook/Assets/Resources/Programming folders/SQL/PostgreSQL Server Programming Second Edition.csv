Larger Text,Smaller Text,Symbol
PostgreSQL Server Programming ,NA,NA
Second Edition,NA,NA
Extend PostgreSQL using PostgreSQL server ,NA,NA
"programming to create, test, debug, and optimize ",NA,NA
a range of user-defined functions in your favorite ,NA,NA
programming language,NA,NA
Usama Dar,NA,NA
Hannu Krosing,NA,NA
Jim Mlodgenski,NA,NA
Kirk Roybal,BIRMINGHAM - MUMBAI,NA
PostgreSQL Server Programming ,NA,NA
Second Edition,"Copyright © 2015 Packt Publishing
  
 All rights reserved. No part of this book may be reproduced, stored in a retrieval 
 system, or transmitted in any form or by any means, without the prior written 
 permission of the publisher, except in the case of brief quotations embedded in 
 critical articles or reviews.
  
 Every effort has been made in the preparation of this book to ensure the accuracy of 
 the information presented. However, the information contained in this book is sold 
 without warranty, either express or implied. Neither the authors, nor Packt 
 Publishing, and its dealers and distributors will be held liable for any damages 
 caused or alleged to be caused directly or indirectly by this book.
  
 Packt Publishing has endeavored to provide trademark information about all of the 
 companies and products mentioned in this book by the appropriate use of capitals. 
  
 However, Packt Publishing cannot guarantee the accuracy of this information.
  
 First published: June 2013
  
 Second edition: February 2015
  
 Production reference: 1210215
  
 Published by Packt Publishing Ltd.
  
 Livery Place 
  
 35 Livery Street 
  
 Birmingham B3 2PB, UK.
  
 ISBN 978-1-78398-058-1
  
 www.packtpub.com",NA
Credits,"Authors 
  
 Usama Dar 
  
 Hannu Krosing 
  
 Jim Mlodgenski 
  
 Kirk Roybal
  
 Reviewers 
  
 Stephen Frost 
  
 Rick van Hattem 
 Vibhor Kumar 
  
 Jeff Lawson 
  
 Mariano Reingart 
 Julien Tachoires
  
 Commissioning Editor 
 Usha Iyer
  
 Acquisition Editors 
 Antony Lowe 
  
 Meeta Rajani 
  
 Sam Wood
  
 Content Development Editor 
 Adrian Raposo
  
 Technical Editors 
 Mrunmayee Patil
  
 Chinmay Puranik
  
 Copy Editors 
 Dipti Kapadia
  
 Aarti Saldanha
  
 Project Coordinator 
 Kinjal Bari
  
 Proofreaders 
  
 Maria Gould
  
 Linda Morris
  
 Indexer 
  
 Monica Ajmera Mehta
  
 Production Coordinator 
 Nitesh Thakur
  
 Cover Work 
  
 Nitesh Thakur",NA
About the Authors,"Usama Dar
  is a seasoned software developer and architect. During his 14 years' 
 career, he has worked extensively with PostgreSQL and other database technologies. 
  
 He worked on PostgreSQL internals extensively while he was working for 
  
 EnterpriseDB. Currently, he lives in Munich where he works for Huawei's European 
 Research Center. He designs the next generation of high-performance database systems 
 based on open source technologies, such as PostgreSQL, which are used under high 
 workloads and strict performance requirements.
  
 Hannu Krosing
  was a PostgreSQL user before it was rewritten to use SQL as its 
 main query language in 1995. Therefore, he has both the historic perspective of its 
 development, as well as almost 20 years of experience in using it to solve various real-
 life problems.
  
 He was the first database administrator and database architect at Skype, where he 
 invented the sharding language PL/Proxy that allows you to scale the user database in 
 order to work with billions of users.
  
 After he left Skype at the end of 2006—about a year after it was bought by eBay—
 he has been working as a PostgreSQL consultant with 2ndQuadrant, the premier 
 PostgreSQL consultancy with a global reach and local presence in most parts of the 
 world.
  
 He has coauthored 
 PostgreSQL 9 Administration Cookbook
 , 
 Packt Publishing
 , together 
 with one of the main PostgreSQL developers, Simon Riggs.
  
 I want to sincerely thank my wife, Evelyn, for her support while 
 writing this book.",NA
About the Reviewers,"Stephen Frost
  is a major contributor and committer to PostgreSQL, who has been 
 involved with PostgreSQL since 2002, and has developed features such as the role 
 system and column-level privileges.
  
 He is the chief technology officer at Crunchy Data Solutions, Inc., the PostgreSQL 
 company for Secure Enterprises. He is involved in the advancement of PostgreSQL's 
 capabilities, particularly in the area of security in order to support the needs of 
 government and financial institutions who have strict security and 
  
 regulatory requirements.
  
 Rick van Hattem
  is an entrepreneur with a computer science background and a long-
 time open source developer with vast experience in the C, C++, Python, and Java 
 languages. Additionally, he has worked with most large database servers such as Oracle, 
 MS SQL, and MySQL, but he has been focusing on PostgreSQL since Version 7.4.
  
 He is one of the founders of the Fashiolista.com social network, and until recently, he 
 was the CTO. Here, he used PostgreSQL to scale the feeds for millions of users to show 
 that PostgreSQL can hold up to NoSQL solutions, given some tuning and additional 
 tools. After Fashiolista, he worked as a freelance consultant for several companies, 
 including 2ndQuadrant.
  
 He is currently the founder of PGMon.com, a monitoring service that analyzes your 
 databases, indexes, and queries to keep them running at peak performance. In addition 
 to analyzing your database settings, the system actively monitors your queries and 
 gives you recommendations to enhance performance.
  
 He is also the creator and maintainer of a large number of open source projects, 
 such as pg_query_analyser, pg_cascade_timestamp, QtQuery, Python-Statsd, and 
 Django-Statsd.",NA
www.PacktPub.com,NA,NA
"Support files, eBooks, discount offers, and more","For support files and downloads related to your book, please visit 
 www.PacktPub.com
 .
  
 Did you know that Packt offers eBook versions of every book published, with PDF and 
 ePub files available? You can upgrade to the eBook version at 
 www.PacktPub.com 
 and 
 as a print book customer, you are entitled to a discount on the eBook copy. Get in touch 
 with us at 
 service@packtpub.com
  for more details.
  
 At 
 www.PacktPub.com
 , you can also read a collection of free technical articles, sign 
 up for a range of free newsletters and receive exclusive discounts and offers on Packt 
 books and eBooks.
  
 TM
  
 https://www2.packtpub.com/books/subscription/packtlib
  
 Do you need instant solutions to your IT questions? PacktLib is Packt's online digital 
 book library. Here, you can search, access, and read Packt's entire library of books.",NA
Why subscribe?,"• 
  
 Fully searchable across every book published by Packt
  
 • 
  
 Copy and paste, print, and bookmark content
  
 • 
  
 On demand and accessible via a web browser",NA
Free access for Packt account holders,"If you have an account with Packt at 
 www.PacktPub.com
 , you can use this to access 
 PacktLib today and view 9 entirely free books. Simply use your login credentials for 
 immediate access.",NA
Table of Contents,"Preface 
  
 1
  
 Chapter 1: What Is a PostgreSQL Server? 
  
 7
  
 Why program in the server? 
  
 Using PL/pgSQL for integrity checks 
  
 About this book's code examples 
  
 Switching to the expanded display 
  
 Moving beyond simple functions 
  
 Data comparisons using operators 
  
 Managing related data with triggers 
  
 Auditing changes 
  
 Data cleaning 
  
 Custom sort orders 
  
 Programming best practices 
  
 KISS – keep it simple stupid 
  
 DRY – don't repeat yourself 
  
 YAGNI – you ain't gonna need it 
  
 SOA – service-oriented architecture 
  
 Type extensibility 
  
 Caching 
  
 Wrapping up – why program in the server? 
  
 Performance 
  
 Ease of maintenance 
  
 Improved productivity 
  
 Simple ways to tighten security 
 Summary 
  
 9
  
  
 1
 0 
  
 1
 3 
  
 1
 3 
  
 1
 5 
  
 1
 5 
  
 1
 6 
  
 2
 0 
  
 2
 6 
  
 2
 7 
  
 2
 8 
  
 2
 8 
  
 2
 9 
  
 2
 9 
  
 3
 0 
  
 3
 0 
  
 3
 1 
  
 3
 2 
  
 3",NA
Preface,"This fascinating guide to server programming will take your skills of PostgreSQL to a 
 whole new level. A step-by-step approach with illuminating examples will educate you 
 about the full range of possibilities. You will understand the extension framework of 
 PostgreSQL and leverage it in ways you haven't even invented yet. You will learn how 
 to write functions and create your own data types, all in your favorite programming 
 language. It is a step-by-step tutorial, with plenty of tips and tricks to kick-start server 
 programming.",NA
What this book covers,"Chapter 1
 , 
 What Is a PostgreSQL Server?
 ,introduces you to the PostgreSQL server and 
 will set the tone for the rest of the book. It introduces you to the ways in which a 
 PostgreSQL server is extendible, and shows you that it can be treated as a complete 
 software development framework instead of just a database server.
  
 Chapter 2
 , 
 Server Programming Environments
 ,elaborates that PostgreSQL is built to 
 handle user needs, but more importantly, it is built not to change underneath users in 
 the future. It will touch upon the environments and will highlight some of the important 
 things to be kept in mind when programming on the server in PostgreSQL.
  
 Chapter3
 , 
 Your First PL/pgSQL Function
 ,builds the foundations by demonstrating 
 how to write simple PL/pgSQL functions.
  
 Chapter4
 , 
 Returning Structured Data
 ,builds on the knowledge of writing PL/pgSQL 
 functions and demonstrates how to write functions that return a set of values such as 
 rows, arrays, and cursors.",NA
What you need for this book ,"In 
 order to follow this book, you need the following software:
  
 • 
  
 PostgreSQL Database Server 9.4
  
 • 
  
 Linux/Unix Operating System
  
 • 
  
 Python 2, Perl, and Tcl",NA
Who this book is for ,"This book is for moderate to advanced level PostgreSQL database professionals. To 
 get a better understanding of this book, you should have a general experience in 
 writing SQL, a basic idea of query tuning, and some coding experience in a language 
 of your choice.",NA
Conventions ,"In this book, you will find a number of text styles that distinguish between different 
 kinds of information. Here are some examples of these styles and an explanation of their 
 meaning.
  
 Code words in text, database table names, folder names, filenames, file extensions, 
 pathnames, dummy URLs, user input, and Twitter handles are shown as follows: ""If 
 any of the checks fail, you should do 
 ROLLBACK
  instead of 
 COMMIT
 .""
  
 A block of code is set as follows:
  
 CREATE TABLE accounts(owner text, balance numeric, amount  
 numeric); 
  
 INSERT INTO accounts VALUES ('Bob',100); 
  
 INSERT INTO accounts VALUES ('Mary',200);
  
 When we wish to draw your attention to a particular part of a code block, the 
 relevant lines or items are set in bold:
  
 CREATE OR REPLACE FUNCTION fibonacci_seq(num integer)
  
  RETURNS SETOF integer AS $$ 
  
 DECLARE
  
  
  a int := 0;
  
  
  b int := 1; 
  
 BEGIN
  
  
  IF (num <= 0)
  
  
  THEN 
 RETURN
 ;
  
 [
  3 
 ]",NA
Reader feedback ,"Feedback from our readers is always welcome. Let us know what you think about this 
 book—what you liked or disliked. Reader feedback is important for us as it helps us 
 develop titles that you will really get the most out of.
  
 To send us general feedback, simply e-mail 
 feedback@packtpub.com
 , and mention the 
 book's title in the subject of your message.
  
 If there is a topic that you have expertise in and you are interested in either writing or 
 contributing to a book, see our author guide at 
 www.packtpub.com/authors
 .
  
 [
  4 
 ]",NA
Customer support,"Now that you are the proud owner of a Packt book, we have a number of things to help 
 you to get the most from your purchase.",NA
Downloading the example code,"You can download the example code files from your account at 
 http://www.
  
 packtpub.com
  for all the Packt Publishing books you have purchased. If you 
 purchased this book elsewhere, you can visit 
 http://www.packtpub.com/support 
 and register to have the files e-mailed directly to you.",NA
Errata,"Although we have taken every care to ensure the accuracy of our content, mistakes do 
 happen. If you find a mistake in one of our books—maybe a mistake in the text or the 
 code—we would be grateful if you could report this to us. By doing so, you can save 
 other readers from frustration and help us improve subsequent versions of this book. If 
 you find any errata, please report them by visiting 
 http://www.packtpub. 
 com/submit-errata
 , selecting your book, clicking on the 
 Errata Submission Form 
 link, and entering the details of your errata. Once your errata are verified, your 
 submission will be accepted and the errata will be uploaded to our website or added to 
 any list of existing errata under the Errata section of that title.
  
 To view the previously submitted errata, go to 
 https://www.packtpub.com/books/ 
 content/support
  and enter the name of the book in the search field. The required 
 information will appear under the 
 Errata
  section.",NA
Piracy,"Piracy of copyrighted material on the Internet is an ongoing problem across all media. At 
 Packt, we take the protection of our copyright and licenses very seriously. If you come 
 across any illegal copies of our works in any form on the Internet, please provide us with 
 the location address or website name immediately so that we can pursue a remedy.
  
 Please contact us at 
 copyright@packtpub.com
  with a link to the suspected 
 pirated material.
  
 We appreciate your help in protecting our authors and our ability to bring you 
 valuable content.
  
 [
  5 
 ]",NA
Questions ,"If you have a problem with any aspect of this book, you can contact us at 
 questions@packtpub.com
 , and we will do our best to address the problem.
  
 [
  6 
 ]",NA
What Is a PostgreSQL ,NA,NA
Server?,"If you think that a PostgreSQL Server is just a storage system and the only way to 
 communicate with it is by executing SQL statements, you are limiting yourself 
 tremendously. That is, you are using just a tiny part of the database's features.
  
 A PostgreSQL Server is a powerful framework that can be used for all kinds of data 
 processing, and even some non-data server tasks. It is a server platform that allows you 
 to easily mix and match functions and libraries from several popular languages.
  
 Consider this complicated, multilanguage sequence of work:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Call a string parsing function in Perl 
  
 Convert the string to XSLT and process the result using JavaScript Ask for 
 a secure stamp from an external timestamping service, such as 
 http://guardtime.com/
 , using their SDK for C 
  
 Write a Python function to digitally sign the result
  
 This multilanguage sequence of work can be implemented as a series of simple 
 function calls using several of the available server programming languages. The 
 developer who needs to accomplish all this work can just call a single PostgreSQL 
 function without the need to be aware of how the data is being passed between 
 languages and libraries:
  
 SELECT convert_to_xslt_and_sign(raw_data_string);",NA
Why program in the server?,"Developers program their code in a number of different languages, and it can be 
 designed to run just about anywhere. When writing an application, some people follow 
 the philosophy that as much of the logic as possible for the application should be 
 pushed to the client. We see this in the explosion of applications leveraging JavaScript 
 inside browsers. Others like to push the logic into the middle tier, with an application 
 server handling the business rules. These are all valid ways to design an application, so 
 why will you want to program in the database server?
  
 Let's start with a simple example. Many applications include a list of customers 
 who have a balance in their account. We'll use this sample schema and data:
  
 CREATE TABLE accounts(owner text, balance numeric, amount  
 numeric);
  
 INSERT INTO accounts VALUES ('Bob',100);
  
 INSERT INTO accounts VALUES ('Mary',200);
  
  
 Downloading the example code
  
  
 You can download the example code files for all the Packt books you 
  
 have purchased from your account at 
 http://www.packtpub.
  
 com
 . If you purchased this book elsewhere, you can visit 
 http://
  
 www.packtpub.com/support
  and register to have the files 
  
 e-mailed directly to you.
  
 When using a database, the most common way to interact with it, is to use SQL queries. 
 If you want to move 14 dollars from Bob's account to Mary's account with simple SQL, 
 you can do so using the following:
  
 UPDATE accounts SET balance = balance - 14.00 WHERE owner = 'Bob';
  
 UPDATE accounts SET balance = balance + 14.00 WHERE owner = 'Mary';
  
 [
  9 
 ]",NA
Using PL/pgSQL for integrity checks,"PostgreSQL includes its own programming language named PL/pgSQL that is aimed to 
 integrate easily with SQL commands. PL stands for procedural language, and this is 
 just one of the many languages available for writing server code. pgSQL is the 
 shorthand for PostgreSQL.
  
 Unlike basic SQL, PL/pgSQL includes procedural elements, such as the ability to 
 use the 
 if
 /
 then
 /
 else
  statements and loops. You can easily execute SQL 
 statements, or even loop over the result of a SQL statement in the language.
  
 [
  10 
 ]",NA
About this book's code examples,"The sample output shown here has been created with the 
 psql
  utility of PostgreSQL, 
 usually running on a Linux system. Most of the code will work the same way if you are 
 using a GUI utility such as 
 pgAdmin3
  to access the server instead. Take an example of the 
 following line of code:
  
 postgres=# SELECT 1;
  
 The 
 postgres=#
  part is the prompt shown by the 
 psql
  command.
  
 The examples in this book have been tested using PostgreSQL 9.3. They will probably 
 work on PostgreSQL Version 8.3 and later. There haven't been many major changes to 
 how server programming happens in the last few versions of PostgreSQL. The syntax 
 has become stricter over time to reduce the possibility of mistakes in the server 
 programming code. Due to the nature of these changes, most code from newer versions 
 will still run on the older ones, unless it uses very new features. However, the older code 
 can easily fail to run due to one of the newly enforced restrictions.",NA
Switching to the expanded display,"When using the 
 psql
  utility to execute a query, PostgreSQL normally outputs the 
 result using vertically aligned columns:
  
 $ psql -c ""SELECT 1 AS test""
  
  test 
  
 ------
  
  1
  
 (1 row)
  
 $ psql
  
 psql (9.3.2)
  
 [
  13 
 ]",NA
Moving beyond simple functions ,"Server programming can mean a lot of different things. Server programming is not just 
 about writing server functions. There are many other things you can do in the server, 
 which can be considered as programming.",NA
Data comparisons using operators ,"For more complex tasks, you can define your own types, operators, and casts 
 from one type to another, letting you actually compare apples and oranges.
  
 As shown in the next example, you can define the type 
 fruit_qty
  for fruit-with -
 quantity and then teach PostgreSQL to compare apples and oranges, say to make 
 one orange to be worth 1.5 apples, in order to convert apples to oranges:
  
 postgres=# CREATE TYPE FRUIT_QTY as (name text, qty int);
  
 postgres=# SELECT '(""APPLE"", 3)'::FRUIT_QTY; 
 fruit_qty
  
 ----------------
  
  (APPLE,3) 
  
 (1 row)
  
 CREATE FUNCTION fruit_qty_larger_than(left_fruit FRUIT_QTY,  
 right_fruit FRUIT_QTY) 
  
 RETURNS BOOL 
  
 AS $$ 
  
 BEGIN
  
  IF (left_fruit.name = 'APPLE' AND right_fruit.name = 'ORANGE') 
 THEN
  
  
   
  RETURN left_fruit.qty > (1.5 * right_fruit.qty);
  
  END IF;
  
  IF (left_fruit.name = 'ORANGE' AND  
  
  
  
  right_fruit.name = 'APPLE' )
  
  THEN
  
  
   
  RETURN (1.5 * left_fruit.qty) > right_fruit.qty;
  
  END IF;
  
  RETURN  left_fruit.qty > right_fruit.qty; 
  
 END; 
  
 $$ 
  
 LANGUAGE plpgsql;
  
 postgres=# SELECT fruit_qty_larger_than('(""APPLE"", 
  
  
 3)'::FRUIT_QTY,'(""ORANGE"", 2)'::FRUIT_QTY);
  
 [
  15 
 ]",NA
Managing related data with triggers ,"Server 
 programming can also mean setting up automated actions (triggers), so that some 
 operations in the database cause some other things to happen as well. For example, 
 you can set up a process where making an offer on some items is automatically 
 reserved to them being in the stock table.
  
 So, let's create a fruit stock table, as shown here:
  
 CREATE TABLE fruits_in_stock 
 (
  
  name text PRIMARY KEY,
  
 [
  16 
 ]",NA
Auditing changes ,"If you need to know who did what to the data and when it was done, one way to find out 
 is to log every action that is performed in an important table. In PostgreSQL 9.3, you can 
 also audit the 
 data definition language
  (
 DDL
 ) changes to the database using event 
 triggers. We will learn more about this in the later chapters.
  
 There are at least two equally valid ways to perform data auditing:
  
 • 
  
 • 
  
 Using auditing triggers 
  
 Allowing tables to be accessed only through functions and auditing inside 
 these functions
  
 Here, we will take a look at a minimal number of examples for both the approaches.
  
 First, let's create the tables:
  
 CREATE TABLE salaries(
  
  emp_name text PRIMARY KEY,
  
  salary integer NOT NULL 
  
 );
  
 CREATE TABLE salary_change_log(
  
  changed_by text DEFAULT CURRENT_USER,
  
  changed_at timestamp DEFAULT CURRENT_TIMESTAMP, 
 salary_op text,
  
  emp_name text,
  
  old_salary integer,
  
 [
  20 
 ]",NA
Data cleaning ,"In the preceding code, we notice that employee names don't have consistent cases. 
  
 It will be easy to enforce consistency by adding a constraint, as shown here:
  
 CHECK (emp_name = upper(emp_name))
  
 However, it is even better to just make sure that the name is stored as uppercase, and 
 the simplest way to do this is by using 
 trigger
 :
  
 CREATE OR REPLACE FUNCTION uppercase_name () 
  
  RETURNS trigger AS $$
  
  
  BEGIN
  
    
  NEW.emp_name = upper(NEW.emp_name);
  
   
  RETURN NEW;
  
  
  END; 
  
 $$ LANGUAGE plpgsql;
  
 CREATE TRIGGER uppercase_emp_name 
  
 BEFORE INSERT OR UPDATE OR DELETE ON salaries
  
  FOR EACH ROW EXECUTE PROCEDURE uppercase_name ();
  
 The next 
 set_salary()
  call for a new employee will now insert 
 emp_name 
 in uppercase:
  
 postgres=# SELECT set_salary('arnold',80);
  
 -[ RECORD 1 ]-------------------
  
 set_salary | INSERTED USER arnold
  
 As the uppercasing happens inside a trigger, the function's response still shows a 
 lowercase name, but in the database, it is uppercased:
  
 postgres=# SELECT * FROM 
 salaries;
  
 -[ RECORD 1 ]---
  
 emp_name | Bob 
  
 salary   | 1300
  
 -[ RECORD 2 ]---
  
 emp_name | Fred 
  
 salary   | 750
  
 -[ RECORD 3 ]---
  
 emp_name | Frank 
  
 salary   | 100
  
 -[ RECORD 4 ]---
  
 emp_name |  ARNOLD 
  
 salary   | 80
  
 [
  26 
 ]",NA
Custom sort orders ,"The last example in this chapter, is about using functions for different ways of sorting.
  
 Say we are given a task to sort words by their vowels only, and in addition to this, to 
 make the last vowel the most significant one when sorting. While this task may seem 
 really complicated at first, it can be easily solved with functions:
  
 CREATE OR REPLACE FUNCTION reversed_vowels(word text) 
  
  RETURNS text AS $$
  
  
  vowels = [c for c in word.lower() if c in 'aeiou']
  
  vowels.reverse()
  
  
  return ''.join(vowels) 
  
 $$ LANGUAGE plpythonu IMMUTABLE;
  
 postgres=# select word,reversed_vowels(word) from words order by 
  
  
  reversed_vowels(word);
  
  word     | reversed_vowels
  
 -------------+-----------------
  
  Abracadabra | aaaaa
  
  Great       | ae
  
  Barter      | ea
  
  Revolver    | eoe 
  
 (4 rows)
  
  
 Before performing this code, please make sure you have Python 
  
  
 2.x installed. We will discuss PL/Python in much detail in the later 
  
 chapters of this book.
  
 [
  27 
 ]",NA
Programming best practices,"Developing application software is complicated. Some of the approaches that help 
 manage this complexity are so popular that they have been given simple acronyms 
 that can be remembered. Next, we'll introduce some of these principles and show you 
 how server programming helps make them easier to follow.",NA
KISS – keep it simple stupid,"One of the main techniques to successful programming is writing simple code. That is, 
 writing code that you can easily understand 3 years from now and that others can 
 understand as well. It is not always achievable, but it almost always makes sense to 
 write your code in the simplest way possible. You can rewrite parts of it later for 
 various reasons such as speed, code compactness, to show off how clever you are, and 
 so on. However, always write the code in a simple way first, so that you can be 
 absolutely sure that it does what you want. Not only do you get working on the code 
 quickly, but you also have something to compare to when you try more advanced ways 
 to do the same thing.
  
 Remember, debugging is harder than writing code; so, if you write the code in the 
 most complex way you can, you will have a really hard time debugging it.
  
 It is often easier to write a set returning function instead of a complex query. Yes, it will 
 probably run slower than the same thing implemented as a single complex query, due to 
 the fact that the optimizer can do very little to the code written as functions, but the 
 speed may be sufficient for your needs. If more speed is required, it's very likely to 
 refactor the code piece by piece, joining parts of the function into larger queries where 
 the optimizer has a better chance of discovering better query plans until the 
 performance is acceptable again.
  
 Remember that most of the time, you don't need the absolutely fastest code. For your 
 clients or bosses, the best code is the one that does the job well and arrives on time.
  
 [
  28 
 ]",NA
DRY – don't repeat yourself,"This principle means you should implement any piece of business logic just once and put 
 the code for doing it in the right place.
  
 This may be hard sometimes; for example, you want to do some checks on your web 
 forms in the browser, but still do the final checks in the database. However, as a 
 general guideline, it is very much valid.
  
 Server programming helps a lot here. If your data manipulation code is in the 
 database near the data, all the data users have easy access to it, and you will not 
 need to manage a similar code in a C++ Windows program, two PHP websites, and a 
 bunch of Python scripts doing nightly management tasks. If any of them need to do 
 this thing to a customer's table, they just call:
  
 SELECT * FROM do_this_thing_to_customers(arg1, arg2, arg3);
  
 That's it!
  
 If the logic behind the function needs to be changed, you just change the function 
 with no downtime and no complicated orchestration of pushing database query 
 updates to several clients. Once the function is changed in the database, it is changed 
 for all the users.",NA
YAGNI – you ain't gonna need it,"In other words, don't do more than you absolutely need to.
  
 If you have a creepy feeling that your client is not yet well aware of how the final 
 database will look or what it will do, it's helpful to resist the urge to design 
 everything 
 into the database. A much better way is to do a minimal implementation that satisfies 
 the current specifications, but do it with extensibility in mind. It is very easy to ""paint 
 yourself into a corner"" when implementing a big specification with large 
  
 imaginary parts.
  
 If you organize your access to the database through functions, it is often possible to do 
 even large rewrites of business logic without touching the frontend application code. 
 Your application still performs 
 SELECT * FROM do_this_thing_to_ 
  
 customers(arg1, arg2, arg3)
 , even after you have rewritten the function five 
 times and changed the whole table structure twice.
  
 [
  29 
 ]",NA
SOA – service-oriented architecture,"Usually, when you hear the acronym SOA, it will be from enterprise software people 
 trying to sell you a complex set of SOAP services. But the essence of SOA is to 
 organize your software platform as a set of services that clients, and other services, 
 call in order to perform certain well-defined atomic tasks, as follows:
  
 • 
  
 • 
  
 • 
  
 Checking a user's password and credentials 
  
 Presenting him/her with a list of his/her favorite websites 
  
 Selling him/her a new red dog collar with a complementary membership in 
 the red-collared dog club
  
 These services can be implemented as SOAP calls with corresponding WSDL 
  
 definitions and Java servers with servlet containers, as well as a complex management 
 infrastructure. They can also be a set of PostgreSQL functions, taking a set of 
  
 arguments and returning a set of values. If the arguments or return values are 
  
 complex, they can be passed as XML or JSON, but a simple set of standard PostgreSQL data 
 types is often enough. In 
 Chapter 10
 , 
 Scaling Your Database with PL/Proxy
 , you will learn 
 how to make such a PostgreSQL-based SOA service infinitely scalable.",NA
Type extensibility,"Some of the preceding techniques are available in other databases, but PostgreSQL's 
 extensibility does not stop here. In PostgreSQL, you can just write UDFs in any of the 
 most popular scripting languages. You can also define your own types, not just domains, 
 which are standard types with some extra constraints attached, and new full-fledged 
 types too.
  
 For example, a Dutch company, MGRID, has developed a 
 value with unit
  set of data 
 types, so that you can divide 10 km by 0.2 hours and get the result in 50 km/h. Of 
 course, you can also cast the same result to meters per second or any other unit of 
 speed. And yes, you can get this as a fraction of 
 c
 —the speed of light.
  
 This kind of functionality needs both the types and overloaded operands, which know 
 that if you divide distance by time, then the result is speed. You will also need user-
 defined casts, which are automatically or manually-invoked conversion functions 
 between types.
  
 [
  30 
 ]",NA
Caching,"Yet another place where server-side programming can be used is to cache values, 
 which are expensive to compute. The following is the basic pattern here:
  
 1. Check whether the value is cached.
  
 2. If it isn't, or the value is too old, compute and cache it.
  
 3. Return the cached value.
  
 [
  31 
 ]",NA
Wrapping up – why program in the ,NA,NA
server?,"The main advantages of doing most data manipulation code on the server-side are 
 stated in the following sections.",NA
Performance,"Doing the computation near the data is almost always a performance win, as the 
 latencies to get the data are minimal. In a typical data-intensive computation, most of 
 the time is spent in getting the data. Therefore, making data access inside the 
 computation faster is the best way to make the whole thing fast. On my laptop, it takes 
 2.2 ms to query one random row from a 1,000,000-row database into the client, but it 
 takes only 0.12 ms to get the data inside the database. This is 20 times faster and inside 
 the same machine over Unix sockets. The difference can be bigger if there is a network 
 connection between the client and the server.
  
 [
  32 
 ]",NA
Ease of maintenance,"If all the data manipulation code is in a database, either as database functions or views, 
 the actual upgrade process becomes very easy. All that is needed is to run a DDL script 
 that redefines the functions; all the clients automatically use the new code with no 
 downtime and no complicated coordination between several frontend systems and 
 teams.",NA
Improved productivity,"Server-side functions are perhaps the best way to achieve code reuse. Any client 
 application written in any language or framework can make use of the server-side 
 functions, ensuring maximum reuse in all environments.",NA
Simple ways to tighten security,"If all the access for some possibly insecure servers goes through functions, the database 
 user of these servers can only be granted access to the needed functions and nothing else. 
 They can't see the table data or even the fact that these tables exist. So, even if the server 
 is compromised, all it can do is continue to call the same functions. Also, there is no 
 possibility of stealing passwords, e-mails, or other sensitive information by issuing its 
 own queries such as 
 SELECT * FROM users;
  and getting all the data there is in the 
 database.
  
 Also, the most important thing is that programming in a server is fun!
  
 [
  33 
 ]",NA
Summary,"Programming inside the database server is not always the first thing that comes to mind 
 to many developers, but its unique placement inside the application stack gives it some 
 powerful advantages. Your application can be faster, more secure, and more 
 maintainable by pushing logic into the database. With server-side programming in 
 PostgreSQL, you can secure your data using functions, audit access to your data and 
 structural changes using triggers, and improve productivity by achieving code reuse. 
  
 Also, you can enrich your data using custom data types, analyze your data using custom 
 operators, and extend the capabilities of the database by dynamically loading new 
 functions.
  
 This is just the start of what you can do inside PostgreSQL. Throughout the rest of 
 this book, you will learn many other ways to write powerful applications by 
 programming inside PostgreSQL.
  
 [
  34 
 ]",NA
Server Programming ,NA,NA
Environments,"You've had a chance to get acquainted with the general idea of using PostgreSQL, but now 
 we are going to answer the question of why anyone will choose PostgreSQL as a 
 development platform. As much as I'd like to believe that it's an easy decision for 
 everyone, it's not.
  
 For starters, let's get rid of the optimistic idea that you choose a database platform for 
 technical reasons. Sure, we all like to think that we are objective, and we base our 
 decisions on a preponderance of the technical evidence. This preponderance of evidence 
 then indicates which features are available and relevant to our application. We will then 
 proceed to make a weighted choice in favor of the most advantageous platform, and use a 
 balance of the evidences to create workarounds and alternatives where our choice falls 
 short. The fact is that we don't really understand all the requirements of the application 
 until we are halfway through the development cycle. 
  
 Here are some reasons why:
  
 • 
  
 • 
  
 • 
  
 We don't know how the application will evolve over time. Many start-ups 
 pivot from their initial idea as the market tells them to change.
  
 We don't know how many users there will 
 really
  be until we have some 
 registrations and can begin to measure the curve.
  
 We don't realize how important a particular feature can be until we get user 
 feedback. The truth is, that we don't really know much about the long-term 
 needs of the application until we're writing version 2 or maybe even version 
 3.",NA
Cost of acquisition,"One of biggest the factors that decides which technology is used in the application stack is 
 the cost of acquisition. I've seen many application architectures drawn on a whiteboard 
 where the technical team was embarrassed to show them, but they justified the design by 
 trying to keep software licensing costs down. When it comes to the database 
 environment, the usual suspects are Oracle, SQL Server, MySQL, and PostgreSQL. Oracle, 
 the dominant player in the database space, is also the most costly. At the low end, Oracle 
 does have reasonably priced offerings and even a free Express Edition, but they are 
 limited. Most people have needs beyond the low-priced offerings and fall into the 
 enterprise sales machine of Oracle. This usually results in a high-price quote that makes 
 your CFO fall out of his/her chair, and you're back to designing your solution in order to 
 keep your licensing costs down.
  
 Then comes Microsoft SQL Server. This is your first reasonably viable option. The 
 pricing is listed on the Microsoft website. I will not reproduce it here because the pricing 
 schedule is too volatile for a book that will remain in print for a longer time. 
 Nonetheless, an experienced thumb value of the purchase cost for SQL Server will get 
 you running with a web-capable model for about $5,000. This does not include a service 
 contract. In the grand scheme of development costs, this is reasonable and not too high 
 of a barrier to enter.
  
 Then, we have the open source offerings such as MySQL and PostgreSQL. They cost 
 nothing and the service contracts cost—wait for it—nothing. This is a very hard cost of 
 acquisition to beat.
  
 [
  36 
 ]",NA
Availability of developers,"This has been one of the most hilarious parts of my development life. I recently 
 recommended a local company to use PostgreSQL for a reporting system. The company 
 in question wanted to know that if they chose PostgreSQL, would anyone on staff be able 
 to maintain it. So, I began to interview the developers to find out about their experiences 
 with PostgreSQL.
  
 Me: Do you have any experience with PostgreSQL?
  
 Developer 1: Yes, I used it at the last job for a product fulfillment project, but I don't 
 think many people have that experience. We should probably stick to using MySQL.
  
 Me: Do you have any experience with PostgreSQL?
  
 Developer 2: Yes, I used it at the last job for a reporting project, but I don't think 
 many people have that experience. We should probably stick to using MySQL.
  
 After interviewing all seven developers that were influential on the project, I found that 
 the only person without hands-on experience with PostgreSQL was the project manager. 
 Since the project manager didn't expect to have any technical involvement in the 
 project, he approved the selection of PostgreSQL.
  
 [
  37 
 ]",NA
Licensing,"About 2 months after Oracle bought MySQL, they announced a plan that divided the 
 development into two camps: a MySQL community edition and a professional version. 
 The community edition would no longer gain any new features, and the professional 
 version would become a commercial product.
  
 There was a vast and thunderous sucking sound in the open source community, as 
 they thrashed wildly about to find a new platform for Free and Open Source 
 Software (FOSS) development.
  
 Oracle immediately (in about 2 weeks) countermanded the order and declared that 
 things will stay as they were for the indefinite future. Those with short memories, 
 forgiving hearts, or who just weren't paying attention went on about their business. 
 Many other open source projects either switched to PostgreSQL or suddenly grew 
 PostgreSQL database support.
  
 Today, we have MySQL and MySQL Enterprise Edition. If you want backup, high 
 availability, enterprise scalability, and the MySQL Enterprise Monitor, you now have to 
 pony up some dough. Capitalism is fine, and corporations have a right to charge money 
 for their services and products in order to exist. But why should you, as a project manager 
 or developer, have to pay for something that you can get for free?
  
 Licensing is all about continued product availability and distribution. The 
 PostgreSQL licensing model specifically states that you can have the source code, do 
 anything with it, redistribute it however you jolly well please, and these rights 
 extend indefinitely. Try to get this deal with a commercial vendor.
  
 [
  38 
 ]",NA
Predictability,"This section could just as well have been titled 
 standards compliance
 , but I decided 
 against it because the benefits of standards compliance in corporate projects are not 
 obvious. The limitations of the common databases are well-documented, and I will show 
 you a few websites in a moment where you can make a comparison of who has the most 
 unintended behavior
 . I will encourage you to read the following material while thinking 
 about the question, ""Which method of feature development is most likely to make my 
 application break in the future?"":
  
 • 
  
 http://www.sql-info.de/postgresql/postgres-gotchas.html
  
  
 • 
  
 http://www.sql-info.de/mysql/gotchas.html
  
  
 Spoiler alert:
  
 A stricter adherence to standards comes at the cost of not allowing 
 ambiguous behavior. Not allowing ambiguous behavior makes the 
 developer's life more difficult. Making the developer's life more difficult 
 ensures that the interpretation of the commands that the developer 
 gives will not change later, breaking the application.
  
 Just how lazy can you afford to be? I'm not sure how to measure this. 
 PostgreSQL is available for no-cost future predictability, so I don't have 
 to answer the question.
  
 Sure, PostgreSQL also has some bugs listed. However, changes to the 
 database core have a tendency to make the engine work like the 
 documentation says it does, not like the documentation should have 
 said. PostgreSQL developers don't have to say, ""Oops, I didn't think of 
 that,"" very often. When they do, PostgreSQL just becomes more 
 standards compliant.
  
 [
  39 
 ]",NA
Community,"Oracle and SQL Server don't have a community. Please understand when I say that, I 
 mean that the chance that you will get to talk to a developer of the core database is 
 about the same as your chance of winning the lottery. By the time you do, it's probably 
 because you found a bug so heinous that it couldn't be ignored and the only person who 
 can understand your report is the guy who wrote the code in question. They have paid 
 technical support and this support has proven in my experience to be generally 
 competent, but not stellar. I have had to work around the problem that I originally 
 requested help with about 40 percent of the time.
  
 Compare this to MySQL and PostgreSQL, where just about anybody can speak to 
 just about anybody else all day long. Many of the core developers of both the 
 platforms can be found on IRC, met at conventions, contacted for contract 
 development work, and for the most part, bribed remarkably easily with beer (hint, 
 hint, wink, wink, nudge, nudge).
  
 They are actively concerned about the health of the overall community and will 
 answer just about any kind of question you ask, even if the question has a very tenuous 
 relationship to database development. My personal experience, is that the PostgreSQL 
 team has more core developers readily available than MySQL. They are also more 
 personally available at conventions and meetings.
  
 Did I mention they like beer?",NA
Procedural languages,"SQL Server allows you to create a 
 dynamic link library
  (
 DLL
 ) in any language that 
 produces the 
 Common Language Runtime
  (
 CLR
 ). These DLLs must be loaded into the 
 server at boot time. To create a procedure at runtime and have it immediately available, 
 the only choice is the built-in SQL dialect, Transact SQL (TSQL).
  
 MySQL has a feature called 
 plugins
 . One of the legal plugin types is a procedural language. 
 Several languages have been tooled to work with MySQL via the plugin system, including 
 most of the popular ones such as PHP and Python. These functions cannot be used for 
 stored procedures or triggers, but they can be invoked from the common SQL statements. 
 For the rest, you are stuck with the built-in SQL.
  
 PostgreSQL has full support for additional procedural languages, which can be used to 
 create any legal entity in the database that can be created with PL/pgSQL. 
  
 The language can be added (or removed) from a running version of PostgreSQL and 
 any function defined using this language can also be created or dropped while 
 PostgreSQL is running. 
  
 [
  40 
 ]",NA
Third-party tools,"A frequent point of comparison among the database platforms is the number of 
 third-party applications available. I'm not so sure that the total number matters, as 
 much as the existence of the applications you actually need.
  
 To this end, the following is a list of the products that I have used extensively with 
 PostgreSQL:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Pentaho data integration (kettle)
 : This is an outstanding 
 Extract, 
  
 Transform and Load
  (
 ETL
 ) tool 
  
 Pentaho Report Server
 : This is a great reporting engine 
  
 pgAdmin3
 : This is an awesome database administration tool 
  
 php5-pgsql
 : This is a package that allows native access to 
  
 PostgreSQL from PHP 
  
 QCubed
 : This is the PHP development framework with PostgreSQL support
  
 [
  41 
 ]",NA
Platform compatibility,"SQL Server is a Microsoft product. As such, it was, and will always be, a Microsoft 
 platform tool. It is accessible to some limited degree via ODBC, but it is not a serious 
 choice for cross-platform development.
  
 MySQL and PostgreSQL support every operating system currently available today. This 
 ability (or the lack of limitation) is a strong argument for long-term stability. If any 
 particular operating system is no longer available, or no longer supports open source 
 software, it is fairly simple to move the database server to another platform.
  
  
 Lesson learned
  
  
 In the commercial operating system wars, just say no.",NA
Application design,"""The thing that hath been, it is that which shall be; and that which is done is 
 that which shall be done: and there is no new thing under the sun.""
  
 —Ecclesiastes 1:9 (KJV)
  
 ""... old things are passed away; behold, all things are become new.""
  
 —2 Corinthians 5:16-18 (KJV)
  
 [
  42 
 ]",NA
Databases are considered harmful,"The simplest and least powerful way of looking at server programming, is to view the 
 database as a data bucket. Using only the most basic SQL statements such as 
 INSERT
 , 
 SELECT
 , 
 UPDATE
 , and 
 DELETE
 , you can manipulate data, a single row at a time, and 
 create application libraries for multiple databases easily.
  
 This approach has some major drawbacks. Moving data back and forth to the 
 database server one row at a time is extremely inefficient, and you will find that 
 this method is simply not viable in a web-scale application.
  
 This idea is usually associated with the concept of a 
 database abstraction layer
 , a 
 client library that allows the developer to switch the database out from under the 
 application with little effort. This abstraction layer is very useful in the open source 
 development community, which allows the use of many databases, but they have no 
 financial incentive to get the best possible performance.
  
 SQL, being based on relational algebra and tuple relational calculus, has the ability to 
 quickly and efficiently perform set-based processing on large amounts of data; the 
 application-side processing usually involves iterative looping, which is generally much 
 slower.
  
 In my 27-year career, I have never actually changed the database of an installed 
 application without throwing away the application. One of the principles of agile 
 software development is YAGNI (you ain't gonna need it). This is one of those cases.
  
 [
  43 
 ]",NA
Encapsulation,"Another technique used in more client-centric development philosophies, is to isolate the 
 database-specific calls into a library of procedures. This design is usually aimed at leaving 
 the application in control of all the business logic. The application is still the king, and the 
 database is still just a necessary evil.
  
 This view of database architecture sells the application developer short by ignoring a 
 toolbox full of tools and choosing only the hammer. Everything in the application is then 
 painted to look like a nail and is smacked with the hammer.
  
  
 Lesson learned
  
  
 Don't give up on the power of the database just because it is not 
  
 familiar. Use procedural languages and check out extension toolkits. 
  
 There are some awesome pieces of work in there.",NA
What does PostgreSQL offer?,"So far, we've mentioned procedural languages, functions, triggers, custom data types, 
 and operators. These things can be created directly in the database via the 
 CREATE
  
 commands or added as libraries using extensions.
  
 Now, we will show you some things that you need to keep in mind when 
 programming on the server in PostgreSQL.",NA
Data locality,"If possible, keep the data on the server. Believe me, it's happier there, and 
 performance is much better when modifying data. If everything was done in the 
 application layer, the data will need to be returned from the database with the 
 modifications and then finally sent back to the database for a commit. If you are 
 building a web-scalable application, this should be your last resort.
  
 [
  44 
 ]",NA
More basics,"It helps to have some basic background information before you start programming for 
 the server. In the next few sections, we will explore the general technical environment 
 in which you will be working. We will cover a lot of information, but don't worry too 
 much about remembering it all right now. Just try to pick up the general idea.",NA
Transactions,"The default transaction isolation level in PostgreSQL is called 
 Read Committed
 . 
  
 This means that if multiple transactions attempt to modify the same data, they must 
 wait for each other to finish before acting on the resulting data. They wait in a first-
 come-first-serve order. The final result of the data is what most people will naturally 
 expect: the last chronological change being reflected.
  
 PostgreSQL does not provide any way to do a dirty read. A dirty read is the ability to 
 view the data the way it appears in someone else's transaction and to use it as if it were 
 committed. This ability is not available in PostgreSQL because of the way in which the 
 multiversion concurrency control works.
  
 There are other transaction isolation methods available; you can read about them in 
 detail at 
 http://www.postgresql.org/docs/current/static/transaction-iso. 
 html
 .
  
 It is important to note, that when no transaction blocks are specified (
 BEGIN .. END
 ), 
 PostgreSQL will treat each individual statement like a private transaction and commit 
 immediately when the statement is finished. This gives other transactions a chance to 
 settle between your statements. Some programming languages provide a transaction 
 block around your statements, while some do not. Please check your language 
 documentation to find out whether you are running in a transacted session.
  
 [
  46 
 ]",NA
General error reporting and error handling,"If you want to provide a status to the user during execution, you should be familiar with 
 the commands 
 RAISE
 , 
 NOTICE
 , and 
 NOTIFY
 . From a transactional perspective, the 
 difference is that 
 RAISE
  and 
 NOTICE
  will send the message immediately, even when 
 wrapped in a transaction, while 
 NOTIFY
  will wait for the transaction to settle before 
 sending a message. 
 NOTIFY
  will, therefore, actually not notify you of anything if the 
 transaction fails and is rolled back.",NA
User-defined functions,"The ability to write user-defined functions is the powerhouse feature of PostgreSQL. 
  
 Functions can be written in many different programming languages, can use any kind of 
 control structures that the language provides, and in the case of ""untrusted"" languages, 
 can perform any operation that is available in PostgreSQL.
  
 Functions can provide features that are not even directly related to SQL. Some of the 
 upcoming examples will show you how to get network address information, query the 
 system, move files around, and do just about anything that your heart desires.
  
 So, how do we access the sugary goodness of PostgreSQL? We start by declaring that we 
 want a function:
  
 CREATE OR REPLACE FUNCTION addition (integer, integer) RETURNS integer
  
 AS $$
  
 DECLARE retval integer;
  
 BEGIN
  
 [
  47 
 ]",NA
Other parameters ,"There is more than one way to get data into a function and out of it. We can also 
 declare 
 IN
 /
 OUT
  parameters, return a table, return a set of records, and use cursors 
 for both the input and output.
  
 This brings us to a pseudotype called 
 ANY
 . It allows the parameter type to be 
 undefined, and it allows any basic data type to be passed to the function. Then, it is up 
 to the function to decide what to do with the data. There are several other 
 pseudotypes available in PostgreSQL, also called the 
 polymorphic
  types. These are 
 anyelement
 , 
 anyarray
 , 
 anynonarray
 , 
 anyenum
 , and 
 anyrange
 . You can read more 
 about these pseudotypes at 
 http://www.postgresql.org/docs/9.3/static/ 
 datatype-pseudo.html
 .
  
 Here is an example of the pseudotype 
 anyarray
 . The following simple function 
 takes an array of any type as an argument and returns an array by removing all the 
 duplicates:
  
 CREATE OR REPLACE FUNCTION 
 remove_duplicates(anyarray) RETURNS anyarray AS 
  
 $$
  
  
  SELECT ARRAY(SELECT DISTINCT unnest($1)); 
  
 $$ 
  
 LANGUAGE 'sql' ;
  
 postgres=# SELECT remove_duplicates(ARRAY[1,1,2,2,3,3]); 
 remove_duplicates 
  
 -------------------
  
  {1,2,3} 
  
 (1 row)
  
 postgres=# SELECT remove_duplicates(ARRAY['a','a','b','c']); 
 remove_duplicates 
  
 -------------------
  
  {b,a,c} 
  
 (1 row)",NA
More control ,"Once you have your function written the way you need, PostgreSQL gives you 
 additional control over how the function executes. You can control what data the 
 function can access and how PostgreSQL will interpret the expense of running the 
 function.
  
 [
  49 
 ]",NA
Summary,"Now you know a few things about the PostgreSQL environment, as well as some things 
 that will help you in the unforeseeable future. PostgreSQL is built to handle your 
 needs, but more importantly, it is built 
 not
  to change underneath you in the future.
  
 We touched upon the environment and called out some of the more important things to 
 be kept in mind when programming on the server in PostgreSQL. Don't worry too much if 
 you don't remember all of it. It is fine to go on to the next chapter, where we will actually 
 start making some useful functions and learn about writing our first PL/pgSQL functions. 
 You will also learn how to write conditional statements, loops, and different ways to 
 return data. Then, come back and review this chapter when you have a clearer 
 understanding of the features available to the function writer.
  
 [
  51 
 ]",NA
Your First PL/pgSQL Function,"A function is the basic building block for extending PostgreSQL. A function accepts input 
 in the form of parameters, and it can create output in the form of output parameters or 
 return values. Many functions are provided by PostgreSQL itself, that is, common 
 mathematical functions such as square roots and absolute values. For a comprehensive 
 list of the functions that are already available, go to 
 http://www. 
 postgresql.org/docs/current/static/functions.html
 .
  
 The functions that you create have the same privileges and ability that the built-in 
 functions possess. The developers of PostgreSQL use the same libraries to extend the 
 database that you use, as a developer, to write your business logic.
  
 This means, that you have the tools available to be a first-class citizen of the 
 PostgreSQL development community. In fact, there are no second-class seats 
 on this bus.
  
 A function accepts parameters that can be of any data type available in PostgreSQL, and 
 it returns results to the caller using the same type. What you do within the function is 
 entirely up to you. You have been empowered to do anything that PostgreSQL is capable 
 of doing. You are herewith also warned that you are capable of doing anything that 
 PostgreSQL is capable of doing. The training wheels are off.
  
 In this chapter, you will learn the following topics:
  
 • 
  
 The basic building blocks of a PostgreSQL function
  
 • 
  
 Passing parameters into a function
  
 • 
  
 The basic control structures inside a function
  
 • 
  
 Returning results out of a function",NA
Why PL/pgSQL?,"PL/pgSQL is a powerful SQL scripting language, that is heavily influenced by PL/SQL, 
 the stored procedure language distributed with Oracle. It is included in the vast 
 majority of PostgreSQL installations as a standard part of the product, so it usually 
 requires no setup at all to begin.
  
 PL/pgSQL also has a dirty little secret. The PostgreSQL developers don't want you to 
 know that it is a full-fledged SQL development language, capable of doing pretty much 
 anything within the PostgreSQL database.
  
 Why is this a secret? For years, PostgreSQL did not claim to have stored procedures. 
 PL/pgSQL functions were originally designed to return scalar values and were intended 
 for simple mathematical tasks and trivial string manipulations.
  
 Over the years, PL/pgSQL developed a rich set of control structures and gained the 
 ability to be used by triggers, operators, and indexes. In the end, developers were 
 grudgingly forced to admit that they had a complete, stored procedure development 
 system on their hands.
  
 Along the way, the goal of PL/pgSQL changed from simple scalar functions, to providing 
 access to all of the PostgreSQL system internals, with full control structure. 
  
 The full list of what is available in the current version is provided at 
 http://www. 
 postgresql.org/docs/current/static/plpgsql-overview.html
 .
  
 Today, the following are some of the benefits of using PL/pgSQL:
  
 • 
  
 It is easy to use
  
 • 
  
 It is available by default on most deployments of PostgreSQL
  
 • 
  
 It is optimized for the performance of data-intensive tasks
  
 In addition to PL/pgSQL, PostgreSQL also allows many other languages such as PL/Perl, 
 PL/Python, PL/Proxy, and PL/Tcl to be plugged in to the database, some of which will be 
 covered in this book. You may also choose to write your functions in Perl, Python, PHP, 
 bash, and a host of other languages, but they will likely need to be added to your instance 
 of PostgreSQL.
  
 [
  54 
 ]",NA
The structure of a PL/pgSQL function ,"It 
 doesn't take much to get a PL/pgSQL function working. Here's a basic example:
  
 CREATE FUNCTION mid(varchar, integer, integer) RETURNS 
 varchar AS $$ 
  
 BEGIN
  
  
  RETURN substring($1,$2,$3); 
  
 END; 
  
 $$ 
  
 LANGUAGE plpgsql;
  
 The preceding function shows the basic elements of a PL/pgSQL function. It creates an 
 alias for the 
 substring
  built-in function called 
 mid
 . This is a handy alias to have around 
 for developers that come from Microsoft SQL Server or MySQL and are wondering what 
 happened to the 
 mid
  function. It also illustrates the most basic parameter-passing 
 strategy: parameters are not named and are accessed in the function by their relative 
 location from left to right. The 
 $$
  character in this example represents the start and end 
 of the code block. This character sequence can be arbitrary and you can use something 
 else of your choice, but this book uses 
 $$
  in all the examples.
  
 The basic elements of a PL/pgSQL function are name, parameters, return type, body, and 
 language. It can be argued that parameters are not mandatory for a function and neither 
 is the return value. This might be useful for a procedure that operates on data without 
 providing a response, but it will be prudent to return the value 
 TRUE
  to indicate that the 
 procedure succeeded.",NA
Accessing function arguments ,"Function arguments can also be passed and accessed by name, instead of just by the 
 ordinal order. By accessing the parameters by name, it makes the resulting function 
 code a little more readable. The following is an example of a function that uses named 
 parameters:
  
 CREATE FUNCTION mid(keyfield varchar, starting_point integer) 
  
  RETURNS varchar 
  
 AS 
  
 $$ 
  
 BEGIN
  
  
  RETURN substring(keyfield,starting_point); 
  
 END 
  
 $$ 
  
 LANGUAGE plpgsql;
  
 [
  55 
 ]",NA
Conditional expressions ,"Conditional expressions allow developers to control the action of the function, based on 
 a defined criteria. PostgreSQL provides the 
 CASE
  and 
 IF
  statements to execute different 
 commands based on conditions. The following is an example of the usage of a 
 CASE
  
 statement to control how a string is treated based on its value. If the value is null or 
 contains a zero-length string, it is treated the same as null:
  
 CREATE OR REPLACE FUNCTION format_us_full_name(
   
  prefix text, firstname text, 
   
  mi text, lastname text, 
  
   
  suffix text)
  
  
  RETURNS text AS 
  
 $$ 
  
 DECLARE
  
  
  fname_mi text;
  
  
  fmi_lname text;
  
  
  prefix_fmil text;
  
  
  pfmil_suffix text; 
  
 BEGIN
  
  
  fname_mi := CONCAT_WS(' ',
  
   
  CASE trim(firstname) 
  
   
  WHEN '' 
  
   
  THEN NULL 
  
   
  ELSE firstname 
  
   
  END, 
  
   
  CASE trim(mi) 
  
   
  WHEN '' 
  
   
  THEN NULL 
  
   
  ELSE mi 
  
   
  END || '.');
  
  
  fmi_lname := CONCAT_WS(' ',
  
   
  CASE fname_mi 
  
   
  WHEN '' 
  
   
  THEN NULL 
  
 [
  57 
 ]",NA
Loops with counters ,"The PL/pgSQL language provides a simple way to loop through some elements. 
  
 The following is a function that returns the 
 n
 th Fibonacci sequence number:
  
 CREATE OR REPLACE FUNCTION fib(n integer) 
  
 RETURNS INTEGER AS $$
  
 DECLARE 
  
  
  counter integer := 0;
  
  
  a integer := 0;
  
  
  b integer := 1; 
  
 BEGIN
  
  
  IF (n < 1) THEN
  
  
  RETURN 0;
  
  
  END IF;
  
  
  LOOP    
  
  
  EXIT WHEN counter = n;
  
  
  counter := counter + 1;
  
  
  SELECT  b,a+b INTO a,b;
  
   
  END LOOP;
  
  
  RETURN a; 
  
 END; 
  
 $$
  
 [
  60 
 ]",NA
Statement termination ,"In PL/pgSQL, all blocks and the statements within the blocks, must end with a 
 semicolon. The exceptions are the statements that start a block with 
 IF
  or 
 BEGIN
 . 
 Block-starting statements are not complete statements; therefore, the semicolon is 
 always after the block-ending statement, such as 
 END;
  or 
 END IF;
 .",NA
Looping through query results ,"Before we embark on this journey of query result loops, I think I should warn you, that if 
 you are using this method, you are probably 
 doing it wrong
 . This is one of the most 
 processor- and memory-intensive operations that PostgreSQL offers. There are 
 exceedingly few reasons to iterate through a result set on the database server that offset 
 this cost. I would encourage you to think hard about how to implement the same idea 
 using a VALUES list in a query, temporary table, and permanent table, or to precompute 
 the values in any way possible, in order to avoid this operation. So, do you still think you 
 have an overwhelming reason to use this technique? Okay, then read on. The following 
 is the simple version:
  
  
  FOR row IN 
  
 EXECUTE 'SELECT * FROM job_queue q WHERE NOT processed LIMIT 100' 
 LOOP 
  
  
  CASE row.process_type
  
    
  WHEN 'archive_point_of_sale'
  
     
  THEN  INSERT INTO hist_orders (...) 
  
     
  SELECT ... FROM orders 
  
      
  
  INNER JOIN order_detail ... 
  
  
    
   
  INNER JOIN item ...;
  
  
  
  WHEN 'prune_archived_orders'
  
  
   
  THEN DELETE FROM order_detail 
  
  
    
  WHERE order_id in (SELECT order_id FROM  
  
  
    
   
  hist_orders);
  
  
    
  DELETE FROM orders 
  
  
    
  WHERE order_id IN (SELECT order_id FROM  
  
  
    
   
  hist_orders);
  
  
  
  ELSE
  
  
   
  RAISE NOTICE 'Unknown process_type: %', row.process_type; 
 END;
  
  UPDATE job_queue SET processed = TRUE WHERE id = q.id; 
  
 END LOOP;
  
 [
  62 
 ]",NA
PERFORM versus SELECT,"You may have noticed a statement in the previous example that we haven't covered yet. 
 Use the 
 PERFORM
  command when you want to just discard the results of a statement. 
 Change the previous example to the following line:
  
 SELECT cs_log(""Done refreshing materialized views"");
  
 The query engine will return 
 No destination for result data
 .
  
 We can retrieve the results into variables, and then proceed to ignore the variables, but 
 that's just a little too sloppy for my taste. By using the 
 PERFORM
  statement, we have 
 indicated that ignoring the results was not accidental. We were happy with the fact that 
 the log was appended to blindly, and if it wasn't, oh well, we didn't fail to continue the 
 execution because of a log entry issue.",NA
Looping Through Arrays,"There is a very convenient loop construct called 
 FOREACH
 , which allows you to loop 
 through the elements of an array. Let's dive into an example:
  
 CREATE FUNCTION findmax(int[]) RETURNS int8 AS $$ 
 DECLARE
  
  
  max int8 := 0;
  
  
  x int; 
  
 BEGIN
  
  
  FOREACH x IN ARRAY $1
  
  
  LOOP
  
  
  IF x > max THEN
  
    
  max := x;
  
  
  END IF;
  
  
  END LOOP;
  
  
  RETURN max; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 This function is quite self-explanatory. It finds the maximum values from a given integer 
 array. The point to be noted here is that unlike a normal 
 FOR
  loop, a 
 FOREACH 
 loop can 
 only use a counter variable that is already declared. You can see that we have declared 
 x
  
 before we used it as a counter in the loop. You can run this function to see if it works:
  
 postgres=# select findmax(ARRAY[1,2,3,4,5, -1]); 
 findmax 
  
 ---------
  
  5 
  
 (1 row)
  
 [
  64 
 ]",NA
Returning a record ,"So far, all of our function examples have featured a simple scalar value in the 
 RETURN
  
 clause. In PL/pgSQL, you can also define 
 set-returning functions
  (
 SRF
 ). These 
 functions can return either a type defined by an existing table or a generic record 
 type. Let's take a look at a simple example:
  
 CREATE TABLE  names(id serial, name varchar);
  
 INSERT  INTO names(name) VALUES('John');
  
 INSERT  INTO names(name) VALUES('Martin');
  
 INSERT  INTO names(name) VALUES('Peter');
  
 CREATE OR REPLACE FUNCTION GetNames() RETURNS SETOF names AS 
  
  'SELECT * FROM names;' LANGUAGE 'sql';
  
 We just defined a very simple function, 
 GetNames()
 , which will simply return all the rows 
 from our newly defined 
 names
  table.
  
 If you run the 
 GetNames()
  function now, you will get the following output:
  
 postgres=# select GetNames();
  
  
  getnames  
  
 ------------
  
  (1,John)
  
  (2,Martin)
  
  (3,Peter) 
  
 (3 rows)
  
 You can use an SRF in place of a table or as a subquery in the 
 FROM
  clause of a query. 
 Here's an example:
  
 postgres=# select * from GetNames() where id > 2;
  
  id | name 
  
 ----+-------
  
  3 | Peter
  
 (1 row)
  
 In addition to the table types, we can also return generic types from an SRF. We can 
 change our example a little to demonstrate this. Let's define a new return type and a 
 new function:
  
 CREATE TYPE nametype AS (id int, name varchar);
  
 CREATE FUNCTION PlpgGetNames() RETURNS SETOF nametype AS
  
 $$
  
 DECLARE
  
 r nametype%rowtype;
  
 [
  65 
 ]",NA
Acting on the function's results,"The previous example showed one way to retrieve, and further process, function 
 results. The following are a few more useful ways to call a function:
  
 SELECT fib(25);
  
 SELECT (flatten_application_settings('9.08.97')).*;
  
 SELECT * FROM flatten_application_settings('9.08.97');
  
 Any of the preceding methods will create a legal field list in PostgreSQL, which, in 
 turn, can be used in any way the fields in a simple 
 SELECT
  statement on a table 
 are used.
  
 The example in the previous section used the results of the 
 flatten_application_ 
 settings()
  function, a source of data for an 
 INSERT
  statement. The following is an 
 example of how to use the same function as a data source for 
 UPDATE
 :
  
 UPDATE application_settings_new 
  
  SET full_name = flat.full_name,
  
  description  = flat.description,
  
  print_certificate = flat.print_certificate,
  
  show_advertisements = flat.show_advertisements,
  
  show_splash_screen = flat.show_splash_screen  
  
  FROM flatten_application_settings('9.08.97') flat;
  
 Using the application version as a key, we can update the records in the new table. Isn't 
 this a really handy way to keep up with the changes to the application settings, while 
 both the old and new applications are still active? I'll take any compliments in the form 
 of cash (or beer), please.",NA
Summary,"Writing functions in PostgreSQL is an extremely powerful tool. PostgreSQL functions 
 provide the ability to add functionality to the database core, in order to increase 
 performance, security, and maintainability.
  
 These functions can be written in just about any language that is available to the 
 open source community and several that are proprietary. If the language that you 
 want to write them in is not available, it can be made available quickly and easily 
 through a very robust and complete compatibility layer. In this chapter, we only 
 looked at PL/pgSQL functions.
  
 In the next chapter, we will focus more on PL/pgSQL functions, and take a look at the 
 different ways in which data can be returned using 
 OUT
  parameters and return values.
  
 [
  69 
 ]",NA
Returning Structured Data,"In the previous chapter, we saw functions that return single values. These functions 
 return either a ""scalar,"" simple type such as an integer, text, or data; or a more 
 complex type, similar to a row in the database table. In this chapter, we will expand 
 these concepts and show you how to return your data to the client in more 
 powerful ways.
  
 We will also examine the following topics:
  
 • 
  
 • 
  
 • 
  
 Differences between SETOF scalars, rows, and arrays
  
 Returning CURSORs, which are kind of ""lazy"" tables, that is, something that 
 can be used to get a set of rows, but which may not have actually evaluated 
 or fetched the rows yet, as the modern world is not about rigid table-
 structured data
  
 Ways to deal with more complex data structures, both predefined and 
 dynamically created
  
 Let's start with a simple example and then add more features and variants as we go.",NA
Sets and arrays,"Rowsets are similar to arrays in many ways, but they mainly differ in their usage. 
  
 For most data manipulations, you will want to use rowsets, as the SQL language is 
 designed to deal with them. Arrays, however, are most useful for static storage. They are 
 more complicated for client applications to use than rowsets, with usability features 
 missing, such as no simple and straightforward built-in ways to iterate over them.",NA
Returning sets ,"When you write a set returning function, there are some differences from a normal 
 scalar function. First, let's take a look at how to return a set of integers.",NA
Returning a set of integers ,"We will revisit our Fibonacci number generating function; however, this time we will 
 not just return the 
 n
 th number, but the whole sequence of numbers up to the 
 n
 th 
 number, as shown here:
  
 CREATE OR REPLACE FUNCTION fibonacci_seq(num integer)
  
  RETURNS SETOF integer AS $$ 
  
 DECLARE
  
  
  a int := 0;
  
  
  b int := 1; 
  
 BEGIN
  
  
  IF (num <= 0)
  
  
  THEN 
 RETURN
 ;
  
  
  END IF;
  
  RETURN NEXT a;
  
  LOOP
  
  EXIT WHEN num <= 1; 
  
  
 RETURN NEXT
  b;
  
   
  num = num - 1;
  
   
  SELECT b, a + b INTO a, b;
  
  
  END LOOP; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 The first difference we see, is that instead of returning a single integer value, this 
 function is defined to return a 
 SETOF
  integer.
  
 Then, if you examine the code carefully, you will see that there are two different 
 types of 
 RETURN
  statements. The first is the ordinary 
 RETURN
  function in the 
 following code snippet:
  
 IF (num <= 0)
  
  THEN RETURN;
  
 In this case, the 
 IF
  function is used to terminate the 
 fibonacci_seq
  function early, if 
 the length of the desired sequence of Fibonacci numbers is zero or less.
  
 [
  72 
 ]",NA
Using a set returning function ,"A set returning function (also known as a table function) can be used in most places 
 where a table, view, or subquery can be used. They are a powerful and flexible way to 
 return data.
  
 You can call the function in the 
 SELECT
  clause, as you do with a scalar function:
  
 postgres=# SELECT fibonacci_seq(3);
  
  fibonacci_seq 
  
 ---------------
  
  
  0
  
  
  1
  
  
  1 
  
 (3 rows)
  
 [
  73 
 ]",NA
Functions based on views ,"Creating a function based on a view definition is a very powerful and flexible way of 
 providing information to users. As an example of this, I will tell you a story of how I 
 started a simple utility view to answer the question, ""What queries are running now, 
 and which queries have been running for the longest time?"" This evolved into a 
 function based on this view, plus a few more views based on the function.
  
 The way to get all the data to answer this question in PostgreSQL is by using the 
 following query. Please note that the output is using an expanded mode of psql. 
  
 You can turn it on using the 
 \x
  meta-command:
  
 hannu=# SELECT * FROM pg_stat_activity WHERE state='active';
  
 -[ RECORD 1 ]----+-------------------------------
 -datid            | 17557 
  
 datname          | hannu 
  
 pid              | 8933 
  
 usesysid         | 10 
  
 usename          | postgres 
  
 application_name | psql 
  
 client_addr      | 
  
 client_hostname  | 
  
 client_port      | -1 
  
 backend_start    | 2013-03-19 13:47:45.920902-04 
 xact_start       | 2013-03-19 14:05:47.91225-04 
 query_start      | 2013-03-19 14:05:47.91225-04 
 state_change     | 2013-03-19 14:05:47.912253-04 
 waiting          | f 
  
 state            | active 
  
 query            | select * from pg_stat_activity 
  
  |   where state='active';
  
 [
  76 
 ]",NA
OUT parameters and records ,"Using a pre-existing type, table, or view for compound return types is a simple 
 mechanism for returning more complex structures. However, there is often a need to 
 define the return type of the function with the function itself and not depend on other 
 objects. This is especially true when managing changes to a running application; so, 
 over a period of time, two better ways to handle this have been added to PostgreSQL.
  
 [
  80 
 ]",NA
OUT parameters ,"Until now, all the functions we created used parameters that are defined as 
 IN 
 parameters. The 
 IN
  parameters are meant to just pass information into the function 
 that can be used, but not returned. Parameters can also be defined as 
 OUT
  or 
 INOUT 
 parameters if you want the function to return some information as well:
  
 CREATE OR REPLACE FUNCTION positives(
  
  
  
  INOUT a int, 
  
  
  
  INOUT b int, 
  
  
  
  INOUT c int) 
  
 AS $$ 
  
 BEGIN
  
  IF a < 0 THEN a = null; END IF;
  
  IF b < 0 THEN b = null; END IF;
  
  IF c < 0 THEN c = null; END IF; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 When we run the previous function, it only returns a single row of data with three 
 columns, as shown here:
  
 hannu=# SELECT * FROM positives(-1, 1, 2);
  
 -[ RECORD 1 ] 
  
 a | 
  
 b | 1 
  
 c | 2",NA
Returning records ,"If multiple rows of data need to be returned, a similar function that returns a set is 
 achieved by adding 
 RETURNS SETOF RECORD
 . This technique can only be used with 
 functions using the 
 INOUT
  or 
 OUT
  arguments:
  
 CREATE FUNCTION permutations(INOUT a int, 
  
  
  
  INOUT b int, 
  
  
  
  INOUT c int) 
  
 RETURNS SETOF RECORD 
  
 AS $$ 
  
 BEGIN
  
  RETURN NEXT;
  
  SELECT b,c INTO c,b; RETURN NEXT;
  
  SELECT a,b INTO b,a; RETURN NEXT;
  
 [
  81 
 ]",NA
Using RETURNS TABLE ,"You may think that if there are no visible 
 OUT
  parameters in a function declared as 
 RETURNS TABLE(...)
 , the following code might work:
  
 CREATE FUNCTION permutations2(a int, b int, c int) 
  
  RETURNS TABLE(a int, b int, c int) 
  
 AS $$ 
  
 BEGIN
  
  
  RETURN NEXT a,b,c; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 However, when we try to do it this way, we get an error:
  
 ERROR:  parameter name ""a"" used more than once 
  
 CONTEXT:  compilation of PL/pgSQL function ""permutations2"" near 
  
  line 1
  
 This error hints that the fields in the return table definition are also actually just 
 OUT
  
 parameters and the whole 
 RETURNS TABLE
  syntax is just another way to spell 
 CREATE FUNCTION f(OUT ..., OUT...) RETURNS RECORD …
 .
  
 This can be verified further by changing the input parameters, so that the definition can 
 be fed into PostgreSQL:
  
 CREATE FUNCTION permutations2(ia int, ib int, ic int)
  
  RETURNS TABLE(a int, b int, c int) 
  
 AS $$ 
  
 BEGIN
  
  
  RETURN NEXT a,b,c; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 When we try to create the function, we get the following output:
  
 ERROR:  RETURN NEXT cannot have a parameter in function with OUT 
  
  parameters 
  
 LINE 5:     RETURN NEXT a,b,c;
  
  
  ^
  
 So yes, the fields of the table in the 
 RETURNS
  definition are actually just 
 OUT
  parameters.
  
 [
  83 
 ]",NA
Returning with no predefined structure ,"Sometimes, you really need to write a function where the return structure is unknown. 
 One good thing about PostgreSQL function declarations, is that you can use the return 
 type 
 RECORD
 , which can be left undefined until the function is called:
  
 CREATE OR REPLACE FUNCTION run_a_query(query 
 TEXT)
  
  RETURNS SETOF RECORD 
  
 AS $$ 
  
 DECLARE
  
  
  retval RECORD; 
  
 BEGIN
  
  
  FOR retval IN EXECUTE query LOOP
  
    
  RETURN NEXT retval;
  
  
  END LOOP ; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 This is a function that lets a user execute a query; it is quite useless as such, but it can be 
 used as the basis for more useful functions that, for example, let users run queries only at 
 a certain time, or can be used to perform some checks on queries before running them.
  
 Simply run the following query:
  
 SELECT * FROM run_a_query('SELECT usename, usesysid FROM 
  
  pg_user');
  
 You will get the following error:
  
 ERROR:  a column definition list is required for functions 
  
  
 returning ""record"" 
  
 LINE 1: select * from run_a_query('select usename, usesysid from 
  
  pg_...
  
 To use this kind of a function, you need to tell PostgreSQL what the return values will 
 be, by adding a column definition list at the time of calling a function in the following 
 way:
  
 SELECT * FROM run_a_query('SELECT usename,usesysid FROM pg_user') 
  
  AS (""user"" text, uid int);
  
 [
  84 
 ]",NA
Returning SETOF ANY ,"There is another way to define functions that can operate on, and return, incomplete 
 type definitions: using the 
 ANY*
  pseudotypes.
  
 Let's define a function, which turns a simple one-dimensional PostgreSQL array of any 
 type into a set of rows with one element of the same type:
  
 CREATE OR REPLACE FUNCTION array_to_rows( array_in ANYARRAY )
  
  RETURNS TABLE(row_out ANYELEMENT) 
  
 AS $$ 
  
 BEGIN
  
  
  FOR i IN 1.. array_upper(array_in,1) LOOP
  
   
  row_out =  array_in[i];   
  
   
  RETURN NEXT ;
  
  
  END LOOP; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 [
  85 
 ]",NA
Variadic argument lists ,"PostgreSQL allows you to write a function with a variable number of arguments. 
  
 This is accomplished using 
 VARIADIC
 . Let's take a look at a practical example. 
 Suppose you want to limit the results of a query in your function, based on the 
 VARIADIC
  list of arguments; here's one way to do this:
  
 CREATE OR REPLACE FUNCTION get_nspc_tbls(VARIADIC arr name[]) 
 RETURNS TABLE(table_name name,id oid,nspname name)
  
 [
  87 
 ]",NA
A summary of the RETURN SETOF ,NA,NA
variants ,"You learned that you can return table-like datasets from a function using one of the 
 following:
  
 RETURNS
  
 RECORD structure
  
 INSIDE function
  
 SETOF <type>
  
 This is obtained from the type 
 definition
  
 DECLARE row variable of 
 the ROW or RECORD type
  
 ASSIGN to row variable
  
 RETURN NEXT var;
  
 SETOF <table/ 
 view>
  
 This is the same as the table or 
 view structure
  
  
 [
  88 
 ]",NA
Returning cursors,"Another method that can be used to get tabular data out of a function, is using 
 CURSOR
 .
  
 CURSOR
 , or a portal, as it is sometimes referred to in PostgreSQL documentation, is an 
 internal structure that contains a prepared query plan, ready to return rows from the 
 query. Sometimes, the cursor needs to retrieve all the data for the query at once, but 
 for many queries it does lazy fetching. For example, queries that need to scan all of the 
 data in a table, such as 
 SELECT * FROM xtable
 , only read the amount of data that is 
 needed for each 
 FETCH
  from the cursor.
  
 In plain SQL, 
 CURSOR
  is defined as follows:
  
 DECLARE mycursor CURSOR  FOR <query >;
  
 Later, the rows are fetched using the following statement:
  
 FETCH NEXT FROM  mycursor;
  
 While you can use a cursor to handle the data from a set returning function the usual way, 
 by simply declaring the cursor as 
 DECLARE mycursor CURSOR  FOR SELECT * FROM 
 mysetfunc();
 , many times it is more beneficial to have the function itself just return a 
 cursor.
  
 You will want to do this if you need different cursors based on argument values, or if you 
 need to return dynamically structured data out of a function, without defining the 
 structure when calling the function.
  
 [
  89 
 ]",NA
Iterating over cursors returned from another ,NA,NA
function ,"To wrap up our cursors' discussion, let's go through an example of returning a cursor 
 and then iterating over the returned cursor in another PL/pgSQL function:
  
 1. First, let's create a five-row table and fill it with data:
  
 CREATE TABLE fiverows(id serial PRIMARY KEY, data text); 
 INSERT INTO fiverows(data) VALUES ('one'), ('two'),
  
  
 ('three'), ('four'), ('five');
  
 2. Next, let's define our cursor returning function. This function will open a cursor 
 for a query, based on its argument and then return that cursor:
  
 CREATE FUNCTION curtest1(cur refcursor, tag text) 
  
  
 RETURNS refcursor 
  
 AS $$ 
  
 BEGIN
  
  
  OPEN cur FOR SELECT id, data || '+' || tag FROM  
  
   
  fiverows;
  
  
  RETURN cur; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 3. Next, we define a function, which uses the function we just created to open two 
 additional cursors, and then process the query results. To show that we are not 
 cheating and that the function really creates the cursors, we use the function 
 twice and iterate over the results in parallel:
  
 CREATE FUNCTION curtest2(tag1 text, tag2 text) 
  
  RETURNS SETOF fiverows 
  
 AS $$
  
 [
  91 
 ]",NA
Wrapping up of functions returning cursors ,"The pros of using cursors are as follows:
  
 • 
  
 • 
  
 Cursors are a useful tool if you don't want to always execute the query 
 and wait for the full result set before returning from a function
  
 Currently, they are also the only way to return multiple result sets out of a 
 user-defined function
  
 The cons of using cursors are as follows:
  
 • 
  
 • 
  
 They mainly work to pass data between functions on the server, and you are 
 still limited to one record set per call returned to the database client
  
 They are sometimes confusing to use, and bound and unbound cursors are 
 not always interchangeable
  
 [
  92 
 ]",NA
Other ways to work with structured data,"We now have covered the traditional ways of returning sets of structured data from 
 functions. We will now start with the more interesting part. Other methods to pass 
 around complex data structures have evolved in the world.",NA
Complex data types for the modern ,NA,NA
world – XML and JSON,"In the real world, most of the data is not in a single table and the database is not the 
 main thing that most programmers focus on. Often, they don't even think about it at all, 
 or at least they would rather not think about it.
  
 If you are a database developer who works on the database side of things, it is often 
 desirable to talk to the clients (be it web or application developers as your client, or 
 programs as database clients) in the language they speak. Currently, the two most 
 widely spoken data languages by the web applications and their developers are XML 
 and JSON.
  
 Both XML and JSON are text-based data formats, and as such, they can be easily saved 
 into fields of the type 
 text
 . PostgreSQL, being a DBMS, built for being user-
 extendable, also has extensive support for both these formats.",NA
XML data type and returning data as XML ,NA,NA
from functions,"One of the extensions added to PostgreSQL, in order to support XML data, is a native XML 
 data type. While the XML data type is largely just a text field, it differs from text in the 
 following ways:
  
 • 
  
 • 
  
 The XML stored in an XML field is checked on inserts and updates to 
 be well-formed
  
 There are support functions to produce and work with known 
 well-formed XML
  
 [
  93 
 ]",NA
Returning data in the JSON format,"In PostgreSQL 9.2, there is a new native data type for JSON values. This new support for 
 JSON followed the same growth pattern as XML. It initially started with two functions to 
 convert arrays and records to the JSON format, but in PostgreSQL 9.3, there are many new 
 functions introduced. The following is a summary of improvements in the PostgreSQL 9.3 
 JSON support:
  
 • 
  
 There are dedicated JSON operators such as 
 ->
 ,
  ->>
 , and 
 #>
  
 • 
  
 There are ten new JSON functions
  
 The JSON parser is exposed as an API. The API provides hooks for the significant parser 
 event, such as the beginning and end of objects and arrays, and providing functions to 
 handle these hooks allows a wide variety of JSON processing functions to be created.
  
 The 
 hstore
  extension has two new JSON-related functions, 
 hstore_to_ 
 json(hstore)
  and 
 hstore_to_json_loose(hstore)
 .
  
  
 You can read about the new JSON operators and the functions in the 
  
  
 official documentation at 
 http://www.postgresql.org/docs/
  
 current/static/functions-json.html
 .
  
 The 
 row_to_json(record, bool)
  function is used to convert any record to JSON, 
 and 
 array_to_json(anyarray, bool)
  is used to convert any array to JSON.
  
 The following are some simple examples of the usage of these functions:
  
 hannu=# SELECT array_to_json(array[1,2,3]);
  
 -[ RECORD 1 ]-+--------
  
 array_to_json | [1,2,3]
  
 hannu=# SELECT * FROM test;
  
 [
  96 
 ]",NA
Summary ,"The main points you learned in this chapter are that you can return multiple rows and 
 return data using 
 OUT
  parameters, as well as return multiple rows of complex data, 
 similar to a 
 SELECT
  query. You also learned that several sets of tables can be returned, 
 and you can possibly have them evaluated in a lazy manner using refcursors. 
  
 Moreover, you can return data as complex as you want using either XML or JSON.
  
 So, there really are very few reasons for not using database functions as your main 
 interaction mechanism with the database. In the next chapter, we will learn how to 
 call functions when different types of events occur in the database.
  
 [
  99 
 ]",NA
PL/pgSQL Trigger Functions,"While it is generally a good practice to keep related code together and avoid 
 hidden 
 actions as part of the main application code workflows, there are valid cases where it is a 
 good practice to add some kind of general or cross-application functionality to the 
 database using automated actions, which happen each and every time a table is modified. 
 That is, actions are part of your data model and not your application code and you want to 
 be sure that it is not possible to forget or bypass them, which is similar to how constraints 
 make it impossible to insert invalid data.
  
 The method used to add automated function calls to a table modifying event is called a 
 trigger
 . Triggers are especially useful for the cases where there are multiple different 
 client applications—possibly from different sources and using different programming 
 styles—accessing the same data using multiple different functions or simple SQL.
  
 From a standard's perspective, triggers are standardized in SQL3 (SQL1999). Triggers are 
 generally defined using an 
 event-condition-action
  (
 ECA
 ) model. Events occur in the 
 database, which trigger the invocation of a certain function if the conditions are satisfied. 
 All data actions performed by the trigger execute within the same transaction in which 
 the trigger is executing. Triggers cannot contain transaction control statements such as 
 COMMIT
  and 
 ROLLBACK
 . Triggers can be statement level or row level (more on this later in 
 the chapter).
  
 In PostgreSQL, a trigger is defined in two steps:
  
 1. Define a trigger function using 
 CREATE FUNCTION
 .
  
 2. Bind this trigger function to a table using 
 CREATE TRIGGER
 .
  
 In this chapter, we will cover the following topics:
  
 • 
  
 • 
  
 Creating a trigger and a trigger function 
  
 Taking a look at some of the use cases of triggers, such as an audit trigger, and 
 disallowing certain operations using triggers",NA
Creating the trigger function,"The trigger function's definition looks mostly like an ordinary function definition, 
 except that it has a return value type, 
 trigger
 , and it does not take any arguments:
  
 CREATE FUNCTION mytriggerfunc() RETURNS trigger AS $$ …
  
 Trigger functions are passed information about their calling environment through a 
 special 
 TriggerData
  structure, which in the case of PL/pgSQL is accessible through a 
 set of local variables. The local variables, 
 OLD
  and 
 NEW
 , represent the row in which the 
 trigger is in before and after triggering the event. Additionally, there are several other 
 local variables that start with the prefix 
 TG_
 , such as 
 TG_WHEN
  or 
 TG_TABLE_ NAME
  for 
 general context. Once your trigger function is defined, you can bind it to a specific set of 
 actions on a table.",NA
Creating the trigger,"The simplified syntax to create a user-defined 
 TRIGGER
  statement is given as follows:
  
 CREATE TRIGGER name
  
  { BEFORE | AFTER | INSTEAD OF } { event [ OR ... ] }
  
  ON table_name
  
  [ FOR [ EACH ] { ROW | STATEMENT } ]
  
  EXECUTE PROCEDURE function_name ( arguments )
  
 In the preceding code, the event is either 
 INSERT
 , 
 UPDATE
 , 
 DELETE
 , or 
 TRUNCATE
 . 
 There are a few more options which we will come back to in a later section.
  
 The 
 arguments
  variable seemingly passed to the trigger function in the trigger 
 definition are not used as arguments when calling the trigger. Instead, they are available 
 to the trigger function as a text array (
 text[]
 ) in the 
 TG_ARGV
  variable (the length of 
 this array is in 
 TG_NARGS
 ). Let's slowly start investigating how triggers and trigger 
 functions work.
  
 Starting from PostgreSQL 9.3, there is support for DDL triggers. We will learn more 
 about DDL triggers, also called 
 event triggers
 , in the next chapter.
  
 First, we will use a simple trigger example and move on to more complex examples step 
 by step.
  
 [
  102 
 ]",NA
"Working on a simple ""Hey, I'm called"" ",NA,NA
trigger ,"The first trigger we will work on simply sends a notice back to the database client 
 each time the trigger is fired and provides some feedback on its firing conditions:
  
 CREATE OR REPLACE FUNCTION notify_trigger()
  
  
  RETURNS TRIGGER AS $$ 
  
 BEGIN 
  
  
  RAISE NOTICE 'Hi, I got % invoked FOR % % % on %', 
    
  TG_NAME, 
  
    
  TG_LEVEL, 
  
    
  TG_WHEN, 
  
    
  TG_OP, 
  
    
  TG_TABLE_NAME; 
  
 END; 
  
 $$ LANGUAGE plpgsql;
  
 Next, we need a table to bind this function to the following line of code:
  
 CREATE TABLE notify_test(i int);
  
 Now we are ready to define the trigger. As this is a simple example, we define a 
 trigger which is invoked on 
 INSERT
  and calls the function once on each row:
  
 CREATE TRIGGER notify_insert_trigger
  
  
  AFTER INSERT ON notify_test
  
  
  FOR EACH ROW 
  
 EXECUTE PROCEDURE notify_trigger();
  
 Let's test this out:
  
 postgres=# INSERT INTO notify_test VALUES(1),(2); 
  
 NOTICE:  Hi, I got notify_insert_trigger invoked FOR ROW AFTER INSERT 
 on notify_test 
  
 ERROR:  control reached end of trigger procedure without RETURN 
 CONTEXT:  PL/pgSQL function notify_trigger()
  
 Hmm, it seems we need to return something from the function even though it is not 
 needed for our purposes. The function definition says 
 CREATE FUNCTION … RETURNS 
 but we definitely cannot return a trigger from a function.
  
 Let's get back to the documentation. OK, here it is. The trigger needs to return a 
 value of the 
 ROW
  or 
 RECORD
  type and it is ignored in the 
 AFTER
  triggers. 
  
 [
  103 
 ]",NA
The audit trigger ,"One of the most common uses of triggers is to log data changes to tables in a 
 consistent and transparent manner. When creating an audit trigger, we first must 
 decide what we want to log.
  
 A logical set of things that can be logged are who changed the data, when the data was 
 changed, and which operation changed the data. This information can be saved in the 
 following table:
  
 CREATE TABLE audit_log (
  
  username text, -- who did the change
  
  event_time_utc timestamp, -- when the event was recorded 
 table_name text, -- contains schema-qualified table name 
 operation text, -- INSERT, UPDATE, DELETE or TRUNCATE 
 before_value json, -- the OLD tuple value
  
  after_value json -- the NEW tuple value 
  
 );
  
 [
  106 
 ]",NA
Disallowing DELETE ,"What if our business requirements are such that the data can only be added and 
 modified in some tables, but not deleted?
  
 One way to handle this will be to just revoke the 
 DELETE
  rights on these tables from all 
 the users (remember to also revoke 
 DELETE
  from 
 PUBLIC
 ), but this can also be achieved 
 using triggers because of reasons such as auditing or returning custom exception 
 messages.
  
 A generic 
 cancel
  trigger can be written as follows:
  
 CREATE OR REPLACE FUNCTION cancel_op() 
  
  
  RETURNS TRIGGER AS $$ 
  
 BEGIN 
  
  
  IF TG_WHEN = 'AFTER' THEN 
  
    
  RAISE EXCEPTION 'YOU ARE NOT ALLOWED TO % ROWS IN %.%', 
    
  TG_OP, TG_TABLE_SCHEMA, TG_TABLE_NAMENAME; 
  
  
  END IF; 
  
  
  RAISE NOTICE '% ON ROWS IN %.% WON'T HAPPEN', 
  
    
  TG_OP, TG_TABLE_SCHEMA, TG_TABLE_NAMENAME; 
  
  
  RETURN NULL; 
  
 END; 
  
 $$ LANGUAGE plpgsql; 
  
 [
  109 
 ]",NA
Disallowing TRUNCATE,"You may have noticed that the preceding trigger can easily be bypassed for 
 DELETE
  if you 
 delete everything using 
 TRUNCATE
 .
  
 While you cannot simply skip 
 TRUNCATE
  by returning 
 NULL
  as opposed to the row-
 level 
 BEFORE
  triggers, you can still make it impossible by raising an error if 
 TRUNCATE
  
 is attempted. Create an 
 AFTER
  trigger using the same function as the one used 
 previously for 
 DELETE
 :
  
 CREATE TRIGGER disallow_truncate 
  
  AFTER TRUNCATE ON delete_test1 
  
  FOR EACH STATEMENT 
  
 EXECUTE PROCEDURE cancel_op(); 
  
 Here you are, without 
 TRUNCATE
 :
  
 postgres=# TRUNCATE delete_test1; 
  
 ERROR:  YOU ARE NOT ALLOWED TO TRUNCATE ROWS IN public.delete_test1 
  
 Of course, you can also raise the error in a 
 BEFORE
  trigger, but in that case you will 
 need to write your own unconditional raise-error trigger function instead of 
 cancel_op()
 .",NA
Modifying the NEW record,"Another form of auditing that is frequently used is to log information in fields in the 
 same row as the data. As an example, let's define a trigger that logs the time and the 
 active user in the 
 last_changed_at
  and 
 last_changed_by
  fields at each 
 INSERT
  and 
 UPDATE
  trigger. In the row-level 
 BEFORE
  triggers, you can modify what actually gets 
 written by changing the 
 NEW
  record. You can either assign values to some fields or even 
 return a different record with the same structure. For example, if you return 
 OLD 
 from 
 the 
 UPDATE
  trigger, you effectively make sure that the row can't be updated.",NA
The timestamping trigger,"To form the basis of our audit logging in the table, we start by creating a trigger that sets 
 the user who made the last change and when the change occurred:
  
 CREATE OR REPLACE FUNCTION changestamp() 
  
  RETURNS TRIGGER AS $$ 
  
 BEGIN 
  
  NEW.last_changed_by = SESSION_USER; 
  
  NEW.last_changed_at = CURRENT_TIMESTAMP; 
  
 [
  111 
 ]",NA
The immutable fields trigger ,"When you are depending on the fields in the rows as part of your audit record, you need 
 to ensure that the values reflect reality. We were able to make sure that the 
 last_ 
 changed_*
  fields always contain the correct value, but what about the 
 created_by 
 and 
 created_at
  values? These can be easily changed in later updates, but they should never 
 change. Even initially, they can be set to false values, since the default values can be easily 
 overridden by giving any other value in the 
 INSERT
  statement.
  
 [
  112 
 ]",NA
Controlling when a trigger is called,"While it is relatively easy to perform trigger actions conditionally inside the 
  
 PL/pgSQL trigger function, it is often more efficient to skip invoking the trigger 
 altogether. The performance effects of firing a trigger are not generally noticed when 
 only a few events are fired. However, if you are bulk loading data or updating large 
 portions of your table, the cumulative effects can certainly be felt. To avoid the 
 overhead, it's best to call the trigger function only when it is actually needed.
  
 There are two ways to narrow down when a trigger will be called in the 
 CREATE 
 TRIGGER
  command itself.
  
 So, use the same syntax once more but with all the options this time:
  
 CREATE TRIGGER name
  
  { BEFORE | AFTER | INSTEAD OF } { event [ OR event ... ] }
  
  [ OF column_name  [ OR column_name ... ] ] ON table_name
  
  [ FOR [ EACH ] { ROW | STATEMENT } ]
  
  [ WHEN ( condition ) ]
  
  EXECUTE PROCEDURE function_name ( arguments )
  
 You can use the 
 WHEN
  clause to only fire a trigger based on a condition (such as a certain 
 time of the day) or only when certain fields are updated. We will now take a look at a 
 couple of examples.",NA
Conditional triggers,"A flexible way to control triggers is to use a generic 
 WHEN
  clause that is similar to 
 WHERE
  in SQL queries. With a 
 WHEN
  clause, you can write any expression, except a 
 subquery, that is tested before the trigger function is called. The expression must 
 result in a Boolean value, and if the value is 
 FALSE
  (or 
 NULL
 , which is automatically 
 converted to 
 FALSE
 ), the trigger function is not called.
  
 [
  114 
 ]",NA
Triggers on specific field changes ,"Another way to control when a trigger is fired is using a list of columns. In the 
 UPDATE
  triggers, you can specify one or more comma-separated columns to tell 
 PostgreSQL that the trigger function should only be executed if any of the listed 
 columns change.
  
 It is possible to construct the same conditional expression with a 
 WHEN
  clause, but the 
 list of columns has cleaner syntax:
  
 WHEN(
  
  NEW.column1 IS DISTINCT FROM OLD.column1 
 OR
  
  NEW.column2 IS DISTINCT FROM OLD.column2)
  
 A common example of how this conditional expression is used is raising an error each 
 time someone tries to change a primary key column. The 
 IS DISTINCT FROM 
 function 
 makes sure that the trigger is only executed when the new value of 
 column1 
 is different 
 from the old value. This can be easily done by declaring an 
 AFTER
  trigger using the 
 cancel_op()
  trigger function (defined previously in this chapter), as follows:
  
 CREATE TRIGGER disallow_pk_change
  
  
  AFTER UPDATE OF id ON table_with_pk_id
  
  
  FOR EACH ROW 
  
 EXECUTE PROCEDURE cancel_op();
  
 postgres=# INSERT INTO new_tasks DEFAULT VALUES; 
 INSERT 0 1 
  
 packt=# SELECT * FROM new_tasks ;
  
  id | sample 
  
 ----+--------
  
  
  1 | 
  
 (1 row)
  
 packt=# UPDATE new_tasks SET id=0 where id=1; 
  
 ERROR:  YOU ARE NOT ALLOWED TO UPDATE ROWS IN 
 public.new_tasks",NA
Visibility ,"Sometimes, your trigger functions might run into the 
 Multiversion Concurrency 
 Control
  (
 MVCC
 ) visibility rules of how PostgreSQL's system interacts with changes to 
 data.
  
 A function declared as 
 STABLE
  or 
 IMMUTABLE
  will never see changes applied to the 
 underlying table by the previous triggers.
  
 [
  116 
 ]",NA
Most importantly – use triggers cautiously!,"Triggers are an appropriate tool for use in database-side actions, such as auditing, 
 logging, enforcing complex constraints, and even replication (several logical 
 replication systems such as Slony are based on triggers used in production). However, 
 for most application logic, it is much better to avoid triggers as they can lead to really 
 weird and hard-to-debug problems. As a good practice, follow the rules provided in 
 the following table:
  
 Rule
  
 Description
  
 Rule 1
  
 Do not change data in the primary key, foreign 
 key, or unique key columns of any table
  
 Rule 2
  
 Do not update records in the table that you read 
 during the same transaction
  
 Rule 3
  
 Do not aggregate over the table you are updating
  
 Rule 4
  
 Do not read data from a table that is updated 
 during the same transaction
  
 [
  117 
 ]",NA
Variables passed to the PL/pgSQL ,NA,NA
TRIGGER function ,"The following is a complete list of the variables available to a trigger function written in 
 PL/pgSQL:
  
 OLD
 , 
 NEW
  
 RECORD
  
 This records the before and after images of the row on 
 which the trigger is called. 
 OLD
  is unassigned for 
 INSERT
  and 
 NEW
  is unassigned for 
 DELETE
 .
  
 Both are 
 UNASSIGNED
  in statement-level triggers.
  
 TG_NAME
  
 name
  
 This denotes the name of the trigger (this and 
 following from the trigger definition).
  
 TG_WHEN
  
 text
  
 BEFORE
 , 
 AFTER
 , or 
 INSTEAD OF
  are the possible 
 values of the variable.
  
 TG_LEVEL
  
 text
  
 ROW
  or 
 STATEMENT
  are the possible values of the 
 variable.
  
 TG_OP
  
 text
  
 INSERT
 , 
 UPDATE
 , 
 DELETE
 , or 
 TRUNCATE
  are the 
 possible values of the variable..
  
 TG_RELID
  
 oid
  
 This denotes the 
 OID
  of the table on which the trigger is 
 created.
  
 TG_TABLE_NAME
  
 name
  
 This denotes the name of the table (the old spelling 
 TG_RELNAME
  is deprecated but still available).
  
 TG_TABLE_ 
 SCHEMA
  
 name
  
 This denotes the schema name of the table.
  
 TG_NARGS
 , 
 TG_ 
 ARGV[]
  
 Int
 , 
  
 text[]
  
 This denotes the number of arguments and the array of 
 the arguments from the trigger definition.
  
 TG_TAG
  
 text
  
 This is used in DDL or event triggers. This variable 
 contains the name of the command that resulted in 
 the trigger invocation. More information on this in 
 the next chapter.
  
 You can read more about the variables at 
 http://www.postgresql.org/docs/ 
 current/static/plpgsql-trigger.html
 .
  
 [
  118 
 ]",NA
Summary,"A trigger is a binding of a set of actions to certain operations performed on a table or 
 view. This set of actions is defined in a special trigger function which is distinguished by 
 specifying the type of the returned value to be of a special pseudotype trigger. So, each 
 time an operation (
 INSERT
 , 
 UPDATE
 , 
 DELETE
 , or 
 TRUNCATE
 ) is performed on the table, this 
 trigger function is called by the system.
  
 It can be executed either 
 for each row
  or 
 for each statement
 . If executed for each row 
 (row-level trigger), the function is passed special variables such as 
 OLD
  and 
 NEW
 . 
  
 This will contain the row's contents, as it is currently in the database (
 OLD
 ) and as it is the 
 moment the trigger function is called (
 NEW
 ). Where the 
 OLD
  or 
 NEW
  value is missing, it is 
 passed as undefined. If executed once per statement (the statement-level trigger), both 
 OLD
  and 
 NEW
  are unassigned for all the operations.
  
 The trigger function for row-level triggers on 
 INSERT
 , 
 UPDATE
 , and 
 DELETE
  can be set to 
 execute either 
 BEFORE
  or 
 AFTER
  the operation on a table and can be set to execute the 
 INSTEAD OF
  operation on view.
  
 The trigger function for statement-level triggers on 
 INSERT
 , 
 UPDATE
 , and 
 DELETE
  can be 
 set to execute either 
 BEFORE
  or 
 AFTER
  the operation on both tables and views.
  
 While 
 TRUNCATE
  is logically a special, non-MVCC form of a ""delete all"" statement, no 
 ON 
 DELETE
  triggers will fire in the case of 
 TRUNCATE
 . Instead, you can use a special 
 ON 
 TRUNCATE
  trigger on the same table. Only statement-level on 
 TRUNCATE
  triggers are 
 possible. While you cannot skip statement triggers by returning 
 NULL
 , you can 
 raise an 
 exception
  and abort the transaction.
  
 It is also not possible to define any 
 ON TRUNCATE
  triggers on views.
  
 In the next chapter, we will take a look at the new PostgreSQL event trigger feature. 
 Event triggers allow you to execute triggers when a 
 Data Definition Language 
 (
 DDL
 ) 
 statement is executed on a table.
  
 [
  119 
 ]",NA
PostgreSQL Event Triggers,"PostgreSQL 9.3 introduced a special type of triggers to complement the trigger 
 mechanism we discussed in the preceding chapter. These triggers, are database-
 specific and are not attached to a specific table. Unlike regular triggers, event triggers 
 capture DDL events. Event triggers can be the 
 BEFORE
  and 
 AFTER
  triggers, and the 
 trigger function can be written in any language except SQL. Other popular database 
 vendors such as Oracle and SQL Server also provide a similar functionality.
  
 One can think of several use cases for DDL triggers. The most popular one among the 
 DBAs normally is to do an audit trail. You, as a DBA, might want to audit the users and 
 the DDL commands. Schema changes, in a production environment, should be done very 
 carefully; hence, the need for an audit trail is apparent. Event triggers are disabled in the 
 single user mode and can only be created by a superuser.
  
 In this chapter, we will cover the following topics:
  
 • 
  
 Use cases for event triggers
  
 • 
  
 A full audit trail example using PL/pgsql
  
 • 
  
 Preventing schema changes using event triggers",NA
Use cases for creating event triggers,"In addition to an audit trail, the following can be valid use cases for an event trigger:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Replicating DDL changes to a remote location 
  
 Cascading a DDL 
  
 Preventing changes to tables, except during a predefined window 
 Providing limited DDL capability to developers / support staff using 
 security definer functions",NA
Creating event triggers,"Event triggers are created using the 
 CREATE EVENT TRIGGER
  command. Before you can 
 create an event trigger, you need a function that the trigger will execute. This function 
 must return a special type called 
 EVENT_TRIGGER
 . If you happen to define multiple 
 event triggers, they are executed in the alphabetical order of their names.
  
 Currently, event triggers are supported on three events, as follows:
  
 • 
  
 • 
  
 • 
  
 ddl_command_start
 : This event occurs just before a 
 CREATE
 , 
 ALTER
 , or 
 DROP 
 DDL
  command is executed 
  
 ddl_command_end
 : This event occurs just after a 
 CREATE
 , 
 ALTER
 , or 
 DROP 
 command has finished executing 
  
 sql_drop
 : This event occurs just before the 
 ddl_command_end
  event for the 
 commands that drop database objects
  
 You can specify a 
 WHEN
  clause with the 
 CREATE EVENT TRIGGER
  command, so that the 
 event is fired only for the specified commands.
  
 The event trigger PL/pgSQL functions have access to the following new variables 
 introduced in 9.3:
  
 • 
  
 TG_TAG
 : This variable contains the ""tag"" or the command for which the trigger 
 is executed. This variable does not contain the full command string, but just a 
 tag such as 
 CREATE TABLE
 , 
 DROP TABLE
 , 
 ALTER TABLE
 , and so on.
  
 [
  122 
 ]",NA
Creating an audit trail ,"Let's now take a look at the complete example of an event trigger that creates an 
 audit trail of some DDL commands in the database:
  
 CREATE TABLE track_ddl 
  
 (
  
  
  event text, 
  
  
  command text, 
  
  
  ddl_time timestamptz, 
  
  
  usr text 
  
 );
  
 CREATE OR REPLACE FUNCTION track_ddl_function() 
  
 RETURNS event_trigger 
  
 AS 
  
 $$ 
  
 BEGIN
  
  
  INSERT INTO track_ddl values(tg_tag, tg_event, now(), 
 session_user);
  
  RAISE NOTICE 'DDL logged'; 
  
 END 
  
 $$ LANGUAGE plpgsql SECURITY DEFINER;
  
 CREATE 
 EVENT 
 TRIGGER 
 track_ddl_event 
 ON 
 ddl_command_start WHEN TAG IN ('CREATE TABLE', 'DROP 
 TABLE', 
 'ALTER 
 TABLE') 
 EXECUTE 
 PROCEDURE 
 track_ddl_function();
  
 CREATE TABLE event_check(i int);
  
 SELECT * FROM track_ddl;
  
 -[ RECORD 1 ]------------------------
  
 event    | CREATE TABLE
  
 [
  123 
 ]",NA
Preventing schema changes,"Another common use case for event triggers is to prevent the execution of certain 
 commands based on a specific condition. If you only want to stop users from executing 
 some commands, you can always revoke the privileges using more conventional means. 
 The triggers may come in handy if you want to do this based on a certain condition, let's 
 say, time of the day:
  
 CREATE OR REPLACE FUNCTION abort_create_table_func()
  
 RETURNS event_trigger
  
 AS 
  
 $$
  
 DECLARE
  
  current_hour int := extract(hour from now());
  
 BEGIN
  
  if current_hour BETWEEN 9 AND 16
  
  then
  
 [
  124 
 ]",NA
A roadmap of event triggers,"As you can see, the current implementation of event triggers in PL/pgsql and 
 PostgreSQL 9.4 is rather limited. However, there are more changes and features 
 planned for the upcoming releases that will expand the scope, in which event 
 triggers will become more useful. Here are the highlights of what we can look 
 forward to in the future:
  
 • 
  
 Access to more information
 : More 
 TG_*
  variables are going to be available, in 
 order to provide more information about the command that is running and on 
 which object it is running. We can look forward to the following variables in 
 future PostgreSQL versions:
  
 °
  
 TG_OBJECTID
  
 °
  
 TG_OBJECTNAME
  
 °
  
 TG_SCHEMANAME
  
 °
  
 TG_OPERATION
  
 °
  
 TG_OBTYPENAME
  
 °
  
 TG_CONTEXT
  
 [
  125 
 ]",NA
Summary,"Event triggers are new in PostgreSQL 9.3, and the community is still working on various 
 features in order to make them more useful. You can use these triggers for various 
 purposes, including auditing DDL commands and making local customized policies 
 regarding the execution of DDL commands in the database. Event triggers are disabled 
 in single user mode.
  
 Event triggers are created using the 
 CREATE EVENT TRIGGER
  command, and they 
 execute a function that returns a special type called 
 EVENT_TRIGGER
 . There are three 
 types of events that are currently supported: 
 ddl_command_start
 , 
 ddl_command_ 
 end
 , and 
 sql_drop
 . You can limit a trigger execution by a tag using a 
 WHEN
  clause, if you 
 want to fire it for specific commands only.
  
 Currently, there are two new variables available in the event trigger function called 
 TG_TAG
  and 
 TG_EVENT
  that provide information about the tag and the event of the 
 trigger. Future releases of PostgreSQL will expose more variables that make it possible 
 to audit complete information about firing DDL, including objects and the command 
 string.
  
 In the next chapter, we will discuss the differences between restricted and 
 unrestricted languages and we will take a look at the practical examples of both.
  
 [
  126 
 ]",NA
Debugging PL/pgSQL,"This chapter is entirely optional. Since you have only produced the highest quality 
 bug-free code using the best possible algorithms, this text is probably a waste of your 
 time. Of course, your functions parse perfectly on the first try. Your views show exactly 
 what they should—according to the enviously complete business and technical 
 documentation that you wrote last month. There is no need for version control on your 
 procedures, as there has only ever been a Version 1.
  
 Since you are still reading this, I'm sure that you're a whole lot more like me. I spend 
 about 10 percent of my time writing new code and about 90 percent of it editing the 
 mistakes and oversights that I (and others) made in the first 10 percent. In fact, it can be 
 argued that no new code is ever written at all. Actually, a more accurate description of the 
 process is that a dumb assertion is made and then it is edited until the customer can no 
 longer stand the Quality Assurance (QA) process. We then ship the result in the hopes of 
 being useful to the end user. Was that too much of reality for you? Sorry.
  
 The objective of this chapter is to make you faster at making mistakes. As a by-product, 
 you will also learn how to diagnose and fix them at an alarming rate. The net effect of this, 
 we are hoping, is that your boss will assume you wrote it correctly the first time. This is, 
 of course, a lie but a very useful one.
  
 This concept is critical to agile software development. In this philosophy, it is called 
 ""prototyping."" The idea is to create a feature quickly and demonstrate it as a 
 conversation point, rather than trying to produce an entire system (presumably 
 perfectly) from conceptual documentation. Other authors refer to it as ""failing quickly."" 
 It recognizes the fact that the first three or four development iterations will probably 
 not be acceptable to the customer and shouldn't be advertised as final until some 
 discussion has taken place.
  
 This process effectively requires the developer to ""live"" in the debugger. The 
 developer continually changes the outputs and routines until the desired effect is 
 achieved. PostgreSQL has a wonderful set of debugging tools available to help you 
 fix your mess. Let me show you how they work.",NA
Manual debugging with RAISE NOTICE ,"Here's the first promised example:
  
 CREATE OR REPLACE FUNCTION format_us_full_name_debug(
  
  
  prefix text, 
  
  
  firstname text, 
  
  
  mi text, 
  
  
  lastname text, 
  
  
  suffix text)
  
  
  RETURNS text AS 
  
 $BODY$ 
  
 DECLARE
  
  
  fname_mi text;
  
  
  fmi_lname text;
  
  
  prefix_fmil text;
  
  
  pfmil_suffix text; 
  
 BEGIN 
  
  
  fname_mi := CONCAT_WS(' ', CASE trim(firstname) WHEN ''  
  
  
 THEN NULL ELSE firstname END, CASE trim(mi) WHEN ''  
  
  THEN 
 NULL ELSE mi END || '.');
  
  
  RAISE NOTICE 'firstname mi.: %', fname_mi;
  
  
  fmi_lname := CONCAT_WS(' ', CASE fname_mi WHEN ''  
  
  
  THEN NULL ELSE fname_mi END,CASE trim(lastname) WHEN ''  
  
  
 THEN NULL ELSE lastname END);
  
  
  RAISE NOTICE 'firstname mi. lastname: %', fmi_lname;
  
  
  prefix_fmil := CONCAT_WS('. ', CASE trim(prefix) WHEN ''  
  
  
 THEN NULL ELSE prefix END, CASE fmi_lname WHEN ''  
  
  
  THEN NULL ELSE fmi_lname END);
  
  
  RAISE NOTICE 'prefix. firstname mi lastname: %', prefix_fmil;
  
  
 pfmil_suffix := CONCAT_WS(', ', CASE prefix_fmil WHEN ''  
  
  
 THEN NULL ELSE prefix_fmil END, CASE trim(suffix) WHEN ''  
  
  
 THEN NULL ELSE suffix || '.' END);
  
  
  RAISE NOTICE 'prefix. firstname mi lastname, suffix.: %',  
  
  
 pfmil_suffix;
  
  
  RETURN pfmil_suffix; 
  
 END; 
  
 $BODY$
  
  
  LANGUAGE plpgsql VOLATILE;
  
 In this example, we format a person's full name using the magic of the 
 NULL
  propagation.
  
 [
  128 
 ]",NA
Throwing exceptions ,"The 
 RAISE
  command takes several operators except 
 NOTICE
 . The command will also 
 throw exceptions that are intended for the calling code to catch. The following is an 
 example of how to create an exception:
  
 CREATE OR REPLACE FUNCTION validate_us_zip(zipcode TEXT) 
  
  RETURNS boolean 
  
 AS $$ 
  
 DECLARE
  
  
  digits text; 
  
 BEGIN
  
  
  -- remove anything that is not a digit (POSIX compliantly,  
  
  please)
  
  
  digits := (SELECT regexp_replace  
  
  
  (zipcode,'[^[:digit:]]','','g'));
  
  IF digits = '' THEN
  
  RAISE EXCEPTION 'Zipcode does not contain any digits  
  
  --> 
 %', digits USING HINT = 'Is this a US zip code?',  
  
  
 ERRCODE = 'P9999';
  
 [
  130 
 ]",NA
Logging to a file ,"The 
 RAISE
  statement expression can be sent to a log file using 
 log_min_messages
 . 
 This parameter is set in 
 postgresql.conf
 . The valid values are 
 debug5
 , 
 debug4
 , 
 debug3
 , 
 debug2
 , 
 debug1
 , 
 info
 , 
 notice
 , 
 warning
 , 
 error
 , 
 log
 , 
 fatal
 , and 
 panic
 .
  
 The default logging level is dependent on the packaging system. On Ubuntu, the default 
 logging level is 
 info
 . The logging levels correspond to the same expressions for the 
 RAISE
  statement. As a developer, you can raise any of the messages that are available 
 and have them recorded in the log file for analysis later.
  
 [
  132 
 ]",NA
The advantages of RAISE NOTICE,"Using the 
 RAISE NOTICE
  form of debugging has several advantages. It can be used 
 easily and repeatedly with scripts for regression testing. This is very easily 
 accomplished with the command-line client. Consider the following statement:
  
 psql -qtc ""SELECT format_us_full_name_debug 
  
  ('Mr','Kirk','L.','Roybal','Author');""
  
 The preceding statement produces the following output to stdout:
  
 NOTICE:  firstname mi.: Kirk L..
  
 NOTICE:  firstname mi. lastname: Kirk L.. Roybal
  
 NOTICE:  prefix. firstname mi lastname: Mr. Kirk L.. Roybal
  
 NOTICE:  prefix. firstname mi lastname, suffix.: Mr. Kirk L.. Roybal, 
 Author.
  
  Mr. Kirk L.. Roybal, Author.
  
 Because a constant set of input parameters should always produce a known output, it is 
 very easy to use command-line tools in order to test for expected outputs. When you are 
 ready to deploy your newly modified code to the production system, run your 
 command-line tests to verify that all of your functions still work as expected.
  
 RAISE NOTICE
  is included with the product and requires no installation. Its 
 advantage will become clearer later in the chapter where the rather painful 
 installation procedure for the PL/pgSQL debugger is explained.
  
 The 
 RAISE
  statement is easy to understand. The syntax is straightforward, and it is 
 well documented at 
 http://www.postgresql.org/docs/current/static/ 
 plpgsql-errors-and-messages.html
 .
  
 RAISE
  works in any development environment and has been around for a very long 
 time, in almost every version of PostgreSQL on every operating system. I have used it 
 with pgAdmin3, phpPgAdmin, as well as the command-line tool psql.
  
 These attributes, taken together, make 
 RAISE
  a very attractive tool for small -
 scale debugging.
  
 [
  133 
 ]",NA
The disadvantages of RAISE NOTICE,"Unfortunately, there are some disadvantages to using this method of debugging. 
  
 The primary disadvantage is that you need to remove the 
 RAISE
  statements when they 
 are no longer necessary. The messages tend to clutter up the psql command-line client 
 and are generally annoying to other developers. The log may fill up quickly with useless 
 messages from previous debugging sessions. The 
 RAISE
  statements need to be written, 
 commented out, and restored when needed. They may not cover the actual bug being 
 sought. They also slow down the execution of the routine.
  
 You will also find 
 Chapter 13
 , 
 Publishing Your Code as PostgreSQL
 , quite informative. 
 This chapter includes some examples (and an extremely handy way to install them) that 
 will be useful here in this part of the book. The examples will be shown in the text of this 
 chapter as well, but they will be quite a bit easier for you to install as an extension.",NA
Visual debugging,"The PL/pgSQL debugger is a project hosted on PostgreSQL Git that provides a 
 debugging interface into PostgreSQL Version 8.2 or higher. The project is hosted at 
 http://git.postgresql.org/gitweb/?p=pldebugger.git;a=summary
 .
  
 The PL/pgSQL debugger lets you step through the PL/pgSQL code, set and clear 
 breakpoints, view and modify variables, and walk through the call stack.
  
 As you can see from the description, the PL/pgSQL debugger can be quite a handy little 
 tool to have in your arsenal.",NA
Installing the debugger,"OK, now we will move past the glamour and actually get the debugger running on our 
 system. If you install PostgreSQL with one of the packages that contains the debugger, the 
 installation is pretty simple. Otherwise, you will need to build it from the source.
  
 A detailed discussion of how to install the PL/pgSQL debugger from the source is beyond 
 the scope of this book, but I will just list the set of steps to install the debugger quickly. 
 The best way to build the source will be to pull the latest version from the Git repository 
 and follow the 
 README
  file in the directory. If you want to get started with it quickly and 
 you have a Windows machine available, the simplest way to use the debugger is using the 
 PostgreSQL Windows installer from 
 http://www. 
 postgresql.org/download/windows/
 .
  
 [
  134 
 ]",NA
Installing the debugger from the source,"Here is a set of simple steps that should get you up and running if you want to install from 
 the source:
  
 1. Clone the Git repository as shown. You can view the repository at 
  
  
 http://git.postgresql.org/gitweb/?p=pldebugger.git;a=summary
 :
  
 git clone http://git.postgresql.org/git/pldebugger.git
  
 2. Copy this 
 pldebugger/
  directory to 
 contrib/
  in your PostgreSQL source tree.
  
 3. Run 
 make && make install
  in the 
 pldebugger
  folder.
  
 4. Modify the 
 shared_preload_libraries
  configuration option in 
  
 postgresql.conf
  as follows:
  
 shared_preload_libraries = '$libdir/plugin_debugger'
  
 5. Restart PostgreSQL.
  
 6. Create the debugger extension:
  
 CREATE EXTENSION pldbgapi;
  
 This should install the PL debugger and you should be able to use it with pgAdmin3.
  
  
 You can also install PostgreSQL using EnterpriseDB's one-click installers 
  
  
 for most platforms, which also includes the PL debugger at 
 http://www.
  
 enterprisedb.com/products-services-training/pgdownload
 .",NA
Installing pgAdmin3,"The PL/pgSQL debugger module works with pgAdmin3. You don't need to perform 
 special steps with the installation of pgAdmin3 for the debugger to function. Install it as 
 usual from your package manager on the platform that you are using. For Ubuntu 10.04 
 LTS, the command is as follows:
  
 sudo apt-get install pgadmin3
  
 [
  135 
 ]",NA
Using the debugger,"When the debugger is available for a particular database, it can be seen in the context 
 menu when you right-click on a PL/pgSQL function. We have already created some of the 
 debuggers in the earlier part of this chapter. Using 
 format_us_full_name
  as an example, 
 right-click on it and navigate to 
 Debugging
  | 
 Debug
 :
  
  
 As a result, you will see the following dialog:
  
  
 [
  136 
 ]",NA
The advantages of the debugger,"The PL/pgSQL debugger does not require any resources on the server when not actually 
 in use. Because it is invoked manually from within pgAdmin3, it is not resident in memory 
 until it is actually called upon. This architecture does not require any background 
 processes or additional daemons for the sake of debugging.
  
 Also, the PL/pgSQL debugger does not require any special ""calling"" functions to be 
 written in order to invoke the debugging process. There are no errors to trap and no 
 tables of error codes to interpret. Everything necessary to the debugging process is 
 available in a simple window.
  
 If you connect to your database as a superuser, you also have the ability to set a global 
 break point. This break point can be set on any function or trigger and it will stop the next 
 time any code path calls the function. This is particularly useful if you want to debug your 
 functions or triggers in the context of your entire running application.
  
 The greatest advantage of the PL/pgSQL debugger is that it does not require any 
 special rigging in the functions that are being debugged. 
  
 [
  137 
 ]",NA
The disadvantages of the debugger,"As you have become painfully aware, the installation of the debugger leaves a lot to be 
 desired. This debugger has not become very popular in the PostgreSQL community at 
 large because of the rather large learning curve involved, and that's just to get it 
 installed.
  
 This form of debugging is meant for personal productivity while actively developing 
 functions. It does not work well as an automation tool.",NA
Summary,"The debugging methods that we have seen in this chapter are designed to be used in 
 conjunction with one another. They complement each other at different points in the 
 development process. Where debugging using the PL/pgSQL debugger is highly effective 
 while editing an existing (hopefully well-written) function, other forms of debugging 
 may be better suited to the quality assurance or automated data processing 
 applications.
  
 Because the PL/pgSQL debugger is meant to be a visual tool to work within pgAdmin3, it 
 is possible that the developer may want to forego the visual debugger in the interest of 
 some other feature.
  
 In the next chapter, we will take a look at how to write some advanced functions in C.
  
 [
  138 
 ]",NA
Using Unrestricted ,NA,NA
Languages,"You may have noticed, that some of the PLs in PostgreSQL can be declared as 
 untrusted. They all end in the letter 
 u
  to remind you that they are untrusted each 
 time you use them to create a function. Unrestricted languages allow you to do things 
 that restricted or trusted languages are not allowed to do; for example, interacting 
 with the environment and creating files and opening sockets. In this chapter, we will 
 look at some examples in detail.
  
 This untrustedness brings up many questions:
  
 • 
  
 • 
  
 • 
  
 Does being untrusted mean that such languages are somehow inferior to 
 trusted ones?
  
 Can I still write an important function in an untrusted language? 
 Will they silently eat my data and corrupt the database?
  
 The answers are no, yes, and maybe respectively. Let's now discuss these 
 questions in order.",NA
Are untrusted languages inferior to ,NA,NA
trusted ones?,"No, on the contrary, these languages are untrusted in the same way that a sharp knife is 
 untrusted and should be kept out of the reach of very small children, unless there is 
 adult supervision. They have extra powers that ordinary SQL, or even the trusted 
 languages (such as PL/pgSQL) and trusted variants of the same language (PL/Perl 
 versus PL/PerlU) don't have.",NA
Can you use untrusted languages for ,NA,NA
important functions? ,"Absolutely! Sometimes, it may be the only way to accomplish some tasks from inside the 
 server. Performing simple queries and computations should do nothing harmful to your 
 database, and neither should connecting to the external world for sending e-mails, 
 fetching web pages, or performing SOAP requests. However, be careful about performing 
 operations that may cause delays and even queries that get stuck, but these can usually be 
 dealt with by setting an upper limit as to how long a query can run, by using an 
 appropriate statement time-out value. Setting a reasonable statement time-out value by 
 default is a good practice anyway.
  
 So, if you don't deliberately do risky things, the probability of harming the database is 
 no bigger than using a ""trusted"" (also known as restricted) variant of the language. 
  
 However, if you give the language to someone who starts changing bytes on the 
 production database ""to see what happens"", you will get what you asked for.",NA
Will untrusted languages corrupt the ,NA,NA
database?,"The power to corrupt the database is definitely there, since the functions run as the 
 system user of the database server with full access to the filesystem. So, if you blindly 
 start writing into the data files and deleting important logs, it is possible that your 
 database will be corrupted.
  
 Additional types of denial-of-service attacks are also possible, such as using up all 
 memory or opening all IP ports. But, there are ways to overload the database using plain 
 SQL as well, so that part is not much different from the trusted database access with the 
 ability to just run arbitrary queries.
  
 [
  140 
 ]",NA
Why untrusted?,"PostgreSQL's ability to use an untrusted language is a powerful way to perform some 
 non-traditional things from database functions. Creating these functions in a PL is a 
 task of smaller magnitude than writing an extension function in C. For example, a 
 function to look up a hostname for an IP address is only a few lines in PL/PythonU:
  
 CREATE LANGUAGE plpythonu;
  
 CREATE FUNCTION gethostbyname(hostname text) 
  
  RETURNS inet
  
 AS $$
  
  import socket
  
  return socket.gethostbyname(hostname)
  
 $$ LANGUAGE plpythonu SECURITY DEFINER;
  
 You can test it immediately after creating the function by using 
 psql
 :
  
 hannu=# SELECT gethostbyname('www.postgresql.org');
  
  gethostbyname 
  
 ----------------
  
  98.129.198.126
  
 (1 row)
  
 Creating the same function in the most untrusted language, C, involves writing tens of 
 lines of boilerplate code, worrying about memory leaks, and all the other problems 
 coming from writing code in a low-level language. While we will look at extending 
 PostgreSQL in C in the next chapter, I recommend prototyping in a PL language if possible, 
 and in an untrusted language if the function needs something that the restricted 
 languages do not offer.",NA
Why PL/Python?,"All of these tasks could be done equally well using PL/PerlU or PL/TclU. I chose 
 PL/PythonU mainly because Python is the language I am most comfortable with. This 
 also translates to having written some PL/Python code, which I plan to discuss and 
 share with you in this chapter.
  
 [
  141 
 ]",NA
Quick introduction to PL/Python,"In the previous chapters, we discussed PL/pgSQL which is one of the standard 
 procedural languages distributed with PostgreSQL. PL/pgSQL is a language unique to 
 PostgreSQL and was designed to add blocks of computation and SQL inside the 
 database. While it has grown in its breadth of functionality, it still lacks the 
 completeness of syntax of a full programming language. PL/Python allows your 
 database functions to be written in Python with all the depth and maturity of writing a 
 Python code outside the database.",NA
A minimal PL/Python function,"Let's start from the very beginning, yet again:
  
 CREATE FUNCTION hello(name text)
  
  RETURNS text
  
 AS $$
  
  return 'hello %s !' %  name
  
 $$ LANGUAGE plpythonu;
  
 Here, we see that creating a function starts by defining it as any other PostgreSQL 
 function with a 
 RETURNS
  definition of a 
 text
  field:
  
 CREATE FUNCTION hello(name text)
  
  RETURNS text
  
 The difference from what we have seen before, is that the language part is specifying 
 plpythonu
  (the language ID for the PL/PythonU language):
  
 $$ LANGUAGE plpythonu;
  
 Inside the function body, it is very much a normal Python function returning a value 
 obtained by the name passed as an argument formatted into a string 
 'hello %s !'
 , 
 using the standard Python formatting operator 
 %
 :
  
  return 'hello %s !' %  name
  
 Finally, let's test how this works:
  
 hannu=# SELECT hello('world');
  
  hello     
  
 ---------------
  
  hello world !
  
 (1 row)
  
 And yes, it returns exactly what is expected!
  
 [
  142 
 ]",NA
Data type conversions,"The first and last things happening when a PL function is called by PostgreSQL, are 
 converting argument values between the PostgreSQL and PL types. The PostgreSQL 
 types need to be converted to the PL types on entering the function, and then the return 
 value needs to be converted back into the PostgreSQL types.
  
 Except for PL/pgSQL, which uses PostgreSQL's own native types in computations, the 
 PLs are based on existing languages with their own understanding of what types 
 (integer, string, date, and so on) are, how they should behave, and how they are 
 represented internally. They are mostly similar to PostgreSQL's understanding but quite 
 often are not exactly the same. PL/Python converts data from PostgreSQL types to 
 Python types, as shown in the following table:
  
 PostgreSQL
  
 Python 2
  
 Python 3
  
 Comments
  
 int2
 , 
 int4
  
 int
  
 int
  
  
 int8
  
 long
  
 int
  
  
 real
 , 
 double
 , 
 numeric
  
 float
  
 float
  
 This may lose precision for numeric values.
  
 bytea
  
 str
  
 bytes
  
 No encoding conversion is done, nor should any 
 encoding be assumed.
  
 text
 , 
 char()
 , 
  
 varchar()
 , and 
 other text types
  
 str
  
 str
  
 On Python 2, the string will be in server encoding. On 
 Python 3, it is a unicode string.
  
 All other types
  
 str
  
 str
  
 PostgreSQL's 
 type
  output function is used to 
 convert to this string.
  
 Inside the function, all computation is done using Python types and the return value is 
 converted back to PostgreSQL using the following rules (these rules are the direct 
 quotes from official PL/Python documentation at 
 http://www.postgresql.org/ 
 docs/current/static/plpython-data.html
 ):
  
 • 
  
 • 
  
 • 
  
 When the PostgreSQL return type is Boolean, the return value will be 
 evaluated for truth, according to the Python rules. That is, 
 0
  and empty 
 strings are 
 false
 , but notably 
 f
  is 
 true
 .
  
 When the PostgreSQL return type is 
 bytea
 , the return value will be 
 converted to a string (Python 2) or bytes (Python 3) using the respective 
 Python built-ins, with the result being converted 
 bytea
 .
  
 For all other PostgreSQL return types, the returned Python value is 
 converted to a string using Python's built-in 
 str
 , and the result is 
 passed to the input function of the PostgreSQL data type.
  
 [
  143 
 ]",NA
Writing simple functions in PL/Python,"Writing functions in PL/Python is not much different in principle from writing 
 functions in PL/pgSQL. You still have the exact same syntax around the function body 
 in 
 $$
 , and the argument name, types, and returns all mean the same thing, regardless 
 of the exact PL/language used.",NA
A simple function,"So, a simple 
 add_one()
  function in PL/Python looks like this:
  
 CREATE FUNCTION add_one(i int) 
  
  RETURNS int AS $$
  
 return i + 1;
  
 $$ LANGUAGE plpythonu;
  
 usm=# SELECT add_one(2);
  
  add_one 
  
 ---------
  
  3
  
 (1 row)
  
 It can't get any simpler than that, can it?
  
 What you see here is that the PL/Python arguments are passed to the Python 
 code after converting them to appropriate types, and the result is passed back and 
 converted to the appropriate PostgreSQL type for the return value.
  
 [
  144 
 ]",NA
Functions returning a record ,"To return a record from a Python function, you can use:
  
 • 
  
 A sequence or list of values in the same order as the fields in the return record
  
 • 
  
 A dictionary with keys matching the fields in the return record
  
 • 
  
 A class or type instance with attributes matching the fields in the return record
  
 Here are samples of the three ways to return a record:
  
 First, using an instance:
  
 CREATE OR REPLACE FUNCTION userinfo(
  
  
  
  
    
  INOUT username name, 
  
  
  
  
    
  OUT user_id oid, 
  
  
  
  
    
  OUT is_superuser boolean) 
  
 AS $$
  
  class PGUser:
  
  
  
  def __init__(self,username,user_id,is_superuser):
  
  
  
  self.username = username
  
  
  
  
  self.user_id = user_id
  
  
  
  
  self.is_superuser = is_superuser
  
  u = plpy.execute(""""""\
  
  
  
  
  select usename,usesysid,usesuper
  
  
  
  
  
  from pg_user
  
  
  
  
  
  where usename = '%s'"""""" % username)[0]
  
  user = PGUser(u['usename'], u['usesysid'], u['usesuper']) 
 return user 
  
 $$ LANGUAGE plpythonu;
  
 Then, a little simpler one using a dictionary:
  
 CREATE OR REPLACE FUNCTION userinfo(
  
  
  
    
  INOUT username name, 
  
  
  
    
  OUT user_id oid, 
  
  
  
    
  OUT is_superuser boolean) 
  
 AS $$
  
  u = plpy.execute(""""""\
  
  
  
  select usename,usesysid,usesuper
  
  
  
  
  from pg_user
  
  
  
  
  where usename = '%s'"""""" % username)[0]
  
  return {'username':u['usename'], 'user_id':u['usesysid'], 'is_ 
 superuser':u['usesuper']} 
  
 $$ LANGUAGE plpythonu;
  
 [
  145 
 ]",NA
Table functions ,"When returning a set from PL/Python functions, you have three options:
  
 • 
  
 Return a list or any other sequence of return type
  
 • 
  
 Return an iterator or generator
  
 • 
  
 The 
 yield
  keyword in python just returns a generator
  
 Here, we have three ways to generate all even numbers up to the argument value 
 using these different styles:
  
 First, returning a list of integers:
  
 CREATE FUNCTION even_numbers_from_list(up_to int)
  
  
 RETURNS SETOF int 
  
 AS $$
  
  
  return range(0,up_to,2) 
  
 $$ LANGUAGE plpythonu;
  
 libro=# SELECT * FROM even_numbers_from_list(10); 
 even_numbers_from_list 
  
 ------------------------
  
  0
  
  2
  
  4
  
  6
  
  8 
  
 (5 rows)
  
 The list here, is returned by a built-in Python function called 
 range
 , which returns a 
 result of all even numbers below the argument. This gets returned as a table of integers, 
 one integer per row from the PostgreSQL function. If the 
 RETURNS
  clause of the function 
 definition would say 
 int[]
  instead of 
 SETOF int
 , the same function would return a 
 single number of even integers as a PostgreSQL array.
  
 The next function returns a similar result using a generator and returning both the 
 even number and the odd one following it. Also, notice the different PostgreSQL syntax 
 RETURNS TABLE(...)
  used this time for defining the return set:
  
 CREATE FUNCTION even_numbers_from_generator(up_to 
 int)
  
  RETURNS TABLE (even int, odd int) 
  
 [
  147 
 ]",NA
Running queries in the database,"If you have ever accessed a database in Python, you know that most database 
 adapters conform to a somewhat loose standard called 
 Python Database API 
 Specification v2.0
  or 
 DB API 2
  for short. You can find the reference online at 
  
 http://legacy.python.org/dev/peps/pep-0249/
  
 The first thing you need to know about database access in PL/Python is that in-
 database queries 
 do not
  follow this API.",NA
Running simple queries,"Instead of using the standard API, there are just three functions for doing all 
 database access. There are two variants: 
 plpy.execute
  for running a query, and 
 plpy.prepare()
  for turning a query text into a query plan or a prepared query.
  
 The simplest way to do a query is with:
  
 res = plpy.execute(<query text>, [<row count>])
  
 This takes a textual query and an optional row count, and returns a result object, 
 which emulates a list of dictionaries, one dictionary per row.
  
 As an example, if you want to access a field 
 'name'
  of the third row of the result, you 
 use:
  
 res[2]['name']
  
 The index is 
 2
  and not 
 3
  because Python lists are indexed starting from 0, so the first 
 row is 
 res[0]
 , the second row 
 res[1]
 , and so on.
  
 [
  149 
 ]",NA
Using prepared queries,"In an ideal world, this would be all that is needed, but 
 plpy.execute(query, cnt) 
 has two shortcomings:
  
 • 
  
 • 
  
 It does not support parameters 
  
 The plan for the query is not saved, requiring the query text to be parsed and 
 run through the optimizer at each invocation
  
 We will show a way to properly construct a query string later, but for most uses 
 simple parameter passing is enough. So, the 
 execute(query, [maxrows])
  call 
 becomes a set of two statements:
  
 plan = plpy.prepare(<query text>, <list of argument types>)
  
 res = plpy.execute(plan, <list of values>, [<row count>])
  
 For example, to query if a user 'postgres' is a superuser, use the following:
  
 plan = plpy.prepare(""select usesuper from pg_user where  usename = 
 $1"", [""text""])
  
 res = plpy.execute(plan, [""postgres""])
  
 print res[0][""usesuper""]
  
 The first statement prepares the query, which parses the query string into a query 
 tree, optimizes this tree to produce the best query plan available, and returns the 
 prepared_query
  object. The second row uses the prepared plan to query for a 
 specific user's 
 superuser
  status.
  
 The prepared plan can be used multiple times, so that you could continue to see if user 
 bob
  is 
 superuser
 .
  
 res = plpy.execute(plan, [""bob""])
  
 print res[0][""usesuper""]",NA
Caching prepared queries,"Preparing the query can be quite an expensive step, especially for more complex 
 queries where the optimizer has to choose from a rather large set of possible plans. So, 
 it makes sense to re-use results of this step, if possible.
  
 [
  150 
 ]",NA
Writing trigger functions in PL/Python ,"As with other PLs, PL/PythonU can be used to write trigger functions. The declaration of a 
 trigger function is different from an ordinary function by the return type 
 RETURNS 
 TRIGGER
 . So, a simple trigger function that just notifies the caller that it is indeed called, 
 looks like this:
  
 CREATE OR REPLACE FUNCTION notify_on_call()
  
  
  RETURNS TRIGGER 
  
 AS $$ 
  
 plpy.notice('I was called!') 
  
 $$ LANGUAGE plpythonu;
  
 After creating this function, the trigger can be tested on a table using a trigger 
 function:
  
 hannu=# CREATE TABLE ttable(id int); 
  
 CREATE TABLE 
  
 hannu=# CREATE TRIGGER ttable_notify BEFORE INSERT ON ttable EXECUTE 
 PROCEDURE notify_on_call(); 
  
 CREATE TRIGGER 
  
 hannu=# INSERT INTO ttable VALUES(1); 
  
 NOTICE:  I was called!
  
 CONTEXT:  PL/Python function ""notify_on_call"" 
 INSERT 0 1
  
 [
  151 
 ]",NA
Exploring the inputs of a trigger ,"The following trigger function is useful when developing triggers, so that you can 
 easily see what the trigger function is really getting when called:
  
 CREATE OR REPLACE FUNCTION explore_trigger()
  
  
  RETURNS TRIGGER 
  
 AS $$ 
  
 import pprint 
  
 nice_data = pprint.pformat(
  
  
  (
  
  
  ('TD[""table_schema""]' , TD[""table_schema""] ),
  
  
 ('TD[""event""]'        , TD[""event""] ),
  
  
  ('TD[""when""]'         , TD[""when""] ),
  
  
  ('TD[""level""]'        , TD[""level""] ),
  
  
  ('TD[""old""]'          , TD[""old""] ),
  
  
  ('TD[""new""]'          , TD[""new""] ),
  
  
  ('TD[""name""]'         , TD[""name""] ),
  
  
  ('TD[""table_name""]'   , TD[""table_name""] ),
  
  
 ('TD[""relid""]'        , TD[""relid""] ),
  
  
  ('TD[""args""]'         , TD[""args""] ),
  
  
  ) 
  
 ) 
  
 plpy.notice('explore_trigger:\n' + nice_data) $$ 
 LANGUAGE plpythonu;
  
 This function formats all the data passed to the trigger in TD using 
 pprint.pformat
 , and 
 then sends it to the client as a standard Python info message using 
 plpy.notice
 . 
  
 For testing this out, we create a simple table and then put an 
 AFTER … FOR EACH 
 ROW …
  trigger using this function on that table:
  
 CREATE TABLE test(
  
  id serial PRIMARY KEY,
  
  data text,
  
  ts timestamp DEFAULT clock_timestamp() 
  
 );
  
 CREATE TRIGGER test_explore_trigger
  
 [
  153 
 ]",NA
A log trigger ,"Now, we can put this knowledge to work and write a trigger that logs changes to the table 
 to either a file or to a special log-collector process over the network. Logging to a file is 
 the simplest way to permanently log the changes in transactions which were rolled back. 
 If these were logged to a log table, the 
 ROLLBACK
  command would also remove the log 
 records. This may be a crucial audit requirement for your business.
  
 Of course, this also has a downside. You will be logging the changes that may not be 
 permanent due to the transaction being rolled back. Unfortunately, this is the price 
 you have to pay for not losing the log records.
  
 CREATE OR REPLACE FUNCTION log_trigger()
  
 [
  154 
 ]",NA
Constructing queries,"PL/Python does a good job of managing values passed to prepared query plans, but a 
 standard PostgreSQL query plan can take an argument in a very limited number of places. 
 Sometimes, you may want to construct whole queries, not just pass values to predefined 
 queries. For example, you can't have an argument for a table name, or a field name.
  
 So, how would you proceed if you want to construct a query from the function's 
 arguments and be sure that everything is quoted properly and no SQL injection 
 would be possible? PL/Python provides three functions to help you with proper 
 quoting of identifiers and data, just for this purpose.
  
 The function 
 plpy.quote_ident(name
  is meant for quoting identifiers, that is, 
 anything that names a database object or its attribute like a table, a view, a field name, 
 or function name. It surrounds the name with double quotes and takes care of 
 properly escaping anything inside the string which would break the quoting:
  
 hannu=# DO LANGUAGE plpythonu $$ plpy.notice(plpy.quote_ident 
  
  (r'5""\""'))
  
 $$;
  
 NOTICE:  ""5"""" \""""""
  
 CONTEXT:  PL/Python anonymous code block
  
 DO
  
 And yes, 
 5"" \""
  is a legal table or field name in PostgreSQL; you just have to always 
 quote it if you use it in any statement.
  
 [
  157 
 ]",NA
Handling exceptions ,"With any bit of code, you need to make sure you handle when errors occur and your 
 PL/Python functions are not an exception.
  
 Before Version 9.1 of PostgreSQL, any error in an SQL query caused the surrounding 
 transaction to be rolled back:
  
 DO LANGUAGE plpythonu $$ 
  
 plpy.execute('insert into ttable values(1)') 
  
 plpy.execute('fail!') 
  
 $$; 
  
 ERROR:  spiexceptions.SyntaxError: syntax error at or near ""fail"" 
 LINE 1: fail!
  
 ^ 
  
 QUERY:  fail!
  
 CONTEXT:  Traceback (most recent call last):
  
  
  PL/Python anonymous code block, line 3, in <module>
  
  plpy.execute('fail!') 
  
 PL/Python anonymous code block
  
 [
  158 
 ]",NA
Atomicity in Python ,"While the subtransactions manage data changes in the PostgreSQL database, 
  
 the variables on the Python side of the fence live their separate lives. Python does not 
 provide even a single-statement level atomicity, as demonstrated by the following:
  
 >>> a = 1 
  
 >>> a[1] = a = 2 
  
 Traceback (most recent call last):
  
  
  File ""<stdin>"", line 1, in <module> 
  
 TypeError: 'int' object does not support item assignment 
 >>> a 
  
 1 
  
 >>> a = a[1] = 2 
  
 Traceback (most recent call last):
  
  
  File ""<stdin>"", line 1, in <module> 
  
 TypeError: 'int' object does not support item assignment 
 >>> a 
  
 2
  
 [
  160 
 ]",NA
Debugging PL/Python,"First, let's start by stating that there is no debugger support when running functions in 
 PL/Python; so, it is a good idea to develop and debug a PL/Python function as a pure 
 Python function as much as possible and only do the final integration in PL/Python. To 
 help with this, you can have a similar environment in your Python development 
 environment using the 
 plpy
  module.
  
 Just put the module in your path and do import 
 plpy
  before you try running 
 your prospective PL/PythonU functions in an ordinary interpreter. If you use 
 any of the 
 plpy.execute(...)
  or 
 plpy.prepare()
  functions , you also need 
 to set up a database connection before using these by calling 
 plpy. 
 connect(<connectstring>)
 .",NA
Using plpy.notice() to track the function's progress,"The debugging technology I use most often in any language, is printing out 
 intermediate values as the function progresses. If the printout rolls past too fast, 
 you can slow it down by sleeping a second or two after each print.
  
 In standard Python, it would look like this:
  
 def fact(x):
  
  f = 1
  
  while (x > 0):
  
  f = f * x
  
  x = x – 1
  
  print 'f:%d, x:%d' % (f, x)
  
  return f
  
 It will print out all intermediate values for 
 f
  and 
 x
  as it runs:
  
 >>> fact(3)
  
 f:3, x:2
  
 f:6, x:1
  
 f:6, x:0
  
 6
  
 [
  161 
 ]",NA
Using assert ,"Similar to ordinary Python programming, you can also use Python's 
 assert 
 statement to catch conditions which should not happen:
  
 CREATE OR REPLACE FUNCTION fact(x int) 
  
  
  RETURNS int 
  
 AS $$
  
  
  global x
  
  
  assert x>=0, ""argument must be a positive integer""
  
  f = 1
  
  
  while (x > 0):
  
    
  f = f * x
  
    
  x = x - 1
  
  
  return f 
  
 $$ LANGUAGE plpythonu;
  
 To test this, call 
 fact()
  with a negative number:
  
 hannu=# SELECT fact(-1); 
  
 ERROR:  AssertionError: argument must be a positive integer 
 CONTEXT:  Traceback (most recent call last):
  
  
  PL/Python function ""fact"", line 3, in <module>
  
  
  assert x>=0, ""argument must be a positive integer"" 
 PL/Python function ""fact""
  
 You will get a message about 
 AssertionError
 , together with the location of the 
 failing line number.",NA
Redirecting sys.stdout and sys.stderr ,"If all the code you need to debug is your own, the preceding two techniques will cover 
 most of your needs. However, what do you do in cases where you use some third party 
 libraries which print out debug information to 
 sys.stdout
  and/or 
 sys.stderr
 ?
  
 Well, in those cases you can replace Python's 
 sys.stdout
  and 
 sys.stdin
  with your 
 own pseudo file object that stores everything written there for later retrieval. Here is a 
 pair of functions, the first of which does the capturing of 
 sys.stdout
  or uncapturing if 
 it is called with the argument, 
 do_capture
  set to 
 false
 , and the second one returns 
 everything captured:
  
 CREATE OR REPLACE FUNCTION capture_stdout(do_capture 
 bool)
  
  RETURNS text 
  
 AS $$
  
  
  import sys
  
 [
  163 
 ]",NA
"Thinking out of the ""SQL database ",NA,NA
"server"" box ","We'll wrap up the chapter on PL/Python with a couple of sample PL/PythonU 
 functions for doing some things you would not usually consider doing inside the 
 database function or trigger.",NA
Generating thumbnails when saving images ,"Our first example, uses Python's powerful 
 Python Imaging Librar
 y (
 PIL
 ) module to 
 generate thumbnails of uploaded photos. For ease of interfacing with various client 
 libraries, this program takes the incoming image data as a Base64 encoded string:
  
 CREATE FUNCTION save_image_with_thumbnail(image64 text)
  
  RETURNS int 
  
 AS $$ 
  
 import Image, cStringIO 
  
 size = (64,64) # thumbnail size
  
 # convert base64 encoded text to binary image 
 data raw_image_data = image64.decode('base64')
  
 # create a pseudo-file to read image from 
  
 infile = cStringIO.StringIO(raw_image_data) 
  
 pil_img = Image.open(infile) 
  
 pil_img.thumbnail(size, Image.ANTIALIAS)
  
 # create a stream to write the thumbnail to 
  
 outfile = cStringIO.StringIO() 
  
 pil_img.save(outfile, 'JPEG') 
  
 raw_thumbnail = outfile.getvalue()
  
 # store result into database and return row 
 id q = plpy.prepare('''
  
 [
  165 
 ]",NA
Sending an e-mail ,"The next sample is a function for sending e-mails from inside a database function:
  
 CREATE OR REPLACE FUNCTION send_email(
  
  sender text,      -- sender e-mail
  
  recipients text,  -- comma-separated list of recipient addresses 
 subject text,     -- email subject
  
  message text,     -- text of the message
  
  smtp_server text  -- SMTP server to use for sending 
  
 ) RETURNS void 
  
 AS $$
  
  import smtplib;
  
  msg = ""From: %s\r\nTo: %s\r\nSubject: %s\r\n\r\n%s"" % \
  
  
  (sender, recipients, subject, message)
  
  recipients_list = [r.strip() for r
  
  
  
  
  in recipients.split(',')]
  
  server = smtplib.SMTP(smtp_server)
  
  server.sendmail(sender, recipients_list, msg)
  
  server.quit() 
  
 $$ LANGUAGE plpythonu;
  
 test=# SELECT send_email('dummy@gmail.com', 'abv@postgresql.org', 
  
  'test subject', 'message', 'localhost');
  
 [
  166 
 ]",NA
Listing directory contents ,"Here is another interesting use case for an untrusted language. The function below can 
 list the contents of a directory in your system:
  
  CREATE OR REPLACE FUNCTION list_folder(
  
  directory VARCHAR -- directory that will be 
 walked ) RETURNS  SETOF VARCHAR
  
  AS $$
  
  import os;
  
  file_paths = [];
  
  # Walk the tree.
  
  for root, directories, files in os.walk(directory):
  
  
  for filename in files:
  
  
  
  # Join the two strings in order to form the full 
 filepath.
  
  filepath = os.path.join(root, filename)
  
  file_paths.append(filepath)  # Add it to the 
 list.
  
  return file_paths 
  
  $$ LANGUAGE 
 plpythonu;
  
 Let us now try and run the function:
  
 test_db=# SELECT list_folder('/usr/local/pgsql/bin');
  
  list_folder             
  
 -------------------------------------
  
  /usr/local/pgsql/bin/clusterdb
  
  /usr/local/pgsql/bin/createdb
  
  /usr/local/pgsql/bin/createlang
  
  /usr/local/pgsql/bin/createuser
  
  /usr/local/pgsql/bin/dropdb
  
  /usr/local/pgsql/bin/droplang
  
  /usr/local/pgsql/bin/dropuser
  
 [
  167 
 ]",NA
Summary ,"In this chapter, we saw that it is relatively easy to do things way beyond what a simple 
 SQL database server normally supports; thanks to its pluggable language's support.
  
 In fact, you can do almost anything in the PostgreSQL server that you could do in any 
 other application server. Hopefully, this chapter just scratched the surface of what 
 can be done inside a PostgreSQL server.
  
 In the next chapter, we will learn about writing PostgreSQL's more advanced 
 functions in C. This will give you deeper access to PostgreSQL, allowing you to use a 
 PostgreSQL server for much more powerful things.
  
 [
  168 
 ]",NA
Writing Advanced Functions ,NA,NA
in C,"In the previous chapter, we introduced you to the possibilities of 
 untrusted
  pluggable 
 languages being available to a PostgreSQL developer to achieve things impossible in most 
 other relational databases.
  
 While using a pluggable scripting language is enough for a large class of problems, 
 there are two main categories, where they may fall short: 
 performance
  and 
 depth 
 of 
 functionality.
  
 Most scripting languages are quite a bit slower than optimized C code when executing the 
 same algorithms. For a single function, this may not be the case because common things 
 such as dictionary lookups or string matching have been optimized so well over the years. 
 But in general, C code will be faster than scripted code. Also, in cases where the function is 
 called millions of times per query, the overhead of actually calling the function and 
 converting the arguments and return values to and from the scripting language 
 counterparts can be a significant portion of the run time.
  
 The second potential problem with pluggable languages is that most of them just do not 
 support the full range of possibilities that are provided by PostgreSQL. There are a few 
 things that simply cannot be coded in anything else but C. For example, when you define a 
 completely new type for PostgreSQL, the type input and output functions, which convert 
 the type's text representation to internal representation and back, need to handle 
 PostgreSQL's pseudotype 
 cstring
 . This is basically the C string or a zero-terminated 
 string. Returning 
 cstring
  is simply not supported by any of the PL languages included in 
 the core distribution, at least not as of PostgreSQL Version 9.3. The PL languages also do 
 not support pseudotypes 
 ANYELEMENT
 , 
 ANYARRAY
  and especially ""any"" 
 VARIADIC
 .",NA
The simplest C function – return (a + b),"Let's start with a simple function, which takes two integer arguments and returns the sum 
 of these. We first present the source code and then will move on to show you how to 
 compile it, load it into PostgreSQL, and then use it as any native function.",NA
add_func.c,"A C source file implementing 
 add(int, int)
  returns 
 int
  function looks like the 
 following code snippet:
  
 #include ""postgres.h""
  
 #include ""fmgr.h""
  
 PG_MODULE_MAGIC;
  
 PG_FUNCTION_INFO_V1(add_ab);
  
 Datum
  
 add_ab(PG_FUNCTION_ARGS)
  
 {
  
  int32   arg_a = PG_GETARG_INT32(0);
  
  int32   arg_b = PG_GETARG_INT32(1);
  
  PG_RETURN_INT32(arg_a + arg_b);
  
 }
  
 [
  170 
 ]",NA
Version 0 call conventions,"There is an even simpler way to write PostgreSQL functions in C, called the 
 Version 
 0 calling conventions
 . The preceding 
 a + b
  function can be written as the 
 following code:
  
 int add_ab(int arg_a, int arg_b)
  
 {
  
  return arg_a + arg_b;
  
 }
  
 Version 0 is shorter for very simple functions, but it is severely limited for most other 
 usages—you can't do even some basic things such as checking if a pass by value 
 argument is null, return a set of values, or write aggregate functions. Also, Version 0 
 does not automatically take care of hiding most differences of pass by value and pass by 
 reference types that Version 1 does. Therefore, it is better to just write all your functions 
 using Version 1 calling conventions and ignore the fact that Version 0 even exists.
  
 From this point forward, we are only going to discuss Version 1 calling conventions for a 
 C function.
  
  
 In case you are interested, there is some more information on 
  
  
 Version 0 at 
 http://www.postgresql.org/docs/current/
  
 static/xfunc-c.html#AEN50495
 , in the section titled 
 35.9.3. 
  
 Version 0 Calling Conventions
 .
  
 [
  172 
 ]",NA
Makefile,"The next step is compiling and linking the 
 .c
  source file into a form that can be 
 loaded into the PostgreSQL server. This can all be done as a series of commands 
 defined in a 
 Makefile
  function.
  
 The PostgreSQL manual has a complete section about which flags and included paths 
 you should pass on each of the supported platforms, and how to determine correct 
 paths for including files and libraries.
  
 Fortunately, all of this is also automated nicely for developers via the PostgreSQL 
 extension building infrastructure—or PGXS for short—which makes this really easy for 
 most modules.
  
  
 Depending on which version of PostgreSQL you have installed, 
  
  
 you may need to add the development package for your 
  
 platform. These are usually the 
 -dev
  or 
 -devel
  packages.
  
 Now, let's create our 
 Makefile
  function. It will look like the following code:
  
 MODULES = add_func
  
 PG_CONFIG = pg_config
  
 PGXS := $(shell $(PG_CONFIG) --pgxs)
  
 include $(PGXS)
  
 And you can compile and link the module by simply running make:
  
 $make
  
 gcc ... -c -o add_func.o add_func.c 
  
 gcc ... -o add_func.so add_func.o 
  
 rm add_func.o 
  
 Here, ""..."" stands for quite some amount of flags, includes, and libraries added by 
 PGXS.
  
 This produces a dynamically loadable module in the current directory which can be 
 used directly by PostgreSQL, if your server has access to this directory, which may be 
 the case on a development server.
  
 For a ""standard"" server, as installed by your package management system, you will need 
 to put the module in a standard place. This can be done using the PGXS as well.
  
 [
  173 
 ]",NA
"CREATE FUNCTION add(int, int) ","You are just one step away from being able to use this function in your database. You 
 just need to introduce the module you just compiled to a PostgreSQL database using 
 the 
 CREATE FUNCTION
  statement.
  
 If you followed the samples up to this point, the following statement is all that is 
 needed, along with adjusting the path appropriately to where PostgreSQL is 
 installed on your server:
  
 hannu=# CREATE FUNCTION add(int, int) 
  
 hannu-#   RETURNS int 
  
 hannu-# AS '/usr/lib/postgresql/9.3/lib/add_func', 'add_ab_null' 
 hannu-# LANGUAGE C STRICT; 
  
 CREATE FUNCTION 
  
 And voilá—you have created your first PostgreSQL C-language extension function:
  
 hannu=# select add(1,2); 
  
  add 
  
 ----- 
  
  3 
  
 (1 row)",NA
add_func.sql.in ,"While what we just covered is all that is needed to have a C function in your 
 database, it is often more convenient to put the preceding 
 CREATE FUNCTION 
 statement in an SQL file.
  
 You usually do not know the final path of where PostgreSQL is installed when 
 writing the code, especially in the light of running on multiple versions of 
 PostgreSQL and/or on multiple operation systems. Here also, PGXS can help.
  
 [
  174 
 ]",NA
Summary for writing a C function,"Writing a C function used in PostgreSQL is a straightforward process:
  
 1. Write the C code in 
 modulename.c
 .
  
 2. Write the SQL code for 
 CREATE FUNCTION
  in 
 modulename.sql.in
 .
  
 3. Write a 
 Makefile
  function.
  
 4. Run make to compile a C file and generate 
 modulename.sql
 .
  
 5. Run 
 sudo make install
  to install the generated files.
  
 6. Run the generated 
 modulename.sql
  in your target database:
  
 hannu# \i /<path>/modulename.sql
  
  
 You must run the SQL code in any database you want to use your 
  
  
 function. If you want all your new databases to have access to your 
  
 newly generated function, add the function to your template database 
  
 by running the 
 modulename.sql
  file in database 
 template1
  
 or any other database you are explicitly specifying in the 
 CREATE 
  
 DATABASE
  command.
  
 You may have noticed that while creating the function, you specified the name of the 
 loadable object file and the name of the C function. When the SQL function is called for the 
 first time, the dynamic loader loads the object file in memory. If you would like some 
 object files to be preloaded at server startup, you should specify them in the PostgreSQL 
 shared_preload_libraries
  configuration parameter.",NA
"Adding functionality to add(int, int)","While our function works, it adds nothing in the preceding code just using 
 SELECT A 
 + B
 . But functions written in C are capable of so much more. So let's start adding some 
 more functionality to our function.",NA
Smart handling of NULL arguments,"Notice the use of a 
 STRICT
  keyword in the 
 CREATE FUNCTION add(int a, int b) 
 in 
 the previously mentioned code. This means that the function will not be called if any of 
 the arguments are 
 NULL
 , but instead 
 NULL
  is returned straightaway. This is similar to 
 how most PostgreSQL operators work, including the + sign when adding two integers—
 if any of the arguments are 
 NULL
  the complete result is 
 NULL
  as well.
  
 [
  176 
 ]",NA
Working with any number of arguments ,"After the rewrite to handle 
 NULL
  values, it seems that with just a little more effort, we 
 could make it work with any number of arguments. Just move the following code 
 inside the 
 for(;;)
  cycle over the arguments and we are done:
  
 if (!PG_ARGISNULL(<N>)) {
  
  
  
  sum += PG_GETARG_INT32(<N>);
  
  
  
  not_null = 1;
  
  }
  
 [
  178 
 ]",NA
Basic guidelines for writing C code,"After having written our first function, let's look at some of the basic coding 
 guidelines for PostgreSQL backend coding.",NA
Memory allocation,"One of the places you have to be extra careful when writing C code in general is 
 memory management. For any non-trivial C program, you have to carefully design and 
 implement your programs, so that all your allocated memory is freed when you are 
 done with it, or else you will ""leak memory"" and will probably run out of memory at 
 some point.
  
 As this is also a common concern for PostgreSQL, it has its own solution—memory 
 contexts. Let's take a deeper dive into them.",NA
Use palloc() and pfree(),"Most PostgreSQL memory allocations are done using PostgreSQL's memory allocation 
 function 
 palloc()
  and not standard C 
 malloc()
 . What makes 
 palloc() 
 special is 
 that it allocates the memory in the current context and the whole memory is freed in 
 one go when the context is destroyed. For example, the transaction context—which is 
 the current context when a user-defined function is called—is destroyed and memory 
 allocated is freed at the end of transaction. This means that most times the 
 programmers do not need to worry about tracking 
 palloc() 
 allocated memory and 
 freeing it.
  
 [
  185 
 ]",NA
Zero-fill the structures,"Always make sure that new structures are zero-filled, either by using 
 memset
  after 
 allocating them or using 
 palloc0()
 .
  
 PostgreSQL sometimes relies on logically equivalent data items being also the same for 
 bit-wise comparisons. Even when you set all the items in a structure, it is possible that 
 some alignment issues leave garbage in the areas between structure elements if any 
 alignment padding was done by the compiler.
  
 If you don't do this, then hash indexes and hash joins of PostgreSQL may work 
 inefficiently or even give wrong results. The planner's constant comparisons may also 
 be wrong if constants which are logically the same are not the same via bit-wise 
 equality, resulting in undesirable planning results.",NA
Include files,"Most of PostgreSQL internal types are declared in 
 postgres.h
 , and the function manager 
 interfaces (
 PG_MODULE_MAGIC
 , 
 PG_FUNCTION_INFO_V1
 , 
 PG_FUNCTION_ARGS
 , 
 PG_GETARG_<type>
 , 
 PG_RETURN_<type>
 , and so on) are in 
 fmgr.h
 . Therefore, all your C 
 extension modules need to include at least these two files. It is a good habit to include 
 postgres.h
  first as it gives your code the best portability by (re)defining some platform 
 dependent constants and macros. Including 
 postgres.h
  also includes 
 utils/elog.h
  
 and 
 utils/palloc.h
  for you.
  
 There are other useful include files in the 
 utils/
  subdirectory which you also may 
 need to include like 
 utils/array.h
  used in the last example.
  
 Another often used include directory is 
 catalog/
  which gives you the initial (and by 
 convention constant) part of most system tables, so you do not need to look up things 
 like type identifier for the 
 int4
  data type, but can use its predefined value 
 INT4OID
  
 directly.
  
 The values in 
 catalog/pg_*
  include files are always in sync with what gets put into the 
 database catalogs by virtue of being the definition of the structure and contents of the 
 system catalog tables. The 
 .bki
  files used when the 
 initdb
  command sets up a new 
 empty database cluster are generated from these 
 .h
  files by the 
 genbki.pl
  script.
  
 [
  186 
 ]",NA
Public symbol names,"It is the programmer's task to make sure that any symbol names visible in the 
 .so 
 files 
 do not conflict with those already present in the PostgreSQL backend, including those 
 used by other dynamically loaded libraries. You will have to rename your functions or 
 variables if you get messages to this effect. Pick sufficiently distinctive names for 
 functions to avoid any such problems. Avoid short names that might be a source of 
 potential conflict. This may be a bigger problem if the conflicts come from a third-party 
 library your code is using. So test early in the development if you can link all the planned 
 libraries to your PostgreSQL extension module.",NA
Error reporting from C functions,"One thing which went unexplained in the previous sample was the error 
 reporting part:
  
  if (ARR_NDIM(input_array) > 1)
  
 ereport(ERROR,
  
 (errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),
  
 errmsg
 (""use only one-dimensional arrays!""))); 
  
 All error reporting and other off-channel messaging in PostgreSQL is done using the 
 ereport(<errorlevel>, rest)
  macro whose main purpose is to make error 
 reporting look like a function call.
  
 The only parameter which is processed directly by 
 ereport()
  is the first argument 
 error level, or perhaps more exactly, the severity level or log level. All the other 
 parameters are actually function calls which independently generate and store 
 additional error information in the system to be written to logs and/or be sent to the 
 client. Being placed in the argument list of the 
 ereport()
  makes sure that these other 
 functions are called before the actual error is reported. This is important because in the 
 case of an error level being 
 ERROR
 , 
 FATAL
 , or 
 PANIC
  the system cleans up all current 
 transaction states and anything after the 
 ereport()
  call will never get a chance to run. 
 Error states the end of the transaction.
  
 In case of 
 ERROR
 , the system is returned to a clean state and it will be ready to accept 
 new commands.
  
 Error level 
 FATAL
  will clean up the backend and exit the current session.
  
 PANIC
  is the most destructive one and it will not only end the current connection, but will 
 also cause all other connections to be terminated. 
 PANIC
  means that shared state (shared 
 memory) is potentially corrupted and it is not safe to continue. It is used automatically for 
 things like core dumps or other ""hard"" crashes.
  
 [
  187 
 ]",NA
"""Error"" states that are not errors","WARNING
  is the highest non-error level. It means that something may be wrong and 
 needs user/administrator attention. It is a good practice to periodically scan system logs 
 for warnings. Use this only for unexpected conditions. See the next one for things 
 happening on a regular basis. Warnings go to client and server logs by default.
  
 NOTICE
  is for things which are likely of higher interest to users, like information about 
 creating a primary key index or sequence for serial type (though these stopped to be 
 NOTICE
  in the latest version of PostgreSQL). Like the previous one, 
 NOTICE
  is sent both to 
 client and server logs by default.
  
 INFO
  is for things specifically requested by client, like 
 VACUUM
  / 
 ANALYSE VERBOSE
 . It 
 is always sent to the client regardless of the 
 client_min_messages
  GUC setting, but 
 is not written to a server log when using default settings.
  
 LOG
  and 
 COMMERROR
  are for servers, operational messages, and by default are only 
 written to the server log. The error level 
 LOG
  can also be sent to the client if 
 client_min_messages
  is set appropriately, but 
 COMMERROR
  never is.
  
 There are 
 DEBUG1
  to 
 DEBUG5
  in increasing order of verbosity. They are specifically 
 meant for reporting debugging info and are not really useful in most other cases, 
 except perhaps for curiosity. Setting higher 
 DEBUGx
  levels is not recommended in 
 production servers, as the amount logged or reported can be really huge.",NA
When are messages sent to the client?,"While most communication from the server to the client takes place after 
  
 the command completes (or even after the transaction is committed in case of 
 LISTEN
 /
 NOTIFY
 ), everything emitted by 
 ereport()
  is sent to the client immediately, 
 thus the mention of off-channel messaging previously. This makes 
 ereport()
  a useful 
 tool for monitoring long-running commands such as 
 VACUUM
  and also a simple debugging 
 aid to print out useful debug info.
  
  
 You can read a much more detailed description of error reporting 
  
  
 at 
 http://www.postgresql.org/docs/current/static/
  
 error-message-reporting.html
 .
  
 [
  188 
 ]",NA
Running queries and calling PostgreSQL ,NA,NA
functions,"Our next stop is running SQL queries inside the database. When you want to run a query 
 against the database, you need to use something called 
 Server Programming Interface
  
 (
 SPI
 ). SPI gives programmer the ability to run SQL queries via a set of interface 
 functions for using PostgreSQL's parser, planner, and executor.
  
  
 If the SQL statement you are running via SPI fails, the control is not 
  
  
 returned to the caller, but instead the system reverts to a clean state via 
  
 internal mechanisms for 
 ROLLBACK
 . It is possible to catch SQL errors 
  
 by establishing a sub-transaction around your calls. It is an involved 
  
 process not yet officially declared ""stable"" and therefore, it is not present 
  
 in the documentation on C extensions. If you need it, one good place 
  
 to look at would be the source code for various pluggable languages 
  
 (PL/python, PL/proxy, and so on) which do it and are likely to be 
  
 maintained in good order if the interface changes.
  
 In the PL/python source, the functions to examine are in the 
  
 plpython/plpy_spi.c
  file and are appropriately named 
 Ply_spi_
  
 subtransaction_[begin|commit|abort]()
 .
  
 The SPI functions do return non-negative values for success, either directly via a 
 return value or in a global variable 
 SPI_result
 . Errors produce a negative value or 
 Null
 .",NA
A sample C function using SPI,"Here is a sample function doing an SQL query via 
 SPI_*()
  functions. It is a modified 
 version of the sample from the standard documentation (it uses Version 1 calling 
 conventions and outputs an extra bit of information). The 
 .c
 , 
 .sql.in
 , and 
 Makefile
  
 functions for this sample are available in the 
 spi_samples/
  subdirectory.
  
 Datum 
  
 count_returned_rows(PG_FUNCTION_ARGS) 
  
 { 
  
  char *command; 
  
  int cnt; 
  
  int ret; 
  
  int proc; 
  
  /* get arguments, convert command to C string */ 
  
  command = text_to_cstring(PG_GETARG_TEXT_P(0)); 
  
 [
  189 
 ]",NA
Visibility of data changes,"The visibility rules for data changes in PostgreSQL are that each command cannot see its 
 own changes but usually can see changes made by commands which were started before 
 it, even when the command is started by the outer command or query.
  
 The exception is when the query is executed with a read-only flag set, in which case the 
 changes made by outer commands are invisible to inner or called commands.
  
 The visibility rules are described in the documentation at 
 http://www.postgresql. 
 org/docs/current/static/spi-visibility.html
  and may be quite complex to 
 understand at first, but it may help to think of a read-only 
 SPI_execute()
  call as being 
 command-level, similar to the transaction isolation level serializable; and a read-write 
 call similar to the read-committed isolation level.
  
 This is further explained at 
 http://www.postgresql.org/docs/current/static/ 
 spi-examples.html
  in the 
 Sample session
  section.
  
 [
  191 
 ]",NA
More info on SPI_* functions ,"There is a lot more information on specific 
 SPI_*()
  functions in the official 
 documentation.
  
 For PostgreSQL Version 9.3 functions, 
 http://www.postgresql.org/docs/ 
 current/static/spi.html
  is the starting point for the SPI docs.
  
 More sample code is also available in the PostgreSQL source in regression tests at 
 src/test/regress/regress.c
  and in the 
 contrib/spi/
  module.",NA
Handling records as arguments or ,NA,NA
returned values ,"As our next exercise, let's write a function which takes a record of three integers a, b, 
 and c as an argument and returns a set of different records—all permutations of a, b, 
 and c with an extra field x computed as 
 a * b + c
 .
  
 First, this function is written in PL/python to make it easier to understand what we are 
 trying to do:
  
 hannu=# CREATE LANGUAGE plpythonu; 
  
 CREATE LANGUAGE 
  
 hannu=# CREATE TYPE abc AS (a int, b int, c int); 
 CREATE TYPE 
  
 hannu=# CREATE OR REPLACE FUNCTION 
  
 hannu-#     reverse_permutations(r abc) 
  
 hannu-#   RETURNS TABLE(c int, b int, a int, x int) 
 hannu-# AS $$ 
  
 hannu$#     a,b,c = r['a'], r['b'], r['c'] 
  
 hannu$#     yield a,b,c,a*b+c 
  
 hannu$#     yield a,c,b,a*c+b 
  
 hannu$#     yield b,a,c,b*b+c 
  
 hannu$#     yield b,c,a,b*c+a 
  
 hannu$#     yield c,a,b,c*a+b 
  
 hannu$#     yield c,b,a,c*b+a 
  
 hannu$# $$ LANGUAGE plpythonu; 
  
 CREATE FUNCTION 
  
 [
  192 
 ]",NA
Returning a single tuple of a complex type ,"The 
 first step in constructing a version of the reverse permutations function in C is to start 
 with simply being able to return a single record of type 
 abc
 :
  
 Datum 
  
 c_reverse_tuple(PG_FUNCTION_ARGS) 
  
 { 
  
  
 HeapTupleHeader
  th; 
  
  int32   a,b,c; 
  
  bool    aisnull, bisnull, cisnull; 
  
 TupleDesc
  resultTupleDesc; 
  
 Oid
  resultTypeId; 
  
 Datum
  retvals[4]; 
  
 bool
   retnulls[4]; 
  
 HeapTuple
  rettuple; 
  
  // get the tuple header of 1st argument 
  
  th = 
 PG_GETARG_HEAPTUPLEHEADER(0)
 ; 
  
  // get argument Datum's and convert them to int32 
  
  a = 
 DatumGetInt32(GetAttributeByName
 (th, ""a"", &aisnull));  
 b = 
 DatumGetInt32(GetAttributeByName
 (th, ""b"", &bisnull));  
 c = 
 DatumGetInt32(GetAttributeByName
 (th, ""c"", &cisnull)); 
  
  // debug: report the extracted field values
  
 [
  194 
 ]",NA
Extracting fields from an argument tuple ,"Getting the fields of an argument tuple is easy. First, you fetch the 
 HeapTupleHeader 
 file of the argument into the 
 th
  variable using the 
 PG_GETARG_HEAPTUPLEHEADER(0) 
 macro, and then for each field you get the 
 Datum
  (a generic type which can hold any field 
 value in PostgreSQL) by the field name using the 
 GetAttributeByName() 
 function and 
 then assign its value to a local variable after converting it to 
 int32
  via 
 DatumGetInt32()
 :
  
 a = DatumGetInt32(GetAttributeByName(th, ""a"", &aisnull));
  
 The third argument to 
 GetAttributeByName(...)
  is an address of a 
 bool
  which is set 
 to 
 true
  if the field was 
 NULL
 .
  
 There is also a companion function 
 GetAttributeByNum()
  if you prefer to get the 
 attributes by their numbers instead of names.
  
 [
  195 
 ]",NA
Constructing a return tuple,"Constructing the return tuple(s) is almost as easy.
  
 First, you get the called functions return type descriptor using the 
 get_call_ 
 result_type()
  function:
  
 get_call_result_type(fcinfo, &resultTypeId, &resultTupleDesc);
  
 The first argument to this function is the 
 FunctionCallInfo
  structure 
 fcinfo
  which is 
 used when calling the function you are currently writing (hidden behind the 
 PG_FUNCTION_ARGS
  macro in the C function declaration). The other two arguments are 
 addresses of the return type 
 Oid
  and 
 TupleDesc
  to receive the return tuple descriptor in 
 case the function returns a record type.
  
 Next, there is a safety assert for checking that the return type is really a record (or 
 composite) type:
  
 Assert(resultTypeId == TYPEFUNC_COMPOSITE);
  
 This is to guard against errors in the 
 CREATE FUNCTION
  declaration in SQL which 
 tells PostgreSQL about this new function.
  
 And there still remains one thing before we construct the tuple:
  
 BlessTupleDesc(resultTupleDesc);
  
 The purpose of 
 BlessTupleDesc()
  is to fill in the missing parts of the structure, which 
 are not needed for internal operations on the tuple, but are essential when the tuple is 
 returned from the function.
  
 So we are done with the tuple descriptor and finally, we can construct the tuple or 
 record it to be returned.
  
 The tuple is constructed using the 
 heap_form_tuple( resultTupleDesc, 
  
 retvals, retnulls );
  function which uses 
 TupleDesc
  we just prepared. It also 
 needs an array of 
 Datum
  to be used as values in the return tuple, and an array of 
 bool
 , 
 which is used to determine if any field should be set to 
 NULL
  instead of their 
 corresponding 
 Datum
  value. As all our fields are of type 
 int32
 , their values in 
 retvals
  
 are set using 
 Int32GetDatum(<localvar>)
 . The array 
 retnull
  is a simple array of 
 bool
  and needs no special tricks to set its values.
  
 Finally we return the constructed tuple:
  
 PG_RETURN_DATUM( HeapTupleGetDatum( rettuple ) );
  
 Here, we first construct 
 Datum
  from the tuple we just constructed using 
 HeapTupleGetDatum()
  and then use the 
 PG_RETURN_DATUM
  macro.
  
 [
  196 
 ]",NA
Interlude – what is Datum?,"In this chapter, we use something called 
 Datum
  in several places. This calls for a bit 
 of explanation about what a Datum is.
  
 In short, a 
 Datum
  is any data item the PostgreSQL processes and passes around. 
 Datum
  itself does not contain any type information or info on whether the field is 
 actually 
 NULL
 . It is just a pointer to a memory. You always have to find out (or know 
 beforehand) the type of any 
 Datum
  you use and also how to find out if your data 
 may be 
 NULL
  instead of any real value.
  
 In the preceding example, 
 GetAttributeByName(th, ""b"", &bisnull)
  returns a 
 Datum
 , and it can return something even when the field in the tuple is 
 NULL
 , so 
 always check for null-ness first. Also, the returned 
 Datum
  itself cannot be used for 
 much unless we convert it to some real type, as done in the next step using 
 DatumGetInt32()
 , which simply converts the vague 
 Datum
  to a real 
 int32
  value 
 basically doing a cast from a memory location of an undefined type to 
 int32
 .
  
 The definition of 
 Datum
  in 
 postgresql.h
  is 
 typedef Datum *DatumPtr;
  that is 
 anything pointed to by a 
 DatumPtr
 . Even though 
 DatumPtr
  is defined as 
 typedef 
 uintptr_t Datum;
  it may be easier to think of it as a (slightly restricted) 
 void *
 .
  
 Once more, any real substance is added to a 
 Datum
  by converting it to a real type.
  
 You can also go the other way, turning almost anything into a 
 Datum
  as seen at the end 
 of the function:
  
 HeapTupleGetDatum( rettuple )
  
 Again, for anything else in PostgreSQL to make use of such 
 Datum
 , the type 
 information must be available somewhere else, in our case the return type 
 definitions of the function.",NA
Returning a set of records,"Next, we will modify our function to not just return a single record of reordered 
 fields from an argument record, but to return all possible orderings. We will still 
 add one extra field 
 x
  as an example of how you can use the values you extracted 
 from the argument.
  
 For set-returning functions, PostgreSQL has a special calling mechanism where 
 PostgreSQL's executor machinery will keep calling the function over and over again 
 until it reports back that it does not have any more values to return. This return-
 and-continue behavior is very similar to how the 
 yield
  keyword works in Python 
 or JavaScript. 
  
 [
  197 
 ]",NA
Fast capturing of database changes ,"Some obvious things to code in C are logging or auditing triggers, which get called at each 
 INSERT
 , 
 UPDATE
 , or 
 DELETE
  to a table. We have not set aside enough space in this book to 
 explain everything needed for C triggers, but interested readers can look up the source 
 code for the 
 skytools
  package where you can find more than one way to write triggers in 
 C.
  
 The highly optimized C source for the two main triggers: 
 logtriga
  and 
 logutriga
 , 
 includes everything you need to capture these changes to a table and also to detect table 
 structure changes while the code is running.
  
 The latest source code for 
 skytools
  can be found at 
 http://pgfoundry.org/ 
 projects/skytools
 .
  
 [
  201 
 ]",NA
Doing something at commit/rollback,"As of this writing, there is no possibility to define a trigger function which is 
  
 executed 
 ON COMMIT
  or 
 ON ROLLBACK
 . However, if you really need to have some code 
 executed on these database events, you have a possibility to register a C-language 
 function to be called on these events. Unfortunately, this registration cannot be done in a 
 permanent way like triggers, but the registration function has to be called each time a 
 new connection starts:
  
 RegisterXactCallback(my_xact_callback, NULL); 
  
 Use 
 grep -r RegisterXactCallback
  in the 
 contrib/
  directory of PostgreSQL's 
 source code to find files with examples of actual callback functions.",NA
Synchronizing between backends,"All the preceding functions are designed to run in a single process/backend as if the 
 other PostgreSQL processes did not exist.
  
 But what if you want to log something to a single file from multiple backends?
  
 Seems easy—just open the file and write what you want. Unfortunately, it is not that 
 easy if you want to do it from multiple parallel processes and you do not overwrite or 
 mix up the data with what other processes write.
  
 To have more control over the writing order between backends, you need to have 
 some kind of inter-process synchronization, and the easiest way to do this in 
 PostgreSQL is to use 
 shared memory
  and 
 light-weight locks
  (
 LWLocks
 ).
  
 To allocate its own shared memory segment your 
 .so
  file needs to be preloaded, that is, it 
 should be one of the preloaded libraries given in the 
 postgresql.conf
  variable 
 shared_preload_libraries
 .
  
 In the 
 _PG_init()
  function of your module, you ask for the address of a name shared 
 memory segment. If you are the first one asking for the segment, you are also 
 responsible for initializing the shared structures; including, creating, and storing any 
 LWLocks you wish to use in your module.",NA
Writing functions in C++,"It is a bad idea to mix PostgreSQL with C++ for a number of reasons. It is better to wrap 
 your C++ code into C code behind extern C functions. This can be a problem if you 
 heavily use templates and libraries like boost.
  
 [
  202 
 ]",NA
Additional resources for C,"In this chapter, we were able to only give you a very basic introduction to what is 
 possible in C. Here is some advice on how to get more information.
  
 First, there is of course the chapter 
 C-Language Functions
  in the PostgreSQL manual. 
 This can be found online at 
 http://www.postgresql.org/docs/current/static/ 
 xfunc-c.html
  and as with most of the online PostgreSQL manuals, you usually can get 
 to older versions if they exist.
  
 The next one, surprisingly, is the PostgreSQL source code itself. However, you will usually 
 not get very far by just opening the files or using 
 grep
  to find what you need. 
  
 If you are good with using 
 ctags
  (
 http://en.wikipedia.org/wiki/Ctags
 ) or 
 another similar tool, it is definitely recommended.
  
 Also, if you are new to these types of large-code exploration systems, then a really 
 good resource for finding and examining PostgreSQL internals is maintained at 
 http://doxygen.postgresql.org/
 . This points to the latest 
 git
  master, so it may 
 not be accurate for your version of PostgreSQL, but it is usually good enough and at 
 least provides a nice starting point for digging around in the source code of your 
 version.
  
 Quite often, you will find something to base (parts of) your C source on in the 
 contrib/
  
 directory in the source code. To get an idea of what lies there, read through the 
 Appendix 
 F
 , 
 Additional Supplied Modules
  (
 http://www.postgresql. 
 org/docs/current/static/contrib.html
 ). It's entirely possible that somebody has 
 already written what you need. There are many more modules in 
 http:// 
 pgfoundry.org
  for you to examine and choose from. A word of warning though, while 
 modules in 
 contrib/
  are checked at least by one or two competent 
  
 PostgreSQL core programmers, the things at 
 pgfoundry
  can be of wildly varying 
 quality. The top active projects are really good; however, so the main things to look at 
 when determining if you can use them as a learning source are how active the project is 
 and when was it last updated.
  
 There is also a set of GUC parameters specifically for development and debugging 
 which are usually left out of the sample 
 postgresql.conf
  file. The descriptions and 
 an explanation are available at 
 http://www.postgresql.org/docs/current/ 
 static/runtime-config-developer.html
 .
  
 [
  203 
 ]",NA
Summary,"As C is the language that PostgreSQL itself is written in, it is very hard to draw a 
 distinction on what is an extension function using a defined API and what is hacking 
 PostgreSQL itself.
  
 Some of the topics that we did not touch at all were:
  
 • 
  
 • 
  
 • 
  
 Creating new installable types from scratch—see 
 contrib/hstore/
  for a full 
 implementation of a new type.
  
 Creating new index methods—download an older version of PostgreSQL to see 
 how full text indexing support was provided as an add-on.
  
 Implementing a new 
 PL/*
  language—search for 
 pl/lolcode
  for a language, the 
 sole purpose of which is to demonstrate how a PostgreSQL's 
 PL/* 
 language 
 should be written (see 
 http://pgfoundry.org/projects/ pllolcode/
 ). You 
 may also want to check out the source code for PL/Proxy for a clean and well-
 maintained PL language. (The usage of PL/Proxy is described in the next 
 chapter.)
  
 Hopefully, this chapter gave you enough info to at least start writing PostgreSQL 
 extension functions in C.
  
 If you need more than what is available here or in the official PostgreSQL 
 documentation, then remember that lots of PostgreSQL's backend developer 
 documentation—often including answers to the questions How? and Why?—is in 
 the source files. A lot of that can also be relevant to C extensions.
  
 So remember—Use The Source, Luke!
  
 In the next chapter we will discuss scaling the database using PL/Proxy.
  
 [
  204 
 ]",NA
Scaling Your Database with ,NA,NA
PL/Proxy,"If you have followed the advice in the previous chapters for doing all your database access 
 through functions, you are in a great position to scale your database by 
 horizontally
  
 distributing the data over multiple servers, also known as 
 database sharding
 . Horizontal 
 distribution means that you keep just a portion of a table on each 
 partition
  of the 
 database, and that you have a method to automatically access the right database when 
 accessing the data.
  
 We will gently introduce the concepts leading to the PL/Proxy partitioning 
  
 language, and then delve into the syntax and proper usage of the language itself. Let's 
 start with writing a scalable application from scratch. First, we will write it to be as highly 
 performing as possible on one server. Then, we will scale it by spreading it out on several 
 servers. We will first get this implemented in PL/Pythonu and then in PL/Proxy.
  
  
 This approach is worth taking, only if you have (plans for) a really 
  
  
 large database. For most databases, one server plus one or perhaps 
  
 two hot standby servers should be more than enough.",NA
Creating a simple single-server chat,"Perhaps, the simplest application needing this kind of scalability is a messaging (or 
 chat) application; so, let's write one.",NA
Dealing with success – splitting tables ,NA,NA
over multiple databases ,"Now, let's roll forward a little in time and assume you have been successful enough to 
 attract tens of thousands of users and your single database starts creaking under the 
 load.
  
 My general rule of thumb, is to start planning for a bigger machine, or splitting the 
 database, when you are over 80 percent utilization at least for a few hours a day. It's 
 good to have a plan earlier, but now you have to start doing something about really 
 carrying out the plan.",NA
What expansion plans work and when?,"There are a couple of popular ways to grow database-backed systems. Depending on your 
 use case, not all ways will work.
  
 [
  212 
 ]",NA
Moving to a bigger server,"If you suspect that you are near your top load for the service or product, you can simply 
 move to a more powerful server. This may not be the best long-time scaling solution if 
 you are still in the middle, or even in the beginning of your growth. You will run out of 
 bigger
  machines to buy long before you are done. Servers also become 
 disproportionately more expensive as the size increases, and you will be left with at 
 least one 
 different
 , and thus not easily replaceable, server once you implement a proper 
 scaling solution.
  
 On the other hand, this will work for some time and is often the easiest way to get 
 some headroom while implementing real scaling solutions.",NA
Master-slave replication – moving reads to slave,"Master-slave replication, either trigger-based or WAL-based, works reasonably well in 
 cases where the large majority of the database accesses are reads. Some things that fall 
 under this case, are website content managers, blogs, and other publishing systems.
  
 As our chat system has more or less a 1:1 ratio of writes and reads, moving reads to a 
 separate server will buy us nothing. The replication itself is more expensive than the 
 possible win from reading from a second server.",NA
Multimaster replication,"Multi master replication is even worse than master-slave(s) when the problem is scaling 
 a write-heavy workload. It has all the problems of master-slave, plus it introduces extra 
 load via cross-partition locking or conflict resolution requirements, which further slows 
 down the whole cluster.",NA
Data partitioning across multiple servers,"The obvious solution to scaling writes is to split them between several servers. Ideally 
 you could have, for example, four servers and each of them getting exactly one fourth 
 of the load.
  
 In this case, each server would hold a quarter of users and messages, and serve a 
 quarter of all requests.
  
 To make the change transparent for database clients, we introduce a layer of proxy 
 databases. These proxy databases can either reside on the same hosts as the partition 
 databases or be on their own host. The role of the proxy databases is to pretend to be 
 the database for clients, but in fact delegate the real work to partitions by calling the 
 right function in the right partition database.
  
 [
  213 
 ]",NA
Splitting the data,"If we split the data, we need a simple and efficient way to determine which server stores 
 each data row. If the data had an integer primary key, you could just go round-robin, store 
 the first row on the first server, the second row on the second, and so on. This would give 
 you a fairly even distribution, even when rows with certain IDs are missing.
  
 The partitioning function for selecting between four servers would be simply:
  
 partition_nr = id & 3
  
 The partitioning mask 
 3
  (binary 11) is for the first two bits. For eight partitions, you 
 would use 
 7
  (binary 111), and for 64 servers it would be 
 63
  (00111111). It is not as easy 
 with things like usernames, where putting all names starting with an A first, B second, 
 and so on, does not produce an even distribution.
  
 Turning the username into a fairly evenly distributed integer via the hash function 
 solves this problem and can be used directly to select the partition.
  
 partition_nr = hashtext(username) & 3
  
 This would distribute the users in the following manner:
  
 hannu=# SELECT username, hashtext(username) & 3 as partition_nr FROM 
 user_info; 
  
 -[ RECORD 1 ]+-------- 
  
 username     | bob 
  
 partition_nr | 1 
  
 -[ RECORD 2 ]+-------- 
  
 username     | jane 
  
 partition_nr | 2 
  
 -[ RECORD 3 ]+-------- 
  
 [
  214 
 ]",NA
PL/Proxy – the partitioning language,"The rest of this chapter is devoted to the PL/Proxy language. First, we will install it. 
 Then, we will look at its syntax and ways to configure the partitions for its use. Finally, 
 we will discuss how to do the actual data migration from a single database to a 
 partitioned one and then look at several usage examples.
  
 [
  216 
 ]",NA
Installing PL/Proxy,"If you are on Debian, Ubuntu, or a Red Hat variant, installing the language is easy.
  
 First, you have to install the required packages on your operating system:
  
 sudo apt-get install postgresql-9.4-plproxy 
  
 Or:
  
 sudo yum install plproxy94 
  
 If you need to install PL/Proxy from the source, you can download it from 
 http://pgfoundry.org/projects/plprox
 , extract the sources in the 
 contrib 
 folder of your PostgreSQL source tree and run 
 make
  and 
 make install.
  
 To install PL/Proxy you should run the 
 plproxy.sql
  file, which is part of the source code 
 or the package you installed.",NA
The PL/Proxy language syntax,"The PL/Proxy language itself is very simple. The purpose of a PL/Proxy function is to 
 hand off the processing to another server, so that the language only needs six 
 statements:
  
 • 
  
 • 
  
 • 
  
 CONNECT
  or 
 CLUSTER
  and 
 RUN ON
  for selecting the target database partition 
 SELECT
  and 
 TARGET
  for specifying the query to run 
  
 SPLIT
  for splitting an 
 ARRAY
  argument between several sub arrays for 
 running on multiple partitions",NA
"CONNECT, CLUSTER, and RUN ON","The first group of statements handles the remote connectivity to the partitions. The 
 help determines which database to run the query on. You specify the exact partition 
 to run the query using 
 CONNECT
 :
  
 CONNECT 'connect string' ;
  
 Here, 
 connect string
  determines the database to run. 
 connect string
  is the 
 standard PostgreSQL connect string you would use to connect to the database from 
 a client application, for example: 
 dbname=p0 port=5433 username=test 
 host=localhost
 .
  
 [
  217 
 ]",NA
SELECT and TARGET ,"The default behavior of a PL/Proxy function, if no 
 SELECT
  or 
 TARGET
  is present, is to call 
 the function with the exact same signature as itself in the remote partition.
  
 Suppose we have the function: 
  
  
 CREATE OR REPLACE FUNCTION login(
  
  
  IN i_username text, IN i_pwdhash text,
  
  
 OUT status int, OUT message text ) 
  
 AS $$
  
 [
  218 
 ]",NA
SPLIT – distributing array elements over several ,NA,NA
partitions,"The last PL/Proxy statement is for cases where you want some bigger chunk of work to be 
 done in appropriate partitions. 
  
 [
  219 
 ]",NA
The distribution of data ,"First, what is a cluster in PL/Proxy? Well, the cluster is a set of partitions that make up the 
 whole database. Each cluster consists of a number of partitions, as determined by the 
 cluster configuration. Each partition is uniquely specified by its connect string. The list of 
 connection strings is what makes up a cluster. The position of the partition in this list is 
 what determines the partition number, so the first element in the list is partition 
 0
 , the 
 second partition is 
 1
 , and so on.
  
 The partition is selected by the output of the 
 RUN ON
  function, and then masked by the 
 right number of bits to map it on the partitions. So, if
  hashtext(i_username
 ) returns 
 14 and there are four partitions (2 bits, mask binary 11 or 3 in decimal), the partition 
 number will be 14 and 3 = 2, and the function will be called on partition2 (starting from 
 zero), which is the third element in the partition list.
  
 [
  221 
 ]",NA
Configuring the PL/Proxy cluster using functions ,"This is the original way to configure PL/Proxy, which works on all versions of 
 PostgreSQL. When a query needs to be forwarded to a remote database, the function 
 plproxy.get_cluster_partitions(cluster)
  is invoked by PL/Proxy to get the 
 connection string to use for each partition.
  
 The following function is an example, which returns information for a cluster with four 
 partitions, 
 p0
  to 
 p3
 :
  
 CREATE OR REPLACE FUNCTION plproxy.get_cluster_partitions(cluster_name 
 text) 
  
 RETURNS SETOF text AS $$ 
  
 BEGIN 
  
  IF cluster_name = 'messaging' THEN 
  
  
  
  RETURN NEXT 'dbname=p0'; 
  
  
  
  RETURN NEXT 'dbname=p1'; 
  
  
  
  RETURN NEXT 'dbname=p2'; 
  
  
  
  RETURN NEXT 'dbname=p3'; 
  
  ELSE
  
  
  
  RAISE EXCEPTION 'Unknown cluster'; 
  
  END IF; 
  
 END; 
  
 $$ LANGUAGE plpgsql; 
  
 [
  222 
 ]",NA
Configuring the PL/Proxy cluster using SQL/MED ,"Since version 8.4, PostgreSQL has support for an SQL standard for management of 
 external data, usually referred to as SQL/MED. SQL/MED is simply a standard way to 
 access a database. Using functions to configure partitions is arguably insecure, as any 
 caller of 
 plproxy.get_cluster_partitions()
  can learn connection strings for 
 partitions that may contain sensitive info like passwords. PL/Proxy also provides a way 
 to do the cluster configuration using SQL/MED, which follows the standard SQL security 
 practices.
  
 The same configuration, as discussed earlier, when done using SQL/MED, is 
 as follows:
  
 1. First, create a foreign data wrapper called 
 plproxy
 :
  
 proxy1=# CREATE FOREIGN DATA WRAPPER plproxy;
  
 2. Then, create an external server that defines both the connection options and 
  
 the partitions:
  
 proxy1=# CREATE SERVER messaging FOREIGN DATA WRAPPER plproxy 
 proxy1-# OPTIONS (connection_lifetime '1800', 
  
 proxy1(#          p0 'dbname=p0', 
  
 proxy1(#          p1 'dbname=p1', 
  
 proxy1(#          p2 'dbname=p2', 
  
 proxy1(#          p3 'dbname=p3' 
  
 proxy1(# ); 
  
 CREATE SERVER
  
 3. Then, grant usage on this server to either 
 PUBLIC
 , so all users can use it:
  
 proxy1=# CREATE USER MAPPING FOR PUBLIC SERVER messaging; 
 CREATE USER MAPPING
  
 Or, to some specific users or groups:
  
 proxy1=# CREATE USER MAPPING FOR bob SERVER  messaging 
 proxy1-#   OPTIONS (user 'plproxy', password 
  
  
  'very.secret'); 
  
 CREATE USER MAPPING
  
 4. Finally, grant usage on the cluster to the users who need to use it:
  
 proxy1=# GRANT USAGE ON FOREIGN SERVER messaging TO bob; 
 GRANT
  
 [
  224 
 ]",NA
Moving data from the single to the partitioned ,NA,NA
database,"If you can schedule some downtime and your new partition databases are as big as 
 your original single database, the easiest way to partition the data is to make a full 
 copy of each of the nodes and then simply delete the rows that do not belong to the 
 partition:
  
 pg_dump chap10 | psql p0
  
 psql p0 -c 'delete from message where hashtext(to_user) & 3 <> 0'
  
 psql p0 -c 'delete from user_info where hashtext(username) & 3 <> 0'
  
 Repeat this for partitions 
 p1
  to 
 p3
 , each time deleting rows which don't match the 
 partition number (
 psql chap10p1 -c 'delete … & 3 <> 1
 ).
  
  
 Remember to vacuum when you are finished deleting the rows. 
  
  
 PostgreSQL will leave the dead rows in the data tables, so do a little 
  
 maintenance while you have some downtime.
  
 When trying to delete from 
 user_info
 , you will notice that you can't do it without 
 dropping a foreign key from 
 message.from_user
 .
  
 Here, we could decide that it is Okay to keep the messages on the receivers partition 
 only, and if needed, that the sent messages can be retrieved using a 
 RUN ON ALL 
 function. So, we will drop the foreign key from 
 messages.from_user
 .
  
 psql p0 -c 'alter table message drop constraint 
  
  message_from_user_fkey'
  
 There are other options, when splitting the data, that require less disk space usage for 
 a database system, if you are willing to do more manual work.
  
 For example, you can copy over just the schema using 
 pg_dump -s
  and then use 
 COPY
  from an SQL statement to move over just the needed rows:
  
 [
  225 
 ]",NA
Connection Pooling,"PL/Proxy does not include a connection pooler and it is a good idea to use one like 
 pgbouncer and pgpool. A connection pooler is a utility that helps you reduce the operating 
 cost of database, when its large number of physical connections are pulling performance 
 down.
  
 PL/Proxy opens a connection to each partition from each backend process and a large 
 number of connections can bring the performance of the server down. Using the 
 connection pool will allow you to multiplex a lot of client connections over a small 
 number of database connections. The pgbouncer is a recommended connection pooler 
 because it's lightweight and  very easy to setup. PL/Proxy attempts to connect to 
 pgbouncer using the same database connection interface that it uses to connect to any 
 PostgreSQL database. The client application supplies the IP address of the host running 
 pgbouncer and the port number on which pgbouncer is listening for connections.",NA
Summary,"In this chapter, we have gone over the process of database sharding for databases that are 
 too big to take the write load on a single host, or where you just want to have the added 
 resilience of having a system where one host being down does not bring the whole system 
 down.
  
 In short, the process is:
  
 • 
  
 Decide which tables you want to split over multiple hosts
  
 • 
  
 Define a partitioning key
  
 • 
  
 Add the partition databases and move the data
  
 • 
  
 Set up the proxy functions for all the functions accessing those tables
  
 • 
  
 Watch for a little while that everything is working
  
 • 
  
 Relax
  
 [
  226 
 ]",NA
PL/Perl – Perl Procedural ,NA,NA
Language,"Perl is a feature-rich language, which has been around for a long time. PostgreSQL 
 allows you to write Perl routines that are stored and executed inside the database. 
 This ability is quite unique to PostgreSQL and allows you to do a lot of cool things, such 
 as using Perl's text manipulation features inside a database. PL/Perl is one of the many 
 languages that PostgreSQL supports for writing server-side routines.
  
 As discussed in the earlier chapters, PostgreSQL supports trusted and untrusted 
 languages. PL/Perl is available in both these flavors. The trusted version runs inside a 
 safe container and, therefore, not the entire set of familiar native Perl operations is 
 allowed.
  
 In this chapter, we will cover the following topics:
  
 • 
  
 When to use PL/Perl
  
 • 
  
 How to install and write a basic function
  
 • 
  
 Passing arguments to and from PL/Perl functions
  
 • 
  
 Writing triggers in PL/Perl
  
 • 
  
 A brief introduction to untrusted PL/Perl",NA
When to use PL/Perl,"We will briefly discuss how to create Perl functions and use them in triggers. Some of you 
 might ask when it is beneficial to use PL/Perl, since PostgreSQL supports a variety of 
 languages that can be used to create triggers or write stored procedures/functions.
  
 You might be more familiar with a language such as Perl than with Python or Tcl. This 
 is a pretty good reason to choose a certain language over the other.",NA
Installing PL/Perl,"PL/Perl is not installed by default if you used the standard source distribution to 
 install PostgreSQL. If you compile PostgreSQL from the source, you need to 
 configure the script with the 
 --with-perl
  option.
  
 If you used a binary distribution on your platform, you can normally install 
  
 PL/Perl using your package manager. You can search for 
 postgresql-plperl
 , or a 
 similar package name, as it differs across the distributions. Once PostgreSQL is compiled 
 with the correct option, or you have installed the appropriate package, you can create the 
 PL language using the 
 createlang
  utility or the 
 CREATE LANGUAGE 
 command, as shown 
 here:
  
 $ createlang plperl template1
  
 Or the untrusted version, such as the following command:
  
 $ createlang plperlu template1
  
 hon",NA
A simple PL/Perl function,"Now, let's write our first simple Perl function to make sure that PL/Perl is installed 
 correctly. We will use a sample function from the Perl FAQs, at 
 http://perldoc.
  
 perl.org/perlfaq5.html#How-can-I-output-my-numbers-with-commas 
 -added%3f
 , to write a PL/Perl function that adds commas to a number:
  
 CREATE OR REPLACE FUNCTION commafy (integer) RETURNS text 
  
 AS $$
  
  local $_  = shift;
  
  1 while s/^([-+]?\d+)(\d{3})/$1,$2/;
  
  return $_;
  
 $$ LANGUAGE plperl;
  
 [
  230 
 ]",NA
Passing and returning non-scalar types,"If you pass array types as arguments to the PL/Perl function, they are passed as the 
 blessed 
 ostgreSQL::InServer::ARRAY
  objects. In Perl, 
 bless
  associates an object with 
 a class. This object can be treated as an array reference or as a string. If you have to return 
 an array type, you must return an array by reference. Let's take a look at the following 
 example:
  
 CREATE OR REPLACE FUNCTION reverse(int[]) RETURNS int[]
  
 AS $$
  
  my $arg = shift; # get the reference of the argument
  
 [
  231 
 ]",NA
Writing PL/Perl triggers ,"If you want to write trigger functions using Perl, then PL/Perl allows you to do all the 
 good stuff that you have learned so far using PL/PgSQL. Let's rewrite an example we 
 demonstrated in 
 Chapter 5
 , 
 PL/pgSQL Trigger Functions
 , in PL/Perl. Recall the 
 simple, ""Hey, I am called"" trigger. The PL/Perl version of the example looks as shown 
 in the following code. We probably don't need to provide a more complex example, 
 as this simple example demonstrates the PL/Perl syntax in a sufficient way:
  
 CREATE OR REPLACE FUNCTION notify_trigger_plperl() RETURNS TRIGGER 
 AS $$ 
  
  
  $result = sprintf('Hi, I got %s invoked FOR %s %s %s on %s', 
  
     
  $_TD->{name}, 
  
     
  
  $_TD->{level}, 
  
     
  
  $_TD->{when}, 
  
     
  
  $_TD->{event}, 
  
     
  
  $_TD->{table_name}
  
    
  );
  
  
  if(($_TD->{event} cmp 'UPDATE') == 0){
  
    
  $result .= sprintf(' OLD = %s AND NEW=%s', $_TD->{old}{i},  
 $_TD->{new}{i});
  
    
  $_TD->{new}{i} = $_TD->{old}{i} + $_TD->{new}{i};
  
    
  elog(NOTICE, $result);
  
    
  return ""MODIFY"";
  
  
  } elsif(($_TD->{event} cmp 'DELETE') == 0){
  
    
  elog(NOTICE, ""Skipping Delete"");
  
    
  return ""SKIP"";
  
  
  }
  
  
  elog(NOTICE, $result); 
  
 $$ LANGUAGE plperl;
  
 CREATE TABLE notify_test_plperl(i int);
  
 CREATE  TRIGGER notify_insert_plperl_trigger
  
  
  BEFORE INSERT OR UPDATE OR DELETE ON notify_test_plperl
  
  FOR EACH ROW 
  
 EXECUTE PROCEDURE notify_trigger_plperl();
  
 [
  235 
 ]",NA
Untrusted Perl,"We discussed untrusted PL/PythonU in 
 Chapter 8
 , 
 Using Unrestricted Languages
 . 
 PL/Perl is also available as an untrusted language. The trusted version runs inside a 
 security context that does not allow interaction with the environment. Just like 
 PL/Pythonu
 , we can bypass the security restrictions using 
 PL/Perlu
  or the untrusted 
 version. Let's rewrite the directory listing function 
 list_folder 
  
 from 
 Chapter 8
 , 
 Listing directory contents
  to a Perl equivalent:
  
 CREATE OR REPLACE FUNCTION list_folder_plperl(directory VARCHAR) 
 RETURNS SETOF TEXT
  
 AS $$
  
  my $d = shift;
  
  opendir(D, ""$d"") || elog (ERROR,'Cant open directory '.$d) ;
  
  my @list = readdir(D);
  
 [
  237 
 ]",NA
Summary,"The powerful Perl language is available in PostgreSQL as PL/Perl or PL/Perlu. 
  
 This allows you to write stored procedures in Perl and to take advantage of all the cool 
 features that Perl has to offer, such as a very large collection of modules available on 
 CPAN. You can do almost everything you want with PL/pgSQL or PL/Python, including 
 database access and writing triggers. The untrusted version of PL/Perl allows you to 
 interact with the environment. PL/Perl will normally outperform PL/pgSQL in non-
 data intensive functions that focus more on string manipulation and computation.
  
 In the next chapter, we will discuss another popular PL language called Pl/Tcl.
  
 [
  239 
 ]",NA
PL/Tcl – Tcl Procedural ,NA,NA
Language,"Tools Command Language
  (
 Tcl
 ), also commonly known as 
 tickle
 , has been around for a 
 long time. It was created by John Ousterhout in 1988 and got a lot of traction for rapid 
 prototyping and scripted applications.
  
 In this chapter, we will take a brief look at PL/Tcl. Between PL/Perl, PL/Python, and 
 PL/pgSQL, you have very powerful languages available at your disposal that can do 
 almost anything you need. For some other things, you have the option to write your 
 functions in C. You might wonder why it is useful to discuss PL/Tcl. For a long time, in 
 the early days of PostgreSQL, PL/Tclu (untrusted PL/Tcl) was the only way to do things 
 outside
  PostgreSQL, such as interacting with the operating system. A lot of people still 
 use it, and I personally think that it is so clean and powerful that it should not be 
 overlooked.
  
 PL/Tcl is available as a trusted and an untrusted language. This is achieved by 
 providing two different Tcl interpreters. PL/Tclu uses the standard Tcl interpreter, 
 while PL/Tcl uses a special 
 Safe Tcl
  mechanism.
  
  
 You can read more about safe Tcl at 
 http://www.tcl.tk/software/
  
  
 plugin/safetcl.html
 .
  
 In this chapter, we will cover the following topics:
  
 • 
  
 Installing PL/Tcl and writing a simple function
  
 • 
  
 Passing simple and complex parameters
  
 • 
  
 Accessing a database from a Pl/Tcl function
  
 • 
  
 Writing database triggers using Pl/Tcl",NA
Installing PL/Tcl,"PL/Tcl is not installed by default, if you've used the standard source distribution to install 
 PostgreSQL. If you compiled PostgreSQL from the source, you need to run the configure 
 script with the 
 –-with-tcl
  option.
  
 If you've used a binary distribution on your platform, you can normally install PL/Tcl 
 using your package manager. You can search for 
 postgresql-pltcl
 , or a similar package 
 name, as it differs across distributions. Once PostgreSQL is compiled with the correct 
 option, or you have installed the appropriate package, you can create the language using 
 the 
 createlang
  utility or the 
 CREATE LANGUAGE
  command:
  
 $ createlang pltcl template1
  
 You can use also use the untrusted version to create the language, as follows:
  
 $ createlang pltclu template1",NA
A simple PL/Tcl function,"Now, let's write our first simple Tcl function to make sure that PL/Tcl is installed. We 
 will write a simple factorial calculation function, as shown here:
  
 CREATE OR REPLACE FUNCTION tcl_factorial(integer) RETURNS integer
  
 AS $$
  
  set i 1; set fact 1
  
  while {$i <= $1} {
  
  set fact [expr $fact * $i]
  
  incr i
  
  }
  
  return $fact
  
 $$ LANGUAGE pltcl  STRICT;
  
 This function calculates the factorial of a number in an iterative way. Let's try and 
 run it:
  
 postgres=# SELECT tcl_factorial(5);
  
  tcl_factorial 
  
 ---------------
  
  120
  
 (1 row)
  
 It works and the function looks similar to other functions we have been writing in 
 PL/pgSQL and PL/Python. The 
 CREATE FUNCTION
  statement creates a function. It needs a 
 name, function argument type list (you have to use parentheses, even if there are no 
 arguments), a result type, and a language.
  
 [
  242 
 ]",NA
Null checking with Strict functions ,"The 
 STRICT
  keyword will save us from checking the 
 null
  input parameters. If you have 
 specified a function as 
 STRICT
  and any of the input parameters are null, it results in the 
 function not being called and a null result set is returned immediately:
  
 postgres=# SELECT 
 tcl_factorial(null);
  
  tcl_factorial 
  
 ---------------
  
 (1 row)
  
 If you don't want to create a 
 STRICT
  function, or you'd like to do the null checking 
 yourself, you can rewrite the function, as shown in the following code snippet. This is 
 useful if you have multiple parameters and you want to allow some parameters to be 
 null:
  
 CREATE OR REPLACE FUNCTION tcl_factorial_ns(integer) RETURNS integer 
 AS $$
  
   
  if {[argisnull 1]} {
  
      
  elog NOTICE ""input is null""
  
      
  return -1
  
   
  } 
  
   
  set i 1; set fact 1
  
   
  while {$i <= $1} {
  
     
  set fact [expr $fact * $i]
  
     
  incr i
  
   
  }
  
  
  return $fact 
  
 $$ LANGUAGE pltcl;
  
 The 
 argisnull
  function is used to check for null values. In the preceding example, the 
 function returns 
 -1
  if the input argument is null, just to demonstrate that it works. If you 
 want to return a null value from the function, you can use the built-in function 
 return_null
 . In the preceding example, you can also see how to use the 
 elog
  function 
 in PL/Tcl:
  
 [
  243 
 ]",NA
The parameter format,"All input parameters passed to PL/Tcl are converted to text. Within a PL/Tcl function, 
 all values are text. When the function returns, another conversion is performed from 
 the text string to the return type of the function, as long as the text being returned is an 
 appropriate representation of the return type of the Pl/Tcl function; otherwise, the 
 function will result in an error.",NA
Passing and returning arrays,"If you pass array types as an argument to the PL/Tcl function, they are passed as a 
 string value, along with the brackets and the commas. Let's take a look at an example:
  
 CREATE OR REPLACE FUNCTION tcl_array_test(integer[]) RETURNS int
  
 AS $$
  
  set length [string length $1]
  
  return $length
  
 $$ LANGUAGE pltcl;
  
 testdb=# select tcl_array_test(ARRAY[1,2,3]);
  
  tcl_array_test 
  
 ----------------
  
  7
  
  (1 row)
  
 You are probably surprised at the return value of the preceding function. You passed 
 an integer array to the function that is converted to a string value 
 {1,2,3}
 , the length 
 of which is indeed 7. If you want to process array values independently, you need a bit 
 of string manipulation to extract the list out of the string, do the manipulation, and 
 convert it back to the string format that you received it in.
  
 Let's take a look at an example PL/Tcl function that will reverse an integer array and 
 return the reversed integer array:
  
 CREATE OR REPLACE FUNCTION tcl_reverse_array(integer[]) RETURNS 
 integer[]
  
 AS $$
  
  set lst [regexp -all -inline {[0-9]} $1]
  
 [
  244 
 ]",NA
Passing composite-type arguments ,"A composite-type, such as a table, or a user-defined type is passed to the PL/Tcl function 
 as an associative array (
 Hash
  table). The attribute names of the composite-type are the 
 element names in the array. Attributes with 
 NULL
  values are not available in the Tcl array.
  
 Let's take a look at an arbitrary example. We will create a function that takes a 
 composite type as an argument and does some calculations based on the attribute 
 values of the type. Let's create our type and the function:
  
 CREATE TABLE orders(orderid int, num_people integer, order_amount 
 decimal);
  
 INSERT INTO orders VALUES(1,1,23); 
  
 INSERT INTO orders VALUES(2,3,157); 
  
 INSERT INTO orders 
 VALUES(3,5,567.25); 
  
 INSERT INTO orders VALUES(4,1,100);
  
 CREATE OR REPLACE FUNCTION tip_calculator(orders, integer) RETURNS 
 decimal 
  
 AS $$
  
 [
  245 
 ]",NA
Accessing databases ,"PL/Tcl functions provide you with SPI functions to access the database and run DML/DDL 
 statements.
  
 The functions are the following:
  
  
 • 
  
 spi_exec
 : This executes a SQL statement 
  
  
  
 spi_prepare
 : This prepares a SQL statement• 
  
  
 • 
  
 spi_execp
 : This executes a prepared statement 
  
 The 
 spi_exec
  function has the following syntax: 
  
  
 spi_exec ?-count n? ?-array name? command ?loop-body?
  
 The 
 spi_exec
  function runs a SQL statement, and it takes some optional parameters, as 
 follows:
  
 • 
  
 • 
  
 -count
 : This parameter allows you to specify the maximum number of rows 
 processed by the command. If you provide the value 3, only 3 rows will be 
 processed. This is similar to specifying 
 FETCH [n]
  in a cursor.
  
 -array
 : If this parameter is specified, the column values are stored into a 
 named associative array and the column names are used as array indexes. 
  
 If this parameter is not specified, the result values are stored in the Tcl 
 variables of the same name.
  
 If there is a loop body specified, then it is treated as a script that is run for each row.
  
 Let's take a look at an example of how to run SQL statements inside a PL/Tcl 
 function. The following example, creates a function that loops over the rows in a 
 table and updates a column in each row: 
  
  
 CREATE TABLE emp_sales(empid int PRIMARY KEY, 
  
  
  
  sales_amnt decimal, 
  
  
  
  comm_perc decimal, 
  
  
  
  comm_amnt decimal);
  
 INSERT INTO emp_sales VALUES (1,32000, 5, NULL); 
 INSERT INTO emp_sales VALUES (2,5231.23, 3, 
 NULL); INSERT INTO emp_sales VALUES (3,64890, 
 7.5, NULL);
  
 CREATE OR REPLACE FUNCTION tcl_calc_comm() RETURNS int 
 AS $$
  
  
  spi_exec -array C ""SELECT * FROM emp_sales"" {
  
 [
  247 
 ]",NA
Writing PL/Tcl triggers ,"If you want to write trigger functions using Tcl, then PL/PTcl allows you to do all 
 the good stuff that you have learned so far, using PL/pgSQL, PL/Perl, and 
 PL/Python. Let's rewrite an example we demonstrated in 
 Chapter 5
 , 
 PL/pgSQL 
 Trigger Functions
 . Recall the simple, ""Hey, I am called"" trigger. This is how the 
 PL/Tcl version of the example looks:
  
 CREATE OR REPLACE FUNCTION notify_trigger_pltcl() RETURNS TRIGGER 
 AS $$ 
  
  
  set result [format ""Hi, I got %s invoked FOR %s %s %s on %s""  
  
  $TG_name $TG_level $TG_when $TG_op $TG_table_name]
  
  
  if {$TG_op == ""UPDATE""} { 
  
  
  append result [format "" OLD = %s AND NEW=%s"" $OLD(i) $NEW(i)]
  
   
  set NEW(i) [expr $OLD(i) + $NEW(i)]
  
    
  elog NOTICE $result
  
    
  return [array get NEW]
  
  
  } elseif {$TG_op == ""DELETE""} {",NA
Untrusted Tcl ,"Using untrusted Tcl (
 pltclu
 ) is one of the oldest ways to do things outside the 
 database. pltclu is executed using a normal Tcl interpreter and is pretty much free to 
 do anything you'd like. Tcl has a lot of commands available to interact with the 
 operating system and the environment. We will now take a look at a few 
  
 simple examples.
  
 The first example, reads the contents of the file and returns them as text, as shown in the 
 following code:
  
 CREATE OR REPLACE FUNCTION read_file(text) RETURNS text 
 AS $$
  
  
  set fptr [open $1]
  
  
  set file_data [read $fptr]
  
 [
  250 
 ]",NA
Summary,"The powerful, yet clean, Tcl language is available in PostgreSQL as PL/Tcl and the 
 untrusted PL/Tclu. Both the languages use different Tcl interpreters. PL/Tclu is the 
 oldest language used in PostgreSQL, to access things outside the database. It allows 
 you to write stored procedures in Tcl and takes advantage of all the cool features that 
 Tcl has to offer. You can do almost everything you can with other PL languages, 
 including database access and writing triggers. The only major disadvantage of 
 PL/Tcl is that it does not allow you to return composite types and sets from a 
 function.
  
 In the next chapter, we will explore how to publish your code as a 
 PostgreSQL extension.
  
 [
  253 
 ]",NA
Publishing Your Code as ,NA,NA
PostgreSQL Extensions,"If you are new to PostgreSQL, now is the time to dance for joy.
  
 Now that you're done dancing, I'll tell you why. You have managed to avoid the ""bad 
 old days"" of contrib modules. Contrib modules are the installation systems that were 
 used to install related PostgreSQL objects prior to Version 9.1. They may be additional 
 data types, enhanced management functions, or just really any type of module you 
 want to add to PostgreSQL. They consist of any group of related functions, views, 
 tables, operators, types, and indexes that were lumped into an installation file and 
 committed to the database in one fell swoop. Unfortunately, contrib modules only 
 provided for installation, and nothing else. In fact, they were not really an installation 
 system at all. They were just some unrelated SQL scripts that happened to install 
 everything that the author thought you needed.
  
 PostgreSQL extensions provide many new services that a package management system 
 should have. Well, at least the ones that module authors complained the most about not 
 being present.
  
 Some of the new features that you will be introduced to in this chapter include:
  
 • 
  
 Versioning
  
 • 
  
 Dependencies
  
 • 
  
 Updates
  
 • 
  
 Removal",NA
When to create an extension,"Well, first you have to understand that extensions are all about togetherness. Once the 
 objects from a contrib module were installed, PostgreSQL provided no way to show a 
 relationship between them. This led many developers to create their own (and at 
 times rather ingenious) methods to version, update, upgrade, and uninstall all of the 
 necessary ""stuff"" to get a feature to work.
  
 So, the first question to ask yourself when contemplating a PostgreSQL extension as a 
 way to publish your code is ""How does all of the 'stuff' in my extension relate 
 together?""
  
 This question will help you make extensions that are as granular as reasonable. 
  
 If the objective is to enhance PostgreSQL with the ability to provide an inventory 
 management system, it might be better to start with an extension that provides a bill of 
 materials data type first, and subsequently build additional extensions that are 
 dependent upon that one. The moral of the story is to dream big, but create each 
 extension with only the smallest number of related items that make sense.
  
 A good example of an extension that provides a feature to PostgreSQL is OpenFTS. This 
 extension provides full text searching capabilities to PostgreSQL by creating data 
 types, indexes, and functions that are well related to each other.
  
 Another type of extension is PostGIS, which provides a rich set of tools to deal with 
 geographic information systems. Although this extension provides many more bits of 
 functionality than OpenFTS, it is still as granular as possible by virtue of the fact that 
 everything that is provided is necessary for geographic software development.
  
 Possibly, you are a book author, and the only relationship that the objects in your 
 extension have is that they need to be conveniently removed when your poor victim 
 ...ahem...the reader is through with them. Welcome to the wonders of extensions.
  
 For a list of very useful extensions that have gained some community popularity, you 
 might want to take a look at this page fairly often: 
 http://www.postgresql. 
 org/download/products/6/
 .
  
 You should also take a look at the PostgreSQL extension network at 
 http://www. 
 pgxn.org
 . Please note that installing extensions generally means running a script as a 
 superuser; and nearly anyone can upload to pgxn, meaning that individuals really need 
 to vet anything they get from pgxn very carefully.
  
 [
  256 
 ]",NA
Unpackaged extensions,"Starting with Version 9.1, PostgreSQL provides a convenient way to move from the 
 primordial ooze of unversioned contrib modules into the brave new world of 
 extensions. Basically, you provide a SQL file to show the relationship of the objects to 
 the extension. The contrib module's cube provides a good example of this in 
 cube--
 unpackaged--1.0.sql
 :
  
 /* contrib/cube/cube--unpackaged--1.0.sql */
  
 -- complain if script is sourced in psql, rather than via CREATE 
 EXTENSION
  
 \echo Use ""CREATE EXTENSION cube"" to load this file. \quit
  
 ALTER EXTENSION cube ADD type cube;
  
 ALTER EXTENSION cube ADD function cube_in(cstring);
  
 ALTER EXTENSION cube ADD function cube(double precision[],double 
 precision[]);
  
 ALTER EXTENSION cube ADD function cube(double precision[]);
  
 ...
  
 The code that provides multidimensional cubes for PostgreSQL has been stable for quite 
 some time. It is unlikely that a new version will be created any time soon. The only 
 reason for this module to be converted into an extension is to allow for easy installation 
 and removal.
  
 You would then execute the command:
  
 CREATE EXTENSION cube FROM unpackaged;
  
 The unrelated items are now grouped together into the extension named cube. This also 
 makes it easier for the packaging maintainer on any platform to include your extension 
 into the repository. We'll show you how to make the packages to install your extension 
 in the 
 Building an extension
  section.
  
 [
  257 
 ]",NA
Extension versions,"The version mechanism for PostgreSQL extensions is simple. Name it whatever you 
 want and give it whatever alphanumeric version number that suits your fancy. Easy, eh? 
 Just name the files in this format:
  
 extension--version.sql
  
 If you want to provide an upgrade path from one version of your extension to 
 another, you would provide the file:
  
 extension--oldversion--newversion.sql
  
 This simple mechanism allows PostgreSQL to update an extension that is already in 
 place. Gone are the days of painful exporting and re-importing data just to change the 
 definition of a data type. So, let's go ahead and update our example extension using the 
 file 
 postal--1.0--1.1.sql
 . This update is as easy as:
  
 ALTER EXTENSION postal UPDATE TO '1.1';
  
  
 A note of caution: PostgreSQL does not have any concept of what your 
  
  
 version number means. In this example, the extension was updated from 
  
 Version 1.0 to 1.1 because we explicitly provided a script for that specific 
  
 conversion. PostgreSQL did not deduce that 1.1 follows 1.0. We could 
  
 have just as easily used the names of fruits or historical battleships for our 
  
 version numbers and the result would have been the same.
  
 PostgreSQL will use multiple update files if necessary to achieve the desired result. 
 Given the following command:
  
 ALTER EXTENSION postal UPDATE TO '1.4';
  
 PostgreSQL will apply the files 
 postal--1.1--1.2.sql
 , 
 postal--1.2--1.3.sql 
 and 
 postal--1.3--1.4.sql
  in the correct order to achieve the desired version.
  
 You may also use this technique to provide upgrade scripts that are in fact 
  
 downgrade scripts, that is, they actually remove functionality. Be careful with this 
 though. If a path to a desired version is to downgrade before an upgrade, PostgreSQL 
 will take the shortest route. This may result in some unintended results, including data 
 loss. My advice would be to not provide downgrade scripts. The risk just isn't worth it.
  
 [
  258 
 ]",NA
The .control file,"Along with the extension installation script file, you must provide a 
 .control
  file. The 
 .control
  file for our example 
 postal.control
  looks like this:
  
 # postal address processing extension
  
 comment = 'utilities for postal processing'
  
 default_version = '1.0'
  
 module_pathname = '$libdir/postal'
  
 relocatable = true
  
 requires = 'plpgsql'
  
 The purpose of the 
 .control
  file is to provide a description of your extension. This 
 metadata may include 
 directory
 , 
 default_version
 , 
 comment
 , 
 encoding
 , 
 module_ 
 pathname
 , 
 requires
 , 
 superuser
 , 
 relocatable
 , and 
 schema
 .
  
 The main PostgreSQL documentation for this file is located at 
 http://www. 
 postgresql.org/docs/current/static/extend-extensions.html
 .
  
 This example shows the 
 requires
  configuration parameter. Our extension depends on 
 the procedural language PL/pgSQL. On most platforms, it is installed by default. 
  
 Unfortunately, it is not installed on all platforms, and nothing should be taken for 
 granted.
  
 Multiple dependencies can be indicated by separating them with commas. This comes in 
 very handy when constructing a set of services based on multiple extensions.
  
 As we mentioned in the previous section, PostgreSQL does not provide any 
 interpretation of the version number of an extension. Versions can be names as well as 
 numbers, so there is no way for PostgreSQL to interpret that 
 postal--lamb.sql 
 comes before 
 postal--sheep.sql
 . This design limitation poses a problem to the 
 extension developer, in that there is no way to specify that your extension depends on a 
 specific version of another extension. I would love to see this configuration parameter 
 enhanced with a syntax like 
 requires = postgis >= 1.3
 , but alas, no such 
 construction exists at the moment.",NA
Building an extension,"We have already covered the basics of creating a script file and a 
 .control
  file. 
 Actually, that is all that is necessary for a PostgreSQL extension. You may simply copy 
 these files into the shared extension directory on your computer and execute the 
 following command:
  
 CREATE EXTENSION postal;
  
 [
  259 
 ]",NA
Installing an extension,"Extensions that have been packaged for you by your friendly distribution manager are 
 very simple to install using the following command:
  
 CREATE EXTENSION extension_name;
  
 Most of the popular Linux distributions include a package called something like 
 postgresql-contrib-9.4
 . This naming convention is left over from the contrib 
 style installation of PostgreSQL objects. Don't worry, for PostgreSQL 9.4, this package 
 will actually provide extensions rather than contrib modules.
  
 To find out where the files were placed on Ubuntu Linux, you can execute the 
 following command:
  
 pg_config --sharedir
  
 This will show you the installation directory of shared components:
  
 /usr/share/postgresql/9.4
  
 The extensions will be located in a directory called ,
 extension
 , immediately below 
 the shared directory. This will then be named 
 /usr/share/postgresql/9.2/ 
 extension
 .
  
 To see what extensions are available for you to install, try this command:
  
 ls $(pg_config –sharedir)/extension/*.control
  
 You can also see the list of installed extensions using the plsql meta-command 
 \dx
 .
  
 This will show you all the extensions that have been made available to you by your 
 Linux distribution's package management system.
  
 For extensions that you have created yourself, you must copy your SQL script file and 
 the 
 .control
  file to the shared extension directory before invoking 
 CREATE 
 EXTENSION
  in PostgreSQL.
  
 cp postal.control postal--1.0.sql $(pg_config --sharedir)/extension
  
 To see the procedure for doing this reliably on any target platform, refer to the 
 Building an Extension
  section.
  
 [
  261 
 ]",NA
Viewing extensions ,"Querying the  
 pg_extension
  system view or using the meta-command 
 \dx
  will 
 show the extensions currently installed in the database.
  
 postgres=# \dx 
  
 List of installed extensions
  
 -[ RECORD 1 ]-----------------------------
  
 Name        | pgcrypto 
  
 Version     | 1.0 
  
 Schema      | public 
  
 Description | cryptographic functions
  
 -[ RECORD 2 ]-----------------------------
  
 Name        | plperl 
  
 Version     | 1.0 
  
 Schema      | pg_catalog 
  
 Description | PL/Perl procedural language
  
 -[ RECORD 3 ]-----------------------------
  
 Name        | plpgsql 
  
 Version     | 1.0 
  
 Schema      | pg_catalog 
  
 Description | PL/pgSQL procedural language
  
 The extensions that are ready to be installed can be viewed from the 
 pg_available_ 
 extensions
  or 
 pg_available_extension_versions
  system views.",NA
Publishing your extension ,"Thank you for contributing to the PostgreSQL community! Your support will not go 
 unnoticed in this gathering of like-minded individuals who are all slightly smarter than 
 each other. Your work will be seen by dozens of developers looking for community 
 solutions to common problems. You have indeed made the open source world a better 
 place.
  
 Since we are talking about publication, you should consider the licensing model for your 
 extension. The publication methods that we are about to describe assume that the 
 extension will be made available to the general public. As such, please consider the 
 PostgreSQL license for your extension. You can find the current one here:
  
 http://www.postgresql.org/about/licence/
  
 [
  262 
 ]",NA
Introduction to PostgreSQL Extension ,NA,NA
Network,"When you want to publish your module, you could start writing packaging scripts for 
 each of the distribution systems for every operating system. This is the way the 
 PostgreSQL extensions have been distributed in the past. That distribution system has 
 not been very friendly to the open source community, or very well received. 
  
 In an effort to make extension publication more palatable, a group of open source writers 
 and backing companies got together and founded the 
 PostgreSQL Extension Network
  
 (
 PGXN
 ).
  
 The PostgreSQL Extension Network 
 http://pgxn.org/
  provides a central 
 repository for your open source extensions. By the kindness of the maintainers, it 
 also provides installation scripts for your extensions that will work on most of the 
 popular PostgreSQL deployment operating systems.",NA
Signing up to publish your extension,"To sign up to publish your extension, perform the following steps:
  
 1. Start by requesting an account on the management page: 
  
 http://manager.pgxn.org
 .
  
 2. Click on 
 Request Account
  and fill in your personal information. 
  
 The PostgreSQL Extension Network folks will get back to you via e-mail. 
 Enrollment requests are currently processed by an actual human, so the e-
 mail response will not be immediate .
  
 3. Click on the provided link in the e-mail to confirm your account and set 
  
 a 
 new password on the PGXN website:
  
  
 [
  263 
 ]",NA
Creating an extension project the easy way ,"First, let's install some utility packages that will create a lot of boilerplate files that 
 we have already described in earlier sections. The commands below are for a 
 Debian/Ubuntu system:
  
 apt-get install ruby 
  
 apt-get install rubygems 
  
 apt-get install ruby1.8-dev 
  
 apt-get install libopenssl-ruby1.8 
  
 gem install rubygems-update 
  
 /var/lib/gems/1.8/bin/update_rubygems 
  
 gem install pgxn_utils
  
 You will now find that you have a utility installed named 
 pgxn-utils
 . This utility 
 makes it super simple to create an extension project.
  
 pgxn-utils skeleton myextension
  
  create  myextension
  
  create  myextension/myextension.control
  
  create  myextension/META.json
  
  create  myextension/Makefile
  
  create  myextension/README.md
  
  create  myextension/doc/myextension.md
  
  create  myextension/sql/myextension.sql
  
  create  myextension/sql/uninstall_myextension.sql 
 create  myextension/test/expected/base.out
  
  create  myextension/test/sql/base.sql
  
 Wow! All of the files that we have mentioned so far just got created in a single step. 
 Several files also got created to support the old contrib style of deployment. The next 
 few sections will show which ones are important to you for extension development.
  
 This package management system has one notable restriction. In contrast to 
  
 PostgreSQL, which allows version numbers to be any alphanumeric text, this package 
 management requires version numbers to follow the rules of semantic versioning. 
  
 [
  265 
 ]",NA
Providing the metadata about the extension ,"There are three files used to provide data about the extension. The PostgreSQL 
 Extension Network uses one of them on the website, 
 META.json
 , for search criteria 
 and description text for the extension. 
 META.json
  will be located in 
 myextension/ 
 META.json
 .
  
 Here is an example:
  
 {
  
  
  ""name"": ""myextension"",
  
  
  ""abstract"": ""A short description"",
  
  
  ""description"": ""A long description"",
  
  
  ""version"": ""0.0.1"",
  
  
  ""maintainer"": ""The maintainer's name"",
  
  
  ""license"": ""postgresql"",
  
  
  ""provides"": {
  
   
  ""myextension"": {
  
   
  ""abstract"": ""A short description"",
  
   
  ""file"": ""sql/myextension.sql"",
  
   
  ""docfile"": ""doc/myextension.md"",
  
   
  ""version"": ""0.0.1""
  
   
  }
  
  
  },
  
  
  ""release_status"": ""unstable"",
  
  ""generated_by"": ""The maintainer's name"",
  
  
  ""meta-spec"": {
  
   
  ""version"": ""1.0.0"",
  
   
  ""url"": ""http://pgxn.org/meta/spec.txt""
  
  } 
  
 }
  
 You should add some sections to it to describe your keywords and any additional 
 resources that you make available to the user. These sections would look like this:
  
 ""tags"": [
  
  ""cures cancer"",
  
  ""myextension"",
  
 [
  266 
 ]",NA
Writing your extension code,"Put your SQL code in the file that was provided for you in 
 myextension/sql/ 
 myextension.sql
 . This file should contain all of the objects that make up your 
 extension.
  
 /* myextension.sql */
  
 -- complain if script is sourced in psql, rather than via CREATE 
 EXTENSION
  
 \echo Use ""CREATE EXTENSION myextension"" to load this file. \quit
  
 CREATE FUNCTION feed_the_hungry() ...
  
 You can provide any additional SQL files in the same directory for maintaining 
 versions as described in the 
 Extension versions
  section. Anything named 
 *.sql 
 that is located in this directory will be included in the distribution.",NA
Creating the package,"To ultimately submit our extension to the PostgreSQL Extension Network, we need to 
 package all the files into a single 
 .zip
  file. Assuming we're following good practices, and 
 we're keeping all of our source code in a handy Git repository, we can create the package 
 through a simple Git command. Try this one on for size:
  
 git archive --format zip --prefix=myextension-0.0.1/ \
  
  --output ~/Desktop/myextension-0.0.1.zip master
  
 This command will create a package for you that is suitable for submission to the 
 PostgreSQL Extension Network. All we need to do now is submit it.
  
 [
  270 
 ]",NA
Submitting the package to PGXN ,"Now that you have a nice ZIP file in hand, you can go to the PostgreSQL Extension 
 Network and make your accomplishment available to the community.
  
 1. Start by going to 
 http://www.pgxn.org
 :
  
  
 [
  271 
 ]",NA
Installing an extension from PGXN,"The PostgreSQL Extension Network provides a platform-independent tool to install 
 PostgreSQL extensions. This tool is written in Python, and uses the Python installation 
 system for distributing itself. This is handy because the Python distribution system 
 exists virtually on every PostgreSQL supportable platform and makes it very simple to 
 get PostgreSQL extensions distributed to the community. 
  
 The extension installer works with a single set of instructions on all targets:
  
 easy_install pgxnclient
  
 Installing pgxncli.py script to /usr/local/bin
  
 Installing pgxn script to /usr/local/bin
  
 Processing dependencies for pgxnclient
  
 Finished processing dependencies for pgxnclient
  
 Now you have the tools installed to manage PostgreSQL extensions provided by the 
 PostgreSQL Extension Network.
  
 Installing extensions is really simple. For example, if we had a requirement to use a 
 new 
 tinyint
  data type, we could add it with this command:
  
 pgxn install tinyint
  
 INFO: best version: tinyint 0.1.1
  
 INFO: saving /tmp/tmpKvr0kM/tinyint-0.1.1.zip
  
 INFO: unpacking: /tmp/tmpKvr0kM/tinyint-0.1.1.zip
  
 INFO: building extension
  
 ...
  
 The extension is now available in the shared extensions directory on your machine. To 
 activate it for any database, you would use the command that we started the chapter 
 with:
  
 CREATE EXTENSION tinyint;
  
 You will then see the confirmation text letting you know that 
 tinyint
  has 
 been added:
  
 CREATE EXTENSION
  
 You now have the extension available for use in your local database. Enjoy!
  
 [
  274 
 ]",NA
Summary,"Wow, this has been a long hard road toward getting an extension configured and 
 installed. We have used programming skills, system administrative skills, database 
 administrative skills, and wiki editing. Along the way, we saw some Ruby, Python, shell 
 scripting, PL/pgSQL, and MediaWiki.
  
 Believe it or not, this is the simplified process. Hard to imagine, eh? Well, continuous work 
 is being done on the PostgreSQL Extension Network to further simplify this catastrophe of 
 a development system. My thanks go out to David E. Wheeler and crew for making this 
 new system available. As the framework now exists to help with the task, there will be 
 dramatic improvements coming in the months and years ahead.
  
 Now that I'm done complaining about it, this extension system is actually 
  
 revolutionary. I say this because no other database platform provides any such 
 framework at all. PostgreSQL leads the pack when it comes to the ability to make 
 changes to the basic functionality of the product. The fact that extensions can be 
 installed and removed from the product is an indicator of how inviting PostgreSQL is to 
 the open source community.
  
 Extend it to do whatever you want, and they'll give you the tools to do it. This makes a 
 PostgreSQL server the perfect framework to use for your data processing needs.
  
 In the next chapter, we will learn more about PostgreSQL as an extensible database 
 and look at how to create user-defined data types and operators.
  
 [
  275 
 ]",NA
PostgreSQL as an ,NA,NA
Extensible RDBMS,"PostgreSQL is an extensible database. I hope you've learned this much by now. It is 
 extensible by virtue of the design that it has. As discussed before, PostgreSQL uses a 
 catalog-driven design. In fact, PostgreSQL is more catalog-driven than most of the 
 traditional relational databases. The key benefit here is that the catalogs can be 
 changed or added to, in order to modify or extend the database functionality. 
  
 PostgreSQL also supports dynamic loading, that is, a user-written code can be 
 provided as a shared library, and PostgreSQL will load it as required.
  
 Extensibility is critical for many businesses, which have needs that are specific to that 
 business or industry. Sometimes, the tools provided by the traditional database systems 
 do not fulfill those needs. People in those businesses know best how to solve their 
 particular problems, but they are not experts in database internals. It is often not possible 
 for them to cook up their own database kernel or modify the core or customize it 
 according to their needs. A truly extensible database will then allow you to do the 
 following:
  
 • 
  
 Solve domain-specific problems in a seamless way, like a native solution
  
 • 
  
 Build complete features without modifying the core database engine
  
 • 
  
 Extend the database without interrupting availability
  
 PostgreSQL not only allows you to do all of the preceding things, but also does these, and 
 more with utmost ease. In terms of extensibility, you can do the following things in a 
 PostgreSQL database:
  
 1. Create your own data types
  
 2. Create your own functions
  
 3. Create your own aggregates",NA
What can't be extended?,"Although PostgreSQL is an extensible platform, there are certain things that you can't 
 do or change without explicitly doing a fork, as follows:
  
 1. You can't change or plug in a new storage engine. If you are coming from the MySQL 
 world, this might annoy you a little. However, PostgreSQL's storage engine is 
 tightly coupled with its executor and the rest of the system, which has its own 
 benefits.
  
 2. You can't plug in your own planner/parser. One can argue for and against the 
 ability to do that, but at the moment, the planner, parser, optimizer, and so on 
 are baked into the system and there is no possibility of replacing them. 
  
 There has been some talk on this topic, and if you are of the curious kind, you 
 can read some of the discussion at 
 http://bit.ly/1yRMkK7
 .
  
 3. We will now briefly discuss some more of the extensibility capabilities of 
 PostgreSQL. We will not dive deep into the topics, but we will point you to the 
 appropriate link where more information can be found. The chapter material 
 will serve as an easy-to-understand introductory tutorial on the subject 
 matter. It is by no means a comprehensive discussion of the topics.",NA
Creating a new operator,"Now, let's take look at how we can add a new operator in PostgreSQL. Adding new 
 operators is not too different from adding new functions. In fact, an operator is 
 syntactically just a different way to use an existing function. For example, the 
 + 
 operator calls a built-in function called 
 numeric_add
  and passes it the two arguments.
  
 [
  278 
 ]",NA
Overloading an operator ,"Operators can be overloaded in the same way as functions. This means, that an operator 
 can have the same name as an existing operator but with a different set of argument 
 types. More than one operator can have the same name, but two operators can't share 
 the same name if they accept the same types and positions of the 
  
 arguments. As long as there is a function that accepts the same kind and number of 
 arguments that an operator defines, it can be overloaded.
  
 [
  279 
 ]",NA
Optimizing operators,"The optional clauses tell the PostgreSQL server about how the operators behave. 
 These options can result in considerable speedups in the execution of queries that 
 use the operator. However, if you provide these options incorrectly, it can result in a 
 slowdown of the queries. Let's take a look at two optimization clauses called 
 commutator
  and 
 negator
 .
  
 [
  280 
 ]",NA
COMMUTATOR ,"This clause defines the commuter of the operator. An operator 
 A
  is a commutator of 
 operator 
 B
  if it fulfils the following condition: 
  
  
 x A y = y B x.
  
 It is important to provide this information for the operators that will be used in 
 indexes and joins. As an example, the commutator for 
 >
  is 
 <
 , and the commutator of 
 =
  
 is 
 =
  itself.
  
 This helps the optimizer to flip the operator in order to use an index. For example, 
 consider the following query: 
  
  
 SELECT * FROM employee WHERE new_salary > salary; 
  
 If the index is defined on the salary column, then PostgreSQL can rewrite the preceding 
 query as shown: 
  
  
 SELECT * from employee WHERE salary < new_salary 
  
 This allows PostgreSQL to use a range scan on the index column salary. For a user -
 defined operator, the optimizer can only do this flip around if the commutator of a user-
 defined operator is defined: 
  
  
 CREATE OPERATOR > (LEFTARG=integer, RIGHTARG=integer, PROCEDURE=comp, 
  
 COMMUTATOR = <)",NA
NEGATOR ,"The negator clause defines the negator of the operator. For example, 
 <>
  is a negator of 
 =
 . 
 Consider the following query: 
  
  
 SELECT * FROM employee WHERE NOT (dept = 10); 
  
 Since 
 <>
  is defined as a negator of 
 =
 , the optimizer can simplify the preceding query as 
 follows: 
  
  
 SELECT * FROM employee WHERE dept <> 10; 
  
 You can even verify that using the 
 EXPLAIN
  command: 
  
  
 postgres=# EXPLAIN SELECT * FROM employee WHERE NOT 
  
  
   
  dept = 'WATER MGMNT';
  
  
   
  
  QUERY PLAN                        
  
  
 ---------------------------------------------------------
  
  
  Foreign Scan on employee  (cost=0.00..1.10 rows=1 width=160)
  
  
  
  Filter: ((dept)::text <> 'WATER MGMNT'::text)
  
 [
  281 
 ]",NA
Creating index access methods,"So far in this book, you came across examples of creating new data types or user-defined 
 types and operators. What we haven't discussed so far is how to index these types. In 
 PostgreSQL, an index is more of a framework that can be extended or customized for 
 using different strategies. In order to create new index access methods, we have to 
 create an operator class. Let's take a look at a simple example.
  
 Let's consider a scenario where you have to store some special data such as an ID or a 
 social security number in the database. The number may contain non-numeric 
 characters, so it is defined as a text type:
  
 CREATE TABLE test_ssn (ssn text);
  
 INSERT INTO test_ssn VALUES ('222-11-020878');
  
 INSERT INTO test_ssn VALUES ('111-11-020978');
  
 Let's assume that the correct order for this data is such that it should be sorted on the last 
 six digits and not the ASCII value of the string.
  
 The fact that these numbers need a unique sort order presents a challenge when it 
 comes to indexing the data. This is where PostgreSQL operator classes are useful. An 
 operator allows a user to create a custom indexing strategy.
  
 Creating an indexing strategy is about creating your own operators and using them 
 alongside a normal B-tree.
  
 Let's start by writing a function that changes the order of digits in the value and also 
 gets rid of the non-numeric characters in the string to be able to compare them better:
  
 CREATE OR REPLACE FUNCTION fix_ssn(text)
  
  RETURNS text AS $$
  
  BEGIN
  
  RETURN substring($1,8) || replace(substring($1,1,7),'-','');
  
 END; 
  
 $$LANGUAGE 'plpgsql' IMMUTABLE;
  
 [
  282 
 ]",NA
Creating user-defined aggregates,"User-defined aggregate functions are probably a unique PostgreSQL feature, yet they are 
 quite obscure and perhaps not many people know how to create them. However, once you 
 are able to create this function, you will wonder how you have lived for so long without 
 using this feature.
  
 This functionality can be incredibly useful, because it allows you to perform custom 
 aggregates inside the database, instead of querying all the data from the client and doing 
 a custom aggregate in your application code, that is, the number of hits on your website 
 per minute from a specific country.
  
 PostgreSQL has a very simple process for defining aggregates. Aggregates can be defined 
 using any functions and in any languages that are installed in the database. 
  
 Here are the basic steps to building an aggregate function in PostgreSQL:
  
 1. Define a start function that will take in the values of a result set; this function 
  
 can be defined in any PL language you want.
  
 2. Define an end function that will do something with the final output of the 
  
 start 
 function. This can be in any PL language you want.
  
 3. Define the aggregate using the 
 CREATE AGGREGATE
  command, providing the 
  
 start and end functions you just created.
  
 Let's steal an example from the PostgreSQL wiki at 
 http://wiki.postgresql.org/ 
 wiki/Aggregate_Median
 .
  
 In this example, we will calculate the statistical median of a set of data. For this 
 purpose, we will define start and end aggregate functions. 
  
 [
  284 
 ]",NA
Using foreign data wrappers,"PostgreSQL 
 foreign data wrappers
  (
 FDW
 ) are an implementation of 
 SQL 
 Management of External Data
  (
 SQL/MED
 ), which is a standard added to SQL in 
 2013.
  
 FDWs are drivers that allow PostgreSQL database users to read and write data to other 
 external data sources, such as other relational databases, NoSQL data sources, files, 
 JSON, LDAP, and even Twitter.
  
 You can query the foreign data sources using SQL and create joins across different 
 systems or even across different data sources.
  
 There are several different types of data wrappers developed by different developers and 
 not all of them are production quality. You can see a select list of wrappers on the 
 PostgreSQL wiki at 
 http://wiki.postgresql.org/wiki/Foreign_data_wrappers
 .
  
 Another list of FDWs can be found on PGXN at 
 http://pgxn.org/tag/fdw/
 .
  
 [
  286 
 ]",NA
Summary,"PostgreSQL can be extended in many more ways than what we have discussed so far in 
 the book. This includes the ability to add new operators, new index access methods, and 
 create your own aggregates. You can access foreign data sources, such as other databases, 
 files, and web services using PostgreSQL foreign data wrappers. These wrappers are 
 provided as extensions and should be used with caution, as most of them are not officially 
 supported.
  
 Even though PostgreSQL is very extensible, you can't plug in a new storage engine or 
 change the parser/planner and executor interfaces. These components are very tightly 
 coupled with each other and are, therefore, highly optimized and mature.
  
 [
  289 
 ]",NA
Index,NA,NA
Symbol ,".control file, extension  
 259",NA
A,"audit trail 
  
  
 creating  123, 124 
  
 audit trigger  106",NA
B,"acquisition 
  
  
 cost  36, 37 
  
 add 2 arguments function  170 
  
 add_func.c  170-172 
  
 add_func.sql.in  174, 175 
  
 add(int, int) 
  
  
 any number of arguments, 
  
   
 working with  178-185 
  
  
 functionality, adding  176 
  
  
 NULL arguments, handling  176-178 
 AFTER trigger  110 
  
 ALTER EXTENSION ADD command 
  
 URL  257 
  
 ANY parameter  49 
  
 application design 
  
  
 about  43 
  
  
 databases  43 
  
  
 databases, drawbacks  43 
  
  
 data locality  44-46 
  
  
 encapsulation  44 
  
  
 PostgreSQL  44 
  
 arguments 
  
  
 about  178-184 
  
  
 records, handling as  192, 194 
  
 argument tuple 
  
  
 fields, extracting from  195 
  
 arrays 
  
  
 looping  64 
  
 assert, PL/Python 
  
  
 using  163
  
 backends 
  
  
 synchronizing between  202 
  
 BEFORE trigger  110 
  
 BIRT  42",NA
C,"C 
  
  
 additional resources  203 
  
 C++ 
  
  
 functions, writing in  202 
  
 caching  31 
  
 cancel trigger  109 
  
 C code, writing 
  
  
 about  185 
  
  
 files, including  186 
  
  
 memory, allocating  185 
  
  
 palloc(), using  185 
  
  
 pfree(), using  185 
  
  
 structures, zero filling  186 
  
  
 symbol names, public  187 
  
 C function 
  
  
 about  170 
  
  
 add_func.c  170-172 
  
  
 add_func.sql.in function  174, 175 
  
  
 CREATE FUNCTION add(int, int)  174 
  
 error, reporting  187 
  
  
 error, states  188 
  
  
 Makefile function  173",NA
D,"data 
  
 cleaning  26, 27 
  
 distributing  221
  
  
 moving, from single to partitioned 
  
   
 database  225, 226 
  
  
 partitioning, across multiple 
  
   
 servers  213, 214 
  
  
 splitting  214-216 
  
 database 
  
  
 changes, fast capturing  201 
  
 database abstraction layer  43 
  
 database-backed systems, growing 
  
  
 bigger server, moving to  213 
  
  
 Master-slave replication  213 
  
  
 Multi-master replication  213 
  
  
 ways  212 
  
 database, scaling 
  
  
 data, moving from single to partitioned 
   
 database  225, 226 
  
  
 single-server chat, creating  205-212 
  
  
 tables, splitting over multiple 
  
   
 databases  212 
  
 database sharding  205 
  
 databases, PL/Tcl 
  
  
 accessing  247, 248 
  
 data changes 
  
  
 visibility  191 
  
 data comparisons 
  
  
 operators used  15 
  
 data definition language (DDL)  20 
  
 Data Manipulation Language (DML) 
  
   
 operation  106 
  
 data wrappers 
  
  
 URL  289 
  
 Datum  197 
  
 DB API 2  149 
  
 ddl_command_end event  122 
  
 ddl_command_start event  122 
  
 debugging 
  
  
 manual debugging, with RAISE NOTICE 
   
 128-130 
  
  
 visual debugging  134 
  
 debugging, manual 
  
  
 exceptions, throwing  130 
  
  
 file, logging to  132, 133 
  
  
 RAISE NOTICE, advantages  133 
  
  
 RAISE NOTICE, disadvantages  134 
  
  
 URL  131
  
 [
  292 
 ]",NA
E,"EnterpriseDB 
  
  
 URL  135 
  
 error 
  
  
 INFO  188 
  
  
 LOG  188 
  
  
 NOTICE  188 
  
  
 reporting, from C functions  187 
  
 reporting, URL  188 
  
  
 states  188 
  
 error handling  47 
  
 error reporting  47 
  
 ERROR trigger  110 
  
 event triggers 
  
  
 audit trail, creating  123, 124 
  
  
 creating  122, 123 
  
  
 ddl_command_end event  122 
  
  
 ddl_command_start event  122 
  
  
 roadmap  125, 126 
  
  
 sql_drop event  122 
  
  
 URL  123, 126 
  
  
 use cases  121, 122 
  
 event triggers, PL/pgSQL functions 
  
 TG_EVENT  123 
  
  
 TG_TAG  122 
  
 exceptions, PL/Python 
  
  
 handling  158-160 
  
 exceptions, RAISE NOTICE 
  
  
 throwing  130-132 
  
 expanded display 
  
  
 switching to  13, 14 
  
 extensibility  277, 278 
  
 extension 
  
  
 .control file  259 
  
  
 about  175 
  
  
 building  259-261 
  
  
 creating  256
  
  
 installing  261 
  
  
 installing, from PGXN  274 
  
  
 publishing  262 
  
  
 unpackaged  257 
  
  
 URL  256, 259 
  
  
 versions  258 
  
  
 viewing  262 
  
 extension, publishing 
  
  
 extension code, writing  270 
  
  
 extension project, creating  265 
  
  
 metadata, providing  266-269 
  
  
 package, creating  270 
  
  
 PostgreSQL Extension Network  263 
  
 signing up  263-265",NA
F,"file_fdw 
  
  
 URL  289 
  
 fillfactor 
  
  
 URL  211 
  
 foreign data wrappers (FDW) 
  
  
 using  286-289 
  
 function 
  
  
 used, for configuring PL/Proxy 
   
 cluster  222, 223 
  
 function overloading  48",NA
G,"General Inverted Index (GIN)  31 
 Git repository 
  
  
 URL  135",NA
H,horizontal distribution  205,NA
I,"immutable fields trigger  112, 113 
 index access methods 
  
  
 creating  282, 283 
  
  
 URL  284 
  
 integer set 
  
  
 returning  72, 73
  
 [
  293 
 ]",NA
K,"keep it simple stupid (KISS)  28 
 k nearest neighbor (KNN)  31",NA
L,"licensing  38 
  
 light-weight locks (LWLocks)  202 
 log trigger  154-157 
  
 looping syntax 
  
  
 URL  61 
  
 loops 
  
  
 statement, terminating  62 
  
  
 with counters  60, 61",NA
M,"Makefile function  173 
  
 Master-slave replication  213 
  
 metadata 
  
  
 providing, for extension  266-269 
 Multi-master replication  213 
  
 Multiversion Concurrency Control 
   
 (MVCC)  116",NA
N,"NEGATOR clause  281 
  
 NEW record 
  
  
 modifying  111 
  
 NULL arguments 
  
  
 handling  176-178",NA
O,"operator 
  
  
 new operator, creating  278, 279 
  
  
 optimizing  280 
  
  
 overloading  279, 280 
  
  
 PostgreSQL documentation, URL  280 
  
 used, for data comparisons  15 
  
 operator, optimizing 
  
  
 COMMUTATOR  281 
  
  
 NEGATOR  281 
  
 optional clauses 
  
  
 URL  280
  
 os.walk() 
  
  
 URL  168 
  
 OUT parameters 
  
  
 about  81 
  
  
 and records  80 
  
  
 no predefined structure, returning 
   
 with  84, 85 
  
  
 records, returning  81, 82 
  
  
 RETURNS TABLE, using  83 
  
  
 SETOF ANY, returning  85-87 
  
  
 variadic argument lists  87, 88",NA
P,"package 
  
  
 creating  270 
  
  
 submitting, to PGXN  271-273 
  
 palloc() 
  
  
 using  186 
  
 parameters  49 
  
 Pentaho data integration (kettle)  41 
 Pentaho Report Server  41 
  
 PERFORM command 
  
  
 versus SELECT command  64 
  
 Perl  229 
  
 pfree() 
  
  
 using  186 
  
 pgAdmin3  41 
  
  
 installing  135 
  
 pgfoundry 
  
  
 URL  203 
  
 PGXN 
  
  
 about  263 
  
  
 extension, installing from  274 
  
  
 FDWs URL  286 
  
  
 package, submitting  271-273 
  
  
 URL  271 
  
 php5-postgresql  41 
  
 pl/lolcode 
  
  
 URL  204 
  
 PL/Perl 
  
  
 function  230, 231 
  
  
 function, URL  230-237 
  
  
 installing  230 
  
  
 non-scalar types, passing  231-234 
  
 non-scalar types, returning  231-234 
  
 triggers, writing  235-237
  
 [
  294 
 ]",NA
Q,"QCubed  41 
  
 queries, PL/Python 
  
  
 constructing  157, 158 
  
  
 prepared queries, caching  150, 151 
  
 prepared queries, using  150 
  
  
 running, in database  149 
  
  
 simple queries, running  149
  
 query results 
  
  
 looping  62, 63",NA
R,"RAISE NOTICE 
  
  
 advantages  133 
  
  
 disadvantages  134 
  
  
 manual debugging with  129, 130 
  
  
 URL  133 
  
 Read Committed  46, 47 
  
 records 
  
  
 and OUT parameters  80 
  
  
 complex type single tuple, returning   194 
  
 Datum  197 
  
  
 fields, extracting from argument type  195 
  
 handling, as arguments  192-194 
  
  
 return tuple, constructing  196 
  
  
 returning  65-68, 81, 82 
  
  
 returning, from Python function  145, 146 
  
 set, returning  197-199 
  
 replication 
  
  
 Master-slave replication  213 
  
  
 Multi-master replication  213 
  
 return (a + b)  170 
  
 RETURN SETOF variants  88, 89 
  
 RETURNS TABLE 
  
  
 using  83 
  
 return tuple 
  
  
 constructing  196 
  
 rollback  202 
  
 rows 
  
  
 returning, from function  74, 75 
  
 rowsets  71 
  
 RUN ON statement  217, 218",NA
S,"schema changes 
  
  
 preventing  124, 125 
  
 SELECT command 
  
  
 versus PERFORM command  64 
 SELECT statement  218 
  
 server 
  
  
 data, partitioning across multiple 
   
 servers  213
  
 [
  296 
 ]",NA
T,"table functions  147-149 
  
 tables 
  
  
 splitting, over multiple databases  212 
 Talend  42 
  
 TARGET statement  218, 219 
  
 Tcl 
  
  
 URL  241, 246 
  
 Tools Command Language (Tcl)  241 
 transactions 
  
  
 about  46, 47 
  
  
 isolation methods, URL  46 
  
 trigger 
  
  
 about  101 
  
  
 AFTER trigger  110 
  
  
 auditing  106-109 
  
  
 BEFORE trigger  110 
  
  
 cancel trigger  109 
  
  
 conditional triggers  114, 115 
  
  
 creating  102 
  
  
 defining  101 
  
  
 DELETE trigger, disallowing  109, 110 
  
 ERROR trigger  110 
  
  
 fire, controlling  114 
  
  
 function, creating  102
  
 [
  297 
 ]",NA
U,"untrusted languages 
  
  
 about  139, 140 
  
  
 features  141 
  
 untrusted Perl  237-239 
  
 untrusted Tcl  250-252 
  
 user-defined aggregates 
  
  
 creating  284-286 
  
  
 example, URL  284 
  
  
 URL  286 
  
 User-defined functions (UDF)  8, 47, 48",NA
V,"variable parameters 
  
  
 URL  48 
  
 variables 
  
  
 URL  118 
  
 variadic argument lists  87, 
 88 version 0 call conventions 
  
  
 about  172 
  
  
 URL  172 
  
 views 
  
  
 functions based  76-80 
  
 visibility 
  
  
 rules, URL  191 
  
  
 URL  117 
  
 VOLATILE function  117",NA
W,"wrappers 
  
  
 URL  286",NA
X,"XML data type 
  
 about  93-95 
  
 URL  94",NA
Y,"Yii  42 
  
 you ain't gonna need it (YAGNI)  29
  
 [
  298 
 ]",NA
Thank you for buying ,NA,NA
PostgreSQL Server Programming,NA,NA
Second Edition,NA,NA
About Packt Publishing,"Packt, pronounced 'packed', published its first book, 
 Mastering phpMyAdmin for Effective MySQL 
 Management
 , in April 2004, and subsequently continued to specialize in publishing highly 
 focused books on specific technologies and solutions.
  
 Our books and publications share the experiences of your fellow IT professionals in adapting and 
 customizing today's systems, applications, and frameworks. Our solution-based books give you the 
 knowledge and power to customize the software and technologies you're using to get the job done. 
 Packt books are more specific and less general than the IT books you have seen in the past. Our 
 unique business model allows us to bring you more focused information, giving you more of what 
 you need to know, and less of what you don't.
  
 Packt is a modern yet unique publishing company that focuses on producing quality, 
 cutting-edge books for communities of developers, administrators, and newbies alike. For 
 more information, please visit our website at 
 www.packtpub.com
 .",NA
About Packt Open Source,"In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to 
 continue its focus on specialization. This book is part of the Packt Open Source brand, home to 
 books published on software built around open source licenses, and offering information to 
 anybody from advanced developers to budding web designers. The Open Source brand also runs 
 Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each open source project 
 about whose software a book is sold.",NA
Writing for Packt,"We welcome all inquiries from people who are interested in authoring. Book proposals should be 
 sent to 
 author@packtpub.com
 . If your book idea is still at an early stage and you would like to 
 discuss it first before writing a formal book proposal, then please contact us; one of our 
 commissioning editors will get in touch with you. 
  
 We're not just looking for published authors; if you have strong technical skills but no writing 
 experience, our experienced editors can help you develop a writing career, or simply get some 
 additional reward for your expertise.",NA
PostgreSQL 9 High Availability ,NA,NA
Cookbook,"ISBN: 978-1-84951-696-9              Paperback: 398 pages
  
 Over 100 recipes to design and implement a 
  
 highly available server with the advanced features of 
 PostgreSQL
  
 1. 
  
 2. 
  
 3. 
  
 Create a PostgreSQL cluster that stays online 
 even when disaster strikes.
  
 Avoid costly downtime and data loss that can 
 ruin your business.
  
 Perform data replication and monitor your 
 data with hands-on industry-driven recipes 
 and detailed step-by-step explanations.",NA
PostgreSQL Cookbook ,"ISBN: 978-1-78355-533-8             Paperback: 286 pages
  
 Over 90 hands-on recipes to effectively manage, 
 administer, and design solutions using PostgreSQL
  
 1. 
  
 2. 
  
 3. 
  
 Implement a highly available PostgreSQL 
 database server and perform administrative 
 and development functions with PostgreSQL.
  
 Perform database operations in PostgreSQL 
 using Perl and Python.
  
 Step-by-step recipes focusing on providing 
 real-world Postgresql solutions.
  
 .
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
PostgreSQL Server Programming ,"ISBN: 978-1-84951-698-3            Paperback: 264 pages
  
 Extend PostgreSQL and integrate the database layer 
 into your development framework
  
 1. 
  
 2. 
  
 3. 
  
 Understand the extension framework of 
 PostgreSQL, and leverage it in ways that 
 you haven't even invented yet.
  
 Write functions, create your own data types, all 
 in your favourite programming language.
  
 Step-by-step tutorial with plenty of tips and 
 tricks to kick-start server programming.",NA
PostgreSQL Administration ,NA,NA
Essentials,"ISBN: 978-1-78398-898-3             Paperback: 142 pages
  
 Discover efficient ways to administer, monitor, 
 replicate, and handle your PostgreSQL databases
  
 1. 
  
 2. 
  
 3. 
  
 Learn how to detect bottlenecks and make 
 sure your database systems offer superior 
 performance to your end users.
  
 Replicate your databases to achieve full 
 redundancy and create backups quickly 
 and easily.
  
 Optimize PostgreSQL configuration 
  
 parameters and turn your database server 
 into a high-performance machine capable of 
 fulfilling your needs.
  
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
