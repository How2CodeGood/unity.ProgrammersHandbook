Larger Text,Smaller Text,Symbol
Contents at a Glance,"About the Author 
 xvii 
 About the Technical Reviewer 
 xix 
 Acknowledgments 
 xxi 
 Introduction 
 xxiii
  
 ■
  
 Chapter 1: Setting Up for Nodejs Development
  
 1
  
 ■
  
 Chapter 2: Understanding Nodejs
  
 17
  
 ■
  
 Chapter 3: Core Nodejs
  
 41
  
 ■
  
 Chapter 4: Nodejs Packages
  
 65
  
 ■
  
 Chapter 5: Events and Streams
  
 91
  
 ■
  
 Chapter 6: Getting Started with HTTP
  
 115
  
 ■
  
 Chapter 7: Introducing Express
  
 141
  
 ■
  
 Chapter 8: Persisting Data
  
 165
  
 ■
  
 Chapter 9: Front-End Basics
  
 181
  
 ■
  
 Chapter 10: Simplifying Callbacks
  
 197
  
 ■
  
 Chapter 11: Debugging
  
 225
  
 ■
  
 Chapter 12: Testing
  
 241
  
 ■
  
 Chapter 13: Deployment and Scalability
  
 255
  
 Index 
 275
  
 v",NA
Introduction,"Undoubtedly, personal computing has revolutionized the way we live and work today. The Web has further 
 revolutionized the way we use applications. When it was first introduced, the Internet was designed to 
 present information in the form of documents. Later, JavaScript was added, which has been the key ingredient 
 for the innovation we see on the Web today. Web applications are platform-independent, seamlessly updating, 
 safe by default, and available anytime and everywhere. No wonder it is difficult to get started in a developer 
 role today without some knowledge of how the Web works. 
  
 Because of the importance of the Web and the pivotal role that JavaScript plays in web development, you can 
 find a solution for most technical problems in some open source JavaScript project. Node.js allows you to use all 
 these innovative JavaScript projects on the server the same as on the client browser. Using JavaScript on the 
 server also reduces the context switching that needs to happen in your brain as you change programming 
 language and associated code conventions. This is the emotional side of why you should use Node.js.
  
 This book is designed to be a gentle introduction to Node.js as well as JavaScript. No prior software development 
 experience is assumed beyond a basic programming course. Since we clearly present the technical reasons behind the 
 creation of Node.js, this book is also great if you are already comfortable programming in another environment, such 
 as C# or Java, and are curious what all the fuss around Node.js is about. This book covers all the main areas of Node.js 
 software development from setup to deployment so when you finish this book, you should be able to start using Node. 
 js immediately and be ready to share your projects with the world.
  
 xxiii",NA
Chapter 1,NA,NA
Setting Up for Node.js ,NA,NA
Development,"In this chapter, we discuss how to set up a Node.js development environment as we guide you through the 
 installation process of Node.js on various platforms. Then we give you a tour of the Node.js REPL (read-evaluate-
 print-loop) and show you how you can run Node.js applications. Finally, we provide examples of Integrated 
 Development Environments (IDEs) that can help you deliver applications faster and make your journey more 
 enjoyable.",NA
Installing Node.js,"You no longer need to build Node.js from source in order to develop Node.js applications. Node.js now provides 
 installers for Windows as well as Mac OS X, and it can be installed in the same way as any other application on these 
 platforms (Figure 
 1-1
 ). You can download Node.js installers from 
 http://nodejs.org/download/
 .",NA
Installing on Windows ,"The website for Node.js lists “Windows Binary (.exe)” and “Windows Installer (.msi).” You do not want to use the 
 windows binary (.exe) for development, as it does not contain important things such as Node Package Manager 
 (NPM), which we cover in Chapter 4. Node.js provides separate installers (.msi) for 32-bit and 64-bit Windows. We 
 recommend that you install based on your platform. You launch the installer as you would any other installer on 
 Windows (Figure 
 1-2
 ).
  
  
 Figure 1-2.
  Node.js Setup Wizard on Windows
  
  
 We recommend that you install to the default directory and with the default options when starting out for the 
 first time. It is especially important that you let the installer 
 Add to PATH
  (Figure 
 1-3
 )
 .
  
 2",NA
Installing on Mac OS X ,"Download the Mac OS X installer provided by the Node.js team from 
 http://nodejs.org/download/
 . The installer 
  
  is a .pkg file you can launch from Finder (Figure 
 1-6
 ).
  
  
 Figure 1-6.
  Node.js Installer for Mac OS X
  
 When starting out, stick with the defaults and install for all users (Figure 
 1-7
 ).
  
 4",NA
Using the REPL,"Node.js provides you with a REPL (read-evaluate-print-loop) so that you can test arbitrary JavaScript and experiment 
 and explore solutions to the problem you are trying to solve. When you run node without any command line 
 arguments, it puts you in the REPL. To view the options available to you, as shown in Figure 
 1-10
 , type .help and press 
 Enter.
  
  
 Figure 1-10.
  Node.js REPL help
  
 You can execute arbitrary JavaScript in the REPL and see its result immediately, as shown in Figure 
 1-11
 .
  
  
 Figure 1-11.
  Executing JavaScript in the Node.js REPL
  
 At each step, the REPL prints the outcome of the last statement executed. The REPL does not execute your input 
 code until all brackets have been balanced. To execute multiple lines, simply wrap them in parentheses. 
  
 The REPL uses (…) to denote that it is waiting for the complete code before executing. Simply close the parentheses 
 and press Enter for the REPL to evaluate the entered JavaScript (see Figure 
 1-12
 ). To exit from inside a block (...) 
 without executing what you have already entered, simply type .break or press Ctrl+C.
  
 6",NA
Executing Node.js Scripts,"We have seen how to execute JavaScript by typing it into the REPL. However, you will most commonly be writing 
 Node.js programs (script files) and execute them with Node.js. You can execute a JavaScript source file in Node.js by 
 simply passing the file to node on the command line (Figure 
 1-13
 ). Create a new file called helloworld.js containing a 
 simple console.log as shown in Listing 1-1.
  
  
 Figure 1-13.
  Executing a script file in Node.js
  
 Listing 1-1. 
 helloworld.js
  
 console.log(""hello world!"");
  
  
 Then run the file by running node helloworld.js on the command line from the same directory that you saved the 
 file (C:\, in our case).
  
 Notice that we use console.log in the same way as we would use if we were doing front-end web development. 
 One of the philosophies of Node.js is that it should be intuitive to front-end developers. The Node.js team has tried to 
 keep the API consistent with the browser whenever it made sense.
  
  
 Node.js simply executes the input JavaScript from top to bottom as a browser would. It is, however, conventional 
 to name the main file of your application app.js so people know which file to execute in order to run your application.",NA
Setting Up an Integrated Development Environment,"Node.js is great because it is really simple to get started with just a text editor and Terminal. (That does not mean 
 that there aren
 ’
 t more full-featured development environments out there.) Node.js has seen fantastic support from 
 JetBrains (creators of IntelliJ Idea, RubyMine, and PyCharm) in the form of WebStorm as well as from Microsoft in 
 their Visual Studio. WebStorm is available on Windows, Mac OS X, and Linux, whereas Visual Studio is available for 
 Windows only.
  
 7",NA
WebStorm Node.js Support,"WebStorm claims to be “the smartest JavaScript IDE.” It is based on IntelliJ IDEA platform and might be easy 
 for you to migrate to if you are coming from a Java, Ruby, or Python background. You can get it from 
 http://www.jetbrains.com/webstorm/
 .
  
  
 WebStorm works using the concept of “projects.” When you start WebStorm, you are presented with an 
 option, 
 Create a New Project
 . For this example, we will create a simple empty project (Figure 
 1-14
 ).
  
  
 Figure 1-14.
  Create a new project in WebStorm
  
  
 Now right-click the project name in the project window (shown in Figure 
 1-15
 ) once it is open. Add a new 
 JavaScript file and call this file “main” (also shown in Figure 
 1-15
 ).
  
 8",NA
Visual Studio Node.js Support,"If you are coming from a .NET background, you might be glad to hear that Visual Studio has first-class Node.js support. 
  
 This support comes in the form of “Node.js Tools for Visual Studio,” available for both Visual Studio 2012 and Visual 
 Studio 2013 from Microsoft. You can download these tools from 
 https://nodejstools.codeplex.com
 . Installing these 
 tools couldn't be easier. Simply launch the downloaded .msi installer and click through to finish.
  
 Now when you start Visual Studio and create a new project, you will see a new language option, 
 JavaScript
 . 
  
 Select it and create a Blank Node.js Console App, specifying its name and location as shown in Figure 
 1-20
 .
  
 11",NA
Summary,"Node.js has seen fantastic community support since its beginning. Thanks to the installers, you no longer need to 
 compile Node.js from source in order to create Node.js applications on your favorite platform. After setting up 
 Node.js, we showed examples of IDEs that can make working with Node.js easier in order to get you up and running 
 quickly.
  
  
 In the next chapter, we will discuss important JavaScript concepts that you need to understand in order to be 
 successful with Node.js.
  
 15",NA
Chapter 2,NA,NA
Understanding Node.js,"To understand how Node.js works, first you need to understand a few key features of JavaScript that make it well 
 suited for server-side development. JavaScript is a simple language, but it is also extremely flexible. This flexibility is 
 the reason why it has stood the test of time. First-class functions and closures make it an ideal language for web 
 applications.
  
 JavaScript has a bad reputation for being unreliable. However, this notion couldn’t be further from the truth. 
  
 Actually, JavaScript’s bad reputation comes from the DOM’s unreliability. The DOM (docment object model) is the 
 API (application programming interface) that browser vendors provide to interact with the browser using 
 JavaScript. The DOM has idiosyncrasies between browser vendors. However, JavaScript the language is well-defined 
 and can be used reliably across browsers and Node.js. In this chapter, we discuss a few fundamentals of JavaScript 
 followed by how Node.js used JavaScript to provide a highly performant platform for web applications. Other people 
 complain about how JavaScript handles programming errors (it tries to make invalid code work). However, in such 
 cases, the developers are really to blame, as they need to be careful when working with a highly dynamic language.",NA
Variables,"Variables are defined in JavaScript using the var keyword. For example, the following code segment creates a variable 
 foo and logs it to the console. (See Listing 2-1.) As you saw in the previous chapter, you would run this code from your 
 console (terminal on Mac OS X and cmd on Windows) using node variable.js.
  
 Listing 2-1. 
 variable.js
  
 var foo = 123; 
  
 console.log(foo); // 123
  
 The JavaScript runtime (the browser or Node.js) has the opportunity to define a few global variables that we 
 can use in our code. One of them is the console object, which we have been using up to this point. The console object 
 contains a member function (log), which takes any number of arguments and prints them to the console. We will 
 discuss more global objects as they are used. As you will see, JavaScript contains most things you expect a good 
 programming language to have.",NA
Numbers ,"All numbers in JavaScript have the same floating point number type. Arithmetic operations (+,-,*,/,%) work on 
 numbers as you would expect, as shown in Listing 2-2.
  
 17",NA
Boolean,"Two literals are defined for boolean values: true and false. You can assign these to variables and apply boolean 
 operations to them as you would expect. (See Listing 2-3.)
  
 Listing 2-3. 
 boolean.js
  
 var foo = true; 
  
 console.log(foo); // true
  
 // Boolean operations (&&, ||, !) work as expected: 
 console.log(true && true);   // true 
  
 console.log(true && false);  // false 
  
 console.log(true || false);  // true 
  
 console.log(false || false); // false 
  
 console.log(!true);          // false 
  
 console.log(!false);         // true",NA
Arrays ,"You can create arrays quite easily in JavaScript using []. Arrays have many useful functions, a few of which are shown 
 in Listing 2-4.
  
 Listing 2-4. 
 arrays.js
  
 var foo = [];
  
 foo.push(1);         // add at the end 
  
 console.log(foo);    // prints [1]
  
 foo.unshift(2);      // add to the top 
  
 console.log(foo);    // prints [2,1]
  
 // Arrays are zero index based: 
  
 console.log(foo[0]); // prints 2",NA
Object Literals,"By explaining these few fundamental types, we have introduced you to object literals. The most common way 
 of creating an object in JavaScript is using the object notation, {}. Objects can be extended arbitrarily at 
 runtime. An example is shown in Listing 2-5.
  
 18",NA
Functions,"Functions are really powerful in JavaScript. Most of the power of JavaScript comes from the way it handles the 
 function type. We will examine functions in JavaScript in progressively more involved examples that follow.",NA
Functions 101 ,"A normal function structure in JavaScript is defined in Listing 2-10.
  
 Listing 2-10. 
 functionBody.js
  
 function functionName() {
  
  
  // function body
  
  
  // optional return; 
  
 }
  
  
 All functions return a value in JavaScript. In the absence of an explicit return statement, a function 
 returns undefined. When you execute the code in Listing 2-11, you get undefined on the console.
  
 Listing 2-11. 
 functionReturn.js
  
 function foo() { return 123; } 
  
 console.log(foo()); // 123
  
 function bar() { } 
  
 console.log(bar()); // undefined
  
 We will discuss undefined functions more in this chapter when we look at default values.",NA
Immediately Executing Function,"You can execute a function immediately after you define it. Simply wrap the function in parentheses () and invoke it, 
 as shown in Listing 2-12.
  
 Listing 2-12. 
 ief1.js
  
 (function foo() {
  
  
  console.log('foo was executed!'); 
  
 })();
  
 20",NA
Anonymous Function,"A function without a name is called an anonymous function. In JavaScript, you can assign a function to a variable. 
 If you are going to use a function as a variable, you don’t need to name the function. Listing 2-15 demonstrates 
 two ways of defining a function inline. Both of these methods are equivalent.
  
 Listing 2-15. 
 anon.js
  
 var foo1 = function namedFunction() { // no use of name, just wasted characters
  
  
 console.log('foo1'); 
  
 } 
  
 foo1(); // foo1
  
 var foo2 = function () {              // no function name given i.e. anonymous function
  
  
 console.log('foo2'); 
  
 } 
  
 foo2(); // foo2
  
  
 A programming language is said to have 
 first-class functions
  if a function can be treated the same way as any 
 other variable in the language. JavaScript has first-class functions.
  
 21",NA
Higher-Order Functions,"Since JavaScript allows us to assign functions to variables, we can pass functions to other functions. Functions that 
 take functions as arguments are called 
 higher-order functions
 . A very common example of a higher-order function is 
 setTimeout. This is shown in Listing 2-16.
  
 Listing 2-16. 
 higherOrder1.js
  
 setTimeout(function () {
  
  
  console.log('2000 milliseconds have passed since this demo started'); }, 2000);
  
 If you run this application in Node.js, you will see the console.log message after two seconds and then the 
 application will exit. Note that we provided an anonymous function as the first argument to setTimeout. This makes 
 setTimeout a higher-order function.
  
  
 It is worth mentioning that there is nothing stopping us from creating a function and passing that in. An example 
 is shown in Listing 2-17.
  
 Listing 2-17. 
 higherOrder2.js
  
 function foo() {
  
  
  console.log('2000 milliseconds have passed since this demo started'); } 
  
 setTimeout(foo, 2000);
  
 Now that we have a firm understanding of object literals and functions, we can examine the concept of closures.",NA
Closures ,"Whenever we have a function defined inside another function, the inner function has access to the variables declared 
 in the outer function. Closures are best explained with examples.
  
 In Listing 2-18, you can see that the inner function has access to a variable (variableInOuterFunction) from the 
 outer scope. The variables in the outer function have been closed by (or bound in) the inner function. Hence the term 
 closure
 . The concept in itself is simple enough and fairly intuitive.
  
 Listing 2-18. 
 closure1.js
  
 function outerFunction(arg) {
  
  
  var variableInOuterFunction = arg;
  
  function bar() {
  
  
  console.log(variableInOuterFunction); // Access a variable from the outer scope }
  
  
  // Call the local function to demonstrate that it has access to arg
  
  bar(); 
  
 }
  
 outerFunction('hello closure!');              // logs hello closure!
  
 22",NA
Understanding Node.js Performance,"Node.js is focused on creating highly performant applications. In the following section, we introduce the I/O scaling 
 problem. Then we show how it has been solved traditionally, followed by how Node.js solves it.",NA
The I/O Scaling Problem,"Node.js is focused on being the best way to write highly performant web applications. To understand how it achieves 
 this, we need to know about the I/O scaling problem. Let us look at a rough estimate of the speed at which we can 
 access data from various sources in terms of CPU cycles (Figure 
 2-1
 ).
  
 L1 
  
 3 cycles
  
 L2 
  
 14 
 cycles
  
 RAM 
  
 250 cycles
  
 Disk 
  
 41,000,000 cycles
  
 Network 
  
 240,000,000 
 cycles
  
 Figure 2-1.
  Comparing common I/O sources",NA
Traditional Web Servers Using a Process Per Request,"Traditional servers used to spin up a new process to handle every single web request. Spinning a new process for 
 each request is an expensive operation, both in terms of CPU and memory. This is the way technologies like PHP used 
 to work when they were first created.
  
 A demonstration of this concept is shown in Figure 
 2-2
 . In order to successfully reply to an HTTP request “A,” we 
 need some data from a database. This read can potentially take a long time. For this entire read duration, we will have 
 a process taking up CPU and memory while idling and waiting for the database response. Also, processes are slow to 
 start and have a significant overhead in terms of RAM space. This does not scale for very long and that is the reason 
 why modern web applications use a thread pool.
  
 Read Request
  
 User HTTP Request A
  
 New Process A
  
 Handle Request 
  
 Read Response
  
 A
  
 User HTTP Request B
  
 New Process B
  
 Handle Request
  
 B
  
 New Process C
  
 Handle Request
  
 C
  
 User HTTP Request C
  
  
 New Process D 
  
 Handle 
 Request 
  
  
  
 D User 
 HTTP Request D
  
 Figure 2-2.
  Traditional web server using Processes",NA
Traditional Web Servers Using a Thread Pool,"Modern servers use a thread from a thread pool to serve each request. Since we already have a few Operating System 
 (OS) threads created (hence a thread pool), we do not pay the penalty of starting and stopping OS processes (which 
 are expensive to create and take up much more memory than a thread). When a request comes in, we assign a thread 
 to process this request. This thread is reserved for the request in the entire duration that the request is being 
 handled, as demonstrated in Figure 
 2-3
 .
  
 24",NA
The Nginx Way ,"We have seen that creating separate processes and separate threads to handle requests results in wasted OS 
 resources. The way Node.js works is that there is a single thread handling requests. The idea that a single threaded 
 server can perform better than a thread pool server is not new to Node.js. Nginx is built on this principle.
  
  
 Nginx is a single-threaded web server and can handle a tremendous amount of concurrent requests. A simple 
 benchmark comparing Nginx to Apache, both serving a single static file from the file system, is shown in Figure 
 2-4
 .
  
 25",NA
Node.js Performance Secret,"There is a single execution thread in JavaScript. This is the way web browsers have traditionally worked. If you have 
 a long-running operation (such as waiting for a timer to complete or a database query to return), you must continue 
 operation using a callback. Listing 2-20 provides a simple demo that uses the JavaScript runtime setTimeout function 
 to simulate a long-running operation. You can run this code using Node.js.
  
 26",NA
More Node.js Internals,"It is not terribly important to understand the internals of how Node.js works, but a bit more discussion make you 
 more aware of the terminology when you discuss Node.js with your peers. At the heart of Node.js is an 
 event loop
 .
  
 Event loops enable any GUI application to work on any operating system. The OS calls a function within your 
 application when something happens (for example, the user clicks a button), and then your application executes the 
 logic contained inside this function to completion. Afterward, your application is ready to respond to new events that 
 might have already arrived (and are there on the queue) or that might arrive later (based on user interaction).",NA
Thread Starvation ,"Generally during the duration of a function called from an event in a GUI application, no other events can be 
 processed. Consequently, if you do a long-running task within something like a click handler, the GUI will become 
 unresponsive. This is something every computer user I have met has experienced at one point or another. This lack of 
 availability of CPU resources is called 
 starvation
 .
  
 Node.js is built on the same event loop principle as you find in GUI programs. Therefore, it too can suffer from 
 starvation. To understand it better, let’s go through a few code examples. Listing 2-22 shows a small snippet of 
 code that measures the time passed using console.time and console.timeEnd functions.
  
 Listing 2-22. 
 timeit.js
  
 console.time('timer'); 
  
 setTimeout(function(){
  
  
  console.timeEnd('timer'); 
  
 },1000)
  
  
 If you run this code, you should see a number quite close to what you would expect—in other words, 
 1000ms. This callback for the timeout is called from the Node.js event loop.
  
  
 Now let’s write some code that takes a long time to execute, for instance, a nonoptimized method of calculating 
 the nth Fibonacci number as shown in Listing 2-23.
  
 Listing 2-23. 
 largeOperation.js
  
 console.time('timeit'); 
  
 function fibonacci(n) {
  
  
  if (n < 2)
  
  
  
  return 1;
  
  
  elses
  
  
  
  return fibonacci(n - 2) + fibonacci(n - 1); 
  
 } 
  
 fibonacci(44);             // modify this number based on your system performance 
 console.timeEnd('timeit'); // On my system it takes about 9000ms (i.e. 9 seconds)
  
 Now we have an event that can be raised from the Node.js event loop (setTimeout) and a function that can keep 
 the JavaScript thread busy (fibonacci). We can now demonstrate starvation in Node.js. Let’s set up a time-out to 
 execute. But before this time-out completes, we execute a function that takes a lot of CPU time and therefore holds up 
 the CPU and the JavaScript thread. As this function is holding on to the JavaScript thread, the event loop cannot call 
 anything else and therefore the time-out is delayed, as demonstrated in Listing 2-24.
  
 28",NA
Data-Intensive Applications,"Node.js is great for data-intensive applications. As we have seen, using a single thread means that Node.js has an 
 extremely low-memory footprint when used as a web server and can potentially serve a lot more requests. Consider 
 the simple scenario of a data intensive application that serves a dataset from a database to clients via HTTP. We know 
 that gathering the data needed to respond to the client query takes a long time compared to executing code and/or 
 reading data from RAM. Figure 
 2-6
  shows how a traditional web server with a thread pool would look while it is 
 responding to just two requests.
  
 Thread pool
  
  
 User HTTP Request A
  
 Thread
  
 Assign Thread for A
  
 Handle Request
  
 Read Request
  
 Database
  
 A
  
  
 User HTTP Request B
  
 Thread
  
 Assign Thread for B
  
 Read Response
  
 Database
  
 Read Request
  
 Handle Request
  
 B
  
 Read Response
  
 Thread
  
 Figure 2-6.
  How a traditional server handles two requests
  
 29",NA
The V8 JavaScript Engine,"It is worth mentioning that all the JavaScript inside Node.js is executed by the V8 JavaScript engine. V8 came into 
 being with the Google Chrome project. V8 is the part of Chrome that runs the JavaScript when you visit a web page.
  
 Anybody who has done any web development knows how amazing Google Chrome has been for the web. 
  
 The browser usage statistics reflect that quite clearly. According to w3schools.org 
 (
 www.w3schools.com/browsers/ browsers_stats.asp
 ), nearly 56% of Internet users who visit their web site are 
 now using Google Chrome. There are lots of reasons for this, but V8 and its speed is a very important factor. Besides 
 speed, another reason for using V8 is that the Google engineers made it easy to integrate into other projects, and 
 that it is platform independent.",NA
More JavaScript,"Now that we understand the motivation for using Node.js, let’s delve further into JavaScript so that we can write 
 maintainable applications. In addition to the fact that you need to be good at JavaScript if you want to become a Node.
  
 js developer, another reason to be good at JavaScript is to take advantage of the thriving ecosystem around Node.js 
 and JavaScript in general. The language with the greatest number of projects on GitHub is JavaScript. Node.js is the 
 most popular server-side technology on GitHub, as shown in Figure 
 2-8
 , and the third most popular repository 
 overall.
  
  
 Figure 2-8.
  Most popular repositories on GitHub",NA
Everything Is a Reference,"JavaScript was designed to be simple and work with limited computer resources. Whenever we assign one variable to 
 another, JavaScript copies a reference to the variable. To understand what this means, have a look at Listing 2-26.",NA
Default Values,"The default value of any variable in JavaScript is undefined. You can see it being logged out in Listing 2-28, where you 
 create a variable but do not assign anything to it.
  
 Listing 2-28. 
 default1.js
  
 var foo; 
  
 console.log(foo); // undefined
  
 Similarly, a nonexistent property on a variable returns undefined (Listing 2-29).
  
 Listing 2-29. 
 default2.js
  
 var foo = { bar: 123 }; 
  
 console.log(foo.bar); // 123 
  
 console.log(foo.bas); // undefined",NA
Exact Equality,"One thing to be careful about in JavaScript is the difference between == and ===. As JavaScript tries to be resilient 
 against programming errors, == tries to do type coercion between two variables. For example, it converts a string to 
 a number so that you can compare it with a number, as shown in Listing 2-30.
  
 Listing 2-30. 
 equal1.js
  
 console.log(5 == '5');  // true 
  
 console.log(5 === '5'); // false
  
 32",NA
null,"null is a special JavaScript object used to denote an empty object. This is different from undefined, which is used by 
 JavaScript for nonexistent and noninitialized values. You should not set anything to undefined because, by convention, 
 undefined is the default value you should leave to the runtime. A good time to use null is when you want to explicitly 
 say that something is not there, such as as a function argument. You will see its usage in the section on error handling 
 in this chapter.",NA
Truthy and Falsy ,"One important concept in JavaScript is truthy values and falsy values. Truthy values are those that behave like true in 
 boolean operations and falsy values are those that behave like false in boolean operations. It is generally easier to use 
 if / else / ! for null / undefined instead of doing an explicit check. An example of the falsy nature of these values is 
 shown in Listing 2-32.
  
 Listing 2-32. 
 truthyandfalsy.js
  
 console.log(null == undefined);  // true 
  
 console.log(null === undefined); // false
  
 // Are these all falsy?
  
 if (!false) {
  
  
  console.log('falsy'); 
  
 } 
  
 if (!null) {
  
  
  console.log('falsy'); 
  
 } 
  
 if (!undefined) {
  
  
  console.log('falsy'); 
  
 }
  
 Other important falsy values are 0 and the empty string (''). All object literals and arrays are truthy in JavaScript.
  
 33",NA
Revealing Module Pattern,"Functions that return objects are a great way to create similar objects. An object here means data and functionality 
 bundled into a nice package, which is the most basic form of Object Oriented Programming (OOP) that one can do. 
  
 At the heart of the revealing module pattern is JavaScript’s support for closures and ability to return arbitrary 
 (function + data) object literals. Listing 2-33 is a simple example that shows how to create an object using this 
 pattern.
  
 Listing 2-33. 
 revealingModules.js
  
 function printableMessage() {
  
  
  var message = 'hello';
  
  
  function setMessage(newMessage) {
  
  
  
  if (!newMessage) throw new Error('cannot set empty message');
  
  
  message = newMessage;
  
  
  }
  
  
  function getMessage() {
  
  
  
  return message;
  
  
  }
  
  function printMessage() {
  
  console.log(message);
  
  }
  
  
  return {
  
  
  
  setMessage: setMessage,
  
  
  
  getMessage: getMessage,
  
  
  
  printMessage: printMessage
  
  
  }; 
  
 }
  
 // Pattern in use 
  
 var awesome1 = printableMessage(); 
  
 awesome1.printMessage(); // hello
  
 var awesome2 = printableMessage(); 
  
 awesome2.setMessage('hi'); 
  
 awesome2.printMessage(); // hi
  
 // Since we get a new object everytime we call the module function // 
 awesome1 is unaffected by awesome2 
  
 awesome1.printMessage(); // hello
  
 The great thing about this example is that it is a simple pattern to understand because it only utilizes closures, 
 first-class functions, and object literals—concepts that are already familiar to you and that we covered extensively 
 in the beginning of this chapter.",NA
Understanding this,"The JavaScript keyword this has a very special place in the language. It is something that is passed to the function 
 depending upon how you call it (somewhat like a function argument). The simplest way to think of it is that it refers 
 to the calling context. The calling context is the prefix used to call a function. Listing 2-34 demonstrates its basic 
 usage.",NA
Understanding Prototype,"It is a common misunderstanding that JavaScript isn’t object-oriented. It is indeed true that until recently JavaScript 
 has not had the class keyword. But functions in JavaScript are more powerful than they are in many other languages 
 and can be used to mimic traditional object oriented principles. The secret sauce is the new keyword (which you 
 have already seen) and a property called the prototype. Every object in JavaScript has an internal link to another 
 object called the 
 prototype
 . Before we look at creating traditional classes in JavaScript, let’s have a deeper look at 
 prototype.
  
 When you read a property on an object (for example, foo.bar reads the property bar from foo), JavaScript checks 
 that this property exists on foo. If not, JavaScript checks if the property exists on foo.__proto__ and so on till __proto__ 
 itself is not present. If a value is found at any level, it is returned. Otherwise, JavaScript returns undefined (see Listing 
 2-38).
  
 Listing 2-38. 
 prototype1.js
  
 var foo = {}; 
  
 foo.__proto__.bar= 123; 
  
 console.log(foo.bar); // 123
  
 Although this works, the __ prefix in JavaScript is conventionally used for properties that should not be used by 
 user code (in other words, private/internal implementation details). Therefore, you should not use __proto__ directly. 
 The good news is that when you create an object using the new operator on a function, the __proto__ is set to the 
 function’s .prototype member, whichcan be verified with a simple bit of code, as shown in Listing 2-39.
  
 Listing 2-39. 
 prototype2.js
  
 // Lets create a test function and set a member on its prototype function 
 foo() { }; 
  
 foo.prototype.bar = 123;
  
 // Lets create a object using `new` 
  
 // foo.prototype will be copied to bas.__proto__ 
  
 var bas = new foo();
  
 // Verify the prototype has been copied 
  
 console.log(bas.__proto__ === foo.prototype); // true 
 console.log(bas.bar);                         // 123
  
 36",NA
Error Handling,"Error handling
  is an important part of any application. Errors can happen because of your code or even in code not in 
 your controls, for example, database failure.
  
 JavaScript has a great exception handling mechanism that you might already be familiar with from other 
 programming languages. To throw an exception, you simply use the throw JavaScript keyword. To catch an exception, 
 you can use the catch keyword. For code you want to run regardless of whether an exception was caught or not, you 
 can use the finally keyword. Listing 2-43 is a simple example that demonstrates this.
  
 Listing 2-43. 
 error1.js
  
 try {
  
  
  console.log('About to throw an error');
  
  
  throw new Error('Error thrown'); 
  
 } 
  
 catch (e) {
  
  
  console.log('I will only execute if an error is thrown');
  
  
 console.log('Error caught: ', e.message); 
  
 } 
  
 finally {
  
  
  console.log('I will execute irrespective of an error thrown'); }
  
 38",NA
Summary,"In this chapter, we discussed important JavaScript concepts necessary to to be successful with Node.js. You should 
 now have a deep understanding of what makes JavaScript and Node.js great for creating data-intensive applications 
 and why it performs better than the technologies that preceded it. In the next chapter, we will discuss more Node.js 
 specific patterns and practices for creating maintainable applications.",NA
Works Cited,"Ryan Dahl (2009) 
 Node.js
  from JSConf.
  
  
 “Nginx vs. Apache,” 
 http://blog.webfaction.com/2008/12/a-little-holiday-present-10000-reqssec-with-nginx-2/
  
  
 “Browser Statistics,” 
 http://www.w3schools.com/browsers/browsers_stats.asp
  
  
 “GitHub Repository Search by Stars,” 
 https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type= 
 Repositories
  
 40",NA
Chapter 3,NA,NA
Core Node.js,"Node.js ships with a number of built-in modules that provide a core set of features we can build upon. In this 
 chapter, we will show the important parts of Node.js that every serious developer should be familiar with. The 
 great thing about Node.js is that it is quite possible for the average developer to be completely aware of exactly how 
 everything functions.
  
 To successfully deliver large applications and work in sizable teams, we need a way of encapsulating complexity. 
  
 JavaScript was originally designed to be read from top to bottom in a simplistic manner by a web browser, and files 
 were loaded using <script> tags. As larger and larger applications have been written in JavaScript, two module 
 systems (AMD and CommonJS) have been developed. They make code more manageable and reusable. Two patterns 
 exist because the browser and server offer different challenges in terms of module-loading latency (network requests 
 vs. file system). In this chapter, we will discuss these patterns and show how to reuse Node.js code in the browser.
  
 One note about the code samples in this and every other chapter that uses multiple files is that the main entry 
 point of the example is often called app.js following a Node.js community convention. So you should be able to run a 
 sample as node app.js.",NA
Node.js File-Based Module System,"Kevin Dongaoor created CommonJS in 2009 with the goal to specify an ecosystem for JavaScript modules on the 
 server. Node.js follows the CommonJS module specification. Following are a few salient points of the module system: 
  
 
  
 Each file is its own module.
  
 
  
 Each file has access to the current module definition using the module variable.
  
 
  
 The export of the current module is determined by the module.exports variable.
  
 
  
 To import a module, use the globally available require function.
  
 As always, it is best to jump right into the code. Let’s consider a simple example where we want to share a 
 function in file foo.js with various parts of our application. To export the function from the file, we simply assign it to 
 module.exports, as shown in Listing 3-1.
  
 Listing 3-1. 
 intro/base/foo.js
  
 module.exports = function () {
  
  
  console.log('a function in file foo'); 
  
 }; 
  
  
  
 In order to use this function from a file bar.js, we simply import foo using the globally available require 
 function and store the returned value in a local variable, as shown in Listing 3-2.
  
 41",NA
Node.js require Function ,"The Node.js require function is the main way of importing a module into the current file. There are three kinds of 
 modules in Node.js: 
 core modules
 , 
 file modules,
  and external 
 node_modules,
  all of which use the require function. We 
 are discussing file modules at the moment.
  
 When we make a require call with a relative path—for example, something like require('./filename') or 
 require('../foldername/filename')—Node.js runs the destination JavaScript file in a new scope and returns 
 whatever was the final value for module.exports in that file. This is the basis of file modules. Let’s look at the 
 ramifications of this design.",NA
Node.js Is Safe,"Modules in many programming environments are not safe and pollute the global scope. A simple example of this is 
 PHP. Say you have a file foo.php that simply defines a function foo, as shown in Listing 3-3.
  
 Listing 3-3. 
 foo.php
  
 function foo($something){
  
  
  return $something; 
  
 }
  
 If you want to reuse this function in a file bar.php, you can simply include foo.php using the include function, 
 and then everything from the file foo.php becomes a part of the (global) scope of bar.php. This allows you to use the 
 function foo, as shown in Listing 3-4.
  
 Listing 3-4. 
 include Function in PHP
  
 include('foo.php'); 
  
 foo();
  
 This design has quite a few negative implications. For example, what a variable foo means in a current file may 
 change based on what you import. As a result, you cannot safely include two files, foo1 and foo2, if there is a chance 
 that they have some variable with the same name. Additionally, 
 everything
  gets imported, so you cannot have local 
 only variables in a module. You can overcome this in PHP using namespaces, but Node.js avoids the potential of 
 namespace pollution altogether.
  
  
 Using the require function only gives you the module.exports variable, and you need to assign the result to a 
 variable locally in order to use it in scope, as shown in Listing 3-5.
  
 Listing 3-5. 
 Code Snippet to Show That You Control the Name
  
 var yourChoiceOfLocalName = require('./foo');
  
  
 There is no accidental global scope—there are explicit names and files with similar internal local variable 
 names that can coexist peacefully.
  
 42",NA
Conditionally Load a Module,"require behaves just like any other function in JavaScript. It has no special properties. This means that you can 
 choose to call it based on some condition and therefore load the module only if you need it, as shown in Listing 3-6.
  
 Listing 3-6. 
 Code Snippet to Lazy Load a Module
  
 if(iReallyNeedThisModule){
  
  
  var foo = require('./foo'); 
  
 }
  
 This allows you to lazy load a module only on first use, based on your requirements.",NA
Blocking,"The require function blocks further code execution until the module has been loaded. This means that the code 
 following the require call is not executed until the module has been loaded and executed. This allows you to avoid 
 providing an unnecessary callback like you need to do for all async I/O in Node.js, which was discussed in Chapter 2. 
  
 (See Listing 3-7.)
  
 Listing 3-7. 
 Code Snippet to Demonstrate That Modules Are Loaded Synchronously
  
 // Blocks execution till module is loaded 
  
 var foo = require('./foo');
  
 // Continue execution after it is loaded 
  
 console.log('loaded foo'); 
  
 foo();",NA
Cached,"As you know from Chapter 2, reading something from the file system is an order of magnitude slower than reading it 
 from RAM. Hence, after the first time a require call is made to a particular file, the module.exports is cached. The next 
 time a call is made to require that resolves to the same file (in other words, it does not matter what the original 
 relative file path passed to the require call is as long as the destination file is the same), the module.exports variable 
 of the destination file is returned from memory, keeping things fast. Listing 3-8 shows this speed difference with a 
 simple example.
  
 Listing 3-8. 
 intro/cached/bar.js
  
 var t1 = new Date().getTime(); 
  
 var foo1 = require('./foo'); 
  
 console.log(new Date().getTime() - t1); // > 0
  
 var t2 = new Date().getTime(); 
  
 var foo2 = require('./foo'); 
  
 console.log(new Date().getTime() - t2); // approx 0
  
 43",NA
Shared State,"Having some mechanism to share state between modules is useful in various contexts. Since modules are cached, 
 every module that require’s foo.js will get the same (mutable) object if we return an object foo from a module 
 foo.js. Listing 3-9 demonstrates this process with a simple example in which we export an object. This object is 
 modified in app.js, as shown in Listing 3-10. This modification affects what is returned by require in bar.js, as 
 shown in Listing 3-11. This allows you to share in-memory objects between modules that are useful for things like 
 using modules for configuration. A sample execution is shown in Listing 3-12.
  
 Listing 3-9. 
 intro/shared/foo.js
  
 module.exports = {
  
  something: 123 
  
 }; 
  
 Listing 3-10. 
 intro/shared/app.js
  
 var foo = require('./foo'); 
  
 console.log('initial something:', foo.something); // 123
  
 // Now modify something: 
  
 foo.something = 456;
  
 // Now load bar: 
  
 var bas = require('./bar'); 
  
 Listing 3-11. 
 intro/shared/bar.js
  
 var foo = require('./foo'); 
  
 console.log('in another module:', foo.something); // 456
  
 Listing 3-12. 
 Sample Run of intro/shared/app.js
  
 $ node app.js 
  
 initial something: 123 
  
 in another module: 456",NA
Object Factories,"As we have shown, the same object is returned each time a require call resolves to the same file in a Node.js process. 
 If you want some form of new object creation mechanism for each require function call, you can export a function 
 from the source module that returns a new object. Then require the module at your destination and call this 
 imported function to create a new object. An example is shown in Listing 3-13 where we export a function and then 
 use this function to create a new object, as shown in Listing 3-14.
  
 Listing 3-13. 
 intro/factory/foo.js
  
 module.exports = function () {
  
  
  return {
  
  
  
  something: 123
  
  
  }; 
  
 };
  
 44",NA
Node.js Exports,"Now that we understand require a bit more, let’s take a deeper look at module.exports.",NA
module.exports,"As stated earlier, each file in Node.js is a module. The items that we intend to export from a module should be 
 attached to the module.exports variable. It is important to note that module.exports is already defined to be a new 
 empty object in every file. That is, module.exports = {} is implicitly present. By default, every module exports an 
 empty object, in other words, {}. (See Listing 3-15.)
  
 Listing 3-15. 
 intro/module.exports/app.js
  
 console.log(module.exports); // {}",NA
Exports Alias,"So far, we have only been exporting a single object from a module. This can be done quite simply by assigning the 
 object we need exported to module.exports. However, it is a common requirement to export more than one variable 
 from a module. One way of achieving this is to create a new object literal and assign that to module.exports, as shown 
 in Listing 3-16.
  
 Listing 3-16. 
 intro/exports/foo1.js
  
 var a = function () {
  
  
  console.log('a called'); 
  
 };
  
 var b = function () {
  
  
  console.log('b called'); 
  
 };
  
 module.exports = {
  
  
  a: a,
  
  
  b: b 
  
 };
  
 45",NA
Modules Best Practices,"Now that we understand the technology behind the Node.js file-based module system, let’s look at a few best practices 
 followed by the community. Node.js and JavaScript are quite resilient to programming errors and try to be flexible, 
 which is why there are various ways that work. However, you should follow some conventions, and we highlight a few 
 that are common in the community.",NA
Do Not Use the .js Extension,"It is better to do require('./foo') instead of require('./foo.js') even though both work fine for Node.js.
  
 Reason:
  For browser-based module systems (such as RequireJS, which we look at later in this chapter), it 
 is assumed that you do not provide the .js extension since we cannot look at the server filesystem to see what 
 you meant. For the sake of consistency, avoid adding the .js extension in all your require calls.",NA
Relative Paths,"When using file-based modules, you need to use relative paths (in other words, do require('./foo') instead of 
 require('foo')).
  
  
 Reason:
  Non-relative paths are reserved for core modules and node_modules. We discuss core modules in this 
 chapter and node_modules in the next chapter.",NA
Utilize exports,"Try and use the exports alias when you want to export more than one thing.
  
  
 Reason:
  It keeps 
 what is exported
  close to its 
 definition
 . It is also conventional to have a local variable for each 
 thing you export so that you can easily use it locally. Do this all in a single line, as shown in Listing 3-20.
  
 Listing 3-20. 
 Create a Local Variable and Also Export
  
 var foo = exports.foo = /* whatever you want to export as `foo` from this module */ ;",NA
Export an Entire Folder,"If you have too many modules that go together that you keep importing into other files, try to avoid repeating 
 the import, as shown in Listing 3-21.
  
 Listing 3-21. 
 Avoid Repeating Huge Import Blocks
  
 var 
 foo 
 = 
 require('../something/foo'); 
  
 var 
 bar 
 = 
 require('../something/bar'); 
  
 var 
 bas 
 = 
 require('../something/bas'); 
  
 var qux = require('../something/qux');
  
  
 Instead, create a single index.js in the something folder. In index.js, import all the modules once and then 
 export them from this module, as shown in Listing 3-22.
  
 47",NA
Important Globals,"Node.js provides a fair number of globally available utility variables. Some of these variables are true globals 
 (shared between all modules) and some are local globals (variables specific to the current module). We have 
 already seen an example of a few true globals, the require function. And we have seen a few module-level implicitly 
 defined variables—module (used by module.exports) and exports. Let us examine a few more important globals.",NA
console,"console is one of the most useful globals available. Since it is so easy to start and restart a Node.js application from the 
 command line, the console plays an important part in quickly showing what is happening in your application when 
 you need to debug it. We have been using console.log throughout our examples for the same exact purpose. console 
 has a lot more o functions, which we discuss in Chapter 11.",NA
Timers,"We’ve seen setTimeout before when we were discussing the Node.js event loop in Chapter 2. It sets up a function to 
 be called after a specified delay in milliseconds. Note that this delay is the minimum interval after which the specified 
 function is called. The actual duration after which it will be called depends upon the availability of the JavaScript 
 thread as we saw in the section on thread starvation in Chapter 2. It also depends upon when the operating system 
 schedules the Node.js process to execute (normally not an issue). A quick example of setTimeout, which calls a 
 function after 1,000 milliseconds (in other words, one second) is shown in Listing 3-23.
  
 Listing 3-23. 
 globals/timers/setTimeout.js
  
 setTimeout(function () {
  
  
  console.log('timeout completed'); 
  
 }, 1000);
  
 Similar to the setTimeout function is the setInterval function. setTimeout only executes the callback function 
 once
  after the specified duration. But setInterval calls the callback repeatedly after 
 every
  passing of the specified 
 duration. This is shown in Listing 3-24 where we print out second passed after every second. Similar to setTimeout, 
 the actual duration may exceed the specified value depending on the availability of the JavaScript thread.
  
 48",NA
__filename and __dirname,"These variables are available in each file and give you the full path to the file and directory for the current module. 
 Being full paths means that they include everything right up to the root of the current drive this file resides on. Use 
 the code in Listing 3-26 to see these values change as you move the file to different locations on your filesystem and 
 run it.
  
 Listing 3-26. 
 globals/fileAndDir/app.js
  
 console.log(__dirname); 
  
 console.log(__filename);",NA
process,"process is one of the most important globals provided by Node.js. In addition to a few useful member functions and 
 properties that we will examine in the next section, it is a source of a few critical events that we examine in Chapter 
 5 when we take a deeper look at events.",NA
Command Line Arguments,"Since Node.js does not have a main function in the traditional C/C++/JAVA/C# sense, you use the process object to 
 access the command line arguments. The arguments are available as the process.argv member property, which is 
 an array. The first element is node (that is, the node executable), the second element is the name of the JavaScript 
 file passed into Node.js to start the process, and the remaining elements are the command line arguments. As an 
 example, consider a simple file argv.js, which simply logs these out to the console as shown in Listing 3-27. If you 
 run it as node argv.js foo bar bas, you will get output similar to what is shown in Listing 3-28.
  
 49",NA
process.nextTick,"process.nextTick is a simple function that takes a callback function. It is used to put the callback into the next cycle of 
 the Node.js event loop. It is designed to be highly efficient, and it is used by a number of Node.js core libraries. Its 
 usage is simple enough to demonstrate, and an example is shown in Listing 3-29. The output from this sample is 
 shown in Listing 3-30.
  
 Listing 3-29. 
 globals/process/nexttick.js
  
 // nexttick.js 
  
 process.nextTick(function () {
  
  
  console.log('next tick'); 
  
 }); 
  
 console.log('immediate'); 
  
 Listing 3-30. 
 Sample nexttick.js output
  
 immediat
 e 
  
 next tick
  
 As you can see, the immediate call is executed first, whereas the nextTick callback is executed in the next run of 
 the event loop. The reason why you should be aware of this function is because, due to the async nature of Node. js, 
 this function will show up in the call stack quite commonly as this will be the starting point of a Node.js event loop. 
 Everything before this function is in C. Everything after this function in the call stack is in JavaScript.",NA
Buffer,"Buffer World! Pure JavaScript is great for Unicode strings. However, to work with TCP streams and the file system, 
 the developers added native and fast support to handle binary data. The developers did this in Node.js using the 
 Buffer class, which is available globally.
  
 As a Node.js developer working on applications, your main interaction with buffer will most likely be in the form 
 of converting Buffer instances to string or strings to Buffer instances. In order to do either of these conversions, you 
 need to need to tell the Buffer class about what each character means in terms of bytes. This information is called 
 character encoding. Node.js supports all the popular encoding formats like ASCII, UTF-8, and UTF-16.
  
 50",NA
global,"The variable global is our handle to the global namespace in Node.js. If you are familiar with front-end JavaScript 
 development, this is somewhat similar to the window object. All the true globals we have seen (console, setTimeout, 
 and process) are members of the global variable. You can even add members to the global variable to make it 
 available everywhere, as shown in Listing 3-32. The fact that this makes the variable something available 
 everywhere is demonstrated in Listing 3-33.
  
 Listing 3-32. 
 globals/global/addToGlobal.js
  
 global.something = 123;
  
 Listing 3-33. 
 globals/global/app.js
  
 console.log(console === global.console);       // true 
 console.log(setTimeout 
 === 
 global.setTimeout); 
 // 
 true 
 console.log(process === global.process);       // true
  
 // Add something to global 
  
 require('./addToGlobal'); 
  
 console.log(something); // 123
  
 Even though adding a member to global is something that you can do, it is 
 strongly
  discouraged. The reason is 
 that it makes it extremely difficult to know where a particular variable is coming from. The module system is 
 designed to make it easy to analyze and maintain large codebases. Having globals all over the place is not 
 maintainable, scalable, or reusable without risk. It is, however, useful to know the fact that it can be done and, more 
 importantly, as a library developer you can extend Node.js any way you like.
  
 51",NA
Core Modules,"The Node.js design philosophy is to ship with a few battle-tested core modules and let the community build on these 
 to provide advanced functionality. In this section, we examine a few of the important core modules.",NA
Consuming Core Modules,"Consuming core modules is very similar to consuming file-based modules that you write yourself. You still use the 
 require function. The only difference is that instead of a relative path to the file, you simply specify the name of the 
 module to the require function. For example, to consume the core path module, you write a require statement like 
 var path = require('path'). As with file-based modules, there is no implicit global namespace pollution and what you 
 get is a local variable that you name yourself to access the contents of the module. For example, 
  
 in var path = require('path') we are storing it in a local variable called path. Now let’s examine a few core modules 
 that you should be aware of to be successful with Node.js.",NA
Path Module ,"Use require('path') to load this module. The path module exports functions that provide useful string 
  
 transformations common when working with the file system. The key motivation for using the path module is to 
 remove inconsistencies in handling file system paths. For example, path.join uses the forward slash `/` on UNIX-
 based systems like Mac OS X vs. backward slash `\` on Windows systems. Here is a quick discussion and sample of a 
 few of the more useful functions.",NA
path.normalize(str),"This function fixes up slashes to be OS specific, takes care of . and .. in the path, and also removes duplicate slashes. 
 A quick example to demonstrate these features is shown in Listing 3-34.
  
 Listing 3-34. 
 core/path/normalize.js
  
 var path = require('path');
  
 // Fixes up .. and .
  
 // logs on Unix: /foo 
  
 // logs on Windows: \foo 
  
 console.log(path.normalize('/foo/bar/..'));
  
 // Also removes duplicate '//' slashes 
  
 // logs on Unix: /foo/bar 
  
 // logs on Windows: \foo\bar 
  
 console.log(path.normalize('/foo//bar/bas/..'));",NA
"path.join([str1], [str2], …)","This function joins any number of paths together, taking into account the operating system. A sample is shown in 
 Listing 3-35.
  
 52",NA
"dirname, basename, and extname","These functions are three of the most useful functions in the path module. path.dirname gives you the directory 
 portion of a specific path string (OS independent), and path.basename gives you the name of the file. path.extname 
 gives you the file extension. An example of these functions is shown in Listing 3-36.
  
 Listing 3-36. 
 core/path/dir_base_ext.js
  
 var path = require('path');
  
 var completePath = '/foo/bar/bas.html';
  
 // Logs : /foo/bar 
  
 console.log(path.dirname(completePath));
  
 // Logs : bas.html 
  
 console.log(path.basename(completePath));
  
 // Logs : .html 
  
 console.log(path.extname(completePath));
  
 You should now have an understanding of how to use path and what its design goals are. Path has a few 
 other useful functions that you can explore online using the official Node.js documentation 
 (
 http://nodejs.org/api/ path.html
 ).",NA
fs Module,"The fs module provides access to the filesystem. Use require('fs') to load this module. The fs module has 
 functions for renaming files, deleting files, reading files, and writing to files. A simple example to write to the file 
 system and read from the file system is shown in Listing 3-37.
  
 Listing 3-37. 
 core/fs/create.js
  
 var fs = require('fs');
  
 // write 
  
 fs.writeFileSync('test.txt', 'Hello fs!');
  
 // read 
  
 console.log(fs.readFileSync('test.txt').toString());
  
 53",NA
os Module,"The os module provides a few basic (but vital) operating-system related utility functions and properties. You can 
 access it using a require('os') call. For example, if we want to know the current system memory usage, we can use 
 os.totalmem() and os.freemem() functions. These are demonstrated in Listing 3-40.
  
 Listing 3-40. 
 core/os/memory.js
  
 var os = require('os'); 
  
 var gigaByte = 1 / (Math.pow(1024, 3)); 
  
 console.log('Total Memory', os.totalmem() * gigaByte, 'GBs'); 
  
 console.log('Available Memory', os.freemem() * gigaByte, 'GBs'); 
  
 console.log('Percent consumed', 100 * (1 - os.freemem() / os.totalmem()));
  
 54",NA
util Module,"The util module contains a number of useful functions that are general purpose. You can access the util module 
 using a require('util') call. To log out something to the console 
 with a timestamp,
  you can use the util.log function, 
 as shown in Listing 3-42.
  
 Listing 3-42. 
 core/util/log.js
  
 var util = require('util'); 
  
 util.log('sample message'); // 27 Apr 18:00:35 - sample message
  
 Another extremely useful feature is string formatting using the util.format function. This function is similar to 
 the C/C++ printf function. The first argument is a string that contains zero or more 
 placeholders
 . Each placeholder is 
 then replaced using the remaining arguments based on the meaning of the placeholder. Popular placeholders are 
 %s (used for strings) and %d (used for numbers). These are demonstrated in Listing 3-43.
  
 Listing 3-43. 
 core/util/format.js
  
 var util = require('util'); 
  
 var name = 'nate'; 
  
 var money = 33;
  
 // prints: nate has 33 dollars 
  
 console.log(util.format('%s has %d dollars', name, money));
  
  
 Additionally, util has a few functions to check if something is of a particular type (isArray, isDate, isError). 
 These functions are demonstrated in Listing 3-44.
  
 Listing 3-44. 
 core/util/isType.js
  
 var util = require('util'); 
  
 console.log(util.isArray([])); // true 
  
 console.log(util.isArray({ length: 0 })); // false
  
 console.log(util.isDate(new Date())); // true 
  
 console.log(util.isDate({})); // false
  
 console.log(util.isError(new Error('This is an error'))); // true 
 console.log(util.isError({ message: 'I have a message' })); // false
  
 55",NA
Reusing Node.js Code in the Browser,"Before we learn how to reuse Node.js code in the browser, we need to learn a bit more about the various module 
 systems. We need to understand the need for AMD and what differentiates it from CommonJS.",NA
Introducing AMD,"As we discussed in the beginning of this chapter, Node.js follows the CommonJS module specification. This module 
 system is great for the server environment when we have immediate access to the file system. We discussed that 
 loading a module from the file system in Node.js is a blocking call for the first time. Consider the simple case of 
 loading two modules, as shown in Listing 3-45.
  
 Listing 3-45. 
 Code Snippet to Show Loading Two Modules Using CommonJS
  
 var 
 foo 
 = 
 require('./foo'); 
  
 var 
 bar 
 = 
 require('./bar'); 
  
 // continue code here
  
 In this example bar.js is not parsed until all of foo.js has been loaded. In fact, Node.js doesn’t even know that you 
 will need bar.js until foo.js is loaded and the line require('./bar') is parsed. This behavior is acceptable in a server 
 environment where it is considered a part of the bootstrap process for your application. You mostly require things 
 when starting your server and afterward these are returned from memory.
  
  
 However, if the same module system is used in the browser, each require statement would need to trigger 
 an HTTP request to the server. This is an order of magnitude slower and less reliable than a file system access 
 call. 
  
 Loading a large number of modules can quickly degrade the user experience in the browser. The solution is async, in-
 parallel, and upfront loading of modules. To support this async loading, we need a way to declare that this file will 
 depend upon ./foo and ./bar upfront and continue code execution using a callback. There is already a specification 
 for exactly this called async module definition (AMD). The same example from Listing 3-45 in AMD format is shown 
 in Listing 3-46.
  
 Listing 3-46. 
 code snippet to show loading two modules using AMD
  
 define(['./foo', './bar'], function(foo, bar){
  
  
  // continue code here 
  
 });
  
  
 The define function is not native to the browser. These must be provided by a third-party library. The 
 most popular of these for the browser is RequireJS (
 http://requirejs.org/
 ).
  
 To reiterate, the browser has different latency requirements from a server startup. This necessitates a different 
 syntax for loading modules in an async manner. The different nature of the require call is what makes reusing 
 Node.js code in the browser slightly more involved. Before we dig deeper, let’s set up a RequireJS bootstrap 
 application.",NA
Setting Up RequireJS,"Since we need to serve HTML and JavaScript to a web browser, we need to create a basic web server. We will be 
 using Chrome as our browser of choice as it is available on all platforms and has excellent developer tools support. 
 The source code for this sample is available in the chapter3/amd/base folder.",NA
Starting the Web Server,"We will be using server.js, which is a very basic HTTP web server that we will write ourselves in Chapter 6. Start the 
 server using Node.js (node server.js). The server will start listening for incoming requests from the browser on port 
 3000. If you visit http://localhost:3000, the server will try to serve index.html from the same folder as server.js if it is 
 available.",NA
Download RequireJS,"You can download RequireJS from the official web site (
 http://requirejs.org/docs/download.html
 ). It is a simple 
 JavaScript file that you can include in your project. It is already present in chapter3/amd/base folder.",NA
Bootstrapping RequireJS,"Create a simple index.html in the same folder as server.js with the contents shown in Listing 3-47.
  
 Listing 3-47. 
 amd/base/index.html
  
 <html> 
  
 <script 
  
  
  
 src=""./require.js"" 
  
  
  
 data-main=""./client/app""> 
  
 </script> 
  
 <body>
  
  
  <p>Press Ctrl + Shift + J (Windows) or Cmd + Opt + J (MacOSX) to open up the console</p> </body> 
  
 </html>
  
   
 We have a simple script tag to load require.js. When RequireJS loads, it looks at the data-main attribute on the 
 script tag that loaded RequireJS and considers that as the application entry point. In our example, we set the data-
 main attribute to ./client/app and therefore RequireJS will try and load http://localhost:3000/client/app.js.",NA
Client-Side Application Entry Point,"As we set up RequireJS to load /client/app.js, let’s create a client folder and an app.js inside that folder that 
 simply logs out something to the console, as shown in Listing 3-48.
  
 Listing 3-48. 
 amd/base/client/app.js
  
 console.log('Hello requirejs!');
  
  
 Now if you open up the browser http://localhost:3000 and open the dev tools (press F12), you should see the 
 message logged to the console, as shown in Figure 
 3-1
 .
  
 57",NA
Playing with AMD,"Now that we know how to start a RequireJS browser application, let’s see how we can import/export variables 
 in modules. We will create three modules: app.js, foo.js, and bar.js. We will use foo.js and bar.js from app.js using 
 AMD. This demo is available in chapter3/amd/play folder.
  
  
 To export something from a module, you can simply return it from the define callback. For example, let’s create a 
 file foo.js that exports a simple function, as shown in Listing 3-49.
  
 58",NA
More about AMD,"Here are a few useful and interesting facts that you should know about AMD to complete your knowledge:
  
 
  
 Modules are cached. This is similar to how modules are cached in Node.js—that is, the same 
  
 object is returned every time.
  
 
  
 Many of these arguments to define are optional and there are various ways to configure 
 how 
  
 modules are scanned in RequireJS.
  
 
  
 You can still do conditional loading of specific modules using a require call, which is another 
  
 function provided by RequireJS as shown in Listing 3-53. This function is also async and is 
  
 different from the Node.js version of require.
  
 Listing 3-53. 
 Snippet to show how you can conditionally load a module in AMD
  
 define(['./foo', './bar'], function(foo, bar){
  
  
  if(iReallyNeedThisModule){
  
  
  
  require(['./bas'], function(bas){
  
  
  
  
  // continue code here.
  
  
  
  });
  
  
  } 
  
 });
  
  
 The objective here was to give a quick overview of how you can use RequireJS and understand that the browser 
 is different from Node.js.",NA
Converting Node.js Code into Browser Code,"As you can see, there are significant differences between the browser module systems (AMD) and the Node.js module 
 system (CommonJS). However, the good news is that the Node.js community has developed a number of tools to take 
 your CommonJS / Node.js code and transform it to be AMD / RequireJS compatible. The most commonly used one 
 (and the one on which other tools rely) is Browserify (
 http://browserify.org/
 ).
  
 Browserify is a command line tool that is available as an NPM module. NPM modules are discussed in great detail 
 in the next chapter. For now, it is sufficient to know that if you have Node.js installed as specified in Chapter 1, you 
 already have npm available. To install Browserify as on the command line tool, simply execute the command shown 
 in Listing 3-54. (Note: On Mac OS X you need to run this as root (sudo npm install –g browserify).
  
 Listing 3-54. 
 Installing Browserify
  
 npm install -g browserify
  
 This installs Browserify globally (a concept that will become clear in the next chapter) and makes it further 
 available on the command line. Now if you run browserify, you should see output as shown in Figure 
 3-4
  to indicate a 
 successful installation.
  
 61",NA
Browserify Demo,"In this section, we will create a few simple Node.js modules and then use Browserify to convert them to AMD syntax 
 and run them in the browser. All of the code for this example is present in the chapter3/amd/browserify folder.
  
 First off, we will create three files that follow the Node.js / CommonJS module specification (code is in the 
 chapter3/amd/browserify/node folder). We are using foo.js (Listing 3-55) and bar.js (Listing 3-56) from app.js 
 (Listing 3-57) using CommonJS. You can run this code in Node.js to see that it works as expected.
  
 Listing 3-55. 
 amd/browserify/node/foo.js
  
 module.exports = function () {
  
  
  console.log('foo was called'); 
  
 }
  
 Listing 3-56. 
 amd/browserify/node/bar.js
  
 exports.log = function () {
  
  
  console.log('bar.log was called'); 
  
 }
  
 62",NA
Summary,"In this chapter, we discussed a few important maintainability topics that you should be aware of in order to become a 
 successful Node.js developer. We looked closely at require/module.exports combinations, giving you a firm 
 understanding of the principles of Node.js modules and its simplicity. Then we discussed a few core built-in Node.js 
 modules. (We will take a look at more of these core modules as we learn about events, streams, and specific areas 
 such as TCP/HTTP.) Finally, we discussed the differences between AMD and CommonJS and how to reuse Node.js 
 code in the browser.
  
 In the next chapter, we will discuss one of the great things about Node.js—its open source ecosystem. There are 
 packages upon packages of open source Node.js projects available, and we will show you how you can take advantage 
 of them using NPM.
  
 64",NA
Chapter 4,NA,NA
Node.js Packages,"Any application that can be written in JavaScript will eventually be written in JavaScript.
  
 —Atwood’s Law, by Jeff Atwood
  
 As we saw in Chapter 3, core Node.js does not ship with a massive list of libraries. There is a good reason for this. 
  
 Having something shipped as a part of core Node.js has the potential to stifle competition and evolution of ideas. 
  
 As a result, the core Node.js tries to limit what it contains and relies on the open source community to paint the 
 complete picture. Letting the open source community find the best solution to a problem X is better than prescribing 
 a one-hammer-for-everything solution.
  
 The number of JavaScript developers is much larger than for any other programming language. Also, more and 
 more people are writing more and more libraries to do any given task X in the browser, and all of this work is 
 available for you to use in your applications.
  
 To make it easy to consume third-party JavaScript libraries in your applications, Node.js comes with its own 
 package management system called 
 Node Package Manager
  (NPM)
 .
  In this chapter, we will discuss how to consume 
 NPM packages in our applications. We will cover a few important ones every Node.js developer should be aware of. 
 We will also use this as a practical opportunity to learn more about how JavaScript works.",NA
Revisiting Node Modules,"In the previous chapter, we learned that there are three kinds of Node.js modules: file-based modules, core modules, 
 and external node_modules. We discussed file-based modules and core modules, and now we will look at 
 node_modules
 .
  To understand them better, let’s take a deeper look at the file system scanning order of the Node.js 
 require function.
  
 
  
 If the module name passed into the require function is prefixed with ‘./’ or ‘../’ or ‘/’, then it is 
  
 assumed to be a file-based module and the file is loaded, as we saw in the Chapter 3. Some 
  
 sample calls: require('./bar), require('../bar/bar'), require('/full/path/to/a/node/ 
  
 module/file')
  
 
  
 Otherwise, we look for core modules with the same name, for example, bar if the call was 
  
 require('bar'). If no core module matching this name is found, we look for a node_module 
  
 called bar.
  
 65",NA
Scanning for node_modules,"Let’s just look at an example first. If a file /home/ryo/project/foo.js has a require call require('bar'), Node.js 
 scans the file system for node_modules in the following order. The first bar.js that is found is returned.
  
 
  
 /home/ryo/project/node_modules/bar.js
  
 
  
 /home/ryo/node_modules/bar.js
  
 
  
 /home/node_modules/bar.js
  
 
  
 /node_modules/bar.js
  
 In other words, Node.js looks for node_modules/bar.js in the current folder followed by every parent folder until 
 it reaches the root of the file system tree for the current file or until a bar.js is found. A simple example of this is a 
 module foo.js that loads a module node_modules/bar.js, as shown in Listing 4-1 and Listing 4-2.
  
 Listing 4-1. 
 hello/foo.js
  
 var bar = require('bar'); 
  
 bar(); // hello node_modules! 
  
 Listing 4-2. 
 hello/node_modules/bar.js
  
 module.exports = function () {
  
  
  console.log('hello node_modules!'); 
  
 }
  
 As you can see, our module bar.js looks 
 exactly
  the same as it would if we were simply using file-based modules. 
 And that is intentional. The 
 only
  difference between file-based modules and node_modules is 
 the way in which the file 
 system is scanned
  to load the JavaScript file. All other behavior is the same.",NA
Folder-Based Modules,"Before we discuss all the advantages of node_modules mechanism, we need to learn one final code organization trick 
 supported by the Node.js require function. It is not uncommon to have a several files working toward a singular goal. 
 It makes sense to organize these files into a single module that can be loaded with a single require call. We discussed 
 organizing such files into a single folder and having an index.js to represent the folder in Chapter 3.
  
 This is such a common scenario that Node.js has explicit support for this mechanism. That is, if a path to the 
 module resolves to a folder (instead of a file), Node.js will look for an index.js file in that folder and return that as the 
 module file. This is demonstrated in a simple example (chapter4/folderbased/indexbased1) where we export two 
 modules bar1.js and bar2.js using an index.js and load the module bar (and implicitly bar/index.js) in a module foo, as 
 shown in Listing 4-3 (run node folderbased/indexbased1/foo.js).
  
 Listing 4-3. 
 Implicit Loading of index.js from a Folder (Code: folderbased/indexbased1)
  
 // bar/bar1.js 
  
 module.exports = function () {
  
  
  console.log('bar1 was called'); 
  
 }
  
 // bar/bar2.js 
  
 module.exports = function () {
  
  
  console.log('bar2 was called'); 
  
 }
  
 66",NA
Advantages of node_modules,"We now understand that node_modules are the same as file-based modules with just a different file system scanning 
 mechanism used on loading the module JavaScript file. The obvious question at this point is, “What are the 
 advantages?”
  
 67",NA
Simplify Long File Relative Paths,"Say you have a module foo/foo.js that provides a number of utilities that you need to use at a variety of different 
 places in your application. In a section bar/bar.js, you would have a require call require('../foo/foo.js'), and in a 
 section bas/nick/scott.js, you would have a require call like require('../../../foo/foo.js'). At this point, you should ask 
 yourself, “Is this foo module self-contained?” If so, it is a great candidate to move into node_modules/foo/ index.js in 
 the root of your project folder. This way you can simplify your calls to be just require('foo') throughout your code.",NA
Increasing Reusability,"If you want to share a module foo with another project, you only need to copy node_modules/foo to that project. In 
 fact, if you are working on two similar sub projects, you can move node_modules/foo to a folder that contains both 
 your projects, as shown in Listing 4-5. This makes it easier for you to maintain foo from a single place.
  
 Listing 4-5. 
 Sample Code Organization for Sub Projects Using Shared node_modules
  
 projectroot
  
  |-- node_modules/foo
  
  |-- subproject1/project1files
  
  |-- subproject2/project2files",NA
Decreasing Side Effects,"Because of the way the node_modules are scanned, you can limit the availability of a module to a particular section of 
 your codebase. This allows you to safely do partial upgrades, assuming that your original code organization was as 
 shown in Listing 4-6.
  
 Listing 4-6. 
 Demo Project Using a Module foo
  
 projectroot
  
  |-- node_modules/foo/fooV1Files
  
  |-- moduleA/moduleAFiles
  
  |-- moduleB/moduleBFiles
  
  |-- moduleC/moduleCFiles
  
  
 Now, when you are working on a new module (say moduleD) that needs a new (and backward 
 incompatible) version of module foo, you can simply organize your code as shown in Listing 4-7.
  
 Listing 4-7. 
 Partial Upgrade of Module foo
  
 projectroot
  
  
  |-- node_modules/foo/fooV1Files
  
  
  |-- moduleA/moduleAFiles
  
  
  |-- moduleB/moduleBFiles
  
  
  |-- moduleC/moduleCFiles
  
  
  |-- moduleD
  
   
  |-- node_modules/foo/fooV2Files
  
   
  |-- moduleDFiles
  
  
 In this way, moduleA, moduleB, and moduleC continue to function as always and you get to use the new version 
 of foo in moduleD.
  
 68",NA
Overcoming Module Incompatibility,"Node.js does not suffer from the module dependency/incompatibility hell that is present in a number of traditional 
 systems. In many traditional module systems, a moduleX cannot work with moduleY because they depend on 
 different (and incompatible) versions of moduleZ. In Node.js, each module can have its own node_modules folder and 
 different versions of moduleZ can coexist. Modules do not 
 need
  to be global in Node.js!",NA
Module Caching and node_modules,"You might recall from our discussion in Chapter 3 that require caches the result of a require call after the first time. 
 The reason is the performance boost you get from not needing to load JavaScript and run it from the file system again 
 and again. We said that require returns the same object each time the path resolves 
 to the same file
 .
  
 As we have already shown, node_modules are just a different way of scanning for file-based modules. Therefore, 
 they follow the same module caching rule. If you have two folders where moduleA and moduleB require module foo 
 i.e require('foo'), which is present in some parent folder as shown in Listing 4-8, they get the same object (as exported 
 from node_modules/foo/index.js in the given example).
  
 Listing 4-8. 
 Both Modules Get the Same foo Module
  
 projectroot
  
  |-- node_modules/foo/index.js
  
  |-- moduleA/a.js
  
  |-- moduleB/b.js
  
 However, consider the code organization as shown in Listing 4-9. Here moduleB’s require('foo') call will resolve 
 to moduleB/node_modules/foo/index.js, whereas moduleA’s require call will resolve to node_modules/foo/index.js 
 and therefore they do not get the same object.
  
 Listing 4-9. 
 Module A and B Get Different foo Module
  
 projectroot
  
  
  |-- node_modules/foo/index.js
  
  
  |-- moduleA/a.js
  
  
  |-- moduleB
  
  
  
  |-- node_modules/foo/index.js
  
  
  
  |-- b.js
  
  
 This is a good thing as we have already seen that it prevents you from going into a dependency problem. But this 
 disconnection is something you should be conscious of.",NA
JSON ,"NPM uses JSON files for configuring modules. Before we delve deeper into NPM, let’s take a look at JSON.",NA
Beginner’s Introduction to JSON,"JSON is a standard format used to transfer data over the network. It can be considered a subset of JavaScript object 
 literals for most purposes. It basically restricts what JavaScript object literals are considered valid. JSON objects to 
 make it easier to implement the specification and shield users from edge cases they need to worry about. We will take 
 a practical look at JSON in this section.
  
 69",NA
Loading JSON in Node.js,"Since JSON is such an important part of the web, Node.js has fully embraced it as a data format, even locally. You can 
 load a JSON object from the local file system the same way you load a JavaScript module. Every single time within the 
 module loading sequence, if a file.js is 
 not found
 , Node.js looks for a file.json. If it is found, it returns a JavaScript object 
 representing the JSON object. Let’s work on a simple example. Create a file config.json with a single key foo and a 
 string value (shown in Listing 4-14).
  
 Listing 4-14. 
 json/filebased/config.js
  
 {
  
  
  ""foo"": ""this is the value for foo"" 
  
 }
  
 Now, let’s load this file as a JavaScript object in app.js and log out the value for the key foo (shown in Listing 4-
 15).
  
 Listing 4-15. 
 json/filebased/app.js
  
 var config = require('./config'); 
  
 console.log(config.foo); // this is the value for foo
  
  
 This simplicity of loading JSON explains why so many libraries in the Node.js community rely on using a JSON 
 file as a configuration mechanism.",NA
The JSON Global,"Data transfer over the wire takes place in the form of bytes. To write a JavaScript object from memory onto the wire 
 or to save to a file, you need a way to convert this object into a JSON string. There is a global object in JavaScript called 
 JSON that provides utility functions for converting a string representation of JSON to JavaScript objects and 
 converting JavaScript objects into a JSON string ready to be sent over the wire or written to the file or anything else 
 you want to do with it. This JSON global is available in Node.js as well all modern browsers.
  
 To convert a JavaScript object to a JSON string, you simply call JSON.stringify passing in the JavaScript object as 
 an argument. This function returns the JSON string representation of the JavaScript object. To convert a JSON string 
 into a JavaScript object, you can use the JSON.parse function, which simply parses the JSON string and returns a 
 JavaScript object matching the information contained in the JSON string, as shown in Listing 4-16 and Listing 4-17.
  
 71",NA
NPM,"Now we know how to create reusable modules using node_modules. The next piece of the puzzle answers the 
 question, “How do I get what the community has shared with me?”
  
  
 The answer: Node Package Manger, lovingly called 
 NPM
 . If you installed Node.js as specified in Chapter 1, it not 
 only added “node” to the command line, but it also added “npm”, which is simply a command line tool that integrates 
 with the online NPM registry (
 www.npmjs.org/
 ). A screenshot of NPM is shown in Figure 
 4-1
 .
  
 72",NA
package.json,"An integral part of the NPM ecosystem is a simple JSON file called package.json. This file has special meaning for NPM. 
 It is vital to have it set up properly when you want to share your module with the world, but it is just as useful if you 
 are consuming modules from other people. To create a package.json file in the current folder, just run the code in 
 Listing 4-18 on the command line.
  
 Listing 4-18. 
 Initializing a package.json File
  
 $ npm init
  
 This will ask you a few questions such as the name of the module and its version. I tend to just press enter 
 until the end. This creates a boilerplate package.json in the current folder with the name set to the current folder, 
 version set to 0.0.0, and a few other reasonable defaults as shown in Listing 4-19.
  
 Listing 4-19. 
 A Default package.json
  
 {
  
  ""name"": ""foo"",
  
  ""version"": ""0.0.0"",
  
  ""description"": """",
  
  ""main"": ""index.js"",
  
  ""scripts"": {
  
   
  ""test"": ""echo \""Error: no test specified\"" && exit 1"" },
  
  ""author"": """",
  
  ""license"": ""ISC"" 
  
 }
  
 73",NA
Installing an NPM Package,"Let’s install a module, for example, underscore (
 www.npmjs.org/package/underscore
 ) to a folder. To download the 
 latest version of underscore, you simply run the command shown in Listing 4-20.
  
 Listing 4-20. 
 Installing an NPM Module
  
 $ npm install underscore
  
 This will download the latest version of underscore from npmjs.org and put it into node_modules/underscore 
 in the current folder. TO load this module, all you need to do now is make a require('underscore') call. This is 
 demonstrated in Listing 4-21, where we load the underscore library and simply output the minimum element of an 
 array to the console.
  
 Listing 4-21. 
 Using an Installed Module
  
 // npm/install/app.js 
  
 var _ = require('underscore'); 
  
 console.log(_.min([3, 1, 2])); // 1
  
  
 We will take a look at underscore and other popular NPM packages later in this chapter; however, at this point 
 the focus is on the NPM command line tool.",NA
Saving Dependencies,"Whenever you run npm install, you have an optional command line flag available (--save) that tells NPM to write the 
 information about what you installed into package.json, as shown in Listing 4-22.
  
 Listing 4-22. 
 Installing an NPM Module and Updating package.json
  
 $ npm install underscore --save
  
  
 If you run install with –-save, not only will it download underscore into node_modules, it will also update 
 dependencies inside package.json to point to the installed version of underscore, as shown in Listing 4-23.
  
 Listing 4-23. 
 Updated Section of package.json
  
 ""dependencies"": {
  
   
  ""underscore"": ""^1.6.0""
  
  }
  
 There are quite a few advantages of keeping track of dependencies this way. For one, it is easy to know which 
 published version of a particular library you are using (depend upon), simply by looking at your package.json. The 
 same holds true when you are browsing the source code of other people’s modules. Just open up their package.json to 
 see what they are relying on.",NA
Refresh the node_modules Folder,"To refresh the node_modules folder from your package.json, you can run the following command:
  
 $ npm install
  
 74",NA
Listing All Dependencies ,"To see which packages you have installed, you can run npm ls command, as shown in Listing 4-24.
  
 Listing 4-24. 
 Listing Dependencies
  
 $ npm ls 
  
 foo@0.0.0
  
 └──
  underscore@1.6.0",NA
Removing a Dependency,"Remove a dependency using npm rm. For example, npm rm underscore --save deletes the underscore folder from 
 node_modules locally and modifies the dependencies section of your package.json. This command has an intuitive 
 synonym npm uninstall since the command was npm install for installation.",NA
package.json Online Dependency Tracking,"One more advantage of using package.json for dependency tracking is that if at some later point you decide to share 
 your module with the rest of the world (that is, share at npmjs.org), you do not need to ship the dependencies as your 
 users can download them online.
  
 If your package.json is set up properly and they install your module, NPM will automatically download and install 
 the dependencies of your module. To see a simple example, let’s install a package (request) that has dependencies, as 
 shown in Listing 4-25.
  
 Listing 4-25. 
 Installing a Module with Lots of Dependencies
  
 $ npm install request 
  
 npm http GET 
 https://registry.npmjs.org/request 
  
 npm http GET 
 https://registry.npmjs.org/tunnel-agent 
  
 npm http GET 
 https://registry.npmjs.org/json-stringify-safe 
 ...truncated...
  
 npm http 304 
 https://registry.npmjs.org/delayed-stream/0.0.5 
 request@2.34.0 node_modules\request
  
 ├──
  aws-sign2@0.5.0 
  
 ...truncated...
  
 ├──
  json-stringify-safe@5.0.0
  
 └──
  form-data@0.1.2 (async@0.2.10, combined-stream@0.0.4)
  
 You can see that NPM not only installed request but also brought down a number of other packages that request 
 depends upon. Each of these packages can, in turn, depend on other packages (for example, form-data depends upon 
 async and combined-stream), and they get their own local copy of the packages they depend upon (and will be 
 downloaded into their own node_modules folder, for example, node_modules/request/node_modules/form-data/ 
 node_modules/async). As discussed earlier, because of the way the require function works in Node.js, you can safely 
 use sub modules that depend on different version of the same module since they each get their own copy when set up 
 using NPM.
  
 75",NA
Semantic Versioning,"Good Node.js packages/NPM follow semantic versioning, which is an industry standard and should be followed as a 
 good software development practice. Semantics is the study of meaning. 
 Semantic versioning
  means versioning your 
 software in a way that the version numbers have significant meaning. There is much that can be said about semantic 
 versioning, but the following is a simplified but practical explanation for a Node.js developer:
  
 
  
 Put simply, Node.js developers follow a three-digit versioning scheme X.Y.Z where all X, Y, and 
  
 Z are non-negative integers. X is the major version, Y is the minor, and Z is the patch version.
  
 
  
 Patch versions 
 must
  be incremented if backward compatible fixes are introduced.
  
 
  
 Minor versions 
 must
  be incremented if backward compatible new features are introduced.
  
 
  
 Major versions 
 must
  be incremented if backward 
 incompatible
  fixes/features/changes are 
  
 introduced.
  
  
 Keeping these points in mind, you can see that v1.5.0 of a package should be in-place replaceable by v1.6.1 as the 
 new version should be backward compatible (major version 1 is same). This is something that good packages strive 
 for.
  
 However, the reality is that errors are sometimes inevitably introduced with new versions, or the code was 
 used in a manner that the original authors of the package did not predict. In such a case, some breaking change may 
 be introduced unknowingly.",NA
Semantic Versioning in NPM / package.json ,"NPM and package.json have great support for semantic versioning. You can tell NPM which version of a package you 
 want. For example, the following code installs the exact version 1.0.3 of underscore:
  
 $ npm install underscore@1.0.3
  
 You can tell NPM that you are okay with all patch versions of 1.0 using a tilde “~”:
  
 $ npm install underscore@""~1.0.0""
  
 Next up, to tell NPM that you are okay with any minor version changes use “^”:
  
 $ npm install underscore@""^1.0.0""
  
 Other version string operators supported include “>=” and “>”, which have intuitive mathematical meanings, 
 such as “>=1.4.2”. Similarly there are “<=” and “<” , for example, “<1.4.2”. There is also a * that can be used at various 
 locations to match any number such as 1.0.* (for example, 1.0.0, 1.0.1, and so on) or 1.* (for example, 1.1.0, 1.3.4, and 
 so on) and simply *, which will get you the latest version 
 every time
 .
  
  
 You can use these semantic version strings in package.json as well. For example, the following package.json tells 
 NPM that your package is compatible with any minor upgrade on v1.6.0 of underscore:
  
 ""dependencies"": {
  
  
  ""underscore"": ""^1.6.0""
  
  }
  
 76",NA
Updating Dependencies,"Whenever you use --save flag, the default that NPM uses for updating package.json dependencies section is “^”, 
 preceded by the downloaded version. The reason is that you should always try to use the latest version where the 
 major version number hasn’t changed. This way, you get any new features plus latest bug fixes for free, and there 
 should not be any breaking changes.
  
 As an example, if run the following command, you get a package.json dependencies section:
  
 $ npm install request@1.0.0 -save
  
 Following is the default version string added to package.json:
  
 ""dependencies"": {
  
  ""request"": ""^1.0.0"" 
  
 }
  
 However 1.0.0 is not the latest published version of request. To find the latest online version that is compatible 
 with the current semantic version specified in package.json (in this example ^1.0.0), you can run npm outdated, as 
 shown in Listing 4-26.
  
 Listing 4-26. 
 Check Latest Version of Packages
  
 $ npm outdated 
  
 npm http GET 
 https://registry.npmjs.org/request 
  
 npm http 304 
 https://registry.npmjs.org/request 
  
 Package  Current  Wanted  Latest  Location 
  
 request    1.0.0   1.9.9  2.34.0  request
  
  
 You can see that the latest version compatible with ^1.0.0 is ^1.9.9, which is the wanted version based on the 
 semantic string found in our package.json. It also shows you that there is an even more recent version available.
  
 To update these packages to the newest compatible version and save the results into your package.json (to 
 match the version numbers with what is downloaded), you can simply run the following command. Your updated 
 package.json is shown in Listing 4-27.
  
 $ npm update -save
  
 Listing 4-27. 
 Updated package.json
  
 ""dependencies"": {
  
  
  ""request"": ""^1.9.9"" 
  
 }
  
 Having this basic knowledge of package.json and the commands npm install, npm rm, npm update, and the --
 save NPM flag along with a respect for semantic versioning is mostly all you need to know about managing NPM 
 packages in your project.
  
 77",NA
Global Node.js Packages,"It is really simple to make command line utilities in Node.js. One of the most common motivations for learning Node.js 
 nowadays is the fact that a lot of management utilities for front-end projects are written in Node.js. There are projects 
 to test your web front ends, compile various new programming languages like CoffeeScript and TypeScript into 
 JavaScript and Sass, stylus, and less into CSS, minify your JavaScript and CSS and so on. Popular front-end JavaScript 
 projects such as jQuery, AngularJS, Ember.js, and React depend on Node.js scripts to manage their projects.
  
 The objective of global Node.js packages is to provide command line utilities that you can use from (surprise) the 
 command line. All of the NPM commands we have seen take an optional -g flag to indicate that you are working with 
 global modules.
  
 Remember in Chapter 3 we used a utility Browserify to convert Node.js code into browser AMD compatible code. 
 Browserify is a Node.js package we installed globally (npm install -g browserify). This put 
 browserify
  on the command 
 line, which we used in the previous chapter.
  
 Similarly, you can update global packages (npm update -g package-name), list global packages (npm ls -g), and 
 uninstall packages (npm rm -g package-name). For example, to uninstall Browserify, you would run 
  
 npm rm -g browserify.
  
 When installing modules globally, NPM does not modify your system configuration. The reason the command 
 line utility suddenly becomes available is because global modules are placed in a location (for example, 
 /usr/local/bin on Mac OSX and the user roaming profile’s NPM folder on Windows) where they become available on 
 the command line.",NA
Using require with Global Modules,"Modules installed globally are not meant to be used with a require function call in your code, although many 
 packages that support the global flag also support being installed locally in your project (the node_modules folder). 
 If installed locally, that is, without the –g flag, you can use them with the require function as we have already seen. A 
 good and simple example of this is the rimraf module (
 www.npmjs.org/package/rimraf
 ).
  
 If rimraf is installed globally (npm install -g rimraf), it provides a command line utility that you can use cross 
 platform for recursively and forcefully removing a directory (effectively rm -rf in Unix command line lingo). To 
 remove a directory foo after installing rimraf globally, simply run rimraf foo.
  
  
 To do the same from your Node.js code, install rimraf locally (npm install rimraf), create an app.js as shown in 
 Listing 4-28, and run it (node app.js).
  
 Listing 4-28. 
 global/rimrafdemo/app.js
  
 var rimraf = require('rimraf'); 
  
 rimraf('./foo', function (err) {
  
  
  if (err) console.log('Error occured:', err);
  
  
  else console.log('Directory foo deleted!'); 
  
 })
  
 For the sake of completeness, it is worth mentioning that there is a way to load modules from a global location 
 if you set the NODE_PATH environment variable. But this is 
 strongly
  discouraged when consuming modules and you 
 should use dependencies locally (package.json and node_modules).",NA
Package.json and require,"Most of package.json we have seen has been for NPM. All it was doing was managing our dependencies and putting 
 them in node_modules. From this point on, require works the way we have already shown. It looks for a JavaScript 
 file/folder in node_modules that matches what we asked require to load, for example, foo in require('foo'). We have 
 already shown that if it resolves to a folder, Node.js tries to load index.js from that folder as the result of the module 
 load.",NA
Modules Recap,"At this point, it might seem that require has a lot to do. Indeed it does, but in our honest opinion, it is all 
 kept quite simple and here is a recap to prove that you are already a Node.js modules expert! Assume 
 you require('something'). Then the follow is the logic followed by Node.js:
  
 
  
 If something is a core module, return it.
  
 
  
 If something is a relative path (starts with ‘./’ , ‘../’) return that file OR folder.
  
 
  
 If not, look for node_modules/filename or node_modules/foldername each level up until you 
  
 find a file OR folder that matches something.
  
 When matching a file OR folder:, follow these steps:
  
 
  
 If it matched a file name, return it.
  
 
  
 If it matched a folder name and it has package.json with main, return that file.
  
 
  
 If it matched a folder name and it has an index file, return it.
  
  
 Of course, the file can be a file.js or file.json since JSON is first class in Node.js! For JSON, we return the parsed 
 JSON and for JavaScript files we simply execute the file and return ‘module.exports’.
  
  
 That’s all there is to it. Having this knowledge allows you to open up and look at thousands of open source Node. 
 js packages available on npmjs.org and Github.",NA
Popular Node.js Packages,"Now that we know all the important details of consuming Node.js packages, let’s look at a few of the most popular 
 ones.",NA
Underscore ,"Underscore (npm install underscore) is by far the most popular JavaScript library available on NPM. It is the library 
 with the largest number of dependents (other packages that depend on this package).
  
 It is called 
 underscore
  because it creates a global variable ‘_’ when used in the browser. In node, you are 
 free to name the variable returned from require('underscore') whatever you want, but it is conventional to still 
 do var _ = require('underscore').
  
 Underscore provides a lot of functional programming support to JavaScript that you find in other languages such 
 as Ruby and Python. Every good JavaScript developer should be familiar with it. Note that bits of functionality of 
 underscore is being added to core JavaScript with new versions, but to work across all browsers and Node.js it is 
 recommended that you use underscore if only for consistency and lesser cognitive load (so you have less stuff to keep 
 in your head at a time).
  
  
 Let’s say we have an array of numbers and we only need the ones that are greater than 100. Doing this in plain 
 old JavaScript would look tedious, as shown in Listing 4-32.
  
 Listing 4-32. 
 popular/underscore/filter/raw.js
  
 var foo = [1, 10, 50, 200, 900, 90, 40];
  
 var rawResults = [] 
  
 for (i = 0; i < foo.length; i++) {
  
  
  if (foo[i] > 100) {
  
  
  
  rawResults.push(foo[i]);
  
  
  } 
  
 } 
  
 console.log(rawResults);",NA
Handling Command Line Arguments,"We looked at process.argv in Chapter 3. This is a simple array of all the command line argument passed into the node 
 process. We promised in the previous chapter that once we learn about NPM, we will look at a library that offers 
 better command line handling. Well, here it is. It’s called optimist. Because of the sheer number of command line 
 tools published on NPM, this is one of the most downloaded packages out there.
  
  
 As always, install using npm install optimist. It simply exports an object that contains the parsed command 
 line arguments as the argv property. So instead of using process.argv, you just use require('optimist').argv.
  
  
 Enough talk. Let’s code. Create a JavaScript file that simply logs out the processed arguments, as shown in 
 Listing 4-36.
  
 Listing 4-36. 
 popular/optimist/app1.js
  
 var argv = require('optimist').argv; 
  
 console.log(argv);
  
 If you run this right now, you will notice output similar to that in Listing 4-37.
  
 Listing 4-37. 
 Simple Run of popular/optimist/app1.js
  
 $ node app.js 
  
 { _: [],
  
  '$0': 'node /path/to/your/app.js' }
  
 Optimist preserves the first two members of the process.argv array (which are the node executable and the path 
 to your JavaScript file) as '$0'. Since we’d like to keep our output clean for this demo, let’s just delete this property so 
 we can log everything else to the console. To do this, modify your code to be what is shown in Listing 4-38.
  
 Listing 4-38. 
 popular/optimist/app.js
  
 var argv = require('optimist').argv; 
  
 delete argv['$0']; 
  
 console.log(argv);
  
 Now, if you run the app you get the following output:
  
 $ node app.js 
  
 { _: [] }
  
 Ahh, much better. The property argv._ is an array of all the command line arguments that are passed in which 
 are not flags. Flags are arguments that begin with a minus '-' sign, for example, '-f'. Let’s run app.js and pass in a 
 bunch of arguments, as shown in Listing 4-39.
  
 82",NA
Handling Date/Time Using Moment,"The built-in JavaScript Date type is fairly limited. It is good enough for simple cases, for example, you can create a date 
 representing the current time with a simple constructor call. There is also a constructor that lets you create dates at 
 the resolution that you want to work on such as year, month, day, hours, minutes, seconds, and milliseconds. One 
 thing to be aware of with JavaScript dates is that the month is 0 index based. So January is 0, February is 1, and so on. 
 You can see a few dates created in Listing 4-42.
  
 83",NA
Serializing Dates,"Since we are discussing dates, let’s discuss a good practice to follow when serializing dates for saving to a JSON file or 
 sending JSON over the wire. When we discussed JSON earlier, you might have noticed that Date is not supported as a 
 valid JSON value type. There are various ways for passing dates over the wire, but it is simplest to send them as 
 strings.
  
 What a particular date string means in terms of its actual date value differs based on the local culture (for 
 example, month before date or date before month), so it is best to follow a global standard. The ISO8601 standard 
 specifically relates to how a particular date should be represented as a string.
  
 Various formats are supported by ISO8601, but the one natively supported by JavaScript is something that looks 
 like 2014-05-08T17:35:16Z, where the date and time is expressed in the same string relative to UTC. Since it is 
 always relative to UTC, it is user time zone independent. This is a good thing since the user might not be in the same 
 time zone as the server and UTC is a global time reference.
  
 85",NA
Customizing Console Colors,"When working on large Node.js projects, it is not uncommon to end up with quite a few pieces of information getting 
 logged on the console for monitoring purposes. Over time, this simple output begins to look boring and is another 
 place where you will need to manage complexity. Syntax highlighting helps you manage code complexity. The colors 
 package (npm install colors) brings a similar benefit to your console output, making it easier to follow what is going 
 on. It is also one of the most used NPM packages (nearly 50,000 downloads a day).
  
 The API provided by colors is extremely simple. It adds functions to the native JavaScript string so that you can 
 do things such as ""some string"".red and, if you print this string it, will be colored red on the console. A small sample of 
 the various options used and the output is shown in Listing 4-49.
  
 86",NA
How Does It Work?,"There are two sides of this API:
  
 
  
 how to print colors on the console
  
 
  
 how to modify JavaScript strings and add functions to them
  
 Printing a string in a particular color is something that is supported by most consoles (windows and UNIX) using 
 ANSI escape codes
 . If you print one of these codes, the behavior of the console changes. Create a simple JavaScript file 
 that prints the JavaScript string surrounded by a few of these codes as shown in Listing 4-50. If you run it, you will 
 see a red string logged to the console.
  
 Listing 4-50. 
 popular/colors/2raw.js
  
 function getRed(str) {
  
  // Changes the console foreground to red
  
  var redCode = '\x1b[31m';
  
  // Resets the console foreground
  
  var clearCode = '\x1b[39m';
  
  
  return redCode + str + clearCode; 
  
 }
  
 console.log(getRed('Hello World!'));
  
 87",NA
Additional Resources,"NPM online registry: 
 http://npmjs.org/ 
  
 Semantic versioning the official guide: 
 http://semver.org/ 
  
 Semantic versioning parser in NPM: 
 https://github.com/isaacs/node-semver
  
 89",NA
Summary,"In this chapter, we discussed the remaining intricacies of the Node.js module system. Along the way, we presented 
 advantages for why the module system needs to work the way it does. The greatest advantage we feel is not having 
 a dependency hell problem that plagues so many other environments, where module incompatibilities prevent you 
 from using two modules that depend on different versions of a third module.
  
 We showed how NPM works. It is simply a way to manage node_modules based modules shared by the 
 Node.js community. We took a tour of the important command line options offered by NPM to manage the 
 community packages you use.
  
  
 You also learned about JSON and semantic versioning. Both pieces of information are vital information for all 
 developers (not just Node.js developers).
  
 Finally, we showed a number of important Node.js packages and the lessons that you can learn from them. 
  
 These should help make you a world-class Node.js and JavaScript developer, and you should not be afraid to 
 pop open the node_modules folder and see what makes a library you like 
 tick
 .
  
 90",NA
Chapter 5,NA,NA
Events and Streams,"Before we examine specific areas of Node.js development, we need to address a few more core concepts about 
 JavaScript in general and Node.js in particular. Node.js is focused on being the best-performing, simplest way to create 
 server applications. Events and streams play an important role in achieving this goal.
  
 Node.js is single-threaded; we have already discussed advantages of this fact. Due to this evented nature of Node.
  
 js, it has first-class support for an event subscription/unsubscription pattern. This pattern is very similar to the 
 way you would handle events using JavaScript in the browser.
  
  
 Streaming data is one of those areas that fit very naturally in Node.js. Streams are great for improving 
 user experience plus decreasing server resource utilization.
  
  
 To understand how we can create our own event emitters and streams, we first need to understand 
 JavaScript inheritance.",NA
Classical Inheritance in JavaScript,"We saw how prototype works in Chapter 2. JavaScript supports prototypal inheritance. In JavaScript, a member is 
 looked up on the current item (such as item.foo), followed by its prototype (item.__proto__.foo), which is followed by 
 its prototype’s prototype (item.__proto__.__proto__.foo), and so on until the prototype itself (for example, 
 item.__proto__.__proto__.__proto__) is null. We have already seen how this can be used to emulate a classical object 
 oriented 
 class
  construct in JavaScript. Now let’s look at how it can be used to implement classical object oriented 
 inheritance
 .",NA
Arriving at an Inheritance Pattern,"Let’s create an Animal class. It has a simple member function called walk. We’ve already discussed that `this` 
 within a function refers to the newly created object when a function is called with the `new` operator (for 
 example, `new Animal`). We also discussed that the prototype member of the constructor function 
 (Animal.prototype) is referenced by the object prototype (animal.__proto__) because of using the new operator. 
 (See Listing 5-1).
  
 Listing 5-1. 
 oo/1animal.js
  
 function Animal(name) {
  
  
  this.name = name; 
  
 } 
  
 Animal.prototype.walk = function (destination) {
  
  
  console.log(this.name, 'is walking to', destination); };
  
 var animal = new Animal('elephant'); 
  
 animal.walk('melbourne'); // elephant is walking to melbourne
  
 91",NA
Calling the Parent Constructor,"We cannot simply call the parent Animal constructor from Bird. That is because if we do so, then `this` within 
 Animal will not refer to the newly created Bird object (created from new Bird). Hence, we need to force the 
 meaning of this within the Animal function to point to the value of this inside the Bird function. Fortunately, we can 
 force the meaning by using the '.call' member function available on all JavaScript functions (it comes from 
 Function.prototype). Listing 5-3 presents a demonstration of the call member. As usual, the comments explain what 
 is going on.
  
 Listing 5-3. 
 oo/2call.js
  
 var foo = {}; 
  
 var bar = {};
  
 // A function that uses `this` 
  
 function func(val) {
  
  
  this.val = val; 
  
 }
  
 // Force this in func to be foo 
  
 func.call(foo, 123);
  
 // Force this in func to be bar 
  
 func.call(bar, 456);
  
 // Verify: 
  
 console.log(foo.val); // 123 
  
 console.log(bar.val); // 456
  
  
 You can see that we forced `this` inside the `func` function to be foo and then bar. Great! Now that we know 
 how to force this, let’s use it to call the parent, as shown in Listing 5-4.
  
 Listing 5-4. 
 Calling the Parent Constructor
  
 function Bird(name){
  
  
  Animal.call(this,name);
  
  // Any additional initialization code you want 
  
 } 
  
 // Copy all Animal prototype members to Bird
  
  
 You use this pattern (Parent.call(this, /* additional args */)) every time you need to call a parent 
 constructor. Now you have a firm functional understanding of why this is.
  
 93",NA
Setting Up the Prototype Chain,"We need a mechanism so that when we create a new Bird (such as, bird = new Bird), its prototype chain contains all 
 the parent prototype functions (for example, bird.__proto__.__proto__.walk). This can be done quite simply if we do 
 Bird.prototype.__proto__ = Animal.prototype.
  
 This process works because when we will do bird = new Bird, we will effectively get bird.__proto__.__ proto__ = 
 Animal.prototype and that will make the parent prototype members (for example, Animal.prototype.walk) available 
 on the child prototype (bird.__proto__.__proto__.walk), which was the desired result. Listing 5-5 shows a simple code 
 sample.
  
 Listing 5-5. 
 oo/3prototype.js
  
 // Animal Base class 
  
 function Animal(name) {
  
  
  this.name = name; 
  
 } 
  
 Animal.prototype.walk = function (destination) {
  
  
  console.log(this.name, 'is walking to', destination); };
  
 var animal = new Animal('elephant'); 
  
 animal.walk('melbourne'); // elephant is walking to melbourne
  
 // Bird Child class 
  
 function Bird(name) {
  
  
  Animal.call(this, name); 
  
 } 
  
 Bird.prototype.__proto__ = Animal.prototype; 
  
 Bird.prototype.fly = function (destination) {
  
  
  console.log(this.name, 'is flying to', destination); }
  
 var bird = new Bird('sparrow'); 
  
 bird.walk('sydney'); // sparrow is walking to sydney 
 bird.fly('melbourne'); // sparrow is flying to melbourne
  
  
 To understand how an inherited member (bird.walk in our example) lookup is performed, take a look at 
 Figure 
 5-2
 .
  
 94",NA
The Constructor Property ,"By default, every function gets a member called `prototype`, which we have already seen. By default, this member 
 has a constructor property that points to the function itself. Listing 5-6 demonstrates this.
  
 Listing 5-6. 
 oo/4constructor/1basic.js
  
 function Foo() { } 
  
 console.log(Foo.prototype.constructor === Foo); // true
  
 What is the advantage of having this property? After you create an instance using a function (for example, 
 instance = new Foo), you can get access to the constructor using a simple lookup instance.constructor (which is really 
 going to be looking at instance.__proto__.constructor). Listing 5-7 shows this in an example where we use the 
 property name of named functions (function Foo) to log out what created the object.
  
 95",NA
The Proper Node.js Way,"The util core module (require('utils')) we discussed in Chapter 3 provides a lovely little function to create the 
 prototype chain for us so you don’t need to mess with `__proto__` (the nonstandard property) yourself. The function 
 is called `inherits` and takes a child class followed by parent class, as shown in the example in Listing 5-8. Bird class 
 extends the Animal class, which we saw earlier.
  
 Listing 5-8. 
 oo/5nodejs/util.js
  
 // util function 
  
 var inherits = require('util').inherits;
  
 // Bird Child class 
  
 function Bird(name) { 
  
  
 // Call parent constructor 
  
  
 Animal.call(this, name);
  
  // Additional construction code 
  
 } 
  
 inherits(Bird, Animal);
  
 // Additional member functions 
  
 Bird.prototype.fly = function (destination) {
  
  
  console.log(this.name, 'is flying to', destination); }
  
 var bird = new Bird('sparrow'); 
  
 bird.walk('sydney'); // sparrow is walking to sydney 
 bird.fly('melbourne'); // sparrow is flying to melbourne
  
 There are two things of note:
  
 
  
 Call the parent constructor: Animal.call(this, /* any original arguments */)
  
 
  
 Set up the prototype chain: inherits(Bird, Animal);
  
 Simple enough to become second nature, this is all you need to do to inherit classes!
  
 96",NA
Overriding Functions in Child Classes,"To override parent functions but still utilize some of the original functionality, simply do the following:
  
 
  
 Create a function on the child prototype with the same name.
  
 
  
 Call the parent function similar to the way we called the parent constructor, basically using 
 the 
  
 Parent.prototype.memberfunction.call(this, /*any original args*/) syntax.
  
 Listing 5-9 demonstrates this.
  
 Listing 5-9. 
 oo/6override.js
  
 // util function 
  
 var inherits = require('util').inherits;
  
 // Base 
  
 function Base() { this.message = ""message""; }; 
  
 Base.prototype.foo = function () { return this.message + "" base foo"" };
  
 // Child 
  
 function Child() { Base.call(this); }; 
  
 inherits(Child, Base);
  
 // Overide parent behaviour in child 
  
 Child.prototype.foo = function () {
  
  
  // Call base implementation + customize
  
  
  return Base.prototype.foo.call(this) + "" child foo""; }
  
 // Test: 
  
 var child = new Child(); 
  
 console.log(child.foo()); // message base foo child foo
  
  
 We simply created the child function Child.prototype.foo and called the parent function from within 
 Base.prototype.foo.call(this).",NA
Checking Inheritance Chain,"Setting up a prototype chain (__proto__.__proto__) as we have seen has an additional advantage in that it allows you to 
 check if a particular object instance is of a particular class, or its parent class, or its parent’s parent class, and so on. 
 This is done using the instanceof operator.
  
 In pseudocode when you do someObj instanceof Func the you use this algorithm:
  
 
  
 Check someObj.__proto__ == Func.prototype and if so return true.
  
 
  
 If not, check someObj.__proto__.__proto__ == Func.prototype and if so return true.
  
 
  
 Repeat moving up the prototype chain.
  
 
  
 If __proto__ is null and we haven’t found a match, return false.
  
 From the pseudocode, you can see that it is very similar to how a property lookup is performed. We travel up the 
 prototype chain until we find a __proto__ equal to the Func.prototype. Finding a match is an indication of the new 
 operator being used on the specified Func as the new operator copies prototype to __proto__. A quick demonstration 
 of using instanceof is shown in Listing 5-10.
  
 97",NA
Deeper Understanding of the Internals of util.inherits,"You do not 
 need
  to go through this section, but it is worthwhile just so you can sit at the cool kids’ table. We said 
 that setting __proto__ manually is not recommended as it is not a part of the standardized JavaScript.
  
  
 Fortunately, there is a function in JavaScript that can create a blank object with a specified __proto__ already set. 
 The function is called Object.create and the way it works is shown in Listing 5-11.
  
 Listing 5-11. 
 oo/8internals/1check.js
  
 var foo = {}; 
  
 var bar = Object.create(foo); 
  
 console.log(bar.__proto__ === foo); // true
  
  
 In this example, we simply verified that the newly created object (that is, bar) has its __proto__ member set to 
 what we passed into Object.create (in other words, foo). It can be used for inheritance as shown in Listing 5-12.
  
 Listing 5-12. 
 oo/8internals/2inherit.js
  
 // Animal Base class 
  
 function Animal() { 
  
 } 
  
 Animal.prototype.walk = function () {
  
  
  console.log('walking'); 
  
 };
  
 // Bird Child class 
  
 function Bird() { 
  
 } 
  
 Bird.prototype = Object.create(Animal.prototype);
  
 var bird = new Bird(); 
  
 bird.walk();
  
 Compared to the original non-standard __proto__ mechanism we showed before, here we simply replaced Bird.
  
 prototype.__proto__ = Animal.prototype with what is effectively Bird.prototype = { __proto__ : Animal. prototype }.
  
 This mechanism correctly inherits the members from the parent, but it creates one slight problem. When we 
 reassigned Bird.prototype, the constructor information that was there in Bird.prototype.constructor was lost because 
 we reassigned Bird.prototype to a completely new object. To bring the constructor property back, a simple 
  
 98",NA
Node.js Events,"We already have a way to execute some code based on some occurrence (event) using callbacks. A more general 
 concept for handling occurrences of significance is events. An event is like a broadcast, while a callback is like a 
 handshake. A component that raises events knows nothing about its clients, while a component that uses callbacks 
 knows a great deal. This makes events ideal for scenarios where the significance of the occurrence is determined by 
 the client. Maybe the client wants to know, maybe it doesn’t. Registering multiple clients is also simpler with this 
 even as we will see in this section.
  
 Node.js comes with built-in support for events baked into the core events module. As always, use 
 require('events') to load the module. The events module has one simple class ""EventEmitter"", which we 
 present next.",NA
EventEmitter class,"EventEmitter is a class designed to make it easy to emit events (no surprise there) and subscribe to raised events. 
 Listing 5-15 provides a small code sample where we subscribe to an event and then raise it.
  
 Listing 5-15. 
 events/1basic.js
  
 var EventEmitter = require('events').EventEmitter;
  
 var emitter = new EventEmitter();
  
 // Subscribe 
  
 emitter.on('foo', function (arg1, arg2) {
  
  
  console.log('Foo raised, Args:', arg1, arg2); 
  
 });
  
 // Emit 
  
 emitter.emit('foo', { a: 123 }, { b: 456 });
  
 As shown in the example, you can create a new instance with a simple `new EventEmitter` call. To subscribe to 
 events, you use the `on` function passing in the event name (always a string) followed by an event handling function 
 (also called a 
 listener
 ). Finally, we raise an event using the emit function passing in the event name followed by any 
 number of arguments we want passed into the listeners (in Listing 5-15 we used two arguments for demonstration).",NA
Multiple Subscribers,"As we mentioned previously, having 
 built-in support
  for multiple subscribers is one of the advantages of using events. 
 Listing 5-16 is a quick sample where we have multiple subscribers for an event.
  
 Listing 5-16. 
 events/2multiple.js
  
 var EventEmitter = require('events').EventEmitter; 
  
 var emitter = new EventEmitter();
  
 emitter.on('foo', function () {
  
  
  console.log('subscriber 1'); 
  
 });
  
 100",NA
Unsubscribing,"The next question to ask is how do we unsubscribe from an event. EventEmitter has a removeListener function that 
 takes an event name followed by a function object to remove from the listening queue. One thing to note is that you 
 must have a reference to the function you want removed from the listening queue and, therefore, should not use an 
 anonymous (inline) function. This is because two functions in JavaScript are not equal if their bodies are the same, as 
 shown below in Listing 5-18, since these are two different and distinct function objects.
  
 Listing 5-18. 
 Sample to Demonstrate Function Inequality
  
 $ node -e ""console.log(function(){} == function(){})"" false
  
 101",NA
Has This Event Ever Been Raised?,"It is a common use case that you don’t care about every time an event is raised—just that it is raised once. For this, 
 EventEmitter provides a function `once` that calls the registered listener only once. Listing 5-20 demonstrates its 
 usage.
  
 Listing 5-20. 
 events/5once.js
  
 var EventEmitter = require('events').EventEmitter; 
  
 var emitter = new EventEmitter();
  
 emitter.once('foo', function () {
  
  
  console.log('foo has been raised'); 
  
 });
  
 // Emit twice 
  
 emitter.emit('foo'); 
  
 emitter.emit('foo');
  
 The event listener for foo will only be called once.",NA
Listener Management,"There are a few additional utility functions available on the EventEmitter that you need to be aware of to be a Node.js 
 event-handling expert.
  
 EventEmitter has a member function, listeners, that takes an event name and returns all the listeners 
 subscribed to that event. This can be very useful when you are debugging event listeners. Listing 5-21 
 demonstrates its usage.",NA
EventEmitter Memory Leaks,"A common source of memory leaks when working with events is subscribing to events in a callback but forgetting to 
 unsubscribe at the end. By default, EventEmitter will tolerate 10 listeners for 
 each
  event type—anymore and it will 
 print a warning to the console. This warning is specifically for your assistance. All you code will continue to function. 
 In other words, more listeners will be added without warning and all listeners will be called when an event is raised, 
 as shown in Listing 5-23.
  
 103",NA
Error Event,"An 'error' event is treated as a special 
 exceptional
  case in Node.js. If there is 
 no
  listener for it, then the default action is 
 to print a stack trace and exit the program. Listing 5-26 gives a quick sample to demonstrate this.
  
 Listing 5-26. 
 events/10errorEvent.js
  
 var EventEmitter = require('events').EventEmitter; 
  
 var emitter = new EventEmitter();
  
 // Emit an error event 
  
 // Since there is no listener for this event the process terminates 
 emitter.emit('error', new Error('Something horrible happened'));
  
 console.log('this line never executes');
  
 If you run this code, you will get an output, as shown in Listing 5-27. You should use an Error object if you ever 
 need to raise an error event, as we did in this example. You can also see from the example that the last line containing 
 the console.log never executes as the process terminated.
  
 Listing 5-27. 
 Sample Run of Error Event Sample
  
 $ node 10errorEvent.js
  
 events.js:72
  
  
  
  throw er; // Unhandled 'error' event
  
  
  
  
  ^ 
  
 Error: Something horrible happened
  
  
  at Object.<anonymous> (/path/to/10errorEvent.js:6:23)
  
  
 at Module._compile (module.js:456:26)
  
  
  at Object.Module._extensions..js (module.js:474:10)
  
  at 
 Module.load (module.js:356:32)
  
 105",NA
Creating Your Own Event Emitters ,"Now that you are an expert at handling and raising events in Node.js, a lot of open source surface area opens up to 
 you. A number of libraries export classes that inherit from EventEmitter and, therefore, follow the same event 
 handling mechanism. At this stage, it is useful for you to know how you can extend EventEmitter and create a public 
 class that has all of the functionality of EventEmitter baked in.
  
 All you need to do to create your own EventEmitter is call the EventEmitter constructor from your class’s 
 constructor and use the util.inherits function to set up the prototype chain. This should be second nature to you 
 considering the amount of discussion we gave this at the start of this chapter. Listing 5-28 is a quick example to 
 demonstrate this.
  
 Listing 5-28. 
 events/11custom.js
  
 var EventEmitter = require('events').EventEmitter; var 
 inherits = require('util').inherits;
  
 // Custom class 
  
 function Foo() { 
  
  
 EventEmitter.call(this); 
  
 } 
  
 inherits(Foo, EventEmitter);
  
 // Sample member function that raises an event 
  
 Foo.prototype.connect = function () {
  
  
  this.emit('connected'); 
  
 }
  
 // Usage 
  
 var foo = new Foo(); 
  
 foo.on('connected', function () {
  
  
  console.log('connected raised!'); 
  
 }); 
  
 foo.connect();
  
  
 You can see that usage of your class is exactly the same as if it was an EventEmitter. With these two simple lines, 
 you have a fully functional custom event emitter.",NA
Process Events,"A number of classes inside core Node.js inherit from EventEmitter. The global process object is also an instance of 
 EventEmitter, as you can see in Listing 5-29.
  
 106",NA
Global Exception Handler,"Any global unhandled exceptions can be intercepted by listening on the `uncaughtException` event on process. You 
 should not resume execution outside this event handler because this only happens when your application is in an 
 unstable state. The best strategy is to log the error for your convenience and 
 exit the process with an error code
 , as 
 shown in Listing 5-30.
  
 Listing 5-30. 
 process/1uncaught.js
  
 process.on('uncaughtException', function (err) {
  
  
  console.log('Caught exception: ', err);
  
  
  console.log('Stack:', err.stack); 
  
  
  
 process.exit(1); 
  
 });
  
 // Intentionally cause an exception, but don't try/catch it. 
 nonexistentFunc();
  
 console.log('This line will not run.');
  
 If you run the code in Listing 5-30, you get a nice error log, as shown in Listing 5-31.
  
 Listing 5-31. 
 Sample Run of an Uncaught Exception
  
 $ node 1uncaught.js 
  
 Caught exception:  [ReferenceError: nonexistentFunc is not defined] 
  
 Stack: ReferenceError: nonexistentFunc is not defined
  
  
  at Object.<anonymous> (E:\DRIVE\Google Drive\BEGINNING NODEJS\code\chapter5\ 
 process\1uncaught.js:8:1)
  
  
  at Module._compile (module.js:456:26)
  
  
  at Object.Module._extensions..js (module.js:474:10)
  
  
  at Module.load (module.js:356:32)
  
  
  at Function.Module._load (module.js:312:12)
  
  
  at Function.Module.runMain (module.js:497:10)
  
  
  at startup (node.js:119:16)
  
  
  at node.js:902:3
  
  
 The `uncaughtError` event is also raised on a process if any event emitter raises the `
 error
 ` event and there 
 are no listeners subscribed to the event emitter for this event.",NA
Exit,"The exit event is emitted when the process is about to exit. There is no way to abort exiting at this point. The event 
 loop is already in teardown so you 
 cannot
  do any 
 async
  operations at this point. (See Listing 5-32.)
  
 107",NA
Signals,"Node.js process object also supports the UNIX concept of signals, which is a form of inter-process communication. 
  
 It emulates the most important ones on Windows systems as well. A common scenario that is supported on both 
 Windows and UNIX is when the user tries to interrupt the process using Ctrl+C key combination in the terminal. By 
 default, Node.js will exit the process. However, if you have a listener subscribed to the `SIGINT` (signal interrupt) 
 event, the listener is called and you can choose if you want to exit the process (process.exit) or continue execution. 
 Listing 5-33 provides a small sample where we chose to continue running and exit after five seconds.
  
 Listing 5-33. 
 process/3signals.js
  
 setTimeout(function () {
  
  
  console.log('5 seconds passed. Exiting'); 
  
 }, 5000); 
  
 console.log('Started. Will exit in 5 seconds');
  
 process.on('SIGINT', function () {
  
  
  console.log('Got SIGINT. Ignoring.'); 
  
 });
  
  
 If you execute this example and press Ctrl+C, you will get a message that we are choosing to ignore this. Finally, 
 the process will exit after five seconds naturally once we don’t have any pending tasks (demonstrated in Listing 5-34).
  
 Listing 5-34. 
 Sample Run of Ignoring Ctrl+C Messages Demo
  
 $ node 3signals.js 
  
 Started. Will exit in 5 seconds 
  
 Got SIGINT. Ignoring.
  
 Got SIGINT. Ignoring.
  
 5 seconds passed. Exiting",NA
Streams,"Drop by drop is the water pot filled.
  
 —Buddha
  
 Streams play an important role in creating performant web applications. To understand what streams bring to 
 the table, consider the simple case of serving a large file (1GB) from a web server. In the absence of streams, it would 
 look like Figure 
 5-3
 . The user would have to wait a long time before they get any sign of the file they requested. 
  
 108",NA
Pipe,"All the streams support a pipe operation that can be done using the pipe member function. This is one of the things 
 that make streams in Node.js so awesome. Consider our simple initial scenario of loading a file from the file system 
 and streaming it to the client. This can be as simple as a code segment fileSystemStream.pipe(userSocket).
  
 You can pipe from a stream you can read from (Readable/Duplex/Transform) to a stream you can write to 
 (Writable/Duplex/Transform). This function is called 
 pipe
  because it mimics the behavior of the command line pipe 
 operator, for example, cat file.txt | grep lol.
  
  
 The fs core module provides utility functions to create readable or writable streams from a file. Listing 5-36 is 
 an example that streams a file from the file system to the user console.
  
 Listing 5-36. 
 streams/2pipe/1basic.js
  
 var fs = require('fs');
  
 // Create readable stream 
  
 var readableStream = fs.createReadStream('./cool.txt');
  
 // Pipe it to stdout 
  
 readableStream.pipe(process.stdout);
  
 You can also chain multiple streams using pipe. For example, the code in Listing 5-37 creates a read stream from 
 a file, pipes it through a zip transform stream, and then pipes it to a writable file stream. This creates a zip file on the 
 file system.
  
 Listing 5-37. 
 streams/2pipe/2chain.js
  
 var fs = require('fs'); 
  
 var gzip = require('zlib').createGzip();
  
 var inp = fs.createReadStream('cool.txt'); 
  
 var out = fs.createWriteStream('cool.txt.gz');
  
 // Pipe chain 
  
 inp.pipe(gzip).pipe(out);
  
 110",NA
Consuming Readable Streams,"We’ve said it many times already that streams work based on events. The most important event for a readable stream 
 is 'readable'. This event is raised whenever there is new data to be read from a stream. Once inside the event handler, 
 you can call the read function on the stream to read data from the stream. If this is the end of the stream, the read 
 function returns null, as demonstrated in Listing 5-38.
  
 Listing 5-38. 
 streams/3readable/basic.js
  
 process.stdin.on('readable', function () {
  
  
  var buf = process.stdin.read();
  
  
  if (buf != null) {
  
  
  
  console.log('Got:');
  
  
  
  process.stdout.write(buf.toString());
  
  
  }
  
  
  else {
  
  
  
  console.log('Read complete!');
  
  
  } 
  
 });
  
 A sample run of this code is shown in Listing 5-39, where we pipe data into process.stdin from the command 
 line.
  
 Listing 5-39. 
 Sample Run of streams/3readable/basic.js
  
 $ echo 'foo bar bas' | node basic.js 
  
 Got: 
  
 'foo bar bas' 
  
 Read complete!",NA
Writing to Writable Streams,"To write to a stream, you simply call write to write some data. When you have finished writing (end of stream), you 
 simply call end. You can also write some data using the end member function if you want, as shown in Listing 5-40.
  
 Listing 5-40. 
 streams/4writable/basic.js
  
 var fs = require('fs'); 
  
 var ws = fs.createWriteStream('message.txt');
  
 ws.write('foo bar '); 
  
 ws.end('bas');
  
 In this sample, we simply wrote foo bar bas to a writable file stream.
  
 111",NA
Creating Your Own Stream,"Creating your own stream is very similar to how you create your own EventEmitter. For streams you inherit from the 
 relevant base, stream class and implement a few base methods. This is detailed in Table 
 5-1
 .
  
 Table 5-1.
  Creating Your Own Custom Streams
  
 Use-case
  
 Class
  
 Method(s) to Implement
  
 Reading only
  
 Readable
  
 _read
  
 Writing only
  
 Writable
  
 _write
  
 Reading and writing
  
 Duplex
  
 _read, _write
  
 Operate on read data and write the result
  
 Transform
  
 _transform, _flush
  
  
 The inheritance mechanism is the same as we have seen before. That is, you call the base constructor from your 
 class constructor and call utils.inherits after declaring you class.",NA
Creating a Readable Stream,"As stated, you simply inherit from Readable class. You implement the _read member in your class, which is called by 
 the stream API internally when someone requests data to be read. If you have data that you want to be passed on 
 (pushed), you call the inherited member function push passing in the data. If you call push(null), this signals the end 
 of the read stream.
  
  
 Listing 5-41 is a simple example of a readable stream that returns 1-1000. If you run this, you will see all 
 these numbers printed (as we pipe to stdout).
  
 Listing 5-41. 
 streams/5createReadable/counter.js
  
 var Readable = require('stream').Readable; 
  
 var util = require('util');
  
 function Counter() {
  
  
  Readable.call(this);
  
  
  this._max = 1000;
  
  
  this._index = 1; 
  
 } 
  
 util.inherits(Counter, Readable);
  
 Counter.prototype._read = function () {
  
  
  var i = this._index++;
  
  
  if (i > this._max)
  
  
  
  this.push(null);
  
  
  else {
  
  
  
  var str = ' ' + i;
  
  
  
  this.push(str);
  
  
  } 
  
 };
  
 112",NA
Creating a Writable Stream,"Creating your own writable stream class is similar to how we created a readable stream. You inherit from the 
 Writable class and implement the _write method. The _write method is passed in a chunk that needs processing as its 
 first argument.
  
  
 Listing 5-42 is a simple writable stream that logs to the console all the data passed in. In this example, we simply 
 pipe from the readable file stream to this writeable stream (Logger).
  
 Listing 5-42. 
 streams/6createWritable/logger.js
  
 var Writable = require('stream').Writable; 
  
 var util = require('util');
  
 function Logger() {
  
  
  Writable.call(this); 
  
 } 
  
 util.inherits(Logger, Writable);
  
 Logger.prototype._write = function (chunk) {
  
  
  console.log(chunk.toString()); 
  
 };
  
 // Usage, same as any other Writable stream 
  
 var logger = new Logger();
  
 var readStream = require('fs').createReadStream('message.txt'); 
 readStream.pipe(logger);
  
 Again, for most purposes, the bulk of the functionality is handled internally by the Writable base class.",NA
Summary,"Hopefully, this chapter has given you a greater appreciation of JavaScript as a language. There are a few simple ideas 
 that provide a lot of expressive power. We started this chapter providing a crash course on JavaScript prototypal 
 inheritance along with explaining how simple it is to do in Node.js. We then showed how Node.js comes with built-in 
 support for common event-handling paradigms. We also demonstrated how you can create your own event emitter 
 with simple inheritance. Finally, we looked at streams and why you would want to add them to your arsenal. You saw 
 how easy it is to consume and write to streams in Node.js. It’s almost as if Node.js was designed for them! We ended 
 the chapter with a discussion on how you can create your own custom streams utilizing the built-in functionality 
 provided by Node.js core base classes.
  
 113",NA
Chapter 6,NA,NA
Getting Started with HTTP,"Node.js was created specifically to make scalable server-side and networking applications. It ships with battle-
 tested functionality to handle network connections effectively. This provides the groundwork on which the 
 community can build full-blown application servers.
  
 In this chapter, we will look at the 
 core
  functionality provided by Node.js for creating web applications. We will 
 then review the 
 connect
  middleware framework that allows you to create reusable web server components. Finally, 
 we will take a look at securing your web servers with 
 HTTPS
 .",NA
Basics of Node.js HTTP,"Following are the main core networking modules for creating web applications in Node.js:
  
 
  
 net / require('net'): provides the foundation for creating TCP server and clients
  
 
  
 dgram / require('dgram'): provides functionality for creating UDP / Datagram sockets
  
 
  
 http / require('http'): provides a high-performing foundation for an HTTP stack
  
 
  
 https / require('https'): provides an API for creating TLS / SSL clients and servers
  
 We will start by using the http module to create our simple server to serve static files. Creating our web server 
 from scratch will give us a deeper appreciation of the features provided by the community NPM modules that we will 
 explore later.
  
 ■
 Note
  We will be using 
 curl
  to test our web applications to start with. It is available by default on Mac 
 OS X / Linux. 
  
 You can get 
 curl
  for windows as a part of Cygwin (
 www.cygwin.com/
 ).
  
 The http module has a lovely little function, createServer, which takes a callback and returns an HTTP server. On 
 each client request, the callback is passed in two arguments—the incoming request stream and an outgoing server 
 response stream. To start the returned HTTP server, simply call its listen function passing in the port number you 
 want to listen on.
  
  
 Listing 6-1 provides  a simple server) that listens on port 3000 and simply returns “hello client!” on every 
 HTTP request.
  
 115",NA
Inspecting Headers,"Even at this point, a lot of HTTP logic has been silently taken care of. The actual request sent by curl contained a few 
 important HTTP headers. To see these, let’s modify the server to log the headers received in the client request (sent 
 by curl), as shown in Listing 6-4.
  
 Listing 6-4. 
 1create/2defaultheaders.js
  
 var http = require('http');
  
 var server = http.createServer(function (req, res) {
  
  
 console.log('request headers...'); 
  
  
  
 console.log(req.headers);
  
  // respond
  
  res.write('hello client!');
  
  res.end();
  
 }).listen(3000); 
  
 console.log('server running on port 3000');
  
 116",NA
Using a Debugging Proxy,"A great tool to use to help you explore and experiment with HTTP is a web debugging proxy. A debugging proxy is an 
 application that sits between the client and the server, and logs all the requests and responses exchanged between 
 the two. A brief overview of how this exchange is going to take place is shown in Figure 
 6-1
 .
  
  
 Figure 6-1.
  A web debugging proxy sits between the client and the server
  
 117",NA
Key Members of the Response Stream,"Beyond the fact that the response implements a writable stream, there are a few other useful methods that you need 
 to be aware of. The response is split into two sections: writing the headers and writing the body. The reason for this is 
 that the body might potentially contain a large chunk of data that needs streaming. The headers specify how this data 
 is going to be presented and needs to be interpreted by the client before such streaming can begin.
  
 As soon as you call response.write or response.end, the HTTP headers that you set up are sent, followed by the 
 portion of body that you want written. After this, you cannot modify the headers anymore. At any point, you can check 
 if the headers are sent using the read-only response.headersSent boolean value.",NA
Setting the Status Code,"By default, the status code is going to be 200 OK. As long as the headers are not sent, you can explicitly set the status 
 code using the statusCode response member (for example, to send a 404 NOT FOUND you would use the following 
 code:
  
 response.statusCode = 404;",NA
Setting Headers,"You can explicitly queue any HTTP header in the response using the response.setHeader(name, value) member 
 function. One common header that you need to set is the Content-Type of the response so that the client knows how 
 to interpret the data the server sends in the body. For example, if you are sending down an HTML file to the client, 
 you should set the Content-Type to text/html, which you can with the following code:
  
 response.setHeader(""Content-Type"", ""text/html"");
  
  
 The formal term for the value of the Content-Type header is 
 MIME type
 . MIME types for a few key content types 
 are shown in Table 
 6-1
 .
  
 Table 6-1.
  Popular MIME Types
  
 Name
  
 MIME type
  
 HyperText Markup Language (HTML) 
 Cascading Style Sheets (CSS) 
  
 JavaScript 
  
 JavaScript Object Notation (JSON) 
 JPEG Image 
  
 Portable Network Graphics (PNG)
  
 text/html 
  
 text/css 
  
 application/javascript 
 application/json 
  
 image/jpeg 
  
 image/png
  
 119",NA
Send Headers Only,"When you want to 
 explicitly
  send the headers (not just queue them) and move the response into 
 body only
  mode, you 
 can call the response.writeHead member function. This function takes the status code along with optional headers 
 that will be added on to any headers you might have already queued using response.setHeader. For example, here is 
 a snippet that sets the status code to 200 and sets the Content-Type header for serving HTML:
  
 response.writeHead(200, { 'Content-Type': 'text/html' });",NA
Key Members of the Request Stream,"The request is also a readable stream. This is useful for cases when the client wants to stream data to the server , 
 for example, file upload. The client HTTP request is also split into a head and body part. We can get useful 
 information about the client request HTTP head. For example, we have already seen the request.headers property, 
 which is simply a read-only map (JavaScript Object Literal) of header names and values (shown in Listing 6-10).
  
 Listing 6-10. 
 Snippet for Demonstrating Reading the Request Headers
  
 // Prints something like: 
  
 // 
  
 // { 'user-agent': 'curl/7.30.0', 
  
 //  host: '127.0.0.1:3000', 
  
 //  accept: '*/*' } 
  
 console.log(request.headers);
  
 To check an individual header, you can index this object like you index any other JavaScript object literal:
  
 console.log(request.headers['user-agent']); // 'curl/7.30.0'
  
 120",NA
Creating Your Own File Web ,NA,NA
Server ,NA,NA
Serving Base HTML,"Now that we have a deeper understanding of the response stream and MIME types, we can create a simple web 
 server that returns HTML files from a folder. Create a simple HTML file, called index.html, that we plan to return on 
 every request to GET ‘/’ on the server, as shown in Listing 6-11.
  
 Listing 6-11. 
 2server/public/index.html
  
 <html> 
  
 <head>
  
  
  <title>Hello there</title> 
  
 </head> 
  
 <body>
  
  
  You are looking lovely!
  
 </body
 > 
  
 </html>
  
 To start with, let’s create a few utility functions. It is always better to break out the functionality into separate 
 functions instead of one monolithic block of code. If we receive a request for a url that we do not accept, we should 
 return a 404 (Not Found) HTTP response. Listing 6-12 provides a function that does exactly that.
  
 Listing 6-12. 
 A Utility Function to Return 404 Not Found HTTP Response
  
 function send404(response) {
  
  
  response.writeHead(404, { 'Content-Type': 'text/plain' });
  
  
 response.write('Error 404: Resource not found.');
  
  
  response.end(); 
  
 }
  
 If we can satisfy the request, we should return HTTP 200 along with a MIME type for the content. Returning 
 the HTML file is as simple as creating a read file stream and piping it to the response. Listing 6-13 shows the 
 complete server code.
  
 Listing 6-13. 
 Code from 2server/server.js
  
 var http = require('http'); 
  
 var fs = require('fs');
  
 function send404(response) {
  
  
  response.writeHead(404, { 'Content-Type': 'text/plain' });
  
 121",NA
Serving a Directory,"That’s a good start for a quick handwritten static file server. However, it only serves one file. Let’s open it up a bit to 
 serve all the contents of a directory. First off, create a simple client-side JavaScript file that appends to the body 
 after HTML loading is complete, as shown in Listing 6-14. We plan to request this JavaScript file from the server.
  
 122",NA
Introducing Connect,"As we have seen, core Node.js modules provide basic but essential features for building your own web applications. 
  
 There are quite a few web frameworks available on NPM that build on this. A very popular one is connect 
 (npm install connect), which is a middleware framework.
  
 Middleware
  is basically any software that sits between your application code and some low level API. Connect 
 extends the built-in HTTP server functionality and adds a plug-in framework. The plug-ins act as middleware and, 
 hence, connect is a middleware framework.
  
 Connect has received a recent overhaul (connect 3.0) and now the core connect is just the middleware 
 framework. Each middleware is its own separate NPM module and a part of the larger Connect/ExpressJS ecosystem. 
 We will explore these middlewares in the next chapter. Here we are focused on using connect and authoring our own 
 middlewares.",NA
Creating a Bare-Bones Connect Application,"At the heart of connect is the connect function. Calling this function creates the connect dispatcher. The connect 
 dispatcher is simply a function that takes the request/response arguments, meaning that the dispatcher can be 
 used as an argument to http.createServer (which we saw earlier). Listing 6-17 shows the basic connect application.
  
 Listing 6-17. 
 4connect/1basic.js
  
 var connect = require('connect')
  
  , http = require('http');
  
 // Create a connect dispatcher 
  
 var app = connect();
  
 // Register with http 
  
 http.createServer(app)
  
  
  .listen(3000); 
  
 console.log('server running on port 3000');
  
 If you run this server, it will return a 404 (Not Found) for every client request. This is the built-in behavior 
 provided by connect. In the absence of some middleware to handle the client request, connect will return a 404. 
 We will create middleware in the next section.
  
 In addition to being a function that can accept a request and response objects, the connect dispatcher has a 
 member function, use, that is used to register a middleware with connect. We will look at this function shortly when 
 we create our own connect middleware.
  
 One utility function is listen. We will internally call http.createServer, register the connect dispatcher with it as 
 we shown previously, (that is, http.createServer(app)), and finally call the created server’s listen function. So you can 
 simply do what is shown in Listing 6-18, but it is useful to know that it is still a dispatcher on top of node core http.
  
 125",NA
Creating a Connect Middleware,"To register a middleware with connect, call the ‘use’ member method on the connect dispatcher passing in a function 
 that takes three arguments—a request, a response, and a next callback:
  
 
  
 request derives from the Node.js HTTP request class we saw earlier
  
 
  
 response derives from the Node.js HTTP response class we saw earlier
  
 
  
 next allows you to 
 optionally
  pass the control onto the next middleware registered with 
  
 connect OR inform connect about an error
  
  
 The simplest no-op (no operation) middleware would be one that does not look at the request, doesn’t modify 
 the response, and doesn’t simply pass over the control to the next middleware, as shown in Listing 6-19.
  
 Listing 6-19. 
 5middleware/1noop.js
  
 var connect = require('connect');
  
 // Create a connect dispatcher and register with http var app = 
 connect() 
  
  
  
 // register a middleware 
  
  
  
 .use(function (req, res, next) { next(); })
  
  
 .listen(3000); 
  
 console.log('server running on port 3000');
  
  
 Now that we are familiar with the fundamentals of middleware, let’s create one that logs the client 
 request’s method and url, as shown in Listing 6-20.
  
 Listing 6-20. 
 5middleware/2logit.js
  
 var util = require('util');
  
 // a simple logging middleware 
  
 function logit(req, res, next) {
  
  
  util.log(util.format('Request recieved: %s, %s', req.method, req.url));
  
  next(); 
  
 }
  
 var connect = require('connect');
  
 connect()
  
  .use(logit)
  
  .listen(3000);
  
 126",NA
Mounting Middleware by Path Prefix,"The use function takes an optional first argument to specify the endpoint for which the specified middleware will be 
 triggered. This is called 
 mounting
  as it is similar to OS disk mounting. For example, say we want to echo only when 
 requests come for ‘/echo’. For all other requests, we will return the message ‘Wassup’. This can be achieved as shown 
 in Listing 6-22.
  
 Listing 6-22. 
 5middleware/4prefix.js
  
 function echo(req, res, next) {
  
  
  req.pipe(res); 
  
 }
  
 var connect = require('connect');
  
 connect() 
  
  
  
 .use('/echo', echo)
  
  
  .use(function (req, res) { res.end('Wassup!'); })
  
  
 .listen(3000);
  
  
 All requests starting with ‘/echo’ will be handled by the echo middleware, whereas others will be passed to our 
 Wassup! responder. As you can see in Listing 6-23, it behaves as expected.
  
 Listing 6-23. 
 Demonstrating Mounting
  
 $ curl http://127.0.0.1:3000/echo -d ""hello world!"" hello 
 world!
  
 $ curl http://127.0.0.1:3000/ -d ""hello world!"" 
  
 Wassup!
  
 127",NA
Using an Object as Middleware,"As a middleware author, you have the option to use an object (instead of a simple function) to create a middleware 
 as long as the object has a handle method. For example, the echo middleware as an object would be as shown in 
 Listing 6-24.
  
 Listing 6-24. 
 5middleware/5object.js
  
 var echo = {
  
  
  handle: function (req, res, next) {
  
  
  
  req.pipe(res);
  
  
  } 
  
 };
  
 var connect = require('connect');
  
 connect()
  
  .use(echo)
  
  .listen(3000);
  
  
 This allows you to use class instances as middleware as long as they have a handle member function. This is just 
 for convenience and you can safely ignore this for your own creations.",NA
Creating Configurable Middleware,"You can use the power of JavaScript closures to create a configurable middleware. For example, in Listing 6-25, we 
 show a middleware that always returns the same message based on how it was configured. The configuration 
 message is captured in a closure by the function that we return.
  
 Listing 6-25. 
 5middleware/6configurable.js
  
 // Configurable middleware creator 
  
 function greeter(message) {
  
  
  return function (req, res, next) {
  
  
  
  res.end(message);
  
  
  }; 
  
 }
  
 var helloWorldGreeter = greeter('Hello world!'); 
  
 var heyThereGreeter = greeter('Hey there!');
  
 var connect = require('connect'); 
  
 connect()
  
  
  .use('/hello', helloWorldGreeter)
  
  
  .use('/hey', heyThereGreeter)
  
  
  .listen(3000);
  
 128",NA
The Power of Chaining,"There are many reasons why the chaining of middleware is awesome. For example, it allows middleware to share the 
 functionality of processing requests and responses. You can also use it for providing authorization and authentication. 
 Let’s consider a few practical examples.",NA
Sharing Request/Response Information,"The request and the response objects passed into each middleware are mutable and shared. You can use this to have a 
 middleware partially process requests for you to make it easier for consumption by a later middleware. As an 
 example, consider a simple middleware that tries to process the body into a JavaScript object if it detects that it is a 
 JSON request, as shown in Listing 6-27.
  
 Listing 6-27. 
 Snippet from 6chain/1parse.js
  
 function parseJSON(req, res, next) {
  
  
  if (req.headers['content-type'] == 'application/json') {
  
 // Load all the data
  
  var readData = '';
  
  req.on('readable', function () {
  
   
  readData += req.read();
  
  });
  
  
  
  // Try to parse
  
  
  
  req.on('end', function () { 
  
  
  
   
  
 try {
  
  
  
   
   
  req.body = JSON.parse(readData);
  
  
  
   
  }
  
  
  
   
  catch (e) { } 
  
  
  
   
  
 next();
  
  
  
  })
  
  
  }
  
  
  else { 
  
  
  
  
 next();
  
  
  } 
  
 }
  
 129",NA
Chaining Sample: Verifying Requests/Restricting Access ,"Because we need to explicitly pass control onto the next middleware by calling next, we can optionally stop 
 execution from proceeding at any time by not calling next and terminating the response ourselves (res.end).
  
 130",NA
Raising a connect Error,"One final thing worth mentioning is the fact that you can optionally pass an argument to ‘next’, which informs connect 
 that an error occurred in your middleware. No other middlewares in the chain are called and the error message is 
 sent to the client request with HTTP status code of 500 INTERNAL SERVER ERROR. Listing 6-35 is a simple example 
 to demonstrate this.
  
 Listing 6-35. 
 8error/1error.js
  
 var connect = require('connect');
  
 connect()
  
  .use(function (req, res, next) { next('An error has occurred!') }) .use(function (req, 
 res, next) { res.end('I will never get called'); }) .listen(3000);
  
 133",NA
HTTPS,"HTTPS is one of those things that have been difficult to implement in many previous web frameworks. Node.js has 
 first-class support for HTTPS built-ins. Before we show how simple it is to use HTTPS with Node.js, we will provide 
 a quick overview for the beginner.",NA
Asymmetric Cryptography,"The fundamental concept that makes HTTPS possible is 
 public-key encryption
  (also called 
 asymmetric encryption
 ). 
 For this type of encryption, you need two cryptography keys:
  
 
  
 A public key that 
 everyone knows
 , even possible malicious users
  
 
  
 A private key that 
 only you know
  
 135",NA
Making the Communication Secure Two Ways,"So, having a public key shared makes it secure to start a conversation with the server. How can you securely talk 
 back? Simple. The user (the browser basically) generates a pre-master secret on the fly and sends it to the server 
 securely in an encrypted message (encrypted with the server public key). The pre-master secret is used to generate a 
 session key, which is only valid for this session between the client and the server. Now the server and the client can 
 talk to each other if they encrypt the messages with this shared session key.
  
 This is the simplified description of the SSL (or the newer version called TLS) handshake. After the handshake, 
 the actual 
 standard HTTP
  conversation takes place where the entire HTTP message (including headers) is encrypted 
 using a session key. HTTPS is simply HTTP communication over an SSL secured communication channel.",NA
Generating Keys,"In this section, we will generate the public/private key pair ourselves. For this, we will use OpenSSL command line 
 tool, which is available on Mac OS X as well as Windows (as a part of Cygwin, which we also needed for curl).
  
  
 To generate a private key, we specify an encryption algorithm and its complexity. Use the command in 
 Listing 6-40 to generate a 1024 bit RSA key. A sample of the created key.pem file is shown in Listing 6-41.
  
 136",NA
Create an HTTPS Server,"Node.js has a core https module that you load using require('https'). It has a createServer function, which behaves 
 exactly the same as http.createServer except that it takes an additionally first 'options' argument that you can use to 
 provide the public/private keys.
  
 Listing 6-43 is the simple HTTP server we saw earlier in this chapter updated to use HTTPS.
  
 Listing 6-43. 
 9ssl/1basic.js
  
 var https = require('https');
  
 var fs = require('fs'); 
  
 var options = { 
  
  
 key: fs.readFileSync('./key.pem'), 
  
  
 cert: fs.readFileSync('./cert.pem') 
  
 };
  
 https.createServer(options, function (req, res) {
  
  
  res.end('hello client!'); 
  
 }).listen(3000);",NA
Use HTTPS by Default ,"Making a safe web site is easier to ensure if you only use HTTPS and redirect all HTTP traffic to use HTTPS. A lot of 
 web sites do that now. For example, if you visit 
 http://www.facebook.com
 , it will send you a 301 (MOVED 
 PERMANENTLY) HTTP response with the Location header set to 
 https://www.facebook.com
 . You can try it yourself; 
 if you open 
 http://www.facebook.com
  in your browser, the address will change to 
 https://www.facebook.com
 . So, 
 whenever possible, just use HTTPS and redirect all HTTP traffic to HTTPS.
  
 Although possible, it is not easy to run HTTP and HTTPS servers on the same port. However, you don’t have to. 
 By convention when you request an HTTP web site without specifying a port (for example, http://127.0.0.1) the 
 client tries to connect to port 80. However, if you request an HTTPS web site without specifying a port (such as 
 https://127.0.0.1), the client tries to connect to port 443 on the server. This allows you run an HTTPS server on port 
 443 and an HTTP server on port 80 that is simply redirecting client requests to use HTTPS. The code to do this is 
 shown in Listing 6-46.
  
 138",NA
Summary,"We started this chapter with a thorough examination of the built-in HTTP functionality found in core Node.js. This 
 close look was necessary because the community frameworks depend on it to provide advanced features, and having 
 a good grasp of the basics will go a long way in making you a successful Node.js developer.
  
 We followed this with a deep dive into the connect middleware framework. Connect allows you to share and 
 create manageable HTTP plug-ins. This helps you manage complexity, which increases your productivity as you can 
 divide and conquer software requirements. Connect is focused on HTTP applications and not very focused on web 
 sites, which is something that ExpressJS (another framework from the connect team) is well-suited for. We will look 
 at ExpressJS later in this book. But know that all of connect middleware is usable with ExpressJS since it has the same 
 middleware convention (that is, the use member function).
  
  
 Although we wrote about a few middlewares in this chapter, there are a ton of great (tested and secure) 
 middlewares already written by the Node.js community. We will examine those in the next chapter.
  
  
 Finally, we demonstrated how simple HTTPS is in Node.js. Whenever possible, you should just use HTTPS for all 
 your servers.
  
 Node.js is focused on Application servers. That means you can plug in deeply into HTTP and fully embrace all the 
 innovation that the Web has to offer. In fact, it is not uncommon to find network protocol developers moving to Node. 
 js for prototyping because of its low level access, yet great memory management.
  
 140",NA
Chapter 7,NA,NA
Introducing Express,"If you are going to make a Node.js web site today, you are probably going to use the Express web application 
 framework.
  
 In the previous chapter, we discussed the core of HTTP/HTTPS functionality provided by Node.js. We also 
 demonstrated how Connect provides a middleware framework on top of a raw createServer call. ExpressJS provides 
 everything that Connect provides (the same use function we saw in the last chapter as well as a dispatcher) and goes 
 a lot further. It forms the foundation of many web applications that we will explore it in this chapter.
  
  
 Along the way, we will introduce a few concepts that are relevant on your journey to becoming an 
 HTTP/Node.js expert.",NA
Basics of Express,"Express is available on NPM as express (npm install express). Let’s start with what is common with Connect. 
  
 Express is from the same team of developers that work on Connect. When you call require('express'), you get a 
 function that you can call to create an express app. This app has all of the behavior of the Connect dispatcher that we 
 saw in the previous chapter. For example, it can accept middleware using the ‘use’ function and can be registered with 
 http.createServer, as shown in Listing 7-1.
  
 Listing 7-1. 
 intro/1basic.js
  
 var express = require('express'),
  
  
  http = require('http');
  
 // Create an express application 
  
 var app = express()
  
  
  // register a middleware 
  
  
  
 .use(function (req, res, next) {
  
  
   
  res.end('hello express!');
  
  
  });
  
 // Register with http 
  
 http.createServer(app)
  
  
  .listen(3000);
  
 Having the ability to be registered as a listener with HTTP allows you to use HTTPS if you want to (the same 
 as Connect). Similar to Connect, Express provides a utility listen function to register itself with http. The simplest 
 Express application can be as simple as Listing 7-2.
  
 141",NA
Popular Connect /ExpressJS Middleware,"All Connect middleware is Express middleware. However, not all Express middleware is Connect middleware since 
 Express modifies the request and response a bit more for your convenience. For most simple middleware, it is not 
 an issue, but it is a fact that you need to be aware of.
  
 In this section, we will show popular Connect/Express middleware from the core team.",NA
Serving Static Pages,"One of the most common things you will want to right away is serve static web-site content. The serve-static 
 middleware (npm install serve-static) is designed specifically for that. We introduced a similar concept in the 
 previous chapter, but we left the matter unfinished (for example, vulnerable to path-based exploits) because, 
 while the concepts are valuable, you are better off using serve-static. (See Listing 7-3.)
  
 Listing 7-3. 
 static/1basic.js
  
 var express = require('express'); 
  
 var serveStatic = require('serve-static');
  
 var app = express()
  
  .use(serveStatic(__dirname + '/public'))
  
  .listen(3000);
  
  
 Run this code using node and you get a simple web server serving web pages from the `/public` directory. This 
 little server does quite a few nice things including the following:
  
 
  
 Sets the proper mime type of the response
  
 
  
 Has good HTTP response codes (For example, if you refresh the page and the HTML hasn’t 
  
 changed, you will notice that it sends the response 304 Not Modified, instead of 200 OK. If you 
  
 request a file that doesn’t exist, you get a 404. If the file cannot be accessed for some reason, it 
  
 sends a 500 Internal Server Error response.)
  
 
  
 By default, does not allow you to get files above the directory you want to serve (not 
 vulnerable 
  
 to the ../path bug we had in our simple server from the previous chapter)
  
 
  
 Serves the index.html from a directory if the path resolves to a directory 
  
 142",NA
Listing Directory Contents,"To list the contents of a directory, there is the serve-index (npm install serve-index) middleware. Since it only lists 
 the contents of the directory, it is conventional to use it in conjunction with the serve-static middleware to actually 
 allow the user to get the file. Listing 7-5 demonstrates its usage.
  
 Listing 7-5. 
 serveindex/basic.js
  
 var express = require('express'); 
  
 var serveIndex = require('serve-index');
  
 var app = express()
  
  .use(express.static(__dirname + '/public'))
  
  .use(serveIndex(__dirname + '/public'))
  
  .listen(3000);
  
 By default, it gives a nice directory listing page with a search box as shown in Figure 
 7-1
 .",NA
Accepting JSON Requests and HTML Form Inputs,"Body parsing
  is the act of parsing a string based 
 client request body
  into a JavaScript object that you can easily 
 consume in your application code. This is a 
 very
  common task in web development, making the body-parser 
 middleware (npm install body-parser) a must-have in your tool-belt. It simply does the following two things:
  
 
  
 Parses the request body into a JavaScript object if the content-type matches JSON 
  
  
 (application/JSON) or a user submitted HTML form (the browser sends it as the MIME type 
  
 application/x-www-form-urlencoded)
  
 
  
 Puts this JavaScript object (if the parse was successful) in req.body for easy access in later 
  
 middleware
  
  
 Listing 7-6 provides a simple example to respond to the client based on what the body-parser middleware has 
 parsed.
  
 Listing 7-6. 
 bodyparser/basic.js
  
 var express = require('express'); 
  
 var bodyParser = require('body-parser');
  
 var app = express()
  
  
  .use(bodyParser())
  
  
  .use(function (req, res) {
  
  
  
  if (req.body.foo) {
  
  
  
  
  res.end('Body parsed! Value of foo: ' + req.body.foo);
  
  
  }
  
  
  
  else {
  
  
  
  
  res.end('Body does not have foo!');
  
  
  
  }
  
  
  })
  
  
  .use(function (err, req, res, next) {
  
  
  
  res.end('Invalid body!');
  
  
  })
  
  
  .listen(3000);
  
 144",NA
Handling Cookies,"A 
 cookie
  is some data sent from the web server and stored in the user’s web browser. Every time the user’s browser 
 makes a request to the web server, the web browser will send back the cookie that it received from the server. 
 Cookies provide a great foundation for creating user sessions.
  
 145",NA
Preventing Cookie User Modification Using Signing,"Since cookies are stored on the client system (and sent in the client request), it is possible for the user to forge a 
 cookie. We can digitally sign a cookie to detect any client cookie forgery. This feature is also provided by the same 
 cookie-parser middleware.
  
 A 
 digital signature
  assures the authenticity of a piece of data. Express cookie signing is done using a 
 Key-Hash 
 message authentication code (HMAC)
 . The HMAC is calculated by taking a secret key (known only to the server) and 
 combining it with a hashing algorithm to calculate a hash for the content of the cookie. Since the secret is only known 
 to the server, the HMAC can only be calculated and verified by the server.
  
 If we create a signed cookie using Express (by providing a secret key), the HMAC value is appended to the value 
 of the cookie we send to the client. So when the client sends the cookie back to us in a request, we can look at the 
 HMAC value in the cookie and compare it with the re-calculated HMAC value to check if it matches the content. If it 
 doesn’t match, we know the cookie is bad, and we can discard it. All of this is done for you by the cookie-parser 
 middleware.
  
 You can 
 set the secret key
  for cookie signing by passing it into the cookie-parser middleware creation function—
 in other words, use(cookieParser('optional secret string')). To 
 set a signed cookie,
  you simply call res.cooki 
 e(name,value,{
 signed:true
 }) (in other words, name and value as normal and passing in an option of signed=true). To 
 read the signed cookies
  sent in the client request, use the req.signedCookies just like you use req.cookies. Using a 
 different property when reading makes it easy for you to know that the cookie signature has been validated. As an 
 example, Listing 7-14 shows our toggle cookie server updated to use signed cookies.
  
 Listing 7-14. 
 cookie/5sign.js
  
 var express = require('express'); 
  
 var cookieParser = require('cookie-parser');
  
 var app = express()
  
  
  .use(cookieParser('my super secret sign key'))
  
  
  .use('/toggle', function (req, res) {
  
  
  
  if (req.signedCookies.name) {
  
  
  
  
  res.clearCookie('name');
  
  
  
  
  res.end('name cookie cleared! Was:' + req.signedCookies.name);
  
  
  }
  
  
  
  else {
  
  
  
  
  res.cookie('name', 'foo', { signed: true });
  
  
  
  
  res.end('name cookie set!');
  
  
  
  }
  
  
  })
  
  
  .listen(3000);",NA
httpOnly and Secure,"By default, the user’s browser JavaScript can read the cookie set for the current web page (using document.cookie). 
  
 This makes a cookie vulnerable to cross-side scripting (XSS). That is, if some malicious user manages to inject 
 JavaScript into your web-site content, it allows that JavaScript to read cookies that might contain sensitive 
 information for the currently logged in user and ship it to a malicious web site. To prevent JavaScript access to 
 cookies, you can set httpOnly to true (res.cookie(name,value,{
 httpOnly:true
 })). This tells the browser that it should 
 not allow any JavaScript access to this cookie and that it should only be used when communicating with the server.
  
 Additionally, as we showed in the previous chapter, you should use HTTPS on all your public servers. HTTPS 
 ensures that all headers, including the cookie header, are encrypted and safe from man in the middle attacks. To ask 
 the browser to never send a particular cookie over HTTP and only use it for HTTPS, you can set the secure flag, that is, 
 res.cookie(name,value,{
 secure:true
 }).
  
  
 Based on what you know now, for sensitive cookies you should always use httpOnly and secure set to true. 
 Of course, secure needs an HTTPS server.",NA
Setting a Cookie Expiry,"Cookies are great to store persistent bits of information relevant to a particular user, with the user, in their browser. 
  
 However, in the absence of a cookie expiry time mentioned in the Set-Cookie header, the browser clears the cookie 
 after the browser is closed! Such a cookie is commonly called a browser-session cookie (since it only valid for current 
 browser session). You should always set the expiry if you want cookies to persist over a period of time. You can do 
 that by passing in a maxAge option to setCookie, which takes the number of milliseconds for which this cookie will be 
 valid and sent in client requests. For example:
  
 res.cookie('foo', 'bar', { maxAge: 900000, httpOnly: true })",NA
Cookie-Based Sessions,"To provide a consistent user experience between different HTTP requests by the same user, it is common to have user 
 session
  information available alongside a client HTTP request. As a simple example, we might want to know if the user 
 has logged in or not.
  
 Cookies provide a nice foundation for small bits of such user-specific information we want to be associated 
 with a client request. However, the API is too low level. For example, the cookie value can only be a string. If you 
 want JavaScript objects, you need to do JSON parsing. This is where the cookie-session middleware (npm install 
 cookie-session) comes in. It allows you to use a single cookie to store information you feel relevant is for this user 
 session.
  
 When using the cookie-session middleware, the user session object is exposed as req.session. You can set or 
 update a value using a simple assignment to a member of req.session. You can clear a value by deleting a key from 
 req.session. By default, the cookie-session middleware must be passed at least one secret key, which it uses to 
 ensure the integrity of the session cookie by signing, as we saw earlier. Listing 7-15 gives a simple example to 
 demonstrate using the cookie-session middleware.
  
 Listing 7-15. 
 cookiesession/counter.js
  
 var express = require('express'); 
  
 var cookieSession = require('cookie-session');
  
 var app = express()
  
  
  .use(cookieSession({ 
  
  
   
 keys: ['my super secret sign key'] 
  
  
  
 }))
  
 149",NA
Use Cookie-Session Sparingly,"Note that cookies need to be sent with 
 every
  client request (if path matches) in the HTTP header so large cookies have 
 a performance impact. There are also limitations on how much information browsers will let you store in cookies (a 
 general guidance for most browsers is 4093 bytes and a maximum of 20 cookies per site). Consequently, it is not 
 feasible to have all the user information become a part of the cookie.
  
 A more powerful session-management strategy is to use a database to store user session information instead of 
 using cookies. In that scenario, you would only store a token in the user cookie and that token would point to session 
 information we can read from our database at the server. You can use the express-session middleware (npm install 
 express-session) with a database (like Mongo or Redis) for that purpose. We will see an example in Chapter 8.",NA
Compression,"Zip compression over the wire is extremely easy to enable in Express- and Connect-based applications, thanks to the 
 compression (npm install compression) middleware. Listing 7-16 is a simple server that compresses pages greater 
 than 1kb before sending them to the client.
  
 Listing 7-16. 
  compression/compress.js
  
 var express = require('express'); 
  
 var compression = require('compression');
  
 var app = express()
  
  .use(compression())
  
  .use(express.static(__dirname + '/public'))
  
  .listen(3000);
  
 150",NA
Time-out Hanging Requests,"It is possible to get in a situation where some middleware fails to end the request 
 and
  fails to call next. For example, 
 this can happen if your database server is down and your middleware is waiting for a response from the database 
 server. In such a case, the client HTTP request will be left hanging, taking up server memory. In these circumstances, 
 you should time-out the client request instead of leaving it hanging.
  
  
 This is exactly what the connect-timeout (npm install connect-timeout) middleware is for, as Listing 7-18 
 demonstrates.
  
 Listing 7-18. 
 timeout/basic.js
  
 var express = require('express'); 
  
 var timeout = require('connect-timeout');
  
 var app = express()
  
  
  .use('/api', timeout(5000),
  
  
  
  function (req, res, next) { 
  
  
  
  
 // simulate a hanging request by doing nothing
  
  
  })
  
  
  .listen(3000);
  
 If you start this webserver and visit http://localhost:3000/api, the request will hang for five seconds, after 
 which the connect-timeout middleware kicks in and terminates the request, sending the client a 503 Service 
 Unavailable HTTP response.
  
  
 You can customize the response of a timeout by adding an error handling middleware and checking if a timeout 
 occurred by checking the req.timedout property, as shown in Listing 7-19.
  
 151",NA
Be Careful of the Sleeping Middleware,"When you use this middleware, you need to be mindful of the case when your hanging middleware suddenly awakens 
 and calls next (for example, a database request took longer than expected but did succeed eventually). In such a case, 
 you should check req.timedout and prevent the middleware from going further as the error handling response has 
 already been sent. This is demonstrated in Listing 7-20, which will crash on the first request.
  
 Listing 7-20. 
 timeout/propogateError.js
  
 var express = require('express'); 
  
 var timeout = require('connect-timeout');
  
 var app = express()
  
  
  .use(timeout(5000))
  
  
  .use(function (req, res, next) {
  
  
  
  // simulate database action that takes 6s
  
  
  
  setTimeout(function () { 
  
  
  
   
 next();
  
  
  
  }, 6000)
  
  
  })
  
  
  .use(function (req, res, next) { 
  
  
  
  
 res.end('Done'); // ERROR request already terminated
  
  })
  
  
  .listen(3000);
  
 152",NA
Express Response Object,"Express response derives from the standard Node.js server response object we saw in the previous chapter. It 
 also adds quite a few useful utility functions that make your web development experience more fun. In fact, we 
 have already seen the res.cookie/res.clearCookie functions, which were provided by Express.
  
  
 The response has a function res.status, which has the same effect as setting the res.statusCode except that it is 
 chainable. For example:
  
 res.status(200).end('Hello world!');
  
  
 To set individual or 
 multiple
  response headers at once, instead of the good old res.setHeader we saw earlier, you 
 can use the res.set function, as shown in Listing 7-22.
  
 Listing 7-22. 
 Using the set Method
  
 res.set('Content-Type', 'text/plain');
  
 res.set({
  
  'Content-Type': 'text/plain',
  
  'Content-Length': '123',
  
  'ETag': '12345' 
  
 })
  
 153",NA
Simplifying Send,"There is one extremely useful function that, once you learn, you can’t stop using. Its res.send([body|status], 
 [body]). This is the function that you should use whenever you want to send a non-streaming response. This 
 greatly simplifies a common pattern we have been using up to this point of stating a status and sending a body, 
 as demonstrated in Listing 7-24.
  
 Listing 7-24. 
 Send Saves Lines, Lines Save Kittens
  
 // instead of 
  
 res.statusCode = 404; 
  
 res.end('These are not the droids you are looking for');
  
 // you can do 
  
 res.send(404, 'These are not the droids you are looking for');
  
  
 It also allows you to send JavaScript objects as JSON in one go. If you pass in a JavaScript object as the body, it 
 also sets the content-type header to be application/json for you:
  
 res.send({ some: 'json' });
  
 154",NA
Express Request Object,"Similar to the response, the Express request object derives from the Node.js request object we saw in Chapter 6. 
 Express adds a few nice features that we will explore in this section.
  
  
 Express simplifies your access to the request headers (req.headers as we saw in the previous chapter) with a 
 req.get function, which allows case in-sensitive lookup, as shown in Listing 7-25.
  
 Listing 7-25. 
 Demonstrating the Get Method
  
 req.get('Content-Type'); 
 // 
 ""text/plain"" 
  
 req.get('content-type'); 
 // 
 ""text/plain"" 
  
 req.get('not-present');  // undefined
  
  
 If all you want to do is look up the content-type of the request, you can use the utility req.is(type) function as 
 shown in Listing 7-26, which even does a mime-type inspection for you.
  
 Listing 7-26. 
 Using the Is Method
  
 // When Content-Type is application/json 
  
 req.is('json');             // true 
  
 req.is('application/json'); // true 
  
 req.is('application/*');    // true 
  
 req.is('html');             // false
  
 You can get the client IP address using the req.ip property.
  
  
 To check if the request came over HTTPS, you can use the req.secure flag, which is true if the request was over 
 HTTPS, and false otherwise.",NA
URL Handling,"Express parses the query parameters from the URL into the req.query JavaScript object. Query parameters are great 
 for when you want to return search results. Listing 7-27 provides a sample of how the URL query section gets 
 parsed into a JavaScript object.
  
 Listing 7-27. 
 Demo to Show the Built-in Query Parsing
  
 // GET /shoes?order=desc&shoe[color]=blue&shoe[type]=converse 
 req.query.order      // ""desc"" 
  
 req.query.shoe.color // ""blue"" 
  
 req.query.shoe.type  // ""converse""
  
 If you just want the path section of the URL (that is, the section before the query), it can be found in req.path:
  
 // GET /users?sort=desc 
  
 req.path // ""/users""
  
 155",NA
Making Request and Response Cross Visible,"Express also assigns the response object to req.res and the request object to res.req. This makes it possible for you to 
 pass only one of them around (request or response) and, when debugging, get access to the corresponding request or 
 the corresponding response object.",NA
Understanding REST,"REST (
 Representational State Transfer
 ) is a term coined by Roy Fielding (one of the principal authors of the HTTP 
 specification) as a general architectural style, specifying constraints on how connected components in a distributed 
 Hypermedia system should behave. Web APIs that adhere to these constraints are called RESTful.
  
 In REST, there are two broad kinds of URLs. URLs that point to collections (such as http://example.com/ 
 resources), and URLs that point to an individual item in the collection (such as http://example.com/resources/ 
 item5identifier). In order to be RESTful, you need to adhere to the behavior as shown in Table 
 7-1
  for collection 
 and Table 
 7-2
  for item URLs, based on the kind of URL and the HTTP method used by the client. Also notice the 
 relationship between the collection URL and the item in the collection URL (which is the collection URL + item 
 identifier in the collection).
  
 Table 7-1.
  RESTful API HTTP Method Behavior for 
 collection
  URLs
  
 HTTP method
  
 Behavior
  
 GET
  
 Get
  the summarized details of the members of the collection, including their unique 
  
 identifiers.
  
 PUT 
  
 Replace
  the entire collection with a new collection.
  
 POST 
  
 Add
  a new 
 item
  in the collection. It is common to return a unique identifier for the 
 created 
  
 resource.
  
 DELETE
  
 Delete
  the entire collection
  
 156",NA
Express Application Routes ,"Because of the importance of HTTP verbs when making good web APIs, Express provides first-class verb + URL based 
 routing support.
  
 Let’s start with the basics. You can call app.get / app.put / app.post /app.delete—in other words, 
 app.VERB(path, [callback...], callback)—to register a middleware chain that is only called when the path + HTTP 
 verb in the client request matches. Also you can call app.all to register a middleware that is called whenever the 
 path matches (irrespective of the HTTP verb). Listing 7-29 is a simple demo that drives this point home.
  
 Listing 7-29. 
 approute/1verbs.js
  
 var express = require('express');
  
 var app = express(); 
  
 app.all('/', function (req, res, next) {
  
  
  res.write('all\n'); 
  
  
  
 next(); 
  
 }); 
  
 app.get('/', function (req, res, next) { 
  
  
  
 res.end('get'); 
  
 }); 
  
 app.put('/', function (req, res, next) {
  
  
  res.end('put'); 
  
 }); 
  
 app.post('/', function (req, res, next) {
  
  
  res.end('post'); 
  
 }); 
  
 app.delete('/', function (req, res, next) {
  
  
  res.end('delete'); 
  
 }); 
  
 app.listen(3000);
  
 All of these methods form a standard middleware chain where order + calling next matters. If you run this server 
 you will note that the .all middleware is always called followed by the relevant verb middleware. We can test it by 
 using curl and specifying the request (-X) verb to use, as shown in Listing 7-30.
  
 157",NA
Creating a Route Object,"Now specifying the path in each of these routes can be cumbersome (and prone to spelling mistakes). Therefore, 
 Express has a nice little app.route member function to specify the prefix only once, which returns a route object that 
 has the same all/get/put/post/delete functions. All of these are demonstrated in the example in Listing 7-31. The 
 output is exactly the same as we saw in the previous example (Listing 7-20).
  
 Listing 7-31. 
 approute/2route.js
  
 var express = require('express');
  
 var app = express(); 
  
 app.route('/') 
  
  
  
 .all(function (req, res, next) {
  
  
   
  res.write('all\n'); 
  
  
   
  
 next();
  
  
  }) 
  
  
  
 .get(function (req, res, next) {
  
  
   
  res.end('get');
  
  
  })
  
  
  .put(function (req, res, next) {
  
  
   
  res.end('put');
  
  
  })
  
  
  .post(function (req, res, next) {
  
  
   
  res.end('post');
  
  
  })
  
  
  .delete(function (req, res, next) {
  
  
   
  res.end('delete');
  
  
  }); 
  
 app.listen(3000);",NA
A Deeper Look at the Path Option,"Unlike the app.use function, which takes a 
 path
 prefix
 , the verb-based routing in ExpressJS matches the 
 exact path
  
 (not 
 exact URL
  because the query string part is ignored). If you want to match a path prefix you can use the * 
 placeholder to match anything after the prefix. You can also set up a route based on a regular expression. All of these 
 options are demonstrated in Listing 7-32.
  
 158",NA
Parameter-Based Routing,"Instead of putting too much filtering logic into your path prefix matching, a much nicer option is to use path 
 parameters. You can specify path parameters using the :parameterName syntax. For example `/user/:userId` will 
 match `/user/123` and populate the userId request parameter for you. Express puts all the parameter values in 
 req.params object. Listing 7-34 demonstrates its usage (along with a sample run shown in Listing 7-35).
  
 Listing 7-34. 
 approute/4param.js
  
 var express = require('express');
  
 var app = express(); 
  
 app.get('/user/:userId', function (req, res) {
  
  
 res.send('userId is: ' + req.params['userId']); }); 
  
 app.listen(3000); 
  
 Listing 7-35. 
 testing approute/4param.js using curl
  
 $ curl http://127.0.0.1:3000/user/123 
  
 userId is: 123
  
 159",NA
Express Router Object,"An Express router is an isolated instance of middleware + routes. It can be thought of as a “mini” Express 
 application. You can create a router object quite easily using express.Router() function.
  
  
 At its root level, it has the use, all, get, post, put, delete, param, and route functions, which behave exactly the 
 same as their Express app counterparts that we have already seen.
  
 In addition to all of this, the router object behaves like any other middleware. That is, once you set up a router 
 object, you can register it with an Express using the app.use function. And obviously you can mount it at a specify 
 mount point by passing in a first argument to the app.use function, as we have already seen before.
  
  
 To demonstrate the power of this pattern, let’s create a simple Router, which follows the principles of REST, 
 to create a web API for managing an in-memory collection of arbitrary objects, as shown in Listing 7-37.
  
 Listing 7-37. 
 router/basic.js
  
 var express = require('express'); 
  
 var bodyParser = require('body-parser');
  
 // An in memory collection of items 
  
 var items = [];
  
 // Create a router 
  
 var router = express.Router(); 
  
 router.use(bodyParser());
  
 160",NA
Additional Resources,"Roy Fielding’s original mention of REST as a part of his dissertation can be found at 
 www.ics.uci.edu/~fielding/ 
 pubs/dissertation/rest_arch_style.htm
 .",NA
Summary,"In this chapter, we dug deeper into various HTTP concepts. We started by showing popular middleware for a variety 
 of web-related tasks. This includes serving static web pages to working with cookies.
  
  
 We demonstrated that Express offers the same middleware framework as Connect. Then we dug into a few nice 
 value-added features provided by Express on top of the standard request and response objects.
  
  
 Finally, we covered the principles of REST. We showed how the Express framework embraces the web and 
 makes creating maintainable RESTful web APIs a breeze.
  
  
 We didn’t cover the view rendering side of Express, which will be covered in Chapter 9, when we discuss 
 front-end design. But, first, let’s persist (pun on next chapter).
  
 163",NA
Chapter 8,NA,NA
Persisting Data,"Data persistence is an important part of any real-world application. In this chapter, we will give a solid data-
 persistence strategy for the beginner. We will introduce MongoDB as well as associated concepts such as 
 NoSQL, ORMs, and ODM.",NA
Introduction to NoSQL ,"NoSQL (Not Only SQL) is a term used to encompass the general trend of a new generation of database servers. 
 These servers were created in response to challenges that were not being met by traditional SQL (Structured Query 
 Language) relational database servers (for example, Oracle Database, Microsoft SQL Server, and MySQL). These 
 servers can be placed into four broad categories:
  
 
  
 Document databases (for example, MongoDB)
  
 
  
 Key-value databases (for example, Redis)
  
 
  
 Column-family databases (for example, Cassandra)
  
 
  
 Graph databases (for example, Neo4J)
  
 Scalability
  is a key motivation common to all of these. For most purposes, a document database provides the 
 largest feature set with an acceptable/scalable performance. For simple cases where you do not want 
 complicated query requirements, key-value databases provide the best performance.",NA
What Is a Document Database? ,"A document database is one that works based on the concept of documents. What is a document? A 
 document
  is a 
 self-contained
  piece of information for a particular entity. Listing 8.1 gives a possible JSON document.
  
 Listing 8-1. 
 Sample JSON Document
  
 {
  
  ""firstName"": ""John"",
  
  ""lastName"": ""Smith"",
  
  ""isAlive"": true,
  
  ""age"": 25,
  
  ""height_cm"": 167.64,
  
 165",NA
What Is a Key-Value Store?,"A key-value store is effectively a stripped-down version of a document database. The key is a unique ID to identify a 
 document, and the values are the actual documents. What separates a key-value store from a document database is 
 the query ability of the data. In most key-value stores, you can only query the key. In a document database, you can 
 query by Document 
 contents
  as well. This gives key-value stores an opportunity to optimize for faster key-based 
 lookup, and they can use a more compressed storage for the values.
  
  
 Redis is an excellent example of a key-value store. It actually keeps the entire database in RAM, with background 
 backups to disk, for lightning fast runtime performance.",NA
Why NoSQL?,"There are two reasons to use document databases and key-value stores:
  
 
  
 Scalability
  
 
  
 Ease of development",NA
Scalability Issues with a Relational Design,"Before we answer what makes relational databases difficult to scale, let’s define a few terms:
  
 
  
 Availability:
  Is the data accessible? That is, can the users read and act on the data.
  
 
  
 Consistency:
  Is there only a single source of truth? If all your data is recorded only once 
 at 
  
 only a single place, then it is consistent for the purpose of this discussion. 
  
 On a single machine, 
 availability
  and 
 consistency
  are strongly tied to each other. If the data is available, it is 
 consistent. A single server serving the data with a simple backup server is sufficient for an average enterprise. 
 Relational servers were born in these situations and didn’t have a problem handling the load.
  
 However, on the Web, no single machine can handle all the workload of every single client request. Additionally, 
 you might want to 
 partition
  your servers to be closer to the client’s geo-location (for example, American clients vs. 
 Australian clients). To have scalability, you need to partition your data across machines. These partitions need to 
 communicate with each other in order to maintain a 
 consistent
  view of the data. Now let’s introduce a third term:
  
 
  
 Partition tolerance:
  The system continues to operate in the face of a communication 
  
 disruption between the partitions.
  
 Consider the case where we have replicated the data over a network partition: one server in America and 
 another in Australia. When the network between the two is down (communication disruption) and a user in Australia 
 asks to update the data, do we allow it (favor 
 availability
 ), or do we deny the request to maintain our 
 consistency
 ?
  
 166",NA
Ease of Development ,"Object-relational mapping is the Vietnam of computer science.
  
 —Ted Neward
  
 Relational databases operate in terms of relations and tables. To use or manipulate the data in a relational 
 database from our applications, we need to convert these “tables” into “objects” and vice versa. This is the process 
 of Object Relational Mapping (ORM). ORM can be a tedious process and can be completely avoided by using a 
 document database. For our example for the person document, it’s a simple matter of JSON.parse. Of course, 
 document databases provide you with an API to get JavaScript objects returned from the database, so you don’t 
 have to do any parsing and don’t have to mess with an ORM.
  
 There are real-world reasons to consider a relational database over a NoSQL database, such as complex 
 queries and data analysis. These things can, however, be done using a relational database in parallel with a NoSQL 
 database serving the main data requirements of your web site.
  
 By now, we’ve hopefully convinced you that there are real technical advantages to document databases on the 
 Web. This means you can enjoy the rest of this chapter more and put in the effort to consider non-relational options in 
 your projects.",NA
Installing MongoDB,"The name MongoDB comes from hu
 mongo
 us. Installing MongoDB is extremely simple. It ships as a simple zip file 
 containing a few binaries that you can extract anywhere on your filesystem. Zipped binaries are available for 
 Windows as well as Mac OS X and Linux systems. It is recommended that you use the 64-bit version of MongoDB as 
 well as a 64-bit OS in production. This allows MongoDB to use all  the memory address space offered by a 64-bit OS. 
 The zipped files are available at 
 www.mongodb.org/downloads
 .
  
 167",NA
Understanding the Binaries,"After you download the zip file, extract it into any folder in your filesystem that has enough space. Everything you 
 need is contained in the bin folder inside the zip file you downloaded for your OS. MongoDB is self-contained and 
 doesn’t care about its location on the filesystem as long as it has write access. Its ease of installation is one of the 
 reasons for its popularity.
  
 The bin folder contains quite a few binary files. The windows binaries have a .exe extension, but the same ones 
 exist for Mac OS X (for example, mongod.exe for windows vs. mongod for Mac OS X). The most important binaries are 
 the following:
  
 
  
 mongod.exe: This is the 
 Mongo
 DB 
 D
 aemon—that is, the main server binary. This is what you 
  
 will execute to start your database server.
  
 
  
 mongo.exe: This is a utility REPL provided with the server that can be used for various 
  
 administrative and code exploration tasks.
  
 Other binaries exist in the bin folder for importing/exporting data, gathering system statistics, and managing 
 other features provided by MongoDB such as sharding and MongoDB’s distributed file system (called GridFS). All of 
 these can be ignored when getting started.
  
  
 To make it easier to follow along, after extracting the zip file, it is best to just put the bin folder into your 
 system path. This will make mongod as well as mongo available from any directory in your command 
 prompt/terminal.",NA
Running Your First Server,"MongoDB requires a data directory (called the dbpath in MongoDB literature) to store all the database data. The 
 default dbpath is /data/db (or \data\db in Windows, based on the drive of your current working directory such 
 as C:\data\db). It is always best to just specify an explicit dbpath.
  
 Start by making a db folder that will contain all your MongoDB database data at a place where you have write 
 access:
  
 $ mkdir db
  
 Now you can start the MongoDB server as shown in Listing 8-2. If everything goes well, you should see the 
 waiting for connections message. Once the server starts, leave the terminal open to keep the server running.
  
 Listing 8-2. 
 Start MongoDB Server with a Specified Database Directory
  
 $ mongod --dbpath=""./db"" 
  
 Sun Jun 15 17:05:56.761 [initandlisten] MongoDB starting : pid=6076 port=27017 
 dbpath=./db 64- ...truncated...
  
 Sun Jun 15 17:05:57.051 [initandlisten] waiting for connections on port 27017 
  
 Sun Jun 15 17:05:57.051 [websvr] admin web console waiting for connections on port 28017",NA
The MongoDB REPL,"The mongo executable is an interactive JavaScript shell interface to MongoDB. You can use it for system 
 administration as well as for testing queries and operations against the database. As soon as you start it, it tries to 
 connect to localhost on the default port (27017).
  
 The mongo REPL provides access to a few global variables and functions that we can use to interact with the 
 database. The most important is the db variable, which is your handle to the current database connection. Once the 
 REPL is started, you can exit at any time by typing in exit. You can get the help of the options available by typing in 
 help. Also, many functions that we will use in the REPL have a 'help' member. Let’s spin up the mongo REPL and see 
 what options we have available on db. (See Listing 8-3.)",NA
Important MongoDB Concepts,"A MongoDB deployment consists of multiple databases
 .
  Each database can contain multiple collections. Each 
 collection can contain multiple documents
 .
  Hence, it is a hierarchy from database to collections to documents.
  
 A document is effectively a JSON document that we are already familiar with, plus a few niceties. For example, 
 these documents have first-class support for the Date data type (we say in Chapter 4 that JSON spec doesn’t allow 
 Date as a valid value and must be serialized to a string). An example document would be the “person document” 
 representing an individual.
  
 A 
 collection
  is simply a name that you give to a 
 collection of documents
 . Storing multiple documents into the same 
 collection 
 doesn't
  enforce any concept of 
 schema
  on the documents. It is up to you to be disciplined about your 
 document semantics. This schema-less-ness makes it possible to do partial upgrades of documents to new schemas in 
 an agile fashion.
  
  
 Finally, a single MongoDB 
 server
  can contain multiple databases
 ,
  allowing you to do a logical separation of a 
 set 
 of collections
  in your server. A common use case for using multiple databases is multi-tenancy applications. 
  
 A multi-tenant application is one that has multiple customers. For each customer, you can have different databases 
 with the same collection names inside each database. This allows you to efficiently use your server resources while 
 still allowing easier development (same collection names) and maintainability (same backup procedures for different 
 databases).
  
  
 Now that we understand documents, collections, and databases, let’s explore them in the mongo REPL. You can 
 specify the database the REPL will use with the ‘use’ command (for example, use demo will use the demo database). 
  
 If a demo database doesn’t exist, it will be created for you. The variable db refers to the currently active database. 
 To use a collection in the current database, you simply access it using the collection name property on db (for 
 example, db.people will look at the people collection in the current database). Next, you manage individual 
 documents using the collection API. Listing 8-4 provides a simple example where we insert a single person into the 
 people collection, followed by querying all the people from the collection.
  
 169",NA
MongoDB _id field,"Every document in MongoDB must have a “_id” field. You are allowed to use any value for _id as long as it is unique in 
 the collection. By default, MongoDB (the provided client driver or the server) will create an ObjectId for you.",NA
Why Not Use a Natural Primary Key?,"The fundamental rule of database development is that the primary key 
 must never change
 . For some data models, you 
 might be tempted to find a 
 natural primary key
  (in other words, something that is unique to the entity in nature and 
 does not change over its lifetime.) For example, for a person, you might be tempted to consider the Social Security 
 Number (SSN). But you will be surprised to find how common seemingly 
 naturalprimary keys
  change. For example, in 
 the United States, you are legally allowed to request a new SSN if, for example, your life is in danger. Similarly, 
 consider the case of the ISBN. If you change the title of the book (seemingly natural thing to request), you will get a 
 new ISBN.
  
 In most cases, you will want to use a surrogate primary key. A surrogate primary key is a key that has no natural 
 meaning to the entity but is used to uniquely identify the entity in the database. The MongoDB generated ObjectId is 
 one such good-quality surrogate key
 .",NA
More about the ObjectId,"Now that we have hopefully convinced you that it is better to use a generated primary key, the question is why an 
 ObjectId? Why not an auto-incrementing number? Because an auto-incrementing number does not scale; it is 
 difficult to manage in a distributed environment as the next number requires knowledge of the last used number. 
  
 The ObjectId that MongoDB generates uses 12 bytes of storage. This means it takes 24 hexadecimal digits (2 digits 
 per byte) to represent it in a string, as we saw earlier (for example, “539ed1d9f7da431c00026e17”). The 12 bytes of 
 the MongoDB ObjectId are generated as shown in Figure 
 8-1
 .
  
  
 Figure 8-1.
  Byte structure of the MongoDB ObjectId
  
 170",NA
MongoDB Document Format,"A MongoDB document is stored internally using BSON (
 B
 inary J
 SON
 ). This is the format that the MongoDB client 
 drivers use on the network as well. BSON is effectively a binary way for storing JSON documents.
  
 One key feature offered by BSON is a length prefix. In other words, each value in a document is prefixed by the 
 length of the value. This makes it easier for someone reading the document to skip a field if it is not relevant to the 
 current client request. In JSON, you would need to read the whole field to get to the closing indicator (“}” or “ , ” or “]”) 
 even though you want to skip a field.
  
  
 A BSON document also contains information about the type of the field value, such as the value of a number, a 
 string, or a Boolean. This aids in parsing as well as performing storage optimizations.
  
  
 Additionally, BSON provides for additional primitive types not supported by raw JSON such as a UTC Datetime, 
 raw binary, and ObjectId.",NA
MongoDB Using Node.js,"Now that we have covered the basics of MongoDB, let’s look at using it from our Node.js applications.
  
 The MongoDB team maintains an official Node.js package (npm install mongodb) for communicating to a 
 MongoDB server from Node.js. All the async API provided by MongoDB follows the Node.js convention of the first 
 argument being an Error followed by the actual data if any. Your main connection to a MongoDB server is using the 
 MongoClient class exported from the NPM package. Listing 8-6 is a demo that inserts an object, queries it, and then 
 deletes it.
  
 171",NA
Update a Document,"The simplest way to update a document is to call the save function of a collection as shown in Listing 8-8.
  
 Listing 8-8. 
 update/1save.js
  
 var MongoClient = require('mongodb').MongoClient;
  
 var demoPerson = { name: 'John', lastName: 'Smith' }; var 
 findKey = { name: 'John' };
  
 MongoClient.connect('mongodb://127.0.0.1:27017/demo', function (err, db) {
  
  if 
 (err) throw err;
  
  var collection = db.collection('people');
  
  collection.insert(demoPerson, function (err, docs) {
  
 demoPerson.lastName = 'Martin'; 
  
 collection.save(demoPerson, function (err) {
  
  
  console.log('Updated');
  
  
  collection.find(findKey).toArray(function (err, results) {
  
  
  console.log(results);
  
  
  
  
  
  // cleanup
  
  
  
  
  
  collection.drop(function () { db.close() });
  
  
  
  });
  
  
  
  });
  
  
  }); 
  
 });
  
 You simply update the object and pass it back in to the database. The database looks up the object by _id and sets 
 the new values as specified. The save function replaces the 
 entire
  document. However, most of the time you will not 
 want to replace the 
 entire
  document with a new version. That would be really bad in a distributed, data-intensive 
 environment. Many people might want to modify different fields of the document at about the same time. This is 
 where the collection.update method and the update operators come in.",NA
Update Operators,"The collection’s update function takes three arguments, an object to match/find the item you want to modify, a 
 second argument that specifies the update operator + property we want to modify in the document, and a final 
 argument which is the callback called once the update has completed.
  
 Let’s consider the simple case of a web-site hit counter. Many users might be visiting the same web site at the 
 same time. If we were to read the counter value from the server, increment it on the client, and then send the server 
 the new value, by the time we send it, the original read value might already be out-of-date. Traditionally, database 
 clients would request the database to lock the document, send the value down over the network, receive an 
 updated value, and then request to unlock the document. That would be very slow as network communication takes 
 time.
  
 This is where the update operators come into play. We simply instruct MongoDB to increment whatever the 
 current view count is for a particular document in a 
 single
  client request using the $inc update operator. Once 
 MongoDB receives the request, it locks the document, reads + increments the value, and unlocks the document all 
 on the server. This means that the database server can handle requests almost as fast as it can receive them over 
 the network and no client request needs to wait or retry because of a pending client request. Listing 8-9 is a simple 
 example that demonstrates this.
  
 173",NA
Mongoose ODM,"As we have seen, MongoDB works with what are effectively simple JSON documents. This means that the business 
 logic that operates on the documents (functions/methods) must live elsewhere. Using an Object Document Mapper 
 (ODM) we can map these simple documents into full-form JavaScript objects (with data + methods for validation and 
 other business logic). The most popular (and supported by MongoDB team) is the Mongoose ODM (npm install 
 mongoose).",NA
Connecting to MongoDB ,"You can connect to MongoDB using Mongoose in a similar fashion to the native driver that we saw earlier. Listing 8-
 10 is a simple example straight from the docs.
  
 Listing 8-10. 
 odm/connection.js
  
 var mongoose = require('mongoose');
  
 mongoose.connect('mongodb://127.0.0.1:27017/demo');
  
 var db = mongoose.connection; 
  
 db.on('error', function (err) { throw err }); 
  
 db.once('open', function callback() {
  
  
  console.log('connected!'); 
  
  
  
 db.close(); 
  
 });
  
 We connect using the connect member function. Then we get access to the database object using mongoose. 
 connection and wait for the open event to fire, indicating a successful connection. Finally, we close the connection to 
 exit the application.",NA
Mongoose Schema and Model,"At the heart of Mongoose is the Schema class. The schema defines all the fields of your document along with their 
 types (for validation purposes) and their behavior during serializing etc.
  
 After you have defined your Schema, you compile it to create a Model function, which is simply a fancy 
 constructor for converting simple Object Literals into JavaScript Objects. These JavaScript objects have the behavior 
 you set up using the Schema. In addition to the ability to create these domain objects, models have static member 
 functions you use to create, query, update, and delete documents from the database.
  
  
 As an example, consider a simple tank Schema. A tank has a name and a size (small, medium large). We can 
 define a tank Schema quite simply:
  
 var tankSchema = new mongoose.Schema({ name: 'string', size: 'string' }); 
  
 tankSchema.methods.print = function () { console.log('I am', this.name, 'the', this.size); };
  
 We also define methods available on the Model instances using the Schema (for example, we specified the print 
 method). Now that we have a Schema, let’s make a Model. The Model links the Schema to a database collection and 
 allows you to manage (CRUD) model instances. Creating a Model from a Schema is really simple:
  
 // Compile it into a model 
  
 var Tank = mongoose.model('Tank', tankSchema);
  
 175",NA
Using a MongoDB as a Distributed Session Store,"In Chapter 7, we saw how we can use cookies to store user session information by using the cookie-session 
 middleware. However, we pointed out that using a cookie for storing 
 all
  your session information is a bad idea since 
 the cookie needs to come from the client with each request and you are limited by the size of a cookie.
  
 Ideally, you should aim for your web applications to be as stateless as possible. However, for certain kinds of 
 applications, large bits of information in user sessions might be what you need. This is where the express-session 
 middleware (npm install express-session) comes in.
  
 By default, express-session middleware will use an in-memory store to maintain the user session information. 
 The cookie sent to the client will only point to the key inside this server memory store. Consider the following server 
 in Listing 8-15, which is based on the cookie-session server we saw in Chapter 7. All that we have done is replace the 
 cookie-session middleware with the express-session middleware.
  
 Listing 8-15. 
 session/inmemory.js
  
 var express = require('express'); 
  
 var expressSession = require('express-session');
  
 var app = express()
  
  
  .use(expressSession({ 
  
  
   
 secret: 'my super secret sign key' 
  
  
  
 }))
  
 177",NA
Managing MongoDB ,"When you are starting out with MongoDB, it can be intimidating to work with just a REPL like mongo. Eventually, 
 it’s useful for quick lookups, but for the beginner better GUI tools can be a lifesaver. For managing a production 
 server, the MongoDB team itself offers a hosted MongoDB Management Service (MMS).
  
 For development time, we would like to recommend the Robomongo desktop application (
 http://robomongo. 
 org/
 ). It’s an open source application, under active development, and they provide installers for Windows (.msi) as 
 well as Mac OS X (.dmg).
  
 Once installed, you simply start the software, connect to a MongoDB server, and you can see the databases, 
 collections, and documents as shown in Figure 
 8-2
 . One great thing about the application is that it integrates 
 the mongo shell into its GUI so all your terminal skills are relevant here as well.
  
  
 Figure 8-2.
  Screenshot of Robomongo with the important sections highlighted
  
 179",NA
Additional Resources,"Multi-tenancy in MongoDB: 
 http://support.mongohq.com/use-cases/multi-tenant.html 
 BSON 
 language specification: 
 http://bsonspec.org/ 
  
 MongoDB ObjectId: 
 http://api.mongodb.org/java/current/org/bson/types/ObjectId.html 
 MongoDB Update operators: 
 http://docs.mongodb.org/manual/reference/operator/update/ 
 Mongoose ODM: 
 http://mongoosejs.com/ 
  
 MongoDB management service: 
 www.mongodb.com/mongodb-management-service",NA
Summary,"In this chapter, we looked at the motivation for using a document database. Then we examined MongoDB, explained 
 the important concepts, and demonstrated the supported ways of utilizing MongoDB from our Node.js applications.
  
  
 There is lot more that can be said about MongoDB, Mongoose, and Querying/Indexing, but what we have 
 covered here should be enough for you to be able to comfortably explore the API on your own.
  
 180",NA
Chapter 9,NA,NA
Front-End Basics,"In the previous chapters, we have discussed how you can create web servers, web services, and APIs. We have also 
 shown how to persist your data in a database. Now we will explore the front end. In this chapter, we will delve into 
 the concept of Single Page Applications (SPA) and create one using AngularJS. Our front end will communicate with a 
 simple Express web service that will store our data in a MongoDB database.
  
  
 As usual, before we embark on this journey, we will explain all the important concepts in this area and provide 
 justifications for our technology choices so that you have a deep appreciation of the underlying principles.",NA
What Is a SPA?,"In a SPA, all the essential code of your application (the HTML/CSS/JavaScript) is loaded upfront in the first request to 
 the web server. A common example given for a SPA is the Gmail (
 www.gmail.com
 ) web site from Google. 
  
  
 As you navigate from page to page in a traditional web site, the complete page is reloaded, as illustrated in Figure 
 9-1
 .
  
  
 Figure 9-1.
  Traditional web-site experience
  
 181",NA
Why AngularJS?,"There are lots of good-quality, community-driven, single-page application frameworks out there such as AngularJS, 
 EmberJS, ReactJS, KnockoutJS, and BackboneJS, but by far the greatest community interest has been around 
 AngularJS, which was created at Google. This is visible by the Google search trend for these frameworks shown in 
 Figure 
 9-3
 .
  
  
 Figure 9-3.
  Google search trends for various SPA/data-binding frameworks
  
 The main reason for its popularity is the fact that it is from a team at Google, it is simple, and it has feature 
 richness:
  
 
  
 Data-Binding/Templating: allows you to update the DOM (the rendered HTML) based on 
  
 changes to the underlying JavaScript objects
  
 
  
 URL Handling/Routing: handles the browsers address bar to give a fluid navigation 
 experience 
  
 to the user by loading and rendering templates as required
  
 
  
 Dependency Injection: gives you clear guidance on organizing your client-side JavaScript for 
  
 enhanced workflow in teams and easing testability",NA
Introduction to Twitter Bootstrap,"We will be using Twitter Bootstrap to design/style the front end of our application. HTML/CSS gives you the 
 foundation to design a UI. You can design anything you want from scratch, but again you are probably better off using 
 stuff that other people have already created for you. Twitter Bootstrap is from a team of designers at 
 twitter.com
 . You 
 can download Bootstrap from 
 http://getbootstrap.com/
 .
  
  
 At its core, Bootstrap is basically a bunch of CSS classes that allow you to quickly customize the way the rendered 
 HTML looks. For example, consider a simple HTML page with a single button:
  
 <button>I am a button</button>
  
 By default, it looks like the following on Windows:
  
  
  
 If you add a reference to the Bootstrap CSS file in your HTML, you get access to a number of CSS classes to style 
 your button. That is exactly what we do in Listing 9-1.
  
 Listing 9-1. 
 bs/bs.html
  
 <!-- Add Bootstrap CSS --> 
  
 <link rel=""stylesheet"" type=""text/css"" href=""./bootstrap/css/bootstrap.css"">
  
 <!-- Use Bootstrap CSS classes --> 
  
 <button class=""btn btn-default"">Default style button</button> <button 
 class=""btn btn-primary"">Primary style button</button> <button 
 class=""btn btn-success"">Success style button</button> <button 
 class=""btn btn-danger"">Danger style button</button>
  
 You get nicely styled consistent buttons on every platform:
  
  
 Bootstrap also ships with a few JavaScript components that provide advanced user interactivity. Bootstrap 
 JavaScript depends upon JQuery (
 http://jquery.com/download/
 ) to provide a consistent DOM manipulation API. In 
 order to use the JavaScript components, we need to include JQuery and the Bootstrap JavaScript file. Listing 9-2 
 shows how you can use Bootstrap tool tips.
  
 Listing 9-2. 
 bs/bsjs.html
  
 <!-- Add JQuery + Bootstrap JS + CSS--> 
  
 <script src=""./jquery/jquery.js""></script> 
  
 <script src=""./bootstrap/js/bootstrap.js""></script> 
  
 <link rel=""stylesheet"" type=""text/css"" href=""./bootstrap/css/bootstrap.css"">
  
 <!-- Use a button with a nice tooltip shown at the bottom --> 
  
 <button class=""btn btn-default"" 
  
  
  
 data-toggle=""tooltip"" data-placement=""bottom"" title=""Nice little tooltip message"">
  
  Hover over 
 me to see the tooltip 
  
 </button>
  
 184",NA
Set Up a Simple AngularJS Application,"The first step to make our single-page app is to create an Express server to serve the client-side JavaScript HTML 
 and CSS, as shown in Listing 9-3. This is a trivial task to achieve with the knowledge we already have (Chapter 7).
  
 Listing 9-3. 
 angularstart/app.js
  
 var express = require('express');
  
 var app = express()
  
  .use(express.static(__dirname + '/public'))
  
  .listen(3000);
  
  
 This serves HTML from the public folder. Now we will create a vendor folder inside our public folder to contain 
 our JQuery, AngularJS, and Bootstrap files, as shown in Listing 9-4. Finally, we have a simple index.html file.",NA
Modules in AngularJS,"Modules in AngularJS allow you to contain and manage all the controllers, directives, and so on. You can associate a 
 particular module to manage a section of HTML using the ng-app directive (which is what we did). We also used the 
 module to register the MainController with Angular (demoApp.controller). Modules are simply a container for 
 easier management purposes.",NA
Directives in AngularJS,"Directives are basically segments of code that you want Angular to execute (to provide behavior) when Angular finds 
 a matching string in the HTML. For example, in our application we ask Angular to run the ng-app directive using the 
 ng-app HTML attribute on our html tag (<html ng-app=""demo"">). Similarly, we triggered the ng-controller and ng-
 model directives.
  
 It is conventional to namespace directives with a prefix. Directives that come with Angular use the ng- prefix. 
 making it easier to observe in the HTML that this tag has a directive on it. If a beginner sees ng-app, he or she gets a 
 clue that it’s an Angular directive and some custom behavior will be applied.
  
  
 It is extremely easy to create your own directives and an integral part of serious AngularJS application 
 development. But for now we will stick with the directives that come with Angular, which can carry you a long 
 way.
  
 187",NA
Controller and $scope,"Controllers are the heart and soul of AngularJS. These are one-half of the two-way data-binding. Everything else we 
 have seen (modules, directives) can be considered a journey to Controllers.
  
 Controllers are called controllers because of the Model-View-Controller (MVC) pattern. In the MVC pattern, the 
 Controller is responsible for keeping the View and the Model in sync. In Angular, this synchronization between the 
 View and the Model is done by Angular using two-way data-binding. The glue between the two (View and the Model) 
 is the Angular $scope, which gets passed to the Controller. The Controller sets up the $scope based on our application 
 logic. The synchronization of $scope between the view and the model is shown in Figure 
 9-5
 .
  
  
 Figure 9-5.
  Demonstration of $scope being the glue between View and Model
  
 Since this model in the Controller is really for the View, it is commonly called the 
 ViewModel
 , or vm for short, 
 as we have called it in our example. Also note that the $scope gets injected into the Controller by Angular. We 
 explicitly ask for the $scope by specifying it in the array members. The relevant segment shown again:
  
 demoApp.controller('MainController', ['$scope', function ($scope) {
  
 The initial array members (only '$scope' in this example) drive what gets passed as the arguments to the final 
 array member, which is our Controller function. This is one form of 
 dependency injection
  supported by Angular. 
 We will see more of Angular’s dependency injection in other examples in this chapter.
  
  
 We use the $scope from our HTML using directives. In our case, the following HTML’s ng-model keeps the input 
 element in sync with the vm.name property:
  
 <input type=""text"" ng-model=""vm.name"" />
  
 Similarly, we can call a function on our controller upon a user click using the ng-click directive:
  
 <button class=""btn btn-danger"" ng-click=""vm.clearName()"">Clear Name</button>",NA
Creating a Simple To-Do List Application,"The great thing about Angular is that it is extremely simple to create and design a front end entirely separate from 
 any server code. Then you can wire it up to the back end once it is ready, which is exactly what we will do here. Now 
 that we have a basic understanding of the fact that $scope is the glue between the View and the Model, we will come 
 up with a simple JavaScript model for a to-do list and design a front end for it. Our entire Model (vm) JavaScript is 
 shown in Listing 9-5.
  
 188",NA
Creating a REST API,"We already have experience with creating a REST API from Chapter 7 when we took a detailed look at ExpressJS. All 
 our REST API needs to do for our simple application is a GET for all the items in the list, a POST to add items to the list 
 (it should return the ID), and a DELETE to delete an item from the list. In Chapter 8, we saw how to use MongoDB. 
  
 Combining our knowledge of the two, Listing 9-9 provides a simple setup for our ExpressJS router-based API that 
 persists the data to MongoDB.
  
 Listing 9-9. 
 todocomplete/app.js
  
 var express = require('express'); 
  
 var bodyParser = require('body-parser');
  
 // The express app 
  
 var app = express();
  
 // Create a mongodb connection 
  
 // and only start express listening once the connection is okay 
  
 var MongoClient = require('mongodb').MongoClient; 
  
 var db, itemsCollection; 
  
 MongoClient.connect('mongodb://127.0.0.1:27017/demo', function (err, database) {
  
  
 if (err) throw err;
  
  // Connected!
  
  db = database; 
  
 itemsCollection = db.collection('items');
  
  
  
 app.listen(3000);
  
  
  console.log('Listening on port 3000'); 
  
 });
  
 // Create a router that can accept JSON 
  
 var router = express.Router(); 
  
 router.use(bodyParser.json());
  
 // Setup the collection routes 
  
 router.route('/')
  
  
  .get(function (req, res, next) { 
  
  
  
  
 itemsCollection.find().toArray(function (err, docs) {
  
  
  
  
  res.send({
  
  
  
   
  
  status: 'Items found',
  
  
  
   
  
  items: docs
  
  
  
   
  });
  
  
  
  });
  
  
  })
  
  
  .post(function (req, res, next) {
  
  
  
  var item = req.body; 
  
  
  
  
 itemsCollection.insert(item, function (err, docs) {
  
 192",NA
Wire Up the Front End with the REST API,"Talking to a REST service from Angular couldn’t be easier. Angular comes with a $http service, which wraps the 
 browser’s XMLHttpRequest object so that it works with the Angular digest loop. It also makes the API consistent 
 across browsers and makes it easier to work with by using promises. Promises are a topic we will discuss in detail in 
 the next chapter, but we will give a brief overview here after you have seen the code.
  
 You can get access to the $http service the same way you got access to our own custom guidService, which we 
 saw earlier. To access the REST API, we will create our own custom Angular service that will use the Angular’s built-
 in $http service to communicate with the server. The complete client-side JavaScript including the controller is 
 shown in Listing 9-10.
  
 Listing 9-10. 
 todocomplete/public/main.js
  
 var demoApp = angular.module('demo', []); 
  
 demoApp.controller('MainController', ['$scope', 'todoWebService', function ($scope, todoWebService) {
  
  // Setup a view model
  
  var vm = {};
  
  vm.list = [];
  
 193",NA
Next Steps,"There is a lot more that can be said about AngularJS. For example, we only used the built-in directives, such as ng-
 click, but you can author your own to create powerful web components. Also we only saw the basic usage of 
 dependency injection (DI) in Angular. The main motivation for DI in Angular is testability. To learn more about 
 testability, it is best to check out the angular-seed project provided by the AngularJS team 
 https://github.com/ 
 angular/angular-seed
 . The angular-seed project also contains guidance for how to split your client-side project into 
 multiple JavaScript files for maintainability.",NA
Additional Resources,"Twitter Bootstrap: 
 http://getbootstrap.com/ 
  
 AngularJS: 
 http://angularjs.org/ 
  
 Angular Seed: 
 https://github.com/angular/angular-seed",NA
Summary,"In this chapter, we looked at how consuming web services can be done quite easily with a mature framework like 
 AngularJS. We made an effort to be clear about what the code does. We focused on guiding you through the code so 
 that you know exactly what is going on. This will allow you to explore larger code bases with a deep understanding 
 and greater confidence.
  
 We also tried to justify our use of the libraries in this chapter. Along the way, we explained what is meant by 
 SPA and why you should care. After the introduction in this chapter, you should be confident to explore more JQuery, 
 Bootstrap, and AngularJS on your own. In the next chapter, we will take a look at promises and other ways to 
 simplify callbacks.
  
 196",NA
Chapter 10,NA,NA
Simplifying Callbacks,"The evented/asynchronous nature of Node.js means that it is possible to end up with deeply nested callbacks. 
 There are various strategies in JavaScript that help reduce the callback nesting. In this chapter, we will explore 
 these patterns. These help maintain readability, but more importantly these strategies will help decrease the 
 likelihood of bugs creeping up on you. I promise.",NA
The Callback Hell,"Note that as we explore this section, it might seem like the situation is dire. Don’t worry. When we look at promises in 
 the next section, you will find the pot of gold at the end of the rainbow.
  
 An obvious issue with callbacks is the 
 increased indentation
 . This has been affectionately called the callback 
 pyramid of doom
 . Consider the simple case in Listing 10-1 where we have three async functions that we need to call 
 (for example, these might be database search, select, and save item).
  
 Listing 10-1. 
 pyramid/indented.js
  
 function first(data, cb) {
  
  
  console.log('Executing first');
  
  
  setTimeout(cb, 1000, data); 
  
 }
  
 function second(data, cb) {
  
  
  console.log('Executing second');
  
  
  setTimeout(cb, 1000, data); 
  
 }
  
 function third(data, cb) {
  
  
  console.log('Executing third');
  
  
  setTimeout(cb, 1000, data); 
  
 }
  
 first('data', function (text1) {
  
  
  second(text1, function (text2) {
  
  
  
  third(text2, function (text3) { 
  
  
  
  
 console.log('done:', text3); // indented
  
  
  });
  
  
  }); 
  
 }); 
  
 197",NA
If/else in an Async World,"If you conditionally need to do an async operation in a function, you must make sure that the whole function is 
 asynchronous. Listing 10-3 is a simple example to demonstrate this complexity.
  
 Listing 10-3. 
 ifelse/bad.js
  
 // WARNING! DO NOT USE!
  
 function maybeSync(arg, cb) {
  
  
  if (arg) { // We already have data 
  
  
  
 // BAD! Do not call synchronously!
  
  
  
  
 cb('cached data');
  
  
  }
  
  
  else { // We need to load data
  
  
  
  // simulate a db load
  
  
  
  setTimeout(function () {
  
  
  
   
  cb('loaded data')
  
  
  
  }, 500);
  
  
  } 
  
 } 
  
 // Without the intimate details of maybeSync 
  
 //   its difficult to determine if 
  
 //     - foo is called first 
  
 //     OR 
  
 //     - bar is called first
  
 maybeSync(true, function (data) { 
  
  
 foo(); 
  
 }); 
  
 bar();
  
 function foo() { console.log('foo') } 
  
 function bar() { console.log('bar') }
  
 Without looking at the code of maybeSync function, it is impossible for a developer to know if foo will be called 
 first or bar will be called first. In fact, in our example foo will be called immediately, whereas an async developer 
 would assume that bar will be called first and foo will be called at 
 some later point
  like any other async operation. 
  
 The reason why this is not the behavior here is because maybeSync was poorly written and called the callback 
 immediately based on some condition. The proper way is to use the process.nextTick function (as we saw in Chapter 
 3) to schedule the callback for the next tick of the event loop. Listing 10-4 shows the maybeSync function fixed up 
 (renamed to alwaysAsync).
  
 Listing 10-4. 
 ifelse/good.js
  
 function alwaysAsync(arg, cb) {
  
  
  if (arg) { // We already have data 
  
  
  
 // setup call for next tick 
  
  
  
 process.nextTick(function () { 
  
  
  
  
 cb('cached data'); 
  
  
  
 });
  
  
  }
  
 199",NA
Loops in an Async World,"Consider the simple case of fetching two items via HTTP requests and using the data contained in them. One simple 
 way to approach this is shown in Listing 10-5.
  
 Listing 10-5. 
 loop/simple.js
  
 // an async function to load an item 
  
 function loadItem(id, cb) {
  
  
  setTimeout(function () { 
  
  
  
 cb(null, { id: id });
  
  
  }, 500); 
  
 }
  
 // functions to manage loading 
  
 var loadedItems = []; 
  
 function itemsLoaded() {
  
  
  console.log('Do something with:', loadedItems); } 
  
 function itemLoaded(err, item) { 
  
  
  
 loadedItems.push(item);
  
  
  if (loadedItems.length == 2) { 
  
  
   
 itemsLoaded();
  
  
  } 
  
 }
  
 // calls to load 
  
 loadItem(1, itemLoaded); 
  
 loadItem(2, itemLoaded);
  
 200",NA
Error Handling,"The worst issue with using callbacks for async tasks is the complexity in error handling. Let’s look at a concrete 
 example to solidify the concept. Consider the simple case of authoring an async version of loading JSON from a file. 
 A synchronous version for such a task is shown in Listing 10-7.
  
 Listing 10-7. 
 errors/sync.js
  
 var fs = require('fs');
  
 function loadJSONSync(filename) { 
  
  
 return JSON.parse(fs.readFileSync(filename)); 
  
 }
  
 // good json file 
  
 console.log(loadJSONSync('good.json'));
  
 // non-existent json file 
  
 try {
  
  
  console.log(loadJSONSync('absent.json')); } 
  
 catch (err) {
  
  
  console.log('absent.json error', err.message); }
  
 // invalid json file 
  
 try {
  
  
  console.log(loadJSONSync('bad.json')); 
  
 } 
  
 catch (err) {
  
  
  console.log('bad.json error', err.message); 
  
 }
  
 There are three behaviors of this simple loadJSONSync function: a valid return value, a file system error, or a 
 JSON.parse error. We handle the errors with a simple try/catch as you are used to when doing synchronous 
 programming in other languages. The obvious performance disadvantage is that while the file is being read from 
 the filesystem, no other JavaScript can execute. Now let’s make a good async version of such a function. A decent 
 initial attempt with a trivial error checking logic is shown in Listing 10-8.
  
 Listing 10-8. 
 Snippet from errors/asyncsimple.js
  
 var fs = require('fs');
  
 function loadJSON(filename, cb) {
  
  
  fs.readFile(filename, function (err, data) {
  
  
  
  if (err) cb(err);
  
  
  
  else cb(null, JSON.parse(data));
  
  
  }); 
  
 }
  
 202",NA
Introduction to Promises,"Before we see how promises greatly simplify asynchronous JavaScript, we need to have a good understanding of how 
 a Promise behaves. Promises are soon (when ECMAScript 6 is finalized) going to be a part of standard JavaScript 
 runtime. Until then, we need to use a third-party library. By far, the most popular one is Q (npm install q), which is 
 what we will use here. Utility functions surrounding promises are different in individual libraries, but Promise 
 instances from all the good libraries are compatible with one another because they all follow the 
 “
 Promises/A+
 ”
  
 specification. This spec is what is going to be a part of ECMAScript 6 as well so your knowledge is future-secure. 
 Throughout this section, we will explain the concept followed by pointing out the Promise advantage over callbacks.",NA
Create a Promise,"We will look at the code before explaining promises in detail. Let’s start with creating a promise, Q style. In Listing 10-
 14, we create a promise, subscribe to its completion using the promise.then member function, and finally resolve the 
 promise.
  
 205",NA
Promise States,"A promise can only be one of three states: Pending, Fulfilled, or Rejected. There is a state transition diagram between 
 them, as shown in Figure 
 10-1
 .
  
  
 Figure 10-1.
  Promsie states and fates
  
 The promise can only transition from one state to another based on these arrows. For example, it is not possible 
 for a fulfilled promise to become rejected. Additionally, the value it is fulfilled to or the reason it is rejected with 
 cannot change. Also shown in the diagram, if the promise is fulfilled or rejected, we say it has settled.
  
 Promise advantage:
  Since a promise’s transition into fulfilled or rejected is immutable, all individual 
 onFulfilled / onRejected handlers will only be called once. Promises don’t have the 
 called again
  problem of 
 callbacks.
  
 You can transition a promise manually using the deferred object we saw earlier. But, most commonly 
  
 (almost always), some function will give you a promise and from that point on you use the then function to create and 
 fulfill or reject promises.",NA
The Then and Catch Basics,"The then member function is the heart of the promise API. At its most basic level, you can use it to subscribe to a 
 promise settled result. It takes two functions (called the onFulfilled and the onRejected handlers), which are 
 called based on the final state of the promise (fulfilled or rejected). However, we recommend only passing in the 
 onFulfilled handler to the then function.
  
 Similar to the then function on a promise, there is the catch function. The catch function only takes the 
 onRejected handler. Therefore, it is conventional to only have the onFulfilled handler in the then function followed by 
 a catch function with the onRejected handler. These two functions are shown in Listing 10-16.
  
 207",NA
Chain-ability of Then,"The chain-ability of promises is 
 their most important feature
 . Once you have a promise, you use the then function to 
 create a chain of fulfilled or rejected promises.
  
 The most important behavior is that the value returned from the onFulfilled handler (or the onRejected 
 handler) is wrapped in a new promise. This new promise is returned from the then function, which allows you 
 to chain promises one after another. This is shown in Listing 10-19.
  
 Listing 10-19. 
 chainability/chain.js
  
 var Q = require('q');
  
 Q.when(null)
  
  
  .then(function () { 
  
  
   
  
 return 'kung foo';
  
  
  }) 
  
  
  
 .then(function (val) {
  
  
   
  console.log(val); // kung foo
  
  
   
  return Q.when('panda');
  
  
  })
  
  
  .then(function (val) {
  
  
   
  console.log(val); // panda 
  
  
   
  
 // Nothing returned
  
  
  })
  
  
  .then(function (val) {
  
  
   
  console.log(val == undefined); // true
  
  
  });
  
 Note that if you return a promise from the then handler (for example, we return the promise resolved to a panda 
 in our second then function), the next then handler will wait for the promise to settle (resolve or rejected) before 
 calling the appropriate handler.
  
 209",NA
Converting Callbacks to Promises,"In this section, we’ll see how to interoperate Promises with classical Node.js async pattern. With this knowledge, we 
 will revisit the loadJSON example to see how much simpler it is.",NA
Interfacing with nodeback,"Now that you can partially appreciate the Promise API, the immediate concern will most likely be interoperability 
 with node callback style functions (lovingly called 
 nodeback
 ). A nodeback is simply a function that
  
 
  
 takes n number of arguments, the last of which is a callback, and
  
 
  
 the callback is called with (error) or (null, value) or (null, value1, value2,...).
  
 This is the style of functions in the core Node.js modules as well as dependable community-written modules. 
 Converting nodeback-style functions to promises is a simple task of calling Q.nbind, which takes a  nodeback-
 style function, wraps it, and returns a new function that does the following:
  
 
  
 Takes the same first n-1 arguments as the nodeback function (that is, all the arguments 
  
 excluding the callback argument) and silently passes them to the nodeback function along 
  
 with an internal callback function
  
 
  
 Returns a promise that is
  
 
  
 rejected if the internal callback is called by the nodeback function with a non-null error 
  
 argument (in other words, the (error) case),
  
 
  
 resolved to a value if the callback is called by the nodeback function like (null, value), 
  
 and
  
 
  
 resolved to an array [value1, value2,...] if the callback is called by the nodeback 
  
 function like (null, value1,value2,...).
  
  
 In Listing 10-22, we show a practical example of converting nodeback-style functions into ones that work with 
 promises.
  
 Listing 10-22. 
 interop/nodeback.js
  
 function data(delay, cb) {
  
  
  setTimeout(function () { 
  
  
  
 cb(null, 'data');
  
  
  }, delay); 
  
 }
  
 function error(delay, cb) {
  
  
  setTimeout(function () { 
  
  
  
 cb(new Error('error'));
  
  
  }, delay); 
  
 }
  
 // Callback style 
  
 data(1000, function (err, data) { console.log(data); }); 
  
 error(1000, function (err, data) { console.log(err.message); });
  
 211",NA
Converting Non-nodeback Callback Functions,"Many functions in the browser (for example, setTimeout) do not follow the nodeback convention of error being the 
 first argument. These functions were ported as is to Node.js for ease of code reusability. To convert these (and other 
 functions that might not follow the nodeback interface) to return promises, you can use the deferred API (deferred. 
 resolve/deferred.reject) that we are already familiar with. For example, in Listing 10-25 we have a simple promise 
 based sleepAsync function created from setTimeout.
  
 213",NA
Providing a Promise + nodeback Interface ,"Now that we have covered how to convert nodeback- and callback-based functions into ones that return promises, it 
 is worthwhile considering the reverse scenario to make it easier for people who are not familiar with promises to use 
 your API with callbacks. Callbacks are, after all, simple enough for any beginner to grasp.
  
 Q promises provide a simple function, promise.nodeify(callback), where if the callback is a function, it assumes 
 it’s a nodeback and calls it with (error) if the promise gets rejected or calls it with (null,resolvedValue) if the 
 promise gets fulfilled. Otherwise, promise.nodeify simply returns the promise.
  
  
 As an example, we can convert our promise based loadJSONAsync to support both promises as well as the 
 nodeback convention as shown with usage in Listing 10-26.
  
 Listing 10-26. 
 interop/dual.js
  
 var Q = require('q'); 
  
 var fs = require('fs'); 
  
 var readFileAsync = Q.nbind(fs.readFile);
  
 function loadJSONAsync(filename, callback) {
  
  
  return readFileAsync(filename)
  
  
  
  .then(JSON.parse) 
  
  
  
  
 .nodeify(callback); 
  
 }
  
 // Use as a promise 
  
 loadJSONAsync('good.json').then(function (val) {
  
  
  console.log(val); 
  
 });
  
 // Use with a callback 
  
 loadJSONAsync('good.json', function (err, val) {
  
  
  console.log(val); 
  
 });
  
 214",NA
Further Notes on the Promise API,"Now that we have covered the most bang-for-buck area of promises, it is worth mentioning a few surrounding areas 
 so that you can claim to be a true Promise expert.",NA
Promises Supports Other Promises as a Value,"Anywhere you see a value being passed into a promise, you can actually pass in another promise and the next 
 onFulfilled/onRejected handler (depending upon how things go) will be called with the final settled value.
  
 We already saw this happen when we returned a promise from our onFulfilled / onRejected handlers during the 
 discussion on chaining then. The next onFulfilled / onRejected handler in the chain got the final settled value of the 
 promise. The same is true for Q.when and deferred.resolve, as shown in Listing 10-27, and any other time you try to 
 pass a promise as a value for resolution.
  
 Listing 10-27. 
 further/thenable.js
  
 var Q = require('Q');
  
 Q.when(Q.when('foo')).then(function (val) {
  
  
  console.log(val); // foo 
  
 });
  
 var def = Q.defer(); 
  
 def.resolve(Q.when('foo')); 
  
 def.promise.then(function (val) {
  
  
  console.log(val); // foo 
  
 });
  
 Q.when(null).then(function () { 
  
  
  
 return Q.when('foo'); 
  
 }) 
  
 .then(function (val) {
  
  
  console.log(val); // foo 
  
 });
  
  
 This behavior is extremely useful since you can just pass the value if you have it or a promise to the value if you 
 need to make an async request to load it inside your chain.
  
 215",NA
Ungracefully Terminating a Promise Chain (Intentionally),"As we saw earlier, uncaught exceptions in the onFulfilled / onRejected handlers only result in the next promise in the 
 chain to be rejected, but they do not throw an error in the application main loop. This is great since it allows you to 
 return the promise from your function and cascade the error to the handler that can reliably see why the promise 
 failed. Consider the simple example in Listing 10-28.
  
 Listing 10-28. 
 further/gracefulcatch.js
  
 var Q = require('q');
  
 function iAsync() {
  
  
  return Q.when(null).then(function () {
  
  
  
  var foo;
  
  
  
  // Simulate an uncaught exception because of a programming error 
  
  
  
 foo.bar; // access a member on an undefined variable
  
  
  }); 
  
 }
  
 iAsync()
  
  
  .then(function () { }) // not called 
  
  
  
 .catch(function (err) { console.log(err.message); });
  
 However, when at the root level of your application where you are not returning this promise to anyone, a 
 simple catch (onRejected handler) is not sufficient. If there is a bug in in our own onRejected handler, no one will get 
 notified. Listing 10-29 provides an example where our catch callback itself has an error and there is no notification at 
 runtime. This is because all that happened was the next promise got rejected and no one cared.
  
 Listing 10-29. 
 further/badcatch.js
  
 var Q = require('q');
  
 function iAsync() {
  
  
  return Q.when(null).then(function () {
  
  
  
  var foo; foo.bar; // Programming error. Will get caught since we return the chain
  
  }); 
  
 }
  
 iAsync()
  
  
  .catch(function (err) {
  
  
   
  var foo; foo.bar; // Uncaught exception, rejects the next promise
  
  }); 
  
  
  
 // But no one is listening to the returned promise
  
 216",NA
Promise Library Compatibility,"The behavior of ‘then’ is what is most important in a promise. How you create promises inside your library is 
 secondary. In fact, the Promises/A+ spec only dictates the behavior of the then function (and the onFulfilled / 
 onRejected handlers) since that is all that is needed for promises from one library to interoperate with another. In 
 Listing 10-31, we show a demonstration of seamlessly using promises between bluebird and Q.
  
 Listing 10-31. 
 further/librarycompat.js
  
 var Q = require('q'); 
  
 var BlueBird = require('bluebird');
  
 new BlueBird(function (resolve) { // A bluebird promise 
  
  
 resolve('foo'); 
  
 })
  
  
  .then(function (val) { 
  
  
   
  
 console.log(val); // foo
  
  
   
  return Q.when('bar'); // A q promise
  
  
  })
  
  
  .then(function (val) { 
  
  
   
  
 console.log(val); // bar
  
  
  });
  
  
 This means that if you are using a Node.js library that returns promises, you can use then with Q or some other 
 library that you like, or even native promises once ES6 is finalized.
  
 217",NA
Inspecting the State of Promises ,"Q promises provide a few useful utility functions to see the state of promises. You can use the promise.
  
 isFulfilled()/ promise.isRejected()/ promise.isPending()to determine the 
 current
  state of a promise. There is also a 
 utility inspect method, promise.inspect, which returns a snapshot of the current state. This is demonstrated in Listing 
 10-32.
  
 Listing 10-32. 
 further/inspect.js
  
 var Q = require('q');
  
 var p1 = Q.defer().promise; // pending 
  
 var p2 = Q.when('fulfill'); // fulfilled 
  
 var p3 = var p3 = Q.reject(new Error('reject')); // rejected
  
 process.nextTick(function () {
  
  console.log(p1.isPending()); // true
  
  console.log(p2.isFulfilled()); // true
  
  console.log(p3.isRejected()); // true
  
  
  console.log(p1.inspect()); // { state: 'pending' }
  
  
  console.log(p2.inspect()); // { state: 'fulfilled', value: 'fulfill' }
  
  console.log(p3.inspect()); // 
 { state: 'rejected', reason: [Error: reject] } });",NA
Parallel Flow Control,"We have seen how trivial doing a serial sequence of async tasks is with promises. It is simply a matter of 
 chaining then calls.
  
 However, you might potentially want to run a series of async tasks and then do something with the results of all 
 of these tasks. Q (as well as most other promise libraries) provides a static all (that is, Q.all) member function that you 
 can use to wait for n number of promises to complete. This is demonstrated in Listing 10-33, where we start a 
 number of async tasks and then continue once they are complete.
  
 Listing 10-33. 
 further/parallel.js
  
 var Q = require('q');
  
 // an async function to load an item 
  
 var loadItem = Q.nbind(function (id, cb) {
  
  
  setTimeout(function () { 
  
  
  
 cb(null, { id: id });
  
  
  }, 500); 
  
 });
  
 Q.all([loadItem(1), loadItem(2)])
  
  
  .then(function (items) { 
  
  
  
 console.log('Items:', items); // Items: [ { id: 1 }, { id: 2 } ]
  
  })
  
  
  .catch(function (reason) { console.log(reason) });
  
 218",NA
Generators,"I am sure that at this point you will agree that promises are a great way to simplify callbacks. But we can actually 
 even do better, almost making asynchronous programming a first-class member of the language. As a thought 
 experiment, imagine the following: a way to tell the JavaScript runtime to pause the executing of code on the await 
 keyword used on a promise and resume *only* once (and if) the promise returned from the function is settled. (See 
 Listing 10-34).
  
 Listing 10-34. 
 generators/thought.js snippet
  
 // Not actual code. A thought experiment 
  
 async function foo() {
  
  
  try {
  
  
  
  var val = await getMeAPromise();
  
  
  
  console.log(val);
  
  
  }
  
  
  catch(err){
  
  
  
  console.log('Error: ',err.message);
  
  
  } 
  
 }
  
 When the promise settles, execution continues if it was fulfilled. Then await will return the value. If it’s 
 rejected, an error will be thrown synchronously, which we can catch. This suddenly (and magically) makes 
 asynchronous programming as easy as synchronous programming
 .
  Three things are needed:
  
 
  
 Ability to 
 pause function execution
  
 
  
 Ability to 
 return a value inside
  the function
  
 
  
 Ability to 
 throw an exception inside
  the function
  
 The good news is this magic is very real and possible to 
 try
  today. The syntax will be 
 slightly different
  because the 
 technology we will be using wasn’t designed 
 only
  for this. It is possible because of JavaScript generators, a technology 
 coming with ECMAScript 6, which you can use today. To use it today, you will need two things, both of which will 
 become unnecessary once ECMAScript 6 is finalized:
  
 
  
 An unstable build of Node.js. Currently this means some version of the form v0.1 , for 
  
 example v0.11.13. You can download prebuilt binaries/installer for Windows as well as Mac 
  
 OS X from 
 http://nodejs.org/dist
  and install them as we did in Chapter 1.
  
 
  
 Run your Node.js executable with the --harmony flag. For example, to run a file app.js you 
  
 would do the following:
  
 $ node --harmony app.js
  
 219",NA
Motivation for Generators,"The main motivation for adding generators to JavaScript is the ability to do lazy evaluation during iteration. A simple 
 example is the desire to return an infinite list from a function. This is demonstrated in Listing 10-35.
  
 Listing 10-35. 
 generators/infiniteSequence.js
  
 function* infiniteSequence(){
  
  
  var i = 0; 
  
  
  
 while(true){ 
  
  
   
 yield i++;
  
  
  } 
  
 }
  
 var iterator = infiniteSequence(); 
  
 while (true){
  
  
  console.log(iterator.next()); // { value: xxxx, done: false } }
  
 You signify that a function is going to return an iterator by using function* (instead of function) and, instead of 
 the return keyword, you yield a result. Each time you yield, control leaves the generator back to the iterator. Each 
 time iterator.next is called, the generator function is resumed from the last yield. If you run this simple example, you 
 will see an infinite list of numbers being returned (yielded) from the iterator. This example shows the motivation of 
 adding generators to the language.",NA
Power of Generators in JavaScript,"Generators are much more powerful in JavaScript than in many other languages. First, let’s look at an example that 
 sheds light on their behavior a bit more (Listing 10-36). Listing 10-37 is a sample run of the application.
  
 Listing 10-36. 
 generators/outside.js
  
 function* generator(){
  
  
  console.log('Execution started'); 
  
  
  
 yield 0;
  
  
  console.log('Execution resumed'); 
  
  
  
 yield 1;
  
  
  console.log('Execution resumed'); 
  
 }
  
 var iterator = generator(); 
  
 console.log('Starting iteration'); 
  
 console.log(iterator.next()); // { value: 0, done: false } 
  
 console.log(iterator.next()); // { value: 1, done: false } 
  
 console.log(iterator.next()); // { value: undefined, done: true }
  
 220",NA
Promises and Generators,"Generators combined with promises allow you to do near synchronous style programming with all the performance 
 benefits of Asynchronous JavaScript. The key ingredients are generator functions (function *) that yield on promises 
 (yield promise) wrapped in Q.spawn function call. Let’s look at an example. Listing 10-40 shows the two cases 
 (fulfilled and rejected) of yielding a promise inside a generator wrapped in Q.spawn.
  
 Listing 10-40. 
 spawn/basics.js
  
 var Q = require('q');
  
 Q.spawn(function* (){
  
  
  // fulfilled
  
  
  var foo = yield Q.when('foo'); 
  
  
  
 console.log(foo); // foo
  
  
  // rejected
  
  
  try{ 
  
  
   
  
 yield Q.reject(new Error('bar'));
  
  
  } 
  
  
  
 catch(err){
  
  
   
  console.log(err.message); // bar
  
  
  } 
  
 });
  
  
 If the promise is rejected, a synchronous error is thrown inside the generator. Otherwise, the value of the 
 promise is returned from the yield call.
  
  
 Q also has the Q.async function, which takes a generator function and wraps it up into a function, which, when 
 called, would return a promise that is
  
 
  
 resolved to the final return value of the generator, and
  
 
  
 rejected if there is an uncaught error in the generator or the last value is a rejected promise.
  
 222",NA
The Future,"There is already an ECMAScript 7 recommendation to make async / await first-class usable keywords of the language, 
 which provide very light sugar to do a transformation similar to what we have demonstrated. That is, ECMAScript 7 
 might potentially do the rewrite in Listing 10-42 silently inside the JavaScript VM:
  
 Listing 10-42. 
 Proposed Future Syntax Using Generators Internally
  
 async function <name>?<argumentlist><body>
  
 =>
  
 function <name>?<argumentlist>{ return spawn(function*() <body>); }",NA
Additional Resources,"Async (npm install async) documentation: 
 www.npmjs.org/package/async 
  
  
 Promises/A+ specification: 
 http://promises-aplus.github.io/promises-spec/ 
  
  
 A description of Promise states and fates: 
 https://github.com/domenic/promises-unwrapping/blob/master/ 
 docs/states-and-fates.md 
  
  
 ECMAScript ES7 async/await: 
 https://github.com/lukehoban/ecmascript-asyncawait",NA
Summary,"In this chapter, we looked at all the areas you need to be careful about when writing high-quality callback style 
 code. Once you start using promises, the lessons of writing good callback-based functions might not be as relevant 
 in your code but will still help you debug other people’s code.
  
 After demonstrating that callbacks can get tricky, we provided promises as an alternative that can greatly 
 simplify the flow of your code. We discussed the behavior of a Promise chain and demonstrated how it simplifies 
 error handling and allows your asynchronous code to follow a natural sequential pattern. We also demonstrated how 
 easy it is to convert existing nodeback-style functions to return promises and how to make your promise returning 
 and consuming functions still support nodeback so you can incrementally update your APIs and applications.
  
 Finally, we discussed generators that offer exciting opportunities for JavaScript code innovation. We combined 
 generators with promises to get the best benefits of synchronous programming—in other words, simplified error 
 handling, error bubbling, and function execution to your highly performing asynchronous code.
  
 224",NA
Chapter 11,NA,NA
Debugging,"Everyone knows that debugging is twice as hard as writing a program in the first place.
  
 —Brian Kernighan
  
 When undertaking a production application, it is important to have good debugging tools in your tool belt. This 
 can save you plenty of hair pulling and head scratching. In this chapter, we will look at the most basic of debugging 
 assistance tools along with high-powered IDEs and tricks of the trade.",NA
The Console Object,"The simplest form of debugging is simple logging. The console is a global object available in Node.js. It is intentionally 
 designed to mimic the behavior of a web browser’s console object. We have been using console.log function since the 
 beginning of this book. Now we will take a more in-depth look at this and other member functions of console.",NA
Simple Logging,"The simplest behavior of console.log is that it will take any number of arguments and call util.inspect (from the 
 Node.js core util module) on each of these
 ,
  separated by a space 
 and
  followed by a new line. This is shown on the 
 following Node.js REPL session:
  
 > console.log(1,2,3); console.log(4,5,6); 
  
 1 2 3 
  
 4 5 6
  
  
 The underlying util.inspect function is quite powerful, and it does some nice printing for arbitrary objects out of 
 the box. For example, we can log arbitrary JavaScript objects:
  
 > console.log({ life: 42 }, { foo: 'I pitty the foo' }, [1, 0]); { life: 42 } { foo: 'I 
 pitty the foo' } [ 1, 0 ]
  
  
 We can customize what is printed by util.inspect for a particular object by adding an inspect function to the 
 object, as shown in Listing 11-1.
  
 225",NA
Simple Benchmark,"Quite commonly, you would like to know the duration of some piece of code. There are two functions, console.time 
 and console.timeEnd, that can be used to instrument your code quickly and painlessly so you don’t waste time 
 looking at the wrong place. They simply take a label and on console.timeEnd print out the label + the duration since 
 the last moment console.time was called with the same label. In the absence of a label, undefined will get used so you 
 can use that as well for quick throwaway tests. Usage of these functions is demonstrated in Listing 11-3.
  
 226",NA
A Quick Way to Get the Call Stack,"You might want to get the stack trace to see why a particular function is being called and by whom. You can get a 
 quick printout of the call stack using the console.trace function, which optionally takes a label to display for the 
 particular stack trace, as shown in Listing 11-4.
  
 Listing 11-4. 
 console/trace.js
  
 function foo() { 
  
  
 console.trace('trace at foo');
  
  
  // Execution continues
  
  
  console.log('Stack trace printed'); 
  
 }
  
 function bar() { 
  
  
 foo(); 
  
 }
  
 bar();
  
  
 A sample output is shown in Listing 11-5. You can see that using console.trace has no impact on the execution 
 flow. It simply logs the stack trace.
  
 Listing 11-5. 
 Sample Run of console/trace.js
  
 $ node trace.js 
  
 Trace: trace at foo
  
  
  at foo (.\code\Chapter 11\console\trace.js:2:13)
  
  
  at bar (.\code\Chapter 11\console\trace.js:9:5)
  
  
  at Object.<anonymous> (.\code\Chapter 11\console\trace.js:12:1)
  
  ... 
 truncated ...
  
  
  at node.js:807:3 
  
 Stack trace printed
  
 227",NA
Print to stderr,"The console.log function outputs to the program’s stdout stream. The console.error function has the same 
 behavior as console.log except that it prints to stderr.
  
 You should use stderr for error messages. If you think of the program as a function, then this function always 
 returns an object with two values, stdout and stderr. This makes stdout clear of any error messages and safe to pipe 
 between applications. For example, consider the simple script shown in Listing 11-8.
  
 Listing 11-8. 
 console/stderr.js
  
 console.log('Good output'); 
  
 console.error('Error message');
  
  
 If we were to redirect this application’s output to a file, only stdout is sent to the file and stderr is sent to the 
 terminal, as shown in Listing 11-9.
  
 228",NA
The Debugger Statement,"All Node.js debuggers that we will look at in this chapter support the debugger statement. If the application is being 
 actively debugged (has a debugger attached), then this statement will cause a breakpoint to occur on this line. In the 
 absence of any debugger, it has absolutely no effect and is ignored completely by the JavaScript runtime. For example, 
 the following code prints hello world and debugger has no effect:
  
 console.log('hello'); 
  
 debugger; 
  
 console.log('world');
  
  
 The debugger statement is a part of the standard JavaScript specification and works in the web-browser debug 
 tools as well.",NA
Node’s Built-in Debugger,"A quick debugging strategy is to insert a debugger statement in your code where you want to break (and debug) 
 and then start your node application by passing in the debug argument to node:
  
 $ node debug yourscript.js 
  
 ■
 Note
  You pass arguments to 
 node
  by mentioning them after 
 node
  but before your script name. We 
 already saw a 
  
 usage of this in the previous chapter where we using the 
 --harmony
  flag. Such arguments do not 
 become a part of 
  
 process.argv
  and therefore do not pollute your argument processing logic inside your code. these are 
 simply 
  
 processed by 
 node
  internally.
  
 Using the debug argument will do the following:
  
 
  
 Start your application
  
 
  
 Pause before executing the first line of your JavaScript
  
 
  
 Give you a shell to control you application flow",NA
Node-inspector,"If all you need is a quick way to debug a Node.js application, we highly recommend node-inspector. Node-inspector is 
 a tool dedicated solely to debugging Node.js applications. It is built on top of the chrome developer tools platform, so 
 if you are used to debugging browser applications using chrome, you will feel right at home with it. First, you need to 
 install it, which is easily done using npm install -g node-inspector.
  
  
 To debug a script using node-inspector, you simply use node-debug instead of node. For example, to debug 
 inspect.js, we would use the command node-debug inspect.js, as shown in Figure 
 11-1
 .
  
 231",NA
Remote Debugging Node.js,"You can start a node process in debug mode using the --debug flag:
  
 $ node --debug yourfile.js
  
  
 Additionally, if you use the --debug-brk flag, the node process starts in debug mode with a break on the first line 
 waiting for a debugger to connect before it executes any JavaScript:
  
 $ node --debug-brk yourfile.js
  
 Once you start a node process in debug mode, by default it listens on port 5858 to accept incoming connections 
 for debugging (using the V8 TCP based debugging protocol). You can change the port node listens on by passing an 
 optional value to –debug. For example, the following node process will listen on port 3333 for a debugger to 
 connect.
  
 $ node --debug=3333 yourfile.js
  
 The same port option exits for --debug-brk (for example, --debug-brk=3333). The principle of remote debugging 
 Node.js is the same in all the debuggers. In other words, start the node process in debug mode and then connect to it. 
 We will look at an example with node-inspector next.",NA
Remote Debugging with Node-inspector,"At its core, node-inspector is simply a web server serving the front end of a debugging application (chrome dev tools) 
 on port 8080 that are communicating with a local node process in debug mode (by default via port 5858). Hints for 
 this are provided in the URL for node-inspector we saw in the last section (http://localhost:8080/debug?port=5858).
  
 234",NA
Node.js Configuration in WebStorm,"We saw how to get started with WebStorm for Node.js development in Chapter 1. In particular, we covered how to 
 open and create Node.js projects. Here we will explore its debugging setup a bit more. We’ve already provided a 
 WebStorm project with the same server.js for you as we have been using in this chapter. This project is available 
 under code/chapter11/webstorm folder. You might recall from Chapter 1, debugging a file is simply a matter of 
 right clicking inside the file and selecting the debug option, as shown in Figure 
 11-7
 .
  
  
 Figure 11-7.
  Launching a debug session from WebStorm
  
  
 All of the node configuration options are available under the Run menu in the Edit Configurations option, as 
 shown in Figure 
 11-8
 . Also shown in Figure 
 11-9
  is the edit configurations dialog that opens up.
  
 237",NA
Additional Resources,"Information on Node’s built-in debugger: 
 http://nodejs.org/api/debugger.html 
  
  
 Source code for Node’s built-in debugger: 
 https://github.com/joyent/node/blob/master/lib/_debugger.js",NA
Summary,"Node.js has excellent support for all the conventional techniques of debugging applications. We started with using the 
 console and saw how it can help us make quick observations, do benchmarks, and print stack traces.
  
 Then we looked at Node.js debuggers, in particular node-inspector (free and open source) and WebStorm (a 
 commercial, inexpensive, and very popular cross-platform IDE). In this chapter, we also saw how Node.js provides 
 first-class support for remote debugging out of the box—something that is lacking in many other platforms.
  
 In the next chapter, we will look at writing tests for our applications so that bugs don’t creep into our system.
  
 240",NA
Chapter 12,NA,NA
Testing,"I never make stupid mistakes. Only very, very clever ones.
  
 —John Peel
  
 Testing is an integral part of any successful software development life cycle. It ensures that defects are noticed as 
 early as possible. It also prevents feature regressions where things you know to work right now can break because of 
 new code. In this chapter, we will look at various libraries that help test Node.js applications.",NA
Assert,"Node.js comes with assert as a core module, which you use simply as require('assert'). The main objective of 
 assert is to provide you with simple logic checks to throw errors based on invalid situations. Consider the simple 
 assert.equal function tested in a node REPL session in Listing 12-1.
  
 Listing 12-1. 
 Sample Session Demonstrating Assert
  
 $ node 
  
 > assert.equal(0,0); 
  
 undefined 
  
 > assert.equal(0,1); 
  
 AssertionError: 0 == 1
  
  
  at repl:1:8
  
  
  at REPLServer.self.eval (repl.js:110:21)
  
  
  ... truncated ...
  
  
  at ReadStream.onkeypress (readline.js:99:10) 
  
 > assert.equal(0,1,""numbers to not match""); 
  
 AssertionError: numbers to not match
  
  
  at repl:1:8
  
  
  at REPLServer.self.eval (repl.js:110:21)
  
  
  ... truncated ...
  
  at ReadStream.onkeypress (readline.js:99:10)
  
 If the two values passed to assert.equal are equal, then the assertion has no effect. However, if they are not 
 equal, an AssertionError is thrown. You can optionally also pass in a message to display if the values are not equal. 
 Using custom messages will help you track down the cause of the AssertionError faster since you can use a more 
 meaningful message that explains the objective (in other words, the reason for the assertion).
  
 241",NA
Mocha,"From the web site (
 http://mochajs.org/
 ): “Mocha is a feature-rich JavaScript test framework running on node.js 
 and the browser, making asynchronous testing simple and fun.” It is best to think of mocha as a test runner (the 
 mocha executable) as well as test API (for example, describe, it, beforeEach functions). We will look at both of these 
 in this section. Quite a few concepts of mocha (such as the concept of a 
 suite
  and 
 spec
 ) are common to other 
 JavaScript frameworks (for example, Jasmine 
 http://jasmine.github.io/
 ). We will discuss these concepts as well.",NA
Installing Mocha,"You install the mocha executable from NPM:
  
 $ npm install -g mocha
  
  
 Once you do this, the mocha executable becomes available on your system path. Any Node.js file you run using 
 mocha (instead of node) will have access to the mocha API, which we will look at next.",NA
Creating and Running a Basic Test,"The two most important functions in the mocha API are describe and it. The describe function is used to 
 encapsulate a 
 test suite
 , which is quite simply a collection of tests. How you decide what goes in a suite is up to you. 
  
 A common choice on grouping would be based on some functional boundary. For example, tests for a single 
 JavaScript class would normally all go in a single suite.
  
 The it function encapsulates a single test (also called a 
 spec
 ). Both the describe and the it functions take a string 
 as a first argument (which is displayed on the console when you run the tests) and a function as a second argument 
 (this is the callback executed when mocha runs the test). Let’s look at an example of a simple test suite. Note that it is 
 a normal Node.js file.  For example, we can require('assert') like any other file, with just a few global functions 
 (describe and it) added by mocha, as shown in Listing 12-5.
  
 244",NA
Mocha API,"We’ve already looked at the describe and it functions. The additional mocha API is quite small (simple but powerful). 
 It can be categorized into hooks and exclusive testing functions.",NA
Hooks,"As we mentioned earlier in the chapter, a good test framework should simplify test setup and teardown. For example, 
 when testing our List class, our setup (. adding an item) and teardown (removing an item) was duplicated for each of 
 the tests.
  
 Mocha provides hook functions before(), after(), beforeEach(), and afterEach(), which can be used to simplify 
 our test setup and teardown significantly. All of these functions simply take a callback. This callback is called at the 
 correct instant (hook) when in a test suite (a describe callback). These hook moments are described below with the 
 names pretty much giving away the meaning:
  
 
  
 before: executes the registered callback before it runs the first test (the first it callback)
  
 
  
 after: executes the callback after it runs the last test
  
 
  
 beforeEach: executes the callback before executing 
 each
  test
  
 
  
 afterEach: executes the callback before executing 
 each
  test
  
  
 As a simple example of using these hooks, consider the following suite where we simply increment a number 
 before we run each test. The test depends upon this number having been incremented to succeed. 
  
 (See Listings 12-7 and 12-8.)
  
 Listing 12-7. 
 mocha/hooks.js
  
 var assert = require(""assert"");
  
 describe('our test suite', function () {
  
  var testExecuting = 0; 
  
 beforeEach(function () { 
  
   
 testExecuting++;
  
  });
  
 it('test 1', function () { 
  
   
 assert.equal(1, 1, 'This should be test 1');
  
  });
  
  
  
 it('test 2', function () { 
  
  
   
 assert.equal(2, 2, 'This should be test 2');
  
  }); 
  
 });
  
 Listing 12-8. 
 Sample Run of mocha/hooks.js
  
 $ mocha hooks.js
  
  our test suite
  
  √ test 1
  
  √ test 2
  
  2 passing (6ms)
  
 246",NA
Single Test Runs and Exclusions,"The describe and the it functions both have member functions skip and only. If you only want to run one test suite 
 (describe) while you are working on large sets of specs, you can force mocha to only run one suite by simply 
 appending .only to the describe (in other words, use describe.only instead of describe). For example, in Listing 12-9, 
 only the first describe will execute.
  
 Listing 12-9. 
 mocha/only.js
  
 describe.only('first', function () {
  
  
  it('test 1', function () {
  
  
  }); 
  
 });
  
 describe('second', function () {
  
  
  it('test 1', function () {
  
  
  }); 
  
 });
  
  
 Similarly, you can use it.only to only run one test. Note that at one time you can only have one usage of 'only' in 
 all your tests and suites combined
 .
  
 If you simply want to exclude a few tests from running (for example, because they are failing and you don’t want 
 to investigate why at the moment), use skip, which also exists for describe and it. For example, the second test suite is 
 skipped in the code in Listing 12-10.
  
 Listing 12-10. 
 mocha/skip.js
  
 describe('first', function () {
  
  
  it('test 1', function () {
  
  
  }); 
  
 });
  
 describe.skip('second', function () {
  
  
  it('test 1', function () {
  
  
  }); 
  
 });
  
 describe('third', function () {
  
  
  it('test 1', function () {
  
  
  }); 
  
 });
  
 You can skip as many describe / it calls as you like. Skip is better than commenting out a test block since you 
 might forget to uncomment the block. But with a skip, mocha will tell you that you have skipped something every 
 time you run it, as shown in Listing 12-11.
  
 247",NA
Async Testing,"In its tag line, mocha claims that it “mak[es] asynchronous testing simple and fun.” Just because a callback exits 
 doesn’t mean that the async work (and therefore the test) is over. For example, consider the test in Listing 12-12 
 where we test a simulated async operation that returns an error.
  
 Listing 12-12. 
 mocha/asyncbad.js
  
 function simulateAsync(cb) {
  
  
  setTimeout(function () { 
  
  
  
 cb(new Error('This async task will fail'));
  
  }, 
 500); 
  
 }
  
 describe('our test suite', function () {
  
  it('test simulateAsync', function () { 
  
  
  
 simulateAsync(function (err) { 
  
  
   
 if (err) throw err;
  
  
  });
  
  });
  
 });
  
  
 However, since the async operation doesn’t take place until after the call to its callback completes, mocha 
 considers the test a success, as shown in Listing 12-13.
  
 Listing 12-13. 
 Sample Run of mocha/asyncbad.js Showing That Tests Pass Incorrectly
  
 $ mocha asyncbad.js
  
  our test suite
  
  
  √ test simulateAsync
  
 1 passing (5ms)
  
 248",NA
Testing Promises,"Mocha has excellent support for promises. Instead of using a done argument, you can simply return a promise from 
 your test or any of the hooks. Mocha will wait until the promise settles. If it gets fulfilled, the test/hook succeeds. 
 Similarly, if it is rejected, the corresponding test/hook will fail. This is shown in Listing 12-16.
  
 Listing 12-16. 
 mocha/asyncpromise.js
  
 var Q = require('q');
  
 describe('our test suite', function () {
  
  it('this should pass', function () {
  
  
  return Q.when('pass');
  
  });
  
  it('this should fail', function () {
  
  
  return Q.reject(new Error('fail'));
  
  });
  
 });
  
 Just one more reason to use promises in your codebase.",NA
Mocha Command Line Options,"As we said earlier, mocha is split between an API and an executable. We’ve seen the API and we’ve seen simple usage 
 of the mocha executable (which simply executes a Node.js file). Among the command line options for mocha, we have 
 already mentioned the --timeout flag used to increase the duration for which mocha will wait on an asynchronous test 
 to settle. Now let’s look at other options we feel will be the most helpful to you.",NA
Specify Files to Test,"Mocha can take not only individual files (like we have been giving it on the command line), but it can also take file 
 globs (pattern matching arguments). If you don’t provide a file argument, by default, the mocha executable will run all 
 the JavaScript files in the './test' folder (that is, the test folder in the current working directory). You can optionally 
 specify the file pattern on the command line. In this code snippet, we ask it to run all the JavaScript files in mytestdir:
  
 mocha ""./mytestdir/*.js""
  
 Earlier we showed the 'only' member function of describe and it. We recommend avoid doing this as you need to 
 be careful to revert this before you commit your code. Otherwise, automated deployment systems could potentially 
 run only one suite (or test) and think that all tests are green. Instead, use the '-g' (short for --grep) command line flag 
 to specify a regular expression match for the tests you want to run. For example, the following will only run tests that 
 contain the phrase my awesome test:
  
 mocha -g ""my awesome test""
  
 250",NA
Debugging Mocha Tests,"The mocha executable provides the command line flags --debug and --debug-brk that behave similarly to the node --
 debug and --debug-brk flags we saw in Chapter 11. The flags are actually just passed to the underlying node process 
 that mocha starts so you can attach a debugger to localhost (port 5858 by default, if you recall). This means there is 
 nothing special to debugging tests started with mocha and it is pretty much same as what you learned in the previous 
 chapter.
  
 The simplest workflow is to start your tests with mocha --debug-brk to begin a paused node process. Then 
 launch node-inspector (by simply running it on the command line like we did in Chapter 11) to start the web server 
 and visit http://localhost:8080/debug?port=5858 to debug the paused node process that mocha starts.
  
 One thing to be aware of is that if you put a breakpoint in an async test, it will probably cause it to time out. 
  
 So, actually, you will most commonly run mocha --debug-brk -t=100000 -g ""my awesome test"". Also, recall the 
 debugger keyword, which will still work (for obvious reasons).",NA
Chai,"Good tests follow the 
 AAA
  pattern:
  
 
  
 Arrange
  all the necessary preconditions and inputs.
  
 
  
 Act
  using the object / method under test.
  
 
  
 Assert
  that the expected results have occurred.
  
 We’ve already seen the core Node.js assert module, which provides a basic set of functions for simple assertions. 
 Chai is a library (npm install chai) that provides you with additional assertion functions.",NA
Additional Assertions in Chai,"The member function list in require('chai').assert is quite large. The additional functions provided by chai 
 mostly focus on stronger type checks (for example, isNull/isNotNull, isDefined/isUndefined, and 
 isFunction/isNotFunction). Usage of one of these is demonstrated in Listing 12-17.
  
 Listing 12-17. 
 Sample Session Demonstrating chai.assert
  
 $ node 
  
 > var assert = require('chai').assert; 
  
 undefined 
  
 > assert.isNull(null,'Null should be null'); 
  
 undefined 
  
 > assert.isNull(undefined,'undefined should be null'); 
  
 AssertionError: undefined should be null: expected undefined to equal null
  
  
 In addition to these functions, the node’s core assert functions are duplicated in require('chai').assert so you 
 only have one assert module loaded.
  
 251",NA
Chai Plug-ins,"Another advantage of chai is that it provides a plug-in framework for others to easily add additional assertions to. For 
 example, chai-datetime plug-in (npm install chai-datetime) provides several assertions that make comparing dates 
 easy. Example functions are equalTime, beforeTime, afterTime, equalDate, beforeDate, and afterDate.
  
  
 To use a chai plug-in, you call the chai.use function to register it with chai. An example with chai-datetime is 
 demonstrated in Listing 12-18.
  
 Listing 12-18. 
 Sample Session Demonstrating Using a Chai Plug-in
  
 $ node 
  
 > var chai = require('chai'); 
  
 > chai.use(require('chai-datetime')); 
  
 > var t1 = new Date(); 
  
 > chai.assert.equalTime(t1,t1); 
  
 > chai.assert.equalTime(t1,new Date()); 
  
 AssertionError: expected Thu Aug 14 2014 19:30:40 GMT+1000 (AUS Eastern Standard Time) to equal Thu Aug 14 
 2014 19:31:22 GMT+1000 (AUS Eastern Standard Time)
  
  
  at Function.assert.equalTime (.\node_modules\chai-datetime\chai-datetime.js:147:40)
  
  
  at repl:1:13
  
  
  at REPLServer.self.eval (repl.js:110:21)
  
  
  at repl.js:249:20",NA
BDD Style Assertions,"Chai also provides BDD (behavior-driven development) style assertions. Here are a few reasons why this is worth 
 your consideration (examples to follow):
  
 
  
 These are easily chainable.
  
 
  
 These read in better 
 English
 , so the test code is closer to the 
 behavior
  that you are trying to test.
  
 These are demonstrated in the code sample in Listing 12-19.
  
 Listing 12-19. 
 chai/chain.js
  
 var expect = require('chai').expect; 
  
 var assert = require('chai').assert;
  
 // Some variable to assert on 
  
 var beverages = { tea: ['chai', 'matcha', 'oolong'] };
  
 // assert 
  
 assert.lengthOf(beverages.tea, 3); 
  
 // BDD 
  
 expect(beverages).to.have.property('tea').with.length(3); // same 
 as 
  
 expect(beverages).property('tea').length(3);
  
 252",NA
Additional Resources,"Assert API documentation: 
 http://nodejs.org/api/assert.html 
  
  
 Mocha documentation: 
 http://visionmedia.github.io/mocha/ 
  
  
 Chai plug-ins: 
 http://chaijs.com/plugins 
  
  
 At the start of this chapter, we saw how easy it is to write quick and simple unit tests using the core assert 
 module. We followed this by a discussion of mocha (a testing framework) and chai (an assertion framework). As 
 usual, instead of listing all the API details for all the libraries, we have focused on the why and the general how, 
 providing you with a deeper understanding of the concepts so that you can jump right in and explore the APIs as 
 you use them.
  
  
 The next chapter is the last in the book. We will look at deploying and sharing our web applications 
 with the world.
  
 253",NA
Chapter 13,NA,NA
Deployment and Scalability,"It
 ’
 s better to have infinite scalability and not need it than to need infinite scalability and not have it.
  
 —Andrew Clay Shafer
  
 We will start this chapter by looking at ways to ensure the stability of our web server. The objective here is to 
 automatically re-spawn our server so that we continue responding to web requests even if there were some 
 unhandled errors that caused our Node.js server to crash.
  
 Next, we will discuss how we can easily scale our application to utilize all the CPU cores of a multi-core single 
 machine. We will follow this with a guide to deploying our application on Amazon Web Services (AWS). To make sure 
 that changing AWS screens does not decrease the usefulness of this guide, we will present the concepts in isolation 
 before doing a step-by-step guide.",NA
Ensuring Uptime,"Like all application platforms, it is not difficult to imagine our server raising an unhandled error and crashing. For 
 example, in Listing 13-1, the Node.js server will only serve the first request and crash due to a programming mistake.
  
 Listing 13-1. 
 uptime/crash.js
  
 var http = require('http'); 
  
 http.createServer(function (req, res) {
  
  
  res.write('Hello\n'); 
  
  
  
 res.foo.bar; // Programming error. Tried to read property 'bar' on an undefined member 'foo'
  
  
 res.end('World!'); 
  
 }).listen(3000); 
  
 console.log(""Server running on http://localhost:3000"");
  
  
 If you run this server, the first client request will result in an error and the client connection will be dropped. 
 After that, no more client requests will be processed, as shown in Listing 13-2.
  
 Listing 13-2. 
 Sample Client Requests with uptime/crash.js
  
 $ curl localhost:3000 
  
 Hello 
  
 curl: (56) Recv failure: Connection was reset
  
 $ curl localhost:3000 
  
 curl: (7) Failed connect to localhost:3000; No error
  
 255",NA
Using Forever,"Forever is available as an NPM package (npm install -g forever). It is a simple command line tool for ensuring that a 
 given script runs continuously (that is, forever). After you install forever, you get access to the forever executable. 
 You can ask it to start any command (not just Node.js scripts) using the -c command line flag. Let’s run the crashing 
 server script (from Listing 13-1) using forever (shown in Listing 13-5).
  
 256",NA
Node.js Clustering,"As you know, Node.js is single-threaded, which means it is optimized for a single processor. However, it is not difficult 
 to make our application utilize all the CPU cores available to us on a multi-core system. This is made simple for an 
 HTTP server thanks to the clustering API, which is a part of core Node.js. We simply use it via require('cluster').
  
 Other platforms have multi-threaded servers. Since a single Node.js process only has a single thread, we 
 effectively need to look at multi-process servers. The main issue is that only a single process is allowed to listen on a 
 particular port by the operating system. Otherwise, the OS would not know which process should get the packet 
 received on a particular port. Therefore, we need a design where one process is the master (and actually listens on 
 the TCP/IP port), and it spawns child processes and distributes the received HTTP traffic between them. This is 
 exactly what the cluster module does.",NA
Spawning Workers,"When you run a script that uses the cluster module, it will, by default, start in master mode:
  
 var cluster = require('cluster'); 
  
 console.log(cluster.isMaster); // true
  
 To start a new worker process (a child process) with the 
 same
  script as the one that is currently executing, you 
 simply call cluster.fork, demonstrated in the previous code snippet. If the script is running in master mode, we get a 
 Node.js process running the same script in worker mode. This is shown in Listings 13-7 and 13-8.
  
 257",NA
The Ideal Worker Count,"Since workers do the heavy lifting, ideally you want workers equal to the number of CPUs you have on your system. 
 Assuming that each worker process, with its single JavaScript thread, is able to fully utilize a CPU, this is the 
 maximum performance you can drive from a system using Node.js.
  
  
 Determining the number for CPUs is trivial, thanks to the core os module’s cpus function. This is demonstrated 
 in Listing 13-9, where we fork the ideal number of worker processes.
  
 Listing 13-9. 
 cluster/cpus.js
  
 var cluster = require('cluster'); 
  
 var numCPUs = require('os').cpus().length;
  
 if (cluster.isMaster) {
  
  
  // Fork workers
  
  
  for (var i = 0; i < numCPUs; i++) {
  
  
  
  console.log(""Starting a worker"");
  
  
  
  cluster.fork();
  
  
  }
  
  
  // Listen to any worker exiting 
  
  
  
 cluster.on('exit', function (worker, code, signal) {
  
  
   
  console.log('worker ' + worker.process.pid + ' exited');
  
  }); 
  
 } else {
  
  
  console.log(""Worker started"");
  
  
  process.exit(); 
  
 }
  
 258",NA
Handling HTTP Requests in Workers,"If any worker calls the server.listen function, cluster serializes the arguments and passes the request to the master 
 process. If the master process already has a listening server matching the worker’s requirement (port), then it passes 
 the handle to the worker. Otherwise, the master creates a listening server with those requirements and then passes 
 the handle to the worker.
  
 From the code perspective, the worker code remains unchanged from what you already know, that is, something 
 as simple as http.createServer().listen. All of this complexity of the master actually doing the listening and distributing 
 the request load to the workers is handled for you by the cluster module. Sharing the HTTP load between workers is 
 demonstrated in Listing 13-10.
  
 Listing 13-10. 
 cluster/http.js
  
 var cluster = require('cluster'); 
  
 var http = require('http'); 
  
 var numCPUs = require('os').cpus().length;
  
 if (cluster.isMaster) {
  
  
  // Fork workers
  
  
  for (var i = 0; i < numCPUs; i++) {
  
  
  
  var worker = cluster.fork();
  
  
  
  console.log(""Started a worker with pid: "" + worker.process.pid);
  
  }
  
  
  // Listen to worker exiting
  
  
  cluster.on('exit', function (worker, code, signal) {
  
  
   
  console.log('worker ' + worker.process.pid + ' exited');
  
  
  }); 
  
 } else { 
  
  
  
 // Workers can share any TCP connection 
  
  
  
 http.createServer(function (req, res) {
  
  
   
  res.writeHead(200); 
  
  
   
  
 res.end(""Hello world from worker: "" + cluster.worker.process.pid);
  
  
 }).listen(3000); 
  
 }",NA
Communicating with the Master,"As we stated earlier, there is no shared JavaScript state between workers. All information sharing must take place 
 with events. You can send a data structure to the master using process.send from the worker. You can receive such a 
 data structure in the master simply by subscribing to the worker’s 'message' event. In the sample in Listing 13-11, we 
 send a simple string to the master.  A sample run is presented in Listing 13-12.
  
 259",NA
Core AWS ,NA,NA
Concepts ,NA,NA
Why Amazon Web Services,"There are a lot of cloud providers out there, but we choose AWS for good reason. It is by far the most popular, and 
 it has the greatest number of resources available at its disposal. Also, you will find plenty of help online for any 
 issues that you might run into, and the concepts you learn from AWS are easily transferrable to other cloud 
 providers.
  
 AWS is free for one year when you start. Most AWS features at a reasonable scale are free for one year after 
 you sign up. This means that you can test your application without paying any fee. After you sign up, the first thing 
 you should do is log on the AWS console (
 https://console.aws.amazon.com/
 ).",NA
AWS Console,"The AWS console is the single launching point for provisioning and managing your usage of various AWS products. 
 At any point in the console, you have the choice of picking an availability zone, as shown in Figure 
 13-1
 . Here you 
 would choose the region of the world you are developing your software for (we selected “Sydney” for this). Also 
 highlighted in the figure is the EC2 service. EC2 is arguably the most fundamental AWS service you need to use for a 
 Node.js deployment and will be discussed shortly.",NA
EC2,"Elastic Compute Cloud (EC2) is a service to launch virtual machines hosted by Amazon. A virtual machine in this 
 context is a single, isolated computer running on Amazon infrastructure. The initial contents of the virtual machine 
 are driven by an Amazon Machine Image (AMI). We will be using the Amazon Linux AMI with EBS-backed storage. 
 Concepts regarding these are explained next.",NA
Which EC2 AMI?,"Amazon Web Services provides ongoing security and maintenance updates to all instances running the Amazon 
 Linux AMI, so this is the one we recommend. Anything else and you will need to be more hands-on with ensuring 
 the integrity and security of your system. We would rather let Amazon deal with it for us. Another advantage of 
 using some form of a Linux distribution is that it allows you to easily migrate to other cloud providers since more 
 providers offer Linux compared to Windows.
  
  
 One thing worth mentioning is that the default user for Amazon Linux is ec2-user. We will need this username to 
 connect to our instance.",NA
EC2 Instance Types,"The EC2 instance type determines the quantity of CPU, RAM, and type of disk storage available to the VM (virtual 
 machine) when it starts. These types are categorized into instance families. Currently, these families are General 
 Purpose, Compute Optimized, Memory Optimized, GPU, and Storage Optimized. Needless to say, unless your 
 application has specific requirements for one of these categories, you should stick with General Purpose.
  
 Within the General Purpose instance family, we recommend using lots of micro instances (the smallest) instead 
 of one heavier (more CPU, RAM) size instances. The reason is that this way, you only pay at a lesser rate during low 
 traffic times. And when high traffic arrives, you can simply create more instances. Of course, you can benchmark other 
 instances as your requirements evolve.",NA
EBS,"Unlike the data stored on a local instance store (which persists only as long as that instance is alive), data stored on 
 an Amazon EBS volume can persist independently of the life of the instance. Therefore, we recommend that you use 
 the local instance store only for temporary data. For data requiring a higher level of durability, we recommend using 
 Amazon EBS volumes. We will see this option when provisioning an EC2 instance.
  
 EBS (like most services) does have an associated per GB cost, but this cost is very minor and worth the quick 
 instance boot time. There are different kinds of EBS storage options with different data throughput (measured in 
 IOPS, input/output operations per second).
  
 Current options are Magnetic (lowest IOPS but cheapest), General Purpose SSD (good balance of IOPS and cost), 
 and Provisioned IOPS SSD (highest IOPS but most expensive). We recommend sticking with General Purpose SSD 
 when starting out.",NA
Key Pair,"We already discussed the concept of asymmetric key cryptography in Chapter 6. To use many Amazon AWS services, 
 you need to work with a public/private key. Amazon stores the public key only, and you store the private key. Anyone 
 who possesses your private key can decrypt your login information, so it’s important that you store your private keys 
 in a secure place.
  
 262",NA
VPC,"Amazon Virtual Private Cloud (VPC) enables you to launch AWS resources into a virtual network. The VPC 
 determines the interconnection IP networking between various AWS resources (such as different EC2 instances) and 
 external interaction. The external interaction and the TCP/IP ports are significant because they allow you to have a 
 database server that is only accessible within the VPC from other EC2 instances, but not accessible from the outside 
 world. A new AWS account comes with a default VPC for each AWS region. By default, this VPC allows internal traffic 
 only. We will alter this VPC security to allow web traffic.
  
 And that’s it. You now have a basic understanding of the fundamental concepts you need when working with 
 AWS.",NA
Creating Your First EC2 Instance,Now we present a step-by-step guide.,NA
Create a Key Pair,"The first step is to create a key pair (your login for the EC2 instance you will create later). This is done from 
 the EC2 dashboard, as shown in Figure 
 13-3
 .
  
  
 Figure 13-3.
  Creating a key pair
  
 263",NA
Configuring the Security Group,"As we stated earlier, the default VPC/Security Group only allows internal traffic within the AWS resources. We 
 will open it up to allow external web traffic. Click the Security Groups section in the EC2 Dashboard, select the 
 default VPC security group, and click to edit the inbound traffic rules, as shown in Figure 
 13-4
 .
  
  
 Figure 13-4.
  Edit the security group inbound ports
  
  
 In the inbound rules, add SSH and HTTP to be accessible from any address. Also enable all ICMP so that you can 
 ping the server. Finally, click the Save button to commit the changes. (See Figure 
 13-5
 .)
  
  
 Figure 13-5.
  Values for the security group inbound ports
  
 264",NA
Provisioning an Instance ,"Select the instances section on the dashboard and click Launch Instance, as shown in Figure 
 13-6
 .
  
  
 Figure 13-6.
  Creating an new EC2 instance
  
  
 This will take you into the AMI selection screen. As we already mentioned, we recommend the Amazon Linux 
 AMI. (See Figure 
 13-7
 .)
  
  
 Figure 13-7.
  Select the Amazon Linux AMI
  
 265",NA
Connecting to the EC2 Instance for the First Time,"We will use the PEM file (key value pair) that we downloaded, along with SSH.
  
 ■
 Note
  on Windows, 
 ssh
  is a part of Cygwin, 
 www.cygwin.com/
 , which we have been using for 
 bash
  as 
 well. many 
  
 Windows users prefer using putty though (
 www.chiark.greenend.org.uk/~sgtatham/putty/
 ). however, we 
 will stick 
  
 with 
 ssh
  for consistency.",NA
Setting Up a SSH Connect Script,"From the dashboard, select your running instance and note down its public IP address, as shown in Figure 
 13-13
 . 
 Note that this IP address and public DNS are only valid for the duration that this instance is running. Restarting the 
 instance results in a new IP and DNS allocation. (See Figure 
 13-13
 .)",NA
Keeping Your Server Safe,"Simply run sudo yum update on the server shell to install any important software updates. As shown in the 
 previous listing (Listing 13-15), the server lets you know on login if there are any software updates you need to 
 install.",NA
Install Node.js and NPM,"The most effective way is to set up GCC/C++ and compile Node.js for your Amazon Linux distribution. Run the 
 commands in Listing 13-16 to set up the prerequisites for building. Basically, gcc + openssl for compiling, git to get 
 Node.js source code. Finally, you use the installed git to get the source code.
  
 Listing 13-16. 
 Getting Node.js Prerequisites
  
 sudo yum install gcc-c++ make 
  
 sudo yum install openssl-devel 
  
 sudo yum install git
  
 git clone git://github.com/joyent/node.git 
  
 cd node
  
 Now the next decision is which version of Node.js do you want to run. To see which versions are tagged inside 
 the Node.js source code, you can use git tag -l to list them. We recommend the latest stable version (listed on 
 nodejs.org), as shown in Listing 13-17.
  
 Listing 13-17. 
 Building/Installing a Particular Version of Node.js
  
 git checkout v0.10.30 
  
 ./configure make 
  
 sudo make install
  
 It might be a while until it’s all done so you might want to grab a coffee. The output will show quite a few 
 warnings, but it is normal as a part of compilation. To verify that it all went smoothly, check that the following 
 (Listing 13-18) code works.
  
 Listing 13-18. 
 Checking a Successful Installation
  
 $ node -v 
  
 v0.10.30 
  
 $ npm -v 
  
 1.4.21 
  
 $ which node 
  
 /usr/local/bin/node 
  
 $ which npm 
  
 /usr/local/bin/npm
  
 272",NA
Allowing Global Package Installation,"As shown in the output in Listing 13-18(from the which node command), node and npm will be installed in 
 /usr/local/bin directory. To be able to install global NPM packages, you will need to run npm using sudo (
 s
 uper 
 u
 ser 
 do
 ). This means you need to add this path to the secure bin paths for sudo. You can do that by editing the 
 /etc/sudoers file and adding to the secure_pathfile. We will be using vi for this. Just in case you are not familiar with 
 vim, we show the keyboard shortcuts below. At any time to exit vim, type in :q and press enter to quit. Now open the 
 file in vim as super user:
  
 sudo vi /etc/sudoers
  
 Find the following within this file (vim tip, type in /secure and press enter):
  
 Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin
  
  
 Go to the end of the line (vim tip, it’s okay to use arrow keys), go into append mode (vim shortcut a), and append 
 what we show in bold:
  
 Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin
  
  
 Now exit insert mode (vim shortcut: ESC key) and save the file overwriting original (vim shortcut :w!). 
 Finally, quit vim :q. Now to test that it went as planned, install forever globally:
  
 $ sudo npm install -g forever
  
 Congratulations—you are now a Linux node setup guru.",NA
Running Your Node.js Application in AWS,"This part should be fairly trivial. You simply execute your script on the server the same way you have been 
 executing on your machine. The best way to get the code on the server is simply to use a source control repository 
 (such as github) and clone the code on the server. For example:
  
 git clone git://github.com/youcompacy/yourproject.git git 
 checkout master 
  
 npm install 
  
 node app.js
  
 And that’s pretty much it. Your application is now ready for public consumption.",NA
Additional Resources,"AWS EC2 Instance Types: 
 http://aws.amazon.com/ec2/instance-types/ 
  
 EBS Types: 
 http://aws.amazon.com/ebs/details/ 
  
 Amazon EBS FAQ: 
 http://aws.amazon.com/ebs/faqs/ 
  
 Amazon Linux AMI: 
 http://aws.amazon.com/amazon-linux-ami/ 
  
 Amazon AWS getting started video series: 
 https://aws.amazon.com/getting-started/ 
  
 The default VPC: 
 http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html
  
 273",NA
Summary,"We started the chapter by discussing how you can ensure better uptime in your web servers. One hundred percent 
 uptime is something that is highly desirable. Libraries like ExpressJS and Connect help making error handling highly 
 localized to a single HTTP request. Additionally, applications like forever can help keep your server running even in 
 the case of drastic errors.
  
 Next, we demonstrated that starting multiple Node.js processes to fully utilize the system resources for an 
 HTTP server is trivial. However, it is worth mentioning that the API is still experimental. Additionally, you are 
 generally better off deploying multiple servers, each with a single CPU (like the AWS micro EC2 instance) instead of 
 a single server with multiple CPUs since you only pay for what you use. It is still comforting to know that single 
 multi-core server clustering is supported by Node.js.
  
 Finally, we covered deployment. Before presenting a step-by-step AWS setup guide, we tried to present the 
 important concepts. This ensures that you will be able to navigate your way around the dashboard if Amazon 
 chooses to update the administration interface design. We also made clear the factors that swayed us in giving you a 
 recommended hardware selection so if those change, you can make similar rational choices.
  
 274",NA
Index,NA,NA
A,"Amazon Web Services (AWS) 
  
  
 console, 
 260–261 
  
  
 EBS, 
 262 
  
  
 EC2 
  
  
  
 Amazon Linux AMI, 
 262,265 
  
  
  
 connect script and sample usage, 
 271 
  
  
 dashboard and Launch Instance, 
 265 
  
  
 global package installation, 
 273 
  
  
  
 hard disk storage, 
 268 
  
  
  
 instance details configuration, 
 267 
  
  
 instance types, 
 262,266 
  
  
  
 IP address and public DNS, 
 270–271 
  
  
 key pair, 
 262–263,269 
  
  
  
 Node.js application, 
 272–273 
  
  
  
 NPM, 
 272 
  
  
  
 security group, 
 264,269 
  
  
  
 service dashboard, 
 261 
  
  
  
 Stop shutdown behavior, 
 267 
  
  
  
 virtual machine, 
 262 
  
  
 features, 
 260 
  
  
 VPC, 
 263 
  
 AngularJS application 
  
  
 controller, 
 187–188 
  
  
 data-binding/templating, 
 183
  
  
  
 JavaScript, 
 188–189 
  
  
  
 MainController function, 
 190 
  
  
 ng-repeat directive, 
 191 
  
  
  
 REST API (
 see
  REST API) 
  
  
  
 execution, 
 191 
  
  
  
 vm.addItem function, 
 191 
  
 URL handling/routing, 
 183 
  
 Async module definition 
  
  
  
 (AMD), 
 56 
  
  
 browserify 
  
  
  
 app.js file, 
 63 
  
  
  
 bar.js file, 
 62 
  
  
  
 command prompt, 
 62 
  
  
  
 foo.js file, 
 62 
  
  
  
 installation, 
 61 
  
  
  
 NPM modules, 
 61 
  
  
  
 reuse Node.js code, 
 64 
  
  
 CommonJS, 
 56 
  
  
 exports variables, 
 58 
  
  
 features, 
 61 
  
  
 loading, 
 56 
  
  
 network tab, 
 60 
  
  
 RequireJS, 
 57 
  
  
 run application, 
 59–60 
  
 async.parallel function, 
 201
  
 dependency injection, 
 183 
 directives, 
 187",NA
B,"Express server, 
 185 
  
 Google search trends, 
 183 
 index.html file, 
 185–186 
 modules, 
 187
  
 Bare-bones connect 
  
 application, 
 125 
  
 Binary JSON (BSON), 
 171
  
 resources, 
 196 
  
 $scope, 
 187–188 
  
 to-do list application 
  
  
 createGuid function, 
 189 
  
 guidService, 
 189 
  
  
 HTML, 
 190",NA
C,"Callback functions 
  
  
 asynchronous code 
  
  
  
 ifelse, 
 199 
  
  
  
 loops, 
 200–201
  
 275",NA
D,"catch function, 
 208–209 
  
  
  
 creation, 
 206 
  
  
  
 deferred.resolve, 
 215 
  
  
  
 fulfilled state, 
 207 
  
  
  
 generators, 
 222–223 
  
  
  
 inspect method, 
 218 
  
  
  
 library compatibility, 
 217 
  
  
  
 nodeback function (
 see
  Nodeback function) 
  
  
 non-nodeback functions, 
 213 
  
  
  
 onFulfilled handler, 
 209 
  
  
  
 parallel flow control, 
 218–219 
  
  
  
 pending state, 
 207 
  
  
  
 promise.done method, 
 217 
  
  
  
 Q.defer() object, 
 206 
  
  
  
 rejected state, 
 207 
  
  
  
 then function, 
 206 
  
  
  
 uncaught exception, 
 210,216 
  
 CAP theorem, 
 167 
  
 Cascading Style Sheets (CSS), 
 119
  
 -c command line flag, 
 256 
  
 Connect/Express middleware 
  
  
 body-parser middleware, 
 144 
  
  
 compression/compress.js, 
 150 
  
  
 connect-timeout, 
 151 
  
  
 cookie-based sessions, 
 149 
  
  
 cookies 
  
  
  
 cookie/1basic.js, 
 146 
  
  
  
 cookie/2show.js, 
 146 
  
  
  
 cookie/3parsed.js, 
 147 
  
  
  
 cookie/4clear.js, 
 147 
  
  
  
 cookie/5sign.js, 
 148 
  
  
  
 with curl, 
 146 
  
  
  
 digital signature, 
 148 
  
  
  
 expiry time, 
 149 
  
  
  
 httpOnly and secure, 
 149 
  
  
  
 res.clearCookie function, 
 147 
  
  
  
 res.cookie function, 
 146
  
 276
  
 Data-intensive applications 
  
  
  
 (DIA), 
 29 
  
 Data persistence 
  
  
 MongoDB 
  
  
  
 binaries, 
 168 
  
  
  
 BSON, 
 171 
  
  
  
 collection, 
 169 
  
  
  
 crud/basic.js, 
 172 
  
  
  
 databases, 
 169 
  
  
  
 dbpath, 
 168 
  
  
  
 distributed session store, 
 177 
  
  
 _id field, 170 
  
  
  
 MMS, 
 179 
  
  
  
 MongoClient, 
 171 
  
  
  
 MongoDB REPL, 
 168 
  
  
  
 ObjectId, 
 170–171 
  
  
  
 odm/connection.js, 
 175 
  
  
  
 primary keys, 
 170 
  
  
  
 Robomongo, 
 179 
  
  
  
 save function, 
 173 
  
  
  
 update function, 
 173–174 
  
  
 Mongoose ODM 
  
  
  
 complex queries, 
 177 
  
  
  
 connect member function, 
 175 
  
  
 Model function, 
 175 
  
  
  
 odm/basic.js, 
 176 
  
  
  
 saving/updating, 
 176 
  
  
  
 Schema class, 
 175 
  
  
  
 single item query, 
 176 
  
  
 NoSQL 
  
  
  
 availability, 
 166–167 
  
  
  
 CAP theorem, 
 167 
  
  
  
 consistency, 
 166–167 
  
  
  
 document database, 
 165 
  
  
  
 key-value store, 
 166 
  
  
  
 ORM, 
 167 
  
  
  
 partition tolerance, 
 166",NA
"F, G",File system module,NA
E ,"CommonJS module specification, 
 41 
  
   
 entire folder, 
 47 
  
 Elastic Compute Cloud (EC2) 
  
 global variable 
  
  
 additional resources, 
 273 
  
  
 app.js, 
 51 
  
  
 Amazon Linux AMI, 
 262,265 
  
  
  
 Buffer class, 
 50 
  
 connect script and sample usage, 
 271 
  
  
 command line arguments, 
 49 
  
 dashboard and Launch Instance, 
 265 
  
  
 console, 
 48 
  
  
 global package installation, 
 273 
  
  
 dirname, 
 49 
  
  
 hard disk storage, 
 268 
  
  
  
 filename, 
 49 
  
 instance details configuration, 
 267 
  
  
 process.nextTick, 
 50 
  
  
 instance types, 
 262,266 
  
  
 timers, 
 48 
  
  
 IP address and public DNS, 
 270 
  
 js extension, 
 47 
  
  
 key pair, 
 262–263,269 
  
  
 module.exports 
  
 Node.js application, 
 273 
  
  
 alias, 
 45 
  
  
 NPM, 
 272 
  
  
 empty object, 
 45 
  
  
 security group, 
 264,269 
  
  
 foo.js file, 
 41 
  
  
 service dashboard, 
 261",NA
I,"utilize exports, 
 47 
  
 Front-end application 
  
  
 SPA 
  
  
  
 AngularJS (
 see
  AngularJS application) 
  
  
 client-side XHRs, 
 182 
  
  
  
 main template, 
 182 
  
  
  
 user experience, 
 182 
  
  
  
 web site, 
 181–182 
  
  
 Twitter Bootstrap, 
 184–185
  
 Inheritance pattern 
  
 Animal class functionality, 
 92 
 Bird class, 
 92 
  
 child classes, 
 97 
  
 code implementation, 
 99 
  
 constructor property, 
 95 
  
 instanceof operator, 
 97 
  
 member function, 
 91 
  
 Object.create function, 
 98–99 
 parent constructor, 
 93",NA
H ,"prototype chain, 
 94
  
 util core module, 
 96
  
 HyperText Markup Language (HTML), 
 119,121 
  
 Integrated Development 
  
 HyperText Transfer Protocol (HTTP) 
  
 Environment (IDE)
  
 connect middleware 
  
 Visual Studio
  
 authentication, 
 130 
  
 bare-bones connect application, 
 125 
 configuration, 
 128
  
 adding debug mode, 
 13 
  
 breakpoint activation, 
 14 
  
 configuration options, 
 15
  
 creation, 
 126 
  
 execution, 
 14
  
 error handler, 
 133 
  
 JSON request and response, 
 129
  
 local variables and call stack, 
 14 
 new Node creation, 
 12
  
 objects, 
 128 
  
 WebStorm
  
 path prefixes, 
 127 
  
 adding new file, 
 9
  
 core networking modules, 
 115 
 cryptography keys, 
 135
  
 configuration options, 
 11 
  
 editing configuration, 
 10
  
 curl, 
 116 
  
 new project creation, 
 8
  
 debugging proxy, 
 117 
  
 file web server 
  
  
 404 Not Found HTTP response, 
 121 
  
 directory serving, 
 122
  
 script execution, 
 9–
 10 
  
 itemsLoaded function, 
 201 
  
 iterator.throw(errorObject) function, 
 221
  
 index.html, 
 121–122 
 2server/public/index.html, 
 121",NA
J,"2server/server.js, 
 121 
  
 headers, 
 116 
  
 listen function, 
 115,125 
  
 MIME NPM package, 
 120 
  
 MIME types, 
 119–120 
  
 1create/1raw.js, 
 116 
  
 public/private key, 
 136 
  
 redirect client requests, 
 138 
  
 redirect response, 
 139 
  
 request stream, 
 120 
  
 response.getHeader function, 
 120 
  
 response.removeHeader function, 
 120 
  
 response.setHeader function, 
 120 
  
 response.setHeader(name, value) member 
  
  
 function, 
 119
  
 278",NA
N,"reference, 
 31 
  
  
 revealing module pattern, 
 34 
  
  
 this keyword, 
 34 
  
  
 truthy and falsy values, 
 33 
  
 JavaScript Object Notation (JSON), 
 119 
  
 array, 
 70 
  
  
 boolean value, 
 70 
  
  
 global object, 
 71 
  
  
 loading object, 
 71 
  
  
 null, 
 70 
  
  
 number value, 
 70 
  
  
 require function, 
 78 
  
  
 specification and shield users, 
 69 
  
 string value, 
 70 
  
 JQuery, 
 185
  
 Nodeback function
  
  
 -Async postfix, 
 212 
  
  
 community-written modules, 
 211 
  
  
 core Node.js modules, 
 211 
  
  
 loadJSONAsync function, 
 212–213 
  
  
 promises+nodeback interface, 
 214–
 215 
  
 Q.nbind, 
 211–212
  
  
 -Sync postfix, 
 212 
  
 Node.js 
  
  
 closures, 
 22 
  
  
 DIA, 
 29 
  
  
 thread starvation, 
 28 
  
  
 V8 JavaScript engine, 
 31 
  
 Node.js clustering 
  
  
 http.createServer().listen, 
 259 
  
  
 ideal worker count, 
 258",NA
K ,"multiple Node.js HTTP processes, 
 260 
  
  
  
  
 spawning workers, 
 257 
  
 Key-Hash message authentication code 
  
 Node.js development 
  
  
  
 (HMAC), 
 148 
  
  
 IDE (
 see
  Integrated Development 
  
  
  
  
  
 Environment (IDE))",NA
L ,"JavaScript execution, 
 7 
  
 Mac OS X installer, 
 4
  
 loadItem function, 
 201 
  
 loadJSONAsync function, 
 215 
 loadJSONSync function, 
 202
  
  
 REPL, 
 6 
  
  
 Windows installer, 
 2 
  
 Node.js functions 
  
  
 anonymous functions, 
 21",NA
M ,"functionBody, 
 20
  
 functionReturn, 
 20
  
 Man-in-the-Middle (MitM) attack, 
 138 
 maybeSync function, 
 199 
  
 Middleware, 
 125 
  
 Model-View-Controller (MVC), 
 188 
 MongoDB 
  
  
 binaries, 
 168 
  
  
 BSON, 
 171 
  
  
 collection, 
 169 
  
  
 crud/basic.js, 
 172 
  
  
 databases, 
 169 
  
  
 dbpath, 
 168 
  
  
 distributed session store, 
 177 
  
  
 _id field, 170 
  
  
 MMS, 
 179 
  
  
 MongoClient, 
 171 
  
  
 MongoDB REPL, 
 168 
  
  
 ObjectId, 
 170–171 
  
  
 odm/connection.js, 
 175 
  
  
 primary keys, 
 170 
  
  
 Robomongo, 
 179
  
  
 higher-order functions, 
 22 
  
  
 immediately executing functions, 
 20 
 Node.js packages 
  
  
 advantages, 
 68–69 
  
  
 bar.js, 
 66 
  
  
 file-based 
 vs
 .node_modules, 
 67 
  
  
 global modules, 
 78 
  
  
 implicit loading, 
 66–67 
  
  
 JSON 
  
  
  
 array, 
 70 
  
  
  
 boolean value, 
 70 
  
  
  
 global object, 
 71 
  
  
  
 loading object, 
 71 
  
  
  
 null, 
 70 
  
  
  
 number value, 
 70 
  
  
  
 require function, 
 78 
  
  
  
 specification and shield users, 
 69 
  
  
 string value, 
 70 
  
  
 module caching and node_modules, 
 69 
  
 modules recap, 
 80",NA
O,"package.json online dependency tracking, 
 73,75 
 removing dependency, 
 75 
  
 saving dependency, 
 74
  
 Object Document Mapper (ODM), 
 175 
 Object Relational Mapping (ORM), 
 167
  
 popular Node packages 
  
 advantage, 
 87",NA
P,"ANSI escape codes, 
 87 
  
 built-in JavaScript Date type, 
 83 
  
 colors package, 
 86 
  
 __defineGetter__/__defineSetter__ member 
  
 Portable Network Graphics (PNG), 
 119 
 process.nextTick function, 
 199 
  
 promise.then member function, 
 205
  
 functions, 
 88 
  
 features, 
 84–
 85",NA
Q,"handling command line arguments, 
 82 
 JavaScript string, 
 87 
  
 reliable string parsing, 
 84 
  
 serializing dates, 
 85
  
 Q.async function, 
 222 
  
 Q.reject function, 
 209 
  
 Q.spawn 
 function, 
 222
  
 String.prototype, 
 89 
  
 underscore, 
 80",NA
R,"resources, 
 89 
  
  
 revisiting Node modules, 
 65 
  
 semantic versioning 
  
  
  
 definition, 
 76 
  
  
  
 NPM/package.json, 
 76 
  
  
 updating dependency, 
 77 
 Node.js performance 
  
  
 I/O scaling problem, 
 23 
  
  
 Nginx way, 
 25 
  
  
 simulateUserClick.js, 
 27 
  
  
 simulateWebRequest.js, 
 27
  
 Read-evaluate-print-loop (REPL), 
 6 
  
 Representational State Transfer 
  
  
 (REST), 
 156 
  
 REST API 
  
  
 $http service, 
 193 
  
  
 ExpressJS router-based API, 
 192–
 193 
  
 JavaScript, 
 193–194 
  
  
 MongoDB execution, 
 195 
  
  
 then member function, 
 195 
  
  
 todoWebService, 
 195
  
 web servers 
  
  
 process per request, 
 24",NA
S,"thread pool, 
 24 
  
 Node.js variables 
  
  
 arrays, 
 18 
  
  
 boolean, 
 18 
  
  
 numbers, 
 17 
  
  
 object literals, 
 18 
  
 Node Package Manager (NPM), 
 2,61 
  
  
 EC2, 
 272 
  
  
 installation, 
 74 
  
  
 listing dependency, 
 75 
  
  
 node_modules folder, 
 74 
  
  
 package.json online dependency tracking, 
 73,75 
  
 removing dependency, 
 75 
  
  
 saving dependency, 
 74 
  
 Not Only SQL (NoSQL) 
  
  
 availability, 
 166–167 
  
  
 CAP theorem, 
 167 
  
  
 consistency, 
 166–167 
  
  
 document database, 
 165 
  
  
 key-value store, 
 166
  
 280
  
 setInterval function, 
 48 
  
 setTimeout function, 
 48 
  
 Single page applications (SPA) 
  
  
 AngularJS (
 see
  AngularJS application) 
  
 client-side XHRs, 
 182 
  
  
 main template, 
 182 
  
  
 user experience, 
 182 
  
  
 web site, 
 181 
  
 Social Security Number (SSN), 
 170 
  
 SSL certification, 
 138 
  
 Streams 
  
  
 buffered web response, 
 109 
  
  
 duplex stream, 
 109 
  
  
 inheritance mechanism, 
 112 
  
  
 performant web applications, 
 108 
  
 pipe, 
 110 
  
  
 readable stream, 
 109,111–112 
  
  
 streaming web response, 
 109 
  
  
 transform stream, 
 109 
  
  
 writable stream, 
 109,111,113",NA
"T, U","Testing 
  
  
 assert 
  
  
  
 action and validation function, 
 244 
  
  
 assert.equal function, 
 241 
  
  
  
 class implementation, 
 242 
  
  
  
 List class, 
 242 
  
  
 Chai 
  
  
  
 additional assertions, 
 251 
  
  
  
 BDD style assertions, 
 252 
  
  
  
 Plug-ins, 
 252 
  
  
 Mocha 
  
  
  
 async testing, 
 248 
  
  
  
 command line options, 
 250–251 
  
  
 definition, 
 244,247 
  
  
  
 hooks, 
 246
  
  
  
 installation, 
 244 
  
  
  
 it functions, 
 244,247 
  
  
  
 testing promises, 
 250 
  
  
 resources, 
 253 
  
 Thread starvation, 
 28 
  
 Twitter Bootstrap, 
 184–185",NA
"V, W","V8 JavaScript engine, 
 31 
  
 ViewModel (VM), 
 188 
  
 Virtual Private Cloud (VPC), 
 263",NA
"X, Y, Z","XMLHttpRequest (XHR), 
 183",NA
Beginning Node.js,NA,NA
Basarat Ali Syed,NA,NA
Contents,"About the Author 
 xvii
  
 About the Technical Reviewer 
 xix
  
 Acknowledgments 
 xxi
  
 Introduction 
 xxiii
  
 ■
  
 Chapter 1: Setting Up for Nodejs Development
  
 1
  
 Installing Node.js 
 ...........................................................................................................................
 1 
 Installing on Windows  
 ...........................................................................................................................................
 2 
 Installing 
 on Mac OS X 
 ...........................................................................................................................................
 4 
 Using 
 the REPL 
 ..............................................................................................................................
 6 
 Executing Node.js Scripts 
 ..............................................................................................................
 7 
 Setting Up 
 an Integrated Development Environment 
 ....................................................................
 7 
 WebStorm Node.js 
 Support....................................................................................................................................
 8 
 Visual 
 Studio Node.js Support 
 .............................................................................................................................
 11 
 Summary 
 ....................................................................................................................................
 .
 15
  
 ■
  
 Chapter 2: Understanding Nodejs
  
 17",NA
About the Author,"Basarat Ali Syed
  is a senior developer and the go-to guy for front end at PicnicSoftware 
 (
 http://picnicsoftware.com/
 ) in Melbourne, Australia. He studied Master of Computing at Australian National 
 University (the top Australian University) and graduated with high distinction in all courses. He is a familiar face at 
 developer meet-ups and conferences in Australia and has been a speaker at events such as Alt.NET, DDDMelbourne, 
 MelbJS, and Melbourne Node.js, among others. He is deeply passionate about web technologies and strongly believes 
 that whenever given a choice, one should always bet on JavaScript. He is a respected member of the TypeScript 
 community and works on the DefinitelyTyped team (
 https://github.com/DefinitelyTyped
 ). He believes Node.js can 
 do for the back end what Chrome did for the front end. In his spare time he enjoys bodybuilding and cycling, and he 
 maintains a YouTube channel for helping fellow developers (
 http://youtube.com/basaratali
 ). You can follow him on 
 Twitter at 
 https://twitter.com/basarat
  and 
 https://github.com/basarat
 .
  
 xvii",NA
About the Technical Reviewer,"Martin Bean
  is a web developer based in the North East of England. Having grown up and studied in Darlington, he 
 moved to Newcastle upon Tyne at the age of 18, where he started his career with a well-respected digital agency. 
 Although he entered with aspirations of being a designer, he switched to development, giving him skills in both 
 disciplines.
  
 After working at agencies for almost five years, Martin left to pursue a career as a freelance developer and 
 consultant and hasn’t looked back. Specializing in development, he has worked with global video game publishers 
 and nationwide retailers on everything from simple web sites to bespoke e-commerce platforms.
  
 Martin is active on various social networks. You can follow him on Twitter at 
 https://twitter.com/martinbean
 .
  
 xix",NA
Acknowledgments,"Man is limited by his fears.
  
 —Babar Ali Syed
  
 This work would not have been possible without the effort of some people very dear and close to me. I’d like to thank 
 the kind folk at PicnicSoftware—Matt Walkenhorst, Nick Josevski, Andrew Browne, Scott Lowe, Tim Buddington, and 
 Dave Churchill—who make my office feel like a second home. My brother Baqar Ali Syed who tolerated my long 
 isolation periods where I would just research technology and then analyze the best way to present this material. 
 Thanks to Steve Fenton, who inspired me to undertake the monumental task of authoring a book. All of the people 
 from DefinitelyTyped—especially Boris Yankov, Masahiro Wakame, Bart van der Schoor, and John Reilly—who have 
 been a constant source of inspiration. To all of the people at Apress—especially Christine Ricketts, Martin Bean, Ann 
 Dickson, Louise Corrigan, and Dhaneesh Kumar—without whose effort this book would not be possible. 
  
 Thanks to the Melbourne development community—especially Andrey Sidorov, Jim Pelletier, Leah Garrett and 
 Michael Lyons—who make this awesome city even better.
  
 xxi",NA
