Larger Text,Smaller Text,Symbol
Architecture of ,NA,NA
Advanced ,NA,NA
Numerical ,NA,NA
Analysis Systems,NA,NA
Designing a Scientific Computing ,NA,NA
System ,NA,NA
using OCaml,NA,NA
—,NA,NA
Liang Wang,NA,NA
Jianxin Zhao,NA,NA
Architecture of ,NA,NA
Advanced ,NA,NA
Numerical ,NA,NA
Analysis ,NA,NA
Systems,NA,NA
Designing a Scientific ,NA,NA
Computing ,NA,NA
System using OCaml,NA,NA
Liang Wang,NA,NA
Jianxin Zhao,NA,NA
"To my wife Maria, our daughters Matilda and Sofia, ",NA,NA
and my beloved family.,NA,NA
—Liang,NA,NA
To my parents and sister; to their unyielding love and support. ,NA,NA
To all those who are fighting for freedom and righteousness against the ,NA,NA
unleashing evil from hell.,NA,NA
"“Namárië, ar nai aistalë Eldar ar Atani ar ilyë Léralieron hilya le. Eleni ",NA,NA
sílar antalyannar.”,NA,NA
—Jianxin,NA,NA
Table of Contents,"About the Authors  xi
  
 Acknowledgments  xiii
  
 Chapter 1:  Introduction 1 
 1.1 Numerical Computing in OCaml 
 .............................................................................................. 1 1.2 Architecture 
 ............................................................................................................................. 3 
 Basic Computing and Analytics with Owl 
 ................................................................................ 4 Advanced Design in Owl 
 .......................................................................................................... 5 Hardware and 
 Deployment ...................................................................................................... 6 
 Research on 
 Owl...................................................................................................................... 7 1.3 
 Summary........................................................................................................................
 ......... 8
  
 Chapter 2:  Core Optimizations  9 
 2.1 N-Dimensional Array in Owl 
 .................................................................................................... 9 2.2 Interface 
 OCaml to C ............................................................................................................. 12 
 2.3 Core Optimization of Computation 
 ........................................................................................ 15 CPU Support of Parallel 
 Computing ....................................................................................... 17 Vectorization 
 .......................................................................................................................... 17 
 Memory 
 ................................................................................................................................. 21 
 2.4 Optimization Techniques 
 ....................................................................................................... 26 Hardware 
 Parallelization ....................................................................................................... 26 
 Cache Optimization 
 ................................................................................................................ 27 Other",NA
About the Authors,"LiangWang
  is the Chief AI Architect at Nokia, the Chief 
 Scientific Officer at iKVA, a Senior Researcher at the 
  
 University of Cambridge, and an Intel Software Innovator. 
 He has a broad research interest in artificial intelligence, 
 machine learning, operating systems, computer networks, 
 optimization theory, and graph theory.
  
 Jianxin Zhao
  is a PhD graduate from the University of 
 Cambridge. His research interests include numerical 
 computation, artificial intelligence, decentralized systems, 
 and their application in the real world. 
  
 xi",NA
Acknowledgments,"Developing a full-featured numerical-analysis system is very complicated. Writing a 
 book to dive deep into its architecture is an even more challenging task. It not only 
 requires skills, enthusiasm, persistence, but also needs strong support from families, 
 colleagues, and communities. For years, we have received so much help from so many 
 individuals and organizations that it is almost impossible to make an exhaustive list. 
  
 Nonetheless, we would particularly like to emphasize that Owl is developed on top of an 
 enormous amount of previous work. Without the continuous efforts of these projects 
 and the intellectual contributions of these people over the years, it would be impossible 
 for us to create this system and deliver this book.
  
 We give our most hearty thanks to those who contribute to the Owl project. Marcello 
 Seri and Ta-Chu Kao developed owl-ode, an Ordinary Differential Equation solver 
 library based on Owl. Pierre Vandenhove worked on the memory optimization of the 
 computation graph module during his internship at the OCaml Labs in Cambridge. 
 Tudor Tiplea participated in developing the base library in Owl so that it could run on 
 various backends such as browsers. Ben Catterall’s thesis work on the PSP provided a 
 theoretical foundation for the Actor system.
  
 We would like to express our sincerest gratitude and appreciation to the OCaml 
 Software Foundation
 1
  and Ahrefs
 2
  for fully sponsoring this open access book as well as 
 their long-term support to the Owl project.
  
 1
 http://ocaml-sf.org/ 
  
 2
 https://ahrefs.com/
  
 xiii",NA
CHAPTER 1,NA,NA
Introduction,"This book introduces Owl, a numerical library we have been developing and 
  
 maintaining for years. We develop Owl for scientific and engineering computing in the 
 OCaml language. It focuses on providing a comprehensive set of high-level numerical 
 functions so that developers can quickly build up any data analytical applications. Over 
 years of intensive development and continuous optimization, Owl has evolved into a 
 powerful software system with competitive performance compared to mainstream 
 numerical libraries. Meanwhile, Owl’s overall architecture remains simple and elegant. 
 Its small codebase can be easily managed by a small group of developers.
  
 In this book, we are going to introduce the design and architecture of Owl, from 
 its designers’ perspective. The target audience is anyone who is interested in not only 
 how to use mathematical functions in numerical applications but also how they are 
 designed, organized, and implemented from scratch. Some prerequisites are needed 
 though. We assume the readers are familiar with basic syntax of the OCaml language. 
  
 We recommend [38] as a good reference book on this matter. Also note that this book 
 focuses on introducing core parts of the Owl codebase, such as the implementation and 
 design of various key modules. If you are more interested in how to use the 
  
 functionalities provided in Owl to solve numerical problems, such as basic mathematical 
 calculation, linear algebra, statistics, signal processing, etc., please refer to our book 
 OCaml Scientific Computing: Functional Programming in Data Science andArtificial 
 Intelligence
  [26].",NA
1.1  Numerical Computing in OCaml,"Scientific computing
  is a rapidly evolving multidisciplinary field that uses advanced 
 computing capabilities to understand and solve complex problems in the real world. 
 It is widely used in various fields in research and industry, for example, simulations in 
 biology and physics, weather forecasting, revenue optimization in finance, etc. One 
  
 © Liang Wang, Jianxin Zhao 2023 
  
 1
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5_1",NA
1.2  Architecture,"Designing and developing a full-fledged numerical library is a nontrivial task, despite 
 that OCaml has been widely used in system programming such as MirageOS. The key 
 difference between the two is fundamental and interesting: system libraries provide a 
 lean set of APIs to abstract complex and heterogeneous physical hardware, while 
 numerical libraries offer a fat set of functions over a small set of abstract number 
 types.
  
 When the Owl project started in 2016, we were immediately confronted by a series 
 of fundamental questions like: “what should be the basic data types”, “what should be 
 the core data structures”, “what modules should be designed”, etc. In the following 
 development and performance optimization, we also tackled many research and 
 engineering challenges on a wide range of different topics such as software engineering, 
 language design, system and network programming, etc. As a result, Owl is a rather 
 complex library, arguably one of the most complicated numerical software system 
 developed in OCaml. It contains about 269K lines of OCaml code, 142K lines of C code, 
 and more than 6500 functions. We have strived for a modular design to make sure that 
 the system is flexible and extendable.
  
 We present the architecture of Owl briefly as in Figure 
 1-1
 . It contains two 
  
 subsystems. The subsystem on the left part is Owl’s numerical subsystem. The modules 
 contained in this subsystem fall into four categories:
  
 • Core modules contain basic data structures, namely, the n-
 dimensional array, including C-based and OCaml-based 
 implementation, and symbolic representation.
  
 • Classic analytics contains basic mathematical and statistical functions, 
 linear algebra, signal processing, ordinary differential equation, etc., 
 and foreign function interface to other libraries (e.g., 
 CBLAS
  and 
 LAPACKE
 ).
  
 • Advanced analytics, such as algorithmic differentiation, optimization, 
 regression, neural network, natural language processing, etc.
  
 • Service composition and deployment to multiple backends such as to the 
 browser, container, virtual machine, and other accelerators via a 
 computation graph.
  
 3",NA
 Basic Computing and Analytics with Owl,"In a numerical library or software such as NumPy, MATLAB, TensorFlow, and Owl, the 
 N
 -dimensional array (ndarray) is the most fundamental data type for building 
 scientific computing applications. In most such applications, solely using scalar values 
 is insufficient. Both matrices and vectors are special cases of ndarray. Owl provides the 
 Ndarray module, the usability and performance of which are essential to the whole 
 architecture in Owl. In Chapter 
 2
 , we will introduce this module, including how it is 
 designed and its performance optimization.
  
 4",NA
 Advanced Design in Owl,"We have introduced the various classic analytics fields that are supported in Owl. What 
 makes Owl more powerful are a series of advanced analytics. At the heart of them lies 
 algorithmic differentiation (AD), which will be introduced in Chapter 
 3
 . Performing 
 differentiation is so crucial to modern numerical applications that it is utilized almost 
 everywhere. Moreover, it is proven that AD provides both efficient and accurate 
 differentiation computations that are not affected by numerical errors.
  
 One of the prominent analytics that benefit from the power of AD is optimization 
 (Chapter 
 4
 ), which is about finding minima or maxima of a function. Though 
  
 optimization methods do not necessarily require computing differentiation, it is often a 
 key element in efficient optimization methods. One most commonly used optimization 
 method is gradient descent, which iteratively applies optimization on a complex object 
 function by small steps until it is minimized by some standard. It is exactly how another 
 type of advanced application, regression, works. Regression covers a variety of methods, 
 including linear regression, logistic regression, etc. Each can be applied to different 
 scenarios and can be solved by various methods, but they share a similar solution 
 method, namely, iterative optimization.
  
 Depending on applications, the computation generated from AD can be quite 
 complex. They often form a directed acyclic graph, which can be referred to as a 
 computation graph. Based on the computation graph, we can perform various 
 optimizations to improve the execution speed, reduce memory usage, etc. A 
  
 computation graph plays a critical role in our system, and we introduce it in Chapter 
 6
 .
  
 5",NA
 Hardware and Deployment,"In the next part, we discuss the execution backends Owl executes on in Chapter 
 7
 . The 
 first important backend we consider is hardware accelerators such as GPUs. Scientific 
 computing often involves intensive computations, and a GPU is important to accelerate 
 these computations by performing parallel computation on its massive cores. Besides, 
 there are growingly more types of hardware accelerators. For example, Google 
 develops the Tensor Processing Unit (TPU), specifically for neural network machine 
 learning. It is highly optimized for large batches in using TensorFlow. To improve 
 performance of a numerical library such as Owl, it is necessary to support multiple 
 hardware platforms.
  
 One idea is to “freeride” existing libraries that already support various hardware 
 platforms. We believe that a computation graph is a suitable intermediate 
 representation (IR) to achieve interoperability between different libraries. Along this 
 line, we develop a prototype symbolic layer system by using which the users can define a 
 computation in Owl and then turn it into an ONNX structure, which can be executed on 
 many different platforms such as TensorFlow. By using the symbolic layer, we show the 
 system workflow and how powerful features of Owl, such as algorithmic differentiation, 
 can be used in TensorFlow.
  
 Besides aiming for maximal performance on accelerators, sometimes a numerical 
 library also targets to run in a wide range of environments, for example, to execute on 
 web browsers based on JavaScript for ease of access to many end users. Besides, with 
 the trending of edge computing and the Internet of Things, it is also crucial for numerical 
 libraries to support running on computation resource–limited devices. One approach is 
 to construct 
 unikernel
 , a specialized minimal virtual machine image that only contains 
 necessary libraries and modules to support an application. In this book, we also 
 introduce Owl’s support for execution in this environment.
  
 6",NA
 Research on Owl,"In the last part of this book, we introduce two components in Owl: 
 Zoo
 , for service 
 composition and deployment, and 
 Actor
 , for distributed computing. The focus of these 
 two chapters is to present two pieces of research based on Owl.
  
 In Chapter 
 9
 , we introduce the Zoo subsystem. It was originally developed to share 
 OCaml scripts. It is known that we can use OCaml as a scripting language as Python (at 
 certain performance cost because the code is compiled into bytecode). Even though 
 compiling into native code for production use is recommended, scripting is still useful 
 and convenient, especially for light deployment and fast prototyping. In fact, the 
 performance penalty in most Owl scripts is almost unnoticeable because the heaviest 
 numerical computation part is still offloaded to Owl which runs native code. 
  
 While designing Owl, our goal is always to make the whole ecosystem open, flexible, 
 and extensible. Programmers can make their own “small” scripts and share them with 
 others conveniently, so they do not have to wait for such functions to be implemented 
 in Owl’s master branch or submit something “heavy” to OPAM. Based on these basic 
 functionalities, we extend the Zoo system to address the computation service 
 composition and deployment issues.
  
 Next, we discuss the topic of distributed computing. The design of distributed and 
 parallel computing module essentially differentiates Owl from other mainstream 
 numerical libraries. For most libraries, the capability of distributed and parallel 
 computing is often implemented as a third-party library, and the users have to deal with 
 low-level message passing interfaces. However, Owl achieves such capability through its 
 Actor subsystem. Distributed computing includes techniques that combine multiple 
 machines through a network, sharing data and coordinating progresses. With the fast- 
 growing number of data and processing power they require, distributed computing has 
 been playing a significant role in current smart applications in various fields. Its 
 application is extremely prevalent in various fields, such as providing large computing 
 power jointly, fault-tolerant databases, file system, web services, and managing large- 
 scale networks, etc.
  
 In Chapter 
 10
 , we give a brief bird’s-eye view of this topic. Specifically, we introduce 
 an OCaml-based distributed computing engine, Actor. It has implemented three 
 mainstream programming paradigms: parameter server, map-reduce, and peer-to-peer. 
 Orthogonal to these paradigms, Actor also implements all four types of synchronization 
  
 7",NA
1.3  Summary,"In this chapter, we introduced the theme of this book, which centers on the design of 
 Owl, a numerical library we have been developing. We briefly discussed why we build 
 Owl based on the OCaml language. And then we set a road map for the whole book, 
 which can be categorized into four parts: basic building block, advanced analytics, 
 execution in various environments, and research topics based on Owl. We hope you will 
 enjoy the journey ahead!
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the 
 chapter’s Creative Commons license, unless indicated otherwise in a credit line to the 
 material. If material is not included in the chapter’s Creative Commons license and your 
 intended use is not permitted by statutory regulation or exceeds the permitted use, you 
 will need to obtain permission directly from the copyright holder.
  
 8",NA
CHAPTER 2,NA,NA
Core Optimizations,"Perhaps one of the most important questions in a numerical library or software is, “how 
 to make it run faster?” You can never have a piece of scientific computation program that 
 runs too fast or takes too little memory. That is surely also the primary concern when we 
 are designing Owl. In this chapter, we discuss the optimization of the Ndarray module, 
 the core module that underlies almost all computation in Owl. We first introduce this 
 module and how it interfaces to C code. Next, we briefly introduce the basic principles in 
 optimizing numerical code, including some common techniques. Then we use the code 
 examples in Owl to demonstrate how these techniques are applied in Owl. We finish this 
 chapter with a bit of touch on the topic of automatic performance tuning in numerical 
 libraries.",NA
2.1  N-Dimensional Array in Owl,"Owl numerical functions are organized in multiple modules, such as linear algebra, 
 signal processing, statistics, optimization, etc. In real-world applications though, it is 
 quite rare for users to only deal with scalar values in data analytics. In linear algebra, 
 matrices are a common data object. In a neural network, the input images are often 
 represented by three-dimensional arrays, each element being in the range of [0, 255]. 
 Indeed, a solid implementation of n-dimensional array, or ndarray, lies at the heart of 
 any industry-level numerical library, such as NumPy, Julia, MATLAB, etc.
  
 The Ndarray module provides this core data structure in Owl. It relies on the 
 Bigarray module provided in OCaml. Compared to normal OCaml arrays, a Bigarray is 
 not limited in size, supports up to 16 dimensions, and supports more space-efficient 
 storage. Here shows the definition of an Ndarray type:
  
 type ('a, 'b) t = ('a, 'b, c_layout) Genarray.t
  
 © Liang Wang, Jianxin Zhao 2023 
  
 9
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5_2",NA
2.2  Interface OCaml to C,"To ensure its performance, we implement the core computation in the Ndarray module 
 mostly in the C language and then interface them to OCaml. In this section, we briefly 
 introduce how it is done. The corresponding code is mostly included in the src/owl/ 
 core/ directory in the source code. Let’s use the sine function in the Ndarray module as 
 an example. This OCaml function is a wrapper of another OCaml function _owl_sin:
  
 let _owl_sin : type a b. (a, b) kind -> int ->
  
  (a, b) 
 owl_arr -> (a, b) owl_arr -> unit =
  
  fun k l x y ->
  
   
  match k with
  
   
  | Float32   -> owl_float32_sin l x y
  
   
  | Float64   -> owl_float64_sin l x y
  
   
  | Complex32 -> owl_complex32_sin l x y
   
  
 | Complex64 -> owl_complex64_sin l x y
  
 This function serves as an entrance to implementations of different sine functions. 
 As we have introduced, Owl provides four data types: float, double, complex float, and 
 complex double. This function takes a data type as input and applies the sine function 
 on l elements in input array x, and the output is stored in array y. One of the called 
 functions, for example, owl_float32_sin, is declared as an external function that 
 interfaces from outside the OCaml world, a C function called “float32_sin”.
  
 external owl_float32_sin : int
  
  -> (`a, `b) owl_arr
  
  -> (`a, `b) owl_arr -> unit = ""float32_sin""
  
 12",NA
2.3  Core Optimization of Computation,"Ever since the creation of computers, one question kept resurfacing from time to 
 time: “How to improve the performance of a piece of code?” Back in the early days of 
 computers, using computers was a privilege, and users had to utilize every second of 
 their limited computing time to the utmost. And nowadays, every PC gamer wants 
 high-quality visual effects. Not to mention those scientific researchers who would be 
 overjoyed if their simulation program running time can be cut by half.
  
 In the good old days, the answer was simple. We can simply freeride Moore’s Law: 
 “the number of transistors in a dense integrated circuit doubles about every two years.” 
 A similar exponential increase shows in quite a lot of related areas. With more CPU clock 
 speed, any code can automatically run faster, not to mention the dramatic increase of 
 memory and storage capacity. All you need to do is to wait for a while.
  
 However, the free lunch is over, and the clock speed has stagnated for years.
 2
  Faced 
 with this situation, programmers need to achieve performance gains in fundamentally 
 different ways on modern hardware. It generally includes two aspects: utilizing 
 parallel computing mechanisms provided in processors and reducing accessing data 
 from slow storage media.
  
 Figure 
 2-1
  shows a much simplified and yet thorough architecture illustration of a 
 quad-core CPU.
 3
  It shows some of the core components in a processor. The CPU follows 
 a “fetch-decode-execute” cycle to process instructions. It basically goes as described as 
 follows. When the program counter indicates the next instruction to be executed, the 
 Fetch/Decode
  fetches the instruction value from memory and decodes it into actions the 
 computer can understand. Next, the 
 Arithmetic Logic Unit
  (ALU) is invoked to 
  
 2
 Microprocessor chronology
 , Wikipedia, URL: 
 https://en.wikipedia.org/wiki/ 
  
 Microprocessor_chronology 
  
 3
  This figure is inspired by the CMU 15-418/15-618 course 
 Parallel Computer Architecture and 
 Programming
 , which is recommended to anyone who is serious about learning parallel 
 computing.
  
 15",NA
 CPU Support of Parallel Computing,NA,NA
 Instruction-Level Parallelism,"A program consists of a stream of instructions. In a basic processor model, it executes 
 one instruction in one clock cycle. But instructions do not strictly sequentially depend 
 on one another. Instead, some of them can be executed in parallel without interference. 
 Architecture that exploits such instruction-level parallelism (ILP) is called 
 superscalar
 , 
 which can date back to the 1960s. It can execute more than one instruction per clock 
 cycle by dispatching multiple instructions at the same time. Therefore, the processor 
 contains multiple fetch/decode and ALU modules, as Figure 
 2-2
  shows. The parallelism 
 among an instruction stream is automatically and dynamically discovered by the 
 hardware during execution and thus is not visible to programmers.
  
  
  
  
  
  
 (a) Scalar operation 
  
 (b) SIMD operation
  
 Figure 2-3. 
 Comparison of computing conventional and SIMD",NA
 Vectorization,"The ALU also has the potential to provide more than one piece of data in one clock cycle. 
 The 
 single instruction multiple data
  (SIMD) refers to a computing method that uses one 
 single instruction to process multiple data. It is in contrast with the conventional 
 computing method that processes one piece of data, such as load, add, etc., with one 
 construction. The comparison of these two methods is shown in Figure 
 2-3
 . In this 
 example, one instruction processes four float numbers at the same time.
  
 17",NA
 Context Switching,"As we have explained, the execution context in a processor contains necessary state 
 information when executing instructions. But that does not mean one processor can only 
 have one execution context. For example, the Intel Core i9-9900K contains two 
 execution contexts, or “hardware threads.” That provides the possibility of concurrent 
 processing. Specifically, a core can apply 
 context switching
  to store execution state in one 
 context while running instructions on the other if possible.
  
 Note that unlike previous methods we have introduced, context switching does not 
 enable parallel processing at exactly the same cycle; a core still has one ALU unit to 
 process instructions. Instead, at each clock a core can choose to run an instruction on an 
 available context. This is especially useful to deal with instruction streams that contain 
 high-latency operations such as memory read or write. As shown in Figure 
 2-4
 , while 
 one instruction is executing, perhaps waiting for a long while to read some data from 
 memory, the core can switch to the other contexts and run another set of instructions. In 
 this way, the execution latency is hidden and increases overall throughput.
  
 19",NA
 Multicore Processor,"What we have introduced so far focuses on one single core. But another idea about 
 improving computation performance is more straightforward to a wider audience: 
 multicore processor. It means integrating multiple processing units, or cores, on one 
 single processor and enables the possibility of parallel processing at the same time. The 
 processor development trend is to add more and more cores in a processor. For 
 example, Apple M1 contains eight cores, with four high-performance cores and four 
 high- efficiency cores. Intel Core i9-12900HX processor contains 16 cores.
  
 Similar to previous mechanisms, just because a processor provides the 
 possibility of parallel processing does not mean a piece of code can magically 
 perform better by itself. The challenge is how to utilize the power of multiple cores 
 properly from the OS and applications’ perspective. There are various approaches 
 for programmers to take advantage of the capabilities provided by multicore 
 processors. For example, on Unix systems the IEEE 
 POSIX
  1003.1c standard 
 specifies a standard programming interface, and its implementations on various 
 hardware are called POSIX threads, or Pthreads. 
  
 It manages threads, such as their creation and joining, and provides synchronization 
 primitives such as mutex, condition variable, lock, barrier, etc. The OCaml language is 
 also adding native support for multicore, including parallelism via the shared memory 
 parallel approach and concurrency. It will be officially supported in the OCaml 5.0 
 release.
  
  
  
  
  
  
 Figure 2-5. 
 Memory hierarchy",NA
 Memory,"Besides the processor architecture, another core component, memory, also plays a 
 pivotal role in the performance of programs. In the rest of this section, we introduce 
 several general principles to better utilize memory properties. For more detailed 
 knowledge about this topic, we highly recommend the article by U. Drepper [18].
  
 Ideally, a processing unit only needs to access one whole memory, as the Von 
 Neumann architecture indicates. However, such a universal memory would struggle to 
 meet various real-world requirements: permanent storage, fast access speed, cheap, etc. 
 Modern memory design thus has adopted the “memory hierarchy” approach which 
 divides memory into several layers, each layer being a different type of memory, as 
 shown in Figure 
 2-5
 . Broadly speaking, it consists of two categories: (1) internal 
 memory that is directly accessible by the processor, including CPU registers, cache, and 
 main memory; (2) external memory that is accessible to the processor through the I/O 
 module, including flash disk, traditional disk, etc. As the layer goes down, the storage 
 capacity increases, but the access speed and cost also decrease significantly.
  
 The register is the closest to a computer’s processor. For example, an Intel Xeon Phi 
 processor contains 16 general-purpose registers and 32 floating-point registers, each of 
 64-bit size. It also contains 32 registers for AVX instructions; the size of each is 256 or 
 512 bits. The access speed to registers is the fastest in the memory hierarchy, only 
 taking one processor cycle. The next level is caches of various levels. In Figure 
 2-1
 , we 
 have seen how a processor core connects directly to the various levels of caches. On an 
 Intel Core i9-9900K, the cache size is 64KB (L1, each core), 256KB (L2, each core), and 
 16MB (shared), respectively. Its access speed is about ten cycles, with L1 being the 
 fastest.",NA
 Cache,"Due to the fast access time of cache compared to memory, utilizing cache is key to 
 improving performance of computing. Imagine that if only all a program’s data accesses 
 are directly from cache, its performance will reach orders of magnitude faster. Short of 
 reaching that ideal scenario, one principle is to utilize cache as much as possible. 
 Specifically, we need to exploit the 
 locality
  of data access in programs, which means that 
 a program tends to reuse data that is “close” to what it has already used. The meaning of 
 “close” is twofold: first, recently used data is quite likely to be used again in the near 
 future, called 
 temporal locality
 ; second, data with nearby addresses are also likely to be 
 used recently. Later, we will discuss techniques based on these principles.
  
 21",NA
 Prefetching,"To mitigate the long loading time of memory, 
 prefetching
  is another popular approach. 
  
 As the name suggests, the processor fetches data into cache before it is demanded, so 
 that when it is actually used, the data can be accessed directly from the cache. 
 Prefetching can be triggered in two ways: via certain hardware events or explicit request 
 from the software.
  
 Naturally, prefetching faces challenges from two aspects. The first is to know what 
 content should be prefetched from memory. A cache is so precious that we don’t want 
 to preload useless content, which leads to a waste of time and resources. Secondly, it is 
 equally important to know when to fetch. For example, fetching content too early risks 
 getting it removed from the cache before even being used.
  
 For hardware prefetching, the processor monitors memory accesses and makes 
 predictions about what to fetch based on certain patterns, such as a series of cache 
 misses. The predicted memory addresses are placed in a queue, and the prefetch would 
 look just like a normal READ request to the memory. Modern processors often have 
 different prefetching strategies. One common strategy is to fetch the next 
 N
  lines of data. 
  
 Similarly, it can follow a stride pattern: if currently the program uses data at address 
 x, then prefetch that at x+k, x+2k, x+3k, etc.
  
  
 Compared with the hardware approach, software prefetching allows control from 
 programmers. For example, a GCC intrinsics is for this purpose:
  
 void __builtin_prefetch (const void *addr, ...)
  
 It contains three arguments. The first is the data address to be prefetched; the 
 second is a compile-time integer that indicates if the prefetch is preparing for a read 
 from or write to memory; and the final one indicates the temporal locality of fetched 
 data to decide if it should be evicted from cache once accessed.
  
 The programmer can insert __builtin_prefetch into code if the corresponding data is 
 anticipated to be accessed soon. This intrinsic will be compiled into data prefetch 
 instructions via the compiler. If the prefetch is executed at a proper moment before the 
 access, ideally the required data will be already in the cache by the time it is used. The 
 following code is a simple example to demonstrate how it works in a C code:
  
 24",NA
 NUMA,"Finally, we briefly talk about non-uniform memory access (NUMA), since it 
  
 demonstrates the hardware aspect of improving memory access efficiency. We 
  
 have mentioned the multicore design of processors. While improving parallelism in 
 processors, it has also brought challenges to memory performance. When an application 
 is processed on multiple cores, only one of them can access the computer’s memory at a 
 time, since they access a single entity of memory uniformly. In a memory-intensive 
 application, this leads to a contention for the shared memory and a bottleneck. One 
 approach to mitigate this problem is the non-uniform memory access (NUMA) design. 
  
 25",NA
2.4  Optimization Techniques,"The topic of computation optimization is a classic topic in computer science, and there is 
 still a lot of work about it in both academia and industry. Explaining even only a part of 
 them in detail requires a whole book. Instead, in this section we give some optimization 
 technique examples to demonstrate how the principles in the previous sections are 
 implemented.",NA
 Hardware Parallelization,"Utilizing multicore is a straightforward way to improve computation performance, 
 especially complex computation on arrays with a huge amount of elements. Except for 
 the Pthread we have introduced, Open Multi-Processing (OpenMP) is another tool that is 
 widely used. OpenMP is a library that provides APIs to support shared memory 
 multiprocessing programming in C/FORTRAN languages on many platforms. An OpenMP 
 program uses multiple threads in its parallel section, and it also sets up the environment 
 in the sequential execution section at the beginning. The parallel section is marked by 
 the OpenMP directive omp pragma. Each thread executes the parallel section and then 
 joins together after finishing. Here is an example:
  
 #include <omp.h>
  
 int main(int argc, char **argv) {
  
  
  int x[200000];
  
  
  #pragma omp parallel for
  
  
  for (int i = 0; i < 100000; i++) {
  
  
  x[i] = sinf(i);
  
  
  }
  
  
  return 0; 
  
 }
  
 26",NA
 Cache Optimization,"There are numerous cache optimization techniques, but most of them share the same 
 theme: improve data locality (both spatial and temporal) and align the code and data. 
  
 27",NA
 Other Techniques,"Besides processor parallelism and cache utilization, there are still many techniques to 
 improve code performance. We will only briefly introduce some of them in this part.
  
  
 Compilers surely have a great impact on the code performance. For example, 
 compilers such as LLVM or GCC can be configured with plenty of options and flags. 
  
 Choosing the most suitable options can actually be a challenging task. Besides, 
  
 programmers can add inline assembly code to C to further increase the execution speed. 
  
 Another optimization technique, 
 unrolling
 , is also partly about understanding how 
 compilers work. For example, we can unroll the for-loop into eight parts:
  
 for(i=0;i<n;i++) {
  
  
  a[i] = b[i] + 1 
  
 } 
  
 for(i=0; i<n; i+=8) {
  
  
  a[i] = b[i] + 1
  
  
  a[i] = b[i+1] + 1
  
  
  a[i] = b[i+2] + 1
  
  
  a[i] = b[i+3] + 1
  
  
  a[i] = b[i+4] + 1
  
  
  a[i] = b[i+5] + 1
  
  
  a[i] = b[i+6] + 1
  
  
  a[i] = b[i+7] + 1 
  
 }
  
  
 It allows the compiler to decrease the number of conditional branches, thus reducing 
 potential branch mispredictions and condition evaluations.
  
 Despite what we have explained, note that cache is not always helping. Sometimes, 
 the data is put into cache, but won’t be used again in a short while. That means the cache 
 is just wasting time on writing without being read. In that case, it is necessary to bypass 
 the caching phase. Processors support nontemporal write directly to memory. SIMD also 
 provides intrinsics to do that, such as the following intrinsics.
  
 31",NA
2.5  Example: Convolution,"Convolution is a family of mathematical operations that is arguably the most important 
 operation in deep neural networks. It makes up the backbone of a majority of deep 
 neural network architectures and takes up a large part of computation resources 
 involved in their training and inference. According to the shape of input, convolution 
 operations can be categorized into one dimensional, two dimensional, and three 
 dimensional. It can also be categorized according to usage in the forward or backward 
 propagation phase as normal convolution, backward convolution on kernel, and 
 backward convolution on input. There are special operations such as transpose 
 convolution, dilated convolution, etc. But their implementation principles are quite 
 similar. There is a lot of work on optimizing convolution operations due to their 
 importance [55]. It takes significant engineering effort to implement only part of them. 
 In this section, we use the two-dimensional convolution operation Conv2D as an 
 example to demonstrate how we apply various optimization techniques on convolution 
 operations in Owl.
  
 A convolution operation takes two ndarrays as input: image (
 I
 ) and kernel (
 F
 ). In a 
 two-dimensional convolution, both ndarrays are of four dimensions. The image ndarray 
 has 
 B
  batches; each image has size 
 H
  × 
 W
  and has 
 IC
  channels. The kernel ndarray has 
 R 
 rows, 
 C
  columns, the same input channel 
 IC
 , and output channel 
 K
 . The convolution can 
 then be expressed as in Eq. 
 2.1
 .
  
 32",NA
∑∑∑,"I
  
 ic 
 = 1 
 r 
 = 1 
 c 
 = 
 1
  
 b h r w c ic 
 + 
 +
  
 F 
 r c ic k 
 ,
 . 
 (2.1)
  
 The convolution operation is first implemented in Owl by interfacing to the Eigen 
 library, which is also used in TensorFlow for convolution implementation on the CPU. 
 However, interfacing to this C++ library proves to be problematic and leads to a lot of 
 installation issues. That is why we turn to handcrafting convolution operations. 
  
 They consist of three types: Conv, ConvBackwardKernel, and ConvBackwardInput. 
  
 The Conv operation calculates the output given the input image and kernel. Similarly, 
 ConvBackwardKernel calculates the kernel given the input and output ndarrays, and 
 ConvBackwardInput gets input ndarray from the kernel and output. The last two are 
 mainly used in the backpropagation phase in training a DNN, but all three operations 
 share a similar calculation algorithm.
  
 A naive convolution algorithm is to implement Eq. 
 2.1
  with nested for-loops. It is 
 easy to see that this approach does not benefit from any parallelization and thus is not 
 suitable for production code.
  
  
 The next version of implementation uses the im2col algorithm. This algorithm is 
 illustrated in Figure 
 2-8
 . In this example, we start with an input image of shape 4x4 and 
 three output channels. Each channel is denoted by a different color. Besides, the index of 
 each element is also shown in the figure. The kernel is of shape 2x2 and has three input 
 channels as the input image. Each channel has the same color as the corresponding 
 channel of the input image. The two output channels are differentiated by various levels 
 of transparency in the figure. According to the definition of convolution operation, we 
 use the kernel to slide over the input image step by step, and at each position, an 
 element-wise multiplication is applied. Here, in this example, we use a stride of 1 and a 
 valid padding. In the first step, the kernel starts with the position where the element 
 indices are [1,2,5,6] in the first input channel, [17,18,21,22] in the second input channel, 
 and [33,34,37,38] in the third input channel. The element-wise multiplication result is 
 filled into the corresponding position in the output ndarray. Moving on to the second 
 position, the input indices become [2,3,6,7,18,19,22,23,34,35,38,39], so on and so forth. 
 It turns out that this process can be simplified as one matrix multiplication. The first 
 matrix is just the flattened kernel. The second matrix is based on the input ndarray. Each 
 column is a flattened subblock of the same size as one channel of the kernel. This 
 approach is the basic idea of the im2col algorithm. Since the matrix multiplication is a 
 highly optimized operation in linear algebra packages such as OpenBLAS, this algorithm 
 can be executed efficiently. Here, we show its implementation code: 
  
  
  
 33",NA
2.6  Automated Empirical Optimization of ,NA,NA
Software,"We have introduced various optimization techniques so far, and together they comprise 
 a very large optimization space. For example, we can apply both multicore and SIMD 
 parallelism while utilizing certain cache techniques. We need to consider many different 
 methods to apply, each with numerous possible parameters to tune. Sometimes, these 
 techniques even conflict with each other, for example, each trying to take as much 
 processor resource as possible. Worse still, with the heterogeneity of hardware devices, 
 an “optimal” configuration on one machine may lead to degraded performance. It is thus 
 of utmost importance for a numerical library to provide good performance on all devices 
 it supports. For example, ATLAS and the recent Intel Math Kernel Library both provide 
 optimized mathematical routines for science and engineering computations. They are 
 widely used in many popular high-level platforms such as Matlab and TensorFlow. 
  
 One of the reasons these libraries can provide good performance is that they have 
 adopted the paradigm of 
 Automated Empirical Optimization of Software
 , or AEOS. That 
 is, a library chooses the best method and parameter to use on a given platform to do a 
 required operation. One highly optimized routine may run much faster than a naively 
 coded one. Optimized code is usually platform and hardware specific, so an optimized 
 routine on one machine could perform badly on the other. Though Owl currently does 
 not plan to improve the low-level libraries it depends on, as an initial endeavor to 
 apply the AEOS paradigm in Owl, one ideal tuning point is the parameters of OpenMP 
 used in Owl.
  
 42",NA
2.7  Summary,"In this chapter, we focused on the optimization of core ndarray operations in Owl. We 
 started by introducing the Ndarray module in Owl and its pivotal role in a numerical 
 library and then introduced how we interface the OCaml code to the C language. The rest 
 of this chapter mostly focused on optimizations at the C level. As an important 
 background, we explained the principles in optimizing scientific computing code, such as 
 utilizing parallelism of processors and locality of caches. Next, we briefly introduced 
 some techniques based on these principles. As an example, we demonstrated how we 
 apply some of them to optimize one of the most important operations in deep neural 
 networks: the convolution. Finally, we briefly introduced the automatic tuning approach 
 to optimize library performance across various platforms, using multicore parallel 
 computing on Owl as an example.
  
 47",NA
CHAPTER 3,NA,NA
Algorithmic ,NA,NA
Differentiation,"Differentiation is key to numerous scientific applications including maximizing or 
 minimizing functions, solving systems of ODEs, physical simulation, etc. Of existing 
 methods, algorithmic differentiation, or AD, is a computer-friendly technique for 
 performing differentiation that is both efficient and accurate. AD is a central component 
 of the architecture design of Owl. In this chapter, we will show, with hands-on examples, 
 how the AD engine is designed and implemented in Owl. AD will be used in some of the 
 other chapters to show its application in optimization and machine learning.",NA
3.1  Introduction,"Assume an object moves a distance of 
 Δs
  in a time 
 Δt
 , the average velocity of this object 
 during this period can be defined as the ratio between 
 Δs
  and 
 Δt
 . As both values get 
 smaller and smaller, we can get the instantaneous velocity:
  
 v
  = lim 
 ∆
 s
  
  
 = 
 ds
  
 (3.1)
  
  
  
 t
  
 →
  
 0
 ∆
 t
  
  
  
 dt
  
  
 The term 
 ds dt
  is referred to as “the 
 derivative
  of 
 s
  with respect to 
 t
 .”
  
 Differentiation
  is the process of finding a derivative in mathematics. It studies the 
 functional relationship between variables, that is, how much one variable changes when 
 the value of another variable changes. Differentiation has many important applications, 
 for example, finding minimum and maximum values of a function, finding the rate of 
 change of quantity, computing linear approximations to functions, and solving systems 
 of differential equations. Its critical roles in these key mathematical fields mean it is 
 widely used in various fields. One example is calculating marginal cost and revenue in 
 economics.",NA
 Three Ways of Differentiating,"If we are to support differentiation at a scale as large as a deep neural network, manual 
 calculation based on the chain rule in Calculus 101 is far from being enough. To do that, 
 we need the power of automation, which is what a computer is good at. Currently, 
 there are three ways to calculate differentiation: numerical differentiation, symbolic 
 differentiation, and algorithmic differentiation.
  
 The first method is 
 numerical differentiation
 . Derived directly from the 
 definition, numerical differentiation uses a small step 
 δ
  to compute an approximate 
 value toward the limit, as shown in Eq. 
 3.2
 .
  
 f
  
 ′",NA
( ),"= 
 lim 
 f x",NA
( ,+ δ,NA
),−,NA
 ( ),". 
 (3.2)
  
  
  
 δ→0
   
 δ 
  
  
 By treating the function 
 f
  as a black box, this method is straightforward to implement 
 as long as 
 f
  can be evaluated. However, this method is unfortunately subject to multiple 
 types of errors, such as the round-off error. It is caused by representing numbers with 
 only a finite precision during the numerical computation. The round-off error can be so 
 large as to that the computer thinks 
 f
  (
 x
  + 
 δ
 ) and 
 f
  (
 x
 ) are identical.
  
 The second is 
 symbolic differentiation
 . By manipulating the underlying 
  
 mathematical expressions, symbolic differentiation obtains analytical results without 
 numerical approximation, using mathematical derivative rules. For example, consider 
 the function 
 f
  (
 x
 0
 , 
 x
 1
 , 
 x
 2
 ) = 
 x
 0
 ∗
 x
 1
 ∗
 x
 2
 . Computing ∇
 f
  symbolically, we end up with
  
 ∇ = 
  ∂
 f
   
 , 
 ∂
 f
  
 , 
 ∂
 f
   
  
  =",NA
(,"x
  ∗ 
 x x
   ∗ 
 x x
  ∗ 
 x",NA
),"∂
 x
  
 0
   ∂
 x 
 1
  
  ∂
 x
  
 2
   
  
  
 1
   
 2
  
 0
   
 2 1
    
 2
   
 50",NA
( ),"= 
 n
 −1",NA
∏,"x
  
 i 
 = 0
  
 i
  
 : the 
  
 result would be terribly long, if not that complex. As a result, symbolic differentiation 
 can easily consume a huge amount of computing resource and becomes impractically 
 slow in the end. Besides, unlike in numerical differentiation, we must know how a 
 function is constructed to use symbolic differentiation.
  
 Finally, there is the 
 algorithmic differentiation
  (AD). It is a chain rule–based 
 technique for calculating derivatives with respect to input variables of functions defined 
 in a computer program. Algorithmic differentiation is also known as automatic 
 differentiation, though strictly speaking it does not fully automate differentiation and 
 can sometimes lead to inefficient code. In general, AD combines the best of both worlds: 
 on one hand, it efficiently generates exact results and so is highly applicable in many 
 real-world applications; on the other hand, it does not rely on listing all the intermediate 
 results, and its computing process can be efficient. Therefore, it is the mainstream 
 implementation of many numerical computing tools and libraries, such as JuliaDiff in 
 Julia, ad in Python, ADMAT, etc. The rest of this chapter focuses mainly on algorithmic 
 differentiation.",NA
 Architecture of Algodiff Module,"The Algodiff module plays a pivotal role in the whole Owl library stack. It unifies 
 various fundamental data types in Owl and is the most important functionality that 
 serves the advanced analytics such as regression, neural networks, etc. Its design is 
 elegant thanks to OCaml’s powerful module system.
  
 51",NA
3.2  Types,"We start with type definition. The data type in AD is defined in the owl_algodiff_types. 
 ml file, as shown in the following. Even if you are familiar with the type system in OCaml, 
 it may still seem a bit confusing. The essence of AD type is to express the forward and 
 reverse differentiation modes. So first, we use an example to demonstrate how these 
 two AD modes work.
  
 module Make (A : Owl_types_ndarray_algodiff.Sig) = struct
  
  type 
 t =
  
   
  | F   of A.elt
  
   
  | Arr of A.arr
  
   
  | DF  of t * t * int
  
   
  | DR  of t * t ref * op * int ref * int * int ref
  
  
  and adjoint = t -> t ref -> (t * t) list -> (t * t) list
  
  and register = 
 t list -> t list
  
  
  and label = string * t list
  
  
  and op = adjoint * register * label 
  
 end
  
 53",NA
 Forward and Reverse Modes,"In this part, we illustrate the two modes of differentiation with an example, which is 
 based on this simple function:
  
 y x x",NA
( ,"0 
  
 1",NA
),=,NA
(,"sin 
 x x 
 0 
  
 1",NA
),"∂
 y
  , ∂
 y
   
  
 (3.3)
  
 This function takes two inputs, and our aim is to compute 
 ∇ = 
  
 ∂
 x
  
 0
   ∂
 x 
 1
  
  
 . 
  
  
 Computations can be represented as a graph shown in Figure 
 3-2
 . Each node 
 represents either an input/output or intermediate variables generated by the 
  
 corresponding mathematical function. Each node is named 
 v
 i
 . Herein, the input 
 v
 0
  = 
 x
 0 
 and 
 v
 1
  = 
 x
 1
 . The output 
 y
  = 
 v
 4
 .
  
 Both the forward and reverse modes rely on basic rules to calculate differentiation. 
 On 
  
 one hand, there are the basic forms of derivative equations, such as 
 d 
  
 dx
  
 sin",NA
( ),=cos,NA
( ),", 
  
 dx u x v x d",NA
( ) ( ),"= 
 u x v x
 ′",NA
( ) ( ),+,NA
 ( ) ( ),", etc. On the other is the 
 chain rule
 . It states that, 
  
 suppose we have two functions 
 f
  and 
 g
  that can be composed to create a function 
 F
 (
 x
 ) = 
 f
 (
 g
 (
 x
 )), then the derivative of 
 F
  can be calculated as
  
 F
  
 ′",NA
( ),"= 
 f
  
 ′",NA
( ,NA,NA
( ),NA,NA
),"g
  
 ′",NA
( ),"(3.4)
  
 Table 3-1. 
 The Computation Process of Forward Differentiation, Shown to Three 
 Significant Figures
  
  
 Primal Computation
  
 Tangent Computation
  
 0
  
 v
 0
  = 
 x
 0
  = 2
  
 v
 0
  
 = 
 1 
 1
  
 v
 1
  = 
 x
 1
  = 2
  
 v
 1
  
 = 
 0 
 2
  
 v
 2
  = 
 v
 0
 v
 1
  = 4
  
 v
 2
  = 
 v v 
 0 1
  
 + 
 v v 
 1
 0
  
 = ∗ + ∗ = 
 2 
 3
  
 v
  
 3
  = sin",NA
(,"v
  
 2",NA
),"= −0 757 
 y
  = 
 v
 3
  
 = cos",NA
(,"v
  
 2",NA
),"∗ 
 v
 2
  
 = −0 654 2∗ = −1 308 
 The question is how to implement them in a differentiation system.
  
 54",NA
 Forward Mode,"Let’s look at the first way, namely, the “forward” mode, to calculate derivatives. We 
  
 ultimately wish to calculate 
 ∂
 y
  
 ∂
 x
 0
  
  (and 
 ∂
 y
  
 ∂
 x
 1
  
 , which can be calculated in a similar way). 
  
 We begin by calculating some intermediate results that will prove to be useful. Using 
  
 the labels 
 v
 i
  to refer to the intermediate computations, we have 
 ∂
 v 
 = 
 ∂
 x 
 0
  
 1
  and 
 ∂
 v 
 = 
 ∂
 x 
 0
  
 0 
 immediately, since 
 v
 0
  = 
 x
 0
  and 
 v
 1
  = 
 x
 1
  actually.
  
 Next, consider 
 ∂
 v
  
 2
  
 , which requires us to use the derivative rule on multiplication. It 
  
 ∂
 x
  
 0
  
 is a bit trickier and requires the use of the chain rule:
  
 ∂
 v
  
 2
  = ∂",NA
( ,"x x 
 0 1",NA
),"= 
 x
  ∂",NA
(,"x
  
 0",NA
),"+ 
   
 ∂",NA
(,"x 
 1",NA
),"= 
 x
  
 ∂
 x
  
 0
  
 ∂
 x 
 0
    
 1
  ∂
 x
  
 0
     
 0
  ∂
 x
  
 0
    
 1
  
 After calculating 
 ∂
 v 
 2 
 , we proceed to compute partial derivatives of 
 v
 4
  which is the 
  
 ∂
 x 
 0
  
 final result 
 ∂
 y
  
  we are looking for. This process starts with the input variables and 
 ends 
  
 ∂
 x
 0
  
 with the output variables, and that’s where the name “forward differentiation” comes 
  
 from. We can simplify the notation by letting 
 v 
 i
  
 = ∂
 v 
 i
  
 ∂
 x 
 0
  
 . The 
 v
 i
   is called the 
 tangent
  of 
  
 function 
 v
 i
 (
 x
 0
 , 
 x
 1
 , …, 
 x
 n
 ) regarding the input variable 
 x
 0
 , and the results of evaluating the 
 function at each intermediate point are called the 
 primal value
 .
  
 Let’s calculate 
 y
   when setting 
 x
 0
  = 2 and 
 x
 1
  = 2. The full forward differentiation 
 calculation process is shown in Table 
 3-1
  where two simultaneous computation 
 processes take place in the two computation columns: the primal just performs 
 computation following the computation graph; the tangent gives the derivative for each 
 intermediate variable with regard to 
 x
 0
 .
  
 Two things need to be noted in this calculation process. The first is that in 
  
 algorithmic differentiation, unlike symbolic differentiation, the computation is 
  
 performed step by step, instead of after the whole computation is unwrapped into one 
 big formula following the chain rule. Second, in each step, we only need to keep two 
 values: 
 primal
  and 
 tangent
 . Besides, each step only needs to have access to its 
 “parents,” using graph theory’s term. For example, to compute 
 v
 2
  and 
 v
 2
  , we need to 
 know the primal and tangent of 
 v
 0
  and 
 v
 1
 ; to compute that of 
 v
 3, we need to know the 
 primal and tangent of 
 v
 2
 ; etc. These observations are key to our implementation.
  
 55",NA
Reverse Mode ,"Now let’s rethink about this problem from the other direction: from outputs to 
 inputs. 
  
 The problem remains the same, that is, to calculate 
 ∂
 y
  
 ∂
 x
 0
  
 . We still follow the same “step- 
  
 by- step” procedure as in the previous forward mode. The only difference is that this 
 time 
  
 we calculate it backward. For example, in our example 
 y
  = 
 v
  
 3
  = sin",NA
(,"v
  2",NA
),", so if only we 
  
 know 
 ∂
 y
  
 , we would move a step closer to our target solution.
  
 ∂
 v
 2
  
 We first observe that 
 ∂
 y 
 = 1
 , since 
 y
  and 
 v
 3
  are the same. We then compute 
 ∂
 y
  
 ∂
 v
 3
  
 ∂
 v
 2
  
  by 
  
 applying the chain rule:
  
 ∂
 y
  = ∂
 y
  ∗ 
 ∂
 v
  
 3
  
 = ∗ ∂ sin",NA
(,"v
  
 2",NA
),"= 
 co 
  
 v",NA
),". 
 (3.5)
  
 ∂
 v
  
 2
  ∂
 v 
 3
  ∂
 v
  
 2
   
 ∂
 v
  
 2
      
   
 2
  
   
  
 We can simplify it by applying a substitution:
  
 Table 3-2. 
 The Forward Pass in Reverse Mode
  
 Primal Computation
  
 0
  
 v
 0
  = 
 x
 0
  = 2
  
 = −0 757 
 1
  
 v
 1
  = 
 x
 1
  = 2
  
 2
  
 v
 2
  = 
 v
 0
 ∗
 v
 1 = 4
  
 3
  
 y
  = sin",NA
(,"v
  
 2",NA
),"= 
 v v 
 0 
  
 1
  
 56",NA
(,sin,NA
(,"v
  2",NA
),NA,NA
),cos,NA
(,"5
  
 v
  
 2
  
  
 v
  
 3
  
  
 = 
 =",NA
),"v
    
 6
  
  
 ∂
 v
  
 2
   
 ∂
 v
  
 2
      
  
 2
  
  
 v 
 1
  
  
 v
  
  
 ∂
 v
  
 2
  
  
 v
  
  
 ∂",NA
(,"v v 
 0 1",NA
),"= −0 
 654 
 ∗ 
 v
  
 0
  = −1 308 
  
 = 
 2
  
 = 
 2
  
 7
  
  
 ∂
 v 
 1
  
  
 ∂
 v 
 1
  
 v
  
  
 = 
 v
  
  
 ∂
 v
  
 2
  = 
 v
  
  
 ∂",NA
(,"v v 
 0 1",NA
),"= −0 
 654 
 ∗ 
 v 
 1
  
 = −1 308 
  
 0
   
 2
  ∂
 v 
 1
  
  
 2
  ∂
 v
  
 0
  
  
   
  
  
  
 v 
 i
  
 = ∂
 y
  
 ∂
 v 
 i
  
 for the derivative of output variable 
 y
  with regard to intermediate node 
 v
 i
 . 
 v
 i
   is called the 
 “
 adjoint
  of variable 
 v
 i
  with respect to the output variable 
 y
 .” Using this notation, Eq. 
 3.5 
 can be rewritten as
  
 v 
 2
  = 
 v 
 3
  
 ∗ ∂
 v 
 3
  
 ∂
 v 
 2
  
 = ∗ 
 cos",NA
(,"v 
 2",NA
),"Note the difference between tangent and adjoint. In forward mode, we know 
 v
 0
   and 
 v
 1
  and then calculate 
 v
 2
  , 
 v
 3
  , … until we get the target. In reverse mode, we start with 
  
 v
 n
  =1
  and calculate 
 v
 n
 −1
  , 
 v
 n
 −2
  , … until we have our target 
 v
  
 0
  
 = ∂
 y
  
 ∂
 v 
 0
  
 = ∂
 y
  
 ∂
 x 
 0
  
 . 
 v 
 3
  
 =
 0
  
  in this 
  
 example, given that we are talking about derivatives with respect to 
 x
 0
  when we use 
 v
 3
  . As a result, the reverse mode is also called the 
 adjoint mode
 .
  
 Following this procedure, we can now perform the complete reverse mode 
  
 differentiation. Note one major difference compared to the forward mode. In Table 
 3-1
 , 
 we can compute the primal and tangent in one pass, since computing one of them does 
 not require the other. However, as shown in the previous analysis, it is possible to 
 require 
  
 the value of 
 v
 2
  and possibly other previous primal values to compute 
 v
 2
  . Therefore, 
  
 57",NA
 Data Types,"Now that we understand the basic elements in computing a derivative, let’s turn to the 
 data type used in the AD system. It is built upon two basic types: scalar number F and 
 ndarray Arr. They are of type A.elt and A.arr. Here, A presents an interface that mostly 
 resembles that of an ndarray module. It means that their specific types, such as single or 
 double precision, C implementation or base implementation, etc., all depend on this A 
 ndarray module. Therefore, the AD module does not need to deal with all the lower-level 
 details. We will talk about how the AD module interacts with the other modules later in 
 this chapter. For now, it suffices to simply understand them as, for example, single- 
 precision float number and ndarray with single-precision float as elements, so as to 
 better grasp the core ideas in AD.
  
 1
  Not to be confused with the “forward differentiation mode” introduced before.
  
 58",NA
 Operations on AD Type,"After understanding the data type defined in AD, let’s take a look at what sorts of 
 operations can be applied to them. They are defined in the owl_algodiff_core.ml file. 
 The most notable ones are the “get” functions that retrieve certain information from an 
 AD type data, such as its primal, tangent, and adjoint values. In the following code, the 
 primal' is a “deep” function that recursively finds the primal value as float or ndarray 
 format.
  
 let primal = function
  
  | DF (ap, _, _)          -> ap
  
  | DR (ap, _, _, _, _, _) -> ap
  
  | ap                     -> ap
  
 let rec primal' = function
  
  | DF (ap, _, _)          -> primal' ap | DR (ap, _, _, _, 
 _, _) -> primal' ap | ap                     -> ap
  
 let tangent = function
  
  | DF (_, at, _) -> at
  
  | DR _          -> failwith ""error: no tangent for DR"" | ap            -> zero 
 ap
  
 let adjval = function
  
  | DF _                   -> failwith ""error: no adjval for DF"" | DR (_, at, _, _, _, _) -> !at
  
  | ap                     -> zero ap
  
 61",NA
3.3  Operators,"The graph is constructed with a series of operators that can be used to process AD type 
 data as well as building up a computation graph that is differentiable. They are divided 
 into submodules: Maths is the most important component, and it contains a full set of 
 mathematical functions to enable constructing various computation graphs; Linalg 
 contains a subset of linear algebra functions; NN contains functions used in neural 
 networks, such as two-dimensional convolution, dropout, etc.; Mat is specifically for 
 matrix operations, such as eye that generates an identity matrix; and Arr provides 
 functions such as shape and numel for ndarrays.
  
 As shown in Figure 
 3-1
 , the implementation of an operation can be abstracted into 
 two parts: (a) what the derivative and calculation rules of it are and (b) how these rules 
 are applied into the AD system. The first part is defined in the owl_algodiff_ops.ml, and 
 the latter is in owl_algodiff_ops_builder.ml.",NA
 Calculation Rules,"Let’s look at some examples from the first to see what these calculation rules are and 
 how they are expressed in OCaml. We can use the sine function as an example. It takes an 
 input and computes its sine value as output. This module specifies four computing rules, 
 each corresponding to one type of AD data. Here, module A is the underlying “normal” 
 ndarray module that implements functions for ndarray and scalar values. It can be single 
 precision or double precision, implemented using OCaml or C. For the F scalar type, ff_f 
 specifies using the sin function from the Scalar submodule of A. If the data is an AD 
 ndarray, ff_arr states that the sine functions should be applied on all of its elements by 
 using the A.sin function. Next, if the data is of type DF, the df function is used. As shown 
 in the example in Table 
 3-1
 , it computes 
 tangent (at) * derivative of primal (ap)
 . In the 
 case of the sine function, it computes at * cos ap. Finally, the dr computes what we have 
 shown in Table 
 3-3
 . It computes 
 adjoint (ca) * derivative of primal (a)
 . Therefore, here it 
 computes !ca * cos a. Using the get reference operator !ca is because the adjoint value in 
 the DR type is a reference that can be updated.
  
 module struct
  
  let label = ""sin""
  
  let ff_f a = F A.Scalar.(sin a)
  
  let ff_arr a = Arr A.(sin a)
  
 63",NA
(,x,NA
),"= 2 
 1 
 x
  
 .
  
 module struct
  
  
  let label = ""sqrt""
  
  
  let ff_f a = F A.Scalar.(sqrt a)
  
  
  let ff_arr a = Arr A.(sqrt a)
  
  
  let df cp _ap at = at / (pack_flt 2. * cp)
  
  let 
 dr _a cp ca = !ca / (pack_flt 2. * cp) end
  
   
 However, things get more complicated once an operator needs to deal with more 
 than one input. The problem is that for each of these four computation rules, we need to 
 consider multiple possible cases. Take the divide operation as an example. For a simple 
 primal value computation, we need to consider four cases: both inputs are scalar, both 
 are ndarray, and one of them is ndarray and the other is scalar. It corresponds to four 
 rules: ff_aa, ff_bb, ff_ab, and ff_ba. For the forward computation of tangent regarding 
 a 
  
 b
 , we also need to consider three cases:
  
   
  
 • 
  
 df_da corresponds to the derivative when the second input is 
  
   
   
 constant:
  
  
  
 a x",NA
( ,NA,NA
),"′
  
  
  = 
 a x
 ′",NA
( ),"(3.6)
  
 • 
  
  
  
 b
  
  
  
 b
  
  
 In code, it is at / bp. Here, at is the tangent of the first input 
 a
 ′
 (
 x
 ), 
  
 and bp is the primal value of the second input 
 b
 .
  
 • 
  
 df_db corresponds to the derivative when the first input is constant:
  
 (3.7)
  
  
  
 a
  
 ′
   = − 
 a
  
  
  
 b x
 ′",NA
( ),= −,NA
 ( ),"a
  
 1 
  
 , 
  
   
 b x",NA
( ,NA,NA
),b x,NA
( ,NA,NA
),2,NA
 ( ) ( ),64,NA
( ),"df_dab is for the case that both inputs are of nonconstant AD type, 
  
 • 
  
 that is, DF or DR. It thus calculates
  
 • 
  
  
  
  
 a x",NA
( ,NA,NA
),"′
  
  
  
 = 
 a x
 ′",NA
( ),−,NA
( ) ,"b x b 
 x
 ′",NA
( ),", 
 (3.8)
  
 b x",NA
( ,NA,NA
),b x,NA
( ),"And the corresponding code is (at-(bt*cp))/bp.
  
 Expressing the rules in computing the reverse mode is more straightforward. If both 
  
 inputs 
 a
  and 
 b
  are nonconstant, then the function dr_ab computes 
 a
  
 ∂ 
 y
  
  and 
 b
  
 ∂ 
  
 y
  
 , 
  
 where 
 y
  
 a 
 =
 b
  
 . Thus, dr_ab returns two values; the first is 
 a b
  
 ∂ 
 a
  
 ∂ 
  (!ca / b), and the 
  
  
 b
   
 second is 
 −
 a b
 2
   (!ca * (neg a / (b * b))). In the code, squeeze_broadcast x s is an 
  
 internal helper function that squeezes array x so that it has shape s. If one of the inputs is 
 constant, then we can just omit the corresponding result, as shown in dr_a and dr_b.
  
 module struct
  
  
  let label = ""div""
  
  
  let ff_aa a b = F A.Scalar.(div a b)
  
  
  let ff_ab a b = Arr A.(scalar_div a b)
  
  
  let ff_ba a b = Arr A.(div_scalar a b)
  
  
  let ff_bb a b = Arr A.(div a b)
  
  
  let df_da _cp _ap at bp = at / bp
  
  
  let df_db cp _ap bp bt = neg bt * cp / bp
  
  
  let df_dab cp _ap at bp bt = (at - (bt * cp)) / bp
  
  
  let dr_ab a b _cp ca =
  
   
  ( _squeeze_broadcast (!ca / b) (shape a)
  
   
  , _squeeze_broadcast (!ca * (neg a / (b * b))) (shape b) )
  
  let dr_a a b 
 _cp ca = _squeeze_broadcast (!ca / b) (shape a)
  
  let dr_b a b _cp ca 
 = _squeeze_broadcast
  
   
  (!ca * (neg a / (b * b))) (shape b) 
  
 end
  
 65",NA
 Generalize Rules into Builder Template,"So far, we have talked about the calculation rules, but there is still a question: how to 
 utilize these rules to build an operator of type t that we have described in Section 
 3.2
 . 
  
 To do that, we need to use the power of functor in OCaml. In the AD module in Owl, the 
 operators are categorized according to the number of inputs and outputs, each with its 
 own template. Let’s take the “single-input-single-output” (SISO) operators such as sine 
 as an example. This template takes a module of type Siso as input, as shown in the 
 following. Notice that the calculation rules of the sine function shown in the previous 
 section exactly forms such a module.
  
 66",NA
3.4  API,"The previous section introduces AD operators, the building blocks to construct an AD 
 computation graph. The next thing we need is an “engine” that begins the differentiation 
 process. For that purpose, we first introduce several low-level APIs provided by the AD 
 module and explain how they are used to build up user-friendly advanced APIs such as 
 diff and grad.",NA
 Low-Level APIs,"We differentiate between the two differentiation modes: forward mode and backward 
 mode. As explained in the previous section, if an input x is of type DF, then by applying 
 operations such as sin x, a computation graph is constructed, and the primal and tangent 
 values are also computed during this process. All we need to do is to retrieve the 
 required value once this process is finished. To start a forward mode differentiation, we 
 need to create a DF type data as initial input, using the primal value, the initial tangent 
 (equals to 1), and an integer tag as arguments:
  
 let make_forward p t i = DF (p, t, i)
  
  
 For example, if we are to calculate the derivative of 
 f
  =  
 sin
  (
 x
 2
 ) at 
 x
  = 2, we can 
 first create an initial point as
  
 let x = make_forward (pack_flt 2.) (pack_flt 1.) 1 let y = 
 Maths.(pow x (pack_flt 2.) |> sin) 
  
 let t = tangent y
  
  
 That’s it. Once the computation y is constructed, we can directly retrieve the tangent 
 value using the tangent function.
  
 70",NA
 High-Level APIs,"Based on the basic low-level APIs, we are able to build more high-level and easy-to- 
 access differentiation functions. The most commonly used function for differentiating is 
 diff in the AD module. Given a function f that maps one scalar value to another, we can 
 calculate its derivative at point x by diff f x. For example, given the triangular function 
 tanh, we can easily calculate its derivative at position 
 x
  = 0.1, as follows:
  
 open Algodiff.D 
  
 let f x = Maths.(tanh x);; 
  
 let d = diff f (F 0.1);;
  
 Its implementation using the forward mode low-level API is quite simple:
  
 let diff' f x =
  
  
  if not (is_float x) then
  
  
  
  failwith ""input must be a scalar"";
  
  
  let x = make_forward x (pack_flt 1.) (tag ()) in
  
  let y = 
 f x in
  
  
  primal y, tangent y
  
 let diff f x = diff' f x |> snd
  
 Next, we can generalize derivatives of scalar functions to gradients of multivariate 
 functions. For a function that maps a vector input to a scalar, the grad function calculates 
 its gradient at a given point. For example, in a three-dimensional space, the gradient at 
  
 73",NA
( ),"= 
  
 ∂
 x
  
 0
  
 ∂
 x 
 1
   
 ∂
 x
  
 n
  
 −1
   
  
  
  
  
  
  
 … 
  
 ∂
 y
  
 m
  
 −1
  
 ∂
 y
  
 m
  
 −1
  
 … 
 ∂
 y
  
 m
  
 −1
  
  
  
 ∂
 x
  
 0
  
 ∂
 x 
 1
   
 ∂
 x
  
 n
  
 −1
  
  
 The intuition behind the Jacobian is similar to that of the gradient. At a particular 
 point in the domain of the target function, the Jacobian shows how the output vector 
 changes given a small change in the input vector. Its implementation is as follows:
  
 let jacobianv' f x v =
  
  if shape x <> shape v
  
  then failwith ""jacobianv': vector not the same dimension as input""; let x = 
 make_forward x v (tag ()) in
  
 74",NA
 (,"∇
 2 
 f x
  
 f x",NA
( ,"n
  
 n",NA
),NA,NA
),", starting 
  
 from a random position and iterating until convergence according to Eq. 
 3.9
 . Its 
 implementation in OCaml is shown as follows:
  
 x
  
 n
  
 + 1
  = 
 x
  
 n
  − 
 α
 H
 1
 ∇",NA
 ( ,"f x
  
 n",NA
),"(3.9)
  
 open Algodiff.D
  
 let rec newton ?(eta=F 0.01) ?(eps=1e-6) f x =
  
  let g = grad f x in
  
  let h = hessian f x in
  
  if (Maths.l2norm' g |> unpack_flt) < eps then x
  
  else newton ~eta ~eps f Maths.(x - eta * g *@ (inv h))
  
 As an example, we can apply this method to a two-dimensional triangular function, 
 starting from a random initial point, to find a local minimum. Note that the newton 
 function takes a vector as input and outputs a scalar.
  
 let _ =
  
  let f x = Maths.(cos x |> sum') in
  
  newton f (Mat.uniform 1 2)
  
 75",NA
3.5  More Implementation Details,"Besides the main structure we have mentioned so far, there are some other details that 
 should be mentioned to build an industry-grade AD module. We introduce them in this 
 section.",NA
 Perturbation Confusion and Tag,"We have explained some of the fields in the DR type. But one of them is not covered yet: 
 tag of type int, which is used to solve a particular problem when calculating higher-
 order derivatives with nested forward and backward modes. This problem is referred 
 to as 
 perturbation confusion
 . It is crucial for an AD engine to function properly to 
 handle this problem. Here, we only scratch the surface of it. Let’s look at an example. 
 Suppose we want to compute the derivative of
  
 f x",NA
( ),"= 
 x d x",NA
( ,"+ 
 dy
  
 y",NA
),"that is, a function that contains another derivative function. It initially seems 
 straightforward, and we don’t even need a computer’s help: as 
 d x",NA
( ,"+ 
 y",NA
),"=1
   
 so 
  
  
 dy
  
 f
 ′
 (
 x
 ) = 
 x
 ′
  = 1. Unfortunately, applying the simple implementation without tag leads to 
 wrong answer.
  
 # let diff f x =
  
  
  match x with
  
  
  | DF (_, _)    ->
  
  
  
  f x |> tangent
  
  
  | DR (_, _, _) ->
  
  
  
  let r = f x in
  
 76",NA
 Lazy Evaluation,"We have seen how separating building template and operation definitions makes it 
 convenient to add new operations, simplifying code and improving productivity. But it 
 comes with a price: efficiency. Imagine a large calculation that contains thousands of 
 operations, with one operation occurring many times. Such situations are actually quite 
 common when using AD with neural networks where large computation graphs are 
 created that use functions such as add and mul many hundreds of times. With the 
 Builder approach described earlier, the operation will be recreated every time it is used, 
 which is rather inefficient. Fortunately, we can simply use OCaml’s lazy evaluation 
 mechanism to perform caching.
  
 val lazy: 'a -> 'a lazy_t
  
 77",NA
 Extending AD,"A significant benefit of the module design described earlier is that it can be easily 
 extended by providing modules representing new functions. For example, suppose 
 that the AD system did not support the natural logarithm, 
 sin
 x
  , whose derivative 
  
 is 
 sin 
 ′
  
 x 
 = cos 
 x
 . Including this function is a simple matter of defining the necessary 
  
 functions for calculating primal, tangent, and adjoint values in a module and applying 
 the relevant function from the Builder module – in this case, build_siso for building 
 “single input, single output” functions.
  
 open Algodiff.D
  
 module Sin = struct
  
  
  let label = ""sin""
  
  
  let ff_f a = F A.Scalar.(sin a)
  
  
  let ff_arr a = Arr A.(sin a)
  
  
  let df _cp ap at = Maths.(at * cos ap)
  
  
  let dr a _cp ca = Maths.(!ca * cos (primal a)) end
  
 let new_sin_ad = Builder.build_siso (module Sin : Builder.Siso)
  
  
 We can directly use this new operator as if it is a native operation in the AD module. 
 For example:
  
 79",NA
 Graph Utility,"Though not core functions, various utility functions provide convenience to users, for 
 example, tools to visualize the computation graph built up by AD. They come in handy 
 when we are trying to debug or understand how AD works. The core of the 
 visualization function is a recursive traverse routine:
  
 let _traverse_trace x =
  
  
  let nodes = Hashtbl.create 512 in
  
  
  let index = ref 0 in
  
  
  (* local function to traverse the nodes *)
  
  
  let rec push tlist =
  
  
  
  match tlist with
  
  
  
  | []       -> ()
  
  
  
  | hd :: tl ->
  
  
   
  if Hashtbl.mem nodes hd = false
  
  
   
  then (
  
  
    
  let op, prev =
  
  
     
  match hd with
  
  
     
  | DR (_ap, _aa, (_, _, label), _af, _ai, _) -> label
  
  
  
   
  | F _a -> Printf.sprintf ""Const"", []
  
  
     
  | Arr _a -> Printf.sprintf ""Const"", []
  
  
     
  | DF (_, _, _) -> Printf.sprintf ""DF"", []
  
  
    
  in
  
  
    
  (* check if the node has been visited before *)
  
  
    
  Hashtbl.add nodes hd (!index, op, prev);
  
  
    
  index := !index + 1;
  
  
    
  push (prev @ tl))
  
  
   
  else push tl
  
  
  in
  
 80",NA
3.6  How AD Is Built upon Ndarray,"We have been saying how the AD does not need to deal with the details of computation 
 implementation and thus can focus on the logic of differentiation. In previous examples, 
 we assume the A module to be any Ndarray module. In the final section of this chapter, 
 we will explain how the AD module is built upon the Ndarray modules. We hope to 
 illustrate the power of the functor system in promoting a modular style system design.
  
 First, the Ndarray module used in the AD module is not purely Ndarray as 
  
 introduced in Chapter 
 2
 , but also contains several other modules, including the scalar 
 functions, the ones that are specific to matrix and linear algebra. Together, they are 
 called the Owl_algodiff_primal_ops module. Based on the specific precision of the 
 modules included, it also divides the S and D submodules. For example, here are the 
 components of Owl_algodiff_primal_ops.S:
  
 module S = struct
  
  
  include Owl_dense_ndarray.S
  
  module Scalar = Owl_maths
  
  module Mat = struct
  
  let eye  = Owl_dense_matrix_s.eye
  
  let tril = Owl_dense_matrix_s.tril
  
  let triu = Owl_dense_matrix_s.triu
  
  ...
  
  end
  
  module Linalg = struct
  
  include Owl_linalg_s
  
 81",NA
3.7  Summary,"In this chapter, we discussed the design of one of the core modules in Owl: the 
  
 algorithmic differentiation module. We started from its basic theory and difference 
 among three types of differentiations. Then we presented the overall architecture of the 
 AD module in Owl. We explained several parts in detail in the following sections: the 
 definition of types in this system, the operators, and the APIs built on existing 
 mechanisms. We also discussed more subtle issues that should be paid attention to when 
 building an industry-level AD engine, such as avoiding the perturbation confusion issue 
 and using lazy evaluation to improve performance, graph visualization, etc. Finally, we 
 explained how the AD system is built upon the Ndarray module in Owl.
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 85",NA
CHAPTER 4,NA,NA
Mathematic,NA,NA
al ,NA,NA
Optimizatio,NA,NA
n,"Mathematical optimization is the process of searching for optimal values from a 
 selection of parameters, based on a certain metric. It can be formalized as follows:
  
 minimise 
 f x",NA
( ),"(4.1)
  
 → 
 subject to 
 g
  
 i",NA
( ),"< 
 b i 
 i
  = 
 1 2 , ,…, . 
 Here, vector 
 x
  = {
 x
 1
 , 
 x
 2
 , …, 
 x
 n
 } is the optimization variables, and function 
 f
  : 
 n
  
 is the target function. The functions 
 g
  
 i
  : 
  
 n
  
 →  , 
 i
  = 1 2…, 
 n
  
  are the constraints, 
  
 with constants 
 b
 i
  being the constraint boundaries. The target of solving 
 optimization problems is to find 
 x
 ∗ to minimize 
 f
 .
  
 An optimization problem aims to find a solution that minimizes some quantity; 
 therefore, it arises in a wide range of disciplines, such as finance, engineering, computer 
 science, etc. For example, in portfolio management in the finance industry, an optimal 
 solution is required to divide the given total capital into 
 n
  types of investments, where 
 x
 i 
 is the amount of capital invested in financial asset 
 i
 . The target might be to maximize to 
 the expected return or to minimize the risk. The constraints might be requiring that the 
 smallest return be larger than a predefined value, etc.
  
 An optimization problem can be categorized into multiple types. The general form in 
 Eq. 
 4.1
  contains several constraints. If there are no constraints, the problem is called an 
 unconstrained optimization
 ; otherwise, it’s a constrained optimization problem. From 
 another perspective, some optimization target is to find the global minimal point (e.g., 
 minimize 
 f
  (
 x
 ) = 
 x
 2
 ), while the others only need to find the optimum in a certain range 
 (e.g., minimize 
 f
  (
 x
 ) =  sin (
 x
 ) in the range of [0, 2
 π
 ]). In this chapter, and in the 
 implemented module in Owl, we focus on the unconstrained and local optimization",NA
4.1  Gradient Descent,"The gradient descent method is one of the most commonly used family of iterative 
 optimization processes. Its basic idea is to start from an initial value and then find a 
 certain search direction along a function to decrease the value by a certain step size until 
 it converges to a local minimum. We can thus describe the 
 n
 th iteration of the descent 
 method as follows:
  
  1. Calculate a descent direction 
 d
 .
  
  2. Choose a step size 
 μ
 .
  
  3. Update the location: 
 x
 n
  + 1
  = 
 x
 n
  + 
 μ d
 .
  
 Repeat this process until a stopping condition is met, such as the update being 
 smaller than a threshold. Among the descent methods, 
 gradient descent
  is one of the 
 most widely used algorithms to perform optimization and the most common way to 
 optimize neural networks. Based on the preceding descent process, a gradient descent 
 method uses the function gradient to decide its direction 
 d
  and can be described as 
 follows:
  
  1. Calculate a descent direction −∇
 f 
 (
 x
 n
 ).
  
  2. Choose a step size 
 μ
 .
  
  3. Update the location: 
 x
 n
  + 1
  = 
 x
 n
  + 
 μ
 ∇
 f 
 (
 x
 n
 ).
  
 Here, ∇ denotes the gradient. The distance 
 μ
  along a certain direction is also called 
 the 
 learning rate
  of this iteration. In a gradient descent process, when searching for the 
 minimum, it always follows the direction that is against the direction represented by the 
 negative gradient. The gradient can be calculated based on the algorithm differentiation 
 module we have introduced in Chapter 
 3
 . That’s why the whole Optimisation module is 
 built on Algodiff.
  
 The implementation of gradient descent according to this definition is plain enough. 
  
 For example, for a certain differentiable function f that does have one global minimal 
 point, the following simple Owl code would do:
  
 module n = Dense.Ndarray.D 
  
 open Algodiff.D
  
 88",NA
4.2  Components,"The core of the Optimise module in Owl abstracts several aspects of the gradient descent 
 method in applications: learning rate, gradient method, momentum, etc. Each of them is 
 represented as a submodule. All computation in these modules relies on the AD module. 
  
 The following code shows an outline of this optimization module. It is designed as a 
 functor parameterized by the AD module. In this section, we introduce a part of these 
 submodules and how they implement different methods.
  
 module Make (Algodiff : Owl_algodiff_generic_sig.Sig) = struct module 
 Algodiff = Algodiff
  
  open Algodiff
  
  module Learning_Rate = struct
  
  
  ...
  
  end
  
  module Gradient = struct
  
  
  ...
  
  end
  
  ...
  
 end
  
 89",NA
 Learning Rate,"When training a machine learning model, the learning rate is arguably the most 
 important hyperparameter that affects the training speed and quality. It specifies how 
 much the model weight should be changed given the estimated error in each training 
 round. A large learning rate may lead to suboptimal solutions and unstable training 
 processes, whereas a small rate may result in a long training time. That’s why choosing a 
 proper learning rate is both crucial and challenging in model training. There exist 
 various methods to decide the learning rate, and we have incorporated them in the 
 Learning_rate module, as follows:
  
 module Learning_Rate = struct
  
  
  type typ =
  
  
  
  | Adagrad   of float
  
  
  
  | Const     of float
  
  
  
  | Decay     of float * float
  
  
  
  | RMSprop   of float * float
  
  let run = function
  
  
  | Adagrad a -> fun _ _ c ->
  
   
  Maths.(_f a / sqrt (c.(0) + _f 1e-32))
  
  
  | Const a -> fun _ _ _ -> _f a
  
  
  | Decay (a, k) -> fun i _ _ ->
  
   
  Maths.(_f a / (_f 1. + (_f k * _f (float_of_int i))))
  
  | 
 RMSprop (a, _)   -> fun _ _ c ->
  
   
  Maths.(_f a / sqrt (c.(0) + _f 1e-32))
  
  let default = function
  
  | Adagrad _   -> Adagrad 0.01
  
  | Const _     -> Const 0.001
  
  | Decay _     -> Decay (0.1, 0.1) | RMSprop _   -
 > RMSprop (0.001, 0.9)
  
  let update_ch typ g c =
  
  
  match typ with
  
  
  | Adagrad _ -> [| Maths.(c.(0) + (g * g)); c.(1) |]
  
  | RMSprop (_, k) -
 > [| Maths.(
  
   
  (_f k * c.(0)) + ((_f 1. - _f k) * g * g)); c.(1) |]
  
  | _ -> c
  
 90",NA
∑,"g
  
 i 
 2
  , or
  
 G 
 t
  = 
 G 
 t
  
 −1 
 + 
 g
  
 i 
 2
 . 
 Therefore, the code is
  
 [| Maths.(c.(0) + (g * g)); c.(1) |]
  
 at each iteration. The second element in this array is not used, so it remains the same. 
  
 The RMSprop method, is an adaptive learning rate method proposed by Geoff 
 Hinton. 
  
 It is an extension of Adagrad. It follows the update rule in Eq. 
 4.2
 . Only that here
  
 G 
 t
  = 
 kG 
 t
  
 −1
  
 +",NA
( ,"− 
 k g",NA
),"2
  
 (4.3)
  
 t
  
  
 Note that 
 k
  is a factor that is normally set to 0.9. Therefore, the run function keeps 
 the same; the update_ch function for RMSprop becomes
  
 (_f k * c.(0)) + ((_f 1. - _f k) * g * g)
  
 Compared with Adagrad, by using a decaying moving average of previous 
 gradients, RMSprop enables forgetting early gradients and focuses on the most 
 recently observed gradients.
  
 To demonstrate how this seemingly simple framework can accommodate more 
 complex methods, let’s consider the implementation of the Adam optimizer [33]. Its 
 parameters are two decaying rates 
 β
 1
  and 
 β
 2
 . Updating its learning rate requires two 
 values: the estimated mean 
 m
 t
  and uncentered variance 
 v
 t
  of the gradients. They are 
 updated according to the following rules:
  
 m 
 t
  = 
 β
 1
  
 m 
 t
  
 −1
  
 +",NA
( ,"−β
 1
  
 +",NA
( ,"−β
 2",NA
) ,"g 
 t",NA
) ,"g 
 t 
 2
  
 v 
 t
  = 
 β
 2
  
 v 
 t
  
 −1
  
 Accordingly, its update_ch can be implemented as
  
 let update_ch typ g c =
  
  
  match typ with
  
  
  | Adam (_, b1, b2) ->
  
   
  let m = Maths.((_f b1 * c.(0)) + ((_f 1. - _f b1) * g)) in
  
 92",NA
(,"µ,…",NA
),"g
  . 
  
 But the final 
 g
  multiplication item is not in the end of Eq. 
 4.4
 . That’s why we divide it 
 back in the run function.
  
  
 So far, we have introduced multiple aspects in a learning rate method, most 
 notably run and update_cn, but we have not yet explained how they will be used in an 
  
 93",NA
 Gradient,"We have provided the framework of gradient methods in Section 
 4.1
 . However, there 
 exist many variants of gradient descent algorithms. They are included in the Gradient 
 module. The code is shown as follows. Its structure is similar to that of Learning_rate. 
 The typ contains all the supported gradient methods; these methods do not carry type 
 parameters. The to_string function prints helper information for each method.
  
 module Gradient = struct
  
  
  type typ =
  
  
  
  | GD (* classic gradient descent *)
  
  
  
  | CG (* Hestenes and Stiefel 1952 *)
  
  
  
  | CD (* Fletcher 1987 *)
  
  
  
  | NonlinearCG (* Fletcher and Reeves 1964 *)
  
  
  | DaiYuanCG (* Dai and Yuan 1999 *)
  
  
  
  | NewtonCG (* Newton conjugate gradient *)
  
  
  | Newton
  
  let run = function
  
  
  | GD          -> fun _ _ _ _ g' -> Maths.neg g'
  
  
  | CG          ->
  
   
  fun _ _ g p g' ->
  
    
  let y = Maths.(g' - g) in
  
    
  let b = Maths.(sum' (g' * y) / (sum' (p * y) + _f 1e-32)) in
  
  
  
  Maths.(neg g' + (b * p))
  
  
  | CD          ->
  
   
  fun _ _ g p g' ->
  
    
  let b = Maths.(l2norm_sqr' g' / sum' (neg p * g)) in
  
    
  Maths.(neg g' + (b * p))
  
  
  | NonlinearCG ->
  
   
  fun _ _ g p g' ->
  
    
  let b = Maths.(l2norm_sqr' g' / l2norm_sqr' g) in
  
    
  Maths.(neg g' + (b * p))
  
 94",NA
(,"g
  
 n
  − 
 g
  
 n
  
 −1",NA
),". 
 (4.5)
  
 n",NA
(,"n
   − 
 d 
 n 
 T
  
 −1",NA
(,"g
  
 n
  
 − 
 g
  
 n
  
 −1",NA
),"Here, 
 g
 n
 , 
 d
 n
 , etc. are assumed to be vectors. Note how this parameter and the CG 
 method framework utilize information such as gradient and direction from the previous 
 iteration (
 g
 n
 − 1
  and 
 d
 n
 − 1
 ). We can implement Eq. 
 4.5
  as
  
 let b =
  
  let y = Maths.(g' - g) in
  
  Maths.(sum' (g' * y) / (sum' (p * y) + _f 1e-32))
  
  
 It uses the sum' function to perform vector multiplication, and the extra epsilon 
 value 1e-32 is used to make sure the denominator is not zero.
  
 In the nonlinear conjugate method (NonlinearCG) [21]
  
 β = 
  
 g
  
 T 
 n
  
 g
  
 n
  . 
 n
   
 g
  
 T
  
 g
  
 n
  
 −1
   
 n
  
 −1
  
 It can thus be implemented as
  
 let b = Maths.(l2norm_sqr' g' / l2norm_sqr' g)
  
  
 Here, l2norm_sqr' g calculates the square of l2 norm (or Euclidean norm) of all 
 elements in g, which is 
 g
 T
 g
 .
  
  
 Similarly, in the conjugate gradient method proposed by Dai and Yuan 
 (DaiYuanCG) in [16]
  
 β = − 
  
  
 g g 
 n 
 T
  
 n
  
   
 . 
 n
   
 d 
 n",NA
(,"g
  
 n
  
 − 
 g
  
 n
  
 −1",NA
),97,NA
 ( ,"f x
  
 n",NA
),"(4.6)
  
  
 Here, 
 H
  is the hessian of 
 f
 , that is, the second-order gradient of 
 f
 . The code is a direct 
 translation of this equation:
  
 fun f w _ _ _ ->
  
  let g', h' = gradhessian f w in
  
  Maths.(neg (g' *@ inv h'))
  
 Here, the Algodiff.gradhessian function returns both gradient and hessian of function 
 f at point w. The *@ operator is the alias for matrix multiplication, and inv is the inverse 
 of a matrix. Rather than moving only in the direction of the gradient, Newton’s method 
 combines the gradient with the second-order gradients of a function, starting from a 
 random position and iterating until convergence. Newton’s method guarantees quadratic 
 convergence provided that 
 f
  is strongly convex with Lipschitz Hessian and that the initial 
 point 
 x
 0
  is close enough to the prima 
 x
 ∗
 . Note that this method is not to be confused with 
 the Newton method that aims to find the root of a function.",NA
 Momentum,"The basic gradient descent process can be further enhanced by the 
 momentum 
  
 mechanism. It allows some “inertia” in choosing the optimization search direction, which 
 utilizes previous direction information. It helps to reduce noisy gradient descent that 
 bounces in search direction. The code of the Momentum module is listed as follows. Its 
 key component is the run function.
  
 module Momentum = struct
  
  
  type typ =
  
  
  
  | Standard of float
  
  
  
  | Nesterov of float
  
 98",NA
 ( ,n,NA
),"With momentum, this process is revised to be
  
 d 
 n
  = − ∇",NA
 ( ,"f x
  
 n",NA
),"+ 
 md 
 n
  
 −1
 . 
 (4.7)
  
  
 The float number 
 m
  is the momentum parameter that indicates the impact of 
 direction information in the previous iteration.
  
 The run function in this module returns a function that takes two inputs: the 
 previous direction u and the current direction u' (calculated using any combination of 
 learning rate and gradient methods). Therefore, the momentum method described 
 earlier can be simply implemented as Maths.((_f m * u) + u'). This is the standard 
 momentum method. If we decide not to use any momentum (None), it simply returns the 
 current direction u'.
  
 99",NA
(,"n
  
 + 
 d 
 n
  
 −1 
 m",NA
),"+ 
 md 
 n
  
 −1
 . 
 In this module, we have implemented the solution by Bengio et al.",NA
 Batch,"There is one more submodule we need to mention: the Batch module. It is about how the 
 input data are divided into chunks and then fed into a training process. From the 
 previous introduction about gradient descent, you might assume the function accepts 
 scalar as input. However, in many applications, we should consider applying 
 optimization on a vector 
 x
 . That means in calculating the gradients, we need to consider 
 using a group of data points instead of only one.
  
 From the perspective of calculation, there is not much difference, and we can still use 
 all the data in calculating the gradients. However, one big application field of such an 
 optimization method is regression or, more broadly, machine learning, where there 
 could be millions of data points just to find the optima. We will talk about regression in 
 Section 
 4.4
 . In practice, computing optimization with large quantities of input data can 
 be unavailable due to the limit of hardware factors such as memory size of the computer. 
 Therefore, optimization for such problems is often repeated for several executive 
 rounds, each round called an 
 epoch
 . In each epoch, the given input data are split into 
 batches. 
  
 Each batch can choose to use a batch strategy, as the run function code shown as 
 follows:
  
  module Batch = struct
  
  
  type typ =
  
   
  | Full
  
   
  | Mini       of int
  
   
  | Sample     of int
  
   
  | Stochastic
  
 100",NA
 Checkpoint,"So far, we have introduced how the learning rate, gradients, and momentum modules 
 return functions which utilize information such as the gradient or direction of the 
 previous iteration. But where are they stored? The answer lies in the Checkpoint 
 module. 
  
 It stores all the information during optimization for later use and saves them as files on 
 the hard disk if necessary. Its code is shown as follows:
  
 module Checkpoint = struct
  
  
  type state = {
  
   
  mutable current_batch : int;
  
   
  mutable batches_per_epoch : int;
  
 101",NA
 Params,"The Params submodules are what brings all the other submodules together. It 
 provides an entry point for users to access various aspects of optimization. The code is 
 shown as follows:
  
  module Params = struct
  
  
  type typ =
  
   
  { mutable epochs : float
  
   
  ; mutable batch : Batch.typ
  
   
  ; mutable gradient : Gradient.typ
  
   
  ; mutable learning_rate : Learning_Rate.typ
  
  
  ; mutable momentum : Momentum.typ
  
 104",NA
4.3  Gradient Descent Implementation,"Putting all the aforementioned submodules together, we can now turn to the 
 implementation of a robust gradient descent optimization method. In the Optimise 
 module, we implement the minimise_fun, and we will explain its code as follows:
  
 106",NA
( ,",",NA
),=,NA
(,"x
  
 2
  + 
 y
  − 1
 1",NA
),+,NA
(,"x
  
 + 
 y
  
 2
  − 7",NA
),"2
  . 
 (4.8)
  
 Its definition can be expressed using Owl code as
  
 open Algodiff.D 
  
 module N = Dense.Ndarray.D
  
 let himmelblau a =
  
  
  let x = Mat.get a 0 0 in
  
  
  let y = Mat.get a 0 1 in
  
  
  Maths.(x ** (F 2.) + y - (F 11.) ** (F 2.) +
  
  
  (x + 
 y ** (F 2.) - (F 7.)) ** (F 2.) |> sum')
  
  
 First, let’s look at what the code would look like without using the Optimise module. 
 Let’s apply the gradient descent method according to its definition in Section 
 4.1
 .
  
 let v = N.of_array [|-2.; 0.|] [|1; 2|] let traj = ref 
 (N.copy v) 
  
 let a = ref v 
  
 let eta = 0.0001 
  
 let n = 2000;;
  
  
 Here, we use an initial starting point [-2., 0.]. The step size eta is set to 0.0001, and 
 the iteration number is 2000. Then we can perform the iterative descent process.
  
 let _ =
  
  for i = 1 to n - 1 do
  
  let u = grad himmelblau (Arr !a) |> unpack_arr in a := 
 N.(sub !a (scalar_mul eta u));
  
  traj := N.concatenate [|!traj; (N.copy !a)|] done;;
  
 We apply the grad method in the Algodiff module to the Himmelblau function 
 iteratively, and the updated data a is stored in the traj array. Utilizing the Plot module 
 in Owl, we can visualize this function and the optimization trajectory using the 
 following code:
  
 109",NA
4.4  Regression,"In this section, we introduce a broad area that heavily relies on optimization: 
 regression
 . 
 Regression is an important topic in statistical modeling and machine learning. It’s about 
 modeling problems which include one or more variables (also called “features” or 
 “predictors”) and require us to make predictions of another variable (“output variable”) 
 based on previous values of the predictors. Regression analysis includes a wide range of 
 models, from linear regression to isotonic regression, each with different theoretical 
 backgrounds and applications. In this section, we use the most widely used linear 
 regression as an example to demonstrate how optimization plays a key part in solving 
 regression problems.
  
 111",NA
 Linear Regression,"Linear regression models the relationship between input features and the output 
 variable with a linear model. It is the most widely used regression model. Without loss of 
 generality, let’s look at an example with a single variable in the model. Such a linear 
 regression problem can be informally stated as follows. Suppose we have a series of (
 x
 , 
 y
 ) data points:
  
  ----------------------------
  
  |x| 5.16 | 7.51 | 6.53 | ...
  
  ----------------------------
  
  |y| 0.36 | 5.84 | 16.9 | ...
  
  ----------------------------
  
 Given that the relationship between these two quantities is 
 y
 ≈
 h
 θ
 (
 x
 )
 , where 
  
 h
 θ
 (
 x
 ) = 
 θ
 0
  + 
 θ
 1
 x
 , can we find out the 
 θ
 0
  and 
 θ
 1
  values that can fit the observed data points 
 as closely as possible? This problem can be further formalized later. Denote the list of 
 x
 ’s 
 and 
 y
 ’s as two vectors 
 x
  and 
 y
 . Suppose we have a function 
 C
  that measures the distances 
 between 
 x
  and 
 y
 : 
 C
 θ
 (
 x
 , 
 y
 ). The target is to find suitable parameters 
 θ
  that minimize the 
 distance. That’s where optimization comes to help.",NA
 Loss,"So the next question is: How to represent this distance mathematically? One good choice 
 is to use the Euclidean distance. That means the target is to minimize function:
  
 C",NA
(,"x y
 ,",NA
),"= 1 
 n",NA
∑,NA,NA
(,h,NA
(,"x
  
 ( )",NA
),"− 
 y
  
 ( )",NA
),"2
  
 (4.9)
  
  
 θ
 θ
  
  
  
 2 
 n
  
 i 
 = 1",NA
(,"θθ
      
     
  
 Here, 
 x
 (
 i
 )
  indicates the 
 i
 th element in the vector 
 x
 . The factor 
 1 2
 n
  is used to 
 normalize 
  
 the distance. Other forms of distance can also be applied here. Due to its importance, this 
 distance is called the loss and abstracted as the Loss submodule in the optimization 
 module. Its code is shown as follows:
  
  module Loss = struct
  
  
  type typ =
  
   
  | Hinge
  
   
  | L1norm
  
 112",NA
∑,"i
  
 x
  
 ( )
  −
 ( )
  
  and",NA
∑,i,NA
(,"x
  
 ( )
  −
 ( )",NA
),"2
  
 . The cross-entropy measures the performance 
  
 of a classification model, the output of which is a probability value between 0 and 1. It is 
 calculated as 
 −",NA
∑,"x 
 ( ) 
 log
 y",NA
( ,( ),NA
) ,". The cross-entropy loss is most commonly used in 
  
 i
  
 training neural networks, as we will show in Chapter 
 5
 .",NA
 Implementation of Linear Regression,"In the source code of Owl, the owl_regression_generic.ml file lists all several 
  
 regression methods, and they are all based on a core linear regression implementation. 
 This function optimizes parameters 
 w
  and 
 b
  in a general regression problem: minimize 
 l
 w
 , 
 b
 (
 x
 T
 w
  + 
 b
 −
 y
 ). Here, each data point 
 x
  can be a vector of the same length as 
 T
 , since 
 there can be more parameters than that shown in Eq. 
 4.9
 . The code is listed as follows:
  
 module Make (Optimise : Owl_optimise_generic_sig.Sig) = struct module 
 Optimise = Optimise
  
  open Optimise
  
  open Optimise.Algodiff
  
 113",NA
 Other Types of Regression,"Even though linear regression is powerful and widely used, the linear model cannot fit 
 all problems. A lot of data follow other patterns than a linear one. For example, 
 sometimes the relationship between the feature 
 x
  and the output variable can be 
 modeled as an 
 n
 th degree polynomial with regard to feature 
 x
 :
  
 h
 θθ",NA
( ),"= θ
 0
  + θ
 1
  
 x
  + θ
 2
  
 x
  
 2
  + θ
 3
  
 x
  
 3
  … 
 (4.10)
  
 This is called a 
 polynomial
  regression. Owl provides a function poly in the 
  
 Regression module to get the model parameter. The first two parameters are still 
 x
  and 
 y
 , and the third parameter limits the order of the polynomial model. Its implementation 
 can also be concisely expressed with _linear_reg:
  
  let poly x y n =
  
  
  let z =
  
   
  Array.init (n + 1) (fun i -> A.(pow_scalar x
  
   
  (float_of_int i |> float_to_elt)))
  
  
  in
  
  
  let x = A.concatenate ~axis:1 z in
  
  
  let params =
  
   
  Params.config
  
    
  ~batch:Batch.Full
  
    
  ~learning_rate:(Learning_Rate.Const 1.)
  
   
  ~gradient:Gradient.Newton
  
    
  ~loss:Loss.Quadratic
  
    
  ~verbosity:false
  
    
  ~stopping:(Stopping.Const 1e-16)
  
    
  100.
  
  in
  
  (_linear_reg false params x y).(0)
  
 The key is to first process the data, so that each data point 
 x
  can be projected to 
 a series of new features 
 z
 , so that 
 z
 i
  = 
 x
 i
 . Eq. 
 4.10
  then becomes a multivariable linear 
 regression:
  
 h
 θθ",NA
( ),"= θ
 0
  + θ
 1 1 
 z
  
 + θ
 2
  
 z
  
 2
  + θ
 3
  
 z
  
 3
  … 
 116",NA
(,"x 
 y
 ,",NA
),= 1,NA
∑,g h,NA
( ,m,NA
(,"x
  
 ( )",NA
),", 
 y
  
 ( )",NA
),", 
 (4.11)
  
  
 θ
 θ
  
   
  
 m
  
 i
  
 = 1
  
 θθ
  
  
         
  
 where 
 m
  is the total number of data points in input data 
 x
  and 
 y
 ; the function 
 g
  is 
 defined as
  
 g h",NA
(,θθ,NA
( ) ,"x 
 y",NA
),"= 
 −
 − 
 log",NA
( ,"h
 θθ",NA
( ) ,NA,NA
) ,",",NA
( ,"1−
 h
 θθ",NA
( ,NA,NA
),NA,NA
),", 
 if 
 y
  = 1 
 (4.12)
  
 log 
 if 
 y
  = 0 
 The logistic gradient can be implemented by using the cross-entropy loss function:
  
  let logistic ?(i = false) x y =
  
  
  let params =
  
   
  Params.config
  
    
  ~batch:Batch.Full
  
    
  ~learning_rate:(Learning_Rate.Adagrad 1.)
  
   
  ~gradient:Gradient.GD
  
    
  ~loss:Loss.Cross_entropy
  
    
  ~verbosity:false
  
    
  ~stopping:(Stopping.Const 1e-16)
  
    
  1000.
  
  in
  
  _linear_reg i params x y",NA
 Regularization,"There is one thing we need to understand: regression is more than just optimization 
 after all. Its purpose is to create a model that fits the given data, and all too often, this 
 model should be used to predict the output of future input. Therefore, if a model fits 
 the given data 
 too well
 , it may lose generality for future data. That’s where the idea of 
 regularization
  comes in. This technique prevents a model from being tuned too closely 
 to a particular dataset and thus may fail to predict future observations well.
  
 117",NA
4.5  Summary,"In this chapter, we introduced optimization and its implementation in Owl. Focusing on 
 gradient descent, one of the most widely used optimization methods, we introduced 
 various aspects, such as the gradient method, learning rate, momentum, etc. Together 
 they provide a powerful and robust implementation. As an important example, we 
 further introduced regression, a machine learning technique that heavily relies on 
 optimization. We showed how various regression methods can be built efficiently using 
 the optimization module.
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 119",NA
CHAPTER 5,NA,NA
Deep Neural ,NA,NA
Networks,"There are many articles teaching people how to build intelligent applications using 
 different frameworks such as TensorFlow, PyTorch, etc. However, except those 
  
 very professional research papers, very few articles can give us a comprehensive 
 understanding on how to develop such frameworks. In this chapter, rather than just 
 “casting spells,” we focus on explaining how to make the magic work in the first place. 
 We will dissect the deep neural network module in Owl, then demonstrate how to 
 assemble different building blocks to build a working framework. Owl’s neural network 
 module is a full-featured DNN framework. You can define a neural network in a very 
 compact and elegant way thanks to OCaml’s expressiveness. The DNN applications built 
 on Owl can achieve state-of-the-art performance.",NA
5.1  Module Architecture,"To explain in layman’s terms, you can imagine a neural network as a communication 
 network where data flow from one node to another node without loops. Nodes are 
 referred to as neurons. Every time data pass a neuron, it will be processed in different 
 ways depending on the type of a neuron. The link between neurons represents nonlinear 
 transformation of the data. Neurons can be wired in various ways to exhibit different 
 architectures which specialize in different tasks. During the training phase, data can be 
 fed into a neural network to let it form the knowledge of certain patterns. During the 
 inference phase, the neural network can apply previously learned knowledge to the 
 input data.
  
 A DNN framework is built to let us define the network structure and orchestrate its 
 learning and inference tasks. The framework is a complicated artifact containing lots of 
 technologies. However, from the high-level system perspective, there is only a limited 
 amount of core functions which a framework must implement. Let us take a look at the 
 key functionalities required by Owl’s neural network module:",NA
5.2  Neurons,"Neurons are implemented as modules. Each type of neuron corresponds to a specific 
 module. These modules share many common functions such as mktag, mkpar, update, 
 etc., but their implementation might slightly differ. Every neuron has its own neuron_typ 
 which specifies the shape of the neuron’s input and output.
  
 module Linear : sig
  
  
  type neuron_typ =
  
   
  { mutable w : t
  
   
  ; mutable b : t
  
   
  ; mutable init_typ : Init.typ
  
   
  ; mutable in_shape : int array
  
   
  ; mutable out_shape : int array
  
   
  }
  
  
  val create : ?inputs:int -> int -> Init.typ -> neuron_typ
  
  val 
 connect : int array -> neuron_typ -> unit
  
  
  val init : neuron_typ -> unit
  
  
  val reset : neuron_typ -> unit
  
  
  val mktag : int -> neuron_typ -> unit
  
  
  val mkpar : neuron_typ -> t array
  
  
  val mkpri : neuron_typ -> t array
  
  
  val mkadj : neuron_typ -> t array
  
  
  val update : neuron_typ -> t array -> unit
  
  
  val copy : neuron_typ -> neuron_typ
  
  
  val run : t -> neuron_typ -> t
  
  
  val to_string : neuron_typ -> string
  
  
  val to_name : unit -> string 
  
 end
  
 The neuron_typ also includes all the parameters associated with this neuron. 
  
 For example, the preceding code presents the signature of the Linear neuron which 
 performs 
 wx
  + 
 b
  for input 
 x
 . As you can see, fields x and b are used for storing the weight 
 and bias of this linear function.
  
 123",NA
 Core Functions,"The record type neuron_typ is created by calling the create function when constructing 
 the network.
  
 let create ?inputs o init_typ =
  
  
  let in_shape =
  
   
  match inputs with
  
   
  | Some i -> [| i |]
  
   
  | None   -> [| 0 |]
  
  
  in
  
  
  { w = Mat.empty 0 0
  
  
  ; b = Mat.empty 0 0
  
  
  ; init_typ
  
  
  ; in_shape
  
  
  ; out_shape = [| o |]
  
  
  }
  
 The parameters are created, but their values need to be initialized in the init 
 function. The bias parameter is set to zero, while the initialization of weight depends on 
 init_typ.
  
 let init l =
  
  let m = l.in_shape.(0) in
  
  let n = l.out_shape.(0) in
  
  l.w <- Init.run l.init_typ [| m; n |] l.w; l.b <- 
 Mat.zeros 1 n
  
 How the weights are initialized matters a lot in the training. If weights are not 
 initialized properly, it takes much longer to train the network. In the worse case, the 
 training may fail to converge. The Init module provides various ways to initialize 
 weights, specified by different type constructors. Some initialization methods require 
 extra parameters. For example, if we want to randomize the weight with Gaussian 
 distribution, we need to specify the mean and variance. As discussed by X. Glorot in [24], 
 the initialization method has a nontrivial impact on model training performance. Besides 
 those supported here, users can also use Custom to implement their own initialization 
 methods.
  
 124",NA
 Activation Module,"One reason we can pile up even just linear neurons to construct a deep neural network is 
 its property of 
 nonlinearity
 , which is introduced by activation neurons. Nonlinearity is a 
 useful property in our models as most real-world data demonstrate nonlinear features. 
 Without nonlinearity, all the linear neurons can be reduced to just one matrix. Activation 
 functions are aggregated in one module called Activation. Similar to other neuron 
 modules, the Activation also has neuron_typ and many similar functions.
  
 module Activation = struct
  
  
  type typ =
  
   
  | Elu (* Exponential linear unit *)
  
   
  | Relu (* Rectified linear unit *)
  
   
  | Sigmoid (* Element-wise sigmoid *)
  
   
  | HardSigmoid (* Linear approximation of sigmoid *)
  
   
  | Softmax of int (* Softmax along specified axis *)
  
   
  | Softplus (* Element-wise softplus *)
  
   
  | Softsign (* Element-wise softsign *)
  
   
  | Tanh (* Element-wise tanh *)
  
   
  | Relu6 (* Element-wise relu6 *)
  
   
  | LeakyRelu of float (* Leaky version of a Rectified Linear Unit *)
   
  | 
 TRelu of float (* Thresholded Rectified Linear Unit *)
  
   
  | Custom of (t -> t) (* Element-wise customised activation *)
  
  
  | None
  
 126",NA
5.3  Networks,"Essentially, a neural network is a computation graph. Nodes represent neurons which 
 aggregate more complicated data processing logic than nodes in a vanilla computation 
 graph. The following code presents the type definition of the node. A node can have a 
 name for reference. The prev and next fields are used for linking to ancestors and 
 descendants, respectively. The output field is used to store the output of the 
 computation.
  
 The node per se does not contain any data processing logic. Rather, the node refers 
 to the neuron which implements actual numerical operations on the data. The 
 motivation of this design is to separate the mechanism of a network and the logic of 
 neurons. The network field refers to the network that the current node belongs to. 
 Note that the network structure is not necessarily the same in training and inference 
 phases. Some nodes may be dropped during the inference phase, such as dropout. The 
 train field is used for specifying whether a node is only for training purposes.
  
 type node =
  
  { mutable name : string
  
  ; mutable prev : node array
  
  ; mutable next : node array
  
  ; mutable neuron : neuron
  
 127",NA
5.4  Training,"Training is a complicated, time-consuming, and computation-intensive process. There 
 are many parameters to configure different components in a neural network framework 
 to control the process. The following functor definition can give us a good understanding 
 about what needs to be configured. Fortunately, the Optimise module does all the heavy-
 lifting job; it implements several engines for different optimization tasks.
  
 module Flatten (Graph : Owl_neural_graph_sig.Sig) = struct module 
 Graph = Graph
  
  ...
  
  
  module Params = Graph.Neuron.Optimise.Params
  
  
  module Batch = Graph.Neuron.Optimise.Batch
  
  
  module Learning_Rate = Graph.Neuron.Optimise.Learning_Rate
  
  
 module Loss = Graph.Neuron.Optimise.Loss
  
  
  module Gradient = Graph.Neuron.Optimise.Gradient
  
  
  module Momentum = Graph.Neuron.Optimise.Momentum
  
  
  module Regularisation = Graph.Neuron.Optimise.Regularisation
  
  
 module Clipping = Graph.Neuron.Optimise.Clipping
  
  
  module Stopping = Graph.Neuron.Optimise.Stopping
  
  
  module Checkpoint = Graph.Neuron.Optimise.Checkpoint 
  
 end
  
 We have already introduced these optimization modules in Chapter 
 4
 . The main logic 
 of training is encoded in the train_generic function. In addition to the network to be 
 trained nn, inputs x, and labeled data y, the function also accepts optional parameters 
 like state if we want to resume previous training.
  
 let train_generic ?state ?params ?(init_model = true) nn x y = if init_model 
 = true then init nn;
  
  let f = forward nn in
  
  let b = backward nn in
  
  let u = update nn in
  
  let s = save nn in
  
 130",NA
 Forward and Backward Pass,"Evaluating a neural network can be done in two directions. The direction from inputs to 
 outputs is referred to as 
 forward pass
 , while the opposite direction from outputs to 
 inputs is 
 backward pass
 . The inference only requires a forward pass, but the training 
 requires both a forward and a backward pass in many iterations. The forward function 
 has only two steps. The first call to mktag is a necessary step required by the algorithmic 
 differentiation, so that we can use AD to calculate derivatives in the following backward 
 pass. For inference, this step is not necessary. The run function pushes the data x into 
 network nn and iterates through all the neurons’ calculations. mkpar nn returns all the 
 parameters of the network, for example, weights.
  
 let forward nn x =
  
  mktag (tag ()) nn;
  
  run x nn, mkpar nn
  
 The core logic of the run function is iterating all the neurons in a topological order. 
 For each neuron, the inputs are collected from its ancestors’ outputs first, then the 
 neuron’s activation function is triggered to process the inputs. The neuron’s output 
  
 131",NA
5.5  Neural Network Compiler,"If we consider a neural network as a complicated function, then training this neural 
 network is an iterative process of optimizing the function. In every iteration, the 
 optimization engine first runs the forward pass of the function, then calls an algorithmic 
 differentiation module to obtain the derivative function, and finally runs the backward 
 pass to update the weight. As we can see, there are two computation graphs created 
 dynamically in each iteration, one for the forward pass and the other for the backward 
 pass. In fact, given a fixed neural network, the structure of both computation graphs is 
 identical. This observation serves as the basis of further optimizing the training process. 
  
 The optimization consists of three parts:
  
 • We dry run the neural network to derive the computation graphs for 
 both forward and backward pass. We reuse these computation graphs in 
 the following iterative process rather than regenerating them.
  
 • We optimize the graph structure to optimize both computation 
 performance and memory usage.
  
 • We replace eager evaluation with lazy evaluation when evaluating a 
 computation graph, which can further optimize the performance.
  
 The Owl_neural_compiler functor is designed to automate these steps. Compared to 
 directly using an optimization engine, the neural network compiled by Owl_neural_ 
 compiler can be trained much faster with much less memory footprint. Let us create a 
 VGG-like convolution neural network to illustrate. The VGG neural network is for image 
 classification tasks, for example, using the CIFAR10 dataset. The network structure is 
 defined by the following code:
  
 let make_network input_shape =
  
  input input_shape
  
  |> normalisation ~decay:0.9
  
  |> conv2d [|3;3;3;32|] [|1;1|] ~act_typ:Activation.Relu
  
  |> conv2d [|3;3;32;32|] [|1;1|] ~act_typ:Activation.Relu ~padding:VALID |> 
 max_pool2d [|2;2|] [|2;2|] ~padding:VALID
  
  |> dropout 0.1
  
  |> conv2d [|3;3;32;64|] [|1;1|] ~act_typ:Activation.Relu
  
  |> conv2d [|3;3;64;64|] [|1;1|] ~act_typ:Activation.Relu ~padding:VALID |> 
 max_pool2d [|2;2|] [|2;2|] ~padding:VALID
  
 133",NA
5.6  Case Study: Object Detection,"As a case study to demonstrate how we apply the Neural module in Owl in real-world 
 applications, in this final section we show how to perform instance segmentation using 
 the Mask R-CNN network. It provides both a complex network structure and an 
 interesting application. Object detection is one of the most common DNN tasks, which 
 takes an image of a certain object as input and infers what object this is. However, a 
 neural network can easily get confused if it is applied on an image with multiple objects. 
  
 For that purpose, 
 object detection
  is another classical computer vision task. Given an 
  
 138",NA
 Object Detection Network Architectures,"To present some background knowledge, before explaining the details of Mask R-CNN, 
 we briefly introduce how deep network architectures for object detection and instance 
 segmentation are developed.",NA
 R-CNN Architecture,"The idea of using a CNN to enhance the object detection task was first proposed in [23]. 
  
 This paper proposes a “Regions with CNN features” (R-CNN) object detection system. It 
 is divided into several phases. The first phase is to localize possible objects of interest in 
 an input image. Instead of using a sliding window, R-CNN uses a different approach 
 called “regions”: for each input image, it first generates a number of (e.g., 2000) 
 region 
 proposals
  that are independent of the object categories used. They are rectangle regions 
 of the image, of different aspects and sizes. The content in each region is then checked to 
 see if it contains any object of interest. Each region proposal is then processed by a CNN 
 to get a 4096-dimension feature vector. This CNN takes an input of fixed size 227 × 227, 
 and thus each region, regardless of its shape, is morphed into this fixed size before being 
 processed by CNN. As to the output feature vector, it is processed by a trained SVM 
 model to be classified into the accepted results.",NA
 Fast R-CNN Architecture,"Compared to the previous state of the art, R-CNN improves the mean average precision 
 by more than 30%. However, it has several problems. For one, the R-CNN pipeline 
 consists of several parts, and so training takes multiple stages, including the training of 
 the CNN, the SVM models, etc. Moreover, training is expensive in both space and time. 
  
 Besides, since the CNN inference pass needs to be performed on all region proposals, 
 the whole object detection process is slow.
  
 To mitigate these challenges, the 
 Fast R-CNN
  approach was proposed by Girshick et 
 al. Similar to R-CNN, the region proposals are first generated based on the input images. 
 But to reduce training costs, the Fast R-CNN consists of one CNN network that can be 
 trained in a single stage. Furthermore, it does not need to perform a forward pass on 
 each region proposal. Instead, it first computes a convolutional feature map for the 
 whole input image and then projects the region of interest (RoI) from the input image to 
 this feature map, deciding which part of it should be extracted. Such a region on the 
  
 140",NA
 Faster R-CNN Architecture,"However, the Fast R-CNN is also not perfect. Region proposals are first generated based 
 on the input image, and object detection is performed using both the convolutional 
 feature map and the region proposal. Note that the feature map, which abstractly 
 represents features in the input image, may already contain sufficient information to 
 perform not just object detection but also to find regions where the objects may be. That 
 important observation leads to the development of the 
 Faster R-CNN
  method.
  
 This approach is developed based on the Fast R-CNN network. It introduces a new 
 network structure: the 
 Region Proposal Network
  (RPN). This extra network operates on 
 the feature maps that are generated by the CNN in Fast R-CNN, generating region 
 proposals that are passed to the RoI pooling step for successive detections. The RPN uses 
 a sliding window over the feature map. At each sliding window location, nine different 
 proposals that are centered around this location are checked to see their coordinates 
 (represented by four real-valued numbers) and the probability that an object exists in 
 the given region. Each such proposal is called an 
 anchor
 . In Faster R-CNN, the authors 
 trained the RPN and used the generated proposals to train Fast R-CNN; the trained 
 network is then used to initialize RPN, so on and so forth. Thus, these two parts are 
 trained iteratively, and so there is no need to use external methods to produce region 
 proposals. Everything is performed in a unified network for the object detection task.",NA
 Mask R-CNN Architecture,"Based on this existing work, the Mask R-CNN was proposed to perform the task of both 
 object detection and semantic segmentation. It keeps the architecture of Faster R-CNN, 
 adding only one extra branch in the final stage of the RoI feature layer. Where previously 
 outputs included object classification and location, now a third branch contains 
  
 141",NA
 Implementation of Mask R-CNN,"This section outlines the main parts of the architecture of Mask R-CNN, explaining how it 
 differs from its predecessors. For a more detailed explanation, please refer to the 
 original paper [27]. The OCaml implementation of the inference model is available in the 
 code repository.
 1",NA
 Building Mask R-CNN,"After a quick introduction to the MRCNN and its development, let’s look at the code to 
 understand how it is constructed:
  
 open Owl
  
 module N = Dense.Ndarray.S
  
 open CGraph 
  
 open Graph 
  
 open AD
  
 module RPN = RegionProposalNetwork 
  
 module PL = ProposalLayer 
  
 module FPN = FeaturePyramidNetwork 
  
 module DL = DetectionLayer 
  
 module C = Configuration
  
 let image_shape = C.get_image_shape () in 
  
 let inps = inputs
  
  
  ~names:[|""input_image""; ""input_image_meta""; ""input_anchors""|]
  
  
 [|image_shape; [|C.image_meta_size|]; [|num_anchors; 4|]|] in
  
 1
 https://github.com/pvdhove/owl-mask-rcnn
 . Work in this chapter was conducted by 
 Pierre 
 Vandenhove
  during his internship in the OCaml Labs group at the University of Cambridge 
 Computer Laboratory. The code was ported from the 
 Keras/TensorFlow implementation
 .
  
 142",NA
 Feature Extractor,"The picture is first fed to a convolutional neural network to extract features of the 
 image. The first few layers detect low-level features of an image, such as edges and basic 
 shapes. 
  
 As you go deeper into the network, these simply features are assembled into higher- 
 level features such as “people” and “cars.” Five of these layers (called “feature maps”) 
 of various sizes, both high and low levels, are then passed on to the next parts. This 
 implementation uses Microsoft’s ResNet101 network as a feature extractor.
  
 let tdps = C.top_down_pyramid_size in let str = 
 [|1; 1|] in 
  
 let p5 = conv2d [|1; 1; 2048; tdps|] str
  
  
 ~padding:VALID ~name:""fpn_c5p5"" c5 in
  
 let p4 = 
  
 add ~name:""fpn_p4add""
  
  
  [|upsampling2d [|2; 2|] ~name:""fpn_p5upsampled"" p5;
  
  
  conv2d [|1; 1; 1024; tdps|]
  
    
  str ~padding:VALID ~name:""fpn_c4p4"" c4|] in let p3 = 
  
 add ~name:""fpn_p3add""
  
  
  [|upsampling2d [|2; 2|] ~name:""fpn_p4upsampled"" p4;
  
  
  conv2d [|1; 1; 512; tdps|]
  
    
  str ~padding:VALID ~name:""fpn_c3p3"" c3|] in let p2 = 
  
 add ~name:""fpn_p2add""
  
  
  [|upsampling2d [|2; 2|] ~name:""fpn_p3upsampled"" p3;
  
  
  conv2d [|1; 1; 256; tdps|]
  
    
  str ~padding:VALID ~name:""fpn_c2p2"" c2|] in
  
 143",NA
 Proposal Generation,"To try to locate the objects, about 250,000 overlapping rectangular regions or anchors 
 are generated.
  
 let nb_ratios = Array.length C.rpn_anchor_ratios in 
  
 let rpns = Array.init 5 (fun i ->
  
  
  RPN.rpn_graph rpn_feature_maps. (i)
  
  
  nb_ratios C.rpn_anchor_stride
  
  
  (""_p"" ^ string_of_int (i + 2))) in 
  
 let rpn_class = concatenate 1 ~name:""rpn_class""
  
   
  (Array.init 5 (fun i -> rpns. (i).(0))) in let rpn_bbox 
 = concatenate 1 ~name:""rpn_bbox""
  
   
  (Array.init 5 (fun i -> rpns. (i).(1)))
  
 Single RPN graphs are applied on different features in rpn_features_maps, and the 
 results from these networks are concatenated. For each bounding box on the image, 
 the RPN returns the likelihood that it contains an object, called its 
 objectness
 , and a 
 refinement for the anchor; both are represented by rank 3 ndarrays.
  
 144",NA
 Classification,"All anchor proposals from the previous layer are resized to a given fixed size and fed into 
 a ten-layer neural network. The network assigns each of them the probability that it 
 belongs to each class. The network is pretrained on fixed classes; changing the set of 
 classes requires retraining the whole network. Note that this step does not take as much 
 time for each anchor as a full-fledged image classifier such as Inception, since it reuses 
 the precomputed feature maps from the Feature Pyramid Network. Therefore, there is 
 no need to go back to the original picture. The class with the highest probability is 
 chosen for each proposal, and thanks to the class predictions, the anchor proposals are 
 even more refined. 
  
 Proposals classified in the background class are deleted. Eventually, only the 
 proposals with an objectness over some threshold are kept, and we have our final 
 detections, each with a bounding box and a label. This process can be described by 
 the following code:
  
 let mrcnn_class, mrcnn_bbox = 
  
 FPN.fpn_classifier_graph rpn_rois
  
  
  mrcnn_feature_maps input_image_meta
  
  
  C.pool_size C.num_classes C.fpn_classif_fc_layers_size in
  
 let detections = MrcnnUtil.delay_lambda_array
  
  [|C.detection_max_instances; 6|]
  
  (DL.detection_layer ()) ~name:""mrcnn_detection""
  
  [|rpn_rois; mrcnn_class; mrcnn_bbox; input_image_meta|] in",NA
5.7  Summary,"In this chapter, we provided an insight into the neural network module in Owl. 
  
 Benefiting from solid implementation of algorithmic differentiation and optimization, 
 the Neural module is concise and expressive. We explained the neurons and network 
 components in this module and then showed how network training is done in Owl. This 
 chapter also covered how we implement a neural network compiler to automatically 
 optimize the network structure and memory usage. Finally, we introduced in detail a 
 DNN application, instance segmentation, that drives the development of the 
  
 computation graph module in Owl.
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 147",NA
CHAPTER 6,NA,NA
Computation Graph,"A computation graph is a basic theoretical tool that underlies modern deep learning 
 libraries. It is also an important component in Owl. This chapter first gives a bird’s- eye 
 view on the computation graph in Owl and its importance in computing. We then 
 demonstrate how to use it in Owl with some examples. Then we will continue to cover 
 the design and implementation details of the computation graph module and how it is 
 fitted into Owl’s functor stack.",NA
6.1  The Definition of Computation Graph,"As a functional programmer, it is basic knowledge that a function takes an input and 
 then produces an output. The input of a function can be the output of another function 
 which then creates dependency. If we view a function as one node in a graph, and its 
 input and output as incoming and outgoing links to other functions, respectively, as the 
 computation continues, these functions are chained together to form a 
 directed acyclic 
 graph
  (DAG). Such a DAG is often referred to as a computation graph.
  
  
 Figure 6-1. 
 Computation graph of a simple function: 
 sin(x*y)
 © Liang Wang, Jianxin Zhao 2023 
  
 149
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5_6",NA
 Dynamic Graph and Static Graph,"The computation graph can be either implicitly constructed or explicitly declared in the 
 code. Implicit graph construction is often achieved by operator overloading and a graph 
 is constructed during the runtime, while explicit declaration may use 
 domain-specific 
 languages
  (DSL) to construct a graph during the compilation phase with a fix structure. 
 The two methods lead to two different kinds of computation graphs: 
 dynamic graph
  and 
 static graph
 ; each has its own pros and cons.
  
  
 A dynamic graph is constructed during the runtime. Due to 
 operator overloading
 , its 
 construction can be naturally blended with a language’s native constructs such as if ... 
 else ... and for loops. This renders greatest flexibility and expressiveness. By using a 
 dynamic computation graph, users are even free to construct a different network for 
 each training sample. On the other hand, a static graph needs to be declared using a 
 specific DSL, which tends to have a steeper learning curve. It is defined only once in 
 training. Because the structure of a graph is already known during the compilation 
 phase, there is a great space for optimization. However, it is sometimes very difficult to 
 use static graphs to express conditions and loops when using with native code together. 
  
 The flexibility of a dynamic graph comes at the price of lower performance. 
  
 Facebook’s PyTorch and Google’s TensorFlow are the typical examples of dynamic and 
 static graphs, respectively. Many programmers need to make a choice between these two 
 different types. A common practice is “using PyTorch at home and using TensorFlow in 
 the company.” In other words, PyTorch is preferred for prototyping, and TensorFlow is 
 an ideal option for production use.
 2 
  
  
 Owl does something slightly different from these two in order to get the best parts of 
 both worlds. Owl achieves this by converting a dynamic graph into a static one during 
  
 1
  This figure is generated with the tool provided by the CGraph module in Owl, which we will 
 discuss in detail in this chapter.
  
 2
  In Sep. 2019, TensorFlow rolled out version 2.0. Starting from this version, TensorFlow uses 
 eager execution by default, which aims to be easier for users to get started with.
  
 150",NA
 Significance in Computing,"Now that you know the basic ideas of a computation graph, you may ask why it 
 matters. Actually, a computation graph plays a core role in any machine learning 
 framework. Both TensorFlow [1] and PyTorch [42], the most popular deep learning 
 libraries, use a computation graph as the central data structure. A computation graph 
 makes many things a lot easier. Here is an incomplete list of its potential benefits:
  
 • Simulating lazy evaluation in a language with eager evaluation
  
 • Incremental computation (a.k.a. self-adjusting computation)
  
 • Reducing computation complexity by optimizing the structure of 
 a graph
  
 • Reducing memory management overhead by preallocating space
  
 • Reducing memory footprint by reusing allocated memory space
  
 • Natural support for parallel and distributed computing
  
 • Natural support for heterogeneous computing
  
 • Natural support for symbolic maths
  
 Some of the benefits are very obvious. Memory usage can certainly be optimized if 
 the graph structure is fixed and the input shapes are known beforehand. One 
  
 optimization is reusing previously allocated memory, which is especially useful for those 
 applications involving large ndarray calculations. In fact, this optimization can also be 
 performed by a compiler by tracking the reference number of allocated memory, a 
 technique referred to as 
 linear types
  [50]. Some may appear less obvious at first glance. 
  
 For example, we can decompose a computation graph into multiple independent 
 subgraphs
 , and each can be evaluated in parallel on different cores or even computers. 
  
 Maintaining the graph structure also improves fault tolerance, by providing natural 
 support for rollback mechanisms.
  
 The computation graph provides a way to abstract the flow of computations; 
 therefore, it is able to bridge the high-level applications and low-level machinery 
 of various hardware devices. This is why it has natural support for 
 heterogeneous computing.
  
 152",NA
6.2  Applications Inside the Computing ,NA,NA
System,"Before diving into the details of the design of the computation graph module, in 
 this section let’s first show some examples of using the CGraph module and how a 
 computation can be transformed into lazy evaluation.",NA
 Basic Numerical Operations,"Let’s start with a simple operation that adds up one ndarray and one scalar. Normally, 
 with the Ndarray module, what we do is
  
 module N = Dense.Ndarray.D
  
 let x = N.ones [|2;2|];; 
  
 let y = 2.;; 
  
 let g = N.add_scalar x y;;
  
  
 Now, let’s make this function into a computation graph which can be lazy evaluated 
 by CGraph.
  
 module N = Owl_computation_cpu_engine.Make
  
  (Owl_algodiff_primal_ops.D)
  
 The Make function here is actually a 
 functor
 . For those who are not familiar with the 
 idea of functor, it is a powerful tool in OCaml to build generic code and structure large-
 scale systems. To put into plain words, a functor is a function that creates modules from 
 modules. As we will explain in Section 
 6.3
 , the computation graph is designed as a 
 functor stack
 . Different aspects of the computation graph, such as memory management 
 and graph optimization, are added into the CGraph by creating a new module based 
  
 153",NA
 Algorithmic Differentiation with CGraph,"In real applications, we often need to deal with CGraphs that are constructed in the 
 algorithmic differentiation process. Here is an example of using the dense ndarray 
 module to compute the gradients of a function:
  
 include Owl_algodiff_generic.Make
  
  
  (Owl_algodiff_primal_ops.D)
  
 let f x y = Maths.((x * sin (x + x) + ((pack_flt 1.) * sqrt x)
  
  / (pack_flt 
 7.)) * (relu y) |> sum')
  
 let x = Dense.Ndarray.D.ones [|2;2|] |> pack_arr let y = 
 pack_elt 2.
  
 let z = (grad (f x)) y |> unpack_elt
  
 Based on the chain rule, the Algodiff module automatically constructs a graph that 
 computes the gradient of input function f. The result is contained in scalar z. However, 
 the graph is constructed internally, but sometimes we need to have access to this graph 
 and apply optimizations. Obviously, it is extremely difficult for the users to manually 
 construct the computation graph that computes the gradient of the function f. Note that 
 the Algodiff module is also built using functors. Its base module follows the Ndarray 
 interface. By changing it from the Ndarray to CGraph module, we can make z to be a 
 computation graph instead of a scalar value, as the following code snippet shows:
  
 module G = Owl_computation_cpu_engine.Make
  
  (Owl_algodiff_primal_ops.D) 
  
 include Owl_algodiff_generic.Make (G)
  
 let f x y =
  
  Maths.((x * sin (x + x) + ((pack_flt 1.) * sqrt x) / 
 (pack_flt 7.)) * (relu y) |> sum')
  
 155",NA
 Deep Neural Network,"Since the optimization and neural network modules are built on the algorithmic 
 differentiation module, they can also benefit from the power of the computation graph. 
 Suppose we have a network built from CGraph-based neural network nn, we can then 
 use the forward and backward functions to get the forward inference and backward 
 propagation computation graphs from the neural network graph module, with the 
 CGraph ndarray variable. Actually, for ease of access, Owl has provided another functor 
 to build the neural network module based on the CGraph module:
  
 module CPU_Engine = Owl_computation_cpu_engine.Make
  
  
  (Owl_algodiff_primal_ops.S) 
  
 module CGCompiler = Owl_neural_compiler.Make (CPU_Engine)
  
 open CGCompiler.Neural 
  
 open CGCompiler.Neural.Graph 
  
 open CGCompiler.Neural.Algodiff
  
 let make_network input_shape =
  
  input input_shape
  
  |> lambda (fun x -> Maths.(x / pack_flt 256.))
  
  |> conv2d [|5;5;1;32|] [|1;1|] ~act_typ:Activation.Relu |> 
 max_pool2d [|2;2|] [|2;2|]
  
  |> dropout 0.1
  
  |> fully_connected 1024 ~act_typ:Activation.Relu
  
  |> linear 10 ~act_typ:Activation.(Softmax 1)
  
  |> get_network ~name:""mnist""
  
 158",NA
6.3  Design of Computation Graph Module ,"Owl implements the computation graph in a very unique and interesting way. Let’s first 
 see several principles we followed in designing and developing this module:
  
  
  
 • 
  
 Nonintrusive, the original functor stack should work as it was
  
  
  
 • 
  
 Transparent to the programmers as much as possible
  
  
  
 • 
  
 Support both eager and lazy evaluations
  
  
  
 • 
  
 Flexible enough for future extension on other devices 
  
  
 The computation graph is implemented in a self-contained stack. We have devised a 
 way to “inject” it into Owl’s original functor stack. If it sounds too abstract, please have a 
 look at the final product in Figure 
 6-3
 .
  
  
 Figure 6-3. 
 Computation graph functor stack in Owl 
 160",NA
 Computing Device,"A computation graph is an abstract construct to express the logic of a function. To 
 calculate the outcome of a function, computation graphs need to be evaluated on a 
 physical device. The device can be anything as long as it has the capability to perform 
 numerical operations, such as the CPU, GPU, etc. To extend Owl on a new device, we 
 only need to create a new device module and define how the basic operations can be 
 performed on this device. Because a majority of the CGraph module is device 
 independent, the device layer becomes very lightweight, which further makes Owl very 
 easy to extend.
  
 163",NA
 Types of Operation,"The Owl_computation_type functor takes a device module as its input, then specifies all 
 the possible operations on the given device. Whenever we want to extend the set of 
 operations, we need to add the corresponding constructor of the new operation to the 
 sum type op. The current set of operations covers a wide range of unary and binary 
 numerical functions, such as Abs, Neg, Add, as well as functions for neural networks 
 such as MaxPool3d.
  
 module Make (Device : Owl_types_computation_device.Sig) = struct module 
 Device = Device
  
  open Device
  
  type state =
  
  | Valid
  
  | Invalid
  
  type t = attr Owl_graph.node
  
  and block =
  
  { size : int
  
  ; block_id : int
  
  ; mutable active : t option
  
  ; mutable memory : value
  
  ; mutable nodes : t list
  
  }
  
  and attr =
  
  { mutable op : op
  
  ; mutable freeze : bool
  
  ; mutable reuse : bool
  
  ; mutable state : state
  
 166",NA
 Shape Inference,"The shape of data might change while traveling through different nodes in a 
  
 computation graph. The shape information is very valuable for debugging and 
 optimization purposes. When all the inputs of a given function are known, the shape 
 of the outcome can be decided; hence, the shape information of a computation graph 
 becomes available. The Owl_computation_shape functor is created for automating 
 shape inference. The core function of this functor is infer_shape which calls the 
 corresponding shape inference function of an operator using pattern matching.
  
 module Make (Type : Owl_computation_type_sig.Sig) = struct
  
  
 module Type = Type
  
  let infer_shape operator args =
  
 167",NA
 Creating and Linking Nodes,"Now we come to a higher level of abstraction: the graph itself. For inputs, outputs, and 
 operators in a computation graph, we need to create their corresponding nodes. Because 
 numerical operations are composable, the output of a function can become the inputs of 
 other functions. We also need to link the nodes according to their input and output 
 dependencies. The Owl_computation_symbol and Owl_computation_operator functors 
 are developed for this purpose. The input of Owl_computation_symbol is a shape 
 module, so the functor can infer the shape automatically while constructing a graph. The 
 most basic functions like arr_to_node and node_to_arr pack and unpack ndarrays to and 
 from a CGraph node.
  
 169",NA
 Optimization of Graph Structure,"The Optimiser functor is in charge of graph structure manipulation. It searches for 
 various 
 structural patterns
  in a graph and then performs various optimizations, such as 
 removing unnecessary computations, fusing computation nodes, etc. All the patterns are 
 defined in the Owl_computation_optimiser functor.
  
 Let us first look at the heart of this functor, _optimise_term function, as follows. 
  
 This function traverses backward from leaves to their ancestors recursively, looking 
 for certain patterns to optimize. The patterns which the functor can recognize are 
 coded in various pattern_* functions which call each other recursively. The source code 
 is organized so that it is very straightforward to plug in more patterns to extend the 
 optimizer’s capability.
  
 module Make (Operator : Owl_computation_operator_sig.Sig) = struct
  
  
 module Operator = Operator
  
  let rec _optimise_term x =
  
  Owl_log.debug ""optimise %s ..."" (node_to_str x); if is_valid 
 x = false
  
 173",NA
 Computation Engine,"Finally, we have reached the top of the CGraph functor stack: the 
 computation engine
 . 
 Because a computation graph has to be evaluated on the hardware, each type of device 
 must implement its own computing engine. The following code shows the engine for 
 CPU devices. The core function eval_gen consists of two steps. The first step is to 
 initialize the graph by calling _init_terms. The second step is to evaluate the graph by 
 calling _eval_terms.
  
 module Make_Nested (Graph : Owl_computation_graph_sig.Sig) = struct 
 module Graph = Graph
  
  module CG_Init = Owl_computation_cpu_init.Make (Graph)
  
  module CG_Eval = Owl_computation_cpu_eval.Make (Graph)
  
  let eval_gen nodes =
  
  CG_Init._init_terms nodes;
  
  CG_Eval._eval_terms nodes
  
  let eval_elt xs = Array.map elt_to_node xs |> eval_gen
  
  let eval_arr xs = Array.map arr_to_node xs |> eval_gen
  
 180",NA
6.4  Optimizing Memory ,NA,NA
Usage ,NA,NA
in Computation ,NA,NA
Graph,"In the previous sections, we have introduced the CGraph stack. Before concluding this 
 chapter, we would like to show optimizations we have made to reduce memory usage in 
 the CGraph module. One principle we have been following during developing Owl is to 
 always get driven by real-world applications. Besides the image recognition example, we 
 have built an image segmentation application, a challenging and interesting use case for 
 Owl. Seeking to push the performance of this application, we manage to further optimize 
 the design of the CGraph module. We will present this deep neural network, Mask R-
 CNN, in Chapter 
 5
 . This section is mainly based on the work done by Pierre Vandenhove 
 on Owl during his internship in the OCaml Labs [49].
  
 The first issue after constructing the network, called 
 Mask R-CNN
 , or MRCNN, in 
 Owl was that its memory usage. in inference mode was huge. The network has over 
 400 layers. A reasonable input image size for this network is a 1024-pixel-wide square. 
 To avoid reinitializing the network for every picture, it is a good practice to keep its 
 input size fixed and to resize instead all the images to that size. Unfortunately, 
 obtaining detections for one picture with such size required over 11GB of RAM, which 
 was too much for a normal laptop. There is surely a big room for improvement.
  
 We first try to apply the graph structure optimization we have mentioned in the 
 previous section. The number of nodes of the Mask R-CNN network drops from 4095 to 
 3765, but its effect in memory reduction is limited. To this end, we need to add in the 
 CGraph functor stack another important layer: memory management. Specifically, we 
 need the ability to preallocate a memory space to each node, to decrease the overall 
 memory consumption and reduce the garbage collector overhead. The key is to find the 
 allocated memory block in the graph that is no longer required and assign it to other 
 nodes that are in need.
  
 183",NA
6.5  Summary,"In this chapter, we introduced the core computation graph module in Owl. We started 
 with a general introduction of the computation graph in numerical computing and why 
 we build that in Owl. Then we used several examples to demonstrate how the 
 computation graph module is used in Owl. This was followed by the internal design of 
 this module, most importantly the CGraph stack and its position in the Owl architecture. 
  
 The computation graph creates a large optimization space, and in this chapter, we 
 presented two of them in detail. The first is the graph structure optimization, and the 
 second is to optimize the memory allocation in the computation graph. A computation 
 graph is an important research topic, and we believe there is still much potential in 
 this module for performance improvement.
  
 188",NA
CHAPTER 7,NA,NA
Performance ,NA,NA
Accelerators,NA,NA
7.1  Hardware Accelerators,"The Graphics Processing Unit (GPU) has become one of the most important types of 
 hardware accelerators. It is designed to render 3D graphics and videos and still is core 
 to the gaming industry. Besides creating stunning visual effects, programmers also 
 take advantage of the GPU’s advantage in parallel processing in many fields to perform 
 computing-heavy tasks, such as in health data analytics, physical simulation, artificial 
 intelligence, etc.
  
 Recall from Chapter 
 2
  the architecture of a typical CPU. The architecture of a GPU 
 core is somewhat similar. Figure 
 7-1
  shows one core of an Nvidia GTX 1080, which 
 contains 20 such cores. Compared with a CPU core, it contains much more 
 multithreading units, including more single-instruction-multiple-data (SIMD) function 
 units, and more cores in a GPU. Another character of the GPU is its small cache. This GPU 
 contains only two levels of caches (this figure only shows the level 1 cache; the level 2 
 cache is shared by all 20 cores), each smaller than a typical CPU’s. Besides, the GPU also 
 focuses on throughput, and thus its bandwidth between cores and memory is much 
 larger than that of a CPU.",NA
 Utilizing Accelerators,"There is no doubt that numerical computing heavily relies on hardware accelerators: 
 TensorFlow, PyTorch, Julia, MATLAB, etc. They all support multiple types of devices, 
 including at least the CPU and GPU. In general, there are two methods to do that.
  
 The first, and most widely used, is direct support of the hardware. Take the GPU as 
 an example. When programming a GPU, Nvidia CUDA is a widely used choice. CUDA is a 
 parallel computing platform and programming model for computing on Nvidia GPUs. 
  
 In TensorFlow, a computation graph is first expressed in Python on the frontend and is 
 then accordingly built up using the C++ backend. This graph is further optimized and 
 partitioned onto multiple devices, which can be CPU, GPU, or TPU devices. Each device 
 invokes the corresponding executor to run the assigned computation on its subgraph. 
  
 For TPU device execution, TensorFlow incorporates a compiler and software stack that 
 translates API calls from TensorFlow computation graphs into TPU instructions. In Julia, 
 support for Nvidia GPUs is provided by its CUDA.jl package. Built on the CUDA toolkit, it 
 enables both interfacing with the CUDA API directly and writing CUDA kernels. NumPy 
 does not support GPUs, but in the vast Python world, there are a lot of GPU-friendly 
 alternatives, such as Numba, CuPy, etc.
  
 Compared with CUDA, the Open Computing Language (OpenCL) serves as an open 
 source standard for cross-platform parallel programming and is not limited to Nvidia 
 GPUs. Therefore, some numerical libraries and software also support it to work on non- 
 Nvidia GPUs. The StreamExecutor that TensorFlow utilizes to process computation tasks 
 on a GPU device is actually a unified wrapper around the CUDA and OpenCL runtimes.
  
 Recently, given a growing number of deep learning frameworks and equally growing 
 number of hardware accelerator platforms, a new approach is to utilize intermediate 
 representations. For example, the 
 deep learning compiler
  has gained rapid growth. A DL 
 compiler takes the model definition described in a deep learning framework and 
 generates efficient implementation specific to certain target hardware. TVM [10] is one 
 popular DL compiler that works with a wide range of frameworks and hardware devices. 
  
 A closely related idea is an open neural network standard that can be converted to and 
 from various frameworks and can also be compiled and executed on various hardware. 
 One such example is the Open Neural Network Exchange (ONNX) format. In summary, 
 it is a growing trend that the definition of computation can be separated out and the 
 low-level compilers to deal with optimization, code generation, etc. to pursue best 
 computation performance. We can think of DL compilers and open standards as the 
 neck of an hourglass that bridges the gap between two types of ecosystems. In the rest 
  
 193",NA
7.2  Design,"Except for the requirement to be executed on accelerators, the development of the 
 owl_symbolic library is motivated by several other factors. For one thing, scientific 
 computation can be considered as consisting of two broad categories: numerical 
 computation and symbolic computation. Owl has achieved a solid foundation in the 
 former, but as yet to support the latter one, which is heavily utilized in a lot of fields.
  
 Besides, tasks such as visualizing a computation also require some form of 
 intermediate representation (IR). Owl has already provided a computation graph layer 
 to separate the definition and execution of computation to improve the performance, 
 as introduced in Chapter 
 6
 , but it’s not an IR layer to perform these different tasks as 
 mentioned before. Toward this end, we begin to develop an intermediate symbolic 
 representation of computations and facilitate various tasks based on this symbol 
 representation.
  
 One thing to note is that do not mistake our symbolic representation as the classic 
 symbolic computation (or computer algebra system) that manipulates mathematical 
 expressions in a symbolic way, which is similar to the traditional manual computations. 
 It is indeed one of our core motivations to pursue the symbolic computation with Owl. 
 Currently, we provide a symbolic representation layer as the first step toward that 
 target. More discussion will be added in future versions of the development with the 
 support of symbolic math in Owl.
  
 The owl_symbolic library is divided into two parts: the core symbolic representation 
 that constructs a symbolic graph and various engines that perform different tasks based 
 on the graph. The architecture design of this system is shown in Figure 
 7-2
 .
  
  
 Figure 7-2. 
 Architecture of the symbolic system
  
 194",NA
 Core Abstraction,"The core part is designed to be minimal and contains only necessary information. 
  
 Currently, it has already covered many common computation types, such as math 
 operations, tensor manipulations, neural network–specific operations such as 
  
 convolution, pooling, etc. Each symbol in the symbolic graph performs a certain 
 operation. Input to a symbolic graph can be constants such as integer, float number, 
 complex number, and tensor. The input can also be variables with certain shapes. An 
 empty shape indicates a scalar value. The users can then provide values to the variable 
 after the symbolic graph is constructed.
  
  
 Symbol 
  
  
 The symbolic representation is defined mainly as an array of symbol. Each symbol is 
 a graph node that has an attribution of type Owl_symbolic_symbol.t. It means that we 
 can traverse through the whole graph by starting with one symbol. Besides symbols, the 
 name field is the graph name, and node_names contains all the nodes’ names contained 
 in this graph.
  
 type symbol = Owl_symbolic_symbol.t Owl_graph.node
  
 type t =
  
  { mutable sym_nodes : symbol array
  
  ; mutable name : string
  
  ; mutable node_names : string array
  
  }
  
 195",NA
 Engines,"Based on this simple core abstraction, we use different 
 engines
  to provide functionalities: 
 converting to and from other computation expression formats, print out to human- 
 readable format, graph optimization, etc. As we have said, the core part is kept minimal. 
 If the engines require information other than what the core provides, each symbol has an 
 attr property as an extension point. All engines must follow the following signature:
  
 type t
  
 val of_symbolic : Owl_symbolic_graph.t -> t val 
 to_symbolic : t -> Owl_symbolic_graph.t val save : t 
 -> string -> unit 
  
 val load : string -> t
  
 It means that each engine has its own core type t, be it a string or another format of 
 graph, and it needs to convert t to and from the core symbolic graph type or save/load 
 a type t data structure to a file. An engine can also contain extra functions besides these 
 four. Now that we have explained the design of owl_symbolic, let’s look at the details of 
 some engines in the next few sections.",NA
7.3  ONNX Engine,"The ONNX engine is the current focus of development in owl_symbolic. 
 ONNX
  is a widely 
 adopted Open Neural Network Exchange format. A neural network model defined in 
 ONNX can be, via suitable converters, run on different frameworks and thus hardware 
 accelerators. The main target of ONNX is to promote the interchangeability of neural 
 network and machine learning models, but it is worth noting that the standard covers a 
 lot of basic operations in scientific computation, such as power, logarithms, 
 trigonometric functions, etc. Therefore, the ONNX engine serves as a good starting point 
 for its coverage of operations.
  
 Taking a symbolic graph as input, how would then the ONNX engine produce the 
 ONNX model? We use the 
 ocaml-protoc
 , a protobuf compiler for OCaml, as the tool. The 
 ONNX specification is defined in an 
 onnx.proto
  file, and the ocaml-protoc can compile 
 this protobuf files into OCaml types along with serialization functions for a variety 
  
 202",NA
 Example 1: Basic Operations,"Let’s look at several examples of using the ONNX engine, starting with a simple one:
  
 open Owl_symbolic 
  
 open Op 
  
 open Infix
  
 let x = variable ""X"" 
  
 let y = variable ""Y"" 
  
 let z = exp ((sin x ** float 2.)
  
  
  + (cos x ** float 2.))
  
  
  + (float 10. * (y ** float 2.)) let g = 
 SymGraph.make_graph [| z |] ""sym_graph"" let m = 
 ONNX_Engine.of_symbolic g 
  
 let _ = ONNX_Engine.save m ""test.onnx""
  
 After including necessary library components, the first three lines of code create a 
 symbolic representation z using the symbolic operators such as sin, pow, and float. The 
 x and y are variables that accept user inputs. It is then used to create a symbolic graph. 
 This step mainly checks if there is any duplication of node names. Then the of_symbolic 
 function in the ONNX engine takes the symbolic graph as input and generates a model_ 
 proto data structure, which can be further saved as a model named test.onnx.
  
 204",NA
 Example 2: Variable Initialization,"We can initialize the variables with tensor values so that these default values are 
 used even if no data are passed in. Here is one example:
  
 open Owl_symbolic 
  
 open Op
  
 let _ =
  
  let flt_val = [| 1.; 2.; 3.; 4.; 5.; 6. |] in let t = Type.make_tensor 
 ~flt_val [| 2; 3 |] in let x = variable ~init:t ""X"" in
  
  let y = sin x in
  
  let g = SymGraph.make_graph [| y |] ""sym_graph"" in let z = 
 ONNX_Engine.of_symbolic g in
  
  ONNX_Engine.save z ""test.onnx""
  
 205",NA
 Example 3: Neural Network,"The main purpose of the ONNX standard is to express neural network models, and we 
 have already covered most of the common operations that are required to construct 
 neural networks. However, to construct a neural network model directly from existing 
 owl_symbolic operations requires a lot of details such as input shapes or creating extra 
 nodes. For example, if we want to build a neural network with operators directly, we 
 need to write something like
  
 let dnn =
  
  let x = variable ~shape:[| 100; 3; 32; 32 |] ""X"" in let t_conv0 
 = conv ~padding:Type.SAME_UPPER x
  
 206",NA
7.4  LaTeX Engine,"The LaTeX engine takes a symbolic representation as input and produces LaTeX 
 strings which can then be visualized using different tools. Its design is simple, 
 mainly about matching symbol type and projecting it to correct implementation. 
  
 Again, let’s look at an example that builds up a symbolic representation of a 
 calculation 
 exp sin",NA
( ,NA,NA
( ,"x 
 0",NA
),+ cos,NA
( ,"x 
 0",NA
) ,2,NA
),"+ 10×
 x 
 0 
    
  
  
  
  
     
  
      
 2 
 + exp",NA
(,π,NA
) ,":
  
 208",NA
7.5  Owl Engine,"An Owl engine enables converting an Owl computation graph to or from a symbolic 
 representation. A symbolic graph can thus benefit from the concise syntax and powerful 
 features such as algorithmic differentiation in Owl.
  
 The conversion between Owl CGraph and the symbolic representation is 
  
 straightforward, since both are graph structures. We only need to focus on making the 
 operation projection between these two systems correct.
  
 let cnode_attr = Owl_graph.attr node in 
  
 match cnode_attr.op with 
  
 | Sin -> Owl_symbolic_operator.sin ~name sym_inputs.(0) 
  
 | Sub -> Owl_symbolic_operator.sub ~name sym_inputs.(0) sym_inputs.(1)
  
 210",NA
7.6  Summary,"In this chapter, we briefly discussed the topic of supporting hardware accelerators in 
 Owl. To improve the performance of computation, it is necessary to utilize the power of 
 hardware accelerators, such as GPU, TPU, etc. It is a growing trend that the definition 
 and execution of computation can be separated out. To this end, we built a symbolic 
 representation based on Owl to facilitate exporting computations to other frameworks 
 that support multiple hardware accelerators. This representation can be executed by 
 multiple backend engines. Currently, it supports the ONNX, LaTeX, and Owl itself as 
 engines. This chapter introduced the design of this symbolic representation and used 
 several examples to demonstrate how the computation in Owl can be executed on other 
 frameworks or visualized.
  
 212",NA
CHAPTER 8,NA,NA
Compiler Backends,"For a numerical library, it is always beneficial and challenging to extend to multiple 
 execution backends. We have seen how we support accelerators such as the GPU by 
 utilizing a symbolic representation and computation graph standard such as ONNX. In 
 this chapter, we introduce how Owl can be used on more edge-oriented backends, 
 including JavaScript and unikernel.",NA
8.1  Base Library,"Before we start, we need to understand how Owl enables compiling to multiple 
 backends by providing different implementations. Owl, as well as many of its external 
 libraries, is actually divided into two parts: a 
 base
  library and a 
 core
  library. The base 
 library is implemented with pure OCaml. For some backends such as JavaScript, we can 
 only use the functions implemented in OCaml.
  
 You may wonder how much we will be limited by the base library. Fortunately, the 
 most advanced modules in Owl are often implemented in pure OCaml, and they live in 
 the base, which includes the modules we have introduced in the previous chapters: 
 algorithmic differentiation, optimization, even neural networks, and many others. 
 Figure 
 8-1
  shows the structure of the core functor stack in Owl.
  
 As we have introduced in Chapter 
 2
 , the Ndarray module is the core building block in 
 Owl. The base library aims to implement all the necessary functions as the core library 
 Ndarray module. The stack is implemented in such a way that the user can switch 
 between these two different implementations without the modules of higher layer. In 
 the Owl functor stack, Ndarray is used to support the computation graph module to 
 provide lazy evaluation functionality. Here, we use the Owl_base_algodiff_primal_ops 
 module, which is simply a wrapper around the base Ndarray module. It also includes a 
 small number of matrix and linear algebra functions. By providing this wrapper instead 
 of 
  
 © Liang Wang, Jianxin Zhao 2023 
  
 215
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5_8",NA
8.2  Backend: JavaScript,"At first glance, JavaScript has very little to do with high-performance scientific 
  
 computing. One important reason we aim to include that in Owl is that the web 
  
 browser is arguably the most widely deployed technology on various edge devices, for 
 example, mobile phones, tablets, laptops, etc. More and more functionalities are being 
 pushed from data centers to edge for reduced latency, better privacy, and security. And 
 JavaScript applications running in a browser are getting more complicated and powerful. 
  
 Moreover, JavaScript interpreters are being increasingly optimized, and even relatively 
 complicated computational tasks can run with reasonable performance.
  
 217",NA
 Native OCaml,"We rely on the tool js_of_ocaml to convert native OCaml code into JavaScript. Js_of_ 
 ocaml is a compiler from OCaml bytecode programs to JavaScript. The process can thus 
 be divided into two phases: first, compile the OCaml source code into bytecode 
 executables, and then apply the js_of_ocaml command to it. It supports the core 
 Bigarray module among most of the OCaml standard libraries. However, since the Sys 
 module is not fully supported, we are careful to not use functions from this module in 
 the base library.
  
 We have described how algorithmic differentiation plays a core role in the 
 ecosystem of Owl, so now we use an example of AD to demonstrate how we convert a 
 numerical program into JavaScript code and then get executed. The example is about 
 optimizing the mathematical function sin. The first step is writing down our application 
 in OCaml as follows, then save it into a file demo.ml.
  
 module AlgodiffD = Owl_algodiff_generic.Make
  
  (Owl_base_algodiff_primal_ops.D) 
  
 open AlgodiffD
  
 let rec desc ?(eta=F 0.01) ?(eps=1e-6) f x = let g = 
 (diff f) x in
  
  if (unpack_flt g) < eps then x
  
  else desc ~eta ~eps f Maths.(x - eta * g)
  
 let _ =
  
  let f = Maths.sin in
  
  let y = desc f (F 0.1) in
  
  Owl_log.info ""argmin f(x) = %g"" (unpack_flt y)
  
 218",NA
 Facebook Reason,"Facebook Reason
  leverages OCaml as a backend to provide type-safe JavaScript. It is 
 gaining its momentum and becoming a popular choice of developing web applications. 
  
 It actually uses another tool, 
 BuckleScript
 , to convert the Reason/OCaml code to 
 JavaScript. Since Reason is basically a syntax layer built on top of OCaml, it is very 
 straightforward to use Owl in Reason to develop advanced numerical applications.
  
 In this example, we use reason code to manipulate multidimensional arrays, the core 
 data structure in Owl. First, we save the following code into a reason file called demo.re. 
 Note the suffix is 
 .re
  now. It includes several basic math and Ndarray operations in Owl.
  
  open! Owl_base;
  
  /* calculate math functions */
  
  let x = Owl_base_maths.sin(5.);
  
  Owl_log.info(""Result is %f"", x);
  
  /* create random ndarray then print */
  
  let y = Owl_base_dense_ndarray.D.uniform([|3,4,5|]); 
 Owl_base_dense_ndarray.D.set(y,[|1,1,1|],1.);
  
  Owl_base_dense_ndarray.D.print(y);
  
  /* take a slice */
  
  let z = Owl_base_dense_ndarray.D.get_slice([[],[],[0,3]],y); 
 Owl_base_dense_ndarray.D.print(z);
  
 The preceding code is simple. It creates a random ndarray, takes a slice, and then 
 prints them out. The Owl library can be seamlessly used in Reason. Next, instead of using 
 Reason’s own translation of this frontend syntax with bucklescript, we still turn to 
 js_of_ocaml for help. Let’s look at the dune file, which turns out to be the same as that in 
 the previous example:
  
  (executable
  
  (name demo)
  
  (modes js)
  
  (libraries owl-base))
  
 220",NA
8.3  Backend: MirageOS,"Besides JavaScript, another choice of backend we aim to support is the MirageOS. It is 
 an approach to build 
 unikernels
 . A unikernel is a specialized, single address space 
 machine image constructed with library operating systems. Unlike a normal virtual 
 machine, it only contains a minimal set of libraries required for one application. It can 
 run directly on a hypervisor or hardware without relying on operating systems such 
 as Linux and Windows. The unikernel is thus concise and secure, and extremely 
 efficient for distribution and execution on either cloud or edge devices.
  
 MirageOS is one solution to building unikernels. It utilizes the high-level language 
 OCaml and a runtime to provide an API for operating system functionalities. In using 
 MirageOS, the users can think of the 
 Xen hypervisor
  as a stable hardware platform, 
 without worrying about the hardware details such as devices. Furthermore, since the 
 Xen hypervisor is widely used in platforms such as Amazon EC2 and Rackspace Cloud, 
 MirageOS-built unikernel can be readily deployed on these platforms. Besides, benefiting 
 from its efficiency and security, MirageOS also aims to form a core piece of the 
 Nymote/MISO tool stack to power the Internet of Things.",NA
 Example: Gradient Descent,"Since MirageOS is based around the OCaml language, we can safely integrate the Owl 
 library with it. To demonstrate how we use MirageOS as a backend, we again use the 
 previous algorithmic differentiation–based optimization example. Before we start, 
 please make sure to follow the 
 installation instruction
  of the MirageOS. Let’s look at the 
 code:
  
 221",NA
 Example: Neural Network,"As a more complex example, we have also built a simple neural network to perform the 
 MNIST handwritten digit recognition task with MirageOS:
  
 module N  = Owl_base_algodiff_primal_ops.S 
 module NN = Owl_neural_generic.Make (N) open 
 NN 
  
 open NN.Graph 
  
 open NN.Algodiff
  
 let make_network input_shape =
  
  input input_shape
  
  |> lambda (fun x -> Maths.(x / F 256.))
  
  |> fully_connected 25 ~act_typ:Activation.Relu |> linear 
 10 ~act_typ:Activation.(Softmax 1) |> get_network
  
 This neural network has two hidden layers, has a small weight size (146KB), and 
 works well in testing (92% accuracy). We can write the weight into a text file. This file 
 is named simple_mnist.ml, and similar to the previous example, we can add a unikernel 
 entry point function in the file:
  
 module Main = struct
  
  
  let start = infer (); Lwt.return_unit end
  
 Here, the infer function creates a neural network, loads the weight, and then 
 performs inference on an input image. We also need a configuration file. Again, it’s 
 mostly the same:
  
 open Mirage
  
 let main =
  
  
  foreign
  
    
  ~packages:[package ""owl-base""]
  
   
  ""Simple_mnist.Main"" job
  
 let () =
  
  
  register ""Simple_mnist"" [main]
  
 224",NA
8.4  Evaluation,"In this section, we mainly compare the performance of different backends. Specifically, 
 we observe three representative groups of operations: (1) map and fold operations on 
 ndarray; (2) using gradient descent, a common numerical computing subroutine, to get 
 argmin
  of a certain function; (3) conducting inference on complex DNNs, including 
 SqueezeNet and a VGG-like convolution network. The evaluations are conducted on a 
 ThinkPad T460S laptop with an Ubuntu 16.04 operating system. It has an Intel Core i5-
 6200U CPU and 12GB RAM.
  
 The OCaml compiler can produce two kinds of executables: bytecode and native. 
 Native executables are compiled for specific architectures and are generally faster, while 
 bytecode executables have the advantage of being portable.
  
 For JavaScript, we use the js_of_ocaml approach as described in the previous 
 sections. Note that for convenience we refer to the pure implementation of OCaml and 
 the mix implementation of OCaml and C as base-lib and owl-lib separately, but they are 
 in fact all included in the Owl library. For Mirage compilation, we use both libraries.
  
 225",NA
8.5  Summary,"The base library in Owl was separated from the core module mainly to accommodate 
 multiple possible execution backends. This chapter introduced how the base module 
 works. Then we showed two possible backends: the JavaScript and the unikernel virtual 
 machine. Both backends are helpful to extend the application of Owl to more devices. 
 Finally, we used several examples to demonstrate how these backends are used and 
 their performances.
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 228",NA
CHAPTER 9,NA,NA
Composition ,NA,NA
and ,NA,NA
Deployment,"In this chapter, we first present Zoo, a script subsystem we have originally developed for 
 OCaml file sharing. We will introduce how it is used and its design. Based on this system, 
 we discuss the problem of computation composition and deployment in a numerical 
 library.",NA
9.1  Script Sharing with Zoo,"The core functionality of the Zoo is simple: sharing OCaml scripts. It is known that we 
 can use OCaml as a scripting language as Python (at certain performance cost because 
 the code is compiled into bytecode). Even though compiling into native code for 
 production use is recommended, scripting is still useful and convenient, especially for 
 light deployment and fast prototyping. In fact, the performance penalty in most Owl 
 scripts is almost unnoticeable because the heaviest numerical computation part is still 
 offloaded to Owl which runs native code.
  
 While designing Owl, our goal is always to make the whole ecosystem open, flexible, 
 and extensible. Programmers can make their own “small” scripts and share them with 
 others conveniently, so they do not have to wait for such functions to be implemented in 
 Owl’s master branch or submit something “heavy” to OPAM.",NA
 Example,"To illustrate how to use Zoo, let’s start with a simple synthetic scenario. Alice is a 
 data analyst and uses Owl in her daily job. One day, she realized that the functions 
 she needed had not been implemented yet in Owl. Therefore, she spent an hour in 
 her",NA
 Version Control,"Alice has modified and uploaded her scripts several times. Each version of her code is 
 assigned a unique version id. Different versions of code may work differently, so how 
 could Bob specify which version to use? The good news is that he barely needs to change 
 his code.
  
 #!/usr/bin/env owl 
  
 #zoo ""9f0892ab2b96f81baacd7322d73a4b08?
  
  vid=71261b317cd730a4dbfb0ffeded02b10fcaa5948""
  
 let _ = Coolmodule.sqr_magic 4 |> Owl.Mat.print
  
 The only thing he needs to add is a version id using the parameter vid. The naming 
 scheme of Zoo is designed to be similar with the field-value pair in a RESTful query. The 
 version id can be obtained from a gist’s revisions page.
  
 Besides specifying a version, it is also quite possible that Bob prefers to use the 
 newest version Alice provides, whatever its id may be. The problem here is that, how 
 often does Bob need to contact the Gist server to retreat the version information? 
 Every 
  
 231",NA
9.2  Service Deployment and Composition,"Based on the Zoo system, in the rest of this chapter, we discuss the computation service 
 deployment and composition problem. First, let’s briefly present some background. 
 Recently, computation on edge and mobile devices has gained rapid growth in both the 
 industry and academia, such as personal data analytics in the home, DNN application on 
 a tiny stick, semantic search and recommendation on a web browser [53], etc. 
  
 HUAWEI has identified speed and responsiveness of native AI processing on mobile 
 devices as the key to a new era in smartphone innovation. Many challenges arise when 
 moving machine learning (ML) analytics from cloud to edge devices.
  
 One problem is not yet well defined and investigated: model composition. Training 
 a model often requires large datasets and rich computing resources, which are often 
 not available to normal users. That is one of the reasons that they are bound to the 
 models and services provided by large companies. To this end, we propose the idea of 
 composable service
 . Its basic idea is that many services can be constructed from basic 
 ones such as image recognition, speech-to-text, and recommendation to meet new 
 application requirements. Modularity and composition will be the key to increasing 
 usage of ML-based data analytics.
  
 Composing components into a more complex entity is not uncommon to see in the 
 computer science. One such example is the composition of web services. A web service 
 is a software application that is identified by a URI and supports machine-to-machine 
 interaction over a network. Messages in formats such as XML and JSON are transferred 
 among web services according to their prescribed interfaces. The potential of the web 
 service application architecture lies in that the developers can compose multiple 
 services and build a larger application via the network. In web service composition, 
 one problem is to select proper participant services so that they can work together 
 properly. A lot of research effort has been made on composition methods that consider 
 information such as interfaces, message types, and dynamic message sequences 
 exchanged.
  
 A similar paradigm is the microservices architecture. With this architecture, a large 
 monolithic software application should be decomposed into small components, each 
 with distinct functionalities. These components can communicate with each other via 
 predefined APIs. This approach provides multifolds of benefits, such as module 
 reusability, service scalability, fault isolation, etc. Many companies, such as Netflix, have 
 successfully adopted this approach. In the composition of different microservices, the 
  
 233",NA
9.3  System Design,"Based on these basic functionalities, we extend the Zoo system to address the 
  
 composition and deployment challenges. Specifically, we design a small DSL to enable 
 script sharing, type-checked composition of different data analytics services with 
 version control, and deployment of services to multiple backends. First, we would like to 
 briefly introduce the workflow of Zoo as shown in Figure 
 9-1
 . The workflow consists of 
 two parts: 
 development
  on the left side and 
 deployment
  on the right.
  
  
 Figure 9-1. 
 Zoo system architecture
  
 Development
  concerns the design of interaction workflow and the computational 
 functions of different services. One basic component is the Gist. By using Zoo, a normal 
 Gist script will be loaded as a module in OCaml. To compose functionalities from 
 different Gists only requires a developer to add one configuration file to each Gist. 
  
 This file is in JSON format. It consists of one or more name-value pairs. Each pair is a 
 signature for a function the script developer wants to expose as a service. These Gists 
 can be imported and composed to make new services. When a user is satisfied with the 
 result, they can save the new service as another Zoo Gist.
  
 Deployment
  takes a Gist and creates models in different backends. These models can 
 be published and deployed to edge devices. It is separated from the logic of 
 development. Basic services and composed ones are treated equally. Besides, users can 
 move services from being local to remote and vice versa, without changing the structure 
  
 235",NA
 Service,"Gist is a core abstraction in Zoo. It is the center of code sharing. However, to compose 
 multiple analytics snippets, Gist alone is insufficient. For example, it cannot express the 
 structure of how different pieces of code are composed together. Therefore, we 
 introduce another abstraction: service.
  
 A service consists of three parts: 
 Gists
 , 
 types
 , and the 
 dependency graph
 . A 
 Gist
  is the 
 list of Gist ids this service requires. 
 Types
  are the parameter types of this service. Any 
 service has zero or more input parameters and one output. This design follows that of an 
 OCaml function. A 
 dependency graph
  is a graph structure that contains information about 
 how the service is composed. Each node in it represents a function from a Gist and 
 contains the Gist’s name, id, and a number of parameters of this function.
  
 Zoo provides three core operations about a service: create, compose, and publish. 
  
 The 
 create_service
  creates a dictionary of services given a Gist id. This operation reads 
 the service configuration file from that Gist and creates a service for each function 
 specified in the configuration file. The 
 compose_service
  provides a series of operations to 
 combine multiple services into a new service. A compose operation does type checking 
 by comparing the “types” field of two services. An error will be raised if incompatible 
 services are composed. A composed service can be saved to a new Gist or be used for 
 further composition. The 
 publish_service
  makes a service’s code into such forms that can 
 be readily used by end users. Zoo is designed to support multiple backends for these 
 publication forms. Currently, it targets the Docker container, JavaScript, and MirageOS 
 [37] as backends.",NA
 Type Checking,"As mentioned in Section 
 9.3
 , one of the most important tasks of service composition is to 
 make sure the type matches. For example, suppose there is an image analytics service 
 that takes a PNG format image, and if we connect to it another one that produces a JPEG 
 image, the resulting service will only generate meaningless output for data type 
  
 236",NA
 DSL,"Zoo provides a minimal DSL for service composition and deployment.
  
 Composition:
  To acquire services from a Gist of id 
 gid
 , we use $
 gid
  to create a 
 dictionary, which maps from service name strings to services. I implement the dictionary 
 data structure using Hashtbl in OCaml. The # operator is overloaded to represent the 
 “get item” operation. Therefore
  
 $gid#sname
  
 can be used to get a service that is named “sname.” Now suppose we have 
 n
  services: 
 f
 1
 , 
 f
 2
 , …, 
 f
 n
 . Their outputs are of type 
 t
 f
 1
 , 
 t
 f
 2
 , …, 
 t
 fn
 . Each service 
 s
  accepts 
 m
 s
  input parameters, 
 which have type 
 t
 s 
 1
  , 
 t
 s 
 2
  , …, 
 t
 s m
 s
  . Also, there is a service 
 g
  that takes 
 n
  inputs, each of 
  
 them has type 
 t
 g 
 1
  , 
 t
 g 
 2
  , …, 
 t
 g n
  . Its output type is 
 t
 o
 . Here, Zoo provides the $> operator to 
  
 compose a list of services with another:",NA
[,"f f 
 1 
 , , 
 2 
 .…, 
 f
  
 n",NA
],"> 
 g
  
 s
  
  inputs and is of output type 
 t
 o
 . 
  
 This operation returns a new service that 
 has 
  
 n 
  
 =",NA
∑,"m
  
 fi
  
 = 
 i g
  , ∀ ∈ 1 2 , 
 ,…, 
 n
  
 .
  
 This operation does type checking to make sure that 
 t
  
  
 Deployment:
  Taking a service 
 s
 , be it a basic or composed one, it can be deployed 
 using the following syntax: 
  
  
  
 s
 $@ backend 
  
  
  
  
 237",NA
 Service Discovery,"The services require a service discovery mechanism. For simplicity’s sake, each newly 
 published service is added to a public record hosted on a server. The record is a list of 
 items, and each item contains the following: a Gist id that the service is based on; a 
 one-line description of this service; a string representing the input and output types of 
 this service, such as “image → int → string → text,”; a service URI. For the container 
 deployment, the URI is a Docker Hub link, and for the JavaScript backend, the URI is a 
 URL link to the JavaScript file itself. The service discovery mechanism is implemented 
 using an off-the-shelf database.",NA
9.4  Use Case,"To illustrate the preceding workflow, let us consider a synthetic scenario. Alice is a 
 French data analyst. She knows how to use ML and DL models on existing platforms, but 
 is not an expert. Her recent work is about testing the performance of different image 
 classification neural networks. To do that, she needs to first modify the image using the 
 DNN-based Neural Style Transfer (NST) algorithm. NST takes two images and outputs to 
 a new image, which is similar to the first image in content and the second in style. This 
 new image should be passed to an image classification DNN for inference. Finally, the 
 classification result should be translated to French. She does not want to put academic- 
 related information on Google’s server, but she cannot find any single pretrained model 
 that performs this series of tasks.
  
 Here comes the Zoo system to help. Alice finds Gists that can do image recognition, 
 NST, and translation separately. Even better, she can perform image segmentation to 
 greatly improve the performance of NST using another Gist. All she has to provide is 
 some simple code to generate the style images she needs to use. She can then assemble 
 these parts together easily using Zoo.
  
 238",NA
9.5  Discussion,"One thing to note is that, in service composition, type checking is a nice property to 
 have, but not the only one. From web services to microservices, the industry and 
 researchers have studied the composition issue for years. Besides checking the static 
 information such as message types, interfaces, etc., sometimes the dynamic behavior 
 between services should also be checked. It is the same in our data analytics services 
 composition scenario.
  
 For example, the Generative Adversarial Network (GAN) is a huge family of 
 networks. A GAN consists of two parts: generator and discriminator. The generator tries 
 its best to synthesize images based on existing parameters. The discriminator takes the 
  
 239",NA
9.6  Summary,"In this chapter, we first introduced Zoo, a scripting sharing tool in Owl, including its 
 usage and design. Based on it, we explored two topics: service composition and 
 deployment. Zoo provides a small DSL to enable type-checked composition of different 
 data analytics services with version control and deployment of services to multiple 
 backends. It benefits from OCaml’s powerful type system. A use case was presented to 
 demonstrate the expressiveness of this DSL in composing advanced ML services such as 
 image recognition, text translation, etc. The Zoo DSL also enables deploying composed 
 services to multiple backends: containers, unikernels, and JavaScripts; service 
  
 deployment often requires choosing a suitable one.
  
 240",NA
CHAPTER 10,NA,NA
Distributed ,NA,NA
Computing,"Distributed computing has been playing a significant role in current smart applications 
 in various fields. In this chapter, we first briefly give a bird’s-eye view of this topic, 
 introducing various programming paradigms. Next, we introduce Actor, an OCaml- 
 based distributed computing engine, and how it works together with Owl. We then 
 focus on one key element in distributed computing: the synchronization. We introduce 
 four different types of synchronization methods or “barriers” that are commonly used 
 in current systems. Next, we elaborate how these barriers are designed and provide 
 illustrations from the theoretical perspective. Finally, we use evaluations to show the 
 performance trade-offs in using different barriers.",NA
10.1  Distributed Machine Learning,"Machine learning has achieved phenomenal breakthroughs in various fields, such as 
 image recognition, language processing, gaming industry, product management, 
 healthcare, etc. The power of machine learning lies in utilizing the growing size of 
 training data as well as models so as to achieve high accuracy. As a large amount of data 
 is increasingly generated from mobile and edge devices (smart homes, mobile phones, 
 wearable devices, etc.), it becomes essential for many applications to train machine 
 learning models in parallel across many nodes. In distributed learning, a model is 
 trained via the collaboration of multiple workers. One of the most commonly used 
 training methods is the stochastic gradient descent (SGD), which iteratively optimizes 
 the given objective function until it converges by following the gradient direction of the 
 objective. In each iteration of SGD, typically a descent gradient is calculated using a batch 
 of training data, and then the model parameters are updated by changing along the 
 direction of the gradient at a certain step. There are mainly three types of paradigms to 
 perform distributed machine learning: parameter servers (PS), All-Reduce, and 
 decentralized approaches (or peer-to-peer).",NA
10.2  The Actor Distributed Engine,"Actor is an OCaml language–based distributed data processing system. It is developed to 
 support the aforementioned distributed computing paradigms in Owl. It has 
  
 implemented core APIs in both map-reduce and parameter server engines. Both map- 
 reduce and parameter server engines need a (logical) centralized entity to coordinate all 
 the nodes’ progress. We also extended the parameter server engine to the peer-to-peer 
 (p2p) engine. The p2p engine can be used to implement both data and model parallel 
 applications; both data and model parameters can be, although not necessarily, divided 
 into multiple parts and then distributed over different nodes. Orthogonal to these 
 paradigms, Actor also implements all four types of synchronization barriers.
  
 Each engine has its own set of APIs. For example, the map-reduce engine includes 
 map, reduce, join, collect, etc., while the peer-to-peer engine provides four major APIs: 
 push, pull, schedule, and barrier. It is worth noting there is one function shared by all 
 the engines, that is, the barrier function which implements various barrier control 
 mechanisms. Next, we will introduce these three different kinds of engines of Actor.",NA
 Map-Reduce Engine,"Following the 
 MapReduce
  programming model, nodes can be divided by tasks: 
  
 either map or reduce. A map function processes a key/value pair to generate a set of 
 intermediate key/value pairs, and a reduce function aggregates all the intermediate 
 key/ value pairs with the same key. Execution of this model can automatically be 
 paralleled. 
  
 Mappers compute in parallel while reducers receive the output from all mappers and 
 combine to produce the accumulated result. This parameter update is then broadcast 
 to all nodes. Details such as distributed scheduling, data divide, and communication 
  
 245",NA
 Parameter Server Engine,"The parameter server module is similar. Nodes are divided into servers, holding the 
 shared global view of the up-to-date model parameters, and workers, each holding its 
 own view of the model and executing training. The workers and servers communicate in 
 the format of key-value pairs. It mainly consists of four APIs for the users:
  
 246",NA
 Compose Actor with Owl,"One of the most notable advantages of Actor lies in that it can compose with Owl. 
 Parallel and distributed computing in Owl is achieved by composing the different data 
 structures in Owl’s core library with specific engines in the Actor system.
  
  
 As shown in Figure 
 10-1
 , all the three distributed engines can be used to compose 
 with the Ndarray module in Owl. And the composition is quite straightforward:
  
 module M = Owl.Parallel.Make (Dense.Ndarray.S) (Actor.Mapre)
  
 That’s all it takes. By using a functor provided in Owl, it builds up a distributed 
 version of the n-dimensional array module. In this functor, we choose to use the single- 
 precision dense Ndarray module and the MapReduce engine as parameters. Using this 
 distributed Ndarray module is also easy. The following code shows an example. You can 
 see that the composed Ndarray module provides all the normal ndarray operations, 
 including initialization, map, fold, sum, slicing, adding, etc. And these computations 
 perform on a distributed cluster.
  
 249",NA
10.3  Synchronization: Barrier Control ,NA,NA
Methods,"One critical component of distributed and federated machine learning systems is barrier 
 synchronization: the mechanism by which participating nodes coordinate in the iterative 
 distributed computation. As noted earlier, the statistical and iterative nature of machine 
 learning means that errors are incrementally removed from the system. To be perfectly 
 consistent, where every node proceeds to the next iteration together risks reducing 
 throughput. Relaxing consistency can improve system performance without ultimately 
 sacrificing accuracy. This trade-off is embodied in the 
 barrier control
  mechanism. In the 
 rest of this chapter, we will focus on this aspect in distributed computing.
  
 In parallel computing, a barrier is used for synchronization. If in the source code a 
 barrier is applied on a group of threads or processes, at this point a thread or process 
 cannot proceed until all others have finished their workload before the barrier. With 
 this, it is guaranteed that certain calculations are finished. For example, the following 
 code shows the barrier pragma in OpenMP, an application programming interface that 
 supports shared memory multiprocessing programming. Here, the calculation is 
 distributed among multiple threads and executed in parallel, but the computation of y 
 cannot proceed until the other threads have computed their own values of x. Execution 
 past the barrier point continues in parallel.
  
 #pragma omp parallel 
  
 {
  
  
  x = some_calculation(); 
  
 #pragma omp barrier
  
  
  y = x + 1 
  
 }
  
  
 The preceding example shows a strict version of barrier methods. In distributed 
 training, there exist multiple forms of barrier control methods for synchronization. 
  
 Current barrier control mechanisms can be divided into four types, discussed in detail 
 later. These barrier methods provide different trade-offs between system performance 
 and model accuracy. They can be applied in different distributed machine learning 
 systems we have discussed in the previous section, including parameter servers, peer-to- 
 peer, etc. In the rest of this section, we will introduce them.
  
 252",NA
10.4  System Design Space and Parameters,"In the previous section, we have introduced the existing barrier control methods. In 
 this section, we will further explain how they are designed and their relationship 
 with each other.
  
 In a distributed training system that utilizes an iterative learning algorithm, the 
 main target is to achieve faster convergence, a state in which loss keeps to be within an 
 error range around the final value of training. The convergence of training is positively 
 correlated with two factors: 
 consistency
  and 
 iteration rate
 . Consistency is the agreement 
 between multiple nodes in a distributed training system to achieve convergence. It can 
 be indicated by the difference of training iterations of the nodes. Weak consistency can 
 be detrimental to the update quality of the model in each iteration. On the other hand, 
 the iteration rate is how fast the training processes. This relationship can be captured by 
 Eq. 
 10.1
 .
  
 Convergence α consistency iteration rate 
 (10.1)
  
 BSP and ASP are good examples to illustrate these two factors. In BSP, workers 
 must wait for others to finish in a training round, and all the workers are of the same 
 progress. Therefore, of all barrier methods the BSP can offer the best consistency and 
  
 256",NA
 Compatibility,"As a more general framework, one noteworthy advantage of PSP lies in that it is 
  
 straightforwardly compatible with existing synchronization methods, which provides 
 the tuning dimension of consistency. In classic BSP and SSP, their barrier control 
 mechanisms are invoked by a central server to test the synchronization condition with 
 the given inputs. For BSP and SSP to use the 
 sampling
  primitive, they simply need to use 
 the sampled states rather than the global states when evaluating the barrier control 
 condition. Within each sampled subset, these traditional mechanisms can then be 
 applied. Users can thus easily derive probabilistic versions of BSP and SSP, namely, 
 pBSP 
 and 
 pSSP
 . For example, Figure 
 10-8
  shows that PSP can be applied to other synchronous 
 machines as a higher-order function to derive probabilistic versions.
  
 Formally, at the barrier control point, a worker samples 
 β
  out of 
 P
  workers without 
 replacement. If one lags more than 
 s
  updates behind the current worker, then the 
 worker waits. This process is pBSP (based on BSP) if the staleness parameter 
 s
  = 0 and 
 pSSP (based on SSP) if 
 s
  > 0. If 
 s
  = ∞, PSP reduces to ASP.
  
 260",NA
10.5  Convergence Analysis,"In this section, we present a theoretical analysis of PSP and show how it affects the 
 convergence of ML algorithms (SGD used in the analysis). The analysis mainly shows 
 that (1) under PSP, the algorithm only has a small probability not to converge, and the 
 upper limit of this probability decreases with the training iterations; (2) instead of 
 choosing large sampling size, it is proved that a small number is already sufficient to 
 provide a good performance. The notations used in the following analysis are presented 
 in Table 
 10-2
 .
  
 The analysis is based on the model shown in Figure 
 10-4
 . In a distributed machine 
 learning process, these 
 N
  workers keep generating updates, and a shared model is 
 updated with them continuously. We count these updates by first looping over all 
 workers at one iteration and then across all the iterations. In this process, each one is 
 incrementally indexed by integer 
 t
 . The total length of this update sequence is 
 T
 . We 
 apply an analysis framework similar to that of [15]. At each barrier control point, every 
 worker A samples 
 β
  out of 
 N
  workers without replacement. If one of these sampled 
 workers lags more than 
 s
  steps behind worker A, it waits. The probabilities of a node 
 lagging 
 r
  steps are drawn from a distribution with a probability mass function 
 f
  (
 r
 ) and 
 cumulative distribution function (CDF) 
 F
 (
 r
 ). Without loss of generality, we set both 
 staleness 
 r
  and sample size 
 β
  parameters to be constants.
  
 Ideally, in a fully deterministic barrier control system such as BSP, the ordering of 
 updates in this sequence should be deterministic. We call it a 
 true sequence
 . However, 
 in reality, what we get is often a 
 noisy sequence
 , where updates are reordered 
 irregularly due to sporadic and random network and system delays. These two 
 sequences share the same length. We define 
 sequence inconsistency
  as the number of 
 index difference",NA
[ ,NA,NA
],"= 
 T",NA
∑,"t
  
 f
  
 t",NA
(,"x
  
 t",NA
),"− 
 f 
 t 
  
 t",NA
(,"x
 . This is the sum of the differences between the optimal",NA
),"value of the function and the current value given a noisy state. To put it plainly, it shows 
 the difference between “the computation result we get if all the parameter updates we 
 receive are in perfect ideal order” and “the computation result we get in the real world 
 when using, for example, PSP barrier.” Now we show the noisy system state, 
 x
 t
  , 
  
 converges in expectation toward the optimal, 
 x
 ⋆
 , in probability. Specifically, since 
 R
 [
 X
 ] is 
  
 accumulated over time, to get a time-independent metric, we need to show the value 
 R X",NA
[ ],"is bounded.
  
 T 
  
  
   
 T 
  
  
 Theorem: SGD under PSP, convergence in probability
  Let 
 f",NA
( ),"= 
 =",NA
∑,"f 
 t",NA
( ),"be a 
  
 convex function where each 
 f
 t
 ∈
 R
  is also convex. Let 
 x
 ⋆
 ∈
 R
 d
  be the minimizer of this 
  
 function. Assume that 
 f
 t
  are L-Lipschitz and that the distance between two points 
 x
  and 
  
 x
 ′
  is bounded: 
 D",NA
( ,"x x
 ′",NA
),"= 1 
 || 
 x
 −
 x
 ′|| 
 2
  
 2
 ≤
 F 
 2
  , where 
 F
  is constant. Let an update be given by 
  
 2 
 u 
 t 
 = − ∇",NA
 ( ,"x 
 t",NA
),"and the learning rate by 
 η
 t 
 =σ 
 . We have bound:
  
 t
  
  
  
   
      
     
  
 −
  
 T
  
 δ
 2
  
  
  
 R X
  
 1   
  2 
 F
  
 2
  
     
    
 c
  
 +
  
 b
 δ
  
  
  
 P 
  
  
 T
  
  − 
  
  
  
 σ
 L 
 2
  − 2 
 F
   
 − 
 q
  ≥ δ  
 ≤
  
 e
    
 3
  , 
 (10.3)
  
 T
  
 σ 
 where 
 δ
  is a constant and 
 b
 ≤ 4
 NTLσ
 . The 
 b
  term here is the upper bound on the random 
 variables which are drawn from the lag distribution 
 f
  (
 r
 ). The 
 q
  and 
 c
  are two values that 
 are related to the mean and variance of 
 γ
 t
 . If we assume that 0 < 
 a
  < 1, then it can be 
  
 proved that both 
 q
  and 
 c
  are bounded. Furthermore, if we assume with probability 
 Φ
  
 that ∀
 t
 . 4
 NLσγ
 t
  < 
 O
 (
 T
 ), then 
 b
  < 
 O
 (
 T
 ). That means 
 R X",NA
[ ],"converges to 
 O
 (
 T
 −1/2
 ), in 
 probability 
  
  
 T
  
 Φ
  with an exponential tail bound that decreases as time increases.
  
 265",NA
 How Effective Is Sampling,"One key step in proving the preceding theorem is to prove the sequence inconsistency 
 γ
 t 
 is bounded. We have proved that the mean and variance of vector 
 γ
 t
  are both bounded. 
 Specifically, the average inconsistency (normalized by sequence length 
 T
 ) is bounded by
  
 1 
 T",NA
∑,NA,NA
 ( ,NA,NA
),"≤ 
 S r r",NA
( ,"+ 
 1",NA
),"+ 
 a r",NA
( ,"+ 
 2",NA
),", 
 (10.4)
  
 T
  
 =
 t
  
  
 2",NA
( ,"− 
 a",NA
),"2
  
  
  
  
 and the variance has a similar bound:
  
 1 
  
 T",NA
∑,NA,NA
 ( ,2,NA
),"< 
 S r r",NA
( ,"+ 
 1 2",NA
) (,"r
  + 1",NA
),"+ 
 a r",NA
(,"2
  + 4",NA
),", 
 (10.5)
  
 T
  
 =
 t
  
  
   
 6",NA
( ,"− 
 a",NA
),"3
   
  
  
  
 where
  
 S
  = 
  
  
 1 − 
 a
  
  
 . 
 (10.6)
  
  
  
 F r",NA
( ,NA,NA
),NA,NA
( ,"− 
 a",NA
),"+ 
 a
  − 
 a
 T r
 − + 1
  
  
 As intimidating as these bounds may seem, they can both be treated as constants 
 for fixed 
 a
 , 
 T
 , 
 r
 , and 
 β
  values. They provide a means to quantify the impact of the PSP 
 sampling
  primitive and provide stronger convergence guarantees than ASP, shown in 
 Figure 
 10-11
 . They do not depend upon the entire lag distribution.
  
 266",NA
 Implementation Technique,"As shown in Table 
 10-1
 , barrier control methods are widely used in existing systems, 
 such as parameter servers, Hadoop, etc. Indeed, PSP is not yet widely available in many 
 systems, which means the completeness dimension in synchronization method design 
 cannot be readily utilized. The good news is that bringing the extra design dimension 
 requires minimal effort. To implement PSP atop of current data analytics frameworks, 
 developers only need to add a new primitive: sampling. As shown in Section 
 10.4
 , it is 
 straightforward to compose existing barrier methods in a distributed system.
  
 By default, we choose the trainers randomly. There are various ways to guarantee 
 the random sampling, for example, organizing the nodes into a structural overlay such as 
 the Distributed Hash Table (DHT). The random sampling is based on the fact that node 
 identifiers are uniformly distributed in a namespace. Nodes can estimate the population 
 size based on the allocated ID density in the namespace.
  
  
 The choice of samples has a great impact on the performance of PSP. The sampling 
 of PSP provides an estimate of the total distribution of the progress of all the workers. 
  
 In a worst-case scenario where the sampled subset happens to be all stragglers, this 
 subset cannot provide a very efficient estimation of all the workers. Different sampling 
 strategies can be used in certain scenarios.
  
 For example, we can change how frequently the sample changes during distributed 
 computing. Or, we can choose the workers according to their previous computation 
 time. Specifically, at each round, all the workers are categorized into two groups 
 according to their historical computing time per iteration, one slow and one fast, and 
 then choose equal numbers of workers from both groups to form the target subset. We 
 can use clustering algorithms such as K-Means.
  
 268",NA
10.6  Evaluation,"In this section, we investigate the performance of various barrier control methods in 
 experiments and the trade-off they make. We focus on two common metrics in 
 evaluating barrier strategies: the accuracy and system progress. Using these metrics, we 
 explore various barrier controls with regard to the impact of sample settings and 
 stragglers in the Federated Learning system. Besides, we also use a new metric called 
 progress inconsistency
  as a metric of training accuracy, but without the impact of specific 
 application hyperparameters.",NA
 Experiment Setup,"We perform extensive experiments on the real-world dataset FEMNIST, which is part 
 of LEAF, a modular benchmarking framework for learning in federated settings, and 
 includes a suite of open source federated datasets [8]. Similar to MNIST, the FEMNIST 
 dataset is for image classification tasks. But it contains 62 different classes (10 digits, 
 26 lowercases, and 26 uppercases). Each image is of size 28 by 28 pixels. The dataset 
 contains 805,263 samples in total. The number of samples is distributed evenly across 
 different classes.
  
 To better study the performance of the proposed method with non-IID data 
 distribution in Federated Learning, we follow the data partition setting in [7]. We first 
 sort the data by class labels, divide them into 2
 n
  shards, and assign each of 
 n
  workers 2 
 shards. This pathological non-IID partition makes the training data on different workers 
 overlap as little as possible. The validation set is 10% of the total data. Besides, we 
 preprocess it so that the validation set is roughly balanced in each class. As for training 
 hyperparameters, we use a batch size of 128, and we use the Adam optimizer, with 
 learning rate of 0.001 and coefficient of (0.9, 0.999).
  
 We conduct our experiment on a server that has 56 Intel(R) Xeon(R) CPU E5-2680 
 v4 and a memory of 256G. In the rest of this section, if not otherwise mentioned, we use 
 16 workers by default. Besides, one extra worker is used for model validation to 
 compute its accuracy. In the rest of this section, we aim to show the wide range of tuning 
 space enabled by the sampling parameter and how existing barrier methods can be 
 incorporated into PSP.
  
 269",NA
 Accuracy,"We execute the training process using each method on the non-IID FEMNIST dataset 
 for about eight epochs. The results are shown in Figure 
 10-12
 . The subfigure uses time 
 as the x axis. It shows the change of trained model accuracy in about 10,000 seconds. It 
 compares the ASP, BSP, and pBSP (composing PSP with BSP) where the sampling size 
 equals 4.
  
 The first thing to note here is, though the performance of ASP looks optimal at the 
 beginning due to its quick accumulation of updates from different workers, it quickly 
 deteriorates and fails to converge. Compared to the unstable performance of ASP, BSP 
 steadily converges. Then the pBSP clearly outperforms these two regarding model 
 accuracy, especially in the later part of training. Due to its probabilistic nature, the pBSP 
 line shows larger jitters than BSP, but also follows the general trend of BSP toward 
 convergence steadily.
  
 The strength of PSP lies in that it combines the advantages of existing methods. In 
 the lower subfigure of Figure 
 10-12
 , we use the accumulated total number of updates 
 the parameter server has received as the x axis to compare the “efficiency” of the 
 updates in ASP, SSP, and pSSP. The staleness parameter of SSP and pSSP is set to 4 here. 
 We can see that as updates are accumulating, despite using sampling, the accuracy 
 increase of pSSP is similar to that of SSP.
  
 Meanwhile, pSSP is much faster than SSP with regard to the update progress or the 
 rate at which the updates accumulate at the parameter server. Figure 
 10-13
  shows the 
 number of updates at the server with regard to time (here, we show only results from 
 the beginning of evaluations). As can be seen, at any given time, both pBSP and pSSP 
 progress faster than BSP and SSP correspondingly. Of course, ASP progresses the fastest 
 since it does not require any synchronization among workers, but its nonconverged 
 updates make this advantage obsolete.
  
 The difference of the number of updates can be directly interpreted as the 
  
 communication cost, since each update means the transmission of weight and gradient 
 between the server and clients. For example, 
 at about 600s, the pSSP incurs 35% 
 more traffic than SSP; and pBSP even doubles the traffic in BSP
 . In our experiments, 
 the PSP can reduce communication overhead without sacrificing the final model 
 accuracy.
  
 270",NA
 System Progress,"In this section, we use 32 workers and run the evaluation for 400 seconds. Figure 
 10-
 14a 
 shows the distribution of all nodes’ progress when evaluation is finished.
  
 As expected, the most strict BSP leads to a tightly bounded progress distribution, but 
 at the same time, using BSP makes all the nodes progress slowly. At the end of the 
 experiment, all the nodes only proceed to about the 80th update. As a comparison, using 
 ASP leads to a much faster progress of around 200 updates. But the cost is a much 
 loosely spread distribution, which shows no synchronization at all among nodes. SSP 
 allows certain staleness (4 in our experiment) and sits between BSP and ASP.
  
 272",NA
 Robustness to Straggler,"Stragglers are not uncommon in traditional distributed training and are pervasive in 
 the workers of Federated Learning. In this section, we show the impact of stragglers 
 on system performance and accuracy of model updates and how probabilistic 
 synchronization control by a sampling primitive can be used to mitigate such impacts.
  
 As explained before, we model the system stragglers by increasing the training time 
 of each slow trainer to 
 n
 -fold, namely, on average they spend 
 n
  times as much time as 
 normal nodes to finish one iteration. The parameter 
 n
  here is the “slowness” of the 
 system. In the experiment shown in Figure 
 10-15
 , we keep the portion of slow nodes 
 fixed and increase the slowness from 2 to 8. Then we measure the accuracy of using a 
 certain barrier control method at the end of training. To be more precise, we choose a 
 period of results before the ending and use their mean value and standard for each 
 observation point.
  
 Figure 
 10-15
  plots the decreasing model accuracy due to stragglers as a function of 
 the straggler slowness. As we can see, both ASP and BSP are sensitive to stragglers, both 
 dropping about 20% accuracy by increasing slowness from 2x to 8x, while that of pBSP 
 only drops by less than 10%. For BSP, this is mainly because the stragglers severely 
 reduce the training update progress; for ASP, this can be explained as the result of its 
 asynchronous nature, where updates from slow workers are delayed. This problem is 
 exacerbated by the non-IID data, where the data overlap between different workers is 
 limited, if not none at all. Once again, PSP takes the best of both worlds. As we have 
 shown before, its probabilistic sampling mitigates the effect of data distribution and is 
 also less prone to the progress reduction caused by stragglers.
  
 PSP is less prone to the stragglers in the system. When the slowness increases from 
 2x to 8x, both ASP and BSP are sensitive to stragglers, both dropping about 20% 
 accuracy, while that of pBSP only decreases by less than 10%.
  
 274",NA
 Sampling Settings,"In Section 
 10.6
 , we investigate how the choice of sampling size affects the progress in 
 PSP. One question is then: How to choose the suitable sample size? As pointed out in 
 Section 
 10.5
 , one important observation that can be derived from our theory proof is 
 that a small number of sampling can achieve similar performance as that using large 
 sample numbers.
  
 275",NA
 Progress Inconsistency,"In the previous section, we have evaluated the impact of barrier control methods on the 
 accuracy of three different models. However, the training accuracy is affected not only 
 by the barrier method, which controls training inconsistency, but also hyperparameters 
 such as learning rate. The tolerance of error in training for different applications also 
 varies greatly. To better understand the impact of barriers on model consistency during 
 training without considering the influence of these factors, we use progress 
 inconsistency as a metric to compare barriers.
  
 In distributed training, for a worker, between the time it pulls a model from a server 
 and updates its own local model, the server likely has already received several 
 updates from other workers. These updates are the source of training inconsistency. 
  
 We define progress inconsistency as the number of these updates between a worker’s 
 corresponding read and update operations. In this experiment, we collect the progress 
 inconsistency value of each node at its every step during training.
  
 We investigate the relationship between the number of nodes and inconsistency of 
 pBSP. All executions run for 100 seconds, and we increase workers from 50 to 500. We 
 measure the average and variance of progress inconsistency, both normalized with the 
  
 277",NA
10.7  Summary,"In this chapter, we explored the topic of distributed computing in Owl, with a focus on 
 the topic of synchronization barriers. We showed Actor, an OCaml-based distributed 
 computing engine, which has implemented three different computing paradigms, 
 namely, map-reduce, parameter server, and peer-to-peer. Orthogonal to that, it also has 
 implemented four different types of barrier control methods.
  
 We proposed the Probabilistic Synchronous Parallel, which is suitable for data 
 analytic applications deployed in large and unreliable distributed systems. It strikes a 
 good trade-off between the efficiency and accuracy of iterative learning algorithms by 
 probabilistically controlling how data is sampled from distributed workers. In Actor, 
 we implemented PSP with a core system primitive of “
 sampling
 .” We showed that the 
 sampling
  primitive can be combined with existing barrier control methods to derive 
 fully distributed solutions. We then evaluated the performance of various barrier 
 control methods. The effectiveness of PSP in different application scenarios depends 
 on the suitable parameter, that is, the sample size. Similar to the performance tuning 
 in numerical computation, we suggest resorting to prior knowledge and empirical 
 measurement for its parameter tuning and regard this as the challenge for the future 
 exploration.
  
 278",NA
CHAPTER 11,NA,NA
Testing Framework,"Every proper software requires testing, and so is Owl. All too often, we have found that 
 testing can help us discover potential errors we failed to notice during development. In 
 this chapter, we briefly introduce the philosophy of testing in Owl, the tool we use for 
 conducting the unit test, and examples to demonstrate how to write unit tests. Issues 
 such as using functors in tests and other things to notice in writing test code for Owl, etc. 
 are also discussed in this chapter.",NA
11.1  Unit Test,"There are multiple ways to perform test on your code. One common way is to use 
 assertion or catching/raising errors in the code. These kinds of tests are useful, but the 
 testing code is mixed with the function code itself, while we need separate test modules 
 that check the implementation of functions against expected behaviors.
  
 In Owl, we apply a 
 unit test
  to ensure the correctness of numerical routines as much 
 as possible. A unit test is a software test method that checks the behavior of individual 
 units in the code. In our case, the “unit” often means a single numerical function.
  
 There is an approach of software development that is called 
 test-driven development
 , 
 where you write test code even before you implement the function to be tested itself. 
 Though we don’t enforce such approach, there are certain testing principles we follow 
 during the development of Owl. For example, we generally don’t trust code that is not 
 tested, so in a GitHub pull request, it is always a good practice to accompany your 
 implementation with a unit test in the test/ directory in the source code. Besides, try to 
 keep the function short and simple, so that a test case can focus on a certain aspect.
  
 We use the alcotest framework for testing in Owl. It is a lightweight test framework 
 with simple interfaces. It exposes a simple TESTABLE module type, a check function to 
 assert test predicates, and a run function to perform a list of unit -> unit test callbacks.
  
 © Liang Wang, Jianxin Zhao 2023 
  
 281
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5_11",NA
11.2  Example,"Let’s look at an example of using alcotest in Owl. Suppose you have implemented 
 some functions in the linear algebra module, including the functions such as rank, 
 determinant, inversion, etc., and try to test them before making a pull request. The 
 testing code can be included in one test unit, and each unit consists of four major 
 sections.
  
 In the first section, we define some utility function and common constants which 
 will be used in the unit. For this example, we specify the required precision and some 
 predefined input data. Here, we use 1e-6 as the precision threshold. Two ndarrays are 
 deemed the same if the sum of their difference is less than 1e-6, as shown in mpow. 
 The predefined input data can also be defined in each test case, as in is_triu_1.
  
 open Owl 
  
 open Alcotest
  
 module M = Owl.Linalg.D
  
 (* Section #1 *)
  
 let approx_equal a b =
  
  let eps = 1e-6 in
  
  Stdlib.(abs_float (a -. b) < eps)
  
 let x0 = Mat.sequential ~a:1. 1 6
  
 The second section is the core. It contains the actual testing logic, for example, 
 whether the det function can correctly calculate the determinant of a given matrix. Every 
 testing function defined in the To_test module has self-contained logic to validate the 
 implementation of the target function.
  
 (* Section #2 *)
  
 module To_test = struct
  
  
  let rank () =
  
   
  let x = Mat.sequential 4 4 in
  
   
  M.rank x = 2
  
  let det () =
  
  
  let x = Mat.hadamard 4 in
  
 282",NA
11.3  What Could Go Wrong,"“Who’s watching the watchers?”
  Beware that the test code itself is still code and thus can 
 also be wrong. We need to be careful in implementing the testing code. There are certain 
 cases that you may want to check.",NA
 Corner Cases,"Corner cases involve situations that occur outside of normal operating parameters. That 
 is obvious in the testing of convolution operations. As the core operation in deep neural 
 networks, convolution is complex: it contains input, kernel, strides, padding, etc. as 
 parameters. Therefore, special cases such as 1x1 kernel, strides of different height and 
 width, etc. are tested in various combinations, sometimes with different input data.
  
 module To_test_conv2d_back_input = struct
  
  
  (* conv2D, 1x1 kernel *)
  
  
  let fun00 () =
  
  
  
  let expected =
  
  
   
  [| 30.0; 36.0; 42.0; 66.0; 81.0; 96.0; 102.0
  
  
  
  
  ; 126.0; 150.0; 138.0; 171.0; 204.0
  
  
    
  ; 174.0; 216.0; 258.0; 210.0; 261.0; 312.0 |]
   
  in
  
  
  
  verify_value test_conv2d
  
  
   
  [| 1; 2; 3; 3 |] [| 1; 1; 3; 3 |] [| 1; 1 |]
  
  
  
  
 VALID expected
  
  (* conv2D, 1x2 kernel, stride 3. width 5 *)
  
  let fun01 () =
  
  
  let expected =
  
   
  [| 2271.0; 2367.0; 2463.0
  
    
  ; 2901.0; 3033.0; 3165.0 |]
  
  
  in
  
  
  verify_value test_conv2d
  
   
  [| 1; 2; 3; 3 |] [| 2; 2; 3; 3 |] [| 1; 1 |]
  
  
  VALID 
 expected
  
 287",NA
 Test Coverage,"Another issue is test coverage. It means the percentage of code for which an associated 
 test has existed. Though we don’t seek a strict 100% coverage for now, wider test 
 coverage is always a good idea. For example, in our implementation of the repeat 
 operation, depending on whether the given axes contain one or multiple integers, the 
 implementation changes. Therefore, in the test functions, it is crucial to cover both cases.",NA
11.4  Use Functor,"Note that we can still benefit from all the powerful features in OCaml such as the functor. 
 For example, in testing the convolution operations, we would like to test the 
 implementation of both that in the core library (which is implemented in C) and that in 
 the base library (in pure OCaml). Apparently, there is no need to write the same unit test 
 code twice for these two sets of implementation. To solve that problem, we have a test 
 file unit_conv2d_genericl.ml that has a large module that contains all the previous four 
 sections:
  
 module Make (N : Ndarray_Algodiff with type elt = float) = struct (* Section 
 #1 - #4 *)
  
  ...
  
 end
  
 288",NA
11.5  Performance Tests,"For a numerical library, being able to calculate correct results is not enough. How fast a 
 function can calculate also matters; actually, it matters a lot in modern real-time data 
 analysis, which has wide applications in many fields such finance, robotics, flight 
 control, etc. In addition to correctness, a performance test is also included in the Owl 
 testing framework. The following simple generic function runs a target function for 
 certain amount of times, then calculates the average speed:
  
 (* test one operation c times, output the mean time *) let 
 test_op s c op =
  
  
  let ttime = ref 0. in
  
  
  for i = 1 to c do
  
   
  Gc.compact ();
  
   
  let t0 = Unix.gettimeofday () in
  
   
  let _ = op () in
  
   
  let t1 = Unix.gettimeofday () in
  
   
  ttime := !ttime +. (t1 -. t0)
  
  
  done;
  
  
  ttime := !ttime /. (float_of_int c);
  
  
  Printf.printf ""| %s :\t %.8fs \n"" s !ttime;
  
  
  flush stdout
  
  
 This function for testing each operation is similar but prints out the traces more 
 eagerly in every iteration:
  
 (* test one operation c time, output the used time in each evaluation *) let 
 test_op_each c op =
  
  
  Printf.printf ""| test some fun %i times\n"" c;
  
 289",NA
11.6  Summary,"In this chapter, we briefly introduced how the unit tests are performed with the alcotest 
 framework in the existing Owl codebase. We used one example piece of test code for the 
 linear algebra module in Owl to demonstrate the general structure of the Owl test code. 
 We then discussed some tips we find helpful in writing tests, such as considering corner 
 cases, test coverage, and using functors to simplify the test code. In practice, we find the 
 unit tests come really handy in development, and we just cannot have too much of them.
  
  
 Open Access
   This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 291",NA
 APPENDIX A,NA,NA
Basic Analytics ,NA,NA
Examples,"Owl supports a variety of classical numerical analytics methods, including mathematical 
 functions, linear algebra, statistics, ordinary differential equations, signal processing, 
 etc. In this appendix, we introduce some of the classic analytics supported in Owl and 
 architecture design.",NA
 A.1 Mathematical Functions,"As the first step in classic analytics, Owl provides various sorts of mathematical 
 operations, from the basic ones such as addition or subtraction to complex ones such 
 as trigonometry functions to even more complex special functions such as the Beta 
 functions. They are included in the Maths module. These functions are fundamental 
 to any scientific computation applications in different fields including mathematical 
 analysis, physics, and so on.
  
 To ensure performance, these functions are implemented in C. Many rely on the 
 standard math library in C such as those included in the math.h, and many special 
 functions are interfaced to the 
 Cephes Mathematical Functions Library
  [39], a C 
 language library that provides implementation of such functions of interest to scientists 
 and engineers. It provides good cross-platform consistency and accurate long double 
 computation.",NA
 A.2 Linear Algebra,"Linear algebra is an important area of mathematics. It is widely used in all scientific 
 domains. As a result, optimized linear algebra routines have been well studied. The core 
 implementation of linear algebra follows the Basic Linear Algebra Subprograms (BLAS) 
 and Linear Algebra Package (LAPACK) interfaces. There are well-established libraries to 
 implement these interfaces, such as OpenBLAS and Intel MKL.",NA
 A.3 Statistical Distributions,"Statistical distributions are one of the most fundamental analytic tools in mathematics, 
 but are also prevalent in modern applications. For example, in both softmax activation 
 layers in a neural network and logistic regression, their outputs can be interpreted as 
 posterior distribution. To facilitate statistical functions, Owl provides the Stats module. 
  
 It contains many frequently used distributions, both discrete and continuous, such as 
 Gaussian, exponential, logistic, Laplace, Poisson, Rayleigh, etc. For each distribution, 
 there is a set of related functions having the distribution’s name as a common prefix. For 
 example, the module provides the following functions for the Gaussian distribution:
  
 • gaussian_rvs, random number generator
  
 • gaussian_pdf, probability density function
  
 • gaussian_cdf, cumulative distribution function
  
 • gaussian_ppf, percent point function (inverse of CDF)
  
 • gaussian_sf, survival function (1 – CDF)
  
 • gaussian_isf, inverse survival function (inverse of SF)
  
 • gaussian_logpdf, logarithmic probability density function
  
 • gaussian_logcdf, logarithmic cumulative distribution function
  
 • gaussian_logsf, logarithmic survival function
  
 Of these functions, the random number generator, probability density function, 
 and cumulative distribution function are the key functions for a given distribution. We 
 can use a simple binomial probability distribution as an example to demonstrate their 
 usages, as the following code shows. These routines provide intuitive interfaces.
  
 # let x = [|0; 1; 2; 3|] 
  
 # let p = Array.map (Stats.binomial_pdf ~p:0.3 ~n:3) x val p : 
 float array =
  
  
  [|0.342999999999999916; 0.440999999999999837;
  
   
  0.188999999999999918; 0.0269999999999999823|]
  
 295",NA
 A.4 Signal Processing,"We rely on signals such as sound and images to convey information. Signal processing 
 is then the field concerned with analyzing, generating, and transforming signals. 
  
 Its applications can be found in a wide range of fields: audio processing, speech 
  
 recognition, image processing, communication system, data science, etc. The most 
 important functionality in signal processing is no doubt the fast Fourier transform 
 (FFT). Owl provides a series of FFT functions.
  
 296",NA
 A.5 Ordinary Differential Equation,"A 
 differential equation
  is an equation that contains a function and one or more of its 
 derivatives. They have been studied ever since the invention of calculus, driven by 
 applications in fields including mechanics, astronomy, geometry, biology, engineering, 
 economics, and many more [56]. If the function and its derivatives in a differential 
 equation concern only one variable, we call it an 
 ordinary differential equation
  (ODE), 
 and it can model a one-dimensional dynamical system. Owl provides the functionalities 
 to solve ODEs. In general, an ODE can be expressed as
  
 F x y y
  , , 
 y
  
 n
  
 0. 
 (A.1)
  
 298",NA
 APPENDIX B,NA,NA
System Conventions,"All software systems have their own rules and conventions with which 
 developers must comply, and Owl is no exception. In this appendix, we cover 
 function naming and various other conventions in the Owl library.",NA
 B.1 Pure vs. Impure,"The Ndarray module contains functions to manipulate and perform mathematical 
 operations over ndarrays. The 
 pure
  (or immutable) functions are those that do not 
 modify the provided variables but instead return a new one as a result. In contrast, 
 impure
  (or mutable) functions are those which modify the passed-in variables in 
 place.
  
 Functional programming in general promotes the use of immutable data structures 
 as impure functions make it more difficult to reason about the correctness of code. On 
 the other hand, generating a fresh 1000 × 1000 matrix every time you modify a single 
 element does not seem very practical. The introduction of impure functions into Owl is 
 thus only done under careful, practical consideration. In-place modification avoids 
 expensive memory allocation and deallocation which can significantly improve runtime 
 performance of a numerical application, especially when large ndarrays and matrices 
 are involved. Thus, many pure functions in the Ndarray module have corresponding 
 impure versions indicated by an underscore “_” added at the end of the function name. 
 For example, the following pure functions in the Arr module, Arr.sin and Arr.add, have 
 corresponding impure functions, Arr.sin_ and Arr.add_.
  
 For unary operators such as Arr.sin_ x, the situation is rather straightforward when x 
 will be modified in place. However, binary operators are more complex. For example, in 
 Arr.add_scalar x a, x will be modified in place to store the final result – relatively 
 straightforward as a is a scalar. In Arr.add_ x y, the question is where to store the final 
 result when both inputs are ndarrays. Let’s look at the type of Arr.add_ function:
  
 val Arr.add_ : ?out:Arr.arr -> Arr.arr -> Arr.arr -> unit
  
 © Liang Wang, Jianxin Zhao 2023 
  
 303
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5",NA
 B.2 Ndarray vs. Scalar,"There are three general ndarray operators: 
 map
 , 
 scan
 , and 
 reduce
 . Many Ndarray 
 functions can be categorized as reduce operations, such as Arr.sum, Arr.prod, Arr.min, 
 Arr.mean, and Arr.std. For example, we can sum all the elements in an ndarray using 
 sum function.
  
 let x = Arr.sequential [|3;3;3|];; 
  
 let a = Arr.sum x;;
  
  
 The result is a one-element ndarray, so to treat it as a scalar, you must retrieve 
 the single value by calling the get function.
  
 304",NA
 B.3 Module Structures,"Owl’s Dense module contains modules supporting dense data structures such as Dense. 
 Matrix supporting operations on dense matrices and the Sparse module with modules 
 supporting sparse data structures. We have four basic modules: dense ndarray, dense 
 matrix, sparse ndarray, and sparse matrix. In this book, we mostly use Dense modules, 
 and, indeed, the matrix modules are a special case that’s built upon the corresponding 
 ndarray modules.
  
 With Dense.Ndarray, we can create a dense 
 n
 -dimensional array of no more than 16 
 dimensions. This constraint originates from the underlying Bigarray.Genarray module in 
 OCaml. In practice, this constraint makes sense since the space requirement will explode 
 as the dimension increases. If you need anything higher than 16 dimensions, you need to 
 use Sparse.Ndarray to create a sparse data structure. All four modules consist of five 
 submodules to handle different types of numbers:
  
 • The S module supports single-precision float numbers float32.
  
 • The D module supports double-precision float numbers float64.
  
 305",NA
 B.4 Infix Operators,"In each Ndarray and Matrix module, some frequently used functions have the 
 corresponding infix operators as convenient aliases. Table 
 B-2
  summarizes the 
 operators. In the table, both x and y represent either a matrix or an ndarray, while a 
 represents a scalar value.
  
 Several things in this table should be noted:
  
 • +$ has its corresponding operator $+ if we flip the order of 
  
 parameters. However, it is important to be very careful about operator 
 precedence in these cases: 
 OCaml determines precedence based on the 
 first character of the infix operator
 . +$ preserves the precedence, 
 whereas $+ does not. We thus recommend using $+ with great care and 
 always using parentheses to explicitly specify precedence. The same 
 applies to $-, $*, and $/.
  
 • For comparison operators, for example, both = and =. compare all the 
 elements in two variables x and y. The difference is that = returns a 
 boolean value, whereas =. returns a matrix or ndarray of the same shape 
 and type as x and y. In the returned result, the value in a given position is 
 1 if the values of the corresponding position in x and y satisfy the 
 predicate; otherwise, it is 0.
  
 • The comparison operators ending with $ are used to compare a 
 matrix/ndarray to a scalar value.
  
 307",NA
 APPENDIX C,NA,NA
Metric ,NA,NA
Systems and ,NA,NA
Constants,"In many scientific computing problems, numbers are not abstract but reflect the realistic 
 meanings. In other words, these numbers only make sense on top of a well-defined 
 metric system. For example, when we talk about the distance between two objects, we 
 write down a number 30, but what does 30 mean in reality? Is it meters, kilometers, 
 miles, or lightyears? Another example, what is the speed of light? Well, this really 
 depends on what metrics you are using, for example, 
 km
 /
 s
 , 
 m
 /
 s
 , 
 mile
 /
 h
 , etc. Things can 
 get really messy in computation if we do not unify the metric system in a numerical 
 library. The translation between different metrics is often important in a real-world 
 application; therefore, a full-featured numerical library is obliged to provide sufficient 
 support for managing different metric systems. In this appendix, we briefly introduce 
 the metric system and constants provided in the Owl library to support scientific 
 computing.",NA
 C.1 Four Metric Systems,"There are four metrics adopted in Owl, and all of them are wrapped in the Owl. 
 Const module:
  
 • 
  
 Const.SI: International System of Units
  
 311
  
 • 
  
 Const.MKS: MKS System of Units
  
 • 
  
 Const.CGS: Centimeter-Gram-Second System of Units
  
 • 
  
 Const.CGSM: Electromagnetic System of Units",NA
 C.2 International System of Units,"Now that you know how to use constants, we will use the International System of Units 
 (SI) module as an example to show the constants we include in Owl. The source code is 
 included as follows from where you can check the real values defined for all constants:
  
 module SI = struct
  
  
  let speed_of_light = 2.99792458e8
  
  
  let gravitational_constant = 6.673e-11
  
  
  let plancks_constant_h = 6.62606896e-34
  
  
  let plancks_constant_hbar = 1.05457162825e-34
  
  
 let astronomical_unit = 1.49597870691e11
  
  
  let light_year = 9.46053620707e15
  
  
  let parsec = 3.08567758135e16
  
  
  let grav_accel = 9.80665e0
  
  
  let electron_volt = 1.602176487e-19
  
  
  let mass_electron = 9.10938188e-31
  
  
  let mass_muon = 1.88353109e-28
  
  
  let mass_proton = 1.67262158e-27
  
  
  let mass_neutron = 1.67492716e-27
  
  
  let rydberg = 2.17987196968e-18
  
  
  let boltzmann = 1.3806504e-23
  
  
  let molar_gas = 8.314472e0
  
  
  let standard_gas_volume = 2.2710981e-2
  
  
  let minute = 6e1
  
  
  let hour = 3.6e3
  
  
  let day = 8.64e4
  
  
  let week = 6.048e5
  
  
  let inch = 2.54e-2
  
  
  let foot = 3.048e-1
  
  
  let yard = 9.144e-1
  
  
  let mile = 1.609344e3
  
  
  let nautical_mile = 1.852e3
  
  
  let fathom = 1.8288e0
  
  
  let mil = 2.54e-5
  
  
  let point = 3.52777777778e-4
  
  
  let texpoint = 3.51459803515e-4 
  
 316",NA
 APPENDIX D,NA,NA
Algodiff Module,"In the rest of appendixes, we provide some important pieces of source code of several 
 Owl modules. It complements existing materials we have discussed in this book, so that 
 the readers can have a deeper understanding of how they work.
  
 In this appendix, we provide the full source code of several components in the 
 algorithmic differentiation module. It consists of three parts. First are the templates to 
 generate operators (owl_algodiff_ops_builder.ml) and examples that generate operators 
 using these templates (owl_algodiff_ops.ml). Learning these code is instrumental in 
 understanding how AD works. Second are the core functionalities provided in 
 owl_algodiff_core.ml and owl_algodiff_generic.ml. These 
  
 functionalities look simple enough, but they make the backbone of the whole module. 
 Third is the graph traversal module that can convert the AD graph into multiple formats. 
  
 It comes handy when debugging and better understanding the details of an AD graph.",NA
 D.1 Operator Building,"The builder templates:
  
 (** owl_algodiff_ops_builder.ml *)
  
 module Make (Core : Owl_algodiff_core_sig.Sig) = struct 
  
 open Core
  
 let cmp_tag ai bi = if ai > bi then 1 else if ai < bi then -1 else 0
  
 module type Siso = sig 
  
 val label : string 
  
 val ff_f : A.elt -> t 
  
 val ff_arr : A.arr -> t
  
 © Liang Wang, Jianxin Zhao 2023 
  
 325
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5",NA
 D.2 Core Modules ,"(** Owl_algodiff_core.ml *) 
  
 module Make (A : Owl_types_ndarray_algodiff.Sig) = struct 
  
 include Owl_algodiff_types.Make (A) 
  
  
 module A = A 
  
  
 (* generate global tags *) 
  
  
 let _global_tag = ref 0
  
 352",NA
 D.3 Graph Converter,"(** owl_algodiff_graph_converter.ml *)
  
 odule Make (Core : Owl_algodiff_core_sig.Sig) = struct 
  
 open Core
  
 (* _traverse_trace and its related functions are used to convert the 
  
 computation graph 
 generated in backward mode into human-readable format.
  
  
 You can make your own convert function to generate needed format. *) 
 let 
 _traverse_trace x = 
  
   
 (* init variables for tracking nodes and indices *) 
  
   
 let nodes = Hashtbl.create 512 in 
  
   
 let index = ref 0 in 
  
   
 (* local function to traverse the nodes *) 
  
   
 let rec push tlist = 
  
     
 match tlist with
  
    
  | [] -> ()
  
    
  | hd :: tl -> 
  
      
 if Hashtbl.mem nodes hd = false 
  
      
 then (
  
 358",NA
 APPENDIX E,NA,NA
Neural Network ,NA,NA
Module,NA,NA
 E.1 Graph Module,"(* 
  
 * OWL - OCaml Scientific Computing 
  
 * Copyright (c) 2016-2022 Liang Wang <liang@ocaml.xyz> *)
  
 (** Neural network: Graphical neural network *)
  
 open Owl_types
  
 (* Make functor starts *)
  
 module Make (Neuron : Owl_neural_neuron_sig.Sig) = struct 
 module Neuron = Neuron 
  
 open Neuron 
  
 open Neuron.Optimise.Algodiff
  
 (* graph network and node definition *)
  
 type node =
  
  
  { mutable name : string
  
  
  ; (* name of a node *) 
  
   
 mutable prev : node array
  
  
  ; (* parents of a node *) 
  
   
 mutable next : node array
  
  
  ; (* children of a node *) 
  
   
 mutable neuron : neuron
  
  
  ; (* neuron contained in a node *) 
  
   
 mutable output : t option
  
 © Liang Wang, Jianxin Zhao 2023 
  
 363",NA
 E.2 Neuron Modules,"Activation Module
  
 module Activation = struct 
  
  
 type typ =
  
   
  | Elu (* Exponential linear unit *)
  
   
  | Relu (* Rectified linear unit *)
  
   
  | Sigmoid (* Element-wise sigmoid *)
  
   
  | HardSigmoid (* Linear approximation of sigmoid *)
  
   
  | Softmax of int (* Softmax along specified axis *)
  
   
  | Softplus (* Element-wise softplus *)
  
   
  | Softsign (* Element-wise softsign *)
  
   
  | Tanh (* Element-wise tanh *)
  
   
  | Relu6 (* Element-wise relu6 *)
  
   
  | LeakyRelu of float (* Leaky version of a Rectified Linear Unit *)
   
  | 
 TRelu of float (* Thresholded Rectified Linear Unit *)
  
   
  | Custom of (t -> t) (* Element-wise customised activation *)
  
  
  | 
 None
  
 (* None activation *)
  
 type neuron_typ =
  
  { mutable activation : typ
  
  ; mutable in_shape : int array
  
  ; mutable out_shape : int array
  
  }
  
 let create activation = { activation; in_shape = [||]; out_shape = [||] }
  
 let connect out_shape l =
  
  l.in_shape <- Array.copy out_shape;
  
  l.out_shape <- Array.copy out_shape
  
 let run_activation x activation = 
  
   
 match activation with
  
  
  |  Elu        -> Maths.(relu x + (x |> neg |> relu |> neg |> 
  
  
  
 exp) - _f 1.)
  
  
  | Relu        -> Maths.relu x
  
 386",NA
 APPENDIX F,NA,NA
Actor System for ,NA,NA
Distributed ,NA,NA
Computing,NA,NA
 F.1 MapReduce Engine ,"Interface 
  
 (* 
  
  
 * Actor - Parallel & Distributed Engine of Owl System 
  
  
 * Copyright (c) 2016-2018 Liang Wang <liang.wang@cl.cam.ac.uk> 
  
 *) 
  
 (* Data Parallel: Map-Reduce module *) 
  
 val init : string -> string -> unit 
  
 val map : ('a -> 'b) -> string -> string 
  
 val map_partition : ('a list -> 'b list) -> string -> string val flatmap : ('a -> 'b 
 list) -> string -> string 
  
 val reduce : ('a -> 'a -> 'a) -> string -> 'a option 
  
 val reduce_by_key : ('a -> 'a -> 'a) -> string -> string 
  
 val fold : ('a -> 'b -> 'a) -> 'a -> string -> 'a 
  
 val filter : ('a -> bool) -> string -> string 
  
 val flatten : string -> string 
  
 val shuffle : string -> string
  
 © Liang Wang, Jianxin Zhao 2023 
  
 423",NA
 F.2 Parameter Server Engine,"Interfaces
  
 (* 
  
 * Actor - Parallel & Distributed Engine of Owl System 
  
 * Copyright (c) 2016-2018 Liang Wang <liang.wang@cl.cam.ac.uk> *)
  
 (* Model Parallel: Parameter server module *)
  
 open Actor_types
  
 (* context type, duplicate from Actor_types *) 
 type 
 param_context = Actor_types.param_context
  
 type barrier =
  
  | ASP
 (* Asynchronous Parallel *)
  
  | BSP
 (* Bulk Synchronous Parallel *)
  
  | SSP
 (* Stale Synchronous Parallel *)
  
  | PSP
 (* Probabilistic Synchronous Parallel *)
  
 (** core interfaces to parameter server *)
  
 val start : ?barrier:barrier -> string -> string -> unit 
 (** start 
 running the model loop *)
  
 val register_barrier : ps_barrier_typ -> unit 
  
 (** register user-defined barrier function at p2p server *)
  
 val register_schedule : ('a, 'b, 'c) ps_schedule_typ -> unit 
 (** register 
 user-defined scheduler *)
  
 val register_pull : ('a, 'b, 'c) ps_pull_typ -> unit 
  
 (** register user-defined pull function executed at master *)
  
 val register_push : ('a, 'b, 'c) ps_push_typ -> unit 
  
 (** register user-defined push function executed at worker *)
  
 val register_stop : ps_stop_typ -> unit 
 (** register 
 stopping criterion function *)
  
 435",NA
 F.3 Peer-to-Peer Engine,"Interfaces
  
 (* 
  
 * Actor - Parallel & Distributed Engine of Owl System 
  
 * Copyright (c) 2016-2018 Liang Wang <liang.wang@cl.cam.ac.uk> *)
  
 (* Peer-to-Peer Parallel *)
  
 open Actor_types
  
 (** start running the model loop *) 
  
 val start : string -> string -> unit
  
 (** register user-defined barrier function at p2p server *) 
 val 
 register_barrier : p2p_barrier_typ -> unit
  
 (** register user-defined pull function at p2p server *) 
 val 
 register_pull : ('a, 'b) p2p_pull_typ -> unit
  
 (** register user-defined scheduler at p2p client *) 
 val 
 register_schedule : 'a p2p_schedule_typ -> unit
  
 442",NA
 Bibliography,"[1]. Mart
 ı
 ́n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy 
 Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey 
 Irving, Michael Isard, et al. Tensorflow: A system for large-scale 
 machine learning. In 
 12th
  {
 USENIX
 } 
 symposium on operating 
 systems design and implementation (
 {
 OSDI
 } 
 16)
 , pages 265–283, 
 2016.
  
  [2]. Amr Ahmed, Mohamed Aly, Joseph Gonzalez, Shravan 
  
 Narayananmuthy, and Alexander Smola. Scalable inference in 
 latent variable models. 
 WSDM
 , pages 123–132, 2012.
  
  [3]. Istemi Ekin Akkus, Ruichuan Chen, Ivica Rimac, Manuel Stein, 
 Klaus Satzke, Andre Beck, Paarijaat Aditya, and Volker Hilt. 
 Sand: Towards high-performance serverless computing. In 
 2018 
 USENIX Annual Technical Conference (USENIX ATC’18)
 , pages 
 923–935, 2018.
  
  [4]. Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and 
 distributed deep learning: An in-depth concurrency analysis. 
 ACM Computing Surveys (CSUR),
  52(4):1–43, 2019.
  
  [5]. Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan 
 Pascanu. Advances in optimizing recurrent networks. In 
 2013 
 IEEE international conference on acoustics, speech and signal 
 processing
 , pages 8624–8628. IEEE, 2013.
  
  [6]. Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry 
 Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub 
 Konecny, Stefano Mazzocchi, H Brendan McMahan, and Others. 
 Towards federated learning at scale: System design. 
 arXiv preprint 
 arXiv:1902.01046
 , 2019.
  
 © Liang Wang, Jianxin Zhao 2023 
  
 457
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5",NA
Index,NA,NA
A,"Activation function, 126, 127, 131 
  
 Actor distributed engine 
  
  
 barrier control methods, 252–256 
  
  
 composing, OWl, 249, 250 
  
  
 definition, 245 
  
  
 framework, 249 
  
  
 map-reduce, 245, 246 
  
  
 parameter server module, 246, 248 
  
 synchronization methods, 254 
  
 Actor system 
  
  
 MapReduce engine, 423–428, 430–
 434 
  
 parameter server engine, 435–
 442 
  
  
 peer-to-peer engine, 442–455 
  
 Adagrad and RMSprop methods, 93 
  
 Adagrad method, 91 
  
 Adam optimizer, 92 
  
 add_node function, 128 
  
 add_scalar function, 154 
  
 adjoint function, 72 
  
 adjval function, 73 
  
 alcotest framework, 281, 291 
  
 Algodiff.gradhessian function, 98 
  
 Algodiff module, 51, 107, 109, 155, 156 
 Algorithmic differentiation (AD), 5, 51 
  
 Algodiff module, 51, 52 
  
  
 APIs, 70, 72, 73, 75 
  
  
 computation, 53 
  
  
 core modules, 352–356, 358 
  
  
 data types, 53, 58–60 
  
  
 definition, 49 
  
  
 forward/reverse modes, 54–58
  
  
 graph converter, 358, 359, 361 
  
  
 graph utility, 80, 81 
  
  
 lazy expression, 78 
  
  
 module design, 79 
  
  
 Ndarray module, 81–84 
  
  
 operations, 61, 62 
  
  
 operator building, 325–334, 336–
 352 
  
 operators 
  
  
  
 Mat, 63 
  
  
  
 rules, 63–65, 68 
  
  
  
 SISO, 66, 69 
  
  
 perturbation confusion/tag, 76, 77 
 Amazon Lambda, 234 
  
 Arithmetic Logic Unit (ALU), 15 
  
 Arr.add_ function, 303 
  
 Artificial Intelligence (AI), 1, 191, 233 
 Asynchronous barrier method, 265 
  
 Asynchronous Parallel (ASP), 253 
  
 Automated Empirical Optimization of 
  
  
 Software (AEOS), 42–47",NA
B,"backward function, 132 
  
 Barrier control mechanisms, 245, 252, 
 260, 263 
  
 Basic Linear Algebra Subprograms 
  
 (BLAS), 293 
  
 Batch module, 100 
  
 Bigarray module, 9, 218 
  
 Builder module, 83 
  
 Bulk Synchronous Parallel (BSP), 253, 262
  
 © Liang Wang, Jianxin Zhao 2023 
  
 465
  
 L. Wang and J. Zhao, 
 Architecture of Advanced Numerical Analysis Systems
 , 
 https://doi.org/10.1007/978-1-4842-8853-5",NA
C,"Cache optimization 
  
  
 loop alignment, 30 
  
  
 loop merging, 29 
  
  
 matrix multiplication, 28 
  
  
 tiling, 28 
  
 CGCompiler.model function, 159 
 CGCompiler.Neural module, 159 
 CGraph module, 153, 155, 188 
  
 Checkpoint module, 101 
  
 Classic numerical analytics 
  
  
  
  
 methods, 5 
  
 Compiler backends 
  
  
 base library, 215–217 
  
  
 core function stack, 216 
  
  
 fold operation, 226 
  
  
 gradient descent, 227 
  
  
 JavaScript, 217, 225 
  
  
 MirageOS 
  
  
  
 gradient descent, 221, 223 
  
  
  
 neural network, 224 
  
  
  
 unikernels, 221 
  
  
 size of executables, 228 
  
 Composition/deployment 
  
  
 microservices architecture, 233 
  
 ML-based data analytics, 234 
  
  
 ML-model based prediction, 234 
  
 ML services, 240 
  
  
 modularity, 233 
  
  
 NST, 238 
  
  
 serverless computing, 234 
  
  
 session types, 240 
  
  
 Zoo, script sharing 
  
  
  
 designing Owl, 229 
  
  
  
 example, 230 
  
  
  
 Python, 229 
  
  
  
 version control, 231, 232 
  
  
 Zoo system architecture, 235
  
 466
  
  
 Zoo system, design 
  
  
  
 development, 235 
  
  
  
 DSL, 237, 238 
  
  
  
 service, 236 
  
  
  
 service discovery, 238 
  
  
  
 type checking, 236, 237 
  
 Compressed Sparse 
  
  
  
 Column (CSC), 10 
  
 Compressed Sparse Row (CSR) format, 10 
 Computation graph 
  
  
 algorithmic differentiation process, 
  
  
  
  
 CGraph, 155–157 
  
  
 benefits, 152 
  
  
 CGraph memory optimization, 
  
  
  
   
 DNN, 187 
  
  
 DAG, 149 
  
  
 definition, 149 
  
  
 design 
  
  
  
 CGraph stack, 161 
  
  
  
 computing device, 163, 164, 166 
  
  
 computing engine, 180–182 
  
  
  
 engine functor, 162 
  
  
  
 functor stack, Owl, 160 
  
  
  
 linking nodes, creating, 169–172 
  
  
 Ndarray, 161 
  
  
  
 operation types, 166 
  
  
  
 optimization, 173, 175–177, 179, 
 180 
   
 principles, 160 
  
  
  
 shape inference function, 167–169 
  
 DNN, 158, 159 
  
  
 dynamic graph, 150, 151 
  
  
 function, 150 
  
  
 machine learning framework, 152 
  
  
 math function, 157 
  
  
 memory allocation, 188 
  
  
 memory usage, 183–186 
  
  
 numerical operation, 153, 154 
  
  
 static graph, 151",NA
D,"“Deep” function, 61 
  
 Deep learning compiler, 193
  
 analytical model, 258 
 BSP/ASP, 256 
  
 compatibility, 260–
 264 consistency, 256
  
 467",NA
E ,"Eigen library, 33 
  
 Elastic_net method, 118
  
 Himmelblau function, 110 
 implementation, 106 
  
 iterate, 107, 108 
  
 methods, 94, 96
  
  
 ENABLE_OPENMP flag, 45 
  
 Euler-Mascheroni constant, 315
  
  
 newton’s method, 98 
  
 Gradient descent method, 88, 89, 96, 109, 
  
 eval_gen function, 181 
  
 111, 222
  
 _eval_terms function, 181
  
 Graph.backward function, 138 
  
 Graphics Processing Unit (GPU), 6, 161,",NA
F,"163–166, 191, 192, 215, 244 
 Graph.train function, 134
  
 Fast Fourier transform (FFT), 14, 296, 298
  
 Fast R-CNN approach, 140
  
 “Feature maps”, 141, 143–145 
  
 Federated Learning, 244, 245, 253, 
  
 269, 274",NA
H ,"Himmelblau function, 110 
 Horovod framework, 244
  
 fft function, 297
  
 “float32_sin”, 12 
  
 foreign function, 223 
  
 forward function, 131 
  
 Full-fledged numerical library, 3 
  
 functor stack, 153, 160, 161, 215–
 217",NA
I,"im2col algorithm, 33 
  
 infer function, 224 
  
 infer_shape function, 201 
  
 Instance segmentation, 138–
 140",NA
G,"Instruction-level parallelism (ILP), 17 
 Intel Xeon Phi processor, 21
  
  
 Garbage collector (GC), 162, 183 
 Gaussian distribution, 124, 295
  
 468
  
 Intermediate representation (IR), 6, 
 193, 194",NA
"J, K","JavaScript 
  
  
 edge devices, 217 
  
  
 Facebook reason leverages, 220 
  
 native Ocaml, 218, 219 
  
 js_of_ocaml command, 218 
  
 Js_of_ocaml, 218 
  
 Just-in-time compilation (JIT), 151
  
 batch module, 100 
  
 checkpoint module, 101–103 
 components 
  
  
 learning rate, 90–93 
  
  
 submodules, 89, 90 
  
 definition, 87 
  
 gradient descent method, 88 
 momentum, 98 
  
 params submodules, 104, 106",NA
L,"problem, 87 
  
 Math Kernel Library (MKL), 14, 42
  
  
 LaTeX engine, 208–210 
  
 Learning_rate module, 90 
  
 Linear algebra, 293, 294 
  
 Linear Algebra Package (LAPACK) 
  
 interfaces, 293 
  
 _linear_reg function, 114 
  
 Linear regression models, 112 
  
 Loop alignment technique, 30 
  
 L2norm regularization function, 
 118
  
 Maths.sin function, 219 
  
 Matrix Multiply Unit (MXU), 192 
 MaxPool operation, 201 
  
 Memory 
  
  
 cache, 21–23 
  
  
 categories, 21 
  
  
 definition, 21 
  
  
 NUMA, 25 
  
  
 prefetching, 24, 25 
  
  
 register, 21",NA
M,"Memory hierarchy, 20, 21 
  
 Metric systems and constants
  
  
 M2.train function, 250 
  
 Machine learning (ML), 2, 50, 90, 233, 243
  
 four metrics, 311, 312 
  
 International System of 
  
 Make function, 153 
  
 Units, 316–323
  
 make_node function, 128, 200 
 make_reverse function, 71
  
 math constants, 315 
  
 physical constants, 313, 314
  
 make_then_connect function, 171 
  
 SI module, 316
  
 map and fold operations, 225 
  
 MapReduce programming model, 245 
  
 MapReduce engine, 249 
  
 Mask Region-based Convolutional Neural 
  
 Network (MR-CNN), 139
  
 well-defined metric system, 311 
 minimise_fun function, 110 
  
 minimise_network function, 131 
 mirage build command, 223 
  
 Mirage module, 223
  
 Mat, 63 
  
 MirageOS, 3
  
 Mathematical operations, 11, 32, 43, 161, 
  
 Model.detect function, 147
  
 197, 293 
  
 Momentum mechanism, 98
  
 Mathematical optimization
  
 Monolithic software application, 233",NA
N,"Numerical differentiation, 50, 
 51 NumPy, 2, 4, 9, 12, 193, 205
  
 Ndarray functions, 304
  
 Ndarray module, 4, 5, 9, 11, 47, 81, 82, 85, 
  
 153, 215–217, 249, 303",NA
O,"N-dimensional array (ndarray), 4 
  
 Nesterov Accelerated Gradient (Nesterov) 
  
 method, 100 
  
 Neural network compiler module, 
  
 Object detection 
  
 definition, 138 
  
 MRCNN, 139, 141–145, 
 147 network architecture
  
 122, 133–138 
  
 R-CNN, 140, 141
  
 Neural network model, 202, 206 
  
 OCaml-based distributed computing 
  
 Neural network module 
  
 engine, 7, 243
  
  
 graph, 363–376, 378–385 
  
  
 neuron, 386–422 
  
 Neural networks, 6, 51, 58, 77, 88, 192, 
 215
  
 OCaml-MirageOS applications, 225 
  
 OCaml’s lazy evaluation mechanism, 77 
 of_symbolic function, 204
  
 Neural Style Transfer 
  
 ONNX engine
  
 (NST) algorithm, 238 
  
 example, 205
  
 Neurons 
  
 ModelProto, 203
  
 activation module, 126, 
 127 core function, 124–
 126 
  
 definition, 123
  
 neural network, 206–208 
 neural network model, 202 
 OCaml types/serialization 
  
 newton function, 75 
  
 function, 203
  
 Newton’s method, 75, 98 
  
 variable initialization, 205, 206
  
 Nonlinearity, 126 
  
 OpenLambda, 234
  
 Non-uniform memory access 
  
 Open Multi-Processing (OpenMP), 
  
 (NUMA), 25–26 
  
 26, 43, 45
  
 Numerical applications, 1, 5, 220 
 Numerical computing, OCaml 
  
 architecture 
  
  
  
 advanced design, Owl, 5, 6 
  
  
 hardware/depolyment, 6
  
 Open Neural Network Exchange (ONNX) 
 format, 193, 202 
  
 Optimise module, 89, 106, 109, 114, 130 
 Optimiser functor, 173 
  
 Optimiser module, 118
  
 Owl, 4, 5 
  
 _optimise_term function, 173
  
 subsystem, 3 
  
 Optimization
  
 system programming, 3 
  
 AEOS, 42, 43, 45, 47
  
 Zoo subsystem, 7 
  
 convolution, 32
  
 470",NA
R,"owl-zoo library, 231
  
 Region Proposal Network (RPN), 141 
 register function, 60",NA
"P, Q ","package parameter, 
 223
  
 register_* functions, 248 
 Regression 
  
  
 definition, 111
  
  
 Parallel computing, 4, 7, 15, 17, 30, 
 252 Parameter server module, 246
  
 features/predictors, 111 
 implementation, 113–
 115
  
 Parameter servers 
  
 linear, 112
  
 (PS), 243, 248, 252, 268 
  
 loss, 112
  
 pattern_* functions, 173 
  
 types, 116, 117
  
 pattern_002 function, 179 
 Peer-to-peer approach, 
 244
  
 reverse function, 52 
  
 RMSprop method, 92
  
 Peer-to-peer (p2p) engine, 245 
  
 run function, 95",NA
S,"Scientific computing, 1, 2, 4, 6, 28, 153, 
 217 SciPy, 2, 301 
  
 Semantic segmentation, 139, 141 
  
 Service discovery mechanism, 238 
  
 sess.run command, 205 
  
 Shared memory multiprocessing 
  
  
 programming, 26, 252 
  
 Signal processing, 1, 3, 5, 9, 296–298 
  
 SIMD-oriented Fast Mersenne Twister 
  
 (SFMT) generator, 296
  
 “Single-input-single-output” (SISO), 66, 69 
 Single instruction multiple data (SIMD), 
  
 15, 17–19, 27, 31, 38, 40, 191, 296
  
 Test-driven development, 281 
 Testing 
  
  
 alcotest, example, 282–286 
  
 corner cases, 287, 288 
  
  
 functors, 291 
  
  
 performance tests, 289, 290 
  
 test coverage, 288 
  
  
 unit, 281 
  
  
 use functor, 288, 289 
  
 Tiling, 28 
  
 to_string function, 94 
  
 to_string method, 103 
  
 To_test module, 282 
  
 train_generic function, 130
  
 Software system, 1, 3, 303 
 sqr_magic function, 230",NA
U,"Stale Synchronous Parallel (SSP), 254 
 Statistical distributions, 295–296 
  
 Stochastic gradient descent (SGD), 243, 
  
 244, 246, 253
  
 Unconstrained optimization, 87 
  
 Unit test, 281, 285, 288 
  
 unpack_arr or unpack_elt function, 154
  
 StreamExecutor, 193 
  
 Superscalar, 17 
  
 Symbolic differentiation, 50, 51, 55 
 System conventions 
  
  
 Dense module, 305, 306 
  
  
 infix operators, 307, 309 
  
  
 Ndarray 
 vs
 . scalar, 304",NA
"V, W, X, Y","Vectorization 
  
 AVX instructions, 18 
  
 memory, 20 
  
 multicore processor, 
 20 SIMD, 18
  
  
 pure 
 vs
 . impure, 303
  
  
 switching context, 19 
  
 Vector Processing Unit (VPU), 192",NA
T,"Visualization function, 80 
  
 Von Neumann architecture, 21
  
 tangent function, 70
  
 tanh function, 53 
 TensorFlow, 6, 151",NA
Z,"Tensor Processing Unit (TPU), 6, 192
  
 472
  
 zero function, 62",NA
