Larger Text,Smaller Text,Symbol
Purely Functional Data Structures,NA,NA
Chris Okasaki,NA,NA
September 1996,"CMU-CS-96-177
  
 School of Computer Science 
  
 Carnegie Mellon University 
  
 Pittsburgh, PA 15213
  
 Submitted in partial fulfillment of the requirements
  
 for the degree of Doctor of Philosophy.
  
 Thesis Committee: 
  
 Peter Lee, Chair 
  
 Robert Harper 
  
 Daniel Sleator 
  
 Robert Tarjan, Princeton University
  
 Copyright c 1996 Chris Okasaki
  
 This research was sponsored by the Advanced Research Projects Agency (ARPA) under Contract No. F19628-
 95-C-0050.
  
 The views and conclusions contained in this document are those of the author and should not be interpreted as 
 representing the official policies, either expressed or implied, of ARPA or the U.S. Government.",NA
Abstract,"When a C programmer needs an efficient data structure for a particular prob-
 lem, he or she can often simply look one up in any of a number of good text-
 books or handbooks. Unfortunately, programmers in functional languages such as 
 Standard ML or Haskell do not have this luxury. Although some data struc-tures 
 designed for imperative languages such as C can be quite easily adapted to a 
 functional setting, most cannot, usually because they depend in crucial ways on 
 as-signments, which are disallowed, or at least discouraged, in functional 
 languages. To address this imbalance, we describe several techniques for 
 designing functional data structures, and numerous original data structures based 
 on these techniques, including multiple variations of lists, queues, double-ended 
 queues, and heaps, many supporting more exotic features such as random access 
 or efficient catena-tion.
  
 In addition, we expose the fundamental role of lazy evaluation in
  amortized 
 functional data structures. Traditional methods of amortization break down when 
 old versions of a data structure, not just the most recent, are available for further 
 processing. This property is known as
  persistence
 , and is taken for granted in 
 functional languages. On the surface, persistence and amortization appear to be 
 incompatible, but we show how lazy evaluation can be used to resolve this 
 conflict, yielding amortized data structures that are efficient even when used 
 persistently. Turning this relationship between lazy evaluation and amortization 
 around, the notion of amortization also provides the first practical techniques for 
 analyzing the time requirements of non-trivial lazy programs.
  
 Finally, our data structures offer numerous hints to programming language 
 de-signers, illustrating the utility of combining strict and lazy evaluation in a 
 single language, and providing non-trivial examples using polymorphic recursion 
 and higher-order, recursive modules.",NA
Acknowledgments,"Without the faith and support of my advisor, Peter Lee, I probably wouldn’t 
 even be a graduate student, much less a graduate student on the eve of finishing. 
 Thanks for giving me the freedom to turn my hobby into a thesis.
  
 I am continually amazed by the global nature of modern research and how e-
 mail allows me to interact as easily with colleagues in Aarhus, Denmark and 
 York, England as with fellow students down the hall. In the case of one such 
 colleague, Gerth Brodal, we have co-authored a paper without ever having met. 
 In fact, sorry Gerth, but I don’t even know how to pronounce your name!
  
 I was extremely fortunate to have had excellent English teachers in high 
 school. Lori Huenink deserves special recognition; her writing and public 
 speaking classes are undoubtedly the most valuable classes I have ever taken, in 
 any subject. In the same vein, I was lucky enough to read my wife’s copy of Lyn 
 Dupr´e’s
  BUGS in Writing
  just as I was starting my thesis. If your career 
 involves writing in any form, you owe it to yourself to buy a copy of this book.
  
 Thanks to Maria and Colin for always reminding me that there is more to life 
 than grad school. And to Amy and Mark, for uncountable dinners and other out-
 ings. We’ll miss you. Special thanks to Amy for reading a draft of this thesis.
  
 And to my parents: who would have thought on that first day of kindergarten that 
 I’d still be in school 24 years later?",NA
Contents ,"Abstract 
  
 v
  
 1
  
 Acknowledgments
  
 vii
  
 Introduction
  
 1
  
 2
  
 1.1
  
 Functional vs. Imperative Data Structures . . . . . . . . . . . . . . . . . . . .
  
 1
  
 1.2
  
 Strict vs. Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 2
  
 1.3
  
 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 3
  
 1.4
  
 Source Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 4
  
 1.5
  
 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 4
  
 1.6
  
 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 5
  
 Lazy Evaluation and $-Notation
  
 7
  
 3
  
 2.1
  
 Streams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 9
  
 2.2
  
 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 10
  
 Amortization and Persistence via Lazy Evaluation
  
 13
  
 3.1
  
 Traditional Amortization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 13
  
 3.1.1
  
 Example: Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 15
  
 3.2
  
 Persistence: The Problem of Multiple Futures . . . . . . . . . . . . . . . . . .
  
 19
  
 3.2.1
  
 Execution Traces and Logical Time . . . . . . . . . . . . . . . . . . .
  
 19
  
 3.3
  
 Reconciling Amortization and Persistence . . . . . . . . . . . . . . . . . . . .
  
 20
  
 3.3.1
  
 The Role of Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . .
  
 20
  
 3.3.2
  
 A Framework for Analyzing Lazy Data Structures
  
 . . . . . . . . . . .
  
 21",NA
Chapter 1,NA,NA
Introduction,"Efficient data structures have been studied extensively for over thirty years, resulting in a vast 
 literature from which the knowledgeable programmer can extract efficient solutions to a stun-
 ning variety of problems. Much of this literature purports to be language-independent, but 
 unfortunately it is language-independent only in the sense of Henry Ford: Programmers can 
 use any language they want, as long as it’s imperative.
 1
 Only a small fraction of existing data 
 structures are suitable for implementation in functional languages, such as Standard ML or 
 Haskell. This thesis addresses this imbalance by specifically considering the design and 
 analysis of functional data structures.",NA
1.1 ,NA,NA
Functional vs. Imperative Data Structures,"The methodological benefits of functional languages are well known [Bac78, Hug89, HJ94], 
 but still the vast majority of programs are written in imperative languages such as C. This 
 apparent contradiction is easily explained by the fact that functional languages have 
 historically been slower than their more traditional cousins, but this gap is narrowing. 
 Impressive advances have been made across a wide front, from basic compiler technology to 
 sophisticated analyses and optimizations. However, there is one aspect of functional 
 programming that no amount of cleverness on the part of the compiler writer is likely to 
 mitigate — the use of inferior or inappropriate data structures. Unfortunately, the existing 
 literature has relatively little advice to offer on this subject.
  
 Why should functional data structures be any more difficult to design and implement than 
 imperative ones? There are two basic problems. First, from the point of view of designing 
 and implementing efficient data structures, functional programming’s stricture against 
 destructive
  
  
 1
 Henry Ford once said of the available colors for his Model T automobile, “[Customers] can have any color 
 they want, as long as it’s black.”",NA
1.2 Strict vs. Lazy Evaluation,"Most (sequential) functional programming languages can be classified as either
  strict
  or
  lazy
 , 
 according to their order of evaluation. Which is superior is a topic debated with religious 
 fervor by functional programmers. The difference between the two evaluation orders is most 
 apparent in their treatment of arguments to functions. In strict languages, the arguments to a 
 function are evaluated before the body of the function. In lazy languages, arguments are 
 evaluated in a demand-driven fashion; they are initially passed in unevaluated form and are 
 evaluated only when (and if!) the computation needs the results to continue. Furthermore, 
 once a given argument is evaluated, the value of that argument is cached so that if it is ever 
 needed again, it can be looked up rather than recomputed. This caching is known as
  
 memoization
  [Mic68].
  
 Each evaluation order has its advantages and disadvantages, but strict evaluation is clearly 
 superior in at least one area: ease of reasoning about asymptotic complexity. In strict lan-
 guages, exactly which subexpressions will be evaluated, and when, is for the most part syn-
 tactically apparent. Thus, reasoning about the running time of a given program is relatively 
 straightforward. However, in lazy languages, even experts frequently have difficulty 
 predicting when, or even if, a given subexpression will be evaluated. Programmers in such 
 languages",NA
1.3 ,NA,NA
Contributions,NA,NA
1.4 Source Language,"All source code will be presented in Standard ML [MTH90], extended with primitives for 
 lazy evaluation. However, the algorithms can all easily be translated into any other functional 
 language supporting both strict and lazy evaluation. Programmers in functional languages 
 that are either entirely strict or entirely lazy will be able to use some, but not all, of the data 
 structures in this thesis.
  
 In Chapters 7 and 8, we will encounter several recursive data structures that are difficult 
 to describe cleanly in Standard ML because of the language’s restrictions against certain 
 sophisti-cated and difficult-to-implement forms of recursion, such as polymorphic recursion 
 and recur-sive modules. When this occurs, we will first sacrifice executability for clarity and 
 describe the data structures using ML-like pseudo-code incorporating the desired forms of 
 recursion. Then, we will show how to convert the given implementations to legal Standard 
 ML. These examples should be regarded as challenges to the language design community to 
 provide a programming language capable of economically describing the appropriate 
 abstractions.",NA
1.5 Terminology,"Any discussion of data structures is fraught with the potential for confusion, because the term 
 data structure
  has at least four distinct, but related, meanings.",NA
1.6 ,NA,NA
Overview,"This thesis is structured in two parts. The first part (Chapters 2–4) concerns algorithmic 
 aspects of lazy evaluation. Chapter 2 sets the stage by briefly reviewing the basic concepts of 
 lazy evaluation and introducing
  $
 -notation.
  
 Chapter 3 is the foundation upon which the rest of the thesis is built. It describes the 
 mediating role lazy evaluation plays in combining amortization and persistence, and gives 
 two methods for analyzing the amortized cost of data structures implemented with lazy 
 evaluation.
  
 Chapter 4 illustrates the power of combining strict and lazy evaluation in a single 
 language. It describes how one can often derive a worst-case data structure from an 
 amortized data struc-ture by systematically scheduling the premature execution of lazy 
 components.
  
  
 2
 The persistent identity of an ephemeral data structure can be reified as a reference cell, but this is 
 insufficient for modelling the persistent identity of a persistent data structure.",NA
Chapter 2,NA,NA
Lazy Evaluation and $-Notation,"Lazy evaluation is an evaluation strategy employed by many purely functional programming 
 languages, such as Haskell [H 
 +
 92]. This strategy has two essential properties. First, the evalu-
 ation of a given expression is delayed, or
  suspended
 , until its result is needed. Second, the 
 first time a suspended expression is evaluated, the result is
  memoized
  (i.e., cached) so that the 
 next time it is needed, it can be looked up rather than recomputed.
  
 Supporting lazy evaluation in a strict language such as Standard ML requires two primi-
 tives: one to suspend the evaluation of an expression and one to resume the evaluation of a 
 suspended expression (and memoize the result). These primitives are often called 
 delay
  and
  
 for c e
 . For example, Standard ML of New Jersey offers the following primitives for lazy eval-
 uation:
  
 type
  susp
  
 val
  delay : (unit 
 !
  
 val
  force : susp 
 !
  
 )
  
 !
  
  susp
  
 These primitives are sufficient to encode all the algorithms in this thesis. However, program-
 ming with these primitives can be rather inconvenient. For instance, to suspend the evaluation 
 of some expression 
 e
 , one writes 
 delay
  (
 fn
  () 
 ) e
 ). Depending on the use of whitespace, this introduces 
 an overhead of 13–17 characters! Although acceptable when only a few expressions are to be 
 suspended, this overhead quickly becomes intolerable when many expressions must be 
 delayed.
  
  
 To make suspending an expression as syntactically lightweight as possible, we instead use 
 $
 -notation — to suspend the evaluation of some expression 
 e
 , we simply write
  $
 e
 .
  $ 
 e
  is called a
  
 suspension
  and has type 
  
 susp
 , where
  
  is the type of 
 e
 . The scope of the
  $
  operator extends 
 as far to the right as possible. Thus, for example,
  $
 f 
  
 x
  parses as
  $
 (
 f 
  
 x
 ) rather than (
 $
 f
  ) 
 x
  
 and
  $
 x
 +
 y
  parses as
  $
 (
 x
 +
 y
 ) rather than (
 $
 x
 )+
 y
 . Note that
  $
 e
  is itself an expression and can be 
 suspended by writing
  $$
 e
 , yielding a nested suspension of type 
  
 susp susp
 .",NA
2.1 ,NA,NA
Streams,"As an extended example of lazy evaluation and
  $
 -notation in Standard ML, we next develop
  
 a small streams package. These streams will also be used by several of the data structures in
  
 subsequent chapters.
  
 Streams (also known as lazy lists) are very similar to ordinary lists, except that every cell
  
 is systematically suspended. The type of streams is
  
 datatype 
 withtype
  
  StreamCell = Nil 
 j
  Cons
  of
  
  Stream = StreamCell susp
  
  Stream
  
 A simple stream containing the elements 1, 2, and 3 could be written
  
 $
 Cons (1,
  $
 Cons (2,
  $
 Cons (3,
  $
 Nil)))
  
 It is illuminating to contrast streams with simple suspended lists of type 
  
 list susp
 . The
  
 computations represented by the latter type are inherently
  monolithic
  — once begun by forcing
  
 the suspended list, they run to completion. The computations represented by streams, on the
  
 other hand, are often
  incremental
  — forcing a stream executes only enough of the computation
  
 to produce the outermost cell and suspends the rest. This behavior is common among datatypes
  
 such as streams that contain nested suspensions.
  
 To see this difference in behavior more clearly, consider the append function, written 
 s
  ++
  
 t
 . On suspended lists, this function might be written
  
 fun 
 s
  ++ 
 t
  =
  $
 (force 
 s
  @ force 
 t
 )
  
 Once begun, this function forces both its arguments and then appends the two lists, producing
  
 the entire result. Hence, this function is monolithic. On streams, the append function is written
  
 fun
  
 s
  ++
  
 t
  =
  $case 
 s
  of
  
 $
 Nil 
 )
  force
  
 j
  $
 Cons (
 x
 , 
 s 0
 )
  
 t
  
 s
  
 0
  ++
  
 t
 )
  
 )
  Cons (
 x
 ,
  
 Once begun, this function forces the first cell of 
 s
  (by matching against a
  $
  pattern). If this cell
  
 is 
 Nil
 , then the first cell of the result is the first cell of 
 t
 , so the function forces 
 t
 . Otherwise,
  
 the function constructs the first cell of the result from the first element of 
 s
  and — this is the
  
 key point — the suspension that will eventually calculate the rest of the appended list. Hence,
  
 this function is incremental. The 
 take
  function described earlier is similarly incremental.
  
 However, consider the function to drop the 
 first
  
 fun
  drop (
 n
 ,
  
 s
 ) =
  let fun
  drop 
 0
  (0, 
 s 0
 ) = force 
 s
  
 j
  drop 
 0
  (
 n
 ,
  $
 Nil) = Nil
  
 0",NA
2.2 Historical Notes,"Lazy Evaluation 
 Wadsworth [Wad71] first proposed lazy evaluation as an optimization of 
 normal-order reduction in the lambda calculus. Vuillemin [Vui74] later showed that, under 
 certain restricted conditions, lazy evaluation is an optimal evaluation strategy. The formal 
 semantics of lazy evaluation has been studied extensively [Jos89, Lau93, OLT94, AFM 
 +
 95].",NA
Chapter 3,NA,NA
Amortization and Persistence via Lazy ,NA,NA
Evaluation,"Over the past fifteen years, amortization has become a powerful tool in the design and 
 analysis of data structures. Implementations with good amortized bounds are often simpler 
 and faster than implementations with equivalent worst-case bounds. Unfortunately, standard 
 techniques for amortization apply only to ephemeral data structures, and so are unsuitable for 
 designing or analyzing functional data structures, which are automatically persistent.
  
 In this chapter, we review the two traditional techniques for analyzing amortized data 
 struc-tures — the
  banker’s method
  and the
  physicist’s method
  — and show where they break 
 down for persistent data structures. Then, we demonstrate how lazy evaluation can mediate 
 the con-flict between amortization and persistence. Finally, we adapt the banker’s and 
 physicist’s meth-ods to analyze lazy amortized data structures.
  
 The resulting techniques are both the first techniques for designing and analyzing persis-
 tent amortized data structures and the first practical techniques for analyzing non-trivial lazy 
 programs.",NA
3.1 ,NA,NA
Traditional Amortization,"The notion of amortization arises from the following observation. Given a sequence of oper-
 ations, we may wish to know the running time of the entire sequence, but not care about the 
 running time of any individual operation. For instance, given a sequence of 
 n
  operations, we 
 may wish to bound the total running time of the sequence by 
 O (n)
  without insisting that each 
 individual operation run in 
 O ()
  time. We might be satisfied if a few operations run in 
 O (log n) 
 or even 
 O (n)
  
 time, provided the total cost of the sequence is only 
 O (n)
 . This freedom opens",NA
3.1.1 ,NA,NA
Example: Queues,NA,NA
3.2 ,NA,NA
Persistence: The Problem of Multiple Futures,"In the above analyses, we implicitly assumed that queues were used ephemerally (i.e., in a
  
 single-threaded fashion). What happens if we try to use these queues persistently?
  
 Let 
 q
  be the result of inserting 
 n
  elements into an initially empty queue, so that the front
  
 list of 
 q
  contains a single element and the rear list contains 
 n
  
  elements. Now, suppose
  
 we use 
 q
  persistently by taking its tail 
 n
  times. Each call of 
 tail q
  takes 
 n
  actual steps. The
  
 total actual cost of this sequence of operations, including the time to build 
 q
 , is 
 n 
  
 + n
 . If the
  
 operations truly took 
 O ()
  amortized time each, then the total actual cost would be only 
 O (n)
 .
  
 Clearly, using these queues persistently invalidates the 
 O ()
  amortized time bounds proved
  
 above. Where do these proofs go wrong?
  
 In both cases, a fundamental requirement of the analysis is violated by persistent data struc-
  
 tures. The banker’s method requires that no credit be spent more than once, while the physi-
  
 cist’s method requires that the output of one operation be the input of the next operation (or,
  
 more generally, that no output be used as input more than once). Now, consider the second
  
 call to 
 tail q
  in the example above. The first call to 
 tail q
  spends all the credits on the rear list
  
 of 
 q
 , leaving none to pay for the second and subsequent calls, so the banker’s method breaks.
  
 And the second call to 
 tail q
  reuses 
 q
  rather than the output of the first call, so the physicist’s
  
 method breaks.
  
 Both these failures reflect the inherent weakness of any accounting system based on ac-
  
 cumulated savings — that savings can only be spent once. The traditional methods of amor-
  
 tization operate by accumulating savings (as either credits or potential) for future use. This
  
 works well in an ephemeral setting, where every operation has only a single logical future. But
  
 with persistence, an operation might have multiple logical futures, each competing to spend
  
 the same savings.",NA
3.2.1 ,NA,NA
Execution Traces and Logical Time,"What exactly do we mean by the “logical future” of an operation?
  
 We model logical time with
  execution traces
 , which give an abstract view of the history
  
 of a computation. An execution trace is a directed graph whose nodes represent “interesting”
  
 operations, usually just update operations on the data type in question. An edge from 
 v
  to 
 v
  
 0
  
 indicates that operation 
 v 0
  uses some result of operation 
 v
 . The
  logical history
  of operation
  
 v
 , denoted 
 v
 , is the set of all operations on which the result of 
 v
  depends (including 
 v
  itself).
  
 In other words, 
 v
  is the set of all nodes 
 w
  such that there exists a path (possibly of length 0)
  
 from 
 w
  to 
 v
 . A
  logical future
  of a node 
 v
  is any path from 
 v
  to a terminal node (i.e., a node",NA
3.3 Reconciling Amortization and Persistence,"In the previous section, we saw that traditional methods of amortization break in the presence 
 of persistence because they assume a unique future, in which the accumulated savings will be 
 spent at most once. However, with persistence, multiple logical futures might all try to spend 
 the same savings. In this section, we show how the banker’s and physicist’s methods can be 
 repaired by replacing the notion of accumulated savings with accumulated debt, where debt 
 measures the cost of unevaluated lazy computations. The intuition is that, although savings 
 can only be spent once, it does no harm to pay off debt more than once.",NA
3.3.1 The Role of Lazy Evaluation,"Recall that an
  expensive
  operation is one whose actual costs are greater than its (desired) 
 amor-tized costs. For example, suppose some application 
 f x
  is expensive. With persistence, a",NA
3.3.2 ,NA,NA
A Framework for Analyzing Lazy Data Structures,"We have just shown that lazy evaluation is necessary to implement amortized data structures 
 purely functionally. Unfortunately, analyzing the running times of programs involving lazy 
 evaluation is notoriously difficult. Historically, the most common technique for analyzing 
 lazy programs has been to pretend that they are actually strict. However, this technique is 
 completely inadequate for analyzing lazy amortized data structures. We next describe a basic 
 frameworkto support such analyses. In the remainder of this chapter, we will adapt the 
 banker’s and physicist’s methods to this framework, yielding both the first techniques for 
 analyzing persistent amortized data structures and the first practical techniques for analyzing 
 non-trivial lazy programs.
  
 We classify the costs of any given operation into several categories. First, the
  unshared 
 cost 
 of an operation is the actual time it would take to execute the operation under the 
 assumption that every suspension in the system at the beginning of the operation has already 
 been forced and memoized (i.e., under the assumption that 
 for c e
  always takes 
 O ()
  time, except for 
 those suspensions that are created and forced within the same operation). The
  shared cost
  of 
 an operation is the time that it would take to execute every suspension created but not 
 evaluated by the operation (under the same assumption as above). The
  complete cost
  of an 
 operation is",NA
3.4 ,NA,NA
The Banker’s Method,"We adapt the banker’s method to account for accumulated debt rather than accumulated 
 savings by replacing credits with debits. Each debit represents a constant amount of 
 suspended work. When we initially suspend a given computation, we create a number of 
 debits proportional to its shared cost and associate each debit with a location in the object. 
 The choice of location for each debit depends on the nature of the computation. If the 
 computation is
  monolithic
  (i.e., once begun, it runs to completion), then all debits are usually 
 assigned to the root of the result. On the other hand, if the computation is
  incremental
  (i.e., 
 decomposable into fragments that may be executed independently), then the debits may be 
 distributed among the roots of the partial results.
  
 The amortized cost of an operation is the unshared cost of the operation plus the number 
 of debits discharged by the operation. Note that the number of debits created by an operation 
 is
  not 
 included in its amortized cost. The order in which debits should be discharged depends 
 on how the object will be accessed; debits on nodes likely to be accessed soon should be 
 discharged first. To prove an amortized bound, we must show that, whenever we access a 
 location (possibly triggering the execution of a suspension), all debits associated with that 
 location have already been discharged (and hence the suspended computation has been paid 
 for). This guarantees that the total number of debits discharged by a sequence of operations is 
 an upper bound on the realized shared costs of the operations. The total amortized costs are 
 therefore an upper bound on the total actual costs. Debits leftover at the end of the 
 computation correspond to unrealized shared costs, and are irrelevant to the total actual costs.
  
 Incremental functions play an important role in the banker’s method because they allow 
 debits to be dispersed to different locations in a data structure, each corresponding to a nested 
 suspension. Then, each location can be accessed as soon as its debits are discharged, without 
 waiting for the debits at other locations to be discharged. In practice, this means that the initial 
 partial results of an incremental computation can be paid for very quickly, and that subsequent 
 partial results may be paid for as they are needed. Monolithic functions, on the other hand, are 
 much less flexible. The programmer must anticipate when the result of an expensive 
 monolithic computation will be needed, and set up the computation far enough in advance to 
 be able to discharge all its debits by the time its result is needed.",NA
3.4.1 ,NA,NA
Justifying the Banker’s Method,"In this section, we justify the claim that the total amortized cost is an upper bound on the total 
 actual cost. The total amortized cost is the total unshared cost plus the total number of debits 
 discharged (counting duplicates); the total actual cost is the total unshared cost plus the 
 realized shared costs. Therefore, we must show that the total number of debits discharged is 
 an upper bound on the realized shared costs.",NA
3.4.2 ,NA,NA
Example: Queues,"We next develop an efficient persistent implementation of queues, and prove that every opera-
  
 tion takes only 
 O ()
  amortized time using the banker’s method.
  
 Based on the discussion in the previous section, we must somehow incorporate lazy eval-
  
 uation into the design of the data structure, so we replace the pair of lists in the previous
  
 implementation with a pair of streams.
 1
 To simplify later operations, we also explicitly track
  
 the lengths of the two streams.
  
 datatype
  
  Queue = Queue
  
 f
 F :
  
  Stream, LenF : int, R :
  
  Stream, LenR : int
 g
  
 Note that a pleasant side effect of maintaining this length information is that we can trivially
  
 support a constant-time 
 size
  function.
  
 Now, waiting until the front list becomes empty to reverse the rear list does not leave suf-
  
 ficient time to pay for the reverse. Instead, we periodically
  rotate
  the queue by moving all the
  
 elements of the rear stream to the end of the front stream, replacing 
 F
  with 
 F
  ++ 
 r everse R
  and
  
 setting the new rear stream to empty (
 $ 
 Nil
 ). Note that this transformation does not affect the
  
 relative ordering of the elements.
  
 When should we rotate the queue? Recall that 
 r everse
  is a monolithic function. We must
  
 therefore set up the computation far enough in advance to be able to discharge all its debits by
  
 the time its result is needed. The 
 r everse
  computation takes 
 jRj
  steps, so we will allocate 
 jRj
  
 debits to account for its cost. (For now we ignore the cost of the ++ operation). The earliest the
  
 r everse
  suspension could be forced is after 
 jF j
  applications of 
 tail
 , so if we rotate the queue
  
 when 
 jRj 
  
 jF j
  and discharge one debit per operation, then we will have paid for the reverse
  
 by the time it is executed. In fact, we will rotate the queue whenever 
 R
  becomes one longer
  
 than 
 F
 , thereby maintaining the invariant that 
 jF j 
  
 jRj
 . Incidentally, this guarantees that 
 F
  
 is empty only if 
 R
  is also empty. The major queue functions can now be written as follows:
  
 fun
  snoc (Queue 
 f
 F = 
 f
  , LenF = 
 lenF
 , R = 
 r
 , LenR = 
 lenR g
 , 
 x
 ) =
  
 queue 
 f
 F = 
 f
  , LenF = 
 lenF
  , R =
  $
 Cons (
 x
 , 
 r
 ), LenR = 
 lenR
 +1
 g
  
 fun
  head (Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), ... 
 g
 ) = 
 x
  
 fun
  tail (Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 queue 
 f
 F = 
 f
  , LenF = 
 lenF 
  
 1, R = 
 r
 , LenR = 
 lenR g
  
 where the pseudo-constructor 
 queue
  guarantees that 
 jF j 
  
 jRj
 .
  
 fun
  queue (
 q
  as 
 f
 F = 
 f
  , LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 if 
 lenR lenF
  then
  Queue 
 q
  
 else
  Queue 
 f
 F = 
 f
  ++ reverse 
 r
 , LenF = 
 lenF
  +
 lenR
 , R =
  $
 Nil, LenR = 0
 g
  
 The complete code for this implementation appears in Figure 3.3.
  
 1
 Actually, it would be enough to replace only the front list with a stream, but we replace both for simplicity.",NA
3.5 The Physicist’s Method,"Like the banker’s method, the physicist’s method can also be adapted to work with 
 accumulated debt rather than accumulated savings. In the traditional physicist’s method, one 
 describes a potential function
  
  that represents a lower bound on the accumulated 
 savings. To work with debt instead of savings, we replace
  
  with a function
  
  that maps 
 each object to a potential representing an upper bound on the accumulated debt (or at least, an 
 upper bound on this object’s portion of the accumulated debt). Roughly speaking, the 
 amortized cost of an operation is then the complete cost of the operation (i.e., the shared and 
 unshared costs) minus the change in potential. Recall that an easy way to calculate the 
 complete cost of an operation is to pretend that all computation is strict.
  
 Any changes in the accumulated debt are reflected by changes in the potential. If an op-
 eration does not pay any shared costs, then the change in potential is equal to its shared cost,",NA
3.5.1 Example: Queues,"We next adapt our implementation of queues to use the physicist’s method. Again, we show
  
 that every operation takes only 
 O ()
  amortized time.
  
 Because there is no longer any reason to prefer incremental suspensions over monolithic
  
 suspensions, we use suspended lists instead of streams. In fact, the rear list need not be sus-
  
 pended at all, so we represent it with an ordinary list. Again, we explicitly track the lengths of
  
 the lists and guarantee that the front list is always at least as long as the rear list.
  
 Since the front list is suspended, we cannot access its first element without executing the
  
 entire suspension. We therefore keep a working copy of a prefix of the front list. This working
  
 copy is represented as an ordinary list for efficient access, and is non-empty whenever the front
  
 list is non-empty. The final datatype is
  
 datatype
  
  Queue = Queue
  of
  
 f
 W :
  
  list, F :
  
  list susp, LenF : int, R :
  
  list, LenR : int
 g
  
 The major functions on queues may then be written
  
 fun
  snoc (Queue 
 f
 W = 
 w
 , F = 
 f
  , LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 , 
 x
 ) =
  
 queue 
 f
 W = 
 w
 , F = 
 f
  , LenF = 
 lenF
  , R = 
 x
  :: 
 r
 , LenR = 
 lenR
 +1
 g
  
 fun
  head (Queue 
 f
 W = 
 x
  :: 
 w
 , ... 
 g
 ) = 
 x
  
 fun
  tail (Queue 
 f
 W = 
 x
  :: 
 w
 , F = 
 f
  , LenF = 
 lenF
 , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 queue 
 f
 W = 
 w
 , F =
  $
 tl (force 
 f
  ), LenF = 
 lenF 
  
 1, R = 
 r
 , LenR = 
 lenR g
 )
  
 The pseudo-constructor 
 queue
  must enforce two invariants: that 
 R
  is no longer than 
 F
 , and
  
 that 
 W
  is non-empty whenever 
 F
  is non-empty.
  
 fun
  checkW 
 f
 W = [ ], F = 
 f
  , LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 Queue 
 f
 W = force 
 f
  , F = 
 f
  , LenF = 
 lenF
 , R = 
 r
 , LenR = 
 lenR g
 )
  
 j
  checkW 
 q
  = Queue 
 q
  
 fun
  checkR (
 q
  as 
 f
 W = 
 w
 , F = 
 f
  , LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 if 
 lenR lenF
  then 
 q
  
 else let val 
 w 0
  = force 
 f
  
 in 
 f
 W = 
 w 0
 , F =
  $
 (
 w 0
  @ rev 
 r
 ), LenF = 
 lenF
  +
 lenR
 , R = [ ], LenR = 0
 g
  end
  
 fun
  queue 
 q
  = checkW (checkR 
 q
 )
  
 The complete implementation of these queues appears in Figure 3.4.
  
 To analyze these queues using the physicist’s method, we choose a potential function
  
  in
  
 such a way that the potential will be zero whenever we force the suspended list. This happens
  
 in two situations: when 
 W
  becomes empty and when 
 R
  becomes longer than 
 F
 . We therefore
  
 choose
  
  to be
  
 (q ) = min (jW j; jF j 
  
 jRj)",NA
3.5.2 Example: Bottom-Up Mergesort with Sharing,"The majority of examples in the remaining chapters use the banker’s method rather than the
  
 physicist’s method. Therefore, we give a second example of the physicist’s method here.
  
 Imagine that you want to sort several similar lists, such as 
 xs
  and 
 x
  :: 
 xs
 , or 
 xs
  @ 
 zs
  and
  
 ys
  @ 
 zs
 . For efficiency, you wish to take advantage of the fact that these lists share common
  
 tails, so that you do not repeat the work of sorting those tails. We call an abstract data type for
  
 this problem a
  sortable collection
 .
  
 Figure 3.5 gives a signature for sortable collections. Note that the 
 new
  function, which
  
 creates an empty collection, is parameterized by the “less than” relation on the elements to be
  
 sorted.
  
 Now, if we create a sortable collection 
 xs 0
  by adding each of the elements in 
 xs
 , then we
  
 can sort both 
 xs
  and 
 x
  :: 
 xs
  by calling 
 sort xs 0
  and 
 sort
  (
 add
  (
 x
 , 
 xs 0
 )).",NA
3.6 ,NA,NA
Related Work,"Debits 
  
 Some analyses using the traditional banker’s method, such as Tarjan’s analysis of 
 path compression [Tar83], include both credits and debits. Whenever an operation needs 
 more credits than are currently available, it creates a credit-debit pair and immediately spends 
 the credit. The debit remains as an obligation that must be fulfilled. Later, a surplus credit 
 may be used to discharge the credit.
 2
 Any debits that remain at the end of the computation add 
 to the total actual cost. Although there are some similarities between the two kinds of debits, 
 there are also some clear differences. For instance, with the debits introduced in this chapter, 
 any debits leftover at the end of the computation are silently discarded.
  
  
 It is interesting that debits arise in Tarjan’s analysis of path compression since path com-
 pression is essentially an application of memoization to the 
 nd
  function.
  
 Amortization and Persistence 
  
 Until this work, amortization and persistence were thought 
 to be incompatible. Several researchers [DST94, Ram92] had noted that amortized data struc-
 tures could not be made efficiently persistent using existing techniques for adding persistence 
 to ephemeral data structures, such as [DSST89, Die89], for reasons similar to those cited in 
 Sec-tion 3.2. Ironically, these techniques produce persistent data structures with amortized 
 bounds, but the underlying data structure must be worst-case. (These techniques have other 
 limitations as well. Most notably, they cannot be applied to data structures supporting 
 functions that com-bine two or more versions. Examples of offending functions include list 
 catenation and set union.)
  
 The idea that lazy evaluation could reconcile amortization and persistence first appeared, 
 in rudimentary form, in [Oka95c]. The theory and practice of this technique was further 
 devel-oped in [Oka95a, Oka96b].
  
 Amortization and Functional Data Structures 
  
 In his thesis, Schoenmakers [Sch93] 
 studies amortized data structures in a strict functional language, concentrating on formal 
 derivations of amortized bounds using the traditional physicist’s method. He avoids the 
 problems of per-sistence by insisting that data structures only be used in a single-threaded 
 fashion.
  
 Queues 
  
 Gries [Gri81, pages 250–251] and Hood and Melville [HM81] first proposed the 
 queues in Section 3.1.1. Burton [Bur82] proposed a similar implementation, but without the 
 restriction that the front list be non-empty whenever the queue is non-empty. (Burton 
 combines
  
 he ad
  and 
 tail
  into a single operation, and so does not require this restriction to support 
 he ad
  
 efficiently.) The queues in Section 3.4.2 first appeared in [Oka96b].
  
  
 2
 There is a clear analogy here to the spontaneous creation and mutual annihilation of particle-antiparticle 
 pairs in physics. In fact, a better name for these debits might be “anticredits”.",NA
Chapter 4,NA,NA
Eliminating Amortization,"Most of the time, we do not care whether a data structure has amortized bounds or worst-case 
 bounds; our primary criteria for choosing one data structure over another are overall 
 efficiency and simplicity of implementation (and perhaps availability of source code). 
 However, in some application areas, it is important to bound the running times of individual 
 operations, rather than sequences of operations. In these situations, a worst-case data structure 
 will often be preferable to an amortized data structure, even if the amortized data structure is 
 simpler and faster overall. Raman [Ram92] identifies several such application areas, 
 including
  
  Real-time systems:
  In real-time systems, predictability is more important than raw 
 speed [Sta88]. If an expensive operation causes the system to miss a hard deadline, it 
 does not matter how many cheap operations finished well ahead of schedule.
  
  Parallel systems:
  If one processor in a synchronous system executes an expensive 
 oper-ation while the other processors execute cheap operations, then the other 
 processors may sit idle until the slow processor finishes.
  
  Interactive systems:
  Interactive systems are similar to real-time systems — users often 
 value consistency more than raw speed [But83]. For instance, users might prefer 100 1-
 second response times to 99 0.25-second response times and 1 25-second response 
 time, even though the latter scenario is twice as fast.
  
 Remark:
  Raman also identified a fourth application area — persistent data structures. As 
 dis-cussed in the previous chapter, amortization was thought to be incompatible with 
 persistence. But, of course, we now know this to be untrue.
  
 Does this mean that amortized data structures are of no interest to programmers in these 
 areas? Not at all. Since amortized data structures are often simpler than worst-case data struc-
 tures, it is sometimes easier to design an amortized data structure, and then convert it to a 
 worst-case data structure, than to design a worst-case data structure from scratch.",NA
4.1 Scheduling,"Amortized and worst-case data structures differ mainly in when the computations charged to 
 a given operation occur. In a worst-case data structure, all computations charged to an 
 operation occur during the operation. In an amortized data structure, some computations 
 charged to an operation may actually occur during later operations. From this, we see that 
 virtually all nominally worst-case data structures become amortized when implemented in an 
 entirely lazy language because many computations are unnecessarily suspended. To describe 
 true worst-case data structures, we therefore need a strict language. If we want to describe 
 both amortized and worst-case data structures, we need a language that supports both lazy 
 and strict evaluation. Given such a language, we can also consider an intriguing hybrid 
 approach: worst-case data structures that use lazy evaluation internally. We will obtain such 
 data structures by beginning with lazy amortized data structures and modifying them in such 
 a way that every operation runs in the allotted time.
  
 In a lazy amortized data structure, any specific operation might take longer than the stated 
 bounds. However, this only occurs when the operation forces a suspension that has been paid 
 off, but that takes a long time to execute. To achieve worst-case bounds, we must guarantee 
 that every suspension executes in less than the allotted time.
  
 Define the
  intrinsic cost
  of a suspension to be the amount of time it takes to force the 
 suspension under the assumption that all other suspensions on which it depends have already 
 been forced and memoized, and therefore each take only 
 O ()
  time to execute. (This is similar to 
 the definition of the unshared cost of an operation.) The first step in converting an amortized 
 data structure to a worst-case data structure is to reduce the intrinsic cost of every suspension 
 to less than the desired bounds. Usually, this involves rewriting expensive monolithic 
 functions as incremental functions. However, just being incremental is not always good 
 enough — the granularity of each incremental function must be sufficiently fine. Typically, 
 each fragment of an incremental function will have an 
 O ()
  intrinsic cost.
  
 Even if every suspension has a small intrinsic cost, however, some suspensions might still 
 take longer than the allotted time to execute. This happens when one suspension depends on 
 another suspension, which in turn depends on a third, and so on. If none of the suspensions 
 have been previously executed, then forcing the first suspension will result in a cascade of",NA
4.2 ,NA,NA
Real-Time Queues,"As an example of this technique, we convert the amortized banker’s queues of Section 3.4.2 
 to worst-case queues. Queues such as these that support all operations in 
 O ()
  worst-case time are 
 called
  real-time queues
  [HM81].
  
  
 In the original data structure, queues are rotated using ++ and 
 r everse
 . Since 
 r everse
  is monolithic, our 
 first task is finding a way to perform rotations incrementally. This can be done by executing 
 one step of the reverse for every step of the ++. We define a function 
 r 
  
 otate
  such that 
  
  
  
 rotate (
 f
  , 
 r
 , 
 a
 ) = 
 f
  ++ reverse 
 r
  ++ 
 a
  
 Then 
  
  
 rotate (
 f
  , 
 r
 ,
  $
 Nil) = 
 f
  ++ reverse 
 r",NA
4.3 Bottom-Up Mergesort with Sharing,"As a second example, we modify the sortable collections from Section 3.5.2 to support 
 add
  in
  
 O (log n)
  worst-case time and 
 sort
  in 
 O (n)
  worst-case time.
  
 The only use of lazy evaluation in the amortized implementation is the suspended call
  
 to 
 addSe g
  in 
 add
 . This suspension is clearly monolithic, so the first task is to perform this",NA
4.4 Related Work,"Eliminating Amortization 
  
 Dietz and Raman [DR91, DR93, Ram92] have devised a frame-
  
 work for eliminating amortization based on
  pebble games
 , where the derived worst-case algo-
  
 rithms correspond to winning strategies in some game. Others have used ad hoc techniques
  
 similar to scheduling to eliminate amortization from specific data structures such as
  relaxed
  
 heaps
  [DGST88] and
  implicit binomial queues
  [CMP88]. The form of scheduling described
  
 here was first applied to queues in [Oka95c] and later generalized in [Oka96b].
  
 Queues 
  
 The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and
  
 Melville [HM81] presented the first purely functional implementation of real-time queues,
  
 based on a technique known as
  global rebuilding
  [Ove83], which will be discussed further in
  
 the next chapter. Their implementation does not use lazy evaluation and is more complicated
  
 than ours.",NA
Chapter 5,NA,NA
Lazy Rebuilding,"The next four chapters describe general techniques for designing functional data structures. 
 We begin in this chapter with
  lazy rebuilding
 , a variant of
  global rebuilding
  [Ove83].",NA
5.1 ,NA,NA
Batched Rebuilding,"Many data structures obey balance invariants that guarantee efficient access. The canonical 
 ex-ample is balanced binary search trees, which improve the worst-case running time of 
 many tree operations from the 
 O (n)
  required by unbalanced trees to 
 O (log n)
 . One approach to main-
 taining a balance invariant is to rebalance the structure after every update. For most balanced 
 structures, there is a notion of
  perfect balance
 , which is a configuration that minimizes the 
 cost of subsequent operations. However, since it is usually too expensive to restore perfect 
 balance after every update, most implementations settle for approximations of perfect balance 
 that are at most a constant factor slower. Examples of this approach include AVL trees 
 [AVL62] and red-black trees [GS78].
  
  
 However, provided no update disturbs the balance too drastically, an attractive alternative 
 is to postpone rebalancing until after a sequence of updates, and then to rebalance the entire 
 structure, restoring it to perfect balance. We call this approach
  batched rebuilding
 . Batched 
 rebuilding yields good amortized time bounds provided that (1) the data structure is not 
 rebuilt too often, and (2) individual updates do not excessively degrade the performance of 
 later op-erations. More precisely, condition (1) states that, if one hopes to achieve a bound of 
 O (f (n)) 
 amortized time per operation, and the global transformation requires 
 O (g (n))
  time, then the global 
 transformation cannot be executed any more frequently than every 
 c 
  
  
 g (n)=f (n)
  oper-ations, for 
 some constant 
 c
 . For example, consider binary search trees. Rebuilding a tree to perfect 
 balance takes 
 O (n)
  time, so if one wants each operation to take 
 O (log n)
  amortized",NA
5.2 ,NA,NA
Global Rebuilding,"Overmars [Ove83] developed a technique for eliminating the amortization from batched re-
 building. He called this technique
  global rebuilding
 . The basic idea is to execute the 
 rebuilding transformation incrementally, performing a few steps per normal operation. This 
 can be use-fully viewed as running the rebuilding transformation as a coroutine. The tricky 
 part of global rebuilding is that the coroutine must be started early enough that it can finish by 
 the time the rebuilt structure is needed.
  
 Concretely, global rebuilding is accomplished by maintaining two copies of each object. 
 The primary, or
  working
 , copy is the ordinary structure. The secondary copy is the one that is 
 gradually being rebuilt. All queries and updates operate on the working copy. When the 
 secondary copy is completed, it becomes the new working copy and the old working copy is 
 discarded. A new secondary copy might be started immediately, or the object may carry on 
 for a while without a secondary structure, before eventually starting the next rebuilding 
 phase.
  
 There is a further complication to handle updates that occur while the secondary copy is 
 being rebuilt. The working copy will be updated in the normal fashion, but the secondary 
 copy must be updated as well or the effect of the update will be lost when the secondary copy 
 takes over. However, the secondary copy will not in general be represented in a form that can 
 be efficiently updated. Thus, these updates to the secondary copy are buffered and executed, 
 a few at a time, after the secondary copy has been rebuilt, but before it takes over as the 
 working copy.
  
 Global rebuilding can be implemented purely functionally, and has been several times. 
 For example, the real-time queues of Hood and Melville [HM81] are based on this technique. 
 Unlike batched rebuilding, global rebuilding has no problems with persistence. Since no one 
 operation is particularly expensive, arbitrarily repeating operations has no effect on the time 
 bounds. Unfortunately, global rebuilding is often quite complicated. In particular, 
 representing the secondary copy, which amounts to capturing the intermediate state of a 
 coroutine, can be quite messy.",NA
5.3 ,NA,NA
Lazy Rebuilding,"The implementation of queues in Section 3.5.1, based on the physicist’s method, is closely 
 related to global rebuilding, but there is an important difference. As in global rebuilding, this 
 implementation keeps two copies of the front list, the working copy 
 W
  and the secondary copy 
 F
 , 
 with all queries being answered by the working copy. Updates to 
 F
  (i.e., 
 tail
  operations) are 
 buffered, to be executed during the next rotation, by writing
  
 ... F =
  $
 tl (force 
 f
  ) ...",NA
5.4 Double-Ended Queues,"As further examples of lazy rebuilding, we next present several implementations of double-
 ended queues, also known as
  deques
 . Deques differ from FIFO queues in that elements can be 
 both inserted and deleted from either end of the queue. A signature for deques appears in 
 Figure 5.1. This signature extends the signature for queues with three new functions: 
 c ons
  (in-sert 
 an element at the front), 
 last
  (return the rearmost element), and 
  
 init
  (remove the rearmost element).
  
 Remark:
  Note that the signature for queues is a strict subset of the signature for deques — 
 the same names have been chosen for the types, exceptions, and overlapping functions. 
 Because deques are thus a strict extension of queues, Standard ML will allow us to use a 
 deque module wherever a queue module is expected.",NA
5.4.1 ,NA,NA
Output-restricted Deques,"First, note that extending the queue implementations from Chapters 3 and 4 to support 
 c ons
 ,
  
 in addition to 
 sno c
 , is trivial. A queue that supports insertions at both ends, but deletions from
  
 only one end, is called an
  output-restricted deque
 .
  
 For example, we can implement 
 c ons
  for the banker’s queues of Section 3.4.2 as follows:
  
 fun
  cons (
 x
 , Queue 
 f
 F = 
 f
  , LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), LenF = 
 lenF
 +1, R = 
 r
 , LenR = 
 lenR g
  
 Note that we invoke the true constructor 
 Queue
  rather than the pseudo-constructor 
 queue
  be-
  
 cause adding an element to 
 F
  cannot possibly make 
 F
  shorter than 
 R
 .
  
 Similarly, we can easily extend the real-time queues of Section 4.2.
  
 fun
  cons (
 x
 , Queue 
 f
 F = 
 f
  , R = 
 r
 , S = 
 s g
 ) =
  
 Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), R = 
 r
 , S =
  $
 Cons (
 x
 ,
  
 s
 )
 g
 )
  
 jRj
 . Again, we invoke the true
  
 We add 
 x
  to 
 S
  only to maintain the invariant that 
 jS j
  
 constructor 
 Queue
  rather than the pseudo-constructor
  
 = jF j
  
 queue
 .",NA
5.4.2 Banker’s Deques,"Deques can be represented in essentially the same way as queues, as two streams (or lists) 
 F
  
 and 
 R
 , plus some associated information to help maintain balance. For queues, the notion of
  
 perfect balance is for all the elements to be in the front stream. For deques, the notion of perfect
  
 balance is for the elements to be evenly divided between the front and rear streams. Since we
  
 cannot afford to restore perfect balance after every operation, we will settle for guaranteeing
  
 that neither stream is more than about 
 c
  times longer than the other, for some constant 
 c > 
  
 .
  
 Specifically, we maintain the following balance invariant:
  
 jF j 
  
 cjRj + ^ 
  
 jRj cjF j +
  
 The “+1” in each term allows for the only element of a singleton queue to be stored in either
  
 stream. Note that both streams will be non-empty whenever the queue contains at least two
  
 elements. Whenever the invariant would otherwise be violated, we restore the queue to perfect
  
 balance by transferring elements from the longer stream to the shorter stream until both streams
  
 have the same length.
  
 Using these ideas, we can adapt either the banker’s queues of Section 3.4.2 or the 
 physicist’s
  
 queues of Section 3.5.1 to obtain deques that support every operation in 
 O ()
  amortized time.
  
 Because the banker’s queues are slightly simpler, we choose to begin with that implementation.
  
 The type of double-ended queues is precisely the same as for ordinary queues.
  
 datatype
  
  Queue = Queue
  
 f
 F :
  
  Stream, LenF : int, R :
  
  Stream, LenR : int
 g
  
 The functions on the front element are defined as follows:
  
 fun
  cons (Queue 
 f
 F = 
 f
  , LenF = 
 lenF
 , R = 
 r
 , LenR = 
 lenR g
 , 
 x
 ) =
  
 queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), LenF = 
 lenF
 +1, R = 
 r
 , LenR = 
 lenR g
  
 fun
  head (Queue 
 f
 F =
  $
 Nil, R =
  $
 Cons (
 x
 , ), ... 
 g
  = 
 x
  
 j
  head (Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), ... 
 g
 ) = 
 x
  
 fun
  tail (Queue 
 f
 F =
  $
 Nil, R =
  $
 Cons (
 x
 , ), ... 
 g
  = empty
  
 j
  tail (Queue 
 f
 F =
  $
 Cons (
 x
 , 
 f
  ), LenF = 
 lenF
  , R = 
 r
 , LenR = 
 lenR g
 ) =
  
 queue 
 f
 F = 
 f
  , LenF = 
 lenF 
  
 1, R = 
 r
 , LenR = 
 lenR g
  
 The first clauses of 
 he ad
  and 
 tail
  handle singleton queues where the single element is stored
  
 in the rear stream. The functions on the rear element —
 sno c
 , 
 last
 , and 
 init
  — are defined
  
 symmetrically on 
 R
  rather than 
 F
 .
  
 The interesting portion of this implementation is the pseudo-constructor 
 queue
 , which re-
  
 stores the queue to perfect balance when one stream becomes too long by first truncating
  
 the longer stream to half the combined length of both streams and then transferring the re-",NA
5.4.3 ,NA,NA
Real-Time Deques,"Real-time deques
  support every operation in 
 O ()
  worst-case time. We obtain real-time deques
  
 from the deques of the previous section by scheduling both the front and rear streams.
  
 As always, the first step in applying the scheduling technique is to convert all monolithic
  
 functions to incremental functions. In the previous implementation, the rebuilding transfor-
  
 mation rebuilt 
 F
  and 
 R
  as 
 take
  (
 i
 , 
 F
 ) and 
 R
  ++ 
 r everse
  (
 dr op
  (
 i
 , 
 F
 )) (or vice versa). 
  
 take
  
 and ++ are already incremental, but 
 r everse
  and 
 dr op
  are monolithic. We therefore rewrite
  
 R
  ++ 
 r everse
  (
 dr op
  (
 i
 , 
 F
 )) as 
 r otateDr op
  (
 R
 , 
 i
 , 
 F
 ) where 
 r otateDr op
  performs 
 c
  steps of the
  
 dr op
  for every step of the ++ and eventually calls 
 r otateR ev
 , which in turn performs 
 c
  steps of
  
 the 
 r everse
  for every remaining step of the ++. 
 r otateDr op
  can be implemented as
  
 fun
  rotateDrop (
 r
 , 
 i
 , 
 f
  ) =
  
 if 
 i < c
  then
  rotateRev (
 r
 , drop (
 i
 , 
 f
  ),
  $
 Nil)
  
 else let val
  (
 $
 Cons (
 x
 , 
 r 0
 )) = 
 r
  in $
 Cons (
 x
 , rotateDrop (
 r 0
 , 
 i 
  
 c
 , drop (
 c
 , 
 f
  )))
  end
  
 Initially, 
 jf j = cjr j + 
  
 + k
  where 
  
 k 
  
 c
 . Every call to 
 r otateDr op
  drops 
 c
  elements of 
 f
  
 and processes one element of 
 r
 , except the last, which drops 
 i mo d c
  elements of 
 f
  and leaves
  
 r
  unchanged. Therefore, at the time of the first call to 
 r otateR ev
 , 
 jf j = cjr j + 
  
 + k (i mo d c)
 .
  
 It will be convenient to insist that 
 jf j 
  
 cjr j
 , so we require that 
  
 + k 
  
 (i mo d c) 
  
 0
 . This
  
 is guaranteed only if 
 c
  is two or three, so these are the only values of 
 c
  that we allow. Then we
  
 can implement 
 r otateR ev
  as
  
 fun
  rotateRev (
 $
 Nil, 
 f
  , 
 a
 ) = reverse 
 f
  ++ 
 a
  
 j
  rotateRev (
 $
 Cons (
 x
 , 
 r
 ), 
 f
  , 
 a
 ) =
  
 $
 Cons (
 x
 , rotateRev (
 r
 , drop (
 c
 , 
 f
  ), reverse (take (
 c
 , 
 f
  )) ++ 
 a
 ))
  
 Note that 
 r otateDr op
  and 
 r otateR ev
  make frequent calls to 
 dr op
  and 
 r everse
 , which were
  
 exactly the functions we were trying to eliminate. However, now 
 dr op
  and 
 r everse
  are always
  
 called with arguments of bounded size, and therefore execute in 
 O ()
  steps.
  
 Once we have converted the monolithic functions to incremental functions, the next step is
  
 to schedule the execution of the suspensions in 
 F
  and 
 R
 . We maintain a separate schedule for
  
 each stream and execute a few suspensions per operation from each schedule. As with the real-
  
 time queues of Section 4.2, the goal is to ensure that both schedules are completely evaluated",NA
5.5 Related Work,"Global Rebuilding 
  
 Overmars introduced global rebuilding in [Ove83]. It has since been 
 used in many situations, including real-time queues [HM81], real-time deques [Hoo82, GT86, 
 Sar86, CG93], catenable deques [BT95], and the order maintenance problem [DS87].
  
 Deques 
 Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time 
 deques based on global rebuilding. Several other researchers later duplicated this work 
 [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate 
 multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord [Hoo92b] proposed 
 amortized deques based on batched rebuilding, but, as always with batched rebuilding, his 
 implementation is not efficient when used persistently. The real-time deques in Figure 5.3 
 first appeared in [Oka95c].
  
 Coroutines and Lazy Evaluation 
  
 Streams (and other lazy data structures) have frequently 
 been used to implement a form of coroutining between the producer of a stream and the con-
 sumer of a stream. Landin [Lan65] first pointed out this connection between streams and 
 coroutines. See [Hug89] for some compelling applications of this feature.",NA
Chapter 6,NA,NA
Numerical Representations,"Consider the usual representations of lists and natural numbers, along with several typical 
 functions on each data type.
  
 datatype
  List =
  
 Nil
  
 j
  Cons
  of
  
  List
  
 datatype
  Nat = 
  
  
  
 Zero 
  
 j
  Succ
  of
  Nat
  
 fun
  tail (Cons (
 x
 ,
  
 xs
 )) =
  
 xs
  
 fun
  pred (Succ
  
 n
 ) =
  
 n
  
 fun
  append (Nil, 
 ys
 ) = 
 ys
  
 j
  append (Cons (
 x
 , 
 xs
 ),
  
 Cons (
 x
 , append (
 xs
 ,
  
 ys
 ) = 
 ys
 ))
  
 fun
  plus (Zero, 
 n
 ) = 
 n
  
 j
  plus (Succ 
 m
 , 
 n
 ) =
  
 Succ (plus (
 m
 , 
 n
 ))
  
 Other than the fact that lists contain elements and natural numbers do not, these two imple-
 mentations are virtually identical. This suggests a strong analogy between representations of 
 the number 
 n
  and representations of container objects of size 
 n
 . Functions on the container 
 strongly resemble arithmetic functions on the number. For example, inserting an element re-
 sembles incrementing a number, deleting an element resembles decrementing a number, and 
 combining two containers resembles adding two numbers. This analogy can be exploited to 
 design new implementations of container abstractions — simply choose a representation of 
 nat-ural numbers with certain desired properties and define the functions on the container 
 objects accordingly. Call an implementation designed in this fashion a
  numerical 
 representation
 .
  
 The typical representation of lists can be viewed as a numerical representation based on 
 unary numbers. However, numerical representations based on binary numbers are also com-
 mon; the best known of these is the binomial queues of Vuillemin [Vui78]. Incrementing a 
 unary number takes 
 O ()
  time, so inserting an element into a unary representation also usu-ally 
 takes 
 O ()
  time. However, adding two unary numbers takes 
 O (n)
  time, so combining two containers in 
 a unary representation takes 
 O (n)
  time. Binary numbers improve the time",NA
6.1 Positional Number Systems,"A
  positional number system
  [Knu73] is a notation for writing a number as a sequence of digits
  
 b 0 : : : b m
 . The digit 
 b 0
 is called the
  least significant digit
  and the digit 
 b m
 is called the
  
 most significant digit
 . Except when writing ordinary, decimal numbers, we will always write
  
 sequences of digits from least significant to most significant.
  
 Each digit 
 b i
 has weight 
 w i
 , so the value of the sequence 
 b 0 : : : b m
 is 
 m
  
 i=0 
  
 b i w i
 . For any
  
 given positional number system, the sequence of weights is fixed, as is the set of digits 
 P 
  
 D i
 from
  
 which each 
 b i
 is chosen. For unary numbers, 
 w i =
  
  and 
 D i = f
 1
 g
  for all 
 i
 , and for binary
  
 numbers 
 w i 
  
 = 
  
 i
  and 
 D i = 
  
 f
 0
 ;
  1 
 g
 . (By convention, we write all digits in typewriter font
  
 except for ordinary, decimal digits.) A number is said to be written in base 
 B
  if 
 w i = B i
  and
  
 D i = f
 0
 ; : : : ; B 
  
 g
 . Usually, but not always, weights are increasing sequences of powers,
  
 and the set 
 D i
 is the same for every digit.
  
 A number system is said to be
  redundant
  if there is more than one way to represent some
  
 numbers. For example, we can obtain a redundant system of binary numbers by taking 
 w i = 
  
 i
  
 and 
 D i = f
 0 
 ;
  1
 ;
  2
 g
 . Then the decimal number 13 can be written
  1011
 , or
  1201
 , or
  122
 . If
  
 we allow trailing
  0
 s, then almost all positional number systems are redundant, since 
 b 0 : : : b m
  
 is always equivalent to 
 b 0 : : : b m
 0
 . Therefore, we disallow trailing
  0
 s.
  
 Computer representations of positional number systems can be
  dense
  or
  sparse
 . A dense
  
 representation is simply a list (or some other kind of sequence) of digits, including those digits
  
 that happen to be
  0
 . A sparse representation, on the other hand, includes only non-zero digits.
  
 It must then include information on either the rank (i.e., the index) or the weight of each digit.
  
 For example, Figure 6.1 shows two different representations of binary numbers in Standard
  
 ML— one dense and one sparse — along with several representative functions on each.",NA
6.2 Binary Representations,"Given a positional number system, we can implement a numerical representation based on 
 that number system as a sequence of trees. The number and sizes of the trees representing a 
 collection of size 
 n
  are governed by the representation of 
 n
  in the positional number system. For 
 each weight 
 w i
 , there are 
 b i
 trees of that size. For example, the binary representation of 73 is
  
 1001001
 , so a collection of size 73 in a binary numerical representation would comprise 
 three trees, of sizes 1, 8, and 64, respectively.
  
 Trees in numerical representations typically exhibit a very regular structure. For example, 
 in binary numerical representations, all trees have sizes that are powers of 2. Three com-mon 
 kinds of trees that exhibit this structure are
  complete binary leaf trees
  [KD96],
  binomial trees
  
 [Vui78], and
  pennants
  [SS90].
  
 Definition 6.1 (Complete binary leaf trees)
  A complete binary tree of rank 0 is a leaf and a 
 complete binary tree of rank 
 r > 0
  is a node with two children, each of which is a complete binary 
 tree of rank 
 r 
  
 . A leaf tree is a tree that contains elements only at the leaves, unlike 
 ordinary trees that contain elements at every node. A complete binary tree of rank 
 r
  has 
  
 r +
  
 nodes, but only 
  
 r
  leaves. Hence, a complete binary leaf tree of rank 
 r
  contains 
  
 r
  elements.
  
 Definition 6.2 (Binomial trees)
  A binomial tree of rank 
 r
  is a node with 
 r
  children 
 c 
  
 : : : c r
 , where 
 c i
 is 
 a binomial tree of rank 
 r i
 . Alternatively, a binomial tree of rank 
 r 
  
 > 
  
  
 0
  is a binomial tree 
 of rank 
 r
  
  to which another binomial tree of rank 
 r
  
  has been added as the leftmost 
 child. From the second definition, it is easy to see that a binomial tree of rank 
 r 
 contains 
 r
  
 nodes.
  
 Definition 6.3 (Pennants)
  A pennant of rank 0 is a single node and a pennant of rank 
 r > 0 
 is a 
 node with a single child that is a complete binary tree of rank 
 r 
  
 . The complete binary tree 
 contains 
  
 r
  
  elements, so the pennant contains 
  
 r
  elements.
  
 Figure 6.2 illustrates the three kinds of trees. Which kind of tree is superior for a given data 
 structure depends on the properties the data structure must maintain, such as the order in 
 which elements should be stored in the trees. A key factor in the suitability of a particular 
 kind of tree for a given data structure is how easily the tree supports functions analogous to 
 carries and borrows in binary arithmetic. When simulating a carry, we
  link
  two trees of rank 
 r
  
 to form a tree of rank 
 r + 
  
 . Symmetrically, when simulating a borrow, we
  unlink
  a tree of 
 rank 
 r > 0 
 to obtain two trees of rank 
 r 
  
 . Figure 6.3 illustrates the link operation (denoted 
  
  
 ) on each of the three kinds of trees. Assuming that elements are not rearranged, each of 
 the three kinds of trees can be linked or unlinked in 
 O ()
  time.
  
 We next describe two existing data structures in terms of this framework: the one-sided 
 flex-ible arrays of Kaldewaij and Dielissen [KD96], and the binomial queues of Vuillemin 
 [Vui78, Bro78].",NA
6.2.1 Binary Random-Access Lists,"A
  random-access list
 , also called a one-sided flexible array, is a data structure that supports 
 array-like 
 lo okup
  and 
 up date
  functions, as well as the usual 
 c ons
 , 
 he ad
 , and 
 tail
  functions on lists. A signature for 
 random-access lists is shown in Figure 6.4.
  
 Kaldewaij and Dielissen [KD96] describe an implementation of random-access lists in terms 
 of leftist left-perfect leaf trees. We can easily translate their implementation into the 
 framework of numerical representations as a binary representation using complete binary leaf 
 trees. A binary random-access list of size 
 n
  thus contains a complete binary leaf tree for each 
 1
  
 in the binary representation of 
 n
 . The rank of each tree corresponds to the rank of the corre-
 sponding digit; if the 
  
 i
 th bit of 
 n
  is
  1
 , then the random-access list contains a tree of size 
  
 i
 . For 
 this example, we choose a dense representation, so the type of binary random-access lists is
  
 datatype
  Tree = Leaf
  of 
  
 j
  Node
  of
  int
  
  Tree
  
  Tree 
 datatype
  Digit = Zero 
 j
  One
  of
  
  
  Tree 
 type
  RList =
  
  Digit list
  
 The integer in each node is the size of the tree. This number is redundant since the size of 
 every tree is completely determined by the size of its parent or by its position in the list of 
 digits, but we include it anyway for convenience. Trees are stored in increasing order of size, 
 and the order of elements (both within and between trees) is left-to-right. Thus, the head of 
 the random-access list is the leftmost leaf of the smallest tree. Figure 6.5 shows a binary 
 random-access list of size 7. Note that the maximum number of trees in a list of size 
 n
  is 
 blog (n + )c",NA
6.2.2 Binomial Heaps,"Binomial queues
  [Vui78, Bro78] are a classical implementation of mergeable priority queues.
  
 To avoid confusion with FIFO queues, we will henceforth refer to priority queues as
  heaps
  and
  
 binomial queues as
  binomial heaps
 . Heaps support four main functions: inserting an element",NA
6.3 Segmented Binary Numbers,"We next explore two variations of binary numbers that allow a number to be incremented or 
 decremented in 
 O ()
  worst-case time. Basing a numerical representation on these variations, 
 rather than ordinary binary numbers, reduces the running time of many insertion and deletion 
 functions from 
 O (log n)
  to 
 O ()
 . First, we present a somewhat complicated representation and sketch the 
 design of random-access lists and heaps based on this representation. In the next section, we 
 present a much simpler representation that is usually superior in practice.
  
 The problem with ordinary binary numbers is that carries and borrows can cascade. For 
 example, incrementing 
  
 k
  
  causes 
 k
  carries in binary arithmetic. Symmetrically, decrement-
 ing 
   
 k
  causes 
 k
  borrows.
  Segmented binary numbers
  solve this problem by allowing 
 multiple carries or borrows to be executed in a single step.",NA
6.3.1 ,NA,NA
Segmented Binomial Random-Access Lists and Heaps,"In both the binary random-access lists of Section 6.2.1 and the binomial heaps of Section 
 6.2.2, we linked two trees into a new, larger tree for every carry. In a cascade of 
 k
  carries, we 
 linked a new singleton tree with existing trees of sizes 
  
 0 
     
  
  
  
  
  
   
 ; 
   
 ; : : : ; 
              
  
  
  
  
  
 k
  
  to obtain a new tree of size 
  
 k
 . Similarly, in 
 binary random-access lists, a cascade of borrows decomposes a tree of size 
  
 k
  into 
 a singleton tree and 
 k
  trees of sizes 
  
 0 
   
  
 ; ; : : : ; 
          
 k 
   
 .
  
  
 Segmented binary numbers support fast carries and borrows, but to take advantage of this 
 in a numerical representation, we must choose a tree representation that will allow us to link 
 and unlink many trees in a single step. Of the three kinds of trees described earlier, only 
 binomial trees support this behavior. A node of rank 
 r
  consists of an element and a sequence 
 of trees of ranks 
 0; : : : ; r 
  
 . Therefore, we can combine an element and a sequence of trees into a new 
 tree — or decompose a tree into an element and a sequence of trees — in 
 O ()
  time.
  
 Adapting the earlier implementations of binary random-access lists and binomial heaps to 
 use segmented binary arithmetic rather than ordinary binary arithmetic, and in the case of 
 binary random-access lists, to use binomial trees rather than complete binary leaf trees, is 
 tedious, but mostly straightforward, except for the following issues:
  
  To link and unlink multiple trees in a single step, we must use the same representation for 
 the sequence of trees corresponding to a block of
  1
 s (called a
  segment
 ) and for the 
 children of a node. So, for example, we cannot maintain one in increasing order of rank 
 and the other in decreasing order of rank as we did for binomial heaps. For both 
 segmented binomial heaps and segmented binomial random-access lists, we need easy 
 access to the smallest tree in a segment, but we also need easy access to the largest child 
 of a node. Therefore, we represent both kinds of sequences as real-time deques.
  
  For binomial heaps, the cascade of links that produces a new tree also compares the 
 roots of trees as it goes to find the minimum element in the tree. For segmented 
 binomial heaps, we do not have time to search a segment for the root with the 
 minimum element, so we insist that the smallest tree in any segment always have the 
 minimum root. Then, whenever we create a new tree from a new element and a 
 segment of trees of ranks
  
 0; : : : ; r 
  
 , we simply compare the new element with the first root in the segment (i.e., the root 
 of the rank 0 tree). The smaller element becomes the new root and the larger element 
 becomes the rank 0 child of the root. Whenever we add a new tree of rank 
 r
  to a segment 
 whose smallest tree has rank 
 r + 
  
 , we decompose the tree of rank 
 r +
  
  into two 
 trees of rank 
 r
 . We then keep the tree with the smallest root, and link the remaining two 
 trees into a new tree of rank 
 r + 
  
 .
  
 With these changes segmented binomial random-access lists support 
 c ons
 , 
 he ad
 , and 
 tail
  in",NA
6.4,NA,NA
Skew Binary Numbers,"Numerical representations based on segmented binary numbers rather than ordinary binary 
 numbers improve the asymptotic behavior of certain operations from 
 O (log n)
  to 
 O ()
 , while retaining the 
 same asymptotic behavior for all other operations. Unfortunately, such data struc-tures are too 
 complicated to be useful in practice. We next consider another number system, 
 skew binary 
 numbers
 , that usually achieves similar asymptotic benefits, but that is simpler and faster in 
 practice.
  
 In skew binary numbers [Mye83, Oka95b], the weight 
 w i
 of the 
 i
 th digit is 
  
 i+ 
   
 , rather than 
 i
  as in 
 ordinary binary numbers. Digits may be
  0
 ,
  1
 , or
  2
  (i.e., 
 D i 
  
 = f
 0
 ;
  1
 ;
  2 
 g
 ). For example, the 
 decimal number 92 could be written
  002101
  (least-significant digit first).
  
 This number system is redundant, but, if we add the further constraint that only the lowest 
 non-
 0
  digit may be
  2
 , then we regain unique representations. Such a number is said to be in 
 canonical form
 . Henceforth, we will assume that all skew binary numbers are in canonical 
 form.
  
 Theorem 6.1 (Myers [Mye83])
  Every natural number has a unique skew binary canonical 
 form.
  
 Recall that the weight of digit 
 i
  is 
  
 i+
  
  and note that 
  
 + ( i+ 
   
 ) = i+ 
  
 . This implies that we 
 can increment a skew binary number whose lowest non-
 0
  digit is
  2
  by resetting the
  2
  to
  0
  
 and incrementing the next digit from
  0
  to
  1
  or from
  1
  to
  2
 . (The next digit cannot already 
 be
  2
 .) Incrementing a skew binary number that does not contain a
  2
  is even easier —simply 
 increment the lowest digit from
  0
  to
  1
  or from
  1
  to
  2
 . In both cases, the result is still in 
 canonical form. And, assuming we can find the lowest non-
 0
  digit in 
  
 O ()
  time, both cases take only 
 O ()
  time!
  
 We cannot use a dense representation for skew binary numbers since scanning for the 
 lowest non-
 0
  digit would take more than 
 O ()
  time. Instead, we choose a sparse representation, so 
 that we always have immediate access to the lowest non-
 0
  digit.
  
 type
  Nat = int list
  
 The integers represent either the rank or weight of each non-
 0
  digit. For now, we use 
 weights. The weights are stored in increasing order, except that the smallest two weights may 
 be identi-cal, indicating that the lowest non-
 0
  digit is
  2
 . Given this representation, we 
 implement 
 inc
  as follows:",NA
6.4.1,NA,NA
Skew Binary Random-Access Lists,"We next design a numerical representation for random-access lists, based on skew binary 
 num-bers. The basic representation is a list of trees, with one tree for each
  1
  digit and two 
 trees for each
  2
  digit. The trees are maintained in increasing order of size, except that the 
 smallest two trees are the same size when the lowest non-
 0
  digit is
  2
 .
  
  
 The sizes of the trees should correspond to the weights in skew binary numbers, so a tree 
 representing the 
 i
 th digit should have size 
  
 i+ 
  
 . Up until now, we have mainly considered 
 trees whose sizes are powers of two, but we have also encountered a kind of tree whose sizes 
 have the desired form: complete binary trees. Therefore, we represent skew binary random-
 access lists as lists of complete binary trees.
  
 To support 
 he ad
  efficiently, the first element in the random-access list should be the root of 
 the first tree, so we store the elements within each tree in left-to-right preorder and with the 
 elements in each tree preceding the elements in the next tree.
  
 In previous examples, we have stored a size or rank in every node, even when that infor-
 mation was redundant. For this example, we adopt the more realistic approach of maintaining 
 size information only for the root of each tree in the list, and not for every subtree as well. 
 The type of skew binary random-access lists is therefore
  
 datatype
  Tree = Leaf
  of 
  
 j
  Node
  of
  
 type
  RList = (int
  
  Tree) list
  
  Tree
  
  Tree",NA
6.4.2 Skew Binomial Heaps,"Finally, we consider a hybrid numerical representation for heaps based on both skew binary
  
 numbers and ordinary binary numbers. Incrementing a skew binary number is both quick and
  
 simple, and serves admirably as a template for the 
 insert
  function. Unfortunately, addition
  
 of two arbitrary skew binary numbers is awkward. We therefore base the 
 mer ge
  function on
  
 ordinary binary addition, rather than skew binary addition.
  
 to
  
 A
  skew binomial tree
  is a binomial tree in which every node is augmented with a list of up 
 r
  
 elements, where 
 r
  is the rank of the node in question.
  
 datatype
  Tree = Node
  of
  int
  
  Elem.T
  
  Elem.T list
  
  Tree list
  
 Unlike ordinary binomial trees, the size of a skew binomial tree is not completely determined
  
 by its rank; rather the rank of a skew binomial tree determines a range of possible sizes.
  
 Lemma 6.2
  If 
 t
  is a skew binomial tree of rank 
 r
 , then 
  
 r
  
 jtj
  
 r + 
 .
  
 Proof: 
 By induction. 
  
 t
  has 
 r
  children 
 t : : : t r
 , where 
 t i
 is a skew binomial tree of rank
  
 r i
 , and 
 r i
  
 jt i j
  
 r i+ 
 . In addition, the root of 
 t
  is augmented with a list of 
 k
  
 elements, where 
 0 
  
 k 
  
 r
 . Therefore, 
 jtj 
  
 + 0 +
  
 r
  
 i=0
  
 i 
  
 = 
  
 + ( r 
  
 ) 
  
 = 
  
 r
  and
  
 jtj
  
 Note that a tree of rank 
  
  
 + r +
  
 P 
   
 r
  
 i=0 ( i+
  
 r
  is always larger than a tree of rank 
  
 ) = 
  
 + r + ( r +
  
 r 
  
 ) = P r +
  
 r 
  
  
 .
  
 .
  
 Skew binomial trees may be
  linked
  or
  skew linked
 . The 
 link
  function combines two trees
  
 of rank 
 r
  to form a tree of rank 
 r +
  by making the tree with the larger root a child of the tree
  
 with the smaller root.
  
 fun
  link (
 t 
 as
  Node (
 r
 , 
 x 
 , 
 xs 
  
 , 
 c 
 ), 
 t 
  
 as
  Node ( , 
 x 
 , 
 xs 
  
 , 
 c 
 )) =
  
 if
  Elem.leq (
 x 
 , 
 x 
 )
  then
  Node (
 r
 +1, 
 x 
 , 
 xs 
 , 
 t 
 :: 
 c 
 )
  else
  Node (
 r
 +1, 
 x 
 , 
 xs 
  
 , 
 t 
  
 :: 
 c 
 )",NA
6.5 Discussion,"In designing numerical representations, we draw analogies between container data structures 
 and representations of natural numbers. However, this analogy can also be extended to other 
 kinds of numbers. For example,
  difference lists
  [SS86] in Prolog support a notion of lists with 
 negative length; appending a list of length 15 and a list of length 
  
 10 results in a list of length 
 5. This behavior is also possible using the catenable lists of Hughes [Hug86], which are the 
 functional counterpart of difference lists.
 1
  
 As another example, Brodal and Okasaki [BO96] support a 
 delete
  function on heaps using two 
 primitive heaps, one containing positive elements and one containing negative elements. The 
 negative elements are ones that have been deleted, but that have not yet been physically 
 removed from the positive heap. In this representation, it is possible to delete elements that 
 have not yet been inserted. If the negative heap is larger than the positive heap, then the 
 overall“size” of the heap is negative.
  
 Can this analogy between data structures and representations of numbers be extended 
 even further, to non-integral numbers? We know of no such examples, but it is intriguing to 
 speculate on possible uses for such data structures. For instance, might a numerical 
 representation based on floating point numbers be useful in approximation algorithms?",NA
6.6 Related Work,"Numerical Representations 
  
 Data structures that can be cast as numerical representations 
 are surprisingly common, but only rarely is the connection to a variant number system noted 
 explicitly [GMPR77, Mye83, CMP88, KT96b].
  
 1
 Thanks to Phil Wadler for this observation.",NA
Chapter 7,NA,NA
Data-Structural Bootstrapping,"The term
  bootstrapping
  refers to “pulling yourself up by your bootstraps”. This seemingly 
 nonsensical image is representative of a common situation in computer science: problems 
 whose solutions require solutions to (simpler) instances of the same problem.
  
 For example, consider loading an operating system from disk or tape onto a bare 
 computer. Without an operating system, the computer cannot even read from the disk or tape! 
 One solu-tion is a
  bootstrap loader
 , a very tiny, incomplete operating system whose only 
 purpose is to read in and pass control to a somewhat larger, more capable operating system 
 that in turn reads in and passes control to the actual, desired operating system. This can be 
 viewed as a instance of bootstrapping a complete solution from an incomplete solution.
  
 Another example is bootstrapping a compiler. A common activity is to write the compiler 
 for a new language in the language itself. But then how do you compile that compiler? One 
 solution is to write a very simple, inefficient interpreter for the language in some other, 
 existing language. Then, using the interpreter, you can execute the compiler on itself, thereby 
 obtain-ing an efficient, compiled executable for the compiler. This can be viewed as an 
 instance of bootstrapping an efficient solution from an inefficient solution.
  
 In his thesis [Buc93], Adam Buchsbaum describes two algorithmic design techniques he 
 collectively 
 calls
  
 data-structural 
 bootstrapping
 . 
 The 
 first 
 technique,
  
 structural 
 decomposition
 , involves bootstrapping complete data structures from incomplete data 
 structures. The second technique,
  structural abstraction
 , involves bootstrapping efficient data 
 structures from ineffi-cient data structures. In this chapter, we reexamine data-structural 
 bootstrapping, and describe several functional data structures based on these techniques.",NA
7.1 Structural Decomposition,"Structural decomposition
  is a technique for bootstrapping complete data structures from in-
 complete data structures. Typically, this involves taking an implementation that can handle 
 objects only up to some bounded size (perhaps even zero), and extending it to handle objects 
 of unbounded size.
  
 Consider typical recursive datatypes such as lists and binary leaf trees:
  
 datatype 
 datatype
  
  List = Nil 
 j
  Cons
  of
  
  List
  
  Tree = Leaf
  of 
  
 j
  Node
  of
  
  Tree
  
  Tree
  
 In some ways, these can be regarded as instances of structural decomposition. Both consist of 
 a simple implementation of objects of some bounded size (zero for lists and one for trees) and 
 a rule for recursively decomposing larger objects into smaller objects until eventually each 
 object is small enough to be handled by the bounded case.
  
 However, both of these definitions are particularly simple in that the recursive component in 
 each definition is identical to the type being defined. For instance, the recursive component in 
 the definition of 
  
 List
  is also 
  
 List
 . Such a datatype is called
  uniformly recursive
 .
  
 In general, we reserve the term
  structural decomposition
  to describe recursive data struc-tures 
 that are
  non-uniform
 . For example, consider the following definition of sequences:
  
 datatype
  Seq = Empty 
 j
  Seq
  of
  
  ( ) Seq
  
 Here, a sequence is either empty or a single element together with a sequence of pairs of 
 elements. The recursive component ( ) 
 Se q
  is different from 
  
 Se q
  so this datatype is 
 non-uniform. (In Chapter 8, we will consider an implementation of queues that is similar to 
 this definition of sequences.)
  
 Why might such a non-uniform definition be preferable to a uniform definition? The 
 more sophisticated structure of non-uniform types often supports more efficient algorithms 
 than their uniform cousins. For example, compare the following 
 size
  functions on lists and 
 sequences.
  
 fun
  sizeL Nil = 0 
  
 j
  sizeL (Cons (
 x
 ,
  
 xs
 )) = 1 + sizeL
  
 xs
  
 fun
  sizeS Empty = 0 
  
  
 j
  sizeS (Seq (
 x
 , 
 ps
 )) = 1 + 2
  
  sizeS
  
 ps
  
 n)
  
 The function on lists runs in 
 time.
  
 O
  
 (n)
  time whereas the function on sequences runs in
  
 O
  
 (log",NA
7.1.1,NA,NA
Non-Uniform Recursion and Standard ML,"Although Standard ML allows the definition of non-uniform recursive datatypes, the type sys-
 tem disallows the definition of most useful functions on such datatypes. For instance, consider",NA
7.1.2 ,NA,NA
Queues Revisited,"Consider the use of ++ in the banker’s queues of Section 3.4.2. During a rotation, the front
  
 stream 
 F
  is replaced by 
 F
  ++ 
 r everse R
 . After a series of rotations, 
 F
  will have the form
  
 ( 
  
 ((f
  ++ 
 r everse r )
  ++ 
 r everse r )
  
  ++ 
 r everse r k )
  
 Append is well-known to be inefficient in left-associative contexts like this because it repeat-
  
 edly processes the elements of the leftmost streams. For example, in this case, the elements of
  
 f
  will be processed 
 k
  times (once by each ++), and the elements of 
 r i
 will be processed 
 k 
  
 i +
  
 times (once by 
 r everse
  and once for each following ++). In general, left-associative appends",NA
7.2 ,NA,NA
Structural Abstraction,"The second kind of data-structural bootstrapping is
  structural abstraction
 , which is typically
  
 used to extend an implementation of collections, such as lists or heaps, with an efficient 
 join
  
 function for combining two collections. For many implementations, designing an efficient
  
 insert
  function, which adds a single element to a collection, is easy, but designing an efficient
  
 join
  function is difficult. Structural abstraction creates collections that contain other collections
  
 as elements. Then two collections can be joined by simply inserting one collection into the
  
 other.
  
 The ideas of structural abstraction can largely be described at the level of types. Suppose
  
 C
  is a collection datatype with elements of type 
  
 , and that this datatype supports an efficient
  
 insert
  function, with signature
  
 val
  insert :
  
  C 
 !
  
  C
  
 Call 
  
 C
  the
  primitive
  type. From this type, we wish to derive a new datatype 
  
 B
 , called the
  
 bootstrapped
  type, such that 
  
 B
  supports both 
 insert
  and 
 join
  efficiently, with signatures
  
 val
  insert 
 B
 :
  
  B 
 !
  
  B
  
 val
  join 
 B
 :
  
  B B 
 !
  
  B
  
 (We use the 
 B
  subscript to distinguish functions on the bootstrapped type from functions on
  
 the primitive type.) In addition, 
  
 B
  should support an efficient 
 unit
  function for creating a
  
 new singleton collection.
  
 val
  unit 
 B
 : 
  
 !
  B
  
 Then, 
 insert B
 can be implemented simply as
  
 fun
  insert 
 B
 (
 x
 , 
 b
 ) = join 
 B
 (unit 
 B x
 , 
 b
 )
  
 The basic idea of structural abstraction is to somehow represent bootstrapped collections as
  
 primitive collections of other bootstrapped collections. Then 
 join B
 can be implemented in
  
 terms of 
 insert
  (not 
 insert B
 !) roughly as
  
 fun
  join 
 B
 (
 b 
 , 
 b 
 ) = insert (
 b 
 , 
 b 
 )",NA
7.2.1 ,NA,NA
Lists With Efficient Catenation,"The first data structure we will implement using structural abstraction is catenable lists, as
  
 specified by the signature in Figure 7.2. Catenable lists extend the usual list signature with an
  
 efficient append function (++). As a convenience, catenable lists also support 
 sno c
 , even though
  
 we could easily simulate 
 sno c
  (
 xs
 , 
 x
 ) by 
 xs
  ++ 
 c ons
  (
 x
 , 
 empty
 ). Because of this ability to add
  
 elements to the rear of a list, a more accurate name for this data structure would be catenable
  
 output-restricted deques.
  
 We obtain an efficient implementation of catenable lists that supports all operations in 
 O ()
  
 amortized time by bootstrapping an efficient implementation of FIFO queues. The exact choice
  
 of implementation for the primitive queues is largely irrelevant; any of the persistent, constant-
  
 time queue implementations will do, whether amortized or worst-case.
  
 Given an implementation 
 Q
  of primitive queues matching the Q
 UEUE
  signature, structural
  
 abstraction suggests that we can represent catenable lists as
  
 datatype
  
  Cat = Empty
  
 j
  Cat
  of
  
  Cat Q.Queue
  
 One way to interpret this type is as a tree where each node contains an element, and the children",NA
7.2.2 ,NA,NA
Heaps With Efficient Merging,"Next, we apply structural abstraction to heaps to obtain an efficient merge operation. This 
 section reflects joint work with Gerth Brodal.
  
 Assume that we have an implementation of heaps that supports 
 insert
  in 
 O ()
  worst-case time and 
 mer ge
 , 
 ndMin
 , and 
 deleteMin
  in 
 O 
 (log 
 n)
  worst-case time. The skew binomial heaps of Section 6.4.2 are one such 
 implementation. Using structural abstraction, we improve the running time of both 
 ndMin
  and 
 mer ge
  to 
 O ()
  
 worst-case time.
  
 For now, assume that the type of heaps is polymorphic in the type of elements, and that, 
 for any type of elements, we magically know the right comparison function to use. Later we 
 will account for the fact that both the type of elements and the comparison function on those 
 elements are fixed at functor-application time.
  
 Under the above assumption, the type of bootstrapped heaps can be given as
  
 datatype
  
  Heap = Empty
  
 j
  Heap
  of
  
  ( Heap) H.Heap
  
 where 
 H
  is the implementation of primitive heaps. The element stored at any given 
 He ap
  node will 
 be the minimum element in the subtree rooted at that node. The elements of the primitive 
 heaps are themselves bootstrapped heaps. Within the primitive heaps, bootstrapped heaps are 
 ordered with respect to their minimum elements (i.e., their roots).
  
 Since the minimum element is stored at the root, 
 fun
  findMin (Heap (
 x
 , )) = 
 x
  
 ndMin
  is simply
  
 To 
 mer ge
  two bootstrapped heaps, we insert the heap with the larger root into the heap with
  
 the smaller root.
  
 fun
  merge (Empty, 
 h
 ) = 
 h
  
 j
  merge (
 h
 , Empty) = 
 h
  
 j
  merge (
 h 
  
 as
  Heap (
 x
 , 
 p 
 ), 
 h 
  
 as
  Heap (
 y
 , 
 p 
 )) =
  
 if 
 x < y
  then
  Heap (
 x
 , H.insert (
 h 
 , 
 p 
 ))
  else
  Heap (
 y
 , H.insert (
 h 
 , 
 p 
 ))
  
 (In the comparison 
 x 
  
 < y
 , we assume that 
 <
  is the right comparison function for these ele-
  
 ments.) Now, 
 insert
  is defined in terms of 
 mer ge
 .
  
 fun
  insert (
 x
 , 
 h
 ) = merge (Heap (
 x
 , H.empty), 
 h
 )
  
 Finally, we consider 
 deleteMin
 , defined as
  
 fun
  deleteMin (Heap (
 x
 , 
 p
 )) =
  
 if
  H.isEmpty 
 p
  then
  Empty
  
 else let val
  (Heap (
 y
 , 
 p 
 )) = H.findMin 
 p
  
 val 
 p 
 = H.deleteMin 
 p
  
 in
  Heap (
 y
 , H.merge (
 p 
 , 
 p 
 ))
  end",NA
7.3 ,NA,NA
Related Work,"Data-Structural Bootstrapping 
  
 Buchsbaum
  et al.
  identified data-structural bootstrapping 
 as a general data structure design technique in [Buc93, BT95, BST95]. Structural decomposi-
 tion and structural abstraction had previously been used in [Die82] and [DST94], 
 respectively.
  
 Catenable Lists 
  
 Although it is relatively easy to design alternative representations of 
 persis-tent lists that support efficient catenation (see, for example, [Hug86]), such alternative 
 repre-sentations seem almost inevitably to sacrifice efficiency on the 
 he ad
  and/or 
 tail
  functions.
  
  
 Myers [Mye84] described a representation based on AVL trees that supports all relevant 
 list functions in 
 O (log n)
  time.
  
 Driscoll, Sleator, and Tarjan achieved the first sub-logarithmic implementation in 
 [DST94]. They represent catenable lists as 
 n
 -ary trees with the elements at the leaves. To keep 
 the left-most leaves near the root, they use a restructuring operation known as 
 pul l
  that removes 
 the first grandchild of the root and reattaches it directly to the root. Unfortunately, catenation 
 breaks all useful invariants based on this restructuring heuristic, so they are forced to develop 
 quite a bit of machinery to support catenation. The resulting implementation supports 
 catenation in 
 O (log log k )
  worst-case time, where 
 k
  is the number of list operations (note that 
 k
  may be 
 much smaller than 
 n
 ), and all other functions in 
 O ()
  worst-case time.
  
 Buchsbaum and Tarjan [BT95] use structural decomposition to recursively decompose 
 catenable deques of size 
 n
  into catenable deques of size 
 O (log n)
 . They use the 
 pul l
  oper-ation of Driscoll, 
 Sleator, and Tarjan to keep their tree balanced (i.e., of depth 
 O (log n)
 ), and then use the smaller 
 deques to represent the left and right spines of each subtree. This yields an",NA
Chapter 8,NA,NA
Implicit Recursive Slowdown,"Implicit recursive slowdown is a lazy variant of the recursive-slowdown technique of Kaplan 
 and Tarjan [KT95]. We first review recursive slowdown, and then show how lazy evaluation 
 can significantly simplify this technique. Finally, we illustrate implicit recursive slowdown 
 with implementations of queues and catenable deques.",NA
8.1 ,NA,NA
Recursive Slowdown,"The simplest illustration of recursive slowdown is a variant of binary numbers that can be 
 incremented in 
 O ()
  worst-case time. (We have already seen several such variants, including skew 
 binary numbers and segmented binary numbers.) As always, the trick is to avoid cascades of 
 carries. In recursive slowdown, we allow digits to be
  0
 ,
  1
 , or
  2
 .
  2
 s exist only temporarily 
 and represent a carry in progress. To increment a number, we first increment the first digit, 
 which is guaranteed not to be
  2
 . We then find the first non-
 1
  digit. If it is
  0
 , we do nothing, 
 but if it is
  2
 , we convert it to
  0
  and increment the following digit, which is also guaranteed 
 not to be
  2
 . Changing a
  2
  to a
  0
  and incrementing the following digit corresponds to 
 executing a single carry step.
  
 It is easy to show that following the above rules maintains the invariant that the first
  2
  is 
 preceded by at least one
  0
  (and any number of
  1
 s) and that any pair of
  2
 s is separated by 
 at least one
  0
  (and any number of
  1
 s). This invariant guarantees that we never attempt to 
 increment a digit that is already
  2
 .
  
 Since we want the increment function to run in 
 O ()
  worst-case time, we cannot afford to scan 
 the digits to find the first non-
 1
  digit. Instead, we choose a representation that groups 
 contiguous blocks of
  1
 s together.
  
 datatype
  Digit = Zero 
 type
  Nat = Digit list
  
 j
  Ones
  of
  int
  
 j
  Two",NA
8.2 ,NA,NA
Implicit Recursive Slowdown,"The essence of the recursive-slowdown implementation of binary numbers is a method for 
 executing carries incrementally. By now we have seen many examples of incremental 
 functions implemented with lazy evaluation. By combining the ideas of recursive slowdown 
 with lazy evaluation, we obtain a new technique, called
  implicit recursive slowdown
 , that is 
 significantly simpler than the original.
  
  
 Consider the following, straightforward implementation of binary numbers as streams of 
 0
 s and
  1
 s:
  
 datatype
  Digit = Zero 
 j
  One 
  
 type
  Nat = Digit Stream
  
 fun
  inc (
 $
 Nil) =
  $
 Cons (One,
  $
 Nil)
  
 j
  inc (
 $
 Cons (Zero, 
 ds
 )) =
  $
 Cons (One, 
 ds
 )
  
 j
  inc (
 $
 Cons (One, 
 ds
 )) =
  $
 Cons (Zero, inc
  
 ds
 )
  
 This is exactly the same as the original presentation of binary numbers in Chapter 6, except 
 with streams instead of lists.
  
 Remark:
  This implementation is less lazy than it could be. It forces its argument 
 immediately, and then creates a suspension of the result. A reasonable alternative would be to 
 also suspend forcing the argument, as in
  
 fun
  inc
  
 0
  
 ds
  =
  $case
  force 
 ds
  of 
  
  
  
 Nil 
 )
  Cons (One,
  $
 Nil) 
  
  
 j
  Cons (Zero, 
 ds 0
 ) 
 )
  Cons (One,
  
 ds
  
 0
 )
  
 ds
  
 0
 )
  
 j
  Cons (One,
  
 ds
  
 0
 )
  
 )
  Cons (Zero, inc 
 0
  
 However, in this chapter, we will often need to force one level ahead of the current level, so 
 we stick with the first implementation.
  
 Theorem 8.1 
  
 inc
  runs in 
 O ()
  amortized time.
  
 Proof:
  We use the banker’s method. By inspection, the unshared cost of 
 inc
  is 
 O ()
 . There-fore, to 
 show that 
 inc
  runs in 
 O ()
  amortized time, we must merely show that 
 inc
  discharges only 
 O ()
  debits per 
 operation. In fact, we show that 
 inc
  discharges only two debits.",NA
8.3 Supporting a Decrement Function,"We have now presented several implementations of an increment function, but of course such
  
 a function is useless without some other operations on binary numbers, such as addition and
  
 comparisons. These operations typically have an 
 O (log n)
  cost, since they must inspect ev-
  
 ery digit. In the lazy implementation (without scheduling), a digit stream contains at most
  
 O (log n)
  debits, so discharging those debits does not increase the asymptotic complexity of
  
 these operations.
  
 But something interesting happens when we consider the decrement function.",NA
8.4 Queues and Deques,"As our first substantial example of implicit recursive slowdown, we present an implementation
  
 of queues that also integrates aspects of numerical representations and structural decomposi-
  
 tion.
  
 A queue is either
  shallow
  or
  deep
 . A shallow queue contains either zero or one elements. A
  
 deep queue is decomposed into three segments: a
  front
 , containing either one or two elements;
  
 a
  rear
 , containing either zero or one elements; and a
  middle
 , which is a suspended queue of
  
 pairs.
  
 datatype
  ZeroOne = Zero 
 j
  One
  of
  
 datatype
  OneTwo = One 
 0
  of 
  
 j
  Two 
 0
  of
  
 datatype
  Queue = Shallow
  of
  ZeroOne
  
 j
  Deep
  of 
 f
 F : OneTwo, M : ( 
  
 ) Queue susp, R :
  
  ZeroOne
 g
  
 To add an element to a deep queue using 
 sno c
 , we look at 
 R
 . If it is
  0
 , then we add the element
  
 to 
 R
 . If it is
  1
 , then we pair the new element with the existing element, and add the pair to 
 M
  ,
  
 resetting 
 R
  to
  0
 . We also need a few special cases for adding an element to a shallow queue.
  
 fun
  snoc (Shallow Zero, 
 y
 ) = Shallow (One 
 y
 )
  
 j
  snoc (Shallow (One 
 x
 ), 
 y
 ) = Deep 
 f
 F = Two 
 0
  (
 x
 , 
 y
 ), M =
  $
 empty, R = Zero
 g
  
 j
  snoc (Deep 
 f
 F = 
 f
  , M = 
 m
 , R = Zero
 g
 , 
 y
 ) = Deep 
 f
 F = 
 f
  , M = 
 m
 , R = One 
 y g
  
 j
  snoc (Deep 
 f
 F = 
 f
  , M =
  $
 q
 , R = One 
 x g
 , 
 y
 ) =
  
 Deep 
 f
 F = 
 f
  , M =
  $
 snoc (
 q
 , (
 x
 , 
 y
 )), R = Zero
 g
  
 Note that in the final clause of 
 sno c
 , we force 
 M
  earlier than we need to. Instead, we could
  
 write this clause as
  
 j
  snoc (Deep 
 f
 F = 
 f
  , M = 
 m
 , R = One 
 x g
 , 
 y
 ) =
  
 Deep 
 f
 F = 
 f
  , M =
  $
 snoc (force 
 m
 , (
 x
 , 
 y
 )), R = Zero
 g
  
 However, this change has no effect on the running time.
  
 To remove an element from a deep queue using 
 tail
 , we look at 
 F
 . If it is
  2
 , then we simply
  
 remove the element, setting 
 F
  to
  1
 . If it is
  1
 , then we “borrow” a pair from 
 M
  , and set 
 F
  to
  2
 .
  
 Again, there are several special cases dealing with shallow queues.
  
 fun
  tail (Shallow (One 
 x
 )) = Shallow Zero
  
 j
  tail (Deep 
 f
 F = Two 
 0
  (
 x
 , 
 y
 ), M = 
 m
 , R = 
 r g
 ) = Deep 
 f
 F = One 
 0
  
 j
  tail (Deep 
 f
 F = One 
 0 x
 , M =
  $
 q
 , R = 
 r g
 ) =
  
 if
  isEmpty 
 q
  then
  Shallow 
 r
  
 else let val
  (
 y
 , 
 z
 ) = head 
 q
  
 in
  Deep 
 f
 F = Two (
 y
 , 
 z
 ), M =
  $
 tail 
 q
 , R = 
 r g
  end
  
 y
 , M =
  
 m
 , R =
  
 r
  
 g",NA
8.5 ,NA,NA
Catenable Double-Ended Queues,"Finally, we use implicit recursive slowdown to implement catenable double-ended queues, 
 with the signature shown in Figure 8.3. We first describe a relatively simple implementation 
 that supports ++ in 
 O (log n)
  amortized time and all other operations in 
 O ()
  amortized time. We then 
 describe a much more complicated implementation that improves the running time of ++ to 
 O ()
 .
  
 Consider the following representation for catenable double-ended queues, or
  c-deques
 . A 
 c-deque is either
  shallow
  or
  deep
 . A shallow c-deque is simply an ordinary deque, such as 
 those presented in Chapter 5 or in the previous section. A deep c-deque is decomposed into 
 three segments: a
  front
 , a
  middle
 , and a
  rear
 . The front and rear are both ordinary deques 
 containing two or more elements each. The middle is a c-deque of ordinary deques, each 
 containing two",NA
8.6 Related Work,"Recursive Slowdown 
  
 Kaplan and Tarjan introduced recursive slowdown in [KT95], and
  
 used it again in [KT96b], but it is closely related to the regularity constraints of Guibas et",NA
Chapter 9,NA,NA
Conclusions,"In the preceding chapters, we have described a framework for designing and analyzing func-
 tional amortized data structures (Chapter 3), a method for eliminating amortization from such 
 data structures (Chapter 4), four general data structure design techniques (Chapters 5–8), and 
 sixteen new implementations of specific data structures. We next step back and reflect on the 
 significance of this work.",NA
9.1 ,NA,NA
Functional Programming,"Functional programming languages have historically suffered from the reputation of being 
 slow. Regardless of the advances in compiler technology, functional programs will never be 
 faster than their imperative counterparts as long as the algorithms available to functional pro-
 grammers are significantly slower than those available to imperative programmers. This 
 thesis provides numerous functional data structures that are asymptotically just as efficient as 
 the best imperative implementations. More importantly, we also provide numerous design 
 tech-niques so that functional programmers can create their own data structures, customized 
 to their particular needs.
  
 Our most significant contribution to the field of functional programming, however, is the 
 new understanding of the relationship between amortization and lazy evaluation. In the one 
 direction, the techniques of amortized analysis — suitably extended as in Chapter 3 — pro-
 vide the first practical approach to estimating the complexity of lazy programs. Previously, 
 functional programmers often had no better option than to pretend their lazy programs were 
 actually strict.
  
 In the other direction, lazy evaluation allows us to implement amortized data structures 
 that are efficient even when used persistently. Amortized data structures are desirable 
 because they are often both simpler and faster than their worst-case counterparts. Without 
 exception,",NA
9.2 Persistent Data Structures,"We have shown that memoization, in the form of lazy evaluation, can resolve the apparent 
 conflict between amortization and persistence. We expect to see many persistent amortized 
 data structures based on these ideas in the coming years.
  
 We have also reinforced the observation that functional programmingis an excellent 
 medium for developing new persistent data structures, even when the target language is 
 imperative. It is trivial to implement most functional data structures in an imperative language 
 such as C, and such implementations suffer few of the complications and overheads associated 
 with other methods for implementing persistent data structures, such as [DSST89] or [Die89]. 
 Further-more, unlike these other methods, functional programming has no problems with data 
 struc-tures that support combining functions such as list catenation. It is no surprise that the 
 best persistent implementations of data structures such as catenable lists (Section 7.2.1) and 
 caten-able deques (Section 8.5) are all purely functional (see also [KT95, KT96a]).",NA
9.3 Programming Language Design,"Next, we briefly discuss the implications of this work on programming language design.
  
 1
 As partial evidence for this fact, we note that only one of these implementations takes more than one page.",NA
9.4 Open Problems,"We conclude by describing some of the open problems related to this thesis.
  
  What are appropriate empirical measurements for persistent data structures? Standard 
 benchmarks are misleading since they do not measure how well a data structure sup-
 ports access to older versions. Unfortunately, the theory and practice of benchmarking 
 persistent data structures is still in its infancy.
  
  For ephemeral data structures, the physicist’s method is just as powerful as the banker’s 
 method. However, for persistent data structures, the physicist’s method appears to be 
 substantially weaker. Can the physicist’s method, as described in Section 3.5, be im-
 proved and made more widely applicable?
  
  The catenable deques of Section 8.5 are substantially more complicated than the caten-
 able lists of Section 7.2.1. Is there a simpler implementation of catenable deques closer 
 in spirit to that of catenable lists?
  
  Finally, can scheduling be applied to these implementations of catenable lists and de-
 ques? In both cases, maintaining a schedule appears to take more than 
 O ()
  time.",NA
Appendix A,NA,NA
The Definition of Lazy Evaluation ,NA,NA
in Standard ML,"The syntax and semantics of Standard ML are formally specified in
  The Definition of 
 Standard ML
  [MTH90]. This appendix extends the
  Definition
  with the syntax and semantics 
 of the lazy evaluation primitives (
 $
 -notation) described in Chapter 2. This appendix is 
 designed to be read in conjunction with the
  Definition
 ; it describes only the relevant changes 
 and additions.
  
  
 Paragraph headers such as
  [2.8 Grammar (8,9)]
  refer to sections within the
  Definition
 . 
 The numbers in parentheses specify the relevant pages.",NA
A.1 ,NA,NA
Syntax,"[2.1 Reserved Words (3)] 
  
 $
  is a reserved word and may not be used as an identifier.
  
 [2.8 Grammar (8,9)] 
  
 Add the following productions for expressions and patterns.
  
 exp 
  
 ::=
  $
  exp 
  
 and 
  
 pat 
  
 ::=
  $
  pat
  
 [Appendix B: Full Grammar (71–73)] 
 patterns.
  
 exp 
  
 ::=
  $
  exp
  
 Add the following productions for expressions and
  
 and
  
 pat
  
 ::=
  $
  pat
  
  
 These productions have lower precedence than any alternative form (i.e., appear last in the 
 lists of alternatives).",NA
A.2 Static Semantics,"[4.4 Types and Type functions (18)]
  susp
  does not admit equality.
  
 Remark: 
 This is an arbitrary choice. Allowing an equality operator on suspensions that
  
 automatically forces the suspensions and compares the results would also be reasonable, but
  
 would be moderately complicated.
  
 [4.7 Non-expansive Expressions (20)] $
  expressions are non-expansive.
  
 Remark:
  The dynamic evaluation of a
  $
  expression may in fact extend the domain of memory,
  
 but, for typechecking purposes, suspensions should be more like functions than references.
  
 [4.10 Inference Rules (24,29)] 
  
 Add the following inference rules.
  
 C `
  exp 
 ) 
  
 C `
  pat 
 )
  
 and
  
 C `
  $
  exp 
 )
  susp 
 C `
  $
  pat 
 )
  susp
  
 [4.11 Further Restrictions (30)] 
 Because matching against a
  $
  pattern may have effects (in
  
 particular, may cause assignments), it is now more difficult to determine if matches involving
  
 both suspensions and references are
  irredundant
  and
  exhaustive
 . For example, the first function
  
 below is non-exhaustive even though the first and third clauses appear to cover all cases and
  
 the second is irredundant even though the first and fourth clauses appear to overlap.
  
 fun
  f (ref true, 
  
 ) = 0
  
 j
  f (ref false,
  $
 0) = 1
  
 j
  f (ref false, ) = 2
  
 fun
  f (ref true, ) = 0 
  
  
 j
  f (ref false,
  $
 0) = 1 
  
  
 j
  f (ref false, ) = 2 
  
  
 j
  f (ref true, 
  
  
 ) = 3
  
 (Consider the execution of
  f (r,
 $
 (r := true; 1))
  where
  r
  initially equals
  ref 
 false
 .)
  
 [Appendix C: The Initial Static Basis (74,75)] 
 1 and does not admit equality.
  
 Extend
  
 T
  
 0
 to include
  susp
 , which has arity
  
 Add
  force
  to
  VE 
 0
 (Figure 23), where
  
 force
  
 !
  
  ’a 
 :
  ’a 
 susp
  
 !
  ’a",NA
A.3 ,NA,NA
Dynamic Semantics,"[6.3 Compound Objects (47)] 
  
 Add the following definitions to Figure 13.
  
 (
 exp
 ; E ) 
  
 Thunk 
 =
  Exp
  
  Env
  
 mem 
  
 Mem 
 =
  Addr
 fin 
 ! (
 Val 
 [
  Thunk
 )
  
 Remark:
  Addresses and memory are overloaded to represent both references and suspensions.
  
 The values of both references and suspensions are addresses. Addresses representing refer-
  
 ences are always mapped to values, but addresses representing suspensions may be mapped
  
 to either thunks (if unevaluated) or values (if evaluated and memoized). The static semantics
  
 ensures that there will be no confusion about whether a value in memory represents a reference
  
 or a memoized suspension.
  
 [6.7 Inference Rules (52,55,56)] 
 sion.
  
 Add the following inference rule for suspending an expres-
  
 a
  
  Dom
 (
 mem
  of
  
 s)
  
  
 s;
  
 E
  
 `
  $
  exp
  
 )
  
 a;
  
 s
  
 +
  
 fa
  
 !
  
 (
 exp
  
 ;
  
 E
  
 )g
  
 Extend the signatures involving pattern rows and patterns to allow exceptions to be raised
  
 during pattern matching.
  
 E ; r `
  patrow 
 )
  VE 
 =
 FAIL
 =p
  
 E
  
 ;
  
 v
  
 `
  pat
  
 )
  VE
  
 =
 FAIL
 =p
  
 Add the following inference rules for forcing a suspension.
  
 s(a)
  
 =
  
 (
 exp
 ;
  
 E
  
  
 0
  
 )
  
  
 s(a)
  
 =
  
 v
  
 s;
  
 E
  
 ;
  
 v
  
 `
  pat
  
 )
  VE
  
 =
 FAIL
  
 ;
  
  
 0 
  
 s
  
 )
  VE
  
 =
 FAIL
 ;
  
 00 
  
 s
  
 s;
  
 E
  
 0
  
 s;
  
 E
  
 ;
  
 a
  
 `
  $
  pat
  
 )
  VE
  
 =
 FAIL
  
 ;
  
 0 
  
 s
  
 `
  pat
  
 `
  exp
  
 )
  
 v
  
 ;
  
 0 
  
 s
  
  
 0 s
  
 +
  
 fa
  
 !
  
 v
  
 g;
  
 E
  
 ;
  
 v
  
 s;
  
 E
  
 ;
  
 a
  
 `
  $
  pat
  
 )
  VE
 =
 FAIL
  
 ;
  
 00 
  
 s
  
 The first rule looks up a memoized value. The second rule evaluates a suspension and memo-
  
 izes the result.
  
 Finally, modify Rule 158 to reflect the fact that matching against a pattern may change the
  
 state.
  
 s(a) = v 
  
 s; E ; v `
  atpat 
 )
  VE 
 =
 FAIL 
 ; s 
  
  
  
     
  
  
  
  
 0
  
 s; E ; a `
  ref
  atpat 
 )
  VE 
 =
 FAIL
 ; s 
  
  
   
  
  
  
  
 0 
  
 ()",NA
A.4 Recursion,"This section details the changes necessary to support recursive suspensions.
  
 [2.9 Syntactic Restrictions (9)] 
 Lift the syntactic restriction on
  rec
  to allow value bindings 
 of the form 
 v ar
  =
  $
  exp
  within
  rec
 .
  
 [6.7 Inference Rules (54)] 
  
 Modify Rule 137 as follows.
  
 s; E ` v al bind )
  VE
 ; s 
      
  
   
 0 
  
 VE 
 0 =
  Rec
  VE 
  
 s 
    
      
  
  
   
 00 =
  SRec
 (
 VE 
 0 
   
   
  
   
    
      
  
 ; s 
  
   
     
  
   
  
   
    
   
 0 
    
  
  
   
     
  
   
  
   
   
 )
  
  
 s;
  
 E
  
 `
  rec
  
 v
  
 al
  
 bind
  
 )
  VE
  
 0
  
 ;
  
  
 00 
  
 s
  
 where
  
 SRec
  
 :
  VarEnv
  
  State
  
 !
  State
  
 and
  
  ens
  of SRec
 (
 VE
  
 ;
  
 s)
  
 =
  ens
  of
  
 s
  
 ;
  
 s)(a)
  
 =
  
 (
 exp
 ;
  
 E
  
 +
  VE
  
 )
  
  Dom
 (
 mem
  of SRec
 (
 VE
  
 ;
  
 s))
  
 =
  Dom 
 (
 mem
  of
  
 s)
  
  If
  
 a
  
  Ran
 (
 VE
  
 )
 , then SRec
 (
 VE
  
 ;
  
 s)(a)
  
 =
  
 s(a)",NA
Bibliography,"[Ada93] 
  
 Stephen Adams. Efficient sets—a balancing act.
  Journal of Functional Program-
  
 ming
 , 3(4):553–561, October 1993. 
  
 (p. 17)
  
 [AFM 
 +
 95] Zena M. Ariola, Matthias Felleisen, John Maraist, Martin Odersky, and Philip
  
 Wadler. A call-by-need lambda calculus. In
  ACM Symposium on Principles of
  
 Programming Languages
 , pages 233–246, January 1995. 
  
 (p. 10)
  
 [AVL62] G. M. Adel’son-Vel’ski˘ı and E. M. Landis. 
  
 An algorithm for the organization
  
 of information.
  Soviet Mathematics–Doklady
 , 3(5):1259–1263, September 1962.
  
 English translation of Russian orginal appearing in
  Doklady Akademia Nauk SSSR
 ,
  
 146:263-266. 
  
 (p. 49)
  
 [Bac78] John Backus. 
  
 Can programming be liberated from the von Neumann style?
  
 A functional style and its algebra of programs.
  Communications of the ACM
 ,
  
 21(8):613–641, August 1978. 
  
 (p. 1)
  
 [BAG92] 
  
 Amir M. Ben-Amram and Zvi Galil. On pointers versus addresses.
  Journal of the
  
 ACM
 , 39(3):617–648, July 1992. 
  
 (p. 2)
  
 [BC93] 
  
 F. Warren Burton and Robert D. Cameron. Pattern matching with abstract data
  
 types.
  Journal of Functional Programming
 , 3(2):171–190, April 1993. 
 (p. 129)
  
 [Bel57] 
  
 Richard Bellman. 
  
 Dynamic Programming
 . Princeton University Press, 1957.
  
 (p. 12)
  
 [BH89] 
  
 Bror Bjerner and S¨oren Holmstr¨om. A compositional approach to time analysis 
 of
  
 first order lazy functional programs. In
  Conference on Functional Programming
  
 Languages and Computer Architecture
 , pages 157–165, September 1989. 
  
 (p. 38)
  
 [BJdM96] 
  
 Richard S. Bird, Geraint Jones, and Oege de Moor. 
  
 A lazy pure language
  
 versus impure Lisp. 
 http://www.comlab.ox.ac.uk/oucl/users/
  
 geraint.jones/publications/FP-1-96.html
 , 1996. 
  
 (p. 128)",NA
Index,"$-notation, 7–8, 12 
  
  
 formal definition, 131–135 
  
 pattern matching, 8 
  
  
 scope of, 7 
  
 ... (record wildcard), 17
  
 abstract data type, 5 
  
 abstraction, 5 
  
 accumulated debt, 20, 22 
  
 accumulated savings, 14, 20 
  
 accumulating parameter, 42, 46 
  
 actual cost, 22 
  
 amortization 
  
  
 banker’s method,
  see
  banker’s method 
  
 eliminating,
  see
  scheduling 
  
  
 physicist’s method,
  see
  physicist’s 
 method 
  
 problem with persistence, 19 
  
  
 traditional, 13–15 
  
 anticredits, 37 
  
 assignments, 2
  
 banker’s method, 130 
  
  
 justification of, 23–24 
  
  
 traditional, 14–15 
  
  
 with lazy evaluation, 23, 29 
  
 BankersDeque
  (functor), 56 
  
 BankersQueue
  (structure), 26 
  
 batched rebuilding, 49–50 
  
 BatchedQueue
  (structure), 18 
  
 benchmarks, 130 
  
 binary numbers, 6, 61, 62 
  
 BinaryRandomAccessList
  (structure), 69 
 binomial queues,
  see
  heaps, binomial 
 binomial trees, 64, 65, 70, 75
  
 BinomialHeap
  (functor), 73 
  
 blackhole, 135 
  
 boostrapping,
  see also
  data-structural boot-
 strapping 
  
 Bootstrap
  (functor), 102 
  
 bootstrap loader, 85 
  
 BootstrappedQueue
  (structure), 89 
  
 bootstrapping, 85 
  
 bootstrapping a compiler, 85 
  
 bottom-up mergesort, 33–36, 44–48 
  
 BottomUpMergeSort
  (structure), 35 
  
 Brodal, Gerth, 99
  
 c-deques,
  see
  catenable deques 
  
 caching, 2 
  
 call-by-name, 21, 24 
  
 call-by-need,
  see
  lazy evaluation 
  
 call-by-value,
  see
  strict evaluation 
  
 CAML and lazy evaluation, 12 
  
 catenable deques, 101–103, 115–125, 130 
 signature, 117 
  
 catenable lists, 93–98, 101–103, 130 
  
 signature, 94 
  
 catenable lists (Hughes), 82 
  
 catenable lists (Kaplan and Tarjan), 107 
 CatenableDeque
  (signature), 117 
  
 C
 ATENABLE
 L
 IST
  (signature), 94 
  
 CatenableList
  (functor), 97 
  
 cheap operations, 14 
  
 chef’s knives, 2 
  
 complete binary leaf trees, 64–66, 115 
 complete binary trees, 64, 77 
  
 complete cost, 21",NA
