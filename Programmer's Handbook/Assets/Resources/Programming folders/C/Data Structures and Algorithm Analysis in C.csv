Larger Text,Smaller Text,Symbol
Data Structures and Algorithm Analysis ,NA,NA
in C ,NA,NA
by Mark Allen Weiss ,"PREFACE 
  
 CHAPTER 1: INTRODUCTION 
  
 CHAPTER 2: ALGORITHM ANALYSIS 
  
 CHAPTER 3: LISTS, STACKS, AND QUEUES 
  
 CHAPTER 4: TREES 
  
 CHAPTER 5: HASHING 
  
 CHAPTER 6: PRIORITY QUEUES (HEAPS) 
  
 CHAPTER 7: SORTING 
  
 CHAPTER 8: THE DISJOINT SET ADT 
  
 CHAPTER 9: GRAPH ALGORITHMS 
  
 CHAPTER 10: ALGORITHM DESIGN TECHNIQUES 
  
 CHAPTER 11: AMORTIZED ANALYSIS",NA
PREFACE ,Next Chapter,NA
Purpose/Goals ,"This book describes data structures, methods of organizing large amounts of data, 
 and algorithm analysis, the estimation of the running time of algorithms. As 
 computers become faster and faster, the need for programs that can handle large 
 amounts of input becomes more acute. Paradoxically, this requires more careful 
 attention to efficiency, since inefficiencies in programs become most obvious when 
 input sizes are large. By analyzing an algorithm before it is actually coded, 
 students can decide if a particular solution will be feasible. For example, in this 
 text students look at specific problems and see how careful implementations can 
 reduce the time constraint for large amounts of data from 16 years to less than a 
 second. Therefore, no algorithm or data structure is presented without an 
 explanation of its running time. In some cases, minute details that affect the 
 running time of the implementation are explored. 
  
 Once a solution method is determined, a program must still be written. As 
 computers have become more powerful, the problems they solve have become larger 
 and more complex, thus requiring development of more intricate programs to solve 
 the problems. The goal of this text is to teach students good programming and 
 algorithm analysis skills simultaneously so that they can develop such programs 
 with the maximum amount of efficiency. 
  
 This book is suitable for either an advanced data structures (CS7) course or a 
 first-year graduate course in algorithm analysis. Students should have some 
 knowledge of intermediate programming, including such topics as pointers and 
 recursion, and some background in discrete math.",NA
Approach ,"I believe it is important for students to learn how to program for themselves, not 
 how to copy programs from a book. On the other hand, it is virtually 
  
 impossible to discuss realistic programming issues without including sample code. 
  
 For this reason, the book usually provides about half to three-quarters of an 
 implementation, and the student is encouraged to supply the rest. 
  
 The algorithms in this book are presented in ANSI C, which, despite some flaws, 
 is arguably the most popular systems programming language. The use of C instead 
 of Pascal allows the use of dynamically allocated arrays (see for instance 
 rehashing in Ch. 5). It also produces simplified code in several places, usually 
 because the and 
 (&&)
  operation is short-circuited. 
  
 Most criticisms of C center on the fact that it is easy to write code that is 
 barely readable. Some of the more standard tricks, such as the simultaneous 
 assignment and testing against 0 via 
  
 if (x=y)",NA
Overview ,"Chapter 1 contains review material on discrete math and recursion. I believe the 
 only way to be comfortable with recursion is to see good uses over and over. 
 Therefore, recursion is prevalent in this text, with examples in every chapter 
 except Chapter 5. 
  
 Chapter 2 deals with algorithm analysis. This chapter explains asymptotic analysis 
 and its major weaknesses. Many examples are provided, including an in-depth 
 explanation of logarithmic running time. Simple recursive programs are analyzed by 
 intuitively converting them into iterative programs. More complicated divide-and-
 conquer programs are introduced, but some of the analysis (solving recurrence 
 relations) is implicitly delayed until Chapter 7, where it is 
  
 performed in detail. 
  
 Chapter 3 covers lists, stacks, and queues. The emphasis here is on coding these 
 data structures using 
 ADTS
 , fast implementation of these data structures, and an 
 exposition of some of their uses. There are almost no programs (just 
  
 routines), but the exercises contain plenty of ideas for programming assignments. 
  
 Chapter 4 covers trees, with an emphasis on search trees, including external search 
 trees (B-trees). The 
 UNIX
  file system and expression trees are used as examples. 
 AVL
  trees and splay trees are introduced but not analyzed. Seventy-five percent 
 of the code is written, leaving similar cases to be completed by the student. 
 Additional coverage of trees, such as file compression and game trees, is deferred 
 until Chapter 10. Data structures for an external medium are 
  
 considered as the final topic in several chapters. 
  
 Chapter 5 is a relatively short chapter concerning hash tables. Some analysis is 
 performed and extendible hashing is covered at the end of the chapter. 
  
 Chapter 6 is about priority queues. Binary heaps are covered, and there is 
 additional material on some of the theoretically interesting implementations of 
 priority queues. 
  
 Chapter 7 covers sorting. It is very specific with respect to coding details and 
 analysis. All the important general-purpose sorting algorithms are covered and 
 compared. Three algorithms are analyzed in detail: insertion sort, Shellsort, and 
 quicksort. External sorting is covered at the end of the chapter. 
  
 Chapter 8 discusses the disjoint set algorithm with proof of the running time. 
 This is a short and specific chapter that can be skipped if Kruskal's algorithm 
 is not discussed. 
  
 Chapter 9 covers graph algorithms. Algorithms on graphs are interesting not only 
 because they frequently occur in practice but also because their running time is 
 so heavily dependent on the proper use of data structures. Virtually all of the",NA
Exercises ,"Exercises, provided at the end of each chapter, match the order in which material 
 is presented. The last exercises may address the chapter as a whole rather than a 
 specific section. Difficult exercises are marked with an asterisk, and more 
 challenging exercises have two asterisks. 
  
 A solutions manual containing solutions to almost all the exercises is available 
 separately from The Benjamin/Cummings Publishing Company.",NA
References ,"References are placed at the end of each chapter. Generally the references either 
 are historical, representing the original source of the material, or they represent 
 extensions and improvements to the results given in the text. Some references 
 represent solutions to exercises.",NA
Acknowledgments ,"I would like to thank the many people who helped me in the preparation of this and 
 previous versions of the book. The professionals at Benjamin/Cummings made my book 
 a considerably less harrowing experience than I had been led to expect. I'd like to 
 thank my previous editors, Alan Apt and John Thompson, as well as Carter Shanklin, 
 who has edited this version, and Carter's assistant, Vivian McDougal, for answering 
 all my questions and putting up with my delays. Gail Carrigan at Benjamin/Cummings 
 and Melissa G. Madsen and Laura Snyder at Publication Services did a wonderful job 
 with production. The C version was handled by Joe Heathward and his outstanding",NA
CHAPTER 1: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents",NA
INTRODUCTION ,"In this chapter, we discuss the aims and goals of this text and briefly review 
 programming concepts and discrete mathematics. We will 
  
  
  
  See that how a program performs for reasonably large input is just as 
 important as its performance on moderate amounts of input. 
  
  
  
  Review good programming style. 
  
  
  Summarize the basic mathematical background needed for the rest of the book. 
  
  
  Briefly review recursion.",NA
1.1. What's the Book About? ,"Suppose you have a group of n numbers and would like to determine the 
 kth largest. This is known as the selection problem. Most students who 
 have had a programming course or two would have no difficulty writing a 
 program to solve this problem. There are quite a few ""obvious"" 
 solutions. 
  
 One way to solve this problem would be to read the n numbers into an 
 array, sort the array in decreasing order by some simple algorithm such 
 as bubblesort, and then return the element in position k. 
  
 A somewhat better algorithm might be to read the first k elements into 
 an array and sort them (in decreasing order). Next, each remaining 
 element is read one by one. As a new element arrives, it is ignored if 
 it is smaller than the kth element in the array. Otherwise, it is 
 placed in its correct spot in the array, bumping one element out of the 
 array. When the algorithm ends, the element in the kth position is 
 returned as the answer. 
  
 Both algorithms are simple to code, and you are encouraged to do so. 
 The natural questions, then, are which algorithm is better and, more 
 importantly, is either algorithm good enough? A simulation using a 
 random file of 1 million elements and k = 500,000 will show that 
 neither algorithm finishes in a reasonable amount of time--each 
 requires several days of computer processing to terminate (albeit 
 eventually with a correct answer). An alternative method, discussed in 
 Chapter 7, gives a solution in about a second. Thus, although our 
 proposed algorithms work, they cannot be considered good algorithms, 
 because they are entirely impractical for input sizes that a third 
 algorithm can handle in a reasonable amount of time. 
  
 A second problem is to solve a popular word puzzle. The input consists 
 of a two-dimensional array of letters and a list of words. The object 
 is to find the words in the puzzle. These words may be horizontal, 
 vertical, or diagonal in any 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 th 
  
  had a 
  
  solve 
  
 ray, sort 
 rt, and 
  
 an array 
  
 ad one by 
 kth 
  
 e array, 
  
 ent in the 
  
 e natural s 
 either 
  
 ements and 
 mount of 
  
 lbeit 
  
 Chapter 7, 
 hms work, 
 practical t 
 of time. 
  
 of a two-
  
  the words 
 any 
  
 2006-1-27",NA
1.2. Mathematics Review ,"This section lists some of the basic formulas you need to memorize or be able to 
 derive and reviews basic proof techniques.",NA
1.2.1. Exponents ,"x
 a
  x
 b
  = x
 a+b
  
  xa
  
  --  = xa-b
  
  xb
  
  (xa)b = xab
  
 xn + xn = 2xn
  
  
  
  
  x2n
  
 2n + 2n = 2n+1",NA
1.2.2. Logarithms,"In computer science, all logarithms are to base 2 unless specified otherwise. 
  
 DEFINITION: xa = b if and only if logx b = a 
  
 Several convenient equalities follow from this definition. 
  
 THEOREM 1.1. 
  
  
 PROOF: 
  
 Let x = logc b, y = logc a, and z = loga b. Then, by the definition of logarithms, cx = b, cy = 
 a, and az = b. Combining these three equalities yields (cy)z = cx = b. Therefore, x = yz, which 
  
 implies z = x/y, proving the theorem. 
  
 THEOREM 1.2. 
  
 log ab = log a + log b 
  
 PROOF: 
  
 Let x = log a, y = log b, z = log ab. Then, assuming the default base of 2, 2x= a, 2y = b, 2z = 
  
 ab. Combining the last three equalities yields 2x2y = 2z = ab. Therefore, x + y = z, which proves 
  
 the theorem. 
  
 Some other useful formulas, which can all be derived in a similar manner, follow. 
  
 log a/b = log a - log b
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
1.2.3. Series,"The easiest formulas to remember are 
  
  
 and the companion, 
  
  
 In the latter formula, if 0 < a < 1, then 
  
  
 and as n tends to 
  
  
  
  
 , the sum approaches 1/(1 -a). These are the ""geometric series"" formulas. 
  
 We can derive the last formula for 
  in the following manner. Let S be the sum. 
  
 Then 
  
 S = 1 + a + a2 + a3 + a4 + a5 + . . .
  
 Then 
  
 aS = a + a2 + a3 + a4 + a5 + . . .
  
 If we subtract these two equations (which is permissible only for a convergent series), virtually 
  
 all the terms on the right side cancel, leaving 
  
 S - aS = 1
  
 which implies that 
  
  
 We can use this same technique to compute 
 , a sum that occurs frequently. We write 
  
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
1.2.4. Modular Arithmetic,"We say that a is congruent to b modulo n, written a 
  
  
  b(mod n), if n divides a - b. 
  
 Intuitively, this means that the remainder is the same when either a or b is divided by n. Thus, 
  
 81 
  
  
  61 
  
  
  
  1(mod 10). As with equality, if a 
  
  
  b (mod n), then a + c 
  
  
  b + c(mod n) 
  
 and a d 
  
  
  b d (mod n). 
  
 There are a lot of theorems that apply to modular arithmetic, some of which require extraordinary 
 proofs in number theory. We will use modular arithmetic sparingly, and the preceding theorems will 
 suffice.",NA
1.2.5. The P Word,"The two most common ways of proving statements in data structure analysis are proof by induction 
 and proof by contradiction (and occasionally a proof by intimidation, by professors only). The 
 best way of proving that a theorem is false is by exhibiting a counterexample. 
  
 Proof by Induction
  
 A proof by induction has two standard parts. The first step is proving a base case, that is, 
  
 establishing that a theorem is true for some small (usually degenerate) value(s); this step is 
 almost always trivial. Next, an inductive hypothesis is assumed. Generally this means that the 
 theorem is assumed to be true for all cases up to some limit k. Using this assumption, the 
 theorem is then shown to be true for the next value, which is typically k + 1. This proves the 
 theorem (as long as k is finite). 
  
 As an example, we prove that the Fibonacci numbers, F0 = 1, F1 = 1, F2 = 2, F3 = 3, F4 = 5, . . . 
  
 , Fi = Fi-1 + Fi-2, satisfy Fi < (5/3)i, for i 
  
  
  
  1. (Some definitions have F0 = 0, which 
  
 shifts the series.) To do this, we first verify that the theorem is true for the trivial cases. 
 It is easy to verify that F1 = 1 < 5/3 and F2 = 2 <25/9; this proves the basis. We assume that 
 the theorem is true for i = 1, 2, . . . , k; this is the inductive hypothesis. To prove the 
 theorem, we need to show that Fk+1 < (5/3)k+1. We have 
  
 Fk + 1= Fk + Fk-1
  
 by the definition, and we can use the inductive hypothesis on the right-hand side, obtaining 
  
 Fk+1 < (5/3)k + (5/3)k-1
  
 < (3/5)(5/3)k+1 + (3/5)2(5/3)k+1",NA
1.3. A Brief Introduction to Recursion,"Most mathematical functions that we are familiar with are described by a simple formula. For 
  
 instance, we can convert temperatures from Fahrenheit to Celsius by applying the formula 
  
 C = 5(F - 32)/9
  
 Given this formula, it is trivial to write a C function; with declarations and braces removed, 
 the one-line formula translates to one line of C. 
  
 Mathematical functions are sometimes defined in a less standard form. As an example, we can define 
 a function f, valid on nonnegative integers, that satisfies f(0) = 0 and f(x) = 2f(x - 1) + x2. 
 From this definition we see that f(1) = 1, f(2) = 6, f(3) = 21, and f(4) = 58. A function that is 
 defined in terms of itself is called recursive. C allows functions to be recursive.* It is 
 important to remember that what C provides is merely an attempt to follow the recursive spirit. 
 Not all mathematically recursive functions are efficiently (or correctly) implemented by C's 
 simulation of recursion. The idea is that the recursive function f ought to be expressible in",NA
Printing Out Numbers,"Suppose we have a positive integer, n, that we wish to print out. Our routine will have the 
  
 heading print_out(n). Assume that the only I/O routines available will take a single-digit number 
 and output it to the terminal. We will call this routine print_digit; for example, print_digit(4) 
 will output a 4 to the terminal. 
  
 Recursion provides a very clean solution to this problem. To print out 76234, we need to first 
 print out 7623 and then print out 4. The second step is easily accomplished with the statement 
 print_digit(n%10), but the first doesn't seem any simpler than the original problem. Indeed it is 
 virtually the same problem, so we can solve it recursively with the statement print_out(n/10). 
  
 This tells us how to solve the general problem, but we still need to make sure that the program 
 doesn't loop indefinitely. Since we haven't defined a base case yet, it is clear that we still 
  
 have something to do. Our base case will be print_digit(n) if 0 
  
  
  n < 10. Now print_out(n) is 
  
 defined for every positive number from 0 to 9, and larger numbers are defined in terms of a 
 smaller positive number. Thus, there is no cycle. The entire procedure* is shown Figure 1.4. 
  
 *The term procedure refers to a function that returns void. 
  
 We have made no effort to do this efficiently. We could have avoided using the mod routine (which 
  
 is very expensive) because n%10 = n - 
  
  
  
 n
 /10
  
  
  
  * 10.",NA
Recursion and Induction,"Let us prove (somewhat) rigorously that the recursive number-printing program works. To do so, 
  
 we'll use a proof by induction. 
  
 THEOREM 1.4",NA
Summary,"This chapter sets the stage for the rest of the book. The time taken by an algorithm confronted 
 with large amounts of input will be an important criterion for deciding if it is a good 
  
 algorithm. (Of course, correctness is most important.) Speed is relative. What is fast for one 
 problem on one machine might be slow for another problem or a different machine. We will begin to 
 address these issues in the next chapter and will use the mathematics discussed here to establish 
 a formal model.",NA
Exercises,"1.1 Write a program to solve the selection problem. Let k = n/2. Draw a table showing the running 
  
 time of your program for various values of n. 
  
 1.2 Write a program to solve the word puzzle problem. 
  
 1.3 Write a procedure to output an arbitrary real number (which might be negative) using only 
  
 print_digit for I/O. 
  
 1.4 C allows statements of the form 
  
 #include filename
  
 which reads filename and inserts its contents in place of the include statement. Include 
 statements may be nested; in other words, the file filename may itself contain an include 
 statement, but, obviously, a file can't include itself in any chain. Write a program that reads 
 in a file and outputs the file as modified by the include statements. 
  
 1.5 Prove the following formulas: 
  
 a. log x < x for all x > 0 
  
 b. log(ab) = b log a 
  
 1.6 Evaluate the following sums:",NA
References,"There are many good textbooks covering the mathematics reviewed in this chapter. A small subset is 
 [1], [2], [3], [11], [13], and [14]. Reference [11] is specifically geared toward the analysis 
  
 of algorithms. It is the first volume of a three-volume series that will be cited throughout this 
 text. More advanced material is covered in [6]. 
  
 Throughout this book we will assume a knowledge of C [10]. Occasionally, we add a feature where 
 necessary for clarity. We also assume familiarity with pointers and recursion (the recursion 
 summary in this chapter is meant to be a quick review). We will attempt to provide hints on their 
 use where appropriate throughout the textbook. Readers not familiar with these should consult [4], 
 [8], [12], or any good intermediate programming textbook.",NA
CHAPTER 2: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents
  
  
 Next Chapter",NA
ALGORITHM ANALYSIS ,"An algorithm is a clearly specified set of simple instructions to be followed to 
 solve a problem. Once an algorithm is given for a problem and decided (somehow) to 
 be correct, an important step is to determine how much in the way of 
  
 resources, such as time or space, the algorithm will require. An algorithm that 
 solves a problem but requires a year is hardly of any use. Likewise, an algorithm 
 that requires a gigabyte of main memory is not (currently) useful. 
  
 In this chapter, we shall discuss 
  
  
  How to estimate the time required for a program. 
  
  
  
  How to reduce the running time of a program from days or years to fractions 
 of a second. 
  
  
  
  The results of careless use of recursion. 
  
  
  Very efficient algorithms to raise a number to a power and to compute the 
  
 greatest common divisor of two numbers.",NA
2.1. Mathematical Background ,"The analysis required to estimate the resource use of an algorithm is generally a 
 theoretical issue, and therefore a formal framework is required. We begin with some 
 mathematical definitions. 
  
 Throughout the book we will use the following four definitions: 
  
 DEFINITION: T(n) = O(f(n)) if there are constants c and n
 0 
 such that T(n) 
  
  
  
  cf 
  
 (n) when n 
  
  
  
  n
 0
 . 
  
 DEFINITION: T(n) = 
  
  
 (g(n)) if there are constants c and n
 0
  such that T(n) 
  
  
 cg(n) when n 
  
  
  n
 0
 .",NA
2.2. Model ,"In order to analyze algorithms in a formal framework, we need a model of 
  
 computation. Our model is basically a normal computer, in which instructions are 
 executed sequentially. Our model has the standard repertoire of simple 
  
 instructions, such as addition, multiplication, comparison, and assignment, but, 
 unlike real computers, it takes exactly one time unit to do anything (simple). To 
 be reasonable, we will assume that, like a modern computer, our model has fixed 
 size (say 32-bit) integers and that there are no fancy operations, such as matrix 
 inversion or sorting, that clearly cannot be done in one time unit. We also assume 
 infinite memory. 
  
 This model clearly has some weaknesses. Obviously, in real life, not all 
 operations take exactly the same time. In particular, in our model one disk read 
 counts the same as an addition, even though the addition is typically several 
 orders of magnitude faster. Also, by assuming infinite memory, we never worry 
 about page faulting, which can be a real problem, especially for efficient 
 algorithms. This can be a major problem in many applications.",NA
2.3. What to Analyze ,"The most important resource to analyze is generally the running time. Several 
 factors affect the running time of a program. Some, such as the compiler and 
 computer used, are obviously beyond the scope of any theoretical model, so, 
 although they are important, we cannot deal with them here. The other main 
 factors are the algorithm used and the input to the algorithm. 
  
 Typically, the size of the input is the main consideration. We define two 
 functions, T
 avg
 (n) and T
 worst
 (n), as the average and worst-case running time, 
  
 respectively, used by an algorithm on input of size n. Clearly, T
 avg
 (n) 
  
  
 T
 worst
 (n). If there is more than one input, these functions may have more than 
 one argument.",NA
2.4. Running Time Calculations ,"There are several ways to estimate the running time of a program. The previous 
 table was obtained empirically. If two programs are expected to take similar 
 times, probably the best way to decide which is faster is to code them both up 
 and run them! 
  
 Generally, there are several algorithmic ideas, and we would like to eliminate 
 the bad ones early, so an analysis is usually required. Furthermore, the ability 
 to do an analysis usually provides insight into designing efficient algorithms. 
  
 The analysis also generally pinpoints the bottlenecks, which are worth coding 
 carefully. 
  
 To simplify the analysis, we will adopt the convention that there are no 
 particular units of time. Thus, we throw away leading constants. We will also 
 throw away low-order terms, so what we are essentially doing is computing a Big-
 Oh running time. Since Big-Oh is an upper bound, we must be careful to never 
 underestimate the running time of the program. In effect, the answer provided is 
 a guarantee that the program will terminate within a certain time period. The 
 program may stop earlier than this, but never later.",NA
2.4.1. A Simple Example ,"Here is a simple program fragment to calculate 
  
 unsigned int 
  
 sum( int n ) 
  
 { 
  
 unsigned int i, partial_sum; 
  
 /*1*/       partial_sum = 0; 
  
 /*2*/       for( i=1; i<=n; i++ ) 
  
 /*3*/            partial_sum += i*i*i; 
  
 /*4*/       return( partial_sum ); 
  
 } 
  
 The analysis of this program is simple. The declarations count for no time. Lines 1 
 and 4 count for one unit each. Line 3 counts for three units per time executed (two 
 multiplications and one addition) and is executed n times, for a total of 3n
  
 units. Line 2 has the hidden costs of initializing i, testing i 
  
  
  
  n, and 
  
 incrementing i. The total cost of all these is 1 to initialize, n + 1 for all the 
 tests, and n for all the increments, which is 2n + 2. We ignore the costs of 
 calling the function and returning, for a total of 5n + 4. Thus, we say that this 
 function is O (n).",NA
2.4.2. General Rules ,"RULE 1-FOR LOOPS: 
  
 The running time of a for loop is at most the running time of the statements 
 inside the for loop (including tests) times the number of iterations. 
  
 RULE 2-NESTED FOR LOOPS: 
  
 Analyze these inside out. The total running time of a statement inside a group of 
 nested for loops is the running time of the statement multiplied by the product of 
 the sizes of all the for loops. 
  
 As an example, the following program fragment is O(n
 2
 ): 
  
 for( i=0; i<n; i++ ) 
  
 for( j=0; j<n; j++ ) 
  
 k++; 
  
 RULE 3-CONSECUTIVE STATEMENTS: 
  
 These just add (which means that the maximum is the one that counts -- see 1(a) 
 on page 16). 
  
 As an example, the following program fragment, which has O(n) work followed by O 
 (n
 2
 ) work, is also O (n
 2
 ): 
  
 for( i=0; i<n; i++) 
  
 a[i] = 0; 
  
 for( i=0; i<n; i++ ) 
  
 for( j=0; j<n; j++ ) 
  
 a[i] += a[j] + i + j; 
  
 RULE 4-lF/ELSE: 
  
 For the fragment 
  
 if( cond )",NA
2.4.3 Solutions for the Maximum Subsequence Sum ,NA,NA
Problem ,"We will now present four algorithms to solve the maximum subsequence sum problem 
 posed earlier. The first algorithm is depicted in Figure 2.5. The indices in the 
 for loops reflect the fact that, in C, arrays begin at 0, instead of 1. Also, the 
 algorithm computes the actual subsequences (not just the sum); additional code is 
 required to transmit this information to the calling routine.",NA
2.4.4 Logarithms in the Running Time ,"The most confusing aspect of analyzing algorithms probably centers around the 
 logarithm. We have already seen that some divide-and-conquer algorithms will run 
 in O(n log n) time. Besides divide-and-conquer algorithms, the most frequent 
 appearance of logarithms centers around the following general rule: An algorithm 
 is O(log n) if it takes constant (O(1)) time to cut the problem size by a 
  
 fraction (which is usually 
 ). On the other hand, if constant time is required 
 to merely reduce the problem by a constant amount (such as to make the problem 
 smaller by 1), then the algorithm is O(n). 
  
 Something that should be obvious is that only special kinds of problems can be O 
 (log n). For instance, if the input is a list of n numbers, an algorithm must 
  
 take 
  
  
 (n) merely to read the input in. Thus when we talk about O(log n) 
  
 algorithms for these kinds of problems, we usually presume that the input is 
 preread. We provide three examples of logarithmic behavior.",NA
Binary Search ,NA,NA
Euclid's Algorithm ,"A second example is Euclid's algorithm for computing the greatest common divisor. 
  
 The greatest common divisor (gcd) of two integers is the largest integer that 
 divides both. Thus, gcd (50, 15) = 5. The algorithm in Figure 2.10 computes gcd
  
 (m, n), assuming m 
  
  
  n. (If n > m, the first iteration of the loop swaps 
  
 them). 
  
 The algorithm works by continually computing remainders until 0 is reached. The 
 last nonzero remainder is the answer. Thus, if m = 1,989 and n = 1,590, then the 
 sequence of remainders is 399, 393, 6, 3, 0. Therefore, gcd (1989, 1590) = 3. As 
 the example shows, this is a fast algorithm. 
  
 As before, the entire running time of the algorithm depends on determining how long 
 the sequence of remainders is. Although log n seems like a good answer, it is not 
 at all obvious that the value of the remainder has to decrease by a constant 
 factor, since we see that the remainder went from 399 to only 393 in the example. 
 Indeed, the remainder does not decrease by a constant factor in one",NA
Exponentiation ,"Our last example in this section deals with raising an integer to a power (which 
 is also an integer). Numbers that result from exponentiation are generally quite",NA
2.4.5 Checking Your Analysis,"Once an analysis has been performed, it is desirable to see if the answer is correct and as good 
  
 as possible. One way to do this is to code up the program and see if the empirically observed 
 running time matches the running time predicted by the analysis. When n doubles, the running time 
 goes up by a factor of 2 for linear programs, 4 for quadratic programs, and 8 for cubic programs. 
  
 Programs that run in logarithmic time take only an additive constant longer when n doubles, and 
 programs that run in O(n log n) take slightly more than twice as long to run under the same 
 circumstances. These increases can be hard to spot if the lower-order terms have relatively large 
 coefficients and n is not large enough. An example is the jump from n = 10 to n = 100 in the 
 running time for the various implementations of the maximum subsequence sum problem. It also can 
 be very difficult to differentiate linear programs from O(n log n) programs purely on empirical 
 evidence. 
  
 Another commonly used trick to verify that some program is O(f(n)) is to compute the values T(n)/ 
  
 f(n) for a range of n (usually spaced out by factors of 2), where T(n) is the empirically 
 observed running time. If f(n) is a tight answer for the running time, then the computed values 
 converge to a positive constant. If f(n) is an over-estimate, the values converge to zero. If f 
 (n) is an under-estimate and hence wrong, the values diverge. 
  
 As an example, the program fragment in Figure 2.12 computes the probability that two distinct 
 positive integers, less than or equal to n and chosen randomly, are relatively prime. (As n gets 
  
 large, the answer approaches 6/
  
  
  
 2.) 
  
 You should be able to do the analysis for this program instantaneously. Figure 2.13 shows the 
 actual observed running time for this routine on a real computer. The table shows that the last 
 column is most likely, and thus the analysis that you should have gotten is probably correct. 
  
 Notice that there is not a great deal of difference between O(n2) and O(n2 log n), since 
 logarithms grow so slowly.",NA
2.4.6. A Grain of Salt,"Sometimes the analysis is shown empirically to be an over-estimate. If this is the case, then 
 either the analysis needs to be tightened (usually by a clever observation), or it may be the case 
 that the average running time is significantly less than the worst-case running time and no 
 improvement in the bound is possible. There are many complicated algorithms for which the worst-
 case bound is achievable by some bad input but is usually an over-estimate in practice. 
  
 Unfortunately, for most of these problems, an average-case analysis is extremely complex (in many 
 cases still unsolved), and a worst-case bound, even though overly pessimistic, is the best 
 analytical result known. 
  
 rel = 0; tot = 0;",NA
Summary,"This chapter gives some hints on how to analyze the complexity of programs. Unfortunately, it is 
  
 not a complete guide. Simple programs usually have simple analyses, but this is not always the 
  
 case. As an example, we shall see, later in the text, a sorting algorithm (Shellsort, Chapter 7) 
  
 and an algorithm for maintaining disjoint sets (Chapter 8) each of which requires about 20 lines 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
Exercises,"2.1 Order the following functions by growth rate: n, 
 , n1.5, n2, n log n, n log log n, n log2 
  
 n, n log(n2), 2/n, 2, 2n/2, 37, n2 log n, n3. Indicate which functions grow at the same rate. 
  
 2.2 Suppose Tl(n) = O(f(n)) and T2(n) = O(f(n)). Which of the following are true? 
  
 a. T1(n) + T2(n) = O(f(n)) 
  
 b. T1(n) - T2(n) = o(f(n)) 
  
  
 d. T1(n) = O(T2(n)) 
  
 2.3 Which function grows faster: n log n or n1+
  
  
  
 / 
  
  
  
  
  > 0 ?
  
 2.4 Prove that for any constant, k, logkn = o(n). 
  
 2.5 Find two functions f(n) and g(n) such that neither 
  
  
  
 (n) = O(g(n)) nor g(n) = O(f(n)). 
  
 2.6 For each of the following six program fragments: 
  
 a. Give an analysis of the running time (Big-Oh will do). 
  
 b. Implement the code in the language of your choice, and give the running time for several 
 values of n.",NA
References,"Analysis of the running time of algorithms was first made popular by Knuth in the three-part 
 series [5], [6], and [7]. The analysis of the gcd algorithm appears in [6]. Another early text on 
 the subject is [1]. 
  
 Big-Oh, big-omega, big-theta, and little-oh notation were advocated by Knuth in [8]. There is 
  
 still not a uniform agreement on the matter, especially when it comes to using 
  
  
  
 ( ). Many 
  
 people prefer to use O( ), even though it is less expressive. Additionally, O( ) is still used in",NA
CHAPTER 3: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents
  
  
  
  
 Next Chapter",NA
"LISTS, STACKS, AND QUEUES ","This chapter discusses three of the most simple and basic data structures. 
  
 Virtually every significant program will use at least one of these structures 
 explicitly, and a stack is always implicitly used in your program, whether or not 
 you declare one. Among the highlights of this chapter, we will 
  
  
  
  Introduce the concept of Abstract Data Types (
 ADT
 s). 
  
  
  Show how to efficiently perform operations on lists. 
  
  
  Introduce the stack 
 ADT
  and its use in implementing recursion. 
  
  
  Introduce the queue 
 ADT
  and its use in operating systems and algorithm 
  
 design. 
  
 Because these data structures are so important, one might expect that they are 
 hard to implement. In fact, they are extremely easy to code up; the main 
 difficulty is keeping enough discipline to write good general-purpose code for 
 routines that are generally only a few lines long.",NA
3.1. Abstract Data Types (ADTs) ,"One of the basic rules concerning programming is that no routine should ever exceed 
 a page. This is accomplished by breaking the program down into modules. Each module 
 is a logical unit and does a specific job. Its size is kept small by calling other 
 modules. Modularity has several advantages. First, it is much easier to debug small 
 routines than large routines. Second, it is easier for several people to work on a 
 modular program simultaneously. Third, a well-written modular program places 
 certain dependencies in only one routine, making changes easier. For instance, if 
 output needs to be written in a certain format, it is certainly important to have 
 one routine to do this. If printing statements are scattered throughout the 
 program, it will take considerably longer to make modifications. The idea that 
 global variables and side effects are bad is directly attributable to the idea that 
 modularity is good. 
  
 An abstract data type (
 ADT
 ) is a set of operations. Abstract data types are 
 mathematical abstractions; nowhere in an 
 ADT
 's definition is there any mention 
 of how the set of operations is implemented. This can be viewed as an extension 
 of modular design. 
  
 Objects such as lists, sets, and graphs, along with their operations, can be viewed 
 as abstract data types, just as integers, reals, and booleans are data types. 
 Integers, reals, and booleans have operations associated with them, and so do 
 abstract data types. For the set 
 ADT
 , we might have such operations as union,",NA
3.2. The List ADT ,"We will deal with a general list of the form a
 1
 , a
 2
 , a
 3
 , . . . , a
 n
 . We say that 
 the size of this list is n. We will call the special list of size 0 a null list. 
  
 For any list except the null list, we say that a
 i+l
  follows (or succeeds) a
 i 
 (i < 
 n) and that a
 i-1
  precedes a
 i
  (i > 1). The first element of the list is a
 1
 , and the 
 last element is a
 n
 . We will not define the predecessor of a
 1
  or the successor of 
 a
 n
 . The position of element a
 i
  in a list is i. Throughout this discussion, we will 
 assume, to simplify matters, that the elements in the list are integers, but in 
 general, arbitrarily complex elements are allowed. 
  
 Associated with these ""definitions"" is a set of operations that we would like to 
 perform on the list 
 ADT
 . Some popular operations are print_list and make_null, 
 which do the obvious things; find, which returns the position of the first 
 occurrence of a key; insert and delete, which generally insert and delete some key 
 from some position in the list; and find_kth, which returns the element in some 
 position (specified as an argument). If the list is 34, 12, 52, 16, 12, then 
 find(52) might return 3; insert(x,3) might make the list into 34, 12, 52, x, 16, 12 
 (if we insert after the position given); and delete(3) might turn that list into 
 34, 12, x, 16, 12. 
  
 Of course, the interpretation of what is appropriate for a function is entirely up 
 to the programmer, as is the handling of special cases (for example, what does 
 find(1) return above?). We could also add operations such as next and previous, 
 which would take a position as argument and return the position of the successor 
 and predecessor, respectively.",NA
3.2.1. Simple Array Implementation of Lists ,"Obviously all of these instructions can be implemented just by using an array. 
 Even if the array is dynamically allocated, an estimate of the maximum size of 
 the list is required. Usually this requires a high over-estimate, which wastes",NA
3.2.2. Linked Lists ,"In order to avoid the linear cost of insertion and deletion, we need to ensure 
 that the list is not stored contiguously, since otherwise entire parts of the 
 list will need to be moved. Figure 3.1 shows the general idea of a linked list. 
  
 The linked list consists of a series of structures, which are not necessarily 
 adjacent in memory. Each structure contains the element and a pointer to a 
 structure containing its successor. We call this the next pointer. The last cell's 
 next pointer points to ; this value is defined by C and cannot be confused with 
 another pointer. ANSI C specifies that is zero. 
  
 Recall that a pointer variable is just a variable that contains the address where 
 some other data is stored. Thus, if p is declared to be a pointer to a structure, 
 then the value stored in p is interpreted as the location, in main memory, where a 
 structure can be found. A field of that structure can be accessed by p
  
  
 field_name, where field_name is the name of the field we wish to examine. 
  
 Figure 3.2 shows the actual representation of the list in Figure 3.1. The list 
 contains five structures, which happen to reside in memory locations 1000, 800, 
 712, 992, and 692 respectively. The next pointer in the first structure has the 
 value 800, which provides the indication of where the second structure is. The 
 other structures each have a pointer that serves a similar purpose. Of course, in 
 order to access this list, we need to know where the first cell can be found. A 
 pointer variable can be used for this purpose. It is important to remember that a 
 pointer is just a number. For the rest of this chapter, we will draw pointers with 
 arrows, because they are more illustrative.",NA
3.2.3. Programming Details ,"The description above is actually enough to get everything working, but there are 
 several places where you are likely to go wrong. First of all, there is no really 
 obvious way to insert at the front of the list from the definitions given. Second, 
 deleting from the front of the list is a special case, because it changes the start 
 of the list; careless coding will lose the list. A third problem concerns deletion 
 in general. Although the pointer moves above are simple, the deletion algorithm 
 requires us to keep track of the cell before the one that we want to delete.",NA
3.2.4. Common Errors ,"The most common error that you will get is that your program will crash with a 
 nasty error message from the system, such as ""memory access violation"" or 
  
 ""segmentation violation."" This message usually means that a pointer variable 
 contained a bogus address. One common reason is failure to initialize the 
  
 variable. For instance, if line 1 in Figure 3.14 is omitted, then p is undefined 
 and is not likely to be pointing at a valid part of memory. Another typical error 
 would be line 6 in Figure 3.13. If p is , then the indirection is illegal. This 
 function knows that p is not , so the routine is OK. Of course, you should comment 
 this so that the routine that calls insert will insure this. Whenever you do an 
 indirection, you must make sure that the pointer is not NULL. Some C compliers will 
 implicity do this check for you, but this is not part of the C standard. When you 
 port a program from one compiler to another, you may find that it no longer works. 
 This is one of the common reasons why. 
  
 The second common mistake concerns when and when not to use malloc to get a new 
 cell. You must remember that declaring a pointer to a structure does not create the 
 structure but only gives enough space to hold the address where some 
  
 structure might be. The only way to create a record that is not already declared is 
 to use the malloc command. The command malloc(size_p) has the system create, 
 magically, a new structure and return a pointer to it. If, on the other hand, you 
 want to use a pointer variable to run down a list, there is no need to declare a 
 new structure; in that case the malloc command is inappropriate. A type cast is",NA
3.2.5. Doubly Linked Lists ,"Sometimes it is convenient to traverse lists backwards. The standard 
  
 implementation does not help here, but the solution is simple. Merely add an extra 
 field to the data structure, containing a pointer to the previous cell. The cost of 
 this is an extra link, which adds to the space requirement and also doubles the 
 cost of insertions and deletions because there are more pointers to fix. On the 
 other hand, it simplifies deletion, because you no longer have to refer to a key by 
 using a pointer to the previous cell; this information is now at hand. Figure 3.16 
 shows a doubly linked list.",NA
3.2.6. Circularly Linked Lists ,"A popular convention is to have the last cell keep a pointer back to the first. 
  
 This can be done with or without a header (if the header is present, the last cell 
 points to it), and can also be done with doubly linked lists (the first cell's 
 previous pointer points to the last cell). This clearly affects some of the tests, 
 but the structure is popular in some applications. Figure 3.17 shows a double 
 circularly linked list with no header.",NA
3.2.7. Examples ,"We provide three examples that use linked lists. The first is a simple way to 
 represent single-variable polynomials. The second is a method to sort in linear 
 time, for some special cases. Finally, we show a complicated example of how 
 linked lists might be used to keep track of course registration at a university.",NA
The Polynomial ADT ,"We can define an abstract data type for single-variable polynomials (with 
  
 nonnegative exponents) by using a list. Let 
 . If most of the 
 coefficients a
 i
  are nonzero, we can use a simple array to store the coefficients. 
  
 We could then write routines to perform addition, subtraction, multiplication, 
 differentiation, and other operations on these polynomials. In this case, we might 
 use the type declarations given in Figure 3.18. We could then write routines to 
 perform various operations. Two possibilities are addition and multiplication. 
 These are shown in Figures 3.19 to 3.21. Ignoring the time to initialize the output 
 polynomials to zero, the running time of the multiplication routine is proportional 
 to the product of the degree of the two input 
  
 polynomials. This is adequate for dense polynomials, where most of the terms are 
 present, but if p
 1
 (x) = 10x
 1000
  + 5x
 14
  + 1 and p
 2
 (x) = 3x
 1990
  - 2x
 1492
  + 11x + 5, 
 then the running time is likely to be unacceptable. One can see that most of the 
 time is spent multiplying zeros and stepping through what amounts to nonexistent 
 parts of the input polynomials. This is always undesirable. 
  
  
 Figure 3.17 A double circularly linked list 
  
 typedef struct 
  
 { 
  
 int coeff_array[ MAX_DEGREE+1 ]; 
  
 unsigned int high_power; 
  
 } *POLYNOMIAL; 
  
 Figure 3.18 Type declarations for array implementation of the polynomial 
 ADT
  
 An alternative is to use a singly linked list. Each term in the polynomial is 
 contained in one cell, and the cells are sorted in decreasing order of exponents. 
 For instance, the linked lists in Figure 3.22 represent p
 1
 (x) and p
 2
 (x). We could 
 then use the declarations in Figure 3.23. 
  
 void",NA
Radix Sort ,"A second example where linked lists are used is called radix sort. Radix sort is 
 sometimes known as card sort, because it was used, until the advent of modern 
 computers, to sort old-style punch cards. 
  
 If we have n integers in the range 1 to m (or 0 to m - 1) 9, we can use this 
 information to obtain a fast sort known as bucket sort. We keep an array called 
 count, of size m, which is initialized to zero. Thus, count has m cells (or 
 buckets), which are initially empty. When a
 i
  is read, increment (by one) count 
 [a
 i
 ]. After all the input is read, scan the count array, printing out a 
  
 representation of the sorted list. This algorithm takes O(m + n); the proof is 
  
 left as an exercise. If m = 
  
  
 (n), then bucket sort is O(n). 
  
 Radix sort is a generalization of this. The easiest way to see what happens is by 
 example. Suppose we have 10 numbers, in the range 0 to 999, that we would like to 
 sort. In general, this is n numbers in the range 0 to n
 p
  - 1 for some constant p. 
 Obviously, we cannot use bucket sort; there would be too many buckets. The trick is 
 to use several passes of bucket sort. The natural algorithm would be to bucket-sort 
 by the most significant ""digit"" (digit is taken to base n), then next most 
 significant, and so on. That algorithm does not work, but if we perform bucket 
 sorts by least significant ""digit"" first, then the algorithm works. Of course, more 
 than one number could fall into the same bucket, and, unlike the original bucket 
 sort, these numbers could be different, so we keep them in a list. Notice that all 
 the numbers could have some digit in common, so if a simple array were used for the 
 lists, then each array would have to be of size n, for a 
  
 total space requirement of 
  
  
 (n
 2
 ). 
  
 The following example shows the action of radix sort on 10 numbers. The input is 
 64, 8, 216, 512, 27, 729, 0, 1, 343, 125 (the first ten cubes arranged randomly). 
 The first step bucket sorts by the least significant digit. In this case the math 
 is in base 10 (to make things simple), but do not assume this in general. The 
 buckets are as shown in Figure 3.24, so the list, sorted by least significant 
 digit, is 0, 1, 512, 343, 64, 125, 216, 27, 8, 729. These are now sorted by the 
 next least significant digit (the tens digit here) (see Fig. 3.25). Pass 2 gives 
 output 0, 1, 8, 512, 216, 125, 27, 729, 343, 64. This list is now sorted with 
 respect to the two least significant digits. The final pass, shown in Figure 3.26, 
 bucket-sorts by most significant digit. The final list is 0, 1, 8, 27, 64, 125, 
 216, 343, 512, 729. 
  
 To see that the algorithm works, notice that the only possible failure would 
 occur if two numbers came out of the same bucket in the wrong order. But the 
 previous passes ensure that when several numbers enter a bucket, they enter in 
 sorted order. The running time is O(p(n + b)) where p is the number of passes, n 
 is the number of elements to sort, and b is the number of buckets. In our case, b 
 = n. 
  
  0  1  512  343  64  125  216  27  8  729",NA
Multilists ,"Our last example shows a more complicated use of linked lists. A university with 
 40,000 students and 2,500 courses needs to be able to generate two types of 
 reports. The first report lists the class registration for each class, and the 
 second report lists, by student, the classes that each student is registered for. 
  
 The obvious implementation might be to use a two-dimensional array. Such an array 
 would have 100 million entries. The average student registers for about three 
 courses, so only 120,000 of these entries, or roughly 0.1 percent, would actually 
 have meaningful data. 
  
 What is needed is a list for each class, which contains the students in the 
 class. We also need a list for each student, which contains the classes the 
 student is registered for. Figure 3.27 shows our implementation.",NA
3.2.8. Cursor Implementation of Linked Lists ,"Many languages, such as 
 BASIC
  and 
 FORTRAN
 , do not support pointers. If linked 
 lists are required and pointers are not available, then an alternate 
  
 implementation must be used. The alternate method we will describe is known as a 
 cursor implementation. 
  
 The two important items present in a pointer implementation of linked lists are",NA
3.3. The Stack ADT ,NA,NA
3.3.1. Stack Model ,"A stack is a list with the restriction that inserts and deletes can be performed 
 in only one position, namely the end of the list called the top. The fundamental 
 operations on a stack are push, which is equivalent to an insert, and pop, which 
 deletes the most recently inserted element. The most recently inserted element 
 can be examined prior to performing a pop by use of the top routine. A pop or top 
 on an empty stack is generally considered an error in the stack 
 ADT
 . On the other 
 hand, running out of space when performing a push is an implementation 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
3.3.2. Implementation of Stacks ,"Of course, since a stack is a list, any list implementation will do. We will give 
 two popular implementations. One uses pointers and the other uses an array, but, as 
 we saw in the previous section, if we use good programming principles the calling 
 routines do not need to know which method is being used.",NA
Linked List Implementation of Stacks ,"The first implementation of a stack uses a singly linked list. We perform a push by 
 inserting at the front of the list. We perform a pop by deleting the element at the 
 front of the list. A top operation merely examines the element at the front of the 
 list, returning its value. Sometimes the pop and top operations are combined into 
 one. We could use calls to the linked list routines of the previous",NA
Array Implementation of Stacks ,"An alternative implementation avoids pointers and is probably the more popular 
 solution. The only potential hazard with this strategy is that we need to declare 
 an array size ahead of time. Generally this is not a problem, because in typical 
 applications, even if there are quite a few stack operations, the actual number of 
 elements in the stack at any time never gets too large. It is usually easy to 
 declare the array to be large enough without wasting too much space. If this is not 
 possible, then a safe course would be to use a linked list implementation. 
  
 If we use an array implementation, the implementation is trivial. Associated with 
 each stack is the top of stack, tos, which is -1 for an empty stack (this is how an 
 empty stack is initialized). To push some element x onto the stack, we increment 
 tos and then set STACK[tos] = x, where STACK is the array representing the actual 
 stack. To pop, we set the return value to STACK[tos] and then 
  
 decrement tos. Of course, since there are potentially several stacks, the STACK 
 array and tos are part of one structure representing a stack. It is almost always a 
 bad idea to use global variables and fixed names to represent this (or any) data 
 structure, because in most real-life situations there will be more than one stack. 
 When writing your actual code, you should attempt to follow the model as closely as 
 possible, so that no part of your code, except for the stack routines, can attempt 
 to access the array or top-of-stack variable implied by each stack. 
  
 This is true for all 
 ADT
  operations. Modern languages such as Ada and C++ can 
 actually enforce this rule. 
  
 typedef struct node 
 *
 node_ptr;",NA
3.3.3. Applications ,"It should come as no surprise that if we restrict the operations allowed on a list, 
 those operations can be performed very quickly. The big surprise, however, is that 
 the small number of operations left are so powerful and important. We give three of 
 the many applications of stacks. The third application gives a deep insight into 
 how programs are organized.",NA
Balancing Symbols ,"Compilers check your programs for syntax errors, but frequently a lack of one 
 symbol (such as a missing brace or comment starter) will cause the compiler to 
 spill out a hundred lines of diagnostics without identifying the real error. 
  
 A useful tool in this situation is a program that checks whether everything is 
 balanced. Thus, every right brace, bracket, and parenthesis must correspond to 
 their left counterparts. The sequence [()] is legal, but [(]) is wrong. 
  
 Obviously, it is not worthwhile writing a huge program for this, but it turns out",NA
Postfix Expressions ,"Suppose we have a pocket calculator and would like to compute the cost of a 
 shopping trip. To do so, we add a list of numbers and multiply the result by 
 1.06; this computes the purchase price of some items with local sales tax added. 
 If the items are 4.99, 5.99, and 6.99, then a natural way to enter this would be 
 the sequence 
  
 4.99 + 5.99 + 6.99 
 *
  1.06 =
  
 Depending on the calculator, this produces either the intended answer, 19.05, or 
 the scientific answer, 18.39. Most simple four-function calculators will give the 
 first answer, but better calculators know that multiplication has higher 
  
 precedence than addition. 
  
 On the other hand, some items are taxable and some are not, so if only the first 
 and last items were actually taxable, then the sequence 
  
 4.99 
 *
  1.06 + 5.99 + 6.99 
 *
  1.06 =
  
 would give the correct answer (18.69) on a scientific calculator and the wrong 
 answer (19.37) on a simple calculator. A scientific calculator generally comes 
 with parentheses, so we can always get the right answer by parenthesizing, but 
 with a simple calculator we need to remember intermediate results. 
  
 A typical evaluation sequence for this example might be to multiply 4.99 and 
 1.06, saving this answer as a
 1
 . We then add 5.99 and a
 1
 , saving the result in a
 1
 . 
 We multiply 6.99 and 1.06, saving the answer in a
 2
 , and finish by adding a
 l
  and 
 a
 2
 , leaving the final answer in a
 l
 . We can write this sequence of operations as 
 follows: 
  
 4.99 1.06 
 *
  5.99 + 6.99 1.06 
 *
  +",NA
3.4. The Queue ADT,"Like stacks, queues are lists. With a queue, however, insertion is done at one end, whereas 
  
 deletion is performed at the other end.",NA
3.4.1. Queue Model,"The basic operations on a queue are enqueue, which inserts an element at the end of the list 
  
 (called the rear), and dequeue, which deletes (and returns) the element at the start of the list 
  
 (known as the front). Figure 3.56 shows the abstract model of a queue. 
  
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
3.4.2. Array Implementation of Queues,"As with stacks, any list implementation is legal for queues. Like stacks, both the linked list 
 and array implementations give fast O(1) running times for every operation. The linked list 
 implementation is straightforward and left as an exercise. We will now discuss an array 
 implementation of queues. 
  
 For each queue data structure, we keep an array, QUEUE[], and the positions q_front and q_rear, 
 which represent the ends of the queue. We also keep track of the number of elements that are 
 actually in the queue, q_size. All this information is part of one structure, and as usual, except 
 for the queue routines themselves, no routine should ever access these directly. The following 
 figure shows a queue in some intermediate state. By the way, the cells that are blanks have 
 undefined values in them. In particular, the first two cells have elements that used to be in the 
 queue. 
  
  
 The operations should be clear. To enqueue an element x, we increment q_size and q_rear, then set 
 QUEUE[q_rear] = x. To dequeue an element, we set the return value to QUEUE[q_front], decrement 
 q_size, and then increment q_front. Other strategies are possible (this is discussed later). We 
 will comment on checking for errors presently. 
  
 There is one potential problem with this implementation. After 10 enqueues, the queue appears to 
 be full, since q_front is now 10, and the next enqueue would be in a nonexistent position. 
  
 However, there might only be a few elements in the queue, because several elements may have 
 already been dequeued. Queues, like stacks, frequently stay small even in the presence of a lot 
 of operations. 
  
 The simple solution is that whenever q_front or q_rear gets to the end of the array, it is 
 wrapped around to the beginning. The following figure shows the queue during some operations. 
 This is known as a circular array implementation.",NA
3.4.3. Applications of Queues ,"There are several algorithms that use queues to give efficient running times. Several of these 
  
 are found in graph theory, and we will discuss them later in Chapter 9. For now, we will give 
 some simple examples of queue usage. 
  
 struct queue_record
  
 {
  
 unsigned int q_max_size;  /* Maximum # of elements */
  
 /* until Q is full */
  
 unsigned int q_front;
  
 unsigned int q_rear;
  
 unsigned int q_size;      /* Current # of elements in Q */
  
 element_type *q_array;
  
 };
  
 typedef struct queue_record * QUEUE;
  
 Figure 3.57 Type declarations for queue--array implementation
  
 int
  
 is_empty( QUEUE Q )
  
 {
  
 return( Q->q_size == 0 );
  
 }
  
 Figure 3.58 Routine to test whether a queue is empty-array implementation
  
 void
  
 make_null ( QUEUE Q )
  
 {
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
Summary,"This chapter describes the concept of 
 ADT
 s and illustrates the concept with three of the most 
 common abstract data types. The primary objective is to separate the implementation of the 
 abstract data types from their function. The program must know what the operations do, but it is 
 actually better off not knowing how it is done. 
  
 Lists, stacks, and queues are perhaps the three fundamental data structures in all of computer 
 science, and their use is documented through a host of examples. In particular, we saw how stacks 
 are used to keep track of procedure and function calls and how recursion is actually implemented. 
  
 This is important to understand, not just because it makes procedural languages possible, but 
 because knowing how recursion is implemented removes a good deal of the mystery that surrounds 
 its use. Although recursion is very powerful, it is not an entirely free operation; misuse and 
 abuse of recursion can result in programs crashing.",NA
Exercises,"3.1 Write a program to print out the elements of a singly linked list. 
  
 3.2 You are given a linked list, L, and another linked list, P, containing integers, sorted in 
 ascending order. The operation print_lots(L,P) will print the elements in L that are in positions 
 specified by P. For instance, if P = 1, 3, 4, 6, the first, third, fourth, and sixth elements in L 
 are printed. Write the routine print_lots(L,P). You should use only the basic list operations. 
 What is the running time of your routine?",NA
CHAPTER 4: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents",NA
TREES ,"For large amounts of input, the linear access time of linked lists is 
  
 prohibitive. In this chapter we look at a simple data structure for which the 
 running time of most operations is O(log n) on average. We also sketch a 
  
 conceptually simple modification to this data structure that guarantees the above 
 time bound in the worst case and discuss a second modification that essentially 
 gives an O(log n) running time per operation for a long sequence of instructions. 
  
 The data structure that we are referring to is known as a binary search tree. 
  
 Trees in general are very useful abstractions in computer science, so we will 
 discuss their use in other, more general applications. In this chapter, we will 
  
  
  
  See how trees are used to implement the file system of several popular 
 operating systems. 
  
  
  See how trees can be used to evaluate arithmetic expressions. 
  
  
  
  Show how to use trees to support searching operations in O(log n) average 
  
 time, and how to refine these ideas to obtain O(log n) worst-case bounds. We will 
 also see how to implement these operations when the data is stored on a disk.",NA
4.1. Preliminaries ,"A tree can be defined in several ways. One natural way to define a tree is 
 recursively. A tree is a collection of nodes. The collection can be empty, which 
 is sometimes denoted as A. Otherwise, a tree consists of a distinguished node r, 
 called the root, and zero or more (sub)trees T
 1
 , T
 2
 , . . . , T
 k
 , each of whose 
 roots are connected by a directed edge to r. 
  
 The root of each subtree is said to be a child of r, and r is the parent of each 
 subtree root. Figure 4.1 shows a typical tree using the recursive definition. 
  
 From the recursive definition, we find that a tree is a collection of n nodes, 
 one of which is the root, and n - 1 edges. That there are n - 1 edges follows 
 from the fact that each edge connects some node to its parent, and every node 
 except the root has one parent (see Fig. 4.2).",NA
4.1.1. Implementation of Trees ,"One way to implement a tree would be to have in each node, besides its data, a 
 pointer to each child of the node. However, since the number of children per node 
 can vary so greatly and is not known in advance, it might be infeasible to make the 
 children direct links in the data structure, because there would be too much wasted 
 space. The solution is simple: Keep the children of each node in a linked list of 
 tree nodes. The declaration in Figure 4.3 is typical.",NA
4.1.2. Tree Traversals with an Application ,"There are many applications for trees. One of the popular uses is the directory 
 structure in many common operating systems, including 
 UNIX, VAX/VMS
 , and 
 DOS
 . 
 Figure 4.5 is a typical directory in the 
 UNIX
  file system. 
  
 The root of this directory is /usr. (The asterisk next to the name indicates that 
 /usr is itself a directory.) /usr has three children, mark, alex, and bill, which 
 are themselves directories. Thus, /usr contains three directories and no regular 
 files. The filename /usr/mark/book/ch1.r is obtained by following the leftmost 
 child three times. Each / after the first indicates an edge; the result is the 
 full pathname. This hierarchical file system is very popular, because it allows 
 users to organize their data logically. Furthermore, two files in different 
 directories can share the same name, because they must have different paths from 
 the root and thus have different pathnames. A directory in the 
 UNIX 
 file system 
 is just a file with a list of all its children, so the directories",NA
4.2. Binary Trees ,"mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
4.2.1. Implementation ,"Because a binary tree has at most two children, we can keep direct pointers to 
 them. The declaration of tree nodes is similar in structure to that for doubly 
 linked lists, in that a node is a structure consisting of the key information 
 plus two pointers (left and right) to other nodes (see 
  
 typedef struct tree_node *tree_ptr; 
  
 struct tree_node 
  
 { 
  
 element_type element;",NA
4.2.2. Expression Trees ,"Figure 4.14 shows an example of an expression tree. The leaves of an expression 
 tree are operands, such as constants or variable names, and the other nodes contain 
 operators. This particular tree happens to be binary, because all of the operations 
 are binary, and although this is the simplest case, it is possible for nodes to 
 have more than two children. It is also possible for a node to have only one child, 
 as is the case with the unary minus operator. We can evaluate an expression tree, 
 T, by applying the operator at the root to the values obtained by recursively 
 evaluating the left and right subtrees. In our example, the left subtree evaluates 
 to a + (b * c) and the right subtree evaluates to ((d *e) + f ) *g. The entire tree 
 therefore represents (a + (b*c)) + (((d * e) + f)* g). 
  
 We can produce an (overly parenthesized) infix expression by recursively 
 producing a parenthesized left expression, then printing out the operator at the 
 root, and finally recursively producing a parenthesized right expression. This 
 general strattegy ( left, node, right ) is known as an inorder traversal; it is 
 easy to remember because of the type of expression it produces. 
  
 An alternate traversal strategy is to recursively print out the left subtree, the 
 right subtree, and then the operator. If we apply this strategy to our tree above, 
 the output is a b c * + d e * f + g * +, which is easily seen to be the postfix 
 representation of Section 3.3.3. This traversal strategy is generally known as a 
 postorder traversal. We have seen this traversal strategy earlier in Section 4.1.",NA
Constructing an Expression Tree ,"We now give an algorithm to convert a postfix expression into an expression tree. 
  
 Since we already have an algorithm to convert infix to postfix, we can generate 
 expression trees from the two common types of input. The method we describe 
 strongly resembles the postfix evaluation algorithm of Section 3.2.3. We read our 
 expression one symbol at a time. If the symbol is an operand, we create a one-node 
 tree and push a pointer to it onto a stack. If the symbol is an operator, we pop 
 pointers to two trees T
 1
  and T
 2
  from the stack (T
 1
  is popped first) and form a new 
 tree whose root is the operator and whose left and right children point to T
 2
  and 
 T
 1
  respectively. A pointer to this new tree is then pushed onto the stack. 
  
 As an example, suppose the input is 
  
 a b + c d e + * * 
  
 The first two symbols are operands, so we create one-node trees and push pointers 
 to them onto a stack.* 
  
 *For convenience, we will have the stack grow from left to right in the diagrams. 
  
  
 Next, a '+' is read, so two pointers to trees are popped, a new tree is formed,",NA
4.3. The Search Tree ADT-Binary Search ,NA,NA
Trees ,"An important application of binary trees is their use in searching. Let us assume 
 that each node in the tree is assigned a key value. In our examples, we will assume 
 for simplicity that these are integers, although arbitrarily complex keys are 
 allowed. We will also assume that all the keys are distinct, and deal with 
 duplicates later. 
  
 The property that makes a binary tree into a binary search tree is that for every 
 node, X, in the tree, the values of all the keys in the left subtree are smaller 
 than the key value in X, and the values of all the keys in the right subtree are 
 larger than the key value in X. Notice that this implies that all the elements in 
 the tree can be ordered in some consistent manner. In Figure 4.15, the tree on the 
 left is a binary search tree, but the tree on the right is not. The tree on the 
 right has a node with key 7 in the left subtree of a node with key 6 (which",NA
4.3.1. Make_null ,"This operation is mainly for initialization. Some programmers prefer to 
 initialize the first element as a one-node tree, but our implementation follows 
 the recursive definition of trees more closely. It is also a simple routine, as 
 evidenced by Figure 4.17.",NA
4.3.2. Find ,This operation generally requires returning a pointer to the node in tree T that,NA
4.3.3. Find_min and find_max ,NA,NA
4.3.4. Insert ,"The insertion routine is conceptually simple. To insert x into tree T, proceed 
 down the tree as you would with a find. If x is found, do nothing (or ""update"" 
 something). Otherwise, insert x at the last spot on the path traversed. Figure 
 4.21 shows what happens. To insert 5, we traverse the tree as though a find were 
 occurring. At the node with key 4, we need to go right, but there is no subtree, 
 so 5 is not in the tree, and this is the correct spot. 
  
 Duplicates can be handled by keeping an extra field in the node record indicating 
 the frequency of occurrence. This adds some extra space to the entire tree, but is 
 better than putting duplicates in the tree (which tends to make the tree very 
 deep). Of course this strategy does not work if the key is only part of a larger 
 record. If that is the case, then we can keep all of the records that have the same 
 key in an auxiliary data structure, such as a list or another search tree. 
  
  
 Figure 4.21 Binary search trees before and after inserting 5 
  
 Figure 4.22 shows the code for the insertion routine. Since T points to the root 
 of the tree, and the root changes on the first insertion, insert is written as a 
 function that returns a pointer to the root of the new tree. Lines 8 and 10 
 recursively insert and attach x into the appropriate subtree. 
  
 tree_ptr 
  
 insert( element_type x, SEARCH_TREE T ) 
  
 { 
  
 /*1*/       if( T == NULL ) 
  
 {  /* Create and return a one-node tree */ 
  
 /*2*/            T = (SEARCH_TREE) malloc ( sizeof (struct tree_node) ); 
  
 /*3*/            if( T == NULL )",NA
4.3.5. Delete ,"As is common with many data structures, the hardest operation is deletion. Once we 
 have found the node to be deleted, we need to consider several possibilities. If 
 the node is a leaf, it can be deleted immediately. If the node has one child, the 
 node can be deleted after its parent adjusts a pointer to bypass the node (we will 
 draw the pointer directions explicitly for clarity). See Figure 4.23. Notice that 
 the deleted node is now unreferenced and can be disposed of only if a pointer to it 
 has been saved. 
  
 The complicated case deals with a node with two children. The general strategy is 
 to replace the key of this node with the smallest key of the right subtree (which 
 is easily found) and recursively delete that node (which is now empty). Because the 
 smallest node in the right subtree cannot have a left child, the second delete is 
 an easy one. Figure 4.24 shows an initial tree and the result of a deletion. The 
 node to be deleted is the left child of the root; the key value is 2. It is 
 replaced with the smallest key in its right subtree (3), and then that node is 
 deleted as before.",NA
4.3.6. Average-Case Analysis ,"Intuitively, we expect that all of the operations of the previous section, except 
 make_null, should take O(log n) time, because in constant time we descend a level 
 in the tree, thus operating on a tree that is now roughly half as large. Indeed, 
 the running time of all the operations, except make_null, is O(d), where d is the 
 depth of the node containing the accessed key. 
  
 We prove in this section that the average depth over all nodes in a tree is O(log 
 n) on the assumption that all trees are equally likely. 
  
 The sum of the depths of all nodes in a tree is known as the internal path length. 
 We will now calculate the average internal path length of a binary search tree, 
 where the average is taken over all possible binary search trees. 
  
 Let D(n) be the internal path length for some tree T of n nodes. D(1) = 0. An n-
 node tree consists of an i-node left subtree and an (n - i - 1)-node right 
  
 subtree, plus a root at depth zero for 0 
  
  
  
  i < n. D(i) is the internal path 
  
 length of the left subtree with respect to its root. In the main tree, all these 
 nodes are one level deeper. The same holds for the right subtree. Thus, we get 
 the recurrence 
  
 D(n) = D(i) + D(n - i -1) + n -1 
  
 If all subtree sizes are equally likely, which is true for binary search trees 
 (since the subtree size depends only on the relative rank of the first element 
 inserted into the tree), but not binary trees, then the average value of both D
  
 (i) and D(n - i -1) is 
 . This yields 
  
  
 This recurrence will be encountered and solved in Chapter 7, obtaining an average 
 value of D(n) = O(n log n). Thus, the expected depth of any node is O(log n). As an 
 example, the randomly generated 500-node tree shown in Figure 4.26 has nodes at 
 expected depth 9.98. 
  
 It is tempting to say immediately that this result implies that the average running 
 time of all the operations discussed in the previous section is O(log n), but this 
 is not entirely true. The reason for this is that because of deletions, it is not 
 clear that all binary search trees are equally likely. In particular, the deletion 
 algorithm described above favors making the left subtrees deeper than the right, 
 because we are always replacing a deleted node with a node from the right subtree. 
 The exact effect of this strategy is still unknown, but it seems only to be a 
 theoretical novelty. It has been shown that if we alternate",NA
4.4. AVL Trees ,"An 
 AVL
  (Adelson-Velskii and Landis) tree is a binary search tree with a balance 
 condition. The balance condition must be easy to maintain, and it ensures that the 
 depth of the tree is O(log n). The simplest idea is to require that the left and 
 right subtrees have the same height. As Figure 4.28 shows, this idea does not force 
 the tree to be shallow. 
  
  
 Figure 4.28 A bad binary tree. Requiring balance at the root is not enough. 
  
 Another balance condition would insist that every node must have left and right 
 subtrees of the same height. If the height of an empty subtree is defined to be -1 
 (as is usual), then only perfectly balanced trees of 2
 k
  - 1 nodes would satisfy 
 this criterion. Thus, although this guarantees trees of small depth, the balance 
 condition is too rigid to be useful and needs to be relaxed.",NA
4.4.1. Single Rotation ,"The two trees in Figure 4.31 contain the same elements and are both binary search 
 trees. First of all, in both trees k
 1
  < k
 2
 . Second, all elements in the subtree X 
 are smaller than k
 1
  in both trees. Third, all elements in subtree Z are larger than 
 k
 2
 . Finally, all elements in subtree Y are in between k
 1
  and k
 2
 . The 
  
 conversion of one of the above trees to the other is known as a rotation. A 
 rotation involves only a few pointer changes (we shall see exactly how many later), 
 and changes the structure of the tree while preserving the search tree property. 
  
 The rotation does not have to be done at the root of a tree; it can be done at any 
 node in the tree, since that node is the root of some subtree. It can transform 
 either tree into the other. This gives a simple method to fix up an 
 AVL
  tree if an 
 insertion causes some node in an 
 AVL
  tree to lose the balance property: Do a 
 rotation at that node. The basic algorithm is to start at the node inserted and 
 travel up the tree, updating the balance information at every node on the path. If 
 we get to the root without having found any badly balanced nodes, we are done. 
 Otherwise, we do a rotation at the first bad node found, adjust its balance, and 
 are done (we do not have to continue going to the root). In many cases, this is 
 sufficient to rebalance the tree. For instance, in Figure 4.32, 
  
 after the insertion of the 
  in the original 
 AVL
  tree on the left, node 8 
 becomes unbalanced. Thus, we do a single rotation between 7 and 8, obtaining the 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
4.4.2. Double Rotation ,"The algorithm described in the preceding paragraphs has one problem. There is a 
 case where the rotation does not fix the tree. Continuing our example, suppose we 
 insert keys 8 through 15 in reverse order. Inserting 15 is easy, since it does not 
 destroy the balance property, but inserting 14 causes a height imbalance at node 7.",NA
4.5. Splay Trees ,"We now describe a relatively simple data structure, known as a splay tree, that 
 guarantees that any m consecutive tree operations take at most O(m log n) time.",NA
4.5.1. A Simple Idea (That Does Not Work) ,"One way of performing the restructuring described above is to perform single 
 rotations, bottom up. This means that we rotate every node on the access path with 
 its parent. As an example, consider what happens after an access (a find) on k
 1
  in 
 the following tree.",NA
4.5.2. Splaying ,"The splaying strategy is similar to the rotation idea above, except that we are a 
 little more selective about how rotations are performed. We will still rotate 
 bottom up along the access path. Let x be a (nonroot) node on the access path at 
 which we are rotating. If the parent of x is the root of the tree, we merely rotate 
 x and the root. This is the last rotation along the access path. 
  
 Otherwise, x has both a parent (p) and a grandparent (g), and there are two",NA
4.6. Tree Traversals (Revisited) ,"Because of the ordering information in a binary search tree, it is simple to list 
 all the keys in sorted order. The recursive procedure in Figure 4.60 does this. 
 Convince yourself that this procedure works. As we have seen before, this kind of 
 routine when applied to trees is known as an inorder traversal (which makes sense, 
 since it lists the keys in order). The general strategy of an inorder traversal is 
 to process the left subtree first, then perform processing at the current node, and 
 finally process the right subtree. The interesting part about this algorithm, aside 
 from its simplicity, is that the total running time is O (n). This is because there 
 is constant work being performed at every node in the tree. Each node is visited 
 once, and the work performed at each node is testing against NULL, setting up two 
 procedure calls, and doing a print_element. Since there is constant work per node 
 and n nodes, the running time is O(n). 
  
 void 
  
 print_tree( SEARCH_TREE T ) 
  
 { 
  
 if( T != NULL ) 
  
 {",NA
4.7. B-Trees ,NA,NA
Summary ,"We have seen uses of trees in operating systems, compiler design, and searching. 
 Expression trees are a small example of a more general structure known as a parse 
 tree, which is a central data structure in compiler design. Parse trees are not 
 binary, but are relatively simple extensions of expression trees (although the 
 algorithms to build them are not quite so simple). 
  
 Search trees are of great importance in algorithm design. They support almost all 
 the useful operations, and the logarithmic average cost is very small. 
  
 Nonrecursive implementations of search trees are somewhat faster, but the recursive 
 versions are sleeker, more elegant, and easier to understand and debug. 
  
 The problem with search trees is that their performance depends heavily on the 
 input being random. If this is not the case, the running time increases 
 significantly, to the point where search trees become expensive linked lists. 
  
 We saw several ways to deal with this problem. 
 AVL
  trees work by insisting that 
 all nodes' left and right subtrees differ in heights by at most one. This ensures 
 that the tree cannot get too deep. The operations that do not change the tree, as 
 insertion does, can all use the standard binary search tree code. Operations that 
 change the tree must restore the tree. This can be somewhat complicated, 
  
 especially in the case of deletion. We showed how to restore the tree after 
 insertions in O(log n) time. 
  
 We also examined the splay tree. Nodes in splay trees can get arbitrarily deep, 
 but after every access the tree is adjusted in a somewhat mysterious manner. The 
 net effect is that any sequence of m operations takes O(m log n) time, which is 
 the same as a balanced tree would take. 
  
 B-trees are balanced m-way (as opposed to 2-way or binary) trees, which are well 
 suited for disks; a special case is the 2-3 tree, which is another common method 
 of implementing balanced search trees. 
  
 In practice, the running time of all the balanced tree schemes is worse (by a 
 constant factor) than the simple binary search tree, but this is generally 
 acceptable in view of the protection being given against easily obtained worst-
 case input. 
  
 A final note: By inserting elements into a search tree and then performing an 
 inorder traversal, we obtain the elements in sorted order. This gives an O(n log 
 n) algorithm to sort, which is a worst-case bound if any sophisticated search 
 tree is used. We shall see better ways in Chapter 7, but none that have a lower 
 time bound.",NA
Exercises ,Questions 4.1 to 4.3 refer to the tree in Figure 4.63.,NA
References ,"More information on binary search trees, and in particular the mathematical 
 properties of trees can be found in the two books by Knuth [23] and [24]. 
  
 Several papers deal with the lack of balance caused by biased deletion algorithms 
 in binary search trees. Hibbard's paper [20] proposed the original deletion 
 algorithm and established that one deletion preserves the randomness of the trees. 
 A complete analysis has been performed only for trees with three [21] and four 
 nodes[5]. Eppinger's paper [15] provided early empirical evidence of nonrandomness, 
 and the papers by Culberson and Munro, [11], [12], provide some analytical evidence 
 (but not a complete proof for the general case of intermixed insertions and 
 deletions). 
  
 AVL trees were proposed by Adelson-Velskii and Landis [1]. Simulation results for 
 AVL
  trees, and variants in which the height imbalance is allowed to be at most k 
 for various values of k, are presented in [22]. A deletion algorithm for 
 AVL 
 trees 
 can be found in [24]. Analysis of the averaged depth of 
 AVL
  trees is incomplete, 
 but some results are contained in [25]. 
  
 [3] and [9] considered self-adjusting trees like the type in Section 4.5.1. Splay 
 trees are described in [29]. 
  
 B-trees first appeared in [6]. The implementation described in the original paper 
 allows data to be stored in internal nodes as well as leaves. The data structure we 
 have described is sometimes known as a B
 + 
 tree. A survey of the different types of 
 B-trees is presented in [10]. Empirical results of the various schemes is reported 
 in [18]. Analysis of 2-3 trees and B-trees can be found in [4], [14], and [33].",NA
CHAPTER 5: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents",NA
HASHING ,"In Chapter 4, we discussed the search tree 
 ADT
 , which allowed various operations 
 on a set of elements. In this chapter, we discuss the hash table 
 ADT
 , which 
 supports only a subset of the operations allowed by binary search trees. 
  
 The implementation of hash tables is frequently called hashing. Hashing is a 
 technique used for performing insertions, deletions and finds in constant average 
 time. Tree operations that require any ordering information among the elements are 
 not supported efficiently. Thus, operations such as find_min, find_max, and the 
 printing of the entire table in sorted order in linear time are not 
  
 supported. 
  
 The central data structure in this chapter is the hash table. We will 
  
  
  See several methods of implementing the hash table. 
  
  
  Compare these methods analytically. 
  
  
  
  Show numerous applications of hashing. 
  
  
  Compare hash tables with binary search trees.",NA
5.1. General Idea ,"The ideal hash table data structure is merely an array of some fixed size, 
 containing the keys. Typically, a key is a string with an associated value (for 
 instance, salary information). We will refer to the table size as H_SIZE, with the 
 understanding that this is part of a hash data structure and not merely some 
 variable floating around globally. The common convention is to have the table run 
 from 0 to H_SIZE-1; we will see why shortly. 
  
 Each key is mapped into some number in the range 0 to H_SIZE - 1 and placed in the 
 appropriate cell. The mapping is called a hash function, which ideally should be 
 simple to compute and should ensure that any two distinct keys get different cells. 
 Since there are a finite number of cells and a virtually inexhaustible supply of 
 keys, this is clearly impossible, and thus we seek a hash function that distributes 
 the keys evenly among the cells. Figure 5.1 is typical of a perfect situation. In 
 this example, john hashes to 3, phil hashes to 4, dave hashes to 6, and mary hashes 
 to 7.",NA
5.2. Hash Function ,"If the input keys are integers, then simply returning key mod H_SIZE is generally a 
 reasonable strategy, unless key happens to have some undesirable properties. In 
 this case, the choice of hash function needs to be carefully considered. For 
 instance, if the table size is 10 and the keys all end in zero, then the standard 
 hash function is obviously a bad choice. For reasons we shall see later, and to 
 avoid situations like the one above, it is usually a good idea to ensure that the 
 table size is prime. When the input keys are random integers, then this function is 
 not only very simple to compute but also distributes the keys evenly. 
  
 Usually, the keys are strings; in this case, the hash function needs to be chosen 
 carefully. 
  
 One option is to add up the 
 ASCII
  values of the characters in the string. In 
 Figure 5.2 we declare the type INDEX, which is returned by the hash function. The 
 routine in Figure 5.3 implements this strategy and uses the typical C method of 
 stepping through a string. 
  
 The hash function depicted in Figure 5.3 is simple to implement and computes an 
 answer quickly. However, if the table size is large, the function does not 
 distribute the keys well. For instance, suppose that H_SIZE = 10,007 (10,007 is a 
 prime number). Suppose all the keys are eight or fewer characters long. Since a 
 char has an integer value that is always at most 127, the hash function can only 
 assume values between 0 and 1016, which is 127 * 8. This is clearly not an 
 equitable distribution! 
  
 typedef unsigned int INDEX; 
  
 Figure 5.2 Type returned by hash function",NA
5.3. Open Hashing (Separate Chaining) ,"The first strategy, commonly known as either open hashing, or separate chaining, is 
 to keep a list of all elements that hash to the same value. For convenience, our 
 lists have headers. This makes the list implementation the same as in Chapter 3. If 
 space is tight, it might be preferable to avoid their use. We assume for this 
 section that the keys are the first 10 perfect squares and that the hashing 
 function is simply hash(x) = x mod 10. (The table size is not prime, but is used 
 here for simplicity.) Figure 5.6 should make this clear. 
  
 To perform a find, we use the hash function to determine which list to traverse. We 
 then traverse this list in the normal manner, returning the position where the",NA
5.4. Closed Hashing (Open Addressing),"Open hashing has the disadvantage of requiring pointers. This tends to slow the algorithm down a 
 bit because of the time required to allocate new cells, and also essentially requires the 
 implementation of a second data structure. Closed hashing, also known as open addressing, is an 
 alternative to resolving collisions with linked lists. In a closed hashing system, if a collision 
 occurs, alternate cells are tried until an empty cell is found. More formally, cells h0(x), h1
  
 (x), h2(x), . . . are tried in succession where hi(x) = (hash(x) + 
  
  
 (i))mod H_SIZE, with 
  
  
  
 (0) = 0. The function, 
  
  
  
 , is the collision resolution strategy. Because all the data goes 
  
 inside the table, a bigger table is needed for closed hashing than for open hashing. Generally, 
  
 the load factor should be below 
  
  
  
  = 0.5 for closed hashing. We now look at three common 
  
 collision resolution strategies. 
  
 5.4.1. Linear Probing 
  
 5.4.2. Quadratic Probing 
  
 5.4.3. Double Hashing",NA
5.4.1. Linear Probing,"In linear probing, 
  
  
  
  is a linear function of i, typically 
  
  
  
 (i) = i. This amounts to trying 
  
 cells sequentially (with wraparound) in search of an empty cell. Figure 5.11 shows the result of 
 inserting keys {89, 18, 49, 58, 69} into a closed table using the same hash function as before 
  
 and the collision resolution strategy, 
  
  
 (i) = i. 
  
 The first collision occurs when 49 is inserted; it is put in the next available spot, namely spot",NA
5.4.2. Quadratic Probing,"Quadratic probing is a collision resolution method that eliminates the primary clustering problem 
 of linear probing. Quadratic probing is what you would expect-the collision function is 
  
 quadratic. The popular choice is 
  
  
 (i) = i2. Figure 5.13 shows the resulting closed table with 
  
 this collision function on the same input used in the linear probing example. 
  
 When 49 collides with 89, the next position attempted is one cell away. This cell is empty, so 49 
 is placed there. Next 58 collides at position 8. Then the cell one away is tried but another 
 collision occurs. A vacant cell is found at the next cell tried, which is 22 = 4 away. 58 is thus 
 placed in cell 2. The same thing happens for 69. 
  
 For linear probing it is a bad idea to let the hash table get nearly full, because performance 
 degrades. For quadratic probing, the situation is even more drastic: There is no guarantee of 
 finding an empty cell once the table gets more than half full, or even before the table gets half 
 full if the table size is not prime. This is because at most half of the table can be used as 
 alternate locations to resolve collisions. 
  
 Indeed, we prove now that if the table is half empty and the table size is prime, then we are 
 always guaranteed to be able to insert a new element. 
  
 THEOREM 5.1. 
  
 If quadratic probing is used, and the table size is prime, then a new element can always be 
 inserted if the table is at least half empty. 
  
 PROOF: 
  
 Let the table size, H_SIZE, be an (odd) prime greater than 3. We show that the first 
  
  
  
 H_SIZ
 E/2
  
  
  
  
  alternate locations are all distinct. Two of these locations are h(x) + i2(mod 
  
 H_SIZE) and h(x) + j2(mod H_SIZE), where 0 < i, j 
  
  
  
  
 H_SIZE
 /2
  
  
 . Suppose, for the sake 
  
 of contradiction, that these locations are the same, but i 
  
  
  j. Then 
  
 h(x) + i2 = h(x) + j2     (mod H_SIZE)
  
 i2 = j2             (mod H_SIZE)
  
 i2 - j2 = 0            (mod H_SIZE)
  
 (i - j)(i + j) = 0            (mod H_SIZE)",NA
5.4.3. Double Hashing,"The last collision resolution method we will examine is double hashing. For double hashing, one 
  
 popular choice is f(i) = i 
  
  
  
  
  h2(x). This formula says that we apply a second hash function to 
  
 x and probe at a distance h2(x), 2h2(x), . . ., and so on. A poor choice of h2(x) would be 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
5.5. Rehashing,"If the table gets too full, the running time for the operations will start taking too long and 
 inserts might fail for closed hashing with quadratic resolution. This can happen if there are too 
 many deletions intermixed with insertions. A solution, then, is to build another table that is 
 about twice as big (with associated new hash function) and scan down the entire original hash 
 table, computing the new hash value for each (non-deleted) element and inserting it in the new 
 table. 
  
 As an example, suppose the elements 13, 15, 24, and 6 are inserted into a closed hash table of 
 size 7. The hash function is h(x) = x mod 7. Suppose linear probing is used to resolve 
 collisions. The resulting hash table appears in 
  
 Figure 5.19. 
  
 If 23 is inserted into the table, the resulting table in Figure 5.20 will be over 70 percent full. 
 Because the table is so full, a new table is created. The size of this table is 17, because this 
 is the first prime which is twice as large as the old table size. The new hash function is then 
 h(x) = x mod 17. The old table is scanned, and elements 6, 15, 23, 24, and 13 are inserted into 
 the new table. The resulting table appears in Figure 5.21. 
  
 This entire operation is called rehashing. This is obviously a very expensive operation -- the 
 running time is O(n), since there are n elements to rehash and the table size is roughly 2n, but 
 it is actually not all that bad, because it happens very infrequently. In particular, there must 
 have been n/2 inserts prior to the last rehash, so it essentially adds a constant cost to each 
 insertion.* If this data structure is part of the program, the effect is not noticeable. On the 
 other hand, if the hashing is performed as part of an interactive system, then the unfortunate 
 user whose insertion caused a rehash could see a slowdown. 
  
 *This is why the new table is made twice as large as the old table.",NA
5.6. Extendible Hashing,"Our last topic in this chapter deals with the case where the amount of data is too large to fit 
 in main memory. As we saw in Chapter 4, the main consideration then is the number of disk 
 accesses required to retrieve data. 
  
 As before, we assume that at any point we have n records to store; the value of n changes over 
 time. Furthermore, at most m records fit in one disk block. We will use m = 4 in this section. 
  
 If either open hashing or closed hashing is used, the major problem is that collisions could 
 cause several blocks to be examined during a find, even for a well-distributed hash table. 
  
 Furthermore, when the table gets too full, an extremely expensive rehashing step must be 
 performed, which requires O(n) disk accesses. 
  
 A clever alternative, known as extendible hashing, allows a find to be performed in two disk 
 accesses. Insertions also require few disk accesses. 
  
 We recall from Chapter 4 that a B-tree has depth O(logm/2 n). As m increases, the depth of a B-
  
 tree decreases. We could in theory choose m to be so large that the depth of the B-tree would be 
 1. Then any find after the first would take one disk access, since, presumably, the root node 
 could be stored in main memory. The problem with this strategy is that the branching factor is so 
 high that it would take considerable processing to determine which leaf the data was in. If the 
 time to perform this step could be reduced, then we would have a practical scheme. This is exactly 
 the strategy used by extendible hashing. 
  
 Let us suppose, for the moment, that our data consists of several six-bit integers. Figure 5.23 
 shows an extendible hashing scheme for this data. The root of the ""tree"" contains four pointers 
 determined by the leading two bits of the data. Each leaf has up to m = 4 elements. It happens 
 that in each leaf the first two bits are identical; this is indicated by the number in 
  
 parentheses. To be more formal, D will represent the number of bits used by the root, which is 
 sometimes known as the directory. The number of entries in the directory is thus 2D. dl is the 
 number of leading bits that all the elements of some leaf l have in common. dl will depend on the",NA
Summary ,"Hash tables can be used to implement the insert and find operations in constant average time. It 
 is especially important to pay attention to details such as load factor when using hash tables, 
 since otherwise the time bounds are not valid. It is also important to choose the hash function 
 carefully when the key is not a short string or integer. 
  
 For open hashing, the load factor should be close to 1, although performance does not 
  
 significantly degrade unless the load factor becomes very large. For closed hashing, the load 
 factor should not exceed 0.5, unless this is completely unavoidable. If linear probing is used, 
 performance degenerates rapidly as the load factor approaches 1. Rehashing can be implemented to 
 allow the table to grow (and shrink), thus maintaining a reasonable load factor. This is important 
 if space is tight and it is not possible just to declare a huge hash table. 
  
 Binary search trees can also be used to implement insert and find operations. Although the 
 resulting average time bounds are O(log n), binary search trees also support routines that require 
 order and are thus more powerful. Using a hash table, it is not possible to find the minimum 
 element. It is not possible to search efficiently for a string unless the exact string is known. A 
 binary search tree could quickly find all items in a certain range; this is not supported by hash 
 tables. Furthermore, the O(log n) bound is not necessarily that much more than O (1), especially 
 since no multiplications or divisions are required by search trees. 
  
 On the other hand, the worst case for hashing generally results from an implementation error, 
 whereas sorted input can make binary trees perform poorly. Balanced search trees are quite 
 expensive to implement, so if no ordering information is required and there is any suspicion that 
 the input might be sorted, then hashing is the data structure of choice. 
  
 Hashing applications are abundant. Compilers use hash tables to keep track of declared variables 
 in source code. The data structure is known as a symbol table. Hash tables are the ideal 
 application for this problem because only inserts and finds are performed. Identifiers are 
 typically short, so the hash function can be computed quickly. 
  
 A hash table is useful for any graph theory problem where the nodes have real names instead of 
 numbers. Here, as the input is read, vertices are assigned integers from 1 onwards by order of 
 appearance. Again, the input is likely to have large groups of alphabetized entries. For example, 
 the vertices could be computers. Then if one particular installation lists its computers as ibm1, 
 ibm2, ibm3, . . . , there could be a dramatic effect on efficiency if a search tree is used. 
  
 A third common use of hash tables is in programs that play games. As the program searches through 
 different lines of play, it keeps track of positions it has seen by computing a hash function 
 based on the position (and storing its move for that position). If the same position reoccurs, 
 usually by a simple transposition of moves, the program can avoid expensive recomputation. This 
 general feature of all game-playing programs is known as the transposition table.",NA
Exercises,"5.1 Given input {4371, 1323, 6173, 4199, 4344, 9679, 1989} and a hash function h(x) = x(mod 10), 
 show the resulting 
  
 a. open hash table 
  
 b. closed hash table using linear probing 
  
 c. closed hash table using quadratic probing 
  
 d. closed hash table with second hash function h2(x) = 7 - (x mod 7) 
  
 5.2 Show the result of rehashing the hash tables in Exercise 5.1. 
  
 5.3 Write a program to compute the number of collisions required in a long random sequence of 
 insertions using linear probing, quadratic probing, and double hashing. 
  
 5.4 A large number of deletions in an open hash table can cause the table to be fairly empty, 
 which wastes space. In this case, we can rehash to a table half as large. Assume that we rehash 
 to a larger table when there are twice as many elements as the table size. How empty should an 
 open table be before we rehash to a smaller table? 
  
 5.5 An alternative collision resolution strategy is to define a sequence, f(i) = ri, where r0 = 0 
 and r1, r2, . . . , rn is a random permutation of the first n integers (each integer appears",NA
References,"Despite the apparent simplicity of hashing, much of the analysis is quite difficult and there are 
 still many unresolved questions. There are also many interesting theoretical issues, which 
 generally attempt to make it unlikely that the worst-case possibilities of hashing arise. 
  
 An early paper on hashing is 
  
 [17]. A wealth of information on the subject, including an analysis 
 of closed hashing with linear probing can be found in [11]. An excellent survey on the subject is 
 [14]; [15] contains suggestions, and pitfalls, for choosing hash functions. Precise analytic and 
 simulation results for all of the methods described in this chapter can be found in [8]. 
  
 An analysis of double hashing can be found in [9] and [13]. Yet another collision resolution 
 scheme is coalesced hashing, as described in [18]. Yao [20] has shown that uniform hashing, in 
 which no clustering exists, is optimal with respect to cost of a successful search. 
  
 If the input keys are known in advance, then perfect hash functions, which do not allow 
 collisions, exist [2], [7]. Some more complicated hashing schemes, for which the worst case 
 depends not on the particular input but on random numbers chosen by the algorithm, appear in [3] 
 and [4]. 
  
 Extendible hashing appears in [5], with analysis in [6] and [19]. 
  
 One method of implementing Exercise 5.5 is described in [16]. Exercise 5.11 (a-d) is from [10]. 
 Part (e) is from [12], and part (f) is from [1]. 
  
 1. R. S. Boyer and J. S. Moore, ""A Fast String Searching Algorithm,"" Communications of the ACM 20 
 (1977), 762-772. 
  
 2. J. L. Carter and M. N. Wegman, ""Universal Classes of Hash Functions,"" Journal of Computer and 
 System Sciences 18 (1979), 143-154.",NA
CHAPTER 6: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents",NA
PRIORITY QUEUES (HEAPS) ,"Although jobs sent to a line printer are generally placed on a queue, this might 
 not always be the best thing to do. For instance, one job might be particularly 
 important, so that it might be desirable to allow that job to be run as soon as 
 the printer is available. Conversely, if, when the printer becomes available, 
 there are several one-page jobs and one hundred-page job, it might be reasonable 
 to make the long job go last, even if it is not the last job submitted. 
  
 (Unfortunately, most systems do not do this, which can be particularly annoying 
 at times.) 
  
 Similarly, in a multiuser environment, the operating system scheduler must decide 
 which of several processes to run. Generally a process is only allowed to run for a 
 fixed period of time. One algorithm uses a queue. Jobs are initially placed at the 
 end of the queue. The scheduler will repeatedly take the first job on the queue, 
 run it until either it finishes or its time limit is up, and place it at the end of 
 the queue if it does not finish. This strategy is generally not appropriate, 
 because very short jobs will seem to take a long time because of the wait involved 
 to run. Generally, it is important that short jobs finish as fast as possible, so 
 these jobs should have preference over jobs that have already been running. 
 Furthermore, some jobs that are not short are still very important and should also 
 have preference. 
  
 This particular application seems to require a special kind of queue, known as a 
 priority queue. In this chapter, we will discuss 
  
  
  
  Efficient implementation of the priority queue 
 ADT
 . 
  
  
  Uses of priority queues. 
  
  
  Advanced implementations of priority queues. 
  
 The data structures we will see are among the most elegant in computer science.",NA
6.1. Model ,"A priority queue is a data structure that allows at least the following two 
 operations: insert, which does the obvious thing, and delete_min, which finds, 
 returns and removes the minimum element in the heap. The insert operation is the 
 equivalent of enqueue, and delete_min is the priority queue equivalent of the 
 queue's dequeue operation. The delete_min function also alters its input. Current 
 thinking in the software engineering community suggests that this is no longer a 
 good idea. However, we will continue to use this function because of historical 
 reasons--many programmers expect delete_min to operate this way.",NA
6.2. Simple Implementations ,"There are several obvious ways to implement a priority queue. We could use a simple 
 linked list, performing insertions at the front in O(1) and traversing the list, 
 which requires O(n) time, to delete the minimum. Alternatively, we could insist 
 that the list be always kept sorted; this makes insertions expensive (O (n)) and 
 delete_mins cheap (O(1)). The former is probably the better idea of the two, based 
 on the fact that there are never more delete_mins than insertions. 
  
 Another way of implementing priority queues would be to use a binary search tree. 
 This gives an O(log n) average running time for both operations. This is true in 
 spite of the fact that although the insertions are random, the deletions are not. 
 Recall that the only element we ever delete is the minimum. Repeatedly removing a 
 node that is in the left subtree would seem to hurt the balance of the tree by 
 making the right subtree heavy. However, the right subtree is random. In the worst 
 case, where the delete_mins have depleted the left subtree, the right subtree would 
 have at most twice as many elements as it should. This adds only a small constant 
 to its expected depth. Notice that the bound can be made into a worst-case bound by 
 using a balanced tree; this protects one against bad 
  
 insertion sequences. 
  
 Using a search tree could be overkill because it supports a host of operations that 
 are not required. The basic data structure we will use will not require pointers 
 and will support both operations in O(log n) worst-case time. Insertion will 
 actually take constant time on average, and our implementation will allow building 
 a heap of n items in linear time, if no deletions intervene. We will then discuss 
 how to implement heaps to support efficient merging. This additional operation 
 seems to complicate matters a bit and apparently requires the use of pointers.",NA
6.3. Binary Heap ,"The implementation we will use is known as a binary heap. Its use is so common for 
 priority queue implementations that when the word heap is used without a qualifier, 
 it is generally assumed to be referring to this implementation of the data 
 structure. In this section, we will refer to binary heaps as merely heaps. Like 
 binary search trees, heaps have two properties, namely, a structure property and a 
 heap order property. As with 
 AVL
  trees, an operation on a heap can destroy one of 
 the properties, so a heap operation must not terminate until all heap properties 
 are in order. This turns out to be simple to do. 
  
 6.3.1. Structure Property 
  
 6.3.2. Heap Order Property 
  
 6.3.3. Basic Heap Operations 
  
 6.3.4. Other Heap Operations",NA
6.3.1. Structure Property ,"A heap is a binary tree that is completely filled, with the possible exception of 
 the bottom level, which is filled from left to right. Such a tree is known as a 
 complete binary tree. Figure 6.2 shows an example. 
  
 It is easy to show that a complete binary tree of height h has between 2
 h
  and 
  
 2
 h+1
  - 1 nodes. This implies that the height of a complete binary tree is 
  
  
 log 
  
 n
  
  
 , which is clearly O(log n). 
  
 An important observation is that because a complete binary tree is so regular, it 
 can be represented in an array and no pointers are necessary. The array in Figure 
 6.3 corresponds to the heap in Figure 6.2.",NA
6.3.2. Heap Order Property ,"The property that allows operations to be performed quickly is the heap order 
 property. Since we want to be able to find the minimum quickly, it makes sense 
 that the smallest element should be at the root. If we consider that any subtree 
 should also be a heap, then any node should be smaller than all of its 
  
 descendants. 
  
 Applying this logic, we arrive at the heap order property. In a heap, for every 
 node X, the key in the parent of X is smaller than (or equal to) the key in X, 
 with the obvious exception of the root (which has no parent).* In Figure 6.5 the 
 tree on the left is a heap, but the tree on the right is not (the dashed line 
 shows the violation of heap order). As usual, we will assume that the keys are 
 integers, although they could be arbitrarily complex. 
  
 *Analogously, we can declare a (max) heap, which enables us to efficiently find 
 and remove the maximum element, by changing the heap order property. Thus, a 
 priority queue can be used to find either a minimum or a maximum, but this needs 
 to be decided ahead of time. 
  
 By the heap order property, the minimum element can always be found at the root. 
 Thus, we get the extra operation, find_min, in constant time.",NA
6.3.3. Basic Heap Operations ,"It is easy (both conceptually and practically) to perform the two required 
 operations. All the work involves ensuring that the heap order property is 
 maintained. 
  
 Insert 
  
 Delete_min",NA
Insert ,"To insert an element x into the heap, we create a hole in the next available 
 location, since otherwise the tree will not be complete. If x can be placed in 
 the hole without violating heap order, then we do so and are done. Otherwise we 
 slide the element that is in the hole's parent node into the hole, thus bubbling 
 the hole up toward the root. We continue this process until x can be placed in 
 the hole. Figure 6.6 shows that to insert 14, we create a hole in the next 
 available heap location. Inserting 14 in the hole would violate the heap order 
 property, so 31 is slid down into the hole. This strategy is continued in Figure 
 6.7 until the correct location for 14 is found. 
  
 This general strategy is known as a percolate up; the new element is percolated 
 up the heap until the correct location is found. Insertion is easily implemented 
 with the code shown in Figure 6.8. 
  
 We could have implemented the percolation in the insert routine by performing 
 repeated swaps until the correct order was established, but a swap requires three 
 assignment statements. If an element is percolated up d levels, the number of 
 assignments performed by the swaps would be 3d. Our method uses d + 1 
  
 assignments.",NA
Delete_min ,"Delete_mins are handled in a similar manner as insertions. Finding the minimum is 
 easy; the hard part is removing it. When the minimum is removed, a hole is created 
 at the root. Since the heap now becomes one smaller, it follows that the last 
 element x in the heap must move somewhere in the heap. If x can be placed in the 
 hole, then we are done. This is unlikely, so we slide the smaller of the hole's 
 children into the hole, thus pushing the hole down one level. We repeat this step 
 until x can be placed in the hole. Thus, our action is to place x in its correct 
 spot along a path from the root containing minimum children. 
  
 In Figure 6.9 the left figure shows a heap prior to the delete_min. After 13 is 
 removed, we must now try to place 31 in the heap. 31 cannot be placed in the hole, 
 because this would violate heap order. Thus, we place the smaller child (14) in the 
 hole, sliding the hole down one level (see Fig. 6.10). We repeat this again, 
 placing 19 into the hole and creating a new hole one level deeper. We then place 26 
 in the hole and create a new hole on the bottom level. Finally, we are able to 
 place 31 in the hole (Fig. 6.11). This general strategy is known as a percolate 
 down. We use the same technique as in the insert routine to avoid the use of swaps 
 in this routine.",NA
6.3.4. Other Heap Operations ,"Notice that although finding the minimum can be performed in constant time, a heap 
 designed to find the minimum element (also known as a (min) heap) is of no help 
 whatsoever in finding the maximum element. In fact, a heap has very little ordering 
 information, so there is no way to find any particular key without a linear scan 
 through the entire heap. To see this, consider the large heap structure (the 
 elements are not shown) in Figure 6.13, where we see that the only information 
 known about the maximum element is that it is at one of the leaves. 
  
 Half the elements, though, are contained in leaves, so this is practically useless 
 information. For this reason, if it is important to know where elements are, some 
 other data structure, such as a hash table, must be used in addition to the heap. 
 (Recall that the model does not allow looking inside the heap.) 
  
 If we assume that the position of every element is known by some other method, then 
 several other operations become cheap. The three operations below all run in 
 logarithmic worst-case time. 
  
 Decrease_key 
  
 Increase_key 
  
 Delete 
  
 Build_heap",NA
Decrease_key ,"The decrease_key(x,
  
  
 , H) operation lowers the value of the key at position x 
  
 by a positive amount 
  
  
  
 . Since this might violate the heap order, it must be 
  
 fixed by a percolate up. This operation could be useful to system administrators: 
 they can make their programs run with highest priority",NA
Increase_key ,"The increase_key(x, 
  
  
 , H) operation increases the value of the key at position 
  
 x by a positive amount 
  
  
  
 . This is done with a percolate down. Many schedulers 
  
 automatically drop the priority of a process that is consuming excessive 
 CPU 
 time.",NA
Delete ,"The delete(x, H) operation removes the node at position x from the heap. This is 
  
 done by first performing decrease_key(x, 
  
  
 , H) and then performing delete_min 
  
 (H). When a process is terminated by a user (instead of finishing normally), it 
 must be removed from the priority queue.",NA
Build_heap ,"The build_heap(H) operation takes as input n keys and places them into an empty 
 heap. Obviously, this can be done with n successive inserts. Since each insert will 
 take O(1) average and O(log n) worst-case time, the total running time of this 
 algorithm would be O(n) average but O(n log n) worst-case. Since this is a special 
 instruction and there are no other operations intervening, and we already know that 
 the instruction can be performed in linear average time, it is 
  
 reasonable to expect that with reasonable care a linear time bound can be 
 guaranteed. 
  
 The general algorithm is to place the n keys into the tree in any order, 
 maintaining the structure property. Then, if percolate_down(i) percolates down 
 from node i, perform the algorithm in Figure 6.14 to create a heap-ordered tree. 
  
 The first tree in Figure 6.15 is the unordered tree. The seven remaining trees in 
 Figures 6.15 through 6.18 show the result of each of the seven percolate downs. 
 Each dashed line corresponds to two comparisons: one to find the smaller child",NA
6.4. Applications of Priority Queues ,"We have already mentioned how priority queues are used in operating systems 
 design. In Chapter 9, we will see how priority queues are used to implement 
 several graph algorithms efficiently. Here we will show how to use priority 
 queues to obtain solutions to two problems. 
  
 6.4.1. The Selection Problem 
  
 6.4.2. Event Simulation",NA
6.4.1. The Selection Problem ,"The first problem we will examine is the selection problem from Chapter 1. Recall 
 that the input is a list of n elements, which can be totally ordered, and an 
 integer k. The selection problem is to find the kth largest element. 
  
 Two algorithms were given in Chapter 1, but neither is very efficient. The first 
 algorithm, which we shall call Algorithm 1A, is to read the elements into an array 
 and sort them, returning the appropriate element. Assuming a simple sorting 
 algorithm, the running time is O(n
 2
 ). The alternative algorithm, 1B, is to read k 
 elements into an array and sort them. The smallest of these is in the kth position. 
 We process the remaining elements one by one. As an element arrives, it is compared 
 with kth element in the array. If it is larger, then the kth element is removed, 
 and the new element is placed in the correct place among the 
  
 remaining k - 1 elements. When the algorithm ends, the element in the kth 
  
 position is the answer. The running time is O(n 
  
  
  k) (why?). If k = 
  
  
  
 n
 /2
  
  
 , then both algorithms are O(n
 2
 ). Notice that for any k, we can solve the 
  
 symmetric problem of finding the (n - k + 1)th smallest element, so k = 
  
  
 n
 /2
  
  
  
  
  is really the hardest case for these algorithms. This also happens to be the 
 most interesting case, since this value of k is known as the median.",NA
Algorithm 6A ,"For simplicity, we assume that we are interested in finding the kth smallest 
 element. The algorithm is simple. We read the n elements into an array. We then 
 apply the build_heap algorithm to this array. Finally, we'll perform k delete_min 
 operations. The last element extracted from the heap is our answer. It should be 
 clear that by changing the heap order property, we could solve the original 
 problem of finding the kth largest element. 
  
 The correctness of the algorithm should be clear. The worst-case timing is O(n) to 
 construct the heap, if build_heap is used, and O(log n) for each delete_min. Since 
 there are k delete_mins, we obtain a total running time of O(n + k log n). If k = 
 O(n/log n), then the running time is dominated by the build_heap operation and is 
 O(n). For larger values of k, the running time is O(k log n). If k = 
  
  
  
 n
 /2
  
  
 , then the running time is 
  
  
 (n log n). 
  
 Notice that if we run this program for k = n and record the values as they leave 
 the heap, we will have essentially sorted the input file in O(n log n) time. In 
 Chapter 7, we will refine this idea to obtain a fast sorting algorithm known as 
 heapsort.",NA
Algorithm 6B ,"For the second algorithm, we return to the original problem and find the kth 
 largest element. We use the idea from Algorithm 1B. At any point in time we will 
 maintain a set S of the k largest elements. After the first k elements are read, 
 when a new element is read, it is compared with the kth largest element, which we 
 denote by S
 k
 . Notice that S
 k
  is the smallest element in S. If the new element is 
 larger, then it replaces S
 k
  in S. S will then have a new smallest element, which 
 may or may not be the newly added element. At the end of the input, we find the 
 smallest element in S and return it as the answer. 
  
 This is essentially the same algorithm described in Chapter 1. Here, however, we 
 will use a heap to implement S. The first k elements are placed into the heap in 
 total time O(k) with a call to build_heap. The time to process each of the 
 remaining elements is O(1), to test if the element goes into S, plus O(log k), to 
 delete S
 k
  and insert the new element if this is necessary. Thus, the total time is 
 O(k + (n - k ) log k ) = O (n log k ) . This algorithm also gives a bound of 
  
  
  
 (n log n) for finding the median.",NA
6.4.2. Event Simulation ,"In Section 3.4.3, we described an important queuing problem. Recall that we have 
 a system, such as a bank, where customers arrive and wait on a line until one of 
 k tellers is available. Customer arrival is governed by a probability 
  
 distribution function, as is the service time (the amount of time to be served 
 once a teller is available). We are interested in statistics such as how long on 
 average a customer has to wait or how long the line might be. 
  
 With certain probability distributions and values of k, these answers can be 
 computed exactly. However, as k gets larger, the analysis becomes considerably more 
 difficult, so it is appealing to use a computer to simulate the operation of the 
 bank. In this way, the bank officers can determine how many tellers are needed to 
 ensure reasonably smooth service. 
  
 A simulation consists of processing events. The two events here are (a) a 
 customer arriving and (b) a customer departing, thus freeing up a teller. 
  
 We can use the probability functions to generate an input stream consisting of 
 ordered pairs of arrival time and service time for each customer, sorted by 
 arrival time. We do not need to use the exact time of day. Rather, we can use a 
 quantum unit, which we will refer to as a tick. 
  
 One way to do this simulation is to start a simulation clock at zero ticks. We 
 then advance the clock one tick at a time, checking to see if there is an event. 
 If there is, then we process the event(s) and compile statistics. When there are 
 no customers left in the input stream and all the tellers are free, then the 
 simulation is over. 
  
 The problem with this simulation strategy is that its running time does not 
 depend on the number of customers or events (there are two events per customer), 
 but instead depends on the number of ticks, which is not really part of the 
 input. To see why this is important, suppose we changed the clock units to 
 milliticks and multiplied all the times in the input by 1,000. The result would 
 be that the simulation would take 1,000 times longer! 
  
 The key to avoiding this problem is to advance the clock to the next event time at 
 each stage. This is conceptually easy to do. At any point, the next event that can 
 occur is either (a) the next customer in the input file arrives, or (b) one of the 
 customers at a teller leaves. Since all the times when the events will happen are 
 available, we just need to find the event that happens nearest in the future and 
 process that event. 
  
 If the event is a departure, processing includes gathering statistics for the 
 departing customer and checking the line (queue) to see whether there is another 
 customer waiting. If so, we add that customer, process whatever statistics are",NA
6.5. d-Heaps ,"Binary heaps are so simple that they are almost always used when priority queues 
 are needed. A simple generalization is a d-heap, which is exactly like a binary 
 heap except that all nodes have d children (thus, a binary heap is a 2-heap). 
 Figure 6.19 shows a 3-heap. 
  
 Notice that a d-heap is much more shallow than a binary heap, improving the running 
 time of inserts to O(log
 d
 n). However, the delete_min operation is more expensive, 
 because even though the tree is shallower, the minimum of d children must be found, 
 which takes d - 1 comparisons using a standard algorithm. This raises the time for 
 this operation to O(d log
 d
 n). If d is a constant, both running times are, of 
 course, O(log n). Furthermore, although an array can still be used, the 
 multiplications and divisions to find children and parents are now by d, which 
 seriously increases the running time, because we can no longer implement division 
 by a bit shift. d-heaps are interesting in theory, because there are many 
 algorithms where the number of insertions is much greater than the number of 
 delete_mins (and thus a theoretical speedup is possible). They are also of interest 
 when the priority queue is too large to fit entirely in main memory. 
  
 In this case, a d-heap can be advantageous in much the same way as B-trees. 
  
 The most glaring weakness of the heap implementation, aside from the inability to 
 perform finds is that combining two heaps into one is a hard operation. This extra 
 operation is known as a merge. There are quite a few ways of implementing heaps so 
 that the running time of a merge is O(log n). We will now discuss three data 
 structures, of various complexity, that support the merge operation 
  
 efficiently. We will defer any complicated analysis until Chapter 11.",NA
6.6. Leftist Heaps ,"It seems difficult to design a data structure that efficiently supports merging 
 (that is, processes a merge in o(n) time) and uses only an array, as in a binary 
 heap. The reason for this is that merging would seem to require copying one array 
  
 into another which would take 
  
  
 (n) time for equal-sized heaps. For this 
  
 reason, all the advanced data structures that support efficient merging require the 
 use of pointers. In practice, we can expect that this will make all the other 
 operations slower; pointer manipulation is generally more time-consuming than 
 multiplication and division by two. 
  
 Like a binary heap, a leftist heap has both a structural property and an ordering 
 property. Indeed, a leftist heap, like virtually all heaps used, has the same heap 
 order property we have already seen. Furthermore, a leftist heap is also a binary 
 tree. The only difference between a leftist heap and a binary heap is that leftist 
 heaps are not perfectly balanced, but actually attempt to be very unbalanced. 
  
 6.6.1. Leftist Heap Property 
  
 6.6.2. Leftist Heap Operations",NA
6.6.1. Leftist Heap Property ,"We define the null path length, npl(X) of any node X to be the length of the 
 shortest path from X to a node without two children. Thus, the npl of a node with 
 zero or one child is 0, while npl(NULL) = -1. In the tree in Figure 6.20, the null 
 path lengths are indicated inside the tree nodes. 
  
 Notice that the null path length of any node is 1 more than the minimum of the 
 null path lengths of its children. This applies to nodes with less than two 
 children because the null path length of is -1. 
  
 The leftist heap property is that for every node X in the heap, the null path 
 length of the left child is at least as large as that of the right child. This 
 property is satisfied by only one of the trees in Figure 6.20, namely, the tree 
 on the left. This property actually goes out of its way to ensure that the tree 
 is unbalanced, because it clearly biases the tree to get deep towards the left.",NA
6.6.2. Leftist Heap Operations ,"The fundamental operation on leftist heaps is merging. Notice that insertion is 
 merely a special case of merging, since we may view an insertion as a merge of a 
 one-node heap with a larger heap. We will first give a simple recursive solution 
 and then show how this might be done nonrecursively. Our input is the two leftist",NA
6.7. Skew Heaps,"A skew heap is a self-adjusting version of a leftist heap that is incredibly simple to implement. 
 The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees 
 and 
 AVL
  trees. Skew heaps are binary trees with heap order, but there is no structural 
  
 constraint on these trees. Unlike leftist heaps, no information is maintained about the null path 
 length of any node. The right path of a skew heap can be arbitrarily long at any time, so the 
 worst-case running time of all operations is O(n). However, as with splay trees, it can be shown 
 (see Chapter 11) that for any m consecutive operations, the total worst-case running time is O(m 
 log n). Thus, skew heaps have O(log n) amortized cost per operation. 
  
 As with leftist heaps, the fundamental operation on skew heaps is merging. The merge routine is 
 once again recursive, and we perform the exact same operations as before, with one exception. The 
 difference is that for leftist heaps, we check to see whether the left and right children satisfy 
 the leftist heap order property and swap them if they do not. For skew heaps, the swap is 
 unconditional -- we always do it, with the one exception that the smallest of all the nodes on the 
 right paths does not have its children swapped. This one exception is what happens in the natural 
 recursive implementation, so it is not really a special case at all. Furthermore, it is not 
 necessary to prove the bounds, but since this node is guaranteed not to have a right child, it 
 would be silly to perform the swap and give it one. (In our example, there are no children of this 
 node, so we do not worry about it.) Again, suppose our input is the same two heaps as before, 
 Figure 6.31.",NA
6.8. Binomial Queues,"Although both leftist and skew heaps support merging, insertion, and delete_min all effectively 
 in O(log n) time per operation, there is room for improvement because we know that binary heaps 
 support insertion in constant average time per operation. Binomial queues support all three 
 operations in O(log n) worst-case time per operation, but insertions take constant time on 
 average. 
  
 < P>",NA
6.8.1. Binomial Queue Structure,"Binomial queues differ from all the priority queue implementations that we have seen in that a 
 binomial queue is not a heap-ordered tree but rather a collection of heap-ordered trees, known as 
 a forest. Each of the heap-ordered trees are of a constrained form known as a binomial tree (the",NA
6.8.2. Binomial Queue Operations,"mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
6.8.3. Implementation of Binomial Queues,"The delete_min operation requires the ability to find all the subtrees of the root quickly, so the 
 standard representation of general trees is required: The children of each node are kept in a 
 linked list, and each node has a pointer to its first child (if any). This operation also requires 
 that the children be ordered by the size of their subtrees, in essentially the same way as we have 
 been drawing them. The reason for this is that when a delete_min is performed, the children will 
 form the binomial queue H''.",NA
Summary ,"In this chapter we have seen various implementations and uses of the priority queue 
 ADT
 . The 
  
 standard binary heap implementation is elegant because of its simplicity and speed. It requires 
  
 no pointers and only a constant amount of extra space, yet supports the priority queue operations 
  
 efficiently. 
  
 We considered the additional merge operation and developed three implementations, each of which 
  
 is unique in its own way. The leftist heap is a wonderful example of the power of recursion. The 
  
 skew heap represents a remarkable data structure because of the lack of balance criteria. Its 
  
 analysis, which we will perform in 
  
 Chapter 11, is interesting in its own right. The binomial 
  
 queue shows how a simple idea can be used to achieve a good time bound. 
  
 We have also seen several uses of priority queues, ranging from operating systems scheduling to 
  
 simulation. We will see their use again in Chapters 7, 9, 10. 
  
 PRIORITY_QUEUE
  
 merge( PRIORITY_QUEUE H1, PRIORITY_QUEUE H2 )
  
 {
  
 PRIORITY_QUEUE H3;
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
Exercises,"6.1 Suppose that we replace the delete_min function with find_min. Can both insert and find_min 
  
 be implemented in constant time? 
  
 6.2 a. Show the result of inserting 10, 12, 1, 14, 6, 5, 8, 15, 3, 9, 7, 4, 11, 13, and 2, one at 
  
 a time, into an initially empty binary heap. 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
References,"The binary heap was first described in 
  
  
 [21]. The linear-time algorithm for its construction is 
 from [9]. 
  
 The first description of d -heaps was in [14]. Leftist heaps were invented by Crane [7] and 
 described in Knuth [15]. Skew heaps were developed by Sleator and Tarjan [17]. Binomial queues 
 were invented by Vuillemin [20]; Brown provided a detailed analysis and empirical study showing 
 that they perform well in practice [2], if carefully implemented. 
  
 Exercise 6.7 (b-c) is taken from [12]. A method for constructing binary heaps that uses about 
 1.52n comparisons on average is described in [16]. Lazy deletion in leftist heaps (Exercise 6.21) 
 is from [6]. A solution to Exercise 6.33 can be found in [5]. 
  
 Min-max heaps (Exercise 6.15) were originally described in [1]. More efficient implementation of 
 the operations is given in [13] and [18]. An alternate representation for double ended priority 
 queues is the deap. Details can be found in [3] and [4]. 
  
 A theoretically interesting priority queue representation is the Fibonacci heap [11], which we 
 will describe in Chapter 11. The Fibonacci heap allows all operations to be performed in O(1) 
 amortized time, except for deletions, which are O(log n). Relaxed heaps [8] achieve identical 
 bounds in the worst case. Another interesting implementation is the pairing heap [10]. Finally, a",NA
CHAPTER 7: ,"Previous Chapter
  
  
  
 Return to Table of Contents
  
  
  
 Next Chapter",NA
SORTING ,"In this chapter we discuss the problem of sorting an array of elements. To simplify 
 matters, we will assume in our examples that the array contains only integers, 
 although, obviously, more complicated structures are possible. For most of this 
 chapter, we will also assume that the entire sort can be done in main memory, so 
 that the number of elements is relatively small (less than a million). 
  
 Sorts that cannot be performed in main memory and must be done on disk or tape 
 are also quite important. This type of sorting, known as external sorting, will 
 be discussed at the end of the chapter. 
  
 Our investigation of internal sorting will show that 
  
  
  There are several easy algorithms to sort in O(n
 2
 ), such as insertion sort. 
  
  
  
  There is an algorithm, Shellsort, that is very simple to code, runs in o 
 (n
 2
 ), and is efficient in practice. 
  
  
  
  There are slightly more complicated O(n log n) sorting algorithms. 
  
  
  Any general-purpose sorting algorithm requires 
  
  
 (n log n) comparisons. 
  
 The rest of this chapter will describe and analyze the various sorting 
  
 algorithms. These algorithms contain interesting and important ideas for code 
 optimization as well as algorithm design. Sorting is also an example where the 
 analysis can be precisely performed. Be forewarned that where appropriate, we 
 will do as much analysis as possible.",NA
7.1. Preliminaries ,"The algorithms we describe will all be exchangeable. Each will be passed an array 
 containing the elements and an integer containing the number of elements. 
  
 We will assume that n, the number of elements passed to our sorting routines, has 
 already been checked and is legal. For some of the sorting routines, it will be 
 convenient to place a sentinel in position 0, so we will assume that the array 
 ranges from 0 to n. The actual data will start at position 1 for all the sorts. 
  
 We will also assume the existence of the ""<"" and "">"" operators, which can be used 
 to place a consistent ordering on the input. Besides the assignment operator, these 
 are the only operations allowed on the input data. Sorting under these conditions 
 is known as comparison-based sorting.",NA
7.2. Insertion Sort ,"mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
7.2.1. The Algorithm ,"One of the simplest sorting algorithms is the insertion sort. Insertion sort 
 consists of n - 1 passes. For pass p = 2 through n, insertion sort ensures that 
 the elements in positions 1 through p are in sorted order. Insertion sort makes 
 use of the fact that elements in positions 1 through p - 1 are already known to 
 be in sorted order. Figure 7.1 shows a sample file after each pass of insertion 
 sort. 
  
 Figure 7.1 shows the general strategy. In pass p, we move the pth element left 
 until its correct place is found among the first p elements. The code in Figure 7.2 
 implements this strategy. The sentinel in a[0] terminates the while loop in the 
 event that in some pass an element is moved all the way to the front. Lines 3 
 through 6 implement that data movement without the explicit use of swaps. The 
 element in position p is saved in tmp, and all larger elements (prior to position 
 p) are moved one spot to the right. Then tmp is placed in the correct spot. This is 
 the same technique that was used in the implementation of binary heaps. 
  
 Original        34   8  64  51  32  21     Positions Moved 
  
 ---------------------------------------------------------- 
  
 After p = 2      8  34  64  51  32  21             1 
  
 After p = 3      8  34  64  51  32  21             0 
  
 After p = 4      8  34  51  64  32  21             1 
  
 After p = 5      8  32  34  51  64  21             3 
  
 After p = 6      8  21  32  34  51  64             4 
  
 Figure 7.1 Insertion sort after each pass 
  
 void 
  
 insertion_sort( input_type a[ ], unsigned int n ) 
  
 { 
  
 unsigned int j, p; 
  
 input_type tmp; 
  
 /*1*/       a[0] = MIN_DATA;         /* sentinel */ 
  
 /*2*/       for( p=2; p <= n; p++ ) 
  
 { 
  
 /*3*/            tmp = a[p]; 
  
 /*4*/            for( j = p; tmp < a[j-1]; j-- ) 
  
 /*5*/                 a[j] = a[j-1];",NA
7.2.2. Analysis of Insertion Sort ,"Because of the nested loops, each of which can take n iterations, insertion sort 
 is O(n
 2
 ). Furthermore, this bound is tight, because input in reverse order can 
 actually achieve this bound. A precise calculation shows that the test at line 4 
 can be executed at most p times for each value of p. Summing over all p gives a 
 total of 
  
  
 On the other hand, if the input is presorted, the running time is O(n), because 
 the test in the inner for loop always fails immediately. Indeed, if the input is 
 almost sorted (this term will be more rigorously defined in the next section), 
 insertion sort will run quickly. Because of this wide variation, it is worth 
 analyzing the average-case behavior of this algorithm. It turns out that the 
  
 average case is 
  
  
  
 (n
 2
 ) for insertion sort, as well as for a variety of other 
  
 sorting algorithms, as the next section shows.",NA
7.3. A Lower Bound for Simple Sorting ,NA,NA
Algorithms ,"An inversion in an array of numbers is any ordered pair (i, j) having the property 
 that i < j but a[i] > a[j]. In the example of the last section, the input list 34, 
 8, 64, 51, 32, 21 had nine inversions, namely (34,8), (34,32), (34,21), (64,51), 
 (64,32), (64,21), (51,32), (51,21) and (32,21). Notice that this is exactly the 
 number of swaps that needed to be (implicitly) performed by insertion sort. This is 
 always the case, because swapping two adjacent elements that are out of place 
 removes exactly one inversion, and a sorted file has no inversions. Since there is 
 O(n) other work involved in the algorithm, the running time of insertion sort is 
 O(I + n), where I is the number of inversions in the original file. Thus, insertion 
 sort runs in linear time if the number of 
  
 inversions is O(n). 
  
 We can compute precise bounds on the average running time of insertion sort by 
 computing the average number of inversions in a permutation. As usual, defining 
 average is a difficult proposition. We will assume that there are no duplicate 
 elements (if we allow duplicates, it is not even clear what the average number of 
 duplicates is). Using this assumption, we can assume that the input is some 
 permutation of the first n integers (since only relative ordering is important)",NA
7.4. Shellsort ,NA,NA
7.4.1. Worst-Case Analysis of Shellsort ,"Although Shellsort is simple to code, the analysis of its running time is quite 
 another story. The running time of Shellsort depends on the choice of increment 
 sequence, and the proofs can be rather involved. The average-case analysis of 
 Shellsort is a long-standing open problem, except for the most trivial increment 
 sequences. We will prove tight worst-case bounds for two particular increment 
 sequences. 
  
 THEOREM 7.3. 
  
 The worst-case running time of Shellsort, using Shell's increments, is 
  
  
  
 (n
 2
 ). 
  
 PROOF: 
  
 The proof requires showing not only an upper bound on the worst-case running time 
  
 but also showing that there exists some input that actually takes 
  
  
  
 (n
 2
 ) time 
  
 to run. We prove the lower bound first, by constructing a bad case. First, we",NA
7.5. Heapsort ,"As mentioned in Chapter 6, priority queues can be used to sort in O(n log n) time. 
 The algorithm based on this idea is known as heapsort and gives the best Big-Oh 
 running time we have seen so far. In practice however, it is slower than a version 
 of Shellsort that uses Sedgewick's increment sequence. 
  
 Recall, from Chapter 6, that the basic strategy is to build a binary heap of n 
 elements. This stage takes O(n) time. We then perform n delete_min operations. 
 The elements leave the heap smallest first, in sorted order. By recording these 
 elements in a second array and then copying the array back, we sort n elements. 
 Since each delete_min takes O(log n) time, the total running time is O(n log n). 
  
 The main problem with this algorithm is that it uses an extra array. Thus, the 
 memory requirement is doubled. This could be a problem in some instances. Notice 
 that the extra time spent copying the second array back to the first is only O 
 (n), so that this is not likely to affect the running time significantly. The 
 problem is space.",NA
7.6. Mergesort ,"We now turn our attention to mergesort. Mergesort runs in O(n log n) worst-case 
 running time, and the number of comparisons used is nearly optimal. It is a fine 
 example of a recursive algorithm. 
  
 The fundamental operation in this algorithm is merging two sorted lists. Because 
 the lists are sorted, this can be done in one pass through the input, if the output 
 is put in a third list. The basic merging algorithm takes two input arrays a and b, 
 an output array c, and three counters, aptr, bptr, and cptr, which are initially 
 set to the beginning of their respective arrays. The smaller of a[aptr] and b[bptr] 
 is copied to the next entry in c, and the appropriate counters are advanced. When 
 either input list is exhausted, the remainder of the other list is copied to c. An 
 example of how the merge routine works is provided for the following input. 
  
  
 If the array a contains 1, 13, 24, 26, and b contains 2, 15, 27, 38, then the 
 algorithm proceeds as follows: First, a comparison is done between 1 and 2. 1 is 
 added to c, and then 13 and 2 are compared. 
  
  
 2 is added to c, and then 13 and 15 are compared.",NA
7.6.1. Analysis of Mergesort ,"Mergesort is a classic example of the techniques used to analyze recursive 
 routines. It is not obvious that mergesort can easily be rewritten without 
 recursion (it can), so we have to write a recurrence relation for the running 
 time. We will assume that n is a power of 2, so that we always split into even 
 halves. For n = 1, the time to mergesort is constant, which we will denote by 1. 
  
 Otherwise, the time to mergesort n numbers is equal to the time to do two 
 recursive mergesorts of size n/2, plus the time to merge, which is linear. The 
 equations below say this exactly: 
  
 T(1) = 1 
  
 T(n) = 2T(n/2) + n 
  
 void 
  
 mergesort( input_type a[], unsigned int n ) 
  
 { 
  
 input_type *tmp_array; 
  
 tmp_array = (input_type *) malloc 
  
 ( (n+1) * sizeof (input_type) ); 
  
 if( tmp_array != NULL ) 
  
 { 
  
 m_sort( a, tmp_array, 1, n ); 
  
 free( tmp_array ); 
  
 } 
  
 else 
  
 fatal_error(""No space for tmp array!!!""); 
  
 }",NA
7.7. Quicksort ,"As its name implies, quicksort is the fastest known sorting algorithm in 
 practice. Its average running time is O(n log n). It is very fast, mainly due to 
 a very tight and highly optimized inner loop. It has O(n
 2
 ) worst-case 
  
 performance, but this can be made exponentially unlikely with a little effort. 
 The quicksort algorithm is simple to understand and prove correct, although for 
 many years it had the reputation of being an algorithm that could in theory be 
 highly optimized but in practice was impossible to code correctly (no doubt 
 because of 
 FORTRAN
 ). Like mergesort, quicksort is a divide-and-conquer 
 recursive algorithm. The basic algorithm to sort an array S consists of the 
 following four easy steps: 
  
 1. If the number of elements in S is 0 or 1, then return. 
  
 2. Pick any element v in S. This is called the pivot. 
  
 3. Partition S - {v} (the remaining elements in S) into two disjoint groups: S
 1
  = 
  
 {x 
  
  
  S - {v}| x 
  
  
  v}, and S
 2
  = {x 
  
  
  S -{v}| x 
  
  
  v}. 
  
 4. Return { quicksort(S
 1
 ) followed by v followed by quicksort(S
 2
 )}. 
  
 Since the partition step ambiguously describes what to do with elements equal to 
 the pivot, this becomes a design decision. Part of a good implementation is 
 handling this case as efficiently as possible. Intuitively, we would hope that 
 about half the keys that are equal to the pivot go into S
 1
  and the other half 
 into S
 2
 , much as we like binary search trees to be balanced. 
  
 Figure 7.11 shows the action of quicksort on a set of numbers. The pivot is 
 chosen (by chance) to be 65. The remaining elements in the set are partitioned 
 into two smaller sets. Recursively sorting the set of smaller numbers yields 0, 
 13, 26, 31, 43, 57 (by rule 3 of recursion). The set of large numbers is 
 similarly sorted. The sorted arrangement of the entire set is then trivially 
 obtained. 
  
 It should be clear that this algorithm works, but it is not clear why it is any 
 faster than mergesort. Like mergesort, it recursively solves two subproblems and 
 requires linear additional work (step 3), but, unlike mergesort, the subproblems 
 are not guaranteed to be of equal size, which is potentially bad. The reason that 
 quicksort is faster is that the partitioning step can actually be performed in 
 place and very efficiently. This efficiency more than makes up for the lack of 
 equal-sized recursive calls. 
  
 The algorithm as described so far lacks quite a few details, which we now fill",NA
7.7.1. Picking the Pivot ,"Although the algorithm as described works no matter which element is chosen as 
 pivot, some choices are obviously better than others.",NA
A Wrong Way ,"The popular, uninformed choice is to use the first element as the pivot. This is 
 acceptable if the input is random, but if the input is presorted or in reverse 
 order, then the pivot provides a poor partition, because virtually all the elements 
 go into S
 1
  or S
 2
 . Worse, this happens consistently throughout the recursive calls. 
 The practical effect is that if the first element is used as the pivot and the 
 input is presorted, then quicksort will take quadratic time to do essentially 
 nothing at all, which is quite embarrassing. Moreover, presorted input (or input 
 with a large presorted section) is quite frequent, so using the first element as 
 pivot is an absolutely horrible idea and should be discarded immediately. An 
 alternative is choosing the larger of the first two distinct keys as pivot, but 
 this has the same bad properties as merely choosing the first key. Do not use that 
 pivoting strategy either.",NA
A Safe Maneuver ,"A safe course is merely to choose the pivot randomly. This strategy is generally 
 perfectly safe, unless the random number generator has a flaw (which is not as 
 uncommon as you might think), since it is very unlikely that a random pivot would 
 consistently provide a poor partition. On the other hand, random number 
  
 generation is generally an expensive commodity and does not reduce the average 
 running time of the rest of the algorithm at all.",NA
Median-of-Three Partitioning ,"The median of a group of n numbers is the 
  
  
 n
 /2
  
  
  
  
 th largest number. The best 
  
 choice of pivot would be the median of the file. Unfortunately, this is hard to 
 calculate and would slow down quicksort considerably. A good estimate can be 
 obtained by picking three elements randomly and using the median of these three 
 as pivot. The randomness turns out not to help much, so the common course is to 
 use as pivot the median of the left, right and center elements. For instance, 
 with input 8, 1, 4, 9, 6, 3, 5, 2, 7, 0 as before, the left element is 8, the 
  
 right element is 0 and the center (in position 
  
  
 (
 left
  + right)/2
  
  
  
 ) element 
  
 is 6. Thus, the pivot would be v = 6. Using median-of-three partitioning clearly 
 eliminates the bad case for sorted input (the partitions become equal in this 
 case) and actually reduces the running time of quicksort by about 5 percent.",NA
7.7.2. Partitioning Strategy ,NA,NA
7.7.3. Small Files ,"For very small files (n 
  
  
  20), quicksort does not perform as well as insertion 
  
 sort. Furthermore, because quicksort is recursive, these cases will occur 
 frequently. A common solution is not to use quicksort recursively for small files, 
 but instead use a sorting algorithm that is efficient for small files, such as 
 insertion sort. An even better idea is to leave the file slightly unsorted and 
 finish up with insertion sort. This works well, because insertion sort is efficient 
 for nearly sorted files. Using this strategy can actually save about 15 percent in 
 the running time (over doing no cutoff at all). A good cutoff range is n = 10, 
 although any cutoff between 5 and 20 is likely to produce similar results. This 
 also saves nasty degenerate cases, such as taking the median of three elements when 
 there are only one or two. Of course, if there is a bug in the basic quicksort 
 routine, then the insertion sort will be very, very slow.",NA
7.7.4. Actual Quicksort Routines ,"The driver for quicksort is shown in Figure 7.12. 
  
 The general form of the routines will be to pass the array and the range of the 
 array (left and right) to be sorted. The first routine to deal with is pivot",NA
7.7.5. Analysis of Quicksort ,"Like mergesort, quicksort is recursive, and hence, its analysis requires solving 
 a recurrence formula. We will do the analysis for a quicksort, assuming a random 
 pivot (no median-of-three partitioning) and no cutoff for small files. We will 
 take T(0) = T(1) = 1, as in mergesort. The running time of quicksort is equal to 
 the running time of the two recursive calls plus the linear time spent in the 
 partition (the pivot selection takes only constant time). This gives the basic 
 quicksort relation 
  
 T(n) = T(i) + T(n - i - 1) + cn 
  
 (7.1) 
  
 where i = |S
 1
 | is the number of elements in S
 1
 . We will look at three cases. 
  
 void 
  
 q_sort( input_type a[], int left, int right ) 
  
 { 
  
 int i, j; 
  
 input_type pivot; 
  
 /*1*/       if( left + CUTOFF <= right )",NA
Worst-Case Analysis ,"The pivot is the smallest element, all the time. Then i = 0 and if we ignore T(0) = 
 1, which is insignificant, the recurrence is 
  
 T(n) = T(n - 1) + cn, n > 1 
  
 (7.2) 
  
 We telescope, using Equation (7.2) repeatedly. Thus 
  
 T(n -1) = T(n - 2) + c(n - 1) 
  
 (7.3) 
  
 T(n - 2) = T(n - 3) + c(n - 2) 
  
 (7.4) 
  
 ... 
  
 T(2) = T(1) + c(2) 
  
 (7.5) 
  
 Adding up all these equations yields 
  
  
 (7.6) 
  
 as claimed earlier.",NA
Best-Case Analysis ,"In the best case, the pivot is in the middle. To simplify the math, we assume that 
 the two subfiles are each exactly half the size of the original, and although this 
 gives a slight overestimate, this is acceptable because we are only interested in a 
 Big-Oh answer. 
  
 T(n) = 2T(n/2) + cn 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
Average-Case Analysis ,"This is the most difficult part. For the average case, we assume that each of the 
 file sizes for S
 1
  is equally likely, and hence has probability 1/n. This 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
7.7.6. A Linear-Expected-Time Algorithm for Selection ,"Quicksort can be modified to solve the selection problem, which we have seen in 
 chapters 1 and 6. Recall that by using a priority queue, we can find the kth 
 largest (or smallest) element in O(n + k log n). For the special case of finding 
 the median, this gives an O(n log n) algorithm. 
  
 Since we can sort the file in O(n log n) time, one might expect to obtain a 
 better time bound for selection. The algorithm we present to find the kth 
 smallest element in a set S is almost identical to quicksort. In fact, the first 
 three steps are the same. We will call this algorithm quickselect. Let |S
 i
 | 
 denote the number of elements in S
 i
 . The steps of quickselect are 
  
 1. If |S| = 1, then k = 1 and return the elements in S as the answer. If a cutoff 
  
 for small files is being used and |S| 
  
  
  CUTOFF, then sort S and return the kth 
  
 smallest element. 
  
 2. Pick a pivot element, v 
  
  
  S. 
  
 3. Partition S - {v} into S
 1
  and S
 2
 , as was done with quicksort. 
  
 4. If k 
  
  
  |S
 1
 |, then the kth smallest element must be in S
 1
 . In this case, 
  
 return quickselect (S
 1
 , k). If k = 1 + |S
 1
 |, then the pivot is the kth smallest 
 element and we can return it as the answer. Otherwise, the kth smallest element 
 lies in S
 2
 , and it is the (k - |S
 1
 | - 1)st smallest element in S
 2
 . We make a 
 recursive call and return quickselect (S
 2
 , k - |S
 1
 | - 1). 
  
 In contrast to quicksort, quickselect makes only one recursive call instead of two. 
 The worst case of quickselect is identical to that of quicksort and is O (n
 2
 ). 
 Intuitively, this is because quicksort's worst case is when one of S
 1
  and S
 2
  is 
 empty; thus, quickselect is not really saving a recursive call. The average running 
 time, however, is O(n). The analysis is similar to quicksort's and is left as an 
 exercise. 
  
 The implementation of quickselect is even simpler than the abstract description 
 might imply. The code to do this shown in Figure 7.16. When the algorithm 
 terminates, the kth smallest element is in position k. This destroys the original 
 ordering; if this is not desirable, then a copy must be made. 
  
 /* q_select places the kth smallest element in a[k]*/ 
  
 void 
  
 q_select( input_type a[], int k, int left, int right ) 
  
 {",NA
7.8. Sorting Large Structures ,"Throughout our discussion of sorting, we have assumed that the elements to be 
 sorted are simply integers. Frequently, we need to sort large structures by a 
 certain key. For instance, we might have payroll records, with each record 
 consisting of a name, address, phone number, financial information such as salary, 
 and tax information. We might want to sort this information by one particular 
 field, such as the name. For all of our algorithms, the fundamental operation is 
 the swap, but here swapping two structures can be a very expensive operation, 
 because the structures are potentially large. If this is the case, a practical 
 solution is to have the input array contain pointers to the structures. 
  
 We sort by comparing the keys the pointers point to, swapping pointers when 
 necessary. This means that all the data movement is essentially the same as if we 
 were sorting integers. This is known as indirect sorting; we can use this technique 
 for most of the data structures we have described. This justifies our assumption 
 that complex structures can be handled without tremendous loss efficiency.",NA
7.9. A General Lower Bound for Sorting ,"Although we have O(n log n) algorithms for sorting, it is not clear that this is 
 as good as we can do. In this section, we prove that any algorithm for sorting 
  
 that uses only comparisons requires 
  
  
  
 (n log n) comparisons (and hence time) in 
  
 the worst case, so that mergesort and heapsort are optimal to within a constant 
  
 factor. The proof can be extended to show that 
  
  
 (n log n) comparisons are 
  
 required, even on average, for any sorting algorithm that uses only comparisons, 
 which means that quicksort is optimal on average to within a constant factor. 
  
 Specifically, we will prove the following result: Any sorting algorithm that uses 
  
 only comparisons requires 
  
  
 log n!
  
  
  comparisons in the worst case and log n! 
  
 comparisons on average. We will assume that all n elements are distinct, since 
 any sorting algorithm must work for this case. 
  
 7.9.1 Decision Trees",NA
7.9.1 Decision Trees ,"A decision tree is an abstraction used to prove lower bounds. In our context, a 
 decision tree is a binary tree. Each node represents a set of possible orderings, 
 consistent with comparisons that have been made, among the elements. The results of 
 the comparisons are the tree edges.",NA
7.10. Bucket Sort ,"Although we proved in the previous section that any general sorting algorithm 
  
 that uses only comparisons requires 
  
  
  
 (n log n) time in the worst case, recall 
  
 that it is still possible to sort in linear time in some special cases. 
  
 A simple example is bucket sort. For bucket sort to work, extra information must be 
 available. The input a
 1
 , a
 2
 , . . . , a
 n
  must consist of only positive integers 
 smaller than m. (Obviously extensions to this are possible.) If this is the case, 
 then the algorithm is simple: Keep an array called count, of size m, which is 
 initialized to all 0s. Thus, count has m cells, or buckets, which are initially 
 empty. When a
 i
  is read, increment count[a
 i
 ] by 1. After all the input is read, scan 
 the count array, printing out a representation of the sorted list. This algorithm 
 takes O(m + n); the proof is left as an exercise. If m is O(n), then the total is 
 O(n). 
  
 Although this algorithm seems to violate the lower bound, it turns out that it does 
 not because it uses a more powerful operation than simple comparisons. By 
 incrementing the appropriate bucket, the algorithm essentially performs an m-way 
 comparison in unit time. This is similar to the strategy used in extendible hashing 
 (Section 5.6). This is clearly not in the model for which the lower bound was 
 proven. 
  
 This algorithm does, however, question the validity of the model used in proving 
 the lower bound. The model actually is a strong model, because a general-purpose 
 sorting algorithm cannot make assumptions about the type of input it can expect to 
 see, but must make decisions based on ordering information only. Naturally, if 
 there is extra information available, we should expect to find a more efficient 
 algorithm, since otherwise the extra information would be wasted. 
  
 Although bucket sort seems like much too trivial an algorithm to be useful, it 
 turns out that there are many cases where the input is only small integers, so 
 that using a method like quicksort is really overkill.",NA
7.11. External Sorting ,"So far, all the algorithms we have examined require that the input fit into main 
 memory. There are, however, applications where the input is much too large to fit 
 into memory. This section will discuss external sorting algorithms, which are 
 designed to handle very large inputs.",NA
7.11.1. Why We Need New Algorithms ,NA,NA
7.11.2. Model for External Sorting ,"The wide variety of mass storage devices makes external sorting much more device-
 dependent than internal sorting. The algorithms that we will consider work on 
 tapes, which are probably the most restrictive storage medium. Since access to an 
 element on tape is done by winding the tape to the correct location, tapes can be 
 efficiently accessed only in sequential order (in either direction). 
  
 We will assume that we have at least three tape drives to perform the sorting. We 
 need two drives to do an efficient sort; the third drive simplifies matters. If 
 only one tape drive is present, then we are in trouble: any algorithm will 
  
 require 
  
  
 (n
 2
 ) tape accesses.",NA
7.11.3. The Simple Algorithm ,"The basic external sorting algorithm uses the merge routine from mergesort. 
 Suppose we have four tapes, T
 a1
 , T
 a2
 , T
 b1
 , T
 b2
 , which are two input and two output 
 tapes. Depending on the point in the algorithm, the a and b tapes are either 
 input tapes or output tapes. Suppose the data is initially on T
 a1
 . Suppose further 
 that the internal memory can hold (and sort) m records at a time. A natural first 
 step is to read m records at a time from the input tape, sort the records 
 internally, and then write the sorted records alternately to T
 b1
  and T
 b2
 . We will 
 call each set of sorted records a run. When this is done, we rewind all the 
 tapes. Suppose we have the same input as our example for Shellsort. 
  
  
 If m = 3, then after the runs are constructed, the tapes will contain the data 
 indicated in the following figure.",NA
7.11.4. Multiway Merge ,"If we have extra tapes, then we can expect to reduce the number of passes 
 required to sort our input. We do this by extending the basic (two-way) merge to 
 a k-way merge. 
  
 Merging two runs is done by winding each input tape to the beginning of each run. 
 Then the smaller element is found, placed on an output tape, and the appropriate",NA
7.11.5. Polyphase Merge ,"The k-way merging strategy developed in the last section requires the use of 2k 
 tapes. This could be prohibitive for some applications. It is possible to get by 
 with only k + 1 tapes. As an example, we will show how to perform two-way merging 
 using only three tapes. 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
7.11.6. Replacement Selection,"The last item we will consider is construction of the runs. The strategy we have used so far is 
 the simplest possible: We read as many records as possible and sort them, writing the result to 
 some tape. This seems like the best approach possible, until one realizes that as soon as the 
 first record is written to an output tape, the memory it used becomes available for another 
 record. If the next record on the input tape is larger than the record we have just output, then 
 it can be included in the run. 
  
 Using this observation, we can give an algorithm for producing runs. This technique is commonly 
 referred to as replacement selection. Initially, m records are read into memory and placed in a 
 priority queue. We perform a delete_min, writing the smallest record to the output tape. We read 
 the next record from the input tape. If it is larger than the record we have just written, we can 
 add it to the priority queue. Otherwise, it cannot go into the current run. Since the priority 
 queue is smaller by one element, we can store this new element in the dead space of the priority 
 queue until the run is completed and use the element for the next run. Storing an element in the 
 dead space is similar to what is done in heapsort. We continue doing this until the size of the 
 priority queue is zero, at which point the run is over. We start a new run by building a new 
 priority queue, using all the elements in the dead space. Figure 7.18 shows the run construction 
 for the small example we have been using, with m = 3. Dead elements are indicated by an asterisk. 
  
 In this example, replacement selection produces only three runs, compared with the five runs 
 obtained by sorting. Because of this, a three-way merge finishes in one pass instead of two. If 
 the input is randomly distributed, replacement selection can be shown to produce runs of average 
 length 2m. For our large example, we would expect 160 runs instead of 320 runs, so a five-way 
 merge would require four passes. In this case, we have not saved a pass, although we might if we 
 get lucky and have 125 runs or less. Since external sorts take so long, every pass saved can make 
 a significant difference in the running time. 
  
  3 Elements In Heap Array    Output     Next Element Read
  
  H[1]   H[2]     H[3]
  
 ---------------------------------------------------------------
  
 Run 1    11     94       81          11             96
  
  81     94       96          81             12*
  
  94     96       12*         94             35*
  
  96     35*      12*         96             17*
  
  17*    35*      12*       End of Run.  Rebuild Heap
  
 ---------------------------------------------------------------
  
 Run 2    12     35       17          12             99
  
  17     35       99          17             28
  
  28     99       35          28             58
  
  35     99       58          35             41
  
  41     99       58          41             75*
  
  58     99       75*         58         end of tape",NA
Summary,"For most general internal sorting applications, either insertion sort, Shellsort, or quicksort 
 will be the method of choice, and the decision of which to use will depend mostly on the size of 
 the input. Figure 7.19 shows the running time obtained for each algorithm on various file sizes. 
  
 The data was chosen to be random permutations of n integers, and the times given include only the 
 actual time to sort. The code given in Figure 7.2 was used for insertion sort. Shellsort used the 
 code in Section 7.4 modified to run with Sedgewick's increments. Based on literally millions of 
 sorts, ranging in size from 100 to 25 million, the expected running time of Shellsort with these 
 increments is conjectured to be O(n7/6). The heapsort routine is the same as in Section 7.5. Two 
 versions of quicksort are given. The first uses a simple pivoting strategy and does not do a 
 cutoff. Fortunately, the input files were random. The second uses median-of-three partitioning and 
 a cutoff of ten. Further optimizations were possible. We could have coded the median-of-three 
 routine in-line instead of using a function, and we could have written quicksort nonrecursively. 
 There are some other optimizations to the code that are fairly tricky to implement, and of course 
 we could have used an assembly language. We have made an honest attempt to code all routines 
 efficiently, but of course the performance can vary somewhat from machine to machine. 
  
 The highly optimized version of quicksort is as fast as Shellsort even for very small input sizes. 
 The improved version of quicksort still has an O(n2) worst case (one exercise asks you to 
 construct a small example), but the chances of this worst case appearing are so negligible as to 
 not be a factor. If you need to sort large files, quicksort is the method of choice. But never, 
 ever, take the easy way out and use the first element as pivot. It is just not safe to assume that 
 the input will be random. If you do not want to worry about this, use Shellsort. Shellsort will 
 give a small performance penalty but could also be acceptable, especially if simplicity is 
 required. Its worst case is only O(n4/3); the chance of that worst case occuring is likewise 
 negligible. 
  
 Heapsort, although an O (n log n) algorithm with an apparently tight inner loop, is slower than 
 Shellsort. A close examination of the algorithm reveals that in order to move data, heapsort does 
 two comparisons. Carlsson has analyzed an improvement suggested by Floyd that moves data with 
 essentially only one comparison, but implementing this improvement makes the code somewhat longer. 
 We leave it to the reader to decide whether the extra coding effort is worth the increased speed 
 (Exercise 7.39). 
  
  Insertion Sort  Shellsort  Heapsort    Quicksort   Quicksort(opt.)
  
  n        O(n2)        O(n7/6)    O(n log n)  O(n log n)   O(n log n)
  
 ---------------------------------------------------------------------------
  
  10    0.00044        0.00041     0.00057     0.00052      .00046",NA
Exercises,"7.1 Sort the sequence 3, 1, 4, 1, 5, 9, 2, 6, 5 using insertion sort. 
  
 7.2 What is the running time of insertion sort if all keys are equal? 
  
 7.3 Suppose we exchange elements a[i] and a[i + k], which were originally out of order. Prove 
 that at least 1 and at most 2k - 1 inversions are removed. 
  
 7.4 Show the result of running Shellsort on the input 9, 8, 7, 6, 5, 4, 3, 2, 1 using the 
 increments { 1, 3, 7 }. 
  
 7.5 What is the running time of Shellsort using the two-increment sequence 1, 2 }? 
  
 7.6 *a. Prove that the running time of Shellsort is 
  
  
 (n2) using increments of the form 1, c, 
  
 c2, ..., ci for any integer c. 
  
 **b. Prove that for these increments, the average running time is 
  
  
  
 (n3/2). 
  
 *7.7 Prove that if a k-sorted file is then h-sorted, it remains k-sorted. 
  
 **7.8 Prove that the running time of Shellsort, using the increment sequence suggested by 
  
 Hibbard, is 
  
  
  
 (n3/2) in the worst case. Hint: You can prove the bound by considering the 
  
 special case of what Shellsort does when all elements are either 0 or 1. Set input_data[i] = 1 if 
  
 i is expressible as a linear combination of ht, ht-1, ..., h
  
  
 t/2
  
  
  
 +1 and 0 otherwise. 
  
 7.9 Determine the running time of Shellsort for 
  
 a. sorted input 
  
 *b. reverse-ordered input",NA
References,"Knuth's book [10] is a comprehensive, though somewhat dated, reference for sorting. Gonnet and 
 Baeza-Yates [4] has some more recent results, as well as a huge bibliography. 
  
 The original paper detailing Shellsort is [21]. The paper by Hibbard [5] suggested the use of the 
 increments 2k - 1 and tightened the code by avoiding swaps. Theorem 7.4 is from [12]. Pratt's 
 lower bound, which uses a more complex method than that suggested in the text, can be found in 
 [14]. Improved increment sequences and upper bounds appear in [9], [20], and [23]; matching lower 
 bounds have been shown in [24]. A recent unpublished result by Poonen shows that no increment 
 sequence gives an O(n log n) worst-case running time. An identical result was obtained 
  
 independently and appears in [13]. The average-case running time for Shellsort is still 
  
 unresolved. Yao [26] has performed an extremely complex analysis for the three-increment case. 
  
 The result has yet to be extended to more increments. Experiments with various increment 
 sequences appear in [22]. 
  
 Heapsort was invented by Williams [25]; Floyd [1] provided the linear-time algorithm for heap 
 construction. The analysis of its average case has only recently been obtained [15]. 
  
 An exact average-case analysis of mergesort has been claimed in [3]; the paper detailing the 
 results is forthcoming. An algorithm to perform merging in linear time without extra space is 
 described in [8]. 
  
 Quicksort is from Hoare [6]. This paper analyzes the basic algorithm, describes most of the 
 improvements, and includes the selection algorithm. A detailed analysis and empirical study was 
 the subject of Sedgewick's dissertation [19]. Many of the important results appear in the three 
 papers [16], [17], and [18]. 
  
 Decision trees and sorting optimality are discussed in Ford and Johnson [2]. This paper also 
 provides an algorithm that almost meets the lower bound in terms of number of comparisons (but not 
 other operations). This algorithm was eventually shown to be slightly suboptimal by Manacher [11]. 
  
 External sorting is covered in detail in [10]. Stable sorting, described in Exercise 7.24, has 
 been addressed by Horvath [7]. 
  
 1. R. W. Floyd, ""Algorithm 245: Treesort 3,"" Communications of the ACM 7 (1964), 701. 
  
 2. L. R. Ford and S. M. Johnson, ""A Tournament Problem,"" American Mathematics Monthly 66 (1959), 
 387-389. 
  
 3. M. Golin and R. Sedgewick, ""Exact Analysis of Mergesort,"" Fourth SIAM Conference on Discrete 
 Mathematics, 1988. 
  
 4. G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures, second edition, 
 Addison-Wesley, Reading, MA, 1991.",NA
CHAPTER 8: ,"Previous Chapter
  
  
  
  
  
 Return to Table of Contents",NA
THE DISJOINT SET ADT ,"In this chapter, we describe an efficient data structure to solve the equivalence 
 problem. The data structure is simple to implement. Each routine requires only a 
 few lines of code, and a simple array can be used. The implementation is also 
 extremely fast, requiring constant average time per operation. This data 
  
 structure is also very interesting from a theoretical point of view, because its 
 analysis is extremely difficult; the functional form of the worst case is unlike 
 any we have yet seen. For the disjoint set 
 ADT
 , we will 
  
  
  Show how it can be implemented with minimal coding effort. 
  
  
  Greatly increase its speed, using just two simple observations. 
  
  
  Analyze the running time of a fast implementation. 
  
  
  See a simple application.",NA
8.1. Equivalence Relations ,"A relation R is defined on a set S if for every pair of elements (a, b), a, b 
  
  
  
  S, a R b is either true or false. If a R b is true, then we say that a is 
  
 related to b. 
  
 An equivalence relation is a relation R that satisfies three properties: 
  
 1. (Reflexive) a R a, for all a 
  
  
  S. 
  
 2. (Symmetric) a R b if and only if b R a. 
  
 3. (Transitive) a R b and b R c implies that a R c. 
  
 We'll consider several examples. 
  
 The 
  
  
  relationship is not an equivalence relationship. Although it is 
  
 reflexive, since a 
  
  
  a, and transitive, since a 
  
  
  b and b 
  
  
  
  c implies a 
  
  
  
  c, it is not symmetric, since a 
  
  
  b does not imply b 
  
  
  
  a. 
  
 Electrical connectivity, where all connections are by metal wires, is an 
 equivalence relation. The relation is clearly reflexive, as any component is",NA
8.2. The Dynamic Equivalence Problem ,"Given an equivalence relation ~, the natural problem is to decide, for any a and 
 b, if a ~ b. If the relation is stored as a two-dimensional array of booleans, 
 then, of course, this can be done in constant time. The problem is that the 
 relation is usually not explicitly, but rather implicitly, defined. 
  
 As an example, suppose the equivalence relation is defined over the five-element 
 set {a
 1
 , a
 2
 , a
 3
 , a
 4
 , a
 5
 }. Then there are 25 pairs of elements, each of which is 
 either related or not. However, the information a
 1
  ~ a
 2
 , a
 3
  ~ a
 4
 , a
 5
  ~ a
 1
 , a
 4
  ~ a
 2 
 implies that all pairs are related. We would like to be able to infer this 
 quickly. 
  
 The equivalence class of an element a 
  
  
  S is the subset of S that contains all 
  
 the elements that are related to a. Notice that the equivalence classes form a 
 partition of S: Every member of S appears in exactly one equivalence class. To 
 decide if a ~ b, we need only to check whether a and b are in the same 
  
 equivalence class. This provides our strategy to solve the equivalence problem. 
  
 The input is initially a collection of n sets, each with one element. This 
 initial representation is that all relations (except reflexive relations) are 
  
 false. Each set has a different element, so that S
 i 
  
  
  S
 j
  = 
  
  
  
 ; this makes the 
  
 sets disjoint. 
  
 There are two permissible operations. The first is find, which returns the name of 
 the set (that is, the equivalence class) containing a given element. The second 
 operation adds relations. If we want to add the relation a ~ b, then we first see 
 if a and b are already related. This is done by performing finds on both a and b 
 and checking whether they are in the same equivalence class. If they are not, then 
 we apply union. This operation merges the two equivalence classes containing a and 
 b into a new equivalence class. From a set point of view, the 
  
 result of 
  
  
  is to create a new set S
 k
  = S
 i
  
  
  S
 j
 , destroying the originals 
  
 and preserving the disjointness of all the sets. The algorithm to do this is 
 frequently known as the disjoint set union/find algorithm for this reason.",NA
8.3. Basic Data Structure ,"Recall that the problem does not require that a find operation return any specific 
 name, just that finds on two elements return the same answer if and only if they 
 are in the same set. One idea might be to use a tree to represent each set, since 
 each element in a tree has the same root. Thus, the root can be used to name the 
 set. We will represent each set by a tree. (Recall that a collection of trees is 
 known as a forest.) Initially, each set contains one element. The trees we will use 
 are not necessarily binary trees, but their representation is easy, because the 
 only information we will need is a parent pointer. The name of a set is given by 
 the node at the root. Since only the name of the parent is required, we can assume 
 that this tree is stored implicitly in an array: each entry p[i] in the array 
 represents the parent of element i. If i is a root, then 
  
 p[i] = 0. In the forest in Figure 8.1, p[i] = 0 for 1
  
  
  
  
  i 
  
  
  
  8. As with heaps, 
  
 we will draw the trees explicitly, with the understanding that an array is being 
 used. Figure 8.1 shows the explicit representation. We will draw the root's 
 parent pointer vertically for convenience. 
  
 To perform a union of two sets, we merge the two trees by making the root of one 
 tree point to the root of the other. It should be clear that this operation takes 
 constant time. Figures 8.2, 8.3, and 8.4 represent the forest after each of union 
 (5,6) union(7,8), union(5,7), where we have adopted the convention that the new 
 root after the union(x,y) is x. The implicit representation of the last forest is 
 shown in Figure 8.5. 
  
 A find(x) on element x is performed by returning the root of the tree containing x. 
 The time to perform this operation is proportional to the depth of the node 
 representing x, assuming, of course, that we can find the node representing x in 
 constant time. Using the strategy above, it is possible to create a tree of depth n 
 - 1, so the worst-case running time of a find is O(n). Typically, the running time 
 is computed for a sequence of m intermixed instructions. In this case, m 
 consecutive operations could take O(mn) time in the worst case. 
  
 The code in Figures 8.6 through 8.9 represents an implementation of the basic 
 algorithm, assuming that error checks have already been performed. In our 
 routine, unions are performed on the roots of the trees. Sometimes the operation",NA
8.4. Smart Union Algorithms ,"The unions above were performed rather arbitrarily, by making the second tree a 
 subtree of the first. A simple improvement is always to make the smaller tree a 
 subtree of the larger, breaking ties by any method; we call this approach union-
 by-size. The three unions in the preceding example were all ties, and so we can 
 consider that they were performed by size. If the next operation were union (4, 
 5), then the forest in Figure 8.10 would form. Had the size heuristic not been 
 used, a deeper forest would have been formed (Fig. 8.11). 
  
  
 Figure 8.10 Result of union-by-size",NA
8.5. Path Compression ,"The union/find algorithm, as described so far, is quite acceptable for most 
 cases. It is very simple and linear on average for a sequence of m instructions 
 (under all models). However, the worst case of O(m log n ) can occur fairly 
 easily and naturally. 
  
 /
 *
  assume root1 and root2 are roots 
 *
 /
  
 /
 *
  union is a C keyword, so this routine is named set_union 
 *
 /
  
 void 
  
 set_union (DISJ_SET S, set_type root1, set_type root2 ) 
  
 { 
  
 if( S[root2] < S[root1] )  /
 *
  root2 is deeper set */
  
 S[root1] = root2;     /* make root2 new root */ 
  
 else 
  
 { 
  
 if( S[root2] == S[root1] ) /
 *
  same height, so update */",NA
8.6. Worst Case for Union-by-Rank and ,NA,NA
Path Compression ,"When both heuristics are used, the algorithm is almost linear in the worst case. 
  
 Specifically, the time required in the worst case is 
  
  
  
  
 (m
  
  
  
 (m, n)) (provided m",NA
8.6.1 Analysis of the Union/Find Algorithm ,"In this section we establish a fairly tight bound on the running time of a 
  
 sequence of m = 
  
  
 (n) union/find operations. The unions and finds may occur in 
  
 any order, but unions are done by rank and finds are done with path compression. 
  
 We begin by establishing some lemmas concerning the number of nodes of rank r. 
  
 Intuitively, because of the union-by-rank rule, there are many more nodes of 
 small rank than large rank. In particular, there can be at most one node of rank",NA
8.7. An Application,"As an example of how this data structure might be used, consider the following problem. We have a 
 network of computers and a list of bidirectional connections; each of these connections allows a 
 file transfer from one computer to another. Is it possible to send a file from any computer on the 
 network to any other? An extra restriction is that the problem must be solved on-line. Thus, the 
 list of connections is presented one at a time, and the algorithm must be prepared to give an 
 answer at any point.",NA
Summary,"We have seen a very simple data structure to maintain disjoint sets. When the union operation is 
 performed, it does not matter, as far as correctness is concerned, which set retains its name. A 
 valuable lesson that should be learned here is that it can be very important to consider the 
 alternatives when a particular step is not totally specified. The union step is flexible; by 
 taking advantage of this, we are able to get a much more efficient algorithm. 
  
 Path compression is one of the earliest forms of self-adjustment, which we have seen elsewhere 
 (splay trees, skew heaps). Its use is extremely interesting, especially from a theoretical point 
 of view, because it was one of the first examples of a simple algorithm with a not-so-simple 
 worst-case analysis.",NA
Exercises,"8.1 Show the result of the following sequence of instructions: union(1, 2), union(3, 4), union(3, 
  
 5), union(1, 7), union(3, 6), union(8, 9), union(1, 8), union(3, 10), union(3, 11), union(3, 12), 
 union(3, 13), union(14, 15), union(16, 17), union(14, 16), union(1, 3), union(1, 14), when the 
 unions are 
  
 a. performed arbitrarily 
  
 b. performed by height 
  
 c. performed by size 
  
 8.2 For each of the trees in the previous exercise, perform a find with path compression on the 
  
 deepest node. 
  
 8.3 Write a program to determine the effects of path compression and the various unioning 
  
 strategies. Your program should process a long sequence of equivalence operations using all six 
 of the possible strategies. 
  
 8.4 Show that if unions are performed by height, then the depth of any tree is O(log n) .",NA
References,"Various solutions to the union/find problem can be found in [5], [8], and [10]. Hopcroft and 
  
 Ullman showed the O(m log* n) bound of Section 8.6. Tarjan [14] obtained the bound O(m
  
  
 (m,n)). 
  
 A more precise (but asymptotically identical) bound for m < n appears in [2] and [17]. Various 
  
 other strategies for path compression and unions also achieve the same bound; see [17] for 
  
 details. 
  
 A lower bound showing that under certain restrictions 
  
  
  
 (m
  
  
 (m,n)) time is required to 
  
 process m union/find operations was given by Tarjan [15]. Identical bounds under less restrictive 
  
 conditions have been recently shown in [6] and [13]. 
  
 Applications of the union/find data structure appear in [1] and [9]. Certain special cases of the 
  
 union/find problem can be solved in O(m) time [7]. This reduces the running time of several 
  
 algorithms, such as [1], graph dominance, and reducibility (see references in Chapter 9) by a 
  
  
 factor of 
  
  
 (m,n). Others, such as [9] and the graph connectivity problem in this chapter, are 
  
 unaffected. The paper lists 10 examples. Tarjan has used path compression to obtain efficient 
  
 algorithms for several graph problems [16]. 
  
 Average-case results for the union/find problem appear in [4], [11], and [19]. Results bounding 
  
 the running time of any single operation (as opposed to the entire sequence) appear in [3] and 
  
 [12]. 
  
 Exercise 8.8 is solved in [18]. 
  
 1. A. V. Aho, J. E. Hopcroft, J. D. Ullman, ""On Finding Lowest Common Ancestors in Trees,"" SIAM 
  
 Journal on Computing 5 (1976), 115-132. 
  
 2. L. Banachowski, ""A Complement to Tarjan's Result about the Lower Bound on the Complexity of 
  
 the Set Union Problem,"" Information Processing Letters 11 (1980), 59-65. 
  
 3. N. Blum, ""On the Single-operation Worst-case Time Complexity of the Disjoint Set Union 
  
 Problem,"" SIAM Journal on Computing 15 (1986), 1021-1024. 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
CHAPTER 9: ,"Previous Chapter
  
  
  
  
 Return to Table of Contents",NA
GRAPH ALGORITHMS ,"In this chapter we discuss several common problems in graph theory. Not only are 
 these algorithms useful in practice, they are interesting because in many real-
 life applications they are too slow unless careful attention is paid to the 
 choice of data structures. We will 
  
  
  
  Show several real-life problems, which can be converted to problems on 
  
 graphs. 
  
  
  
  Give algorithms to solve several common graph problems. 
  
  
  
  
  Show how the proper choice of data structures can drastically reduce the 
 running time of these algorithms. 
  
  
  
  See an important technique, known as depth-first search, and show how it can 
 be used to solve several seemingly nontrivial problems in linear time.",NA
9.1 Definitions ,"A graph G = (V, E) consists of a set of vertices, V, and a set of edges, E. Each 
  
 edge is a pair (v,w), where v,w 
  
  
  V. Edges are sometimes referred to as arcs. 
  
 If the pair is ordered, then the graph is directed. Directed graphs are sometimes 
  
 referred to as digraphs. Vertex w is adjacent to v if and only if (v,w) 
  
  
  E. 
  
 In an undirected graph with edge (v,w), and hence (w,v), w is adjacent to v and v 
 is adjacent to w. Sometimes an edge has a third component, known as either a 
 weight or a cost. 
  
 A path in a graph is a sequence of verices w
 1
 , w
 2
 , w
 3
 , . . . , w
 n
  such that (w
 i
 , 
  
 w
 i+i
 ) 
  
  
  E for 1 
  
  
  
  i < n. The length of such a path is the number of edges on 
  
 the path, which is equal to n - 1. We allow a path from a vertex to itself; if this 
 path contains no edges, then the path lenght is 0. This is a convenient way to 
 define an otherwise special case. If the graph contains an edge (v,v) from a vertex 
 to itself, then the path v, v is sometimes referred to as a loop. The graphs we 
 will consider will generally be loopless. A simple path is a path such that all 
 vertices are distinct, except that the first and last could be the same. 
  
 A cycle in a directed graph is a path of length at least 1 such that w
 1
  = w
 n
 ; this 
 cycle is simple if the path is simple. For undirected graphs, we require that the 
 edges be distinct. The logic of these requirements is that the path u, v, u in an",NA
9.1.1. Representation of Graphs ,"We will consider directed graphs (undirected graphs are similarly represented). 
  
 Suppose, for now, that we can number the vertices, starting at 1. The graph shown 
 in Figure 9.1 represents 7 vertices and 12 edges. 
  
 One simple way to represent a graph is to use a two-dimensional array. This is 
 known as an adjacency matrix representation. For each edge (u, v), we set a[u][v] = 
 1; otherwise the entry in the array is 0. If the edge has a weight associated with 
 it, then we can set a[u][v] equal to the weight and use either a very large or a 
 very small weight as a sentinel to indicate nonexistent edges. For instance, if we 
 were looking for the cheapest airplane route, we could represent 
  
 nonexistent flights with a cost of 
  
  
 . If we were looking, for some strange 
  
 reason, for the most expensive airplane route, we could use - 
  
  
  
  (or perhaps 0)",NA
9.2. Topological Sort ,"A topological sort is an ordering of vertices in a directed acyclic graph, such 
 that if there is a path from v
 i
  to v
 j
 , then v
 j
  appears after v
 i
  in the ordering. 
 The graph in Figure 9.3 represents the course prerequisite structure at a state 
 university in Miami. A directed edge (v,w) indicates that course v must be 
 completed before course w may be attempted. A topological ordering of these 
 courses is any course sequence that does not violate the prerequisite 
  
 requirement. 
  
 It is clear that a topological ordering is not possible if the graph has a cycle, 
 since for two vertices v and w on the cycle, v precedes w and w precedes v. 
 Furthermore, the ordering is not necessarily unique; any legal ordering will do. In 
 the graph in Figure 9.4, v
 1
 , v
 2
 , v
 5
 , v
 4
 , v
 3
 , v
 7
 , v
 6
  and v
 1
 , v
 2
 , v
 5
 , v
 4
 , v
 7
 , v
 3
 ,",NA
9.3. Shortest-Path Algorithms,"In this section we examine various shortest-path problems. The input is a weighted graph: 
  
 associated with each edge (vi, vj) is a cost ci,j to traverse the arc. The cost of a path v1v2 
 ... vn is 
  This is referred to as the weighted path length. The unweighted path 
  
 length is merely the number of edges on the path, namely, n - 1. 
  
 SINGLE-SOURCE SHORTEST-PATH PROBLEM: 
  
 Given as input a weighted graph, G = (V, E), and a distinguished vertex, s , find the shortest 
  
 weighted path from s to every other vertex in G. 
  
 For example, in the graph in 
  
 Figure 9.8, the shortest weighted path from v1 to v6 has a cost of 6 
 and goes from v1 to v4 to v7 to v6. The shortest unweighted path between these vertices is 2. 
 Generally, when it is not specified whether we are referring to a weighted or an unweighted path, 
  
 the path is weighted if the graph is. Notice also that in this graph there is no path from v6 to 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
9.3.1. Unweighted Shortest Paths,"Figure 9.10 shows an unweighted graph, G. Using some vertex, s, which is an input parameter, we 
 would like to find the shortest path from s to all other vertices. We are only interested in the 
 number of edges contained on the path, so there are no weights on the edges. This is clearly a 
 special case of the weighted shortest-path problem, since we could assign all edges a weight of 1. 
  
 For now, suppose we are interested only in the length of the shortest paths, not in the actual 
 paths themselves. Keeping track of the actual paths will turn out to be a matter of simple 
 bookkeeping. 
  
  
 Figure 9.10 An unweighted directed graph G
  
 Suppose we choose s to be v3. Immediately, we can tell that the shortest path from s to v3 is 
 then a path of length 0. We can mark this information, obtaining the graph in 
  
  
 Figure 9.11. 
  
 Now we can start looking for all vertices that are a distance 1 away from s. These can be found 
 by looking at the vertices that are adjacent to s. If we do this, we see that v1 and v6 are one 
 edge from s. This is shown in Figure 9.12.",NA
9.3.2. Dijkstra's Algorithm,"If the graph is weighted, the problem (apparently) becomes harder, but we can still use the ideas 
 from the unweighted case. 
  
 We keep all of the same information as before. Thus, each vertex is marked as either known or 
 unknown. A tentative distance dv is kept for each vertex, as before. This distance turns out to be 
 the shortest path length from s to v using only known vertices as intermediates. As before, we 
 record pv, which is the last vertex to cause a change to dv. 
  
 void
  
 unweighted( TABLE T )   /* assume T is initialized (Fig 9.30) */
  
 {
  
 QUEUE Q;
  
 vertex v, w;
  
 /*1*/        Q = create_queue( NUM_VERTEX ); make_null( Q );
  
 /* enqueue the start vertex s, determined elsewhere */
  
 /*2*/        enqueue( s, Q );
  
 /*3*/        while( !is empty( Q ) )
  
 {
  
 /*4*/             v = dequeue( Q );",NA
9.3.3. Graphs with Negative Edge Costs,"If the graph has negative edge costs, then Dijkstra's algorithm does not work. The problem is 
 that once a vertex u is declared known, it is possible that from some other, unknown vertex v 
 there is a path back to u that is very negative. In such a case, taking a path from s to v back 
 to u is better than going from s to u without using v. 
  
 A combination of the weighted and unweighted algorithms will solve the problem, but at the cost 
 of a drastic increase in running time. We forget about the concept of known vertices, since our 
 algorithm needs to be able to change its mind. We begin by placing s on a queue. Then, at each 
 stage, we dequeue a vertex v. We find all vertices w adjacent to v such that dw > dv + cv,w. We 
 update dw and pw, and place w on a queue if it is not already there. A bit can be set for each 
 vertex to indicate presence in the queue. We repeat the process until the queue is empty. 
  
  
 Figure 
 9.33 (almost) implements this algorithm. 
  
 Although the algorithm works if there are no negative-cost cycles, it is no longer true that the 
 code in lines 6 through 10 is executed once per edge. Each vertex can dequeue at most |V| times, 
  
 so the running time is O(|E| 
  
  
  |V|) if adjacency lists are used (Exercise 9.7b). This is quite 
  
 an increase from Dijkstra's algorithm, so it is fortunate that, in practice, edge costs are 
 nonnegative. If negative-cost cycles are present, then the algorithm as written will loop 
 indefinitely. By stopping the algorithm after any vertex has dequeued |V| + 1 times, we can 
 guarantee termination.",NA
9.3.4. Acyclic Graphs,"If the graph is known to be acyclic, we can improve Dijkstra's algorithm by changing the order in 
 which vertices are declared known, otherwise known as the vertex selection rule. The new rule is 
 to select vertices in topological order. The algorithm can be done in one pass, since the 
 selections and updates can take place as the topological sort is being performed. 
  
 This selection rule works because when a vertex v is selected, its distance, dv, can no longer be 
 lowered, since by the topological ordering rule it has no incoming edges emanating from unknown 
 nodes. 
  
 There is no need for a priority queue with this selection rule; the running time is O(|E| + |V|), 
 since the selection takes constant time. 
  
 An acyclic graph could model some downhill skiing problem -- we want to get from point a to b, but 
 can only go downhill, so clearly there are no cycles. Another possible application might be the 
 modeling of (nonreversible) chemical reactions. We could have each vertex represent a particular 
 state of an experiment. Edges would represent a transition from one state to another, and the edge",NA
9.3.5. All-Pairs Shortest Path,"Sometimes it is important to find the shortest paths between all pairs of vertices in the graph. 
  
 Although we could just run the appropriate single-source algorithm |V| times, we might expect a 
 somewhat faster solution, especially on a dense graph, if we compute all the information at once. 
  
 In 
  
 Chapter 10, we will see an O(|V|3) algorithm to solve this problem for weighted graphs. 
  
 Although, for dense graphs, this is the same bound as running a simple (non-priority queue) 
 Dijkstra's algorithm |V| times, the loops are so tight that the specialized all-pairs algorithm 
 is likely to be faster in practice. On sparse graphs, of course, it is faster to run |V| 
 Dijkstra's algorithms coded with priority queues.",NA
9.4. Network Flow Problems ,"Suppose we are given a directed graph G = (V, E) with edge capacities cv,w. These capacities could 
 represent the amount of water that could flow through a pipe or the amount of traffic that could 
 flow on a street between two intersections. We have two vertices: s, which we call the source, and 
 t, which is the sink. Through any edge, (v, w), at most cv,w units of ""flow"" may pass. At any 
 vertex, v, that is not either s or t, the total flow coming in must equal the total flow going 
 out. The maximum flow problem is to determine the maximum amount of flow that can pass from s to 
 t. As an example, for the graph in Figure 9.39 on the left the maximum flow is 5, as indicated by 
 the graph on the right. 
  
  
 Figure 9.39 A graph (left) and its maximum flow
  
 As required by the problem statement, no edge carries more flow than its capacity. Vertex a has 
 three units of flow coming in, which it distributes to c and d. Vertex d takes three units of flow 
 from a and b and combines this, sending the result to t. A vertex can combine and distribute flow 
 in any manner that it likes, as long as edge capacities are not violated and as long as flow 
 conservation is maintained (what goes in must come out). 
  
 9.4.1. A Simple Maximum-Flow Algorithm",NA
9.4.1. A Simple Maximum-Flow Algorithm,"A first attempt to solve the problem proceeds in stages. We start with our graph, G, and construct 
 a flow graph Gf. Gf tells the flow that has been attained at any stage in the 
  
 algorithm. Initially all edges in Gf have no flow, and we hope that when the algorithm 
  
 terminates, Gf contains a maximum flow. We also construct a graph, Gr, called the residual graph. 
  
 Gr tells, for each edge, how much more flow can be added. We can calculate this by subtracting 
 the current flow from the capacity for each edge. An edge in Gr is known as a residual edge. 
  
 At each stage, we find a path in Gr from s to t. This path is known as an augmenting path. The 
 minimum edge on this path is the amount of flow that can be added to every edge on the path. We do 
 this by adjusting Gf and recomputing Gr. When we find no path from s to t in Gr, we terminate.",NA
9.5. Minimum Spanning Tree,"The next problem we will consider is that of finding a minimum spanning tree in an undirected 
 graph. The problem makes sense for directed graphs but appears to be more difficult. Informally, a 
 minimum spanning tree of an undirected graph G is a tree formed from graph edges that connects all 
 the vertices of G at lowest total cost. A minimum spanning tree exists if and only if G is 
 connected. Although a robust algorithm should report the case that G is unconnected, we will 
 assume that G is connected, and leave the issue of robustness as an exercise for the reader. 
  
 In 
  
 Figure 9.48 the second graph is a minimum spanning tree of the first (it happens to be unique, 
 but this is unusual). Notice that the number of edges in the minimum spanning tree is |V| - 1. The 
 minimum spanning tree is a tree because it is acyclic, it is spanning because it covers every 
 edge, and it is minimum for the obvious reason. If we need to wire a house with a minimum of 
 cable, then a minimum spanning tree problem needs to be solved. There are two basic algorithms to 
 solve this problem; both are greedy. We now describe them.",NA
9.5.1. Prim's Algorithm,"One way to compute a minimum spanning tree is to grow the tree in successive stages. In each 
 stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the 
 tree. 
  
 At any point in the algorithm, we can see that we have a set of vertices that have already been 
 included in the tree; the rest of the vertices have not. The algorithm then finds, at each stage, 
 a new vertex to add to the tree by choosing the edge (u, v) such that the cost of (u, v) is the 
 smallest among all edges where u is in the tree and v is not. 
  
 Figure 9.49 shows how this 
 algorithm would build the minimum spanning tree, starting from v1. Initially, v1 is in the tree 
 as a root with no edges. Each step adds one edge and one vertex to the tree. 
  
 We can see that Prim's algorithm is essentially identical to Dijkstra's algorithm for shortest 
 paths. As before, for each vertex we keep values dv and pv and an indication of whether it is 
 known or unknown. dv is the weight of the shortest arc connecting v to a known vertex, and pv, as 
 before, is the last vertex to cause a change in dv. The rest of the algorithm is exactly the",NA
9.5.2. Kruskal's Algorithm,NA,NA
9.6. Applications of Depth-First Search,"Depth-first search is a generalization of preorder traversal. Starting at some vertex, v, we 
  
 process v and then recursively traverse all vertices adjacent to v. If this process is performed 
  
 on a tree, then all tree vertices are systematically visited in a total of O(|E|) time, since |E| 
  
 = 
  
  
 (|V|). If we perform this process on an arbitrary graph, we need to be careful to avoid 
  
 cycles. To do this, when we visit a vertex v, we mark it visited, since now we have been there, 
  
 and recursively call depth-first search on all adjacent vertices that are not already marked. We 
  
 implicitly assume that for undirected graphs every edge (v, w) appears twice in the adjacency 
  
 lists: once as (v, w) and once as (w, v). The procedure in Figure 9.59 performs a depth-first 
  
 search (and does absolutely nothing else) and is a template for the general style. 
  
 void
  
 dfs( vertex v )
  
 {
  
 visited[v] = TRUE;
  
 for each w adjacent to v
  
 if( !visited[w] )
  
 dfs( w );
  
 }
  
 Figure 9.59 Template for depth-first search
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
9.6.1 Undirected Graphs,"An undirected graph is connected if and only if a depth-first search starting from any node visits 
 every node. Because this test is so easy to apply, we will assume that the graphs we deal with are 
 connected. If they are not, then we can find all the connected components and apply our algorithm 
 on each of these in turn. 
  
 As an example of depth-first search, suppose in the graph of 
  
 Figure 9.60 we start at vertex A. 
  
 Then we mark A as visited and call dfs(B) recursively. dfs(B) marks B as visited and calls dfs(C) 
 recursively. dfs(C) marks C as visited and calls dfs(D) recursively. dfs(D) sees both A and B, but 
 both these are marked, so no recursive calls are made. dfs(D) also sees that C is adjacent but 
 marked, so no recursive call is made there, and dfs(D) returns back to dfs(C). dfs(C) sees B 
 adjacent, ignores it, finds a previously unseen vertex E adjacent, and thus calls dfs(E). dfs(E) 
 marks E, ignores A and C, and returns to dfs(C). dfs(C) returns to dfs(B). dfs(B) ignores both A 
 and D and returns. dfs(A) ignores both D and E and returns. (We have actually touched every edge 
 twice, once as (v, w) and again as (w, v), but this is really once per adjacency list entry.) 
  
  
 Figure 9.60 An undirected graph",NA
9.6.2. Biconnectivity,"A connected undirected graph is biconnected if there are no vertices whose removal disconnects the 
 rest of the graph. The graph in the example above is biconnected. If the nodes are computers and 
 the edges are links, then if any computer goes down, network mail is unaffected, except, of 
 course, at the down computer. Similarly, if a mass transit system is biconnected, users always 
 have an alternate route should some terminal be disrupted. 
  
  
 Figure 9.61 Depth-first search of previous graph
  
 If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as",NA
9.6.3. Euler Circuits,"Consider the three figures in Figure 9.68. A popular puzzle is to reconstruct these figures using 
 a pen, drawing each line exactly once. The pen may not be lifted from the paper while the drawing 
 is being performed. As an extra challenge, make the pen finish at the same point at which it 
 started. This puzzle has a surprisingly simple solution. Stop reading if you would like to try to 
 solve it. 
  
  
 Figure 9.64 Depth-first tree that results if depth-first search starts at C
  
 /* assign num and compute parents */
  
 void
  
 assign_num( vertex v )
  
 {
  
 vertex w;",NA
9.6.4. Directed Graphs,"Using the same strategy as with undirected graphs, directed graphs can be traversed in linear 
 time, using depth-first search. If the graph is not strongly connected, a depth-first search 
 starting at some node might not visit all nodes. In this case we repeatedly perform depth-first 
 searches, starting at some unmarked node, until all vertices have been visited. As an example, 
 consider the directed graph in Figure 9.74. 
  
 We arbitrarily start the depth-first search at vertex B. This visits vertices B, C, A, D, E, and 
 F. We then restart at some unvisited vertex. Arbitrarily, we start at H, which visits I and J. 
  
 Finally, we start at G, which is the last vertex that needs to be visited. The corresponding 
 depth-first search tree is shown in Figure 9.75.",NA
9.6.5. Finding Strong Components,"By performing two depth-first searches, we can test whether a directed graph is strongly 
 connected, and if it is not, we can actually produce the subsets of vertices that are strongly 
 connected to themselves. This can also be done in only one depth-first search, but the method 
 used here is much simpler to understand. 
  
 First, a depth-first search is performed on the input graph G. The vertices of G are numbered by a 
 postorder traversal of the depth-first spanning forest, and then all edges in G are reversed, 
 forming Gr. The graph in 
  
  
 Figure 9.76 represents Gr for the graph G shown in Figure 9.74; the 
 vertices are shown with their numbers. 
  
 The algorithm is completed by performing a depth-first search on Gr, always starting a new depth-
 first search at the highest-numbered vertex. Thus, we begin the depth-first search of Gr at vertex 
 G, which is numbered 10. This leads nowhere, so the next search is started at H. This call visits 
 I and J. The next call starts at B and visits A, C, and F. The next calls after this are dfs(D) 
 and finally dfs(E). The resulting depth-first spanning forest is shown in Figure 9.77. 
  
 Each of the trees (this is easier to see if you completely ignore all nontree edges) in this 
 depth-first spanning forest forms a strongly connected component. Thus, for our example, the 
 strongly connected components are {G}, {H, I, J}, {B, A, C, F}, {D}, and {E}. 
  
 To see why this algorithm works, first note that if two vertices v and w are in the same strongly 
 connected component, then there are paths from v to w and from w to v in the original graph G, and 
 hence also in Gr. Now, if two vertices v and w are not in the same depth-first spanning tree",NA
9.7. Introduction to NP-Completeness,"In this chapter, we have seen solutions to a wide variety of graph theory problems. All these 
 problems have polynomial running times, and with the exception of the network flow problem, the 
 running time is either linear or only slightly more than linear (O(|E| log |E|)). We have also 
 mentioned, in passing, that for some problems certain variations seem harder than the original. 
  
 Recall that the Euler circuit problem, which finds a path that touches every edge exactly once, 
 is solvable in linear time. The Hamiltonian cycle problem asks for a simple cycle that contains 
 every vertex. No linear algorithm is known for this problem. 
  
 The single-source unweighted shortest-path problem for directed graphs is also solvable in linear 
 time. No linear-time algorithm is known for the corresponding longest-simple-path problem. 
  
 The situation for these problem variations is actually much worse than we have described. Not only 
 are no linear algorithms known for these variations, but there are no known algorithms that are 
 guaranteed to run in polynomial time. The best known algorithms for these problems could take 
 exponential time on some inputs. 
  
 In this section we will take a brief look at this problem. This topic is rather complex, so we 
 will only take a quick and informal look at it. Because of this, the discussion may be 
 (necessarily) somewhat imprecise in places. 
  
 We will see that there are a host of important problems that are roughly equivalent in 
  
 complexity. These problems form a class called the NP-complete problems. The exact complexity of 
 these NP-complete problems has yet to be determined and remains the foremost open problem in 
 theoretical computer science. Either all these problems have polynomial-time solutions or none of 
 them do. 
  
 9.7.1. Easy vs. Hard 
  
 9.7.2. The Class NP 
  
 9.7.3. NP-Complete Problems",NA
9.7.1. Easy vs. Hard,"When classifying problems, the first step is to examine the boundaries. We have already seen that 
 many problems can be solved in linear time. We have also seen some O(log n) running times, but 
 these either assume some preprocessing (such as input already being read or a data structure 
 already being built) or occur on arithmetic examples. For instance, the gcd algorithm, when 
 applied on two numbers m and n, takes O(log n) time. Since the numbers consist of log m and log n 
 bits respectively, the gcd algorithm is really taking time that is linear in the amount or size",NA
9.7.2. The Class NP,"A few steps down from the horrors of undecidable problems lies the class NP. NP stands for 
 nondeterministic polynomial-time. A deterministic machine, at each point in time, is executing an 
 instruction. Depending on the instruction, it then goes to some next instruction, which is unique. 
 A nondeterministic machine has a choice of next steps. It is free to choose any that it wishes, 
 and if one of these steps leads to a solution, it will always choose the correct one. A 
 nondeterministic machine thus has the power of extremely good (optimal) guessing. This probably 
 seems like a ridiculous model, since nobody could possibly build a nondeterministic computer, and 
 because it would seem to be an incredible upgrade to your standard computer (every problem might 
 now seem trivial). We will see that nondeterminism is a very useful theoretical construct.",NA
9.7.3. NP-Complete Problems,"Among all the problems known to be in NP, there is a subset, known as the NP-complete problems, 
 which contains the hardest. An NP-complete problem has the property that any problem in NP can be 
 polynomially reduced to it. 
  
 A problem P1 can be reduced to P2 as follows: Provide a mapping so that any instance of P1 can be 
 transformed to an instance of P2. Solve P2, and then map the answer back to the original. As an 
 example, numbers are entered into a pocket calculator in decimal. The decimal numbers are 
 converted to binary, and all calculations are performed in binary. Then the final answer is 
 converted back to decimal for display. For P1 to be polynomially reducible to P2, all the work 
 associated with the transformations must be performed in polynomial time. 
  
 The reason that NP-complete problems are the hardest NP problems is that a problem that is NP-
 complete can essentially be used as a subroutine for any problem in NP, with only a polynomial 
 amount of overhead. Thus, if any NP-complete problem has a polynomial-time solution, then every",NA
Summary,"In this chapter we have seen how graphs can be used to model many real-life problems. Many of the 
 graphs that occur are typically very sparse, so it is important to pay attention to the data 
 structures that are used to implement them. 
  
 We have also seen a class of problems that do not seem to have efficient solutions. In 
  
  
 Chapter 
 10, some techniques for dealing with these problems will be discussed.",NA
Exercises,NA,NA
References,"Good graph theory textbooks include 
  
  
 [7], [12], [21], and [34]. More advanced topics, including 
 the more careful attention to running times, are covered in [36], [38], and [45]. 
  
 Use of adjacency lists was advocated in [23]. The topological sort algorithm is from [28], as 
 described in [31]. Dijkstra's algorithm appeared in [8]. The improvements using d-heaps and 
 Fibonacci heaps are described in [27] and [14], respectively. The shortest-path algorithm with 
 negative edge weights is due to Bellman [3]; Tarjan [45] describes a more efficient way to 
 guarantee termination.",NA
CHAPTER 10: ,"Previous Chapter
  
  
  
 Return to Table of Contents",NA
ALGORITHM DESIGN TECHNIQUES ,"So far, we have been concerned with the efficient implementation of algorithms. We 
 have seen that when an algorithm is given, the actual data structures need not be 
 specified. It is up to the programmer to choose the approriate data structure in 
 order to make the running time as small as possible. 
  
 In this chapter, we switch our attention from the implementation of algorithms to 
 the design of algorithms. Most of the algorithms that we have seen so far are 
 straightforward and simple. Chapter 9 contains some algorithms that are much more 
 subtle, and some require an argument (in some cases lengthy) to show that they are 
 indeed correct. In this chapter, we will focus on five of the common types of 
 algorithms used to solve problems. For many problems, it is quite likely that at 
 least one of these methods will work. Specifically, for each type of algorithm we 
 will 
  
  
  
  See the general approach. 
  
  
  Look at several examples (the exercises at the end of the chapter provide 
  
 many more examples). 
  
  
  Discuss, in general terms, the time and space complexity, where appropriate.",NA
10.1. Greedy Algorithms ,"The first type of algorithm we will examine is the greedy algorithm. We have 
 already seen three greedy algorithms in Chapter 9: Dijkstra's, Prim's, and 
 Kruskal's algorithms. Greedy algorithms work in phases. In each phase, a decision 
 is made that appears to be good, without regard for future consequences. 
  
 Generally, this means that some local optimum is chosen. This ""take what you can 
 get now"" strategy is the source of the name for this class of algorithms. When 
 the algorithm terminates, we hope that the local optimum is equal to the global 
 optimum. If this is the case, then the algorithm is correct; otherwise, the 
 algorithm has produced a suboptimal solution. If the absolute best answer is not 
 required, then simple greedy algorithms are sometimes used to generate 
  
 approximate answers, rather than using the more complicated algorithms generally 
 required to generate an exact answer. 
  
 There are several real-life examples of greedy algorithms. The most obvious is 
 the coin-changing problem. To make change in U.S. currency, we repeatedly 
 dispense the largest denomination. Thus, to give out seventeen dollars and sixty-
 one cents in change, we give out a ten-dollar bill, a five-dollar bill, two one-
 dollar bills, two quarters, one dime, and one penny. By doing this, we are 
 guaranteed to minimize the number of bills and coins. This algorithm does not",NA
10.1.1. A Simple Scheduling Problem ,"We are given jobs j
 1
 , j
 2
 , . . . , j
 n
 , all with known running times t
 1
 , t
 2
 , . . . , 
 t
 n
 , respectively. We have a single processor. What is the best way to schedule 
 these jobs in order to minimize the average completion time? In this entire 
 section, we will assume nonpreemptive scheduling: Once a job is started, it must 
 run to completion. 
  
 As an example, suppose we have the four jobs and associated running times shown 
 in Figure 10.1. One possible schedule is shown in Figure 10.2. Because j
 1 
  
 finishes in 15 (time units), j
 2
  in 23, j
 3
  in 26, and j
 4
  in 36, the average 
 completion time is 25. A better schedule, which yields a mean completion time of 
 17.75, is shown in Figure 10.3. 
  
 The schedule given in Figure 10.3 is arranged by shortest job first. We can show 
 that this will always yield an optimal schedule. Let the jobs in the schedule be 
 j
 i1
 , j
 i2
 , . . . , j
 in
 . The first job finishes in time t
 i1
 . The second job finishes 
 after t
 i1
  + t
 i2
 , and the third job finishes after t
 i1
  + t
 i2
  + t
 i3
 . From this, we 
 see that the total cost, C, of the schedule is 
  
  
 (10.1) 
  
  
 (10.2) 
  
 Job  Time",NA
The Multiprocessor Case ,"We can extend this problem to the case of several processors. Again we have jobs 
 j
 1
 , j
 2
 , . . . , j
 n
 , with associated running times t
 1
 , t
 2
 , . . . , t
 n
 , and a number P 
 of processors. We will assume without loss of generality that the jobs are ordered, 
 shortest running time first. As an example, suppose P = 3, and the jobs are as 
 shown in Figure 10.4. 
  
 Figure 10.5 shows an optimal arrangement to minimize mean completion time. Jobs 
 j
 1
 , j
 4
 , and j
 7
  are run on Processor 1. Processor 2 handles 
 j
 2, 
 j
 5
 , and j
 8
 , and 
 Processor 3 runs the remaining jobs. The total time to completion is 165, for an",NA
Minimizing the Final Completion Time ,"We close this section by considering a very similar problem. Suppose we are only 
 concerned with when the last job finishes. In our two examples above, these 
 completion times are 40 and 38, respectively. Figure 10.7 shows that the minimum 
 final completion time is 34, and this clearly cannot be improved, because every 
 processor is always busy. 
  
 Although this schedule does not have minimum mean completion time, it has merit in 
 that the completion time of the entire sequence is earlier. If the same user owns 
 all these jobs, then this is the preferable method of scheduling. Although these 
 problems are very similar, this new problem turns out to be NP-complete; it is just 
 another way of phrasing the knapsack or bin-packing problems, which we will 
 encounter later in this section. Thus, minimizing the final completion time is 
 apparently much harder than minimizing the mean completion time. 
  
  
 Figure 10.6 A second optimal solution for the multiprocessor case 
  
  
 Figure 10.7 Minimizing the final completion time",NA
10.1.2. Huffman Codes ,"In this section, we consider a second application of greedy algorithms, known as 
 file compression. 
  
 The normal 
 ASCII
  character set consists of roughly 100 ""printable"" characters. 
  
 In order to distinguish these characters, 
  
  
 log 100
  
 = 7 bits are required.",NA
Huffman's Algorithm ,"Throughout this section we will assume that the number of characters is C. 
 Huffman's algorithm can be described as follows: We maintain a forest of trees.",NA
10.1.3. Approximate Bin Packing ,"In this section, we will consider some algorithms to solve the bin packing 
 problem. These algorithms will run quickly but will not necessarily produce 
 optimal solutions. We will prove, however, that the solutions that are produced 
 are not too far from optimal. 
  
 We are given n items of sizes s
 1
 , s
 2
 , . . . , s
 n
 . All sizes satisfy 0 < s
 i
  
  
  
  
 1. 
  
 The problem is to pack these items in the fewest number of bins, given that each 
 bin has unit capacity. As an example, Figure 10.20 shows an optimal packing for 
 an item list with sizes 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8. 
  
  
 Figure 10.20 Optimal packing for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 
  
 There are two versions of the bin packing problem. The first version is on-line bin 
 packing. In this version, each item must be placed in a bin before the next item 
 can be processed. The second version is the off-line bin packing problem. In an 
 off-line algorithm, we do not need to do anything until all the input has been 
 read. The distinction between on-line and off-line algorithms was discussed in 
 Section 8.2.",NA
On-line Algorithms ,"The first issue to consider is whether or not an on-line algorithm can actually 
 always give an optimal answer, even if it is allowed unlimited computation. 
 Remember that even though unlimited computation is allowed, an on-line algorithm 
 must place an item before processing the next item and cannot change its 
 decision. 
  
 To show that an on-line algorithm cannot always give an optimal solution, we will 
 give it particularly difficult data to work on. Consider an input sequence I
 1
  of 
  
 m small items of weight 
  
  followed by m large items of weight 
 , 0 < 
  
  
  
 < 0.01. It is clear that these items can be packed in m bins if we place one 
 small item and one large item in each bin. Suppose there were an optimal on-line 
 algorithm A that could perform this packing. Consider the operation of algorithm",NA
Next Fit ,"Probably the simplest algorithm is next fit. When processing any item, we check to 
 see whether it fits in the same bin as the last item. If it does, it is placed 
 there; otherwise, a new bin is created. This algorithm is incredibly simple to 
 implement and runs in linear time. Figure 10.21 shows the packing produced for the 
 same input as Figure 10.20. 
  
 Not only is next fit simple to program, its worst-case behavior is also easy to 
 analyze. 
  
 THEOREM 10.2. 
  
 Let m be the optimal number of bins required to pack a list I of items. Then next 
 fit never uses more than 2m bins. There exist sequences such that next fit uses 2m 
 - 2 bins. 
  
 PROOF: 
  
 Consider any adjacent bins B
 j
  and B
 j + 1.
  The sum of the sizes of all items in B
 j 
 and B
 j + 1
  must be larger than 1, since otherwise all of these items would have 
 been placed in B
 j.
  If we apply this result to all pairs of adjacent bins, we see 
 that at most half of the space is wasted. Thus next fit uses at most twice the 
 number of bins. 
  
 To see that this bound is tight, suppose that the n items have size s
 i
  = 0.5 if i 
 is odd and s
 i
  = 2/n if i is even. Assume n is divisible by 4. The optimal 
 packing, shown in Figure 10.22, consists of n/4 bins, each containing 2 elements 
 of size 0.5, and one bin containing the n/2 elements of size 2/n, for a total of 
 (n/4) + 1. Figure 10.23 shows that next fit uses n/2 bins. Thus, next fit can be 
 forced to use almost twice as many bins as optimal.",NA
First Fit ,"Although next fit has a reasonable performance guarantee, it performs poorly in 
 practice, because it creates new bins when it does not need to. In the sample 
 run, it could have placed the item of size 0.3 in either B
 1
  or B
 2
 , rather than 
 create a new bin. 
  
 The first fit strategy is to scan the bins in order and place the new item in the 
 first bin that is large enough to hold it. Thus, a new bin is created only when the 
 results of previous placements have left no other alternative. Figure 10.24 shows 
 the packing that results from first fit on our standard input. 
  
 A simple method of implementing first fit would process each item by scanning 
 down the list of bins sequentially. This would take O(n
 2
 ). It is possible to 
 implement first fit to run in O(n log n); we leave this as an exercise. 
  
 A moment's thought will convince you that at any point, at most one bin can be 
 more than half empty, since if a second bin were also half empty, its contents 
 would fit into the first bin. Thus, we can immediately conclude that first fit 
 guarantees a solution with at most twice the optimal number of bins.",NA
First Fit ,"Although next fit has a reasonable performance guarantee, it performs poorly in 
 practice, because it creates new bins when it does not need to. In the sample 
 run, it could have placed the item of size 0.3 in either B
 1
  or B
 2
 , rather than 
 create a new bin. 
  
 The first fit strategy is to scan the bins in order and place the new item in the 
 first bin that is large enough to hold it. Thus, a new bin is created only when the 
 results of previous placements have left no other alternative. Figure 10.24 shows 
 the packing that results from first fit on our standard input. 
  
 A simple method of implementing first fit would process each item by scanning 
 down the list of bins sequentially. This would take O(n
 2
 ). It is possible to 
 implement first fit to run in O(n log n); we leave this as an exercise. 
  
 A moment's thought will convince you that at any point, at most one bin can be 
 more than half empty, since if a second bin were also half empty, its contents 
 would fit into the first bin. Thus, we can immediately conclude that first fit 
 guarantees a solution with at most twice the optimal number of bins. 
  
  
 Figure 10.24 First fit for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 
  
 On the other hand, the bad case that we used in the proof of next fit's 
 performance bound does not apply for first fit. Thus, one might wonder if a",NA
10.2. Divide and Conquer ,"Another common technique used to design algorithms is divide and conquer. Divide 
 and conquer algorithms consist of two parts: 
  
 Divide: Smaller problems are solved recursively (except, of course, base cases). 
  
 Conquer: The solution to the original problem is then formed from the solutions 
 to the subproblems. 
  
 Traditionally, routines in which the text contains at least two recursive calls are 
 called divide and conquer algorithms, while routines whose text contains only one 
 recursive call are not. We generally insist that the subproblems be disjoint (that 
 is, essentially nonoverlapping). Let us review some of the recursive algorithms 
 that have been covered in this text. 
  
 We have already seen several divide and conquer algorithms. In Section 2.4.3, we 
 saw an O (n log n) solution to the maximum subsequence sum problem. In Chapter 4, 
 we saw linear-time tree traversal strategies. In Chapter 7, we saw the classic 
 examples of divide and conquer, namely mergesort and quicksort, which have O (n log 
 n) worst-case and average-case bounds, respectively. 
  
 We have also seen several examples of recursive algorithms that probably do not 
 classify as divide and conquer, but merely reduce to a single simpler case. In 
 Section 1.3, we saw a simple routine to print a number. In Chapter 2, we used 
 recursion to perform efficient exponentiation. In Chapter 4, we examined simple 
 search routines for binary search trees. In Section 6.6, we saw simple recursion 
 used to merge leftist heaps. In Section 7.7, an algorithm was given for selection 
 that takes linear average time. The disjoint set find operation was written 
 recursively in Chapter 8. Chapter 9 showed routines to recover the shortest path in 
 Dijkstra's algorithm and other procedures to perform depth-first search in graphs. 
 None of these algorithms are really divide and conquer algorithms, because only one 
 recursive call is performed. 
  
 We have also seen, in Section 2.4, a very bad recursive routine to compute the 
 Fibonacci numbers. This could be called a divide and conquer algorithm, but it is 
 terribly inefficient, because the problem really is not divided at all. 
  
 In this section, we will see more examples of the divide and conquer paradigm. Our 
 first application is a problem in computational geometry. Given n points in a 
 plane, we will show that the closest pair of points can be found in O(n log n) 
 time. The exercises describe some other problems in computational geometry which 
 can be solved by divide and conquer. The remainder of the section shows some 
 extremely interesting, but mostly theoretical, results. We provide an algorithm 
 which solves the selection problem in O(n) worst-case time. We also show that 2 n-
 bit numbers can be multiplied in o(n
 2
 ) operations and that two n x n matrices can 
 be multiplied in o(n
 3
 ) operations. Unfortunately, even though these 
  
 algorithms have better worst-case bounds than the conventional algorithms, none are 
 practical except for very large inputs.",NA
10.2.1. Running Time of Divide and Conquer Algorithms ,"All the efficient divide and conquer algorithms we will see divide the problems 
 into subproblems, each of which is some fraction of the original problem, and then 
 perform some additional work to compute the final answer. As an example, we have 
 seen that mergesort operates on two problems, each of which is half the size of the 
 original, and then uses O(n) additional work. This yields the running time equation 
 (with appropriate initial conditions) 
  
 T(n) = 2T(n/2) + O(n) 
  
 We saw in Chapter 7 that the solution to this equation is O(n log n). The 
 following theorem can be used to determine the running time of most divide and 
 conquer algorithms. 
  
 THEOREM 10.6. 
  
 The solution to the equation T(n) = aT(n/b) + 
  
  
 (n
 k
 ), where a 
  
  
  1 and b > 1, 
  
 is 
  
  
 PROOF: 
  
 Following the analysis of mergesort in Chapter 7, we will assume that n is a 
 power of b; thus, let n = b
 m
 . Then n/b = b
 m-l
  and n
 k
  = (b
 m
 )
 k
  = b
 mk
  = b
 km
  = (b
 k
 )
 m
 . 
  
 Let us assume T(1) = 1, and ignore the constant factor in 
  
  
  
 (n
 k
 ). Then we have 
  
 T(b
 m
 ) = aT(b
 m-l
 )+(b
 k
 )
 m
  
 If we divide through by am, we obtain the equation 
  
  
 (10.3)
  
 We can apply this equation for other values of m, obtaining 
  
  
 (10.4)",NA
10.2.2. Closest-Points Problem,"The input to our first problem is a list P of points in a plane. If pl = (x1, y1) and p2 = (x2, 
 y2), then the Euclidean distance between p
 l
  and p
 2
  is [(x1 - x2)2 + (yl - y2)2]l/2. We are 
 required to find the closest pair of points. It is possible that two points have the same 
 position; in that case that pair is the closest, with distance zero. 
  
 If there are n points, then there are n (n - 1)/2 pairs of distances. We can check all of these, 
 obtaining a very short program, but at the expense of an O(n2) algorithm. Since this approach is 
 just an exhaustive search, we should expect to do better. 
  
 Let us assume that the points have been sorted by x coordinate. At worst, this adds O(n log n) to 
 the final time bound. Since we will show an O(n log n) bound for the entire algorithm, this sort 
 is essentially free, from a complexity standpoint.",NA
10.2.3. The Selection Problem,"The selection problem requires us to find the kth smallest element in a list S of n elements. Of 
  
 particular interest is the special case of finding the median. This occurs when k = 
  
  
 n
 /2
  
  
  
 . 
  
 In Chapters 1, 6, 7 we have seen several solutions to the selection problem. The solution in 
  
 Chapter 7 uses a variation of quicksort and runs in O(n) average time. Indeed, it is described in 
 Hoare's original paper on quicksort. 
  
 Although this algorithm runs in linear average time, it has a worst case of O (n2). Selection can 
 easily be solved in O(n log n) worst-case time by sorting the elements, but for a long time it was 
 unknown whether or not selection could be accomplished in O(n) worst-case time. The 
  
 quickselect algorithm outlined in Section 7.7.6 is quite efficient in practice, so this was mostly 
 a question of theoretical interest. 
  
 Recall that the basic algorithm is a simple recursive strategy. Assuming that n is larger than 
 the cutoff point where elements are simply sorted, an element v, known as the pivot, is chosen. 
  
 The remaining elements are placed into two sets, S1 and S2. S1 contains elements that are 
 guaranteed to be no larger than v, and S2 contains elements that are no smaller than v. Finally, 
  
 if k 
  
  
  |S1|, then the kth smallest element in S can be found by recursively computing the kth 
  
 smallest element in S1. If k = |S1| + 1, then the pivot is the kth smallest element. Otherwise, 
 the kth smallest element in S is the (k - |S1| -1 )st smallest element in S2. The main difference 
 between this algorithm and quicksort is that there is only one subproblem to solve instead of two. 
  
 In order to obtain a linear algorithm, we must ensure that the subproblem is only a fraction of 
 the original and not merely only a few elements smaller than the original. Of course, we can 
 always find such an element if we are willing to spend some time to do so. The difficult problem 
 is that we cannot spend too much time finding the pivot. 
  
 For quicksort, we saw that a good choice for pivot was to pick three elements and use their 
 median. This gives some expectation that the pivot is not too bad, but does not provide a 
 guarantee. We could choose 21 elements at random, sort them in constant time, use the 11th largest 
 as pivot, and get a pivot that is even more likely to be good. However, if these 21 elements were 
 the 21 largest, then the pivot would still be poor. Extending this, we could use up to O (n / log 
 n) elements, sort them using heapsort in O(n) total time, and be almost certain, from a 
 statistical point of view, of obtaining a good pivot. In the worst case, however, this does not 
 work because we might select the O (n / log n) largest elements, and then the pivot would be the 
 [n - O(n / log n)]th largest element, which is not a constant fraction of n. 
  
 The basic idea is still useful. Indeed, we will see that we can use it to improve the expected 
 number of comparisons that quickselect makes. To get a good worst case, however, the key idea is 
 to use one more level of indirection. Instead of finding the median from a sample of random 
 elements, we will find the median from a sample of medians. 
  
 The basic pivot selection algorithm is as follows: 
  
 1. Arrange the n elements into 
  
  
  
 n
 /5
  
  
  
  groups of 5 elements, ignoring the (at most four) 
  
 extra elements.",NA
10.2.4. Theoretical Improvements for Arithmetic Problems,"In this section we describe a divide and conquer algorithm that multiplies two n-digit numbers. 
  
 Our previous model of computation assumed that multiplication was done in constant time, because 
 the numbers were small. For large numbers, this assumption is no longer valid. If we measure 
 multiplication in terms of the size of numbers being multiplied, then the natural multiplication 
 algorithm takes quadratic time. The divide and conquer algorithm runs in subquadratic time. We 
 also present the classic divide and conquer algorithm that multiplies two n by n matrices in 
 subcubic time. 
  
 Multiplying Integers 
  
 Matrix Multiplication 
  
 Multiplying Integers
  
 Suppose we want to multiply two n-digit numbers x and y. If exactly one of x and y is negative, 
 then the answer is negative; otherwise it is positive. Thus, we can perform this check and then 
  
 assume that x, y 
  
  
  0. The algorithm that almost everyone uses when multiplying by hand 
  
 requires 
  
  
  
 (n2) operations, because each digit in x is multiplied by each digit in y. 
  
 If x = 61,438,521 and y = 94,736,407, xy = 5,820,464,730,934,047. Let us break x and y into two 
 halves, consisting of the most significant and least significant digits, respectively. Then xl = 
 6,143, xr = 8,521, yl = 9,473, and yr = 6,407. We also have x = xl104 + xr and y = yl104 + yr. It 
 follows that 
  
 xy = xlyl108 + (xlyr + xryl)104 + xryr
  
 Notice that this equation consists of four multiplications, xlyl, xlyr, xryl, and xryr, which are 
 each half the size of the original problem (n/2 digits). The multiplications by 108 and 104 amount 
 to the placing of zeros. This and the subsequent additions add only O(n) additional work.",NA
10.3. Dynamic Programming,"In the previous section, we have seen that a problem that can be mathematically expressed 
  
 recursively can also be expressed as a recursive algorithm, in many cases yielding a significant 
 performance improvement over a more na
 ï
 ve exhaustive search. 
  
 Any recursive mathematical formula could be directly translated to a recursive algorithm, but the 
 underlying reality is that often the compiler will not do justice to the recursive algorithm, and 
 an inefficient program results. When we suspect that this is likely to be the case, we must 
 provide a little more help to the compiler, by rewriting the recursive algorithm as a 
  
 nonrecursive algorithm that systematically records the answers to the subproblems in a table. One 
 technique that makes use of this approach is known as dynamic programming.",NA
10.3.1. Using a Table Instead of Recursion,"In Chapter 2, we saw that the natural recursive program to compute the Fibonacci numbers is very 
  
 inefficient. Recall that the program shown in Figure 10.40 has a running time T(n) that satisfies 
  
 T(n) 
  
  
  T(n - 1) + T(n - 2). Since T(n) satisfies the same recurrence relation as the Fibonacci 
  
 numbers and has the same initial conditions, T(n) in fact grows at the same rate as the Fibonacci 
 numbers, and is thus exponential. 
  
 On the other hand, since to compute Fn, all that is needed is Fn-1 and Fn-2, we only need to 
 record the two most recently computed Fibonacci numbers. This yields the O(n) algorithm in Figure 
 10.41 
  
 The reason that the recursive algorithm is so slow is because of the algorithm used to simulate 
 recursion. To compute Fn, there is one call to Fn-1 and Fn-2. However, since Fn-1 recursively 
 makes a call to Fn-2 and Fn-3, there are actually two separate calls to compute Fn-2. If one 
 traces out the entire algorithm, then we can see that Fn-3 is computed three times, Fn-4 is 
 computed five times, Fn-5 is computed eight times, and so on. As Figure 10.42 shows, the growth of 
 redundant calculations is explosive. If the compiler's recursion simulation algorithm were able to 
 keep a list of all precomputed values and not make a recursive call for an already solved 
 subproblem, then this exponential explosion would be avoided. This is why the program in Figure 
 10.41 is so much more efficient. calculations is explosive. If the compiler's recursion 
  
 simulation algorithm were able to keep a list of all precomputed values and not make a recursive 
 call for an already solved subproblem, then this exponential explosion would be avoided. This is 
 why the program in Figure 10.41 is so much more efficient. 
  
 /* Compute Fibonacci numbers as described in Chapter 1 */
  
 unsigned int
  
 fib( unsigned int n )
  
 {",NA
10.3.2. Ordering Matrix Multiplications,"Suppose we are given four matrices, A, B, C, and D, of dimensions A = 50 X 10, B = 10 X 40, C = 
  
 40 X 30, and D = 30 X 5. Although matrix multiplication is not commutative, it is associative, 
  
 which means that the matrix product ABCD can be parenthesized, and thus evaluated, in any order. 
  
 The obvious way to multiply two matrices of dimensions p X
 q
  and q X r, respectively, uses pqr 
  
 scalar multiplications. (Using a theoretically superior algorithm such as Strassen''s algorithm 
  
 does not significantly alter the problem we will consider, so we will assume this performance 
  
 bound.) What is the best way to perform the three matrix multiplications required to compute 
  
 ABCD? 
  
 In the case of four matrices, it is simple to solve the problem by exhaustive search, since there 
  
 are only five ways to order the multiplications. We evaluate each case below: 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
10.3.3. Optimal Binary Search Tree,"mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/... 
  
 2006-1-27",NA
10.3.4. All-Pairs Shortest Path,"Our third and final dynamic programming application is an algorithm to compute shortest weighted 
  
 paths between every pair of points in a directed graph G = (V, E). In Chapter 9, we saw an 
 algorithm for the single-source shortest-path problem, which finds the shortest path from some 
  
 arbitrary vertex s to all others. That algorithm (Dijkstra's) runs in O(
  
  
 V
  
  
 2) time on 
  
 dense graphs, but substantially faster on sparse graphs. We will give a short algorithm to solve 
  
 the all-pairs problem for dense graphs. The running time of the algorithm is O(
  
  
 V
  
  
 3), 
  
 which is not an asymptotic improvement over 
  
  
  
 V
  
  
  iterations of Dijkstra's algorithm but 
  
 could be faster on a very dense graph, because its loops are tighter. The algorithm also performs 
 correctly if there are negative edge costs, but no negative-cost cycles; Dijkstra's algorithm 
 fails in this case. 
  
 Let us recall the important details of Dijkstra's algorithm (the reader may wish to review Section 
 9.3). Dijkstra's algorithm starts at a vertex s and works in stages. Each vertex in the graph is 
 eventually selected as an intermediate vertex. If the current selected vertex is v, then 
  
 for each w 
  
  
  V, we set dw = min(dw, dv + cv,w). This formula says that the best distance to w 
  
 (from s) is either the previously known distance to w from s, or the result of going from s to v 
 (optimally) and then directly from v to w. 
  
 Dijkstra's algorithm provides the idea for the dynamic programming algorithm: we select the 
 vertices in sequential order. We will define Dk,i,j to be the weight of the shortest path from vi 
 to vj that uses only v1, v2, . . . ,vk as intermediates. By this definition, D0,i,j = ci,j, where 
  
 ci,j is 
  
  
  if (vi, vj) is not an edge in the graph. Also, by definition, D|V|,i,j is the",NA
10.4. Randomized Algorithms,"Suppose you are a professor who is giving weekly programming assignments. You want to make sure 
  
 that the students are doing their own programs or, at the very least, understand the code they are 
 submitting. One solution is to give a quiz on the day that each program is due. On the other hand, 
 these quizzes take time out of class, so it might only be practical to do this for roughly half of 
 the programs. Your problem is to decide when to give the quizzes. 
  
 Of course, if the quizzes are announced in advance, that could be interpreted as an implicit 
 license to cheat for the 50 percent of the programs that will not get a quiz. One could adopt the 
 unannounced strategy of giving quizzes on alternate programs, but students would figure out the 
 strategy before too long. Another possibility is to give quizzes on what seems like the important 
 programs, but this would likely lead to similar quiz patterns from semester to semester. Student 
 grapevines being what they are, this strategy would probably be worthless after a semester. 
  
 One method that seems to eliminate these problems is to use a coin. A quiz is made for every 
 program (making quizzes is not nearly as time-consuming as grading them), and at the start of 
 class, the professor will flip a coin to decide whether the quiz is to be given. This way, it is",NA
10.4.1. Random Number Generators,"Since our algorithms require random numbers, we must have a method to generate them. Actually, 
  
 true randomness is virtually impossible to do on a computer, since these numbers will depend on 
 the algorithm, and thus cannot possibly be random. Generally, it suffices to produce pseudorandom 
 numbers, which are numbers that appear to be random. Random numbers have many known statistical",NA
10.4.2. Skip Lists,"Our first use of randomization is a data structure that supports both searching and insertion in 
  
 O(log n) expected time. As mentioned in the introduction to this section, this means that the 
  
 running time for each operation on any input sequence has expected value O(log n), where the 
  
 expectation is based on the random number generator. It is possible to add deletion and all the 
  
 operations that involve ordering and obtain expected time bounds that match the average time 
  
 bounds of binary search trees. 
  
 The simplest possible data structure to support searching is the linked list. Figure 10.56 shows 
  
 a simple linked list. The time to perform a search is proportional to the number of nodes that 
  
 have to be examined, which is at most n. 
  
 Figure 10.57 shows a linked list in which every other node has an additional pointer to the node 
  
 two ahead of it in the list. Because of this, at most 
  
  
 n/2
  
  
  + 1 nodes are examined in the 
  
 worst case. 
  
 We can extend this idea and obtain Figure 10.58. Here, every fourth node has a pointer to the 
  
 node four ahead. Only 
  
  
 n/4
  
  
  + 2 nodes are examined. 
  
 The limiting case of this argument is shown in Figure 10.59. Every 2ith node has a pointer to the 
  
 node 2i ahead of it. The total number of pointers has only doubled, but now at most 
  
  
 log n
  
  
  
  nodes are examined during a search. It is not hard to see that the total time spent for a 
  
 search is O(log n), because the search consists of either advancing to a new node or dropping to 
  
 a lower pointer in the same node. Each of these steps consumes at most O(log n) total time during 
  
 a search. Notice that the search in this data structure is essentially a binary search. 
  
  
 Figure 10.56 Simple linked list
  
  
 Figure 10.57 Linked list with pointers to two cells ahead",NA
10.4.3. Primality Testing,"In this section we examine the problem of determining whether or not a large number is prime. As 
  
 was mentioned at the end of Chapter 2, some cryptography schemes depend on the difficulty of 
 factoring a large, 200-digit number into two 100-digit primes. In order to implement this scheme, 
 we need a method of generating these two primes. The problem is of major theoretical interest, 
 because nobody now knows how to test whether a d-digit number n is prime in time polynomial in d. 
  
 For instance, the obvious method of testing for the divisibility by odd numbers from 3 to 
  
 requires roughly 
  divisions, which is about 2d/2. On the other hand, this problem is not 
 thought to be NP-complete; thus, it is one of the few problems on the fringe--its complexity is 
 unknown at the time of this writing. 
  
  
 Figure 10.61 Before and after an insertion
  
 In this chapter, we will give a polynomial-time algorithm that can test for primality. If the 
  
 algorithm declares that the number is not prime, we can be certain that the number is not prime. 
  
 If the algorithm declares that the number is prime, then, with high probability but not 100 
 percent certainty, the number is prime. The error probability does not depend on the particular 
 number that is being tested but instead depends on random choices made by the algorithm. Thus, 
 this algorithm occasionally makes a mistake, but we will see that the error ratio can be made 
 arbitrarily negligible. 
  
 The key to the algorithm is a well-known theorem due to Fermat. 
  
 THEOREM 10.10. 
  
 Fermat's Lesser Theorem: If p is prime, and 0 < a < p, then ap-1
  
  
  
  1(mod p). 
  
 PROOF: 
  
 A proof of this theorem can be found in any textbook on number theory.",NA
10.5. Backtracking Algorithms ,"The last algorithm design technique we will examine is backtracking. In many cases, a 
  
 backtracking algorithm amounts to a clever implementation of exhaustive search, with generally 
 unfavorable performance. This is not always the case, however, and even so, in some cases, the 
 savings over a brute force exhaustive search can be significant. Performance is, of course, 
 relative: An O(n2) algorithm for sorting is pretty bad, but an O(n5) algorithm for the traveling 
 salesman (or any NP-complete) problem would be a landmark result. 
  
 A practical example of a backtracking algorithm is the problem of arranging furniture in a new 
  
 house. There are many possibilities to try, but typically only a few are actually considered. 
 Starting with no arrangement, each piece of furniture is placed in some part of the room. If all 
 the furniture is placed and the owner is happy, then the algorithm terminates. If we reach a point 
 where all subsequent placement of furniture is undesirable, we have to undo the last step and try 
 an alternative. Of course, this might force another undo, and so forth. If we find that we undo 
 all possible first steps, then there is no placement of furniture that is satisfactory. 
  
 Otherwise, we eventually terminate with a satisfactory arrangement. Notice that although this 
 algorithm is essentially brute force, it does not try all possibilities directly. For instance, 
 arrangements that consider placing the sofa in the kitchen are never tried. Many other bad 
 arrangements are discarded early, because an undesirable subset of the arrangement is detected. 
 The elimination of a large group of possibilities in one step is known as pruning. 
  
 We will see two examples of backtracking algorithms. The first is a problem in computational 
  
 geometry. Our second example shows how computers select moves in games, such as chess and 
 checkers.",NA
10.5.1. The Turnpike Reconstruction Problem,"Suppose we are given n points, p1, p2, . . . , pn, located on the x-axis. xi is the x coordinate 
  
 of pi. Let us further assume that x1 = 0 and the points are given from left to right. These n 
 points determine n(n - 1)/2 (not necessarily unique) distances d1, d2, . . . , dn between every 
  
 pair of points of the form | xi - xj | (i 
  
  
  j ). It is clear that if we are given the set of 
  
 points, it is easy to construct the set of distances in O(n2) time. This set will not be sorted, 
 but if we are willing to settle for an O(n2 log n) time bound, the distances can be sorted, too. 
 The turnpike reconstruction problem is to reconstruct a point set from the distances. This finds 
 applications in physics and molecular biology (see the references for pointers to more specific 
 information). The name derives from the analogy of points to turnpike exits on East Coast 
 highways. Just as factoring seems harder than multiplication, the reconstruction problem seems 
 harder than the construction problem. Nobody has been able to give an algorithm that is 
  
 guaranteed to work in polynomial time. The algorithm that we will present seems to run in O(n2log 
 n); no counterexample to this conjecture is known, but it is still just that - a conjecture. 
  
 enum test_result { PROBABLY_PRIME, DEFINITELY_COMPOSITE };
  
 typedef enum test_result test_result;
  
 /* Compute result = ap mod n. */
  
 /* If at any point x2
  
  
  
  1(mod n) is detected with x 
  
  
  
  1, x 
  
  
  
  
  n - 1, */",NA
10.5.2. Games,"As our last application, we will consider the strategy that a computer might use to play a 
  
 strategic game, such as checkers or chess. We will use, as an example, the much simpler game of 
 tic-tac-toe, because it makes the points easier to illustrate. 
  
 Tic-tac-toe is, of course, a draw if both sides play optimally. By performing a careful case-by-
 case analysis, it is not a difficult matter to construct an algorithm that never loses and always 
 wins when presented the opportunity. This can be done, because certain positions are known traps 
 and can be handled by a lookup table. Other strategies, such as taking the center square when it 
 is available, make the analysis simpler. If this is done, then by using a table we can always 
 choose a move based only on the current position. Of course, this strategy requires the 
  
 programmer, and not the computer, to do most of the thinking. 
  
 /* Backtracking algorithm to place the points */
  
 /* x[left]...x[right]. */
  
 /* x[1]...[left-1] and x[right+1]...x[n]
  
 /* are already tentatively placed * /
  
 /* If place returns true,",NA
Summary,"This chapter illustrates five of the most common techniques found in algorithm design. When 
  
 confronted with a problem, it is worthwhile to see if any of these methods apply. A proper choice 
  
 of algorithm, combined with judicious use of data structures, can often lead quickly to efficient 
  
 solutions.",NA
Exercises,"10.1 Show that the greedy algorithm to minimize the mean completion time for multiprocessor job 
  
 scheduling works. 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
References,"The original paper on Huffman codes is [21]. Variations on the algorithm are discussed in [29], 
  
 mk:@MSITStore:K:\Data.Structures.and.Algorithm.Analysis.in.C.chm::/...
  
 2006-1-27",NA
CHAPTER 11: ,Previous Chapter,NA
AMORTIZED ANALYSIS ,"In this chapter, we will analyze the running time for several of the advanced 
 data structures that have been presented in Chapters 4 and 6. In particular, we 
 will consider the worst-case running time for any sequence of m operations. This 
 contrasts with the more typical analysis, in which a worst-case bound is given 
 for any single operation. 
  
 As an example, we have seen that 
 AVL
  trees support the standard tree operations 
 in O(log n) worst-case time per operation. 
 AVL
  trees are somewhat complicated to 
 implement, not only because there are a host of cases, but also because height 
 balance information must be maintained and updated correctly. The reason that 
  
 AVL
  trees are used is that a sequence of 
  
  
 (n) operations on an unbalanced 
  
 search tree could require 
  
  
 (n
 2
 ) time, which would be expensive. For search 
  
 trees, the O(n) worst-case running time of an operation is not the real problem. 
 The major problem is that this could happen repeatedly. Splay trees offer a 
  
 pleasant alternative. Although any operation can still require 
  
  
  
  
 (n) time, this 
  
 degenerate behavior cannot occur repeatedly, and we can prove that any sequence of 
 m operations takes O(m log n) worst-case time (total). Thus, in the long run this 
 data structure behaves as though each operation takes O(log n). We call this an 
 amortized time bound. 
  
 Amortized bounds are weaker than the corresponding worst-case bounds, because there 
 is no guarantee for any single operation. Since this is generally not important, we 
 are willing to sacrifice the bound on a single operation, if we can retain the same 
 bound for the sequence of operations and at the same time simplify the data 
 structure. Amortized bounds are stronger than the equivalent average-case bound. 
 For instance, binary search trees have O (log n) average time per operation, but it 
 is still possible for a sequence of m operations to take O (mn) time. 
  
 Because deriving an amortized bound requires us to look at an entire sequence of 
 operations instead of just one, we expect that the analysis will be more tricky. 
 We will see that this expectation is generally realized. 
  
 In this chapter we shall 
  
  
  
  Analyze the binomial queue operations. 
  
  
  
  Analyze skew heaps.",NA
11.1. An Unrelated Puzzle ,"Consider the following puzzle: Two kittens are placed on opposite ends of a 
 football field, 100 yards apart. They walk towards each other at the speed of ten 
 yards per minute. At the same time, their mother is at one end of the field. She 
 can run at 100 yards per minute. The mother runs from one kitten to the other, 
 making turns with no loss of speed, until the kittens (and thus the mother) meet at 
 midfield. How far does the mother run? 
  
 It is not hard to solve this puzzle with a brute force calculation. We leave the 
 details to you, but one expects that this calculation will involve computing the 
 sum of an infinite geometric series. Although this straightforward calculation 
 will lead to an answer, it turns out that a much simpler solution can be arrived 
 at by introducing an extra variable, namely, time. 
  
 Because the kittens are 100 yards apart and approach each other at a combined 
 velocity of 20 yards per minute, it takes them five minutes to get to midfield. 
 Since the mother runs 100 yards per minute, her total is 500 yards. 
  
 This puzzle illustrates the point that sometimes it is easier to solve a problem 
 indirectly than directly. The amortized analyses that we will perform will use 
 this idea. We will introduce an extra variable, known as the potential, to allow 
 us to prove results that seem very difficult to establish otherwise.",NA
11.2. Binomial Queues ,"The first data structure we will look at is the binomial queue of Chapter 6, 
 which we now review briefly. Recall that a binomial tree B
 0
  is a one-node tree, 
 and for k > 0, the binomial tree B
 k
  is built by melding two binomial trees B
 k-1 
 together. Binomial trees B
 0
  through B
 4
  are shown in Figure 11.1. 
  
  
 Figure 11.1 Binomial trees B
 0
 , B
 1
 , B
 2
 , B
 3
 , and B
 4",NA
11.3. Skew Heaps,The analysis of binomial queues is a fairly easy example of an amortized analysis. We now look at,NA
11.4. Fibonacci Heaps,"In Section 9.3.2, we showed how to use priority queues to improve on the na
 ï
 ve O(|V|2) running 
 time of Dijkstra's shortest-path algorithm. The important observation was that the running time 
 was dominated by|E|decrease_key operations and |V| insert and delete_min operations. These 
 operations take place on a set of size at most |V|. By using a binary heap, all these operations 
 take O(log |V|) time, so the resulting bound for Dijkstra's algorithm can be reduced to O(|E| log 
 |V|). 
  
 In order to lower this time bound, the time required to perform the decrease_key operation must be 
 improved. d-heaps, which were described in Section 6.5, give an O(logd |V|) time bound for the 
 delete_min operation as well as for insert, but an O(d logd |V|) bound for delete_min. By choosing 
 d to balance the costs of |E| decrease_key operations with |V| delete_min operations, and 
 remembering that d must always be at least 2, we see that a good choice for d is 
  
  
  
  
   
 d = max(2,
  
  
  
 E
  
  
 /
  
  
 V
  
  
  
 ).
  
 This improves the time bound for Dijkstra's algorithm to 
  
  
  
  
  
  
   
   
 O(
  
  
 E
  
  
 log(2+
  
  
  
  
  
 E
  
  
  
  
 /
  
  
  
 V
  
  
  
  
  
 )
  
  
 V
  
  
 ).
  
 The Fibonacci heap is a data structure that supports all the basic heap operations in O(1) 
 amortized time, with the exception of delete_min and delete, which take O (log n) amortized time. 
 It immediately follows that the heap operations in Dijkstra's algorithm will require a total of O 
 (|E| + |V| log |V|) time. 
  
 Fibonacci heaps* generalize binomial queues by adding two new concepts: 
  
 *The name comes from a property of this data structure, which we will prove later in the section. 
  
 A different implementation of decrease_key: The method we have seen before is to percolate the 
 element up toward the root. It does not seem reasonable to expect an O(1) amortized bound for 
 this strategy, so a new method is needed. 
  
 Lazy merging: Two heaps are merged only when it is required to do so. This is similar to lazy 
 deletion. For lazy merging, merges are cheap, but because lazy merging does not actually combine 
 trees, the delete_min operation could encounter lots of trees, making that operation expensive. 
  
 Any one delete_min could take linear time, but it is always possible to charge the time to 
 previous merge operations. In particular, an expensive delete_min must have been preceded by a 
 large number of unduly cheap merges, which have been able to store up extra potential.",NA
11.4.1. Cutting Nodes in Leftist Heaps,"In binary heaps, the decrease_key operation is implemented by lowering the value at a node and 
 then percolating it up toward the root until heap order is established. In the worst case, this 
 can take O(log n) time, which is the length of the longest path toward the root in a balanced 
 tree. 
  
 This strategy does not work if the tree that represents the priority queue does not have O(log n)",NA
11.4.2. Lazy Merging for Binomial Queues,"The second idea that is used by Fibonacci heaps is lazy merging. We will apply this idea to 
 binomial queues and show that the amortized time to perform a merge operation (as well as 
 insertion, which is a special case) is O(1). The amortized time for delete_min will still be O 
 (log n). 
  
 The idea is as follows: To merge two binomial queues, merely concatenate the two lists of binomial 
 trees, creating a new binomial queue. This new queue may have several trees of the same size, so 
 it violates the binomial queue property. We will call this a lazy binomial queue in order to 
 maintain consistency. This is a fast operation, which always takes constant (worst-case) time. As 
 before, an insertion is done by creating a one-node binomial queue and merging. The difference is 
 that the merge is lazy. 
  
 The delete_min operation is much more painful, because it is where we finally convert the lazy 
 binomial queue back into a standard binomial queue, but, as we will show, it is still O (log n) 
 amortized time-but not O(log n) worst-case time, as before. To perform a delete_min, we find (and 
 eventually return) the minimum element. As before, we delete it from the queue, making each of its 
 children new trees. We then merge all the trees into a binomial queue by merging two equal-sized 
 trees until it is no longer possible. 
  
 As an example, Figure 11.15 shows a lazy binomial queue. In a lazy binomial queue, there can be 
 more than one tree of the same size. We can tell the size of a tree by examining the root's rank 
 field, which gives the number of children (and thus implicitly the type of tree). To perform the 
 delete_min, we remove the smallest element, as before, and obtain the tree in Figure 11.16. 
  
  
 Figure 11.15 Lazy binomial queue",NA
11.4.3. The Fibonacci Heap Operations,"As we mentioned before, the Fibonacci heap combines the leftist heap decrease_key operation with 
 the lazy binomial queue merge operation. Unfortunately, we cannot use both operations without a",NA
11.4.4. Proof of the Time Bound,"Recall that the reason for marking nodes is that we needed to bound the rank (number of children) 
 r of any node. We will now show that any node with n descendants has rank O(log n).",NA
11.5. Splay Trees,"As a final example, we analyze the running time of splay trees. Recall, from Chapter 4, that 
 after an access of some item x is performed, a splaying step moves x to the root by a series of 
 three operations: zig, zig-zag, and zig-zig. These tree rotations are shown in Figure 11.21. We 
 adopt the convention that if a tree rotation is being performed at node x, then prior to the 
 rotation p is its parent and g is its grandparent (if x is not the child of the root). 
  
 Recall that the time required for any tree operation on node x is proportional to the number of 
 nodes on the path from the root to x. If we count each zig operation as one rotation and each zig-
 zig or zig-zag as two rotations, then the cost of any access is equal to 1 plus the number of 
 rotations.",NA
Summary,"In this chapter, we have seen how an amortized analysis can be used to apportion charges among 
 operations. To perform the analysis, we invent a fictitious potential function. The potential 
 function measures the state of the system. A high-potential data structure is volatile, having 
 been built on relatively cheap operations. When the expensive bill comes for an operation, it is 
 paid for by the savings of previous operations. One can view potential as standing for potential",NA
Exercises,"11.1 When do m consecutive insertions into a binomial queue take less than 2m time units? 
  
 11.2 Suppose a binomial queue of n = 2k - 1 elements is built. Alternately perform m insert and 
 delete_min pairs. Clearly, each operation takes O(log n) time. Why does this not contradict the 
 amortized bound of O(1) for insertion? 
  
 *11.3 Show that the amortized bound of O(log n) for the skew heap operations described in the text 
 cannot be converted to a worst-case bound, by giving a sequence of operations that lead to a 
  
 merge requiring 
  
  
  
 (n) time. 
  
 *11.4 Show how to merge two skew heaps with one top-down pass and reduce the merge cost to O(1) 
 amortized time. 
  
 11.5 Extend skew heaps to support the decrease_key operation in O(log n) amortized time. 
  
 11.6 Implement Fibonacci heaps and compare their performance with binary heaps when used in 
 Dijkstra's algorithm. 
  
 11.7 A standard implementation of Fibonacci heaps requires four pointers per node (parent, child, 
 and two siblings). Show how to reduce the number of pointers, at the cost of at most a constant 
 factor in the running time.",NA
References ,"An excellent survey of amortized analysis is provided in 
  
 [9]. 
  
 Most of the references below duplicate citations in earlier chapters. We cite them again for 
 convenience and completeness. Binomial queues were first described in [10] and analyzed in [1]. 
  
 Solutions to 11.3 and 11.4 appear in [8]. Fibonacci heaps are described in [3]. Exercise 11.9a 
 shows that splay trees are optimal, to within a constant factor of the the best static search 
 trees. 11.9b shows that splay trees are optimal, to within a constant factor of the best optimal 
 search trees. These, as well as two other strong results, are proved in the original splay tree 
 paper [6]. 
  
 The merge operation for splay trees is described in [5]. Exercise 11.12 is solved, with an 
 implicit use of amortization, in [2]. The paper also shows how to merge 2-3 trees efficiently. A 
 solution to 11.13 can be found in [4]. 
  
 Amortized analysis is used in [7] to design an on-line algorithm that processes a series of 
 queries in time only a constant factor larger than any off-line algorithm in its class. 
  
 1. M. R. Brown, ""Implementation and Analysis of Binomial Queue Algorithms,"" SIAM Journal on 
 Computing 7 (1978), 298-319. 
  
 2. M. R. Brown and R. E. Tarjan, ""Design and Analysis of a Data Structure for Representing Sorted 
 Lists,"" SIAM Journal on Computing 9 (1980), 594-614. 
  
 3. M. L. Fredman and R. E. Tarjan, ""Fibonacci Heaps and Their Uses in Improved Network 
 Optimization Algorithms,"" Journal of the ACM 34 (1987), 596-615. 
  
 4. H. Gajewska and R. E. Tarjan, ""Deques with Heap Order,"" Information Processing Letters 22 
 (1986), 197-200. 
  
 5. G. Port and A. Moffat, ""A Fast Algorithm for Melding Splay Trees,"" Proceedings of the First 
 Workshop on Algorithms and Data Structures, 1989, 450-459. 
  
 6. D. D. Sleator and R. E. Tarjan, ""Self-adjusting Binary Search Trees,"" Journal of the ACM 32 
 (1985), 652-686. 
  
 7. D. D. Sleator and R. E. Tarjan, ""Amortized Efficiency of List Update and Paging Rules,"" 
 Communications of the ACM 28 (1985), 202-208. 
  
 8. D. D. Sleator and R. E. Tarjan, ""Self-adjusting Heaps,"" SIAM Journal on Computing 15 (1986), 
 52-69.",NA
