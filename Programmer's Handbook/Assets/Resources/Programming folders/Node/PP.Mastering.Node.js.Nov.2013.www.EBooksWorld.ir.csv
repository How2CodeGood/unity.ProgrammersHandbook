Larger Text,Smaller Text,Symbol
Mastering Node.js,NA,NA
"Expert techniques for building fast servers and scalable, ",NA,NA
real-time network applications with minimal effort,NA,NA
Sandro Pasquali,"BIRMINGHAM - MUMBAI
  
 www.EBooksWorld.ir",NA
Mastering Node.js,"Copyright © 2013 Packt Publishing
  
 All rights reserved. No part of this book may be reproduced, stored in a retrieval 
 system, or transmitted in any form or by any means, without the prior written 
 permission of the publisher, except in the case of brief quotations embedded in 
 critical articles or reviews.
  
 Every effort has been made in the preparation of this book to ensure the accuracy of 
 the information presented. However, the information contained in this book is sold 
 without warranty, either express or implied. Neither the author, nor Packt Publishing 
 and its dealers and distributors, will be held liable for any damages caused or alleged 
 to be caused directly or indirectly by this book.
  
 Packt Publishing has endeavored to provide trademark information about all of the 
 companies and products mentioned in this book by the appropriate use of capitals. 
  
 However, Packt Publishing cannot guarantee the accuracy of this information.
  
 First published: November 2013
  
 Production Reference: 1191113
  
 Published by Packt Publishing Ltd.
  
 Livery Place 
  
 35 Livery Street 
  
 Birmingham B3 2PB, UK.
  
 ISBN 978-1-78216-632-0 
  
 www.packtpub.com
  
 Cover Image by Jarek Blaminsky (
 milak6@wp.pl
 )
  
 www.EBooksWorld.ir",NA
Credits,"Author 
  
 Sandro Pasquali
  
 Reviewers 
  
 Alex Kolundzija 
 Abhijeet Sutar 
 Kevin Faaborg
  
 Acquisition Editors 
 Edward Gordan 
  
 Gregory Wild
  
 Lead Technical Editor 
 Sweny M. Sukumaran
  
 Technical Editors 
 Tanvi Bhatt 
  
 Jalasha D'costa 
  
 Akashdeep Kundu 
 Nikhil Potdukhe 
  
 Tarunveer Shetty 
 Sonali Vernekar
  
 Project Coordinator 
 Kranti Berde
  
 Proofreader 
  
 Amy Johnson
  
 Indexer 
  
 Hemangini Bari
  
 Graphics 
  
 Valentina D'Silva
  
 Disha Haria
  
 Yuvraj Manari
  
 Production Coordinator 
 Kirtee Shingan
  
 Cover Work 
  
 Kirtee Shingan
  
 www.EBooksWorld.ir",NA
About the Author,"Sandro Pasquali
  began writing games on a Commodore PET in grade school, and 
 hasn't looked back. A polyglot programmer, who started with BASIC and assembly, his 
 journey through C, Perl, and PHP led to JavaScript and the browser in 1995. He was 
 immediately hooked on a vision of browsers as the software delivery mechanism of the 
 future. By 1997 he had formed 
 Simple.com
 , a technology company selling the world's 
 first JavaScript-based application development framework, patenting several technologies 
 and techniques that have proven prescient. Node represents for him only the natural next 
 step in an inevitable march towards the day when all software implementations, and 
 software users, are joined within a collaborative information network.
  
 He has led the design of enterprise-grade applications for some of the largest 
 companies in the world, including Nintendo, Major League Baseball, Bang and Olufsen, 
 LimeWire, and others. He has displayed interactive media exhibits during the Venice 
 Biennial, won design awards, built knowledge management tools for research 
 institutes and schools, and has started and run several startups. Always seeking new 
 ways to blend design excellence and technical innovation, he has made significant 
 contributions across all levels of software architecture, from data management and 
 storage tools to innovative user interfaces and frameworks.
  
 He now works to mentor a new generation of developers also bitten by the 
 collaborative software bug, especially the rabid ones.
  
 www.EBooksWorld.ir",NA
Acknowledgments,"Many people are responsible for the writing of this book. The team at Packt is owed many 
 thanks for their diligent editing and guidance, not to mention their patience as my work 
 evolved…slowly. Several dear colleagues and friends contributed ideas, feedback, and 
 support. Heartfelt thanks go out to Kevin Faaborg, Michael Nutt, and Ernie Yu, whose 
 insights regarding technology, software, society, and of course Node.js were invaluable in 
 guiding me through the development of this book, and of my work in general. The 
 reinforcing encouragement of Dre Labre, Stuart McDonald, David Furfero, John 
 Politowski, Andy Ross, Alex Gallafent, Paul Griffin, Diana Barnes-Brown, and the others 
 who listened politely while I thought out loud will remain with me as fond memories of 
 this long process. I thank Joseph Werle for his energy and commitment, which was of 
 great help as I grappled with some of the more obscure nuances of the Node.js platform.
  
 In particular I would like to thank Alexander Kolundzija, whose early advocacy began 
 this process, and who is, as T.S. Eliot once said of Ezra Pound, ""il miglior fabbro"".
  
 The writing of this book kept me away from my family and friends for many days and 
 nights, so I thank them all for putting up with my absences. Most importantly, to my 
 darling wife Elizabeth, who faithfully supported me throughout, I send my love.
  
 www.EBooksWorld.ir",NA
About the Reviewers,"Kevin Faaborg
  is a professional software developer and avid software hobbyist. 
 Along with JavaScript and Node.js, his work and interests include event-driven 
 programming, open source software development, and peer-to-peer technology.
  
 Alex Kolundzija
  is a full stack web developer with over a decade of experience at 
 companies including Google, Meebo, and MLB.com. He's the founder and principal 
 developer of Blend.io, a music collaboration network built with Node.js and a part of the 
 Betaworks Studio of companies.
  
 He has previously reviewed Kito Mann's 
 Java Server Faces in Action
  (Manning). 
  
 Abhijeet Sutar
  is a computer science graduate from Mumbai University. He is a self-
 taught software developer, and enthusiastic about learning new technologies. 
  
 His goto language is Java. He has mainly worked on middleware telephony 
  
 applications for contact centers. He has also successfully implemented a highly available 
 data store with MongoDB NoSQL database for a contact center application. 
  
 He is currently moving onto Node.js platform for development of the next generation 
 Operational Technology (OT). He blogs at 
 http://blog.ajduke.in
 , codes at 
 http://github.com/ajduke
  and tweets via handle 
 @_ajduke
 .
  
 I would like to thank the people at Packt Publishing, Krunal, Sweny, 
 for providing reviewing opportunity for new technology, Node. I also 
 want to thank Kranti for providing the chapters and putting 
 reminders on due date, and promptly providing necessary 
 information.
  
 www.EBooksWorld.ir",NA
www.PacktPub.com,NA,NA
"Support files, eBooks, discount offers and more","You might want to visit 
 www.PacktPub.com
  for support files and downloads related to 
 your book.
  
 Did you know that Packt offers eBook versions of every book published, with PDF and 
 ePub files available? You can upgrade to the eBook version at 
 www.PacktPub.com
  and as 
 a print book customer, you are entitled to a discount on the eBook copy. Get in touch with 
 us at 
 service@packtpub.com
  for more details.
  
 At 
 www.PacktPub.com
 , you can also read a collection of free technical articles, sign up for 
 a range of free newsletters and receive exclusive discounts and offers on Packt books and 
 eBooks.
  
 TM
  
 http://PacktLib.PacktPub.com 
  
 Do you need instant solutions to your IT questions? PacktLib is Packt's online digital 
 book library. Here, you can access, read and search across Packt's entire library of 
 books.",NA
Why Subscribe?,"• 
  
 • 
  
 • 
  
 Fully searchable across every book published by Packt 
 Copy and paste, print and bookmark content 
  
 On demand and accessible via web browser",NA
Free Access for Packt account holders,"If you have an account with Packt at 
 www.PacktPub.com
 , you can use this to access 
 PacktLib today and view nine entirely free books. Simply use your login credentials for 
 immediate access.
  
 www.EBooksWorld.ir",NA
Table of Contents,"Preface 
  
 1
  
 Chapter 1: Understanding the Node Environment 
  
 7
  
 Extending JavaScript 
  
 Events 
  
 Modularity 
  
 The Network 
  
 V8 
  
 Memory and other limits 
  
 Harmony 
  
 The process object 
  
 The Read-Eval-Print Loop and executing a Node program 
 Summary 
  
 9
  
  
 1
 0 
  
 1
 2 
  
 1
 3 
  
 1
 5 
  
 1
 6 
  
 1
 8 
  
 1
 9 
  
 2
 1 
  
 2
 3
  
 Chapter 2: Understanding Asynchronous 
  
 Event-Driven Programming 
  
 25
  
 Broadcasting events 
  
 Collaboration 
  
 Queueing 
  
 Listening for events 
  
 Signals 
  
 Forks 
  
 File events 
  
 Deferred execution 
  
  
 process.nextTick 
  
 Timers 
  
 setTimeout 
  
 setInterval 
  
 unref and ref 
  
 26 
  
 28 
  
 29 
  
 30 
  
 30 
  
 32 
  
 34 
  
 35 
  
 36 
  
 38 
  
 38 
  
 39 
  
 40
  
 www.EBooksWorld.ir",NA
Preface,"The Internet is no longer a collection of static websites to be passively consumed. The 
 browser user has come to expect a much richer, interactive experience. Over the last 
 decade or so, network applications have come to resemble desktop applications. 
  
 Also, recognition of the social characteristics of information has inspired the 
 development of new kinds of interfaces and visualizations modeling dynamic network 
 states, where the user is viewing change over real time rather than fading snapshots 
 trapped in the past.
  
 Even though our expectations for software have changed, the tools available to us as 
 software developers developers have not changed much. Computers are faster, and 
 multicore chip architectures are common. Data storage is cheaper, as is bandwidth. 
  
 Yet we continue to develop with tools designed before billion-user websites and 
 push-button management of cloud-based clusters of virtual machines.
  
 The development of network applications remains an overly expensive and slow 
 process because of this. Developers use different languages, programming styles, 
 complicating code maintenance, debugging, and more. Too regularly, scaling issues 
 arrive too early, overwhelming the ability of what is often a small and inexperienced 
 team. Popular modern software features, such as real-time data, multiplayer games, and 
 collaborative editing spaces, demand systems capable of carrying thousands of 
 simultaneous connections without bending. Yet we remain restricted to frameworks 
 designed to assist us in building CRUD applications binding a single relational database 
 on a single server to a single user running a multipage website 
  
 in a browser on a desktop computer. 
  
 Node helps developers build more resilient network applications at scale. Built on C++ 
 and bundled with Google's V8 engine, Node is fast, and it understands JavaScript. Node 
 has brought together the most popular programming language in the world and the 
 fastest JavaScript compiler around, and has given that team easy access to an 
 operating system through C++ bindings. Node represents a change in how network 
 software is designed and built.
  
 www.EBooksWorld.ir",NA
What this book covers,"Chapter 1
 ,
  Understanding the Node Environment
 , gives a brief description of the 
 particular problems Node attempts to solve, with a focus on how its single-threaded 
 event-loop is designed, implemented, and used. We will also learn about how Google's 
 V8 engine can be configured and managed, as well as best practices when building Node 
 programs.
  
 Chapter 2
 ,
  Understanding Asynchronous Event-Driven Programming
 , digs deep into the 
 fundamental characteristic of Node's design: event-driven, asynchronous 
  
 programming. By the end of this chapter you will understand how events, callbacks, and 
 timers are used in Node, as well as how the event loop works to enable high-speed I/O 
 across filesystems, networks, and processes.
  
 Chapter 3
 ,
  Streaming Data Across Nodes and Clients,
  describes how streams of I/O data 
 are knitted through most network software, emitted by file servers or broadcast in 
 response to an HTTP GET request. Here we learn how Node facilitates the design, 
 implementation, and composition of network software, using examples of HTTP servers, 
 readable and writable file streams, and other I/O focused Node modules and patterns.
  
 Chapter 4
 , 
 Using Node to Access the Filesystem
 , lays out what you need to know when 
 accessing the filesystem with Node, along with techniques for handling file uploads and 
 other networked file operations.
  
 Chapter 5
 ,
  Managing Many Simultaneous Client Connections
 , shows you how Node helps 
 in solving problems accompanying the high volume, high concurrency environments 
 that contemporary, collaborative web applications demand. Through examples, learn 
 how to efficiently track user state, route HTTP requests, handle sessions, and 
 authenticate requests using the Redis database and Express web application framework.
  
 Chapter 6
 , 
 Creating Real-Time Applications
 , explores AJAX, Server-Sent-Events, and the 
 WebSocket protocol, discussing their pros and cons, and how to implement each using 
 Node. We finish the chapter by building a collaborative document editing application.
  
 Chapter 7
 , 
 Utilizing Multiple Processes
 , teaches how to distribute clusters of Node 
 processes across multi-core processors, and other techniques for scaling Node 
 applications. An investigation of the differences between programming in single and 
 multithreaded environments leads to a discussion of how to spawn, fork, and 
 communicate with child processes in Node, and we build an analytics tool that records, 
 and displays, the mouse actions of multiple, simultaneous clients connected through a 
 cluster of web sockets.
  
 [
  2 
 ]
  
 www.EBooksWorld.ir",NA
What you need for this book,"You will need to have some familiarity with JavaScript, and have a copy of Node installed 
 on your development machine or server, Version 0.10.21 or higher. You should know 
 how to install programs on this machine, as you will need to install Redis, along with 
 other libraries, like PhantomJS. Having Git installed, and learning how to clone GitHub 
 repositories, will greatly improve your experience.
  
 You should install RabbitMQ so that you can follow with the examples using message 
 queues. The sections on using NGINX to proxy Node servers will of course require that 
 you can install and use that web server. To build C++ add-ons you will need to install the 
 appropriate compiler on your system.
  
 The examples in this book are built and tested within UNIX-based environments 
 (including Mac OS X), but you should be able to run all Node examples on 
  
 Windows-based operating systems as well. You can obtain installers for your system, and 
 binaries, from 
 http://www.nodejs.org
 .
  
 [
  3 
 ]
  
 www.EBooksWorld.ir",NA
Who this book is for,"This book is for developers who want to build high-capacity network applications, such 
 as social networks, collaborative document editing environments, real time data-driven 
 web interfaces, networked games, and other I/O-heavy software. If you're a client-side 
 JavaScript developer, reading this book will teach you how to become a server-side 
 programmer using a language you already know. If you're a C++ hacker, Node is an 
 open-source project built using that language, offering you an excellent opportunity to 
 make a real impact within a large and growing community, even gaining fame, by 
 helping to develop this exciting new technology.
  
 This book is also for technical managers and others seeking an explanation of the 
 capabilities and design philosophy of Node. The book is filled with examples of how 
 Node solves the problems modern software companies are facing in terms of high-
 concurrency, real-time applications pushing enormous volumes of data through growing 
 networks. Node has already been embraced by the enterprise, and you should consider 
 it for your next project.",NA
Conventions,"In this book, you will find a number of styles of text that distinguish between 
 different kinds of information. Here are some examples of these styles, and an 
 explanation of their meaning.
  
 Code words in text, database table names, folder names, filenames, file extensions, 
 pathnames, dummy URLs, user input, and Twitter handles are shown as follows: ""To 
 import modules into your Node program use the 
 require
  directive.""
  
 A block of code is set as follows:
  
 var EventEmitter = require('events').EventEmitter;
  
 var Counter = function(init) {
  
  this.increment = function() {
  
  init++; 
  
  this.emit('incremented', init);
  
  }
  
 When we wish to draw your attention to a particular part of a code block, the 
 relevant lines or items are set in bold:
  
 var size = process.argv[2];
  
 var totl = process.argv[3] || 100;
  
 var buff = [];
  
 for(var i=0; i < totl; i++) {
  
 [
  4 
 ]
  
 www.EBooksWorld.ir",NA
Reader feedback,"Feedback from our readers is always welcome. Let us know what you think about this 
 book—what you liked or may have disliked. Reader feedback is important for us to 
 develop titles that you really get the most out of.
  
 To send us general feedback, simply send an e-mail to 
 feedback@packtpub.com
 , 
 and mention the book title via the subject of your message.
  
 If there is a topic that you have expertise in and you are interested in either writing or 
 contributing to a book, see our author guide on 
 www.packtpub.com/authors
 .",NA
Customer support,"Now that you are the proud owner of a Packt book, we have a number of things to 
 help you to get the most from your purchase.
  
 [
  5 
 ]
  
 www.EBooksWorld.ir",NA
Errata,"Although we have taken every care to ensure the accuracy of our content, mistakes do 
 happen. If you find a mistake in one of our books—maybe a mistake in the text or the 
 code—we would be grateful if you would report this to us. By doing so, you can save 
 other readers from frustration and help us improve subsequent versions of this book. If 
 you find any errata, please report them by visiting 
  
 http://www.packtpub.com/submit-errata
 , selecting your book, clicking on the 
 erratasubmissionform
  link, and entering the details of your errata. Once your errata 
 are verified, your submission will be accepted and the errata will be uploaded on our 
 website, or added to any list of existing errata, under the Errata section of that title. Any 
 existing errata can be viewed by selecting your title from 
 http://www.packtpub.com/support
 .",NA
Piracy,"Piracy of copyright material on the Internet is an ongoing problem across all media. At 
 Packt, we take the protection of our copyright and licenses very seriously. If you come 
 across any illegal copies of our works, in any form, on the Internet, please provide us 
 with the location address or website name immediately so that we can pursue a remedy.
  
 Please contact us at 
 copyright@packtpub.com
  with a link to the suspected 
 pirated material.
  
 We appreciate your help in protecting our authors, and our ability to bring you 
 valuable content.",NA
Questions,"You can contact us at 
 questions@packtpub.com
  if you are having a problem with any 
 aspect of the book, and we will do our best to address it.
  
 www.EBooksWorld.ir",NA
Understanding the Node ,NA,NA
Environment,"Node's goal is to provide an easy way to build scalable network programs.
  
 — Ryan Dahl, creator of Node.js
  
 The 
 WWW
  (
 World Wide Web
 ) makes it possible for hypermedia objects on the 
 Internet to interconnect, communicating through a standard set of Internet protocols, 
 commonly 
 HTTP
  (
 Hyper Text Transfer Protocol
 ). The growth in the complexity, 
 number, and type of web applications delivering curated collections of these objects 
 through the browser has increased interest in technologies that aid in the construction 
 and management of intricate networked applications. Node is one such technology. 
  
 By mastering Node you are learning how to build the next generation of software.
  
 The hold that any one person has on information is tenuous. Complexity follows 
 scale; confusion follows complexity. As resolution blurs, errors happen.
  
 Similarly, the activity graph describing all expected 
 I/O
  (
 Input/Output
 ) 
  
 interactions an application may potentially form between clients and providers must 
 be carefully planned and managed, lest the capacity of both the system and its creator 
 be overwhelmed. This involves controlling two dimensions 
  
 of information: volume and shape.
  
 As a network application scales, the volume of information it must recognize, 
 organize, and maintain increases. This volume, in terms of I/O streams, memory 
 usage, and 
 CPU
  (
 Central Processing Unit
 ) load, expands as more clients connect, 
 and even as they leave (in terms of persisting user-specific data).
  
 www.EBooksWorld.ir",NA
Extending JavaScript,"When he designed Node, JavaScript was not Ryan Dahl's original language choice. 
  
 Yet, after exploring it, he found a very good modern language without opinions on 
 streams, the filesystem, handling binary objects, processes, networking, and other 
 capabilities one would expect to exist in a system's programming language. JavaScript, 
 strictly limited to the browser, had no use for, and had not implemented, these features.
  
 Dahl was guided by a few rigid principles:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 A Node program/process runs on a single thread, ordering execution 
  
 through an event loop 
  
 Web applications are I/O intensive, so the focus should be on making I/O fast 
 Program flow is always directed through asynchronous callbacks 
  
 Expensive CPU operations should be split off into separate parallel processes, 
 emitting events as results arrive 
  
 Complex programs should be assembled from simpler programs
  
 The general principle is, operations must never block. Node's desire for speed (high 
 concurrency) and efficiency (minimal resource usage) demands the reduction of 
 waste. A waiting process is a wasteful process, especially when waiting for I/O.
  
 JavaScript's asynchronous, event-driven design fits neatly into this model. 
  
 Applications express interest in some future event and are notified when that event 
 occurs. This common JavaScript pattern should be familiar to you:
  
 Window.onload = function() {
  
  // When all requested document resources are loaded, 
  
  // do something with the resulting environment 
  
 }
  
 element.onclick = function() {
  
  // Do something when the user clicks on this element
  
 }
  
 [
  9 
 ]
  
 www.EBooksWorld.ir",NA
Events,"Many of the JavaScript extensions in Node emit events. These events are instances of 
 events.EventEmitter
 . Any object can extend 
 EventEmitter
 , providing the developer 
 with an elegant toolkit for building tight asynchronous interfaces to object methods.
  
 Work through this example demonstrating how to set an 
 EventEmitter
  object as the 
 prototype of a function constructor. As each constructed instance now has the 
 EventEmitter
  object exposed to its prototype chain, 
 this
  provides a natural 
 reference to the event 
 API
  (
 Application Programming Interface
 ). The 
 counter 
 instance methods can therefore emit events, and these can be listened for. Here we 
 emit the latest count whenever the 
 counter.increment
  method is called, and bind a 
 callback to the incremented event, which simply prints the current counter value to 
 the command line:
  
 var EventEmitter = require('events').EventEmitter;
  
 var Counter = function(init) {
  
  this.increment = function() {
  
  init++; 
  
  this.emit('incremented', init);
  
  }
  
 }
  
 Counter.prototype = new EventEmitter();
  
 var counter = new Counter(10);
  
 var callback = function(count) {
  
  console.log(count);
  
 }
  
 counter.addListener('incremented', callback);
  
 counter.increment(); // 11
  
 counter.increment(); // 12
  
 [
  10 
 ]
  
 www.EBooksWorld.ir",NA
Modularity,"In his book 
 The Art of Unix Programming
 , 
 Eric Raymond
  proposed the 
 Rule of 
 Modularity
 :
  
 Developers should build a program out of simple parts connected by well defined 
 interfaces, so problems are local, and parts of the program can be replaced in 
 future versions to support new features. This rule aims to save time on 
 debugging complex code that is complex, long, and unreadable.
  
 This idea of building complex systems out of small pieces, loosely joined is seen in the 
 management theory, theories of government, physical manufacturing, and many other 
 contexts. In terms of software development, it advises developers to contribute only 
 the simplest and most useful component necessary within a larger system. Large 
 systems are hard to reason about, especially if the boundaries of its components are 
 fuzzy.
  
 One of the primary difficulties when constructing scalable JavaScript programs is the 
 lack of a standard interface for assembling a coherent program out of many smaller 
 ones. For example, a typical web application might load dependencies using a sequence 
 of 
 <script>
  tags in the 
 <head>
  section of an HTML document:
  
 <head>
  
 <script src=""fileA.js""></script>
  
 <script src=""fileB.js""></script>
  
 </head>
  
 There are many problems with this sort of solution:
  
 1. All potential dependencies must be declared prior to being needed—
  
 dynamic inclusion requires complicated hacks.
  
 2. The introduced scripts are not forcibly encapsulated—nothing stops code in 
 both files from writing to the same global object. Namespaces can easily 
 collide, making arbitrary injection dangerous.
  
 3. 
 fileA
  cannot address 
 fileB
  as a collection—an addressable context such as 
  
 fileB.method
  isn't available.
  
 4. The 
 <script>
  method itself isn't systematic, precluding the design of useful 
  
 module services, such as dependency awareness or version control.
  
 5. Scripts cannot be easily removed, or overridden.
  
 6. Because of these dangers and difficulties, sharing is not effortless, 
 diminishing opportunities for collaboration in an open ecosystem.
  
 Ambivalently inserting unpredictable code fragments into an application frustrates 
 attempts to predictably shape functionality. What is needed is a standard way to load 
 and share discreet program modules.
  
 [
  12 
 ]
  
 www.EBooksWorld.ir",NA
The Network,"I/O in the browser is mercilessly hobbled, for very good reasons—if the JavaScript on 
 any given website could access your filesystem, or open up network connections to any 
 server, the WWW would be a less fun place.
  
 For Node, I/O is of fundamental importance, and its focus from the start was to 
 simplify the creation of scalable systems with high I/O requirements. It is likely 
 that your first experience with Node was in writing an HTTP server.
  
 [
  13 
 ]
  
 www.EBooksWorld.ir",NA
V8,"V8 is Google's JavaScript engine, written in C++. It compiles and executes JavaScript 
 code inside of a 
 VM
  (
 Virtual Machine
 ). When a webpage loaded into Google Chrome 
 demonstrates some sort of dynamic effect, like automatically updating a list or news 
 feed, you are seeing JavaScript, compiled by V8, at work.
  
 While Node itself will efficiently manage I/O operations, its 
 process
  object refers to 
 the V8 runtime. As such, it is important to understand how to configure the V8 
 environment, especially as your application grows in size.
  
 By typing 
 node -h
  into a console, something like the following will be displayed:
  
  
 We can see how a list of V8 options is accessible via the 
 –-v8-options
  flag.
  
 The list of configuration options for V8 is a long one, so we're not going to cover each 
 option here. As we progress through the book, relevant options will be discussed 
 with more depth. It is nevertheless useful to summarize some of the options 
 provided for managing system limits and memory, as well as those used to configure 
 JavaScript's command set, introducing some of the new features in 
 ES6
  
 (
 EcmaScript6
 ), often referred to as 
 Harmony
 . 
  
 The version of V8 used by your Node installation can be viewed by typing:
  
 node –e ""console.log(process.versions.v8)""
  
 [
  15 
 ]
  
 www.EBooksWorld.ir",NA
Memory and other limits,"One very powerful V8 configuration option is important enough to make it into Node's 
 own collection: 
 --max-stack-size
 . Let's look into some of the new powers a Node 
 developer has been given in being able to configure a specific JavaScript runtime.
  
 Trying to break a system is an excellent way to discover its limits and shape. Let's 
 write a program that will crash V8:
  
 var count = 0;
  
 (function curse() { 
  
  console.log(++count);
  
  curse();
  
 })()
  
 This self-contained, self-executing function will recursively call itself forever, or until it is 
 forced to stop. Each iteration of 
 curse
  adds another frame to the call stack. This 
 uncontrolled growth will eventually cause the JavaScript runtime to collapse, citing a 
 RangeError: Maximum call stack size exceeded
 .
  
 The purpose of 
 --max-stack-size
  should now be clear. The direct V8 option 
 equivalent is 
 –-stack_size
 , which is passed a value, in 
 KB
  (
 Kilobytes
 ), to raise this 
 limit. Experiment with the above program, noting the number of iterations possible at 
 different settings.
  
 While it is likely that hitting this limit represents an incorrectly designed algorithm, 
 being able to expand the width of the operating space available to a Node process adds 
 to the collection of solutions available to developers.
  
 On 32 bit and 64 bit machines V8's memory allocation defaults are, respectively, 700 
 MB and 1400 MB. In newer versions of V8, memory limits on 64 bit systems are no 
 longer set by V8, theoretically indicating no limit. However, the 
 OS
  (
 Operating 
 System
 ) on which Node is running can always limit the amount of memory V8 can 
 take, so the true limit of any given process cannot be generally stated.
  
 V8 makes available the 
 --max_old_space_size
  option, which allows control over the 
 amount of memory available to a process, accepting a value in MB. Should you need to 
 increase memory allocation, simply pass this option the desired value when spawning a 
 Node process.
  
 It is often an excellent strategy to reduce the available memory allocation for a given 
 Node instance, especially when running many instances. As with stack limits, consider 
 whether massive memory needs are better delegated to a dedicated storage layer, such 
 as an in-memory database or similar.
  
 [
  16 
 ]
  
 www.EBooksWorld.ir",NA
Harmony ,"JavaScript has never stopped evolving and it is now experiencing something of a 
 renaissance, helped in no small part by the popularity of Node. The language's next 
 version, named Harmony, introduces some significant new features and concepts.
  
  
 More information on ES6 Harmony can be found at: 
  
  
 http://wiki.ecmascript.org/doku.php?id=harmony:harmony
 .
  
 The available Harmony options are:
  
 Flag
  
 Description
  
 --harmony_typeof
  
 --harmony_scoping
  
 --harmony_modules
  
 --harmony_proxies
  
 --
 harmony_collections--
 harmony
  
 Enable semantics for typeof 
  
 Enable block scoping 
  
 Enable modules (implies block scoping) 
  
 Enable proxies 
  
 Enable collections (sets, maps, and weak maps) 
 Enable all features (except typeof)
  
 It is beyond the scope of this book to discuss these new features in any depth. 
 Nonetheless, it should be stated that this ability to use the very latest JavaScript 
 features, now, through Node, offers the developer a great advantage—there is no 
 browser war on a server to hold back innovation.
  
 For example, ES6's 
 weakMap
  allows the use of non-strings as keys in a HashMap:
  
 ""use strict"" 
  
 let owners = new WeakMap(); 
  
 let task = {
  
  
  title  : ""Big Project"" 
  
 }; 
  
 owners.set(task, 'John'); 
  
 function owner(task) {
  
  
  if(owners.has(task)) {
  
  
  return console.log(owners.get(task));
  
  
  }
  
  
  console.log(""No owner for this task.""); 
  
 } 
  
 owner(task);   // ""John"" 
  
 owner({});   // ""No owner for this task""
  
 As an exercise, the reader might map (fixed) input streams to (variable) output 
 streams in a similar manner.
  
 [
  18 
 ]
  
 www.EBooksWorld.ir",NA
The process object,"By now it should be clear as to how Node is structured, in terms of V8, the event loop, 
 and so forth. We are now going to discuss, in detail, how instructions that you write (a 
 JavaScript program) are compiled by V8 into a list of instructions whose execution 
 context is accessible via the native Node 
 process
  object.
  
 The single thread forming the spine of Node's event loop is V8's event loop. When I/O 
 operations are initiated within this loop they are delegated to 
 libuv
 , which manages 
 the request using its own (multi-threaded, asynchronous) environment. libuv 
 announces the completion of I/O operations, allowing any callbacks waiting on this 
 event to be re-introduced to the main V8 thread for execution:
  
  
 V8
  
  
 delegate /0 to lib uv
   
 Thread Pool
   
 multi threaded async
  
 Function call
  
 callback 
  
 Filesystem
  
 Network
  
  
  
  
 [
  19 
 ]
  
 www.EBooksWorld.ir",NA
The Read-Eval-Print Loop and executing ,NA,NA
a Node program,"Node's 
 REPL
  (
 Read-Eval-Print-Loop
 ) represents the Node shell. To enter the shell 
 prompt, enter Node via your terminal without passing a filename:
  
 > node
  
 You now have access to a running Node process, and may pass JavaScript 
  
 commands to this process. For example, after entering 
 2+2
  the shell would send 
 4
  to 
 stdout
 . Node's REPL is an excellent place to try out, debug, test, or otherwise play with 
 JavaScript code.
  
 [
  21 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"In this chapter we've outlined the key problems Node's designers sought to solve, and 
 how their solution has made the creation of easily scalable, high-concurrency 
 networked systems easier for an open community of developers. We've seen how 
 JavaScript has been given very useful new powers, how its evented model has been 
 extended, and how V8 can be configured to further customize the JavaScript runtime.
  
 Through examples, we've learned how I/O is handled by Node, how to program the 
 REPL, as well as how to manage inputs and outputs to the process object. The goal of 
 demonstrating how Node allows applications to be intelligently constructed out of well-
 formed pieces in a principled way has begun. In the next chapter, we will delve deeper 
 into asynchronous programming, learn how to manage more complex event chains, and 
 develop more powerful programs using Node's model.
  
 [
  23 
 ]
  
 www.EBooksWorld.ir",NA
Understanding Asynchronous ,NA,NA
Event-Driven Programming,"The best way to predict the future is to invent it.
  
 — Alan Kay
  
 Eliminating blocking processes through the use of event-driven, asynchronous I/O is 
 Node's primary organizational principle. We've learned how this design helps 
 developers in shaping information and adding capacity: lightweight, independent, and 
 share-nothing processes communicating through callbacks synchronized within a 
 predictable event loop.
  
 Accompanying the growth in the popularity of Node is a growth in the number of well-
 designed evented systems and applications. For a new technology to be successful, it must 
 eliminate existing problems and/or offer to consumers a better solution at a lower cost in 
 terms of time or effort or price. In its short and fertile lifespan, the Node community has 
 collaboratively proven that this new development model is a viable alternative to existing 
 technologies. The number and quality of Node-based solutions powering enterprise-level 
 applications provide further proof that these new ideas are not only novel, but preferred.
  
 In this chapter we will delve deeper into how Node implements event-driven 
 programming. We will begin by unpacking the ideas and theories that event-driven 
 languages and environments derive from and grapple with, in an effort to clear away 
 misconceptions and encourage mastery. Following this introduction, more detail on how 
 timers, callbacks, I/O events, flow control, and the event loop are implemented and used 
 will be laid out. Theory will be practiced as we build up a simple but exemplary file and 
 data-driven applications, highlighting Node's strengths, and how it is succeeding in its 
 ambition to simplify network application designs.
  
 www.EBooksWorld.ir",NA
Broadcasting events,"It is always good to have an accurate understanding of the total eventual cost of 
 asking for a service to be performed.
  
 I/O is expensive. In the following chart (taken from 
 Ryan Dahl
 's original presentation on 
 Node) we can see how many clock cycles typical system tasks consume. The relative cost 
 of I/O operations is striking.
  
 L1 cache 
  
 L2 cache 
  
 RAM 
  
 Disk 
  
 Network
  
 3 cycles 
  
 14 cycles 
  
 250 cycles 
  
 41,000,000 cycles 
  
 240,000,000 
 cycles
  
 The reasons are clear enough: a disk is a physical device, a spinning metal platter that 
 buses data at a speed that cannot possibly match the speed of an on-chip or near-chip 
 cache moving data between the CPU and RAM (Random Access Memory). 
  
 Similarly, a network is bound by the speed in which data can travel through its 
 connecting ""wires"", modulated by its controllers. Even through fiber optic cables, 
 light itself needs 0.1344 seconds to travel around the world. In a network used by 
 billions of people regularly interacting across great distances, this sort of latency 
 builds up.
  
 In the traditional marketplace described by an application running on a blocking system 
 the purchase of a file operation requires a significant expenditure of resources, as we 
 can see in the preceding table. Primarily this is due to scarcity: a fixed number of 
 processes, or ""units of labor"" are available, each able to handle only a single task, and as 
 the availability of labor decreases, its cost (to the client) increases.
  
 The breakthrough in thinking reflected by Node's design is simple to understand once 
 one recognizes that most worker threads spend their time waiting—for more 
 instructions, a sub-task to complete, and so on. For example, a process assigned to 
 service the command 
 format my hard drive
  will dedicate all of its allotted resources to 
 managing a workflow something like the following:
  
 • 
  
 Communicate to a device driver that a format request has been made
  
 • 
  
 Idle, waiting for an ""unknowable"" length of time
  
 • 
  
 Receive the signal 
 format is complete
  
 • 
  
 Notify the client
  
 • 
  
 Clean up; shut down
  
 [
  26 
 ]
  
 www.EBooksWorld.ir",NA
Collaboration,"The worker flow described in the previous section is an example of a blocking server. 
 Each worker is assigned a task or process, with each process able to accept only one 
 request for work. They'll be blocking other requests, even if idling:
  
 I/O Process
  
 BLOCKED
  
 Process
  
 Process
  
 Process
  
 I/O
  
 BLOCKED
  
 Process
  
 What would be preferable is a collaborative work environment, where workers could 
 be assigned new tasks to do, instead of idling. In order to achieve such a goal what is 
 needed is a virtual switchboard, where requests for services could be dispatched to 
 available workers, and where workers could notify the switchboard of their 
 availability.
  
 One way to achieve this goal would be to maintain the idea of having a pool of 
 available labors, but improving efficiency by delegating tasks to different workers as 
 they come in:
  
 Worker
  
 Client
  
 Dispatcher
  
 Worker
  
 Worker
  
 [
  28 
 ]
  
 www.EBooksWorld.ir",NA
Queueing,"In order to avoid overwhelming anyone, we might add a buffer between the clients and 
 the dispatcher.
  
 This new worker is responsible for managing customer relations. Instead of speaking 
 directly with the dispatcher, the client speaks to the services manager, passing the 
 manager requests, and at some point in the future getting a call that their task has been 
 completed. Requests for work are added to a prioritized work queue (a stack of orders 
 with the most important one on top), and this manager waits for another client to walk 
 through the door. The following figure describes the situations:
  
  
 Client
  
 Client
  
 Client
  
 Dispatcher fetches from
  
 Prioritized queue of
  
 client tasks is
  
 queue and passes back
  
 managed
  
 completed packages
  
  
 Worker
  
 Worker
  
 Worker
  
   
 [
  29 
 ]
  
 www.EBooksWorld.ir",NA
Listening for events,"In the previous chapter we were introduced to the 
 EventEmitter
  interface. This is the 
 primary event interface we will be encountering as we move chapter to chapter, as it 
 provides the prototype class for the many Node objects exposing evented interfaces, 
 such as file and network streams. Various 
 close
 , 
 exit
 , 
 data
 , and other events exposed 
 by different module APIs signal the presence of an 
 EventEmitter 
 interface, and we 
 will be learning about these modules and use cases as we progress.
  
 Instead, the primary purpose of this section is to discuss some lesser-known 
 event 
 sources
 —signals, child process communication, filesystem change events, and 
 deferred execution.",NA
Signals,"In many ways, evented programming is like hardware interrupt programming. 
  
 Interrupts do exactly what their name suggests. They use their ability to interrupt 
 whatever a controller or the CPU or any other device is doing, demanding that their 
 particular need be serviced immediately.
  
 In fact, the Node 
 process
  object exposes standard 
 Portable Operating System 
 Interface
  (
 POSIX
 ) signal names, such that a node process can subscribe to these 
 system events.
  
 A signal is a limited form of inter-process communication used in Unix, Unix-like, 
 and other POSIX-compliant operating systems. It is an asynchronous notification 
 sent to a process or to a specific thread within the same process in order to notify 
 it of an event that occurred.
  
 —http://en.wikipedia.org/wiki/POSIX_signal
  
 [
  30 
 ]
  
 www.EBooksWorld.ir",NA
Forks,"A fundamental part of Node's design is to create or fork processes when parallelizing 
 execution or scaling a system—as opposed to creating a thread pool, for instance. We will 
 be using child processes in various ways throughout this book, and learn how to create 
 and use them. Here the focus will be on understanding how communication events 
 between child processes are to be handled.
  
 To create a child process one need simply call the 
 fork
  method of the 
 child_process 
 module, passing it the name of a program file to execute within the new process:
  
 var cp = require('child_process');
  
 var child = cp.fork(__dirname + '/lovechild.js');
  
 In this way any number of subprocesses can be kept running. Additionally, on 
 multicore machines, forked processes will be distributed (by the OS) to different cores. 
 Spreading node processes across cores (even other machines) and managing IPC is 
 (one) way to scale a Node application in a stable, understandable, and predictable way.
  
 Extending the preceding, we can now have the forking process (parent) send, and 
 listen for, messages from the forked process (child):
  
 child.on('message', function(msg) {
  
  console.log('Child said: ', msg);
  
 });
  
 child.send(""I love you"");
  
 Similarly, the child process (its program is defined in 
 lovechild.js
 ) can send and 
 listen for messages:
  
 // lovechild.js
  
 process.on('message', function(msg) {
  
  console.log('Parent said: ', msg);
  
  process.send(""I love you too"");
  
 });
  
 [
  32 
 ]
  
 www.EBooksWorld.ir",NA
File events,"Most applications make some use of the filesystem, in particular those that function as 
 web services. As well, a professional application will likely log information about usage, 
 cache pre-rendered data views, or make other regular changes to files and directory 
 structures.
  
 Node allows developers to register for notifications on file events through the 
 fs.watch
  method. The 
 watch
  method will broadcast changed events on both files 
 and
  
 directories.
  
 watch
  accepts three arguments, in order:
  
 1. The file or directory path being watched. If the file does not exist an 
  
 ENOENT
  (
 no entity
 ) error will be thrown, so using 
 fs.exists
  at some prior 
 useful point is encouraged.
  
 2. An optional options object:
  
 °
  
 persistent (Boolean): Node keeps processes alive as long as there is 
 ""something to do"". An active file watcher will by default function as a 
 persistence flag to Node. Setting this option to false flags 
 not 
 keeping the 
 general process alive if the watcher is the only activity keeping it 
 running.
  
 3. The listener function, which receives two arguments:
  
 °
  
 The name of the change event (one of 
 rename
  or 
 change
 ).
  
 °
  
 The filename that was changed (important when watching directories).
  
 Some operating systems will 
 not
  return this argument.
  
  
 [
  34 
 ]
  
 www.EBooksWorld.ir",NA
Deferred execution,"One occasionally needs to defer the execution of a function. Traditional JavaScript uses 
 timers for this purpose, the well-known 
 setTimeout
  and 
 setInterval 
 functions. 
 Node introduces another perspective on defers, primarily as means of controlling the 
 order in which a callback executes in relation to I/O events, as well as timer events 
 properly.
  
 We'll learn more about this ordering in the event loop discussion that follows. For now 
 we will examine two types of deferred event sources that give a developer the ability to 
 schedule callback executions to occur either before, or after, the processing of queued 
 I/O events.
  
 [
  35 
 ]
  
 www.EBooksWorld.ir",NA
process.nextTick,"A method of the native Node 
 process
  module, 
 process.nextTick
  is similar to the 
 familiar 
 setTimeout
  method in which it delays execution of its callback function until 
 some point in the future. However, the comparison is not exact; a list of all requested 
 nextTick
  callbacks are placed at the head of the event queue and is processed, in its 
 entirety and in order, 
 before
  I/O or timer events and 
 after
  execution of the current script 
 (the JavaScript code executing synchronously on the V8 thread).
  
 The primary use of 
 nextTick
  in a function is to postpone the broadcast of result 
 events to listeners on the current execution stack until the caller has had an 
 opportunity to register event listeners—to give the currently executing program a 
 chance to bind callbacks to 
 EventEmitter.emit
  events. It may be thought of as a 
 pattern used wherever asynchronous behavior should be emulated. For instance, 
 imagine a lookup system that may either fetch from a cache or pull fresh data from a 
 data store. The cache is fast and doesn't need callbacks, while the data I/O call would 
 need them. The need for callbacks in the second case argues for emulation of the 
 callback behavior with 
 nextTick
  in the first case. This allows a consistent API, 
 improving clarity of implementation without burdening the developer with the 
 responsibility of determining whether or not to use a callback.
  
 The following code seems to set up a simple transaction; when an instance of 
 EventEmitter
  emits a 
 start
  event, log 
 ""Started""
  to the console:
  
 var events = require('events');
  
 function getEmitter() {
  
  var emitter = new events.EventEmitter(); 
 emitter.emit('start');
  
  return emitter; 
  
 }
  
 var myEmitter = getEmitter();
  
 myEmitter.on(""start"", function() 
 {
  
  console.log(""Started""); 
  
 });
  
 However, the expected result will not occur. The event emitter instantiated within 
 getEmitter
  emits 
 ""start""
  previous to being returned, wrong-footing the subsequent 
 assignment of a listener, which arrives a step late, missing the event notification.
  
 To solve this race condition we can use 
 process.nextTick
 :
  
 var events = require('events');
  
 function getEmitter() {
  
 [
  36 
 ]
  
 www.EBooksWorld.ir",NA
setImmediate,"setImmediate
  is technically a member of the class of timers (
 setInterval
 , 
  
 setTimeout
 ). However, there is no sense of time associated with it—there is no 
 number 
 of milliseconds to wait
  argument to be sent. This method is really more of a sister to 
 process.nextTick
 , differing in one very important way; while callbacks queued by 
 nextTick
  will execute 
 before
  I/O and timer events, callbacks queued by 
  
 setImmediate
  will be called 
 after
  I/O events.
  
  
 The naming of these two methods is confusing: 
  
  
 nextTick
  occurs 
 before
 setImmediate
 .
  
 This method does reflect the standard behavior of timers in that its invocation will 
 return an object which can be passed to 
 cancelImmediate
 , cancelling 
 setImmediate 
 in the same way 
 cancelTimeout
  cancels timers set with 
 setTimeout
 .
  
 [
  37 
 ]
  
 www.EBooksWorld.ir",NA
Timers,"Timers are used to schedule events in the future. They are used when one seeks to delay 
 the execution of some block of code until a specified number of milliseconds have 
 passed, to schedule periodic execution of a particular function, or to slot some 
 functionality immediately to the following.
  
 JavaScript provides two asynchronous timers: 
 setInterval()
  and 
 setTimeout()
 .
  
 It is assumed that the reader is fully aware of how to set (and cancel) these timers, so 
 very little time will be spent discussing the syntax. We'll instead focus more on 
 ""gotchas"" and ""less well-known"" details about timeouts and intervals.
  
 The key takeaway will be this: when using timers one should make no assumptions 
 about the amount of 
 actual
  time that will expire before the callback registered for this 
 timer fires, or about the ordering of callbacks. Node timers are 
 not
  interrupts. Timers 
 simply promise to execute as close as possible to the specified time (though never 
 before), beholden, as with every other event source, to event loop scheduling.
  
  
 At least one thing you may not know about timers...
  
  
 We are all familiar with the standard arguments to 
 setTimeout
 : 
  
 a callback function and timeout interval. Did you know that many 
  
 additional arguments are passed to the callback function?
  
 setTimeout(callback, time, [passArg1, passArg2…])",NA
setTimeout,"Timeouts are used to defer the execution of a function until some number of 
 milliseconds into the future:
  
 Consider the following code:
  
 setTimeout(a, 1000);
  
 setTimeout(b, 1001);
  
 One would expect that function 
 b
  would execute after function 
 a
 . However, this 
 cannot be guaranteed—
 a
  may follow 
 b
 , or the other way around.
  
 Now, consider the subtle difference present in the following code snippet:
  
 setTimeout(a, 1000);
  
 setTimeout(b, 1000);
  
 [
  38 
 ]
  
 www.EBooksWorld.ir",NA
setInterval,"One can think of many cases where being able to periodically execute a function would be 
 useful. Polling a data source every few seconds and pushing updates is a common pattern. 
 Running the next step in an animation every few milliseconds is another use case, as is 
 collecting garbage. For these cases 
 setInterval
  is a good tool:
  
 var intervalId = setInterval(function() { ... }, 100);
  
 Every 100 milliseconds the sent callback function will execute, a process that can be 
 cancelled with 
 clearInterval(intervalId)
 .
  
 Unfortunately, as with 
 setTimeout
 , this behavior is not always reliable. Importantly, if a 
 system delay (such as some badly written blocking 
 while
  loop) occupies the event loop 
 for some period of time, intervals set prior and completing within that interim will have 
 their results queued on the stack. When the event loop becomes unblocked and unwinds, 
 all
  the interval callbacks will be fired in sequence, 
  
 essentially immediately, losing any sort of timing delays they intended.
  
 Luckily, unlike browser-based JavaScript, intervals are rather more reliable in Node, 
 generally able to maintain expected periodicity in normal use scenarios.
  
 [
  39 
 ]
  
 www.EBooksWorld.ir",NA
unref and ref,"A Node program does not 
 stay alive
  without a reason to do so. A process will keep running 
 for as long as there are callbacks still waiting to be processed. Once those are cleared, the 
 Node process has nothing left to do, and it will exit.
  
 For example, the following silly code fragment will keep a Node process running 
 forever:
  
 Var intervalId = setInterval(function() {}, 1000);
  
 Even though the set callback function does nothing useful or interesting, it continues to 
 be called—and this is the correct behavior, as an interval should keep running until 
 clearInterval
  is used to stop it.
  
 There are cases of using a timer to do something interesting with external I/O, or some 
 data structure, or a network interface where once those external event sources stop 
 occurring or disappear, the timer itself stops being necessary. Normally one would trap 
 that 
 irrelevant
  state of a timer somewhere else in the program and cancel the timer from 
 there. This can become difficult or even clumsy, as an unnecessary tangling of concerns 
 is now necessary, an added level of complexity.
  
 The 
 unref
  method allows the developer to assert the following instructions: 
 when 
 this timer is the only event source remaining for the event loop to process, go ahead and 
 terminate the process
 .
  
 Let's test this functionality to our previous silly example, which will result in the 
 process terminating rather than running forever:
  
 var intervalId = setInterval(function() {}, 1000);
  
 intervalId.unref();
  
 Note that 
 unref
  is a method of the opaque value returned when starting a timer 
 (which is an object).
  
 Now let's add an external event source, a timer. Once that external source gets 
 cleaned up (in about 100 milliseconds), the process will terminate. We send 
 information to the console to log what is happening:
  
 setTimeout(function() {
  
  console.log(""now stop"");
  
 }, 100);
  
 var intervalId = setInterval(function() {
  
  console.log(""running"")
  
 }, 1);
  
 intervalId.unref();
  
 [
  40 
 ]
  
 www.EBooksWorld.ir",NA
Understanding the event loop,"Node processes JavaScript instructions using a single thread. Within your JavaScript 
 program no two operations will ever execute at exactly the same moment, as might 
 happen in a multithreaded environment. Understanding this fact is essential to 
 understanding how a Node program, or process, is designed and runs.
  
 This does not mean that only one thread is being used on the machine hosting this a 
 Node process. Simply writing a callback does not magically create parallelism! 
  
 Recall 
 Chapter 1
 , 
 Understanding the Node Environment
 , and our discussion about the 
 process
  object—Node's ""single thread"" simplicity is in fact an abstraction created for the 
 benefit of developers. It is nevertheless crucial to remember that there are many threads 
 running in the background managing I/O (and other things), and these threads 
 unpredictably insert instructions, originally packaged as callbacks, into the single 
 JavaScript thread for processing.
  
 Node executes instructions one by one until there are no further instructions to 
 execute, no more input or output to stream, and no further callbacks waiting to be 
 handled.
  
 Even deferred events (such as timeouts) require an eventual interrupt in the event 
 loop to fulfill their promise.
  
 For example, the following 
 while
  loop will never terminate:
  
 var stop = false;
  
 setTimeout(function() {
  
  stop = true;        
  
 }, 1000);
  
 while(stop === false) {}; 
  
 [
  41 
 ]
  
 www.EBooksWorld.ir",NA
Four sources of truth,"We have learned about the four main groups of deferred event sources, whose 
 position and priority on the stack we will now demonstrate:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Execution blocks
 : The blocks of JavaScript code comprising the Node 
 program, being expressions, loops, functions, and so on. This includes 
 EventEmitter
  events emitted within the current execution context.
  
 Timers
 : Callbacks deferred to sometime in the future specified in 
 milliseconds, such as 
 setTimeout
  and 
 setInterval
 .
  
 I/O
 : Prepared callbacks returned to the main thread after being delegated to 
 Node's managed thread pool, such as filesystem calls and network listeners.
  
 Deferred execution blocks
 : Mainly the functions slotted on the stack 
 according to the rules of 
 setImmediate
  and 
 nextTick
 .
  
 We have learned how the deferred execution method 
 setImmediate
  slots its 
 callbacks 
 after
  I/O callbacks in the event queue, and 
 nextTick
  slots its callbacks 
 before
  I/O and timer callbacks.
  
  
 A challenge for the reader
  
  
 After running the following code, what is the expected order of 
  
 logged messages?
  
 [
  42 
 ]
  
 www.EBooksWorld.ir",NA
Callbacks and errors,"Members of the Node community develop new packages and projects every day. 
  
 Because of Node's evented nature, callbacks permeate these codebases. We've 
 considered several of the key ways in which events might be queued, dispatched, and 
 handled through the use of callbacks. Let's spend a little time outlining the best 
 practices, in particular about conventions for designing callbacks and handling errors, 
 and discuss some patterns useful when designing complex chains of events and 
 callbacks.
  
 [
  44 
 ]
  
 www.EBooksWorld.ir",NA
Conventions,"Luckily, Node creators agreed upon sane conventions on how to structure callbacks 
 early on. It is important to follow this tradition. Deviation leads to surprises, sometimes 
 very bad surprises, and in general to do so automatically makes an API awkward, a 
 characteristic other developers will rapidly tire of.
  
 One is either returning a function result by executing a callback, handling the arguments 
 received by a callback, or designing the signature for a callback within your API. 
 Whichever situation is being considered, one should follow the convention relevant to 
 that case:
  
 • 
  
 • 
  
 • 
  
 The first argument returned to a callback function is any error message, 
 preferably in the form of an error object. If no error is to be reported, this slot 
 should contain a 
 null
  value.
  
 When passing a callback to a function it should be assigned the last slot of the 
 function signature. APIs should be consistently designed this way.
  
 Any number of arguments may exist between the error and the callback slots.
  
  
 To create an error object:
  
  
 new Error(""Argument must be a String!"")",NA
Know your errors,"It is excellent that the Node community has automatically adopted a convention that 
 compels developers to be diligent and report errors. However, what does one do with 
 errors once they are received?
  
 It is generally a very good idea to centralize error handling in a program. Often, a custom 
 error handling system will be designed, which may send messages to clients, add to a 
 log, and so on. Sometimes it is best to 
 throw
  errors, halting the process.
  
 Node provides more advanced tools for error handling. In particular, Node's 
 domain
  
 system helps with a problem that evented systems have: how can a stack trace be 
 generated if the full route of a call has been obliterated as it jumped from callback to 
 callback?
  
 The goal of 
 domain
  is simple: fence and label an execution context such that all 
 events that occur within it are identified as such, allowing more informative stack 
 traces. By creating several different domains for each significant segment of your 
 program, a chain of errors can be properly understood.
  
 [
  45 
 ]
  
 www.EBooksWorld.ir",NA
Building pyramids,"Simplifying control flows has been a concern of the Node community since the very 
 beginning of the project. Indeed, this potential criticism was one of the very first 
 anticipated by Ryan Dahl, who discussed it at length during the talk in which he 
 introduced Node to the JavaScript developer community.
  
 Because deferred code execution often requires the nesting of callbacks within 
 callbacks a Node program can sometimes begin to resemble a sideways pyramid, also 
 known as ""The Pyramid of Doom"".
  
 Accordingly, there are several Node packages available which take the problem on, 
 employing strategies as varied as futures, fibers, even C++ modules exposing system 
 threads directly. The reader is encouraged to experiment with these:
  
 Async 
  
 Tame 
  
 Fibers
  
 Promises
  
 https://github.com/caolan/async 
 https://github.com/maxtaco/tamejs 
 https://github.com/laverdet/node-
 fibers 
  
 https://github.com/kriskowal/q
  
 [
  47 
 ]
  
 www.EBooksWorld.ir",NA
Considerations,"Any developer is regularly making decisions with a far-reaching impact. It is very hard 
 to predict all the possible consequences resulting from a new bit of code or a new design 
 theory. For this reason, it may be useful to keep the shape of your code simple, and to 
 force yourself to consistently follow the common practices of other Node developers. 
 These are some guidelines you may find useful, as follows:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Generally, try to aim for shallow code. This type of refactoring is uncommon in 
 non-evented environments—remind yourself of it by regularly 
  
 re-evaluating entry and exit points, and shared functions.
  
 Where possible provide a common context for callback re-entry. Closures are 
 very powerful tools in JavaScript, and by extension, Node. As long as the context 
 frame length of the enclosed callbacks is not excessive.
  
 Name you functions. In addition to being useful in deeply recursive 
 constructs, debugging code is much easier when a stack trace contains 
 distinct function names, as opposed to 
 anonymous
 .
  
 Think hard about priorities. Does the order, in which a given result arrives or a 
 callback is executed, actually matter? Importantly, does it matter in relation to 
 I/O operations? If so, consider 
 nextTick
  and 
 setImmediate
 .
  
 Consider using finite state machines for managing your events. State 
 machines are (surprisingly) under-represented in JavaScript codebases. 
 When a callback re-enters program flow it has likely changed the state of 
 your application, and the issuing of the asynchronous call itself is a likely 
 indicator that state is about to change.
  
 [
  48 
 ]
  
 www.EBooksWorld.ir",NA
Listening for file changes,"Let's apply what we've learned. The goal is to create a server that a client can connect 
 to and receive updates from Twitter. We will first create a process to query Twitter for 
 any messages with the 
 hashtag
  #nodejs, and writes any found messages to a 
 tweets.txt
  file in 140-byte chunks. We will then create a network server that 
 broadcasts these messages to a single client. Those broadcasts will be triggered by 
 write events on the 
 tweets.txt
  file. Whenever a write occurs, 140-byte chunks are 
 asynchronously read from the last known client read pointer. This will happen until 
 we reach the end of the file, broadcasting as we go. Finally, we will create a simple 
 client.html
  page, which asks for, receives, and displays these messages.
  
 While this example is certainly contrived, it demonstrates:
  
 • 
  
 Listening to the filesystem for changes and responding to those events
  
 • 
  
 Using data stream events for reading and writing files
  
 • 
  
 Responding to network events
  
 • 
  
 Using timeouts for polling state
  
 • 
  
 Using a Node server itself as a network event broadcaster
  
 To handle server broadcasting we are going to use the 
 Server Sent Events
  (
 SSE
 ) 
 protocol, a new protocol being standardized as part of HTML5.
  
 We're first going to create a Node server that listens for changes on a file and 
  
 broadcasts any new content to the client. Open your editor and create a file 
 server.js
 :
  
 var fs = require(""fs""); 
  
 var http = require('http');
  
 var theUser = null; 
  
 var userPos = 0; 
  
 var tweetFile = ""tweets.txt"";
  
 We will be accepting a single user connection, whose pointer will be 
 theUser
 . 
 The 
 userPos
  will store the last position this client read from in 
 tweetFile
 :
  
 http.createServer(function(request, response) { 
 response.writeHead(200, {
  
  
  
  'Content-Type': 'text/event-stream',
  
  
  
  'Cache-Control': 'no-cache',
  
  
  
  'Access-Control-Allow-Origin': '*'
  
  });
  
  theUser = response;
  
  response.write(':' + Array(2049).join(' ') + '\n'); 
  
 [
  49 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"Programming with events is not always easy. The control and context switches, defining 
 the paradigm often confound those new to evented systems. This seemingly reckless loss 
 of control and the resulting complexity drives many developers away from these ideas. 
 Students in introductory programming courses normally develop a mindset in which 
 program flow can be dictated, where a program whose execution flow does not proceed 
 sequentially from A to B can bend understanding.
  
 By examining the evolution of the architectural problems Node is now attempting to solve 
 for network applications—in terms of scaling, in terms of code organization, in general 
 terms of data and complexity volume, in terms of state awareness, and in terms of well-
 defined data and process boundaries—we have learned how managing these event 
 queues can be done intelligently. We have seen how different event sources are 
 predictably stacked for an event loop to process, and how far-future events can enter and 
 re-enter contexts using closures and smart callback ordering.
  
 We now have a basic domain understanding of the design and characteristics of Node, in 
 particular how evented programming is done using it. Let's now move into larger, more 
 advanced applications of this knowledge.
  
 [
  53 
 ]
  
 www.EBooksWorld.ir",NA
Streaming Data Across ,NA,NA
Nodes and Clients,"A jug fills drop by drop.
  
 — Buddha
  
 We now have a clearer picture of how the evented, I/O-focused design ethic of Node is 
 reflected across its various module APIs, delivering a consistent and predictable 
 environment for development. In this chapter we will discover how data, of many shapes 
 and sizes, pulled from files or other sources, can be read, written, and manipulated just as 
 easily using Node. Ultimately we will learn how to use Node to develop networked servers 
 with rapid I/O interfaces that support highly concurrent applications sharing real-time 
 data across thousands of clients simultaneously.
  
 Those who work with Internet-based software will have heard about ""Big Data"". The 
 importance of I/O efficiency is not lost on those witnessing this explosive growth in data 
 volume.
  
 Implied by this expansion in data is an increase in the size of individual media objects 
 transmitted over the network. The videos being watched are longer. The images being 
 viewed are bigger. The searches are wider and the result sets are larger.
  
 Additionally, scaled network applications are normally spread across many servers, 
 requiring that the processing of data streams be distributed across processes. 
 Additionally, the funneling of disparate streams into a single custom output stream has 
 become the signature of cloud services and various other APIs.
  
 Anyone who has visited a website specializing in the display of large media objects 
 (such as videos) has likely also experienced latency. In an ideal world, one would make 
 a request to a server for 
 lolcats.mpg
  and would receive this file instantly. 
  
 www.EBooksWorld.ir",NA
Exploring streams,"According to 
 Bjarne Stoustrup
  in his book 
 The C++ Programming Language, Third Edition
 :
  
 Designing and implementing a general input/output facility for a programming 
 language is notoriously difficult... An I/O facility should be easy, convenient, and 
 safe to use; efficient and flexible; and, above all, complete.
  
 It shouldn't surprise anyone that a design team, focused on providing efficient and 
 easy I/O, has delivered such a facility through Node. Through a symmetrical and 
 simple interface, which handles data buffers and stream events so that the 
 implementer does not have to, Node's 
 Stream
  module is the preferred way to 
 manage asynchronous data streams for both internal modules and, hopefully, for the 
 modules developers will create.
  
 A stream in Node is simply a sequence of bytes. At any time, a stream contains a 
 buffer of bytes, and this buffer has a zero or greater length:
  
  
 Because each character in a stream is well defined, and because every type of digital 
 data can be expressed in bytes, any part of a stream can be redirected, or ""piped"", to any 
 other stream, different chunks of the stream can be sent do different handlers, and so 
 on. In this way stream input and output interfaces are both flexible and predictable and 
 can be easily coupled.
  
 [
  57 
 ]
  
 www.EBooksWorld.ir",NA
Implementing readable streams ,"Streams producing data that another process may have an interest in are normally 
 implemented using a 
 Readable
  stream. A 
 Readable
  stream saves the implementer all 
 the work of managing the read queue, handling the emitting of data events, and so on.
  
 To create a 
 Readable
  stream:
  
 var stream = require('stream'); 
  
 var readable = new stream.Readable({
  
  encoding : ""utf8"",
  
  highWaterMark : 16000,
  
  objectMode: true 
  
 });
  
 As previously mentioned, 
 Readable
  is exposed as a base class, which can be 
 initialized through three options:
  
 • 
  
 • 
  
 • 
  
 encoding
 : Decode buffers into the specified encoding, defaulting to UTF-8.
  
 highWaterMark
 : Number of bytes to keep in the internal buffer before 
 ceasing to read from the data source. The default is 16 KB.
  
 objectMode
 : Tell the stream to behave as a stream of objects instead of a 
 stream of bytes, such as a stream of JSON objects instead of the bytes in a 
 file. Default 
 false
 .
  
 In the following example we create a mock 
 Feed
  object whose instances will inherit the 
 Readable
  stream interface. Our implementation need only implement the abstract 
 _read
  method of 
 Readable
 , which will 
 push
  data to a consumer until there is nothing 
 more to push, at which point it triggers the 
 Readable
  stream to emit an ""end"" event by 
 pushing a 
 null
  value:
  
 var Feed = function(channel) {
  
  var readable = new stream.Readable({
  
  
  
  encoding : ""utf8""
  
  });
  
  var news = [
  
  
  
  ""Big Win!"",
  
  
  
  ""Stocks Down!"",
  
  
  
  ""Actor Sad!""
  
  ];
  
  readable._read = function() {
  
  
  
  if(news.length) {
  
  
  
  
  return readable.push(news.shift() + 
 ""\n"");
   
  }
  
  
  
  readable.push(null);
  
  };
  
  return readable; 
  
 }
  
 [
  59 
 ]
  
 www.EBooksWorld.ir",NA
Pushing and pulling,"We have seen how a 
 Readable
  implementation will use 
 push
  to populate the stream 
 buffer for reading. When designing these implementations it is important to consider how 
 volume is managed, at either end of the stream. Pushing more data into a stream than can 
 be read can lead to complications around exceeding available space (memory). At the 
 consumer end it is important to maintain awareness of termination events, and how to 
 deal with pauses in the data stream.
  
 One might compare the behavior of data streams running through a network with that 
 of water running through a hose.
  
 As with water through a hose, if a greater volume of data is being pushed into the read 
 stream than can be efficiently drained out of the stream at the consumer end through 
 read
 , a great deal of back pressure builds, causing a data backlog to begin accumulating 
 in the stream object's buffer. Because we are dealing with strict mathematical 
 limitations, 
 read 
 simply cannot be compelled to release this pressure by reading more 
 quickly—there may be a hard limit on available memory space, or other limitation. As 
 such, memory usage can grow dangerously high, buffers can overflow, and so forth.
  
 [
  61 
 ]
  
 www.EBooksWorld.ir",NA
Writable streams,"A 
 Writable
  stream is responsible for accepting some value (a stream of bytes, a 
 string) and writing that data to a destination. Streaming data into a file container is a 
 common use case.
  
 To create a 
 Writable
  stream:
  
 var stream = require('stream');
  
 var readable = new stream.Writable({
  
  highWaterMark : 16000,
  
  decodeStrings: true
  
 });
  
 The 
 Writable
  streams constructor can be instantiated with two options:
  
 • 
  
 • 
  
 highWaterMark
 : The maximum number of bytes the stream's buffer will 
 accept prior to returning false on writes. Default is 16 KB 
  
 decodeStrings
 : Whether to convert strings into buffers before writing. 
 Default is true.
  
 As with 
 Readable
  streams, custom 
 Writable
  stream implementations must 
 implement a 
 _write
  handler, which will be passed the arguments sent to the 
 write 
 method of instances.
  
 [
  62 
 ]
  
 www.EBooksWorld.ir",NA
Duplex streams,"A duplex stream is both readable and writeable. For instance, a TCP server created in 
 Node exposes a socket that can be both read from and written to:
  
 var stream     = require(""stream"");
  
 var net      = require(""net"");
  
 net
  
 .createServer(function(socket) {
  
  socket.write(""Go ahead and type something!"");
  
  socket.on(""readable"", function() {
  
  process.stdout.write(this.read())
  
  });
  
 })
  
 .listen(8080);
  
 When executed, this code will create a TCP server that can be connected to via Telnet:
  
 telnet 127.0.0.1 8080
  
 Upon connection, the connecting terminal will print out 
 Go ahead and type 
 something!
 —
 writing
  to the socket. Any text entered in the connecting terminal will be 
 echoed to the 
 stdout
  of the terminal running the TCP server (
 reading
  from the socket). 
 This implementation of a bi-directional (duplex) communication protocol demonstrates 
 clearly how independent processes can form the nodes of a complex and responsive 
 application, whether communicating across a network or within the scope of a single 
 process.
  
 The options sent when constructing a 
 Duplex
  instance merge those sent to 
 Readable 
 and 
 Writable
  streams, with no additional parameters. Indeed, this stream type simply 
 assumes both roles, and the rules for interacting with it follow the rules for the 
 interactive mode being used.
  
 As a Duplex stream assumes both read and write roles, any implementation is 
 required to implement both  
 _write
  and 
 _read
  methods, again following the 
 standard implementation details given for the relevant stream type.",NA
Transforming streams,"On occasion stream data needs to be processed, often in cases where one is writing some 
 sort of binary protocol or other ""on the fly"" data transformation. A 
 Transform 
 stream is 
 designed for this purpose, functioning as a 
 Duplex
  stream that sits between a 
 Readable
  
 stream and a 
 Writable
  stream.
  
 [
  65 
 ]
  
 www.EBooksWorld.ir",NA
Using PassThrough streams,"This sort of stream is a trivial implementation of a Transform stream, which simply 
 passes received input bytes through to an output stream. This is useful if one doesn't 
 require any transformation of the input data, and simply wants to easily pipe a 
 Readable
  stream to a 
 Writable
  stream.
  
 [
  66 
 ]
  
 www.EBooksWorld.ir",NA
Creating an HTTP server ,"HTTP
  is a stateless data transfer protocol built upon a request/response model: clients 
 make requests to servers, which then return a response. Facilitating this sort of rapid 
 patter network communication is the sort of I/O Node is designed to excel at, it has 
 unsurprisingly become identified as primarily a toolkit for creating servers—though it 
 can certainly be used to do much, much more. Throughout this book we will be creating 
 many implementations of HTTP servers, as well as other protocol servers, and will be 
 discussing best practices in more depth, contextualized within specific business cases. It 
 is expected that you have already had some experience doing the same. For both of 
 these reasons we will quickly move through a general overview into some more 
 specialized uses.
  
 At its simplest, an HTTP server responds to connection attempts, and manages data 
 as it arrives and as it is sent along. A Node server is typically created using the 
 createServer 
 method of the http module:
  
 var http = require('http'); 
  
 var server = http.createServer(function(request, response) { 
 console.log(""Got Request Headers:"");
  
  console.log(request.headers);
  
  response.writeHead(200, { 
  
  
  
  'Content-Type': 'text/plain'
  
  });
  
  response.write(""PONG"");
  
  response.end(); 
  
 });
  
 server.listen(8080);
  
 [
  67 
 ]
  
 www.EBooksWorld.ir",NA
Making HTTP requests ,"It is often necessary for a network application to make external HTTP calls. HTTP 
 servers are also often called upon to perform HTTP services for clients making 
 requests. Node provides an easy interface for making external HTTP calls.
  
 For example, the following code will fetch the front page of 
 google.com
 :
  
 var http = require('http'); 
  
 http.request({ 
  
  host: 'www.google.com', 
  
  method: 'GET',
  
  path: ""/"" 
  
 }, function(response) {
  
  response.setEncoding(""utf8"");
  
  response.on(""readable"", function() {
  
  
  
  console.log(response.read())
  
  }); 
  
 }).end();
  
 As we can see, we are working with a Readable stream, which can be written to a file. 
  
  
 A popular Node module for managing HTTP requests is 
 Mikeal 
  
  
 Rogers
 ' 
 request
 :
  
 https://github.com/mikeal/request
  
 [
  69 
 ]
  
 www.EBooksWorld.ir",NA
Proxying and tunneling ,"Sometimes it is useful to provide a means for one server to function as a proxy, or 
 broker, for other servers. This would allow one server to distribute load to other 
 servers, for example. Another use would be to provide access to a secured server to 
 users who are unable to connect to that server directly. It is also common to have one 
 server answering for more than one URL—by using a proxy, that one server can forward 
 requests to the right recipient.
  
 Because Node has a consistent streams interface throughout its network interfaces, we 
 can build a simple HTTP proxy in just a few lines of code. For example, the following 
 program will set up an HTTP server on port 8080 which will respond to any request by 
 fetching the front page of Google and piping that back to the client:
  
 var http = require('http'); 
  
 var server = new http.Server(); 
  
 server.on(""request"", function(request, socket) { 
 http.request({ 
  
  
  
  host: 'www.google.com', 
  
  
  
  method: 'GET',
  
  
  
  path: ""/"",
  
  
  
  port: 80
  
  }, function(response) {
  
  
  
  response.pipe(socket);
  
  }).end(); 
  
 }); 
  
 server.listen(8080);
  
 Once this server receives the client socket, it is free to push content from any readable 
 stream back to the client, and here the result of GET of 
 www.google.com
  is so streamed. 
 One can easily see how an external content server managing a caching layer for your 
 application might become a proxy endpoint, for example.
  
 [
  70 
 ]
  
 www.EBooksWorld.ir",NA
"HTTPS, TLS (SSL), and securing your ",NA,NA
server,"The security of web applications has become a significant discussion topic in recent years. 
 Traditional applications normally benefited from the well-tested and mature security 
 models designed into the major servers and application stacks underpinning major 
 deployments. For one reason or another, web applications were allowed to venture into 
 the experimental world of client-side business logic and open web services shielded by a 
 diaphanous curtain.
  
 As Node is regularly deployed as a web server, it is imperative that the community 
 begins to accept responsibility for securing these servers. HTTPS is a secure 
 transmission protocol—essentially encrypted HTTP formed by layering the HTTP 
 protocol on top of the SSL/TLS protocol.",NA
Creating a self-signed certificate for ,NA,NA
development,"In order to support SSL connections a server will need a properly signed certificate. 
 While developing, it is much easier to simply create a self-signed certificate, which will 
 allow one to use Node's HTTPS module.
  
 These are the steps needed to create a certificate for development. Note that this 
 process does not create a real certificate and is 
 not
  secure—it simply allows us to 
 develop within a HTTPS environment. From a terminal:
  
 openssl genrsa -out server-key.pem 2048
  
 openssl req -new -key server-key.pem -out server-csr.pem
  
 openssl x509 -req -in server-csr.pem -signkey server-key.pem -out 
 server-cert.pem
  
 These keys may now be used to develop HTTPS servers. The contents of these files 
 need simply be passed along as options to a Node server:
  
 var https = require('https');
  
 var fs = require('fs');
  
 https.createServer({
  
  key: fs.readFileSync('server-key.pem'),
  
  cert: fs.readFileSync('server-cert.pem')
  
 }, function(req,res) {
  
  ...
  
 }).listen(443)
  
 [
  72 
 ]
  
 www.EBooksWorld.ir",NA
Installing a real SSL certificate,"In order to move a secure application out of a development environment and into an 
 Internet-exposed environment a real certificate will need to be purchased. The prices of 
 these certificates has been dropping year by year, and it should be easy to find reasonably 
 priced providers of certificates with a high-enough level of security. Some providers even 
 offer free person-use certificates.
  
 Setting up a professional cert simply requires changing the HTTPS options we 
 introduced above. Different providers will have different processes and filenames. 
  
 Typically you will need to download or otherwise receive from your provider a private 
 .key
  file, your signed domain certificate 
 .crt
  file, and a bundle describing certificate 
 chains:
  
 var options = {
  
  key        : fs.readFileSync(""mysite.key""),
  
  cert    : fs.readFileSync(""mysite.com.crt""),
  
  ca        : [ fs.readFileSync(""gd_bundle.crt"") ]
  
 };
  
 It is important to note that the 
 ca
  parameter must be sent as an 
 array
 , even if the 
 bundle of certificates has been concatenated into one file.",NA
The request object,"HTTP request and response messages are similar, consisting of:
  
 • 
  
 • 
  
 A status line, which for a request would resemble GET/
 index.html 
 HTTP/1.1, and for a response would resemble HTTP/1.1 200 OK
  
 Zero or more headers, which in a request might include 
 Accept-Charset: 
 UTF-8 or From: user@server.com
 , and in responses might resemble 
  
 Content-Type: text/html and Content-Length: 1024
  
 • 
  
 A message body, which for a response might be an HTML page, and for a 
 POST request might be some form data
  
 We've seen how HTTP server interfaces in Node are expected to expose a request 
 handler, and how this handler will be passed some form of a request and response 
 object, each of which implement a readable or writable stream.
  
 [
  73 
 ]
  
 www.EBooksWorld.ir",NA
The URL module,"Whenever a request is made to an HTTP server the request object will contain url 
 property, identifying the targeted resource. This is accessible via 
 request.url
 . Node's 
 URL module is used to decompose a typical URL string into its constituent parts. 
 Consider the following figure:
  
  
 We see how the 
 url.parse
  method decomposes strings, and the meaning of each 
 segment should be clear. It might also be clear that the 
 query
 field would be more 
 useful if it was itself parsed into Key/Value pairs. This is accomplished by passing 
 true
  as the second argument of to the 
 parse 
 method, which would change the query 
 field value given above into a more useful key/value map:
  
 query: { filter: 'sports', maxresults: '20' }
  
 This is especially useful when parsing GET requests.
  
 There is one final argument for 
 url.parse
  that relates to the difference between 
 these two URLs:
  
 • 
  
 http://www.example.org
  
 • 
  
 //www.example.org
  
 The second URL here is an example of a (relatively unknown) design feature of the 
 HTTP protocol: the protocol-relative URL (technically, a 
 network-path reference
 ), as 
 opposed to the more common 
 absolute
  URL.
  
 [
  74 
 ]
  
 www.EBooksWorld.ir",NA
The Querystring module,"As we saw with the 
 URL
  module, query strings often need to be parsed into a map of 
 key/value pairs. The 
 Querystring
  module will either decompose an existing query 
 string into its parts, or assemble a query string from a map of key/value pairs.
  
 For example, 
 querystring.parse(""foo=bar&bingo=bango"") 
 will return:
  
 {     foo: 'bar', 
  
  bingo: 'bango' }
  
 If our query strings are not formatted using the normal ""&"" separator and ""="" 
 assignment character, the 
 Querystring
  module offers customizable parsing. The 
 second argument to Querystring can be a custom separator string, and the third a 
 custom assignment string. For example, the following will return the same mapping 
 as given previously on a query string with custom formatting:
  
 var qs = require(""querystring"");
  
 console.log(qs.parse(""foo:bar^bingo:bango"", ""^"", "":""))
  
 // { foo: 'bar', bingo: 'bango' }
  
 One can compose a query string using the 
 Querystring.stringify
  method:
  
 console.log(qs.stringify({ foo: 'bar', bingo: 'bango' }))
  
 // foo=bar&bingo=bango
  
 As with 
 parse
 , 
 stringify
  also accepts custom separator and assignment arguments:
  
 console.log(qs.stringify({ foo: 'bar', bingo: 'bango' }, ""^"", 
  
  "":""))
  
 // foo:bar^bingo:bango
  
 Query strings are commonly associated with GET requests, seen following the 
 ?
  
 character. As we've seen above, in these cases automatic parsing of these strings using 
 the url module is the most straightforward solution. However, strings formatted in 
 such a manner also show up when we're handling POST data, and in these cases the 
 Querystring
  module is of real use. We'll discuss this usage shortly. But first, 
 something about HTTP headers.
  
 [
  76 
 ]
  
 www.EBooksWorld.ir",NA
Working with headers,"Each HTTP request made to a Node server will likely contain useful header information, 
 and clients normally expect to receive similar package information from a server. Node 
 provides straightforward interfaces for reading and writing headers. We'll briefly go over 
 those simple interfaces, clarifying some details. Finally, we'll discuss how to more 
 advanced header usage might be implemented in Node, studying some common network 
 responsibilities a Node server will likely need to accommodate.
  
 A typical request header will look something like the following:
  
  
  
  
  
  
 Headers are simple Key/Value pairs. Request keys are always lowercased. You may use 
 any case format when setting response keys.
  
 Reading headers is straightforward. Read header information by examining the 
 request.header
  object, which is a 1:1 mapping of the header's Key/Value pairs. 
  
 To fetch the ""accept"" header from the previous example, simply read 
 request. 
 headers.accept
 .
  
  
 The number of incoming headers can be limited by setting 
  
  
 the 
 maxHeadersCount
  property of your HTTP server.
  
 If it is preferred that headers are read programmatically, Node provides the 
 response.getHeader
  method, accepting the header key as its first argument.
  
 [
  77 
 ]
  
 www.EBooksWorld.ir",NA
Using cookies,"The HTTP protocol is stateless. Any given request has no information on previous 
 requests. For a server this meant that determining if two requests originated from the 
 same browser was not possible. Cookies were invented to solve this problem. Cookies 
 are primarily used to share state between clients (usually a browser) and a server, 
 existing as small text files stored in browsers.
  
 Cookies are insecure. Cookie information flows between a server and a client in plain text. 
 There is any number of tamper points in between. Browsers allow easy access to them, 
 for example. This is a good idea, as nobody wants information on their browser or local 
 machine to be hidden from them, beyond their control.
  
 Nevertheless, cookies are also used rather extensively to maintain state information, or 
 pointers to state information, particularly in the case of user sessions or other 
 authentication scenarios.
  
 [
  78 
 ]
  
 www.EBooksWorld.ir",NA
Understanding content types,"A client will often pass along a request header indicating the expected response 
 MIME 
 (
 Multi-purpose Internet Mail Extension
 ) type. Clients will also indicate the MIME type 
 of a request body. Servers will similarly provide header information about the MIME 
 type of a response body. The MIME type for HTML is 
 text/html
 , for example.
  
 As we have seen, it is the responsibility of an HTTP response to set headers 
  
 describing the entity it contains. Similarly, a GET request will normally indicate the 
 resource type, the MIME type, it expects as a response. Such a request header might look 
 like this:
  
 Accept: text/html
  
 It is the responsibility of a server receiving such instructions to prepare a body entity 
 conforming to the sent MIME type, and if it is able to do so it should return a similar 
 response header:
  
 Content-Type: text/html; charset=utf-8
  
 Because requests also identify the specific resource desired (such as 
 /files/index. 
 html
 ), the server must ensure that the requested resource it is streaming back to the 
 client is in fact of the correct MIME type. While it may seem obvious that a resource 
 identified by the extension 
 html
  is in fact of the MIME type 
 text/html
 , this is not at all 
 certain—a filesystem does nothing to prevent an image file from being given an ""html"" 
 extension. Parsing extensions is an imperfect method of determining file type. We need 
 to do more.
  
 The UNIX 
 file
  program is able to determine the MIME type of a system file. For 
 example, one might determine the MIME type of a file without an extension (for 
 example, 
 resource
 ) by running this command:
  
 file --brief --mime resource
  
 We pass arguments instructing 
 file
  to output the MIME type of resource, and that the 
 output should be brief (only the MIME type, and no other information).
  
 This command might return something like 
 text/plain; charset=us-ascii
 . 
 Here we have a tool to solve our problem.
  
  
 For more information about the 
 file
  utility consult go 
  
  
 to the following link:
  
 http://unixhelp.ed.ac.uk/CGI/man-cgi?file
  
 [
  80 
 ]
  
 www.EBooksWorld.ir",NA
Handling favicon requests,"When visiting a URL via a browser one will often notice a little icon in the browser tab 
 or in the browser's address bar. This icon is an image, named 
 favicon.ico
  and it is 
 fetched on each request. As such, an HTTP GET request is normally combines two 
 requests—one for the favicon, and another for the requested resource.
  
 Node developers are often surprised by this doubled request. Any implementation of an 
 HTTP server must deal with favicon requests. To do so, the server must check the 
 request type, and handle it accordingly. The following example demonstrates one 
 method of doing so:
  
 var http = require('http'); 
  
 http.createServer(function(request, response) { 
  
  if(request.url === '/favicon.ico') {
  
     
  response.writeHead(200, {
  
      
  'Content-Type': 'image/x-icon'
  
     
  });
  
     
  return response.end();
  
  
  }
  
  
  response.writeHead(200, {
  
    
  'Content-Type': 'text/plain'
  
  
  });
  
  
  response.write('Some requested resource');
  
  response.end();
  
 }).listen(8080);
  
 This code will simply send an empty image stream for the favicon. If there is a favicon 
 to send, one would simply push that data through the response stream, as we've 
 discussed previously.
  
 [
  81 
 ]
  
 www.EBooksWorld.ir",NA
Handling POST data ,"One of the most common REST methods used in network applications is POST. 
 According to the REST specification a POST is 
 not
  idempotent, as opposed to most of the 
 other well-known methods (GET, PUT, DELETE, and so on) that are. This is mentioned in 
 order to point out that the handling of POST data will very often have a consequential 
 effect on an application's state, and should therefore 
  
 be handled with care.
  
 We will now discuss handling of the most common type of POST data, that which is 
 submitted via forms. The more complex type of POST—multipart uploads—will be 
 discussed in 
 Chapter 4
 , 
 Using Node to Access the Filesystem
 .
  
 Let's create a server which will return a form to clients, and echo back any data that 
 client submits with that form. We will need to first check the request URL, 
 determining if this is a form request or a form submission, returning HTML for a 
 form in the first case, and parsing submitted data in the second:
  
 var http = require('http'); 
  
 var qs = require('querystring'); 
  
 http.createServer(function(request, response) { var 
 body = """";
  
  if(request.url === ""/"") {
  
  
  
  response.writeHead(200, {
  
  
  
  
  ""Content-Type"": ""text/html""
  
  
  
  });
  
  
  
  return response.end(
  
  
  
  
  '<form action=""/submit"" method=""post"">\
  
  
  
  <input type=""text"" name=""sometext"">\
  
  
  
  <input type=""submit"" value=""Upload"">\
  
  
  
  </form>'
  
  
  
  );
  
  }
  
 Note that the form we respond with has a single field named 
 sometext
 . This form 
 should POST data in the form 
 sometext=entered_text
 :
  
  if(request.url === ""/submit"") {
  
  
  request.on('readable', function() {
  
  
  
  body += request.read();
  
  });
  
  request.on('end', function() {
  
  var fields = qs.parse(body);
  
  response.end(""Thanks!"");
  
 [
  82 
 ]
  
 www.EBooksWorld.ir",NA
Creating and streaming images with ,NA,NA
Node,"Having gone over the main strategies for initiating and diverting streams of 
  
 data, let's practice the theory by creating a service to stream (aptly named) 
 PNG 
 (
 Portable Network Graphics
 ) images to a client. This will not be a simple file server, 
 however. The goal is to create PNG data streams by piping the output stream of an 
 ImageMagick
 convert
  operation executing in a separate process into the response 
 stream of an HTTP connection, where the converter is translating another stream of 
 SVG
  (
 Scalable Vector Graphics
 ) data generated within a virtualized 
 DOM 
 (
 Document 
 Object Model
 ) existing in the Node runtime. Let's get started.
  
  
 The full code for this example can be found in your 
  
  
 code bundle.
  
 Our goal is to use Node to generate pie charts dynamically on a server based on client 
 requests. A client will specify some data values, and a PNG representing that data in a pie 
 will be generated. We are going to use the 
 D3.js
  library, which provides a Javascript API 
 for creating data visualizations, and the 
 jsdom
  NPM package, which allows us to create a 
 virtual DOM within a Node process.
  
 Additionally, the PNG we create will be written to a file. If future requests pass the 
 same query arguments to our service, we will then be able to rapidly pipe the 
 existing rendering immediately, without the overhead of regenerating it.
  
 [
  83 
 ]
  
 www.EBooksWorld.ir",NA
"Creating, caching, and sending a PNG ",NA,NA
representation,"Assuming we do not have a cached copy, we will need to create one. Our system works 
 by creating a virtual DOM, using D3 to generate an SVG pie chart within that DOM, 
 passing the generated SVG to the ImageMagick 
 convert
  program, which converts SVG 
 data into a PNG representation. Additionally, we will need to store the created PNG on a 
 filesystem.
  
 We use 
 jsdom
  to create a DOM, and use D3 to create the pie chart. Visit 
  
 https://github.com/tmpvar/jsdom
  to learn about how jsdom works, and 
 http://d3js.org/
  to learn about using D3 to generate SVG. Assume that we have 
 created this SVG, and stored it in a variable 
 svg
 , which will contain a string similar to this:
  
 <svg width=""200"" height=""200"">
  
  <g transform=""translate(100,100)"">
  
  <defs>
  
  
  <radialgradient id=""grad-0"" gradientUnits=""userSpaceOnUse"" 
 cx=""0"" cy=""0"" r=""100"">
  
  <stop offset=""0"" stop-color=""#7db9e8""></stop>
  
 ...
  
 [
  84 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"As we have learned, Node's designers have succeeded in creating a simple, 
  
 predictable, and convenient solution to the very difficult problem of enabling efficient 
 I/O between disparate sources and targets. Its abstract 
 Stream
  interface facilitates the 
 instantiation of consistent readable and writable interfaces, and the extension of this 
 interface into HTTP requests and responses, the filesystem, child processes, and other 
 data channels makes stream programming with Node a pleasant experience.
  
 Now that we've learned how to set up HTTP servers to handle streams of data arriving 
 from many simultaneously connected clients, and how to feed those clients buffets of 
 buffered streams, we can begin to engage more deeply with the task of building 
 enterprise-grade concurrent real-time systems with Node.
  
 [
  87 
 ]
  
 www.EBooksWorld.ir",NA
Using Node to Access the ,NA,NA
Filesystem,"We have persistent objects — they're called files.
  
 — 
 Ken Thompson
  
 A file is simply a chunk of data that is persisted, usually, on some hard medium such as a 
 hard drive. Files are normally composed of a sequence of bytes whose encoding maps 
 onto some other pattern, like a sequence of numbers or electrical pulses. A nearly 
 infinite number of encodings are possible, with some common ones being text files, 
 image files, and music files. Files have a fixed length, and to be read their character 
 encoding must be deciphered by some sort of reader, like an MP3 player or a word 
 processor.
  
 When a file is in transit, moving through a cable after it's been siphoned off of some 
 storage device, it is no different than any other data stream running through the wire. 
 Its previous solid state is just a stable blueprint that can be easily and infinitely copied, 
 sliced up, and reworked.
  
 We've seen how evented streams reflect the core design principles informing Node's 
 design, where byte streams are to be read from and written to, and piped into other 
 streams, emitting relevant stream events, such as end. Files are easily understood as 
 being containers of data, filled with bytes that can be extracted or inserted partially or 
 as a whole.
  
 In addition to their natural similarity to streams, files also display the characteristics of 
 objects. Files have properties that describe the interface available for accessing file 
 contents—data structures with properties and associated access methods.
  
 www.EBooksWorld.ir",NA
"Directories, and iterating over files ",NA,NA
and folders,"Improving Node's performance with regards to the filesystem has been one of the 
 main challenges taken up by the core Node team. For example, a (flawed) negative 
 comparison to the Vert.x environment's file serving speed led to some interesting 
 commentary from developers and users of both systems which, while not all 
 constructive, is a good read if one is interested in how filesystems are accessed by 
 Node and other systems: 
 http://vertxproject.wordpress.com/2012/05/09/ 
 vert-x-vs-node-js-simple-http-benchmarks/
 .
  
 [
  90 
 ]
  
 www.EBooksWorld.ir",NA
Types of files,"There are six types of files commonly encountered on a UNIX system:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 Ordinary files
 : These contain a one-dimensional array of bytes, and cannot 
 contain other files.
  
 Directories
 : These are also files, implemented in a special way such that they 
 can describe collections of other files.
  
 Sockets
 : Used for IPC, allowing processes to exchange data.
  
 Named pipe
 : A command such as 
 psaux|grepnode
  creates a pipe, which 
 is destroyed once the operation terminates. Named pipes are persistent 
 and addressable, and can be used variously by multiple processes for IPC 
 indefinitely.
  
 Device files
 : These are representations of I/O devices, processes that accept 
 streams of data. 
 /dev/null
  is commonly an example of a character device file 
 (accepts serial streams of I/O), and 
 /dev/sda
  is an example of a block device 
 file (allowing random access I/O for blocks of data) representing a data drive.
  
 Links
 : These are pointers to other files of two types, hard links and symbolic 
 links. Hard links directly point to another file, and are indistinguishable from the 
 target file. Symbolic links are indirect pointers, and are distinguishable from 
 normal files.
  
 Most Node filesystem interactions encounter only the first two types, with the third only 
 indirectly via the Node API. A deeper explanation of the remaining types is beyond the 
 scope of this discussion. However, Node provides a full suite of file operations via the 
 file
  module, and the reader should have at least some familiarity with the full range and 
 power of file types.
  
 [
  91 
 ]
  
 www.EBooksWorld.ir",NA
File paths,"Most of the filesystem methods provided by Node will require the manipulation of file 
 paths, and for this purpose we make use of the 
 path
  module. We can compose, 
 decompose, and relate paths with this module. Instead of hand rolling your own path 
 string splitting and regexing and concatenating routines, try to normalize your code by 
 delegating path manipulation to this module.
  
 • 
  
 Use 
 path.normalize
  whenever working with a file path string whose source is 
 untrusted or unreliable, to ensure a predictable format:
  
 var path = require('path');
  
 path.normalize(""../one////two/./three.html"");
  
 // -> ../one/two/three.html
  
 • 
  
 Use 
 path.join
  whenever building a single path out of path segments:
  
 path.join(""../"",""one"",""two"",""three.html"");
  
 // -> ../one/two/three.html
  
 [
  92 
 ]
  
 www.EBooksWorld.ir",NA
File attributes ,"A file object exposes some of its attributes, comprising a useful set of metadata about the 
 file data. If one is using Node to run an HTTP server it will be necessary to determine the 
 file length of any file requested via a GET, for example. Determining the time a file was last 
 modified finds uses across many types of applications.
  
 To read the attributes of a file, use 
 fs.stat
 :
  
 fs.stat(""file.txt"", function(err, stats) {
  
  
  console.log(stats); 
  
 });
  
 In the preceding example, 
 stats
  will be an 
 fs.Stats
  object describing the file 
 through a map of attributes:
  
  dev: 2051, // id of device containing this file
  
  mode: 33188, // bitmask, status of the file
  
  nlink: 1, // number of hard links
  
  uid: 0, // user id of file owner
  
  gid: 0, // group id of file owner
  
  rdev: 0, // device id (if device file)
  
  blksize: 4096, // I/O block size
  
  ino: 27396003, // a unique file inode number
  
  size: 2000736, // size in bytes
  
  blocks: 3920, // number of blocks allocated
  
  atime: Fri May 3 2013 15:39:57 GMT-0500 (CDT), // last access
  
  mtime: Fri May 3 2013 17:22:46 GMT-0500 (CDT), // last modified 
 ctime: Fri May 3 2013 17:22:46 GMT-0500 (CDT)  // last status change
  
 An 
 fs.Stats
  object exposes several useful methods for accessing file attribute data:
  
 • 
  
 Use 
 stats.isFile
  to check for standard files
  
 • 
  
 Use 
 stats.isDirectory
  to check for directories
  
 • 
  
 Use 
 stats.isBlockDevice
  to check for block type device files
  
 • 
  
 Use 
 stats.isCharacterDevice
  to check for character type device files
  
 [
  94 
 ]
  
 www.EBooksWorld.ir",NA
Opening and closing files,"One of the unofficial rules governing the Node project is to not unnecessarily abstract 
 away from existing OS implementation details. As we will see, references to file 
 descriptors appear throughout Node's file API. For 
 POSIX
  (
 Portable Operating System 
 Interface
 ), a file descriptor is simply an (non-negative) integer uniquely referencing a 
 specific file. Since Node modeled its filesystem methods on POSIX, not surprisingly a file 
 descriptor is represented in Node as an integer.
  
 Recalling our discussion of how devices and other elements of the OS are 
 represented as files, it would stand to reason that the standard I/O streams (
 stdin
 , 
 stdout
 , 
 stderr
 ) would also have file descriptors. In fact, that is the case:
  
 console.log(process.stdin.fd); // 0
  
 console.log(process.stdout.fd); // 1
  
 console.log(process.stderr.fd); // 2
  
 [
  95 
 ]",NA
"fs.open(path, flags, [mode], callback) ","Trying to open a file at 
 path
 . 
 callback
  will receive any exceptions with the 
 operation as its first argument, and a file descriptor as its second argument. Here we 
 open a file for reading:
  
 fs.open(""path.js"", ""r"", function(err, fileDescriptor) {
  
  
  console.log(fileDescriptor); // An integer, like `7` or 
 `23` });
  
 flags
  receives a string indicating the types of operations the caller expects to 
 perform on the returned file descriptor. Their meanings should be clear:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 r
 : Opening file for reading, throwing an exception if the file doesn't exist. 
 r+
 : 
 Opening file for both reading and writing, throwing an exception if the file 
 doesn't exist.
  
 w
 : Opening file for writing, creating the file if it doesn't exist, and truncating the 
 file to zero bytes if it does exist.
  
 wx
 : Like 
 w
 , but opens the file in exclusive mode, which means if the file already 
 exists it will 
 not be opened
 , and the open operation will fail. This is useful if 
 multiple processes may be simultaneously trying to create the same file.
  
 w+
 : Opening file for reading and writing, creating the file if it doesn't exist, and 
 truncating the file to zero bytes if it does exist.
  
 wx+
 : Like 
 wx 
 (and
  w
 ), additionally opening the file for reading.
  
 a
 : Opening file for appending, creating the file if it does not exist.
  
 ax
 : Like 
 a
 , but opens the file in exclusive mode, which means if the file already 
 exists it will 
 not be opened
 , and the open operation will fail. This is useful if 
 multiple processes may be simultaneously trying to create the same file. 
 a+
 : 
 Open file for reading and appending, creating the file if it does not exist. 
 ax+
 :Like 
 ax 
 (and
  a
 ), additionally opening the file for reading.
  
 [
  96 
 ]
  
 www.EBooksWorld.ir",NA
"fs.close(fd, callback)","The 
 fs.close(fd, callback)
  method closes a file descriptor. The callback receives 
 one argument, any exception thrown in the call. It's a good habit to close all file 
 descriptors that have been opened.",NA
File operations,"Node implements the standard POSIX functions for working with files, which a UNIX 
 user will be familiar with. We will not be covering each member of this extensive 
 collection in depth, instead focusing on some commonly used examples. 
  
 In particular we will be going into depth discussing the methods for opening file 
 descriptors and manipulating file data, reading and manipulating file attributes, and 
 moving through filesystem directories. Nevertheless, the reader is encouraged to 
 experiment with the entire set, which the following list briefly describes. Note that all of 
 these methods are asynchronous, non-blocking file operations.",NA
"fs.rename(oldName, newName, callback)","The 
 fs.rename(oldName, newName, callback)
  method renames file at 
 oldName 
 to 
 newName
 . The callback receives one argument, any exception thrown in the call.",NA
"fs.truncate(path, len, callback)","The 
 fs.truncate(path, len, callback)
  method changes the length of the file at 
 path
  by 
 len
  bytes. If 
 len
  represents a length shorter than the file's current length, the 
 file is truncated to that length. If 
 len
  is greater, the file length is padded by appending 
 null bytes (\x00) until 
 len
  is reached. The callback receives one argument, any 
 exception thrown in the call.",NA
"fs.ftruncate(fd, len, callback)","The 
 fs.ftruncate(fd, len, callback)
  method is like 
 fs.truncate
 , except that 
 instead of specifying a file, a file descriptor is passed as 
 fd
 .
  
 [
  97 
 ]
  
 www.EBooksWorld.ir",NA
"fs.chown(path, uid, gid, callback)","The 
 fs.chown(path, uid, gid, callback)
  method changes the ownership of the 
 file at 
 path
 . Use this to set whether user 
 uid
  or group 
 gid
  has access to a file. The 
 callback receives one argument, any exception thrown in the call.",NA
"fs.fchown(fd, uid, gid, callback)","The 
 fs.fchown(fd, uid, gid, callback)
  method is like 
 fs.chown
 , except that 
 instead of specifying a file path, a file descriptor is passed as 
 fd
 .",NA
"fs.lchown(path, uid, gid, callback)","The 
 fs.lchown(path, uid, gid, callback)
  method is like 
 fs.chown
 , except that 
 in the case of symbolic links ownership of the link file itself is changed, but not the 
 referenced link.",NA
"fs.chmod(path, mode, callback)","The 
 fs.chmod(path, mode, callback)
  method changes the 
 mode
 (permissions) on a 
 file at 
 path
 . You are setting the read(4), write(2), and execute(1) bits for this file, which 
 can be sent in octal digits:
  
  
 [r]ead
  
 [w]rite
  
 E[x]ecute
  
 Total
  
 Owner
  
 4
  
 2
  
 1
  
 7
  
 Group
  
 4
  
 0
  
 1
  
 5
  
 Other
  
 4
  
 0
  
 1
  
 5
  
 chmod(755)
  
 You may also use symbolic representations, such as 
 g+rw
  for group read and write, 
 similar to the arguments we saw for 
 file.open
  earlier. For more information on 
 setting file modes, consult: 
 http://en.wikipedia.org/wiki/Chmod
 .
  
 The callback receives one argument, any exception thrown in the call.",NA
"fs.fchmod(fd, mode, callback)","The 
 fs.fchmod(fd, mode, callback)
  method is like 
 fs.chmod
 , except that instead of 
 specifying a file path, a file descriptor is passed as 
 fd
 .
  
 [
  98 
 ]
  
 www.EBooksWorld.ir",NA
"fs.lchmod(path, mode, callback)","The 
 fs.lchmod(path, mode, callback)
  method is like 
 fs.chmod
 , except that in the 
 case of symbolic links permissions on the link file itself is changed, but not those of the 
 referenced link.",NA
"fs.link(srcPath, dstPath, callback)","The 
 fs.link(srcPath, dstPath, callback)
  creates a hard link between 
 srcPath 
 and 
 dstPath
 . This is a way of creating many different paths to exactly the same file. For 
 example, the following directory contains a file 
 target.txt
  and two hard links—
 a.txt
  
 and 
 b.txt
 —which each point to this file:
  
  
 Note that 
 target.txt
  is empty. If the content of the target file is changed, the length of 
 the link files will also be changed. Changing the content of the target file:
  
  echo ""hello"" >> target.txt 
  
 This results in this new directory structure, clearly demonstrating the hard references:
  
  
 The callback receives one argument, any exception thrown in the call.",NA
"fs.symlink(srcPath, dstPath, [type], callback)","The 
 fs.symlink(srcPath, dstPath, [type], callback)
  method creates a 
 symbolic link between 
 srcPath
  and 
 dstPath
 . Unlike hard links created with 
 fs.link
 , 
 symbolic links are simply pointers to other files, and do not themselves respond to 
 changes in the target file. The default link 
 type
 is file. Other options are directory and 
 junction, the last being a Windows-specific type that is ignored on other systems. The 
 callback receives one argument, any exception thrown in the call.
  
 [
  99 
 ]
  
 www.EBooksWorld.ir",NA
"fs.readlink(path, callback)","The given symbolic link at 
 path
 , returns the filename of the targeted file:
  
 fs.readlink('a.txt', function(err, targetFName) {
  
  
 console.log(targetFName); // target.txt 
  
 });",NA
"fs.realpath(path, [cache], callback) ","The 
 fs.realpath(path, [cache], callback)
  method attempts to find the real path 
 to file at 
 path
 . This is a useful way to find the absolute path to a file, resolve symbolic 
 links, and even clean up extraneous slashes and other malformed paths. For example:
  
 fs.realpath('file.txt', function(err, resolvedPath) {
  
  
 console.log(resolvedPath); // `/real/path/to/file.txt` 
 });
  
 Or, even:
  
 fs.realpath('.////./file.txt', function(err, resolvedPath) {
  
  // still `/real/path/to/file.txt` 
  
 });
  
 If some of the path segments to be resolved are already known, one can pass a 
 cache 
 of 
 mapped paths:
  
 var cache = {'/etc':'/private/etc'}; 
  
 fs.realpath('/etc/passwd', cache, function(err, resolvedPath) {
  
  
 console.log(resolvedPath); // `/private/etc/passwd` 
  
 });
  
 [
  100 
 ]
  
 www.EBooksWorld.ir",NA
"fs.unlink(path, callback)","The 
 fs.unlink(path, callback)
  method removes the file at 
 path
 —equivalent to 
 deleting a file. The callback receives one argument, any exception thrown in the call.",NA
"fs.rmdir(path, callback)","The 
 fs.rmdir(path, callback)
  method removes the directory at 
 path
 . Equivalent to 
 deleting a directory.
  
 Note that if the directory is not empty this will throw an exception. The callback 
 receives one argument, any exception thrown in the call.",NA
"fs.mkdir(path, [mode], callback)","The 
 fs.mkdir(path, [mode], callback)
  method creates a directory at 
 path
 . 
 To set the mode of the new directory, use the permission bit map described in 
 fs.chmod
 .
  
 Note that if this directory already exists, an exception will be thrown. The callback 
 receives one argument, any exception thrown in the call.",NA
"fs.exists(path, callback)","The 
 fs.exists(path, callback)
  method checks whether a file exists at 
 path
 . The 
 callback will receive a Boolean true or false.",NA
"fs.fsync(fd, callback)","Between the instant a request for some data to be written to a file is made and that 
 data fully exists on a storage device, the candidate data exists within core system 
 buffers. This latency isn't normally relevant but, in some extreme cases, such as system 
 crashes, it is necessary to insist that the file reflect a known state on a stable storage 
 device.
  
 fs.fsync
  copies all in-core data of a file referenced by file descriptor 
 fd
  to disk (or 
 other storage device). The callback receives one argument, any exception thrown in the 
 call.
  
 [
  101 
 ]
  
 www.EBooksWorld.ir",NA
Synchronicity,"Conveniently, Node's 
 file
  module provides synchronous counterparts for each of the 
 asynchronous methods we've covered, indicated by the suffix 
 Sync
 . For example, the 
 synchronous version of 
 fs.mkdir
  is 
 fs.mkdirSync
 .
  
 A synchronous call is also able to directly return its result, obviating the need for 
 callbacks. While demonstrating the creation of HTTPS servers in 
 Chapter 3
 , 
 Streaming 
 Data Across Nodes and Clients
 , we saw both a good use case for synchronous code and an 
 example of direct assignment of results without a callback:
  
 key: fs.readFileSync('server-key.pem'),
  
 cert: fs.readFileSync('server-cert.pem')
  
 Hey! Doesn't Node strictly enforce asynchronous programming? Isn't blocking code 
 always wrong? All developers are encouraged to adhere to non-blocking designs, and 
 you are encouraged to avoid synchronous coding—if facing a problem where a 
 synchronous operation seems the only solution, it is likely that the problem has been 
 misunderstood. Nevertheless, edge cases requiring a file object existing fully in memory 
 prior to executing further instructions (a blocking operation) do exist. Node gives a 
 developer the power to break with asynchronous tradition if it is the only possible 
 solution (which it probably isn't!).
  
 One synchronous operation developers regularly use (perhaps without realizing it) is 
 the 
 require
  directive:
  
 require('fs')
  
 Until the dependency targeted by require is fully initialized, subsequent JavaScript 
 instructions will not execute (file loading blocks the event loop). 
 Ryan Dahl
  struggled 
 with this decision to introduce synchronous operations (in particular file operations) 
 into Node, as he mentioned at a Google Tech Talk on July 9, 2013 (
 http://www.
  
 youtube.com/watch?v=F6k8lTrAE2g
 ):
  
 I think this is an OK compromise. It pained me for months, to drop the purity of 
 having an asynchronous module system. But, I think it's ok. 
  
 And later:
  
 It simplifies the code a lot to be able to just stick in ""require, require, require"" 
 and not have to do an onload callback…I think that's been a relatively OK 
 compromise. 
  
 […] There's really two parts to your program: there's the loading and starting up 
 phase…and you don't really care how fast that runs...you're going to load 
 modules and stuff…the setup phase of your daemon, generally, is synchronous. 
 It's when you get into your event loop for serving requests that you want to be 
 very careful about this. […] I will give people synchronous file I/O. If they do it in 
 servers…it won't be terrible, right? The important thing is to never let them do 
 synchronous network I/O.
  
 [
  102 
 ]",NA
Moving through directories ,"Let's apply what we have learned and create a directory iterator. The goal for this 
 project is to create a function that will accept a directory path and return a JSON 
 object reflecting the directory hierarchy of files, its nodes composed of file objects. 
  
 We will also make our directory walker a more powerful event-based parser, 
 consistent with the Node philosophy.
  
 To move through nested directories one must first be able to read a single directory. 
 Node's filesystem library provides the 
 fs.readdir
  command for this purpose:
  
 fs.readdir('.', function(err, files) {
  
  
  console.log(files); // list of all files in current directory 
 });
  
 Remembering that everything is a file, we will need to do more than simply get a 
 directory listing; we must determine the type of each member of our file list. By 
 adding 
 fs.stat
  we have already completed a large majority of the logic:
  
 (function(dir) {
  
  
  fs.readdir(dir, function(err, list) {
  
  
  list.forEach(function(file) {
  
    
  fs.stat(dir + ""/"" + file, function(err, stat) {
  
    
  if(stat.isDirectory()) {
  
     
  return console.log(""Found directory : "" + file);
  
    
  } 
  
     
  console.log(""Found file : "" + file);
  
    
  });
  
  
  });
  
  
  }); 
  
 })(""."");
  
 [
  103 
 ]
  
 www.EBooksWorld.ir",NA
Reading from a file ,"In our discussion of file descriptors we touched on one method of opening a file, fetching a 
 file descriptor, and ultimately pushing or pulling data through that reference. Reading 
 files is a common operation. Sometimes managing a read buffer precisely may be 
 necessary, and Node allows byte-by-byte control. In other cases one simply wants a no-
 frills stream that is simple to use.
  
 [
  105 
 ]
  
 www.EBooksWorld.ir",NA
Reading byte by byte ,"The 
 fs.read
  method is the most low-level way Node offers for reading files.",NA
"fs.read(fd, buffer, offset, length, position, callback) ","Files are composed of ordered bytes, and these bytes are addressable by their 
 position
 , relative to the beginning of in the file (position zero [0]). Once we have a file 
 descriptor 
 fd
 , we can begin to read 
 length
  number of bytes and insert those into a 
 Buffer
  object 
 buffer
 , insertion beginning at a given buffer 
 offset
 . For example, to 
 copy the 8366 bytes beginning at 
 position
  309 of readable file 
 fd
  into a 
 buffer
  
 beginning at an 
 offset
  of 100, we would use: 
 fs.read(fd,buffer,100, 
 8366,309,callback)
 .
  
 The following code demonstrates how to open and read a file in 512 byte chunks:
  
 fs.open('path.js', 'r', function(err, fd) {
  
  fs.fstat(fd, function(err, stats) {
  
  
  
  var totalBytes = stats.size;
  
  
  
  var buffer    = new Buffer(totalBytes);
  
  
  
  var bytesRead  = 0;
  
  //  Each call to read should ensure that chunk size is
  
  //  within proper size ranges (not too small; not too large).
  
  var read = function(chunkSize) {
  
  
  
  fs.read(fd, buffer, bytesRead, chunkSize, bytesRead, 
 function(err, numBytes, bufRef) {
  
  
  
  if((bytesRead += numBytes) < totalBytes) {
  
  
  
   
  return read(Math.min(512, totalBytes - bytesRead));
  
  
  }
  
  
  
  fs.close(fd);
  
  
  
  console.log(""File read complete. Total bytes read: "" + 
 totalBytes);
  
  
  
    
  // Note that the callback receives a reference to 
 the
  
     
  // accumulating buffer 
  
  
  
  console.log(bufRef.toString());
  
  
  
  });
  
  }
  
  read(Math.min(512, totalBytes));
  
  }); 
  
 });
  
 The resulting buffer can be piped elsewhere (including a server response object). It 
 can also be manipulated using the methods of Node's 
 Buffer
  object, such as 
 conversion into a UTF8 string with 
 buffer.toString(""utf8"")
 .
  
 [
  106 
 ]
  
 www.EBooksWorld.ir",NA
Fetching an entire file at once ,"Often one simply needs to fetch an entire file, without any ceremony or fine control. 
 Node provides a shortcut method for exactly this.",NA
"fs.readFile(path, [options], callback) ","Fetching the data contained by file 
 path
  can be accomplished in one step:
  
 fs.readFile('/etc/passwd', function(err, fileData) {
  
  if(err) {
  
  
  throw err;
  
  
  }
  
  
  console.log(fileData);
  
  
  // <Buffer 48 65 6C 6C 6F … > 
  
 });
  
 We see how 
 callback
  receives a buffer. It may be more desirable to receive the file data 
 in a common encoding, such as UTF8. We are able to specify the encoding of the returned 
 data, as well as the read mode, using the 
 options
  object, which has two possible 
 attributes:
  
 • 
  
 encoding
 : A string, such as 
 utf8
 . Defaults to null (no encoding).
  
 • 
  
 flag
 : The file mode as a string. Defaults to 
 r
 .
  
 Modifying the previous example:
  
 fs.readFile('/etc/passwd', function(err, { encoding : ""utf8"" }, 
 fileData) {
  
  
  ...
  
  
  console.log(fileData);
  
  
  // ""Hello ..."" 
  
 });",NA
Creating a readable stream ,"While 
 fs.readFile
  is an excellent, simple way to accomplish a common task, it does 
 have the significant drawback of requiring that an entire file be read into memory prior to 
 any part of the file being sent to a callback. For large files, especially regularly accessed 
 large files (such as videos), this is problematic as we are unable to accurately predict the 
 volume of data we will be buffering at any given point in time. This means the amount of 
 memory available to Node may be starved unpredictably, taking down an entire 
 application or, worse, make your customers angry as 
  
 responsiveness declines and errors begin to surface.
  
 [
  107 
 ]
  
 www.EBooksWorld.ir",NA
"fs.createReadStream(path, [options])","The 
 fs.createReadStream(path, [options])
  method returns a readable stream 
 object for file at 
 path
 . You may then perform stream operations on the returned 
 object, such as 
 pipe()
 .
  
 The following options are available:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 flags
 : File mode argument as a string. Defaults to 
 r
 .
  
 encoding
 : One of 
 utf8
 , 
 ascii
 , or 
 base64
 . Default to no encoding.
  
 fd
 : One may set 
 path
  to null, instead passing the call a file descriptor.
  
 mode
 : Octal representation of file mode, defaulting to 0666.
  
 bufferSize
 : The chunk size, in bytes, of the internal read stream. Defaults to 64 * 
 1024 bytes. You can set this to any number, but memory allocation is strictly 
 controlled by the host OS, which may ignore a request. See: 
 https://groups. 
 google.com/forum/?fromgroups#!topic/nodejs/p5FuU1oxbeY
 .
  
 autoClose
 : Whether to automatically close the file descriptor (a la 
 fs.close
 ). 
 Defaults to true. You may want to set this to false and close manually if you are 
 sharing a file descriptor across many streams, as closing a descriptor will disrupt 
 any other readers.
  
 start
 : Begin reading from this position. Default is 0.
  
 end
 : Stop reading at this position. Default is the file byte length.",NA
Reading a file line by line,"While reading a file stream byte-by-byte is sufficient for any file-parsing job, text files in 
 particular are often more usefully read line by line, such as when reading logfiles. More 
 precisely, any stream can be understood in terms of the chunks of data separated by 
 newline characters, typically ""\r\n"" on UNIX systems. Node provides a native module 
 whose methods simplify access to newline-separated chunks in data streams.
  
 [
  108 
 ]
  
 www.EBooksWorld.ir",NA
The Readline module,"The 
 Readline
  module has a simple but powerful goal, that is, to make reading a 
 stream of data line-by-line easier. The bulk of its interface is designed to make 
 command-line prompting easier, such that interfaces taking user input are easier to 
 design.
  
 Remembering that Node is designed for I/O, that I/O operations normally involve 
 moving data between readable and writable streams, and that 
 stdout
  and 
 stdin
  are 
 stream interfaces identical with the file streams returned by 
 fs.createReadStream 
 and 
 fs.createWriteStream
 , we will look at how this module can be similarly used to 
 prompt file streams for a line of text.
  
  
 The 
 Readline
  module will be used throughout the book 
  
  
 (and later in this chapter), for purposes beyond the present 
  
 discussion on reading files, such as when designing command-line 
  
 interfaces. For the impatient, more information on this module can 
  
 be found at: 
 http://nodejs.org/api/readline.html
 .
  
 To start working with the 
 Readline
  module one must create an interface defining the 
 input stream and the output stream. The default interface options prioritize usage as a 
 terminal interface. The options we are interested in are:
  
 • 
  
 • 
  
 • 
  
 input
 : Required. The readable stream being listened to.
  
 output
 : Required. The writable stream being written to.
  
 terminal
 : Set this to true if both the input and output streams should be treated 
 like a Unix terminal, or 
 TTY
  (
 TeleTYpewriter)
 . For files, you will set this to 
 false.
  
 Reading the lines of a file is made easy through this system. For example, assuming one 
 has a dictionary file listing common words in the English language one might want to 
 read the list into an array for processing:
  
 var fs = require('fs');
  
 var readline = require('readline');
  
 var rl = readline.createInterface({
  
  input: fs.createReadStream(""dictionary.txt""),
  
  terminal: false
  
 });
  
 var arr = [];
  
 rl.on(""line"", function(ln) {
  
 arr.push(ln.trim())
  
 });
  
 // aardvark
  
 [
  109 
 ]
  
 www.EBooksWorld.ir",NA
Writing to a file ,"As with reading files, Node provides a rich collection of tools for writing to files. We'll 
 see how Node makes it as easy to target a file's contents byte-by-byte, as it is to pipe 
 continuous streams of data into a single writable file.",NA
Writing byte by byte ,"The 
 fs.write
  method is the most low-level way Node offers for writing files. This 
 method gives us precise control over where bytes will be written to in a file.",NA
"fs.write(fd, buffer, offset, length, position, callback) ","To write the collection of bytes between positions 309 and 8675 
 (
 length
 8366) of 
 buffer
  to the file referenced by file descriptor 
 fd
 , insertion beginning at 
 position
  100:
  
 var buffer = new Buffer(8675); 
  
 fs.open(""index.html"", ""w"", function(err, fd) {
  
  
  fs.write(fd, buffer, 309, 8366, 100, function(err, writtenBytes, 
 buffer) {
  
  
  console.log(""Wrote "" + writtenBytes + "" bytes to file"");
  
  
  // Wrote 8366 bytes to file
  
  
  }); 
  
 });
  
 [
  110 
 ]
  
 www.EBooksWorld.ir",NA
Writing large chunks of data,"For straightforward write operations 
 fs.write
  may be overkill. Sometimes all that is 
 needed is a way to create a new file with some content. Just as common is the need to 
 append data to the end of a file, as one might do in a logging system. The 
 fs.writeFile
  and 
 fs.appendFile
  methods can help us with those scenarios.",NA
"fs.writeFile(path, data, [options], callback)","The 
 fs.writeFile(path, data, [options], callback)
  method writes the 
 contents of 
 data
  to the file at 
 path
 . The 
 data
 argument can be either a 
 Buffer
  or a 
 string. The following options are available:
  
 • 
  
 encoding
 : Defaults to 
 utf8
 . If data is a buffer this option is ignored.
  
 • 
  
 mode
 : Octal representation of file mode, defaulting to 0666.
  
 • 
  
 flag
 : Write flags, defaulting to 
 w
 .
  
 Usage is straightforward:
  
 fs.writeFile('test.txt', 'A string or Buffer of data', function(err) {
  
  if(err) {
  
  return console.log(err);
  
  }
  
  // File has been written
  
 });",NA
"fs.appendFile(path, data, [options], callback)","Similar to 
 fs.writeFile
 , except that 
 data
  is appended to the end of the file at 
 path
 . As 
 well, the 
 flag
  option defaults to 
 a
 .
  
 [
  112 
 ]
  
 www.EBooksWorld.ir",NA
Creating a writable stream,"If the data being written to a file arrives in chunks (such as occurs with a file upload), 
 streaming that data through a 
 WritableStream
  object interface provides more flexibility 
 and efficiency.",NA
"fs.createWriteStream(path, [options])","The 
 fs.createWriteStream(path, [options])
  method returns a writable stream 
 object for file at 
 path
 .
  
 The following options are available:
  
 • 
  
 flags
 : File mode argument as a string. Defaults to 
 w
 .
  
 • 
  
 encoding
 : One of 
 utf8
 , 
 ascii
 , or 
 base64
 . Default to no encoding.
  
 • 
  
 mode
 : Octal representation of file mode, defaulting to 0666.
  
 • 
  
 start
 : An offset, indicating the position in the file where writing should begin.
  
 For example, this little program functions as the world's simplest word processor, 
 writing all terminal input to a file, until the terminal is closed:
  
 var writer = fs.createWriteStream(""novel.txt"", 'w');
  
 process.stdin.pipe(writer);",NA
Caveats,"The side effects of opening a file descriptor and reading from it are minimal, such that in 
 normal development very little thought is given to what is actually happening within the 
 system. Normally, reading a file doesn't change data shape, volume, or availability.
  
 When writing to a file a number of concerns must be addressed, such as:
  
 • 
  
 • 
  
 • 
  
 Is there sufficient writable storage space available?
  
 Is another process simultaneously accessing this file, or even erasing it? 
 What must be done if a write operation fails or is unnaturally terminated 
 mid-stream?
  
 We've seen the exclusive write mode flag (
 wx
 ) that can help in the case of multiple 
 write processes simultaneously trying to create a file. Full solutions to all the concerns 
 one might face when writing to files are difficult to derive in general, or state briefly. 
 Node encourages asynchronous programming. Nevertheless, with regards the 
 filesystem in particular, sometimes synchronous, deterministic programming is 
 necessary. You are encouraged to keep these and other issues in mind, and to keep I/O 
 non-blocking whenever possible.
  
 [
  113 
 ]
  
 www.EBooksWorld.ir",NA
Serving static files,"Anyone using Node to create a web server will need to respond intelligently to HTTP 
 requests. A HTTP request to a web server for a resource expects some sort of response. 
  
 A basic file static file server might look like this:
  
 http.createServer(function(request, response) {
  
  if(request.method !== ""GET"") {
  
  return response.end(""Simple File Server only does GET"");
  
  }
  
  fs
  
  .createReadStream(__dirname + request.url)
  
  .pipe(response);
  
 }).listen(8000);
  
 This server services GET requests on port 8000, expecting to find a local file at a relative 
 path equivalent to the URL path segment. We see how easy Node makes it for us to 
 stream local file data, simply piping a 
 ReadableStream
  into a 
 WritableStream 
 representing a client socket connection. This is an enormous amount of functionality to 
 be safely implemented in a handful of lines.
  
 Eventually a great deal more would be added, such as handling routines for standard 
 HTTP methods, handling errors and malformed requests, setting proper headers, 
 managing favicon requests, and so forth.
  
 Let's build a reasonably useful file server with Node, one that will respond to HTTP 
 requests by streaming back a resource and which will respect caching requests. In the 
 process we will touch on how to manage content redirection. Later on in this chapter we 
 will also look at implementing file uploads. Note that a web server fully compliant with 
 all features of HTTP is a complicated beast, so what we are creating should be 
 considered a good start, not an end.",NA
Redirecting requests,"Sometimes a client will try to 
 GET
  a URL that is incorrect or incomplete in some way, the 
 resource may have been moved, or there are better ways to make the same request. 
  
 Other times, a 
 POST
  may create a new resource at a new location the client cannot know, 
 necessitating some response header information pointing to the newly created URI. 
 Let's look into two common redirection scenarios someone implementing a static file 
 server with Node might face.
  
 [
  114 
 ]
  
 www.EBooksWorld.ir",NA
Location,"Responding to a 
 POST
  with a 
 201
  status code indicates a new resource has been created 
 its URI assigned to the 
 Location
  header, and that the client may go ahead and use that 
 URI in the future. Note that it is up to the client to decide whether, and when, to fetch 
 this resource. As such this is not, strictly speaking, a redirect.
  
 For example a system might create new accounts by posting new user information to a 
 server, expecting to receive the location of a new user page:
  
 POST /path/addUser HTTP/1.1
  
 Content-Type: application/x-www-form-urlencoded
  
 name=John&group=friends 
  
 …
  
 Status: 201 
  
 Location: http://website.com/users/john.html
  
 Similarly, in cases where a resource creation request has been accepted but not yet 
 fulfilled, a server would indicate a status of 
 202
 . This would be the case in the preceding 
 example if creation of the new user record had been delegated to a worker queue, which 
 might at some point in the future create a record at the given 
 Location
 .
  
 We will see a realistic implementation demonstrating this usage later on in the 
 chapter, when we discuss file uploads.
  
 [
  115 
 ]
  
 www.EBooksWorld.ir",NA
Content-Location ,"When a 
 GET
  is made to a resource that has multiple representations, and those can be 
 found at distinct resource locations, a 
 content-location
  header for the particular entity 
 should be returned.
  
 For example, content format negotiation is a good candidate for 
 Content-Location 
 handling. One might be interested in retrieving all blog posts for a given month, perhaps 
 available at a URL such as 
 http://example.com/september/
 . GET requests with an 
 Accept
  header of 
 application/json
  will receive a response in JSON format. A request 
 for XML will receive that representation.
  
 If a caching mechanism is being used those resources may have alternate permanent 
 locations, such as 
 http://example.com/cache/september.json
  or 
 http:// 
 example.com/cache/september.xml
 . One would send this additional location 
 information via 
 Content-Location
 , in a response object resembling:
  
 Status: 200 
  
 Content-Type: application/json
  
 Content-Location: http://blogs.com/cache/allArticles.json
  
 … JSON entity body
  
 In cases where the requested URL has been moved, permanently or temporarily, the 
 3xx
  group of status codes can be used with 
 Content-Location
  to indicate this state. 
 For example, to redirect a request to a URL which has been permanently moved one 
 should send a 301 code:
  
 function requestHandler(request,response) {
  
  
  var newPath = ""/thedroids.html"";
  
  
  response.writeHead(301, {
  
  
  'Content-Location': newPath
  
  
  });
  
  
  response.end(); 
  
 }",NA
Implementing resource caching ,"A general rule, never expend resources delivering irrelevant information to clients. For a 
 HTTP server, resending files that the client already possesses is an unnecessary I/O cost, 
 exactly the wrong way to implement a Node server, increasing latency as well as the 
 financial hit of paying for misappropriated bandwidth.
  
 Browsers maintain a cache of the files they have already fetched, and an 
 ETag 
 (
 Entity 
 Tag
 ) identifies these files. An ETag is a response header sent by servers to uniquely 
 identify entities they are returning, such as a file. When a file changes on a server, that 
 server will send a different ETag for said file, allowing file changes to be tracked by 
 clients.
  
 [
  116 
 ]
  
 www.EBooksWorld.ir",NA
Handling file uploads,"It is likely that anyone reading this sentence has had at least one experience with 
 uploading a file from a client to a server. Some may have even implemented a file upload 
 service, a server that will receive and do something useful with a multipart data stream. 
 Within popular development environments this task has been made very easy. In the 
 PHP environment, for example, uploaded data is automatically processed and made 
 globally available, neatly parsed and packaged into an array of files or form field values, 
 without the developer having written a single line of code.
  
 Unfortunately, Node leaves implementation of file upload handling to the 
 developer, a challenging bit of work many developers may be unable to 
 successfully or safely complete.
  
 Fortunately, Felix Geisendorfer created the 
 Formidable
  module, one of the most 
 important early contributions to the Node project. A widely implemented, enterprise-
 grade module with extensive test coverage, it not only makes handling file uploads a 
 snap, but can be used as a complete tool for handling form 
  
 submissions. We will use this library to add file upload capability to our file server.
  
  
 For more information about how HTTP file uploads are 
  
  
 designed, and the tricky implementation problems developers 
  
 must overcome, consult the multipart/form-data specification 
  
 (
 http://www.w3.org/TR/html401/interact/forms.
  
 html#h-17.13.4.2
 ) and Geisendorfer's breakdown of 
  
 how 
 Formidable
  was conceived of and evolved (
 http://
  
 debuggable.com/posts/parsing-file-uploads-at-
  
 500-mb-s-with-node-js:4c03862e-351c-4faa-bb67-
  
 4365cbdd56cb
 ).
  
 First, install 
 formidable
  via npm:
  
 npm install formidable
  
 You can now 
 require
  it:
  
 var formidable = require('formidable');
  
 We will assume that file uploads will be posted to our server along a path of 
 /uploads/
 , and that the upload arrives via a HTML form that looks like this:
  
 <form action=""/uploads"" enctype=""multipart/form-data"" method=""post"">
  
 Title: <input type=""text"" name=""title""><br />
  
 <input type=""file"" name=""upload"" multiple=""multiple""><br />
  
 <input type=""submit"" value=""Upload"">
  
 </form>
  
 [
  118 
 ]
  
 www.EBooksWorld.ir",NA
Putting it all together ,"Recalling our discussion of favicon handling from the previous chapter, and adding what 
 we've learned about file caching and file uploading, we can now construct a simple file 
 server handling 
 GET
  and 
 POST
  requests:
  
 http.createServer(function(request, response) {
  
  
  var rm = request.method.toLowerCase();
  
  
  if(rm === ""post"") {
  
  
  var form = new formidable.IncomingForm();
  
  
  form.uploadDir = process.cwd();
  
  
  form
  
  
  .on(""file"", function(field, file) {
  
    
  // process files
  
  
  })
  
  
  .on(""field"", function(field, value) {
  
    
  // process POSTED field data
  
  
  })
  
  
  .on(""end"", function() {
  
    
  response.end(""Received"");
  
  
  })
  
  
  .parse(request);
  
  
  return;
  
  
  }
  
  
  // We can only handle GET requests at this point
  
  
  if(rm !== ""get"") {
  
  
  return response.end(""Unsupported Method"");
  
  
  }
  
  
  var filename = __dirname + request.url;
  
  
  fs.stat(filename, function(err, stat) {
  
    
  if(err) {
  
     
  response.statusCode = err.errno === 34 ? 404 : 500;
    
  return response.end()
  
    
  } 
  
 [
  120 
 ]
  
 www.EBooksWorld.ir",NA
Summary ,"In this chapter we've seen how Node's API is a comprehensive map to native filesystem 
 bindings, exposing a full range of functionality to the developer while requiring very 
 little code or complexity. Additionally, we've seen how files are easily wrapped into 
 Stream
  objects, and how this consistency with the rest of Node's design simplifies 
 interactions between different types of I/O, such as between network data and files.
  
 We've also learned something about how to build servers with Node that can 
 accommodate regular client expectations, easily implementing file uploading and 
 resource caching. Having covered the key features of Node, it is time to use these 
 techniques in building larger applications able to handle many thousands of clients.
  
 [
  121 
 ]
  
 www.EBooksWorld.ir",NA
Managing Many ,NA,NA
Simultaneous Client ,NA,NA
Connections,"If everyone helps to hold up the sky, then one person does not become tired.
  
 — Tshi Proverb
  
 Maintaining a high level of throughput while managing thousands of simultaneous client 
 transactions in the unpredictable and ""bursty"" environment of networked software is 
 one expectation developers have for their Node implementations. Given a history of 
 failed and unpopular solutions, the problem of concurrency has even been assigned its 
 own numeronym: ""The C10K problem"". How should network software confidently 
 serving 10,000 simultaneous clients be designed?
  
 The question of how to best build high concurrency systems has provoked much 
 theory over the last several decades, with the debate mostly between two 
 alternatives, 
 threads
  and 
 events
 :
  
 Threading allows programmers to write straight-line code and rely on the 
 operating system to overlap computation and I/O by transparently switching 
 across threads. The alternative, events, allows programmers to manage 
 concurrency explicitly by structuring code as a single-threaded handler that reacts 
 to events (such as 
  
 non-blocking I/O completions, application-specific messages, or timer events).
  
 — ""A Design Framework for Highly Concurrent Systems"" (Welsh, Gribble, 
 Brewer & Culler, 2000), p. 2. 
 http://www.eecs.harvard.edu/~mdw/
  
 papers/events.pdf",NA
Understanding concurrency,"We would all agree that there are unexpected events in the world, and that many of 
 them occur at exactly the same time. It is also clear that the state of any given system 
 may be composed of any number of sub-states, where the full consequence of even 
 minor state changes are difficult to predict—the power of a butterfly's wings being 
 enough to tip a much larger system into an alternate state. And we also know that the 
 volume and shape of a system, over time, changes in ways difficult to predict.
  
 In his PHD thesis ""Foundations of Actor Semantics"", written in 1981, 
 William Clinger 
 proposed that his work was:
  
 ...motivated by the prospect of highly parallel computing machines consisting 
 of dozens, hundreds or even thousands of independent microprocessors, each 
 with its own local memory and communications processor, communicating 
 via a high-performance communications network.
  
 As it turns out, Clinger was on to something. Concurrency is a property of systems 
 composed of many simultaneously executing operations, and the network software we 
 are now building resembles the one he envisioned, only much larger, where ""hundreds 
 or even thousands"" is the lower bound, not the higher.
  
 Node makes concurrency accessible, while simultaneously scaling across multiple cores, 
 multiple processes, and multiple machines. It is important to note that Node places as 
 much importance on the simplicity and consistency of programs as it does on being 
 the 
 fastest
  solution, embracing and enforcing non-blocking I/O in an effort to deliver high 
 concurrency through well-designed and predictable interfaces. This is what Dahl meant 
 when he said, ""Node's goal is to provide an 
 easy
  way to build 
 scalable
  network 
 programs"".
  
 Happily, it also turns out that Node is very fast.",NA
Concurrency is not parallelism,"A problem can be solved by dividing some of it into smaller problems, spreading those 
 smaller problems across a pool of available people or workers to work on in parallel, 
 and delivering the parallel results concurrently.
  
 Multiple processes each solving one part of a single mathematical problem 
 simultaneously is an example of parallelism.
  
 [
  126 
 ]
  
 www.EBooksWorld.ir",NA
Routing requests,"HTTP is a data transfer protocol built upon a request/response model. Using this 
 protocol many of us communicate our current status to friends, buy presents for 
 family, or discuss a project over e-mail with colleagues. A staggering number of 
 people have come to depend on this foundational Internet protocol.
  
 Typically, a browser client will issue an HTTP GET request to a server. This server then 
 returning the requested resource, often represented as an HTML document. HTTP is 
 stateless, which simply means that each request or response maintains no information 
 on previous requests or responses—with each back and forward movement through 
 web pages the entire browser state is destroyed and rebuilt from scratch.
  
 [
  127 
 ]
  
 www.EBooksWorld.ir",NA
Understanding routes,"Routes map URLs to actions. Rather than constructing an application interface in 
 terms of URL paths to specific files that contain some logic, designing with routes 
 involves assigning a specific function to a distinct combination of a URL path and 
 request method. For example, a web service that accepts requests for lists of cities 
 might be called in this manner:
  
 GET /services/cities.php?country=usa&state=ohio
  
 When your server receives this request it would pass the URL information to a PHP 
 process that will execute the application logic in 
 cities.php
 , such as reading the query, 
 parsing out the country and state, calling a database, building a response, and returning it.
  
 Node has the great benefit of being able to function both as the server and the 
 application environment. The server can field requests directly. It makes more sense, 
 then, to use URLs as simple statements of intent:
  
 GET /listCities/usa/ohio
  
 [
  129 
 ]
  
 www.EBooksWorld.ir",NA
Using Express to route requests ,"Express simplifies the complexity of defining route-matching routines. Our example 
 might be written in the following way using Express:
  
 var express = require('express'); 
  
 var app = express(); 
  
 app.get('/listCities/:country/:state', function(request, response){
  
  
 var country = request.params.country;
  
  
  var state = request.params.state;
  
  
  response.end(""You asked for country: "" + country + "" and state: "" + 
 state); 
  
 }); 
  
 app.listen(8080);
  
 GET /listCities/usa/ohio 
  
 // You asked for country: usa and state: ohio 
 GET /didnt/define/this 
  
 // Cannot GET /didnt/define/this 
  
 GET /listCities // note missing arguments // 
 Cannot GET /listCities
  
 Instantiating Express delivers a fully-formed web server wrapped in an easy-to-use 
 application development API. Our cities service is clearly defined and its variables 
 stated, expecting to be called via GET (one might also use 
 app.post(...)
  or 
 app. 
 put(...)
  or any other standard HTTP method).
  
 [
  131 
 ]
  
 www.EBooksWorld.ir",NA
Using Redis for tracking client state,"For some of the applications and examples in this chapter we will be using 
 Redis
 , an in-
 memory 
 Key/Value
  (
 KV
 ) database developed by 
 Salvatore Sanfilippo
  and currently 
 supported by Pivotal. More information on Redis can be found at 
 http://redis.io
 . A 
 well-known competitor to Redis is 
 Memcached
  (
 http://memcached.org
 ).
  
 In general, any server that must maintain the session state of many clients will need a 
 high speed data layer with near-instantaneous read/write performance, as request 
 validation and user state transformations can occur multiple times on each request. 
  
 Traditional file-backed relational databases tend to be slower at this task than in-
 memory KV databases. We're going to use Redis for tracking the client state.
  
 Redis is a single-threaded server with a straightforward install:
  
 wget http://download.redis.io/redis-stable.tar.gz
  
 tar xvzf redis-stable.tar.gz
  
 cd redis-stable
  
 make
  
 [
  132 
 ]
  
 www.EBooksWorld.ir",NA
Storing user data ,"Managing many users means at least tracking their user information, some stored long 
 term (for example, address, purchase history, and contact list) and some session data 
 stored for a short time (time since login, last game score, and most recent answer).
  
 Normally we would create a secure interface or similar, allowing administrators to 
 create user accounts. It will be clear to the reader how to create such an interface by the 
 end of this chapter. For the examples that follow, we'll only need to create one user, to 
 act as a volunteer. Let's create 
 Jack
 :
  
 redis> hset jack password ""beanstalk""
  
 redis> hset jack fullname ""Jack Spratt""
  
 This will create a key in Redis—
 Jack
 —containing a hash resembling:
  
 {
  
  ""password"": ""beanstalk"",
  
  ""fullname"": ""Jack Spratt""     
  
 }
  
 If we wanted to create a hash and add several KV pairs all at once, we could achieve the 
 preceding with the 
 hmset
  command:
  
 redis> hmset jack password ""beanstalk"" fullname ""Jack Spratt""
  
 Now 
 Jack
  exists: 
  
 redis> hgetall jack 
  
 1) ""password"" 
  
 2) ""beanstalk"" 
  
 3) ""fullname"" 
  
 4) ""Jack Spratt""
  
 We can use the following command to fetch the value stored for a specific field in 
 Jack
 's account:
  
 redis> hget jack password // ""beanstalk""
  
 [
  134 
 ]
  
 www.EBooksWorld.ir",NA
Handling sessions,"How does a server know if the current client request is part of a chain of previous 
 requests? Web applications engage with clients through long transactional chains—the 
 shopping cart containing items to buy will still be there even if a shopper navigates 
 away to do some comparison-shopping. We will call this a 
 session
 , which may contain 
 any number of KV pairs, such as a username, product list, or the user's login history.
  
 How are sessions started, ended, and tracked? There are many ways to attack this 
 problem, depending on many factors existing in different ways on different 
 architectures. In particular, if more than one server is being used to handle clients, 
 how is session data shared between them?
  
 We will use cookies to store session IDs for clients, while building a simple 
  
 long-polling server. Keep in mind that as applications grow in complexity this simple 
 system will need to be extended. As well, long-polling as a technology is giving ground to 
 the more powerful socket techniques we will explore in our discussions around building 
 real-time systems. However, the key issues faced when holding many connected clients 
 simultaneously on a server, and tracking their sessions, should be demonstrated.",NA
Cookies and client state,"Netscape
  provided the preliminary specification for cookies, in 1997:
  
 Cookies are a general mechanism which server side connections...can use to 
 both store and retrieve information on the client side of the connection. The 
 addition of a simple, persistent, client-side state significantly extends the 
 capabilities of Web-based client/server applications...
  
 A server, when returning an HTTP object to a client, may also send a piece of 
 state information which the client will store... Any future HTTP requests made by 
 the client...will include a transmittal of the current value of the state object from 
 the client back to the server. The state object is called a cookie, for no compelling 
 reason.
  
 — http://lib.ru/WEBMASTER/cookie_spec.txt_with-big-pictures.html
  
 Here we have one of the first attempts to ""fix"" the stateless nature of HTTP, 
 specifically the maintenance of session state. It was such a good attempt, which still 
 remains a fundamental part of the Web.
  
 [
  135 
 ]
  
 www.EBooksWorld.ir",NA
A simple poll,"We need to create a concurrent environment, one with many simultaneously 
 connected clients. We'll use a long-polling server to do this, broadcasting to all 
 connected clients via 
 stdin
 . Additionally, each client will be assigned a unique 
 session ID, used to identify the client's 
 http.serverResponse
  object, which we 
 will push data to.
  
 Long polling is a technique whereby a server holds on to a client connection until there is 
 data available to send. When data is ultimately sent to the client, the client reconnects to 
 the server and the process continues. It was designed as an improvement on 
 short 
 polling
 , which is the inefficient technique of blindly checking with a server for new 
 information every few seconds or so, hoping for new data. Long polling only requires a 
 reconnection following a tangible delivery of data 
  
 to the client.
  
 We'll use two routes. The first route is described using a forward slash (
 /
 ), a root 
 domain request. Calls to this path will return some HTML forming the client UI. The 
 second route is 
 /poll
 , which the client will use to reconnect with the server 
 following the receipt of some data.
  
 [
  136 
 ]
  
 www.EBooksWorld.ir",NA
Centralizing states,"By keeping user sessions centralized in a Redis database we are able to have multiple 
 servers access the same session data. This is the primary advantage of centralizing 
 session data. Additionally, analytics on current sessions (and even past sessions) can be 
 done.
  
 We've seen how we might use cookies to track user connections. The primary problem 
 with this solution is that it only works on a single server. What if our system is running 
 two separate client servers? The connection map we used earlier is determined by 
 connections to the 
 local
  server. What if data is written from a 
 remote 
 server? How do we 
 ensure that all servers are informed of all events, such that each can push new 
 information to all of 
 its
  clients?
  
 [
  138 
 ]
  
 www.EBooksWorld.ir",NA
Authenticating connections,"In conjunction with establishing client session objects, a Node server often demands 
 authentication credentials. The theory and practice of web security is extensive. We 
 want to simplify our understanding into two main authentication scenarios:
  
 • 
  
 When the wire protocol is HTTPS
  
 • 
  
 When it is HTTP
  
 The first is naturally secure, and the second is not. For the first we will learn how to 
 implement Basic authentication in Node, and for the second a challenge-response 
 system will be described.
  
 [
  140 
 ]
  
 www.EBooksWorld.ir",NA
Basic authentication,"As mentioned, Basic authentication sends plain text over the wire containing a 
 username/password combination, using standard HTTP headers. It is a simple and well-
 known protocol. Any server sending the correct headers will cause any browser to display 
 a login dialog, like the following one:
  
  
 Nonetheless this method remains insecure, sending non-encrypted data in plain text 
 over the wire. For the sake of simplicity we will demonstrate this authentication method 
 on a HTTP server, but it must be stressed that in real-world usage the server 
 must
  be 
 communicating via a secure protocol, such as HTTPS.
  
 Let's implement this authentication protocol with Node. Employing the user database 
 developed earlier in Redis, we validate submitted credentials by checking user objects 
 for matching passwords, handling failures and successes.
  
 var http     = require('http');
  
 var redis     = require(""redis"");
  
 var client    = redis.createClient();
  
 http.createServer(function(req, res) {
  
  var auth = req.headers['authorization']; 
  
  if(!auth) 
  
  {   
  
 [
  141 
 ]
  
 www.EBooksWorld.ir",NA
Handshaking,"Another authentication method to consider in situations where an HTTPS connection 
 cannot be established is a challenge-response system:
  
 Pass a client id, requesting access
  
 Client
  
 Challenge publickey
  
 Server
  
 Respond with requested hash
  
 In this scenario a client asks a server for access for a specific user, username, ID, or 
 similar. Typically this data would be sent via a login form. Let's mock up a 
 challenge/response scenario, using for our example the user we created earlier—
 Jack
 .
  
 The general design and purpose of a challenge/response system aims to avoid sending 
 any password data in plain text over the wire. So, we will need to decide on an 
 encryption strategy that both the client and the server share. For our example let's use 
 the 
 SHA256
  algorithm. Node's 
 crypto
  library contains all of the tools necessary for 
 creating this type of hash. The client likely does not, so we must provide one. 
  
 We'll use the one developed by 
 Chris Veness
 , which can be downloaded from the 
 following link:
  
 http://csrc.nist.gov/groups/ST/toolkit/examples.html
  
 To initiate this login the client will need to send an authentication request for the 
 user 
 Jack
 :
  
 GET /authenticate/jack
  
 In response the client should receive a server-generated public key—the challenge. 
  
 The client must now form a string of 
 Jack
 's password prefixed by this key. Create a 
 SHA256 hash from it, and pass the resulting hash to 
 /login/
 . The server will have also 
 created the same SHA256 hash—if the two match, the client is authenticated:
  
 <script src=""sha256.js""></script>
  
 <script>
  
 $.get(""/authenticate/jack"", function(publicKey) {
  
  if(publicKey === ""no data"") {
  
 [
  143 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"Node provides a set of tools that help in the design and maintenance of large-scale 
 network applications facing the C10K problem. In this chapter we've taken our first 
 steps into creating network applications with many simultaneous clients, tracking their 
 session information and their credentials. This exploration into concurrency has 
 demonstrated some techniques for routing, tracking, and responding to clients. We've 
 touched on some simple techniques to use when scaling, such as the implementation of 
 intra-process messaging via a publish/subscribe system built using a Redis database.
  
 We are now ready to go deeper into the design of real-time software—the logical next 
 step after achieving high concurrency and low latency by using Node. We will extend the 
 ideas outlined during our discussion of long polling and place them in the context of 
 more robust problems and solutions.",NA
Further reading,"Concurrency and parallelism are rich concepts that have enjoyed rigorous study and 
 debate. When an application architecture is designed to favor threads, events, or some 
 hybrid, it is likely that the architects are opinionated about both concepts. You are 
 encouraged to dip a toe into the theory and read the following articles. A clear 
 understanding of precisely what the debate is about will provide an objective 
 framework that can be used to qualify a decision to choose (or not choose) Node.
  
 • 
  
 Some numbers:
  
 https://cs.uwaterloo.ca/~brecht/papers/getpaper.
  
 php?file=eurosys-2007.pdf
  
 • 
  
 Threads are a bad idea:
  
 http://www.cs.sfu.ca/~vaughan/teaching/431/papers/ousterhout-
  
 threads-usenix96.pdf
  
 • 
  
 Events are a bad idea:
  
 http://static.usenix.org/events/hotos03/tech/full_papers/
  
 vonbehren/vonbehren.pdf
  
 • 
  
 How about together?
  
 http://repository.upenn.edu/cgi/viewcontent.
  
 cgi?article=1391&context=cis_papers
  
 • 
  
 It's a false dichotomy:
  
 http://swtch.com/~rsc/talks/threads07/
  
 [
  146 
 ]
  
 www.EBooksWorld.ir",NA
Creating Real-time ,NA,NA
Applications,"Nothing endures but change.
  
 —
 Heraclitus
  
 What is real-time software? A list of friends gets updated the instant one joins or exits. 
 Traffic updates automatically stream into the smartphones of drivers looking for the 
 best route home. The sports page of an online newspaper immediately updates 
 scoreboards and standings as points are scored in an actual game. Users of this type of 
 software expect reactions to change to be communicated quickly, and this 
  
 expectation demands a particular focus on reducing network latency from 
  
 the software designer. Data I/O updates 
 must
  occur along subsecond time frames.
  
 Let's step back and consider the general characteristics of the Node environment and 
 community that make it an excellent tool for creating these kinds of responsive 
 network applications.
  
 Some validation of Node's design, it may be argued, is found in the enormous 
 community of open developers contributing enterprise-grade Node systems, some 
 teams financially supported and fully backed by Internet companies as large as 
 Microsoft, LinkedIn, eBay and others. Multicore, multiserver enterprise systems are 
 being created using free software mostly written in JavaScript.
  
 www.EBooksWorld.ir",NA
Introducing AJAX,"In 2005, Jesse James Garrett published an article in which he tried to condense the 
 changes he had been seeing in the way that websites were being designed into a 
 pattern. After studying this trend, Garrett proposed that dynamically updating pages 
 represented a new wave of software, resembling desktop software, and he coined the 
 acronym ""AJAX"" to describe the technological concept powering such rapid movement 
 toward ""web applications"".
  
 [
  149 
 ]
  
 www.EBooksWorld.ir",NA
Responding to calls,"If changes can be introduced into a web application without requiring a complete 
 reconstruction of state and state display, updating client information becomes cheaper. 
 The client and server can talk more often, regularly exchanging information. 
  
 Servers can recognize, remember, and respond immediately to client desires, aided by 
 reactive interfaces gathering user actions and reflecting the impact of those actions 
 within a UI in near real time.
  
 With AJAX, the construction of a multiuser environment supporting real-time updates to 
 each client's view on the overall application state involves regular polling of server by 
 clients checking for important updates:
  
 Polling 
  
 CLIENT
  
 NODE SERVER
  
  
 TIME 
  
 Network chatter is constant and often redundant.
  
 The significant drawback to this method of polling for state is that many of these 
 requests will be fruitless. The client has become a broken record, constantly asking for 
 status updates regardless of whether those updates are available or forthcoming. 
  
 When an application spends time or energy performing unnecessary tasks, there should 
 exist some clear benefit to the user or the provider (or both) offsetting this cost. 
 Additionally, each futile call adds to the cost of building up then tearing down HTTP 
 connections.
  
 Such a system can only take snapshots of the state at periodic intervals, and as that 
 polling interval may increase to several seconds in an effort to reduce redundant 
 network chatter, our awareness of state changes can begin to appear dull, just a little 
 behind the latest news.
  
 [
  151 
 ]
  
 www.EBooksWorld.ir",NA
Creating a stock ticker,"Ultimately, we will create an application that allows clients to pick a stock and watch for 
 changes in the data points related to that stock, such as its price, and to highlight positive 
 or negative changes:
  
 [
  152 
 ]
  
 www.EBooksWorld.ir",NA
Bidirectional communication with ,NA,NA
Socket.IO ,"We're already familiar with what sockets are. In particular, we know how to 
 establish and manage TCP socket connections using Node, as well as how to pipe 
 data through them bidirectionally or unidirectionally.
  
 [
  156 
 ]
  
 www.EBooksWorld.ir",NA
Using the WebSocket API,"Socket communication is efficient, only occurring when one of the parties has 
 something useful to say:
  
 Sockets 
  
 CLIENT
  
 NODE SERVER
  
  
 TIME 
  
 Communication is bi-directional and occurs only when necessary
  
 This lightweight model is an excellent choice for applications that require -frequency 
 message passing between a client and a server, such as found in multiplayer network 
 games or chat rooms.
  
 [
  157 
 ]
  
 www.EBooksWorld.ir",NA
Socket.IO,"As mentioned previously, Socket.IO aims to provide an emulation layer that will use the 
 native WebSocket implementation in browsers that support it, reverting to other 
 methods (such as long-polling) to simulate the native API in browsers that don't. 
  
 This is an important fact to keep in mind: the real benefits of WebSockets will not 
 exist for all client browsers.
  
 Nevertheless, Socket.IO does a very good job of hiding browser differences and remains a 
 good choice when the control flow made available by sockets is a desirable model of 
 communication for your application.
  
 [
  159 
 ]
  
 www.EBooksWorld.ir",NA
Drawing collaboratively,"Let's create a collaborative drawing application using Socket.IO and Node. We want to 
 create a blank canvas that will simultaneously display all the ""pen work"" being done by 
 connected clients:
  
  
 From the server end, there is very little to do. When a client updates coordinates by 
 moving their mouse, the server simply broadcasts this change to all connected 
 clients:
  
 io.sockets.on('connection', function(socket) {
  
  var id = socket.id;
  
  socket.on('mousemove', function(data) {
  
  data.id = id;
  
  socket.broadcast.emit('moving', data);
  
  });
  
  socket.on('disconnect', function() {
  
  socket.broadcast.emit('clientdisconnect', id);
  
  });
  
 });
  
 Socket.IO automatically generates a unique ID for each socket connection. We will pass 
 this ID along whenever new draw events occur, allowing the receiving clients to track 
 how many users are connected. Similarly, when a client disconnects, all other clients are 
 instructed to remove their references to this client. Later, we will see how this ID is used 
 within the application UI to maintain a pointer representing all connected clients.
  
 [
  161 
 ]
  
 www.EBooksWorld.ir",NA
Listening for Server Sent Events,"SSE are uncomplicated and specific. They are to be used when the majority of data 
 transfer proceeds unidirectionally from a server to clients. A traditional and similar 
 concept is the ""push"" technology. SSE passes text messages with simple formatting. 
  
 Many types of applications passively receive brief status updates or data state 
 changes. SSE is an excellent fit for these types of applications.
  
 [
  165 
 ]
  
 www.EBooksWorld.ir",NA
Using the EventSource API ,"The way in which 
 EventSource
  instances emit subscribable data events whenever 
 new data is received from the server is like the way 
 Readable
  streams emit data 
 events in Node, as we can see in this example client:
  
 <script>
  
  
  var eventSource = new EventSource('/login');
  
  
  eventSource.addEventListener('message', function(broadcast) {
  
  
 console.log(""got message: "" + broadcast);
  
  
  });
  
  
  eventSource.addEventListener('open', function() {
  
  
  console.log(""connection opened"");
  
  
  });
  
  
  eventSource.addEventListener('error', function() {
  
  
  console.log(""connection error/closed"");
  
  
  }); 
  
 </script> 
  
 [
  166 
 ]
  
 www.EBooksWorld.ir",NA
The EventSource stream protocol ,"Once a server has established a client connection, it may now send new messages across 
 this persistent connection at any time. These messages consist of one or more lines of 
 text, demarcated by one or several of the following four fields:
  
 • 
  
 • 
  
 • 
  
 • 
  
 event
 : This is an event type. Messages sent without this field will trigger the 
 client's general 
 EventSource
  event handler for any message. If set to a string 
 such as ""latestscore"", the client's 
 message
  handler will 
 not
  be called, with 
 handling being delegated to a handler bound using 
 EventSource.addEvent 
 Listener('latestscore'…)
 .
  
 data
 : This is the message being sent. This is always of the 
 String
  type, though 
 it can usefully transport objects passed through 
 JSON.stringify()
 .
  
 id
 : If set, this value will appear as the 
 lastEventID
  property of the sent 
 message object. This can be useful for ordering, sorting, and other operations on 
 the client.
  
 retry
 : The reconnection interval, in milliseconds.
  
 Sending messages involves composing strings containing relevant field names and 
 ending with newlines. These are all valid messages:
  
 response.write(""id:"" + (++message_counter) + ""\n""); 
  
 response.write(""data: I'm a message\n\n""); 
  
 response.write(""retry: 10000\n\n""); 
  
 response.write(""id:"" + (++message_counter) + ""\n""); 
  
 response.write(""event: stock\n""); 
  
 response.write(""data: "" + JSON.stringify({price: 100, change: -2}) + 
 ""\n\n""); 
  
 response.write(""event: stock\n""); 
  
 response.write(""data: "" + stock.price + ""\n""); 
  
 response.write(""data: "" + stock.change + ""\n""); 
  
 response.write(""data: "" + stock.symbol + ""\n\n""); 
  
 response.write(""data: Hello World\n\n"");
  
 [
  169 
 ]
  
 www.EBooksWorld.ir",NA
Asking questions and getting answers,"What if we wanted to create an interface to interests? Let's build an application enabling 
 any number of people to ask and/or answer questions. Our users will join the 
 community server, see a list of open questions and answers to those questions, and get 
 real-time updates whenever a new question or answer is added. There are two key 
 activities to model:
  
 • 
  
 • 
  
 Each client must be notified whenever another client asks a question or posts an 
 answer
  
 A client can ask questions or supply answers
  
  
 Where would the greatest amount of change happen in a large group of 
 simultaneous contributors to this community?
  
 Any individual client can potentially ask a few questions or provide a few answers. 
  
 Clients will also select questions and have the answers displayed to them. We will need 
 to satisfy merely a handful of client-to-server requests, such as when sending a new 
 question or answer to the server. Most of the work will be in satisfying client requests 
 with data (a list of answers to a question) and broadcasting application state changes to 
 all
  connected clients (new question added; new answer given). The one-to-many 
 relationship existing for clients within such collaborative applications implies that a 
 single client broadcast may create a number of server broadcasts equal to the number of 
 connected clients—1 to 10 K, or more. SSE is a great fit here, so let's get started.
  
 The three main operations for this application are as follows:
  
 • 
  
 Asking a question
  
 • 
  
 Answering a question
  
 • 
  
 Selecting a question
  
 [
  171 
 ]
  
 www.EBooksWorld.ir",NA
Building a collaborative document ,NA,NA
editing application,"Now that we've examined various techniques to consider when building a 
 collaborative application, let's put together a collaborative code editor using one of 
 the most exciting contributions to Node to arrive in the last year: 
 Operational 
 transformation
  (
 OT
 ).
  
 For our discussion here, OT will be understood as a technology that allows many 
 people to edit the same document concurrently—collaborative document editing. 
  
 Google described their (now defunct) Wave project in the following way:
  
 Collaborative document editing means multiple editors are able to edit a shared 
 document at the same time. It is live and concurrent when a user can see the 
 changes another person is making, keystroke by keystroke. Google Wave offers 
 live concurrent editing of rich text documents.
  
 Source: 
 http://www.waveprotocol.org/whitepapers/operational-transform
  
 One of the engineers involved in the Wave project was Joseph Gentle, and Mr. Gentle was 
 kind enough to write a module bringing OT technology to the Node community, named 
 ShareJS
  (
 www.sharejs.org
 ). We are going to use this module to create an application 
 that allows anyone to create a new collaboratively editable document.
  
  
 This example follows (and liberally borrows from) the 
  
  
 many examples contained in the ShareJS GitHub repository. 
  
 To delve deeper into the possibilities of ShareJS, visit 
  
 https://github.com/share/ShareJS
 .
  
 To begin with we will need a code editor to bind our OT layer to. For this project, we will 
 use the excellent Ace editor, which can be cloned from 
 https://github.com/ 
 ajaxorg/ace
 .
  
 Establishing an Ace editor doesn't require much more than cloning the repository 
 and writing the following HTML:
  
 <!DOCTYPE html>
  
 <html lang=""en"">
  
 <head>
  
  <meta charset=""UTF-8"">
  
  <title>Editor</title>
  
  <style type=""text/css"" media=""screen"">
  
  body {
  
 [
  178 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"In this chapter we've gone over three of the major strategies employed when building 
 real-time applications: AJAX, WebSocket, and SSE. We've shown that non-trivial 
 collaborative applications can be developed with very little code by using Node. We've 
 also seen how some of these strategies enable the modeling of client/server 
 communication as an evented data stream interface. We've considered the pros and 
 cons of these various techniques, and we've gone through some clear examples of the 
 best places to use each one.
  
 Additionally, we've shown how client identifiers and state date can be built and 
 managed within a Node server, such that state changes can be safely encapsulated in a 
 central location and broadcasted out to many connected clients safely and predictably. 
 Demonstrating the quality of the modules being developed with the Node community, 
 we created a collaborative code editing system through the use of operational 
 transformation.
  
 In the next chapter we will be looking at how to coordinate the efforts of multiple 
 Node processes running simultaneously. Through examples, we will learn how to 
 achieve parallel processing with Node, from spawning many child processes running 
 Unix programs to creating clusters of load-balancing Node socket servers.
  
 [
  182 
 ]
  
 www.EBooksWorld.ir",NA
Utilizing Multiple Processes,"""It is a very sad thing that nowadays there is so little useless information.""
  
 —Oscar Wilde
  
 The importance of I/O efficiency is not lost on those witnessing the rapidly 
  
 increasing volume of data being produced within a growing number of applications. User-
 generated content (blogs, videos, tweets, posts) is becoming the premier type of internet 
 content, and this trend has moved in tandem with the rise of social software, where 
 mapping the intersections between content generates an exponential rise in yet another 
 level of data.
  
 A number of data silos, such as Google, Facebook, and hundreds of others, expose their 
 data to the public through an API, often for free. These networks each gather astounding 
 volumes of content, opinions, relationships, and so forth from their users, data further 
 augmented by market research and various types of traffic and usage analysis. Most of 
 these APIs are two-way, 
 gathering
  and 
 storing
  data uploaded by their members as well as 
 serving
  that data.
  
 Node has arrived during this period of data expansion. In this chapter we will 
 investigate how Node addresses this need for sorting, merging, searching, and otherwise 
 manipulating large amounts of data. Fine-tuning your software, so that it can process 
 large amounts of data safely and inexpensively, is critical when building fast and 
 scalable network applications.
  
 We will deal with specific scaling issues in the next chapter. In this chapter we will 
 study some best practices when designing systems where multiple Node processes 
 work together on large volumes of data.
  
 www.EBooksWorld.ir",NA
Node's single-threaded model,"Taken in its entirety, the Node environment usefully demonstrates 
 both
  the efficiency of 
 multithreaded parallelism and an expressive syntax amenable to applications featuring 
 high concurrency. Using Node does not constrain the developer, the developer's access to 
 system resources, or the types of applications the developer might like to build.
  
 Nevertheless, a surprising number of persistent criticisms of Node are based on this 
 misunderstanding. As we'll see, the belief that Node is not multithreaded and is, 
 therefore, slow, or not ready for prime time, simply misses the point. JavaScript is 
 single-threaded; the Node stack is not. JavaScript represents the language used to 
 coordinate the execution of several multithreaded C++ processes, even the bespoke C++ 
 add-ons created by you, the developer. Node provides JavaScript, run through V8, 
 primarily as a tool for modeling concurrency. That, additionally, one can write an entire 
 application using just JavaScript is simply another benefit of the platform. You are never 
 stuck with JavaScript—you may write the bulk of your application in C++ if that is your 
 choice.
  
 In this chapter we will attempt to dismantle these misunderstandings, clearing the way 
 for optimistic development with Node. In particular, we will study techniques for 
 spreading effort across cores, processes, and threads. For now, this section will attempt to 
 clarify how much a single thread is capable of (hint: it's usually all you need).
  
 [
  185 
 ]
  
 www.EBooksWorld.ir",NA
The benefits of single-threaded programming,"You will be hard-pressed to find any significant number of professional software 
 engineers working on enterprise-grade software willing to deny that multithreaded 
 software development is painful. However, 
 why
  is it so hard to do well?
  
 It is not that multithreaded programming is difficult per se—the difficultly lies in the 
 complexity of thread synchronization. It is very difficult to build high concurrency using 
 the thread model, especially models in which the state is shared. Anticipating every way 
 that an action taken in one thread might affect all the others is nearly impossible once 
 an application grows beyond the most basic of shapes. 
  
 Entanglements and collisions multiply rapidly, sometimes corrupting shared 
 memory, sometimes creating bugs nearly impossible to track down.
  
 Node's designers chose to recognize the speed and parallelization advantages of threads 
 without demanding that developers did the same. In particular, Node's designers 
 wanted to save developers from managing the difficulties that accompany threaded 
 systems:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Shared memory and the locking behavior leads to systems that are very 
 difficult to reason about as they grow in complexity.
  
 Communication between tasks requires the implementation of a wide range of 
 synchronization primitives, such as mutexes and semaphores, condition 
 variables and so forth. An already challenging environment requires highly 
 complex tools, expanding the level of expertise necessary to complete even 
 relatively simple systems.
  
 Race conditions and deadlocks are common pitfalls in these sorts of systems. 
 Contemporaneous read and write operations within a shared program space 
 lead to problems of sequencing, where two threads may be in an unpredictable 
 ""race"" for the right to influence a state, event, or other key system characteristic.
  
 Because maintaining dependable boundaries between threads and their states is 
 so difficult, ensuring that a library (what for Node would be a ""module"") is 
 thread-safe consumes a great deal of developer time. Can I know that this library 
 will not destroy some part of my application? Guaranteeing thread safety 
 requires great diligence on the part of a library's developer and these guarantees 
 may be conditional: for example, a library may be thread-safe when reading—
 but not when writing.
  
 [
  186 
 ]
  
 www.EBooksWorld.ir",NA
X,NA,NA
X,NA,NA
X,"Threads, each thread 
  
 sharing state, requiring access 
 blocking and other exclusions
  
 MULTI-THREADING
  
 Independent processes no shared state. 
 Filtered data passed via messaging–
  
 no collisions
  
 SINGLE-THREADED, 
  
 ASYNCHRONOUS
  
  
 [
  187 
 ]
  
 www.EBooksWorld.ir",NA
Multithreading is already native and ,NA,NA
transparent,"Node's I/O thread pool executes within the OS scope, and its work is distributed across 
 cores (just as any other job scheduled by the OS would be similarly 
  
 distributed). When you are running Node, you are already taking advantage of its 
 multithreaded execution.
  
 In the upcoming discussion of child processes and the 
 Cluster
  module, we will see this 
 style of parallelism—of multiple parallel processes—in action. We will see how Node is 
 not denied the full power of an OS.
  
 As we saw earlier, when discussing Node's core architecture, the V8 thread in which 
 one executes JavaScript programs is bound to 
 libuv
 , which functions as the main, 
 system-level, I/O event dispatcher. In this capacity, 
 libuv
  handles the timers, 
 filesystem calls, network calls, and other I/O operations requested by the relevant 
 JavaScript process or module commands, such as 
 fs.readFile
 , 
 http.createServer
 , and so on. Therefore, the main V8 event loop is best 
 understood as a control-flow programming interface, supported and powered by the 
 highly-efficient, multithreaded, system delegate 
 libuv
 .
  
 Burt Belder, one of Node's core contributors, is also one of the core contributors to 
 libuv
 . In fact, Node's development has provoked a simultaneous increase in 
 libuv
  
 development, a feedback loop that has only improved the speed and stability of both 
 projects. It has merged and replaced the 
 libeo
  and 
 libev
  libraries that formed the 
 original core of Node's stack.
  
 Consider another of Raymond's rules, the 
 Rule of Separation
 : ""Separate policy from 
 mechanism; separate interfaces from engines"". The engine that powers Node's 
 asynchronous, event-driven style of programming is 
 libuv
 ; the interface to that engine 
 is V8's JavaScript runtime. Continuing with Raymond:
  
 One way to effect that separation is, for example, to write your application as a 
 library of C service routines that are driven by an embedded scripting language, 
 with the application flow of control written in the scripting language rather than 
 C.
  
 [
  189 
 ]
  
 www.EBooksWorld.ir",NA
Creating child processes,"Software development is no longer the realm of monolithic programs. Applications 
 running on networks cannot forego interoperability. Modern applications are 
  
 distributed and decoupled. We now build applications that connect users with resources 
 distributed across the Internet. Many users are accessing shared resources 
 simultaneously. A complex system is easier to understand if the whole is understood as 
 a collection of interfaces to programs that solve one or a few clearly defined, related 
 problems. In such a system it is expected (and desirable) that processes do not sit idle.
  
 An early criticism of Node was that it did not have multicore awareness. That is, if a 
 Node server were running on a machine with several cores, it would not be able to 
 take advantage of this extra horsepower. Within this seemingly reasonable criticism 
 hid an unjustified bias based on a straw man: a program that is unable to explicitly 
 allocate memory and execution ""threads"" in order to implement parallelization cannot 
 handle enterprise-grade problems.
  
 This criticism is a persistent one. It is also not true.
  
 While a single Node process runs on a single core, any number of Node processes can 
 be ""spun up"" through use of the 
 child_process
  module. Basic usage of this module is 
 straightforward: we fetch a 
 ChildProcess
  object, and listen for data events. This 
 example will call the Unix command 
 ls
 , listing the current directory:
  
 var spawn = require('child_process').spawn;
  
 var ls    = spawn('ls', ['-lh', '.']);
  
 ls.stdout.on('readable', function() {
  
  var d = this.read();
  
  d && console.log(d.toString());
  
 });
  
 ls.on('close', function(code) {
  
  console.log('child process exited with code ' + code);
  
 });
  
 [
  190 
 ]
  
 www.EBooksWorld.ir",NA
Spawning processes,"This powerful command allows a Node program to start and interact with processes 
 spawned via system commands. In the preceding example, we used 
 spawn
  to call a 
 native OS process, 
 ls
 , passing that command the 
 lh
  and 
 .
  arguments. In this way, any 
 process can be started just as one might start it via a command line. The method takes 
 three arguments:
  
 • 
  
 command
 : A command to be executed by the OS shell
  
 • 
  
 arguments
  (optional): These are command-line arguments, sent as an array
  
 • 
  
 options
 : An optional map of settings for 
 spawn
  
 The options for 
 spawn
  allow its behavior to be carefully customized:
  
 • 
  
 • 
  
 cwd
  (String): By default, the command will understand its current working 
 directory to be the same as that of the Node process calling 
 spawn
 . Change 
 that setting using this directive.
  
 env
  (Object): This is used to pass environment variables to a child process. 
 For instance, consider spawning a child with an environment object such as 
 the following:
  
 {
  
  name : ""Sandro"",
  
  role : ""admin""
  
 }
  
 The child process environment will have access to these values.
  
 • 
  
 • 
  
 • 
  
 detached
  (Boolean): When a parent spawns a child, both processes form a 
 group, and the parent is normally the leader of that group. To make a child the 
 group leader, use 
 detached
 . This will allow the child to continue running even 
 after the parent exits. This is because the parent will wait for the child to exit by 
 default. You can call 
 child.unref()
  to tell the parent's event loop that it should 
 not count the child reference, and exit if no other work exists.
  
 uid
  (Number): Set the 
 uid
  (user identity) directive for the child process, in 
 terms of standard system permissions, such as a UID that has execute 
 privileges on the child process.
  
 gid
  (Number): Set the 
 gid
  (group identity) directive for the child process, in 
 terms of standard system permissions, such as a GID that has execute 
 privileges on the child process.
  
 [
  192 
 ]
  
 www.EBooksWorld.ir",NA
Forking processes,"Like 
 spawn
 , 
 fork
  starts a child process, but is designed for running Node programs with 
 the added benefit of having a communication channel built in. Rather than passing a 
 system command to 
 fork
  as its first argument, one passes the path to a Node program. 
 As with 
 spawn
 , command-line options can be sent as a second argument, accessible via 
 process.argv
  in the forked child process.
  
 An optional options object can be passed as its third argument, with the following 
 parameters:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 cwd
  (String): By default, the command will understand its current working 
 directory to be the same as that of the Node process calling 
 fork
 . Change that 
 setting using this directive.
  
 env
  (Object): This is used to pass environment variables to a child process. 
 See 
 spawn
 .
  
 encoding
  (String): This sets the encoding of the communication channel.
  
 execPath
  (String): This is the executable used to create the child process.
  
 silent
  (Boolean): By default, a forked child will have its 
 stdio
  associated 
 with the parent's (
 child.stdout
  is identical to 
 parent.stdout
 , for 
 example). Setting this option to 
 true
  disables this behavior.
  
 An important difference between 
 fork
  and 
 spawn
  is that the former's child process 
 does not automatically exit
  when it is finished. Such a child must explicitly exit when it 
 is done, easily accomplished via 
 process.exit()
 .
  
 [
  195 
 ]
  
 www.EBooksWorld.ir",NA
Buffering process output ,"In cases where the complete buffered output of a child process is sufficient, with no 
 need to manage data through events, 
 child_process
  offers the 
 exec
  method. The 
 method takes three arguments:
  
 • 
  
 • 
  
 command
 : A command-line string. Unlike 
 spawn
  and 
 fork
 , which pass 
 arguments to a command via an array, this first argument accepts a full 
 command string, such as 
 ps aux | grep node
 .
  
 options
 : This is an optional argument.
  
 °
  
 cwd
  (String): This sets the working directory for the command 
  
 process.
  
 °
  
 env
  (Object): This is a map of key-value pairs that will be exposed 
  
 to the child process.
  
 °
  
 encoding
  (String): This is the encoding of the child's data stream. 
  
 The default value is 
 'utf8'
 .
  
 °
  
 timeout
  (Number): This specifies the milliseconds to wait for the 
  
 process to complete, at which point the child process will be sent 
 the 
 killSignal.maxBuffer
  value.
  
 °
  
 killSignal.maxBuffer
  (Number): This is the maximum number 
  
 of bytes allowed on 
 stdout
  or 
 stderr
 . When this number is 
 exceeded, the process is killed. This default is 200 KB.
  
 [
  197 
 ]
  
 www.EBooksWorld.ir",NA
Communicating with your child,"All instances of the 
 ChildProcess
  object extend 
 EventEmitter
 , exposing events useful 
 for managing child data connections. Additionally, 
 ChildProcess
  objects expose some 
 useful methods for interacting with children directly. Let's go through those now, 
 beginning with attributes and methods:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 child.connected
 : When a child is disconnected from its parent via 
 child.disconnect()
 , this flag will be set to 
 false
 .
  
 child.stdin
 : This is a 
 WritableStream
  corresponding to the child's 
 standard in.
  
 child.stdout
 : This is a 
 ReadableStream
  corresponding to the child's 
 standard out.
  
 child.stderr
 : This is a 
 ReadableStream
  corresponding to the child's 
 standard error.
  
 child.pid
 : This is an integer representing the process ID (PID) assigned to 
 the child process.
  
 child.kill
 : This tries to terminate a child process, sending it an optional signal. 
 If no signal is specified, the default is 
 SIGTERM
  (for more about signals, see 
 http://unixhelp.ed.ac.uk/CGI/man-cgi?signal+7
 ). While the method 
 name sounds terminal, it is not guaranteed to kill a process—it only sends a 
 signal to a process. Dangerously, if 
 kill
  is attempted on a process that has 
 already exited, it is possible that another process that has been newly assigned 
 the PID of the dead process will receive the signal, with indeterminable 
 consequences. This method should fire a 
 close
  event, which the signal used to 
 close the process.
  
 [
  198 
 ]
  
 www.EBooksWorld.ir",NA
Sending messages to children,"As we saw in our discussion of 
 fork
 , and when using the 
 ipc
  option on 
 spawn
 , child 
 processes can be sent messages via 
 child.send
 , with the message passed as the first 
 argument. A TCP server, or socket handle, can be passed along with the message as a 
 second argument. In this way, a TCP server can spread requests across multiple child 
 processes. For example, the following server distributes socket handling across a 
 number of child processes equaling the total number of CPUs available. Each forked 
 child is given a unique ID, which it reports when started. Whenever the TCP server 
 receives a socket, that socket is passed as a handle to a random child process. That child 
 process then sends a unique response, demonstrating that socket handling is being 
 distributed.
  
 • 
  
 Parent
  
 var fork = require('child_process').fork;
  
 var net = require('net');
  
 var children = [];
  
 require('os').cpus().forEach(function(f, idx) {
  
  children.push(fork(""./child.js"", [idx]));
  
 });
  
 net.createServer(function(socket) { 
  
  var rand = Math.floor(Math.random() * children.length);
  
  children[rand].send(null, socket);
  
 }).listen(8080);
  
 • 
  
 Child
  
 var id = process.argv[2];
  
 process.on('message', function(n, socket) {
  
  socket.write('child ' + id + ' was your server today.\r\n');
  
  socket.end();
  
 });
  
 [
  199 
 ]
  
 www.EBooksWorld.ir",NA
Parsing a file using multiple processes,"One of the tasks many developers will take on is the building of a logfile processor. A 
 logfile can be very large and many megabytes long. Any single program working on a 
 very large file can easily run into memory problems or simply run much too slowly. It 
 makes sense to process a large file in pieces. We're going to build a simple log processor 
 that breaks up a big file into pieces and assigns one to each of several child workers, 
 running them in parallel.
  
 The entire code for this example can be found in the 
 logproc
  folder of the code 
 bundle. We will focus on the main routines:
  
 • 
  
 Determining the number of lines in the logfile
  
 • 
  
 Breaking those up into equal chunks
  
 • 
  
 Creating one child for each chunk and passing it parse instructions
  
 • 
  
 Assembling and displaying the results
  
 To get the word count of our file, we use the 
 wc
  command with 
 child.exec
  as 
 shown in the following code:
  
 child.exec(""wc -l "" + filename, function(e, fL) {
  
  fileLength = parseInt(fL.replace(filename, """"));
  
  var fileRanges = [];
  
  var oStart = 1;
  
  var oEnd = fileChunkLength;
  
  while(oStart < fileLength) {
  
  fileRanges.push({
  
  offsetStart : oStart,
  
  offsetEnd   : oEnd
  
  })
  
 [
  200 
 ]
  
 www.EBooksWorld.ir",NA
Using the cluster module,"As we saw when processing large logfiles, the pattern of a master parent controller for 
 many child processes is just right for vertical scaling in Node. In response to this, the 
 Node API has been augmented by a 
 cluster
  module, which formalizes this pattern and 
 helps to make its achievement easier. Continuing with Node's core purpose of helping to 
 make scalable network software easier to build, the particular goal of cluster is to 
 facilitate the sharing of network ports amongst many children.
  
 For example, the following code creates a cluster of worker processes all sharing the 
 same HTTP connection:
  
 var cluster = require('cluster');
  
 var http = require('http');
  
 var numCPUs = require('os').cpus().length;
  
 if(cluster.isMaster) {
  
  for(var i = 0; i < numCPUs; i++) {
  
  cluster.fork();
  
  }
  
 }
  
 if(cluster.isWorker) {
  
  http.createServer(function(req, res) {
  
  res.writeHead(200);
  
  res.end(""Hello from "" + cluster.worker.id);
  
  }).listen(8080);
  
 }
  
 We'll dig into the details shortly. For now, notice that 
 cluster.fork
  has taken zero 
 arguments. What does 
 fork
  without a command or file argument do? Within a cluster, 
 the default action is to fork the current program. We see during 
 cluster.
  
 isMaster
 , the action is to fork children (one for each available CPU). When this 
 program is re-executed in a forking context, 
 cluster.isWorker
  will be 
 true
  and a 
 new HTTP server 
 running on a shared port
  is started. Multiple processes are sharing 
 the load for a single server.
  
 Start and connect to this server with a browser. You will see something like 
 Hello 
 from 8
 , the integer corresponding to the unique 
 cluster.worker.id
  value of the 
 worker that assigned responsibility for handling your request. Balancing across all 
 workers is handled automatically, such that refreshing your browser a few times will 
 result in different worker IDs being displayed.
  
 [
  203 
 ]
  
 www.EBooksWorld.ir",NA
Cluster events ,"The cluster object emits several events listed as follows:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 fork
 : This is fired when the master tries to fork a new child. This is not the 
 same as 
 online
 . This receives a worker object.
  
 online
 : This is fired when the master receives notification that a child is fully 
 bound. This differs from the 
 fork
  event and receives a worker object.
  
 listening
 : When the worker performs an action that requires a 
 listen() 
 call (such as starting an HTTP server), this event will be fired in the master. 
  
 The event emits two arguments: a worker object, and the address object 
 containing the 
 address
 , 
 port
 , and 
 addressType
  values of the connection.
  
 disconnect
 : This is called whenever a child disconnects, which can happen 
 either through process exit events or after calling 
 child.kill()
 . This will fire 
 prior to the 
 exit
 event—they are not the same. This receives a worker object.
  
 exit
 : Whenever a child dies this event is emitted. The event receives three 
 arguments: a worker object, the exit code number, and the signal string, such as 
 SIGNUP
 , which caused the process to be killed.
  
 setup
 : This is called after 
 cluster.setupMaster
  has executed.",NA
Worker object properties ,"Workers have the following attributes and methods:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 worker.id
 : This is the unique ID assigned to a worker, also representing the 
 worker's key in the 
 cluster.workers
  index.
  
 worker.process
 : This specifies a 
 ChildProcess
  object referencing a worker.
  
 worker.suicide
 : The workers that have recently had 
 kill
  or 
 disconnect 
 called on them will have their 
 suicide
  attribute set to 
 true
 .
  
 worker.send(message, [sendHandle])
 : Refer to 
 child_process.fork()
 , 
 which is previously mentioned.
  
 worker.kill([signal])
 : This kills a worker. The master can check this 
 worker's 
 suicide
  property in order to determine if the death was intentional or 
 accidental. The default 
 signal
  value that is sent is 
 SIGTERM
 .
  
 worker.disconnect()
 : This instructs a worker to disconnect. Importantly, 
 existing connections to the worker are not immediately terminated (as with 
 kill
 ), but are allowed to exit normally prior to the worker fully disconnecting. 
 This is because existing connections may stay in existence for a very long time. It 
 is a good pattern to regularly check if the worker has actually disconnected, 
 perhaps using timeouts.
  
 [
  205 
 ]
  
 www.EBooksWorld.ir",NA
Worker events,"Workers also emit events, such as the ones mentioned in the following list:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 message
 : See 
 child_process.fork 
  
 online
 : This is identical to 
 cluster.online
 , except that the check is against 
 only the specified worker 
  
 listening
 : This is identical to 
 cluster.listening
 , except that the check is 
 against only the specified worker 
  
 disconnect
 : This is identical to 
 cluster.disconnect
 , except that the check is 
 against only the specified worker 
  
 exit
 : See the 
 exit
  event for 
 child_process 
  
 setup
 : This is called after 
 cluster.setupMaster
  has executed
  
 Now, using what we now know about the cluster module, let's implement a real-
 time tool for analyzing the streams of data emitted by many users 
 simultaneously interacting with an application.",NA
Real-time activity updates of multiple ,NA,NA
worker results,"Using what we've learned we are going to construct a multiprocess system to track the 
 behavior of all visitors to a sample web page. This will be composed of two main 
 segments: a WebSocket-powered client library, which will broadcast each time a user 
 moves a mouse, and an administration interface visualizing user interaction as well as 
 when a user connects and disconnects from the system. Our goal is to show how a more 
 complex system might be designed (such as one that tracks and graphs every click, 
 swipe, or other interaction a user might make). The final administration interface will 
 show activity graphs for several users and resemble this:
  
 [
  206 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"This is the first chapter where we've really begun to test Node's 
 scalability
  goal. Having 
 considered the various arguments for and against different ways of thinking about 
 concurrency and parallelism, we arrived at an understanding of how Node has 
 successfully maintained the advantages of threading and parallel processing while 
 wrapping all of that complexity within a concurrency model that is both easy to reason 
 about and robust.
  
 Having gone deeper into how processes work, and in particular how child processes can 
 communicate with each other, even spawn further children, we looked at two use cases. 
 An example of how to combine native Unix command processes seamlessly with custom 
 Node processes led us to a performant and straightforward technique for processing large 
 files. The 
 cluster
  module was then applied to the problem of how to share responsibility 
 for handling a busy socket between multiple workers, this ability to share socket handles 
 between processes demonstrating a powerful aspect of Node's design.
  
 Having seen how Node applications might be scaled vertically, we can now look into 
 horizontal scaling across many systems and servers. In the next chapter we'll learn how to 
 connect Node with third-party services, such as Amazon and Facebook, communicate 
 across networks with message queues, set up multiple Node servers behind proxies, and 
 more.
  
 [
  212 
 ]
  
 www.EBooksWorld.ir",NA
Scaling Your Application,"Evolution is a process of constant branching and expansion.
  
 —Stephen Jay Gould
  
 Scalability and performance are not the same thing:
  
 The terms ""performance"" and ""scalability"" are commonly used interchangeably, 
 but the two are distinct: performance measures the speed with which a single 
 request can be executed, while scalability measures the ability of a request to 
 maintain its performance under increasing load. For example, the performance of 
 a request may be reported as generating a valid response within three seconds, 
 but the scalability of the request measures the request's ability to maintain that 
 three-second response time as the user load increases.
  
 —Steven Haines, ""Pro Java EE 5""
  
 In the previous chapter we looked at how Node clusters might be used to increase the 
 performance of an application. Through the use of clusters of processes and workers 
 we've learned how to efficiently deliver results in the face of many simultaneous 
 requests. We learned to scale Node 
 vertically
 , keeping the same footprint (a single 
 server) and increasing throughput by piling on the power of available CPUs.
  
 In this chapter we will focus on 
 horizontal
  scalability: the idea is that an application 
 composed of self-sufficient and independent units (servers) can be scaled by adding 
 more units without altering the application's code.
  
 www.EBooksWorld.ir",NA
When to scale?,"The theory around application scaling is a complex and interesting topic that continues 
 to be refined and expanded. A comprehensive discussion of the topic would require 
 several books, curated for different environments and needs. For our purposes we will 
 simply learn how to recognize when scaling up (or even scaling down) is necessary.
  
 Having a flexible architecture that can add and subtract resources as needed is 
 essential to a resilient scaling strategy. A vertical scaling solution does not always 
 always suffice (simply adding memory or CPUs will not deliver the necessary 
 improvements). When should horizontal scaling be considered?
  
 [
  214 
 ]
  
 www.EBooksWorld.ir",NA
Network latency,"When network response times are exceeding some threshold, such as each request 
 taking several seconds, it is likely that the system has gone well past a stable state.
  
 While the easiest way to discover this problem is to wait for customer complaints about 
 slow websites, it is better to create controlled stress tests against an equivalent 
 application environment or server.
  
 AB
  (
 Apache Bench
 ) is a simple and straightforward way to do blunt stress tests 
 against a server. This tool can be configured in many ways, but the kind of test you 
 would do for measuring the network response times for your server is generally 
 straightforward.
  
 For example, let's test the response times for this simple Node server:
  
 http.createServer(function(request, response) {
  
  response.writeHeader(200, {""Content-Type"": ""text/plain""});  
  
  response.write(""Hello World"");  
  
  response.end();  
  
 }).listen(2112)
  
 Here's how one might test running 10000 requests against that server, with a 
 concurrency of 100 (the number of simultaneous requests):
  
 ab -n 10000 -c 100 http://yourserver.com/
  
 If all goes well, you will receive a report similar to this:
  
 Concurrency Level:      100
  
 Time taken for tests:   9.658 seconds
  
 Complete requests:      10000
  
 Failed requests:        0
  
 Write errors:           0
  
 Total transferred:      1120000 bytes
  
 HTML transferred:       110000 bytes
  
 Requests per second:    1035.42 [#/sec] (mean)
  
 [
  215 
 ]
  
 www.EBooksWorld.ir",NA
Hot CPUs ,"When CPU usage begins to nudge maximums, start to think about increasing the 
 number of units processing client requests. Remember that while adding one new CPU 
 to a single-CPU machine will bring immediate and enormous improvements, adding 
 another CPU to a 32-core machine will not necessarily bring an equal improvement. 
 Slowdowns are not always about slow calculations.
  
 [
  216 
 ]
  
 www.EBooksWorld.ir",NA
Socket usage,"When the number of persistent socket connections begins to grow past the capacity of 
 any single Node server, however optimized, it will be necessary to think about 
 spreading out the servers handling user sockets. Using 
 socket.io
  it is possible to 
 check the number of connected clients at any time using the following command:
  
 io.sockets.clients()
  
 In general it is best to track web socket connection counts within the application, via 
 some sort of tracking/logging system.",NA
Many file descriptors,"When the number of file descriptors opened in an OS hovers close to its limit it is likely 
 that an excessive number of Node processes are active, files are open, or other file 
 descriptors (such as sockets or named pipes) are in play. If these high numbers are not 
 due to bugs or a bad design, it is time to add a new server.
  
 Checking the number of open file descriptors, of any kind, can be accomplished 
 using 
 lsof
 :
  
 # lsof | wc –l     // 1345",NA
Data creep,"When the amount of data being managed by a single database server begins to exceed 
 many millions of rows or very many gigabytes of memory, it is time to think about 
 scaling. Here you might choose to simply dedicate a single server to your database, 
 begin to shard databases, or even move into a managed cloud storage solution earlier 
 rather than later. Recovering from a data layer failure is rarely a quick fix, and in general 
 it is dangerous to have a single point of failure for something as important as 
 all of your 
 data
 .
  
 [
  218 
 ]
  
 www.EBooksWorld.ir",NA
Tools for monitoring servers,"There are several tools available for monitoring servers, but few designed specifically 
 for Node. One strong candidate is 
 StrongOps
  (
 http://strongloop.com/ 
  
 strongloop-suite/strongops/
 ) from 
 StrongLoop
 , a company founded by the key 
 contributors of Node's core. This cloud service is easily integrated with a Node app, 
 offering a useful dashboard visualizing CPU usage, average response times, and more.
  
 Other good monitoring tools to consider are listed in the following:
  
 • 
  
 Scout
 : 
 https://scoutapp.com/
  
 • 
  
 Nagios
 : 
 http://www.nagios.org/
  
 • 
  
 Munin
 : 
 http://munin-monitoring.org/
  
 • 
  
 Monit
 : 
 http://mmonit.com/monit/",NA
Running multiple Node servers,"It is easy to purchase several servers and then to run some Node processes on them. But 
 how can those distinct servers be coordinated such that they form part of a single 
 application? One aspect of this problem concerns clustering multiple identical servers 
 around a single entry point. How can client connections be shared across a pool of 
 servers?",NA
Forward and reverse proxies,"A 
 proxy
  is someone or something acting on behalf of another.
  
 A 
 forward proxy
  normally works on behalf of clients in a private network, brokering 
 requests to an outside network, such as retrieving data from the Internet. Earlier in this 
 book we looked at how one might set up a proxy server using Node, where the Node 
 server functioned as an intermediary, forwarding requests from clients to other network 
 servers, usually via the Internet. Early web providers such as AOL functioned in the 
 following way:
  
 [
  220 
 ]
  
 www.EBooksWorld.ir",NA
Nginx as a proxy,"Nginx (pronounced as ""Engine X"") is a popular choice when hiding Node servers behind 
 a proxy. Nginx is a very popular high-performance web server that is often used as a 
 proxy server. There is some serendipity in the fact that Nginx is a popular choice with 
 Node developers, given its design:
  
 Nginx is able to serve more requests per second with [fewer] resources because 
 of its architecture. It consists of a master process, which delegates work to one 
 or more worker processes. Each worker handles multiple requests in an event-
 driven or asynchronous manner using special functionality from the Linux 
 kernel (epoll/ select/poll). This allows Nginx to handle a large number of 
 concurrent requests quickly with very little overhead.
  
 —
 http://www.linuxjournal.com/magazine/nginx-high-performance-
  
 web-server-and-reverse-proxy
  
 Nginx also makes simple load balancing very easy. In our examples we will see how 
 proxying through Nginx comes with load balancing ""out of the box"".
  
 It is assumed that you have installed Nginx (if not, please visit 
 http://wiki.nginx. 
 org/Main
  to get started). We must now augment the 
 nginx.conf
  file, in particular the 
 http
  section. Our goal is to create a single Nginx server whose 
 sole
  responsibility is to 
 evenly distribute client requests across some number of Node servers, such that 
 capacity for the entire system can be scaled without changing anything about how a 
 client can or should connect to our application:
  
 internet request
  
 Node one.example.com
  
 upstream
  
 one.example.com
  
 two.example.com 
 Node two.example.com
  
 one.example.com
  
 Node three.example.com
  
 NGINX
  
 [
  222 
 ]
  
 www.EBooksWorld.ir",NA
Using HTTP Proxy ,"Node is designed to facilitate the creation of network software, so it comes as no 
 surprise that several proxying modules have been developed. The team at 
 NodeJitsu 
 has released the proxy they use in production, 
 HTTP proxy
 . Let's take a look at how we 
 would use it to route requests to different Node servers.
  
 Unlike with Nginx, the entirety of our routing stack will exist in Node. One Node server 
 will be running our proxy, listening on port 80. We'll cover the following three scenarios:
  
 • 
  
 Running multiple Node servers on separate ports on the same machine.
  
 • 
  
 Using one box as a pure router, proxying to external URLs. 
  
 • 
  
 Creating a basic round-robin load balancer.
  
 As an initial example, let's look at how to use this module to redirect requests:
  
 var httpProxy = require('http-
 proxy'); 
  
 var proxy = httpProxy.createServer({ 
  
 target: {
  
  
  host: 'www.example.com',
  
  
  port: 80 
  
 } 
  
 }).listen(80);
  
 By starting this server on port 80 of our local machine, we are able redirect the user to 
 another URL.
  
 To run several distinct Node servers, each responding to a different URL, on a single 
 machine, one simply has to define a router:
  
 var httpProxy = httpProxy.createServer({
  
  router: {
  
  
  
  'www.mywebsite.com'        : 
 '127.0.0.1:8001',
  
  
  'www.myothersite.com'    
 : '127.0.0.1:8002', } 
  
 }); 
  
 httpProxy.listen(80);
  
 [
  225 
 ]
  
 www.EBooksWorld.ir",NA
Message queues – RabbitMQ,"One of the best ways to ensure that distributed servers maintain a dependable 
 communication channel is to bundle the complexity of remote procedure calls into a 
 distinct unit—a messaging queue. When one process wishes to send a message to another 
 process, the message can simply be placed on this queue—like a to-do list for your 
 application—with the queue service doing the work of ensuring messages get delivered 
 as well as delivering any important replies back to the original sender.
  
 There are a few enterprise-grade message queues available, many of them deploying 
 AMQP 
 (Advanced Message Queueing Protocol)
 . We will focus on a very stable and 
 well-known implementation: RabbitMQ.
  
  
 To install RabbitMQ in your environment follow the instructions found 
  
  
 in the website: 
 http://www.rabbitmq.com/download.html
 .
  
 You will also need to install 
 Erlang
  (instructions given under the 
  
 Official Clients
  section at the preceding link).
  
 Once installed you will start the RabbitMQ server with this command:
  
 service rabbitmq-server start
  
 To interact with RabbitMQ using Node we will use the node-amqp module created by 
 Theo Schlossnagle
 :
  
 npm install amqp
  
 To use a message queue one must first create a consumer—a binding to RabbitMQ 
 that will listen for messages published to the queue. The most basic consumer will 
 listen for all messages:
  
 var amqp = require('amqp');
  
 var consumer = amqp.createConnection({ host: 'localhost', port: 5672 
 });
  
 var exchange;
  
 consumer.on('ready', function() {
  
  exchange = consumer.exchange('node-topic-exchange', {type:  
  
  ""topic""});
  
  consumer.queue('node-topic-queue', function(q) {
  
  q.bind(exchange, '#');
  
  q.subscribe(function(message) {
  
  //    Messages are buffers
  
  console.log(message.data.toString('utf8'));
  
  });        
  
  });
  
 });
  
 [
  227 
 ]
  
 www.EBooksWorld.ir",NA
Types of exchanges,"RabbitMQ provides three types of exchanges: 
 direct
 , 
 fanout
 , and 
 topic
 . The 
 differences appear in the way each type of exchange processes 
 routing keys
 —the 
 first argument sent to 
 exchange.publish
 .
  
 A direct exchange matches routing keys directly. A queue binding like the following one 
 matches 
 only
  messages sent to 
 'room-1'
 :
  
 queue.bind(exchange, 'room-1');
  
 Because no parsing is necessary, direct exchanges are able to process more messages 
 than topic exchanges in a set period of time.
  
 A fanout exchange is indiscriminate; it routes messages to all of the queues bound to it, 
 ignoring routing keys. This type of exchange is used for wide broadcasts.
  
 A topic exchange matches routing keys based on the wildcards 
 #
  and 
 *
 . Unlike other 
 types, routing keys for topic exchanges 
 must
  be composed of words separated by dots—
 ""animals.dogs.poodle""
 , for example. A 
 #
  matches zero or more words—it will match 
 every message (as we saw in the previous example), just like a fanout exchange. The 
 other wildcard is *, and this matches 
 exactly
  one word.
  
 [
  228 
 ]
  
 www.EBooksWorld.ir",NA
Using Node's UDP module ,"UDP(User Datagram Protocol)
  is a lightweight core Internet messaging protocol, 
 enabling servers to pass around concise 
 datagrams
 . UDP was designed with 
  
 a minimum of protocol overhead, forgoing delivery, ordering, and duplication prevention 
 mechanisms in favor of ensuring high performance. UDP is a good choice when perfect 
 reliability is not required and high-speed transmission is, such as what is found in 
 networked video games and videoconferencing applications.
  
 This is not to say that UDP is normally unreliable. In most applications it delivers 
 messages with high probability. It is simply not suitable when 
 perfect
  reliability is 
 needed, such as in a banking application. It is an excellent candidate for monitoring 
 and logging applications, and for non-critical messaging services.
  
 Creating a UDP server with Node is straightforward:
  
 var dgram = require(""dgram""); 
  
 var socket = dgram.createSocket(""udp4""); 
  
 socket.on(""message"", function(msg, info) {
  
  console.log(""socket got: "" + msg + "" from "" + 
 info.address + "":"" + info.port);
  
 [
  230 
 ]
  
 www.EBooksWorld.ir",NA
UDP multicasting with Node,"The only difference between setting up a multicasting UDP server and a ""standard"" one 
 is binding to a special UDP port for sending, and indicating that we'd like to listen to 
 all
  
 available network adapters. Our multicasting server initialization looks like the 
 following code snippet:
  
 var socket = dgram.createSocket('udp4');
  
 var multicastAddress     = '230.1.2.3';
  
 var multicastPort     = 5554;
  
 socket.bind(multicastPort);
  
 socket.on(""listening"", function() {
  
  this.setMulticastTTL(64);
  
  this.addMembership(multicastAddress);
  
 });
  
 Once we've decided on a multicast port and an address and have bound, we catch the 
 listenng
  event and configure our server. The most important command is 
 socket.addMembership
 , which tells the kernel to join the multicast group at 
 multicastAddress
 . Other UDP sockets can now subscribe to the multicast group at 
 this address.
  
 Datagrams hop through networks just like any network packet. The 
  
 setMulticastTTL
  method is used to set the maximum number of hops (Time To Live) a 
 datagram is allowed to make before it is abandoned, and not delivered. The acceptable 
 range is 0-255, with the default being one (1) on most systems. This is not normally a 
 setting one need worry about, but it is available when precise limits make sense, such as 
 when packets are cheap and hops are costly.
  
  
 If you'd like to also allow listening on the local interface, 
  
  
 use 
 socket.setBroadcast(true)
  and 
 socket.
  
 setMulticastLoopback(true)
 . This is normally not necessary.
  
 We are eventually going to use this server to broadcast messages to all UDP listeners on 
 multicastAddress
 . For now, let's create two clients that will listen for multicasts:
  
 dgram.createSocket('udp4')
  
 .on('message', function(message, remote) {
  
  console.log('Client1 received message ' + message + ' from ' +  
  
  remote.address + ':' + remote.port);
  
 })
  
 .bind(multicastPort, multicastAddress);
  
 dgram.createSocket('udp4')
  
 .on('message', function(message, remote) {
  
 [
  233 
 ]
  
 www.EBooksWorld.ir",NA
Using Amazon Web Services in your ,NA,NA
application,"As a few thousand users become a few million users, as databases scale to terabytes of 
 data, the cost and complexity of maintaining an application begins to overwhelm teams 
 with insufficient experience, funding, and/or time. When faced with rapid growth it is 
 sometimes useful to delegate responsibilities for one or more aspects of your 
 application to cloud-based service providers. 
 AWS(Amazon Web Services) 
 is just such 
 a suite of cloud-computing services, offered by 
 amazon.com
 .
  
  
 You will need an AWS account in order to use these examples. All 
  
  
 of the services we will explore are free or nearly free for low-volume 
  
 development uses. To create an account on AWS, visit the following link:
  
 http://aws.amazon.com/
  
 Once you have created an account, you will be able to manage all of your 
  
 services via the AWS console at:
  
 https://console.aws.amazon.com/console/home
  
 In this section we will learn how to use three popular AWS services:
  
 • 
  
 • 
  
 • 
  
 For storing documents and files we will connect with Amazon 
 S3 
 (Simple Storage Service)
  
 Amazon's Key/Value database, DynamoDB
  
 To manage a large volume of e-mail, we will leverage Amazon's 
 SES 
 (Simple Email Service)
  
 To access these services we will use the AWS SDK for Node, which can be found at 
 the following link:
  
 https://github.com/aws/aws-sdk-js
  
 To install the module run the following command:
  
 npm install aws-sdk
  
  
 Full documentation for the 
 aws-sdk
  module can be found at the 
  
  
 following link:
  
 http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/
  
 frames.html
  
 [
  236 
 ]
  
 www.EBooksWorld.ir",NA
Authenticating ,"Developers registered with AWS are assigned two identifiers:
  
 • 
  
 • 
  
 A public 
 Access Key ID
  (a 20-character, alphanumeric sequence).
  
 A 
 Secret Access Key
  (a 40-character sequence). It is very important to keep 
 your Secret Key private.
  
 Amazon also provides developers with the ability to identify the region with which to 
 communicate, such as 
 ""us-east-1""
 . This allows developers to target the closest 
 servers (regional endpoint) for their requests.
  
 The regional endpoint and both authentication keys are necessary to make requests.
  
  
 For a breakdown of regional endpoints visit the following link:
  
  
 http://docs.aws.amazon.com/general/latest/gr/rande.
  
 html
  
 As we will be using the same credentials in each of the following examples, let's 
 create a single config.json file that is re-used:
  
 {
  
  ""accessKeyId""    : ""your-key"", 
  
  ""secretAccessKey""    : ""your-secret"",
  
  ""region""        : ""us-east-1"",
  
  ""apiVersions""    : {
  
  
  ""s3""        : ""2006-03-01"",
  
  
  ""ses""        : ""2010-12-01"",
  
  
  ""dynamodb""    : ""2012-08-10""
  
  } 
  
 }
  
 We also configure the specific API versions we will be using for services. Should 
 Amazon's services API change, this will ensure our code will continue to work.
  
 An AWS session can now be initialized with just two lines of code. Assume that 
 these two lines exist prior to any of the example code that follows:
  
 var AWS = require('aws-sdk'); 
  
 AWS.config.loadFromPath('./config.json');
  
 [
  237 
 ]
  
 www.EBooksWorld.ir",NA
Errors,"When experimenting with these services it is likely that error codes will appear on 
 occasion. Because of their complexity, and the nature of cloud computing, these services 
 can sometimes emit surprising or unexpected errors. For example, because S3 can only 
 promise eventual consistency in some regions and situations, attempting to read a key 
 that has just been written to may not always succeed. We will be exploring the complete 
 list of error codes for each service, and they can be found at the following locations:
  
 • 
  
 S3
  
 http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.
  
 html
  
 • 
  
 DynamoDB
  
 http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/ 
 Welcome.html
  
 • 
  
 SES
  
 http://docs.aws.amazon.com/ses/latest/DeveloperGuide/api-error-
 codes.html
  
 and
  
 http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-
  
 response-codes.html
  
 As it will be difficult in the beginning to predict where errors might arise, it is 
 important to employ the 
 domain
  module or other error-checking code as you 
 proceed.
  
 Additionally, a subtle but fundamental aspect of Amazon's security and consistency 
 model is the strict synchronization of its web server time and time as understood by a 
 server making requests. A discrepancy of 15 minutes is the maximum 
  
 allowed. While this seems like a long time, in fact time drift is very common. When 
 developing watch out for 403: Forbidden errors that resemble one of the following: 
  
 • 
  
 • 
  
 SignatureDoesNotMatch
 : This error means that the signature has expired 
 RequestTimeTooSkewed
 : The difference between the request time and the 
 current time is too large
  
 [
  238 
 ]
  
 www.EBooksWorld.ir",NA
Using S3 to store files,"S3 can be used to store any file one expects to be able to store on a filesystem. Most 
 commonly, it is used to store media files such as images and videos. S3 is an excellent 
 document storage system as well, especially well-suited for storing small JSON objects or 
 similar data objects.
  
 Also, S3 objects are accessible via HTTP, which makes retrieval very natural, and REST 
 methods such as PUT/DELETE/UPDATE are supported. S3 works very much like one 
 would expect a typical file server to work, is spread across servers that span the globe, 
 and offers storage capacity that is for all practical purposes limitless.
  
 S3 uses the concept of a 
 bucket
  as a sort of corollary to ""hard drive"". Each S3 account 
 can contain 100 buckets (this is a hard limit), with no limits on the number of files 
 contained in each bucket.",NA
Working with buckets,"Creating a bucket is easy:
  
 var S3 = new AWS.S3();
  
 S3.createBucket({
  
  Bucket: 'nodejs-book'
  
 }, function(err, data) {
  
  if(err) { throw new Error(""Unable to create bucket.""); }
  
  console.log(data);
  
 });
  
 [
  239 
 ]
  
 www.EBooksWorld.ir",NA
Working with objects ,"Let's add a document to the 
 nodejs-book
  bucket on S3:
  
 var S3 = new AWS.S3({
  
  params: { Bucket: 'nodejs-book'    } 
  
 }); 
  
 var body = JSON.stringify({ foo: ""bar"" }); 
  
 var s3Obj = {
  
  Key: 'demos/putObject/first.json',
  
  Body: body,
  
  ServerSideEncryption: ""AES256"",
  
  ContentType: ""application/json"",
  
  ContentLength: body.length,
  
  ACL: ""private"" 
  
 }
  
 [
  240 
 ]
  
 www.EBooksWorld.ir",NA
Using AWS with a Node server ,"Putting together what we know about Node servers, streaming file data through 
 pipes, and HTTP, it should be clear how to mount S3 as a filesystem in just a few lines 
 of code:
  
 http.createServer(function(request, response) {  
 var requestedFile = request.url.substring(1); 
 S3.headObject({
  
  
  
  Key : requestedFile
  
  }, function(err, data) {
  
  
  
  //    404, etc.
  
  
  
  if(err) {
  
  
  
  
  response.writeHead(err.statusCode);
  
  
  
  
  return response.end(err.name);
  
  
  
  }
  
  
  
  response.writeHead(200, {
  
  
  
  
  ""Last-Modified""     : data.LastModified,
  
  
  
  ""Content-Length""     : 
 data.ContentLength,
  
  
  
  ""Content-Type""     
 : data.ContentType,
  
  
  
  ""ETag""         : 
 data.ETag
  
  
  
  });
  
  
  
  S3.getObject({
  
  
  
  
  Key    : requestedFile
  
  
  
  }).createReadStream().pipe(response);
  
  }); 
  
 }).listen(8080);
  
 A standard Node HTTP server receives a request URL. We first attempt a HEAD 
 operation using the 
 aws-sdk
  method 
 headObject
 , accomplishing two things:
  
 • 
  
 We'll determine if the file is available
  
 • 
  
 We will have the header information necessary to build a response
  
 After handling any non-200 status code errors we only need to set our response 
 headers and stream the file back to the requester, as previously demonstrated.
  
 Such a system can also operate as a 
 fail-safe
 , in both directions; should S3, or the file, 
 be unavailable, we might bind to another file system, streaming from there. 
 Conversely, if our preferred local filesystem fails, we might fall through to our 
 backup S3 filesystem.
  
  
 See the 
 amazon/s3-redirect.js
  file in the code bundle 
  
  
 available at the Packt website for an example of using 302 
  
 redirects to similarly mount an AWS filesystem.
  
 [
  243 
 ]
  
 www.EBooksWorld.ir",NA
Getting and setting data with DynamoDB,"DynamoDB
  (
 DDB
 ) is a NoSQL database providing very high throughput and 
 predictability that can be easily scaled. DDB is designed for 
 data-intensive 
 applications, performing massive map/reduce and other analytical queries with low 
 latency and reliably. That being said, it is also an excellent database solution for 
 general web applications.
  
  
 The whitepaper announcing DynamoDB was highly influential, 
  
  
 sparking a real interest in NoSQL databases, and inspiring many, 
  
 including Apache 
 Cassandra
 . The paper deals with advanced 
  
 concepts, but rewards careful study:
  
 http://www.allthingsdistributed.com/files/amazon-
  
 dynamo-sosp2007.pdf
  
 A Dynamo database is a collection of tables, which is a collection of items, which are a 
 collection of attributes. Each item in a table (or row, if you prefer) must have a primary 
 key, functioning as an index for the table. Each item can have any number of attributes 
 (up to a limit of 65 KB) in addition to the primary key.
  
 This is an item, with five attributes, one attribute serving as the primary key (
 Id
 ):
  
 {    Id = 123 
  
  Date = ""1375314738466""
  
  UserId = ""DD9DDD8892""
  
  Cart = [ ""song1"", ""song2"" ]
  
  Action = ""buy"" }
  
 Let's create a table with both a primary and a secondary key:
  
 var AWS = require('aws-sdk');
  
 AWS.config.loadFromPath('../config.json');
  
 var db = new AWS.DynamoDB(); 
  
 db.createTable({
  
  TableName: 'purchases',
  
  AttributeDefinitions : [{
  
  AttributeName : ""Id"", AttributeType : ""N""
  
  }, {
  
  AttributeName : ""Date"", AttributeType : ""N""
  
  }],
  
 [
  244 
 ]
  
 www.EBooksWorld.ir",NA
Searching the database ,"There are two types of search operations available: 
 query
  and 
 scan
 . A scan on a table 
 with a single primary key will, without exception, search every item in a table, returning 
 those matching your search criteria. This can be very slow on anything but small 
 databases. A query is a direct key lookup. We'll look at queries first. Note that in this 
 example we will assume that this table has only one primary key.
  
 To fetch the 
 Action
  and 
 Cart
  attributes for item 
 124
 , we use the following code:
  
  
 db.getItem({
  
  
  TableName : ""purchases"",
  
  
  Key : {
  
  
  
  
  Id : { ""N""    : ""124"" }
  
  
  },
  
  
  AttributesToGet : [""Action"", ""Cart""] 
  
  
 }, function(err, res) {
  
  
  console.log(util.inspect(res, { depth: 10 })); 
  
 }); 
  
 Which will return:
  
 {     Item: { 
  
  
  
  Action: { S: 'buy' }, 
  
  
  
  Cart: { SS: [ 'song2', 'song4' ] } 
  
  } 
  
 }
  
 To select all attributes, simply omit the 
 AttributesToGet
  definition.
  
 A scan is more expensive, but allows more involved searches. The usefulness of 
 secondary keys is particularly pronounced when doing scans, allowing us to avoid the 
 overhead of scanning the entire table. In our first example of scan we will work as if 
 there is only a primary key. Then we will show how to filter the scan using the 
 secondary key.
  
 To get all the records whose 
 Cart
  attribute contains 
 song2,
  use the following code:
  
 db.scan({
  
  TableName : ""purchases"",
  
  ScanFilter : {
  
  
  
  ""Cart"": {
  
  
  
   
  ""AttributeValueList"" : [{
  
  
  
     
  ""S"":""song2""
  
  
  
   
  }],
  
  
  
   
  ""ComparisonOperator"" : ""CONTAINS""
  
  
  },
  
  }
  
 [
  247 
 ]
  
 www.EBooksWorld.ir",NA
Sending mail via SES ,"Amazon describes the problems SES is designed to solve in this way:
  
 Building large-scale email solutions to send marketing and transactional 
 messages is often a complex and costly challenge for businesses. To optimize the 
 percentage of emails that are successfully delivered, businesses must deal with 
 hassles such as email server management, network configuration, and meeting 
 rigorous Internet Service Provider (ISP) standards for email content.
  
 [
  248 
 ]
  
 www.EBooksWorld.ir",NA
Authenticating with Facebook Connect,"Building a scalable authentication system is hard. Though it may seem easy—create a 
 table or a list of users and their credentials, change that list using a simple web form, 
 and you're done!
  
 If your goal is to create a scalable system that can grow as your user base grows then 
 you will need to do much more. It will be necessary to create e-mail systems and 
 scalable databases, session management, and the many other subsystems essential to 
 satisfying the user's expectation of a low-latency path from sign-up to sign-in to the 
 desired content.
  
 Let's put together what we've learned with AWS with Connect, Facebook's 
 authentication service. You can see the full code for this example in the 
 /facebook 
 folder of your code bundle available at the Packt website.
  
 To begin, visit 
 developers.facebook.com
 , set up a developers account, and then 
 create an application. Remember to properly set your application path as shown in the 
 following screenshot:
  
  
 Once registered, find and copy your two application keys. You will need these later.
  
 To create a Node server that allows users to login via Connect, we will use the excellent 
 passport
  npm module, which will make it easy to connect to many cloud services, 
 including Facebook. Additionally, we will use S3 to store a simple user object, allowing us 
 to map Connect login IDs to our internal application records. We'll build this all using 
 Express, which works well with 
 passport
 .
  
 [
  250 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"Big data applications have placed significant responsibility on developers of 
 network applications to prepare for scale. Node has offered help in creating a 
 network-friendly application development environment that can easily connect to 
 other devices on a network, such as cloud services and, in particular, other Node 
 servers.
  
 In this chapter we learned some good strategies for scaling Node servers, 
  
 from analyzing CPU usage to communicating across processes. With our new knowledge 
 of message queues and UDP we can build networks of Node servers scaling horizontally, 
 letting us handle more and more traffic by simply replicating existing nodes. Having 
 investigated load balancing and proxying with Node, we can confidently add capacity to 
 our applications. When matched with the cloud services provided by AWS and 
 Facebook, we are able to scale our data stores and user account management with ease 
 and at low cost.
  
 As our applications grow we will need to maintain continuous awareness of how 
 each part, as well as the whole, is behaving. Before a new component is added, that 
 integration will need to be tested. In the next chapter we will learn strategies for 
 testing your systems, both in the large and in the small.
  
 [
  253 
 ]
  
 www.EBooksWorld.ir",NA
Testing your Application,"When the terrain disagrees with the map, trust the terrain.
  
 —Swiss Army Manual
  
 Because a community fully committed to code sharing is building Node, and 
 interoperability between modules is so important, it should come as no surprise that 
 code testing tools and frameworks entered Node's ecosystem right after inception. 
 Indeed, the normally parsimonious core Node team added the 
 assert 
 module early 
 on, suggesting a recognition that testing is a fundamental part of the development 
 process.
  
 Testing software is a complicated and still mostly ill-defined activity. It is also 
  
 essential to all phases of a non-trivial development project. The set of expectations 
 accompanying the adjective ""enterprise"" when associated with ""software"" is a large one, 
 comprising at least security, stability, consistency, predictability, and scale. Software is 
 expected to anticipate and nimbly integrate multifaceted and unpredictable changes in 
 data volume and shape, user expectations, business goals, staffing, even government 
 regulations, and legal climate.
  
 Testing is not solely a bug-detecting and defect-fixing process. Test-driven 
  
 development, for example, insists on having tests precede the existence of any code! 
 Testing, generally, is the process of making comparisons between existing behavior and 
 desired behavior in software, where new information is continuously fed back into the 
 process. In this sense, testing involves modeling expectations and verifying that individual 
 functions, composed units, and implementation paths satisfy the expectations of 
 designers, programmers, product owners, 
  
 customers, and entire organizations.
  
 www.EBooksWorld.ir",NA
Why testing is important,"A good testing strategy builds confidence through the accumulation of proof and 
 increasing clarity. Within a company this might mean that some criteria for the execution 
 of a business strategy have been satisfied, allowing for the release of a new product. The 
 developers within a project team gain the pleasure of an automated judge that confirms 
 or denies whether changes committed to a codebase are sound. With a good testing 
 framework refactoring loses its danger: the ""if you break it you own it"" caveat that once 
 placed negative pressure on developers with new ideas is no longer as ominous. Given a 
 good version control system and test/release process, any breaking change can be rolled 
 back without negative impact, freeing curiosity and experimentation.
  
 Three common types of tests are: unit tests, functional tests, and integration tests. 
  
 While our goal in this chapter is not to put forward a general theory about how to test 
 applications, it will be useful to briefly summarize what unit, functional, and integration 
 tests are, which members of a team are most interested in each, and how we might go 
 about breaking up a codebase into testable units.
  
 [
  256 
 ]
  
 www.EBooksWorld.ir",NA
Unit tests,"Unit tests concern themselves with units of system behavior. Each unit being tested 
 should encapsulate a very small set of code paths, without entanglements. 
  
 When a unit test fails this should, ideally, indicate that an isolated part of the overall 
 functionality is broken. If a program has a well described set of unit tests, the purpose 
 and expected behavior of an entire program should be easy to comprehend, entire 
 program should be easy to comprehend. A unit test applies a limited perspective to 
 small parts of a system, unconcerned with how those parts may be wrapped up into 
 larger functional blocks.
  
 An example unit test might be described in this way: when the value 
 123
  is passed to a 
 method 
 validate_phone_number()
 , the test should return false. There is no 
 confusion about what this unit does, and a programmer can use it with confidence.
  
 Unit tests are normally written and read by programmers. Class methods are good 
 candidates for unit tests, as are other service endpoints whose input signatures are 
 stable and well understood, with expected outputs that can be accurately validated. 
 Generally it is assumed that unit tests run quickly. If a unit test is taking a long time to 
 execute it is likely the code under test is much more complex than it should be.
  
 Unit tests are not concerned with how a function or method under test will receive its 
 inputs, or how it will be used in general. A test for an 
 add
  method shouldn't be 
 concerned with whether the method will be used in a calculator or somewhere else, it 
 should simply test if the two integer inputs (3,4) will cause the unit to emit a correct 
 result (7). A unit test is not interested in where it fits in the dependency tree. For this 
 reason unit tests will often ""mock"" or ""stub"" data sources, such as passing two random 
 integers to an 
 add
  method. As long as the inputs are typical they need not be actual. 
  
 Additionally, good unit tests are reliable: unencumbered by external dependencies 
 they should remain valid regardless of how the system around them changes.
  
 Unit tests only confirm that a single entity works in isolation. Testing whether units can 
 work well when combined is the purpose of functional testing.",NA
Functional tests,"Where unit tests concern themselves with specific behaviors, functional tests are 
 designed to validate pieces of functionality. The ambiguity of the root word ""function"", 
 especially for programmers, can lead to confusion, where ""unit tests"" are called 
 ""functional tests"", and vice versa. A functional test combines many units into a body of 
 functionality, such as, ""when a user enters a username and password and clicks on send, 
 that user will be logged into the system"". We can easily see that this functional group 
 would be comprised of many unit tests, one for validating a username, one for handling 
 a button click, and so on.
  
 [
  257 
 ]
  
 www.EBooksWorld.ir",NA
Integration tests,"Integration tests ensure that the entire system is correctly wired together, such that a 
 user would feel the application is working correctly. In this way integration tests typically 
 validate the expected functionality of an entire application, or one of a small set of 
 significant product functionality.
  
 The most important difference between integration and the other types of tests under 
 discussion is that integration tests are to be executed within a realistic environment, on 
 real databases with actual domain data, on servers, and other systems mirroring the 
 target production environment. In this way, integration tests can easily break formerly 
 passing unit and functional tests.
  
 [
  258 
 ]
  
 www.EBooksWorld.ir",NA
Native Node testing and debugging tools,"A preference for tested code has formed part of the Node community's ethos since its 
 inception, reflected in the fact that most popular Node modules, even simple ones, are 
 distributed with test suites. While browser-side development with JavaScript suffered for 
 many years without usable testing tools, the relatively young Node distribution contains 
 many. Perhaps because of this, many mature and easy to use third-party testing 
 frameworks have been developed for Node. This leaves a developer no excuse for writing 
 untested code! Let's look into some of the 
  
 provided tools for debugging and testing Node programs.",NA
Writing to the console,"Console output is the most basic testing and debugging tool, providing a quick way 
 to see what is happening at some point in a script. The globally accessible 
 console.log
  is commonly used when debugging.
  
 [
  259 
 ]
  
 www.EBooksWorld.ir",NA
Formatting console output,"The preceding methods are all very useful when logging simple strings. More often, 
 useful logging data may need to be formatted, either by composing several values into a 
 single string, or by neatly displaying a complex data object. The 
 util.format 
 and 
 util.inspect
  methods can be used to handle these cases.",NA
"The util.format(format, [arg, arg…]) method","This method allows a formatting string to be composed out of placeholders, each of 
 which captures and displays the additional values passed. For example:
  
 > util.format('%s:%s', 'foo','bar')
  
 'foo:bar'
  
 Here, we see that the two placeholders (prefixed by 
 %
 ) are replaced in order by the 
 passed arguments. Placeholders expect one of the following three types of values:
  
 • 
  
 %s
 : A string
  
 • 
  
 %d
 : A number, either an integer or a float
  
 • 
  
 %j
 : A JSON object
  
 If a greater number of arguments than placeholders is sent, the extra arguments are 
 converted to strings via 
 util.inspect()
 , and concatenated to the end of the output, 
 separated by spaces:
  
 > util.format('%s:%s', 'foo', 'bar', 'baz');
  
 'foo:bar baz'
  
 If no formatting string is sent, the arguments are simply converted to strings 
 and concatenated, separated by a space.
  
 [
  261 
 ]
  
 www.EBooksWorld.ir",NA
"The util.inspect(object, [options]) method ","Use this method when a string representation of an object is desired. Through the 
 setting of various options the look of the output can be controlled:
  
 • 
  
 • 
  
 • 
  
 • 
  
 showHidden
 : Defaults to false. If true, the object's non-enumerable properties 
 will be shown.
  
 depth
 : An object definition, such as a JSON object, can be deeply nested. By 
 default 
 util.inspect
  only traverses two levels into the object. Use this option 
 to increase (or decrease) that depth.
  
 colors
 : Allows the colorization of the output (see the following code snippet).
  
 customInspect
 : If the object being processed has an 
 inspect
  method 
 defined, the output of that method will be used instead of Node's default 
 stringification method (see the following code snippet). Defaults to true.
  
 Setting a custom inspector:
  
 var util = require('util'); 
  
 var obj = function() {
  
  
  this.foo = ""bar""; 
  
 }; 
  
 obj.prototype.inspect = function() {
  
  
  return ""CUSTOM INSPECTOR""; 
  
 } 
  
 console.log(util.inspect(new obj)) 
  
 //  CUSTOM INSPECTOR 
  
 console.log(util.inspect(new obj, { customInspect: false })) 
 //  { foo: 'bar' }
  
 This can be very useful when logging complex objects, or objects whose values are so 
 large as to make console output unreadable.
  
 The 
 color: true
  option of 
 util.inspect
  allows for colorization of object output, 
 which can be useful for complex objects. Colors are assigned to data types. The default 
 assignments are set in 
 util.inspect.styles
 :
  
 { special: 'cyan',
  
  number: 'yellow',
  
  boolean: 'yellow',
  
  undefined: 'grey',
  
  null: 'bold',
  
  string: 'green',
  
  date: 'magenta',
  
  regexp: 'red' }
  
 [
  262 
 ]
  
 www.EBooksWorld.ir",NA
The Node debugger,"Most developers have used an IDE for development. A key feature of all good 
 development environments is access to a debugger, which allows breakpoints to be 
 set in a program in places where state or other aspects of the runtime need to be 
 checked.
  
 V8 is distributed with a powerful debugger (commonly seen powering the Google Chrome 
 browser's developer tools panel), and this is accessible to Node. It is invoked using the 
 debug directive:
  
 > node debug somescript.js
  
 Simple step-through and inspection debugging can now be achieved within a node 
 program. Consider the following program:
  
 myVar = 123;
  
 setTimeout(function () {
  
  debugger;
  
  console.log(""world"");
  
 }, 1000);
  
 console.log(""hello"");
  
 Note the 
 debugger
  directive. Executing this program without using the 
 debugger 
 directive will result in 
 hello
  being displayed, followed by 
 world
 , one second later. 
  
 When using the directive, one would see this:
  
 > node debug somescript.js 
  
 < debugger listening on port 5858
  
 connecting... ok
  
 break in debug-sample.js:1
  
  1 myVar = 123;
  
  2 setTimeout(function () {
  
  3 debugger;
  
 debug>
  
 [
  263 
 ]
  
 www.EBooksWorld.ir",NA
The assert module,"Node's 
 assert
  module is used for simple unit testing. In many cases it suffices as a basic 
 scaffolding for tests, or used as the assertion library for testing frameworks (like Mocha, 
 as we'll see later). Usage is straightforward: we want to assert the truth of something, 
 and throw an error if our assertion is not true. For example:
  
 > require('assert').equal(1,2,""Not equal!"")
  
 AssertionError: Not equal!
  
  at repl:1:20
  
  ...
  
 If the assertion were true (both values are equal) nothing would be returned:
  
 > require('assert').equal(1,1,""Not equal!"")
  
 undefined 
  
 Following the UNIX Rule of Silence (when a program has nothing surprising, 
 interesting or useful to say, it should say nothing), assertions only return a value 
 when the assertion fails. The value returned can be customized using an optional 
 message argument, as seen in the preceding section.
  
 The 
 assert
  module API is composed of a set of comparison operations with 
 identical call signatures: the actual value, the expected value, and an optional 
 message to display when comparison fails. Alternate methods functioning as 
 shortcuts or handlers for special cases are also provided.
  
 A distinction must be made between identity comparison (
 ===
 ) and equality 
 comparison (
 ==
 ) the former often referred to as strict equality comparison (as is the 
 case in the 
 assert
  API). Because JavaScript employs dynamic typing, when two values 
 of different types are compared using the equality operator 
 ==
 , an attempt is made to 
 coerce (or cast) one value into the other, a sort of common denominator operation. For 
 example:
  
 1 == ""1"" // true
  
 false == ""0"" // true
  
 false == null // false
  
 As you might expect, these sorts of comparisons can lead to surprising results. 
 Notice the more predictable results when identity comparison is used:
  
 1 === ""1"" // false
  
 false === ""0"" // false
  
 false === null // false
  
 [
  267 
 ]
  
 www.EBooksWorld.ir",NA
Sandboxing,"For some applications it is useful to be able to run a script within a protected context, 
 isolated from the general application scope. For these purposes Node provides the 
 vm
  
 module, a sandbox environment consisting of a new V8 instance and a limited execution 
 context for running script blocks. For example, one might want to execute code submitted 
 from a web-based editor within such a virtual machine:
  
 var vm = require('vm'); 
  
 var sandbox = {
  
  
  count: 2 
  
 }; 
  
 var someRandomCode = 'var foo = ++count;'; 
 vm.runInNewContext(someRandomCode, sandbox); 
 console.log(require('util').inspect(sandbox))
 ; // { count: 3, foo: 3 }
  
 Here we see how a provided sandbox becomes the local execution scope for the 
 provided script. The running script can only operate within the provided sandbox 
 object, and is denied access to even the standard Node globals, such as the running 
 process:
  
 ...
  
 var someRandomCode = 'var foo = ++count; process.exit(0);'; 
 vm.runInNewContext(someRandomCode, sandbox); 
  
 //  vm.runInNewContext(someRandomCode, sandbox); 
  
 //     ^ 
  
 //  ReferenceError: process is not defined
  
 The core Node team has marked 
 vm
  as unstable, meaning that its API and behavior 
 may change at any given time. Two other points also need to be stated:
  
 • 
  
 • 
  
 This module does not guarantee a perfectly safe ""jail"" within which 
 completely untrusted code can be executed. If this is your need, consider 
 running a separate process with proper system-level permissions.
  
 Because 
 vm
  spins up a new V8 instance, each invocation costs some 
  
 milliseconds of startup time and about two megabytes of memory. It should be 
 used sparingly.
  
 For the purpose of testing code, the 
 vm
  module can be quite effective, in particular in its 
 ability to force code to run in a limited context. When performing a unit test, for 
 example, one can create a special environment with mocked data simulating the 
 environment within which the tested script will run. This can be better than creating an 
 artificial call context with fake data. Additionally, this sandboxing will allow the 
 execution context for new code to be better controlled, providing good protection 
 against memory leaks and other unanticipated collisions that may bubble up while 
 testing.
  
 [
  270 
 ]
  
 www.EBooksWorld.ir",NA
Distinguishing between local scope and execution ,NA,NA
context,"Before covering further examples, we need to distinguish between the local scope of a 
 process and its execution context. The distinction will help with understanding the 
 difference between the two primary 
 vm
  methods, 
 vm.runInThisContext
  and 
 vm.runInNewContext
 .
  
 The execution context of a Node process represents the runtime context within V8, 
 including native Node methods and other global objects (
 process
 , 
 console
 , 
 setTimeout
 , and so on). The members of this object are easily traced using the Node 
 REPL:
  
 node > global
  
 The local scope of a Node program contains runtime definitions, such as variables 
 defined using 
 var
 .
  
 A script executed through 
 vm.runInNewContext
  has no visibility into either scope— its 
 context is limited to the sandbox object to which it was passed, as seen earlier.
  
 A script executed through 
 vm.runInThisContext
  has visibility into the global 
 execution scope, but not into the local scope. We can demonstrate this as follows:
  
 var localVar = 123;
  
 var tc = vm.runInThisContext('localVar = 321;');
  
 console.log(localVar, tc);
  
 var ev = eval('localVar = 321;');
  
 console.log(localVar, ev);
  
 //  123 321
  
 //  321 321
  
 Scripts are therefore run within contexts through 
 vm
 . It is often useful to precompile 
 contexts and scripts, in particular when each will be used repeatedly.
  
 Use 
 vm.createContext([sandbox])
 to compile an execution context, and pass in a 
 key/value map, such as 
 {a: 1, b: 2}
 . We'll now look at how to use these compiled 
 objects together.
  
 [
  271 
 ]
  
 www.EBooksWorld.ir",NA
Using compiled contexts,"After receiving a string of JavaScript code, the V8 compiler will do its best to optimize 
 the code into a compiled version that runs more efficiently. This compilation step must 
 occur each time a 
 vm
  context method receives code as a string. If the code doesn't change 
 and is re-used at least once, use 
 vm.createScript(code, [filename]) 
 to compile it 
 once and for all.
  
 This Script object also inherits 
 runInThisContext
  and 
 runInNewContext
  methods 
 from 
 vm
 . Here we run a compiled script in both contexts, demonstrating how the 
 x
  and 
 y
  variables being incremented exist in fully isolated scopes:
  
 x = 0; 
  
 y = 0; 
  
 var script = vm.createScript('++x, ++y;'); 
  
 var emulation = vm.createContext({ x:0, y:0 }); 
 for(var i=0; i < 1000; i++) {
  
  
  script.runInThisContext();
  
  
  script.runInNewContext(emulation); 
  
 } 
  
 console.log(x, y); 
  
 console.log(emulation.x, emulation.y); 
  
 //  1000  1000  1000 
  
 //  1000  1000  1000
  
 Had both scripts modified the same 
 x
  and 
 y
  variables, then the outputs would have 
 instead been 
 2000 2000
 . Additionally, if the 
 runInNewContext
  script is not passed an 
 emulation layer (sandbox) it will throw a 
 ReferenceError
 , having no access to the 
 global 
 x
  and 
 y
  values. Try it out.
  
  
 The optional 
 filename
  argument is used for debugging, 
  
  
 appearing in stack traces. It might help to provide one 
  
 when debugging.",NA
Errors and exceptions,"The terms ""error"" and ""exception"" are often used interchangeably. Within the Node 
 environment these two concepts are not identical. Errors and exceptions are different. 
 Additionally, the definition of error and exception within Node does not necessarily 
 align with similar definitions in other languages and development environments.
  
 Conventionally, an error condition in a Node program is a non-fatal condition that 
 should be caught and handled, seen most explicitly in the 
 Error as first argument 
 convention displayed by the typical Node callback pattern. An exception is a serious 
 error (a system error) that a sane environment should not ignore or try to handle.
  
 [
  272 
 ]
  
 www.EBooksWorld.ir",NA
The domain module ,"Error handling in asynchronous code is also difficult to trace:
  
 function f() {
  
  
  throw new error(""error somwhere!"") 
  
 } 
  
 setTimeout(f, 1000*Math.random()); 
  
 setTimeout(f, 1000*Math.random());
  
 Which function caused the error? It is difficult to say. It is also difficult to intelligently 
 insert exception management tools. It is difficult to know what to do next. Node's 
 domain
  module attempts to help with this and other exception localization issues. In 
 this way code can be tested, and errors handled, with more precision.
  
 At its simplest, 
 domain
  sets up a context within which a function or other chunk of 
 code can be run such that any errors occurring within that implicit domain binding 
 will be routed to a specific domain error handler. For example:
  
 var dom = domain.create(); 
  
 dom.on('error', function(err) {
  
  
  console.error('error', err.stack); 
  
 }); 
  
 dom.run(function() {
  
  
  throw new Error(""my domain error""); 
  
 }); 
  
 // error Error: my domain error 
  
 //  at /js/basicdomain.js:10:8 
  
 //  ...
  
 Here a function that throws an error is run within the context of a domain able to 
 intelligently catch those exceptions, implicitly binding all event emitters, timers, and 
 other requests created within that context.
  
 [
  275 
 ]
  
 www.EBooksWorld.ir",NA
Headless website testing with ZombieJS ,NA,NA
and Mocha,"Even though we are working on the server with Node, there is no doubt that much of 
 what we are programming will affect those connecting to our services via a browser. 
 As it is expected that we will be using Node to test our server-side code, why not 
 integrate browser-side testing as well?
  
 [
  277 
 ]
  
 www.EBooksWorld.ir",NA
Mocha,"Mocha expects a directory containing testable units to contain a 
 /test
  directory, 
 within which exist various spec, or test specification and files, making up the suite of 
 tests that Mocha will apply to your application. You will describe what you are testing, 
 and Mocha, smartly, provides a 
 describe
  method to do just that:
  
 var assert = require(""assert"")
  
 describe('Array', function() {
  
  describe('#indexOf()', function() {
  
  it('should return -1 when the value is not present',   
 function() {
  
  assert.equal(-1, [1,2,3].indexOf(5));
  
  assert.equal(-1, [1,2,3].indexOf(0));
  
  })
  
  })
  
 })
  
 Here we are describing a test that checks array operations. The 
 describe
  and 
 it
  
 directives are simply organizational tools, providing a title for the test suite being 
 described (
 #indexOf()
 ), and providing a description of each test in the suite (
 should 
 return -1 …
 ). If you were to run this through Mocha, some neatly formatted test 
 reports with these descriptions would be written to the console, and under each the 
 pass/fail state of the test.
  
 We have seen in the preceding example how Node's 
 assert
  module is being used. 
 Mocha is not an assertion library, it is a test runner that makes using an assertion 
 library easier. You need not use Node's library; other popular assertion libraries are 
 Chai (
 http://chaijs.com/api/assert/
 ) and, by the maker of Mocha, Should 
 (
 https://github.com/visionmedia/should.js/
 ).
  
 [
  278 
 ]
  
 www.EBooksWorld.ir",NA
Headless web testing,"One way to test if a UI is working is to pay several people to interact with a website via a 
 browser and report any errors they find. This is expensive, inefficient, and offers 
 incomplete coverage, as there is no guarantee that all errors will be found. Automating 
 the process of testing a UI with browser simulation tools is the topic of this section.
  
 A browser, stripped of its buttons and other controls, is at heart a program that validates 
 and runs JavaScript, HTML, and CSS. That the validated HTML is rendered visually on your 
 screen is simply a consequence of humans only being able to see with their eyes. A server 
 can interpret the logic of compiled code and see the results of your interactions with that 
 code without a visual component. Perhaps because eyes are usually found in one's head, a 
 browser running on a server is typically referred to as a headless browser. We are going 
 to learn how to do headless browser testing, first with ZombieJS (a custom-built headless 
 browser), and later with PhantomJS (a headless version of the WebKit engine).
  
 Most websites will use forms in some way, and a common task for developers is to 
 validate form inputs, such that a user is warned about mismatched passwords or 
 invalid e-mails prior to submitting the form. Consider the following form:
  
 <input type=""text"" id=""email"" size=""20"" />
  
 <input type=""password"" id=""password"" size=""20"" />
  
 <input type=""submit"" value=""Sign In"" onclick=""return 
  
  checkForm(this.parentNode)"" />
  
 This is clearly a login form, requiring an e-mail address and a password. We also see 
 that the submit button of this form will delegate validation to some JavaScript, 
 ultimately preventing submission if the form values are invalid. How do we test that 
 this form is correctly validating form data via Node running on a server?
  
  
 Please consult the 
 mocha-zombie
  folder in your code bundle while 
  
  
 working through this example. To start, run 
 npminstall
  in this 
  
 folder. Consult the 
 readme.md
  file for further instructions.
  
 [
  279 
 ]
  
 www.EBooksWorld.ir",NA
"Using Grunt, Mocha, and PhantomJS to ",NA,NA
test and deploy projects ,"With our new knowledge about headless browsers and test frameworks, let's develop 
 a slightly more realistic build/test/deploy system using Node. We'll continue to use 
 Mocha, and swap out ZombieJS for the solid PhantomJS, which unlike ZombieJS is not 
 an emulator, it is a true WebKit browser, making it a more powerful and accurate 
 testing environment.
  
 [
  281 
 ]
  
 www.EBooksWorld.ir",NA
Working with Grunt ,"The current workflow Grunt tries to simplify is that around Unix 
 make
  utility. A 
 typical 
 make
  file to automatically run Mocha would be named 
 Makefile
  and look 
 something like this:
  
 TESTS = test/*.js 
  
 test: 
  
  
  mocha 
  
 .PHONY: test
  
 It's a little cryptic, but the ideas are straightforward:
  
 1. Set up a test runner that will run the command 
 mocha
 .
  
 2. Have that runner look for JavaScript spec files in the directory 
 test/
 .
  
 Grunt tries to accomplish the same goals within a Node environment, where calling 
 module functions makes more sense than running system commands, and using 
 standard JSON syntax for declaring system dependencies makes more sense.
  
 Grunt tasks are written to a 
 Gruntfile.js
 , and one to run Mocha tests might look like 
 this:
  
 module.exports = function(grunt) {
  
  
  grunt.initConfig({
  
  
  mocha: {
  
    
  all: ['./test/*.js']
  
  
  }
  
  
  });
  
  
  grunt.loadNpmTasks('grunt-mocha');
  
  
  grunt.registerTask('default', ['mocha']); 
  
 }
  
 Adding more tasks follows the same pattern. To add the 
 jshint
  task, which will check 
 your JavaScript source code for possible errors, we add something like the following:
  
 ...
  
 jshint: {
  
  
  options: {
  
  
  jshintrc:  "".jshintrc""
  
  
  },
  
  
  files: [""Gruntfile.js"", ""tasks/*.js""] 
  
 } 
  
 ...
  
 this.loadNpmTasks(""grunt-contrib-jshint""); 
  
 ...
  
 [
  283 
 ]
  
 www.EBooksWorld.ir",NA
Summary,"The Node community has embraced testing from the beginning, and many testing 
 frameworks and native tools are made available to developers. In this chapter we've 
 examined why testing is so important to modern software development, as well as 
 something about functional, unit, and integration testing, what they are and how they 
 can be used. With the 
 vm
  module we learned how to create special contexts for testing 
 JavaScript programs, along the way picking up some techniques for sandboxing 
 untrusted code.
  
 In addition we've learned how to work with the extensive set of Node testing and error-
 handling tools, from more expressive console logging to Move onto one line tracing and 
 debugging. Through the 
 assert
  and 
 domain
  modules we were exposed to error 
 catching and reporting mechanisms, giving us a more robust way to specifically target 
 errors and exceptions, rather than simply catching all in one global handler.
  
 Finally, we learned how to set up a proper build and test system using Grunt and Mocha, 
 in the process experimenting with two different headless browser testing libraries, 
 learning two ways in which such testing might be done in each, and how these virtual 
 browsers can be integrated seamlessly with other testing environments.
  
 [
  284 
 ]
  
 www.EBooksWorld.ir",NA
Organizing Your Work,"""The way to build a complex system that works is to build it from very 
 simple systems that work.""
  
  – Kevin Kelly
  
 Node's straightforward module management system encourages the development of 
 maintainable codebases. The Node developer is blessed with a rich ecosystem of clearly 
 defined packages with consistent interfaces that are easy to combine, typically delivered 
 via npm. When developing both simple and complex solutions, the Node developer will 
 often find many pieces of that system readymade, and can rapidly compose those open 
 source modules into larger, consistent, and predictable systems.
  
 Because the value of any one module can be multiplied by adding useful 
 submodules, the Node module ecosystem has grown rapidly.
  
 We will cover the details of how Node understands modules and module paths, how 
 modules are defined, how to use modules in the npm package repository, and how to 
 create and share new npm. By following some simple rules you will find it easy to 
 shape the structure of your application, and help others work with what you've 
 created.
  
  
 ""Module"" and ""package"" will be used interchangeably to describe 
  
  
 the file or collection of files compiled and returned by 
 require()
 .
  
 www.EBooksWorld.ir",NA
Loading and using modules,"The Node designers believe that most modules should be developed in userland—by 
 developers, for developers. Such an effort is made to limit the growth of the standard 
 library. Node's standard library contains the following short list of modules:
  
 Network and I/O
  
 Strings and Buffers
  
 Utilities
  
 TTY
  
 Path
  
 Utilities
  
 UDP/Datagram
  
 Buffer
  
 VM
  
 HTTP
  
 Url
  
 Readline
  
 HTTPS
  
 StringDecoder
  
 Domain
  
 Net
  
 QueryString
  
 Console
  
 DNS
  
 Assert
  
 TLS/SSL
  
 Readline
  
 FileSystem
  
 Encryption and Compression
  
 Environment
  
 Events and Streams
  
 ZLIB
  
 Process
  
 Child Processes
  
 Crypto
  
 OS
  
 Cluster
  
 PunyCode
  
 Modules
  
 Events
  
 Stream
  
 Modules are loaded via the global 
 require
  statement, which accepts the module 
 name or path as a single argument. You are encouraged to augment the module 
 ecosystem by creating new modules or new combinations of modules.
  
 The module system itself is implemented in the 
 require
  (
 module
 ) module.
  
 [
  286 
 ]
  
 www.EBooksWorld.ir",NA
Understanding the module object,"A Node module is simply a JavaScript file expected to assign a useful value to the 
 module.exports
  property:
  
 module.exports = new function() {
  
  this.it = function(it) {
  
  console.log(it);
  
  }
  
 }
  
 We now have a module that can be required by another file. If this module was 
 defined in the file 
 ./say.js
 , the following is one way to use it:
  
 var say = require(""./say"");
  
 say.it(""Hello"");
  
 // Hello
  
 Note how it was not necessary to use the 
 .js
  suffix. We'll discuss how Node resolves 
 paths shortly.
  
  
 Node's core modules are also defined using the standard 
 module.
  
  
 exports
  pattern, as can be seen by browsing the source code 
  
 defining 
 console
 : 
 https://github.com/joyent/node/blob/
  
 master/lib/console.js
 .
  
 Once loaded, modules are cached based on their resolved filename, resolved relative to 
 the calling module. Subsequent calls to 
 require(./myModule)
  will return identical 
 (cached) objects. Note however that accessing the same module via a different relative 
 path (such as 
 ../../myModule
 ) will return a different object—think of the cache being 
 keyed by relative module paths.
  
 A snapshot of the current cache can be fetched via 
 require('module')._cache)
 .
  
 The module object itself contains several useful readable properties, such as:
  
 • 
  
 • 
  
 • 
  
 • 
  
 module.filename
 : The name of the file defining this module.
  
 module.loaded
 : Whether the module is in process of loading. Boolean true if 
 loaded.
  
 module.parent
 : The module that required this module, if any.
  
 module.children
 : The modules required by this module, if any.
  
 [
  287 
 ]
  
 www.EBooksWorld.ir",NA
Resolving module paths ,"The 
 require
  statement is regularly seen when browsing (or building) a Node 
 program. You will have noticed that the argument passed to require can take many 
 forms, such as the name of a core module or a file path.
  
 The following pseudo code, taken from the Node documentation, is an ordered 
 description of the steps taken when resolving module paths:
  
 require(X) from module at path Y 
  
 1. If X is a core module,
  
  
  a. return the core module
  
  
  b. STOP 
  
 2. If X begins with './' or '/' or '../'
  
  
  a. LOAD_AS_FILE(Y + X)
  
  
  b. LOAD_AS_DIRECTORY(Y + X) 
  
 3. LOAD_NODE_MODULES(X, dirname(Y)) 
  
 4. THROW ""not found"" 
  
 LOAD_AS_FILE(X) 
  
 1. If X is a file, load X as JavaScript text.  STOP 
  
 2. If X.js is a file, load X.js as JavaScript text.  STOP 
 3. If X.node is a file, load X.node as binary addon.  
 STOP 
 LOAD_AS_DIRECTORY(X) 
  
 1. If X/package.json is a file,
  
  
  a. Parse X/package.json, and look for ""main"" field.
  
  
  b. let M = X + (json main field)
  
  
  c. LOAD_AS_FILE(M) 
  
 2. If X/index.js is a file, load X/index.js as JavaScript text.  STOP 
 3. If X/index.node is a file, load X/index.node as binary addon.  
 STOP 
 LOAD_NODE_MODULES(X, START) 
  
 1. let DIRS=NODE_MODULES_PATHS(START) 
  
 2. for each DIR in DIRS:
  
  
  a. LOAD_AS_FILE(DIR/X)
  
  
  b. LOAD_AS_DIRECTORY(DIR/X) 
  
 NODE_MODULES_PATHS(START) 
  
 1. let PARTS = path split(START) 
  
 2. let ROOT = index of first instance of ""node_modules"" in PARTS, or 
 0 3. let I = count of PARTS - 1 
  
 4. let DIRS = [] 
  
 5. while I > ROOT,
  
 [
  288 
 ]
  
 www.EBooksWorld.ir",NA
Using npm,"As mentioned when discussing how Node does path lookups, modules may be contained 
 within a folder. If you are developing a program as a module for others to use you 
 should bundle that module within its own folder and publish it. The npm package 
 management system is designed to help you do just that.
  
 As we've seen throughout the examples in this book, a 
 package.json
  file describes a 
 module, usefully documenting the module's name, version number, dependencies, and 
 so forth. It must exist if you would like to publish your package via npm. In this section 
 we will investigate the key properties of this file, and offer some hints and tips on how 
 to configure and distribute your modules.
  
  
 Try 
 npm help json
  to fetch detailed documentation for all the available 
  
  
 package.json
  fields, or visit 
 https://npmjs.org/doc/json.html
 .
  
 A 
 package.json
  file must conform to the JSON specification. For example, 
 properties and values must be double-quoted.",NA
Initializing a package file,"You can create a package file by hand, or use the handy 
 npm init
  command-line 
 tool, which will prompt you for a series of values and automatically generate a 
 package.json
  file. Let's run through some of these values:
  
 • 
  
 name
 : (Required) This string is what will be passed to 
 require()
  in order 
  
 to load your module. Make it short and descriptive, using only alphanumeric 
  
 characters—this name will be used in URLs, command line arguments, and 
  
 folder names. Try to avoid using ""js"" or ""node"" in the name.
  
 • 
  
 version
 : (Required) npm uses semantic versioning, where these are all valid:
  
 °
  
 >=1.0.2 <2.1.2
  
 °
  
 2.1.x
  
 °
  
 ~1.2
  
  
 For more information on version numbers, visit 
 https://npmjs.org/ 
  
 doc/misc/semver.html
 .
  
  
 • 
  
 description
 : When people search 
 npmjs.org
  for packages, this is what they 
  
 will read. Make it short and descriptive.
  
 [
  290 
 ]",NA
Using scripts,"npm is ultimately a build tool, and the 
 scripts
  field in your package file allows you to 
 set various build directives executed at some point following certain 
 npm 
 commands. 
 For example, you might want to minify JavaScript, or execute some other processes that 
 build dependencies that your module will need whenever 
 npm install
  is executed. 
 The available directives are:
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 • 
  
 prepublish, publish, postpublish: Run by the 
 npm publish
  command. 
 preinstall, install, postinstall: Run by the 
 npm install
  command.
  
 preuninstall, uninstall, postuninstall: Run by the 
 npm uninstall
  command. 
 preupdate, update, postupdate: Run by the 
 npm update
  command.
  
 pretest, test, posttest: Run by the 
 npm test
  command.
  
 prestop, stop, poststop: Run by the 
 npm stop
  command.
  
 prestart, start, poststart: Run by the 
 npm start
  command.
  
 prerestart, restart, postrestart: Run by the 
 npm restart
  command. Note that 
 npm restart
  will run the 
 stop
  and 
 start
  scripts if no 
 restart
  script is 
 provided.
  
 It should be clear that the commands with prefix pre will run before, and the commands 
 with prefix post will run after their primary command (such as 
 publish
 ) is executed.
  
  
 Package files demonstrating the use of script directives can be 
  
  
 found in the 
 mocha-zombie
  folder in the code bundle for 
 Chapter 9
 , 
  
 Testing Your Application
 .
  
 [
  291 
 ]
  
 www.EBooksWorld.ir",NA
Declaring dependencies ,"It is likely that a given module will itself depend on other modules. These 
  
 dependencies are declared within a 
 package.json
  file using four related properties, 
 which are:
  
 • 
  
 • 
  
 • 
  
 • 
  
 dependencies
 : The core dependencies of your module should reside here. 
 devDependencies
 : Some modules are only needed during development (and 
 not in production), such as those used for testing. Declare those modules here.
  
  
 npm install will always install both 
 dependencies
  and 
  
  
 devDependencies
 . To install an npm package without 
  
 loading 
 devDependencies
 , use the command 
 npm 
  
 install --production
 .
  
 bundledDependencies
 : Node is changing rapidly, as are npm packages. 
 You may want to lock a certain bundle of dependencies into a single bundled 
 file and have those published with your package, such that they will not 
 change via the normal 
 npm update
  process.
  
 optionalDependencies
 : Contains modules that are optional. If these modules 
 cannot be found or installed the build process will not stop (as it will with other 
 dependency load failures). You can then check for this module's existence in your 
 application code.
  
 Dependencies are normally defined with a npm package name followed by 
 versioning information: 
  
  
 ""dependencies"" : {
  
  
  
  ""express"" : ""3.3.5"" 
  
  
 } 
  
 However they can also point to a tarball: 
  
  
 ""foo"" : ""http://foo.com/foo.tar.gz"" 
  
 You can point to a GitHub repository: 
  
  
 ""herder"": ""git://github.com/sandro-
 pasquali/herder.git#master"" 
 Or even the shortcut: 
  
  
 ""herder"": ""sandro-pasquali/herder""
  
 [
  292 
 ]
  
 www.EBooksWorld.ir",NA
Publishing packages,"In order to publish to npm, you will need to create a user. 
 npm adduser
  will trigger a 
 series of prompts requesting your name, e-mail, and password. You may then use this 
 command on multiple machines to authorize the same user account.
  
  
 To reset your npm password visit 
 https://npmjs.org/forgot
 .
  
  
 Once you have authenticated with npm you will be able to publish your packages using 
 the command 
 npm publish
 . The easiest path is to run this command from your package 
 folder. You may also target another folder for publishing (remember that a 
 package.json
  file must exist in that folder).
  
 You may also publish a gzipped TAR archive containing a properly configured 
 package folder.
  
 Note that if the 
 version
  field of the current 
 package.json
  file is lower or equal to 
 that of the existing, published package npm will complain and refuse to publish. 
  
 You can override this by using the 
 --force
  argument with 
 publish
 , but you 
 probably want to update the version and republish.
  
 To remove a package use 
 npm unpublish <name>[@<version>]
 . Note that once a 
 package is published other developers may depend on it. For this reason you are 
 strongly discouraged from removing packages that others are using. If what you 
 want to do is discourage the use of a version, use 
 npm deprecate 
 <name>[@<version>] <message>
 .
  
 [
  293 
 ]
  
 www.EBooksWorld.ir",NA
Globally installing packages and binaries,"Some Node modules are useful as command-line programs. Rather than requiring 
 something such as 
 > node module.js
  to run a program, we might want to simply type 
 > module
  on the console and have the program execute. In other words, we might want 
 to treat a module as an executable file installed on the system 
 $PATH
  and therefore 
 accessible from anywhere. There are two ways to achieve this using npm.
  
 The first and simplest way is to install a package using the 
 -g (global)
  argument:
  
 npm install -g module
  
 If a package is intended as a command-line application that should be installed 
 globally, it is a good idea to set the 
 preferGlobal
  property of your 
 package.json 
 file to 
 true
 . The module will still install locally, but users will be warned about its 
 global intentions.
  
 Another way to ensure global access is by setting a package's 
 bin
  property:
  
 ""name"": ""aModule"",
  
 ""bin"" : {
  
  ""aModule"" : ""./path/to/program""
  
 }
  
 When this module is installed, 
 aModule
  will be understood as a global CLI command. 
 Any number of such programs may be mapped to 
 bin
 . As a shortcut, a single 
 program can be mapped like so:
  
 ""name"": ""aModule"",
  
 ""bin"" : ""./path/to/program""
  
 In this case the name of the package itself (
 aModule
 ) will be understood as the 
 active command.
  
 [
  294 
 ]
  
 www.EBooksWorld.ir",NA
Sharing repositories ,"Node modules are often stored in version control systems, allowing several 
 developers to manage package code. For this reason the 
 repository
  field of 
 package.json
  can be used to point developers to such a repository, should 
 collaboration be desired. For example:
  
 ""repository"" : {
  
  
  ""type"" : ""git"",
  
  
  ""url"" : ""http://github.com/sandro-pasquali/herder.git"" 
 } 
  
 ""repository"" : {
  
  
  ""type"" : ""svn"",
  
  
  ""url"" : ""http://v8.googlecode.com/svn/trunk/"" 
  
 }
  
 Similarly, you might want to point users to where bug reports should be filed by 
 using the bugs field:
  
 ""bugs"": {
  
  
  ""url"": ""https://github.com/sandro-pasquali/herder/issues"" 
 }
  
 [
  295 
 ]
  
 www.EBooksWorld.ir",NA
Introducing the Path ,NA,NA
Framework,"""Great acts are made up of small deeds.""
  
  – Lao Tzu
  
 JavaScript is the only language natively installed in browsers. All web-based 
 applications depend on JavaScript to deliver their UI. If you are using a framework built 
 on another language (such as Ruby on Rails), you must merge two completely different 
 development models, one for the client and one for the server. With the arrival of Node 
 and JavaScript on the server, this unfortunate and awkward workflow has been made 
 smooth and elegant.
  
 The Path Framework is a powerful full-stack application development platform that will 
 help you create enterprise-class applications in less time. Unlike Express, which is 
 limited to the server side of an application, and client-side frameworks such as EmberJS 
 that offer no help on building servers, Path unifies your application development 
 process. With its focus on the full stack, Path enables the JavaScript developer to build 
 complete applications using a single language that are scalable, easy to maintain, test, 
 and modify. Of course, Node occupies a central place in the Path design.
  
 Path's goal is to help developers easily build web applications that work like native 
 applications. This means responsive single-page applications with powerful features.
  
 www.EBooksWorld.ir",NA
Managing state,"As we discussed earlier:
  
 If changes can be introduced into a web application without requiring a complete 
  
 reconstruction of state and state display then updating client information becomes 
 cheaper. 
  
 The client and server can talk more often, regularly exchanging information. Servers can 
 recognize, remember, and respond immediately to client desires, aided by reactive 
 interfaces gathering user actions and reflecting the impact of those actions within a UI in 
 near real time.
  
 A very important question every application developer must ask is this one: where 
 should state reside?
  
 Of course, permanent data must ultimately reside in a persistent layer such as a 
 server-side database. As well, transient-state data (such as which navigation item is 
 highlighted in a UI) can happily exist in the client. However, if an application displays 
 real bank records in a data grid within a browser, how do changes made to the 
 canonical record (the database) synchronize with changes in client model?
  
 For example, imagine a banking application where a browser-based UI indicates to the 
 user that she has a balance of USD 100. This UI was ""drawn"" based on an earlier request 
 to a server, asking something like: What is Mary's bank balance? Clearly that number 
 may change in the next second, or even millisecond, given the automated nature of 
 banking. How can Mary be sure that when she pays a USD 100 bill she still has USD 100 
 in the bank?
  
 One solution is to periodically expire the validity of the client state and poll a server for 
 state changes, resynching client and server state. Additionally, client requests are 
 revalidated on the server—Mary would be warned if she tries to spend USD 100 that she 
 doesn't have. Client state is 
 optimistic
 , and transactions are 
 revalidated
 .
  
 However, there are several problems with this model:
  
 • 
  
 • 
  
 Not all state can exist on a client
 : Secured state data must remain on the 
 server, which means clients can handle only pieces of state, making even the 
 best attempt to maintain a synchronized client state imperfect by definition.
  
 Revalidation is not an acceptable solution in many cases
 : Consider a real-
 time shooter. If one player alters the state of another player (let's say by killing 
 that player), it is not acceptable to at some point in the future reanimate the 
 killed player simply because the shooter's client had incorrect position 
 information that renders the original kill invalid.
  
 [
  299 
 ]",NA
Bridging the client/server divide,"Node is designed for high-concurrency environments, where many clients connecting 
 simultaneously can have their needs met quickly and predictably. This ability also 
 implies that each client can expect a Node server to respond rapidly when it itself 
 sends many simultaneous requests.
  
 One of Path's key design beliefs is that keeping state synchronized across clients and 
 servers is complex and difficult to do well. For this reason, applications built with Path 
 exchange data between clients and servers exclusively through the WebSockets 
 protocol.
  
 The following is a high-level view of how Path handles client requests. We see how the 
 cluster
  module is used to share responsibility for handling socket connections 
 among several socket servers:
  
 [
  300 
 ]
  
 www.EBooksWorld.ir",NA
Sending and receiving,"In addition to keeping state on the server, Path also aims at reducing the 
  
 responsibility of a caller for holding on to a call context until a response is received. 
  
 This sort of call frame maintenance anticipating callback execution is an essential 
 condition of nearly all AJAX development patterns, and is typically represented as 
 follows:
  
 //Within some execution context, such as an autocomplete input
  
 someXhrProxy.get(""/a/path/"", function(data) {
  
  //A callback bound to the execution context via closures
  
 });
  
 Some client-side libraries attempt to simplify this pattern with abstractions like 
 Promises, but they miss the point: a call should ""fire and forget"", its job being solely the 
 transmission of a request. The impact of that action, for both the server and the client, 
 is not the caller's concern. A developer only needs to assert a desired change of state, 
 or request some information. Path facilitates this separation of concerns, absolves the 
 call function of any responsibility for maintaining call contexts at the 
 functional
  level 
 and manages the flow of execution itself:
  
 All elements can bind user actions using data-attributes. The 
  
 standard events are all included - mouseover, click, change, etc. 
  
 Event binding is 
 declarative 
 , via 
 data-action 
 attribute.
  
 <element.../> data-action=""click/user/getEmail...
  
 The Path Runtime is bound, via event delegation, to all events 
  
 that occur within a document. When bound events occur, 
  
 Path will prepare an event object and route it.
  
 Path.route
  
 Path bindings: 
  
 path.open(""/User/getEmail) 
  
 .click(function(){...//handler}
  
 Because events are trapped via delegation (rather than 
 explicitly bound on a per-element basis) elements
  
 may be introduced/removed without side effects.
  
 Path Runtime
  
 Path works on a ""fire and forget” model. 
 send 
 to the server via a route, posting some data if needed.
  
 The server will process the request, and respond with a 
 resultData 
 object. Path listens on a socket,
  
 aligns 
 callld 
 with original call data, and routes resultData to the paths listening on the 
 receive 
 method.
  
 Client Server
  
 path.send('/user/getEmail');
  
 callld, [postData]
  
 WS://
  
 callld, resultData
  
 path.open(""/user/getEmail"")
  
 .receive(function(resultData){...})
  
 Any handler or other function may call path.route(""/path/here"",""method"")
  
 [
  302 
 ]
  
 www.EBooksWorld.ir",NA
Achieving a modular architecture ,"Path's declarative model also makes it very easy to load modules and dependencies via 
 HTML. Some of the key goals of its module loading system are as follows:
  
 • 
  
 • 
  
 • 
  
 • 
  
 Bundling
 : When it is known that a group of modules is needed, bundle them into 
 one package and send them in one request.
  
 Caching
 : Once a module has been retrieved from the server, it should not need to 
 be requested again. Path maintains a cache of module data on the client.
  
 Communication
 : Path routes events through paths such as 
 /click/get/ 
 email
 . Modules do the same, broadcasting their data along 
 module/ 
 moduleName/eventName
  paths, allowing listeners to act on 
  
 those transmissions.
  
 Encapsulation
 : No direct access to a module's internals is allowed. All 
 communication is via paths, as described previously.
  
 [
  303 
 ]
  
 www.EBooksWorld.ir",NA
Creating your own ,NA,NA
C++ Add-ons,"""If two (people) on the same job agree all the time then one is useless. If 
 they disagree all the time, then both are useless.""
  
  – Darryl F. Zanuck
  
 A very common description of Node is this: Node.js allows JavaScript to run on the 
 server. While true, this is also misleading. Node's creators organized and linked 
 powerful C++ libraries in such a way that their efficiency could be harnessed without 
 developers needing to comprehend their complexities. Node abstracts away the 
 complexity of multiuser, simultaneous multithreaded I/O by wrapping that concurrency 
 model into a single-threaded environment that is easy to understand and already well 
 understood by millions of web developers.
  
 When you are working with Node you are ultimately working with C++, a language 
 whose suitability for enterprise-level software development no one would question.
  
 Node's C++ foundation puts lie to claims that Node is not enterprise-ready. These claims 
 confuse what JavaScript's role in the Node stack actually is. The bindings to Redis and 
 other database drivers used regularly in Node programs are C bindings–fast, near the 
 ""metal"". Node's process bindings (
 spawn
 , 
 exec
 , and so on) facilitate a smooth 
 integration of powerful system libraries (such as ImageMagick) with headless browsers 
 and HTTP data streams. With Node, we are able to access the enormously powerful suite 
 of native Unix programs through the ease and comfort of JavaScript. And of course, we 
 can write our own C++ add-ons.
  
 www.EBooksWorld.ir",NA
Hello World,"Let's build our first add-on. In keeping with tradition, this add-on will result in a Node 
 module that will print out 
 Hello World!
 . Even though this is a very simple example, 
 it typifies the structure of all further C++ add-ons you will build. This allows you to 
 incrementally experiment with new commands and structures, growing your 
 knowledge in easy to understand steps.
  
 C++ files are typically given a 
 .cc
  extension. To start, we create a 
 hello.cc
  file:
  
 #include <node.h>
  
 #include <v8.h>
  
 using namespace v8;
  
 Handle<Value> Method(const Arguments& args) {
  
  HandleScope scope;
  
  return scope.Close(String::New(""Hello World!""));
  
 }
  
 void init(Handle<Object> target) {
  
  target->Set(String::NewSymbol(""hello""),
  
  FunctionTemplate::New(Method)->GetFunction());
  
 }
  
 NODE_MODULE(hello, init)
  
 This example includes the Node and V8 libraries, building our module using the same 
 libraries Node modules build on 
 NODE_MODULE
  is simply a macro that greatly 
 simplifies the export of an initialization function, which every add-on should provide. 
 The name of the module itself should be passed as a first argument. The second 
 argument should point to the initializing function.
  
 You are embedding C++ code into the V8 runtime such that JavaScript can be bound into 
 the relevant scope. V8 must scope all the new allocations made in your code, and so 
 you'll need to wrap the code you write, extending V8. To this end you'll see several 
 instances of the 
 Handle<Value>
  syntax, wrapping C++ code in the examples that follow. 
 Comparing these wrappers to what will be defined in the initialization function pushed 
 out to 
 NODE_MODULE
  should make it clear how Node is being bound to C++ methods via 
 the V8 bridge.
  
  
 To learn more about how V8 embeds C++ code, visit 
  
  
 https://developers.google.com/v8/embed.
  
 [
  309 
 ]
  
 www.EBooksWorld.ir",NA
Creating a calculator ,"Of course one would never bother to write an add-on to simply echo back strings. 
  
 It is more likely that you will want to expose an API or interface to your Node 
 programs. Let's create a simple calculator, with two methods: 
 add
  and 
 subtract
 . In 
 this example, we will demonstrate how to pass arguments from JavaScript to 
 methods within an add-on, and to send any results back.
  
 The complete code for this example will be found in your code bundle. The meat of 
 the program can be seen in this snippet, where we define an interface for our two 
 methods, each one expect to receive two numbers as arguments:
  
 #include <node.h> 
  
 #include <v8.h>
  
 using namespace v8;
  
 Handle<Value> Add(const Arguments& args) {
  
  
  HandleScope scope;
  
  if(args.Length() < 2) {
  
  ThrowException(Exception::TypeError(String::New(""Wrong number  
  
  of arguments"")));
  
  return scope.Close(Undefined());
  
  }
  
  if(!args[0]->IsNumber() || !args[1]->IsNumber()) {
  
  ThrowException(Exception::TypeError(String::New(""Wrong  
  
  arguments"")));
  
  return scope.Close(Undefined());
  
  }
  
 [
  311 
 ]
  
 www.EBooksWorld.ir",NA
Implementing callbacks ,"In keeping with the typical pattern of a Node program, add-ons also implement the 
 notion of callbacks. As one might expect in a Node program, a C++ add-on 
 performing an expensive and time-consuming operation should comprehend the 
 notion of asynchronously executing functions.
  
 The following code will expose a method that will pass back the current system 
 time to any callback it is sent:
  
 #include <node.h> 
  
 #include <ctime>
  
 using namespace v8;
  
 Handle<Value> GetTime(const Arguments& args) 
 {
  
  HandleScope scope;
  
  Local<Function> cb = Local<Function>::Cast(args[0]); 
 const unsigned argc = 1;
  
  time_t stamp = time(0);
  
  Local<Value> argv[argc] = { 
  
  Local<Value>::New(Number::New(stamp)) 
  
  };
  
  cb->Call(Context::GetCurrent()->Global(), argc, argv);
  
  
  return 
 scope.Close(Undefined()); 
  
 }
  
 [
  313 
 ]",NA
Closing thoughts,"Being able to easily link C++ modules with your Node program is a powerful new 
 paradigm. It may be tempting, then, to exuberantly begin writing C++ add-ons for every 
 identifiable segment of your programs. While this might be a productive way to learn, it 
 is not necessarily the best idea in the long run. While it is certainly true that in general, 
 compiled C++ will run more quickly than JavaScript code, remember that V8 is 
 ultimately using another type of compilation on the JavaScript code it is running. 
 JavaScript running within V8 runs very efficiently. As well, we don't want to lose the 
 simplicity of organization and predictable single-threaded runtime of JavaScript when 
 designing complex interactions within a high-concurrency environment. Remember: 
 Node came into being partly as an attempt to save the developer from working with 
 threads and related complexities when performing I/O. As such, try and keep these two 
 rules in mind:
  
 • 
  
 Is a C++ module actually going to run more quickly?
 : The answer isn't 
 always yes. The extra step of jumping into a different execution context and 
 then back into V8 is wasteful if your add-on is only returning a static string. 
 Felix Geisendorfer's talk describing his work with 
  
 building fast MySQL bindings provides some insight into these decisions: 
 http://www.youtube.com/watch?v=Kdwwvps4J9A
 .
  
 [
  314 
 ]
  
 www.EBooksWorld.ir",NA
Links and resources,"The Node documentation for add-ons is excellent: 
 http://nodejs.org/api/ 
 addons.html
 .
  
 A repository containing many examples of add-ons can be found here: 
 https://github.com/rvagg/node-addon-examples
 .
  
 An excellent resource for those learning C++: 
 http://www.learncpp.com/
 .
  
 When you're feeling more confident, the source code for Node's core modules is 
 an excellent place to both explore and to learn from: 
 https://github.com/ 
 joyent/node/tree/master/src
 .
  
 [
  315 
 ]
  
 www.EBooksWorld.ir",NA
Index,NA,NA
Symbols,"403: Forbidden errors  
 238 .cc extension  309",NA
A,"abstract interface  58 
  
 Add method  312 
  
 add-ons 
  
  
 URL, for examples  315 
  
 AJAX  14 
  
 Amazon Web Services. 
 See
   AWS 
 Apache Bench (ab)  215, 216 
  
 application scaling 
  
  
 about  214 
  
  
 CPU usage, measuring  216, 217 
  
 data creep  218, 219 
  
  
 file descriptors  218 
  
  
 monitoring server tools  220 
  
  
 network latency  215 
  
  
 socket usage  218 
  
 ASCII characters 
  
  
 ASCII codes, converting to  66 
  
 ASCII codes 
  
  
 converting, to ASCII characters  66 
 assert module  267-269 
  
 asynchronous context  273 
  
 AWS 
  
  
 authenticating  237 
  
  
 data, setting with DynamoDB  244 
  
 e-mail, sending via SES  249, 250 
  
 errors  238 
  
  
 S3, using for storing files  239 
  
  
 using, in application  236
  
 using, with Node server  243",NA
B,"bind command 
  
  
 arguments  231 
  
 blocking process  27 
  
 bouncy module  226 
  
 browser testing 
  
  
 performing  279, 280 
  
  
 performing, ZombieJS used  280 
 buckets 
  
  
 working with  239 
  
 bundledDependencies  292 
  
 bundling  303",NA
C,"C++  307 
  
  
 URL  315 
  
 caching  303 
  
 C++ add-ons 
  
  
 calculator, creating  311-313 
  
  
 callbacks, implementing  313, 314 
  
  
 Hello World, building  309-311 
  
 calculator 
  
  
 creating  311-313 
  
 Calculus 
  
  
 features  308 
  
 callbacks 
  
  
 implementing  313, 314 
  
 child process communication, event sources 
  
 about  32 
  
  
 creating  32, 33
  
 www.EBooksWorld.ir",NA
D,"D3.js library  83 
  
 data binding  304 
  
 datagrams  230 
  
 db.stats() command  219 
  
 DDB. 
 See
   DynamoDB 
  
 deferred event sources 
  
  
 deferred execution blocks  42 
  
  
 execution blocks  42 
  
  
 I/O  42 
  
  
 timers  42 
  
 deferred execution, event sources 
  
  
 about  35 
  
  
 process.nextTick  36 
  
 dependencies  292 
  
 detached (Boolean)  192 
  
 devDependencies  292 
  
 digital streams  58 
  
 direct exchange  228 
  
 directives 
  
  
 list  291 
  
 directories 
  
  
 about  103 
  
  
 nested directories, moving through  103, 
   
 104, 105 
  
 domain.bind method  276 
  
 domain module  275, 276 
  
 DOM (Document Object Model)  83 
  
 duplex streams  65 
  
 DynamoDB 
  
  
 about  244 
  
  
 database, searching  247, 248 
  
  
 table, creating  244, 245",NA
E,"encapsulation  303 
  
 encoding (String) parameter  195 
  
 Encryption and Compression modules 
  
 Crypto  286 
  
  
 PunyCode  286 
  
  
 ZLIB  286 
  
 ENOENT (no entity) error  34
  
 [
  318 
 ]",NA
F,"Facebook Connect 
  
  
 used, for authenticating  250-253 
 fail_timeout directive  224 
  
 fanout exchange  228 
  
 favicon requests 
  
  
 handling  81
  
 file 
  
  
 about  89 
  
  
 byte by byte, reading  106 
  
  
 byte by byte, writing  110 
  
  
 caveats  113 
  
  
 fetching, at once  107 
  
  
 large chunks of data, writing  112 
  
  
 line by line, reading  108 
  
  
 parsing, cluster module used  203, 204 
  
  
 parsing, multiple child processes used  200-
   
 202 
  
  
 readable stream, creating  107 
  
  
 reading from  105 
  
  
 writable stream, creating  113 
  
  
 writing to  110 
  
 file changes 
  
  
 listening for  49-52 
  
 file events, event sources  34, 35 
  
 file paths 
  
  
 path.basename  93 
  
  
 path.dirname  93 
  
  
 path.extname  93 
  
  
 path.join  92 
  
  
 path.normalize  92 
  
  
 path.relative  93 
  
  
 path.resolve  93 
  
 filesystem 
  
  
 about  90 
  
  
 directories  91, 103 
  
  
 file attributes  94 
  
  
 file objects  90 
  
  
 file operations  97 
  
  
 file paths  92, 93 
  
  
 files, closing  96 
  
  
 files, opening  95 
  
  
 file types  91 
  
  
 NTFS (New Technology File System)  90 
  
 sSynchronicity  102 
  
  
 UFS (Unix File System)  90 
  
 file types, filesystem 
  
  
 device files  91 
  
  
 directories  91 
  
  
 links  91 
  
  
 named pipe  91 
  
  
 ordinary files  91 
  
  
 sockets  91
  
 [
  319 
 ]
  
 www.EBooksWorld.ir",NA
G,"GC (Garbage Collection)  17 
 gid (Number)  192 
  
 Grunt 
  
  
 about  282 
  
  
 URL  278 
  
  
 working with  283",NA
H,"Harmony 
  
  
 about  15, 18 
  
  
 options  18 
  
 hashtag #nodejs  49 
  
 headers 
  
  
 working with  77, 78 
  
 Hello World 
  
  
 building  309-311 
  
 htop 
  
  
 about  185, 217 
  
  
 URL  185 
  
 HTTP  67 
  
 HTTP Proxy 
  
  
 about  225 
  
  
 using  225, 226 
  
 HTTP requests 
  
  
 making  69, 70 
  
 HTTPS  72 
  
 HTTP server 
  
  
 creating  67, 68
  
 [
  320 
 ]
  
 www.EBooksWorld.ir",NA
I,"idle process  27 
  
 ImageMagick  83 
  
 images 
  
  
 streaming, with Node  83 
  
 installation, SSL certificate  73 
  
 integration tests  258, 259 
  
 Inter-Process Communication (IPC)  31 
 I/O bound  27 
  
 I/O events  26",NA
J,"JavaScript 
  
  
 extending  9 
  
 JavaScript event model 
  
  
 events  10 
  
  
 modularity  12 
  
  
 network  13 
  
 jsdom package  83",NA
K,KB (Kilobytes)  16,NA
L,"libuv 
  
  
 about  19, 189, 308 
  
  
 URL  190 
  
 listenng event  233 
  
 logproc folder  200",NA
M,"max_fails directive  224 
  
 MIME  80 
  
 Mocha 
  
  
 about  278 
  
  
 integrating, with ZombieJS  281 
  
  
 URL  278 
  
  
 used, for browser testing  278 
  
 modular architecture, Path Framework 
  
 achieving  303-306
  
 module 
  
  
 paths, resolving  288 
  
 module.children property  287 
  
 module.filename property  287 
  
 module.loaded property  287 
  
 module loading system 
  
  
 key goals  303 
  
 module object 
  
  
 about  287 
  
  
 module.children property  287 
  
  
 module.filename properties  287 
  
  
 module.filename property  287 
  
  
 module.loaded property  287 
  
  
 module.parent 
  
   
 property  287 
  
 module.parent property  287 
  
 module paths 
  
  
 resolving  288, 289 
  
 modules 
  
  
 about  286 
  
  
 Encryption and Compression  286 
  
  
 Network and I/O  286 
  
 Monit 
  
  
 URL  220 
  
 multicast  232 
  
 multiple child processes 
  
  
 used, for parsing file  200-202 
  
 multiple Node servers 
  
  
 forward proxy  220 
  
  
 HTTP Proxy, using  225 
  
  
 Nginx, as proxy  222 
  
  
 reverse proxy  221 
  
  
 running  220 
  
 multiprocess system 
  
  
 constructing  206-211 
  
 Multi-purpose Internet Mail Extension. 
 See 
    
 MIME 
  
 multithreading  189, 190 
  
 Munin 
  
  
 URL  220
  
 [
  321 
 ]
  
 www.EBooksWorld.ir",NA
N,"Nagios 
  
  
 URL  220 
  
 Network and I/O modules 
  
  
 DNS  286 
  
  
 FileSystem  286 
  
  
 HTTP  286 
  
  
 HTTPS  286 
  
  
 Net  286 
  
  
 Readline  286 
  
  
 TLS/SSL  286 
  
  
 TTY  286 
  
  
 UDP/Datagram  286 
  
 network-path reference  74 
  
 Network Time Protocol (NTP)  239 
  
 Nginx 
  
  
 about  222 
  
  
 connection failures, managing  224 
  
  
 using, as proxy  222-224 
  
  
 wiki URL  222 
  
 Node 
  
  
 about  7, 55 
  
  
 event-driven programming, implementing 
   
 25 
  
  
 multithreading  189, 190 
  
  
 single-threaded model  185 
  
  
 URL, for add-ons  315 
  
  
 used, for streaming images  83 
  
 node-amqp module  230 
  
 Node community 
  
  
 conventions  45 
  
  
 error handling  45-47 
  
  
 guidelines  48 
  
  
 pyramids  47 
  
 Node debugger  263-266 
  
 Node developer  285 
  
 NodeJitsu  225 
  
 Node.js environment 
  
  
 overview  8 
  
 Node program 
  
  
 executing  21, 23 
  
 Node's standard library 
  
  
 modules  286
  
 npm 
  
  
 about  13 
  
  
 dependencies, declaring  292, 293 
  
 package file, initializing  290, 291 
  
 packages, installing globally  294 
  
 packages, publishing  293 
  
  
 repositories, sharing  295 
  
  
 scripts, using  291 
  
  
 using  290 
  
 Number variable  312",NA
O,"objects 
  
  
 deleting  242 
  
  
 multiple objects, deleting  242 
  
 working with  240, 241 
  
 optionalDependencies  292 
  
 OS (Operating System)  16 
  
 output tools 
  
  
 util.debug(String)  260 
  
  
 util.error(String, [String ])  260 
  
 util.log(String)  261 
  
  
 util.print(String, [String ])  261 
  
 util.puts(String, [String ])  260",NA
P,"package file 
  
  
 description  290 
  
  
 entry point (main)  291 
  
  
 initializing  290 
  
  
 keywords  291 
  
  
 license  291 
  
  
 name  290 
  
  
 version  290 
  
 parallelism  126 
  
 PassThrough streams 
  
  
 using  67 
  
 Path Framework 
  
  
 about  297 
  
  
 calls, responding  302, 303 
  
  
 features  298 
  
  
 modular architecture, achieving  303-306 
 persistent (Boolean)  34
  
 [
  322 
 ]
  
 www.EBooksWorld.ir",NA
Q S,"Querystring module  76 
  
 queueing  29, 30",NA
R,"RabbitMQ 
  
  
 about  227 
  
  
 consumer, creating  227 
  
  
 exchange, binding  228 
  
  
 interacting  227 
  
  
 starting  227 
  
  
 types of exchanges  228 
  
 Readable stream 
  
  
 about  59 
  
  
 creating  59 
  
  
 implementing  59-61 
  
  
 push operation  61, 62 
  
 Read-Eval-Print-Loop. 
 See
   REPL 
  
 Readline module  109 
  
 Redis 
  
  
 user data, storing  134 
  
  
 using, for tracking client state  132, 133 
 ref function  40 
  
 REPL  21, 23
  
 S3 
  
  
 about  239 
  
  
 buckets, working with  239 
  
  
 objects, working with  240, 241 
  
  
 used, for storing files  239 
  
 sandboxing 
  
  
 about  270 
  
  
 compiled contexts, using  272 
  
  
 local scope, versus execution context  271 
 Scout 
  
  
 URL  220 
  
 scripts 
  
  
 preinstall, install, postinstall  291 
  
  
 prepublish, publish, postpublish  291 
  
 prerestart, restart, postrestart  291 
  
  
 prestart, start, poststart  291 
  
  
 prestop, stop, poststop  291 
  
  
 pretest, test, posttest  291 
  
  
 preuninstall, uninstall, postuninstall  291 
  
 preupdate, update, postupdate  291 
  
  
 using  291 
  
 self-signed certificate 
  
  
 creating, for development  72 
  
 send method  231
  
 [
  323 
 ]
  
 www.EBooksWorld.ir",NA
T,"TCP (Transmission Control Protocol) server 
   
 23 
  
 templates  304 
  
 testing 
  
  
 benefits  256 
  
 tests 
  
  
 functional tests  257 
  
  
 integration tests  258 
  
  
 unit tests  257 
  
 threading  123 
  
 timers 
  
  
 about  38 
  
  
 ref method  41 
  
  
 setInterval  39 
  
  
 setTimeout  38 
  
  
 unref method  40 
  
 TLS/SSL  14 
  
 topic exchange  228 
  
 Transform stream  66 
  
 TTY (TeleTYpewriter)  109 
  
 tunneling  70, 71",NA
U,"UDP module 
  
  
 about  230 
  
  
 close event  231 
  
  
 error event  231 
  
  
 multicasting UDP, setting up with 
  
   
 Node  233-235 
  
 UDP server 
  
  
 creating, with Node  230 
  
 uid (Number)  192 
  
 uncaughtException handler  274 
  
 unit tests  257 
  
 unref function  40 
  
 URL module  74, 75 
  
 User Datagram Protocol. 
 See
   UDP module 
 util.format(format, [arg, arg ]) method  
 261 util.format method  261 
  
 util.inspect method  261 
  
 util.inspect(object, [options]) method  262
  
 [
  324 
 ]
  
 www.EBooksWorld.ir",NA
V,"V8 
  
  
 about  15, 308, 309 
  
  
 harmony  18 
  
  
 limits  16 
  
  
 memory  16 
  
  
 process object  20 
  
  
 URL  309 
  
 V8 thread  19 
  
 validate_phone_number() method  257 
 VM (Virtual Machine)  15",NA
W,"watch method  34 
  
 wc command  200 
  
 Worker object 
  
  
 disconnect() method  205 
  
 events  206
  
  
 id attribute  205 
  
  
 kill([signal]) method  205 
  
  
 process attribute  205 
  
  
 properties  205 
  
  
 send(message, [sendHandle]) method  205 
  
 suicide attribute  205 
  
 Writable stream 
  
  
 about  62 
  
  
 creating  62 
  
  
 implementing  63, 64 
  
 Writable streams 
  
  
 constructors, instantiating  62 
  
 ws 
  
  
 URL  208",NA
Z,"ZombieJS 
  
 URL  278 
  
 used, for browser testing  280
  
 [
  325 
 ]
  
 www.EBooksWorld.ir",NA
Thank you for buying ,NA,NA
Mastering Node.js,NA,NA
About Packt Publishing,"Packt, pronounced 'packed', published its first book ""
 Mastering phpMyAdmin for Effective 
 MySQL Management
 "" in April 2004 and subsequently continued to specialize in publishing 
 highly focused books on specific technologies and solutions.
  
 Our books and publications share the experiences of your fellow IT professionals in adapting and 
 customizing today's systems, applications, and frameworks. Our solution based books give you the 
 knowledge and power to customize the software and technologies you're using to get the job done. 
 Packt books are more specific and less general than the IT books you have seen in the past. Our 
 unique business model allows us to bring you more focused information, giving you more of what 
 you need to know, and less of what you don't.
  
 Packt is a modern, yet unique publishing company, which focuses on producing quality, 
 cutting-edge books for communities of developers, administrators, and newbies alike. For 
 more information, please visit our website: 
 www.packtpub.com
 .",NA
About Packt Open Source,"In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to 
 continue its focus on specialization. This book is part of the Packt Open Source brand, home to 
 books published on software built around Open Source licences, and offering information to 
 anybody from advanced developers to budding web designers. The Open Source brand also runs 
 Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project 
 about whose software a book is sold.",NA
Writing for Packt,"We welcome all inquiries from people who are interested in authoring. Book proposals should be 
 sent to 
 author@packtpub.com
 . If your book idea is still at an early stage and you would like to 
 discuss it first before writing a formal book proposal, contact us; one of our commissioning 
 editors will get in touch with you. 
  
 We're not just looking for published authors; if you have strong technical skills but no writing 
 experience, our experienced editors can help you develop a writing career, or simply get some 
 additional reward for your expertise.
  
 www.EBooksWorld.ir",NA
Node Web Development - Second ,NA,NA
Edition ,"ISBN: 978-1-78216-330-5             Paperback: 248 pages
  
 A practical introduction to Node.js, an exciting 
 server-side JavaScript web development stack
  
 1. 
  
 2. 
  
 3. 
  
 Learn about server-side JavaScript with Node.js 
 and Node modules.
  
 Website development both with and without 
 the Connect/Express web application 
  
 framework.
  
 Developing both HTTP server and client 
 applications.",NA
Node Cookbook ,"ISBN: 978-1-84951-718-8            Paperback: 342 pages
  
 Over 50 recipes to master the art of asynchronous 
 server-side JavaScript using Node
  
 1. 
  
 2. 
  
 3. 
  
 Packed with practical recipes taking you from 
 the basics to extending Node with your own 
 modules
  
 Create your own web server to see Node’s 
 features in action
  
 Work with JSON, XML, web sockets, and make the 
 most of asynchronous programming
  
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
Using Node.js for UI Testing ,"ISBN: 978-1-78216-052-6            Paperback: 146 pages
  
 Learn how to easily automate testing of your web 
 apps using Node.js, Zombie.js and Mocha
  
 1. 
  
 2. 
  
 3. 
  
 Use automated tests to keep your web app rock 
 solid and bug-free while you code
  
 Use a headless browser to quickly test your web 
 application every time you make a small change 
 to it
  
 Use Mocha to describe and test the capabilities of 
 your web app",NA
CoffeeScript Programming with ,NA,NA
"jQuery, Rails, and Node.js","ISBN: 978-1-84951-958-8            Paperback: 140 pages
  
 Learn CoffeeScript programming with the three most 
 popular web technologies around
  
 1. 
  
 2. 
  
 3. 
  
 Learn CoffeeScript, a small and elegant 
  
 language that compiles to JavaScript and will 
 make your life as a web developer better
  
 Explore the syntax of the language and see how it 
 improves and enhances JavaScript
  
 Build three example applications in 
 CoffeeScript step by step
  
  
 Please check 
 www.PacktPub.com
  for information on our titles",NA
