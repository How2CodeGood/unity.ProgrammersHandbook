Larger Text,Smaller Text,Symbol
First Semester in Numerical Analysis with Julia ,"Book
  · April 
 2019
  
 DOI: 10.33009/jul
  
 CITATIONS 
  
 0
  
 READS 
  
 1,746
  
 1 author:
  
  
 Giray Ökten 
  
 Florida State University
  
 61
 PUBLICATIONS
 657
 CITATIONS
  
 SEE PROFILE
  
 All content following this page was uploaded by 
 Giray Ökten
  on 28 December 2020.
  
 The user has requested enhancement of the downloaded file.",NA
First Semester in ,NA,NA
Numerical Analysis ,NA,NA
with Julia,NA,NA
Giray Ökten,NA,NA
First Semester in Numerical Analysis with Julia,NA,NA
Giray Ökten ,NA,NA
Department of Mathematics ,NA,NA
Florida State University ,NA,NA
Tallahassee FL 32306,NA,NA
Contents,"1
  
 Introduction
  
 6
  
  
 2
  
 1.1
  
 Review of Calculus
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 6
  
 1.2
  
 Julia basics
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 9
  
 1.3
  
 Computer arithmetic
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 27
  
 Solutions of equations: Root-finding
  
 49
  
 3
  
 2.1
  
 Error analysis for iterative methods
  . . . . . . . . . . . . . . . . . . . . . . .
  
 52
  
 2.2
  
 Bisection method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 53
  
 2.3
  
 Newton’s method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 58
  
 2.4
  
 Secant method
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 68
  
 2.5
  
 Muller’s method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 71
  
 2.6
  
 Fixed-point iteration
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 74
  
 2.7
  
 High-order fixed-point iteration
  . . . . . . . . . . . . . . . . . . . . . . . . .
  
 83
  
 Interpolation
  
 86
  
 4
  
 3.1
  
 Polynomial interpolation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 87
  
 3.2
  
 High degree polynomial interpolation
  . . . . . . . . . . . . . . . . . . . . . .
  
 105
  
 3.3
  
 Hermite interpolation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 110
  
 3.4
  
 Piecewise polynomials: spline interpolation
  . . . . . . . . . . . . . . . . . . .
  
 118
  
 Numerical Quadrature and Differentiation
  
 13
 7
  
 4.1
  
 Newton-Cotes formulas
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 137
  
 4.2
  
 Composite Newton-Cotes formulas
  
 . . . . . . . . . . . . . . . . . . . . . . .
  
 143
  
 4.3
  
 Gaussian quadrature
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 148
  
 4.4
  
 Multiple integrals
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 155
  
 4.5
  
 Improper integrals
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 163
  
 4.6
  
 Numerical differentiation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 164
  
 2",NA
Preface,"This book is based on my lecture notes for a junior level undergraduate course in 
 Numerical Analysis. As the title suggests, the content is the standard material covered in 
 the first semester of Numerical Analysis classes in the colleges in the United States. The 
 reader is expected to have studied calculus and linear algebra. Some familiarity with a 
 programming language is beneficial, but not required. The programming language Julia will 
 be introduced in the book.
  
 The book presents the theory and methods, together with the implementation of the 
 algorithms using the Julia programming language (version 1.1.0). Incorporating coding and 
 computing within the main text was my primary objective in writing this book. The sim-
 plicity of Julia allows bypassing the pseudocode, and writing a computer code directly after 
 the description of a method. It also minimizes the distraction the presentation of a 
 computer code might cause to the flow of the main narrative. The Julia codes are written 
 without much concern for efficiency; the priority is to write codes that follow the 
 derivations pre-sented in the text. The Julia software is free, under an MIT license, and can 
 be downloaded at
  https://julialang.org
 .
  
 While writing this book, I badly needed a comic relief, and created a character, a college 
 student, who makes an appearance in each chapter. The character hopefully brings some 
 humor and makes Numerical Analysis more interesting to her fellow students! 
  
 I thank 
 my daughter Arya Ökten, a college student majoring in applied mathematics herself, who 
 graciously agreed to let the character use her name! I also thank her for reading parts of the 
 earlier drafts and making the drawings in the book.
  
 I thank my colleague Paul Beaumont who introduced me to Julia. I thank Sanghyun Lee 
 who used the book in his Numerical Analysis class and suggested several clarifications and 
 corrections that improved the book. I thank my colleagues Steve Bellenot, Kyle Gallivan, 
 Ahmet Göncü, and Mark Sussman for helpful discussions during the writing of the book. 
 Thanks to Steve for inspiring the example
  Arya and the letter NUH
  in Section
  3.4
 , and 
 thanks to Ahmet for his help with the modeling of weather temperature data example in 
 Section
  5.1
 . The book was supported by an Alternative Textbook Grant from the Florida
  
 4",NA
Chapter 1,NA,NA
Introduction,NA,NA
1.1 ,NA,NA
Review of Calculus,"There are several concepts and facts from Calculus that we need in Numerical Analysis. In 
 this section we will list some definitions and theorems that will be needed later. For the 
 most part functions in this book refer to real valued functions defined on real numbers
  R
 , 
 or an interval (
 a, b
 )
  ⊂
  R
 .
  
 Definition 1. 
  
 1. A function
  f
  has the limit
  L
  at
  x
 0
 , written as lim
 x→x
 0
  f
 (
 x
 ) =
  L,
  if for any
  ϵ >
  
 0
 ,
  there exists
  δ >
  0 such that
  |f
 (
 x
 )
  − L| < ϵ
  whenever 0
  < |x − x
 0
 | < δ.
  
 2. A function
  f
  is continuous at
  x
 0
  if lim
 x→x
 0
  f
 (
 x
 ) =
  f
 (
 x
 0
 )
 ,
  and
  f
  is continuous on a set 
  
 A
  if it 
 is continuous at each
  x
 0
  ⊂ A.
  
 3. Let
  {x
 n
 }
 ∞n
 =1
 be an infinite sequence of real numbers. The sequence has the limit
  x, 
 i.e., 
 lim
 n→∞
  x
 n
  =
  x
  (or, written as
  x
 n
  → x
  as
  n → ∞
 ) if for any
  ϵ >
  0
 ,
  there exists an integer
  N >
  
 0 such that
  |x
 n
  − x| < ϵ
  whenever
  n > N.
  
 Theorem 2.
  The following are equivalent for a real valued function f
  :
  
 1.
  f
  is continuous at
  x
 0
  
 2. If
  {x
 n
 }
 ∞n
 =1
 is any sequence converging to
  x
 0
 ,
  then lim
 n→∞
 f
 (
 x
 n
 ) =
  f
 (
 x
 0
 )
 .
  
 Definition 3.
  We say
  f
 (
 x
 ) is differentiable at
  x
 0
  if
  
 f
 ′
 (
 x
 0
 ) = lim 
 x→x
 0
  
 f
 (
 x
 )
  − f
 (
 x
 0
 ) 
 x 
 − x
 0
  
 = lim 
 h→
 0
  
 f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 ) 
 h
  
 exists.
  
 6",NA
1.2,NA,NA
Julia basics,"The first step is to download the Julia programming language from the Julia webpage 
 https://julialang.org
 . In this book I used Julia 1.1.0. There are different environments and 
 editors to run Julia. Here I will use the Jupyter environment
  https://jupyter.org
 . 
 Instructions on how to install software do not age well: I ran the command
  using IJulia; 
 jupyterlab()
  on the Julia terminal to install the Jupyter environment, but this could change 
 in the future versions of the software. There are several tutorials and other resources on 
 Julia at
  https://julialang.org
  where one can find up-to-date information on installing Julia 
 and Jupyter.
  
 The Jupyter environment uses the so-called Jupyter notebook where one can write and 
 edit a Julia code, run the code, and export the work into various file formats including Latex 
 and pdf. Most of our interaction with Julia will be through the Jupyter notebooks. One 
 exception is when we need to install a package. A Julia package provides additional 
 functionality to the core programs, and there are numerous packages from visualization to 
 parallel computing and machine learning. To install a package, open Julia by clicking on the 
 software icon, which will open the Julia terminal, called Julia REPL (Read-Evaluate-Print-
 Loop). In the Julia REPL, press
  ]
  to enter the package mode. (To get back to the Julia REPL 
 press backspace, delete, or shift C.) Once in the package mode, typing ""add PackageName"" 
 will install the package. To learn more about packages, type Pkg in the Julia 1.1 
 documentation
  https://docs.julialang.org/en/v1/index.html
 .
  
 After installing Julia and Jupyter, open a Jupyter notebook. Here is a screenshot of my 
 notebook:",NA
Arrays,"For example, the syntax for the logarithm function is
  
 Here is the basic syntax to create an array:
  
 In [4]:
  x
 =
 [
 10
 ,
 20
 ,
 30
 ]
  
 Out[4]:
  3-element Array{Int64,1}: 
  
 10 
  
 20 
  
 30",NA
Plotting,"There are several packages for plotting functions and we will use the PyPlot package. To 
 install the package, open a Julia terminal. Here is how it looks like:
  
  
 Press
  ]
  to switch to the package mode, and then type and enter ""add PyPlot"". Your terminal 
 will look like this:",NA
Matrix operations,"Let’s create a 3
  ×
  3 matrix:
  
 In [21]:
  A
 =
 [
 -1 0.26 0.74
 ;
  0.09 -1 0.26
 ;
  1 1 1
 ]
  
 Out[21]:
  3
 ×
 3 Array{Float64,2}:
  
  
 -1.0 
  
 0.26 
  
  
 0.74 
  
  
 0.09
  
 -1.0 
  
  
 0.26 
  
  
 1.0 
  
 1.0 
  
 1.0",NA
Logic operations,"Here are some basic logic operations:
  
 In [32]:
  2==3
  
 Out[32]:
  false
  
 In [33]:
  2<=3
  
 Out[33]:
  true
  
 In [34]:
  (
 2==2
 )
 ||
 (
 1<0
 )
  
 Out[34]:
  true
  
 In [35]:
  (
 2==2
 )
 &&
 (
 1<0
 )",NA
Defining functions ,"There are three ways to define a function. Here is the basic syntax: 
  
 In [39]:
  function
  squareit(x) 
  
  
  
  
 return
  x
 ^2 
  
  
  
 end 
  
 Out[39]:
  squareit (generic function with 1 method) 
  
 In [40]:
  squareit(
 3
 ) 
  
 Out[40]:
  9 
  
 There is also a compact form to define a function, if the body of the function is a short, 
 simple expression: 
  
 In [41]:
  cubeit(x)
 =
 x
 ^3 
  
 Out[41]:
  cubeit (generic function with 1 method) 
  
 In [42]:
  cubeit(
 5
 ) 
  
 Out[42]:
  125 
  
 Functions can be defined without being given a name: these are called anonymous func-
 tions:",NA
Types,"In Julia, there are several types for integers and floating-point numbers such as Int8, Int64, 
 Float16, Float64, and more advanced types for Boolean variables, characters, and strings. 
 When we write a function, we do not have to declare the type of its variables: Julia figures 
 what the correct type is when the code is compiled. This is called a dynamic type system.
  
 For example, consider the
  squareit
  function we defined before:
  
 In [46]:
  function
  squareit(x) 
  
  
  
 return
  x
 ^2 
  
  
 end
  
 Out[46]:
  squareit (generic function with 1 method)
  
 The type of
  x
  is not declared in the function definition. We can call it with real or integer 
 inputs, and Julia will know what to do:
  
 In [47]:
  squareit(
 5
 )",NA
Control flow,"Let’s create an array of 10 entries of floating-type. A simple way to do is by using the 
 function
  zeros(n)
 , which creates an array of size
  n
 , and sets each entry to zero. (A similar 
 function is
  ones(n)
  which creates an array of size
  n
  with each entry set to 1.)
  
 In [52]:
  values
 =
 zeros(
 10
 )
  
 Out[52]:
  10-element Array{Float64,1}: 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0 
  
 0.0
  
 Now we will set the elements of the array to values of sin function.
  
 In [53]:
  for
  n
  in
  1:10 
  
  
  
 values[n]
 =
 sin(n
 ^2
 ) 
  
  
 end
  
 In [54]:
  values
  
 Out[54]:
  10-element Array{Float64,1}: 
  
  
 0.8414709848078965
  
  
 -0.7568024953079282 
  
  
 0.4121184852417566
  
  
 -0.2879033166650653
  
  
 -0.13235175009777303
  
  
 -0.9917788534431158
  
  
 -0.9537526527594719 
  
  
 0.9200260381967907
  
  
 -0.6298879942744539
  
  
 -0.5063656411097588",NA
Random numbers,"These are 5 uniform random numbers from (0,1).
  
 In [64]:
  rand(
 5
 )
  
 Out[64]:
  5-element Array{Float64,1}: 
  
  
 0.9376491626727412",NA
1.3,NA,NA
Computer arithmetic,"The way computers store numbers and perform computations could surprise the beginner. 
  
 In Julia if you type (
 √
 3)
 2
 the result will be 2.9....96, where 9 is repeated 15 times. Here are",NA
Floating-point representation of real numbers,"Here is a general model for representing real numbers in a computer:
  
 x
  =
  s
 (
 .a
 1
 a
 2
 ...a
 t
 )
 β
  × β
 e 
 (1.1)
  
 where
  
 s →
  sign of
  x
  =
  ±
 1 
  
  
 e →
  exponent, with bounds
  L ≤ e ≤ U 
 (
 .a
 1
 ...a
 t
 )
 β
  =
 a
 1
 β
 +
  a
 2
 β
 2
  +
  ...
  +
  a
 t
 β
 t
 ; the mantissa
  
 β →
  base 
  
 t →
  number of digits; the precision.
  
 In the floating-point representation (
 1.1
 ), if we specify
  e
  in such a way that
  a
 1
  ̸
 = 0
 ,
  then 
 the representation will be unique. This is called the
  normalized
  floating-point represen-
 tation. For example if
  β
  = 10
 ,
  in the normalized floating-point we would write 0
 .
 012 as 0
 .
 12
  
 ×
  10
 −
 1
 , instead of choices like 0
 .
 012
  ×
  10
 0
 or 0
 .
 0012
  ×
  10.
  
 In most computers today, the base is
  β
  = 2
 .
  Bases 8 and 16 were used in old IBM 
 mainframes in the past. Some handheld calculators use base 10. An interesting historical 
 example is a short-lived computer named Setun developed at Moscow State University 
 which used base 3.
  
 There are several choices to make in the general floating-point model (
 1.1
 ) for the 
 values of
  s, β, t, e
 . The IEEE 64-bit floating-point representation is the specific model used in 
 most computers today:",NA
Representation of integers,"In the previous section, we discussed representing real numbers in a computer. Here we 
 will give a brief discussion of representing integers. How does a computer represent an 
 integer
  n
 ? As in real numbers, we start with writing
  n
  in base 2. We have 64 bits to 
 represent its digits and sign. As in the floating-point representation, we can allocate one bit 
 for the sign, and",NA
Chopping & Rounding,"Let
  x
  be a real number with more digits the computer can handle:
  x
  = 0
 .d
 1
 d
 2
  . . . d
 k
 d
 k
 +1
  . . 
 .×
 10
 n
 .
  How will the computer represent
  x
 ? Let’s use the notation
  fl
 (
 x
 ) for the floating-point 
 representation of
  x.
  There are two choices, chopping and rounding:
  
 • In chopping, we simply take the first
  k
  digits and ignore the rest:
  fl
 (
 x
 ) = 0
 .d
 1
 d
 2
  . . . d
 k
 .
  
 • In rounding, if
  d
 k
 +1
  ≥
  5 we add 1 to
  d
 k
  to obtain
  fl
 (
 x
 )
 .
  If
  d
 k
 +1
  <
  5
 ,
  then we simply 
  
 do as 
 in chopping.
  
 Example 11.
  Find 5-digit (
 k
  = 5) chopping and rounding values of the numbers below:
  
 •
  π
  = 0
 .
 314159265
 ... ×
  10
 1 
  
  
 Chopping gives
  fl
 (
 π
 ) = 0
 .
 31415 and rounding gives
  fl
 (
 π
 ) = 0
 .
 31416
 .
  
 • 0
 .
 0001234567 
  
 We need to write the number in the normalized representation first as 
 0
 .
 1234567
 ×
 10
 −
 3
 . 
 Now chopping gives 0
 .
 12345 and rounding gives 0
 .
 12346
 .",NA
Absolute and relative error,"Since computers only give approximations to real numbers, we need to be clear on how we 
 measure the error of an approximation.
  
 Definition 12.
  Suppose
  x
 ⊂
 is an approximation to
  x
 .
  
 •
  |x
 ⊂
 − x|
  is called the
  absolute error
  
 •
  
 |x
 ⊂
 −x| |x|
  
 is called the
  relative error
  (
 x ̸
 = 0)
  
 Relative error usually is a better choice of measure, and we need to understand why.
  
 Example 13.
  Find absolute and relative errors of
  
 1.
  x
  = 0
 .
 20
  ×
  10
 1
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 1
  
 2.
  x
  = 0
 .
 20
  ×
  10
 −
 2
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 −
 2
  
 3.
  x
  = 0
 .
 20
  ×
  10
 5
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 5
  
 Notice how the only difference in the three cases is the exponent of the numbers. 
  
 The 
 absolute errors are: 0
 .
 01
  ×
  10 , 0
 .
 01
  ×
  10
 −
 2
 , 0
 .
 01
  ×
  10
 5
 . The absolute errors are different 
 since the exponents are different. However, the relative error in each case is the same: 0
 .
 05
 .",NA
Machine epsilon,"Machine epsilon
  ϵ
  is the smallest positive floating point number for which
  fl
 (1+
 ϵ
 )
  >
  1
 .
  This 
 means, if we add to 1
 .
 0 any number less than
  ϵ
 , the machine computes the sum as 1
 .
 0.
  
 The number 1.0 in its binary floating-point representation is simply (1
 .
 0
  . . .
  0)
 2
  where 
 a
 2
  
 =
  a
 3
  =
  ...
  =
  a
 53
  = 0
 .
  We want to find the smallest number that gives a sum larger than 1.0, 
 when it is added to 1.0. The answer depends on whether we chop or round.
  
 If we are chopping, examine the binary addition
  
  
  
 a
 2
  
 a
 52
  
 a
 53
  
 1.
  
 0
  
 ...
  
 0
  
 0
  
 +
  
 0.
  
 0
  
 ...
  
 0
  
 1
  
 and notice (0
 .
 0
 ...
 01)
 2
  =
  
  1
  
 1.
  
 0
  
 ...
  
 0
  
 1
  
 52
  = 2
 −
 52
  is the smallest number we can add to 1.0 such that the
  
 2
  
 sum will be different than 1.0.
  
 If we are rounding, examine the binary addition
  
 +
  
 1.
  
 a
 2
  
 ...
  
 a
 52
  
 a
 53
  
 1
  
 0
  
 0
  
 0
  
 0.
  
 0
  
 ...
  
 0
  
 0
  
  
 1.
  
 0
  
 ...
  
 0
  
 0
  
 1
  
 where the sum has to be rounded to 53 digits to obtain
  
 a
 2
  
 ...
  
 a
 52
  
 a
 53
  
 .
  
 1.
  
 0
  
 0
  
 1
  
 Observe that we have added (0
 .
 0
 ...
 01)
 2
  =
  
  1
  
 53
  = 2
 −
 53
  to 1
 .
 0, which is the smallest number
  
 2
  
 that will make the sum larger than 1.0 with rounding. 
 In summary, we have shown
  
 As a consequence, notice that we can restate the inequality in Remark
  16
  in a compact way
 ϵ
  
 =
 2
 −
 52
  
 2
 −
 53 
  
  
 if chopping
  
 if rounding 
  
 .",NA
Propagation of error,"We discussed the resulting error when chopping or rounding is used to approximate a real 
 number by its machine version. Now imagine carrying out a long calculation with many 
 arithmetical operations, and at each step there is some error due to say, rounding. Would 
 all the rounding errors accumulate and cause havoc? This is a rather difficult question to 
 answer in general. For a much simpler example, consider adding two real numbers
  x, y.
  In 
 the computer, the numbers are represented as
  fl
 (
 x
 )
 , fl
 (
 y
 )
 .
  The sum of these number is
  fl
 (
 x
 ) + 
 fl
 (
 y
 ), however, the computer can only represent its floating-point version,
  fl
 (
 fl
 (
 x
 ) +
  fl
 (
 y
 ))
 .
  
 Therefore the relative error in adding two numbers is:
  
 (
 x
  +
  y
 )
  − fl
 (
 fl
 (
 x
 ) +
  fl
 (
 y
 )) 
  
 x
  +
  y
  
  .
  
 In this section, we will look at some specific examples where roundoff error can cause 
 prob-lems, and how we can avoid them.
  
 Subtraction of nearly equal quantities: Cancellation of leading digits
  
 The best way to explain this phenomenon is by an example. Let
  x
  = 1
 .
 123456
 , y
  = 1
 .
 123447
 . 
 We will compute
  x−y
  and the resulting roundoff error using rounding and 6-digit 
 arithmetic.
  
 First, we find
  fl
 (
 x
 )
 , fl
 (
 y
 ) :
  
 fl
 (
 x
 ) = 1
 .
 12346
 , fl
 (
 y
 ) = 1
 .
 12345
 .",NA
Sources of error in applied mathematics ,"Here is a list of potential sources of error when we solve a problem.
  
 1. Error due to the simplifying assumptions made in the development of a mathematical 
  
 model for the physical problem.
  
 2. Programming errors.
  
 3. Uncertainty in physical data: error in collecting and measuring data.
  
 4. Machine errors: rounding/chopping, underflow, overflow, etc.
  
 5. Mathematical truncation error: error that results from the use of numerical methods 
  
 in solving a problem, such as evaluating a series by a finite sum, a definite integral by 
  
 a numerical integration method, solving a differential equation by a numerical method. 
 Example 23.
  The volume of the Earth could be computed using the formula for the volume 
 of a sphere,
  V
  = 4
 /
 3
 πr
 3
 , where
  r
  is the radius. This computation involves the following 
 approximations: 
  
 1. The Earth is modeled as a sphere (modeling error) 
  
 2. Radius
  r ≈
  6370 km is based on empirical measurements (uncertainty in physical data) 3. 
 All the numerical computations are done in a computer (machine error) 
  
 4. The value of
  π
  has to be truncated (mathematical truncation error)
  
 Exercise 1.3-6:
  
 The following is from ""Numerical mathematics and computing"" by
  
 Cheney & Kincaid [
 7
 ]:
  
 In 1996, the Ariane 5 rocket launched by the European Space Agency exploded 40 seconds 
 after lift-off from Kourou, French Guiana. An investigation determined that the horizontal 
 velocity required the conversion of a 64-bit floating-point number to a 16-bit signed integer. It 
 failed because the number was larger than 32,767, which was the largest integer of this type 
 that could be stored in memory. The rocket and its cargo were valued at $500 million.
  
 Search online, or in the library, to find another example of computer arithmetic gone very 
 wrong! Write a short paragraph explaining the problem, and give a reference.",NA
Chapter 2,NA,NA
Solutions of equations: Root-finding,"Arya and the mystery of the Rhind papyrus
  
 College life is full of adventures, some hopefully of intellectual nature, and Arya is doing her 
 part by taking a history of science class. She learns about the Rhind papyrus; an ancient 
 Egyptian papyrus purchased by an antiquarian named Henry Rhind in Luxor, Egypt, in 
 1858.
  
  
 Figure 2.1: Rhind Mathematical Papyrus. (British Museum Image under a Creative Com-
 mons license.)
  
 The papyrus has a collection of mathematical problems and their solutions; a 
 translation
  
 49",NA
2.1,NA,NA
Error analysis for iterative methods,"Assume we have an iterative method
  {p
 n
 }
  that converges to the root
  p
  of some function. 
 How can we assess the rate of convergence?
  
 Definition 24.
  Suppose
  {p
 n
 }
  converges to
  p
 . If there are constants
  C >
  0 and
  α >
  1 such that
  
 |p
 n
 +1
  − p| ≤ C|p
 n
  − p|
 α
 , 
 (2.1)
  
 for
  n ≥
  1, then we say
  {p
 n
 }
  converges to
  p
  with order
  α
 .
  
 Special cases:
  
 • If
  α
  = 1 and
  C <
  1, we say the convergence is linear, and the rate of convergence is
  
 C
 . In this case, using induction, we can show
  
 |p
 n
 +1
  − p| ≤ C
 n
 |p
 1
  − p|. 
 (2.2)
  
 There are some methods for which Equation (
 2.2
 ) holds, but Equation (
 2.1
 ) does not
  
 hold for any
  C <
  1.
  
 We still call these methods to be of linear convergence.
  
 An
  
 example is the bisection method.
  
 • If
  α >
  1, we say the convergence is superlinear. In particular, the case
  α
  = 2 is called
  
 quadratic convergence.",NA
2.2,NA,NA
Bisection method,"Let’s recall the Intermediate Value Theorem (IVT), Theorem
  6
 : If a continuous function
  f 
 defined on [
 a, b
 ] satisfies
  f
 (
 a
 )
 f
 (
 b
 )
  <
  0
 ,
  then there exists
  p ⊂
  [
 a, b
 ] such that
  f
 (
 p
 ) = 0
 .
  
 Here is the idea behind the method. At each iteration, divide the interval [
 a, b
 ] into two 
 subintervals and evaluate
  f
  at the midpoint. Discard the subinterval that does not contain 
 the root and continue with the other interval.
  
 Example 26.
  Compute the first three iterations by hand for the function plotted in Figure 
 (
 2.2
 ).
  
 2.5
  
 -4
  
 -3
  
 -2
  
 -1
  
 0
  
 1
  
 2
  
 3
  
 4
  
 -2.5
  
 Figure 2.2",NA
Julia code for the bisection method,"In Example
  26
 , we kept track of the intervals and midpoints obtained from the bisection 
 method, by labeling them as [
 a
 1
 , b
 1
 ]
 ,
  [
 a
 2
 , b
 2
 ]
 , ...,
  and
  p
 1
 , p
 2
 , ...
 . So at step
  n
  of the method, we 
 know we are working on the interval [
 a
 n
 , b
 n
 ] and its midpoint is
  p
 n
 . This approach will be 
 useful when we study the convergence of the method in the next theorem. However, 
 keeping track of the intervals and midpoints is not needed in the computer code. Instead, in 
 the Julia code below, we will let [
 a, b
 ] be the current interval we are working on, and when 
 we obtain
  
 1
 Notice how we label the midpoints, as well as the endpoints of the interval, with the step number.",NA
2.3 ,NA,NA
Newton’s method,"Suppose
  f ⊂ C
 2
 [
 a, b
 ]
 ,
  i.e.,
  f, f
 ′
 , f
 ′′
 are continuous on [
 a, b
 ]
 .
  Let
  p
 0
  be a ""good"" approximation to
  p
  
 such that
  f
 ′
 (
 p
 0
 )
  ̸
 = 0 and
  |p − p
 0
 |
  is ""small"". First Taylor polynomial for
  f
  at
  p
 0
  with the 
 remainder term is
  
 f
 (
 x
 ) =
  f
 (
 p
 0
 ) + (
 x − p
 0
 )
 f
 ′
 (
 p
 0
 ) + (
 x − p
 0
 )
 2 
 f
 ′′
 (
 ξ
 (
 x
 ))
  
 where
  ξ
 (
 x
 ) is a number between
  x
  and
  p
 0
 .
  Substitute
  x
  =
  p
  and note
  f
 (
 p
 ) = 0 to get:
  
 0 =
  f
 (
 p
 0
 ) + (
 p − p
 0
 )
 f
 ′
 (
 p
 0
 ) + (
 p − p
 0
 )
 2 
 f
 ′′
 (
 ξ
 (
 p
 ))
  
 where
  ξ
 (
 p
 ) is a number between
  p
  and
  p
 0
 .
  Rearrange the equation to get
  
 p
  =
  p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 −
  (
 p − p
 0
 )
 2
  
 f
 ′′
 (
 ξ
 (
 p
 )) 
  
 f
 ′
 (
 p
 0
 )
 .
  
 (2.4)
  
 If
  |p − p
 0
 |
  is ""small"" then (
 p − p
 0
 )
 2
 is even smaller, and the error term can be dropped to 
 obtain the following approximation:
  
 p ≈ p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 .
  
 The idea in Newton’s method is to set the next iterate,
  p
 1
 , to this approximation:
  
 p
 1
  =
  p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 .
  
 Equation (
 2.4
 ) can be written as
  
 p
  =
  p
 1
  −
 (
 p − p
 0
 )
 2
  
 f
 ′′
 (
 ξ
 (
 p
 )) 
  
 f
 ′
 (
 p
 0
 )
 .
  
 (2.5)
  
 Summary:
  Start with an initial approximation
  p
 0
  to
  p
  and generate the sequence
  {p
 n
 }
 ∞n
 =1
  
 by
  
 p
 n
  =
  p
 n−
 1
  −f
 (
 p
 n−
 1
 ) 
 f
 ′
 (
 p
 n−
 1
 )
 , n ≥
  1
 .
  
 (2.6)
  
 This is called Newton’s method.
  
 Graphical interpretation:
  
 Start with
  p
 0
 .
  Draw the tangent line at (
 p
 0
 , f
 (
 p
 0
 )) and approximate
  p
  by the intercept
  p
 1",NA
Julia code for Newton’s method,"The Julia code below is based on Equation (
 2.6
 ). The variable
  pin
  in the code corresponds to 
 p
 n−
 1
 , and
  p
  corresponds to
  p
 n
 . The code overwrites these variables as the iteration continues.
  
 Also notice that the code has two functions as inputs;
  f
  and
  fprime
  (the derivative
  f
 ′
 ).
  
 In [1]:
  function
  newton(f
 ::
 Function
 ,fprime
 ::
 Function
 ,pin,eps,N) 
  
  
  
 n
 =1 
  
  
  
 p
 =0.
  # to ensure the value of p carries out of the while loop 
  
  
 while
  
 n
 <=
 N 
  
  
  
  
 p
 =
 pin
 -
 f(pin)
 /
 fprime(pin) 
  
  
  
  
 if
  f(p)
 ==0 ||
  abs(p
 -
 pin)
 <
 eps 
  
  
  
  
  
 return
  println(
 ""p is
  $p
  and the iteration number is
  $n
 ""
 ) 
  
  
  
 end 
  
  
  
  
 pin
 =
 p 
  
  
  
  
 n
 =
 n
 +1 
  
  
  
 end 
  
  
  
 y
 =
 f(p) 
  
  
  
 println(
 ""Method did not converge. The last iteration gives
  $p
  with 
  
  
  
 function value
  $y
 ""
 ) 
  
  
 end
  
 Out[1]:
  newton (generic function with 1 method)
  
 Let’s apply Newton’s method to find the root of
  f
 (
 x
 ) =
  x
 5
 + 2
 x
 3
 −
  5
 x −
  2, a function we 
 considered before. First, we plot the function.
  
 In [2]:
  using
  PyPlot
  
 In [3]:
  x
 =
 range(
 -2
 ,
 2
 ,length
 =1000
 ) 
  
 y
 =
 map(x
 ->
 x
 ^5+2*
 x
 ^3-5*
 x
 -2
 ,x) 
  
 ax
  =
  gca() 
  
 ax
 .
 spines[
 ""bottom""
 ]
 .
 set_position(
 ""center""
 ) 
 ax
 .
 spines[
 ""left""
 ]
 .
 set_position(
 ""center""
 ) 
 ax
 .
 spines[
 ""top""
 ]
 .
 set_position(
 ""center""
 ) 
 ax
 .
 spines[
 ""right""
 ]
 .
 set_position(
 ""center""
 ) ylim([
 -
 40
 ,
 40
 ]) 
  
 plot(x,y);",NA
2.4 ,NA,NA
Secant method,"One drawback of Newton’s method is that we need to know
  f
 ′
 (
 x
 ) explicitly to evaluate
  
 f
 ′
 (
 p
 n−
 1
 ) in
  
 p
 n
  =
  p
 n−
 1
  −f
 (
 p
 n−
 1
 ) 
 f
 ′
 (
 p
 n−
 1
 )
 , n ≥
  1
 .
  
 If we do not know
  f
 ′
 (
 x
 ) explicitly, or if its computation is expensive, we might approximate 
 f
 ′
 (
 p
 n−
 1
 ) by the finite difference
  
 f
 (
 p
 n−
 1
  +
  h
 )
  − f
 (
 p
 n−
 1
 ) 
  
 h
  
 (2.11)
  
 for some small
  h.
  We then need to compute two values of
  f
  at each iteration to approximate 
 f
 ′
 . Determining
  h
  in this formula brings some difficulty, but there is a way to get around this. 
 We will use the iterates themselves to rewrite the finite difference (
 2.11
 ) as
  
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 ) 
  
 p
 n−
 1
  − p
 n−
 2
  
 .
  
 Then, the recursion for
  p
 n
  simplifies as
  
 p
 n
  =
  p
 n−
 1
  −
  
 f
 (
 p
 n−
 1
 ) 
  
 f
 (
 p
 n−
 1
 )
 −f
 (
 p
 n−
 2
 ) 
 p
 n−
 1
 −p
 n−
 2
  
 =
  p
 n−
 1
  − f
 (
 p
 n−
 1
 )
  
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 )
 , n ≥
  2
 . p
 n−
 1
  − 
 p
 n−
 2
  
 (2.12)
  
 This is called the secant method. Observe that
  
 1. No additional function evaluations are needed,
  
 2. The recursion requires two initial guesses
  p
 0
 , p
 1
 .
  
 Geometric interpretation
 : The slope of the secant line through the points (
 p
 n−
 1
 , f
 (
 p
 n−
 1
 )) 
 and (
 p
 n−
 2
 , f
 (
 p
 n−
 2
 )) is
 f
 (
 p
 n−
 1
 )
 −f
 (
 p
 n−
 2
 ) 
 p
 n−
 1
 −p
 n−
 2 
  
  
 . The
  x
 -intercept of the secant line, which 
 is set to
  p
 n
 , is
  
 0
  − f
 (
 p
 n−
 1
 ) 
 p
 n
  
 − p
 n−
 1
  
 =
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 ) 
  
 p
 n−
 1
  − p
 n−
 2
  
 ⊂ p
 n
  =
  p
 n−
 1
  − f
 (
 p
 n−
 1
 )
  
 p
 n−
 1
  − p
 n−
 2 
  
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 )
  
 which is the recursion of the secant method.
  
 The following theorem shows that if the initial guesses are ""good"", the secant method has 
 superlinear convergence. A proof can be found in Atkinson [
 3
 ].
  
 Theorem 35.
  Let f ⊂ C
 2
 [
 a, b
 ]
  and assume f
 (
 p
 ) = 0
 , f
 ′
 (
 p
 )
  ̸
 = 0
 , for p ⊂
  (
 a, b
 )
 . If the initial guesses 
 p
 0
 , p
 1
  are sufficiently close to p, then the iterates of the secant method converge to p",NA
Julia code for the secant method,"The following code is based on Equation (
 2.12
 ); the recursion for the secant method. The 
 initial guesses are called
  pzero
  and
  pone
  in the code. The same stopping criterion as in 
 Newton’s method is used. Notice that once a new iterate
  p
  is computed,
  pone
  is updated as 
 p
 , and
  pzero
  is updated as
  pone
 .
  
 In [1]:
  function
  secant(f
 ::
 Function
 ,pzero,pone,eps,N) 
  
  
  
 n
 =1 
  
  
  
 p
 =0.
  # to ensure the value of p carries out of the while loop 
  
  
 while
  
 n
 <=
 N 
  
  
  
  
 p
 =
 pone
 -
 f(pone)
 *
 (pone
 -
 pzero)
 /
 (f(pone)
 -
 f(pzero)) 
  
  
  
  
 if
  f(p)
 ==0 ||
  abs(p
 -
 pone)
 <
 eps 
  
  
  
  
  
 return
  println(
 ""p is
  $p
  and the iteration number is
  $n
 ""
 ) 
  
  
  
 end 
  
  
  
  
 pzero
 =
 pone 
  
  
  
  
 pone
 =
 p 
  
  
  
  
 n
 =
 n
 +1 
  
  
  
 end 
  
  
  
 y
 =
 f(p) 
  
  
  
 println(
 ""Method did not converge. The last iteration gives
  $p
  with 
  
  
  
 function value
  $y
 ""
 ) 
  
  
 end
  
 Out[1]:
  secant (generic function with 1 method)
  
 Let’s find the root of
  f
 (
 x
 ) = cos
  x − x
  using the secant method, using 0.5 and 1 as the initial 
 guesses.
  
 In [2]:
  secant(x
 ->
  cos(x)
 -
 x,
 0.5
 ,
 1
 ,
 10^
 (
 -4.
 ),
 20
 )
  
 p is 0.739085132900112 and the iteration number is 4",NA
2.5,NA,NA
Muller’s method,"The secant method uses a linear function that passes through (
 p
 0
 , f
 (
 p
 0
 )) and (
 p
 1
 , f
 (
 p
 1
 )) to 
 find the next iterate
  p
 2
 .
  Muller’s method takes three initial approximations, passes a 
 parabola (quadratic polynomial) through (
 p
 0
 , f
 (
 p
 0
 ))
 ,
  (
 p
 1
 , f
 (
 p
 1
 )), (
 p
 2
 , f
 (
 p
 2
 )), and uses
  one
  of 
 the roots of the polynomial as the next iterate.
  
 Let the quadratic polynomial written in the following form
  
 P
 (
 x
 ) =
  a
 (
 x − p
 2
 )
 2
 +
  b
 (
 x − p
 2
 ) +
  c. 
  
 (2.13
 )
  
 Solve the following equations for
  a, b, c
  
 P
 (
 p
 0
 ) =
  f
 (
 p
 0
 ) =
  a
 (
 p
 0
  − p
 2
 )
 2
 +
  b
 (
 p
 0
  − p
 2
 ) +
  c 
  
 P
 (
 p
 1
 ) =
  f
 (
 p
 1
 ) =
  a
 (
 p
 1
  − p
 2
 )
 2
 +
  b
 (
 p
 1
  − p
 2
 ) +
  c 
  
 P
 (
 p
 2
 ) =
  f
 (
 p
 2
 ) =
  c
  
 to get
  
 c
  =
  f
 (
 p
 2
 )
  
 b
  = (
 p
 0
  − p
 2
 )(
 f
 (
 p
 1
 )
  − f
 (
 p
 2
 )) 
  
 (
 p
 1
  − p
 2
 )(
 p
 0
  − p
 1
 )
  
 −
 (
 p
 1
  − 
 p
 2
 )(
 f
 (
 p
 0
 )
  − f
 (
 p
 2
 )) (
 p
 0
  − p
 2
 )(
 p
 0
  − p
 1
 )
  
 a
  = 
   
 f
 (
 p
 0
 )
  − f
 (
 p
 2
 ) 
  
  
 f
 (
 p
 1
 )
  − f
 (
 p
 2
 ) 
  
 (
 p
 0
  
 − p
 2
 )(
 p
 0
  − p
 1
 )
 −
 (
 p
 1
  − p
 2
 )(
 p
 0
  − p
 1
 )
 .
  
 (2.14)
  
 Now that we have determined
  P
 (
 x
 ), the next step is to solve
  P
 (
 x
 ) = 0, and set the next iterate
  
 p
 3
  to its solution. To this end, put
  w
  =
  x − p
 2
  in (
 2.13
 ) to rewrite the quadratic equation as 
  
  
  
 aw
 2
 +
  bw
  +
  c
  = 0
 .
  
 From the quadratic formula, we obtain the roots
  
 ˆ
 w
  = ˆ
 x − p
 2
  =
  
 b ±
  
 √−
 2
 c
  
 b
 2
 −
  4
 ac.
  
 (2.15)",NA
Julia code for Muller’s method,"The following Julia code takes initial guesses
  p
 0
 , p
 1
 , p
 2
  (written as
  pzero, pone, ptwo
  in the 
 code), computes the coefficients
  a, b, c
  from Equation (
 2.14
 ), and sets the root
  p
 3
  to
  p
 . It 
 then updates the three initial guesses as the last three iterates, and continues until the 
 stopping criterion is satisfied.
  
 We need to compute the square root, and the absolute value, of possibly complex 
 numbers in Equations (
 2.15
 ) and (
 2.16
 ). The Julia function for the square root of a possibly 
 complex number
  z
  is
  Complex(z)
 0
 .
 5
 , and its absolute value is
  abs(z)
 .
  
 In [1]:
  function
  muller(f
 ::
 Function
 ,pzero,pone,ptwo,eps,N) 
  
 n
 =1",NA
2.6 ,NA,NA
Fixed-point iteration ,"Many root-finding methods are based on the so-called fixed-point iteration; a method we 
 discuss in this section.
  
 Definition 37.
  A number
  p
  is a fixed-point for a function
  g
 (
 x
 ) if
  g
 (
 p
 ) =
  p.
  
 We have two problems that are related to each other:•
  
 Fixed-point problem
 : Find
  p
  such that
  g
 (
 p
 ) =
  p.
  
 •
  Root-finding problem
 : Find
  p
  such that
  f
 (
 p
 ) = 0
 .
  
 We can formulate a root-finding problem as a fixed-point problem, and vice versa. For 
 example, assume we want to solve the root finding problem,
  f
 (
 p
 ) = 0
 .
  Define
  g
 (
 x
 ) =
  x−f
 (
 x
 )
 , 
 and observe that if
  p
  is a fixed-point of
  g
 (
 x
 ), that is,
  g
 (
 p
 ) =
  p − f
 (
 p
 ) =
  p,
  then
  p
  is a root of
  f
 (
 x
 ). 
 Here the function
  g
  is not unique: there are many ways one can represent the root-finding 
 problem
  f
 (
 p
 ) = 0 as a fixed-point problem, and as we will learn later, not all will be useful to 
 us in developing fixed-point iteration algorithms.
  
 The next theorem answers the following questions: When does a function
  g
  have a fixed-
 point? If it has a fixed-point, is it unique?
  
 Theorem 38. 
  
 1. If g is a continuous function on
  [
 a, b
 ]
  and g
 (
 x
 )
  ⊂
  [
 a, b
 ]
  for all x ⊂
  [
 a, b
 ]
 , 
 then g has at least one fixed-point in
  [
 a, b
 ]
 .",NA
Julia code for fixed-point iteration,"The following code starts with the initial guess
  p
 0
  (
 pzero
  in the code), computes
  p
 1
  =
  g
 (
 p
 0
 ), 
 and checks if the stopping criterion
  |p
 1
  − p
 0
 | < ϵ
  is satisfied. 
  
 If it is satisfied the code 
 terminates with the value
  p
 1
 . Otherwise
  p
 1
  is set to
  p
 0
 , and the next iteration is computed.",NA
2.7 ,NA,NA
High-order fixed-point iteration,"In the proof of Theorem
  44
 , we showed
  
 lim 
  
 n→∞
  
 |p
 n
 +1
  − p| 
 |p
 n
  − p|
  
 =
  |g
 ′
 (
 p
 )
 |
  
 which implied that the fixed-point iteration has linear convergence, if
  g
 ′
 (
 p
 )
  ̸
 = 0.
  
 If this limit were zero, then we would have
  
 lim 
  
 n→∞
  
 |p
 n
 +1
  − p| 
 |p
 n
  − p|
  
 = 0
 ,
  
 which means the denominator is growing at a larger rate than the numerator. We could 
 then
  
 ask if
  
 lim 
  
 n→∞
  
 |p
 n
 +1
  − p| |p
 n
  − p|
 α
  = nonzero constant
  
 for some
  α >
  1
 .",NA
Application to Newton’s Method,"Recall Newton’s iteration
  
 p
 n
  =
  p
 n−
 1
  −f
 (
 p
 n−
 1
 ) 
 f
 ′
 (
 p
 n−
 1
 )
 .
  
 Put
  g
 (
 x
 ) =
  x −
 f
 (
 x
 ) 
 f
 ′
 (
 x
 )
 .
  Then the fixed-point iteration
  p
 n
  =
  g
 (
 p
 n−
 1
 ) is Newton’s method. We
  
 have
  
 g
 ′
 (
 x
 ) = 1
  −
 [
 f
  ′
 (
 x
 )]
 2
  − f
 (
 x
 )
 f
  ′′
 (
 x
 ) =
 f
 (
 x
 )
 f
  ′′
 (
 x
 )
  
 [
 f
 ′
 (
 x
 )]
 2
  
 and thus
  
 g
 ′
 (
 p
 ) =
 f
 (
 p
 )
 f
  ′′
 (
 p
 ) 
  
 [
 f
 ′
 (
 p
 )]
 2
  
 = 0
 .",NA
Chapter 3,NA,NA
Interpolation,"In this chapter, we will study the following problem: given data (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
 , find a 
 function
  f
  such that
  f
 (
 x
 i
 ) =
  y
 i
 .
  This problem is called the interpolation problem, and
  f
  is 
 called the interpolating function, or interpolant, for the given data.
  
 Interpolation is used, for example, when we use mathematical software to plot a smooth 
 curve through discrete data points, when we want to find the in-between values in a table, 
 or when we differentiate or integrate black-box type functions.
  
 How do we choose
  f
 ? Or, what kind of function do we want
  f
  to be? There are several 
 options. Examples of functions used in interpolation are polynomials, piecewise 
 polynomials, rational functions, trigonometric functions, and exponential functions. As we 
 try to find a good choice for
  f
  for our data, some questions to consider are whether we want
  
 f
  to inherit the properties of the data (for example, if the data is periodic, should we use a 
 trigonometric function as
  f
 ?), and how we want
  f
  behave between data points. In general
  f
  
 should be easy to evaluate, and easy to integrate & differentiate.
  
 Here is a general framework for the interpolation problem. We are given data, and we pick 
 a family of functions from which the interpolant
  f
  will be chosen:
  
 • Data: (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
  
 • Family: Polynomials, trigonometric functions, etc.
  
 Suppose the family of functions selected forms a vector space. Pick a basis for the vector 
 space:
  φ
 0
 (
 x
 )
 , φ
 1
 (
 x
 )
 , ..., φ
 n
 (
 x
 )
 .
  Then the interpolating function can be written as a linear 
 combination of the basis vectors (functions):
  
 n
  
 f
 (
 x
 ) = 
 a
 k
 φ
 k
 (
 x
 )
 .
  
 k
 =0
  
 86",NA
3.1,NA,NA
Polynomial interpolation,"In polynomial interpolation, we pick polynomials as the family of functions in the interpo-
 lation problem.
  
 • Data: (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
  
 • Family: Polynomials 
  
 The space of polynomials up to degree
  n
  is a vector space. We will consider three choices 
 for the basis for this vector space:
  
 • Basis:
  
  
 –
  Monomial basis:
  φ
 k
 (
 x
 ) =
  x
 k
  
 –
  Lagrange basis:
  φ
 k
 (
 x
 ) =
 n j
 =0
 ,j̸
 =
 k
  
 x−x
 j 
  
 x
 k
 −x
 j
  
 –
  Newton basis:
  φ
 k
 (
 x
 ) =
 k−
 1 
 j
 =0
 (
 x − x
 j
 )
  
 where
  k
  = 0
 ,
  1
 , ..., n
 .
  
 Once we decide on the basis, the interpolating polynomial can be written as a linear combi-
  
 nation of the basis functions:
  
 p
 n
 (
 x
 ) =
  
 n
  
 a
 k
 φ
 k
 (
 x
 )
  
 k
 =0
  
 where
  p
 n
 (
 x
 i
 ) =
  y
 i
 , i
  = 0
 ,
  1
 , ..., n.",NA
Monomial form of polynomial interpolation,"Given data (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
 , we know from the previous theorem that there exists a 
 polynomial
  p
 n
 (
 x
 ) of degree at most
  n
 , that passes through the data points. To represent 
 p
 n
 (
 x
 ), we will use the monomial basis functions, 1
 , x, x
 2
 , ..., x
 n
 , or written more succinctly,
  
 φ
 k
 (
 x
 ) =
  x
 k
 , k
  = 0
 ,
  1
 , ..., n.
  
 The interpolating polynomial
  p
 n
 (
 x
 ) can be written as a linear combination of these basis 
 functions as 
  
  
 p
 n
 (
 x
 ) =
  a
 0
  +
  a
 1
 x
  +
  a
 2
 x
 2
 +
  ...
  +
  a
 n
 x
 n
 .
  
 We will determine
  a
 i
  using the fact that
  p
 n
  is an interpolant for the data:
  
 p
 n
 (
 x
 i
 ) =
  a
 0
  +
  a
 1
 x
 i
  +
  a
 2
 x
 2 
 i
 +
  ...
  +
  a
 n
 x
 n i
 =
  y
 i
  
 for
  i
  = 0
 ,
  1
 , ..., n.
  Or, in matrix form, we want to solve
  
 1
  
 1
  
 1 ...
  
 x
 0
  
 x
 2 0
  
 ...
  
 x
 n 
 0
  
 a
 0
  
 a
 1
  
 =
  
 y
 0
  
 y
 1
  
  
 x
 1
  
 x
 2 1
  
 x
 n 
 1
  
 x
 n
  
 x
 2 
 n
  
 x
 n n
  
  
  
 A
   
  
  
 a
  
  
  
 y",NA
Lagrange form of polynomial interpolation,"The ill-conditioning of the van der Monde matrix, as well as the high complexity of solving 
 the resulting matrix equation in the monomial form of polynomial interpolation, motivate 
 us to explore other basis functions for polynomials. As before, we start with data (
 x
 i
 , y
 i
 )
 , i
  = 
 0
 ,
  1
 , ..., n
 , and call our interpolating polynomial of degree at most
  n
 ,
  p
 n
 (
 x
 ). The Lagrange
  
 1
 The formal definition of the big O notation is as follows: We write
  f
 (
 n
 ) =
  O
 (
 g
 (
 n
 )) as
  n → ∞
  if and only if 
 there exists a positive constant
  M
  and a positive integer
  n
 ⊂
 such that
  |f
 (
 n
 )
 | ≤ Mg
 (
 n
 ) for all
  n ≥ n
 ⊂
 .",NA
Newton’s form of polynomial interpolation,"The Newton basis functions up to degree
  n
  are
  
 k−
 1
  
 π
 k
 (
 x
 ) = 
  
 (
 x − x
 j
 )
 , k
  = 0
 ,
  1
 , ..., n
  
 j
 =0
  
 where
  π
 0
 (
 x
 ) =
 −
 1 
 j
 =0
 (
 x − x
 j
 ) is interpreted as 1. The interpolating polynomial
  p
 n
 , written as
  
 a linear combination of Newton basis functions, is
  
 p
 n
 (
 x
 ) =
  a
 0
 π
 0
 (
 x
 ) +
  a
 1
 π
 1
 (
 x
 ) +
  ...
  +
  a
 n
 π
 n
 (
 x
 )
  
 =
  a
 0
  +
  a
 1
 (
 x − x
 0
 ) +
  a
 2
 (
 x − x
 0
 )(
 x − x
 1
 ) +
  ...
  +
  a
 n
 (
 x − x
 0
 )
  · · ·
  (
 x − x
 n−
 1
 )
 .
  
 We will determine
  a
 i
  from
  
 p
 n
 (
 x
 i
 ) =
  a
 0
  +
  a
 1
 (
 x
 i
  − x
 0
 ) +
  ...
  +
  a
 n
 (
 x
 i
  − x
 0
 )
  · · ·
  (
 x
 i
  − x
 n−
 1
 ) =
  y
 i
 ,",NA
Julia code for Newton interpolation,"Consider the following finite difference table.
  
 x
  
 f
 (
 x
 )
  
 f
 [
 x
 i
 , x
 i
 +1
 ]
  
 f
 [
 x
 i−
 1
 , x
 i
 , x
 i
 +1
 ]
  
 x
 0
  
 y
 0
  
 y
 1
 −y
 0 
  
 f
 [
 x, 
  
  
 x
 1
  
 y
 1
  
 x
 1
 −x
 0
 f
 [
 x
 0
 , 
 1
  
 f
 [
 x
 1
 ,x
 2
 ]
 −f
 [
 x
 0
 ,x
 1
 ] 
 x
 2
 −x
 0
  
 =
  f
 [
 x
 0
 , x
 1
 , x
 2
 ]
  
 y
 2
 −y
 1
  
 x
 2
  
 y
 2
  
 x
 2
 −x
 1
 =
  f
 [
 x
 1
 , x
 2
 ]
  
  
  
 Table 3.1: Divided differences for three data points
  
 There are 2 + 1 = 3 divided differences in the table, not counting the 0th divided differ-
 ences. In general, the number of divided differences to compute is 1 +
  ...
  +
  n
  =
  n
 (
 n
  + 1)
 /
 2. 
 However, to construct Newton’s form of the interpolating polynomial, we need only
  n
  di-
 vided differences and the 0th divided difference
  y
 0
 . These numbers are displayed in red in 
 Table
  3.1
 . The important observation is, even though all the divided differences have to be 
 computed in order to get the ones needed for Newton’s form, they do not have to be all 
 stored. The following Julia code is based on an efficient algorithm that goes through the 
 divided difference calculations recursively, and stores an array of size
  m
  =
  n
 +1 at any given 
 time. In the final iteration, this array has the divided differences needed for Newton’s form.
  
 Let’s explain the idea of the algorithm using the simple example of Table
  3.1
 . The code 
 creates an array
  a
  = (
 a
 0
 , a
 1
 , a
 2
 ) of size
  m
  =
  n
  + 1, which is three in our example, and sets
  
 a
 0
  =
  y
 0
 , a
 1
  =
  y
 1
 , a
 2
  =
  y
 2
 .
  
 (The code actually numbers the components of the array starting at 1, not 0. We keep it 0 
 here so that the discussion uses the familiar divided difference formulas.)",NA
3.2,NA,NA
High degree polynomial interpolation,"Suppose we approximate
  f
 (
 x
 ) using its polynomial interpolant
  p
 n
 (
 x
 ) obtained from (
 n
  + 1) 
 data points. We then increase the number of data points, and update
  p
 n
 (
 x
 ) accordingly. The 
 central question we want to discuss is the following: as the number of nodes (data points) 
 increases, does
  p
 n
 (
 x
 ) become a better approximation to
  f
 (
 x
 ) on [
 a, b
 ]? We will investigate 
 this question numerically, using a famous example: Runge’s function, given by
  f
 (
 x
 ) = 
 1+
 x
 2
 .
  
 We will interpolate Runge’s function using polynomials of various degrees, and plot the 
 function, together with its interpolating polynomial and the data points. We are interested",NA
Divided differences and derivatives,NA,NA
3.3,NA,NA
Hermite interpolation,"In polynomial interpolation, our starting point has been the
  x
  and
  y
 -coordinates of some 
 data we want to interpolate. Suppose, in addition, we know the derivative of the underlying 
 function at these
  x
 -coordinates. Our new data set has the following form.
  
 Data:
  
 x
 0
 , x
 1
 , ..., x
 n
  
 y
 0
 , y
 1
 , ..., y
 n
 ;
  y
 i
  =
  f
 (
 x
 i
 )
  
 y
 ′
 0
 , y
 ′
 1
 , ..., y
 ′n
 ;
  y
 ′i
 =
  f
  ′
 (
 x
 i
 )",NA
Computing the Hermite polynomial,"We do not use Theorem
  61
  to compute the Hermite polynomial: there is a more efficient
  
 method using divided differences for this computation.
  
 We start with the data:
  
 x
 0
 , x
 1
 , ..., x
 n
  
 y
 0
 , y
 1
 , ..., y
 n
 ;
  y
 i
  =
  f
 (
 x
 i
 )
  
 y
 ′
 0
 , y
 ′
 1
 , ..., y
 ′n
 ;
  y
 ′i
 =
  f
  ′
 (
 x
 i
 )
  
 and define a sequence
  z
 0
 , z
 1
 , ..., z
 2
 n
 +1
  by
  
 z
 0
  =
  x
 0
 , z
 2
  =
  x
 1
 , z
 4
  =
  x
 2
 , ..., z
 2
 n
  =
  x
 n
  
 z
 1
  =
  x
 0
 , z
 3
  =
  x
 1
 , z
 5
  =
  x
 2
 , ..., z
 2
 n
 +1
  =
  x
 n
  
 i.e.,
  z
 2
 i
  =
  z
 2
 i
 +1
  =
  x
 i
 ,
  for
  i
  = 0
 ,
  1
 , ..., n.",NA
Julia code for computing Hermite interpolating polynomial,"In [1]:
  using
  PyPlot
  
 The following function
  hdiff
  computes the divided differences needed for Hermite in-
 terpolation. It is based on the function
  diff
  for computing divided differences for Newton 
 interpolation. The inputs to
  hdiff
  are the
  x
 -coordinates, the
  y
 -coordinates, and the deriva-
 tives
  yprime
 .
  
 In [2]:
  function
  hdiff(x
 ::
 Array
 ,y
 ::
 Array
 ,yprime
 ::
 Array
 ) 
  
  
 m
 =
 length(x)
  # here m is the number of data points. Note n=m-1 
  
 #and 
 2n+1=2m-1 
  
  
 l
 =2
 m 
  
  
 z
 =
 Array
 {
 Float64
 }(undef,l) 
  
  
 a
 =
 Array
 {
 Float64
 }(undef,l) 
  
  
 for
  i
  in
  1:
 m 
  
  
  
 z[
 2
 i
 -1
 ]
 =
 x[i] 
  
  
  
 z[
 2
 i]
 =
 x[i] 
  
  
 end",NA
3.4,NA,NA
Piecewise polynomials: spline interpolation,"As we observed in Section
  3.2
 , a polynomial interpolant of high degree can have large oscil-
 lations, and thus provide an overall poor approximation to the underlying function. Recall 
 that the degree of the interpolating polynomial is directly linked to the number of data 
 points: we do not have the freedom to choose the degree of the polynomial.
  
 In spline interpolation, we take a very different approach: instead of finding a single 
 polynomial that fits the given data, we find one low-degree polynomial that fits every
  pair",NA
Cubic spline interpolation ,"This is the most common spline interpolation. It uses cubic polynomials to connect the 
 nodes. Consider the data 
  
  
 (
 x
 0
 , y
 0
 )
 ,
  (
 x
 1
 , y
 1
 )
 , ...,
  (
 x
 n
 , y
 n
 )
 ,
  
 where
  x
 0
  < x
 1
  < ... < x
 n
 . In the figure below, the cubic polynomials interpolating pairs of data 
 are labeled as
  S
 0
 , ..., S
 n−
 1
  (we ignore the
  y
 -coordinates in the plot).",NA
Julia code for spline interpolation,"In [1]:
  using
  PyPlot
  
 The function
  CubicNatural
  takes the
  x
  and
  y
 -coordinates of the data as input, and 
 computes the natural cubic spline interpolating the data, by solving the resulting matrix 
 equation. The code is based on Algorithm 3.4 of Burden, Faires, Burden [
 4
 ]. The output is 
 the coefficients of the
  m −
  1 cubic polynomials,
  a
 i
 , b
 i
 , c
 i
 , d
 i
 , i
  = 1
 , ..., m −
  1 where
  m
  is the 
 number of data points. These coefficients are stored in the arrays
  a, b, c, d
 , which are 
 declared global, so that we can access these arrays later to evaluate the spline for a given 
 value
  w
 .
  
 In [2]:
  function
  CubicNatural(x
 ::
 Array
 ,y
 ::
 Array
 ) 
  
  
 m
 =
 length(x)
  # m is the number of data points 
  
  
 n
 =
 m
 -1 
  
  
 global
  a
 =
 Array
 {
 Float64
 }(undef,m) 
  
  
 global
  b
 =
 Array
 {
 Float64
 }(undef,n) 
  
  
 global
  c
 =
 Array
 {
 Float64
 }(undef,m) 
  
  
 global
  d
 =
 Array
 {
 Float64
 }(undef,n) 
  
  
 for
  i
  in
  1:
 m 
  
  
  
 a[i]
 =
 y[i] 
  
  
 end 
  
  
 h
 =
 Array
 {
 Float64
 }(undef,n) 
  
  
 for
  i
  in
  1:
 n 
  
  
  
 h[i]
 =
 x[i
 +1
 ]
 -
 x[i] 
  
  
 end 
  
  
 u
 =
 Array
 {
 Float64
 }(undef,n) 
  
  
 u[
 1
 ]
 =0 
  
  
 for
  i
  in
  2:
 n 
  
  
  
 u[i]
 =3*
 (a[i
 +1
 ]
 -
 a[i])
 /
 h[i]
 -3*
 (a[i]
 -
 a[i
 -1
 ])
 /
 h[i
 -1
 ] 
  
 end 
  
  
 s
 =
 Array
 {
 Float64
 }(undef,m) 
  
  
 z
 =
 Array
 {
 Float64
 }(undef,m) 
  
  
 t
 =
 Array
 {
 Float64
 }(undef,n) 
  
  
 s[
 1
 ]
 =1 
  
  
 z[
 1
 ]
 =0 
  
  
 t[
 1
 ]
 =0 
  
  
 for
  i
  in
  2:
 n",NA
Chapter 4,NA,NA
Numerical Quadrature and ,NA,NA
Differentiation,"Estimating
  
  b 
  
 a
 f
 (
 x
 )
 dx
  using sums of the form
 n i
 =0
 w
 i
 f
 (
 x
 i
 ) is known as the quadrature
  
 problem. Here
  w
 i
  are called weights, and
  x
 i
  are called nodes. The objective is to determine 
 the nodes and weights to minimize error.",NA
4.1 ,NA,NA
Newton-Cotes formulas,"b 
  
 The idea is to construct the polynomial interpolant
  P
 (
 x
 ) and compute 
  
 a
 P
 (
 x
 )
 dx
  as an
  
  
  b 
  
 approximation to 
  
 a
 f
 (
 x
 )
 dx
 . Given nodes
  x
 0
 , x
 1
 , ..., x
 n
 , the Lagrange form of the interpolant is 
  
  
  
  
 n 
  
  
  
  
 P
 n
 (
 x
 ) = 
    
 f
 (
 x
 i
 )
 l
 i
 (
 x
 ) 
  
  
  
  
  
 i
 =0 
  
 and from the interpolation error formula Theorem
  51
 , we have
  
 f
 (
 x
 ) =
  P
 n
 (
 x
 ) + (
 x − x
 0
 )
  · · ·
  (
 x − x
 n
 )
 f
  (
 n
 +1)
 (
 ξ
 (
 x
 )) 
 ,
  
 where
  ξ
 (
 x
 )
  ⊂
  [
 a, b
 ]. (We have written
  ξ
 (
 x
 ) instead of
  ξ
  to emphasize that
  ξ
  depends on the 
 value of
  x
 .) 
  
 Taking the integral of both sides yields
  
  b
  
 f
 (
 x
 )
 dx
  =
  
  b
  
 +
  
 1
  
  b
  
 n
  
 (
 x x
 )
 f
 (
 n
 +1)
 (
 ξ
 (
 x
 ))
 dx.
  
 (4.1)
  
 P
 (
 x
 )
 dx
  
 a
  
  
 n
  
  
 (
 n
  + 1)!
  
 a
  
 i
 =0
  
  −
 i
  
  
 a
  
  
  
  
  
  
  
 error term
  
  
 quadrature rule
  
 137",NA
4.2,NA,NA
Composite Newton-Cotes formulas,"If the interval [
 a, b
 ] in the quadrature is large, then the Newton-Cotes formulas will give 
 poor approximations. The quadrature error depends on
  h
  = (
 b−a
 )
 /n
  (closed formulas), and 
 if
  b−a 
 is large, then so is
  h,
  hence error. If we raise
  n
  to compensate for large interval, then 
 we face a problem discussed earlier: error due to the oscillatory behavior of high-degree 
 interpolating polynomials that use equally-spaced nodes. A solution is to break up the 
 domain into smaller intervals and use a Newton-Cotes rule with a smaller
  n
  on each 
 subinterval: this is known as a composite rule.
  
 Example 74.
  Let’s compute
  
  2 
  
 0
 e
 x
  sin
  xdx.
  The antiderivative can be computed using inte-
  
 gration by parts, and the true value of the integral to 6 digits is 5.39689. If we apply the 
 Simpson’s rule we get:
  
  2 
  
 0
  
 e
 x
 sin
  xdx ≈
 1 3(
 e
 0
  sin 0 + 4
 e
  sin 1 +
  e
 2
  sin 2) = 5
 .
 28942
 .
  
 If we partition the integration domain (0
 ,
  2) into (0
 ,
  1) and (1
 ,
  2), and apply Simpson’s rule",NA
Julia codes for Newton-Cotes formulas,"We write codes for the trapezoidal and Simpson’s rules, and the composite Simpson’s rule. 
 Coding trapezoidal and Simpson’s rule is straightforward.
  
 Trapezoidal rule
  
 In [1]:
  function
  trap(f
 ::
 Function
 ,a,b) 
  
  
  
 (f(a)
 +
 f(b))
 *
 (b
 -
 a)
 /2 
  
  
 end",NA
Composite rules and roundoff error,"As we increase
  n
  in the composite rules to lower error, the number of function evaluations 
 increases, and a natural question to ask would be whether roundoff error could accumulate 
 and cause problems. Somewhat remarkably, the answer is no. Let’s assume the roundoff 
 error associated with computing
  f
 (
 x
 ) is bounded for all
  x,
  by some positive constant
  ϵ
 . And 
 let’s try to compute the roundoff error in composite Simpson rule. Since each function 
 evaluation in the composite rule incorporates an error of (at most)
  ϵ
 , the total error is 
 bounded by
  
 h
  
 ϵ
 +2
  
 n 
  
 2
 −
 1
  
 ϵ
  
  
 n/
 2
  
  +
  
  
 h
  
  
  +2
  
  
 n
  
 1
  
  
 ϵ
   
  
 n
  
 ϵ
 +
  
  
 = 
 h 
 (
 nϵ
 )=
 hnϵ
  
 3
  
 ϵ
  + 2
  
 j
 =1
  
 ϵ
  
  
 j
 =1
  
  +
  
  ≤
  
  
  +2
  
 2
 −
 1
  
 ϵ
   
  
 2
  
 ϵ
 +
  
  
 = 3(
 nϵ
 )=
 hnϵ
  
 However, since
  h
  = (
 b − a
 )
 /n
 , the bound simplifies as
  hnϵ
  = (
 b − a
 )
 ϵ.
  Therefore no matter 
 how large
  n
  is, that is, how large the number of function evaluations is, the roundoff error is
  
 bounded by the same constant (
 b − a
 )
 ϵ
  which only depends on the size of the interval.
  
 Exercise 4.2-4:
  
 (This problem shows that numerical quadrature is stable with 
 respect
  
 to error in function values.) Assume the function values
  f
 (
 x
 i
 ) are approximated by˜
 f
 (
 x
 i
 ), so 
 that
  |f
 (
 x
 i
 )
  −
 ˜
 f
 (
 x
 i
 )
 | < ϵ
  for any
  x
 i
  ⊂
  (
 a, b
 )
 .
  Find an upper bound on the error of numerical 
 quadrature
 w
 i
 f
 (
 x
 i
 ) when it is actually computed as
 w
 i
 ˜
 f
 (
 x
 i
 )
 .",NA
4.3,NA,NA
Gaussian quadrature,"Newton-Cotes formulas were obtained by integrating interpolating polynomials with 
 equally-spaced nodes. The equal spacing is convenient in deriving simple expressions for 
 the com-posite rules. However, this placement of nodes is not necessarily the optimal 
 placement. For example, the trapezoidal rule approximates the integral by integrating a 
 linear function that joins the endpoints of the function. The fact that this is not the optimal 
 choice can be seen by sketching a simple parabola.
  
 The idea of Gaussian quadrature is the following: in the numerical quadrature rule
  
  b
  
 f
 (
 x
 )
 dx ≈
  
 n
  
 w
 i
 f
 (
 x
 i
 )
  
 i
 =1
  
 a",NA
Julia code for Gauss-Legendre rule with five nodes,"The following code computes the Gauss-Legendre rule for 
 The nodes and weights are from Table
  4.1
 .
  
 In [1]:
  function
  gauss(f
 ::
 Function
 ) 
  
  
  
 0.2369268851*
 f(
 -0.9061798459
 )
 + 
  
  
 0.2369268851*
 f(
 0.9061798459
 )
 + 
  
  
 0.5688888889*
 f(
 0
 )
 + 
  
  
  
 0.4786286705*
 f(
 0.5384693101
 )
 + 
  
  
 0.4786286705*
 f(
 -0.5384693101
 ) 
  
 end
  
 Out[1]:
  gauss (generic function with 1 method)
  
 Now we compute
 1 4
  
  1
  
  t
 +3
  
 t
 +3 
  
 4
  dt
  using the code:
  
 −
 1
  
 4
  
 In [2]:
  0.25*
 gauss(t
 ->
 (t
 /4+3/4
 )
 ^
 (t
 /4+3/4
 ))
  
 Out[2]:
  0.41081564812239885
  
  1
  
 −
 1
 f
 (
 x
 )
 dx
  using
  n
  = 5 nodes.",NA
4.4 ,NA,NA
Multiple integrals,"The numerical quadrature methods we have discussed can be generalized to higher dimen-
 sional integrals. We will consider the two-dimensional integral
  
  
 f
 (
 x, y
 )
 dA. 
  
 R
  
 The domain
  R
  determines the difficulty in generalizing the one-dimensional formulas we 
 learned before. The simplest case would be a rectangular domain
  R
  =
  {
 (
 x, y
 )
 |a ≤ x ≤ b, c ≤y ≤ 
 d}
 . We can then write the double integral as the iterated integral
  
 R
  
 f
 (
 x, y
 )
 dA
  =
  
  b
  
  d
  
 f
 (
 x, y
 )
 dy
  
 dx.
  
 a
  
 c
  
 Consider a numerical quadrature rule
  
  b
  
 f
 (
 x
 )
 dx ≈
  
 n
  
 w
 i
 f
 (
 x
 i
 )
 .
  
 i
 =1
  
 a
  
 Apply the rule using
  n
 2
  nodes to the inner integral to get the approximation
  
  b
  
  n
 2
  
 w
 j
 f
 (
 x, y
 j
 )
  
 dx
  
 j
 =1
  
 a
  
 where the
  y
 j
 ’s are the nodes. Rewrite, by interchanging the integral and summation, to get
  
 n
 2
  
 w
 j
  
  b
  
 f
 (
 x, y
 j
 )
 dx
  
 j
 =1
  
 a
  
 and apply the quadrature rule again, using
  n
 1
  nodes, to get the approximation
  
 n
 2
  
 w
 j
  
  n
 1
  
 w
 i
 f
 (
 x
 i
 , y
 j
 )
  
 .
  
 j
 =1
  
 i
 =1
  
 This gives the two-dimensional rule
  
  b
  
  d
  
 f
 (
 x, y
 )
 dy
  
 dx ≈
  
 n
 2
  
 n
 1
  
 w
 i
 w
 j
 f
 (
 x
 i
 , y
 j
 )
 .
  
 j
 =1
  
 i
 =1
  
 a
  
 c",NA
4.5 ,NA,NA
Improper integrals,"The quadrature rules we have learned so far cannot be applied (or applied with a poor 
 perfor-
  
  b 
  
 mance) to integrals such as 
  
 a
 f
 (
 x
 )
 dx
  if
  a, b
  =
  ±∞
  or if
  a, b
  are finite but
  f
  is not continuous at 
 one or both of the endpoints: recall that both Newton-Cotes and Gauss-Legendre error 
 bound theorems require the integrand to have a number of continuous derivatives on the 
 closed interval [
 a, b
 ]. For example, an integral in the form
  
  1
  
  
 f
 (
 x
 )
  
 dx
  
  
 −
 1
  
 √
 1
  − x
 2
 dx
  
 clearly cannot be approximated using the trapezoidal or Simpson’s rule without any 
 modifi-cations, since both rules require the values of the integrand at the end points which 
 do not exist. One could try using the Gauss-Legendre rule, but the fact that the integrand 
 does not satisfy the smoothness conditions required by the Gauss-Legendre error bound 
 means the error of the approximation might be large.
  
 A simple remedy to the problem of improper integrals is to change the variable of inte-
 gration and transform the integral, if possible, to one that behaves well.
  
 Example 82.
  Consider the previous integral 
 cos
 −
 1
 x.
  Then
  dθ
  =
  −dx/√
 1
  − x
 2
 and
  
  1
   
 f
 (
 x
 )
  
 √
 1
 −x
 2
 dx.
  Try the transformation
  θ
  =
  
 −
 1
  
  1
  
 f
 (
 x
 )
  
  0
  
 f
 (cos
  θ
 )
 dθ
  =
  
  π
   
 −
 1
  
 √
  
 1
  − x
 2
 dx
  =
  −
  
  
 π
  
 0
  
 f
 (cos
  θ
 )
 dθ.
  
 The latter integral can be evaluated using, for example, Simpson’s rule, provided
  f
  is smooth 
 on [0
 , π
 ]
 .
  
 If the interval of integration is infinite, another approach that might work is truncation 
 of the interval to a finite one. The success of this approach depends on whether we can 
 estimate the resulting error.
  
 Example 83.
  Consider the improper integral
  
  ∞
 0
 e
 −x
 2
 dx
 . Write the integral as
  
  ∞
  
 e
 −x
 2
 dx
  =
  
  t
  
 e
 −x
 2
 dx
  +
  
  ∞
  
 e
 −x
 2
 dx,
  
 0
  
 0
  
 t
  
 where
  t
  is the ""level of truncation"" to determine. We can estimate the first integral on the 
 right-hand side using a quadrature rule. The second integral on the right-hand side is the",NA
4.6 ,NA,NA
Numerical differentiation,"The derivative of
  f
  at
  x
 0
  is
  
 f
 ′
 (
 x
 0
 ) = lim 
  
 h→
 0
  
 f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 ) 
 h
  
 .
  
 This formula gives an obvious way to estimate the derivative by
  
 f
 ′
 (
 x
 0
 )
  ≈f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 )
  
 for small
  h
 . What this formula lacks, however, is it does not give any information about the 
 error of the approximation.
  
 We will try another approach. Similar to Newton-Cotes quadrature, we will construct 
 the interpolating polynomial for
  f
 , and then use the derivative of the polynomial as an 
 approximation for the derivative of
  f
 .
  
 Let’s assume
  f ⊂ C
 2
 (
 a, b
 )
 , x
 0
  ⊂
  (
 a, b
 )
 ,
  and
  x
 0
  +
  h ⊂
  (
 a, b
 )
 .
  Construct the linear Lagrange 
 interpolating polynomial
  p
 1
 (
 x
 ) for the data (
 x
 0
 , f
 (
 x
 0
 ))
 ,
  (
 x
 1
 , f
 (
 x
 1
 )) = (
 x
 0
  +
  h, f
 (
 x
 0
  +
  h
 )).
  
 From Theorem
  51
 , we have
  
 f
 (
 x
 ) =
 x − x
 1 
  
 x
 0
  
 − x
 1
  
 f
 (
 x
 0
 ) +
 x − x
 0 
 x
 1
  − 
 x
 0
  
 p
 1
 (
 x
 )
  
 f
 (
 x
 1
 )
  
  
 +
 f
  ′′
 (
 ξ
 (
 x
 )) 
 2!
  
 (
 x − x
 0
 )(
 x − x
 1
 )
  
 interpolation error
  
 =
 x −
  (
 x
 0
  +
  h
 )
  
 x
 0
  −
  (
 x
 0
  +
  h
 )
 f
 (
 x
 0
 ) + 
 x
 0
  +
  h − x
 0 
  
  
 x − x
 0 
  
 f
 (
 x
 0
  +
  h
 ) +
 f
  ′′
 (
 ξ
 (
 x
 ))
  
 2! (
 x − x
 0
 )(
 x − x
 0
  − h
 )
  
 =
 x − x
 0
  − h
  
 −h 
  
 f
 (
 x
 0
 ) +
 x − x
 0
  
 h",NA
Numerical differentiation and roundoff error,"Arya and the mysterious black box
  
 College life is full of mysteries, and Arya 
 faces one in an engineering class: a black 
 box! What is a black box? It is a computer 
 pro-gram, or some device, which produces 
 an out-put when an input is provided. We do 
 not know the inner workings of the system, 
 hence comes the name black box. Let’s think 
 of the black box as a function
  f
 , and 
 represent the input and output as
  x, f
 (
 x
 ). Of 
 course, we do not have a formula for
  f
 .
  
  
 What Arya’s engineering classmates want to do is compute the derivative information 
 of the black box, that is,
  f
 ′
 (
 x
 ), when
  x
  = 2
 .
  (The input to this black box can be any real 
 number.) Students want to use the three-point midpoint formula to estimate
  f
 ′
 (2):
  
 f
 ′
 (2)
  ≈
 1 2
 h
 [
 f
 (2 +
  h
 )
  − f
 (2
  − h
 )]
  .
  
 They argue how to pick
  h
  in this formula. One of them says they should make
  h
  as small as 
 possible, like 10
 −
 8
 . Arya is skeptical. She mutters to herself, ""I know I slept through some of 
 my numerical analysis lectures, but not all!""
  
  
 She tells her classmates about the cancellation of leading digits phenomenon, and to",NA
Chapter 5,NA,NA
Approximation Theory,NA,NA
5.1 ,NA,NA
Discrete least squares,"Arya’s adventures in the physics lab
  
 College life is expensive, and Arya is happy to land a job working at a physics lab for some 
 extra cash. She does some experiments, some data analysis, and a little grading. In one 
 experiment she conducted, where there is one independent variable
  x
 , and one dependent 
 variable
  y
 , she was asked to plot
  y
  against
  x
  values. (There are a total of six data points.) She 
 gets the following plot:
  
  
 Figure 5.1: Scatter plot of data
  
 Arya’s professor thinks the relationship between the variables should be linear, but we 
 do not see data falling on a perfect line because of measurement error. The professor is not
  
 173",NA
Julia code for least squares approximation,"In [1]:
  using
  PyPlot
  
 The function
  leastsqfit
  takes the
  x
  and
  y
 -coordinates of the data, and the degree of the 
 polynomial we want to use,
  n
 , as inputs. It solves the matrix Equation (
 5.2
 ).
  
 In [2]:
  function
  leastsqfit(x
 ::
 Array
 ,y
 ::
 Array
 ,n) 
  
  
 m
 =
 length(x)
  # number of data points 
  
  
 d
 =
 n
 +1
  # number of coefficients to determine 
  
 A
 =
 zeros(d,d) 
  
  
 b
 =
 zeros(d,
 1
 ) 
  
  
 # the linear system we want to solve is Ax=b 
  
 p
 =
 Array
 {
 Float64
 }(undef,
 2*
 n
 +1
 ) 
  
  
 for
  k
  in
  1:
 d 
  
  
  
 sum
 =0 
  
  
  
 for
  i
  in
  1:
 m 
  
  
  
  
 sum
 =
 sum
 +
 y[i]
 *
 x[i]
 ^
 (k
 -1
 ) 
  
  
  
 end 
  
  
  
 b[k]
 =
 sum",NA
Least squares with non-polynomials,"The method of least squares is not only for polynomials. For example, suppose we want to 
 find the function
  
 f
 (
 t
 ) =
  a
  +
  bt
  +
  c
  sin(2
 πt/
 365) +
  d
  cos(2
 πt/
 365) 
  
 (5.3)
  
 that has the best fit to some data (
 t
 1
 , T
 1
 )
 , ...,
  (
 t
 m
 , T
 m
 ) in the least-squares sense. This function 
 is used in modeling weather temperature data, where
  t
  denotes time, and
  T
  denotes the 
 temperature. The following figure plots the daily maximum temperature during a period of 
 1,056 days, from 2016 until November 21, 2018, as measured by a weather station at 
 Melbourne airport, Australia
 1
 .
  
 1
 http://www.bom.gov.au/climate/data/",NA
5.2,NA,NA
Continuous least squares,"In discrete least squares, our starting point was a set of data points. Here we will start with 
 a continuous function
  f
  on [
 a, b
 ] and answer the following question: how can we find the 
 ""best"" polynomial
  P
 n
 (
 x
 ) =
 n j
 =0
 a
 j
 x
 j
  of degree at most
  n,
  that approximates
  f
  on [
 a, b
 ]? As 
 before, ""best"" polynomial will mean the polynomial that minimizes the least squares error:
  
 E
  =
  
  b
  
 f
 (
 x
 )
  −
  
 n
  
 a
 j
 x
 j
  
 2
  
 dx.
  
 (5.10)
  
 j
 =0
  
 a
  
 2
 https://www.census.gov/topics/population/genealogy/data/2000_surnames.html",NA
5.3 ,NA,NA
Orthogonal polynomials and least squares,"Our discussion in this section will mostly center around the continuous least squares 
 problem, however, the discrete problem can be approached similarly. Consider the sets
  
 C
 0
 [
 a, b
 ], the set of all continuous functions defined on [
 a, b
 ]
 ,
  and
  P
 n
 ,
  the set of all 
 polynomials of degree at most
  n
  on [
 a, b
 ]
 .
  These two sets are vector spaces, the latter a 
 subspace of the former, under the usual operations of function addition and multiplying by 
 a scalar. An inner product on this space is defined as follows: given
  f, g ⊂ C
 0
 [
 a, b
 ]
  
 ⊂f, g⊂
  =
  
  b
  
 w
 (
 x
 )
 f
 (
 x
 )
 g
 (
 x
 )
 dx
  
 (5.12)
  
 a
  
 and the norm of a vector under this inner product is
  
 ⊂f⊂
  =
  ⊂f, f⊂
 1
 /
 2
 =
  
  b
  
 w
 (
 x
 )
 f
 2
 (
 x
 )
 dx
  
 1
 /
 2
  
 .
  
 a
  
 Let’s recall the definition of an inner product: it is a real valued function with the following 
 properties:
  
 1.
  ⊂f, g⊂
  =
  ⊂g, f⊂
  
 2.
  ⊂f, f⊂ ≥
  0
 ,
  with the equality only when
  f ≡
  0",NA
Julia code for orthogonal polynomials,"Computing Legendre polynomials
  
 Legendre polynomials satisfy the following recursion:
  
 L
 n
 +1
 (
 x
 ) = 2
 n
  + 1 
 n
  + 1
 xL
 n
 (
 x
 )
  −
  
 for
  n
  = 1
 ,
  2
 , . . .
  , with
  L
 0
 (
 x
 ) = 1, and
  L
 1
 (
 x
 ) =
  x
 .
  
 n 
  
 n
  + 1
 L
 n−
 1
 (
 x
 )
  
 The Julia code implements this recursion, with a little modification: the index
  n
  + 1 is
  
 shifted down to
  n
 , so the modified recursion is:
  L
 n
 (
 x
 ) =
 2
 n−
 1 
 n
 xL
 n−
 1
 (
 x
 )
  −
  n−
 1 
 n
 L
 n−
 2
 (
 x
 ), for
  
 n
  = 2
 ,
  3
 , . . .
  .
  
 In [1]:
  using
  PyPlot 
  
  
 using
  LaTeXStrings
  
 In [2]:
  function
  leg(x,n) 
  
  
  
 n
 ==0 &&
  return
  1 
  
  
  
 n
 ==1 &&
  return
  x 
  
  
  
 ((
 2*
 n
 -1
 )
 /
 n)
 *
 x
 *
 leg(x,n
 -1
 )
 -
 ((n
 -1
 )
 /
 n)
 *
 leg(x,n
 -2
 ) 
  
 end
  
 Out[2]:
  leg (generic function with 1 method)
  
 Here is a plot of the first five Legendre polynomials:
  
 In [3]:
  xaxis
 =-1:1/100:1 
  
 legzero
 =
 map(x
 ->
 leg(x,
 0
 ),xaxis) 
  
 legone
 =
 map(x
 ->
 leg(x,
 1
 ),xaxis) 
  
 legtwo
 =
 map(x
 ->
 leg(x,
 2
 ),xaxis) 
  
 legthree
 =
 map(x
 ->
 leg(x,
 3
 ),xaxis) 
  
 legfour
 =
 map(x
 ->
 leg(x,
 4
 ),xaxis) 
  
 plot(xaxis,legzero,label
 =
 L
 ""L_0(x)""
 ) 
 plot(xaxis,legone,label
 =
 L
 ""L_1(x)""
 ) 
 plot(xaxis,legtwo,label
 =
 L
 ""L_2(x)""
 ) 
 plot(xaxis,legthree,label
 =
 L
 ""L_3(x)""
 ) 
 plot(xaxis,legfour,label
 =
 L
 ""L_4(x)""
 ) 
 legend(loc
 =
 ""lower right""
 );",NA
References,"[1] Abramowitz, M., and Stegun, I.A., 1965. Handbook of mathematical functions: with 
 formulas, graphs, and mathematical tables (Vol. 55). Courier Corporation.
  
 [2] Chace, A.B., and Manning, H.P., 1927. The Rhind Mathematical Papyrus: British 
 Museum 10057 and 10058. Vol 1. Mathematical Association of America.
  
 [3] Atkinson, K.E., 1989. An Introduction to Numerical Analysis, Second Edition, John 
 Wiley & Sons.
  
 [4] Burden, R.L, Faires, D., and Burden, A.M., 2016. Numerical Analysis, 10th Edition, 
 Cengage.
  
 [5] Capstick, S., and Keister, B.D., 1996. Multidimensional quadrature algorithms at higher 
 degree and/or dimension. Journal of Computational Physics, 123(2), pp.267-273.
  
 [6] Chan, T.F., Golub, G.H., and LeVeque, R.J., 1983. Algorithms for computing the sample 
 variance: Analysis and recommendations. The American Statistician, 37(3), pp.242-247. [7] 
 Cheney, E.W., and Kincaid, D.R., 2012. Numerical mathematics and computing. Cen-gage 
 Learning.
  
 [8] Glasserman, P., 2013. Monte Carlo methods in Financial Engineering. Springer. [9] 
 Goldberg, D., 1991. What every computer scientist should know about floating-point 
 arithmetic. ACM Computing Surveys (CSUR), 23(1), pp.5-48.
  
 [10] Heath, M.T., 1997. Scientific Computing: An introductory survey. McGraw-Hill. [11] 
 Higham, N.J., 1993. The accuracy of floating point summation. SIAM Journal on Sci-entific 
 Computing, 14(4), pp.783-799.
  
 [12] Isaacson, E., and Keller, H.B., 1966. Analysis of Numerical Methods. John Wiley & Sons.
  
 211",NA
Index,"Absolute error,
  36
  
 Beasley-Springer-Moro,
  159 
  
 Biased exponent,
  29 
  
 Big O notation,
  88 
  
 Bisection method,
  53 
  
  
 error theorem,
  56 
  
  
 Julia code,
  54 
  
  
 linear convergence,
  56 
  
 Black-Scholes-Merton formula,
  63
  
 geometric interpretation,
  75 
 high-order,
  82 
  
 high-order error theorem,
  83 
 Julia code,
  77 
  
 Floating-point,
  28 
  
 decimal,
  35 
  
 IEEE 64-bit,
  28 
  
 infinity,
  30 
  
 NAN,
  30 
  
 normalized,
  28
  
 Chebyshev nodes,
  108 
  
 toy model,
  31
  
 Chebyshev polynomials,
  196 
  
 zero,
  30
  
  
 Julia code,
  204 
  
 Chopping,
  36 
  
 Composite Newton-Cotes,
  142 
  
 midpoint,
  143 
  
  
 roundoff,
  146 
  
  
 Simpson,
  143 
  
  
 trapezoidal,
  143
  
 Degree of accuracy,
  139 
  
 Divided differences,
  95 
  
  
 derivative formula,
  
 109 
 Dr. Seuss,
  132
  
 Extreme value theorem,
  7
  
 Gamma function,
  98 
  
 Gaussian quadrature,
  147 
  
 error theorem,
  153 
  
 Julia code,
  152 
  
 Legendre polynomials,
  148 
 Gram-Schmidt process,
  195
  
 Hermite interpolation,
  109 
 computation,
  112 
  
 Julia code,
  114
  
 Implied volatility,
  64 
  
 Improper integrals,
  162 
  
 Intermediate value theorem,
  7
  
 Fixed-point iteration,
  73
 ,
  75 
  
 Interpolation,
  85
  
 application to Newton’s method,
  83 
 error theorem,
  78
  
 Inverse interpolation,
  104 
 Iterative method,
  51
  
 212",NA
