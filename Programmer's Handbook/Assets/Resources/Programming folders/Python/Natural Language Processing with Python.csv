Larger Text,Smaller Text,Symbol
Natural Language Processing ,NA,NA
with Python,NA,NA
Natural Language ,NA,NA
Processi,NA,NA
ng with ,NA,NA
Python,NA,NA
"Steven Bird, Ewan Klein, and Edward ",NA,NA
Loper,NA,NA
Table of Contents,"Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . .  ix
  
 1. Language Processing and Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . . . . . 1
  
 1.1 Computing with Language: Texts and Words 
  
 1
  
 1.2 A Closer Look at Python: Texts as Lists of Words 
  
 10
  
 1.3 Computing with Language: Simple Statistics 
  
 16
  
 1.4 Back to Python: Making Decisions and Taking Control 
  
 22
  
 1.5 Automatic Natural Language Understanding 
  
 27
  
 1.6 Summary 
  
 33
  
 1.7 Further Reading 
  
 34
  
 1.8 Exercises 
  
 35
  
 2. Accessing Text Corpora and Lexical Resources . . . . . . . . . . . . . . . 
 . . . . . . . . . . . . . . . . 39
  
 2.1 Accessing Text Corpora 
  
 39
  
 2.2 Conditional Frequency Distributions 
  
 52
  
 2.3 More Python: Reusing Code 
  
 56
  
 2.4 Lexical Resources 
  
 59
  
 2.5 WordNet 
  
 67
  
 2.6 Summary 
  
 73
  
 2.7 Further Reading 
  
 73
  
 2.8 Exercises 
  
 74
  
 3. Processing Raw Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . . 79",NA
Preface,"This is a book about Natural Language Processing. By “natural language” we mean a 
 language that is used for everyday communication by humans; languages such as 
 Eng-lish, Hindi, or Portuguese. In contrast to artificial languages such as 
 programming lan-guages and mathematical notations, natural languages have 
 evolved as they pass from generation to generation, and are hard to pin down with 
 explicit rules. We will take Natural Language Processing—or NLP for short—in a 
 wide sense to cover any kind of computer manipulation of natural language. At one 
 extreme, it could be as simple as counting word frequencies to compare different 
 writing styles. At the other extreme, NLP involves “understanding” complete 
 human utterances, at least to the extent of being able to give useful responses to 
 them.
  
 Technologies based on NLP are becoming increasingly widespread. For example, 
 phones and handheld computers support predictive text and handwriting 
 recognition; web search engines give access to information locked up in 
 unstructured text; machine translation allows us to retrieve texts written in 
 Chinese and read them in Spanish. By providing more natural human-machine 
 interfaces, and more sophisticated access to stored information, language 
 processing has come to play a central role in the multi-lingual information society.
  
 This book provides a highly accessible introduction to the field of NLP. It can be 
 used for individual study or as the textbook for a course on natural language 
 processing or computational linguistics, or as a supplement to courses in artificial 
 intelligence, text mining, or corpus linguistics. The book is intensely practical, 
 containing hundreds of fully worked examples and graded exercises.
  
 The book is based on the Python programming language together with an open 
 source library called the 
 Natural Language Toolkit
  (NLTK). NLTK includes 
 extensive soft-ware, data, and documentation, all freely downloadable from 
 http://www.nltk.org/
 . Distributions are provided for Windows, Macintosh, and Unix 
 platforms. We strongly encourage you to download Python and NLTK, and try out 
 the examples and exercises along the way.
  
 ix",NA
Audience,"NLP is important for scientific, economic, social, and cultural reasons. NLP is 
 experi-encing rapid growth as its theories and methods are deployed in a variety of 
 new lan-guage technologies. For this reason it is important for a wide range of 
 people to have a working knowledge of NLP. Within industry, this includes people 
 in human-computer interaction, business information analysis, and web software 
 development. Within academia, it includes people in areas from humanities 
 computing and corpus linguistics through to computer science and artificial 
 intelligence. (To many people in academia, NLP is known by the name of 
 “Computational Linguistics.”)
  
 This book is intended for a diverse range of people who want to learn how to write 
 programs that analyze written language, regardless of previous programming 
 experience:
  
 New to programming?
  
 The early chapters of the book are suitable for readers with no prior 
 knowledge of programming, so long as you aren’t afraid to tackle new concepts 
 and develop new computing skills. The book is full of examples that you can 
 copy and try for your-self, together with hundreds of graded exercises. If you 
 need a more general intro-duction to Python, see the list of Python resources at 
 http://docs.python.org/
 .
  
 New to Python?
  
 Experienced programmers can quickly learn enough Python using this book to 
 get immersed in natural language processing. All relevant Python features are 
 carefully explained and exemplified, and you will quickly come to appreciate 
 Python’s suit-ability for this application area. The language index will help you 
 locate relevant discussions in the book.
  
 Already dreaming in Python?
  
 Skim the Python examples and dig into the interesting language analysis 
 material that starts in 
 Chapter 1
 . You’ll soon be applying your skills to this 
 fascinating domain.",NA
Emphasis,"This book is a 
 practical
  introduction to NLP. You will learn by example, write real 
 programs, and grasp the value of being able to test an idea through 
 implementation. If you haven’t learned already, this book will teach you 
 programming
 . Unlike other programming books, we provide extensive 
 illustrations and exercises from NLP. The approach we have taken is also 
 principled
 , in that we cover the theoretical underpin-nings and don’t shy away 
 from careful linguistic and computational analysis. We have tried to be 
 pragmatic
  
 in striking a balance between theory and application, identifying the connections 
 and the tensions. Finally, we recognize that you won’t get through this unless it is 
 also 
 pleasurable
 , so we have tried to include many applications and ex-amples that 
 are interesting and entertaining, and sometimes whimsical.",NA
What You Will Learn,"By digging into the material presented here, you will learn:
  
 • How simple programs can help you manipulate and analyze language data, and 
  
 how to write these programs
  
 • How key concepts from NLP and linguistics are used to describe and analyze 
  
 language
  
 • How data structures and algorithms are used in NLP
  
 • How language data is stored in standard formats, and how data can be used to 
  
 evaluate the performance of NLP techniques
  
 Depending on your background, and your motivation for being interested in NLP, 
 you will gain different kinds of skills and knowledge from this book, as set out in 
 Table P-1
 .
  
 Table P-1. Skills and knowledge to be gained from reading this book, depending on readers’ 
 goals and background
  
  
  
  
 Goals
  
 Background in arts and 
 humanities
  
  
 Background in science and 
 engineering
  
  
 Language
  
 Manipulating large corpora, 
 exploring linguistic
  
 Using techniques in data modeling, data 
 mining, and
  
 analysis
  
 models, and testing empirical 
 claims.
  
 knowledge discovery to analyze 
 natural language.
  
 Language
  
 Building robust systems to 
 perform linguistic tasks
  
 Using linguistic algorithms and data 
 structures in robust
  
 technolog
 y
  
 with technological applications.
  
 language processing software.",NA
Organization,"The early chapters are organized in order of conceptual difficulty, starting with a 
 prac-tical introduction to language processing that shows how to explore 
 interesting bodies of text using tiny Python programs (Chapters 
 1
 –
 3
 ). This is 
 followed by a chapter on structured programming (
 Chapter 4
 ) that consolidates the 
 programming topics scat-tered across the preceding chapters. After this, the pace 
 picks up, and we move on to a series of chapters covering fundamental topics in 
 language processing: tagging, clas-sification, and information extraction (Chapters 
 5
 –
 7
 ). The next three chapters look at",NA
Why Python?,"Python is a simple yet powerful programming language with excellent functionality 
 for processing linguistic data. Python can be downloaded for free from 
 http://www.python .org/
 . Installers are available for all platforms.
  
 Here is a five-line Python program that processes 
 file.txt
  and prints all the words 
 ending in 
 ing
 :
  
 >>> for line in open(""file.txt""): 
  
 ...     for word in line.split(): 
  
 ...         if word.endswith('ing'): 
  
 ...             print word
  
 This program illustrates some of the main features of Python. First, whitespace is 
 used to 
 nest
  lines of code; thus the line starting with 
 if
  falls inside the scope of the 
 previous line starting with 
 for
 ; this ensures that the 
 ing
  test is performed for each 
 word. Second, Python is 
 object-oriented
 ; each variable is an entity that has certain 
 defined attributes and methods. For example, the value of the variable 
 line
  is more 
 than a sequence of characters. It is a string object that has a “method” (or 
 operation) called 
 split()
  that",NA
Software Requirements,"To get the most out of this book, you should install several free software packages. 
 Current download pointers and instructions are available at 
 http://www.nltk.org/
 .
  
 Python 
  
 The material presented in this book assumes that you are using Python version 
 2.4 or 2.5. We are committed to porting NLTK to Python 3.0 once the libraries 
 that NLTK depends on have been ported.
  
 NLTK 
  
 The code examples in this book use NLTK version 2.0. Subsequent releases of 
 NLTK will be backward-compatible.
  
 Preface | xiii",NA
Natural Language Toolkit (NLTK),"NLTK was originally created in 2001 as part of a computational linguistics course in 
 the Department of Computer and Information Science at the University of 
 Pennsylva-nia. Since then it has been developed and expanded with the help of 
 dozens of con-tributors. It has now been adopted in courses in dozens of 
 universities, and serves as the basis of many research projects. 
 Table P-2
  lists the 
 most important NLTK modules.
  
 Table P-2. Language processing tasks and corresponding NLTK modules with examples 
 of functionality
  
  
  
  
 Language 
 processing task
  
 NLTK modules
  
 Functionality
  
  
 Accessing corpora
  
 nltk.corpus
  
 Standardized interfaces to corpora and 
 lexicons
  
 String processing
  
 nltk.tokenize, 
 nltk.stem
  
 Tokenizers, sentence tokenizers, stemmers
  
 Collocation 
 discovery
  
 nltk.collocations
  
 t-test, chi-squared, point-wise mutual 
 information
  
 Part-of-speech 
 tagging
  
 nltk.tag
  
 n-gram, backoff, Brill, HMM, TnT
  
 Classification
  
 nltk.classify, 
 nltk.cluster
  
 Decision tree, maximum entropy, naive 
 Bayes, EM, k-means
  
 Chunking
  
 nltk.chunk
  
 Regular expression, n-gram, named entity
  
 Parsing
  
 nltk.parse
  
 Chart, feature-based, unification, 
 probabilistic, dependency
  
 Semantic 
 interpretation
  
 nltk.sem, 
 nltk.inference
  
 Lambda calculus, first-order logic, model 
 checking
  
 Evaluation metrics
  
 nltk.metrics
  
 Precision, recall, agreement coefficients
  
 Probability and 
 estimation
  
 nltk.probability
  
 Frequency distributions, smoothed 
 probability distributions
  
 Applications
  
 nltk.app, 
 nltk.chat
  
 Graphical concordancer, parsers, WordNet 
 browser, chatbots",NA
For Instructors,"Natural Language Processing is often taught within the confines of a single-
 semester course at the advanced undergraduate level or postgraduate level. Many 
 instructors have found that it is difficult to cover both the theoretical and practical 
 sides of the subject in such a short span of time. Some courses focus on theory to 
 the exclusion of practical exercises, and deprive students of the challenge and 
 excitement of writing programs to automatically process language. Other courses 
 are simply designed to teach programming for linguists, and do not manage to 
 cover any significant NLP con-tent. NLTK was originally developed to address this 
 problem, making it feasible to cover a substantial amount of theory and practice 
 within a single-semester course, even if students have no prior programming 
 experience.",NA
Conventions Used in This Book,"The following typographical conventions are used in this book:
  
 Bold 
  
  
 Indicates new terms.
  
 Italic 
  
 Used within paragraphs to refer to linguistic examples, the names of texts, and 
 URLs; also used for filenames and file extensions.
  
 Constant width 
  
 Used for program listings, as well as within paragraphs to refer to program 
 elements such as variable or function names, statements, and keywords; also 
 used for pro-gram names.
  
 Constant width italic 
  
 Shows text that should be replaced with user-supplied values or by values 
 deter-mined by context; also used for metavariables within program code 
 examples.
  
  
  
 This icon signifies a tip, suggestion, or general note.
  
 This icon indicates a warning or caution.",NA
Using Code Examples,"This book is here to help you get your job done. In general, you may use the code in 
 this book in your programs and documentation. You do not need to contact us for 
 permission unless you’re reproducing a significant portion of the code. For 
 example,",NA
Safari® Books Online,"When you see a Safari® Books Online icon on the cover of your favorite 
 technology book, that means the book is available online through the
  
 O’Reilly Network Safari Bookshelf.
  
 Safari offers a solution that’s better than e-books. It’s a virtual library that lets you 
 easily search thousands of top tech books, cut and paste code samples, download 
 chapters, and find quick answers when you need the most accurate, current 
 information. Try it for free at 
 http://my.safaribooksonline.com
 .",NA
How to Contact Us,"Please address comments and questions concerning this book to the publisher:
  
 O’Reilly Media, Inc.
  
 1005 Gravenstein Highway North 
  
 Sebastopol, CA 95472 
  
 800-998-9938 (in the United States or 
 Canada) 707-829-0515 (international or 
 local) 
  
 707-829-0104 (fax)
  
 We have a web page for this book, where we list errata, examples, and any 
 additional information. You can access this page at:
  
 http://www.oreilly.com/catalog/9780596516499
  
 xviii | Preface",NA
Acknowledgments,"The authors are indebted to the following people for feedback on earlier drafts of 
 this book: Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven 
 Bethard, Ondrej Bojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan 
 Garrette, Jean Mark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, 
 Peter Ljunglöf, Stefan Müller, Robin Munn, Joel Nothman, Adam Przepiorkowski, 
 Brandon Rhodes, Stuart Robinson, Jussi Salmela, Kyle Schlansker, Rob Speer, and 
 Richard Sproat. We are thankful to many students and colleagues for their 
 comments on the class materials that evolved into these chapters, including 
 participants at NLP and linguistics summer schools in Brazil, India, and the USA. 
 This book would not exist without the members of the 
 nltk-dev
  developer 
 community, named on the NLTK website, who have given so freely of their time and 
 expertise in building and extending NLTK.
  
 We are grateful to the U.S. National Science Foundation, the Linguistic Data Consor-
 tium, an Edward Clarence Dyason Fellowship, and the Universities of Pennsylvania, 
 Edinburgh, and Melbourne for supporting our work on this book.
  
 We thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O’Reilly team, 
 for organizing comprehensive reviews of our drafts from people across the NLP 
 and Python communities, for cheerfully customizing O’Reilly’s production tools to 
 accom-modate our needs, and for meticulous copyediting work.
  
 Finally, we owe a huge debt of gratitude to our partners, Kay, Mimo, and Jee, for 
 their love, patience, and support over the many years that we worked on this book. 
 We hope that our children—Andrew, Alison, Kirsten, Leonie, and Maaike—catch 
 our enthusi-asm for language and computation from these pages.",NA
Royalties,"Royalties from the sale of this book are being used to support the development of 
 the Natural Language Toolkit.
  
 Preface | xix",NA
CHAPTER 1,NA,NA
Language Processing and ,NA,NA
Python,"It is easy to get our hands on millions of words of text. What can we do with it, 
 assuming we can write some simple programs? In this chapter, we’ll address the 
 following questions:
  
 1. What can we achieve by combining simple programming techniques with large 
 quantities of text?
  
 2. How can we automatically extract key words and phrases that sum up the style 
 and content of a text?
  
 3. What tools and techniques does the Python programming language provide for 
 such work?
  
 4. What are some of the interesting challenges of natural language processing?
  
 This chapter is divided into sections that skip between two quite different styles. In 
 the“computing with language” sections, we will take on some linguistically 
 motivated programming tasks without necessarily explaining how they work. In 
 the “closer look at Python” sections we will systematically review key 
 programming concepts. We’ll flag the two styles in the section titles, but later 
 chapters will mix both styles without being so up-front about it. We hope this style 
 of introduction gives you an authentic taste of what will come later, while covering 
 a range of elementary concepts in linguis-tics and computer science. If you have 
 basic familiarity with both areas, you can skip to 
 Section 1.5
 ; we will repeat any 
 important points in later chapters, and if you miss anything you can easily consult 
 the online reference material at 
 http://www.nltk.org/
 . If the material is completely 
 new to you, this chapter will raise more questions than it answers, questions that 
 are addressed in the rest of this book.",NA
1.1  Computing with Language: Texts and ,NA,NA
Words,"We’re all very familiar with text, since we read and write it every day. Here we will 
 treat text as 
 raw data
  for the programs we write, programs that manipulate and",NA
Getting Started with Python,"One of the friendly things about Python is that it allows you to type directly into the 
 interactive 
 interpreter
 —the program that will be running your Python programs. 
 You can access the Python interpreter using a simple graphical interface called the 
 In-teractive DeveLopment Environment (IDLE). On a Mac you can find this under 
 Ap-plications
 →
 MacPython, and on Windows under All Programs
 →
 Python. Under 
 Unix you can run Python from the shell by typing 
 idle
  (if this is not installed, try 
 typing 
 python
 ). The interpreter will print a blurb about your Python version; simply 
 check that you are running Python 2.4 or 2.5 (here it is 2.5.1):
  
 Python 2.5.1 (r251:54863, Apr 15 2008, 22:57:26) 
  
 [GCC 4.0.1 (Apple Inc. build 5465)] on darwin 
  
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
  
 >>>
  
  
 If you are unable to run the Python interpreter, you probably don’t 
 have Python installed correctly. Please visit 
 http://python.org/
  for 
 detailed in-structions.
  
 The 
 >>>
  prompt indicates that the Python interpreter is now waiting for input. 
 When copying examples from this book, don’t type the “
 >>>
 ” yourself. Now, let’s 
 begin by using Python as a calculator:
  
 >>> 1 + 5 * 2 - 3 
  
 8 
  
 >>>
  
 Once the interpreter has finished calculating the answer and displaying it, the 
 prompt reappears. This means the Python interpreter is waiting for another 
 instruction.
  
  
 Your Turn:
  Enter a few more expressions of your own. You can use 
 asterisk (
 *
 ) for multiplication and slash (
 /
 ) for division, and 
 parentheses for bracketing expressions. Note that division doesn’t 
 always behave as you might expect—it does integer division (with 
 rounding of fractions
  
 downwards) when you type 
 1/3
  and “floating-point” (or decimal) 
 divi-sion when you type 
 1.0/3.0
 . In order to get the expected 
 behavior of division (standard in Python 3.0), you need to type: 
 from 
 __future__ import division
 .
  
 The preceding examples demonstrate how you can work interactively with the 
 Python interpreter, experimenting with various expressions in the language to see 
 what they do. Now let’s try a non-sensical expression to see how the interpreter 
 handles it:",NA
Getting Started with NLTK,"Before going further you should install NLTK, downloadable for free from 
 http://www .nltk.org/
 . Follow the instructions there to download the version 
 required for your platform.
  
 Once you’ve installed NLTK, start up the Python interpreter as before, and install 
 the data required for the book by typing the following two commands at the Python 
 prompt, then selecting the 
 book
  collection as shown in 
 Figure 1-1
 .
  
 >>> import nltk 
  
 >>> nltk.download()
  
  
 Figure 1-1. Downloading the NLTK Book Collection: Browse the available packages using 
 nltk.download()
 . The 
 Collections
  tab on the downloader shows how the packages are grouped 
 into sets, and you should select the line labeled 
 book
  to obtain all data required for the examples 
 and exercises in this book. It consists of about 30 compressed files requiring about 100Mb disk 
 space. The full collection of data (i.e., 
 all
  in the downloader) is about five times this size (at the 
 time of writing) and continues to expand.
  
 Once the data is downloaded to your machine, you can load some of it using the 
 Python interpreter. The first step is to type a special command at the Python 
 prompt, which
  
 1.1  Computing with Language: Texts and Words | 3",NA
Searching Text,"There are many ways to examine the context of a text apart from simply reading it. 
 A concordance view shows us every occurrence of a given word, together with 
 some context. Here we look up the word 
 monstrous
  in 
 Moby Dick
  by entering 
 text1
  
 followed by a period, then the term 
 concordance
 , and then placing 
 ""monstrous""
  in 
 parentheses:
  
 >>> text1.concordance(""monstrous"") 
  
 Building index...
  
 Displaying 11 of 11 matches: 
  
 ong the former , one was of a most monstrous size . ... This came towards us , ON OF THE 
 PSALMS . "" Touching that monstrous bulk of the whale or ork we have r ll over with a 
 heathenish array of monstrous clubs and spears . Some were thick d as you gazed , and 
 wondered what monstrous cannibal and savage could ever hav that has survived the flood ; 
 most monstrous and most mountainous ! That Himmal they might scout at Moby Dick as a 
 monstrous fable , or still worse and more de th of Radney .'"" CHAPTER 55 Of the monstrous 
 Pictures of Whales . I shall ere l ing Scenes . In connexion with the monstrous pictures of 
 whales , I am strongly ere to enter upon those still more monstrous stories of them which are 
 to be fo
  
 4 | Chapter 1:Language Processing and Python",NA
Counting Vocabulary,"The most obvious fact about texts that emerges from the preceding examples is that 
 they differ in the vocabulary they use. In this section, we will see how to use the 
 com-puter to count the words in a text in a variety of useful ways. As before, you 
 will jump right in and experiment with the Python interpreter, even though you 
 may not have studied Python systematically yet. Test your understanding by 
 modifying the examples, and trying the exercises at the end of the chapter.
  
 Let’s begin by finding out the length of a text from start to finish, in terms of the 
 words and punctuation symbols that appear. We use the term 
 len
  to get the length 
 of some-thing, which we’ll apply here to the book of Genesis:
  
 >>> len(text3) 
  
 44764 
  
 >>>
  
 So Genesis has 44,764 words and punctuation symbols, or “tokens.” A 
 token
  is the 
 technical name for a sequence of characters—such as 
 hairy
 , 
 his
 , or 
 :)
 —that we want 
 to treat as a group. When we count the number of tokens in a text, say, the phrase 
 to be or not to be
 , we are counting occurrences of these sequences. Thus, in our 
 example phrase there are two occurrences of 
 to
 , two of 
 be
 , and one each of 
 or
  and 
 not
 . But there are only four distinct vocabulary items in this phrase. How many 
 distinct words does the book of Genesis contain? To work this out in Python, we 
 have to pose the question slightly differently. The vocabulary of a text is just the 
 set
  
 of tokens that it uses, since in a set, all duplicates are collapsed together. In Python 
 we can obtain the vocabulary
  
 1.1  Computing with Language: Texts and Words | 7",NA
1.2  A Closer Look at Python: Texts as Lists ,NA,NA
of Words,"You’ve seen some important elements of the Python programming language. Let’s 
 take a few moments to review them systematically.",NA
Lists,"What is a text? At one level, it is a sequence of symbols on a page such as this one. 
 At another level, it is a sequence of chapters, made up of a sequence of sections, 
 where each section is a sequence of paragraphs, and so on. However, for our 
 purposes, we will think of a text as nothing more than a sequence of words and 
 punctuation. Here’s how we represent text in Python, in this case the opening 
 sentence of 
 Moby Dick
 :
  
 >>> sent1 = ['Call', 'me', 'Ishmael', '.'] 
  
 >>>
  
 After the prompt we’ve given a name we made up, 
 sent1
 , followed by the equals 
 sign, and then some quoted words, separated with commas, and surrounded with 
 brackets. This bracketed material is known as a 
 list
  in Python: it is how we store a 
 text. We can inspect it by typing the name 
 . We can ask for its length 
 . We can 
 even apply our own 
 lexical_diversity()
  function to it .
  
 >>> sent1 
  
 ['Call', 'me', 'Ishmael', '.'] 
  
 >>> len(sent1) 
  
 4 
  
 >>> lexical_diversity(sent1) 
  
 1.0 
  
 >>>",NA
Indexing Lists,"As we have seen, a text in Python is a list of words, represented using a 
 combination of brackets and quotes. Just as with an ordinary page of text, we can 
 count up the total number of words in 
 text1
  with 
 len(text1)
 , and count the 
 occurrences in a text of a particular word—say, 
 heaven
 —using 
 text1.count('heaven')
 .
  
 With some patience, we can pick out the 1st, 173rd, or even 14,278th word in a 
 printed text. Analogously, we can identify the elements of a Python list by their 
 order of oc-currence in the list. The number that represents this position is the 
 item’s 
 index
 . We instruct Python to show us the item that occurs at an index such 
 as 
 173
  in a text by writing the name of the text followed by the index inside square 
 brackets:
  
 >>> text4[173] 
  
 'awaken' 
  
 >>>
  
 We can do the converse; given a word, find the index of when it first occurs:
  
 >>> text4.index('awaken') 
  
 173 
  
 >>>
  
 Indexes are a common way to access the words of a text, or, more generally, the ele-
 ments of any list. Python permits us to access sublists as well, extracting 
 manageable pieces of language from large texts, a technique known as 
 slicing
 .
  
 >>> text5[16715:16735] 
  
 ['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good', 'because', 'you', 'can', 
 'actually', 'play', 'a', 'full', 'game', 'without', 'buying', 'it'] 
  
 >>> text6[1600:1625] 
  
 ['We', ""'"", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We', 'take', 'it', 'in', 'turns', 'to', 
 'act', 'as', 'a', 'sort', 'of', 'executive', 'officer', 'for', 'the', 'week'] 
  
 >>>
  
 Indexes have some subtleties, and we’ll explore these with the help of an artificial 
 sentence:
  
 >>> sent = ['word1', 'word2', 'word3', 'word4', 'word5', ...         
 'word6', 'word7', 'word8', 'word9', 'word10'] >>> sent[0] 
  
 'word1' 
  
 >>> sent[9] 
  
 'word10' 
  
 >>>
  
 Notice that our indexes start from zero: 
 sent
  element zero, written 
 sent[0]
 , is the 
 first word, 
 'word1'
 , whereas 
 sent
  element 9 is 
 'word10'
 . The reason is simple: the 
 moment Python accesses the content of a list from the computer’s memory, it is 
 already at the first element; we have to tell it how many elements forward to go. 
 Thus, zero steps forward leaves it at the first element.
  
 12 | Chapter 1:Language Processing and Python",NA
Variables,"From the start of 
 Section 1.1
 , you have had access to texts called 
 text1
 , 
 text2
 , and so 
 on. It saved a lot of typing to be able to refer to a 250,000-word book with a short 
 name like this! In general, we can make up names for anything we care to calculate. 
 We did this ourselves in the previous sections, e.g., defining a 
 variable
 sent1
 , as 
 follows:
  
 >>> sent1 = ['Call', 'me', 'Ishmael', '.'] 
  
 >>>
  
 Such lines have the form: 
 variable = expression
 . Python will evaluate the expression, 
 and save its result to the variable. This process is called 
 assignment
 . It does not 
 gen-erate any output; you have to type the variable on a line of its own to inspect its 
 contents. The equals sign is slightly misleading, since information is moving from 
 the right side to the left. It might help to think of it as a left-arrow. The name of the 
 variable can be anything you like, e.g., 
 my_sent
 , 
 sentence
 , 
 xyzzy
 . It must start with a 
 letter, and can include numbers and underscores. Here are some examples of 
 variables and assignments:
  
 >>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode', ... 'forth', 'from', 
 'Camelot', '.'] 
  
 >>> noun_phrase = my_sent[1:4] 
  
 >>> noun_phrase 
  
 ['bold', 'Sir', 'Robin'] 
  
 >>> wOrDs = sorted(noun_phrase) 
  
 >>> wOrDs 
  
 ['Robin', 'Sir', 'bold'] 
  
 >>>",NA
Strings,"Some of the methods we used to access the elements of a list also work with 
 individual
  
 words, or 
 strings
 . For example, we can assign a string to a variable 
 , index a 
 string
  
  
 , and slice a string .",NA
1.3  Computing with Language: Simple ,NA,NA
Statistics,"Let’s return to our exploration of the ways we can bring our computational 
 resources to bear on large quantities of text. We began this discussion in 
 Section 
 1.1
 , and saw how to search for words in context, how to compile the vocabulary of a 
 text, how to generate random text in the same style, and so on.
  
 In this section, we pick up the question of what makes a text distinct, and use 
 automatic methods to find characteristic words and expressions of a text. As in 
 Section 1.1
 , you can try new features of the Python language by copying them into 
 the interpreter, and you’ll learn about these features systematically in the following 
 section.
  
 Before continuing further, you might like to check your understanding of the last 
 sec-tion by predicting the output of the following code. You can use the interpreter 
 to check whether you got it right. If you’re not sure how to do this task, it would be 
 a good idea to review the previous section before continuing further.
  
 >>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ...           'more', 'is', 
 'said', 'than', 'done'] 
  
 >>> tokens = set(saying) 
  
 >>> tokens = sorted(tokens) 
  
 >>> tokens[-2:] 
  
 what output do you expect here?
  
 >>>",NA
Frequency Distributions,"How can we automatically identify the words of a text that are most informative 
 about the topic and genre of the text? Imagine how you might go about finding the 
 50 most frequent words of a book. One method would be to keep a tally for each 
 vocabulary item, like that shown in 
 Figure 1-3
 . The tally would need thousands of 
 rows, and it would be an exceedingly laborious process—so laborious that we 
 would rather assign the task to a machine.
  
  
 Figure 1-3. Counting words appearing in a text (a frequency distribution).
  
 The table in 
 Figure 1-3
  is known as a 
 frequency distribution 
 , and it tells us the 
 frequency of each vocabulary item in the text. (In general, it could count any kind of 
 observable event.) It is a “distribution” since it tells us how the total number of 
 word tokens in the text are distributed across the vocabulary items. Since we often 
 need frequency distributions in language processing, NLTK provides built-in 
 support for them. Let’s use a 
 FreqDist
  to find the 50 most frequent words of 
 Moby 
 Dick
 . Try to work out what is going on here, then read the explanation that follows.
  
 >>> fdist1 = FreqDist(text1) 
  
 >>> fdist1 
  
 <FreqDist with 260819 outcomes> 
  
 >>> vocabulary1 = fdist1.keys() 
  
 >>> vocabulary1[:50] 
  
 [',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', ""'"", '-', 'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', 
 '""', 'all', 'for', 'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on', 'so', 'whale', 'one', 
 'you', 'had', 'have', 'there', 'But', 'or', 'were', 'now', 'which', '?', 'me', 'like'] 
  
 >>> fdist1['whale'] 
  
 906 
  
 >>>
  
 When we first invoke 
 FreqDist
 , we pass the name of the text as an argument 
 . We 
 can inspect the total number of words (“outcomes”) that have been counted up 
 —260,819 in the case of 
 Moby Dick
 . The expression 
 keys()
  gives us a list of all the 
 distinct types in the text , and we can look at the first 50 of these by slicing the list .
  
 1.3  Computing with Language: Simple Statistics | 17",NA
Fine-Grained Selection of Words,"Next, let’s look at the 
 long
  words of a text; perhaps these will be more characteristic 
 and informative. For this we adapt some notation from set theory. We would like to 
 find the words from the vocabulary of the text that are more than 15 characters 
 long. Let’s call this property 
 P
 , so that 
 P
 (
 w
 ) is true if and only if 
 w
  is more than 15 
 characters long. Now we can express the words of interest using mathematical set 
 notation as shown in 
 (1a)
 . This means “the set of all 
 w
  such that 
 w
  is an element of 
 V
  (the vocabu-lary) and 
 w
  has property 
 P
 .”
  
 (1) a. {
 w
  | 
 w
 ∈
 V
  & 
 P
 (
 w
 )}
  
 b.
  [w for w in V if p(w)]
  
 The corresponding Python expression is given in 
 (1b)
 . (Note that it produces a list, 
 not a set, which means that duplicates are possible.) Observe how similar the two 
 notations are. Let’s go one more step and write executable Python code:
  
 >>> V = set(text1) 
  
 >>> long_words = [w for w in V if len(w) > 15] 
  
 >>> sorted(long_words) 
  
 ['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically', 
 'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations', 
 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 
 'physiognomically', 'preternaturalness', 'responsibilities', 'simultaneousness', 'subterraneousness', 
 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 
 'undiscriminating', 'uninterpenetratingly'] >>>
  
 For each word 
 w
  in the vocabulary 
 V
 , we check whether 
 len(w)
  is greater than 15; all 
 other words will be ignored. We will discuss this syntax more carefully later.
  
  
 Your Turn:
  Try out the previous statements in the Python 
 interpreter, and experiment with changing the text and changing 
 the length condi-tion. Does it make an difference to your results if 
 you change the variable names, e.g., using 
 [word for word in vocab if 
 ...]
 ?
  
 1.3  Computing with Language: Simple Statistics | 19",NA
Collocations and Bigrams,"A 
 collocation
  is a sequence of words that occur together unusually often. Thus 
 red 
 wine
  is a collocation, whereas 
 the wine
  is not. A characteristic of collocations is that 
 they are resistant to substitution with words that have similar senses; for example, 
 maroon wine
  sounds very odd.
  
 To get a handle on collocations, we start off by extracting from a text a list of word 
 pairs, also known as 
 bigrams
 . This is easily accomplished with the function 
 bigrams()
 :
  
 >>> bigrams(['more', 'is', 'said', 'than', 'done']) 
  
 [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')] >>>
  
 Here we see that the pair of words 
 than-done
  is a bigram, and we write it in Python 
 as 
 ('than', 'done')
 . Now, collocations are essentially just frequent bigrams, except 
 that we want to pay more attention to the cases that involve rare words. In 
 particular, we want to find bigrams that occur more often than we would expect 
 based on the fre-quency of individual words. The 
 collocations()
  function does this 
 for us (we will see how it works later):
  
 >>> text4.collocations() 
  
 Building collocations list 
  
 United States; fellow citizens; years ago; Federal Government; General Government; 
 American people; Vice President; Almighty God; Fellow citizens; Chief Magistrate; 
 Chief Justice; God bless; Indian tribes; public debt; foreign nations; political parties; 
 State governments;
  
 20 | Chapter 1:Language Processing and Python",NA
Counting Other Things,"Counting words is useful, but we can count other things too. For example, we can 
 look at the distribution of word lengths in a text, by creating a 
 FreqDist
  out of a long 
 list of numbers, where each number is the length of the corresponding word in the 
 text:
  
 >>> [len(w) for w in text1] 
  
 [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...] >>> fdist = FreqDist([len(w) for 
 w in text1]) 
  
 >>> fdist 
  
 <FreqDist with 260819 outcomes> 
  
 >>> fdist.keys() 
  
 [3, 1, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20] >>>
  
 We start by deriving a list of the lengths of words in 
 text1
 , and the 
 FreqDist
  then 
 counts the number of times each of these occurs 
 . The result 
  is a distribution 
 containing a quarter of a million items, each of which is a number corresponding to 
 a word token in the text. But there are only 20 distinct items being counted, the 
 numbers 1 through 20, because there are only 20 different word lengths. I.e., there 
 are words consisting of just 1 character, 2 characters, ..., 20 characters, but none 
 with 21 or more characters. One might wonder how frequent the different lengths 
 of words are (e.g., how many words of length 4 appear in the text, are there more 
 words of length 5 than length 4, etc.). We can do this as follows:
  
 >>> fdist.items() 
  
 [(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399), (8, 9966), (9, 
 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177), (15, 70), (16, 22), (17, 12), (18, 1), 
 (20, 1)] 
  
 >>> fdist.max() 
  
 3 
  
 >>> fdist[3] 
  
 50223 
  
 >>> fdist.freq(3) 
  
 0.19255882431878046 
  
 >>>
  
 From this we see that the most frequent word length is 3, and that words of length 
 3 account for roughly 50,000 (or 20%) of the words making up the book. Although 
 we will not pursue it here, further analysis of word length might help us 
 understand
  
 1.3  Computing with Language: Simple Statistics | 21",NA
1.4  Back to Python: Making Decisions and ,NA,NA
Taking Control,"So far, our little programs have had some interesting qualities: the ability to work 
 with language, and the potential to save human effort through automation. A key 
 feature of programming is the ability of machines to make decisions on our behalf, 
 executing instructions when certain conditions are met, or repeatedly looping 
 through text data until some condition is satisfied. This feature is known as 
 control
 , and is the focus of this section.",NA
Conditionals,"Python supports a wide range of operators, such as 
 <
  and 
 >=
 , for testing the 
 relationship between values. The full set of these 
 relational operators
  are shown 
 in 
 Table 1-3
 .
  
 Table 1-3. Numerical comparison operators
  
  
  
 Opera
 tor
  
 Relationship
  
  
 < 
  
 <= 
  
 ==
  
 Less than 
  
 Less than or equal to 
  
 Equal to (note this is two 
 “
 =
 ”signs, not one)",NA
Operating on Every Element,"In 
 Section 1.3
 , we saw some examples of counting items other than words. Let’s 
 take a closer look at the notation we used:
  
 >>> [len(w) for w in text1] 
  
 [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...] >>> [w.upper() for w in text1] 
  
 ['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...] >>>
  
 These expressions have the form 
 [f(w) for ...]
  or 
 [w.f() for ...]
 , where 
 f
  is a function that 
 operates on a word to compute its length, or to convert it to uppercase. For now, 
 you don’t need to understand the difference between the notations 
 f(w)
  and 
 w.f()
 . 
 Instead, simply learn this Python idiom which performs the same operation on 
 every element of a list. In the preceding examples, it goes through each word in 
 text1
 , assigning each one in turn to the variable 
 w
  and performing the specified 
 oper-ation on the variable.
  
  
 The notation just described is called a “list comprehension.” This is 
 our first example of a Python idiom, a fixed notation that we use 
 habitually without bothering to analyze each time. Mastering such 
 idioms is an important part of becoming a fluent Python 
 programmer.
  
 Let’s return to the question of vocabulary size, and apply the same idiom here:
  
 >>> len(text1) 
  
 260819
  
 24 | Chapter 1:Language Processing and Python",NA
Nested Code Blocks,"Most programming languages permit us to execute a block of code when a 
 conditional expression
 , or 
 if
  statement, is satisfied. We already saw examples of 
 conditional tests in code like 
 [w for w in sent7 if len(w) < 4]
 . In the following program, 
 we have created a variable called 
 word
  containing the string value 
 'cat'
 . The 
 if
  
 statement checks whether the test 
 len(word) < 5
  is true. It is, so the body of the 
 if
  
 statement is invoked and the 
 print
  statement is executed, displaying a message to 
 the user. Remember to indent the 
 print
  statement by typing four spaces.
  
 >>> word = 'cat' 
  
 >>> if len(word) < 5: 
  
 ...     print 'word length is less than 5' 
  
 ... 
  
  
 word length is less than 5 
  
 >>>
  
 When we use the Python interpreter we have to add an extra blank line  in order for 
 it to detect that the nested block is complete.
  
 If we change the conditional test to 
 len(word) >= 5
 , to check that the length of 
 word
  
 is greater than or equal to 
 5
 , then the test will no longer be true. This time, the body 
 of the 
 if
  statement will not be executed, and no message is shown to the user:
  
 >>> if len(word) >= 5: 
  
 ...   print 'word length is greater than or equal to 5' ...
  
 >>>
  
 1.4  Back to Python: Making Decisions and Taking Control | 25",NA
Looping with Conditions,"Now we can combine the 
 if
  and 
 for
  statements. We will loop over every item of the 
 list, and print the item only if it ends with the letter 
 l
 . We’ll pick another name for 
 the variable to demonstrate that Python doesn’t try to make sense of variable 
 names.
  
 >>> sent1 = ['Call', 'me', 'Ishmael', '.'] 
  
 >>> for xyzzy in sent1: 
  
 ...     if xyzzy.endswith('l'): 
  
 ...         print xyzzy 
  
 ...
  
 Call 
  
 Ishmael 
  
 >>>
  
 You will notice that 
 if
  and 
 for
  statements have a colon at the end of the line, before 
 the indentation begins. In fact, all Python control structures end with a colon. The 
 colon indicates that the current statement relates to the indented block that 
 follows.
  
 We can also specify an action to be taken if the condition of the 
 if
  statement is not 
 met. Here we see the 
 elif
  (else if) statement, and the 
 else
  statement. Notice that 
 these also have colons before the indented code.
  
 >>> for token in sent1: 
  
 ...     if token.islower(): 
  
 ...         print token, 'is a lowercase word' ...     elif 
 token.istitle(): 
  
 ...         print token, 'is a titlecase word' ...     else: 
  
 ...         print token, 'is punctuation' 
  
 ...
  
 Call is a titlecase word 
  
 me is a lowercase word
  
 26 | Chapter 1:Language Processing and Python",NA
1.5  Automatic Natural Language ,NA,NA
Understanding,"We have been exploring language bottom-up, with the help of texts and the Python 
 programming language. However, we’re also interested in exploiting our 
 knowledge of language and computation by building useful language technologies. 
 We’ll take the opportunity now to step back from the nitty-gritty of code in order to 
 paint a bigger picture of natural language processing.
  
 At a purely practical level, we all need help to navigate the universe of information 
 locked up in text on the Web. Search engines have been crucial to the growth and 
 popularity of the Web, but have some shortcomings. It takes skill, knowledge, and 
 some luck, to extract answers to such questions as: 
 What tourist sites can I visit 
 between Philadelphia and Pittsburgh on a limited budget?What do experts say about 
 digital SLR cameras?What predictions about the steel market were made by credible 
 commentators in the past week?
  Getting a computer to answer them automatically 
 involves a range of language processing tasks, including information extraction, 
 inference, and summari-zation, and would need to be carried out on a scale and 
 with a level of robustness that is still beyond our current capabilities.
  
 On a more philosophical level, a long-standing challenge within artificial 
 intelligence has been to build intelligent machines, and a major part of intelligent 
 behavior is un-derstanding language. For many years this goal has been seen as too 
 difficult. However, as NLP technologies become more mature, and robust methods 
 for analyzing unre-stricted text become more widespread, the prospect of natural 
 language understanding has re-emerged as a plausible goal.
  
 1.5  Automatic Natural Language Understanding | 27",NA
Word Sense Disambiguation,"In 
 word sense disambiguation
  we want to work out which sense of a word was 
 in-tended in a given context. Consider the ambiguous words 
 serve
  and 
 dish
 :
  
 (2) a.
  serve
 : help with food or drink; hold an office; put ball into play
  
 b.
  dish
 : plate; course of a meal; communications device
  
 In a sentence containing the phrase: 
 he served the dish
 , you can detect that both 
 serve 
 and 
 dish
  are being used with their food meanings. It’s unlikely that the topic of 
 discus-sion shifted from sports to crockery in the space of three words. This would 
 force you to invent bizarre images, like a tennis pro taking out his frustrations on a 
 china tea-set laid out beside the court. In other words, we automatically 
 disambiguate words using context, exploiting the simple fact that nearby words 
 have closely related meanings. As another example of this contextual effect, 
 consider the word 
 by
 , which has several meanings, for example, 
 the book by 
 Chesterton
  (agentive—Chesterton was the author of the book); 
 the cup by the stove
  
 (locative—the stove is where the cup is); and 
 submit by Friday
  (temporal—Friday is 
 the time of the submitting). Observe in 
 (3)
  that the meaning of the italicized word 
 helps us interpret the meaning of 
 by
 .
  
 (3) a. The lost children were found by the 
 searchers
  (agentive)
  
 b. The lost children were found by the 
 mountain
  (locative)
  
 c. The lost children were found by the 
 afternoon
  (temporal)",NA
Pronoun Resolution,"A deeper kind of language understanding is to work out “who did what to whom,” 
 i.e., to detect the subjects and objects of verbs. You learned to do this in elementary 
 school, but it’s harder than you might think. In the sentence 
 the thieves stole the 
 paintings
 , it is easy to tell who performed the stealing action. Consider three 
 possible following sen-tences in 
 (4)
 , and try to determine what was sold, caught, 
 and found (one case is ambiguous).
  
 (4) a. The thieves stole the paintings. They were subsequently 
 sold
 .
  
 b. The thieves stole the paintings. They were subsequently 
 caught
 .
  
 c. The thieves stole the paintings. They were subsequently 
 found
 .
  
 Answering this question involves finding the 
 antecedent
  of the pronoun 
 they
 , 
 either thieves or paintings. Computational techniques for tackling this problem 
 include 
 ana-phora resolution
 —identifying what a pronoun or noun phrase refers 
 to—and",NA
Generating Language Output,"If we can automatically solve such problems of language understanding, we will be 
 able to move on to tasks that involve generating language output, such as 
 question 
 answering
  and 
 machine translation
 . In the first case, a machine should be able to 
 answer a user’s questions relating to collection of texts:
  
 (5) a.
  Text:
  ... The thieves stole the paintings. They were subsequently sold. ...
  
 b.
  Human:
  Who or what was sold?
  
 c.
  Machine:
  The paintings.
  
 The machine’s answer demonstrates that it has correctly worked out that 
 they
  
 refers to paintings and not to thieves. In the second case, the machine should be 
 able to translate the text into another language, accurately conveying the meaning 
 of the original text. In translating the example text into French, we are forced to 
 choose the gender of the pronoun in the second sentence: 
 ils
  (masculine) if the 
 thieves are sold, and 
 elles
  (fem-inine) if the paintings are sold. Correct translation 
 actually depends on correct under-standing of the pronoun.
  
 (6) a. The thieves stole the paintings. They were subsequently found.
  
 b. Les voleurs ont volé les peintures. Ils ont été trouvés plus tard. (the 
 thieves)
  
 c. Les voleurs ont volé les peintures. Elles ont été trouvées plus tard. (the 
 paintings)
  
 In all of these examples, working out the sense of a word, the subject of a verb, and 
 the antecedent of a pronoun are steps in establishing the meaning of a sentence, 
 things we would expect a language understanding system to be able to do.",NA
Machine Translation,"For a long time now, machine translation (MT) has been the holy grail of language 
 understanding, ultimately seeking to provide high-quality, idiomatic translation be-
 tween any pair of languages. Its roots go back to the early days of the Cold War, 
 when the promise of automatic translation led to substantial government 
 sponsorship, and with it, the genesis of NLP itself.
  
 Today, practical translation systems exist for particular pairs of languages, and 
 some are integrated into web search engines. However, these systems have some 
 serious shortcomings. We can explore them with the help of NLTK’s “babelizer” 
 (which is automatically loaded when you import this chapter’s materials using 
 from 
 nltk.book import *
 ). This program submits a sentence for translation into a specified 
 language,",NA
Spoken Dialogue Systems,"In the history of artificial intelligence, the chief measure of intelligence has been a 
 lin-guistic one, namely the 
 Turing Test
 : can a dialogue system, responding to a 
 user’s text input, perform so naturally that we cannot distinguish it from a human-
 generated re-sponse? In contrast, today’s commercial dialogue systems are very 
 limited, but still perform useful functions in narrowly defined domains, as we see 
 here:
  
 S: How may I help you?
  
 U: When is Saving Private Ryan playing?
  
 S: For what theater?
  
 U: The Paramount theater.
  
 S: Saving Private Ryan is not playing at the Paramount theater, but 
 it’s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.
  
 You could not ask this system to provide driving instructions or details of nearby 
 res-taurants unless the required information had already been stored and suitable 
 question-answer pairs had been incorporated into the language processing system.
  
 Observe that this system seems to understand the user’s goals: the user asks when 
 a movie is showing and the system correctly determines from this that the user 
 wants to see the movie. This inference seems so obvious that you probably didn’t 
 notice it was made, yet a natural language system needs to be endowed with this 
 capability in order to interact naturally. Without it, when asked, 
 Do you know when
  
 Saving Private Ryan 
 is playing
 ?, a system might unhelpfully respond with a cold 
 Yes
 . 
 However, the devel-opers of commercial dialogue systems use contextual 
 assumptions and business logic to ensure that the different ways in which a user 
 might express requests or provide information are handled in a way that makes 
 sense for the particular application. So, if you type 
 When is ...
 , or 
 I want to know 
 when ...
 , or 
 Can you tell me when ...
 , simple rules will always yield screening times. 
 This is enough for the system to provide a useful service.
  
 Dialogue systems give us an opportunity to mention the commonly assumed 
 pipeline for NLP. 
 Figure 1-5
  shows the architecture of a simple dialogue system. 
 Along the top of the diagram, moving from left to right, is a “pipeline” of some 
 language understand-ing 
 components
 . These map from speech input via syntactic 
 parsing to some kind of meaning representation. Along the middle, moving from 
 right to left, is the reverse pipeline of components for converting concepts to 
 speech. These components make up the dynamic aspects of the system. At the 
 bottom of the diagram are some repre-sentative bodies of static information: the 
 repositories of language-related data that the processing components draw on to 
 do their work.
  
  
 Your Turn:
  For an example of a primitive dialogue system, try 
 having a conversation with an NLTK chatbot. To see the available 
 chatbots, run 
 nltk.chat.chatbots()
 . (Remember to 
 import nltk
  first.)",NA
Textual Entailment,"The challenge of language understanding has been brought into focus in recent 
 years by a public “shared task” called Recognizing Textual Entailment (RTE). The 
 basic scenario is simple. Suppose you want to find evidence to support the 
 hypothesis: 
 Sandra Goudie was defeated by Max Purnell
 , and that you have another 
 short text that seems to be relevant, for example, 
 Sandra Goudie was first elected to 
 Parliament in the 2002 elections, narrowly winning the seat of Coromandel by 
 defeating Labour candidate Max Purnell and pushing incumbent Green MP Jeanette 
 Fitzsimons into third place
 . Does the text provide enough evidence for you to accept 
 the hypothesis? In this particular case, the answer will be “No.” You can draw this 
 conclusion easily, but it is very hard to come up with automated methods for 
 making the right decision. The RTE Challenges provide data that allow competitors 
 to develop their systems, but not enough data for“brute force” machine learning 
 techniques (a topic we will cover in 
 Chapter 6
 ). Con-sequently, some linguistic 
 analysis is crucial. In the previous example, it is important for the system to note 
 that 
 Sandra Goudie
  names the person being defeated in the hypothesis, not the 
 person doing the defeating in the text. As another illustration of the difficulty of the 
 task, consider the following text-hypothesis pair:
  
 (7) a. Text: David Golinkin is the editor or author of 18 books, and over 150 
  
 responsa, articles, sermons and books
  
 b. Hypothesis: Golinkin has written 18 books
  
 32 | Chapter 1:Language Processing and Python",NA
Limitations of NLP,"Despite the research-led advances in tasks such as RTE, natural language systems 
 that have been deployed for real-world applications still cannot perform common-
 sense reasoning or draw on world knowledge in a general and robust manner. We 
 can wait for these difficult artificial intelligence problems to be solved, but in the 
 meantime it is necessary to live with some severe limitations on the reasoning and 
 knowledge capa-bilities of natural language systems. Accordingly, right from the 
 beginning, an impor-tant goal of NLP research has been to make progress on the 
 difficult task of building technologies that “understand language,” using superficial 
 yet powerful techniques instead of unrestricted knowledge and reasoning 
 capabilities. Indeed, this is one of the goals of this book, and we hope to equip you 
 with the knowledge and skills to build useful NLP systems, and to contribute to the 
 long-term aspiration of building intelligent machines.",NA
1.6  Summary,"• Texts are represented in Python using lists: 
 ['Monty', 'Python']
 . We can use in-
  
 dexing, slicing, and the 
 len()
  function on lists.
  
 • A word “token” is a particular appearance of a given word in a text; a word 
 “type”is the unique form of the word as a particular sequence of letters. We 
 count word tokens using 
 len(text)
  and word types using 
 len(set(text))
 .
  
 • We obtain the vocabulary of a text 
 t
  using 
 sorted(set(t))
 .
  
 • We operate on each item of a text using 
 [f(x) for x in text]
 .
  
 • To derive the vocabulary, collapsing case distinctions and ignoring punctuation, 
  
 we can write 
 set([w.lower() for w in text if w.isalpha()])
 .
  
 • We process each word in a text using a 
 for
  statement, such as 
 for w in t:
  or 
 for 
 word in text:
 . This must be followed by the colon character and an indented 
 block of code, to be executed each time through the loop.
  
 • We test a condition using an 
 if
  statement: 
 if len(word) < 5:
 . This must be fol-lowed 
 by the colon character and an indented block of code, to be executed only if the 
 condition is true.
  
 • A frequency distribution is a collection of items along with their frequency 
 counts 
  
 (e.g., the words of a text and their frequency of appearance).
  
 1.6  Summary | 33",NA
1.7  Further Reading,"This chapter has introduced new concepts in programming, natural language 
 process-ing, and linguistics, all mixed in together. Many of them are consolidated in 
 the fol-lowing chapters. However, you may also want to consult the online 
 materials provided with this chapter (at 
 http://www.nltk.org/
 ), including links to 
 additional background materials, and links to online NLP systems. You may also 
 like to read up on some linguistics and NLP-related concepts in Wikipedia (e.g., 
 collocations, the Turing Test, the type-token distinction).
  
 You should acquaint yourself with the Python documentation available at 
 http://docs .python.org/
 , including the many tutorials and comprehensive reference 
 materials 
 linked 
 there. 
 A 
 Beginner’s Guide to Python
  
 is 
 available 
 at 
 http://wiki.python.org/moin/ BeginnersGuide
 . Miscellaneous questions about 
 Python might be answered in the FAQ at 
 http://www.python.org/doc/faq/general/
 .
  
 As you delve into NLTK, you might want to subscribe to the mailing list where new 
 releases of the toolkit are announced. There is also an NLTK-Users mailing list, 
 where users help each other as they learn how to use Python and NLTK for 
 language analysis work. Details of these lists are available at 
 http://www.nltk.org/
 .
  
 For more information on the topics covered in 
 Section 1.5
 , and on NLP more 
 generally, you might like to consult one of the following excellent books:
  
 • Indurkhya, Nitin and Fred Damerau (eds., 2010) 
 Handbook of Natural Language 
  
 Processing
  (second edition), Chapman & Hall/CRC.
  
 • Jurafsky, Daniel and James Martin (2008) 
 Speech and Language Processing
  
 (second 
  
 edition), Prentice Hall.
  
 • Mitkov, Ruslan (ed., 2002) 
 The Oxford Handbook of Computational Linguistics
 . 
  
 Oxford University Press. (second edition expected in 2010).
  
 The Association for Computational Linguistics is the international organization that 
 represents the field of NLP. The 
 ACL website
  hosts many useful resources, 
 including: information about international and regional conferences and 
 workshops; the 
 ACL Wiki
  with links to hundreds of useful resources; and the 
 ACL 
 Anthology
 , which contains most of the NLP research literature from the past 50 
 years, fully indexed and freely downloadable.
  
 34 | Chapter 1:Language Processing and Python",NA
1.8  Exercises,"1.
  ○ 
 Try using the Python interpreter as a calculator, and typing expressions like 
 12 / (4 + 1)
 .
  
 2.
  ○ 
 Given an alphabet of 26 letters, there are 26 to the power 10, or 
 26 ** 10
 , 10-
 letter strings we can form. That works out to 
 141167095653376L
  (the 
 L
  at the 
 end just indicates that this is Python’s long-number format). How many 
 hundred-letter strings are possible?
  
 3.
  ○ 
 The Python multiplication operation can be applied to lists. What happens 
 when you type 
 ['Monty', 'Python'] * 20
 , or 
 3 * sent1
 ?
  
 4.
  ○ 
 Review 
 Section 1.1
  on computing with language. How many words are there 
 in 
 text2
 ? How many distinct words are there?
  
 5.
  ○ 
 Compare the lexical diversity scores for humor and romance fiction in 
 Ta-ble 
 1-1
 . Which genre is more lexically diverse?
  
 6.
  ○ 
 Produce a dispersion plot of the four main protagonists in 
 Sense and 
 Sensibility
 : Elinor, Marianne, Edward, and Willoughby. What can you observe 
 about the different roles played by the males and females in this novel? Can 
 you identify the couples?
  
 7.
  ○ 
 Find the collocations in 
 text5
 .
  
 8.
  ○ 
 Consider the following Python expression: 
 len(set(text4))
 . State the purpose of 
 this expression. Describe the two steps involved in performing this 
 computation.
  
 9.
  ○ 
 Review 
 Section 1.2
  on lists and strings.
  
 a. Define a string and assign it to a variable, e.g., 
 my_string = 'My String'
  (but put 
 something more interesting in the string). Print the contents of this 
 variable in two ways, first by simply typing the variable name and pressing 
 Enter, then by using the 
 print
  statement.
  
 b. Try adding the string to itself using 
 my_string + my_string
 , or multiplying it 
 by a number, e.g., 
 my_string * 3
 . Notice that the strings are joined together 
 without any spaces. How could you fix this?
  
 10.
  ○ 
 Define a variable 
 my_sent
  to be a list of words, using the syntax 
 my_sent = 
 [""My"", ""sent""]
  (but with your own words, or a favorite saying).
  
 a. Use 
 ' '.join(my_sent)
  to convert this into a string.
  
 b. Use 
 split()
  to split the string back into the list form you had to start with.
  
 11.
  ○ 
 Define several variables containing lists of words, e.g., 
 phrase1
 , 
 phrase2
 , and so 
 on. Join them together in various combinations (using the plus operator) to 
 form",NA
CHAPTER 2,NA,NA
Accessing Text ,NA,NA
Corpora and ,NA,NA
Lexical ,NA,NA
Resources,"Practical work in Natural Language Processing typically uses large bodies of 
 linguistic data, or 
 corpora
 . The goal of this chapter is to answer the following 
 questions:
  
 1. What are some useful text corpora and lexical resources, and how can we access 
 them with Python?
  
 2. Which Python constructs are most helpful for this work?
  
 3. How do we avoid repeating ourselves when writing Python code?
  
 This chapter continues to present programming concepts by example, in the 
 context of a linguistic processing task. We will wait until later before exploring each 
 Python construct systematically. Don’t worry if you see an example that contains 
 something unfamiliar; simply try it out and see what it does, and—if you’re game—
 modify it by substituting some part of the code with a different text or word. This 
 way you will associate a task with a programming idiom, and learn the hows and 
 whys later.",NA
2.1  Accessing Text Corpora,"As just mentioned, a text corpus is a large body of text. Many corpora are designed 
 to contain a careful balance of material in one or more genres. We examined some 
 small text collections in 
 Chapter 1
 , such as the speeches known as the US 
 Presidential Inau-gural Addresses. This particular corpus actually contains dozens 
 of individual texts—one per address—but for convenience we glued them end-to-
 end and treated them as a single text. 
 Chapter 1
  also used various predefined texts 
 that we accessed by typing 
 from book import *
 . However, since we want to be able to 
 work with other texts, this section examines a variety of text corpora. We’ll see how 
 to select individual texts, and how to work with them.",NA
Gutenberg Corpus,"NLTK includes a small selection of texts from the Project Gutenberg electronic text 
 archive, which contains some 25,000 free electronic books, hosted at 
 http://www.gu tenberg.org/
 . We begin by getting the Python interpreter to load the 
 NLTK package, then ask to see 
 nltk.corpus.gutenberg.fileids()
 , the file identifiers in 
 this corpus:
  
 >>> import nltk 
  
 >>> nltk.corpus.gutenberg.fileids() 
  
 ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 
 'bryant-stories.txt', 'burgess-busterbrown.txt', 
  
 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 
  
 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 
 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-
 leaves.txt']
  
 Let’s pick out the first of these texts—
 Emma
  by Jane Austen—and give it a short 
 name, 
 emma
 , then find out how many words it contains:
  
 >>> emma = nltk.corpus.gutenberg.words('austen-emma.txt') >>> 
 len(emma) 
  
 192427
  
  
 In 
 Section 1.1
 , we showed how you could carry out concordancing 
 of a text such as 
 text1
  with the command 
 text1.concordance()
 . 
 However, this assumes that you are using one of the nine texts 
 obtained as a result of doing 
 from nltk.book import *
 . Now that you 
 have started examining
  
 data from 
 nltk.corpus
 , as in the previous example, you have to 
 employ the following pair of statements to perform concordancing 
 and other tasks from 
 Section 1.1
 :
  
 >>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt')) >>> 
 emma.concordance(""surprize"")
  
 When we defined 
 emma
 , we invoked the 
 words()
  function of the 
 gutenberg
  object in 
 NLTK’s 
 corpus
  package. But since it is cumbersome to type such long names all the 
 time, Python provides another version of the 
 import
  statement, as follows:
  
 >>> from nltk.corpus import gutenberg 
  
 >>> gutenberg.fileids() 
  
 ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...] >>> emma = 
 gutenberg.words('austen-emma.txt')
  
 Let’s write a short program to display other information about each text, by looping 
 over all the values of 
 fileid
  corresponding to the 
 gutenberg
  file identifiers listed 
 earlier and then computing statistics for each text. For a compact output display, 
 we will make sure that the numbers are all integers, using 
 int()
 .
  
 >>> for fileid in gutenberg.fileids(): 
  
 ...     num_chars = len(gutenberg.raw(fileid)) 
  
 ...     num_words = len(gutenberg.words(fileid)) ...     
 num_sents = len(gutenberg.sents(fileid))
  
 40 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Web and Chat Text,"Although Project Gutenberg contains thousands of books, it represents established 
 literature. It is important to consider less formal language as well. NLTK’s small col-
 lection of web text includes content from a Firefox discussion forum, conversations 
 overheard in New York, the movie script of 
 Pirates of the Carribean
 , personal adver-
 tisements, and wine reviews:
  
 >>> from nltk.corpus import webtext 
  
 >>> for fileid in webtext.fileids(): 
  
 ...     print fileid, webtext.raw(fileid)[:65], '...' ...
  
 firefox.txt Cookie Manager: ""Don't allow sites that set removed cookies to se...
  
 grail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop... overheard.txt 
 White guy: So, do you have any plans for this evening? Asian girl... pirates.txt PIRATES OF THE 
 CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr... singles.txt 25 SEXY MALE, seeks attrac 
 older single lady, for discreet encoun...
  
 wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...
  
 There is also a corpus of instant messaging chat sessions, originally collected by the 
 Naval Postgraduate School for research on automatic detection of Internet 
 predators. The corpus contains over 10,000 posts, anonymized by replacing 
 usernames with generic names of the form “UserNNN”, and manually edited to 
 remove any other identifying information. The corpus is organized into 15 files, 
 where each file contains several hundred posts collected on a given date, for an age-
 specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom). The 
 filename contains the date, chat-room, and number of posts; e.g., 
 10-19-
 20s_706posts.xml
  contains 706 posts gathered from the 20s chat room on 
 10/19/2006.
  
 >>> from nltk.corpus import nps_chat 
  
 >>> chatroom = nps_chat.posts('10-19-20s_706posts.xml') 
  
 >>> chatroom[123] 
  
 ['i', 'do', ""n't"", 'want', 'hot', 'pics', 'of', 'a', 'female', ',', 'I', 'can', 'look', 'in', 'a', 
 'mirror', '.']",NA
Brown Corpus,"The Brown Corpus was the first million-word electronic corpus of English, created 
 in 1961 at Brown University. This corpus contains text from 500 sources, and the 
 sources have been categorized by genre, such as 
 news
 , 
 editorial
 , and so on. 
 Table 2-
 1
  
 gives 
 an 
 example 
 of 
 each 
 genre 
 (for 
 a 
 complete 
 list, 
 see 
 http://icame.uib.no/brown/bcm-los.html
 ).
  
 42 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Reuters Corpus,"The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. 
 The documents have been classified into 90 topics, and grouped into two sets, 
 called “train-ing” and “test”; thus, the text with fileid 
 'test/14826'
  is a document 
 drawn from the test set. This split is for training and testing algorithms that 
 automatically detect the topic of a document, as we will see in 
 Chapter 6
 .
  
 >>> from nltk.corpus import reuters 
  
 >>> reuters.fileids() 
  
 ['test/14826', 'test/14828', 'test/14829', 'test/14832', ...] >>> 
 reuters.categories() 
  
 ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-
 oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 
 'crude', 'dfl', 'dlr', ...]
  
 Unlike the Brown Corpus, categories in the Reuters Corpus overlap with each other, 
 simply because a news story often covers multiple topics. We can ask for the topics
  
 44 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Inaugural Address Corpus,"In 
 Section 1.1
 , we looked at the Inaugural Address Corpus, but treated it as a single 
 text. The graph in 
 Figure 1-2
  used “word offset” as one of the axes; this is the 
 numerical index of the word in the corpus, counting from the first word of the first 
 address. However, the corpus is actually a collection of 55 texts, one for each 
 presidential ad-dress. An interesting property of this collection is its time 
 dimension:
  
 >>> from nltk.corpus import inaugural 
  
 >>> inaugural.fileids() 
  
 ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...] 
  
 >>> [fileid[:4] for fileid in inaugural.fileids()] 
  
 ['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]
  
 Notice that the year of each text appears in its filename. To get the year out of the 
 filename, we extracted the first four characters, using 
 fileid[:4]
 .
  
 Let’s look at how the words 
 America
  and 
 citizen
  are used over time. The following 
 code converts the words in the Inaugural corpus to lowercase using 
 w.lower()
  , then 
 checks whether they start with either of the “targets” 
 america
  or 
 citizen
  using 
 startswith()
  
 . Thus it will count words such as 
 American’s
  and 
 Citizens
 . We’ll learn about condi-
 tional frequency distributions in 
 Section 2.2
 ; for now, just consider the output, 
 shown in 
 Figure 2-1
 .",NA
Annotated Text Corpora,"Many text corpora contain linguistic annotations, representing part-of-speech tags, 
 named entities, syntactic structures, semantic roles, and so forth. NLTK provides 
 convenient ways to access several of these corpora, and has data packages 
 containing corpora and corpus samples, freely downloadable for use in teaching 
 and research. 
 Table 2-2
   lists some of the corpora. For information about 
 downloading them, see 
 http://www.nltk.org/data
 . For more examples of how to 
 access 
 NLTK 
 corpora, 
 please 
 consult 
 the 
 Corpus 
 HOWTO 
 at 
 http://www.nltk.org/howto
 .
  
 Table 2-2. Some of the corpora and corpus samples distributed with NLTK
  
  
  
  
 Corpus
  
 Compiler
  
  
 Contents
  
  
 Brown Corpus
  
 Francis, 
 Kucera
  
 15 genres, 1.15M words, tagged, 
 categorized
  
 CESS Treebanks
  
 CLiC-UB
  
 1M words, tagged and parsed (Catalan, 
 Spanish)
  
 Chat-80 Data Files
  
 Pereira & 
 Warren
  
 World Geographic Database
  
 CMU Pronouncing 
 Dictionary
  
 CMU
  
 127k entries
  
 CoNLL 2000 Chunking 
 Data
  
 CoNLL
  
 270k words, tagged and chunked
  
 46 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Corpora in Other Languages,"NLTK comes with corpora for many languages, though in some cases you will need 
 to learn how to manipulate character encodings in Python before using these 
 corpora (see 
 Section 3.3
 ).
  
 >>> nltk.corpus.cess_esp.words() 
  
 ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...] 
  
 >>> nltk.corpus.floresta.words() 
  
 ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...] 
  
 >>> nltk.corpus.indian.words('hindi.pos') 
  
 ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3', 
  
 '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4 
 \x82\xe0\xa4\xa7', ...] 
  
 >>> nltk.corpus.udhr.fileids() 
  
 ['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1', 'Adja-UTF8', 
 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1', 'Akuapem_Twi-UTF8', 
 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...] >>> 
 nltk.corpus.udhr.words('Javanese-Latin1')[11:] 
  
 [u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]
  
 The last of these corpora, 
 udhr
 , contains the Universal Declaration of Human Rights 
 in over 300 languages. The fileids for this corpus include information about the 
 char-acter encoding used in the file, such as 
 UTF8
  or 
 Latin1
 . Let’s use a conditional 
 frequency distribution to examine the differences in word lengths for a selection of 
 languages included in the 
 udhr
  corpus. The output is shown in 
 Figure 2-2
  (run the 
 program your-self to see a color plot). Note that 
 True
  and 
 False
  are Python’s built-in 
 Boolean values.
  
 >>> from nltk.corpus import udhr 
  
 >>> languages = ['Chickasaw', 'English', 'German_Deutsch', 
  
 ...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik'] >>> cfd = 
 nltk.ConditionalFreqDist( 
  
 ...           (lang, len(word)) 
  
 ...           for lang in languages 
  
 ...           for word in udhr.words(lang + '-Latin1')) 
  
 >>> cfd.plot(cumulative=True)
  
  
 Your Turn:
  Pick a language of interest in 
 udhr.fileids()
 , and define a 
 variable 
 raw_text = udhr.raw(
 Language-Latin1
 )
 . Now plot a frequency 
 distribution of the letters of the text using
  
 nltk.FreqDist(raw_text).plot().
  
 Unfortunately, for many languages, substantial corpora are not yet available. Often 
 there is insufficient government or industrial support for developing language 
 resour-ces, and individual efforts are piecemeal and hard to discover or reuse. 
 Some languages have no established writing system, or are endangered. (See 
 Section 2.7
  for suggestions on how to locate language resources.)
  
 48 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Text Corpus Structure,"We have seen a variety of corpus structures so far; these are summarized in 
 Fig-ure 
 2-3
 . The simplest kind lacks any structure: it is just a collection of texts. Often, texts 
 are grouped into categories that might correspond to genre, source, author, lan-
 guage, etc. Sometimes these categories overlap, notably in the case of topical 
 categories, as a text can be relevant to more than one topic. Occasionally, text 
 collections have temporal structure, news collections being the most common 
 example.
  
 NLTK’s corpus readers support efficient access to a variety of corpora, and can be 
 used to work with new corpora. 
 Table 2-3
  lists functionality provided by the corpus 
 readers.",NA
Loading Your Own Corpus,"If you have a your own collection of text files that you would like to access using the 
 methods discussed earlier, you can easily load them with the help of NLTK’s 
 Plain 
 textCorpusReader
 . Check the location of your files on your file system; in the 
 following example, we have taken this to be the directory 
 /usr/share/dict
 . 
 Whatever the location, set this to be the value of 
 corpus_root
 . The second 
 parameter of the 
 PlaintextCor pusReader
  initializer  can be a list of fileids, like 
 ['a.txt', 
 'test/b.txt']
 , or a pattern that matches all fileids, like 
 '[abc]/.*\.txt'
  (see 
 Section 3.4
  for 
 information about regular expressions).
  
 >>> from nltk.corpus import PlaintextCorpusReader >>> 
 corpus_root = '/usr/share/dict' 
  
 >>> wordlists = PlaintextCorpusReader(corpus_root, '.*') 
  
 >>> wordlists.fileids() 
  
 ['README', 'connectives', 'propernames', 'web2', 'web2a', 'words'] >>> 
 wordlists.words('connectives') 
  
 ['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]
  
 As another example, suppose you have your own local copy of Penn Treebank 
 (release 3), in 
 C:\corpora
 . We can use the 
 BracketParseCorpusReader
  to access this 
 corpus. We specify the 
 corpus_root
  to be the location of the parsed 
 Wall Street 
 Journal
  component of the corpus 
 , and give a 
 file_pattern
  that matches the files 
 contained within its subfolders  (using forward slashes).
  
 >>> from nltk.corpus import BracketParseCorpusReader 
  
 >>> corpus_root = r""C:\corpora\penntreebank\parsed\mrg\wsj"" 
  
 >>> file_pattern = r"".*/wsj_.*\.mrg"" 
  
 >>> ptb = BracketParseCorpusReader(corpus_root, file_pattern) 
  
 >>> ptb.fileids() 
  
 ['00/wsj_0001.mrg', '00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...] >>> len(ptb.sents()) 
  
 49208 
  
 >>> ptb.sents(fileids='20/wsj_2013.mrg')[19] 
  
 ['The', '55-year-old', 'Mr.', 'Noriega', 'is', ""n't"", 'as', 'smooth', 'as', 'the', 'shah', 'of', 'Iran', ',', 'as', 'well-
 born', 'as', 'Nicaragua', ""'s"", 'Anastasio', 'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 
 'Philippines', 'or', 'as', 'bloody', 'as', 'Haiti', ""'s"", 'Baby', Doc', 'Duvalier', '.']
  
 2.1  Accessing Text Corpora | 51",NA
2.2  Conditional Frequency Distributions,"We introduced frequency distributions in 
 Section 1.3
 . We saw that given some list 
 mylist
  of words or other items, 
 FreqDist(mylist)
  would compute the number of 
 occurrences of each item in the list. Here we will generalize this idea.
  
 When the texts of a corpus are divided into several categories (by genre, topic, 
 author, etc.), we can maintain separate frequency distributions for each category. 
 This will allow us to study systematic differences between the categories. In the 
 previous section, we achieved this using NLTK’s 
 ConditionalFreqDist
  data type. A 
 conditional fre-quency distribution
  is a collection of frequency distributions, 
 each one for a different“condition.” The condition will often be the category of the 
 text. 
 Figure 2-4
  depicts a fragment of a conditional frequency distribution having 
 just two conditions, one for news text and one for romance text.
  
  
  
  
  
  
 Figure 2-4. Counting words appearing in a text collection (a conditional frequency distribution).",NA
Conditions and Events,"A frequency distribution counts observable events, such as the appearance of words 
 in a text. A conditional frequency distribution needs to pair each event with a 
 condition.
  
 So instead of processing a sequence of words 
 pairs :
  
 , we have to process a sequence of
  
 >>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...] 
  
 >>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...] 
  
 Each pair has the form 
 (
 condition
 , 
 event
 )
 . If we were processing the entire Brown 
 Corpus by genre, there would be 15 conditions (one per genre) and 1,161,192 
 events (one per word).",NA
Counting Words by Genre,"In 
 Section 2.1
 , we saw a conditional frequency distribution where the condition 
 was the section of the Brown Corpus, and for each condition we counted words. 
 Whereas 
 FreqDist()
  takes a simple list as input, 
 ConditionalFreqDist()
  takes a list of 
 pairs.",NA
Plotting and Tabulating Distributions,"Apart from combining two or more frequency distributions, and being easy to 
 initialize, a 
 ConditionalFreqDist
  provides some useful methods for tabulation and 
 plotting.
  
 2.2  Conditional Frequency Distributions | 53",NA
Generating Random Text with Bigrams,"We can use a conditional frequency distribution to create a table of bigrams (word 
 pairs, introduced in 
 Section 1.3
 ). The 
 bigrams()
  function takes a list of words and 
 builds a list of consecutive word pairs:
  
 >>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', ...   'and', 'the', 'earth', '.'] 
  
 >>> nltk.bigrams(sent) 
  
 [('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'), ('created', 'the'), ('the', 
 'heaven'), ('heaven', 'and'), ('and', 'the'), ('the', 'earth'), ('earth', '.')]
  
 In 
 Example 2-1
 , we treat each word as a condition, and for each one we effectively 
 create a frequency distribution over the following words. The function 
 gener 
 ate_model()
  contains a simple loop to generate text. When we call the function, we 
 choose a word (such as 
 'living'
 ) as our initial context. Then, once inside the loop, we 
 print the current value of the variable 
 word
 , and reset 
 word
  to be the most likely 
 token in that context (using 
 max()
 ); next time through the loop, we use that word as 
 our new context. As you can see by inspecting the output, this simple approach to 
 text gener-ation tends to get stuck in loops. Another method would be to randomly 
 choose the next word from among the available words.
  
 Example 2-1. Generating random text: This program obtains all bigrams from the text of the 
 book of Genesis, then constructs a conditional frequency distribution to record which words are 
 most likely to follow a given word; e.g., after the word 
 living
 , the most likely word is 
 creature
 ; 
 the 
 generate_model()
  function uses this data, and a seed word, to generate random text.
  
 def generate_model(cfdist, word, num=15):
  
  
  for i in range(num):
  
  
  
  print word,
  
  
  
  word = cfdist[word].max()
  
 2.2  Conditional Frequency Distributions | 55",NA
2.3  More Python: Reusing Code,"By this time you’ve probably typed and retyped a lot of code in the Python 
 interactive interpreter. If you mess up when retyping a complex example, you have 
 to enter it again. Using the arrow keys to access and modify previous commands is 
 helpful but only goes so far. In this section, we see two important ways to reuse 
 code: text editors and Python functions.",NA
Creating Programs with a Text Editor,"The Python interactive interpreter performs your instructions as soon as you type 
 them. Often, it is better to compose a multiline program using a text editor, then ask 
 Python to run the whole program at once. Using IDLE, you can do this by going to 
 the File menu and opening a new window. Try this now, and enter the following 
 one-line program:
  
 print 'Monty Python'
  
 56 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Functions,"Suppose that you work on analyzing text that involves different forms of the same 
 word, and that part of your program needs to work out the plural form of a given 
 singular noun. Suppose it needs to do this work in two places, once when it is 
 processing some texts and again when it is processing user input.
  
 Rather than repeating the same code several times over, it is more efficient and 
 reliable to localize this work inside a 
 function
 . A function is just a named block of 
 code that performs some well-defined task, as we saw in 
 Section 1.1
 . A function is 
 usually defined to take some inputs, using special variables known as 
 parameters
 , 
 and it may produce a result, also known as a 
 return value
 . We define a function 
 using the keyword 
 def 
 followed by the function name and any input parameters, 
 followed by the body of the function. Here’s the function we saw in 
 Section 1.1
  
 (including the 
 import
  statement that makes division behave as expected):
  
 2.3  More Python: Reusing Code | 57",NA
Modules,"Over time you will find that you create a variety of useful little text-processing 
 functions, and you end up copying them from old programs to new ones. Which file 
 contains the latest version of the function you want to use? It makes life a lot easier 
 if you can collect your work into a single place, and access previously defined 
 functions without making copies.
  
 To do this, save your function(s) in a file called (say) 
 textproc.py
 . Now, you can 
 access your work simply by importing it from the file:
  
 >>> from textproc import plural 
  
 >>> plural('wish') 
  
 wishes 
  
 >>> plural('fan') 
  
 fen
  
 Our plural function obviously has an error, since the plural of 
 fan
  is 
 fans
 . Instead of 
 typing in a new version of the function, we can simply edit the existing one. Thus, at 
 every stage, there is only one version of our plural function, and no confusion about 
 which one is being used.
  
 A collection of variable and function definitions in a file is called a Python 
 module
 . 
 A collection of related modules is called a 
 package
 . NLTK’s code for processing the 
 Brown Corpus is an example of a module, and its collection of code for processing 
 all the different corpora is an example of a package. NLTK itself is a set of packages, 
 sometimes called a 
 library
 .
  
  
 Caution!
  
 If you are creating a file to contain some of your Python code, do 
 not 
 name your file 
 nltk.py
 : it may get imported in place of the “real” NLTK
  
 package. When it imports modules, Python first looks in the current 
 directory (folder).",NA
2.4  Lexical Resources,"A lexicon, or lexical resource, is a collection of words and/or phrases along with 
 asso-ciated information, such as part-of-speech and sense definitions. Lexical 
 resources are secondary to texts, and are usually created and enriched with the 
 help of texts. For example, if we have defined a text 
 my_text
 , then 
 vocab = 
 sorted(set(my_text))
  builds the vocabulary of 
 my_text
 , whereas 
 word_freq = 
 FreqDist(my_text)
  counts the fre-quency of each word in the text. Both 
 vocab
  and 
 word_freq
  are simple lexical resources. Similarly, a concordance like the one we saw 
 in 
 Section 1.1
  gives us information about word usage that might help in the 
 preparation of a dictionary. Standard terminology for lexicons is illustrated in 
 Figure 2-5
 . A 
 lexical entry
  consists of a 
 headword
  (also known as a 
 lemma
 ) along 
 with additional information, such as the part-of-speech and
  
 2.4  Lexical Resources | 59",NA
Wordlist Corpora,"NLTK includes some corpora that are nothing more than wordlists. The Words 
 Corpus is the 
 /usr/dict/words
  file from Unix, used by some spellcheckers. We can 
 use it to find unusual or misspelled words in a text corpus, as shown in 
 Example 2-
 3
 .
  
 Example 2-3. Filtering a text: This program computes the vocabulary of a text, then removes all 
 items that occur in an existing wordlist, leaving just the uncommon or misspelled words.
  
 def unusual_words(text):
  
  text_vocab = set(w.lower() for w in text if w.isalpha())
  
  english_vocab = set(w.lower() for w in nltk.corpus.words.words()) unusual = 
 text_vocab.difference(english_vocab)
  
  return sorted(unusual)
  
 >>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt')) 
  
 ['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary', 'adieus', 
 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham', 'amiably', 'annamaria', 'annuities', 
 'apologising', 'arbour', 'archness', ...] >>> unusual_words(nltk.corpus.nps_chat.words()) 
  
 ['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros', 
  
 'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk', 'agaibn', 'agurlwithbigguns', 
 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]
  
 There is also a corpus of 
 stopwords
 , that is, high-frequency words such as 
 the
 , 
 to
 , 
 and 
 also
  that we sometimes want to filter out of a document before further 
 processing. Stopwords usually have little lexical content, and their presence in a 
 text fails to dis-tinguish it from other texts.
  
 >>> from nltk.corpus import stopwords 
  
 >>> stopwords.words('english') 
  
 ['a', ""a's"", 'able', 'about', 'above', 'according', 'accordingly', 'across',
  
 60 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
A Pronouncing Dictionary,"A slightly richer kind of lexical resource is a table (or spreadsheet), containing a 
 word plus some properties in each row. NLTK includes the CMU Pronouncing 
 Dictionary for U.S. English, which was designed for use by speech synthesizers.
  
 >>> entries = nltk.corpus.cmudict.entries() 
  
 >>> len(entries) 
  
 127012 
  
 >>> for entry in entries[39943:39951]: 
  
 ...     print entry 
  
 ...
  
 ('fir', ['F', 'ER1']) 
  
 ('fire', ['F', 'AY1', 'ER0']) 
  
 ('fire', ['F', 'AY1', 'R']) 
  
 ('firearm', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M']) 
  
 ('firearm', ['F', 'AY1', 'R', 'AA2', 'R', 'M']) 
  
 ('firearms', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z']) ('firearms', ['F', 
 'AY1', 'R', 'AA2', 'R', 'M', 'Z']) ('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 
 'L'])
  
 For each word, this lexicon provides a list of phonetic codes—distinct labels for 
 each contrastive sound—known as 
 phones
 . Observe that 
 fire
  has two 
 pronunciations (in U.S. English): the one-syllable 
 F AY1 R
 , and the two-syllable 
 F AY1 
 ER0
 . The symbols in the CMU Pronouncing Dictionary are from the 
 Arpabet
 , 
 described in more detail at 
 http://en.wikipedia.org/wiki/Arpabet
 .
  
 Each entry consists of two parts, and we can process these individually using a 
 more complex version of the 
 for
  statement. Instead of writing 
 for entry in entries:
 , we 
 replace 
 entry
  with 
 two
  variable names, 
 word, pron
  . Now, each time through the loop, 
 word
  is assigned the first part of the entry, and 
 pron
  is assigned the second part of 
 the entry:
  
 >>> for word, pron in entries: 
  
 ...     if len(pron) == 3: 
  
 ...         ph1, ph2, ph3 = pron 
  
 ...         if ph1 == 'P' and ph3 == 'T': 
  
 ...             print word, ph2, 
  
 ...
  
 pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1 pet EH1 pete 
 IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1 pott AA1 pout AW1 puett 
 UW1 purt ER1 put UH1 putt AH1
  
 The program just shown scans the lexicon looking for entries whose pronunciation 
 consists of three phones 
 . If the condition is true, it assigns the contents of 
 pron
  to 
 three new variables: 
 ph1
 , 
 ph2
 , and 
 ph3
 . Notice the unusual form of the statement 
 that does that work .
  
 Here’s another example of the same 
 for
  statement, this time used inside a list 
 compre-hension. This program finds all words whose pronunciation ends with a 
 syllable sounding like 
 nicks
 . You could use this method to find rhyming words.
  
 2.4  Lexical Resources | 63",NA
Comparative Wordlists,"Another example of a tabular lexicon is the 
 comparative wordlist
 . NLTK includes 
 so-called 
 Swadesh wordlists
 , lists of about 200 common words in several 
 languages. The languages are identified using an ISO 639 two-letter code.
  
 >>> from nltk.corpus import swadesh 
  
 >>> swadesh.fileids() 
  
 ['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk', 'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 
 'sw', 'uk'] 
  
 >>> swadesh.words('en') 
  
 ['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',
  
 2.4  Lexical Resources | 65",NA
Shoebox and Toolbox Lexicons,"Perhaps the single most popular tool used by linguists for managing data is 
 Toolbox
 , previously known as 
 Shoebox
  since it replaces the field linguist’s 
 traditional shoebox full of file cards. Toolbox is freely downloadable from 
 http://www.sil.org/computing/ toolbox/
 .
  
 A Toolbox file consists of a collection of entries, where each entry is made up of one 
 or more fields. Most fields are optional or repeatable, which means that this kind of 
 lexical resource cannot be treated as a table or spreadsheet.
  
 Here is a dictionary for the Rotokas language. We see just the first entry, for the 
 word 
 kaa
 , meaning “to gag”:
  
 66 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
2.5  WordNet,"WordNet
  is a semantically oriented dictionary of English, similar to a traditional 
 the-saurus but with a richer structure. NLTK includes the English WordNet, with 
 155,287 words and 117,659 synonym sets. We’ll begin by looking at synonyms and 
 how they are accessed in WordNet.",NA
Senses and Synonyms,"Consider the sentence in 
 (1a)
 . If we replace the word 
 motorcar
  in 
 (1a)
  with 
 automo-
 bile
 , to get 
 (1b)
 , the meaning of the sentence stays pretty much the same:
  
 (1) a. Benz is credited with the invention of the motorcar.
  
 b. Benz is credited with the invention of the automobile.
  
 Since everything else in the sentence has remained unchanged, we can conclude 
 that the words 
 motorcar
  and 
 automobile
  have the same meaning, i.e., they are 
 synonyms
 .
  
 We can explore these words with the help of WordNet:
  
 >>> from nltk.corpus import wordnet as wn 
  
 >>> wn.synsets('motorcar') 
  
 [Synset('car.n.01')]
  
 Thus, 
 motorcar
  has just one possible meaning and it is identified as 
 car.n.01
 , the first 
 noun sense of 
 car
 . The entity 
 car.n.01
  is called a 
 synset
 , or “synonym set,” a 
 collection of synonymous words (or “lemmas”):
  
 2.5  WordNet | 67",NA
The WordNet Hierarchy,"WordNet synsets correspond to abstract concepts, and they don’t always have 
 corre-sponding words in English. These concepts are linked together in a hierarchy. 
 Some concepts are very general, such as 
 Entity
 , 
 State
 , 
 Event
 ; these are called 
 unique begin-ners
  or root synsets. Others, such as 
 gas guzzler
  and 
 hatchback
 , are 
 much more specific. A small portion of a concept hierarchy is illustrated in 
 Figure 
 2-8
 .
  
  
 Figure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges 
 indicate the hypernym/hyponym relation, i.e., the relation between superordinate and 
 subordinate concepts.
  
 WordNet makes it easy to navigate between concepts. For example, given a concept 
 like 
 motorcar
 , we can look at the concepts that are more specific—the (immediate) 
 hyponyms
 .
  
 >>> motorcar = wn.synset('car.n.01') 
  
 >>> types_of_motorcar = motorcar.hyponyms() 
  
 >>> types_of_motorcar[26] 
  
 Synset('ambulance.n.01') 
  
 >>> sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas]) 
 ['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 
 'cab', 'compact', 'compact_car', 'convertible', 
  
 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 
  
 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 
  
 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 
  
 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',",NA
More Lexical Relations,"Hypernyms and hyponyms are called 
 lexical relations
  because they relate one 
 synset to another. These two relations navigate up and down the “is-a” hierarchy. 
 Another important way to navigate the WordNet network is from items to their 
 components (
 meronyms
 ) or to the things they are contained in (
 holonyms
 ). For 
 example, the parts of a 
 tree
  are its 
 trunk
 , 
 crown
 , and so on; these are the 
 part_meronyms()
 . The 
 substance 
 a tree is made of includes 
 heartwood
  and 
 sapwood
 , 
 i.e., the 
 substance_meronyms()
 . A collection of trees forms a 
 forest
 , i.e., the 
 member_holonyms()
 :
  
 >>> wn.synset('tree.n.01').part_meronyms() 
  
 [Synset('burl.n.02'), Synset('crown.n.07'), Synset('stump.n.01'), 
 Synset('trunk.n.01'), Synset('limb.n.02')] 
  
 >>> wn.synset('tree.n.01').substance_meronyms() 
  
 [Synset('heartwood.n.01'), Synset('sapwood.n.01')]
  
 70 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
Semantic Similarity,"We have seen that synsets are linked by a complex network of lexical relations. 
 Given a particular synset, we can traverse the WordNet network to find synsets 
 with related meanings. Knowing which words are semantically related is useful for 
 indexing a col-lection of texts, so that a search for a general term such as 
 vehicle
  will 
 match documents containing specific terms such as 
 limousine
 .
  
 2.5  WordNet | 71",NA
2.6  Summary,"• A text corpus is a large, structured collection of texts. NLTK comes with many 
  
 corpora, e.g., the Brown Corpus, 
 nltk.corpus.brown
 .
  
 • Some text corpora are categorized, e.g., by genre or topic; sometimes the 
 categories 
  
 of a corpus overlap each other.
  
 • A conditional frequency distribution is a collection of frequency distributions, 
 each one for a different condition. They can be used for counting word 
 frequencies, given a context or a genre.
  
 • Python programs more than a few lines long should be entered using a text 
 editor, 
  
 saved to a file with a 
 .py
  extension, and accessed using an 
 import
  
 statement.
  
 • Python functions permit you to associate a name with a particular block of code, 
  
 and reuse that code as often as necessary.
  
 • Some functions, known as “methods,” are associated with an object, and we give 
 the object name followed by a period followed by the method name, like this: 
 x.funct(y)
 , e.g., 
 word.isalpha()
 .
  
 • To find out about some variable 
 v
 , type 
 help(v)
  in the Python interactive 
 interpreter 
  
 to read the help entry for this kind of object.
  
 • WordNet is a semantically oriented dictionary of English, consisting of synonym 
  
 sets—or synsets—and organized into a network.
  
 • Some functions are not available by default, but must be accessed using Python’s 
  
 import
  statement.",NA
2.7  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web. The corpus methods are summarized in 
 the Corpus HOWTO, at 
 http://www.nltk.org/howto
 , and documented extensively in 
 the online API documentation.
  
 Significant sources of published corpora are the 
 Linguistic Data Consortium
  (LDC) 
 and the 
 European Language Resources Agency
  (ELRA). Hundreds of annotated text 
 and speech corpora are available in dozens of languages. Non-commercial licenses 
 permit the data to be used in teaching and research. For some corpora, commercial 
 licenses are also available (but for a higher fee).
  
 2.7  Further Reading | 73",NA
2.8  Exercises,"1.
  ○ 
 Create a variable 
 phrase
  containing a list of words. Experiment with the 
 opera-tions described in this chapter, including addition, multiplication, 
 indexing, slic-ing, and sorting.
  
 2.
  ○ 
 Use the corpus module to explore 
 austen-persuasion.txt
 . How many word 
 tokens does this book have? How many word types?
  
 3.
  ○ 
 Use the Brown Corpus reader 
 nltk.corpus.brown.words()
  or the Web Text Cor-
 pus reader 
 nltk.corpus.webtext.words()
  to access some sample text in two differ-
 ent genres.
  
 4.
  ○ 
 Read in the texts of the 
 State of the Union
  addresses, using the 
 state_union
  
 corpus reader. Count occurrences of 
 men
 , 
 women
 , and 
 people
  in each document. 
 What has happened to the usage of these words over time?
  
 5.
  ○ 
 Investigate the holonym-meronym relations for some nouns. Remember that 
 there are three kinds of holonym-meronym relation, so you need to use 
 member_mer
  
 onyms()
 , 
  
 part_meronyms()
 , 
  
 substance_meronyms()
 , 
  
 member_holonyms(
 )
 ,
  
 part_holonyms()
 , and 
 substance_holonyms()
 .
  
 6.
  ○ 
 In the discussion of comparative wordlists, we created an object called 
 trans 
 late
 , which you could look up using words in both German and Italian in order
  
 74 | Chapter 2:Accessing Text Corpora and Lexical Resources",NA
CHAPTER 3,NA,NA
Processing Raw Text,"The most important source of texts is undoubtedly the Web. It’s convenient to have 
 existing text collections to explore, such as the corpora we saw in the previous 
 chapters. However, you probably have your own text sources in mind, and need to 
 learn how to access them.
  
 The goal of this chapter is to answer the following questions:
  
 1. How can we write programs to access text from local files and from the Web, in 
 order to get hold of an unlimited range of language material?
  
 2. How can we split documents up into individual words and punctuation 
 symbols, so we can carry out the same kinds of analysis we did with text 
 corpora in earlier chapters?
  
 3. How can we write programs to produce formatted output and save it in a file?
  
 In order to address these questions, we will be covering key concepts in NLP, 
 including tokenization and stemming. Along the way you will consolidate your 
 Python knowl-edge and learn about strings, files, and regular expressions. Since so 
 much text on the Web is in HTML format, we will also see how to dispense with 
 markup.
  
  
 Important:
  From this chapter onwards, our program samples will 
 as-sume you begin your interactive session or your program with 
 the fol-lowing import statements:
  
 >>> from __future__ import division 
  
 >>> import nltk, re, pprint
  
 79",NA
3.1  Accessing Text from the Web and from ,NA,NA
Disk,NA,NA
Electronic Books,"A small sample of texts from Project Gutenberg appears in the NLTK corpus 
 collection. However, you may be interested in analyzing other texts from Project 
 Gutenberg. You can browse the catalog of 25,000 free online books at 
 http://www.gutenberg.org/cata log/
 , and obtain a URL to an ASCII text file. 
 Although 90% of the texts in Project Gutenberg are in English, it includes material 
 in over 50 other languages, including Catalan, Chinese, Dutch, Finnish, French, 
 German, Italian, Portuguese, and Spanish (with more than 100 texts each).
  
 Text number 2554 is an English translation of 
 Crime and Punishment
 , and we can 
 access it as follows.
  
 >>> from urllib import urlopen 
  
 >>> url = ""http://www.gutenberg.org/files/2554/2554.txt"" 
  
 >>> raw = urlopen(url).read() 
  
 >>> type(raw) 
  
 <type 'str'> 
  
 >>> len(raw) 
  
 1176831 
  
 >>> raw[:75] 
  
 'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n'
  
  
 The 
 read()
  process will take a few seconds as it downloads this large 
 book. If you’re using an Internet proxy that is not correctly detected 
 by Python, you may need to specify the proxy manually as follows:
  
 >>> proxies = {'http': 'http://www.someproxy.com:3128'} >>> 
 raw = urlopen(url, proxies=proxies).read()
  
 The variable 
 raw
  contains a string with 1,176,831 characters. (We can see that it is 
 a string, using 
 type(raw)
 .) This is the raw content of the book, including many 
 details we are not interested in, such as whitespace, line breaks, and blank lines. 
 Notice the 
 \r
  and 
 \n
  in the opening line of the file, which is how Python displays the 
 special carriage return and line-feed characters (the file must have been created on 
 a Windows ma-chine). For our language processing, we want to break up the string 
 into words and punctuation, as we saw in Chapter 1. This step is called 
 tokenization
 , and it produces our familiar structure, a list of words and 
 punctuation.
  
 >>> tokens = nltk.word_tokenize(raw) 
  
 >>> type(tokens) 
  
 <type 'list'> 
  
 >>> len(tokens) 
  
 255809 
  
 >>> tokens[:10] 
  
 ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']",NA
Dealing with HTML,"Much of the text on the Web is in the form of HTML documents. You can use a web 
 browser to save a page as text to a local file, then access this as described in the 
 later section on files. However, if you’re going to do this often, it’s easiest to get 
 Python to do the work directly. The first step is the same as before, using 
 urlopen
 . 
 For fun we’ll
  
 3.1  Accessing Text from the Web and from Disk | 81",NA
Processing Search Engine Results,"The Web can be thought of as a huge corpus of unannotated text. Web search 
 engines provide an efficient means of searching this large quantity of text for 
 relevant linguistic examples. The main advantage of search engines is size: since 
 you are searching such a large set of documents, you are more likely to find any 
 linguistic pattern you are interested in. Furthermore, you can make use of very 
 specific patterns, which would match only one or two examples on a smaller 
 example, but which might match tens of thousands of examples when run on the 
 Web. A second advantage of web search en-gines is that they are very easy to use. 
 Thus, they provide a very convenient tool for quickly checking a theory, to see if it 
 is reasonable. See 
 Table 3-1
  for an example.
  
 82 | Chapter 3:Processing Raw Text",NA
Processing RSS Feeds,"The blogosphere is an important source of text, in both formal and informal 
 registers. With the help of a third-party Python library called the 
 Universal Feed 
 Parser
 , freely downloadable from 
 http://feedparser.org/
 , we can access the content 
 of a blog, as shown here:
  
 >>> import feedparser 
  
 >>> llog = feedparser.parse(""http://languagelog.ldc.upenn.edu/nll/?feed=atom"") >>> 
 llog['feed']['title'] 
  
 u'Language Log' 
  
 >>> len(llog.entries) 
  
 15 
  
 >>> post = llog.entries[2] 
  
 >>> post.title 
  
 u""He's My BF"" 
  
 >>> content = post.content[0].value 
  
 >>> content[:70] 
  
 u'<p>Today I was chatting with three of our visiting graduate students f' 
  
 >>> nltk.word_tokenize(nltk.html_clean(content)) 
  
 >>> nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value)) 
  
 [u'Today', u'I', u'was', u'chatting', u'with', u'three', u'of', u'our', u'visiting', u'graduate', u'students', 
 u'from', u'the', u'PRC', u'.', u'Thinking', u'that', u'I',
  
 3.1  Accessing Text from the Web and from Disk | 83",NA
Reading Local Files,"In order to read a local file, we need to use Python’s built-in 
 open()
  function, 
 followed by the 
 read()
  method. Supposing you have a file 
 document.txt
 , you can load 
 its contents like this:
  
 >>> f = open('document.txt') 
  
 >>> raw = f.read()
  
  
 Your Turn:
  Create a file called 
 document.txt
  using a text editor, and 
 type in a few lines of text, and save it as plain text. If you are using 
 IDLE, select the New Window command in the File menu, typing the 
 required text into this window, and then saving the file as 
 document.txt
  inside
  
 the directory that IDLE offers in the pop-up dialogue box. Next, in 
 the Python interpreter, open the file using 
 f = open('document.txt')
 , 
 then inspect its contents using 
 print f.read()
 .
  
 Various things might have gone wrong when you tried this. If the interpreter 
 couldn’t find your file, you would have seen an error like this:
  
 >>> f = open('document.txt') 
  
 Traceback (most recent call last): 
  
 File ""<pyshell#7>"", line 1, in -toplevel-
  
 f = open('document.txt') 
  
 IOError: [Errno 2] No such file or directory: 'document.txt'
  
 To check that the file that you are trying to open is really in the right directory, use 
 IDLE’s Open command in the File menu; this will display a list of all the files in the 
 directory where IDLE is running. An alternative is to examine the current directory 
 from within Python:
  
 >>> import os 
  
 >>> os.listdir('.')
  
 Another possible problem you might have encountered when accessing a text file is 
 the newline conventions, which are different for different operating systems. The 
 built-in 
 open()
  function has a second parameter for controlling how the file is 
 opened: 
 open('do cument.txt', 'rU'). 'r'
  means to open the file for reading (the default), 
 and 
 'U'
  stands for “Universal”, which lets us ignore the different conventions used 
 for marking new-lines.
  
 Assuming that you can open the file, there are several methods for reading it. The 
 read()
  method creates a string with the contents of the entire file:
  
 84 | Chapter 3:Processing Raw Text",NA
"Extracting Text from PDF, MSWord, and Other ",NA,NA
Binary Formats,"ASCII text and HTML text are human-readable formats. Text often comes in binary 
 formats—such as PDF and MSWord—that can only be opened using specialized 
 soft-ware. Third-party libraries such as 
 pypdf
  and 
 pywin32
  provide access to these 
 formats. Extracting text from multicolumn documents is particularly challenging. 
 For one-off conversion of a few documents, it is simpler to open the document with 
 a suitable application, then save it as text to your local drive, and access it as 
 described below. If the document is already on the Web, you can enter its URL in 
 Google’s search box. The search result often includes a link to an HTML version of 
 the document, which you can save as text.",NA
Capturing User Input,"Sometimes we want to capture the text that a user inputs when she is interacting 
 with our program. To prompt the user to type a line of input, call the Python 
 function 
 raw_input()
 . After saving the input to a variable, we can manipulate it just 
 as we have done for other strings.
  
 >>> s = raw_input(""Enter some text: "") 
  
 Enter some text: On an exceptionally hot evening early in July >>> print 
 ""You typed"", len(nltk.word_tokenize(s)), ""words."" You typed 8 words.
  
 3.1  Accessing Text from the Web and from Disk | 85",NA
The NLP Pipeline,"Figure 3-1
  summarizes what we have covered in this section, including the process 
 of building a vocabulary that we saw in Chapter 1. (One step, normalization, will be 
 discussed in 
 Section 3.6
 .)
  
  
 Figure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the 
 markup and select a slice of characters; this is then tokenized and optionally converted into an 
 nltk.Text 
 object; we can also lowercase all the words and extract the vocabulary.
  
 There’s a lot going on in this pipeline. To understand it properly, it helps to be clear 
 about the type of each variable that it mentions. We find out the type of any Python 
 object 
 x
  using 
 type(
 x
 )
 ; e.g., 
 type(1)
  is 
 <int>
  since 
 1
  is an integer.
  
 When we load the contents of a URL or file, and when we strip out HTML markup, 
 we are dealing with strings, Python’s 
 <str>
  data type (we will learn more about 
 strings in 
 Section 3.2
 ):
  
 >>> raw = open('document.txt').read() 
  
 >>> type(raw) 
  
 <type 'str'>
  
 When we tokenize a string we produce a list (of words), and this is Python’s 
 <list> 
 type. Normalizing and sorting lists produces other lists:
  
 >>> tokens = nltk.word_tokenize(raw) 
  
 >>> type(tokens) 
  
 <type 'list'> 
  
 >>> words = [w.lower() for w in tokens] 
  
 >>> type(words) 
  
 <type 'list'> 
  
 >>> vocab = sorted(set(words)) 
  
 >>> type(vocab) 
  
 <type 'list'>
  
 The type of an object determines what operations you can perform on it. So, for ex-
 ample, we can append to a list but not to a string:
  
 86 | Chapter 3:Processing Raw Text",NA
3.2  Strings: Text Processing at the Lowest ,NA,NA
Level,"It’s time to study a fundamental data type that we’ve been studiously avoiding so 
 far. In earlier chapters we focused on a text as a list of words. We didn’t look too 
 closely at words and how they are handled in the programming language. By using 
 NLTK’s corpus interface we were able to ignore the files that these texts had come 
 from. The contents of a word, and of a file, are represented by programming 
 languages as a fun-damental data type known as a 
 string
 . In this section, we 
 explore strings in detail, and show the connection between strings, words, texts, 
 and files.",NA
Basic Operations with Strings,"Strings are specified using single quotes 
  
  or double quotes 
  
 , as shown in the fol-
  
 lowing code example. If a string contains a single quote, we must backslash-escape 
 the quote  so Python knows a literal quote character is intended, or else put the 
 string in double quotes . Otherwise, the quote inside the string  will be interpreted 
 as a close quote, and the Python interpreter will report a syntax error:
  
 >>> monty = 'Monty Python' 
  
 >>> monty 
  
 'Monty Python' 
  
 >>> circus = ""Monty Python's Flying Circus"" 
  
 >>> circus 
  
 ""Monty Python's Flying Circus"" 
  
 >>> circus = 'Monty Python\'s Flying Circus' 
  
 >>> circus 
  
 ""Monty Python's Flying Circus"" 
  
 >>> circus = 'Monty Python's Flying Circus' 
  
  File ""<stdin>"", line 1
  
   
  circus = 'Monty Python's Flying Circus'
  
    
  ^ 
  
 SyntaxError: invalid syntax",NA
Printing Strings,"So far, when we have wanted to look at the contents of a variable or see the result 
 of a calculation, we have just typed the variable name into the interpreter. We can 
 also see the contents of a variable using the 
 print
  statement:
  
 >>> print monty 
  
 Monty Python
  
 Notice that there are no quotation marks this time. When we inspect a variable by 
 typing its name in the interpreter, the interpreter prints the Python representation 
 of its value. Since it’s a string, the result is quoted. However, when we tell the 
 interpreter to 
 print
  the contents of the variable, we don’t see quotation characters, 
 since there are none inside the string.
  
 The 
 print
  statement allows us to display more than one item on a line in various 
 ways, as shown here:
  
 >>> grail = 'Holy Grail' 
  
 >>> print monty + grail 
  
 Monty PythonHoly Grail 
  
 >>> print monty, grail 
  
 Monty Python Holy Grail 
  
 >>> print monty, ""and the"", grail 
  
 Monty Python and the Holy Grail",NA
Accessing Individual Characters,"As we saw in 
 Section 1.2
  for lists, strings are indexed, starting from zero. When we 
 index a string, we get one of its characters (or letters). A single character is nothing 
 special—it’s just a string of length 
 1
 .
  
 >>> monty[0] 
  
 'M' 
  
 >>> monty[3] 
  
 't' 
  
 >>> monty[5] 
  
 ' '
  
 3.2  Strings: Text Processing at the Lowest Level | 89",NA
Accessing Substrings,"A substring is any continuous section of a string that we want to pull out for further 
 processing. We can easily access substrings using the same slice notation we used 
 for lists (see 
 Figure 3-2
 ). For example, the following code accesses the substring 
 starting at index 
 6
 , up to (but not including) index 
 10
 :
  
 >>> monty[6:10] 
  
 'Pyth'
  
 90 | Chapter 3:Processing Raw Text",NA
More Operations on Strings,"Python has comprehensive support for processing strings. A summary, including 
 some operations we haven’t seen yet, is shown in 
 Table 3-2
 . For more information 
 on strings, type 
 help(str)
  at the Python prompt.
  
 Table 3-2. Useful string methods: Operations on strings in addition to the string tests shown 
 in 
 Table 1-4
 ; all methods produce a new string or list
  
  
  
 Method
  
 Functionality
  
  
 s.find(t) 
  
 s.rfind(t) 
  
 s.index(t) 
  
 s.rindex(t) 
  
 s.join(text) 
  
 s.split(t) 
  
 s.splitlines() 
 s.lower() 
  
 s.upper() 
  
 s.titlecase() 
  
 s.strip() 
  
 s.replace(t, u)
  
 Index of first instance of string 
 t
  inside 
 s
  (
 -1
  if 
 not found) Index of last instance of string 
 t
  
 inside 
 s
  (
 -1
  if not found) Like 
 s.find(t)
 , except 
 it raises 
 ValueError
  if not found Like 
 s.rfind(t)
 , 
 except it raises 
 ValueError
  if not found 
 Combine the words of the text into a string 
 using 
 s
  as the glue Split 
 s
  into a list wherever 
 a 
 t
  is found (whitespace by default) Split 
 s
  
 into a list of strings, one per line 
  
 A lowercased version of the string 
 s 
  
 An uppercased version of the string 
 s 
  
 A titlecased version of the string 
 s 
  
 A copy of 
 s
  without leading or trailing 
 whitespace 
  
 Replace instances of 
 t
  with 
 u
  inside 
 s",NA
The Difference Between Lists and Strings,"Strings and lists are both kinds of 
 sequence
 . We can pull them apart by indexing 
 and slicing them, and we can join them together by concatenating them. However, 
 we can-not join strings and lists:
  
 >>> query = 'Who knows?' 
  
 >>> beatles = ['John', 'Paul', 'George', 'Ringo'] 
  
 >>> query[2] 
  
 'o' 
  
 >>> beatles[2] 
  
 'George' 
  
 >>> query[:2] 
  
 'Wh' 
  
 >>> beatles[:2] 
  
 ['John', 'Paul'] 
  
 >>> query + "" I don't"" 
  
 ""Who knows? I don't"" 
  
 >>> beatles + 'Brian' 
  
 Traceback (most recent call last):
  
  File ""<stdin>"", line 1, in <module> 
  
 TypeError: can only concatenate list (not ""str"") to list >>> beatles 
 + ['Brian'] 
  
 ['John', 'Paul', 'George', 'Ringo', 'Brian']
  
 92 | Chapter 3:Processing Raw Text",NA
3.3  Text Processing with Unicode,"Our programs will often need to deal with different languages, and different 
 character sets. The concept of “plain text” is a fiction. If you live in the English-
 speaking world you probably use ASCII, possibly without realizing it. If you live in 
 Europe you might use one of the extended Latin character sets, containing such 
 characters as “
 ø
 ” for Danish and Norwegian, “
 ő
 ” for Hungarian, “ñ” for Spanish and 
 Breton, and “
 ň
 ” for Czech and Slovak. In this section, we will give an overview of 
 how to use Unicode for processing texts that use non-ASCII character sets.
  
 3.3  Text Processing with Unicode | 93",NA
What Is Unicode?,"Unicode supports over a million characters. Each character is assigned a number, 
 called a 
 code point
 . In Python, code points are written in the form 
 �
 XXXX
 , where 
 XXXX 
 is the number in four-digit hexadecimal form.
  
 Within a program, we can manipulate Unicode strings just like normal strings. 
 How-ever, when Unicode characters are stored in files or displayed on a terminal, 
 they must be encoded as a stream of bytes. Some encodings (such as ASCII and 
 Latin-2) use a single byte per code point, so they can support only a small subset of 
 Unicode, enough for a single language. Other encodings (such as UTF-8) use 
 multiple bytes and can represent the full range of Unicode characters.
  
 Text in files will be in a particular encoding, so we need some mechanism for 
 translating it into Unicode—translation into Unicode is called 
 decoding
 . 
 Conversely, to write out Unicode to a file or a terminal, we first need to translate it 
 into a suitable encoding—this translation out of Unicode is called 
 encoding
 , and is 
 illustrated in 
 Figure 3-3
 .
  
  
 Figure 3-3. Unicode decoding and encoding.
  
 From a Unicode perspective, characters are abstract entities that can be realized as 
 one or more 
 glyphs
 . Only glyphs can appear on a screen or be printed on paper. A 
 font is a mapping from characters to glyphs.",NA
Extracting Encoded Text from Files,"Let’s assume that we have a small text file, and that we know how it is encoded. For 
 example, 
 polish-lat2.txt
 , as the name suggests, is a snippet of Polish text (from the 
 Polish Wikipedia; see 
 http://pl.wikipedia.org/wiki/Biblioteka_Pruska
 ). This file is 
 encoded as Latin-2, also known as ISO-8859-2. The function 
 nltk.data.find()
  locates 
 the file for us.
  
 94 | Chapter 3:Processing Raw Text",NA
Using Your Local Encoding in Python,"If you are used to working with characters in a particular local encoding, you 
 probably want to be able to use your standard methods for inputting and editing 
 strings in a Python file. In order to do this, you need to include the string 
 '# -*- 
 coding: <coding>-*-'
  as the first or second line of your file. Note that 
 <coding>
  has to 
 be a string like 
 'latin-1'
 , 
 'big5'
 , or 
 'utf-8'
  (see 
 Figure 3-4
 ).
  
 Figure 3-4
  also illustrates how regular expressions can use encoded strings.",NA
3.4  Regular Expressions for Detecting ,NA,NA
Word Patterns,"Many linguistic processing tasks involve pattern matching. For example, we can 
 find words ending with 
 ed
  using 
 endswith('ed')
 . We saw a variety of such “word 
 tests” in 
 Table 1-4
 . Regular expressions give us a more powerful and flexible 
 method for de-scribing the character patterns we are interested in.
  
  
 There are many other published introductions to regular 
 expressions, organized around the syntax of regular expressions 
 and applied to searching text files. Instead of doing this again, we 
 focus on the use of regular expressions at different stages of 
 linguistic processing. As usual,
  
 we’ll adopt a problem-based approach and present new features 
 only as they are needed to solve practical problems. In our 
 discussion we will mark regular expressions using chevrons like 
 this: «
 patt
 ».",NA
Using Basic Metacharacters,"Let’s find words ending with 
 ed
  using the regular expression «
 ed$
 ». We will use the 
 re.search(p, s)
  function to check whether the pattern 
 p
  can be found somewhere 
 inside the string 
 s
 . We need to specify the characters of interest, and use the dollar 
 sign, which has a special behavior in the context of regular expressions in that it 
 matches the end of the word:
  
 >>> [w for w in wordlist if re.search('ed$', w)] 
  
 ['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]
  
 The 
 .
 wildcard
  symbol matches any single character. Suppose we have room in a 
 crossword puzzle for an eight-letter word, with 
 j
  as its third letter and 
 t
  as its sixth 
 letter.
  
 In place of each blank cell we use a period:
  
 >>> [w for w in wordlist if re.search('^..j..t..$', w)] 
  
 ['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]
  
 98 | Chapter 3:Processing Raw Text",NA
Ranges and Closures,"The 
 T9
  system is used for entering text on mobile phones (see 
 Figure 3-5
 ). Two or 
 more words that are entered with the same sequence of keystrokes are known as 
 textonyms
 . For example, both 
 hole
  and 
 golf
  are entered by pressing the sequence 
 4653. What other words could be produced with the same sequence? Here we use 
 the regular expression «
 ^[ghi][mno][jlk][def]$
 »:
  
 >>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)] ['gold', 'golf', 
 'hold', 'hole']
  
 The first part of the expression, «
 ^[ghi]
 », matches the start of a word followed by 
 g
 , 
 h
 , or 
 i
 . The next part of the expression, «
 [mno]
 », constrains the second character to 
 be 
 m
 , 
 n
 , or 
 o
 . The third and fourth characters are also constrained. Only four words 
 satisfy all these constraints. Note that the order of characters inside the square 
 brackets is not significant, so we could have written «
 ^[hig][nom][ljk][fed]$
 » and 
 matched the same words.
  
  
 Figure 3-5. T9: Text on 9 keys.
  
  
 Your Turn:
  Look for some “finger-twisters,” by searching for words 
 that use only part of the number-pad. For example «
 ^[ghijklmno]+$
 », 
 or more concisely, «
 ^[g-o]+$
 », will match words that only use keys 4, 
 5, 6 in the center row, and «
 ^[a-fj-o]+$
 » will match words that use 
 keys
  
 2, 3, 5, 6 in the top-right corner. What do 
 -
  and 
 +
  mean?
  
 3.4  Regular Expressions for Detecting Word Patterns | 99",NA
3.5  Useful Applications of Regular ,NA,NA
Expressions,"The previous examples all involved searching for words 
 w
  that match some regular 
 expression 
 regexp
  using 
 re.search(regexp, w)
 . Apart from checking whether a regular 
 expression matches a word, we can use regular expressions to extract material 
 from words, or to modify words in specific ways.",NA
Extracting Word Pieces,"The 
 re.findall()
  (“find all”) method finds all (non-overlapping) matches of the given 
 regular expression. Let’s find all the vowels in a word, then count them:
  
 >>> word = 'supercalifragilisticexpialidocious' 
  
 >>> re.findall(r'[aeiou]', word) 
  
 ['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u'] >>> len(re.findall(r'[aeiou]', word)) 
  
 16
  
 Let’s look for all sequences of two or more vowels in some text, and determine their 
 relative frequency:
  
 >>> wsj = sorted(set(nltk.corpus.treebank.words())) 
  
 >>> fd = nltk.FreqDist(vs for word in wsj 
  
 ...                       for vs in re.findall(r'[aeiou]{2,}', word)) 
  
 >>> fd.items() 
  
 [('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253), ('ee', 217), ('oo', 174), 
 ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95), ('ei', 86), ('oi', 65), ('oa', 59), ('eo', 39), ('iou', 27), 
 ('eu', 18), ...]
  
  
 Your Turn:
  In the W3C Date Time Format, dates are represented 
 like this: 2009-12-31. Replace the 
 ?
  in the following Python code 
 with a regular expression, in order to convert the string 
 '2009-12-31'
  
 to a list of integers 
 [2009, 12, 31]
 :
  
 [int(n) for n in re.findall(?, '2009-12-31')]",NA
Doing More with Word Pieces,"Once we can use 
 re.findall()
  to extract material from words, there are interesting 
 things to do with the pieces, such as glue them back together or plot them.
  
 It is sometimes noted that English text is highly redundant, and it is still easy to 
 read when word-internal vowels are left out. For example, 
 declaration
  becomes 
 dclrtn
 , and 
 inalienable
  becomes 
 inlnble
 , retaining any initial or final vowel 
 sequences. The regular expression in our next example matches initial vowel 
 sequences, final vowel sequences, and all consonants; everything else is ignored. 
 This three-way disjunction is processed left-to-right, and if one of the three parts 
 matches the word, any later parts of the regular expression are ignored. We use 
 re.findall()
  to extract all the matching pieces, and 
 ''.join()
  to join them together (see 
 Section 3.9
  for more about the join operation).",NA
Finding Word Stems,"When we use a web search engine, we usually don’t mind (or even notice) if the 
 words in the document differ from our search terms in having different endings. A 
 query for 
 laptops
  finds documents containing 
 laptop
  and vice versa. Indeed, 
 laptop
  
 and 
 laptops 
 are just two forms of the same dictionary word (or lemma). For some 
 language pro-cessing tasks we want to ignore word endings, and just deal with 
 word stems.
  
 There are various ways we can pull out the stem of a word. Here’s a simple-minded 
 approach that just strips off anything that looks like a suffix:
  
 >>> def stem(word): 
  
 ...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']: ...         if word.endswith(suffix): 
  
 ...             return word[:-len(suffix)] 
  
 ...     return word
  
 Although we will ultimately use NLTK’s built-in stemmers, it’s interesting to see 
 how we can use regular expressions for this task. Our first step is to build up a 
 disjunction of all the suffixes. We need to enclose it in parentheses in order to limit 
 the scope of the disjunction.
  
 >>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing') ['ing']
  
 Here, 
 re.findall()
  just gave us the suffix even though the regular expression matched 
 the entire word. This is because the parentheses have a second function, to select 
 sub-strings to be extracted. If we want to use the parentheses to specify the scope 
 of the disjunction, but not to select the material to be output, we have to add 
 ?:
 , 
 which is just one of many arcane subtleties of regular expressions. Here’s the 
 revised version.
  
 >>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing') ['processing']
  
 However, we’d actually like to split the word into stem and suffix. So we should just 
 parenthesize both parts of the regular expression:
  
 >>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing') [('process', 
 'ing')]
  
 This looks promising, but still has a problem. Let’s look at a different word, 
 processes
 :
  
 >>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes') [('processe', 's')]
  
 The regular expression incorrectly found an 
 -s
  suffix instead of an 
 -es
  suffix. This 
 dem-onstrates another subtlety: the star operator is “greedy” and so the 
 .*
  part of 
 the ex-pression tries to consume as much of the input as possible. If we use the 
 “non-greedy”version of the star operator, written 
 *?
 , we get what we want:
  
 104 | Chapter 3:Processing Raw Text",NA
Searching Tokenized Text,"You can use a special kind of regular expression for searching across multiple 
 words in a text (where a text is a list of tokens). For example, 
 ""<a> <man>""
  finds all 
 instances of 
 a man
  in the text. The angle brackets are used to mark token 
 boundaries, and any whitespace between the angle brackets is ignored (behaviors 
 that are unique to NLTK’s 
 findall()
  method for texts). In the following example, we 
 include 
 <.*>
  , which will match any single token, and enclose it in parentheses so 
 only the matched word (e.g., 
 monied
 ) and not the matched phrase (e.g., 
 a monied 
 man
 ) is produced. The second example finds three-word phrases ending with the 
 word 
 bro
  . The last example finds sequences of three or more words starting with 
 the letter 
 l
  .
  
 >>> from nltk.corpus import gutenberg, nps_chat 
  
 >>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt')) >>> 
 moby.findall(r""<a> (<.*>) <man>"") 
  
 monied; nervous; dangerous; white; white; white; pious; queer; good; mature; 
 white; Cape; great; wise; wise; butterless; white; fiendish; pale; furious; better; 
 certain; complete; dismasted; younger; brave; brave; brave; brave 
  
 >>> chat = nltk.Text(nps_chat.words()) 
  
 >>> chat.findall(r""<.*> <.*> <bro>"") 
  
 you rule bro; telling you bro; u twizted bro
  
 3.5  Useful Applications of Regular Expressions | 105",NA
3.6  Normalizing Text,"In earlier program examples we have often converted text to lowercase before 
 doing anything with its words, e.g., 
 set(w.lower() for w in text)
 . By using 
 lower()
 , we 
 have 
 normalized
  the text to lowercase so that the distinction between 
 The
  and 
 the
  
 is ignored. Often we want to go further than this and strip off any affixes, a task 
 known as stem-ming. A further step is to make sure that the resulting form is a 
 known word in a dictionary, a task known as lemmatization. We discuss each of 
 these in turn. First, we need to define the data we will use in this section:
  
 >>> raw = """"""DENNIS: Listen, strange women lying in ponds distributing swords ... is no basis 
 for a system of government.  Supreme executive power derives from ... a mandate from the 
 masses, not from some farcical aquatic ceremony."""""" >>> tokens = nltk.word_tokenize(raw)",NA
Stemmers,"NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you 
 should use one of these in preference to crafting your own using regular 
 expressions, since NLTK’s stemmers handle a wide range of irregular cases. The 
 Porter and Lan-caster stemmers follow their own rules for stripping affixes. 
 Observe that the Porter stemmer correctly handles the word 
 lying
  (mapping it to 
 lie
 ), whereas the Lancaster stemmer does not.
  
 >>> porter = nltk.PorterStemmer() 
  
 >>> lancaster = nltk.LancasterStemmer() 
  
 >>> [porter.stem(t) for t in tokens] 
  
 ['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 
  
 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Suprem', 'execut', 'power', 
 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 
 'ceremoni', '.'] >>> [lancaster.stem(t) for t in tokens] 
  
 ['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 
 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', 
 ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']
  
 Stemming is not a well-defined process, and we typically pick the stemmer that 
 best suits the application we have in mind. The Porter Stemmer is a good choice if 
 you are indexing some texts and want to support search using alternative forms of 
 words (il-lustrated in 
 Example 3-1
 , which uses 
 object-oriented
  programming 
 techniques that are outside the scope of this book, string formatting techniques to 
 be covered in 
 Sec-tion 3.9
 , and the 
 enumerate()
  function to be explained in 
 Section 
 4.2
 ).
  
 Example 3-1. Indexing a text using a stemmer.
  
 class IndexedText(object):
  
  def __init__(self, stemmer, text):
  
  self._text = text
  
  self._stemmer = stemmer
  
 3.6  Normalizing Text | 107",NA
Lemmatization,"The WordNet lemmatizer removes affixes only if the resulting word is in its 
 dictionary. This additional checking process makes the lemmatizer slower than the 
 stemmers just mentioned. Notice that it doesn’t handle 
 lying
 , but it converts 
 women
  
 to 
 woman
 .
  
 >>> wnl = nltk.WordNetLemmatizer() 
  
 >>> [wnl.lemmatize(t) for t in tokens] 
  
 ['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 
 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 
 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 
 'aquatic', 'ceremony', '.']
  
 The WordNet lemmatizer is a good choice if you want to compile the vocabulary of 
 some texts and want a list of valid lemmas (or lexicon headwords).
  
  
 Another normalization task involves identifying 
 non-standard 
 words
 , including numbers, abbreviations, and dates, and mapping 
 any such tokens to a special vocabulary. For example, every decimal 
 number could be mapped to a single token 
 0.0
 , and every acronym 
 could be
  
 mapped to 
 AAA
 . This keeps the vocabulary small and improves the ac-
 curacy of many language modeling tasks.
  
 108 | Chapter 3:Processing Raw Text",NA
3.7  Regular Expressions for Tokenizing ,NA,NA
Text,"Tokenization is the task of cutting a string into identifiable linguistic units that 
 consti-tute a piece of language data. Although it is a fundamental task, we have 
 been able to delay it until now because many corpora are already tokenized, and 
 because NLTK includes some tokenizers. Now that you are familiar with regular 
 expressions, you can learn how to use them to tokenize text, and to have much 
 more control over the process.",NA
Simple Approaches to Tokenization,"The very simplest method for tokenizing text is to split on whitespace. Consider the 
 following text from 
 Alice’s Adventures in Wonderland
 :
  
 >>> raw = """"""'When I'M a Duchess,' she said to herself, (not in a very hopeful tone ... though), 'I 
 won't have any pepper in my kitchen AT ALL. Soup does very 
  
 ... well without--Maybe it's always pepper that makes people hot-tempered,'...""""""
  
 We could split this raw text on whitespace using 
 raw.split()
 . To do the same using a 
 regular expression, it is not enough to match any space characters in the string , 
 since this results in tokens that contain a 
 \n
  newline character; instead, we need to 
 match any number of spaces, tabs, or newlines :
  
 >>> re.split(r' ', raw) 
  
 [""'When"", ""I'M"", 'a', ""Duchess,'"", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 
 'tone\nthough),', ""'I"", ""won't"", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 
 'very\nwell', 'without--Maybe', ""it's"", 'always', 'pepper', 'that', 'makes', 'people', ""hot-tempered,'...""] 
  
 >>> re.split(r'[ \t\n]+', raw) 
  
 [""'When"", ""I'M"", 'a', ""Duchess,'"", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 
 'though),', ""'I"", ""won't"", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 
 'well', 'without--Maybe', ""it's"", 'always', 'pepper', 'that', 'makes', 'people', ""hot-tempered,'...""]
  
 The regular expression «
 [ \t\n]+
 » matches one or more spaces, tabs (
 \t
 ), or 
 newlines (
 \n
 ). Other whitespace characters, such as carriage return and form feed, 
 should really be included too. Instead, we will use a built-in 
 re
  abbreviation, 
 \s
 , 
 which means any whitespace character. The second statement in the preceding 
 example can be rewritten as 
 re.split(r'\s+', raw)
 .
  
  
 Important:
  Remember to prefix regular expressions with the letter 
 r 
 (meaning “raw”), which instructs the Python interpreter to treat 
 the string literally, rather than processing any backslashed 
 characters it contains.
  
 Splitting on whitespace gives us tokens like 
 '(not'
  and 
 'herself,'
 . An alternative is to 
 use the fact that Python provides us with a character class 
 \w
  for word characters, 
 equivalent to 
 [a-zA-Z0-9_]
 . It also defines the complement of this class, 
 \W
 , i.e., all
  
 3.7  Regular Expressions for Tokenizing Text | 109",NA
NLTK’s Regular Expression Tokenizer,"The function 
 nltk.regexp_tokenize()
  is similar to 
 re.findall()
  (as we’ve been using it for 
 tokenization). However, 
 nltk.regexp_tokenize()
  is more efficient for this task, and 
 avoids the need for special treatment of parentheses. For readability we break up 
 the regular expression over several lines and add a comment about each line. The 
 special 
 (?x)
  “verbose flag” tells Python to strip out the embedded whitespace and 
 comments.
  
 >>> text = 'That U.S.A. poster-print costs $12.40...' 
  
 >>> pattern = r'''(?x)    # set flag to allow verbose regexps ...     ([A-Z]\.)+        
 # abbreviations, e.g. U.S.A.
  
 ...   | \w+(-\w+)*        # words with optional internal hyphens ...   | \$?\d+(\.\d+)?%?  
 # currency and percentages, e.g. $12.40, 82% ...   | \.\.\.            # ellipsis 
  
 ...   | [][.,;""'?():-_`]  # these are separate tokens 
  
 ... ''' 
  
 >>> nltk.regexp_tokenize(text, pattern) 
  
 ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
  
 When using the verbose flag, you can no longer use 
 ' '
  to match a space character; 
 use 
 \s
  instead. The 
 regexp_tokenize()
  function has an optional 
 gaps
  parameter. When 
 set to 
 True
 , the regular expression specifies the gaps between tokens, as with 
 re.split()
 .
  
  
 We can evaluate a tokenizer by comparing the resulting tokens with 
 a wordlist, and then report any tokens that don’t appear in the 
 wordlist, using 
 set(tokens).difference(wordlist)
 . You’ll probably want 
 to lowercase all the tokens first.",NA
Further Issues with Tokenization,"Tokenization turns out to be a far more difficult task than you might have expected. 
 No single solution works well across the board, and we must decide what counts as 
 a token depending on the application domain.
  
 When developing a tokenizer it helps to have access to raw text which has been 
 man-ually tokenized, in order to compare the output of your tokenizer with high-
 quality (or",NA
3.8  Segmentation,"This section discusses more advanced concepts, which you may prefer to skip on the 
 first time through this chapter.
  
 Tokenization is an instance of a more general problem of 
 segmentation
 . In this 
 section, we will look at two other instances of this problem, which use radically 
 different tech-niques to the ones we have seen so far in this chapter.",NA
Sentence Segmentation,"Manipulating texts at the level of individual words often presupposes the ability to 
 divide a text into individual sentences. As we have seen, some corpora already 
 provide access at the sentence level. In the following example, we compute the 
 average number of words per sentence in the Brown Corpus:
  
 >>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()) 
 20.250994070456922
  
 In other cases, the text is available only as a stream of characters. Before tokenizing 
 the text into words, we need to segment it into sentences. NLTK facilitates this by 
 including the Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example 
 of its use in segmenting the text of a novel. (Note that if the segmenter’s internal 
 data has been updated by the time you read this, you will see different output.)
  
 >>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle') 
  
 >>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt') 
  
 >>> sents = sent_tokenizer.tokenize(text) 
  
 >>> pprint.pprint(sents[171:181]) 
  
 ['""Nonsense!',
  
  '"" said Gregory, who was very rational when anyone else\nattempted paradox.',
  
  '""Why do all the clerks and navvies in the\nrailway trains look so sad and tired,...', 'I will\ntell you.',
  
  'It is because they know that the train is going right.',
  
  'It\nis because they know that whatever place they have taken a ticket\nfor that ...', 'It is because 
 after they have\npassed Sloane Square they know that the next stat...', 'Oh, their wild rapture!',
  
  'oh,\ntheir eyes like stars and their souls again in Eden, if the next\nstation w...' '""\n\n""It is you who 
 are unpoetical,"" replied the poet Syme.']
  
 112 | Chapter 3:Processing Raw Text",NA
Word Segmentation,"For some writing systems, tokenizing text is made more difficult by the fact that 
 there is no visual representation of word boundaries. For example, in Chinese, the 
 three-character string: 
 爱
 国人
  (ai4 “love” [verb], guo3 “country”, ren2 “person”) 
 could be tokenized as 
 爱
 国
  / 
 人
 , “country-loving person,” or as 
 爱
  / 
 国人
 , “love 
 country-person.”
  
 A similar problem arises in the processing of spoken language, where the hearer 
 must segment a continuous speech stream into individual words. A particularly 
 challenging version of this problem arises when we don’t know the words in 
 advance. This is the problem faced by a language learner, such as a child hearing 
 utterances from a parent. Consider the following artificial example, where word 
 boundaries have been removed:
  
 (1) a. doyouseethekitty
  
 b. seethedoggy
  
 c. doyoulikethekitty
  
 d. likethedoggy
  
 Our first challenge is simply to represent the problem: we need to find a way to 
 separate text content from the segmentation. We can do this by annotating each 
 character with a boolean value to indicate whether or not a word-break appears 
 after the character (an idea that will be used heavily for “chunking” in Chapter 7). 
 Let’s assume that the learner is given the utterance breaks, since these often 
 correspond to extended pauses. Here is a possible representation, including the 
 initial and target segmentations:
  
 >>> text = ""doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"" >>> 
 seg1 = ""0000000000000001000000000010000000000000000100000000000"" 
 >>> seg2 = 
 ""0100100100100001001001000010100100010010000100010010000""
  
 Observe that the segmentation strings consist of zeros and ones. They are one 
 character shorter than the source text, since a text of length 
 n
  can be broken up in 
 only 
 n–
 1 places. The 
 segment()
  function in 
 Example 3-2
  demonstrates that we can 
 get back to the orig-inal segmented text from its representation.",NA
3.9  Formatting: From Lists to Strings,"Often we write a program to report a single data item, such as a particular element 
 in a corpus that meets some complicated criterion, or a single summary statistic 
 such as a word-count or the performance of a tagger. More often, we write a 
 program to produce a structured result; for example, a tabulation of numbers or 
 linguistic forms, or a re-formatting of the original data. When the results to be 
 presented are linguistic, textual output is usually the most natural choice. However, 
 when the results are numerical, it may be preferable to produce graphical output. 
 In this section, you will learn about a variety of ways to present program output.",NA
From Lists to Strings,"The simplest kind of structured object we use for text processing is lists of words. 
 When we want to output these to a display or a file, we must convert these lists into 
 strings. To do this in Python we use the 
 join()
  method, and specify the string to be 
 used as the“glue”:
  
 >>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.'] >>> ' '.join(silly) 
  
 'We called him Tortoise because he taught us .' 
  
 >>> ';'.join(silly) 
  
 'We;called;him;Tortoise;because;he;taught;us;.' 
  
 >>> ''.join(silly) 
  
 'WecalledhimTortoisebecausehetaughtus.'
  
 So 
 ' '.join(silly)
  means: take all the items in 
 silly
  and concatenate them as one big 
 string, using 
 ' '
  as a spacer between the items. I.e., 
 join()
  is a method of the string 
 that you want to use as the glue. (Many people find this notation for 
 join()
  counter-
 intuitive.) The 
 join()
  method only works on a list of strings—what we have been 
 calling a text—a complex type that enjoys some privileges in Python.
  
 116 | Chapter 3:Processing Raw Text",NA
Strings and Formats,"We have seen that there are two ways to display the contents of an object:
  
 >>> word = 'cat' 
  
 >>> sentence = """"""hello 
  
 ... world"""""" 
  
 >>> print word 
  
 cat 
  
 >>> print sentence 
  
 hello 
  
 world 
  
 >>> word 
  
 'cat' 
  
 >>> sentence 
  
 'hello\nworld'
  
 The 
 print
  command yields Python’s attempt to produce the most human-readable 
 form of an object. The second method—naming the variable at a prompt—shows us 
 a string that can be used to recreate this object. It is important to keep in mind that 
 both of these are just strings, displayed for the benefit of you, the user. They do not 
 give us any clue as to the actual internal representation of the object.
  
 There are many other useful ways to display an object as a string of characters. This 
 may be for the benefit of a human reader, or because we want to 
 export
  our data to 
 a particular file format for use in an external program.
  
 Formatted output typically contains a combination of variables and pre-specified 
 strings. For example, given a frequency distribution 
 fdist
 , we could do:
  
 >>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat']) >>> for word in fdist: 
  
 ...     print word, '->', fdist[word], ';', 
  
 dog -> 4 ; cat -> 3 ; snake -> 1 ;
  
 Apart from the problem of unwanted whitespace, print statements that contain 
 alter-nating variables and constants can be difficult to read and maintain. A better 
 solution is to use 
 string formatting expressions
 .
  
 >>> for word in fdist: 
  
 ...    print '%s->%d;' % (word, fdist[word]), 
  
 dog->4; cat->3; snake->1;
  
 To understand what is going on here, let’s test out the string formatting expression 
 on its own. (By now this will be your usual method of exploring new syntax.)
  
 >>> '%s->%d;' % ('cat', 3) 
  
 'cat->3;' 
  
 >>> '%s->%d;' % 'cat' 
  
 Traceback (most recent call last):
  
  File ""<stdin>"", line 1, in <module> 
  
 TypeError: not enough arguments for format string
  
 3.9  Formatting: From Lists to Strings | 117",NA
Lining Things Up,"So far our formatting strings generated output of arbitrary width on the page (or 
 screen), such as 
 %s
  and 
 %d
 . We can specify a width as well, such as 
 %6s
 , producing 
 a string that is padded to width 6. It is right-justified by default , but we can include 
 a minus sign to make it left-justified 
 . In case we don’t know in advance how wide 
 a displayed value should be, the width value can be replaced with a star in the 
 formatting string, then specified using a variable .
  
 >>> '%6s' % 'dog' 
  
 '   dog' 
  
 >>> '%-6s' % 'dog' 
  
 'dog   ' 
  
 >>> width = 6 
  
 >>> '%-*s' % (width, 'dog') 
  
 'dog   '
  
 118 | Chapter 3:Processing Raw Text",NA
Writing Results to a File,"We have seen how to read text from files (
 Section 3.1
 ). It is often useful to write 
 output to files as well. The following code opens a file 
 output.txt
  for writing, and 
 saves the program output to the file.
  
 >>> output_file = open('output.txt', 'w') 
  
 >>> words = set(nltk.corpus.genesis.words('english-kjv.txt')) >>> for 
 word in sorted(words): 
  
 ...     output_file.write(word + ""\n"")
  
  
 Your Turn:
  What is the effect of appending 
 \n
  to each string before 
 we write it to the file? If you’re using a Windows machine, you may 
 want to use 
 word + ""\r\n""
  instead. What happens if we do
  
 output_file.write(word)
  
 When we write non-text data to a file, we must convert it to a string first. We can do 
 this conversion using formatting strings, as we saw earlier. Let’s write the total 
 number of words to our file, before closing it.
  
 >>> len(words) 
  
 2789 
  
 >>> str(len(words)) 
  
 '2789' 
  
 >>> output_file.write(str(len(words)) + ""\n"") 
  
 >>> output_file.close()
  
  
 Caution!
  
 You should avoid filenames that contain space characters, such as 
 output file.txt
 , or that are identical except for case distinctions, e.g.,
  
 Output.txt
  and 
 output.TXT
 .",NA
Text Wrapping,"When the output of our program is text-like, instead of tabular, it will usually be 
 nec-essary to wrap it so that it can be displayed conveniently. Consider the 
 following output, which overflows its line, and which uses a complicated 
 print
  
 statement:
  
  
 >>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', ...           'more', 'is', 
 'said', 'than', 'done', '.'] 
  
 >>> for word in saying: 
  
 ...     print word, '(' + str(len(word)) + '),',
  
 After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), 
  
 We can take care of line wrapping with the help of Python’s 
 textwrap
  module. For 
 maximum clarity we will separate each step onto its own line:
  
 >>> from textwrap import fill 
  
 >>> format = '%s (%d),'
  
 120 | Chapter 3:Processing Raw Text",NA
3.10  Summary,"• In this book we view a text as a list of words. A “raw text” is a potentially long 
 string containing words and whitespace formatting, and is how we typically 
 store and visualize a text.
  
 • A string is specified in Python using single or double quotes: 
 'Monty Python'
 , 
  
 ""Monty Python""
 .
  
 • The characters of a string are accessed using indexes, counting from zero: 
 'Monty 
  
 Python'[0]
  gives the value 
 M
 . The length of a string is found using 
 len()
 .
  
 • Substrings are accessed using slice notation: 
 'Monty Python'[1:5]
  gives the value 
 onty
 . If the start index is omitted, the substring begins at the start of the string; 
 if the end index is omitted, the slice continues to the end of the string.
  
 • Strings can be split into lists: 
 'Monty Python'.split()
  gives 
 ['Monty', 'Python']
 . Lists 
 can be joined into strings: 
 '/'.join(['Monty', 'Python'])
  gives 
 'Monty/ Python'
 .
  
 • We can read text from a file 
 f
  using 
 text = open(f).read()
 . We can read text from a 
 URL 
 u
  using 
 text = urlopen(u).read()
 . We can iterate over the lines of a text file 
 using 
 for line in open(f)
 .
  
 • Texts found on the Web may contain unwanted material (such as headers, footers, 
 and markup), that need to be removed before we do any linguistic processing.
  
 • Tokenization is the segmentation of a text into basic units—or tokens—such as 
 words and punctuation. Tokenization based on whitespace is inadequate for 
 many applications because it bundles punctuation together with words. NLTK 
 provides an off-the-shelf tokenizer 
 nltk.word_tokenize()
 .
  
 • Lemmatization is a process that maps the various forms of a word (such as 
 ap-
 peared
 , 
 appears
 ) to the canonical or citation form of the word, also known as 
 the lexeme or lemma (e.g., 
 appear
 ).
  
 • Regular expressions are a powerful and flexible method of specifying patterns. 
 Once we have imported the 
 re
  module, we can use 
 re.findall()
  to find all sub-
 strings in a string that match a pattern.
  
 3.10  Summary | 121",NA
3.11  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web. Remember to consult the Python 
 reference ma-terials at 
 http://docs.python.org/
 . (For example, this documentation 
 covers “universal newline support,” explaining how to work with the different 
 newline conventions used by various operating systems.)
  
 For more examples of processing words with NLTK, see the tokenization, 
 stemming, and corpus HOWTOs at 
 http://www.nltk.org/howto
 . Chapters 2 and 3 of 
 (Jurafsky & Martin, 2008) contain more advanced material on regular expressions 
 and morphology. For more extensive discussion of text processing with Python, see 
 (Mertz, 2003). For information about normalizing non-standard words, see (Sproat 
 et al., 2001).
  
 There are many references for regular expressions, both practical and theoretical. 
 For an introductory tutorial to using regular expressions in Python, see Kuchling’s 
 Regular Expression HOWTO
 , 
 http://www.amk.ca/python/howto/regex/
 . For a 
 comprehensive and detailed manual in using regular expressions, covering their 
 syntax in most major programming languages, including Python, see (Friedl, 2002). 
 Other presentations in-clude Section 2.1 of (Jurafsky & Martin, 2008), and Chapter 
 3 of (Mertz, 2003).
  
 There are many online resources for Unicode. Useful discussions of Python’s 
 facilities for handling Unicode are:
  
 • PEP-100 
 http://www.python.org/dev/peps/pep-0100/
  
 • Jason Orendorff, 
 Unicode for Programmers
 , 
 http://www.jorendorff.com/articles/uni 
  
 code/
  
 • A. M. Kuchling, 
 Unicode HOWTO
 , 
 http://www.amk.ca/python/howto/unicode
  
 • Frederik Lundh, 
 Python Unicode Objects
 , 
 http://effbot.org/zone/unicode-objects 
  
 .htm
  
 • Joel Spolsky, 
 The Absolute Minimum Every Software Developer Absolutely, Posi-
 tively Must Know About Unicode and Character Sets (No Excuses!)
 , 
 http://www.joe lonsoftware.com/articles/Unicode.html",NA
3.12  Exercises,"1.
  ○ 
 Define a string 
 s = 'colorless'
 . Write a Python statement that changes this 
 to“colourless” using only the slice and concatenation operations.
  
 2.
  ○ 
 We can use the slice notation to remove morphological endings on words. For 
 example, 
 'dogs'[:-1]
  removes the last character of 
 dogs
 , leaving 
 dog
 . Use slice 
 notation to remove the affixes from these words (we’ve inserted a hyphen to 
 indi-cate the affix boundary, but omit this from your strings): 
 dish-es
 , 
 run-ning
 , 
 nation-ality
 , 
 un-do
 , 
 pre-heat
 .
  
 3.
  ○ 
 We saw how we can generate an 
 IndexError
  by indexing beyond the end of a 
 string. Is it possible to construct an index that goes too far to the left, before the 
 start of the string?
  
 4.
  ○ 
 We can specify a “step” size for the slice. The following returns every second 
 character within the slice: 
 monty[6:11:2]
 . It also works in the reverse direction: 
 monty[10:5:-2]
 . Try these for yourself, and then experiment with different step 
 values.
  
 5.
  ○ 
 What happens if you ask the interpreter to evaluate 
 monty[::-1]
 ? Explain why 
 this is a reasonable result.
  
 6.
  ○ 
 Describe the class of strings matched by the following regular expressions:
  
 a.
  [a-zA-Z]+
  
 b.
  [A-Z][a-z]*
  
 c.
  p[aeiou]{,2}t
  
 d.
  \d+(\.\d+)?
  
 e.
  ([^aeiou][aeiou][^aeiou])*
  
 f.
  \w+|[^\w\s]+
  
 Test your answers using 
 nltk.re_show()
 .
  
 3.12  Exercises | 123",NA
CHAPTER 4,NA,NA
Writing Structured ,NA,NA
Programs,"By now you will have a sense of the capabilities of the Python programming 
 language for processing natural language. However, if you’re new to Python or to 
 programming, you may still be wrestling with Python and not feel like you are in 
 full control yet. In this chapter we’ll address the following questions:
  
 1. How can you write well-structured, readable programs that you and others will 
 be able to reuse easily?
  
 2. How do the fundamental building blocks work, such as loops, functions, and 
 assignment?
  
 3. What are some of the pitfalls with Python programming, and how can you avoid 
 them?
  
 Along the way, you will consolidate your knowledge of fundamental programming 
 constructs, learn more about using features of the Python language in a natural and 
 concise way, and learn some useful techniques in visualizing natural language data. 
 As before, this chapter contains many examples and exercises (and as before, some 
 exer-cises introduce new material). Readers new to programming should work 
 through them carefully and consult other introductions to programming if 
 necessary; experienced programmers can quickly skim this chapter.
  
 In the other chapters of this book, we have organized the programming concepts as 
 dictated by the needs of NLP. Here we revert to a more conventional approach, 
 where the material is more closely tied to the structure of the programming 
 language. There’s not room for a complete presentation of the language, so we’ll 
 just focus on the language constructs and idioms that are most important for NLP.",NA
4.1  Back to the Basics,NA,NA
Assignment,"Assignment would seem to be the most elementary programming concept, not 
 deserv-ing a separate discussion. However, there are some surprising subtleties 
 here. Consider the following code fragment:
  
 >>> foo = 'Monty' 
  
 >>> bar = foo >>> foo = 
 'Python' 
  
 >>> bar 
  
 'Monty'
  
 This behaves exactly as expected. When we write 
 bar = foo
  in the code 
 , the value 
 of 
 foo
  (the string 
 'Monty'
 ) is assigned to 
 bar
 . That is, 
 bar
  is a 
 copy
  of 
 foo
 , so when we 
 overwrite 
 foo
  with a new string 
 'Python'
  on line , the value of 
 bar
  is not affected.
  
 However, assignment statements do not always involve making copies in this way. 
 Assignment always copies the value of an expression, but a value is not always 
 what you might expect it to be. In particular, the “value” of a structured object such 
 as a list is actually just a 
 reference
  to the object. In the following example, 
  assigns 
 the refer-ence of 
 foo
  to the new variable 
 bar
 . Now when we modify something 
 inside 
 foo
  on line
  
  
 , we can see that the contents of 
 bar
  have also been changed.
  
 >>> foo = ['Monty', 'Python'] 
  
 >>> bar = foo 
  
 >>> foo[1] = 'Bodkin' 
  
 >>> bar 
  
 ['Monty', 'Bodkin']
  
 The line 
 bar = foo
   does not copy the contents of the variable, only its “object refer-
 ence.” To understand what is going on here, we need to know how lists are stored 
 in the computer’s memory. In 
 Figure 4-1
 , we see that a list 
 foo
  is a reference to an 
 object stored at location 3133 (which is itself a series of pointers to other locations 
 holding strings). When we assign 
 bar = foo
 , it is just the object reference 3133 that 
 gets copied. This behavior extends to other aspects of the language, such as 
 parameter passing (
 Section 4.4
 ).
  
 130 | Chapter 4:Writing Structured Programs",NA
Equality,"Python provides two ways to check that a pair of items are the same. The 
 is
  
 operator tests for object identity. We can use it to verify our earlier observations 
 about objects. First, we create a list containing several copies of the same object, 
 and demonstrate that they are not only identical according to 
 ==
 , but also that they 
 are one and the same object:
  
 >>> size = 5 
  
 >>> python = ['Python'] 
  
 >>> snake_nest = [python] * size 
  
 >>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4] True 
  
 >>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4] True
  
 Now let’s put a new python in this nest. We can easily show that the objects are not 
 all identical:
  
 >>> import random 
  
 >>> position = random.choice(range(size)) 
  
 >>> snake_nest[position] = ['Python'] 
  
 >>> snake_nest 
  
 [['Python'], ['Python'], ['Python'], ['Python'], ['Python']] 
  
 >>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4] True 
  
 >>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4] False
  
 You can do several pairwise tests to discover which position contains the 
 interloper, but the 
 id()
  function makes detection is easier:
  
 >>> [id(snake) for snake in snake_nest] 
  
 [513528, 533168, 513528, 513528, 513528]
  
 This reveals that the second item of the list has a distinct identifier. If you try 
 running this code snippet yourself, expect to see different numbers in the resulting 
 list, and don’t be surprised if the interloper is in a different position.
  
 Having two kinds of equality might seem strange. However, it’s really just the type-
 token distinction, familiar from natural language, here showing up in a 
 programming language.
  
 132 | Chapter 4:Writing Structured Programs",NA
Conditionals,"In the condition part of an 
 if
  statement, a non-empty string or list is evaluated as 
 true, while an empty string or list evaluates as false.
  
 >>> mixed = ['cat', '', ['dog'], []] 
  
 >>> for element in mixed: 
  
 ...     if element: 
  
 ...         print element 
  
 ...
  
 cat 
  
 ['dog']
  
 That is, we 
 don’t
  need to say 
 if len(element) > 0:
  in the condition.
  
 What’s the difference between using 
 if...elif
  as opposed to using a couple of 
 if 
 statements in a row? Well, consider the following situation:
  
 >>> animals = ['cat', 'dog'] 
  
 >>> if 'cat' in animals: 
  
 ...     print 1 
  
 ... elif 'dog' in animals: 
  
 ...     print 2 
  
 ...
  
 1
  
 Since the 
 if
  clause of the statement is satisfied, Python never tries to evaluate the 
 elif
  clause, so we never get to print out 
 2
 . By contrast, if we replaced the 
 elif
  by an 
 if
 , 
 then we would print out both 
 1
  and 
 2
 . So an 
 elif
  clause potentially gives us more 
 information than a bare 
 if
  clause; when it evaluates to true, it tells us not only that 
 the condition is satisfied, but also that the condition of the main 
 if
  clause was 
 not
  
 satisfied.
  
 The functions 
 all()
  and 
 any()
  can be applied to a list (or other sequence) to check 
 whether all or any items meet some condition:
  
 >>> sent = ['No', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '.'] >>> all(len(w) > 4 for w 
 in sent) 
  
 False 
  
 >>> any(len(w) > 4 for w in sent) 
  
 True",NA
4.2  Sequences,"So far, we have seen two kinds of sequence object: strings and lists. Another kind of 
 sequence is called a 
 tuple
 . Tuples are formed with the comma operator , and 
 typically enclosed using parentheses. We’ve actually seen them in the previous 
 chapters, and sometimes referred to them as “pairs,” since there were always two 
 members. However, tuples can have any number of members. Like lists and strings, 
 tuples can be indexed
  
  
  and sliced , and have a length . 
  
  
 >>> t = 'walk', 'fem', 3 
  
 >>> t 
  
 ('walk', 'fem', 3)",NA
Operating on Sequence Types,"We can iterate over the items in a sequence 
 s
  in a variety of useful ways, as shown 
 in 
 Table 4-1
 .
  
 Table 4-1. Various ways to iterate over sequences
  
  
  
 Python expression
  
 Comment
  
  
 for item in s 
  
 for item in sorted(s) for 
 item in set(s)
  
 Iterate over the items of 
 s 
  
 Iterate over the items of 
 s
  in order Iterate over 
 unique elements of 
 s",NA
Combining Different Sequence Types,"Let’s combine our knowledge of these three sequence types, together with list com-
 prehensions, to perform the task of sorting the words in a string by their length.
  
 >>> words = 'I turned off the spectroroute'.split() >>> wordlens = [(len(word), word) for word in 
 words] 
  
 >>> wordlens.sort() 
  
 >>> ' '.join(w for (_, w) in wordlens) 
  
 'I off the turned spectroroute'
  
 Each of the preceding lines of code contains a significant feature. A simple string is 
 actually an object with methods defined on it, such as 
 split()
 . We use a list com-
 prehension to build a list of tuples , where each tuple consists of a number (the 
 word length) and the word, e.g., 
 (3, 'the')
 . We use the 
 sort()
  method  to sort the list in 
 place. Finally, we discard the length information and join the words back into a 
 single string . (The underscore  is just a regular Python variable, but we can use 
 underscore by convention to indicate that we will not use its value.)
  
 We began by talking about the commonalities in these sequence types, but the 
 previous code illustrates important differences in their roles. First, strings appear 
 at the beginning and the end: this is typical in the context where our program is 
 reading in some text and producing output for us to read. Lists and tuples are used 
 in the middle, but for different purposes. A list is typically a sequence of objects all 
 having the 
 same type
 , of 
 arbitrary length
 . We often use lists to hold sequences of 
 words. In contrast, a tuple is typically a collection of objects of 
 different types
 , of 
 fixed length
 . We often use a tuple to hold a 
 record
 , a collection of different 
 fields
  
 relating to some entity. This distinction between the use of lists and tuples takes 
 some getting used to, so here is another example:
  
 136 | Chapter 4:Writing Structured Programs",NA
Generator Expressions,"We’ve been making heavy use of list comprehensions, for compact and readable 
 pro-cessing of texts. Here’s an example where we tokenize and normalize a text:
  
 >>> text = '''""When I use a word,"" Humpty Dumpty said in rather a scornful tone, ... ""it means 
 just what I choose it to mean - neither more nor less.""''' 
  
 >>> [w.lower() for w in nltk.word_tokenize(text)] 
  
 ['""', 'when', 'i', 'use', 'a', 'word', ',', '""', 'humpty', 'dumpty', 'said', ...]",NA
4.3  Questions of Style,"Programming is as much an art as a science. The undisputed “bible” of 
 programming, a 2,500 page multivolume work by Donald Knuth, is called 
 The Art of 
 Computer Pro-gramming
 . Many books have been written on 
 Literate Programming
 , 
 recognizing that humans, not just computers, must read and understand programs. 
 Here we pick up on some issues of programming style that have important 
 ramifications for the readability of your code, including code layout, procedural 
 versus declarative style, and the use of loop variables.",NA
Python Coding Style,"When writing programs you make many subtle choices about names, spacing, com-
 ments, and so on. When you look at code written by other people, needless 
 differences in style make it harder to interpret the code. Therefore, the designers of 
 the Python language have published a style guide for Python code, available at 
 http://www.python .org/dev/peps/pep-0008/
 . The underlying value presented in 
 the style guide is 
 consis-tency
 , for the purpose of maximizing the readability of code. 
 We briefly review some of its key recommendations here, and refer readers to the 
 full guide for detailed dis-cussion with examples.
  
 Code layout should use four spaces per indentation level. You should make sure 
 that when you write Python code in a file, you avoid tabs for indentation, since 
 these can be misinterpreted by different text editors and the indentation can be 
 messed up. Lines should be less than 80 characters long; if necessary, you can break 
 a line inside paren-theses, brackets, or braces, because Python is able to detect that 
 the line continues over to the next line, as in the following examples:
  
 >>> cv_word_pairs = [(cv, w) for w in rotokas_words 
  
 ...                          for cv in re.findall('[ptksvr][aeiou]', w)]
  
 138 | Chapter 4:Writing Structured Programs",NA
Procedural Versus Declarative Style,"We have just seen how the same task can be performed in different ways, with 
 impli-cations for efficiency. Another factor influencing program development is 
 programming style
 . Consider the following program to compute the average length 
 of words in the Brown Corpus:
  
 >>> tokens = nltk.corpus.brown.words(categories='news') >>> 
 count = 0 
  
 >>> total = 0 
  
 >>> for token in tokens: 
  
 ...     count += 1 
  
 ...     total += len(token) 
  
 >>> print total / count 
  
 4.2765382469
  
 In this program we use the variable 
 count
  to keep track of the number of tokens 
 seen, and 
 total
  to store the combined length of all words. This is a low-level style, 
 not far removed from machine code, the primitive operations performed by the 
 computer’s CPU. The two variables are just like a CPU’s registers, accumulating 
 values at many intermediate stages, values that are meaningless until the end. We 
 say that this program is written in a 
 procedural
  style, dictating the machine 
 operations step by step. Now consider the following program that computes the 
 same thing:
  
 4.3  Questions of Style | 139",NA
Some Legitimate Uses for Counters,"There are cases where we still want to use loop variables in a list comprehension. 
 For example, we need to use a loop variable to extract successive overlapping n-
 grams from a list:
  
 >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper'] >>> n = 3 
  
 >>> [sent[i:i+n] for i in range(len(sent)-n+1)] 
  
 [['The', 'dog', 'gave'],
  
  ['dog', 'gave', 'John'],
  
  ['gave', 'John', 'the'],
  
  ['John', 'the', 'newspaper']]
  
 It is quite tricky to get the range of the loop variable right. Since this is a common 
 operation in NLP, NLTK supports it with functions 
 bigrams(text)
  and 
 trigrams(text)
 , 
 and a general-purpose 
 ngrams(text, n)
 .
  
 Here’s an example of how we can use loop variables in building multidimensional 
 structures. For example, to build an array with 
 m
  rows and 
 n
  columns, where each 
 cell is a set, we could use a nested list comprehension:
  
 >>> m, n = 3, 7 
  
 >>> array = [[set() for i in range(n)] for j in range(m)] 
  
 >>> array[2][5].add('Alice') 
  
 >>> pprint.pprint(array) 
  
 [[set([]), set([]), set([]), set([]), set([]), set([]), set([])], [set([]), set([]), set([]), set([]), 
 set([]), set([]), set([])], [set([]), set([]), set([]), set([]), set([]), set(['Alice']), set([])]]
  
 4.3  Questions of Style | 141",NA
4.4  Functions: The Foundation of ,NA,NA
Structured Programming,"Functions provide an effective way to package and reuse program code, as already 
 explained in 
 Section 2.3
 . For example, suppose we find that we often want to read 
 text from an HTML file. This involves several steps: opening the file, reading it in, 
 normal-izing whitespace, and stripping HTML markup. We can collect these steps 
 into a func-tion, and give it a name such as 
 get_text()
 , as shown in 
 Example 4-1
 .
  
 Example 4-1. Read text from a file.
  
 import re 
  
 def get_text(file):
  
  
  """"""Read text from a file, normalizing whitespace and stripping HTML markup.""""""
  
  text = 
 open(file).read()
  
  
  text = re.sub('\s+', ' ', text)
  
  
  text = re.sub(r'<.*?>', ' ', text)
  
  
  return text
  
 Now, any time we want to get cleaned-up text from an HTML file, we can just call 
 get_text()
  with the name of the file as its only argument. It will return a string, and 
 we can assign this to a variable, e.g., 
 contents = get_text(""test.html"")
 . Each time we 
 want to use this series of steps, we only have to call the function.
  
 Using functions has the benefit of saving space in our program. More importantly, 
 our choice of name for the function helps make the program 
 readable
 . In the case of 
 the preceding example, whenever our program needs to read cleaned-up text from 
 a file we don’t have to clutter the program with four lines of code; we simply need 
 to call 
 get_text()
 . This naming helps to provide some “semantic interpretation”—it 
 helps a reader of our program to see what the program “means.”",NA
Function Inputs and Outputs,"We pass information to functions using a function’s parameters, the parenthesized 
 list of variables and constants following the function’s name in the function 
 definition.
  
 Here’s a complete example:
  
 >>> def repeat(msg, num): 
  
 ...     return ' '.join([msg] * num) 
  
 >>> monty = 'Monty Python' 
  
 >>> repeat(monty, 3) 
  
 'Monty Python Monty Python Monty Python'
  
 We first define the function to take two parameters, 
 msg
  and 
 num
 . Then, we call 
 the function and pass it two arguments, 
 monty
  and 
 3
 ; these arguments fill the 
 “place-holders” provided by the parameters and provide values for the occurrences 
 of 
 msg
  and 
 num
  in the function body.
  
 It is not necessary to have any parameters, as we see in the following example:
  
 >>> def monty(): 
  
 ...     return ""Monty Python"" 
  
 >>> monty() 
  
 'Monty Python'",NA
Parameter Passing,"Back in 
 Section 4.1
 , you saw that assignment works on values, but that the value of 
 a structured object is a 
 reference
  to that object. The same is true for functions. 
 Python interprets function parameters as values (this is known as 
 call-by-value
 ). 
 In the fol-lowing code, 
 set_up()
  has two parameters, both of which are modified 
 inside the func-tion. We begin by assigning an empty string to 
 w
  and an empty 
 dictionary to 
 p
 . After calling the function, 
 w
  is unchanged, while 
 p
  is changed:
  
 >>> def set_up(word, properties): 
  
 ...     word = 'lolcat' 
  
 ...     properties.append('noun') 
  
 ...     properties = 5 
  
 ...
  
 >>> w = '' 
  
 >>> p = [] 
  
 >>> set_up(w, p) 
  
 >>> w 
  
 '' 
  
 >>> p 
  
 ['noun']
  
 Notice that 
 w
  was not changed by the function. When we called 
 set_up(w, p)
 , the 
 value of 
 w
  (an empty string) was assigned to a new variable 
 word
 . Inside the 
 function, the value
  
 144 | Chapter 4:Writing Structured Programs",NA
Variable Scope,"Function definitions create a new local 
 scope
  for variables. When you assign to a 
 new variable inside the body of a function, the name is defined only within that 
 function. The name is not visible outside the function, or in other functions. This 
 behavior means you can choose variable names without being concerned about 
 collisions with names used in your other function definitions.
  
 When you refer to an existing name from within the body of a function, the Python 
 interpreter first tries to resolve the name with respect to the names that are local to 
 the function. If nothing is found, the interpreter checks whether it is a global name 
 within the module. Finally, if that does not succeed, the interpreter checks whether 
 the name is a Python built-in. This is the so-called 
 LGB rule
  of name resolution: 
 local, then global, then built-in.
  
  
 Caution!
  
 A function can create a new global variable, using the 
 global
  
 declaration. However, this practice should be avoided as much as 
 possible. Defining
  
 global variables inside a function introduces dependencies on 
 context and limits the portability (or reusability) of the function. In 
 general you should use parameters for function inputs and return 
 values for function outputs.
  
 4.4  Functions: The Foundation of Structured Programming | 145",NA
Checking Parameter Types,"Python does not force us to declare the type of a variable when we write a program, 
 and this permits us to define functions that are flexible about the type of their argu-
 ments. For example, a tagger might expect a sequence of words, but it wouldn’t care 
 whether this sequence is expressed as a list, a tuple, or an iterator (a new sequence 
 type that we’ll discuss later).
  
 However, often we want to write programs for later use by others, and want to 
 program in a defensive style, providing useful warnings when functions have not 
 been invoked correctly. The author of the following 
 tag()
  function assumed that its 
 argument would always be a string.
  
 >>> def tag(word): 
  
 ...     if word in ['a', 'the', 'all']: 
  
 ...         return 'det' 
  
 ...     else: 
  
 ...         return 'noun' 
  
 ...
  
 >>> tag('the') 
  
 'det' 
  
 >>> tag('knight') 
  
 'noun' 
  
 >>> tag([""'Tis"", 'but', 'a', 'scratch']) 
  
 'noun'
  
 The function returns sensible values for the arguments 
 'the'
  and 
 'knight'
 , but look 
 what happens when it is passed a list 
 —it fails to complain, even though the 
 result which it returns is clearly incorrect. The author of this function could take 
 some extra steps to ensure that the 
 word
  parameter of the 
 tag()
  function is a string. 
 A naive ap-proach would be to check the type of the argument using 
 if not 
 type(word) is str
 , and if 
 word
  is not a string, to simply return Python’s special empty 
 value, 
 None
 . This is a slight improvement, because the function is checking the type 
 of the argument, and trying to return a “special” diagnostic value for the wrong 
 input. However, it is also dangerous because the calling program may not detect 
 that 
 None
  is intended as a “spe-cial” value, and this diagnostic return value may 
 then be propagated to other parts of the program with unpredictable 
 consequences. This approach also fails if the word is a Unicode string, which has 
 type 
 unicode
 , not 
 str
 . Here’s a better solution, using an 
 assert
  statement together 
 with Python’s 
 basestring
  type that generalizes over both 
 unicode
  and 
 str
 .
  
 >>> def tag(word): 
  
 ...     assert isinstance(word, basestring), ""argument to tag() must be a string"" ...     if word in ['a', 
 'the', 'all']: 
  
 ...         return 'det' 
  
 ...     else: 
  
 ...         return 'noun'
  
 If the 
 assert
  statement fails, it will produce an error that cannot be ignored, since it 
 halts program execution. Additionally, the error message is easy to interpret. 
 Adding
  
 146 | Chapter 4:Writing Structured Programs",NA
Functional Decomposition,"Well-structured programs usually make extensive use of functions. When a block of 
 program code grows longer than 10–20 lines, it is a great help to readability if the 
 code is broken up into one or more functions, each one having a clear purpose. This 
 is analogous to the way a good essay is divided into paragraphs, each expressing 
 one main idea.
  
 Functions provide an important kind of abstraction. They allow us to group 
 multiple actions into a single, complex action, and associate a name with it. 
 (Compare this with the way we combine the actions of 
 go
  and 
 bring back
  into a 
 single more complex action 
 fetch
 .) When we use functions, the main program can 
 be written at a higher level of abstraction, making its structure transparent, as in 
 the following:
  
 >>> data = load_corpus() 
  
 >>> results = analyze(data) 
  
 >>> present(results)
  
 Appropriate use of functions makes programs more readable and maintainable. 
 Addi-tionally, it becomes possible to reimplement a function—replacing the 
 function’s body with more efficient code—without having to be concerned with the 
 rest of the program.
  
 Consider the 
 freq_words
  function in 
 Example 4-2
 . It updates the contents of a 
 frequency distribution that is passed in as a parameter, and it also prints a list of 
 the 
 n
  most frequent words.
  
 Example 4-2. Poorly designed function to compute frequent words.
  
 def freq_words(url, freqdist, n):
  
  
  text = nltk.clean_url(url)
  
  
  for word in nltk.word_tokenize(text):
  
  
  
  freqdist.inc(word.lower())
  
  
  print freqdist.keys()[:n]
  
 >>> constitution = ""http://www.archives.gov/national-archives-experience"" \ ...                
 ""/charters/constitution_transcript.html"" 
  
 >>> fd = nltk.FreqDist() 
  
 >>> freq_words(constitution, fd, 20) 
  
 ['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',', 
  
 'declaration', 'impact', 'freedom', '-', 'making', 'independence']
  
 This function has a number of problems. The function has two side effects: it 
 modifies the contents of its second parameter, and it prints a selection of the results 
 it has com-puted. The function would be easier to understand and to reuse 
 elsewhere if we initialize the 
 FreqDist()
  object inside the function (in the same place 
 it is populated), and if we moved the selection and display of results to the calling 
 program. In 
 Example 4-3
  we 
 refactor
  this function, and simplify its interface by 
 providing a single 
 url
  parameter.",NA
Documenting Functions,"If we have done a good job at decomposing our program into functions, then it 
 should be easy to describe the purpose of each function in plain language, and 
 provide this in the docstring at the top of the function definition. This statement 
 should not explain how the functionality is implemented; in fact, it should be 
 possible to reimplement the function using a different method without changing 
 this statement.
  
 For the simplest functions, a one-line docstring is usually adequate (see 
 Example 4-
 1
 ). You should provide a triple-quoted string containing a complete sentence on a 
 single line. For non-trivial functions, you should still provide a one-sentence 
 summary on the first line, since many docstring processing tools index this string. 
 This should be fol-lowed by a blank line, then a more detailed description of the 
 functionality 
 (see 
 http:// 
 www.python.org/dev/peps/pep-0257/
  
 for 
 more 
 information on docstring conventions).
  
 Docstrings can include a 
 doctest block
 , illustrating the use of the function and the 
 expected output. These can be tested automatically using Python’s 
 docutils
  module. 
 Docstrings should document the type of each parameter to the function, and the 
 return type. At a minimum, that can be done in plain text. However, note that NLTK 
 uses the“epytext” markup language to document parameters. This format can be 
 automatically converted 
 into 
 richly structured 
 API 
 documentation (see 
 http://www.nltk.org/
 ), and in-cludes special handling of certain “fields,” such as 
 @param
 , which allow the inputs and outputs of functions to be clearly documented. 
 Example 4-4
  illustrates a complete docstring.",NA
4.5  Doing More with Functions,"This section discusses more advanced features, which you may prefer to skip on the 
 first time through this chapter.",NA
Functions As Arguments,"So far the arguments we have passed into functions have been simple objects, such 
 as strings, or structured objects, such as lists. Python also lets us pass a function as 
 an argument to another function. Now we can abstract out the operation, and apply 
 a 
 different operation
  on the 
 same data
 . As the following examples show, we can pass 
 the built-in function 
 len()
  or a user-defined function 
 last_letter()
  as arguments to an-
 other function:
  
 >>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the', ...         'sounds', 'will', 'take', 
 'care', 'of', 'themselves', '.'] >>> def extract_property(prop): 
  
 ...     return [prop(word) for word in sent] 
  
 ...
  
 4.5  Doing More with Functions | 149",NA
Accumulative Functions,"These functions start by initializing some storage, and iterate over input to build it 
 up, before returning some final object (a large structure or aggregated result). A 
 standard way to do this is to initialize an empty list, accumulate the material, then 
 return the list, as shown in function 
 search1()
  in 
 Example 4-5
 .
  
 Example 4-5. Accumulating output into a list.
  
 def search1(substring, words):
  
  
  result = []
  
  
  for word in words:
  
  
  
  if substring in word:
  
  
  
  
  result.append(word)
  
  
  return result
  
 def search2(substring, words):
  
  
  for word in words:
  
  
  
  if substring in word:
  
  
  
  
  yield word
  
 150 | Chapter 4:Writing Structured Programs",NA
Higher-Order Functions,"Python provides some higher-order functions that are standard features of 
 functional programming languages such as Haskell. We illustrate them here, 
 alongside the equiv-alent expression using list comprehensions.
  
 Let’s start by defining a function 
 is_content_word()
  which checks whether a word is 
 from the open class of content words. We use this function as the first parameter of 
 filter()
 , which applies the function to each item in the sequence contained in its 
 second parameter, and retains only the items for which the function returns 
 True
 .",NA
Named Arguments,"When there are a lot of parameters it is easy to get confused about the correct 
 order. Instead we can refer to parameters by name, and even assign them a default 
 value just in case one was not provided by the calling program. Now the 
 parameters can be speci-fied in any order, and can be omitted.
  
 >>> def repeat(msg='<empty>', num=1): 
  
 ...     return msg * num 
  
 >>> repeat(num=3) 
  
 '<empty><empty><empty>' 
  
 >>> repeat(msg='Alice') 
  
 'Alice' 
  
 >>> repeat(num=5, msg='Alice') 
  
 'AliceAliceAliceAliceAlice'
  
 These are called 
 keyword arguments
 . If we mix these two kinds of parameters, 
 then we must ensure that the unnamed parameters precede the named ones. It has 
 to be this
  
 152 | Chapter 4:Writing Structured Programs",NA
4.6  Program Development,"Programming is a skill that is acquired over several years of experience with a 
 variety of programming languages and tasks. Key high-level abilities are 
 algorithm 
 design
  and its manifestation in 
 structured programming
 . Key low-level abilities 
 include familiarity with the syntactic constructs of the language, and knowledge of 
 a variety of diagnostic methods for trouble-shooting a program which does not 
 exhibit the expected behavior.
  
 This section describes the internal structure of a program module and how to 
 organize a multi-module program. Then it describes various kinds of error that 
 arise during program development, what you can do to fix them and, better still, to 
 avoid them in the first place.",NA
Structure of a Python Module,"The purpose of a program module is to bring logically related definitions and 
 functions together in order to facilitate reuse and abstraction. Python modules are 
 nothing more than individual 
 .py
  files. For example, if you were working with a 
 particular corpus format, the functions to read and write the format could be kept 
 together. Constants used by both formats, such as field separators, or a 
 EXTN = "".inf""
  
 filename extension, could be shared. If the format was updated, you would know 
 that only one file needed to be changed. Similarly, a module could contain code for 
 creating and manipulating a particular data structure such as syntax trees, or code 
 for performing a particular processing task such as plotting corpus statistics.
  
 When you start writing Python modules, it helps to have some examples to 
 emulate. You can locate the code for any NLTK module on your system using the 
 __file__ 
 variable:
  
 >>> nltk.metrics.distance.__file__ 
  
 '/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'
  
 This returns the location of the compiled 
 .pyc
  file for the module, and you’ll 
 probably see a different location on your machine. The file that you will need to 
 open is the corresponding 
 .py
  source file, and this will be in the same directory as 
 the 
 .pyc
  file.
  
 154 | Chapter 4:Writing Structured Programs",NA
Multimodule Programs,"Some programs bring together a diverse range of tasks, such as loading data from a 
 corpus, performing some analysis tasks on the data, then visualizing it. We may 
 already
  
 4.6  Program Development | 155",NA
Sources of Error,"Mastery of programming depends on having a variety of problem-solving skills to 
 draw upon when the program doesn’t work as expected. Something as trivial as a 
 misplaced symbol might cause the program to behave very differently. We call 
 these “bugs” be-cause they are tiny in comparison to the damage they can cause. 
 They creep into our code unnoticed, and it’s only much later when we’re running 
 the program on some new data that their presence is detected. Sometimes, fixing 
 one bug only reveals an-other, and we get the distinct impression that the bug is on 
 the move. The only reas-surance we have is that bugs are spontaneous and not the 
 fault of the programmer.
  
 156 | Chapter 4:Writing Structured Programs",NA
Debugging Techniques,"Since most code errors result from the programmer making incorrect assumptions, 
 the first thing to do when you detect a bug is to 
 check your assumptions
 . Localize the 
 prob-lem by adding 
 print
  statements to the program, showing the value of 
 important vari-ables, and showing how far the program has progressed.
  
 If the program produced an “exception”—a runtime error—the interpreter will 
 print a 
 stack trace
 , pinpointing the location of program execution at the time of the 
 error. If the program depends on input data, try to reduce this to the smallest size 
 while still producing the error.
  
 Once you have localized the problem to a particular function or to a line of code, 
 you need to work out what is going wrong. It is often helpful to recreate the 
 situation using the interactive command line. Define some variables, and then copy-
 paste the offending line of code into the session and see what happens. Check your 
 understanding of the code by reading some documentation and examining other 
 code samples that purport to do the same thing that you are trying to do. Try 
 explaining your code to someone else, in case she can see where things are going 
 wrong.
  
 Python provides a 
 debugger
  which allows you to monitor the execution of your 
 pro-gram, specify line numbers where execution will stop (i.e., 
 breakpoints
 ), and 
 step through sections of code and inspect the value of variables. You can invoke the 
 debug-ger on your code as follows:
  
 >>> import pdb 
  
 >>> import mymodule 
  
 >>> pdb.run('mymodule.myfunction()')
  
 It will present you with a prompt 
 (Pdb)
  where you can type instructions to the 
 debugger. Type 
 help
  to see the full list of commands. Typing 
 step
  (or just 
 s
 ) will 
 execute the current line and stop. If the current line calls a function, it will enter the 
 function and stop at the first line. Typing 
 next
  (or just 
 n
 ) is similar, but it stops 
 execution at the next line in the current function. The 
 break
  (or 
 b
 ) command can be 
 used to create or list breakpoints. Type 
 continue
  (or 
 c
 ) to continue execution as far 
 as the next breakpoint. Type the name of any variable to inspect its value.
  
 We can use the Python debugger to locate the problem in our 
 find_words()
  function. 
 Remember that the problem arose the second time the function was called. We’ll 
 start by calling the function without using the debugger , using the smallest 
 possible input. The second time, we’ll call it with the debugger .
  
 158 | Chapter 4:Writing Structured Programs",NA
Defensive Programming,"In order to avoid some of the pain of debugging, it helps to adopt some defensive 
 programming habits. Instead of writing a 20-line program and then testing it, build 
 the program bottom-up out of small pieces that are known to work. Each time you 
 combine these pieces to make a larger unit, test it carefully to see that it works as 
 expected. Consider adding 
 assert
  statements to your code, specifying properties of a 
 variable, e.g., 
 assert(isinstance(text, list))
 . If the value of the 
 text
  variable later 
 becomes a string when your code is used in some larger context, this will raise an 
 AssertionError
  and you will get immediate notification of the problem.
  
 Once you think you’ve found the bug, view your solution as a hypothesis. Try to 
 predict the effect of your bugfix before re-running the program. If the bug isn’t 
 fixed, don’t fall into the trap of blindly changing the code in the hope that it will 
 magically start working again. Instead, for each change, try to articulate a 
 hypothesis about what is wrong and why the change will fix the problem. Then 
 undo the change if the problem was not resolved.
  
 As you develop your program, extend its functionality, and fix any bugs, it helps to 
 maintain a suite of test cases. This is called 
 regression testing
 , since it is meant to 
 detect situations where the code “regresses”—where a change to the code has an 
 un-intended side effect of breaking something that used to work. Python provides a 
 simple regression-testing framework in the form of the 
 doctest
  module. This 
 module searches a file of code or documentation for blocks of text that look like an 
 interactive Python session, of the form you have already seen many times in this 
 book. It executes the Python commands it finds, and tests that their output matches 
 the output supplied in the original file. Whenever there is a mismatch, it reports the 
 expected and actual val-ues. For details, please consult the 
 doctest
  documentation at
  
 4.6  Program Development | 159",NA
4.7  Algorithm Design,"This section discusses more advanced concepts, which you may prefer to skip on 
 the first time through this chapter.
  
 A major part of algorithmic problem solving is selecting or adapting an appropriate 
 algorithm for the problem at hand. Sometimes there are several alternatives, and 
 choos-ing the best one depends on knowledge about how each alternative performs 
 as the size of the data grows. Whole books are written on this topic, and we only 
 have space to introduce some key concepts and elaborate on the approaches that 
 are most prevalent in natural language processing.
  
 The best-known strategy is known as 
 divide-and-conquer
 . We attack a problem of 
 size 
 n
  by dividing it into two problems of size 
 n/2
 , solve these problems, and 
 combine their results into a solution of the original problem. For example, suppose 
 that we had a pile of cards with a single word written on each card. We could sort 
 this pile by splitting it in half and giving it to two other people to sort (they could do 
 the same in turn). Then, when two sorted piles come back, it is an easy task to 
 merge them into a single sorted pile. See 
 Figure 4-3
  for an illustration of this 
 process.
  
 Another example is the process of looking up a word in a dictionary. We open the 
 book somewhere around the middle and compare our word with the current page. 
 If it’s earlier in the dictionary, we repeat the process on the first half; if it’s later, we 
 use the second half. This search method is called 
 binary search
  since it splits the 
 problem in half at every step.
  
 In another approach to algorithm design, we attack a problem by transforming it 
 into an instance of a problem we already know how to solve. For example, in order 
 to detect duplicate entries in a list, we can 
 pre-sort
  the list, then scan through it 
 once to check whether any adjacent pairs of elements are identical.",NA
Recursion,"The earlier examples of sorting and searching have a striking property: to solve a 
 prob-lem of size 
 n
 , we have to break it in half and then work on one or more 
 problems of size 
 n/
 2. A common way to implement such methods uses 
 recursion
 . 
 We define a function 
 f
 , which simplifies the problem, and 
 calls itself
  to solve one or 
 more easier",NA
Space-Time Trade-offs,"We can sometimes significantly speed up the execution of a program by building an 
 auxiliary data structure, such as an index. The listing in 
 Example 4-7
  implements a 
 simple text retrieval system for the Movie Reviews Corpus. By indexing the 
 document collection, it provides much faster lookup.
  
 Example 4-7. A simple text retrieval system.
  
 def raw(file):
  
  contents = open(file).read()
  
  contents = re.sub(r'<.*?>', ' ', contents) contents = 
 re.sub('\s+', ' ', contents) return contents
  
 def snippet(doc, term): # buggy
  
  text = ' '*30 + raw(doc) + ' '*30
  
  pos = text.index(term)
  
  return text[pos-30:pos+30]
  
 4.7  Algorithm Design | 163",NA
Dynamic Programming,"Dynamic programming is a general technique for designing algorithms which is 
 widely used in natural language processing. The term “programming” is used in a 
 different sense to what you might expect, to mean planning or scheduling. Dynamic 
 program-ming is used when a problem contains overlapping subproblems. Instead 
 of computing solutions to these subproblems repeatedly, we simply store them in a 
 lookup table. In the remainder of this section, we will introduce dynamic 
 programming, but in a rather different context to syntactic parsing.
  
 Pingala was an Indian author who lived around the 5th century B.C., and wrote a 
 treatise on Sanskrit prosody called the 
 Chandas Shastra
 . Virahanka extended this 
 work around the 6th century A.D., studying the number of ways of combining short 
 and long syllables to create a meter of length 
 n
 . Short syllables, marked 
 S
 , take up 
 one unit of length, while long syllables, marked 
 L
 , take two. Pingala found, for 
 example, that there are five ways to construct a meter of length 4: 
 V
 4
  = {
 LL
 , 
 SSL
 , 
 SLS
 , 
 LSS
 , 
 SSSS
 }. Observe that we can split 
 V
 4
  into two subsets, those starting with 
 L
  and 
 those starting with 
 S
 , as shown in 
 (1)
 .
  
 (1)
  V
 4
  =
  
  LL, LSS
  
  
  i.e. L prefixed to each item of 
 V
 2
  = {L, SS}
  
  SSL, SLS, SSSS
  
  
  i.e. S prefixed to each item of 
 V
 3
  = {SL, LS, SSS}
  
 With this observation, we can write a little recursive function called 
 virahanka1()
  to 
 compute these meters, shown in 
 Example 4-9
 . Notice that, in order to compute 
 V
 4
  
 we first compute 
 V
 3
  and 
 V
 2
 . But to compute 
 V
 3
 , we need to first compute 
 V
 2
  and 
 V
 1
 . 
 This 
 call structure
  is depicted in 
 (2)
 .
  
 4.7  Algorithm Design | 165",NA
4.8  A Sample of Python Libraries,"Python has hundreds of third-party libraries, specialized software packages that 
 extend the functionality of Python. NLTK is one such library. To realize the full 
 power of Python programming, you should become familiar with several other 
 libraries. Most of these will need to be manually installed on your computer.
  
 4.8  A Sample of Python Libraries | 167",NA
Matplotlib,"Python has some libraries that are useful for visualizing language data. The 
 Matplotlib package supports sophisticated plotting functions with a MATLAB-style 
 interface, and is available from 
 http://matplotlib.sourceforge.net/
 .
  
 So far we have focused on textual presentation and the use of formatted print 
 statements to get output lined up in columns. It is often very useful to display 
 numerical data in graphical form, since this often makes it easier to detect patterns. 
 For example, in 
 Example 3-5
 , we saw a table of numbers showing the frequency of 
 particular modal verbs in the Brown Corpus, classified by genre. The program in 
 Example 4-10
  presents the same information in graphical format. The output is 
 shown in 
 Figure 4-4
  (a color figure in the graphical display).
  
 Example 4-10. Frequency of modals in different sections of the Brown Corpus.
  
 colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black def 
 bar_chart(categories, words, counts):
  
  
  ""Plot a bar chart showing counts for each word by category""
  
  
  import pylab
  
  
  ind = pylab.arange(len(words))
  
  
  width = 1 / (len(categories) + 1)
  
  
  bar_groups = []
  
  
  for c in range(len(categories)):
  
  
  
  bars = pylab.bar(ind+c*width, counts[categories[c]], width,
  
  
  
  color=colors[c % len(colors)])
  
  
  
  bar_groups.append(bars)
  
  
  pylab.xticks(ind+width, words)
  
  
  pylab.legend([b[0] for b in bar_groups], categories, loc='upper left')
  
  
 pylab.ylabel('Frequency')
  
  
  pylab.title('Frequency of Six Modal Verbs by Genre')
  
  
  pylab.show()
  
 >>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure'] >>> modals = 
 ['can', 'could', 'may', 'might', 'must', 'will'] 
  
 >>> cfdist = nltk.ConditionalFreqDist( 
  
 ...              (genre, word) 
  
 ...              for genre in genres 
  
 ...              for word in nltk.corpus.brown.words(categories=genre) ...              if word in 
 modals) 
  
 ...
  
 >>> counts = {} 
  
 >>> for genre in genres: 
  
 ...     counts[genre] = [cfdist[genre][word] for word in modals] >>> 
 bar_chart(genres, modals, counts)
  
 From the bar chart it is immediately obvious that 
 may
  and 
 must
  have almost 
 identical relative frequencies. The same goes for 
 could
  and 
 might
 .
  
 It is also possible to generate such data visualizations on the fly. For example, a web 
 page with form input could permit visitors to specify search parameters, submit the 
 form, and see a dynamically generated visualization. To do this we have to specify 
 the
  
 168 | Chapter 4:Writing Structured Programs",NA
NetworkX,"The NetworkX package is for defining and manipulating structures consisting of 
 nodes and edges, known as 
 graphs
 . It is available from 
 https://networkx.lanl.gov/
 . 
 NetworkX",NA
csv,"Language analysis work often involves data tabulations, containing information 
 about lexical items, the participants in an empirical study, or the linguistic features 
 extracted from a corpus. Here’s a fragment of a simple lexicon, in CSV format:
  
 sleep, sli:p, v.i, a condition of body and mind ...
  
 walk, wo:k, v.intr, progress by lifting and setting down each foot ...
  
 wake, weik, intrans, cease to sleep
  
 We can use Python’s CSV library to read and write files stored in this format. For 
 example, we can open a CSV file called 
 lexicon.csv
   and iterate over its rows :
  
 >>> import csv 
  
 >>> input_file = open(""lexicon.csv"", ""rb"") 
  
 >>> for row in csv.reader(input_file): 
  
 ...     print row 
  
 ['sleep', 'sli:p', 'v.i', 'a condition of body and mind ...']
  
 170 | Chapter 4:Writing Structured Programs",NA
NumPy,"The NumPy package provides substantial support for numerical processing in 
 Python. NumPy has a multidimensional array object, which is easy to initialize and 
 access:
  
 >>> from numpy import array 
  
 >>> cube = array([ [[0,0,0], [1,1,1], [2,2,2]], ...                
 [[3,3,3], [4,4,4], [5,5,5]], ...                [[6,6,6], [7,7,7], [8,8,8]] 
 ]) >>> cube[1,1,1] 
  
 4 
  
 >>> cube[2].transpose() 
  
 array([[6, 7, 8],
  
  
  [6, 7, 8],
  
  
  [6, 7, 8]]) 
  
 >>> cube[2,1:] 
  
 array([[7, 7, 7],
  
  
  [8, 8, 8]])
  
 NumPy includes linear algebra functions. Here we perform singular value 
 decomposi-tion on a matrix, an operation used in 
 latent semantic analysis
  to help 
 identify implicit concepts in a document collection:
  
 4.8  A Sample of Python Libraries | 171",NA
Other Python Libraries,"There are many other Python libraries, and you can search for them with the help 
 of the Python Package Index at 
 http://pypi.python.org/
 . Many libraries provide an 
 interface to external software, such as relational databases (e.g., 
 mysql-python
 ) and 
 large docu-ment collections (e.g., 
 PyLucene
 ). Many other libraries give access to file 
 formats such as PDF, MSWord, and XML (
 pypdf
 , 
 pywin32
 , 
 xml.etree
 ), RSS feeds (e.g., 
 feedparser
 ), and electronic mail (e.g., 
 imaplib
 , 
 email
 ).",NA
4.9  Summary,"• Python’s assignment and parameter passing use object references; e.g., if 
 a
  is a 
 list 
  
 and we assign 
 b = a
 , then any operation on 
 a
  will modify 
 b
 , and vice versa.
  
 • The 
 is
  operation tests whether two objects are identical internal objects, 
 whereas 
 ==
  tests whether two objects are equivalent. This distinction parallels 
 the type-token distinction.
  
 • Strings, lists, and tuples are different kinds of sequence object, supporting 
 common operations such as indexing, slicing, 
 len()
 , 
 sorted()
 , and membership 
 testing using 
 in
 .
  
 • We can write text to a file by opening the file for writing
  
 ofile = open('output.txt', 'w'
  
 then adding content to the file 
 ofile.write(""Monty Python"")
 , and finally closing the 
 file 
 ofile.close()
 .
  
 • A declarative programming style usually produces more compact, readable 
 code; manually incremented loop variables are usually unnecessary. When a 
 sequence must be enumerated, use 
 enumerate()
 .
  
 • Functions are an essential programming abstraction: key concepts to 
 understand 
  
 are parameter passing, variable scope, and docstrings.
  
 172 | Chapter 4:Writing Structured Programs",NA
4.10  Further Reading,"This chapter has touched on many topics in programming, some specific to Python, 
 and some quite general. We’ve just scratched the surface, and you may want to read 
 more about these topics, starting with the further materials for this chapter 
 available at 
 http://www.nltk.org/
 .
  
 The Python website provides extensive documentation. It is important to 
 understand 
 the 
 built-in 
 functions 
 and 
 standard 
 types, 
 described 
 at 
 http://docs.python.org/library/ 
 functions.html
  
 and 
 http://docs.python.org/library/stdtypes.html
 . We have learned about generators 
 and their importance for efficiency; for information about iterators, a closely 
 related topic, see 
 http://docs.python.org/library/itertools.html
 . Consult your 
 favorite Py-thon book for more information on such topics. An excellent resource 
 for using Python for multimedia processing, including working with sound files, is 
 (Guzdial, 2005).
  
 When using the online Python documentation, be aware that your installed version 
 might be different from the version of the documentation you are reading. You can 
 easily check what version you have, with 
 import sys; sys.version
 . Version-specific 
 documentation is available at 
 http://www.python.org/doc/versions/
 .
  
 Algorithm design is a rich field within computer science. Some good starting points 
 are (Harel, 2004), (Levitin, 2004), and (Knuth, 2006). Useful guidance on the 
 practice of software development is provided in (Hunt & Thomas, 2000) and 
 (McConnell, 2004).",NA
4.11  Exercises,"1.
  ○ 
 Find out more about sequence objects using Python’s help facility. In the 
 inter-preter, type 
 help(str)
 , 
 help(list)
 , and 
 help(tuple)
 . This will give you a full list 
 of the functions supported by each type. Some functions have special names 
 flanked with underscores; as the help documentation shows, each such 
 function corre-sponds to something more familiar. For example 
 x.__getitem__(y)
  
 is just a long-winded way of saying 
 x[y]
 .
  
 2.
  ○ 
 Identify three operations that can be performed on both tuples and lists. 
 Identify three list operations that cannot be performed on tuples. Name a 
 context where using a list instead of a tuple generates a Python error.",NA
CHAPTER 5,NA,NA
Categorizing and Tagging ,NA,NA
Words,"Back in elementary school you learned the difference between nouns, verbs, 
 adjectives, and adverbs. These “word classes” are not just the idle invention of 
 grammarians, but are useful categories for many language processing tasks. As we 
 will see, they arise from simple analysis of the distribution of words in text. The 
 goal of this chapter is to answer the following questions:
  
 1. What are lexical categories, and how are they used in natural language 
 processing?
  
 2. What is a good Python data structure for storing words and their categories?
  
 3. How can we automatically tag each word of a text with its word class?
  
 Along the way, we’ll cover some fundamental techniques in NLP, including 
 sequence labeling, n-gram models, backoff, and evaluation. These techniques are 
 useful in many areas, and tagging gives us a simple context in which to present 
 them. We will also see how tagging is the second step in the typical NLP pipeline, 
 following tokenization.
  
 The process of classifying words into their 
 parts-of-speech
  and labeling them 
 accord-ingly is known as 
 part-of-speech tagging
 , 
 POS tagging
 , or simply 
 tagging
 . 
 Parts-of-speech are also known as 
 word classes
  or 
 lexical categories
 . The 
 collection of tags used for a particular task is known as a 
 tagset
 . Our emphasis in 
 this chapter is on exploiting tags, and tagging text automatically.",NA
5.1  Using a Tagger,"A part-of-speech tagger, or 
 POS tagger
 , processes a sequence of words, and 
 attaches a part of speech tag to each word (don’t forget to 
 import nltk
 ):
  
 >>> text = nltk.word_tokenize(""And now for something completely different"") >>> 
 nltk.pos_tag(text) 
  
 [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), 
  
 ('completely', 'RB'), ('different', 'JJ')]",NA
5.2  Tagged Corpora,NA,NA
Representing Tagged Tokens,"By convention in NLTK, a tagged token is represented using a tuple consisting of 
 the token and the tag. We can create one of these special tuples from the standard 
 string representation of a tagged token, using the function 
 str2tuple()
 :
  
 >>> tagged_token = nltk.tag.str2tuple('fly/NN') >>> 
 tagged_token 
  
 ('fly', 'NN') 
  
 >>> tagged_token[0] 
  
 'fly' 
  
 >>> tagged_token[1] 
  
 'NN'
  
 We can construct a list of tagged tokens directly from a string. The first step is to 
 tokenize the string to access the individual 
 word/tag
  strings, and then to convert 
 each of these into a tuple (using 
 str2tuple()
 ).
  
 >>> sent = ''' 
  
 ... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN 
  
 ... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC ... Fulton/NP-tl 
 County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS ... said/VBD ``/`` 
 ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB ... accepted/VBN 
 practices/NNS which/WDT inure/VB to/IN the/AT best/JJT ... interest/NN of/IN both/ABX 
 governments/NNS ''/'' ./.
  
 ... ''' 
  
 >>> [nltk.tag.str2tuple(t) for t in sent.split()] 
  
 [('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'), ('on', 'IN'), ('a', 'AT'), 
 ('number', 'NN'), ... ('.', '.')]",NA
Reading Tagged Corpora,"Several of the corpora included with NLTK have been 
 tagged
  for their part-of-
 speech. Here’s an example of what you might see if you opened a file from the 
 Brown Corpus with a text editor:
  
 The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at 
 inves-tigation/nn 
 of/in 
 Atlanta’s/np$ 
 recent/jj 
 primary/nn 
 election/nn 
 produced/vbd 
 /
  no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd 
 place/nn ./.
  
 5.2  Tagged Corpora | 181",NA
A Simplified Part-of-Speech Tagset,"Tagged corpora use many different conventions for tagging words. To help us get 
 star-ted, we will be looking at a simplified tagset (shown in 
 Table 5-1
 ).
  
 Table 5-1. Simplified part-of-speech tagset
  
  
  
  
 Ta
 g
  
 Meaning
  
  
 Examples
  
  
 AD
 J
  
 adjective
  
 new, good, high, special, big, 
 local
  
 AD
 V
  
 adverb
  
 really, already, still, early, 
 now
  
 CN
 J
  
 conjunction
  
 and, or, but, if, while, 
 although
  
 DE
 T
  
 determiner
  
 the, a, some, most, every, 
 no
  
 EX
  
 existential
  
 there, there’s
  
 FW
  
 foreign 
 word
  
 dolce, ersatz, esprit, quo, 
 maitre
  
 MO
 D
  
 modal verb
  
 will, can, would, may, must, 
 should
  
 N
  
 noun
  
 year, home, costs, time, 
 education
  
 NP
  
 proper 
 noun
  
 Alison, Africa, April, 
 Washington
  
 NU
 M
  
 number
  
 twenty-four, fourth, 1991, 
 14:24
  
 PR
 O
  
 pronoun
  
 he, their, her, its, my, I, us
  
 P
  
 preposition
  
 on, of, at, with, by, into, 
 under
  
 TO
  
 the word 
 to
  
 to
  
 UH
  
 interjection
  
 ah, bang, ha, whee, hmpf, 
 oops
  
 V
  
 verb
  
 is, has, get, do, make, see, 
 run
  
 VD
  
 past tense
  
 said, took, told, made, 
 asked
  
 VG
  
 present 
 participle
  
 making, going, playing, 
 working
  
 VN
  
 past 
 participle
  
 given, taken, begun, sung
  
 W
 H
  
 wh
  
 determiner
  
 who, which, when, what, 
 where, how
  
 5.2  Tagged Corpora | 183",NA
Nouns,"Nouns generally refer to people, places, things, or concepts, e.g., 
 woman, Scotland, 
 book, intelligence
 . Nouns can appear after determiners and adjectives, and can be 
 the subject or object of the verb, as shown in 
 Table 5-2
 .
  
 Table 5-2. Syntactic patterns involving some nouns
  
  
  
  
 Word
  
 After a determiner
  
  
 Subject of the verb
  
  
 woman
  
 the
  woman who I saw 
 yesterday ...
  
 the woman 
 sat
  down
  
 Scotlan
 d
  
 the
  Scotland I remember as a 
 child ...
  
 Scotland 
 has
  five million people
  
 book
  
 the
  book I bought yesterday 
 ...
  
 this book 
 recounts
  the 
 colonization of Australia
  
 intellige
 nce
  
 the
  intelligence displayed by 
 the child ...
  
 Mary’s intelligence 
 impressed
  her 
 teachers
  
 The simplified noun tags are 
 N
  for common nouns like 
 book
 , and 
 NP
  for proper 
 nouns like 
 Scotland
 .
  
 184 | Chapter 5:Categorizing and Tagging Words",NA
Verbs,"Verbs are words that describe events and actions, e.g., 
 fall
  and 
 eat
 , as shown in 
 Ta-
 ble 5-3
 . In the context of a sentence, verbs typically express a relation involving the 
 referents of one or more noun phrases.
  
 Table 5-3. Syntactic patterns involving some verbs
  
  
  
  
 Wo
 rd
  
 Simple
  
  
 With modifiers and 
 adjuncts (italicized)
  
 fall
  
 Rome fell
  
 Dot com stocks 
 suddenly
  fell 
 like a stone
  
 eat
  
 Mice eat 
 cheese
  
 John ate the pizza 
 with gusto
  
 What are the most common verbs in news text? Let’s sort all the verbs by 
 frequency:
  
 >>> wsj = nltk.corpus.treebank.tagged_words(simplify_tags=True) 
  
 >>> word_tag_fd = nltk.FreqDist(wsj) 
  
 >>> [word + ""/"" + tag for (word, tag) in word_tag_fd if tag.startswith('V')] ['is/V', 'said/VD', 
 'was/VD', 'are/V', 'be/V', 'has/V', 'have/V', 'says/V', 'were/VD', 'had/VD', 'been/VN', ""'s/V"", 
 'do/V', 'say/V', 'make/V', 'did/VD', 'rose/VD', 'does/V', 'expected/VN', 'buy/V', 'take/V', 
 'get/V', 'sell/V', 'help/V', 'added/VD', 'including/VG', 'according/VG', 'made/VN', 'pay/V', ...]
  
 Note that the items being counted in the frequency distribution are word-tag pairs. 
 Since words and tags are paired, we can treat the word as a condition and the tag as 
 an event, and initialize a conditional frequency distribution with a list of condition-
 event pairs. This lets us see a frequency-ordered list of tags given a word:
  
 >>> cfd1 = nltk.ConditionalFreqDist(wsj) 
  
 >>> cfd1['yield'].keys() 
  
 ['V', 'N'] 
  
 >>> cfd1['cut'].keys() 
  
 ['V', 'VD', 'N', 'VN']
  
 We can reverse the order of the pairs, so that the tags are the conditions, and the 
 words are the events. Now we can see likely words for a given tag:
  
 5.2  Tagged Corpora | 185",NA
Adjectives and Adverbs,"Two other important word classes are 
 adjectives
  and 
 adverbs
 . Adjectives describe 
 nouns, and can be used as modifiers (e.g., 
 large
  in 
 the large pizza
 ), or as predicates 
 (e.g., 
 the pizza is large
 ). English adjectives can have internal structure (e.g., 
 fall+ing
  
 in 
 the falling stocks
 ). Adverbs modify verbs to specify the time, manner, place, or 
 direction of the event described by the verb (e.g., 
 quickly
  in 
 the stocks fell quickly
 ). 
 Adverbs may also modify adjectives (e.g., 
 really
  in 
 Mary’s teacher was really nice
 ).
  
 English has several categories of closed class words in addition to prepositions, 
 such as 
 articles
  (also often called 
 determiners
 ) (e.g., 
 the
 , 
 a
 ), 
 modals
  (e.g., 
 should
 , 
 may
 ), and 
 personal pronouns
  (e.g., 
 she
 , 
 they
 ). Each dictionary and grammar 
 classifies these words differently.
  
  
 Your Turn:
  If you are uncertain about some of these parts-of-
 speech, study them using 
 nltk.app.concordance()
 , or watch some of 
 the 
 School-house Rock!
  grammar videos available at YouTube, or 
 consult 
 Sec-tion 5.9
 .
  
 186 | Chapter 5:Categorizing and Tagging Words",NA
Unsimplified Tags,"Let’s find the most frequent nouns of each noun part-of-speech type. The program 
 in 
 Example 5-1
  finds all tags starting with 
 NN
 , and provides a few example words 
 for each one. You will see that there are many variants of 
 NN
 ; the most important 
 contain 
 $
  for possessive nouns, 
 S
  for plural nouns (since plural nouns typically end 
 in 
 s
 ), and 
 P
  for proper nouns. In addition, most of the tags have suffix modifiers: 
 -NC
  
 for citations,
 -HL
  for words in headlines, and 
 -TL
  for titles (a feature of Brown tags).
  
 Example 5-1. Program to find the most frequent noun tags.
  
 def findtags(tag_prefix, tagged_text):
  
  
  cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text
  
  
  if tag.startswith(tag_prefix))
  
  
  return dict((tag, cfd[tag].keys()[:5]) for tag in cfd.conditions())
  
 >>> tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news')) >>> for tag 
 in sorted(tagdict): 
  
 ...     print tag, tagdict[tag] 
  
 ...
  
 NN ['year', 'time', 'state', 'week', 'man'] 
  
 NN$ [""year's"", ""world's"", ""state's"", ""nation's"", ""company's""] 
  
 NN$-HL [""Golf's"", ""Navy's""] 
  
 NN$-TL [""President's"", ""University's"", ""League's"", ""Gallery's"", ""Army's""] NN-HL ['cut', 
 'Salary', 'condition', 'Question', 'business'] 
  
 NN-NC ['eva', 'ova', 'aya'] 
  
 NN-TL ['President', 'House', 'State', 'University', 'City'] 
  
 NN-TL-HL ['Fort', 'City', 'Commissioner', 'Grove', 'House'] 
  
 NNS ['years', 'members', 'people', 'sales', 'men'] 
  
 NNS$ [""children's"", ""women's"", ""men's"", ""janitors'"", ""taxpayers'""] NNS$-HL [""Dealers'"", 
 ""Idols'""] 
  
 NNS$-TL [""Women's"", ""States'"", ""Giants'"", ""Officers'"", ""Bombers'""] NNS-HL ['years', 
 'idols', 'Creations', 'thanks', 'centers'] 
  
 NNS-TL ['States', 'Nations', 'Masters', 'Rules', 'Communists'] 
  
 NNS-TL-HL ['Nations']
  
 When we come to constructing part-of-speech taggers later in this chapter, we will 
 use the unsimplified tags.",NA
Exploring Tagged Corpora,"Let’s briefly return to the kinds of exploration of corpora we saw in previous 
 chapters, this time exploiting POS tags.
  
 Suppose we’re studying the word 
 often
  and want to see how it is used in text. We 
 could ask to see the words that follow 
 often
 :
  
 >>> brown_learned_text = brown.words(categories='learned') 
  
 >>> sorted(set(b for (a, b) in nltk.ibigrams(brown_learned_text) if a == 'often')) [',', '.', 
 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming', 'became', 'become', 'been', 
 'began', 'call', 'called', 'carefully', 'chose', ...]
  
 However, it’s probably more instructive use the 
 tagged_words()
  method to look at 
 the part-of-speech tag of the following words:
  
 5.2  Tagged Corpora | 187",NA
5.3  Mapping Words to Properties Using ,NA,NA
Python Dictionaries,"As we have seen, a tagged word of the form 
 (word, tag)
  is an association between a 
 word and a part-of-speech tag. Once we start doing part-of-speech tagging, we will 
 be creating programs that assign a tag to a word, the tag which is most likely in a 
 given context. We can think of this process as 
 mapping
  from words to tags. The 
 most natural way to store mappings in Python uses the so-called 
 dictionary
  data 
 type (also known as an 
 associative array
  or 
 hash array
  in other programming 
 languages). In this sec-tion, we look at dictionaries and see how they can represent 
 a variety of language in-formation, including parts-of-speech.",NA
Indexing Lists Versus Dictionaries,"A text, as we have seen, is treated in Python as a list of words. An important 
 property of lists is that we can “look up” a particular item by giving its index, e.g., 
 text1[100]
 . Notice how we specify a number and get back a word. We can think of a 
 list as a simple kind of table, as shown in 
 Figure 5-2
 .
  
  
 Figure 5-2. List lookup: We access the contents of a Python list with the help of an integer index.",NA
Dictionaries in Python,"Python provides a 
 dictionary
  data type that can be used for mapping between 
 arbitrary types. It is like a conventional dictionary, in that it gives you an efficient 
 way to look things up. However, as we see from 
 Table 5-4
 , it has a much wider 
 range of uses.
  
 190 | Chapter 5:Categorizing and Tagging Words",NA
Defining Dictionaries,"We can use the same key-value pair format to create a dictionary. There are a 
 couple of ways to do this, and we will normally use the first:
  
 >>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'} >>> pos = 
 dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')
  
 Note that dictionary keys must be immutable types, such as strings and tuples. If we 
 try to define a dictionary using a mutable key, we get a 
 TypeError
 :
  
 >>> pos = {['ideas', 'blogs', 'adventures']: 'N'} Traceback 
 (most recent call last):
  
  File ""<stdin>"", line 1, in <module> 
  
 TypeError: list objects are unhashable",NA
Default Dictionaries,"If we try to access a key that is not in a dictionary, we get an error. However, it’s 
 often useful if a dictionary can automatically create an entry for this new key and 
 give it a default value, such as zero or the empty list. Since Python 2.5, a special kind 
 of dic-tionary called a 
 defaultdict
  has been available. (It is provided as 
 nltk.defaultdict
  
 for the benefit of readers who are using Python 2.4.) In order to use it, we have to 
 supply a parameter which can be used to create the default value, e.g., 
 int
 , 
 float
 , 
 str
 , 
 list
 , 
 dict
 , 
 tuple
 .
  
 >>> frequency = nltk.defaultdict(int) 
  
 >>> frequency['colorless'] = 4 
  
 >>> frequency['ideas'] 
  
 0 
  
 >>> pos = nltk.defaultdict(list) 
  
 >>> pos['sleep'] = ['N', 'V'] 
  
 >>> pos['ideas'] 
  
 []
  
  
 These default values are actually functions that convert other 
 objects to the specified type (e.g., 
 int(""2"")
 , 
 list(""2"")
 ). When they are 
 called with no parameter—say, 
 int()
 , 
 list()
 —they return 
 0
  and 
 []
  
 respectively.
  
 The preceding examples specified the default value of a dictionary entry to be the 
 default value of a particular data type. However, we can specify any default value 
 we like, simply by providing the name of a function that can be called with no 
 arguments to create the required value. Let’s return to our part-of-speech example, 
 and create a dictionary whose default value for any entry is 
 'N'
  . When we access a 
 non-existent entry , it is automatically added to the dictionary .
  
 >>> pos = nltk.defaultdict(lambda: 'N') 
  
 >>> pos['colorless'] = 'ADJ'
  
 >>> pos['blog'] 
  
 'N'
  
 5.3  Mapping Words to Properties Using Python Dictionaries | 193",NA
Incrementally Updating a Dictionary,"We can employ dictionaries to count occurrences, emulating the method for 
 tallying words shown in 
 Figure 1-3
 . We begin by initializing an empty 
 defaultdict
 ,",NA
Complex Keys and Values,"We can use default dictionaries with complex keys and values. Let’s study the range 
 of possible tags for a word, given the word itself and the tag of the previous word. 
 We will see how this information can be used by a POS tagger.
  
 >>> pos = nltk.defaultdict(lambda: nltk.defaultdict(int)) 
  
 >>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True) >>> for 
 ((w1, t1), (w2, t2)) in nltk.ibigrams(brown_news_tagged): 
  
 ...     pos[(t1, w2)][t2] += 1 
  
 ...
  
 >>> pos[('DET', 'right')] 
  
 defaultdict(<type 'int'>, {'ADV': 3, 'ADJ': 9, 'N': 3})
  
 This example uses a dictionary whose default value for an entry is a dictionary 
 (whose default value is 
 int()
 , i.e., zero). Notice how we iterated over the bigrams of 
 the tagged corpus, processing a pair of word-tag pairs for each iteration . Each time 
 through the loop we updated our 
 pos
  dictionary’s entry for 
 (t1, w2)
 , a tag and its 
 following
  word
  
  
 . When we look up an item in 
 pos
  we must specify a compound key 
  
 , and we get
  
 back a dictionary object. A POS tagger could use such information to decide that the 
 word 
 right
 , when preceded by a determiner, should be tagged as 
 ADJ
 .
  
 196 | Chapter 5:Categorizing and Tagging Words",NA
Inverting a Dictionary,"Dictionaries support efficient lookup, so long as you want to get the value for any 
 key. If 
 d
  is a dictionary and 
 k
  is a key, we type 
 d[k]
  and immediately obtain the 
 value. Finding a key given a value is slower and more cumbersome:
  
 >>> counts = nltk.defaultdict(int) 
  
 >>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'): ...     
 counts[word] += 1 
  
 ...
  
 >>> [key for (key, value) in counts.items() if value == 32] 
  
 ['brought', 'Him', 'virtue', 'Against', 'There', 'thine', 'King', 'mortal', 'every', 'been']
  
 If we expect to do this kind of “reverse lookup” often, it helps to construct a 
 dictionary that maps values to keys. In the case that no two keys have the same 
 value, this is an easy thing to do. We just get all the key-value pairs in the 
 dictionary, and create a new dictionary of value-key pairs. The next example also 
 illustrates another way of initial-izing a dictionary 
 pos
  with key-value pairs.
  
 >>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'} >>> pos2 = dict((value, 
 key) for (key, value) in pos.items()) 
  
 >>> pos2['N'] 
  
 'ideas'
  
 Let’s first make our part-of-speech dictionary a bit more realistic and add some 
 more words to 
 pos
  using the dictionary 
 update()
  method, to create the situation 
 where mul-tiple keys have the same value. Then the technique just shown for 
 reverse lookup will no longer work (why not?). Instead, we have to use 
 append()
  to 
 accumulate the words for each part-of-speech, as follows:
  
 >>> pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'}) >>> pos2 = 
 nltk.defaultdict(list) 
  
 >>> for key, value in pos.items(): 
  
 ...     pos2[value].append(key) 
  
 ...
  
 >>> pos2['ADV'] 
  
 ['peacefully', 'furiously']
  
 Now we have inverted the 
 pos
  dictionary, and can look up any part-of-speech and 
 find all words having that part-of-speech. We can do the same thing even more 
 simply using NLTK’s support for indexing, as follows:
  
 >>> pos2 = nltk.Index((value, key) for (key, value) in pos.items()) >>> 
 pos2['ADV'] 
  
 ['peacefully', 'furiously']
  
 A summary of Python’s dictionary methods is given in 
 Table 5-5
 .
  
 5.3  Mapping Words to Properties Using Python Dictionaries | 197",NA
5.4  Automatic Tagging,"In the rest of this chapter we will explore various ways to automatically add part-
 of-speech tags to text. We will see that the tag of a word depends on the word and 
 its context within a sentence. For this reason, we will be working with data at the 
 level of (tagged) sentences rather than words. We’ll begin by loading the data we 
 will be using.
  
 >>> from nltk.corpus import brown 
  
 >>> brown_tagged_sents = brown.tagged_sents(categories='news') >>> 
 brown_sents = brown.sents(categories='news')",NA
The Default Tagger,"The simplest possible tagger assigns the same tag to each token. This may seem to 
 be a rather banal step, but it establishes an important baseline for tagger 
 performance. In order to get the best result, we tag each word with the most likely 
 tag. Let’s find out which tag is most likely (now using the unsimplified tagset):
  
 >>> tags = [tag for (word, tag) in brown.tagged_words(categories='news')] >>> 
 nltk.FreqDist(tags).max() 
  
 'NN'
  
 Now we can create a tagger that tags everything as 
 NN
 .
  
 >>> raw = 'I do not like green eggs and ham, I do not like them Sam I am!' >>> tokens = 
 nltk.word_tokenize(raw) 
  
 >>> default_tagger = nltk.DefaultTagger('NN') 
  
 >>> default_tagger.tag(tokens) 
  
 [('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'), ('eggs', 'NN'), ('and', 
 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'),
  
 198 | Chapter 5:Categorizing and Tagging Words",NA
The Regular Expression Tagger,"The regular expression tagger assigns tags to tokens on the basis of matching 
 patterns. For instance, we might guess that any word ending in 
 ed
  is the past 
 participle of a verb, and any word ending with 
 ’s
  is a possessive noun. We can 
 express these as a list of regular expressions:
  
 >>> patterns = [ 
  
 ...     (r'.*ing$', 'VBG'),               # gerunds 
  
 ...     (r'.*ed$', 'VBD'),                # simple past 
  
 ...     (r'.*es$', 'VBZ'),                # 3rd singular present ...     (r'.*ould$', 'MD'),               
 # modals 
  
 ...     (r'.*\'s$', 'NN$'),               # possessive nouns ...     (r'.*s$', 'NNS'),                 
 # plural nouns 
  
 ...     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers ...     (r'.*', 'NN')                     
 # nouns (default) ... ]
  
 Note that these are processed in order, and the first one that matches is applied. 
 Now we can set up a tagger and use it to tag a sentence. After this step, it is correct 
 about a fifth of the time.
  
 >>> regexp_tagger = nltk.RegexpTagger(patterns) 
  
 >>> regexp_tagger.tag(brown_sents[3]) 
  
 [('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'), ('of', 'NN'), ('such', 'NN'), 
 ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'), (""''"", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), 
 ('said', 'NN'), (',', 'NN'), ('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ...] >>> 
 regexp_tagger.evaluate(brown_tagged_sents) 
  
 0.20326391789486245
  
 The final regular expression «
 .*
 » is a catch-all that tags everything as a noun. This is 
 equivalent to the default tagger (only much less efficient). Instead of respecifying 
 this as part of the regular expression tagger, is there a way to combine this tagger 
 with the default tagger? We will see how to do this shortly.
  
 5.4  Automatic Tagging | 199",NA
The Lookup Tagger,"A lot of high-frequency words do not have the 
 NN
  tag. Let’s find the hundred most 
 frequent words and store their most likely tag. We can then use this information as 
 the model for a “lookup tagger” (an NLTK 
 UnigramTagger
 ):
  
 >>> fd = nltk.FreqDist(brown.words(categories='news')) 
  
 >>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news')) >>> 
 most_freq_words = fd.keys()[:100] 
  
 >>> likely_tags = dict((word, cfd[word].max()) for word in most_freq_words) >>> 
 baseline_tagger = nltk.UnigramTagger(model=likely_tags) 
  
 >>> baseline_tagger.evaluate(brown_tagged_sents) 
  
 0.45578495136941344
  
 It should come as no surprise by now that simply knowing the tags for the 100 most 
 frequent words enables us to tag a large fraction of tokens correctly (nearly half, in 
 fact).
  
 Let’s see what it does on some untagged input text:
  
 >>> sent = brown.sents(categories='news')[3] 
  
 >>> baseline_tagger.tag(sent) 
  
 [('``', '``'), ('Only', None), ('a', 'AT'), ('relative', None), 
  
 ('handful', None), ('of', 'IN'), ('such', None), ('reports', None), ('was', 'BEDZ'), 
 ('received', None), (""''"", ""''""), (',', ','), 
  
 ('the', 'AT'), ('jury', None), ('said', 'VBD'), (',', ','), 
  
 ('``', '``'), ('considering', None), ('the', 'AT'), ('widespread', None), ('interest', None), 
 ('in', 'IN'), ('the', 'AT'), ('election', None), (',', ','), ('the', 'AT'), ('number', None), ('of', 
 'IN'), 
  
 ('voters', None), ('and', 'CC'), ('the', 'AT'), ('size', None), 
  
 ('of', 'IN'), ('this', 'DT'), ('city', None), (""''"", ""''""), ('.', '.')]
  
 Many words have been assigned a tag of 
 None
 , because they were not among the 
 100 most frequent words. In these cases we would like to assign the default tag of 
 NN
 . In other words, we want to use the lookup table first, and if it is unable to 
 assign a tag, then use the default tagger, a process known as 
 backoff
  (
 Section 5.5
 ). 
 We do this by specifying one tagger as a parameter to the other, as shown next. 
 Now the lookup tagger will only store word-tag pairs for words other than nouns, 
 and whenever it cannot assign a tag to a word, it will invoke the default tagger.
  
 >>> baseline_tagger = nltk.UnigramTagger(model=likely_tags, 
  
 ...                                      backoff=nltk.DefaultTagger('NN'))
  
 Let’s put all this together and write a program to create and evaluate lookup 
 taggers having a range of sizes (
 Example 5-4
 ).
  
 200 | Chapter 5:Categorizing and Tagging Words",NA
Evaluation,"In the previous examples, you will have noticed an emphasis on accuracy scores. In 
 fact, evaluating the performance of such tools is a central theme in NLP. Recall the 
 processing pipeline in 
 Figure 1-5
 ; any errors in the output of one module are 
 greatly multiplied in the downstream modules.
  
 We evaluate the performance of a tagger relative to the tags a human expert would 
 assign. Since we usually don’t have access to an expert and impartial human judge, 
 we make do instead with 
 gold standard
  test data. This is a corpus which has been 
 man-ually annotated and accepted as a standard against which the guesses of an 
 automatic system are assessed. The tagger is regarded as being correct if the tag it 
 guesses for a given word is the same as the gold standard tag.
  
 Of course, the humans who designed and carried out the original gold standard 
 anno-tation were only human. Further analysis might show mistakes in the gold 
 standard, or may eventually lead to a revised tagset and more elaborate guidelines. 
 Nevertheless, the gold standard is by definition “correct” as far as the evaluation of 
 an automatic tagger is concerned.
  
 5.4  Automatic Tagging | 201",NA
5.5  N-Gram Tagging,NA,NA
Unigram Tagging,"Unigram taggers are based on a simple statistical algorithm: for each token, assign 
 the tag that is most likely for that particular token. For example, it will assign the 
 tag 
 JJ
  to any occurrence of the word 
 frequent
 , since 
 frequent
  is used as an adjective 
 (e.g., 
 a fre-quent word
 ) more often than it is used as a verb (e.g., 
 I frequent this cafe
 ). 
 A unigram tagger behaves just like a lookup tagger (
 Section 5.4
 ), except there is a 
 more convenient",NA
Separating the Training and Testing Data,"Now that we are training a tagger on some data, we must be careful not to test it on 
 the same data, as we did in the previous example. A tagger that simply memorized 
 its training data and made no attempt to construct a general model would get a 
 perfect score, but would be useless for tagging new text. Instead, we should split 
 the data, training on 90% and testing on the remaining 10%:
  
 >>> size = int(len(brown_tagged_sents) * 0.9) 
  
 >>> size 
  
 4160 
  
 >>> train_sents = brown_tagged_sents[:size] 
  
 >>> test_sents = brown_tagged_sents[size:] 
  
 >>> unigram_tagger = nltk.UnigramTagger(train_sents) >>> 
 unigram_tagger.evaluate(test_sents) 
  
 0.81202033290142528
  
 Although the score is worse, we now have a better picture of the usefulness of this 
 tagger, i.e., its performance on previously unseen text.",NA
General N-Gram Tagging,"When we perform a language processing task based on unigrams, we are using one 
 item of context. In the case of tagging, we consider only the current token, in 
 isolation from any larger context. Given such a model, the best we can do is tag each 
 word with its 
 a priori
  most likely tag. This means we would tag a word such as 
 wind
  
 with the same tag, regardless of whether it appears in the context 
 the wind
  or 
 to 
 wind
 .
  
 An 
 n-gram tagger
  is a generalization of a unigram tagger whose context is the 
 current word together with the part-of-speech tags of the 
 n-
 1 preceding tokens, as 
 shown in 
 Figure 5-5
 . The tag to be chosen, 
 t
 n
 , is circled, and the context is shaded in 
 grey. In the example of an n-gram tagger shown in 
 Figure 5-5
 , we have 
 n=
 3; that is, 
 we consider",NA
Combining Taggers,"One way to address the trade-off between accuracy and coverage is to use the more 
 accurate algorithms when we can, but to fall back on algorithms with wider 
 coverage when necessary. For example, we could combine the results of a bigram 
 tagger, a unigram tagger, and a default tagger, as follows:
  
 1. Try tagging the token with the bigram tagger.
  
 2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.
  
 3. If the unigram tagger is also unable to find a tag, use a default tagger.
  
 Most NLTK taggers permit a backoff tagger to be specified. The backoff tagger may 
 itself have a backoff tagger:
  
 >>> t0 = nltk.DefaultTagger('NN') 
  
 >>> t1 = nltk.UnigramTagger(train_sents, backoff=t0) >>> t2 = 
 nltk.BigramTagger(train_sents, backoff=t1) >>> 
 t2.evaluate(test_sents) 
  
 0.84491179108940495
  
  
 Your Turn:
  Extend the preceding example by defining a 
 TrigramTag 
 ger
  called 
 t3
 , which backs off to 
 t2
 .
  
 Note that we specify the backoff tagger when the tagger is initialized so that 
 training can take advantage of the backoff tagger. Thus, if the bigram tagger would 
 assign the same tag as its unigram backoff tagger in a certain context, the bigram 
 tagger discards the training instance. This keeps the bigram tagger model as small 
 as possible. We can further specify that a tagger needs to see more than one 
 instance of a context in order to retain it. For example, 
 nltk.BigramTagger(sents, 
 cutoff=2, backoff=t1)
  will dis-card contexts that have only been seen once or twice.
  
 5.5  N-Gram Tagging | 205",NA
Tagging Unknown Words,"Our approach to tagging unknown words still uses backoff to a regular expression 
 tagger or a default tagger. These are unable to make use of context. Thus, if our 
 tagger encountered the word 
 blog
 , not seen during training, it would assign it the 
 same tag, regardless of whether this word appeared in the context 
 the blog
  or 
 to 
 blog
 . How can we do better with these unknown words, or 
 out-of-vocabulary
  
 items?
  
 A useful method to tag unknown words based on context is to limit the vocabulary 
 of a tagger to the most frequent 
 n
  words, and to replace every other word with a 
 special word 
 UNK
  using the method shown in 
 Section 5.3
 . During training, a 
 unigram tagger will probably learn that 
 UNK
  is usually a noun. However, the n-
 gram taggers will detect contexts in which it has some other tag. For example, if the 
 preceding word is 
 to
  (tagged 
 TO
 ), then 
 UNK
  will probably be tagged as a verb.",NA
Storing Taggers,"Training a tagger on a large corpus may take a significant time. Instead of training a 
 tagger every time we need one, it is convenient to save a trained tagger in a file for 
 later reuse. Let’s save our tagger 
 t2
  to a file 
 t2.pkl
 :
  
 >>> from cPickle import dump 
  
 >>> output = open('t2.pkl', 'wb') 
  
 >>> dump(t2, output, -1) 
  
 >>> output.close()
  
 Now, in a separate Python process, we can load our saved tagger:
  
 >>> from cPickle import load 
  
 >>> input = open('t2.pkl', 'rb') 
  
 >>> tagger = load(input) 
  
 >>> input.close()
  
 Now let’s check that it can be used for tagging:
  
 >>> text = """"""The board's action shows what free enterprise 
  
 ...     is up against in our complex maze of regulatory laws ."""""" 
  
 >>> tokens = text.split() 
  
 >>> tagger.tag(tokens) 
  
 [('The', 'AT'), (""board's"", 'NN$'), ('action', 'NN'), ('shows', 'NNS'), 
  
 ('what', 'WDT'), ('free', 'JJ'), ('enterprise', 'NN'), ('is', 'BEZ'), 
  
 ('up', 'RP'), ('against', 'IN'), ('in', 'IN'), ('our', 'PP$'), ('complex', 'JJ'), ('maze', 'NN'), ('of', 'IN'), 
 ('regulatory', 'NN'), ('laws', 'NNS'), ('.', '.')]",NA
Performance Limitations,"What is the upper limit to the performance of an n-gram tagger? Consider the case 
 of a trigram tagger. How many cases of part-of-speech ambiguity does it encounter? 
 We can determine the answer to this question empirically:
  
 206 | Chapter 5:Categorizing and Tagging Words",NA
Tagging Across Sentence Boundaries,"An n-gram tagger uses recent tags to guide the choice of tag for the current word. 
 When tagging the first word of a sentence, a trigram tagger will be using the part-
 of-speech tag of the previous two tokens, which will normally be the last word of 
 the previous sentence and the sentence-ending punctuation. However, the lexical 
 category that closed the previous sentence has no bearing on the one that begins 
 the next sentence.
  
 To deal with this situation, we can train, run, and evaluate taggers using lists of 
 tagged sentences, as shown in 
 Example 5-5
 .
  
 Example 5-5. N-gram tagging at the sentence level.
  
 brown_tagged_sents = brown.tagged_sents(categories='news') 
 brown_sents = brown.sents(categories='news')
  
 size = int(len(brown_tagged_sents) * 0.9) 
  
 train_sents = brown_tagged_sents[:size] 
  
 test_sents = brown_tagged_sents[size:]
  
 t0 = nltk.DefaultTagger('NN') 
  
 t1 = nltk.UnigramTagger(train_sents, backoff=t0) t2 = 
 nltk.BigramTagger(train_sents, backoff=t1)
  
 >>> t2.evaluate(test_sents) 
  
 0.84491179108940495",NA
5.6  Transformation-Based Tagging,"A potential issue with n-gram taggers is the size of their n-gram table (or language 
 model). If tagging is to be employed in a variety of language technologies deployed 
 on mobile computing devices, it is important to strike a balance between model size 
 and tagger performance. An n-gram tagger with backoff may store trigram and 
 bigram ta-bles, which are large, sparse arrays that may have hundreds of millions 
 of entries.
  
 A second issue concerns context. The only information an n-gram tagger considers 
 from prior context is tags, even though words themselves might be a useful source 
 of information. It is simply impractical for n-gram models to be conditioned on the 
 iden-tities of words in the context. In this section, we examine Brill tagging, an 
 inductive tagging method which performs very well using models that are only a 
 tiny fraction of the size of n-gram taggers.
  
 Brill tagging is a kind of 
 transformation-based learning
 , named after its inventor. 
 The general idea is very simple: guess the tag of each word, then go back and fix the 
 mistakes.",NA
5.7  How to Determine the Category of a ,NA,NA
Word,"Now that we have examined word classes in detail, we turn to a more basic 
 question: how do we decide what category a word belongs to in the first place? In 
 general, linguists use morphological, syntactic, and semantic clues to determine the 
 category of a word.
  
 210 | Chapter 5:Categorizing and Tagging Words",NA
Morphological Clues,"The internal structure of a word may give useful clues as to the word’s category. 
 For example, 
 -ness
  is a suffix that combines with an adjective to produce a noun, 
 e.g., 
 happy
 → 
 happiness
 , 
 ill
 → 
 illness
 . So if we encounter a word that ends in 
 -ness
 , 
 this is very likely to be a noun. Similarly, 
 -ment
  is a suffix that combines with some 
 verbs to produce a noun, e.g., 
 govern
 → 
 government
  and 
 establish
 → 
 establishment
 .
  
 English verbs can also be morphologically complex. For instance, the 
 present par-
 ticiple
  of a verb ends in 
 -ing
 , and expresses the idea of ongoing, incomplete action 
 (e.g., 
 falling
 , 
 eating
 ). The 
 -ing
  suffix also appears on nouns derived from verbs, e.g., 
 the falling of the leaves
  (this is known as the 
 gerund
 ).",NA
Syntactic Clues,"Another source of information is the typical contexts in which a word can occur. 
 For example, assume that we have already determined the category of nouns. Then 
 we might say that a syntactic criterion for an adjective in English is that it can occur 
 im-mediately before a noun, or immediately following the words 
 be
  or 
 very
 . 
 According to these tests, 
 near
  should be categorized as an adjective:
  
 (2) a. the near window
  
 b. The end is (very) near.",NA
Semantic Clues,"Finally, the meaning of a word is a useful clue as to its lexical category. For example, 
 the best-known definition of a noun is semantic: “the name of a person, place, or 
 thing.”Within modern linguistics, semantic criteria for word classes are treated 
 with suspicion, mainly because they are hard to formalize. Nevertheless, semantic 
 criteria underpin many of our intuitions about word classes, and enable us to make 
 a good guess about the categorization of words in languages with which we are 
 unfamiliar. For example, if all we know about the Dutch word 
 verjaardag
  is that it 
 means the same as the English word 
 birthday
 , then we can guess that 
 verjaardag
  is 
 a noun in Dutch. However, some care is needed: although we might translate 
 zij is 
 vandaag jarig
  as 
 it’s her birthday to-day
 , the word 
 jarig
  is in fact an adjective in 
 Dutch, and has no exact equivalent in English.",NA
New Words,"All languages acquire new lexical items. A list of words recently added to the Oxford 
 Dictionary of English includes 
 cyberslacker
 , 
 fatoush
 , 
 blamestorm
 , 
 SARS
 , 
 cantopop
 , 
 bupkis
 , 
 noughties
 , 
 muggle
 , and 
 robata
 . Notice that all these new words are nouns, 
 and this is reflected in calling nouns an 
 open class
 . By contrast, prepositions are 
 regarded as a 
 closed class
 . That is, there is a limited set of words belonging to the 
 class (e.g., 
 above
 , 
 along
 , 
 at
 , 
 below
 , 
 beside
 , 
 between
 , 
 during
 , 
 for
 , 
 from
 , 
 in
 , 
 near
 , 
 on
 , 
 outside
 , 
 over
 ,
  
 5.7  How to Determine the Category of a Word | 211",NA
Morphology in Part-of-Speech Tagsets,"Common tagsets often capture some 
 morphosyntactic
  information, that is, 
 informa-tion about the kind of morphological markings that words receive by 
 virtue of their syntactic role. Consider, for example, the selection of distinct 
 grammatical forms of the word 
 go
  illustrated in the following sentences:
  
 (3) a.
  Go
  away!
  
 b. He sometimes 
 goes
  to the cafe.
  
 c. All the cakes have 
 gone
 .
  
 d. We 
 went
  on the excursion.
  
 Each of these forms—
 go
 , 
 goes
 , 
 gone
 , and 
 went
 —is morphologically distinct from 
 the others. Consider the form 
 goes
 . This occurs in a restricted set of grammatical 
 contexts, and requires a third person singular subject. Thus, the following 
 sentences are ungrammatical.
  
 (4) a. *They sometimes 
 goes
  to the cafe.
  
 b. *I sometimes 
 goes
  to the cafe.
  
 By contrast, 
 gone
  is the past participle form; it is required after 
 have
  (and cannot be 
 replaced in this context by 
 goes
 ), and cannot occur as the main verb of a clause.
  
 (5) a. *All the cakes have 
 goes
 .
  
 b. *He sometimes 
 gone
  to the cafe.
  
 We can easily imagine a tagset in which the four distinct grammatical forms just 
 dis-cussed were all tagged as 
 VB
 . Although this would be adequate for some 
 purposes, a more fine-grained tagset provides useful information about these forms 
 that can help other processors that try to detect patterns in tag sequences. The 
 Brown tagset captures these distinctions, as summarized in 
 Table 5-7
 .
  
 Table 5-7. Some morphosyntactic distinctions in the Brown tagset
  
  
  
  
 For
 m
  
 Category
  
  
 Tag
  
 go
  
 base
  
 VB
  
 goe
 s
  
 third singular 
 present
  
 VBZ
  
 gon
 e
  
 past participle
  
 VBN
  
 goi
 ng
  
 gerund
  
 VBG
  
 we
 nt
  
 simple past
  
 VBD
  
 212 | Chapter 5:Categorizing and Tagging Words",NA
5.8  Summary,"• Words can be grouped into classes, such as nouns, verbs, adjectives, and 
 adverbs. These classes are known as lexical categories or parts-of-speech. 
 Parts-of-speech are assigned short labels, or tags, such as 
 NN
  and 
 VB
 .
  
 • The process of automatically assigning parts-of-speech to words in text is called 
  
 part-of-speech tagging, POS tagging, or just tagging.
  
 • Automatic tagging is an important step in the NLP pipeline, and is useful in a 
 variety of situations, including predicting the behavior of previously unseen 
 words, ana-lyzing word usage in corpora, and text-to-speech systems.
  
 • Some linguistic corpora, such as the Brown Corpus, have been POS tagged.
  
 • A variety of tagging methods are possible, e.g., default tagger, regular expression 
 tagger, unigram tagger, and n-gram taggers. These can be combined using a 
 tech-nique known as backoff.
  
 • Taggers can be trained and evaluated using tagged corpora.
  
 • Backoff is a method for combining models: when a more specialized model (such 
 as a bigram tagger) cannot assign a tag in a given context, we back off to a more 
 general model (such as a unigram tagger).
  
 • Part-of-speech tagging is an important, early example of a sequence 
 classification task in NLP: a classification decision at any one point in the 
 sequence makes use of words and tags in the local context.
  
 • A dictionary is used to map between arbitrary types of information, such as a 
 string and a number: 
 freq['cat'] = 12
 . We create dictionaries using the brace 
 notation: 
 pos = {}
 , 
 pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}
 .
  
 • N-gram taggers can be defined for large values of 
 n
 , but once 
 n
  is larger than 3, 
 we usually encounter the sparse data problem; even with a large quantity of 
 training data, we see only a tiny fraction of possible contexts.
  
 5.8  Summary | 213",NA
5.9  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web. For more examples of tagging with NLTK, 
 please see the Tagging HOWTO at 
 http://www.nltk.org/howto
 . Chapters 4 and 5 of 
 (Jurafsky & Martin, 2008) contain more advanced material on n-grams and part-of-
 speech tag-ging. Other approaches to tagging involve machine learning methods 
 (
 Chapter 6
 ). In 
 Chapter 7
 , we will see a generalization of tagging called 
 chunking
  in 
 which a contiguous sequence of words is assigned a single tag.
  
 For tagset documentation, see 
 nltk.help.upenn_tagset()
  and 
 nltk.help.brown_tag set()
 . 
 Lexical categories are introduced in linguistics textbooks, including those listed in 
 Chapter 1
  of this book.
  
 There are many other kinds of tagging. Words can be tagged with directives to a 
 speech synthesizer, indicating which words should be emphasized. Words can be 
 tagged with sense numbers, indicating which sense of the word was used. Words 
 can also be tagged with morphological features. Examples of each of these kinds of 
 tags are shown in the following list. For space reasons, we only show the tag for a 
 single word. Note also that the first two examples use XML-style tags, where 
 elements in angle brackets enclose the word that is tagged.
  
 Speech Synthesis Markup Language (W3C SSML) 
  
 That is a <emphasis>big</emphasis> car!
  
 SemCor: Brown Corpus tagged with WordNet senses 
  
 Space in any <wf pos=""NN"" lemma=""form"" wnsn=""4"">form</wf> is completely meas 
 ured by the three dimensions.
  (Wordnet form/nn sense 4: “shape, form, config-
 uration, contour, conformation”)
  
 Morphological tagging, from the Turin University Italian Treebank 
  
 E' italiano , come progetto e realizzazione , il primo (PRIMO ADJ ORDIN M SING) porto 
 turistico dell' Albania .
  
 Note that tagging is also performed at higher levels. Here is an example of dialogue 
 act tagging, from the NPS Chat Corpus (Forsyth & Martell, 2007) included with 
 NLTK.
  
 Each turn of the dialogue is categorized as to its communicative function:
  
 Statement  User117 Dude..., I wanted some of that 
 ynQuestion User120 m I missing something?
  
 Bye        User117 I'm gonna go fix food, I'll be back later.
  
 System     User122 JOIN 
  
 System     User2   slaps User122 around a bit with a large trout.
  
 Statement  User121 18/m pm me if u tryin to chat
  
 214 | Chapter 5:Categorizing and Tagging Words",NA
5.10  Exercises,"1.
  ○ 
 Search the Web for “spoof newspaper headlines,” to find such gems as: 
 British 
 Left Waffles on Falkland Islands
 , and 
 Juvenile Court to Try Shooting Defendant
 . 
 Manually tag these headlines to see whether knowledge of the part-of-speech 
 tags removes the ambiguity.
  
 2.
  ○ 
 Working with someone else, take turns picking a word that can be either a 
 noun or a verb (e.g., 
 contest
 ); the opponent has to predict which one is likely to 
 be the most frequent in the Brown Corpus. Check the opponent’s prediction, 
 and tally the score over several turns.
  
 3.
  ○ 
 Tokenize and tag the following sentence: 
 They wind back the clock, while we 
 chase after the wind
 . What different pronunciations and parts-of-speech are 
 involved?
  
 4.
  ○ 
 Review the mappings in 
 Table 5-4
 . Discuss any other examples of mappings 
 you can think of. What type of information do they map from and to?
  
 5.
  ○ 
 Using the Python interpreter in interactive mode, experiment with the 
 dictionary examples in this chapter. Create a dictionary 
 d
 , and add some 
 entries. What hap-pens whether you try to access a non-existent entry, e.g., 
 d['xyz']
 ?
  
 6.
  ○ 
 Try deleting an element from a dictionary 
 d
 , using the syntax 
 del d['abc']
 . 
 Check that the item was deleted.
  
 7.
  ○ 
 Create two dictionaries, 
 d1
  and 
 d2
 , and add some entries to each. Now issue 
 the command 
 d1.update(d2)
 . What did this do? What might it be useful for?
  
 8.
  ○ 
 Create a dictionary 
 e
 , to represent a single lexical entry for some word of your 
 choice. Define keys such as 
 headword
 , 
 part-of-speech
 , 
 sense
 , and 
 example
 , and as-
 sign them suitable values.
  
 9.
  ○ 
 Satisfy yourself that there are restrictions on the distribution of 
 go
  and 
 went
 , 
 in the sense that they cannot be freely interchanged in the kinds of contexts 
 illustrated in 
 (3)
 , 
 Section 5.7
 .
  
 10.
  ○ 
 Train a unigram tagger and run it on some new text. Observe that some 
 words are not assigned a tag. Why not?
  
 11.
  ○ 
 Learn about the affix tagger (type 
 help(nltk.AffixTagger)
 ). Train an affix tagger 
 and run it on some new text. Experiment with different settings for the affix 
 length and the minimum word length. Discuss your findings.
  
 12.
  ○ 
 Train a bigram tagger with no backoff tagger, and run it on some of the 
 training data. Next, run it on some new data. What happens to the performance 
 of the tagger? Why?
  
 13.
  ○ 
 We can use a dictionary to specify the values to be substituted into a 
 formatting string. Read Python’s library documentation for formatting strings 
 (
 http://docs.py",NA
CHAPTER 6,NA,NA
Learning to Classify Text,"Detecting patterns is a central part of Natural Language Processing. Words ending 
 in
 -ed
  tend to be past tense verbs (
 Chapter 5
 ). Frequent use of 
 will
  is indicative of 
 news text (
 Chapter 3
 ). These observable patterns—word structure and word 
 frequency—happen to correlate with particular aspects of meaning, such as tense 
 and topic. But how did we know where to start looking, which aspects of form to 
 associate with which aspects of meaning?
  
 The goal of this chapter is to answer the following questions:
  
 1. How can we identify particular features of language data that are salient for 
 clas-sifying it?
  
 2. How can we construct models of language that can be used to perform language 
 processing tasks automatically?
  
 3. What can we learn about language from these models?
  
 Along the way we will study some important machine learning techniques, 
 including decision trees, naive Bayes classifiers, and maximum entropy classifiers. 
 We will gloss over the mathematical and statistical underpinnings of these 
 techniques, focusing in-stead on how and when to use them (see 
 Section 6.9
  for 
 more technical background). Before looking at these methods, we first need to 
 appreciate the broad scope of this topic.",NA
6.1  Supervised Classification,"Classification
  is the task of choosing the correct 
 class label
  for a given input. In 
 basic classification tasks, each input is considered in isolation from all other inputs, 
 and the set of labels is defined in advance. Some examples of classification tasks 
 are:
  
 221",NA
Gender Identification,"In 
 Section 2.4
 , we saw that male and female names have some distinctive 
 characteristics. Names ending in 
 a
 , 
 e
 , and 
 i
  are likely to be female, while names 
 ending in 
 k
 , 
 o
 , 
 r
 , 
 s
 , and 
 t
  are likely to be male. Let’s build a classifier to model these 
 differences more precisely.",NA
Choosing the Right Features,"Selecting relevant features and deciding how to encode them for a learning method 
 can have an enormous impact on the learning method’s ability to extract a good 
 model. Much of the interesting work in building a classifier is deciding what 
 features might be relevant, and how we can represent them. Although it’s often 
 possible to get decent performance by using a fairly simple and obvious set of 
 features, there are usually sig-nificant gains to be had by using carefully 
 constructed features based on a thorough understanding of the task at hand.
  
 Typically, feature extractors are built through a process of trial-and-error, guided 
 by intuitions about what information is relevant to the problem. It’s common to 
 start with a “kitchen sink” approach, including all the features that you can think of, 
 and then checking to see which features actually are helpful. We take this approach 
 for name gender features in 
 Example 6-1
 .
  
 224 | Chapter 6:Learning to Classify Text",NA
Document Classification,"In 
 Section 2.1
 , we saw several examples of corpora where documents have been 
 labeled with categories. Using these corpora, we can build classifiers that will 
 automatically tag new documents with appropriate category labels. First, we 
 construct a list of docu-ments, labeled with the appropriate categories. For this 
 example, we’ve chosen the Movie Reviews Corpus, which categorizes each review 
 as positive or negative.
  
 >>> from nltk.corpus import movie_reviews 
  
 >>> documents = [(list(movie_reviews.words(fileid)), category) ...              
 for category in movie_reviews.categories() ...              for fileid in 
 movie_reviews.fileids(category)] >>> random.shuffle(documents)
  
 Next, we define a feature extractor for documents, so the classifier will know which 
 aspects of the data it should pay attention to (see 
 Example 6-2
 ). For document topic 
 identification, we can define a feature for each word, indicating whether the 
 document contains that word. To limit the number of features that the classifier 
 needs to process, we begin by constructing a list of the 2,000 most frequent words 
 in the overall
  
 6.1  Supervised Classification | 227",NA
Part-of-Speech Tagging,"In 
 Chapter 5
 , we built a regular expression tagger that chooses a part-of-speech tag 
 for a word by looking at the internal makeup of the word. However, this regular 
 expression tagger had to be handcrafted. Instead, we can train a classifier to work 
 out which suf-fixes are most informative. Let’s begin by finding the most common 
 suffixes:
  
 >>> from nltk.corpus import brown 
  
 >>> suffix_fdist = nltk.FreqDist() 
  
 >>> for word in brown.words(): 
  
 ...     word = word.lower() 
  
 ...     suffix_fdist.inc(word[-1:]) 
  
 ...     suffix_fdist.inc(word[-2:]) 
  
 ...     suffix_fdist.inc(word[-3:])
  
 >>> common_suffixes = suffix_fdist.keys()[:100] 
  
 >>> print common_suffixes 
  
 ['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 
 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', ""''"", 'm', 
 ';', 'i', 'ly', 'ion', ...]
  
 Next, we’ll define a feature extractor function that checks a given word for these 
 suffixes:
  
 >>> def pos_features(word): 
  
 ...     features = {} 
  
 ...     for suffix in common_suffixes: 
  
 ...         features['endswith(%s)' % suffix] = word.lower().endswith(suffix) ...     return 
 features
  
 Feature extraction functions behave like tinted glasses, highlighting some of the 
 prop-erties (colors) in our data and making it impossible to see other properties. 
 The classifier will rely exclusively on these highlighted properties when 
 determining how to label inputs. In this case, the classifier will make its decisions 
 based only on information about which of the common suffixes (if any) a given 
 word has.
  
 Now that we’ve defined our feature extractor, we can use it to train a new “decision 
 tree” classifier (to be discussed in 
 Section 6.4
 ):
  
 >>> tagged_words = brown.tagged_words(categories='news') 
  
 >>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]
  
 >>> size = int(len(featuresets) * 0.1) 
  
 >>> train_set, test_set = featuresets[size:], featuresets[:size]
  
 >>> classifier = nltk.DecisionTreeClassifier.train(train_set) >>> 
 nltk.classify.accuracy(classifier, test_set) 
  
 0.62705121829935351
  
 >>> classifier.classify(pos_features('cats')) 
  
 'NNS'
  
 One nice feature of decision tree models is that they are often fairly easy to 
 interpret. We can even instruct NLTK to print them out as pseudocode:
  
 6.1  Supervised Classification | 229",NA
Exploiting Context,"By augmenting the feature extraction function, we could modify this part-of-speech 
 tagger to leverage a variety of other word-internal features, such as the length of 
 the word, the number of syllables it contains, or its prefix. However, as long as the 
 feature extractor just looks at the target word, we have no way to add features that 
 depend on the 
 context
  in which the word appears. But contextual features often 
 provide powerful clues about the correct tag—for example, when tagging the word 
 fly
 , knowing that the previous word is 
 a
  will allow us to determine that it is 
 functioning as a noun, not a verb.
  
 In order to accommodate features that depend on a word’s context, we must revise 
 the pattern that we used to define our feature extractor. Instead of just passing in 
 the word to be tagged, we will pass in a complete (untagged) sentence, along with 
 the index of the target word. This approach is demonstrated in 
 Example 6-4
 , which 
 employs a con-text-dependent feature extractor to define a part-of-speech tag 
 classifier.",NA
Sequence Classification,"In order to capture the dependencies between related classification tasks, we can 
 use 
 joint classifier
  models, which choose an appropriate labeling for a collection of 
 related inputs. In the case of part-of-speech tagging, a variety of different 
 sequence 
 classifier
  models can be used to jointly choose part-of-speech tags for all the words 
 in a given sentence.
  
 6.1  Supervised Classification | 231",NA
Other Methods for Sequence Classification,"One shortcoming of this approach is that we commit to every decision that we 
 make. For example, if we decide to label a word as a noun, but later find evidence 
 that it should have been a verb, there’s no way to go back and fix our mistake. One 
 solution to this problem is to adopt a transformational strategy instead. 
 Transformational joint classi-fiers work by creating an initial assignment of labels 
 for the inputs, and then iteratively refining that assignment in an attempt to repair 
 inconsistencies between related inputs. The Brill tagger, described in 
 Section 5.6
 , is 
 a good example of this strategy.
  
 Another solution is to assign scores to all of the possible sequences of part-of-
 speech tags, and to choose the sequence whose overall score is highest. This is the 
 approach taken by 
 Hidden Markov Models
 . Hidden Markov Models are similar to 
 consecutive classifiers in that they look at both the inputs and the history of 
 predicted tags. How-ever, rather than simply finding the single best tag for a given 
 word, they generate a probability distribution over tags. These probabilities are 
 then combined to calculate probability scores for tag sequences, and the tag 
 sequence with the highest probability is chosen. Unfortunately, the number of 
 possible tag sequences is quite large. Given a tag set with 30 tags, there are about 
 600 trillion (30
 10
 ) ways to label a 10-word sentence. In order to avoid considering 
 all these possible sequences separately, Hidden Markov Models require that the 
 feature extractor only look at the most recent tag (or the most recent 
 n
  tags, where 
 n
  is fairly small). Given that restriction, it is possible to use dynamic programming 
 (
 Section 4.7
 ) to efficiently find the most likely tag sequence. In particular, for each 
 consecutive word index 
 i
 , a score is computed for each possible current and 
 previous tag. This same basic approach is taken by two more advanced models, 
 called 
 Maximum Entropy Markov Models
  and 
 Linear-Chain Conditional 
 Random Field Models
 ; but different algorithms are used to find scores for tag 
 sequences.",NA
6.2  Further Examples of Supervised ,NA,NA
Classification,NA,NA
Sentence Segmentation,"Sentence segmentation can be viewed as a classification task for punctuation: 
 whenever we encounter a symbol that could possibly end a sentence, such as a",NA
Identifying Dialogue Act Types,"When processing dialogue, it can be useful to think of utterances as a type of 
 action 
 performed by the speaker. This interpretation is most straightforward for 
 performative statements such as 
 I forgive you
  or 
 I bet you can’t climb that hill
 . But 
 greetings, questions, answers, assertions, and clarifications can all be thought of as 
 types of speech-based actions. Recognizing the 
 dialogue acts
  underlying the 
 utterances in a dialogue can be an important first step in understanding the 
 conversation.
  
 The NPS Chat Corpus, which was demonstrated in 
 Section 2.1
 , consists of over 
 10,000 posts from instant messaging sessions. These posts have all been labeled 
 with one of 15 dialogue act types, such as “Statement,” “Emotion,” “ynQuestion,” 
 and “Contin-uer.” We can therefore use this data to build a classifier that can 
 identify the dialogue act types for new instant messaging posts. The first step is to 
 extract the basic messaging data. We will call 
 xml_posts()
  to get a data structure 
 representing the XML annotation for each post:
  
 >>> posts = nltk.corpus.nps_chat.xml_posts()[:10000]
  
 Next, we’ll define a simple feature extractor that checks what words the post 
 contains:
  
 >>> def dialogue_act_features(post): 
  
 ...     features = {} 
  
 ...     for word in nltk.word_tokenize(post): 
  
 ...         features['contains(%s)' % word.lower()] = True ...     return 
 features
  
 Finally, we construct the training and testing data by applying the feature extractor 
 to each post (using 
 post.get('class')
  to get a post’s dialogue act type), and create a 
 new classifier:
  
 >>> featuresets = [(dialogue_act_features(post.text), post.get('class')) ...                for 
 post in posts] 
  
 >>> size = int(len(featuresets) * 0.1) 
  
 >>> train_set, test_set = featuresets[size:], featuresets[:size] 
  
 >>> classifier = nltk.NaiveBayesClassifier.train(train_set) 
  
 >>> print nltk.classify.accuracy(classifier, test_set) 
  
 0.66",NA
Recognizing Textual Entailment,"Recognizing textual entailment (RTE) is the task of determining whether a given 
 piece of text 
 T
  entails another text called the “hypothesis” (as already discussed in 
 Sec-tion 1.5
 ). To date, there have been four RTE Challenges, where shared 
 development and test data is made available to competing teams. Here are a couple 
 of examples of text/hypothesis pairs from the Challenge 3 development dataset. 
 The label 
 True
  indi-cates that the entailment holds, and 
 False
  indicates that it fails 
 to hold.",NA
Scaling Up to Large Datasets,"Python provides an excellent environment for performing basic text processing and 
 feature extraction. However, it is not able to perform the numerically intensive 
 calcu-lations required by machine learning methods nearly as quickly as lower-
 level languages such as C. Thus, if you attempt to use the pure-Python machine 
 learning implemen-tations (such as 
 nltk.NaiveBayesClassifier
 ) on large datasets, you 
 may find that the learning algorithm takes an unreasonable amount of time and 
 memory to complete.
  
 If you plan to train classifiers with large amounts of training data or a large number 
 of features, we recommend that you explore NLTK’s facilities for interfacing with 
 external machine learning packages. Once these packages have been installed, 
 NLTK can trans-parently invoke them (via system calls) to train classifier models 
 significantly faster than the pure-Python classifier implementations. See the NLTK 
 web page for a list of rec-ommended machine learning packages that are supported 
 by NLTK.",NA
6.3  Evaluation,"In order to decide whether a classification model is accurately capturing a pattern, 
 we must evaluate that model. The result of this evaluation is important for deciding 
 how trustworthy the model is, and for what purposes we can use it. Evaluation can 
 also be an effective tool for guiding us in making future improvements to the model.",NA
The Test Set,"Most evaluation techniques calculate a score for a model by comparing the labels 
 that it generates for the inputs in a 
 test set
  (or 
 evaluation set
 ) with the correct 
 labels for",NA
Accuracy,"The simplest metric that can be used to evaluate a classifier, 
 accuracy
 , measures 
 the percentage of inputs in the test set that the classifier correctly labeled. For 
 example, a name gender classifier that predicts the correct name 60 times in a test 
 set containing 80 names would have an accuracy of 60/80 = 75%. The function 
 nltk.classify.accu racy()
  will calculate the accuracy of a classifier model on a given test 
 set:
  
 >>> classifier = nltk.NaiveBayesClassifier.train(train_set) 
  
 >>> print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test_set) 0.75
  
 When interpreting the accuracy score of a classifier, it is important to consider the 
 frequencies of the individual class labels in the test set. For example, consider a 
 classifier that determines the correct word sense for each occurrence of the word 
 bank
 . If we evaluate this classifier on financial newswire text, then we may find that 
 the 
 financial-institution
  sense appears 19 times out of 20. In that case, an accuracy of 
 95% would hardly be impressive, since we could achieve that accuracy with a 
 model that always returns the 
 financial-institution
  sense. However, if we instead 
 evaluate the classifier on a more balanced corpus, where the most frequent word 
 sense has a frequency of 40%, then a 95% accuracy score would be a much more 
 positive result. (A similar issue arises when measuring inter-annotator agreement 
 in 
 Section 11.2
 .)",NA
Precision and Recall,"Another instance where accuracy scores can be misleading is in “search” tasks, such 
 as information retrieval, where we are attempting to find documents that are 
 relevant to a particular task. Since the number of irrelevant documents far 
 outweighs the number of relevant documents, the accuracy score for a model that 
 labels every document as irrelevant would be very close to 100%.
  
 It is therefore conventional to employ a different set of measures for search tasks, 
 based on the number of items in each of the four categories shown in 
 Figure 6-3
 :
  
 •
  True positives
  are relevant items that we correctly identified as relevant.
  
 •
  True negatives
  are irrelevant items that we correctly identified as irrelevant.
  
 •
  False positives
  (or 
 Type I errors
 ) are irrelevant items that we incorrectly 
 identi-
  
 fied as relevant.
  
 •
  False negatives
  (or 
 Type II errors
 ) are relevant items that we incorrectly 
 identi-
  
 fied as irrelevant.
  
 6.3  Evaluation | 239",NA
Confusion Matrices,"When performing classification tasks with three or more labels, it can be 
 informative to subdivide the errors made by the model based on which types of 
 mistake it made. A 
 confusion matrix
  is a table where each cell [
 i
 ,
 j
 ] indicates how 
 often label 
 j
  was pre-dicted when the correct label was 
 i
 . Thus, the diagonal entries 
 (i.e., cells [
 i
 ,
 j
 ]) indicate labels that were correctly predicted, and the off-diagonal 
 entries indicate errors. In the following example, we generate a confusion matrix 
 for the unigram tagger developed in 
 Section 5.4
 :
  
 240 | Chapter 6:Learning to Classify Text",NA
Cross-Validation,"In order to evaluate our models, we must reserve a portion of the annotated data 
 for the test set. As we already mentioned, if the test set is too small, our evaluation 
 may not be accurate. However, making the test set larger usually means making the 
 training set smaller, which can have a significant impact on performance if a limited 
 amount of annotated data is available.
  
 One solution to this problem is to perform multiple evaluations on different test 
 sets, then to combine the scores from those evaluations, a technique known as 
 cross-validation
 . In particular, we subdivide the original corpus into 
 N
  subsets 
 called 
 folds
 . For each of these folds, we train a model using all of the data 
 except
  the 
 data in that fold, and then test that model on the fold. Even though the individual 
 folds might be too small to give accurate evaluation scores on their own, the 
 combined evaluation score is based on a large amount of data and is therefore quite 
 reliable.
  
 A second, and equally important, advantage of using cross-validation is that it 
 allows us to examine how widely the performance varies across different training 
 sets. If we get very similar scores for all 
 N
  training sets, then we can be fairly 
 confident that the score is accurate. On the other hand, if scores vary widely across 
 the 
 N
  training sets, then we should probably be skeptical about the accuracy of the 
 evaluation score.",NA
6.4  Decision Trees,"In the next three sections, we’ll take a closer look at three machine learning 
 methods that can be used to automatically build classification models: decision 
 trees, naive Bayes classifiers, and Maximum Entropy classifiers. As we’ve seen, it’s 
 possible to treat these learning methods as black boxes, simply training models and 
 using them for prediction without understanding how they work. But there’s a lot 
 to be learned from taking a closer look at how these learning methods select 
 models based on the data in a training set. An understanding of these methods can 
 help guide our selection of appropriate features, and especially our decisions about 
 how those features should be encoded. And an understanding of the generated 
 models can allow us to extract information about which features are most 
 informative, and how those features relate to one an-other.
  
 A 
 decision tree
  is a simple flowchart that selects labels for input values. This 
 flowchart consists of 
 decision nodes
 , which check feature values, and 
 leaf nodes
 , 
 which assign labels. To choose the label for an input value, we begin at the 
 flowchart’s initial decision node, known as its 
 root node
 . This node contains a 
 condition that checks one of the input value’s features, and selects a branch based 
 on that feature’s value. Following the branch that describes our input value, we 
 arrive at a new decision node, with a new condition on the input value’s features. 
 We continue following the branch selected by each node’s condition, until we arrive 
 at a leaf node which provides a label for the input value. 
 Figure 6-4
  shows an 
 example decision tree model for the name gender task.
  
 Once we have a decision tree, it is straightforward to use it to assign labels to new 
 input values. What’s less straightforward is how we can build a decision tree that 
 models a given training set. But before we look at the learning algorithm for 
 building decision trees, we’ll consider a simpler task: picking the best “decision 
 stump” for a corpus. A",NA
Entropy and Information Gain,"As was mentioned before, there are several methods for identifying the most 
 informa-tive feature for a decision stump. One popular alternative, called 
 information gain
 , measures how much more organized the input values become 
 when we divide them up using a given feature. To measure how disorganized the 
 original set of input values are, we calculate entropy of their labels, which will be 
 high if the input values have highly varied labels, and low if many input values all 
 have the same label. In particular, entropy is defined as the sum of the probability 
 of each label times the log probability of that same label:
  
 (1)
  H =
 Σ
 l
 ∈
  labels
 P
 (
 l
 ) 
 ×
  log
 2
 P
 (
 l
 ).
  
 For example, 
 Figure 6-5
  shows how the entropy of labels in the name gender 
 prediction task depends on the ratio of male to female names. Note that if most 
 input values have the same label (e.g., if 
 P
 (male) is near 0 or near 1), then entropy 
 is low. In particular, labels that have low frequency do not contribute much to the 
 entropy (since 
 P
 (
 l
 ) is small), and labels with high frequency also do not contribute 
 much to the entropy (since log
 2
 P
 (
 l
 ) is small). On the other hand, if the input values 
 have a wide variety of labels, then there are many labels with a “medium” 
 frequency, where neither 
 P
 (
 l
 ) nor log
 2
 P
 (
 l
 ) is small, so the entropy is high. 
 Example 
 6-8
  demonstrates how to calculate the entropy of a list of labels.
  
 6.4  Decision Trees | 243",NA
6.5  Naive Bayes Classifiers,"In 
 naive Bayes
  classifiers, every feature gets a say in determining which label 
 should be assigned to a given input value. To choose a label for an input value, the 
 naive Bayes",NA
Underlying Probabilistic Model,"Another way of understanding the naive Bayes classifier is that it chooses the most 
 likely label for an input, under the assumption that every input value is generated 
 by first choosing a class label for that input value, and then generating each feature, 
 entirely independent of every other feature. Of course, this assumption is 
 unrealistic; features are often highly dependent on one another. We’ll return to 
 some of the consequences of this assumption at the end of this section. This 
 simplifying assumption, known as the 
 naive Bayes assumption
  (or 
 independence 
 assumption
 ), makes it much easier",NA
Zero Counts and Smoothing,"The simplest way to calculate 
 P(f|label)
 , the contribution of a feature 
 f
  toward the 
 label likelihood for a label 
 label
 , is to take the percentage of training instances with 
 the given label that also have the given feature:
  
 (6)
  P
 (
 f|label
 ) = 
 count
 (
 f
 , 
 label
 )
 /count
 (
 label
 )
  
 248 | Chapter 6:Learning to Classify Text",NA
Non-Binary Features,"We have assumed here that each feature is binary, i.e., that each input either has a 
 feature or does not. Label-valued features (e.g., a color feature, which could be 
 red
 , 
 green
 , 
 blue
 , 
 white
 , or 
 orange
 ) can be converted to binary features by replacing them 
 with binary features, such as “color-is-red”. Numeric features can be converted to 
 bi-nary features by 
 binning
 , which replaces them with features such as “4<x<6.”
  
 Another alternative is to use regression methods to model the probabilities of 
 numeric features. For example, if we assume that the height feature has a bell curve 
 distribution, then we could estimate 
 P
 (
 height
 |
 label
 ) by finding the mean and 
 variance of the heights of the inputs with each label. In this case, 
 P
 (
 f=v|label
 ) would 
 not be a fixed value, but would vary depending on the value of 
 v
 .",NA
The Naivete of Independence,"The reason that naive Bayes classifiers are called “naive” is that it’s unreasonable to 
 assume that all features are independent of one another (given the label). In 
 particular, almost all real-world problems contain features with varying degrees of 
 dependence on one another. If we had to avoid any features that were dependent 
 on one another, it would be very difficult to construct good feature sets that 
 provide the required infor-mation to the machine learning algorithm.
  
 6.5  Naive Bayes Classifiers | 249",NA
The Cause of Double-Counting,"The reason for the double-counting problem is that during training, feature 
 contribu-tions are computed separately; but when using the classifier to choose 
 labels for new inputs, those feature contributions are combined. One solution, 
 therefore, is to con-sider the possible interactions between feature contributions 
 during training. We could then use those interactions to adjust the contributions 
 that individual features make.
  
 To make this more precise, we can rewrite the equation used to calculate the 
 likelihood of a label, separating out the contribution made by each feature (or 
 label):
  
 (7)
  P
 (
 features
 , 
 label
 ) 
 =w
 [
 label
 ] × 
 ⊓
 f
 ∈
 features
 w
 [
 f
 , 
 label
 ]
  
 Here, 
 w
 [
 label
 ] is the “starting score” for a given label, and 
 w
 [
 f
 , 
 label
 ] is the 
 contribution made by a given feature towards a label’s likelihood. We call these 
 values 
 w
 [
 label
 ] and 
 w
 [
 f
 , 
 label
 ] the 
 parameters
  or 
 weights
  for the model. Using the 
 naive Bayes algorithm, we set each of these parameters independently:
  
 (8)
  w
 [
 label
 ] 
 =P
 (
 label
 )
  
 (9)
  w
 [
 f
 , 
 label
 ] 
 = P
 (
 f|label
 )
  
 However, in the next section, we’ll look at a classifier that considers the possible in-
 teractions between these parameters when choosing their values.",NA
6.6  Maximum Entropy Classifiers,NA,NA
The Maximum Entropy Model,"The Maximum Entropy classifier model is a generalization of the model used by the 
 naive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier 
 calculates the likelihood of each label for a given input value by multiplying 
 together the parameters that are applicable for the input value and label. The naive 
 Bayes clas-sifier model defines a parameter for each label, specifying its prior 
 probability, and a parameter for each (feature, label) pair, specifying the 
 contribution of individual fea-tures toward a label’s likelihood.
  
 In contrast, the Maximum Entropy classifier model leaves it up to the user to decide 
 what combinations of labels and features should receive their own parameters. In 
 par-ticular, it is possible to use a single parameter to associate a feature with more 
 than one label; or to associate more than one feature with a given label. This will 
 sometimes",NA
Maximizing Entropy,"The intuition that motivates Maximum Entropy classification is that we should 
 build a model that captures the frequencies of individual joint-features, without 
 making any unwarranted assumptions. An example will help to illustrate this 
 principle.
  
 Suppose we are assigned the task of picking the correct word sense for a given 
 word, from a list of 10 possible senses (labeled A–J). At first, we are not told 
 anything more about the word or the senses. There are many probability 
 distributions that we could choose for the 10 senses, such as:
  
  
  
  
  
  
  
  
  
  
  
  
  
 A
  
 B
  
  
 C
  
  
 D
  
  
 E
  
  
 F
  
  
 G
  
  
 H
  
  
 I
  
  
 J
  
  
 (i)
  
 10
 %
  
 10%
  
 10%
  
 10%
  
 10%
  
 10%
  
 10%
  
 10%
  
 10%
  
 10%
  
 (ii)
  
 5
 %
  
 15%
  
 0%
  
 30%
  
 0%
  
 8%
  
 12%
  
 0%
  
 6%
  
 24%
  
 (iii)
  
 0
 %
  
 100
 %
  
 0%
  
 0%
  
 0%
  
 0%
  
 0%
  
 0%
  
 0%
  
 0%
  
 Although any of these distributions 
 might
  be correct, we are likely to choose 
 distribution 
 (i)
 , because without any more information, there is no reason to 
 believe that any word sense is more likely than any other. On the other hand, 
 distributions 
 (ii)
  and 
 (iii)
  reflect assumptions that are not supported by what we 
 know.
  
 One way to capture this intuition that distribution 
 (i)
  is more “fair” than the other 
 two is to invoke the concept of entropy. In the discussion of decision trees, we 
 described",NA
Generative Versus Conditional Classifiers,"An important difference between the naive Bayes classifier and the Maximum 
 Entropy classifier concerns the types of questions they can be used to answer. The 
 naive Bayes classifier is an example of a 
 generative
  classifier, which builds a model 
 that predicts 
 P
 (
 input
 , 
 label
 ), the joint probability of an (
 input
 , 
 label
 ) pair. As a result, 
 generative models can be used to answer the following questions:
  
 1. What is the most likely label for a given input?
  
 2. How likely is a given label for a given input?
  
 3. What is the most likely input value?
  
 4. How likely is a given input value?
  
 5. How likely is a given input value with a given label?
  
 6. What is the most likely label for an input that might have one of two values (but 
 we don’t know which)?
  
 The Maximum Entropy classifier, on the other hand, is an example of a 
 conditional 
 classifier. Conditional classifiers build models that predict 
 P
 (
 label|input
 )—the 
 proba-bility of a label 
 given
  the input value. Thus, conditional models can still be 
 used to answer questions 1 and 2. However, conditional models 
 cannot
  be used to 
 answer the remaining questions 3–6.
  
 In general, generative models are strictly more powerful than conditional models, 
 since we can calculate the conditional probability 
 P
 (
 label|input
 ) from the joint 
 probability 
 P
 (
 input, label
 ), but not vice versa. However, this additional power 
 comes at a price. Because the model is more powerful, it has more “free 
 parameters” that need to be learned. However, the size of the training set is fixed. 
 Thus, when using a more powerful model, we end up with less data that can be 
 used to train each parameter’s value, making it harder to find the best parameter 
 values. As a result, a generative model may not do as good a job at answering 
 questions 1 and 2 as a conditional model, since the condi-tional model can focus its 
 efforts on those two questions. However, if we do need answers to questions like 3–
 6, then we have no choice but to use a generative model.
  
 The difference between a generative model and a conditional model is analogous to 
 the difference between a topographical map and a picture of a skyline. Although the 
 topo-graphical map can be used to answer a wider variety of questions, it is 
 significantly more difficult to generate an accurate topographical map than it is to 
 generate an ac-curate skyline.",NA
6.7  Modeling Linguistic Patterns,"Classifiers can help us to understand the linguistic patterns that occur in natural 
 lan-guage, by allowing us to create explicit 
 models
  that capture those patterns. 
 Typically, these models are using supervised classification techniques, but it is also 
 possible to",NA
What Do Models Tell Us?,"It’s important to understand what we can learn about language from an 
 automatically constructed model. One important consideration when dealing with 
 models of lan-guage is the distinction between descriptive models and explanatory 
 models. Descrip-tive models capture patterns in the data, but they don’t provide 
 any information about 
 why
  the data contains those patterns. For example, as we 
 saw in 
 Table 3-1
 , the syno-nyms 
 absolutely
  and 
 definitely
  are not interchangeable: 
 we say 
 absolutely adore
  not 
 definitely adore
 , and 
 definitely prefer
 , not 
 absolutely 
 prefer
 . In contrast, explanatory models attempt to capture properties and 
 relationships that cause the linguistic pat-terns. For example, we might introduce 
 the abstract concept of “polar adjective” as an adjective that has an extreme 
 meaning, and categorize some adjectives, such as 
 adore 
 and 
 detest
  as polar. Our 
 explanatory model would contain the constraint that 
 abso-lutely
  can combine only 
 with polar adjectives, and 
 definitely
  can only combine with non-polar adjectives. In 
 summary, descriptive models provide information about cor-relations in the data, 
 while explanatory models go further to postulate causal relationships.
  
 Most models that are automatically constructed from a corpus are descriptive 
 models; in other words, they can tell us what features are relevant to a given 
 pattern or con-struction, but they can’t necessarily tell us how those features and 
 patterns relate to one another. If our goal is to understand the linguistic patterns, 
 then we can use this information about which features are related as a starting 
 point for further experiments designed to tease apart the relationships between 
 features and patterns. On the other hand, if we’re just interested in using the model 
 to make predictions (e.g., as part of a language processing system), then we can use 
 the model to make predictions about new data without worrying about the details 
 of underlying causal relationships.",NA
6.8  Summary,"• Modeling the linguistic data found in corpora can help us to understand 
 linguistic 
  
 patterns, and can be used to make predictions about new language 
 data.
  
 • Supervised classifiers use labeled training corpora to build models that predict 
 the 
  
 label of an input based on specific features of that input.
  
 • Supervised classifiers can perform a wide variety of NLP tasks, including 
 document classification, part-of-speech tagging, sentence segmentation, 
 dialogue act type identification, and determining entailment relations, and 
 many other tasks.
  
 • When training a supervised classifier, you should split your corpus into three da-
 tasets: a training set for building the classifier model, a dev-test set for helping 
 select and tune the model’s features, and a test set for evaluating the final 
 model’s performance.
  
 • When evaluating a supervised classifier, it is important that you use fresh data 
 that was not included in the training or dev-test set. Otherwise, your evaluation 
 results may be unrealistically optimistic.
  
 • Decision trees are automatically constructed tree-structured flowcharts that are 
 used to assign labels to input values based on their features. Although they’re 
 easy to interpret, they are not very good at handling cases where feature values 
 interact in determining the proper label.
  
 • In naive Bayes classifiers, each feature independently contributes to the decision 
 of which label should be used. This allows feature values to interact, but can be 
 problematic when two or more features are highly correlated with one 
 another.
  
 • Maximum Entropy classifiers use a basic model that is similar to the model used 
 by naive Bayes; however, they employ iterative optimization to find the set of 
 fea-ture weights that maximizes the probability of the training set.
  
 • Most of the models that are automatically constructed from a corpus are 
 descrip-tive, that is, they let us know which features are relevant to a given 
 pattern or construction, but they don’t give any information about causal 
 relationships be-tween those features and patterns.",NA
6.9  Further Reading,"Please consult 
 http://www.nltk.org/
  for further materials on this chapter and on 
 how to install external machine learning packages, such as Weka, Mallet, TADM, 
 and MegaM. For more examples of classification and machine learning with NLTK, 
 please see the classification HOWTOs at 
 http://www.nltk.org/howto
 .
  
 For a general introduction to machine learning, we recommend (Alpaydin, 2004). 
 For a more mathematically intense introduction to the theory of machine learning, 
 see (Hastie, Tibshirani & Friedman, 2009). Excellent books on using machine 
 learning",NA
6.10  Exercises,"1.
  ○ 
 Read up on one of the language technologies mentioned in this section, such 
 as word sense disambiguation, semantic role labeling, question answering, 
 machine translation, or named entity recognition. Find out what type and 
 quantity of an-notated data is required for developing such systems. Why do 
 you think a large amount of data is required?
  
 2.
  ○ 
 Using any of the three classifiers described in this chapter, and any features 
 you can think of, build the best name gender classifier you can. Begin by 
 splitting the Names Corpus into three subsets: 500 words for the test set, 500 
 words for the dev-test set, and the remaining 6,900 words for the training set. 
 Then, starting with the example name gender classifier, make incremental 
 improvements. Use the dev-test set to check your progress. Once you are 
 satisfied with your classifier, check its final performance on the test set. How 
 does the performance on the test set compare to the performance on the dev-
 test set? Is this what you’d expect?
  
 3.
  ○ 
 The Senseval 2 Corpus contains data intended to train word-sense 
 disambigua-tion classifiers. It contains data for four words: 
 hard
 , 
 interest
 , 
 line
 , 
 and 
 serve
 .
  
 Choose one of these four words, and load the corresponding data:",NA
CHAPTER 7,NA,NA
Extracting Information ,NA,NA
from Text,"For any given question, it’s likely that someone has written the answer down some-
 where. The amount of natural language text that is available in electronic form is 
 truly staggering, and is increasing every day. However, the complexity of natural 
 language can make it very difficult to access the information in that text. The state 
 of the art in NLP is still a long way from being able to build general-purpose 
 representations of meaning from unrestricted text. If we instead focus our efforts 
 on a limited set of ques-tions or “entity relations,” such as “where are different 
 facilities located” or “who is employed by what company,” we can make significant 
 progress. The goal of this chap-ter is to answer the following questions:
  
 1. How can we build a system that extracts structured data from unstructured text?
  
 2. What are some robust methods for identifying the entities and relationships de-
 scribed in a text?
  
 3. Which corpora are appropriate for this work, and how do we use them for 
 training and evaluating our models?
  
 Along the way, we’ll apply techniques from the last two chapters to the problems of 
 chunking and named entity recognition.",NA
7.1  Information Extraction,"Information comes in many shapes and sizes. One important form is 
 structured 
 data
 , where there is a regular and predictable organization of entities and 
 relationships. For example, we might be interested in the relation between 
 companies and locations. Given a particular company, we would like to be able to 
 identify the locations where it does business; conversely, given a location, we 
 would like to discover which com-panies do business in that location. If our data is 
 in tabular form, such as the example in 
 Table 7-1
 , then answering these queries is 
 straightforward.",NA
Information Extraction Architecture,"Figure 7-1
  shows the architecture for a simple information extraction system. It 
 begins by processing a document using several of the procedures discussed in 
 Chapters 
 3
  and 
 5
 : first, the raw text of the document is split into sentences using a 
 sentence segmenter, and each sentence is further subdivided into words using a 
 tokenizer. Next, each sen-tence is tagged with part-of-speech tags, which will prove 
 very helpful in the next step, 
 named entity recognition
 . In this step, we search for 
 mentions of potentially inter-esting entities in each sentence. Finally, we use 
 relation recognition
  to search for likely relations between different entities in the 
 text.
  
  
  
  
  
  
  
  
  
  
 Figure 7-1. Simple pipeline architecture for an information extraction system. This system takes 
 the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as 
 its output. For example, given a document that indicates that the company Georgia-Pacific is 
 located in Atlanta, it might generate the tuple 
 ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])
 .
  
 To perform the first three tasks, we can define a function that simply connects 
 together
  
 NLTK’s default sentence segmenter 
  
 , word tokenizer 
  
 , and part-of-speech
  
 tagger :
  
 >>> def ie_preprocess(document): 
  
 ...    sentences = nltk.sent_tokenize(document) 
  
 ...    sentences = [nltk.word_tokenize(sent) for sent in sentences] 
  
 ...    sentences = [nltk.pos_tag(sent) for sent in sentences]",NA
7.2  Chunking,"The basic technique we will use for entity recognition is 
 chunking
 , which segments 
 and labels multitoken sequences as illustrated in 
 Figure 7-2
 . The smaller boxes 
 show the word-level tokenization and part-of-speech tagging, while the large boxes 
 show higher-level chunking. Each of these larger boxes is called a 
 chunk
 . Like 
 tokenization, which omits whitespace, chunking usually selects a subset of the 
 tokens. Also like tokenization, the pieces produced by a chunker do not overlap in 
 the source text.
  
 In this section, we will explore chunking in some depth, beginning with the 
 definition and representation of chunks. We will see regular expression and n-gram 
 approaches to chunking, and will develop and evaluate chunkers using the CoNLL-
 2000 Chunking Corpus. We will then return in Sections 
 7.5
  and 
 7.6
  to the tasks of 
 named entity rec-ognition and relation extraction.",NA
Noun Phrase Chunking,"We will begin by considering the task of 
 noun phrase chunking
 , or 
 NP-chunking
 , 
 where we search for chunks corresponding to individual noun phrases. For 
 example, here is some 
 Wall Street Journal
  text with 
 NP
 -chunks marked using 
 brackets:",NA
Tag Patterns,"The rules that make up a chunk grammar use 
 tag patterns
  to describe sequences 
 of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using 
 angle brackets, e.g.,
 <DT>?<JJ>*<NN>
 . Tag patterns are similar to regular expression 
 patterns (
 Section 3.4
 ). Now, consider the following noun phrases from the 
 Wall 
 Street Journal
 :
  
 another/DT sharp/JJ dive/NN 
  
 trade/NN figures/NNS 
  
 any/DT new/JJ policy/NN measures/NNS 
  
 earlier/JJR stages/NNS 
  
 Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP
  
 We can match these noun phrases using a slight refinement of the first tag pattern 
 above, i.e., 
 <DT>?<JJ.*>*<NN.*>+
 . This will chunk any sequence of tokens beginning 
 with an optional determiner, followed by zero or more adjectives of any type 
 (including relative adjectives like 
 earlier/JJR
 ), followed by one or more nouns of any 
 type. How-ever, it is easy to find many more complicated examples which this rule 
 will not cover:
  
 his/PRP$ Mansion/NNP House/NNP speech/NN 
  
 the/DT price/NN cutting/VBG 
  
 3/CD %/NN to/TO 4/CD %/NN 
  
 more/JJR than/IN 10/CD %/NN 
  
 the/DT fastest/JJS developing/VBG trends/NNS 
  
 's/POS skill/NN
  
  
 Your Turn:
  Try to come up with tag patterns to cover these cases. 
 Test them using the graphical interface 
 nltk.app.chunkparser()
 . 
 Continue to refine your tag patterns with the help of the feedback 
 given by this tool.",NA
Chunking with Regular Expressions,"To find the chunk structure for a given sentence, the 
 RegexpParser
  chunker begins 
 with a flat structure in which no tokens are chunked. The chunking rules are 
 applied in turn, successively updating the chunk structure. Once all of the rules 
 have been invoked, the resulting chunk structure is returned.
  
 Example 7-2
  shows a simple chunk grammar consisting of two rules. The first rule 
 matches an optional determiner or possessive pronoun, zero or more adjectives, 
 then",NA
Exploring Text Corpora,"In 
 Section 5.2
 , we saw how we could interrogate a tagged corpus to extract phrases 
 matching a particular sequence of part-of-speech tags. We can do the same work 
 more easily with a chunker, as follows:",NA
Chinking,"Sometimes it is easier to define what we want to 
 exclude
  from a chunk. We can 
 define a 
 chink
  to be a sequence of tokens that is not included in a chunk. In the 
 following example, 
 barked/VBD at/IN
  is a chink:
  
 [ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]
  
 Chinking is the process of removing a sequence of tokens from a chunk. If the 
 matching sequence of tokens spans an entire chunk, then the whole chunk is 
 removed; if the sequence of tokens appears in the middle of the chunk, these tokens 
 are removed, leaving two chunks where there was only one before. If the sequence 
 is at the periphery of the chunk, these tokens are removed, and a smaller chunk 
 remains. These three possibilities are illustrated in 
 Table 7-2
 .
  
 Table 7-2. Three chinking rules applied to the same chunk
  
  
  
  
  
  
 Entire chunk
  
 Middle of a 
 chunk
  
  
 End of a chunk
  
  
 Input
  
 [a/DT little/JJ 
 dog/NN]
  
 [a/DT little/JJ 
 dog/NN]
  
 [a/DT little/JJ 
 dog/NN]
  
 Operation
  
 Chink “DT JJ 
 NN”
  
 Chink “JJ”
  
 Chink “NN”
  
 Pattern
  
 }DT JJ NN{
  
 }JJ{
  
 }NN{
  
 Output
  
 a/DT little/JJ 
 dog/NN
  
 [a/DT] little/JJ 
 [dog/NN]
  
 [a/DT little/JJ] 
 dog/NN
  
 268 | Chapter 7:Extracting Information from Text",NA
Representing Chunks: Tags Versus Trees,"As befits their intermediate status between tagging and parsing (
 Chapter 8
 ), chunk 
 structures can be represented using either tags or trees. The most widespread file 
 rep-resentation uses 
 IOB tags
 . In this scheme, each token is tagged with one of 
 three special chunk tags, 
 I
  (inside), 
 O
  (outside), or 
 B
  (begin). A token is tagged as 
 B
  if 
 it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged 
 I
 . 
 All other tokens are tagged 
 O
 . The 
 B
  and 
 I
  tags are suffixed with the chunk type, e.g., 
 B-NP
 , 
 I-NP
 . Of course, it is not necessary to specify a chunk type for tokens that 
 appear outside a chunk, so these are just labeled 
 O
 . An example of this scheme is 
 shown in 
 Figure 7-3
 .
  
  
 Figure 7-3. Tag representation of chunk structures.
  
 IOB tags have become the standard way to represent chunk structures in files, and 
 we will also be using this format. Here is how the information in 
 Figure 7-3
  would 
 appear in a file:
  
 We PRP B-NP 
  
 saw VBD O 
  
 the DT B-NP 
  
 little JJ I-NP 
  
 yellow JJ I-NP 
  
 dog NN I-NP",NA
7.3  Developing and Evaluating Chunkers,"Now you have a taste of what chunking does, but we haven’t explained how to 
 evaluate chunkers. As usual, this requires a suitably annotated corpus. We begin by 
 looking at the mechanics of converting IOB format into an NLTK tree, then at how 
 this is done on a larger scale using a chunked corpus. We will see how to score the 
 accuracy of a chunker relative to a corpus, then look at some more data-driven 
 ways to search for NP chunks. Our focus throughout will be on expanding the 
 coverage of a chunker.",NA
Reading IOB Format and the CoNLL-2000 Chunking ,NA,NA
Corpus,"Using the 
 corpora
  module we can load 
 Wall Street Journal
  text that has been tagged 
 then chunked using the IOB notation. The chunk categories provided in this corpus 
 are 
 NP
 , 
 VP
 , and 
 PP
 . As we have seen, each sentence is represented using multiple 
 lines, as shown here:
  
 he PRP B-NP 
  
 accepted VBD B-VP 
  
 the DT B-NP 
  
 position NN I-NP 
  
 ...",NA
Simple Evaluation and Baselines,"Now that we can access a chunked corpus, we can evaluate chunkers. We start off 
 by establishing a baseline for the trivial chunk parser 
 cp
  that creates no chunks:
  
 >>> from nltk.corpus import conll2000 
  
 >>> cp = nltk.RegexpParser("""") 
  
 >>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP']) >>> print 
 cp.evaluate(test_sents) 
  
 ChunkParse score:
  
  
  IOB Accuracy:  43.4%
  
  
  Precision:      0.0%
  
  
  Recall:         0.0%
  
  
  F-Measure:      0.0%
  
 The IOB tag accuracy indicates that more than a third of the words are tagged with 
 O
 , i.e., not in an 
 NP
  chunk. However, since our tagger did not find 
 any
  chunks, its 
 precision, recall, and F-measure are all zero. Now let’s try a naive regular 
 expression chunker that looks for tags beginning with letters that are characteristic 
 of noun phrase tags (e.g., 
 CD
 , 
 DT
 , and 
 JJ
 ).
  
 >>> grammar = r""NP: {<[CDJNP].*>+}"" 
  
 >>> cp = nltk.RegexpParser(grammar) 
  
 >>> print cp.evaluate(test_sents) 
  
 ChunkParse score:
  
  
  IOB Accuracy:  87.7%
  
  
  Precision:     70.6%
  
  
  Recall:        67.8%
  
  
  F-Measure:     69.2%
  
 As you can see, this approach achieves decent results. However, we can improve on 
 it by adopting a more data-driven approach, where we use the training corpus to 
 find the chunk tag (
 I
 , 
 O
 , or 
 B
 ) that is most likely for each part-of-speech tag. In other 
 words, we can build a chunker using a 
 unigram tagger
  (
 Section 5.4
 ). But rather 
 than trying to determine the correct part-of-speech tag for each word, we are trying 
 to determine the correct chunk tag, given each word’s part-of-speech tag.
  
 272 | Chapter 7:Extracting Information from Text",NA
Training Classifier-Based Chunkers,"Both the regular expression–based chunkers and the n-gram chunkers decide what 
 chunks to create entirely based on part-of-speech tags. However, sometimes part-
 of-speech tags are insufficient to determine how a sentence should be chunked. For 
 ex-ample, consider the following two statements:
  
 (3) a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.
  
 b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.
  
 These two sentences have the same part-of-speech tags, yet they are chunked 
 differ-ently. In the first sentence, 
 the farmer
  and 
 rice
  are separate chunks, while the 
 corre-sponding material in the second sentence, 
 the computer monitor
 , is a single 
 chunk. Clearly, we need to make use of information about the content of the words, 
 in addition to just their part-of-speech tags, if we wish to maximize chunking 
 performance.
  
 One way that we can incorporate information about the content of words is to use a 
 classifier-based tagger to chunk the sentence. Like the n-gram chunker considered 
 in the previous section, this classifier-based chunker will work by assigning IOB 
 tags to
  
 274 | Chapter 7:Extracting Information from Text",NA
7.4  Recursion in Linguistic Structure,NA,NA
Building Nested Structure with Cascaded Chunkers,"So far, our chunk structures have been relatively flat. Trees consist of tagged 
 tokens, optionally grouped under a chunk node such as 
 NP
 . However, it is possible 
 to build chunk structures of arbitrary depth, simply by creating a multistage chunk 
 grammar",NA
Trees,"A 
 tree
  is a set of connected labeled nodes, each reachable by a unique path from a 
 distinguished root node. Here’s an example of a tree (note that they are standardly 
 drawn upside-down):
  
 (4)
  
  
 We use a ‘family’ metaphor to talk about the relationships of nodes in a tree: for ex-
 ample, 
 S
  is the 
 parent
  of 
 VP
 ; conversely 
 VP
  is a 
 child
  of 
 S
 . Also, since 
 NP
  and 
 VP
  are 
 both children of 
 S
 , they are also 
 siblings
 . For convenience, there is also a text 
 format for specifying trees:
  
 (S
  
  
  (NP Alice)
  
  
  (VP
  
   
  (V chased)
  
   
  (NP
  
   
  
  (Det the)
  
   
  
  (N rabbit))))
  
 Although we will focus on syntactic trees, trees can be used to encode 
 any
  
 homogeneous hierarchical structure that spans a sequence of linguistic forms (e.g., 
 morphological structure, discourse structure). In the general case, leaves and node 
 values do not have to be strings.
  
 In NLTK, we create a tree by giving a node label and a list of children:
  
 7.4  Recursion in Linguistic Structure | 279",NA
Tree Traversal,"It is standard to use a recursive function to traverse a tree. The listing in 
 Example 7-
 7 
 demonstrates this.
  
 Example 7-7. A recursive function to traverse a tree.
  
 def traverse(t):
  
  
  try:
  
  
  
  t.node
  
  
  except AttributeError:
  
  
  
  print t,
  
  else:
  
 280 | Chapter 7:Extracting Information from Text",NA
7.5  Named Entity Recognition,"At the start of this chapter, we briefly introduced named entities (NEs). Named 
 entities are definite noun phrases that refer to specific types of individuals, such as 
 organiza-tions, persons, dates, and so on. 
 Table 7-3
  lists some of the more 
 commonly used types of NEs. These should be self-explanatory, except for 
 “FACILITY”: human-made arti-facts in the domains of architecture and civil 
 engineering; and “GPE”: geo-political entities such as city, state/province, and 
 country.
  
 Table 7-3. Commonly used types of named entity
  
  
  
 NE type
  
 Examples
  
  
 ORGANIZ
 ATION 
 PERSON 
  
 LOCATION 
  
 DATE 
  
 TIME 
  
 MONEY 
  
 PERCENT 
  
 FACILITY 
  
 GPE
  
 Georgia-Pacific 
 Corp.
 , 
 WHO Eddy 
 Bonte
 , 
 President 
 Obama Murray 
 River
 , 
 Mount Everest 
 June
 , 
 2008-06-29 
  
 two fifty a m
 , 
 1:30 
 p.m.
  
 175 million Canadian 
 Dollars
 , 
 GBP 10.40 twenty 
 pct
 , 
 18.75 % 
  
 Washington Monument
 , 
 Stonehenge South East Asia
 , 
 Midlothian
  
 The goal of a 
 named entity recognition
  (NER) system is to identify all textual 
 men-tions of the named entities. This can be broken down into two subtasks: 
 identifying the boundaries of the NE, and identifying its type. While named entity 
 recognition is frequently a prelude to identifying relations in Information 
 Extraction, it can also con-tribute to other tasks. For example, in Question 
 Answering (QA), we try to improve the precision of Information Retrieval by 
 recovering not whole pages, but just those parts which contain an answer to the 
 user’s question. Most QA systems take the",NA
7.6  Relation Extraction,"Once named entities have been identified in a text, we then want to extract the 
 relations that exist between them. As indicated earlier, we will typically be looking 
 for relations between specified types of named entity. One way of approaching this 
 task is to initially look for all triples of the form (
 X
 , 
 α
 , 
 Y
 ), where 
 X
  and 
 Y
  are named 
 entities of the required types, and 
 α
  is the string of words that intervenes between 
 X
  and 
 Y
 . We can then use regular expressions to pull out just those instances of 
 α
  
 that express the relation that we are looking for. The following example searches 
 for strings that contain the word 
 in
 . The special regular expression 
 (?!\b.+ing\b)
  is a 
 negative lookahead assertion that allows us to disregard strings such as 
 success in 
 supervising the transition of
 , where 
 in 
 is followed by a gerund.
  
 >>> IN = re.compile(r'.*\bin\b(?!\b.+ing)') 
  
 >>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'): 
  
 ...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, 
  
 ...                                      corpus='ieer', pattern = IN): 
  
 ...         print nltk.sem.show_raw_rtuple(rel) 
  
 [ORG: 'WHYY'] 'in' [LOC: 'Philadelphia'] 
  
 [ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo'] 
  
 [ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington'] 
  
 [ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington'] 
  
 [ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles'] [ORG: 'Open 
 Text'] ', based in' [LOC: 'Waterloo'] 
  
 [ORG: 'WGBH'] 'in' [LOC: 'Boston'] 
  
 [ORG: 'Bastille Opera'] 'in' [LOC: 'Paris'] 
  
 [ORG: 'Omnicom'] 'in' [LOC: 'New York'] 
  
 [ORG: 'DDB Needham'] 'in' [LOC: 'New York'] 
  
 [ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York'] 
  
 [ORG: 'BBDO South'] 'in' [LOC: 'Atlanta'] 
  
 [ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']
  
 Searching for the keyword 
 in
  works reasonably well, though it will also retrieve 
 false positives such as 
 [ORG: House Transportation Committee] , secured the most money 
 in the [LOC: New York]
 ; there is unlikely to be a simple string-based method of ex-
 cluding filler strings such as this.
  
 284 | Chapter 7:Extracting Information from Text",NA
7.7  Summary,"• Information extraction systems search large bodies of unrestricted text for 
 specific types of entities and relations, and use them to populate well-
 organized databases. These databases can then be used to find answers for 
 specific questions.
  
 • The typical architecture for an information extraction system begins by 
 segment-ing, tokenizing, and part-of-speech tagging the text. The resulting data 
 is then searched for specific types of entity. Finally, the information extraction 
 system looks at entities that are mentioned near one another in the text, and 
 tries to de-termine whether specific relationships hold between those entities.
  
 • Entity recognition is often performed using chunkers, which segment multitoken 
 sequences, and label them with the appropriate entity type. Common entity 
 types include ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and 
 GPE (geo-political entity).
  
 7.7  Summary | 285",NA
7.8  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web. For more examples of chunking with 
 NLTK, please see the Chunking HOWTO at 
 http://www.nltk.org/howto
 .
  
 The popularity of chunking is due in great part to pioneering work by Abney, e.g., 
 (Abney, 
 1996a). 
 Abney’s 
 Cass 
 chunker 
 is 
 described 
 in 
 http://www.vinartus.net/spa/97a .pdf
 .
  
 The word 
 chink
  initially meant a sequence of stopwords, according to a 1975 paper 
 by Ross and Tukey (Abney, 1996a).
  
 The IOB format (or sometimes 
 BIO Format
 ) was developed for 
 NP
  chunking by 
 (Ram-shaw & Marcus, 1995), and was used for the shared 
 NP
  bracketing task run 
 by the 
 Conference on Natural Language Learning
  (CoNLL) in 1999. The same format 
 was adopted by CoNLL 2000 for annotating a section of 
 Wall Street Journal
  text as 
 part of a shared task on 
 NP
  chunking.
  
 Section 13.5 of (Jurafsky & Martin, 2008) contains a discussion of chunking. 
 Chapter 22 covers information extraction, including named entity recognition. For 
 information about text mining in biology and medicine, see (Ananiadou & 
 McNaught, 2006).
  
 For 
 more 
 information 
 on 
 the 
 Getty 
 and 
 Alexandria 
 gazetteers, 
 see 
 http://en.wikipedia 
 .org/wiki/Getty_Thesaurus_of_Geographic_Names
  
 and 
 http://www.alexandria.ucsb .edu/gazetteer/
 .",NA
7.9  Exercises,"1.
  ○ 
 The IOB format categorizes tagged tokens as 
 I
 , 
 O
 , and 
 B
 . Why are three tags 
 necessary? What problem would be caused if we used 
 I
  and 
 O
  tags exclusively?
  
 286 | Chapter 7:Extracting Information from Text",NA
CHAPTER 8,NA,NA
Analyzing Sentence ,NA,NA
Structure,"Earlier chapters focused on words: how to identify them, analyze their structure, 
 assign them to lexical categories, and access their meanings. We have also seen how 
 to identify patterns in word sequences or n-grams. However, these methods only 
 scratch the sur-face of the complex constraints that govern sentences. We need a 
 way to deal with the ambiguity that natural language is famous for. We also need to 
 be able to cope with the fact that there are an unlimited number of possible 
 sentences, and we can only write finite programs to analyze their structures and 
 discover their meanings.
  
 The goal of this chapter is to answer the following questions:
  
 1. How can we use a formal grammar to describe the structure of an unlimited set 
 of sentences?
  
 2. How do we represent the structure of sentences using syntax trees?
  
 3. How do parsers analyze a sentence and automatically build a syntax tree?
  
 Along the way, we will cover the fundamentals of English syntax, and see that there 
 are systematic aspects of meaning that are much easier to capture once we have 
 iden-tified the structure of sentences.",NA
8.1  Some Grammatical Dilemmas,NA,NA
Linguistic Data and Unlimited Possibilities,"Previous chapters have shown you how to process and analyze text corpora, and 
 we have stressed the challenges for NLP in dealing with the vast amount of 
 electronic language data that is growing daily. Let’s consider this data more closely, 
 and make the thought experiment that we have a gigantic corpus consisting of 
 everything that has been either uttered or written in English over, say, the last 50 
 years. Would we be justified in calling this corpus “the language of modern 
 English”? There are a number of reasons why we might answer no. Recall that in 
 Chapter 3
 , we asked you to search the Web for instances of the pattern 
 the of
 . 
 Although it is easy to find examples on the Web containing this word sequence, 
 such 
 as 
 New 
 man 
 at 
 the 
 of 
 IMG 
  
 (see 
 http://www 
 .telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html
 ), speakers of English 
 will say that most such examples are errors, and therefore not part of English after 
 all.
  
 Accordingly, we can argue that “modern English” is not equivalent to the very big 
 set of word sequences in our imaginary corpus. Speakers of English can make 
 judgments about these sequences, and will reject some of them as being 
 ungrammatical.
  
 Equally, it is easy to compose a new sentence and have speakers agree that it is 
 perfectly good English. For example, sentences have an interesting property that 
 they can be embedded inside larger sentences. Consider the following sentences:
  
 (1) a. Usain Bolt broke the 100m record.
  
 b. The Jamaica Observer reported that Usain Bolt broke the 100m record.
  
 c. Andre said The Jamaica Observer reported that Usain Bolt broke the 
 100m record.
  
 d. I think Andre said the Jamaica Observer reported that Usain Bolt broke 
  
 the 100m record.
  
 If we replaced whole sentences with the symbol 
 S
 , we would see patterns like 
 Andre 
 said
 S
  and 
 I think
 S
 . These are templates for taking a sentence and constructing a 
 bigger sentence. There are other templates we can use, such as 
 S
 but
 S
  and 
 S
 when
 S
 . 
 With a bit of ingenuity we can construct some really long sentences using these 
 templates. Here’s an impressive example from a Winnie the Pooh story by A.A. 
 Milne, 
 In Which Piglet Is Entirely Surrounded by Water
 :
  
 [You can imagine Piglet’s joy when at last the ship came in sight of him.] In after-
 years he liked to think that he had been in Very Great Danger during the Terrible 
 Flood, but the only danger he had really been in was the last half-hour of his 
 imprisonment, when Owl, who had just flown up, sat on a branch of his tree to 
 comfort him, and told him a very long story about an aunt who had once laid a 
 seagull’s egg by mistake, and the story went on and on, rather like this sentence, 
 until Piglet who was listening out of his window without much hope, went to sleep 
 quietly and naturally, slipping slowly out of the win-dow towards the water until he 
 was only hanging on by his toes, at which moment,",NA
Ubiquitous Ambiguity,"A well-known example of ambiguity is shown in 
 (2)
 , from the Groucho Marx movie, 
 Animal Crackers
  (1930):
  
 (2) While hunting in Africa, I shot an elephant in my pajamas. How an elephant 
 got into my pajamas I’ll never know.
  
 Let’s take a closer look at the ambiguity in the phrase: 
 I shot an elephant in my paja-
 mas
 . First we need to define a simple grammar:
  
 >>> groucho_grammar = nltk.parse_cfg("""""" 
  
 ... S -> NP VP 
  
 ... PP -> P NP 
  
 ... NP -> Det N | Det N PP | 'I' 
  
 ... VP -> V NP | VP PP 
  
 ... Det -> 'an' | 'my' 
  
 ... N -> 'elephant' | 'pajamas' 
  
 ... V -> 'shot' 
  
 ... P -> 'in' 
  
 ... """""")",NA
8.2  What’s the Use of Syntax?,NA,NA
Beyond n-grams,"We gave an example in 
 Chapter 2
  of how to use the frequency information in 
 bigrams to generate text that seems perfectly acceptable for small sequences of 
 words but rapidly degenerates into nonsense. Here’s another pair of examples that 
 we created by com-puting the bigrams over the text of a children’s story, 
 The 
 Adventures of Buster Brown
  (included in the Project Gutenberg Selection Corpus):
  
 (4) a. He roared with me the pail slip down his back
  
 b. The worst part and clumsy looking for whoever heard light
  
 You intuitively know that these sequences are “word-salad,” but you probably find 
 it hard to pin down what’s wrong with them. One benefit of studying grammar is 
 that it provides a conceptual framework and vocabulary for spelling out these 
 intuitions. Let’s take a closer look at the sequence 
 the worst part and clumsy 
 looking
 . This looks like a 
 coordinate structure
 , where two phrases are joined by a 
 coordinating conjunction such as 
 and
 , 
 but
 , or 
 or
 . Here’s an informal (and simplified) 
 statement of how coordi-nation works syntactically:
  
 Coordinate Structure: if 
 v
 1
  and 
 v
 2
  are both phrases of grammatical category 
 X
 , then 
 v
 1 
 andv
 2
  is also a phrase of category 
 X
 .
  
 8.2  What’s the Use of Syntax? | 295",NA
8.3  Context-Free Grammar,NA,NA
A Simple Grammar,"Let’s start off by looking at a simple 
 context-freegrammar
  (CFG). By convention, 
 the lefthand side of the first production is the 
 start-symbol
  of the grammar, 
 typically 
 S
 , and all well-formed trees must have this symbol as their root label. In 
 NLTK, context-free grammars are defined in the 
 nltk.grammar
  module. In 
 Example 
 8-1
  we define a grammar and show how to parse a simple sentence admitted by the 
 grammar.
  
 Example 8-1. A simple context-free grammar.
  
 grammar1 = nltk.parse_cfg(""""""
  
  S -> NP VP
  
  VP -> V NP | V NP PP
  
  PP -> P NP
  
  V -> ""saw"" | ""ate"" | ""walked""
  
  NP -> ""John"" | ""Mary"" | ""Bob"" | Det N | Det N PP Det -> ""a"" | 
 ""an"" | ""the"" | ""my""
  
  N -> ""man"" | ""dog"" | ""cat"" | ""telescope"" | ""park"" P -> ""in"" | 
 ""on"" | ""by"" | ""with""
  
  """""")
  
 >>> sent = ""Mary saw Bob"".split() 
  
 >>> rd_parser = nltk.RecursiveDescentParser(grammar1) >>> 
 for tree in rd_parser.nbest_parse(sent): 
  
 ...      print tree 
  
 (S (NP Mary) (VP (V saw) (NP Bob)))
  
 The grammar in 
 Example 8-1
  contains productions involving various syntactic cate-
 gories, as laid out in 
 Table 8-1
 . The recursive descent parser used here can also be 
 inspected via a graphical interface, as illustrated in 
 Figure 8-3
 ; we discuss this 
 parser in more detail in 
 Section 8.4
 .
  
 Table 8-1. Syntactic categories
  
  
  
  
 Sym
 bol
  
 Meaning
  
  
 Example
  
  
 S
  
 sentence
  
 the man 
 walked
  
 NP
  
 noun phrase
  
 a dog
  
 VP
  
 verb phrase
  
 saw a park
  
 PP
  
 prepositional 
 phrase
  
 with a 
 telescope
  
 Det
  
 determiner
  
 the
  
 N
  
 noun
  
 dog
  
 298 | Chapter 8:Analyzing Sentence Structure",NA
Writing Your Own Grammars,"If you are interested in experimenting with writing CFGs, you will find it helpful to 
 create and edit your grammar in a text file, say, 
 mygrammar.cfg
 . You can then load 
 it into NLTK and parse with it as follows:
  
 >>> grammar1 = nltk.data.load('file:mygrammar.cfg') >>> sent 
 = ""Mary saw Bob"".split() 
  
 >>> rd_parser = nltk.RecursiveDescentParser(grammar1) >>> 
 for tree in rd_parser.nbest_parse(sent): 
  
 ...      print tree
  
 Make sure that you put a 
 .cfg
  suffix on the filename, and that there are no spaces in 
 the string 
 'file:mygrammar.cfg'
 . If the command 
 print tree
  produces no output, this is 
 probably because your sentence 
 sent
  is not admitted by your grammar. In this case, 
 call the parser with tracing set to be on: 
 rd_parser = nltk.RecursiveDescent
  
 300 | Chapter 8:Analyzing Sentence Structure",NA
Recursion in Syntactic Structure,"A grammar is said to be 
 recursive
  if a category occurring on the lefthand side of a 
 production also appears on the righthand side of a production, as illustrated in 
 Exam-ple 8-2
 . The production 
 Nom -> Adj Nom
  (where 
 Nom
  is the category of 
 nominals) involves direct recursion on the category 
 Nom
 , whereas indirect 
 recursion on 
 S
  arises from the combination of two productions, namely 
 S -> NP VP
  
 and 
 VP -> V S
 .
  
 Example 8-2. A recursive context-free grammar.
  
 grammar2 = nltk.parse_cfg(""""""
  
  S  -> NP VP
  
  NP -> Det Nom | PropN
  
  Nom -> Adj Nom | N
  
  VP -> V Adj | V NP | V S | V NP PP
  
  PP -> P NP
  
  PropN -> 'Buster' | 'Chatterer' | 'Joe'
  
  Det -> 'the' | 'a'
  
  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'
  
  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'
  
  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put' P -> 'on'
  
  """""")
  
 To see how recursion arises from this grammar, consider the following trees. 
 (10a) 
 involves nested nominal phrases, while 
 (10b)
  contains nested sentences.
  
 8.3  Context-Free Grammar | 301",NA
8.4  Parsing with Context-Free Grammar,"A 
 parser
  processes input sentences according to the productions of a grammar, 
 and builds one or more constituent structures that conform to the grammar. A 
 grammar is a declarative specification of well-formedness—it is actually just a 
 string, not a pro-gram. A parser is a procedural interpretation of the grammar. It 
 searches through the space of trees licensed by a grammar to find one that has the 
 required sentence along its fringe.
  
 302 | Chapter 8:Analyzing Sentence Structure",NA
Recursive Descent Parsing,"The simplest kind of parser interprets a grammar as a specification of how to break 
 a high-level goal into several lower-level subgoals. The top-level goal is to find an 
 S
 . 
 The 
 S
 →
 NP VP
  production permits the parser to replace this goal with two subgoals: 
 find an 
 NP
 , then find a 
 VP
 . Each of these subgoals can be replaced in turn by sub-
 subgoals, using productions that have 
 NP
  and 
 VP
  on their lefthand side. Eventually, 
 this expansion process leads to subgoals such as: find the word 
 telescope
 . Such 
 subgoals can be directly compared against the input sequence, and succeed if the 
 next word is matched. If there is no match, the parser must back up and try a 
 different alternative.
  
 The recursive descent parser builds a parse tree during this process. With the 
 initial goal (find an 
 S
 ), the 
 S
  root node is created. As the process recursively 
 expands its goals using the productions of the grammar, the parse tree is extended 
 downwards (hence the name 
 recursive descent
 ). We can see this in action using the 
 graphical demonstration 
 nltk.app.rdparser()
 . Six stages of the execution of this 
 parser are shown in 
 Figure 8-4
 .
  
 During this process, the parser is often forced to choose between several possible 
 pro-ductions. For example, in going from step 3 to step 4, it tries to find 
 productions with 
 N
  on the lefthand side. The first of these is 
 N
 → 
 man
 . When this 
 does not work it 
 backtracks
 , and tries other 
 N
  productions in order, until it gets to 
 N
 → 
 dog
 , which matches the next word in the input sentence. Much later, as shown 
 in step 5, it finds a complete parse. This is a tree that covers the entire sentence, 
 without any dangling edges. Once a parse has been found, we can get the parser to 
 look for additional parses. Again it will backtrack and explore other choices of 
 production in case any of them result in a parse.
  
 NLTK provides a recursive descent parser:
  
 >>> rd_parser = nltk.RecursiveDescentParser(grammar1) >>> 
 sent = 'Mary saw a dog'.split() 
  
 >>> for t in rd_parser.nbest_parse(sent): 
  
 ...     print t 
  
 (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))",NA
Shift-Reduce Parsing,"A simple kind of bottom-up parser is the 
 shift-reduce parser
 . In common with all 
 bottom-up parsers, a shift-reduce parser tries to find sequences of words and 
 phrases that correspond to the 
 righthand
  side of a grammar production, and 
 replace them with the lefthand side, until the whole sentence is reduced to an 
 S
 .
  
 304 | Chapter 8:Analyzing Sentence Structure",NA
The Left-Corner Parser,"One of the problems with the recursive descent parser is that it goes into an infinite 
 loop when it encounters a left-recursive production. This is because it applies the 
 grammar productions blindly, without considering the actual input sentence. A left-
 corner parser is a hybrid between the bottom-up and top-down approaches we 
 have seen.
  
 A 
 left-corner parser
  is a top-down parser with bottom-up filtering. Unlike an 
 ordinary recursive descent parser, it does not get trapped in left-recursive 
 productions. Before starting its work, a left-corner parser preprocesses the context-
 free grammar to build a table where each row contains two cells, the first holding a 
 non-terminal, and the sec-ond holding the collection of possible left corners of that 
 non-terminal. 
 Table 8-2
  il-lustrates this for the grammar from 
 grammar2
 .
  
 306 | Chapter 8:Analyzing Sentence Structure",NA
Well-Formed Substring Tables,"The simple parsers discussed in the previous sections suffer from limitations in 
 both completeness and efficiency. In order to remedy these, we will apply the 
 algorithm design technique of 
 dynamic programming
  to the parsing problem. As 
 we saw in 
 Section 4.7
 , dynamic programming stores intermediate results and 
 reuses them when appropriate, achieving significant efficiency gains. This 
 technique can be applied to syntactic parsing, allowing us to store partial solutions 
 to the parsing task and then look them up as necessary in order to efficiently arrive 
 at a complete solution. This approach to parsing is known as 
 chart parsing
 . We 
 introduce the main idea in this section; see the online materials available for this 
 chapter for more implementation details.
  
 Dynamic programming allows us to build the 
 PP
 in my pajamas
  just once. The first 
 time we build it we save it in a table, then we look it up when we need to use it as a 
 sub-constituent of either the object 
 NP
  or the higher 
 VP
 . This table is known as a 
 well-formed substring table
 , or WFST for short. (The term “substring” refers to a 
 contiguous se-quence of words within a sentence.) We will show how to construct 
 the WFST bottom-up so as to systematically record what syntactic constituents 
 have been found.
  
 Let’s set our input to be the sentence in 
 (2)
 . The numerically specified spans of the 
 WFST are reminiscent of Python’s slice notation (
 Section 3.2
 ). Another way to think 
 about the data structure is shown in 
 Figure 8-6
 , a data structure known as a 
 chart
 .
  
  
 Figure 8-6. The chart data structure: Words are the edge labels of a linear graph structure.
  
 In a WFST, we record the position of the words by filling in cells in a triangular 
 matrix: the vertical axis will denote the start position of a substring, while the 
 horizontal axis will denote the end position (thus 
 shot
  will appear in the cell with 
 coordinates (1, 2)). To simplify this presentation, we will assume each word has a 
 unique lexical category,",NA
8.5  Dependencies and Dependency ,NA,NA
Grammar,"Phrase structure grammar is concerned with how words and sequences of words 
 com-bine
  to form constituents. A distinct and complementary approach, 
 dependency grammar
 , focuses instead on how words 
 relate
  to other words. 
 Dependency is a binary asymmetric relation that holds between a 
 head
  and its 
 dependents
 . The head of a sentence is usually taken to be the tensed verb, and 
 every other word is either dependent on the sentence head or connects to it 
 through a path of dependencies.
  
 A dependency representation is a labeled directed graph, where the nodes are the 
 lexical items and the labeled arcs represent dependency relations from heads to 
 dependents. 
 Figure 8-8
  illustrates a dependency graph, where arrows point from 
 heads to their dependents.",NA
Valency and the Lexicon,"Let us take a closer look at verbs and their dependents. The grammar in 
 Example 8-
 2 
 correctly generates examples like 
 (12)
 .
  
 312 | Chapter 8:Analyzing Sentence Structure",NA
Scaling Up,"So far, we have only considered “toy grammars,” small grammars that illustrate the 
 key aspects of parsing. But there is an obvious question as to whether the approach 
 can be scaled up to cover large corpora of natural languages. How hard would it be 
 to construct such a set of productions by hand? In general, the answer is: 
 very hard
 . 
 Even if we allow ourselves to use various formal devices that give much more 
 succinct representations of grammar productions, it is still extremely difficult to 
 keep control of the complex interactions between the many productions required 
 to cover the major constructions of a language. In other words, it is hard to 
 modularize grammars so that one portion can be developed independently of the 
 other parts. This in turn means that it is difficult
  
 314 | Chapter 8:Analyzing Sentence Structure",NA
8.6  Grammar Development,"Parsing builds trees over sentences, according to a phrase structure grammar. Now, 
 all the examples we gave earlier only involved toy grammars containing a handful 
 of pro-ductions. What happens if we try to scale up this approach to deal with 
 realistic corpora of language? In this section, we will see how to access treebanks, 
 and look at the chal-lenge of developing broad-coverage grammars.",NA
Treebanks and Grammars,"The 
 corpus
  module defines the 
 treebank
  corpus reader, which contains a 10% 
 sample of the Penn Treebank Corpus.
  
 >>> from nltk.corpus import treebank 
  
 >>> t = treebank.parsed_sents('wsj_0001.mrg')[0] >>> print t 
  
 (S
  
  (NP-SBJ
  
   
  (NP (NNP Pierre) (NNP Vinken))
  
   
  (, ,)
  
   
  (ADJP (NP (CD 61) (NNS years)) (JJ old))
  
   
  (, ,))
  
  (VP
  
   
  (MD will)
  
   
  (VP
  
    
  (VB join)
  
    
  (NP (DT the) (NN board))
  
    
  (PP-CLR
  
     
  (IN as)
  
     
  (NP (DT a) (JJ nonexecutive) (NN director)))
  
   
  (NP-TMP (NNP Nov.) (CD 29))))
  
  (. .))
  
 We can use this data to help develop a grammar. For example, the program in 
 Exam-ple 8-4
  uses a simple filter to find verbs that take sentential complements. 
 Assuming we already have a production of the form 
 VP -> SV S
 , this information 
 enables us to identify particular verbs that would be included in the expansion of 
 SV
 .
  
 8.6  Grammar Development | 315",NA
Pernicious Ambiguity,"Unfortunately, as the coverage of the grammar increases and the length of the input 
 sentences grows, the number of parse trees grows rapidly. In fact, it grows at an 
 astro-nomical rate.
  
 Let’s explore this issue with the help of a simple example. The word 
 fish
  is both a 
 noun and a verb. We can make up the sentence 
 fish fish fish
 , meaning 
 fish like to fish 
 for other fish
 . (Try this with 
 police
  if you prefer something more sensible.) Here is a 
 toy grammar for the “fish” sentences.
  
 >>> grammar = nltk.parse_cfg("""""" 
  
 ... S -> NP V NP 
  
 ... NP -> NP Sbar 
  
 ... Sbar -> NP V 
  
 ... NP -> 'fish' 
  
 ... V -> 'fish' 
  
 ... """""")
  
 Now we can try parsing a longer sentence, 
 fish fish fish fish fish
 , which among other 
 things, means “fish that other fish fish are in the habit of fishing fish themselves.” 
 We use the NLTK chart parser, which is presented earlier in this chapter. This 
 sentence has two readings.
  
 >>> tokens = [""fish""] * 5 
  
 >>> cp = nltk.ChartParser(grammar) 
  
 >>> for tree in cp.nbest_parse(tokens): 
  
 ...     print tree 
  
 (S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish)) (S (NP fish) (V 
 fish) (NP (NP fish) (Sbar (NP fish) (V fish))))
  
 As the length of this sentence goes up (3, 5, 7, ...) we get the following numbers of 
 parse trees: 1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; …. 
 (These are the 
 Catalan numbers
 , which we saw in an exercise in 
 Chapter 4
 .) The 
 last of these is for a sentence of length 23, the average length of sentences in the 
 WSJ section of Penn Treebank. For a sentence of length 50 there would be over 10
 12
  
 parses, and this is only half the length of the Piglet sentence (
 Section 8.1
 ), which 
 young children process ef-fortlessly. No practical NLP system could construct 
 millions of trees for a sentence and choose the appropriate one in the context. It’s 
 clear that humans don’t do this either!
  
 Note that the problem is not with our choice of example. (Church & Patil, 1982) 
 point out that the syntactic ambiguity of 
 PP
  attachment in sentences like 
 (15)
  also 
 grows in proportion to the Catalan numbers.
  
 (15) Put the block in the box on the table.
  
 So much for structural ambiguity; what about lexical ambiguity? As soon as we try 
 to construct a broad-coverage grammar, we are forced to make lexical entries 
 highly am-biguous for their part-of-speech. In a toy grammar, 
 a
  is only a 
 determiner, 
 dog
  is only a noun, and 
 runs
  is only a verb. However, in a broad-
 coverage grammar, 
 a
  is also a",NA
Weighted Grammar,"As we have just seen, dealing with ambiguity is a key challenge in developing 
 broad-coverage parsers. Chart parsers improve the efficiency of computing 
 multiple parses of the same sentences, but they are still overwhelmed by the sheer 
 number of possible parses. Weighted grammars and probabilistic parsing 
 algorithms have provided an ef-fective solution to these problems.
  
 Before looking at these, we need to understand why the notion of grammaticality 
 could be 
 gradient
 . Considering the verb 
 give
 . This verb requires both a direct object 
 (the thing being given) and an indirect object (the recipient). These complements 
 can be given in either order, as illustrated in 
 (16)
 . In the “prepositional dative” form 
 in 
 (16a)
 , the direct object appears first, followed by a prepositional phrase 
 containing the indirect object.",NA
8.7  Summary,"• Sentences have internal organization that can be represented using a tree. 
 Notable features of constituent structure are: recursion, heads, complements, 
 and modifiers.
  
 • A grammar is a compact characterization of a potentially infinite set of 
 sentences; we say that a tree is well-formed according to a grammar, or that a 
 grammar licenses a tree.
  
 • A grammar is a formal model for describing whether a given phrase can be 
 assigned 
  
 a particular constituent or dependency structure.
  
 • Given a set of syntactic categories, a context-free grammar uses a set of 
 productions to say how a phrase of some category 
 A
  can be analyzed into a 
 sequence of smaller parts 
 α
 1
  ... 
 α
 n
 .
  
 • A dependency grammar uses productions to specify what the dependents are of 
 a 
  
 given lexical head.
  
 • Syntactic ambiguity arises when one sentence has more than one syntactic 
 analysis 
  
 (e.g., prepositional phrase attachment ambiguity).
  
 • A parser is a procedure for finding one or more trees corresponding to a 
 grammat-
  
 ically well-formed sentence.
  
 • A simple top-down parser is the recursive descent parser, which recursively ex-
 pands the start symbol (usually 
 S
 ) with the help of the grammar productions, 
 and tries to match the input sentence. This parser cannot handle left-recursive 
 pro-ductions (e.g., productions such as 
 NP -> NP PP
 ). It is inefficient in the way it 
 blindly expands categories without checking whether they are compatible with 
 the input string, and in repeatedly expanding the same non-terminals and 
 discarding the results.
  
 • A simple bottom-up parser is the shift-reduce parser, which shifts input onto a 
 stack and tries to match the items at the top of the stack with the righthand 
 side of grammar productions. This parser is not guaranteed to find a valid 
 parse for the input, even if one exists, and builds substructures without 
 checking whether it is globally consistent with the grammar.
  
 8.7  Summary | 321",NA
8.8  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web. For more examples of parsing with NLTK, 
 please see the Parsing HOWTO at 
 http://www.nltk.org/howto
 .
  
 There are many introductory books on syntax. (O’Grady et al., 2004) is a general in-
 troduction to linguistics, while (Radford, 1988) provides a gentle introduction to 
 trans-formational grammar, and can be recommended for its coverage of 
 transformational approaches to unbounded dependency constructions. The most 
 widely used term in linguistics for formal grammar is 
 generative grammar
 , 
 though it has nothing to do with generation (Chomsky, 1965).
  
 (Burton-Roberts, 1997) is a practically oriented textbook on how to analyze 
 constitu-ency in English, with extensive exemplification and exercises. (Huddleston 
 & Pullum, 2002) provides an up-to-date and comprehensive analysis of syntactic 
 phenomena in English.
  
 Chapter 12 of (Jurafsky & Martin, 2008) covers formal grammars of English; 
 Sections 13.1–3 cover simple parsing algorithms and techniques for dealing with 
 ambiguity; Chapter 14 covers statistical parsing; and Chapter 16 covers the 
 Chomsky hierarchy and the formal complexity of natural language. (Levin, 1993) 
 has categorized English verbs into fine-grained classes, according to their syntactic 
 properties.
  
 There are several ongoing efforts to build large-scale rule-based grammars, e.g., the 
 LFG Pargram project (
 http://www2.parc.com/istl/groups/nltt/pargram/
 ), the HPSG 
 Lin-GO Matrix framework (
 http://www.delph-in.net/matrix/
 ), and the XTAG Project 
 (
 http: //www.cis.upenn.edu/~xtag/
 ).",NA
8.9  Exercises,"1.
  ○ 
 Can you come up with grammatical sentences that probably have never been 
 uttered before? (Take turns with a partner.) What does this tell you about 
 human language?
  
 2.
  ○ 
 Recall Strunk and White’s prohibition against using a sentence-initial 
 however 
 to mean “although.” Do a web search for 
 however
  used at the start of 
 the sentence. How widely used is this construction?
  
 3.
  ○ 
 Consider the sentence 
 Kim arrived or Dana left and everyone cheered
 . Write 
 down the parenthesized forms to show the relative scope of 
 and
  and 
 or
 . 
 Generate tree structures corresponding to both of these interpretations.
  
 4.
  ○ 
 The 
 Tree
  class implements a variety of other useful methods. See the 
 Tree
  help 
 documentation for more details (i.e., import the 
 Tree
  class and then type 
 help(Tree)
 ).
  
 5.
  ○ 
 In this exercise you will manually construct some parse trees.
  
 322 | Chapter 8:Analyzing Sentence Structure",NA
CHAPTER 9,NA,NA
Building Feature-Based ,NA,NA
Grammars,"Natural languages have an extensive range of grammatical constructions which are 
 hard to handle with the simple methods described in 
 Chapter 8
 . In order to gain 
 more flex-ibility, we change our treatment of grammatical categories like 
 S
 , 
 NP
 , and 
 V
 . In place of atomic labels, we decompose them into structures like dictionaries, 
 where features can take on a range of values.
  
 The goal of this chapter is to answer the following questions:
  
 1. How can we extend the framework of context-free grammars with features so as 
 to gain more fine-grained control over grammatical categories and productions?
  
 2. What are the main formal properties of feature structures, and how do we use 
 them computationally?
  
 3. What kinds of linguistic patterns and grammatical constructions can we now 
 cap-ture with feature-based grammars?
  
 Along the way, we will cover more topics in English syntax, including phenomena 
 such as agreement, subcategorization, and unbounded dependency constructions.",NA
9.1  Grammatical Features,"In 
 Chapter 6
 , we described how to build classifiers that rely on detecting features of 
 text. Such features may be quite simple, such as extracting the last letter of a word, 
 or more complex, such as a part-of-speech tag that has itself been predicted by the 
 clas-sifier. In this chapter, we will investigate the role of features in building rule-
 based grammars. In contrast to feature extractors, which record features that have 
 been au-tomatically detected, we are now going to 
 declare
  the features of words 
 and phrases. We start off with a very simple example, using dictionaries to store 
 features and their values.
  
 >>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'} 
  
 >>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}",NA
Syntactic Agreement,"The following examples show pairs of word sequences, the first of which is 
 grammatical and the second not. (We use an asterisk at the start of a word 
 sequence to signal that it is ungrammatical.)
  
 (1) a. this dog
  
 b. *these dog
  
 (2) a. these dogs
  
 b. *this dogs
  
 In English, nouns are usually marked as being singular or plural. The form of the 
 de-monstrative also varies: 
 this
  (singular) and 
 these
  (plural). Examples 
 (1)
  and 
 (2)
  
 show that there are constraints on the use of demonstratives and nouns within a 
 noun phrase: either both are singular or both are plural. A similar constraint holds 
 between subjects and predicates:
  
 (3) a. the dog runs
  
 b. *the dog run
  
 (4) a. the dogs run
  
 b. *the dogs runs
  
 Here we can see that morphological properties of the verb co-vary with syntactic 
 prop-erties of the subject noun phrase. This co-variance is called 
 agreement
 . If we 
 look further at verb agreement in English, we will see that present tense verbs 
 typically have two inflected forms: one for third person singular, and another for 
 every other combi-nation of person and number, as shown in 
 Table 9-1
 .
  
 9.1  Grammatical Features | 329",NA
Using Attributes and Constraints,"We spoke informally of linguistic categories having 
 properties
 , for example, that a 
 noun has the property of being plural. Let’s make this explicit:
  
 (9)
  N[NUM=pl]
  
 In 
 (9)
 , we have introduced some new notation which says that the category 
 N
  has a 
 (grammatical) 
 feature
  called 
 NUM
  (short for “number”) and that the value of this 
 feature is 
 pl
  (short for “plural”). We can add similar annotations to other categories, 
 and use them in lexical entries:
  
 (10)
  Det[NUM=sg] -> 'this' 
  
 Det[NUM=pl] -> 'these'
  
 N[NUM=sg] -> 'dog' 
  
 N[NUM=pl] -> 'dogs' 
  
 V[NUM=sg] -> 'runs' 
  
 V[NUM=pl] -> 'run'
  
 Does this help at all? So far, it looks just like a slightly more verbose alternative to 
 what was specified in 
 (8)
 . Things become more interesting when we allow 
 variables
  
 over feature values, and use these to state constraints:
  
 (11)
  S -> NP[NUM=?n] VP[NUM=?n] 
  
 NP[NUM=?n] -> Det[NUM=?n] 
 N[NUM=?n] 
  
 VP[NUM=?n] -> V[NUM=?n]
  
 We are using 
 ?n
  as a variable over values of 
 NUM
 ; it can be instantiated either to 
 sg
  
 or 
 pl
 , within a given production. We can read the first production as saying that 
 whatever value 
 NP
  takes for the feature 
 NUM
 , 
 VP
  must take the same value.
  
 In order to understand how these feature constraints work, it’s helpful to think 
 about how one would go about building a tree. Lexical productions will admit the 
 following local trees (trees of depth one):",NA
Terminology,"So far, we have only seen feature values like 
 sg
  and 
 pl
 . These simple values are 
 usually called 
 atomic
 —that is, they can’t be decomposed into subparts. A special 
 case of atomic values are 
 Boolean
  values, that is, values that just specify whether a 
 property is true or false. For example, we might want to distinguish 
 auxiliary
  verbs 
 such as 
 can
 ,",NA
9.2  Processing Feature Structures,"In this section, we will show how feature structures can be constructed and 
 manipulated in NLTK. We will also discuss the fundamental operation of 
 unification, which allows us to combine the information contained in two different 
 feature structures.
  
 Feature structures in NLTK are declared with the 
 FeatStruct()
  constructor. Atomic 
 feature values can be strings or integers.
  
 >>> fs1 = nltk.FeatStruct(TENSE='past', NUM='sg') >>> 
 print fs1 
  
 [ NUM   = 'sg'   ] 
  
 [ TENSE = 'past' ]
  
 A feature structure is actually just a kind of dictionary, and so we access its values 
 by indexing in the usual way. We can use our familiar syntax to 
 assign
  values to 
 features:
  
 >>> fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem') >>> 
 print fs1['GND'] 
  
 fem 
  
 >>> fs1['CASE'] = 'acc'
  
 We can also define feature structures that have complex values, as discussed 
 earlier.
  
 >>> fs2 = nltk.FeatStruct(POS='N', AGR=fs1) 
  
 >>> print fs2 
  
 [       [ CASE = 'acc' ] ] 
  
 [ AGR = [ GND  = 'fem' ] ] 
  
 [       [ NUM  = 'pl'  ] ] 
  
 [       [ PER  = 3     ] ] 
  
 [                        ] 
  
 [ POS = 'N'              ]",NA
Subsumption and Unification,"It is standard to think of feature structures as providing 
 partial information
  about 
 some object, in the sense that we can order feature structures according to how 
 general they are. For example, 
 (25a)
  is more general (less specific) than 
 (25b)
 , 
 which in turn is more general than 
 (25c)
 .
  
 (25) a.
 [NUMBER = 74]
  
 b.
 [NUMBER = 74          ] 
  
 [STREET = 'rue Pascal']
  
 c.
 [NUMBER = 74          ] 
  
 [STREET = 'rue Pascal'] 
  
 [CITY = 'Paris'       ]
  
 This ordering is called 
 subsumption
 ; a more general feature structure 
 subsumes
  a 
 less general one. If 
 FS
 0
  subsumes 
 FS
 1
  (formally, we write 
 FS
 0
 ⊑
 FS
 1
 ), then 
 FS
 1
  must 
 have all the paths and path equivalences of 
 FS
 0
 , and may have additional paths and 
 equiv-alences as well. Thus, 
 (23)
  subsumes 
 (24)
  since the latter has additional path 
 equiva-lences. It should be obvious that subsumption provides only a partial 
 ordering 
 on 
 fea-ture 
 structures, 
 since 
 some 
 feature 
 structures 
 are 
 incommensurable. For example, 
 (26)
  neither subsumes nor is subsumed by 
 (25a)
 .
  
 (26)
  [TELNO = 01 27 86 42 96]
  
 So we have seen that some feature structures are more specific than others. How do 
 we go about specializing a given feature structure? For example, we might decide 
 that addresses should consist of not just a street number and a street name, but 
 also a city. That is, we might want to 
 merge
  graph 
 (27a)
  with 
 (27b)
  to yield 
 (27c)
 .
  
 9.2  Processing Feature Structures | 341",NA
9.3  Extending a Feature-Based Grammar,"In this section, we return to feature-based grammar and explore a variety of 
 linguistic issues, and demonstrate the benefits of incorporating features into the 
 grammar.",NA
Subcategorization,"In Chapter 8, we augmented our category labels to represent different kinds of 
 verbs, and used the labels 
 IV
  and 
 TV
  for intransitive and transitive verbs 
 respectively. This allowed us to write productions like the following:
  
 344 | Chapter 9:Building Feature-Based Grammars",NA
Heads Revisited,"We noted in the previous section that by factoring subcategorization information 
 out of the main category label, we could express more generalizations about 
 properties of verbs. Another property of this kind is the following: expressions of 
 category 
 V
  are heads of phrases of category 
 VP
 . Similarly, 
 N
 s are heads of 
 NP
 s, 
 A
 s 
 (i.e., adjectives) are heads of 
 AP
 s, and 
 P
 s (i.e., prepositions) are heads of 
 PP
 s. Not all 
 phrases have heads—for exam-ple, it is standard to say that coordinate phrases 
 (e.g., 
 the book and the bell
 ) lack heads. Nevertheless, we would like our grammar 
 formalism to express the parent/head-child relation where it holds. At present, 
 V
  
 and 
 VP
  are just atomic symbols, and we need to find a way to relate them using 
 features (as we did earlier to relate 
 IV
  and 
 TV
 ).
  
 X-bar syntax addresses this issue by abstracting out the notion of 
 phrasal level
 . It 
 is usual to recognize three such levels. If 
 N
  represents the lexical level, then 
 N
 ' 
 represents the next level up, corresponding to the more traditional category 
 Nom
 , 
 and 
 N
 '' represents the phrasal level, corresponding to the category 
 NP
 . 
 (36a)
  
 illustrates a representative structure, while 
 (36b)
  is the more conventional 
 counterpart.
  
 (36)
  
 a.
  
  
 b.
  
  
 The head of the structure 
 (36a)
  is 
 N
 , and 
 N
 ' and 
 N
 '' are called 
 (phrasal) 
 projections
  of 
 N
 . 
 N
 '' is the 
 maximal projection
 , and 
 N
  is sometimes called the 
 zero 
 projection
 . One of the central claims of X-bar syntax is that all constituents share a 
 structural similarity. Using 
 X
  as a variable over 
 N
 , 
 V
 , 
 A
 , and 
 P
 , we say that directly 
 subcategorized 
 comple-ments
  of a lexical head 
 X
  are always placed as siblings of the 
 head, whereas 
 adjuncts
  are placed as siblings of the intermediate category, 
 X'
 . Thus, 
 the configuration of the two 
 P
 '' adjuncts in 
 (37)
  contrasts with that of the 
 complement 
 P
 '' in 
 (36a)
 .",NA
Auxiliary Verbs and Inversion ,"Inverted clauses—where the order of subject and verb is switched—occur in 
 English interrogatives and also after “negative” adverbs: 
  
  
 (39) 
   
 a. Do you like children?
  
 b. Can Jody walk?
  
 (40) a. Rarely do you see Kim.
  
 b. Never have I seen this dog.
  
 However, we cannot place just any verb in pre-subject position: 
  
 (41) 
  
 a. *Like you children?
  
 b. *Walks Jody?
  
 (42) a. *Rarely see you Kim.
  
 b. *Never saw I this dog.
  
 Verbs that can be positioned initially in inverted clauses belong to the class known 
 as 
 auxiliaries
 , and as well as 
 do
 , 
 can
 , and 
 have
  include 
 be
 , 
 will
 , and 
 shall
 . One way of 
 capturing such structures is with the following production: 
  
  
 (43)
  S[+INV] -> V[+AUX] NP VP
  
 348 | Chapter 9:Building Feature-Based Grammars",NA
Unbounded Dependency Constructions,"Consider the following contrasts:
  
 (45) a. You like Jody.
  
 b. *You like.
  
 (46) a. You put the card into the slot.
  
 b. *You put into the slot.
  
 c. *You put the card.
  
 d. *You put.
  
 The verb 
 like
  requires an 
 NP
  complement, while 
 put
  requires both a following 
 NP
  
 and
  
 PP
 . 
 (45)
  and 
 (46)
  show that these complements are 
 obligatory
 : omitting them leads 
 to
  
 ungrammaticality. Yet there are contexts in which obligatory complements can be
  
 omitted, as 
 (47)
  and 
 (48)
  illustrate.
  
 (47) a. Kim knows who you like.
  
 b. This music, you really like.
  
 (48) a. Which card do you put into the slot?
  
 b. Which slot do you put the card into?
  
 That is, an obligatory complement can be omitted if there is an appropriate 
 filler
  in
  
 the sentence, such as the question word 
 who
  in 
 (47a)
 , the preposed topic 
 this music
  
 in
  
 (47b)
 , or the 
 wh
  phrases 
 which card/slot
  in 
 (48)
 . It is common to say that sentences 
 like
  
 those in 
 (47)
  and 
 (48)
  contain 
 gaps
  where the obligatory complements have been
  
 omitted, and these gaps are sometimes made explicit using an underscore:
  
 (49) a. Which card do you put __ into the slot?",NA
Case and Gender in German,"Compared with English, German has a relatively rich morphology for agreement. 
 For example, the definite article in German varies with case, gender, and number, 
 as shown in 
 Table 9-2
 .
  
 Table 9-2. Morphological paradigm for the German definite article
  
  
  
  
  
  
 Case
  
 Mascul
 ine
  
 Femin
 ine
  
 Neut
 ral
  
 Plura
 l
  
 Nomina
 tive
  
 der
  
 die
  
 das
  
 die
  
 Genitive
  
 des
  
 der
  
 des
  
 der
  
 Dative
  
 dem
  
 der
  
 dem
  
 den
  
 Accusat
 ive
  
 den
  
 die
  
 das
  
 die
  
 Subjects in German take the nominative case, and most verbs govern their objects 
 in the accusative case. However, there are exceptions, such as 
 helfen
 , that govern 
 the dative case:
  
 (55)
  
 a.
 Die 
  
 Katze sieht 
  
 den 
  
 Hund
  
 the.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.ACC.MASC.SG 
 dog.3.MASC.SG
  
 ‘the cat sees the dog’
  
 b.
 *Die 
  
 Katze sieht 
  
 dem 
  
 Hund 
  
 the.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.DAT.MASC.SG 
 dog.3.MASC.SG
  
 c.
 Die 
  
 Katze hilft dem 
  
 Hund 
  
 the.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.DAT.MASC.SG 
 dog.3.MASC.SG‘the cat helps the dog’
  
 d.
 *Die 
  
 Katze 
  
 hilft 
  
 den 
  
 Hund 
  
  
 the.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.ACC.MASC.SG 
 dog.3.MASC.SG
  
 The grammar in 
 Example 9-4
  illustrates the interaction of agreement (comprising 
 per-son, number, and gender) with case.
  
 9.3  Extending a Feature-Based Grammar | 353",NA
9.4  Summary,"• The traditional categories of context-free grammar are atomic symbols. An 
 impor-tant motivation for feature structures is to capture fine-grained 
 distinctions that would otherwise require a massive multiplication of atomic 
 categories.
  
 • By using variables over feature values, we can express constraints in grammar 
 pro-ductions that allow the realization of different feature specifications to be 
 inter-dependent.
  
 • Typically we specify fixed values of features at the lexical level and constrain the 
 values of features in phrases to unify with the corresponding values in their 
 children.
  
 • Feature values are either atomic or complex. A particular subcase of atomic 
 value 
  
 is the Boolean value, represented by convention as [+/- 
 feat
 ].
  
 • Two features can share a value (either atomic or complex). Structures with 
 shared values are said to be re-entrant. Shared values are represented by 
 numerical indexes (or tags) in AVMs.
  
 • A path in a feature structure is a tuple of features corresponding to the labels on 
 a 
  
 sequence of arcs from the root of the graph representation.
  
 • Two paths are equivalent if they share a value.
  
 • Feature structures are partially ordered by subsumption. 
 FS
 0
  subsumes 
 FS
 1
  
 when 
  
 FS
 0
  is more general (less informative) than 
 FS
 1
 .
  
 • The unification of two structures 
 FS
 0
  and 
 FS
 1
 , if successful, is the feature 
 structure 
  
 FS
 2
  that contains the combined information of both 
 FS
 0
  and 
 FS
 1
 .
  
 • If unification specializes a path 
 π
  in 
 FS
 , then it also specializes every path 
 π
 ' 
 equiv-
  
 alent to 
 π
 .
  
 • We can use feature structures to build succinct analyses of a wide variety of lin-
 guistic phenomena, including verb subcategorization, inversion constructions, 
 unbounded dependency constructions, and case government.
  
 356 | Chapter 9:Building Feature-Based Grammars",NA
9.5  Further Reading,"Please consult 
 http://www.nltk.org/
  for further materials on this chapter, including 
 HOWTOs feature structures, feature grammars, Earley parsing, and grammar test 
 suites.
  
 For an excellent introduction to the phenomenon of agreement, see (Corbett, 2006).
  
 The earliest use of features in theoretical linguistics was designed to capture 
 phono-logical properties of phonemes. For example, a sound like /b/ might be 
 decomposed into the structure 
 [+labial, +voice]
 . An important motivation was to 
 capture gener-alizations across classes of segments, for example, that /n/ gets 
 realized as /m/ preceding any 
 +labial
  consonant. Within Chomskyan grammar, it 
 was standard to use atomic features for phenomena such as agreement, and also to 
 capture generalizations across syntactic categories, by analogy with phonology. A 
 radical expansion of the use of features in theoretical syntax was advocated by 
 Generalized Phrase Structure Grammar (GPSG; [Gazdar et al., 1985]), particularly in 
 the use of features with complex values.
  
 Coming more from the perspective of computational linguistics, (Kay, 1985) 
 proposed that functional aspects of language could be captured by unification of 
 attribute-value structures, and a similar approach was elaborated by (Grosz & 
 Stickel, 1983) within the PATR-II formalism. Early work in Lexical-Functional 
 grammar (LFG; [Kaplan & Bresnan, 1982]) introduced the notion of an 
 f-structure
  
 that was primarily intended to represent the grammatical relations and predicate-
 argument structure associated with a constituent structure parse. (Shieber, 1986) 
 provides an excellent introduction to this phase of research into feature-based 
 grammars.
  
 One conceptual difficulty with algebraic approaches to feature structures arose 
 when researchers attempted to model negation. An alternative perspective, 
 pioneered by (Kasper & Rounds, 1986) and (Johnson, 1988), argues that grammars 
 involve 
 descrip-tions
  of feature structures rather than the structures themselves. 
 These descriptions are combined using logical operations such as conjunction, and 
 negation is just the usual logical operation over feature descriptions. This 
 description-oriented perspective was integral to LFG from the outset (Kaplan, 
 1989), and was also adopted by later versions of Head-Driven Phrase Structure 
 Grammar (HPSG; [Sag & Wasow, 1999]). A com-prehensive bibliography of HPSG 
 literature can be found at 
 http://www.cl.uni-bremen .de/HPSG-Bib/
 .
  
 Feature structures, as presented in this chapter, are unable to capture important 
 con-straints on linguistic information. For example, there is no way of saying that 
 the only permissible values for 
 NUM
  are 
 sg
  and 
 pl
 , while a specification such as 
 [NUM=masc]
  is anomalous. Similarly, we cannot say that the complex value of 
 AGR
 must
  contain spec-ifications for the features 
 PER
 , 
 NUM
 , and 
 GND
 , but 
 cannot
  
 contain a specification such as 
 [SUBCAT=trans]
 . 
 Typed feature structures
  were 
 developed to remedy this deficiency. A good early review of work on typed feature 
 structures is (Emele & Zajac, 1990). A more comprehensive examination of the 
 formal foundations can be found in",NA
9.6  Exercises,"1.
  ○ 
 What constraints are required to correctly parse word sequences like 
 I am 
 hap-py
  and 
 she is happy
  but not *
 you is happy
  or *
 they am happy
 ? Implement 
 two sol-utions for the present tense paradigm of the verb 
 be
  in English, first 
 taking Gram-mar 
 (8)
  as your starting point, and then taking Grammar 
 (20)
  as 
 the starting point.
  
 2.
  ○ 
 Develop a variant of grammar in 
 Example 9-1
  that uses a feature 
 COUNT
  to 
 make the distinctions shown here:
  
 (56) a. The boy sings.
  
 b. *Boy sings.
  
 (57) a. The boys sing.
  
 b. Boys sing.
  
 (58) a. The water is precious.
  
 b. Water is precious.
  
 3.
  ○ 
 Write a function 
 subsumes()
  that holds of two feature structures 
 fs1
  and 
 fs2
  
 just in case 
 fs1
  subsumes 
 fs2
 .
  
 4.
  ○ 
 Modify the grammar illustrated in 
 (30)
  to incorporate a 
 BAR
  feature for 
 dealing with phrasal projections.
  
 5.
  ○ 
 Modify the German grammar in 
 Example 9-4
  to incorporate the treatment of 
 subcategorization presented in 
 Section 9.3
 .
  
 6.
  
 ◑
  
 Develop a feature-based grammar that will correctly describe the following 
 Spanish noun phrases:
  
 (59)
 un 
  
 cuadro hermos-o 
  
 INDEF.SG.MASC picture beautiful-
 SG.MASC
  
 ‘a beautiful picture’
  
 (60)
 un-os cuadro-s hermos-os 
  
 INDEF-PL.MASC picture-PL beautiful-
 PL.MASC
  
 ‘beautiful pictures’",NA
CHAPTER 10,NA,NA
Analyzing the Meaning of ,NA,NA
Sentences,"We have seen how useful it is to harness the power of a computer to process text on 
 a large scale. However, now that we have the machinery of parsers and feature-
 based grammars, can we do anything similarly useful by analyzing the meaning of 
 sentences?
  
 The goal of this chapter is to answer the following questions:
  
 1. How can we represent natural language meaning so that a computer can 
 process these representations?
  
 2. How can we associate meaning representations with an unlimited set of 
 sentences?
  
 3. How can we use programs that connect the meaning representations of 
 sentences to stores of knowledge?
  
 Along the way we will learn some formal techniques in the field of logical 
 semantics, and see how these can be used for interrogating databases that store 
 facts about the world.",NA
10.1  Natural Language Understanding,NA,NA
Querying a Database,"Suppose we have a program that lets us type in a natural language question and 
 gives us back the right answer:
  
 (1) a. Which country is Athens in?
  
 b. Greece.
  
 How hard is it to write such a program? And can we just use the same techniques 
 that we’ve encountered so far in this book, or does it involve something new? In 
 this section, we will show that solving the task in a restricted domain is pretty 
 straightforward. But we will also see that to address the problem in a more general",NA
"Natural Language, Semantics, and Logic","We started out trying to capture the meaning of 
 (1a)
  by translating it into a query 
 in another language, SQL, which the computer could interpret and execute. But this 
 still begged the question whether the translation was correct. Stepping back from 
 database query, we noted that the meaning of 
 and
  seems to depend on being able to 
 specify when statements are true or not in a particular situation. Instead of 
 translating a sentence 
 S 
 from one language to another, we try to say what 
 S
  is 
 about
  
 by relating it to a situation in the world. Let’s pursue this further. Imagine there is a 
 situation 
 s
  where there are two entities, Margrietje and her favorite doll, Brunoke. 
 In addition, there is a relation holding between the two entities, which we will call 
 the 
 love
  relation. If you understand the meaning of 
 (3)
 , then you know that it is true 
 in situation 
 s
 . In part, you know this because you know that 
 Margrietje
  refers to 
 Margrietje, 
 Brunoke
  refers to Brunoke, and 
 houdt van
  refers to the 
 love
  relation.
  
 We have introduced two fundamental notions in semantics. The first is that 
 declarative sentences are 
 true or false in certain situations
 . The second is that 
 definite noun phrases and proper nouns 
 refer to things in the world
 . So 
 (3)
  is true in 
 a situation where Mar-grietje loves the doll Brunoke, here illustrated in 
 Figure 10-
 1
 .
  
 Once we have adopted the notion of truth in a situation, we have a powerful tool for 
 reasoning. In particular, we can look at sets of sentences, and ask whether they 
 could be true together in some situation. For example, the sentences in 
 (5)
  can be 
 both true, whereas those in 
 (6)
  and 
 (7)
  cannot be. In other words, the sentences in 
 (5)
  are 
 con-sistent
 , whereas those in 
 (6)
  and 
 (7)
  are 
 inconsistent
 .
  
 (5) a. Sylvania is to the north of Freedonia.
  
 b. Freedonia is a republic.",NA
10.2  Propositional Logic,"A logical language is designed to make reasoning formally explicit. As a result, it can 
 capture aspects of natural language which determine whether a set of sentences is 
 con-sistent. As part of this approach, we need to develop logical representations of 
 a sen-tence 
 φ
  that formally capture the 
 truth-conditions
  of 
 φ
 . We’ll start off with a 
 simple example:
  
 (8) [Klaus chased Evi] and [Evi ran away].
  
 Let’s replace the two sub-sentences in 
 (8)
  by 
 φ
  and 
 ψ
  respectively, and put 
 &
  for the 
 logical operator corresponding to the English word 
 and
 : 
 φ 
 &
 ψ
 . This structure is the 
 logical form
  of 
 (8)
 .
  
 Propositional logic
  allows us to represent just those parts of linguistic structure 
 that correspond to certain sentential connectives. We have just looked at 
 and
 . Other 
 such connectives are 
 not
 , 
 or
 , and 
 if..., then...
 . In the formalization of propositional 
 logic, the counterparts of such connectives are sometimes called 
 Boolean 
 operators
 . The basic expressions of propositional logic are 
 propositional 
 symbols
 , often written as 
 P
 , 
 Q
 , 
 R
 , etc. There are varying conventions for 
 representing Boolean operators. Since we will be focusing on ways of exploring 
 logic within NLTK, we will stick to the following ASCII versions of the operators:
  
 >>> nltk.boolean_ops() 
  
 negation            -
  
 conjunction         & 
  
 disjunction         | 
  
 implication         -> 
  
 equivalence         <->
  
 From the propositional symbols and the Boolean operators we can build an infinite 
 set of 
 well-formed formulas
  (or just formulas, for short) of propositional logic. 
 First, every propositional letter is a formula. Then if 
 φ
  is a formula, so is 
 -
 φ
 . And if 
 φ
  
 and
 ψ
  are formulas, then so are 
 (
 φ 
 &
 ψ
 )
 , 
 (
 φ 
 |
 ψ
 )
 , 
 (
 φ 
 ->
 ψ
 )
 , and
 (
 φ 
 <->
 ψ
 )
 .
  
 Table 10-2
  specifies the truth-conditions for formulas containing these operators. 
 As before we use 
 φ
  and 
 ψ
  as variables over sentences, and abbreviate 
 if and only if
  
 as 
 iff
 .
  
 Table 10-2. Truth conditions for the Boolean operators in propositional logic
  
  
  
  
  
 Boolean operator
  
 Truth 
 conditions
  
  
  
 negation (
 it is not the 
 case that ...
 )
  
 -
 φ is true in 
 s
  
 iff
  
 φ is false in 
 s
  
 conjunction (
 and
 )
  
 (
 φ 
 &
  ψ
 )
  is true in 
 s
  
 iff
  
 φ is true in 
 s
  and ψ is true in 
 s
  
 368 | Chapter 10:Analyzing the Meaning of Sentences",NA
10.3  First-Order Logic,"In the remainder of this chapter, we will represent the meaning of natural language 
 expressions by translating them into first-order logic. Not all of natural language 
 se-mantics can be expressed in first-order logic. But it is a good choice for 
 computational semantics because it is expressive enough to represent many 
 aspects of semantics, and on the other hand, there are excellent systems available 
 off the shelf for carrying out automated inference in first-order logic.
  
 Our next step will be to describe how formulas of first-order logic are constructed, 
 and then how such formulas can be evaluated in a model.",NA
Syntax,"First-order logic keeps all the Boolean operators of propositional logic, but it adds 
 some important new mechanisms. To start with, propositions are analyzed into 
 predicates and arguments, which takes us a step closer to the structure of natural 
 languages. The standard construction rules for first-order logic recognize 
 terms
  
 such as individual variables and individual constants, and 
 predicates
  that take 
 differing numbers of 
 ar-guments
 . For example, 
 Angus walks
  might be formalized as 
 walk
 (
 angus
 ) and 
 Angus sees Bertieas see
 (
 angus
 , 
 bertie
 ). We will call 
 walk
  a 
 unary 
 predicate
 , and 
 see
  a 
 binary predicate
 . The symbols used as predicates do not have 
 intrinsic meaning, although it is hard to remember this. Returning to one of our 
 earlier examples, there is no 
 logical 
 difference between 
 (13a)
  and 
 (13b)
 .
  
 (13) a.
  love
 (
 margrietje
 , 
 brunoke
 )
  
 b.
  houden_van
 (
 margrietje
 , 
 brunoke
 )
  
 By itself, first-order logic has nothing substantive to say about lexical semantics—
 the meaning of individual words—although some theories of lexical semantics can 
 be en-coded in first-order logic. Whether an atomic predication like 
 see
 (
 angus
 , 
 bertie
 ) is true or false in a situation is not a matter of logic, but depends on the 
 particular valuation that we have chosen for the constants 
 see
 , 
 angus
 , and 
 bertie
 . 
 For this reason, such expressions are called 
 non-logical constants
 . By contrast, 
 logical constants
  (such as the Boolean operators) always receive the same 
 interpretation in every model for first-order logic.
  
 We should mention here that one binary predicate has special status, namely 
 equality, as in formulas such as 
 angus = aj
 . Equality is regarded as a logical 
 constant, since for individual terms 
 t
 1
  and 
 t
 2
 , the formula 
 t
 1
  = t
 2
  is true if and only if 
 t
 1
  and 
 t
 2
  refer to one and the same entity.
  
 It is often helpful to inspect the syntactic structure of expressions of first-order 
 logic, and the usual way of doing this is to assign 
 types
  to expressions. Following 
 the tradition of Montague grammar, we will use two 
 basic types
 : 
 e
  is the type of 
 entities, while 
 t
  is the type of formulas, i.e., expressions that have truth values. 
 Given these two basic
  
 372 | Chapter 10:Analyzing the Meaning of Sentences",NA
First-Order Theorem Proving,"Recall the constraint on 
 to the north of
 , which we proposed earlier as 
 (10)
 :
  
 (22) if 
 x
  is to the north of 
 y
  then 
 y
  is not to the north of 
 x
 .
  
 We observed that propositional logic is not expressive enough to represent 
 generali-zations about binary predicates, and as a result we did not properly 
 capture the argu-ment 
 Sylvania is to the north of Freedonia. Therefore, Freedonia is 
 not to the north of Sylvania
 .
  
 You have no doubt realized that first-order logic, by contrast, is ideal for formalizing 
 such rules:
  
 all x. all y.(north_of(x, y) -> -north_of(y, x))
  
 Even better, we can perform automated inference to show the validity of the 
 argument.
  
 10.3  First-Order Logic | 375",NA
Summarizing the Language of First-Order Logic,"We’ll take this opportunity to restate our earlier syntactic rules for propositional 
 logic and add the formation rules for quantifiers; together, these give us the syntax 
 of first-order logic. In addition, we make explicit the types of the expressions 
 involved. We’ll adopt the convention that 
 〈
 e
 n
 , 
 t
 〉
  is the type of a predicate that 
 combines with 
 n
  argu-ments of type 
 e
  to yield an expression of type 
 t
 . In this case, 
 we say that 
 n
  is the 
 arity 
 of the predicate.
  
 1. If 
 P
  is a predicate of type 
 〈
 e
 n
 , 
 t
 〉
 , and 
 α
 1
 , ... 
 α
 n
  are terms of type 
 e
 , then 
 P
 (
 α
 1
 , ... 
 α
 n
 ) 
 is of type 
 t
 .
  
 2. If 
 α
  and 
 β
  are both of type 
 e
 , then (
 α 
 = 
 β
 ) and (
 α 
 != 
 β
 ) are of type 
 t
 .
  
 3. If 
 φ
  is of type 
 t
 , then so is 
 -
 φ
 .
  
 4. If 
 φ
  and 
 ψ
  are of type 
 t
 , then so are (
 φ 
 &
 ψ
 ), (
 φ 
 |
 ψ
 ), (
 φ 
 ->
 ψ
 ), and (
 φ 
 <->
 ψ
 ).
  
 5. If 
 φ
  is of type 
 t
 , and 
 x
  is a variable of type 
 e
 , then 
 exists x.
 φ
  and 
 all x.
 φ
  are of type 
 t
 .
  
 Table 10-3
  summarizes the new logical constants of the 
 logic
  module, and two of the 
 methods of 
 Expression
 s.
  
 376 | Chapter 10:Analyzing the Meaning of Sentences",NA
Truth in Model,"We have looked at the syntax of first-order logic, and in 
 Section 10.4
  we will 
 examine the task of translating English into first-order logic. Yet as we argued in 
 Section 10.1
 , this gets us further forward only if we can give a meaning to sentences 
 of first-order logic. In other words, we need to give a 
 truth-conditional semantics
  to 
 first-order logic. From the point of view of computational semantics, there are 
 obvious limits to how far one can push this approach. Although we want to talk 
 about sentences being true or false in situations, we only have the means of 
 representing situations in the computer in a symbolic manner. Despite this 
 limitation, it is still possible to gain a clearer picture of truth-conditional semantics 
 by encoding models in NLTK.
  
 Given a first-order logic language 
 L
 , a model 
 M
  for 
 L
  is a pair 
 〈
 D
 , 
 Val
 〉
 , where 
 D
  is an 
 non-empty set called the 
 domain
  of the model, and 
 Val
  is a function called the 
 valu-ation function
 , which assigns values from 
 D
  to expressions of 
 L
  as follows:
  
 1. For every individual constant 
 c
  in 
 L
 , 
 Val
 (
 c
 ) is an element of 
 D
 .
  
 2. For every predicate symbol 
 P
  of arity 
 n
 ≥
  0, 
 Val
 (
 P
 ) is a function from 
 D
 n
  to {
 True
 , 
 False
 }. (If the arity of 
 P
  is 0, then 
 Val
 (
 P
 ) is simply a truth value, and 
 P
  is 
 regarded as a propositional symbol.)
  
 According to 2, if 
 P
  is of arity 2, then 
 Val
 (
 P
 ) will be a function 
 f
  from pairs of 
 elements of 
 D
  to {
 True
 , 
 False
 }. In the models we shall build in NLTK, we’ll adopt a 
 more con-venient alternative, in which 
 Val
 (
 P
 ) is a set 
 S
  of pairs, defined as follows:
  
 (23)
  S
  = {
 s| f
 (
 s
 ) 
 = True
 }
  
 Such an 
 f
  is called the 
 characteristic function
  of 
 S
  (as discussed in the further 
 readings).
  
 Relations are represented semantically in NLTK in the standard set-theoretic way: 
 as sets of tuples. For example, let’s suppose we have a domain of discourse 
 consisting of the individuals Bertie, Olive, and Cyril, where Bertie is a boy, Olive is a 
 girl, and Cyril is a dog. For mnemonic reasons, we use 
 b
 , 
 o
 , and 
 c
  as the 
 corresponding labels in the model. We can declare the domain as follows:
  
 >>> dom = set(['b', 'o', 'c'])
  
 10.3  First-Order Logic | 377",NA
Individual Variables and Assignments,"In our models, the counterpart of a context of use is a variable 
 assignment
 . This is 
 a mapping from individual variables to entities in the domain. Assignments are 
 created using the 
 Assignment
  constructor, which also takes the model’s domain of 
 discourse as a parameter. We are not required to actually enter any bindings, but if 
 we do, they are in a (
 variable
 , 
 value
 ) format similar to what we saw earlier for 
 valuations.
  
 378 | Chapter 10:Analyzing the Meaning of Sentences",NA
Quantification,"One of the crucial insights of modern logic is that the notion of variable satisfaction 
 can be used to provide an interpretation for quantified formulas. Let’s use 
 (24)
  as 
 an example.
  
 (24)
  exists x.(girl(x) & walk(x))
  
 When is it true? Let’s think about all the individuals in our domain, i.e., in 
 dom
 . We 
 want to check whether any of these individuals has the property of being a girl and 
 walking. In other words, we want to know if there is some 
 u
  in 
 dom
  such that 
 g[
 u
 /x] 
 satisfies the open formula 
 (25)
 .
  
 (25)
  girl(x) & walk(x)
  
 Consider the following:
  
 >>> m.evaluate('exists x.(girl(x) & walk(x))', g) True
  
 evaluate()
  returns 
 True
  here because there is some 
 u
  in 
 dom
  such that 
 (25)
  is 
 satisfied by an assignment which binds 
 x
  to 
 u
 . In fact, 
 o
  is such a 
 u
 :
  
 >>> m.evaluate('girl(x) & walk(x)', g.add('x', 'o')) True
  
 One useful tool offered by NLTK is the 
 satisfiers()
  method. This returns a set of all 
 the individuals that satisfy an open formula. The method parameters are a parsed 
 for-mula, a variable, and an assignment. Here are a few examples:
  
 >>> fmla1 = lp.parse('girl(x) | boy(x)') 
  
 >>> m.satisfiers(fmla1, 'x', g) 
  
 set(['b', 'o']) 
  
 >>> fmla2 = lp.parse('girl(x) -> walk(x)') 
  
 >>> m.satisfiers(fmla2, 'x', g) 
  
 set(['c', 'b', 'o']) 
  
 >>> fmla3 = lp.parse('walk(x) -> girl(x)') 
  
 >>> m.satisfiers(fmla3, 'x', g) 
  
 set(['b', 'o'])
  
 It’s useful to think about why 
 fmla2
  and 
 fmla3
  receive the values they do. The truth 
 conditions for 
 ->
  mean that 
 fmla2
  is equivalent to 
 -girl(x) | walk(x)
 , which is satisfied 
 by something that either isn’t a girl or walks. Since neither 
 b
  (Bertie) nor 
 c
  (Cyril) 
 are girls, according to model 
 m
 , they both satisfy the whole formula. And of course 
 o
  
 satisfies the formula because 
 o
  satisfies both disjuncts. Now, since every member of 
 the domain of discourse satisfies 
 fmla2
 , the corresponding universally quantified 
 formula is also true.
  
 >>> m.evaluate('all x.(girl(x) -> walk(x))', g) True
  
 380 | Chapter 10:Analyzing the Meaning of Sentences",NA
Quantifier Scope Ambiguity,"What happens when we want to give a formal representation of a sentence with 
 two 
 quantifiers, such as the following?
  
 (26) Everybody admires someone.
  
 There are (at least) two ways of expressing 
 (26)
  in first-order logic:
  
 (27) a.
  all x.(person(x) -> exists y.(person(y) & admire(x,y)))
  
 b.
  exists y.(person(y) & all x.(person(x) -> admire(x,y)))
  
 Can we use both of these? The answer is yes, but they have different meanings. 
 (27b) 
 is logically stronger than 
 (27a)
 : it claims that there is a unique person, say, 
 Bruce, who is admired by everyone. 
 (27a)
 , on the other hand, just requires that for 
 every person 
 u
 , we can find some person 
 u'
  whom 
 u
  admires; but this could be a 
 different person 
 u'
  in each case. We distinguish between 
 (27a)
  and 
 (27b)
  in terms 
 of the 
 scope
  of the quantifiers. In the first, 
 ∀
  has wider scope than 
 ∃
 , whereas in 
 (27b)
 , the scope ordering is reversed. So now we have two ways of representing 
 the meaning of 
 (26)
 , and they are both quite legitimate. In other words, we are 
 claiming that 
 (26)
  is 
 ambiguous
  with respect to quantifier scope, and the formulas 
 in 
 (27)
  give us a way to make the two readings explicit. However, we are not just 
 interested in associating two distinct rep-resentations with 
 (26)
 ; we also want to 
 show in detail how the two representations lead to different conditions for truth in 
 a model.
  
 In order to examine the ambiguity more closely, let’s fix our valuation as follows:
  
 >>> v2 = """""" 
  
 ... bruce => b 
  
 ... cyril => c 
  
 ... elspeth => e 
  
 ... julia => j 
  
 ... matthew => m 
  
 ... person => {b, e, j, m} 
  
 ... admire => {(j, b), (b, b), (m, e), (e, m), (c, a)} ... """""" 
  
 >>> val2 = nltk.parse_valuation(v2)
  
 The 
 admire
  relation can be visualized using the mapping diagram shown in 
 (28)
 .
  
 10.3  First-Order Logic | 381",NA
Model Building,"We have been assuming that we already had a model, and wanted to check the 
 truth of a sentence in the model. By contrast, model building tries to create a new 
 model, given some set of sentences. If it succeeds, then we know that the set is 
 consistent, since we have an existence proof of the model.
  
 We invoke the Mace4 model builder by creating an instance of 
 Mace()
  and calling its 
 build_model()
  method, in an analogous way to calling the Prover9 theorem prover. 
 One option is to treat our candidate set of sentences as assumptions, while leaving 
 the goal unspecified. The following interaction shows how both 
 [a, c1]
  and 
 [a, c2]
  are 
 con-sistent lists, since Mace succeeds in building a model for each of them, whereas 
 [c1, c2]
  is inconsistent.
  
 >>> a3 = lp.parse('exists x.(man(x) & walks(x))') >>> c1 = 
 lp.parse('mortal(socrates)') 
  
 >>> c2 = lp.parse('-mortal(socrates)') 
  
 >>> mb = nltk.Mace(5) 
  
 >>> print mb.build_model(None, [a3, c1]) 
  
 True 
  
 >>> print mb.build_model(None, [a3, c2]) 
  
 True 
  
 >>> print mb.build_model(None, [c1, c2]) 
  
 False
  
 We can also use the model builder as an adjunct to the theorem prover. Let’s 
 suppose we are trying to prove 
 A
 ⊢
 g
 , i.e., that 
 g
  is logically derivable from 
 assumptions 
 A = [a1, a2, ..., an]
 . We can feed this same input to Mace4, and the model 
 builder will try to find a counterexample, that is, to show that 
 g
  does 
 not
  follow 
 from 
 A
 . So, given this input, Mace4 will try to find a model for the assumptions 
 A
  
 together with the negation of 
 g
 , namely the list 
 A' = [a1, a2, ..., an, -g]
 . If 
 g
  fails to 
 follow from 
 S
 , then Mace4 may well return with a counterexample faster than 
 Prover9 concludes that it cannot find the required proof. Conversely, if 
 g
 is
  provable 
 from 
 S
 , Mace4 may take a long time unsuccessfully trying to find a countermodel, 
 and will eventually give up.
  
 Let’s consider a concrete scenario. Our assumptions are the list [
 There is a woman 
 that every man loves
 , 
 Adam is a man
 , 
 Eve is a woman
 ]. Our conclusion is 
 Adam loves 
 Eve
 . Can Mace4 find a model in which the premises are true but the conclusion is 
 false? In the following code, we use 
 MaceCommand()
 , which will let us inspect the 
 model that has been built.
  
 >>> a4 = lp.parse('exists y. (woman(y) & all x. (man(x) -> love(x,y)))') >>> a5 = 
 lp.parse('man(adam)') 
  
 >>> a6 = lp.parse('woman(eve)') 
  
 >>> g = lp.parse('love(adam,eve)') 
  
 >>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6]) 
  
 >>> mc.build_model() 
  
 True
  
 10.3  First-Order Logic | 383",NA
10.4  The Semantics of English Sentences,NA,NA
Compositional Semantics in Feature-Based ,NA,NA
Grammar,"At the beginning of the chapter we briefly illustrated a method of building semantic 
 representations on the basis of a syntactic parse, using the grammar framework 
 devel-oped in 
 Chapter 9
 . This time, rather than constructing an SQL query, we will 
 build a logical form. One of our guiding ideas for designing such grammars is the 
 Principle of Compositionality
 . (Also known as Frege’s Principle; see [Partee, 
 1995] for the for-mulation given.)
  
 Principle of Compositionality:
  the meaning of a whole is a function of the 
 meanings of the parts and of the way they are syntactically combined.
  
 We will assume that the semantically relevant parts of a complex expression are 
 given by a theory of syntactic analysis. Within this chapter, we will take it for 
 granted that expressions are parsed against a context-free grammar. However, this 
 is not entailed by the Principle of Compositionality.
  
 Our goal now is to integrate the construction of a semantic representation in a 
 manner that can be smoothly with the process of parsing. 
 (29)
  illustrates a first 
 approximation to the kind of analyses we would like to build.
  
 (29)
  
  
 In 
 (29)
 , the 
 SEM
  value at the root node shows a semantic representation for the 
 whole sentence, while the 
 SEM
  values at lower nodes show semantic 
 representations for con-stituents of the sentence. Since the values of 
 SEM
  have to be 
 treated in a special manner, they are distinguished from other feature values by 
 being enclosed in angle brackets.
  
 So far, so good, but how do we write grammar rules that will give us this kind of 
 result? Our approach will be similar to that adopted for the grammar 
 sql0.fcfg
  at the 
 start of this chapter, in that we will assign semantic representations to lexical 
 nodes, and then compose the semantic representations for each phrase from those 
 of its child nodes. However, in the present case we will use function application 
 rather than string con-catenation as the mode of composition. To be more specific, 
 suppose we have 
 NP
  and 
 VP
  constituents with appropriate values for their 
 SEM
  
 nodes. Then the 
 SEM
  value of an 
 S
  is handled by a rule like 
 (30)
 . (Observe that in the 
 case where the value of 
 SEM
  is a variable, we omit the angle brackets.)",NA
The λ-Calculus,"In 
 Section 1.3
 , we pointed out that mathematical set notation was a helpful method 
 of specifying properties 
 P
  of words that we wanted to select from a document. We 
 illus-trated this with 
 (31)
 , which we glossed as “the set of all 
 w
  such that 
 w
  is an 
 element of 
 V
  (the vocabulary) and 
 w
  has property 
 P
 ”.
  
 (31) {
 w
  | 
 w
 ∈
 V
  & 
 P
 (
 w
 )}
  
 It turns out to be extremely useful to add something to first-order logic that will 
 achieve the same effect. We do this with the 
 λ
 -operator
  (pronounced “lambda”). 
 The 
 λ
  coun-terpart to 
 (31)
  is 
 (32)
 . (Since we are not trying to do set theory here, we 
 just treat 
 V
  as a unary predicate.)
  
 (32)
  λ
 w
 . (
 V
 (
 w
 ) & P(
 w
 ))
  
  
 λ
  expressions were originally designed by Alonzo Church to 
 represent computable functions and to provide a foundation for 
 mathematics and logic. The theory in which 
 λ
  expressions are 
 studied is known as the
 λ
 -calculus. Note that the 
 λ
 -calculus is not 
 part of first-order logic—both
  
 can be used independently of the other.
  
 386 | Chapter 10:Analyzing the Meaning of Sentences",NA
Quantified NPs,"At the start of this section, we briefly described how to build a semantic 
 representation for 
 Cyril barks
 . You would be forgiven for thinking this was all too 
 easy—surely there is a bit more to building compositional semantics. What about 
 quantifiers, for instance? Right, this is a crucial issue. For example, we want 
 (42a)
  
 to be given the logical form in 
 (42b)
 . How can this be accomplished?
  
 (42) a. A dog barks.
  
 b.
  exists x.(dog(x) & bark(x))
  
 Let’s make the assumption that our 
 only
  operation for building complex semantic 
 rep-resentations is function application. Then our problem is this: how do we give a 
 se-mantic representation to the quantified 
 NP
 s 
 a dog
  so that it can be combined 
 with 
 bark
  to give the result in 
 (42b)
 ? As a first step, let’s make the subject’s 
 SEM
  
 value act as the function expression rather than the argument. (This is sometimes 
 called 
 type-raising
 .) Now we are looking for a way of instantiating 
 ?np
  so that 
 [SEM=<?np(\x.bark(x))>]
  is equivalent to 
 [SEM=<exists x.(dog(x) & bark(x))>]
 . Doesn’t 
 this look a bit reminiscent of carrying out 
 β
 -reduction in the 
 λ
 -calculus? In other 
 words, we want a 
 λ
 -term 
 M
  to replace 
 ?np
  so that applying 
 M
  to 
 \x.bark(x)
  yields 
 (42b)
 . To do this, we replace the occurrence of 
 \x.bark(x)
  in 
 (42b)
  by a predicate 
 variable 
 P
 , and bind the variable with 
 λ
 , as shown in 
 (43)
 .
  
 (43)
  \P.exists x.(dog(x) & P(x))
  
 We have used a different style of variable in 
 (43)
 —that is, 
 'P'
  rather than 
 'x'
  or 
 'y'
 —
 to signal that we are abstracting over a different kind of object—not an individual, 
 but a function expression of type 
 〈
 e
 , 
 t
 〉
 . So the type of 
 (43)
  as a whole is 
 〈〈
 e
 , 
 t
 〉
 , 
 t
 〉
 . We 
 will take this to be the type of 
 NP
 s in general. To illustrate further, a universally 
 quantified 
 NP
  will look like 
 (44)
 .
  
 390 | Chapter 10:Analyzing the Meaning of Sentences",NA
Transitive Verbs,"Our next challenge is to deal with sentences containing transitive verbs, such as 
 (46)
 .
  
 (46) Angus chases a dog.
  
 The output semantics that we want to build is 
 exists x.(dog(x) & chase(angus, x))
 . Let’s 
 look at how we can use 
 λ
 -abstraction to get this result. A significant constraint on 
 possible solutions is to require that the semantic representation of 
 a dog
  be inde-
 pendent of whether the 
 NP
  acts as subject or object of the sentence. In other words, 
 we want to get the formula just shown as our output while sticking to 
 (43)
  as the 
 NP
  se-mantics. A second constraint is that 
 VP
 s should have a uniform type of 
 interpretation, regardless of whether they consist of just an intransitive verb or a 
 transitive verb plus object. More specifically, we stipulate that 
 VP
 s are always of 
 type 
 〈
 e
 , 
 t
 〉
 . Given these constraints, here’s a semantic representation for 
 chases a 
 dog
  that does the trick.
  
 (47)
  \y.exists x.(dog(x) & chase(y, x))
  
 Think of 
 (47)
  as the property of being a 
 y
  such that for some dog 
 x
 , 
 y
  chases 
 x
 ; or 
 more colloquially, being a 
 y
  who chases a dog. Our task now resolves to designing a 
 semantic representation for 
 chases
  which can combine with 
 (43)
  so as to allow 
 (47)
  
 to be derived.
  
 Let’s carry out the inverse of 
 β
 -reduction on 
 (47)
 , giving rise to 
 (48)
 .
  
 (48)
  \P.exists x.(dog(x) & P(x))(\z.chase(y, z))
  
 (48)
  may be slightly hard to read at first; you need to see that it involves applying 
 the quantified 
 NP
  representation from 
 (43)
  to 
 \z.chase(y,z)
 . 
 (48)
  is equivalent via 
 β
 -
 reduction to 
 exists x.(dog(x) & chase(y, x))
 .
  
 Now let’s replace the function expression in 
 (48)
  by a variable 
 X
  of the same type as 
 an 
 NP
 , that is, of type 
 〈〈
 e
 , 
 t
 〉
 , 
 t
 〉
 .
  
 (49)
  X(\z.chase(y, z))",NA
Quantifier Ambiguity Revisited,"One important limitation of the methods described earlier is that they do not deal 
 with scope ambiguity. Our translation method is syntax-driven, in the sense that 
 the se-mantic representation is closely coupled with the syntactic analysis, and the 
 scope of the quantifiers in the semantics therefore reflects the relative scope of the 
 corresponding 
 NP
 s in the syntactic parse tree. Consequently, a sentence like 
 (26)
 , 
 repeated here, will always be translated as 
 (53a)
 , not 
 (53b)
 .
  
 (52) Every girl chases a dog.
  
 (53) a.
  all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))
  
 b.
  exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))
  
 There are numerous approaches to dealing with scope ambiguity, and we will look 
 very briefly at one of the simplest. To start with, let’s briefly consider the structure 
 of scoped formulas. 
 Figure 10-3
  depicts the way in which the two readings of 
 (52)
  
 differ.
  
  
 Figure 10-3. Quantifier scopings.
  
 Let’s consider the lefthand structure first. At the top, we have the quantifier corre-
 sponding to 
 every girl
 . The 
 φ
  can be thought of as a placeholder for whatever is 
 inside the scope of the quantifier. Moving downward, we see that we can plug in the 
 quantifier corresponding to 
 a dog
  as an instantiation of 
 φ
 . This gives a new 
 placeholder 
 ψ
 , rep-resenting the scope of 
 a dog
 , and into this we can plug the “core” 
 of the semantics, namely the open sentence corresponding to 
 xchasesy
 . The 
 structure on the righthand side is identical, except we have swapped round the 
 order of the two quantifiers.
  
 In the method known as 
 Cooper storage
 , a semantic representation is no longer an 
 expression of first-order logic, but instead a pair consisting of a “core” semantic 
 rep-resentation plus a list of 
 binding operators
 . For the moment, think of a 
 binding op-erator as being identical to the semantic representation of a quantified 
 NP
  such as 
 (44)
  or",NA
10.5  Discourse Semantics,"A 
 discourse
  is a sequence of sentences. Very often, the interpretation of a sentence 
 in a discourse depends on what preceded it. A clear example of this comes from 
 anaphoric pronouns, such as 
 he
 , 
 she
 , and 
 it
 . Given a discourse such as 
 Angus used to 
 have a dog. But he recently disappeared.
 , you will probably interpret 
 he
  as referring 
 to Angus’s dog. However, in 
 Angus used to have a dog. He took him for walks in New 
 Town.
 , you are more likely to interpret 
 he
  as referring to Angus himself.",NA
Discourse Representation Theory,"The standard approach to quantification in first-order logic is limited to single 
 senten-ces. Yet there seem to be examples where the scope of a quantifier can 
 extend over two or more sentences. We saw one earlier, and here’s a second 
 example, together with a translation.
  
 (54) a. Angus owns a dog. It bit Irene.
  
 b.
  
 ∃
 x.
 (
 dog
 (
 x
 ) & 
 own
 (
 Angus
 , 
 x
 ) & 
 bite
 (
 x
 , 
 Irene
 ))
  
 That is, the 
 NP
 a dog
  acts like a quantifier which binds the 
 it
  in the second sentence. 
 Discourse Representation Theory (DRT) was developed with the specific goal of 
 pro-viding a means for handling this and other semantic phenomena which seem to 
 be characteristic of discourse. A 
 discourse representation structure
  (DRS) 
 presents the meaning of discourse in terms of a list of discourse referents and a list 
 of conditions. The 
 discourse referents
  are the things under discussion in the 
 discourse, and they correspond to the individual variables of first-order logic. The 
 DRS conditions
  apply to those discourse referents, and correspond to atomic open 
 formulas of first-order logic. 
 Figure 10-4
  illustrates how a DRS for the first sentence 
 in 
 (54a)
  is augmented to become a DRS for both sentences.
  
 When the second sentence of 
 (54a)
  is processed, it is interpreted in the context of 
 what is already present in the lefthand side of 
 Figure 10-4
 . The pronoun 
 it
  triggers 
 the addi-tion of a new discourse referent, say, 
 u
 , and we need to find an 
 anaphoric 
 antecedent
  for it—that is, we want to work out what 
 it
  refers to. In DRT, the task of 
 finding the antecedent for an anaphoric pronoun involves linking it to a discourse 
 ref-erent already within the current DRS, and 
 y
  is the obvious choice. (We will say 
 more about anaphora resolution shortly.) This processing step gives rise to a new 
 condition 
 u
  = 
 y
 . The remaining content contributed by the second sentence is also 
 merged with the content of the first, and this is shown on the righthand side of 
 Figure 10-4
 .
  
 Figure 10-4
  illustrates how a DRS can represent more than just a single sentence. In 
 this case, it is a two-sentence discourse, but in principle a single DRS could 
 correspond to the interpretation of a whole text. We can inquire into the truth 
 conditions of the righthand DRS in 
 Figure 10-4
 . Informally, it is true in some 
 situation 
 s
  if there are entities 
 a
 , 
 c
 , and 
 i
  in 
 s
  corresponding to the discourse 
 referents in the DRS such that",NA
Discourse Processing,"When we interpret a sentence, we use a rich context for interpretation, determined 
 in part by the preceding context and in part by our background assumptions. DRT 
 pro-vides a theory of how the meaning of a sentence is integrated into a 
 representation of the prior discourse, but two things have been glaringly absent 
 from the processing approach just discussed. First, there has been no attempt to 
 incorporate any kind of inference; and second, we have only processed individual 
 sentences. These omissions are redressed by the module 
 nltk.inference.discourse
 .
  
 Whereas a discourse is a sequence 
 s
 1
 , ... 
 s
 n
  of sentences, a 
 discourse thread
  is a 
 sequence 
 s
 1
 -r
 i
 , ... 
 s
 n
 -r
 j
  of readings, one for each sentence in the discourse. The 
 module processes sentences incrementally, keeping track of all possible threads 
 when there is ambiguity.
  
 For simplicity, the following example ignores scope ambiguity:
  
 >>> dt = nltk.DiscourseTester(['A student dances', 'Every student is a person']) >>> 
 dt.readings() 
  
 s0 readings: s0-r0: exists x.(student(x) & dance(x)) 
  
 s1 readings: s1-r0: all x.(student(x) -> person(x))
  
 When a new sentence is added to the current discourse, setting the parameter 
 consistchk=True
  causes consistency to be checked by invoking the model checker for 
 each thread, i.e., each sequence of admissible readings. In this case, the user has the 
 option of retracting the sentence in question.
  
 400 | Chapter 10:Analyzing the Meaning of Sentences",NA
10.6  Summary,"• First-order logic is a suitable language for representing natural language 
 meaning in a computational setting since it is flexible enough to represent 
 many useful as-pects of natural meaning, and there are efficient theorem 
 provers for reasoning with first-order logic. (Equally, there are a variety of 
 phenomena in natural language semantics which are believed to require more 
 powerful logical mechanisms.)
  
 • As well as translating natural language sentences into first-order logic, we can 
 state the truth conditions of these sentences by examining models of first-order 
 formu-las.
  
 • In order to build meaning representations compositionally, we supplement first-
  
 order logic with the 
 λ
 -calculus.
  
 •
  β
 -reduction in the 
 λ
 -calculus corresponds semantically to application of a 
 function to an argument. Syntactically, it involves replacing a variable bound by 
 λ
  in the function expression with the expression that provides the argument in 
 the function application.
  
 • A key part of constructing a model lies in building a valuation which assigns in-
 terpretations to non-logical constants. These are interpreted as either 
 n
 -ary 
 predi-cates or as individual constants.
  
 • An open expression is an expression containing one or more free variables. Open 
 expressions receive an interpretation only when their free variables receive 
 values from a variable assignment.
  
 • Quantifiers are interpreted by constructing, for a formula 
 φ
 [
 x
 ] open in variable 
 x
 , 
 the set of individuals which make 
 φ
 [
 x
 ] true when an assignment 
 g
  assigns them 
 as the value of 
 x
 . The quantifier then places constraints on that set.
  
 402 | Chapter 10:Analyzing the Meaning of Sentences",NA
10.7  Further Reading,"Consult 
 http://www.nltk.org/
  for further materials on this chapter and on how to 
 install the Prover9 theorem prover and Mace4 model builder. General information 
 about these two inference tools is given by (McCune, 2008).
  
 For more examples of semantic analysis with NLTK, please see the semantics and 
 logic HOWTOs at 
 http://www.nltk.org/howto
 . Note that there are implementations 
 of two other approaches to scope ambiguity, namely 
 Hole semantics
  as described 
 in (Black-burn & Bos, 2005), and 
 Glue semantics
 , as described in (Dalrymple et al., 
 1999).
  
 There are many phenomena in natural language semantics that have not been 
 touched on in this chapter, most notably:
  
 1. Events, tense, and aspect
  
 2. Semantic roles
  
 3. Generalized quantifiers, such as 
 most
  
 4. Intensional constructions involving, for example, verbs such as 
 may
  and 
 believe
  
 While (1) and (2) can be dealt with using first-order logic, (3) and (4) require 
 different logics. These issues are covered by many of the references in the following 
 readings.
  
 A comprehensive overview of results and techniques in building natural language 
 front-ends to databases can be found in (Androutsopoulos, Ritchie & Thanisch, 
 1995).
  
 Any introductory book to modern logic will present propositional and first-order 
 logic. (Hodges, 1977) is highly recommended as an entertaining and insightful text 
 with many illustrations from natural language.
  
 For a wide-ranging, two-volume textbook on logic that also presents contemporary 
 material on the formal semantics of natural language, including Montague 
 Grammar and intensional logic, see (Gamut, 1991a, 1991b). (Kamp & Reyle, 1993) 
 provides the",NA
10.8  Exercises,"1.
  ○ 
 Translate the following sentences into propositional logic and verify that they 
 parse with 
 LogicParser
 . Provide a key that shows how the propositional 
 variables in your translation correspond to expressions of English.
  
 a. If Angus sings, it is not the case that Bertie sulks.
  
 b. Cyril runs and barks.
  
 c. It will snow if it doesn’t rain.
  
 d. It’s not the case that Irene will be happy if Olive or Tofu comes.
  
 e. Pat didn’t cough or sneeze.
  
 f. If you don’t come if I call, I won’t come if you call.
  
 2.
  ○ 
 Translate the following sentences into predicate-argument formulas of first-
 order logic.
  
 a. Angus likes Cyril and Irene hates Cyril.
  
 b. Tofu is taller than Bertie.
  
 c. Bruce loves himself and Pat does too.
  
 d. Cyril saw Bertie, but Angus didn’t.
  
 e. Cyril is a four-legged friend.
  
 f. Tofu and Olive are near each other.
  
 3.
  ○ 
 Translate the following sentences into quantified formulas of first-order logic.
  
 a. Angus likes someone and someone likes Julia.
  
 404 | Chapter 10:Analyzing the Meaning of Sentences",NA
CHAPTER 11,NA,NA
Managing Linguistic Data,"Structured collections of annotated linguistic data are essential in most areas of 
 NLP; however, we still face many obstacles in using them. The goal of this chapter is 
 to answer the following questions:
  
 1. How do we design a new language resource and ensure that its coverage, 
 balance, and documentation support a wide range of uses?
  
 2. When existing data is in the wrong format for some analysis tool, how can we 
 convert it to a suitable format?
  
 3. What is a good way to document the existence of a resource we have created so 
 that others can easily find it?
  
 Along the way, we will study the design of existing corpora, the typical workflow 
 for creating a corpus, and the life cycle of a corpus. As in other chapters, there will 
 be many examples drawn from practical experience managing linguistic data, 
 including data that has been collected in the course of linguistic fieldwork, 
 laboratory work, and web crawling.",NA
11.1  Corpus Structure: A Case Study,"The TIMIT Corpus was the first annotated speech database to be widely 
 distributed, and it has an especially clear organization. TIMIT was developed by a 
 consortium in-cluding Texas Instruments and MIT, from which it derives its name. 
 It was designed to provide data for the acquisition of acoustic-phonetic knowledge 
 and to support the development and evaluation of automatic speech recognition 
 systems.",NA
The Structure of TIMIT,"Like the Brown Corpus, which displays a balanced selection of text genres and 
 sources, TIMIT includes a balanced selection of dialects, speakers, and materials. 
 For each of eight dialect regions, 50 male and female speakers having a range of 
 ages and educa-tional backgrounds each read 10 carefully chosen sentences. Two 
 sentences, read by all speakers, were designed to bring out dialect variation:
  
 407",NA
Notable Design Features,"TIMIT illustrates several key features of corpus design. First, the corpus contains 
 two layers of annotation, at the phonetic and orthographic levels. In general, a text 
 or speech corpus may be annotated at many different linguistic levels, including 
 morphological, syntactic, and discourse levels. Moreover, even at a given level there 
 may be different labeling schemes or even disagreement among annotators, such 
 that we want to rep-resent multiple versions. A second property of TIMIT is its 
 balance across multiple dimensions of variation, for coverage of dialect regions and 
 diphones. The inclusion of speaker demographics brings in many more 
 independent variables that may help to account for variation in the data, and which 
 facilitate later uses of the corpus for pur-poses that were not envisaged when the 
 corpus was created, such as sociolinguistics. A third property is that there is a sharp 
 division between the original linguistic event captured as an audio recording and 
 the annotations of that event. The same holds true of text corpora, in the sense that 
 the original text usually has an external source, and is considered to be an 
 immutable artifact. Any transformations of that artifact which involve human 
 judgment—even something as simple as tokenization—are subject to later 
 revision; thus it is important to retain the source material in a form that is as close 
 to the original as possible.
  
 A fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per 
 sentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These 
 are organized into a tree structure, shown schematically in 
 Figure 11-2
 . At the top 
 level
  
 11.1  Corpus Structure: A Case Study | 409",NA
Fundamental Data Types,"Despite its complexity, the TIMIT Corpus contains only two fundamental data types, 
 namely lexicons and texts. As we saw in Chapter 2, most lexical resources can be 
 rep-resented using a record structure, i.e., a key plus one or more fields, as shown 
 in 
 Figure 11-3
 . A lexical resource could be a conventional dictionary or comparative 
 wordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a 
 phrase rather than a single word. A thesaurus also consists of record-structured 
 data, where we look up entries via non-key fields that correspond to topics. We can 
 also construct special tabulations (known as paradigms) to illustrate contrasts and 
 systematic varia-tion, as shown in 
 Figure 11-3
  for three verbs. TIMIT’s speaker 
 table is also a kind of lexicon.
  
  
 Figure 11-3. Basic linguistic data types—lexicons and texts: Amid their diversity, lexicons have a 
 record structure, whereas annotated texts have a temporal organization.
  
 At the most abstract level, a text is a representation of a real or fictional speech 
 event, and the time-course of that event carries over into the text itself. A text could 
 be a small unit, such as a word or sentence, or a complete narrative or dialogue. It 
 may come with annotations such as part-of-speech tags, morphological analysis, 
 discourse structure, and so forth. As we saw in the IOB tagging technique (Chapter 
 7), it is possible to represent higher-level constituents using tags on individual 
 words. Thus the abstraction of text shown in 
 Figure 11-3
  is sufficient.
  
 11.1  Corpus Structure: A Case Study | 411",NA
11.2  The Life Cycle of a Corpus,"Corpora are not born fully formed, but involve careful preparation and input from 
 many people over an extended period. Raw data needs to be collected, cleaned up, 
 documented, and stored in a systematic structure. Various layers of annotation 
 might be applied, some requiring specialized knowledge of the morphology or 
 syntax of the language. Success at this stage depends on creating an efficient 
 workflow involving appropriate tools and format converters. Quality control 
 procedures can be put in place to find inconsistencies in the annotations, and to 
 ensure the highest possible level of inter-annotator agreement. Because of the scale 
 and complexity of the task, large cor-pora may take years to prepare, and involve 
 tens or hundreds of person-years of effort.
  
 In this section, we briefly review the various stages in the life cycle of a corpus.",NA
Three Corpus Creation Scenarios,"In one type of corpus, the design unfolds over in the course of the creator’s 
 explorations. This is the pattern typical of traditional “field linguistics,” in which 
 material from elic-itation sessions is analyzed as it is gathered, with tomorrow’s 
 elicitation often based on questions that arise in analyzing today’s. The resulting 
 corpus is then used during sub-sequent years of research, and may serve as an 
 archival resource indefinitely. Comput-erization is an obvious boon to work of this 
 type, as exemplified by the popular program Shoebox, now over two decades old 
 and re-released as Toolbox (see 
 Section 2.4
 ). Other software tools, even simple 
 word processors and spreadsheets, are routinely used to acquire the data. In the 
 next section, we will look at how to extract data from these sources.
  
 Another corpus creation scenario is typical of experimental research where a body 
 of carefully designed material is collected from a range of human subjects, then 
 analyzed to evaluate a hypothesis or develop a technology. It has become common 
 for such databases to be shared and reused within a laboratory or company, and 
 often to be published more widely. Corpora of this type are the basis of the 
 “common task” method of research management, which over the past two decades 
 has become the norm in government-funded research programs in language 
 technology. We have already en-countered many such corpora in the earlier 
 chapters; we will see how to write Python",NA
Quality Control,"Good tools for automatic and manual preparation of data are essential. However, 
 the creation of a high-quality corpus depends just as much on such mundane things 
 as documentation, training, and workflow. Annotation guidelines define the task 
 and document the markup conventions. They may be regularly updated to cover 
 difficult cases, along with new rules that are devised to achieve more consistent 
 annotations. Annotators need to be trained in the procedures, including methods 
 for resolving cases not covered in the guidelines. A workflow needs to be 
 established, possibly with sup-porting software, to keep track of which files have 
 been initialized, annotated, validated, manually checked, and so on. There may be 
 multiple layers of annotation, provided by different specialists. Cases of uncertainty 
 or disagreement may require adjudication.
  
 Large annotation tasks require multiple annotators, which raises the problem of 
 achieving consistency. How consistently can a group of annotators perform? We 
 can easily measure consistency by having a portion of the source material 
 independently annotated by two people. This may reveal shortcomings in the 
 guidelines or differing abilities with the annotation task. In cases where quality is 
 paramount, the entire corpus can be annotated twice, and any inconsistencies 
 adjudicated by an expert.
  
 It is considered best practice to report the inter-annotator agreement that was 
 achieved for a corpus (e.g., by double-annotating 10% of the corpus). This score 
 serves as a helpful upper bound on the expected performance of any automatic 
 system that is trained on this corpus.
  
  
 Caution!
  
 Care should be exercised when interpreting an inter-annotator agree-
 ment score, since annotation tasks vary greatly in their difficulty. For
  
 example, 90% agreement would be a terrible score for part-of-
 speech tagging, but an exceptional score for semantic role labeling.
  
 The 
 Kappa
  coefficient 
 κ
  measures agreement between two people making category 
 judgments, correcting for expected chance agreement. For example, suppose an 
 item is to be annotated, and four coding options are equally likely. In this case, two 
 people coding randomly would be expected to agree 25% of the time. Thus, an 
 agreement of",NA
Curation Versus Evolution,"As large corpora are published, researchers are increasingly likely to base their 
 inves-tigations on balanced, focused subsets that were derived from corpora 
 produced for
  
 414 | Chapter 11:Managing Linguistic Data",NA
11.3  Acquiring Data,NA,NA
Obtaining Data from the Web,"The Web is a rich source of data for language analysis purposes. We have already 
 discussed methods for accessing individual files, RSS feeds, and search engine 
 results (see 
 Section 3.1
 ). However, in some cases we want to obtain large quantities 
 of web text.
  
 The simplest approach is to obtain a published corpus of web text. The ACL Special 
 Interest Group on Web as Corpus (SIGWAC) maintains a list of resources at 
 http:// 
 www.sigwac.org.uk/
 . The advantage of using a well-defined web corpus is that they 
 are documented, stable, and permit reproducible experimentation.
  
 If the desired content is localized to a particular website, there are many utilities 
 for capturing all the accessible contents of a site, such as GNU Wget 
 (
 http://www.gnu.org/ software/wget/
 ). For maximal flexibility and control, a web 
 crawler can be used, such as Heritrix (
 http://crawler.archive.org/
 ). Crawlers permit 
 fine-grained control over where to look, which links to follow, and how to organize 
 the results. For example, if we want to compile a bilingual text collection having 
 corresponding pairs of documents in each language, the crawler needs to detect the 
 structure of the site in order to extract the correspondence between the 
 documents, and it needs to organize the downloaded pages in such a way that the 
 correspondence is captured. It might be tempting to write your own web crawler, 
 but there are dozens of pitfalls having to do with detecting MIME types, converting 
 relative to absolute URLs, avoiding getting trapped in cyclic link structures, dealing 
 with network latencies, avoiding overloading the site or being banned from 
 accessing the site, and so on.",NA
Obtaining Data from Word Processor Files,"Word processing software is often used in the manual preparation of texts and 
 lexicons in projects that have limited computational infrastructure. Such projects 
 often provide templates for data entry, though the word processing software does 
 not ensure that the data is correctly structured. For example, each text may be 
 required to have a title and date. Similarly, each lexical entry may have certain 
 obligatory fields. As the data grows",NA
Obtaining Data from Spreadsheets and Databases,"Spreadsheets are often used for acquiring wordlists or paradigms. For example, a 
 com-parative wordlist may be created using a spreadsheet, with a row for each 
 cognate set and a column for each language (see 
 nltk.corpus.swadesh
  and 
 www.rosettapro ject.org
 ). Most spreadsheet software can export their data in CSV 
 format. As we will see later, it is easy for Python programs to access these using the 
 csv
  module.
  
 Sometimes lexicons are stored in a full-fledged relational database. When properly 
 normalized, these databases can ensure the validity of the data. For example, we 
 can require that all parts-of-speech come from a specified vocabulary by declaring 
 that the part-of-speech field is an 
 enumerated type
  or a foreign key that references a 
 separate part-of-speech table. However, the relational model requires the structure 
 of the data (the schema) be declared in advance, and this runs counter to the 
 dominant approach to structuring linguistic data, which is highly exploratory. 
 Fields which were assumed to be obligatory and unique often turn out to be 
 optional and repeatable. A relational database can accommodate this when it is 
 fully known in advance; however, if it is not, or if just about every property turns 
 out to be optional or repeatable, the relational approach is unworkable.
  
 Nevertheless, when our goal is simply to extract the contents from a database, it is 
 enough to dump out the tables (or SQL query results) in CSV format and load them 
 into our program. Our program might perform a linguistically motivated query that 
 cannot easily be expressed in SQL, e.g., 
 select all words that appear in example 
 sentences for which no dictionary entry is provided
 . For this task, we would need to 
 extract enough information from a record for it to be uniquely identified, along 
 with the headwords and example sentences. Let’s suppose this information was 
 now available in a CSV file 
 dict.csv
 :
  
 418 | Chapter 11:Managing Linguistic Data",NA
Converting Data Formats,"Annotated linguistic data rarely arrives in the most convenient format, and it is 
 often necessary to perform various kinds of format conversion. Converting 
 between character encodings has already been discussed (see 
 Section 3.3
 ). Here we 
 focus on the structure of the data.
  
 In the simplest case, the input and output formats are isomorphic. For instance, we 
 might be converting lexical data from Toolbox format to XML, and it is 
 straightforward to transliterate the entries one at a time (
 Section 11.4
 ). The 
 structure of the data is reflected in the structure of the required program: a 
 for
  loop 
 whose body takes care of a single entry.
  
 In another common case, the output is a digested form of the input, such as an 
 inverted file index. Here it is necessary to build an index structure in memory (see 
 Example 4.8), then write it to a file in the desired format. The following example 
 constructs an index that maps the words of a dictionary definition to the 
 corresponding lexeme  for each
  
 lexical entry 
   
 , having tokenized the definition text 
 , and discarded short words 
 .
  
 Once the index has been constructed, we open a file and then iterate over the index 
 entries, to write out the lines in the required format .
  
 >>> idx = nltk.Index((defn_word, lexeme) 
  
 ...                  for (lexeme, defn) in pairs 
  
 ...                  for defn_word in nltk.word_tokenize(defn) 
  
 ...                  if len(defn_word) > 3) 
  
 >>> idx_file = open(""dict.idx"", ""w"") 
  
 >>> for word in sorted(idx): 
  
 ...     idx_words = ', '.join(idx[word]) 
  
 ...     idx_line = ""%s: %s\n"" % (word, idx_words) 
  
 ...     idx_file.write(idx_line) 
  
 >>> idx_file.close()
  
 The resulting file 
 dict.idx
  contains the following lines. (With a larger dictionary, we 
 would expect to find multiple lexemes listed for each index entry.)
  
 11.3  Acquiring Data | 419",NA
Deciding Which Layers of Annotation to Include,"Published corpora vary greatly in the richness of the information they contain. At a 
 minimum, a corpus will typically contain at least a sequence of sound or 
 orthographic symbols. At the other end of the spectrum, a corpus could contain a 
 large amount of information about the syntactic structure, morphology, prosody, 
 and semantic content of every sentence, plus annotation of discourse relations or 
 dialogue acts. These extra layers of annotation may be just what someone needs for 
 performing a particular data analysis task. For example, it may be much easier to 
 find a given linguistic pattern if we can search for specific syntactic structures; and 
 it may be easier to categorize a linguistic pattern if every word has been tagged 
 with its sense. Here are some commonly provided annotation layers:
  
 Word tokenization 
  
 The orthographic form of text does not unambiguously identify its tokens. A to-
 kenized and normalized version, in addition to the conventional orthographic 
 ver-sion, may be a very convenient resource.
  
 Sentence segmentation 
  
 As we saw in 
 Chapter 3
 , sentence segmentation can be more difficult than it 
 seems. Some corpora therefore use explicit annotations to mark sentence 
 segmentation.
  
 420 | Chapter 11:Managing Linguistic Data",NA
Standards and Tools,"For a corpus to be widely useful, it needs to be available in a widely supported 
 format. However, the cutting edge of NLP research depends on new kinds of 
 annotations, which by definition are not widely supported. In general, adequate 
 tools for creation, publication, and use of linguistic data are not widely available. 
 Most projects must develop their own set of tools for internal use, which is no help 
 to others who lack the necessary resources. Furthermore, we do not have adequate, 
 generally accepted stand-ards for expressing the structure and content of corpora. 
 Without such standards, gen-eral-purpose tools are impossible—though at the 
 same time, without available tools, adequate standards are unlikely to be 
 developed, used, and accepted.
  
 One response to this situation has been to forge ahead with developing a generic 
 format that is sufficiently expressive to capture a wide variety of annotation types 
 (see 
 Sec-tion 11.8
  for examples). The challenge for NLP is to write programs that 
 cope with the generality of such formats. For example, if the programming task 
 involves tree data, and the file format permits arbitrary directed graphs, then input 
 data must be validated to check for tree properties such as rootedness, 
 connectedness, and acyclicity. If the input files contain other layers of annotation, 
 the program would need to know how to ignore them when the data was loaded,",NA
Special Considerations When Working with ,NA,NA
Endangered Languages,"The importance of language to science and the arts is matched in significance by the 
 cultural treasure embodied in language. Each of the world’s ~7,000 human 
 languages",NA
11.4  Working with XML,"The Extensible Markup Language (XML) provides a framework for designing 
 domain-specific markup languages. It is sometimes used for representing 
 annotated text and for lexical resources. Unlike HTML with its predefined tags, XML 
 permits us to make up our own tags. Unlike a database, XML permits us to create 
 data without first spec-ifying its structure, and it permits us to have optional and 
 repeatable elements. In this section, we briefly review some features of XML that 
 are relevant for representing lin-guistic data, and show how to access data stored in 
 XML files using Python programs.",NA
Using XML for Linguistic Structures,"Thanks to its flexibility and extensibility, XML is a natural choice for representing 
 linguistic structures. Here’s an example of a simple lexical entry.
  
 (2)
  <entry>
  
  
  <headword>whale</headword>
  
  
  <pos>noun</pos>
  
  
  <gloss>any of the larger cetacean mammals having a streamlined
  
  
  body and breathing through a blowhole on the head</gloss> </entry>
  
 It consists of a series of XML tags enclosed in angle brackets. Each opening tag, such 
 as 
 <gloss>
 , is matched with a closing tag, 
 </gloss>
 ; together they constitute an 
 XML 
 element
 . The preceding example has been laid out nicely using whitespace, but it 
 could equally have been put on a single long line. Our approach to processing XML 
 will usually not be sensitive to whitespace. In order for XML to be 
 well formed
 , all 
 opening tags must have corresponding closing tags, at the same level of nesting 
 (i.e., the XML document must be a well-formed tree).
  
 XML permits us to repeat elements, e.g., to add another gloss field, as we see next. 
 We will use different whitespace to underscore the point that layout does not 
 matter.
  
 (3)
  <entry><headword>whale</headword><pos>noun</pos><gloss>any of the 
 larger cetacean mammals having a streamlined body and breathing through 
 a blowhole on the head</gloss><gloss>a very large person; impressive in 
 size or qualities</gloss></entry>
  
 A further step might be to link our lexicon to some external resource, such as 
 WordNet, using external identifiers. In 
 (4)
  we group the gloss and a synset 
 identifier inside a new element, which we have called “sense.”
  
 (4)
  <entry>
  
  <headword>whale</headword>
  
  <pos>noun</pos>
  
  <sense>
  
  
  <gloss>any of the larger cetacean mammals having a streamlined
  
  
  body and breathing through a blowhole on the head</gloss>
  
  
 <synset>whale.n.02</synset>
  
 11.4  Working with XML | 425",NA
The Role of XML,"We can use XML to represent many kinds of linguistic information. However, the 
 flexibility comes at a price. Each time we introduce a complication, such as by 
 permit-ting an element to be optional or repeated, we make more work for any 
 program that accesses the data. We also make it more difficult to check the validity 
 of the data, or to interrogate the data using one of the XML query languages.
  
 Thus, using XML to represent linguistic structures does not magically solve the data 
 modeling problem. We still have to work out how to structure the data, then define 
 that structure with a schema, and then write programs to read and write the format 
 and convert it to other formats. Similarly, we still need to follow some standard 
 prin-ciples concerning data normalization. It is wise to avoid making duplicate 
 copies of the same information, so that we don’t end up with inconsistent data 
 when only one copy is changed. For example, a cross-reference that was 
 represented as 
 <xref>headword</ xref>
  would duplicate the storage of the headword 
 of some other lexical entry, and the link would break if the copy of the string at the 
 other location was modified. Existential dependencies between information types 
 need to be modeled, so that we can’t create elements without a home. For example, 
 if sense definitions cannot exist independently
  
 426 | Chapter 11:Managing Linguistic Data",NA
The ElementTree Interface,"Python’s 
 ElementTree
  module provides a convenient way to access data stored in 
 XML files. 
 ElementTree
  is part of Python’s standard library (since Python 2.5), and is 
 also provided as part of NLTK in case you are using Python 2.4.
  
 We will illustrate the use of 
 ElementTree
  using a collection of Shakespeare plays that 
 have been formatted using XML. Let’s load the XML file and inspect the raw data, 
 first at the top of the file 
 , where we see some XML headers and the name of a 
 schema called 
 play.dtd
 , followed by the 
 root element
 PLAY
 . We pick it up again at 
 the start of Act 1 . (Some blank lines have been omitted from the output.)
  
 >>> merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml') >>> raw 
 = open(merchant_file).read() 
  
 >>> print raw[0:168] 
  
 <?xml version=""1.0""?> 
  
 <?xml-stylesheet type=""text/css"" href=""shakes.css""?> <!-- 
 <!DOCTYPE PLAY SYSTEM ""play.dtd""> --> 
  
 <PLAY> 
  
 <TITLE>The Merchant of Venice</TITLE> 
  
 >>> print raw[1850:2075] 
  
 <TITLE>ACT I</TITLE> 
  
 <SCENE><TITLE>SCENE I.  Venice. A street.</TITLE> 
  
 <STAGEDIR>Enter ANTONIO, SALARINO, and 
 SALANIO</STAGEDIR> <SPEECH> 
  
 <SPEAKER>ANTONIO</SPEAKER> 
  
 <LINE>In sooth, I know not why I am so sad:</LINE>
  
 We have just accessed the XML data as a string. As we can see, the string at the start 
 of Act 1 contains XML tags for title, scene, stage directions, and so forth.
  
 The next step is to process the file contents as structured XML data, using 
 Element 
 Tree
 . We are processing a file (a multiline string) and building a tree, so it’s not sur-
 prising that the method name is 
 parse
  . The variable 
 merchant
  contains an XML ele-
 ment 
 PLAY
 . This element has internal structure; we can use an index to get its first 
 child, a 
 TITLE
  element 
  
 . We can also see the text content of this element, the title of the play . To get a list of 
 all the child elements, we use the 
 getchildren()
  method .
  
 >>> from nltk.etree.ElementTree import ElementTree >>> 
 merchant = ElementTree().parse(merchant_file) 
  
 >>> merchant
  
 11.4  Working with XML | 427",NA
Using ElementTree for Accessing Toolbox Data,"In 
 Section 2.4
 , we saw a simple interface for accessing Toolbox data, a popular and 
 well-established format used by linguists for managing data. In this section, we 
 discuss a variety of techniques for manipulating Toolbox data in ways that are not 
 supported by the Toolbox software. The methods we discuss could be applied to 
 other record-structured data, regardless of the actual file format.
  
 We can use the 
 toolbox.xml()
  method to access a Toolbox file and load it into an 
 ElementTree
  object. This file contains a lexicon for the Rotokas language of Papua 
 New Guinea.
  
 11.4  Working with XML | 429",NA
Formatting Entries,"We can use the same idea we saw in the previous section to generate HTML tables 
 instead of plain text. This would be useful for publishing a Toolbox lexicon on the 
 Web. It produces HTML elements 
 <table>
 , 
 <tr>
  (table row), and 
 <td>
  (table data).",NA
11.5  Working with Toolbox Data,"Given the popularity of Toolbox among linguists, we will discuss some further 
 methods for working with Toolbox data. Many of the methods discussed in 
 previous chapters, such as counting, building frequency distributions, and 
 tabulating co-occurrences, can be applied to the content of Toolbox entries. For 
 example, we can trivially compute the average number of fields for each entry:
  
 >>> from nltk.corpus import toolbox 
  
 >>> lexicon = toolbox.xml('rotokas.dic') 
  
 >>> sum(len(entry) for entry in lexicon) / len(lexicon) 
 13.635955056179775
  
 In this section, we will discuss two tasks that arise in the context of documentary 
 lin-guistics, neither of which is supported by the Toolbox software.",NA
Adding a Field to Each Entry,"It is often convenient to add new fields that are derived automatically from existing 
 ones. Such fields often facilitate search and analysis. For instance, in 
 Example 11-2
  
 we define a function 
 cv()
 , which maps a string of consonants and vowels to the 
 corre-sponding CV sequence, e.g., 
 kakapua
  would map to 
 CVCVCVV
 . This mapping 
 has four steps. First, the string is converted to lowercase, then we replace any non-
 alphabetic characters 
 [^a-z]
  with an underscore. Next, we replace all vowels with 
 V
 . 
 Finally, any-thing that is not a 
 V
  or an underscore must be a consonant, so we 
 replace it with a 
 C
 . Now, we can scan the lexicon and add a new 
 cv
  field after every 
 lx
  field. 
 Exam-ple 11-2
  shows what this does to a particular entry; note the last line 
 of output, which shows the new 
 cv
  field.
  
 11.5  Working with Toolbox Data | 431",NA
Validating a Toolbox Lexicon,"Many lexicons in Toolbox format do not conform to any particular schema. Some 
 entries may include extra fields, or may order existing fields in a new way. 
 Manually inspecting thousands of lexical entries is not practicable. However, we 
 can easily iden-tify frequent versus exceptional field sequences, with the help of a 
 FreqDist
 :
  
 >>> fd = nltk.FreqDist(':'.join(field.tag for field in entry) for entry in lexicon) >>> fd.items() 
  
 [('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),
  
 432 | Chapter 11:Managing Linguistic Data",NA
11.6  Describing Language Resources Using ,NA,NA
OLAC Metadata,"Members of the NLP community have a common need for discovering language re-
 sources with high precision and recall. The solution which has been developed by 
 the Digital Libraries community involves metadata aggregation.",NA
What Is Metadata?,"The simplest definition of metadata is “structured data about data.” Metadata is de-
 scriptive information about an object or resource, whether it be physical or 
 electronic. Although the term “metadata” itself is relatively new, the underlying 
 concepts behind metadata have been in use for as long as collections of information 
 have been organized. Library catalogs represent a well-established type of 
 metadata; they have served as col-lection management and resource discovery 
 tools for decades. Metadata can be gen-erated either “by hand” or automatically 
 using software.
  
 The Dublin Core Metadata Initiative began in 1995 to develop conventions for 
 finding, sharing, and managing information. The Dublin Core metadata elements 
 represent a broad, interdisciplinary consensus about the core set of elements that 
 are likely to be widely useful to support resource discovery. The Dublin Core 
 consists of 15 metadata elements, where each element is optional and repeatable: 
 Title, Creator, Subject, De-scription, Publisher, Contributor, Date, Type, Format, 
 Identifier, Source, Language, Relation, Coverage, and Rights. This metadata set can 
 be used to describe resources that exist in digital or traditional formats.
  
 The Open Archives Initiative (OAI) provides a common framework across digital 
 re-positories of scholarly materials, regardless of their type, including documents, 
 data, software, recordings, physical artifacts, digital surrogates, and so forth. Each 
 repository consists of a network-accessible server offering public access to 
 archived items. Each item has a unique identifier, and is associated with a Dublin 
 Core metadata record (and possibly additional records in other formats). The OAI 
 defines a protocol for metadata search services to “harvest” the contents of 
 repositories.",NA
OLAC: Open Language Archives Community,"The Open Language Archives Community, or OLAC, is an international partnership 
 of institutions and individuals who are creating a worldwide virtual library of 
 language resources by: (i) developing consensus on best current practices for the 
 digital archiving of language resources, and (ii) developing a network of 
 interoperating repositories and services for housing and accessing such resources. 
 OLAC’s home on the Web is at 
 http: //www.language-archives.org/
 .
  
 OLAC Metadata is a standard for describing language resources. Uniform 
 description across repositories is ensured by limiting the values of certain 
 metadata elements to the use of terms from controlled vocabularies. OLAC 
 metadata can be used to describe data and tools, in both physical and digital 
 formats. OLAC metadata extends the",NA
11.7  Summary,"• Fundamental data types, present in most corpora, are annotated texts and 
 lexicons. 
  
 Texts have a temporal structure, whereas lexicons have a record 
 structure.
  
 • The life cycle of a corpus includes data collection, annotation, quality control, 
 and publication. The life cycle continues after publication as the corpus is 
 modified and enriched during the course of research.
  
 • Corpus development involves a balance between capturing a representative 
 sample of language usage, and capturing enough material from any one source 
 or genre to be useful; multiplying out the dimensions of variability is usually 
 not feasible be-cause of resource limitations.
  
 • XML provides a useful format for the storage and interchange of linguistic data, 
  
 but provides no shortcuts for solving pervasive data modeling problems.
  
 • Toolbox format is widely used in language documentation projects; we can write 
  
 programs to support the curation of Toolbox files, and to convert them to XML.
  
 • The Open Language Archives Community (OLAC) provides an infrastructure for 
  
 documenting and discovering language resources.",NA
11.8  Further Reading,"Extra materials for this chapter are posted at 
 http://www.nltk.org/
 , including links 
 to freely available resources on the Web.
  
 The primary sources of linguistic corpora are the 
 Linguistic Data Consortium
  and 
 the 
 European Language Resources Agency
 , both with extensive online catalogs. 
 More de-tails concerning the major corpora mentioned in the chapter are available: 
 American National Corpus (Reppen, Ide & Suderman, 2005), British National 
 Corpus (BNC, 1999), Thesaurus Linguae Graecae (TLG, 1999), Child Language Data 
 Exchange Sys-tem (CHILDES) (MacWhinney, 1995), and TIMIT (Garofolo et al., 
 1986).
  
 Two special interest groups of the Association for Computational Linguistics that 
 or-ganize regular workshops with published proceedings are SIGWAC, which 
 promotes the use of the Web as a corpus and has sponsored the CLEANEVAL task 
 for removing HTML markup, and SIGANN, which is encouraging efforts toward 
 interoperability of",NA
11.9  Exercises,"1.
  
 ◑
  
 In 
 Example 11-2
  the new field appeared at the bottom of the entry. Modify 
 this program so that it inserts the new subelement right after the 
 lx
  field. (Hint: 
 create the new 
 cv
  field using 
 Element('cv')
 , assign a text value to it, then use the 
 insert()
  method of the parent element.) 
  
 2.
  
 ◑
  
 Write a function that deletes a specified field from a lexical entry. (We could 
 use this to sanitize our lexical data before giving it to others, e.g., by removing 
 fields containing irrelevant or uncertain content.)
  
 3.
  
 ◑
  
 Write a program that scans an HTML dictionary file to find entries having an 
 illegal part-of-speech field, and then reports the headword for each entry.
  
 438 | Chapter 11:Managing Linguistic Data",NA
Afterword: The Language ,NA,NA
Challenge,"Natural language throws up some interesting computational challenges. We’ve ex-
 plored many of these in the preceding chapters, including tokenization, tagging, 
 clas-sification, information extraction, and building syntactic and semantic 
 representations. You should now be equipped to work with large datasets, to create 
 robust models of linguistic phenomena, and to extend them into components for 
 practical language technologies. We hope that the Natural Language Toolkit (NLTK) 
 has served to open up the exciting endeavor of practical natural language 
 processing to a broader audience than before.
  
 In spite of all that has come before, language presents us with far more than a 
 temporary challenge for computation. Consider the following sentences which 
 attest to the riches of language:
  
 1. Overhead the day drives level and grey, hiding the sun by a flight of grey spears. 
 (William Faulkner, 
 As I Lay Dying
 , 1935)
  
 2. When using the toaster please ensure that the exhaust fan is turned on. (sign in 
 dormitory kitchen)
  
 3. Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activi-
 ties with Ki values of 45.1-271.6 
 μ
 M (Medline, PMID: 10718780)
  
 4. Iraqi Head Seeks Arms (spoof news headline)
  
 5. The earnest prayer of a righteous man has great power and wonderful results. 
 (James 5:16b)
  
 6. Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll, 
 Jabberwocky
 , 1872)
  
 7. There are two ways to do this, AFAIK :smile: (Internet discussion archive)
  
 Other evidence for the riches of language is the vast array of disciplines whose 
 work centers on language. Some obvious disciplines include translation, literary 
 criticism, philosophy, anthropology, and psychology. Many less obvious disciplines 
 investigate language use, including law, hermeneutics, forensics, telephony, 
 pedagogy, archaeol-ogy, cryptanalysis, and speech pathology. Each applies distinct 
 methodologies to gather",NA
Language Processing Versus Symbol ,NA,NA
Processing,"The very notion that natural language could be treated in a computational manner 
 grew out of a research program, dating back to the early 1900s, to reconstruct 
 mathematical reasoning using logic, most clearly manifested in work by Frege, 
 Russell, Wittgenstein, Tarski, Lambek, and Carnap. This work led to the notion of 
 language as a formal system amenable to automatic processing. Three later 
 developments laid the foundation for natural language processing. The first was 
 formal language theory
 . This defined a language as a set of strings accepted by a 
 class of automata, such as context-free lan-guages and pushdown automata, and 
 provided the underpinnings for computational syntax.
  
 The second development was 
 symbolic logic
 . This provided a formal method for 
 cap-turing selected aspects of natural language that are relevant for expressing 
 logical proofs. A formal calculus in symbolic logic provides the syntax of a language, 
 together with rules of inference and, possibly, rules of interpretation in a set-
 theoretic model; examples are propositional logic and first-order logic. Given such a 
 calculus, with a well-defined syntax and semantics, it becomes possible to associate 
 meanings with expressions of natural language by translating them into 
 expressions of the formal cal-culus. For example, if we translate 
 John saw Mary
  into 
 a formula 
 saw(j, m)
 , we (im-plicitly or explicitly) interpret the English verb 
 saw
  as a 
 binary relation, and 
 John
  and 
 Mary
  as denoting individuals. More general 
 statements like 
 All birds fly
  require quan-tifiers, in this case 
 ∀
 , meaning 
 for all
 : 
 ∀
 x
  
 (
 bird
 (
 x
 ) 
 →
 fly
 (
 x
 )). This use of logic provided the technical machinery to perform 
 inferences that are an important part of language understanding.
  
 A closely related development was the 
 principle of compositionality
 , namely that 
 the meaning of a complex expression is composed from the meaning of its parts 
 and their mode of combination (
 Chapter 10
 ). This principle provided a useful 
 corre-spondence between syntax and semantics, namely that the meaning of a 
 complex ex-pression could be computed recursively. Consider the sentence 
 It is not 
 true thatp
 , where 
 p
  is a proposition. We can represent the meaning of this sentence 
 as 
 not
 (
 p
 ).",NA
Contemporary Philosophical Divides,"The contrasting approaches to NLP described in the preceding section relate back 
 to early metaphysical debates about 
 rationalism
  versus 
 empiricism
  and 
 realism
  
 versus 
 idealism
  that occurred in the Enlightenment period of Western philosophy. 
 These debates took place against a backdrop of orthodox thinking in which the 
 source of all knowledge was believed to be divine revelation. During this period of 
 the 17th and 18th centuries, philosophers argued that human reason or sensory 
 experience has priority over revelation. Descartes and Leibniz, among others, took 
 the rationalist position, asserting that all truth has its origins in human thought, 
 and in the existence of “innate ideas” implanted in our minds from birth. For 
 example, they argued that the principles of Euclidean geometry were developed 
 using human reason, and were not the result of supernatural revelation or sensory 
 experience. In contrast, Locke and others took the empiricist view, that our primary 
 source of knowledge is the experience of our faculties, and that human reason plays 
 a secondary role in reflecting on that experience. Often-cited evidence for this 
 position was Galileo’s discovery—based on careful observation of the motion of the 
 planets—that the solar system is heliocentric and not geocentric. In the context of 
 linguistics, this debate leads to the following question: to what extent does human 
 linguistic experience, versus our innate “language faculty,” provide the",NA
NLTK Roadmap,"The Natural Language Toolkit is a work in progress, and is being continually 
 expanded as people contribute code. Some areas of NLP and linguistics are not 
 (yet) well sup-ported in NLTK, and contributions in these areas are especially 
 welcome. Check 
 http: //www.nltk.org/
  for news about developments after the 
 publication date of this book.
  
 Contributions in the following areas are particularly encouraged:",NA
Envoi...,"Linguists are sometimes asked how many languages they speak, and have to 
 explain that this field actually concerns the study of abstract structures that are 
 shared by lan-guages, a study which is more profound and elusive than learning to 
 speak as many languages as possible. Similarly, computer scientists are sometimes 
 asked how many programming languages they know, and have to explain that 
 computer science actually concerns the study of data structures and algorithms 
 that can be implemented in any programming language, a study which is more 
 profound and elusive than striving for fluency in as many programming languages 
 as possible.
  
 This book has covered many topics in the field of Natural Language Processing. 
 Most of the examples have used Python and English. However, it would be 
 unfortunate if readers concluded that NLP is about how to write Python programs 
 to manipulate English text, or more broadly, about how to write programs (in any 
 programming lan-guage) to manipulate text (in any natural language). Our 
 selection of Python and Eng-lish was expedient, nothing more. Even our focus on 
 programming itself was only a means to an end: as a way to understand data 
 structures and algorithms for representing and manipulating collections of 
 linguistically annotated text, as a way to build new language technologies to better 
 serve the needs of the information society, and ulti-mately as a pathway into 
 deeper understanding of the vast riches of human language.
  
 But for the present: happy hacking!",NA
Bibliography,"[Abney, 1989] Steven P. Abney. A computational model of human parsing. 
 Journal of 
 Psycholinguistic Research
 , 18:129–144, 1989.
  
 [Abney, 1991] Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P. 
 Abney, and Carol Tenny, editors, 
 Principle-Based Parsing: Computation and Psycho-
 linguistics
 , volume 44 of 
 Studies in Linguistics and Philosophy
 . Kluwer Academic 
 Pub-lishers, Dordrecht, 1991.
  
 [Abney, 1996a] Steven Abney. 
 Part-of-speech tagging and partial parsing
 . In Ken 
 Church, Steve Young, and Gerrit Bloothooft, editors, 
 Corpus-Based Methods in Lan-
 guage and Speech
 . Kluwer Academic Publishers, Dordrecht, 1996.
  
 [Abney, 1996b] Steven Abney. 
 Statistical methods and linguistics
 . In Judith Klavans 
 and Philip Resnik, editors, 
 The Balancing Act: Combining Symbolic and Statistical 
 Ap-proaches to Language
 . MIT Press, 1996.
  
 [Abney, 2008] Steven Abney. 
 Semisupervised Learning for Computational Linguistics
 . 
 Chapman and Hall, 2008.
  
 [Agirre and Edmonds, 2007] Eneko Agirre and Philip Edmonds. 
 Word Sense Disam-
 biguation: Algorithms and Applications
 . Springer, 2007.
  
 [Alpaydin, 2004] Ethem Alpaydin. 
 Introduction to Machine Learning
 . MIT Press, 
 2004.
  
 [Ananiadou and McNaught, 2006] Sophia Ananiadou and John McNaught, editors. 
 Text Mining for Biology and Biomedicine
 . Artech House, 2006.
  
 [Androutsopoulos et al., 1995] Ion Androutsopoulos, Graeme Ritchie, and Peter 
 Tha-nisch. Natural language interfaces to databases—an introduction. 
 Journal of 
 Natural Language Engineering
 , 1:29–81, 1995.
  
 [Artstein and Poesio, 2008] Ron Artstein and Massimo Poesio. Inter-coder 
 agreement for computational linguistics. 
 Computational Linguistics
 , pages 555–596, 
 2008.
  
 [Baayen, 2008] Harald Baayen. 
 Analyzing Linguistic Data: A Practical Introduction to 
 Statistics Using R
 . Cambridge University Press, 2008.",NA
NLTK Index,NA,NA
Symbols,NA,NA
A ,"abspath
 , 50 
  
 accuracy
 , 119, 149, 217 
  
 AnaphoraResolutionException
 , 401 
 AndExpression
 , 369 
  
 append
 , 11, 86, 127, 197 
  
 ApplicationExpression
 , 405 
  
 apply
 , 10 
  
 apply_features
 , 224 
  
 Assignment
 , 378 
  
 assumptions
 , 383",NA
B ,"babelize_shell
 , 30 
  
 background
 , 21 
  
 backoff
 , 200, 201, 205, 208 
 batch_evaluate
 , 393 
  
 batch_interpret
 , 393 
  
 bigrams
 , 20, 55, 56, 141 
  
 BigramTagger
 , 274 
  
 BracketParseCorpusReader
 , 51 
 build_model
 , 383",NA
C ,"chart
 , 168 
  
 Chat
 , 4 
  
 chat
 , 105, 163, 215 
  
 chat80
 , 363 
  
 chat80.sql_query
 , 363 
  
 child
 , 82, 162, 170, 180, 281, 316, 334, 431 
 children
 , 187, 334, 335, 422 
  
 chunk
 , 267, 273, 275, 277 
  
 ChunkParserI
 , 273
  
 classifier
 , 223, 224, 225, 226, 227, 228, 229, 
  
 231, 234, 235, 239 
  
 classify
 , 228 
  
 collocations
 , 20, 21 
  
 common_contexts
 , 5, 6 
  
 concordance
 , 4, 108 
  
 ConditionalFreqDist
 , 52, 53, 56 
  
 conditions
 , 44, 53, 54, 55, 56 
  
 conlltags2tree
 , 273 
  
 ConstantExpression
 , 373 
  
 context
 , 5, 108, 180, 210 
  
 CooperStore
 , 396 
  
 cooper_storage
 , 396 
  
 corpora
 , 51, 85, 94, 270, 363, 427, 434 
  
 corpus
 , 40, 51, 241, 284, 285, 315 
  
 correct
 , 210, 226 
  
 count
 , 9, 119, 139, 224, 225",NA
D ,"data
 , 46, 147, 188 
  
 default
 , 193, 199 
  
 display
 , 200, 201, 308, 309 
  
 distance
 , 155 
  
 draw
 , 265, 280, 323, 398, 429 
  
 draw_trees
 , 324 
  
 DRS
 , 398 
  
 DrtParser
 , 400",NA
E ,"edit_distance
 , 155 
  
 Element
 , 427, 428, 430, 438 
 ElementTree
 , 427, 430, 434 
 ellipsis
 , 111 
  
 em
 , 67 
  
 encoding
 , 50, 95, 96, 434, 436 
 entries
 , 63, 64, 66, 316, 433 
 entropy
 , 244
  
 We’d like to hear your suggestions for improving our indexes. Send email to 
 index@oreilly.com
 .
  
 459",NA
F ,"FeatStruct
 , 337 
  
 feed
 , 83 
  
 fileid
 , 40, 41, 42, 45, 46, 50, 54, 62, 227, 288 
 filename
 , 125, 289 
  
 findall
 , 105, 127, 430 
  
 fol
 , 399 
  
 format
 , 117, 120, 121, 157, 419, 436 
  
 freq
 , 17, 21, 213 
  
 FreqDist
 , 17, 18, 21, 22, 36, 52, 53, 56, 59, 61, 
  
 135, 147, 153, 177, 185, 432 
  
 freqdist
 , 61, 147, 148, 153, 244",NA
G ,"generate
 , 6, 7 
  
 get
 , 68, 185, 194 
  
 getchildren
 , 427, 428 
  
 grammar
 , 265, 267, 269, 272, 278, 308, 311, 
 317, 
  
 320, 321, 396, 433, 434, 436 
  
 Grammar
 , 320, 334, 351, 354, 436",NA
H ,"hole
 , 99 
  
 hyp_extra
 , 236",NA
I ,"ic
 , 176 
  
 ieer
 , 284 
  
 IffExpression
 , 369 
  
 index
 , 13, 14, 16, 24, 90, 127, 134, 308 
 inference
 , 370",NA
J ,"jaccard_distance
 , 155",NA
K ,"keys
 , 17, 192",NA
L ,"LambdaExpression
 , 387 
  
 lancaster
 , 107 
  
 leaves
 , 51, 71, 422 
  
 Lemma
 , 68, 71
  
 lemma
 , 68, 69, 214 
  
 lemmas
 , 68 
  
 length
 , 25, 61, 136, 149 
  
 load
 , 124, 206 
  
 load_corpus
 , 147 
  
 load_earley
 , 335, 352, 355, 363, 392, 400 
 load_parser
 , 334 
  
 logic
 , 376, 389 
  
 LogicParser
 , 369, 370, 373, 375, 388, 400, 
  
 404",NA
M ,"Mace
 , 383 
  
 MaceCommand
 , 383 
  
 maxent
 , 275 
  
 megam
 , 275 
  
 member_holonyms
 , 70, 74 
  
 member_meronyms
 , 74 
  
 metrics
 , 154, 155 
  
 model
 , 200, 201 
  
 Model
 , 201, 382",NA
N ,"nbest_parse
 , 334 
  
 ne
 , 236, 237, 283 
  
 NegatedExpression
 , 369 
  
 ngrams
 , 141 
  
 NgramTagger
 , 204 
  
 nltk.chat.chatbots
 , 31 
  
 nltk.classify
 , 224 
  
 nltk.classify.rte_classify
 , 237 
  
 nltk.cluster
 , 172 
  
 nltk.corpus
 , 40, 42, 43, 44, 45, 48, 51, 53, 54, 
  
 60, 65, 67, 90, 105, 106, 119, 162, 
  
 170, 184, 188, 195, 198, 203, 223, 
  
 227, 258, 259, 271, 272, 285, 315, 
  
 316, 422, 430, 431 
  
 nltk.data.find
 , 85, 94, 427, 434 
  
 nltk.data.load
 , 112, 300, 334 
  
 nltk.data.show_cfg
 , 334, 351, 354, 363 
  
 nltk.downloader
 , 316 
  
 nltk.draw.tree
 , 324 
  
 nltk.etree.ElementTree
 , 427, 430, 432, 434 
 nltk.grammar
 , 298 
  
 nltk.help.brown_tagset
 , 214 
  
 nltk.help.upenn_tagset
 , 180, 214 
  
 nltk.inference.discourse
 , 400 
  
 nltk.metrics.agreement
 , 414 
  
 nltk.metrics.distance
 , 155 
  
 nltk.parse
 , 335, 352, 363, 392, 400 
  
 nltk.probability
 , 219 
  
 nltk.sem
 , 363, 396 
  
 nltk.sem.cooper_storage
 , 396
  
 460 | NLTK Index",NA
O ,"olac
 , 436 
  
 OrExpression
 , 369",NA
P ,"packages
 , 154 
  
 parse
 , 273, 275, 320, 375, 398, 427 
 parsed
 , 51, 373 
  
 ParseI
 , 326 
  
 parse_valuation
 , 378 
  
 part_holonyms
 , 74 
  
 part_meronyms
 , 70, 74 
  
 path
 , 85, 94, 95, 96 
  
 path_similarity
 , 72 
  
 phones
 , 408 
  
 phonetic
 , 408, 409 
  
 PlaintextCorpusReader
 , 51 
  
 porter
 , 107, 108 
  
 posts
 , 65, 235 
  
 ppattach
 , 259 
  
 PPAttachment
 , 258, 259 
  
 productions
 , 308, 311, 320, 334 
 prove
 , 376 
  
 Prover9
 , 376 
  
 punkt
 , 112",NA
R ,"RecursiveDescentParser
 , 302, 304 
 regexp
 , 102, 103, 105, 122 
  
 RegexpChunk
 , 287 
  
 RegexpParser
 , 266, 286 
  
 RegexpTagger
 , 217, 219, 401 
  
 regexp_tokenize
 , 111 
  
 resolve_anaphora
 , 399 
  
 reverse
 , 195 
  
 rte_features
 , 236",NA
S ,"samples
 , 22, 44, 54, 55, 56 
  
 satisfiers
 , 380, 382 
  
 satisfy
 , 155 
  
 score
 , 115, 272, 273, 274, 276, 277 
  
 search
 , 177 
  
 SEM
 , 362, 363, 385, 386, 390, 393, 395, 
 396, 
  
 403
  
 sem
 , 363, 396, 400 
  
 sem.evaluate
 , 406 
  
 Senseval
 , 257 
  
 senseval
 , 258 
  
 ShiftReduceParser
 , 305 
  
 show_clause
 , 285 
  
 show_most_informative_features
 , 228 
 show_raw_rtuple
 , 285 
  
 similar
 , 5, 6, 21, 319 
  
 simplify
 , 388 
  
 sort
 , 12, 136, 192 
  
 SpeakerInfo
 , 409 
  
 sr
 , 65 
  
 State
 , 20, 187 
  
 stem
 , 104, 105 
  
 str2tuple
 , 181 
  
 SubElement
 , 432 
  
 substance_holonyms
 , 74 
  
 substance_meronyms
 , 70, 74 
  
 Synset
 , 67, 68, 69, 70, 71, 72 
  
 synset
 , 68, 69, 70, 71, 425, 426 
  
 s_retrieve
 , 396",NA
T ,"tabulate
 , 54, 55, 119 
  
 tag
 , 146, 164, 181, 184, 185, 186, 187, 188, 
 189, 
  
 195, 196, 198, 207, 210, 226, 231, 
  
 232, 233, 241, 273, 275 
  
 tagged_sents
 , 183, 231, 233, 238, 241, 275 
 tagged_words
 , 182, 187, 229 
  
 tags
 , 135, 164, 188, 198, 210, 277, 433 
  
 Text
 , 4, 284, 436 
  
 token
 , 26, 105, 139, 319, 421 
  
 tokenize
 , 263 
  
 tokens
 , 16, 80, 81, 82, 86, 105, 107, 108, 111, 
  
 139, 140, 153, 198, 206, 234, 308, 
  
 309, 317, 328, 335, 352, 353, 355, 
  
 392 
  
 toolbox
 , 66, 67, 430, 431, 434, 438 
  
 toolbox.ToolboxData
 , 434 
  
 train
 , 112, 225 
  
 translate
 , 66, 74 
  
 tree
 , 268, 294, 298, 300, 301, 311, 316, 317, 
  
 319, 335, 352, 353, 355, 393, 430, 
  
 434 
  
 Tree
 , 315, 322 
  
 Tree.productions
 , 325 
  
 tree2conlltags
 , 273 
  
 treebank
 , 51, 315, 316 
  
 trees
 , 294, 311, 334, 335, 363, 392, 393, 396, 
  
 400 
  
 trigrams
 , 141 
  
 TrigramTagger
 , 205 
  
 tuples
 , 192
  
 NLTK Index | 461",NA
U ,"Undefined
 , 379 
  
 unify
 , 342 
  
 UnigramTagger
 , 200, 203, 219, 274 
  
 url
 , 80, 82, 147, 148",NA
V ,"Valuation
 , 371, 378 
  
 values
 , 149, 192 
  
 Variable
 , 375 
  
 VariableBinderExpression
 , 389",NA
W ,"wordlist
 , 61, 64, 98, 99, 111, 201, 424 
  
 wordnet
 , 67, 162, 170",NA
X ,"xml
 , 427, 436 
  
 xml_posts
 , 235
  
 462 | NLTK Index",NA
General Index,NA,NA
Symbols ,"! (exclamation mark) 
  
  
 != (not equal to) operator, 22, 376 
  
 "" "" (quotation marks, double), in strings, 87 
 $ (dollar sign) in regular expressions, 98, 
 101 % (percent sign) 
  
  
 %% in string formatting, 119 
  
  
 %*s formatting string, 107, 119 
  
  
 %s and %d conversion specifiers, 118 & 
 (ampersand), and operator, 368 
  
 ' ' (quotation marks, single) in strings, 88 ' ' 
 (quotation marks, single), in strings, 87 ' 
 (apostrophe) in tokenization, 110 
  
 ( ) (parentheses) 
  
  
 adding extra to break lines of code, 139 
  
 enclosing expressions in Python, 2 
  
  
 in function names, 9 
  
  
 in regular expressions, 100, 104 
  
  
 in tuples, 134 
  
  
 use with strings, 88 
  
 * (asterisk) 
  
  
 *? non-greedy matching in regular 
  
  
  
  
 expressions, 104 
  
  
 multiplication operator, 2 
  
  
  
 multiplying strings, 88 
  
  
 in regular expressions, 100, 101 
  
 + (plus sign) 
  
  
 += (addition and assignment) operator, 
  
  
  
 195 
  
  
 concatenating lists, 11 
  
  
 concatenating strings, 16, 88 
  
  
 in regular expressions, 100, 101 
  
 , (comma) operator, 133
  
 - (hyphen) in tokenization, 110
  
 - (minus sign), negation operator, 368
  
 -> (implication) operator, 368 
  
 . (dot) wildcard character in regular 
  
  
  
 expressions, 98, 101 
  
 / (slash), 
  
  
 division operator, 2 
  
 : (colon), ending Python statements, 26 
  
 < (less than) operator, 22 
  
 <-> (equivalence) operator, 368 
  
 <= (less than or equal to) operator, 22 
  
 = (equals sign) 
  
  
 == (equal to) operator, 22 
  
  
 == (identity) operator, 132 
  
  
 assignment operator, 14, 130 
  
  
 equality operator, 376 
  
 > (greater than) operator, 22 
  
 >= (greater than or equal to) operator, 22 ? 
 (question mark) in regular expressions, 99, 
  
  
 101 
  
 [ ] (brackets) 
  
  
 enclosing keys in dictionary, 65 
  
  
 indexing lists, 12 
  
  
 omitting in list comprehension used as 
  
  
  
 function parameter, 55 
  
  
 regular expression character classes, 99 
 \ (backslash) 
  
  
 ending broken line of code, 139 
  
  
 escaping string literals, 87 
  
  
 in regular expressions, 100, 101 
  
  
 use with multiline strings, 88 
  
 ^ (caret) 
  
  
 character class negation in regular 
  
  
  
  
 expressions, 100 
  
  
 end of string matching in regular 
  
  
  
  
 expressions, 99
  
 We’d like to hear your suggestions for improving our indexes. Send email to 
 index@oreilly.com
 .
  
 463",NA
A ,"accumulative functions, 150 
  
 accuracy of classification, 239 
  
 ACL (Association for Computational 
  
  
  
 Linguistics), 34 
  
  
 Special Interest Group on Web as Corpus 
  
  
  
 (SIGWAC), 416 
  
 adjectives, categorizing and tagging, 186 
  
 adjuncts of lexical head, 347 
  
 adverbs, categorizing and tagging, 186 
  
 agreement, 329–331 
  
  
 resources for further reading, 357 
  
 algorithm design, 160–167 
  
  
 dynamic programming, 165 
  
  
 recursion, 161 
  
  
 resources for further information, 173 
  
 all operator, 376 
  
 alphabetic variants, 389 
  
 ambiguity 
  
  
 broad-coverage grammars and, 317 
  
  
 capturing structural ambiguity with 
  
  
  
  
 dependency parser, 311 
  
  
 quantifier scope, 381, 394–397 
  
  
 scope of modifier, 314 
  
  
 structurally ambiguous sentences, 300 
  
  
 ubiquitous ambiguity in sentence structure, 
  
  
  
 293 
  
 anagram dictionary, creating, 196 
  
 anaphora resolution, 29 
  
 anaphoric antecedent, 397 
  
 AND (in SQL), 365 
  
 and operator, 24 
  
 annotated text corpora, 46–48 
  
 annotation layers 
  
  
 creating, 412 
  
  
 deciding which to include when acquiring 
  
  
  
 data, 420 
  
  
 quality control for, 413 
  
  
 survey of annotation software, 438
  
 annotation, inline, 421 
  
 antecedent, 28 
  
 antonymy, 71 
  
 apostrophes in tokenization, 110 
  
 appending, 11 
  
 arguments 
  
  
 functions as, 149 
  
  
 named, 152 
  
  
 passing to functions (example), 143 
  
 arguments in logic, 369, 372 
  
 arity, 378 
  
 articles, 186 
  
 assert statements 
  
  
 using in defensive programming, 159 
  
  
 using to find logical errors, 146 
  
 assignment, 130, 378 
  
  
 defined, 14 
  
  
 to list index values, 13 
  
 Association for Computational Linguistics 
 (see 
  
  
 ACL) 
  
 associative arrays, 189 
  
 assumptions, 369 
  
 atomic values, 336 
  
 attribute value matrix, 336 
  
 attribute-value pairs (Toolbox lexicon), 67 
 attributes, XML, 426 
  
 auxiliaries, 348 
  
 auxiliary verbs, 336 
  
  
 inversion and, 348",NA
B ,"\b word boundary in regular expressions, 
 110 backoff, 200 
  
 backtracking, 303 
  
 bar charts, 168 
  
 base case, 161 
  
 basic types, 373 
  
 Bayes classifier (see naive Bayes classifier) 
 bigram taggers, 204 
  
 bigrams, 20 
  
  
 generating random text with, 55 
  
 binary formats, text, 85 
  
 binary predicate, 372 
  
 binary search, 160 
  
 binding variables, 374 
  
 binning, 249 
  
 BIO Format, 286 
  
 book module (NLTK), downloading, 3 
  
 Boolean operators, 368
  
 464 | General Index",NA
C ,"call structure, 165 
  
 call-by-value, 144 
  
 carriage return and linefeed characters, 80 
  
 case in German, 353–356 
  
 Catalan numbers, 317 
  
 categorical grammar, 346 
  
 categorizing and tagging words, 179–219 
  
  
 adjectives and adverbs, 186 
  
  
 automatically adding POS tags to text, 198–
  
  
  
 203 
  
  
 determining word category, 210–213 
  
  
 differences in POS tagsets, 213 
  
  
 exploring tagged corpora using POS tags, 
  
  
  
 187–189 
  
  
 mapping words to properties using Python 
  
  
  
 dictionaries, 189–198 
  
  
 n-gram tagging, 203–208 
  
  
 nouns, 184 
  
  
 resources for further reading, 214 
  
  
 tagged corpora, 181–189 
  
  
 transformation-based tagging, 208–210 
  
 using POS (part-of-speech) tagger, 179 
  
  
 using unsimplified POS tags, 187 
  
  
 verbs, 185 
  
 character class symbols in regular 
 expressions, 
  
  
 110 
  
 character encodings, 48, 54, 94 
  
  
 (see also Unicode) 
  
  
 using your local encoding in Python, 97 
 characteristic function, 377 
  
 chart, 307 
  
 chart parsing, 307 
  
  
 Earley chart parser, 334 
  
 charts, displaying information in, 168
  
 chat text, 42 
  
 chatbots, 31 
  
 child nodes, 279 
  
 chink, 268, 286 
  
 chinking, 268 
  
 chunk grammar, 265 
  
 chunking, 214, 264–270 
  
  
 building nested structure with cascaded 
  
  
  
 chunkers, 278–279 
  
  
 chinking, 268 
  
  
 developing and evaluating chunkers, 270–
  
  
  
 278 
  
  
  
 reading IOB format and CoNLL 2000 
  
  
  
  
 corpus, 270–272 
  
  
  
 simple evaluation and baselines, 272–
  
  
  
  
 274 
  
  
  
 training classifier-based chunkers, 
 274–
   
  
  
 278 
  
  
 exploring text corpora with NP chunker, 
  
  
  
 267 
  
  
 noun phrase (NP), 264 
  
  
 representing chunks, tags versus trees, 
 269 
  
 resources for further reading, 286 
  
  
 tag patterns, 266 
  
  
 Toolbox lexicon, 434 
  
  
 using regular expressions, 266 
  
 chunks, 264 
  
 class labels, 221 
  
 classification, 221–259 
  
  
 classifier trained to recognize named 
  
  
  
  
 entities, 283 
  
  
 decision trees, 242–245 
  
  
 defined, 221 
  
  
 evaluating models, 237–241 
  
  
  
 accuracy, 239 
  
  
  
 confusion matrices, 240 
  
  
  
 cross-validation, 241 
  
  
  
 precision and recall, 239 
  
  
  
 test set, 238 
  
  
 generative versus conditional, 254 
  
  
 Maximum Entropy classifiers, 251–254 
  
 modelling linguistic patterns, 255 
  
  
 naive Bayes classifiers, 246–250 
  
  
 supervised (see supervised classification) 
 classifier-based chunkers, 274–278 
  
 closed class, 212 
  
 closed formula, 375 
  
 closures (+ and *), 100 
  
 clustering package (nltk.cluster), 172
  
 General Index | 465",NA
D ,"\d decimal digits in regular expressions, 110 
 \D nondigit characters in regular expressions, 
  
  
 111 
  
 data formats, converting, 419 
  
 data types 
  
  
 dictionary, 190 
  
  
 documentation for Python standard types, 
  
  
  
 173 
  
  
 finding type of Python objects, 86 
  
  
 function parameter, 146 
  
  
 operations on objects, 86 
  
 database query via natural language, 361–365 
 databases, obtaining data from, 418 
  
 debugger (Python), 158 
  
 debugging techniques, 158 
  
 decimal integers, formatting, 119 
  
 decision nodes, 242 
  
 decision stumps, 243 
  
 decision trees, 242–245 
  
  
 entropy and information gain, 243 
  
 decision-tree classifier, 229 
  
 declarative style, 140 
  
 decoding, 94 
  
 def keyword, 9 
  
 defaultdict, 193 
  
 defensive programming, 159 
  
 demonstratives, agreement with noun, 329 
 dependencies, 310 
  
  
 criteria for, 312 
  
  
 existential dependencies, modeling in 
  
  
  
  
 XML, 427 
  
  
 non-projective, 312 
  
  
 projective, 311 
  
  
 unbounded dependency constructions, 
  
  
  
 349–353 
  
 dependency grammars, 310–315 
  
  
 valency and the lexicon, 312 
  
 dependents, 310 
  
 descriptive models, 255 
  
 determiners, 186 
  
  
 agreement with nouns, 333 
  
 deve-test set, 225 
  
 development set, 225 
  
  
 similarity to test set, 238 
  
 dialogue act tagging, 214 
  
 dialogue acts, identifying types, 235 
  
 dialogue systems (see spoken dialogue 
 systems) dictionaries
  
  
 feature set, 223 
  
  
 feature structures as, 337 
  
  
 pronouncing dictionary, 63–65 
  
  
 Python, 189–198 
  
  
  
 default, 193 
  
  
  
 defining, 193 
  
  
  
 dictionary data type, 190 
  
  
  
 finding key given a value, 197 
  
  
  
 indexing lists versus, 189 
  
  
  
 summary of dictionary methods, 197 
  
  
 updating incrementally, 195 
  
  
 storing features and values, 327 
  
  
 translation, 66 
  
 dictionary 
  
  
 methods, 197 
  
 dictionary data structure (Python), 65 
  
 directed acyclic graphs (DAGs), 338 
  
 discourse module, 401 
  
 discourse semantics, 397–402 
  
  
 discourse processing, 400–402 
  
  
 discourse referents, 397 
  
  
 discourse representation structure (DRS), 
  
  
  
  
 397 
  
  
 Discourse Representation Theory (DRT), 
  
  
  
  
 397–400 
  
 dispersion plot, 6 
  
 divide-and-conquer strategy, 160 
  
 docstrings, 143 
  
  
 contents and structure of, 148 
  
  
 example of complete docstring, 148 
  
  
 module-level, 155 
  
 doctest block, 148 
  
 doctest module, 160 
  
 document classification, 227 
  
 documentation 
  
  
 functions, 148 
  
  
 online Python documentation, versions 
  
  
  
  
 and, 173 
  
  
 Python, resources for further information, 
  
  
  
  
 173 
  
 docutils module, 148 
  
 domain (of a model), 377 
  
 DRS (discourse representation structure), 
 397 DRS conditions, 397 
  
 DRT (Discourse Representation Theory), 
 397–
   
  
 400 
  
 Dublin Core Metadata initiative, 435 
  
 duck typing, 281 
  
 dynamic programming, 165
  
 General Index | 467",NA
E ,"Earley chart parser, 334 
  
 electronic books, 80 
  
 elements, XML, 425 
  
 ElementTree interface, 427–429 
  
  
 using to access Toolbox data, 429 
  
 elif clause, if . . . elif statement, 133 
  
 elif statements, 26 
  
 else statements, 26 
  
 encoding, 94 
  
 encoding features, 223 
  
 encoding parameters, codecs module, 95 
  
 endangered languages, special 
 considerations 
  
  
 with, 423–424 
  
 entities, 373 
  
 entity detection, using chunking, 264–270 
 entries 
  
  
 adding field to, in Toolbox, 431 
  
  
 contents of, 60 
  
  
 converting data formats, 419 
  
  
 formatting in XML, 430 
  
 entropy, 251 
  
  
 (see also Maximum Entropy classifiers) 
  
 calculating for gender prediction task, 
 243 
  
 maximizing in Maximum Entropy 
  
  
  
  
 classifier, 252 
  
 epytext markup language, 148 
  
 equality, 132, 372 
  
 equivalence (<->) operator, 368 
  
 equivalent, 340 
  
 error analysis, 225 
  
 errors 
  
  
 runtime, 13 
  
  
 sources of, 156 
  
  
 syntax, 3 
  
 evaluation sets, 238 
  
 events, pairing with conditions in 
 conditional 
   
 frequency distribution, 
 52 
  
 exceptions, 158 
  
 existential quantifier, 374 
  
 exists operator, 376 
  
 Expected Likelihood Estimation, 249 
  
 exporting data, 117",NA
F ,"f-structure, 357 
  
 feature extractors 
  
  
 defining for dialogue acts, 235 
  
  
 defining for document classification, 228 
  
 defining for noun phrase (NP) chunker, 
  
  
  
 276–278 
  
  
 defining for punctuation, 234 
  
  
 defining for suffix checking, 229 
  
  
 Recognizing Textual Entailment (RTE), 
  
  
  
 236 
  
  
 selecting relevant features, 224–227 
  
 feature paths, 339 
  
 feature sets, 223 
  
 feature structures, 328 
  
  
 order of features, 337 
  
  
 resources for further reading, 357 
  
 feature-based grammars, 327–360 
  
  
 auxiliary verbs and inversion, 348 
  
  
 case and gender in German, 353 
  
  
 example grammar, 333 
  
  
 extending, 344–356 
  
  
 lexical heads, 347 
  
  
 parsing using Earley chart parser, 334 
  
  
 processing feature structures, 337–344 
  
  
 subsumption and unification, 341–344 
  
 resources for further reading, 357 
  
  
 subcategorization, 344–347 
  
  
 syntactic agreement, 329–331 
  
  
 terminology, 336 
  
  
 translating from English to SQL, 362 
  
  
 unbounded dependency constructions, 
  
  
  
 349–353 
  
  
 using attributes and constraints, 331–
 336 features, 223 
  
  
 non-binary features in naive Bayes 
  
  
  
  
 classifier, 249 
  
 fields, 136 
  
 file formats, libraries for, 172 
  
 files 
  
  
 opening and reading local files, 84 
  
  
 writing program output to, 120 
  
 fillers, 349 
  
 first-order logic, 372–385 
  
  
 individual variables and assignments, 
 378 
  
 model building, 383 
  
  
 quantifier scope ambiguity, 381
  
 summary of language, 376 
  
 syntax, 372–375
  
 468 | General Index",NA
G ,"gaps, 349
  
 gazetteer, 282 
  
 gender identification, 222 
  
  
 Decision Tree model for, 242 
  
 gender in German, 353–356 
  
 Generalized Phrase Structure Grammar 
  
  
  
  
 (GPSG), 345 
  
 generate_model ( ) function, 55 
  
 generation of language output, 29 
  
 generative classifiers, 254 
  
 generator expressions, 138 
  
  
 functions exemplifying, 151 
  
 genres, systematic differences between, 42–
 44 German, case and gender in, 353–356 
  
 gerunds, 211 
  
 glyphs, 94 
  
 gold standard, 201 
  
 government-sponsored challenges to 
 machine 
   
  
 learning application in NLP, 
 257 gradient (grammaticality), 318 
  
 grammars, 327 
  
  
 (see also feature-based grammars) 
  
  
 chunk grammar, 265 
  
  
 context-free, 298–302 
  
  
  
 parsing with, 302–310 
  
  
  
 validating Toolbox entries with, 433 
  
  
 writing your own, 300 
  
  
 dependency, 310–315 
  
  
 development, 315–321 
  
  
  
 problems with ambiguity, 317 
  
  
  
 treebanks and grammars, 315–317 
  
  
  
 weighted grammar, 318–321 
  
  
 dilemmas in sentence structure analysis, 
  
  
  
  
 292–295 
  
  
 resources for further reading, 322 
  
  
 scaling up, 315 
  
 grammatical category, 328 
  
 graphical displays of data 
  
  
 conditional frequency distributions, 56 
  
 Matplotlib, 168–170 
  
 graphs 
  
  
 defining and manipulating, 170 
  
  
 directed acyclic graphs, 338 
  
 greedy sequence classification, 232 
  
 Gutenberg Corpus, 40–42, 80",NA
H ,"hapaxes, 19 
  
 hash arrays, 189, 190 
  
  
 (see also 
 dictionaries)
  
 General Index | 469",NA
I ,"identifiers for variables, 15 
  
 idioms, Python, 24 
  
 IDLE (Interactive DeveLopment 
  
  
  
 Environment), 2 
  
 if . . . elif statements, 133 
  
 if statements, 25 
  
  
 combining with for statements, 26 
  
  
 conditions in, 133 
  
 immediate constituents, 297 
  
 immutable, 93 
  
 implication (->) operator, 368 
  
 in operator, 91 
  
 Inaugural Address Corpus, 45 
  
 inconsistent, 366 
  
 indenting code, 138 
  
 independence assumption, 248 
  
  
 naivete of, 249 
  
 indexes 
  
  
 counting from zero (0), 12 
  
  
 list, 12–14 
  
  
 mapping dictionary definition to lexeme, 
  
  
  
 419 
  
  
 speeding up program by using, 163 
  
  
 string, 15, 89, 91 
  
  
 text index created using a stemmer, 107 
  
 words containing a given consonant-vowel 
  
  
  
 pair, 103 
  
 inference, 369 
  
 information extraction, 261–289
  
  
 architecture of system, 263 
  
  
 chunking, 264–270 
  
  
 defined, 262 
  
  
 developing and evaluating chunkers, 270–
  
  
 278 
  
  
 named entity recognition, 281–284 
  
  
 recursion in linguistic structure, 278–281 
  
 relation extraction, 284 
  
  
 resources for further reading, 286 
  
 information gain, 243 
  
 inside, outside, begin tags (see IOB tags) 
  
 integer ordinal, finding for character, 95 
  
 interpreter 
  
  
 >>> prompt, 2 
  
  
 accessing, 2 
  
  
 using text editor instead of to write 
  
  
  
 programs, 56 
  
 inverted clauses, 348 
  
 IOB tags, 269, 286 
  
  
 reading, 270–272 
  
 is operator, 145 
  
  
 testing for object identity, 132 
  
 ISO 639 language codes, 65 
  
 iterative optimization techniques, 251",NA
J ,"joint classifier models, 231 
  
 joint-features (maximum entropy model), 
 252",NA
K ,"Kappa coefficient (k), 414 
 keys, 65, 191 
  
  
 complex, 196 
  
 keyword arguments, 153 
 Kleene closures, 100",NA
L ,"lambda expressions, 150, 386–390 
  
  
 example, 152 
  
 lambda operator (
 λ
 ), 386 
  
 Lancaster stemmer, 107 
  
 language codes, 65 
  
 language output, generating, 29 
  
 language processing, symbol processing 
  
  
  
 versus, 442 
  
 language resources 
  
  
 describing using OLAC metadata, 435–
 437 LanguageLog (linguistics blog), 35
  
 470 | General Index",NA
M ,"machine learning 
  
  
 application to NLP, web pages for 
  
  
  
 government challenges, 257 
  
 decision trees, 242–245 
  
  
 Maximum Entropy classifiers, 251–254 
  
 naive Bayes classifiers, 246–250 
  
  
 packages, 237 
  
  
 resources for further reading, 257 
  
  
 supervised classification, 221–237 
  
 machine translation (MT) 
  
  
 limitations of, 30 
  
  
 using NLTK’s babelizer, 30 
  
 mapping, 189 
  
 Matplotlib package, 168–170 
  
 maximal projection, 347 
  
 Maximum Entropy classifiers, 251–254 
  
 Maximum Entropy Markov Models, 233 
 Maximum Entropy principle, 253 
  
 memoization, 167 
  
 meronyms, 70 
  
 metadata, 435 
  
  
 OLAC (Open Language Archives 
  
  
  
 Community), 435 
  
 modals, 186 
  
 model building, 383 
  
 model checking, 379 
  
 models 
  
  
 interpretation of sentences of logical 
  
  
  
 language, 371 
  
  
 of linguistic patterns, 255 
  
  
 representation using set theory, 367 
  
  
 truth-conditional semantics in first-
 order 
  
  
 logic, 377
  
 General Index | 471",NA
N ,"\n newline character in regular expressions, 
  
  
 111 
  
 n-gram tagging, 203–208 
  
  
 across sentence boundaries, 208 
  
  
 combining taggers, 205 
  
  
 n-gram tagger as generalization of unigram 
  
  
  
 tagger, 203 
  
  
 performance limitations, 206 
  
  
 separating training and test data, 203 
  
  
 storing taggers, 206 
  
  
 unigram tagging, 203 
  
  
 unknown words, 206 
  
 naive Bayes assumption, 248 
  
 naive Bayes classifier, 246–250 
  
  
 developing for gender identification task, 
  
  
  
 223 
  
  
 double-counting problem, 250 
  
  
 as generative classifier, 254 
  
  
 naivete of independence assumption, 249 
  
 non-binary features, 249 
  
  
 underlying probabilistic model, 248 
  
  
 zero counts and smoothing, 248 
  
 name resolution, LGB rule for, 145 
  
 named arguments, 152 
  
 named entities 
  
  
 commonly used types of, 281 
  
  
 relations between, 284 
  
 named entity recognition (NER), 281–284 
 Names Corpus, 61 
  
 negative lookahead assertion, 284 
  
 NER (see named entity recognition) 
  
 nested code blocks, 25
  
 newlines, 84 
  
  
 matching in regular expressions, 109 
  
  
 printing with print statement, 90 
  
  
 resources for further information, 122 
  
 non-logical constants, 372 
  
 non-standard words, 108 
  
 normalizing text, 107–108 
  
  
 lemmatization, 108 
  
  
 using stemmers, 107 
  
 noun phrase (NP), 297 
  
 noun phrase (NP) chunking, 264 
  
  
 regular expression–based NP chunker, 
 267 
  
 using unigram tagger, 272 
  
 noun phrases, quantified, 390 
  
 nouns 
  
  
 categorizing and tagging, 184 
  
  
 program to find most frequent noun tags, 
  
  
  
 187 
  
  
 syntactic agreement, 329 
  
 numerically intense algorithms in Python, 
  
  
 increasing efficiency of, 257 
  
 NumPy package, 171",NA
O ,"object references, 130 
  
  
 copying, 132 
  
 objective function, 114 
  
 objects, finding data type for, 86 
  
 OLAC metadata, 74, 435 
  
  
 definition of metadata, 435 
  
  
 Open Language Archives Community, 435 
 Open Archives Initiative (OAI), 435 
  
 open class, 212 
  
 open formula, 374 
  
 Open Language Archives Community 
  
  
  
 (OLAC), 435 
  
 operators, 369 
  
  
 (see also names of individual operators) 
  
 addition and multiplication, 88 
  
  
 Boolean, 368 
  
  
 numerical comparison, 22 
  
  
 scope of, 157 
  
  
 word comparison, 23 
  
 or operator, 24 
  
 orthography, 328 
  
 out-of-vocabulary items, 206 
  
 overfitting, 225, 245
  
 NetworkX package, 170 
  
 new words in languages, 
 212
  
 472 | General Index",NA
P ,"packages, 59 
  
 parameters, 57 
  
  
 call-by-value parameter passing, 144 
  
  
 checking types of, 146 
  
  
 defined, 9 
  
  
 defining for functions, 143 
  
 parent nodes, 279 
  
 parsing, 318 
  
  
 (see also grammars) 
  
  
 with context-free grammar 
  
  
  
 left-corner parser, 306 
  
  
  
 recursive descent parsing, 303 
  
  
  
 shift-reduce parsing, 304 
  
  
  
 well-formed substring tables, 307–
 310 
  
 Earley chart parser, parsing feature-
 based 
  
  
  
  
 grammars, 334 
  
  
 parsers, 302 
  
  
 projective dependency parser, 311 
  
 part-of-speech tagging (see POS tagging) 
  
 partial information, 341 
  
 parts of speech, 179 
  
 PDF text, 85 
  
 Penn Treebank Corpus, 51, 315 
  
 personal pronouns, 186 
  
 philosophical divides in contemporary NLP, 
  
  
  
 444 
  
 phonetics 
  
  
 computer-readable phonetic alphabet 
  
  
  
  
  
 (SAMPA), 137 
  
  
 phones, 63 
  
  
 resources for further information, 74 
  
 phrasal level, 347 
  
 phrasal projections, 347 
  
 pipeline for NLP, 31 
  
 pixel images, 169 
  
 plotting functions, Matplotlib, 168 
  
 Porter stemmer, 107 
  
 POS (part-of-speech) tagging, 179, 208, 229 
  
 (see also tagging) 
  
  
 differences in POS tagsets, 213 
  
  
 examining word context, 230 
  
  
 finding IOB chunk tag for word's POS tag, 
  
  
  
  
 272 
  
  
 in information retrieval, 263 
  
  
 morphology in POS tagsets, 212 
  
  
 resources for further reading, 214 
  
  
 simplified tagset, 183 
  
  
 storing POS tags in tagged corpora, 181
  
  
 tagged data from four Indian languages, 
  
  
  
  
 182 
  
  
 unsimplifed tags, 187 
  
  
 use in noun phrase chunking, 265 
  
  
 using consecutive classifier, 231 
  
 pre-sorting, 160 
  
 precision, evaluating search tasks for, 239 
  
 precision/recall trade-off in information 
  
  
  
  
 retrieval, 205 
  
 predicates (first-order logic), 372 
  
 prepositional phrase (PP), 297 
  
 prepositional phrase attachment ambiguity, 
  
  
  
 300 
  
 Prepositional Phrase Attachment Corpus, 
 316 prepositions, 186 
  
 present participles, 211 
  
 Principle of Compositionality, 385, 443 
  
 print statements, 89 
  
  
 newline at end, 90 
  
  
 string formats and, 117 
  
 prior probability, 246 
  
 probabilistic context-free grammar (PCFG), 
  
  
  
 320 
  
 probabilistic model, naive Bayes classifier, 
 248 probabilistic parsing, 318 
  
 procedural style, 139 
  
 processing pipeline (NLP), 86 
  
 productions in grammars, 293 
  
  
 rules for writing CFGs for parsing in 
  
  
  
  
  
 NLTK, 301 
  
 program development, 154–160 
  
  
 debugging techniques, 158 
  
  
 defensive programming, 159 
  
  
 multimodule programs, 156 
  
  
 Python module structure, 154 
  
  
 sources of error, 156 
  
 programming style, 139 
  
 programs, writing, 129–177 
  
  
 advanced features of functions, 149–154 
  
 algorithm design, 160–167 
  
  
 assignment, 130 
  
  
 conditionals, 133 
  
  
 equality, 132 
  
  
 functions, 142–149 
  
  
 resources for further reading, 173 
  
  
 sequences, 133–138 
  
  
 style considerations, 138–142 
  
  
  
 legitimate uses for counters, 141 
  
  
  
 procedural versus declarative style, 
 139
  
 General Index | 473",NA
Q ,"quality control in corpus creation, 413 
 quantification 
  
  
 first-order logic, 373, 380 
  
  
 quantified noun phrases, 390 
  
  
 scope ambiguity, 381, 394–397
  
 quantified formulas, interpretation of, 
 380 questions, answering, 29 
  
 quotation marks in strings, 87",NA
R ,"random text 
  
  
 generating in various styles, 6 
  
  
 generating using bigrams, 55 
  
 raster (pixel) images, 169 
  
 raw strings, 101 
  
 raw text, processing, 79–128 
  
  
 capturing user input, 85 
  
  
 detecting word patterns with regular 
  
  
  
  
  
 expressions, 97–101 
  
  
 formatting from lists to strings, 116–121 
  
 HTML documents, 82 
  
  
 NLP pipeline, 86 
  
  
 normalizing text, 107–108 
  
  
 reading local files, 84 
  
  
 regular expressions for tokenizing text, 
 109–
   
  
  
 112 
  
  
 resources for further reading, 122 
  
  
 RSS feeds, 83 
  
  
 search engine results, 82 
  
  
 segmentation, 112–116 
  
  
 strings, lowest level text processing, 87–
 93 summary of important points, 121 
  
  
 text from web and from disk, 80 
  
  
 text in binary formats, 85 
  
  
 useful applications of regular expressions, 
  
  
  
  
 102–106 
  
  
 using Unicode, 93–97 
  
 raw( ) function, 41 
  
 re module, 101, 110 
  
 recall, evaluating search tasks for, 240 
  
 Recognizing Textual Entailment (RTE), 32, 
  
  
  
 235 
  
  
 exploiting word context, 230 
  
 records, 136 
  
 recursion, 161 
  
  
 function to compute Sanskrit meter 
  
  
  
  
  
 (example), 165 
  
  
 in linguistic structure, 278–281 
  
  
  
 tree traversal, 280 
  
  
  
 trees, 279–280 
  
  
 performance and, 163 
  
  
 in syntactic structure, 301 
  
 recursive, 301 
  
 recursive descent parsing, 303
  
 474 | General Index",NA
S ,"\s whitespace characters in regular 
  
  
  
 expressions, 111 
  
 \S nonwhitespace characters in regular 
  
  
  
 expressions, 111 
  
 SAMPA computer-readable phonetic 
 alphabet, 
   
 137 
  
 Sanskrit meter, computing, 165 
  
 satisfies, 379 
  
 scope of quantifiers, 381 
  
 scope of variables, 145 
  
 searches 
  
  
 binary search, 160 
  
  
 evaluating for precision and recall, 239 
  
 processing search engine results, 82 
  
  
 using POS tags, 187 
  
 segmentation, 112–116 
  
  
 in chunking and tokenization, 264 
  
  
 sentence, 112 
  
  
 word, 113–116 
  
 semantic cues to word category, 211 
  
 semantic interpretations, NLTK functions for, 
  
  
 393 
  
 semantic role labeling, 29 
  
 semantics 
  
  
 natural language, logic and, 365–368 
  
  
 natural language, resources for 
  
  
  
  
 information, 403 
  
 semantics of English sentences, 385–397 
  
  
 quantifier ambiguity, 394–397 
  
  
 transitive verbs, 391–394
  
  
 ⋏
 -calculus, 386–390 
  
 SemCor tagging, 214 
  
 sentence boundaries, tagging across, 208 
  
 sentence segmentation, 112, 233 
  
  
 in chunking, 264 
  
  
 in information retrieval process, 263 
  
 sentence structure, analyzing, 291–326 
  
  
 context-free grammar, 298–302 
  
  
 dependencies and dependency grammar, 
  
  
  
 310–315 
  
  
 grammar development, 315–321 
  
  
 grammatical dilemmas, 292 
  
  
 parsing with context-free grammar, 302–
  
  
  
 310 
  
  
 resources for further reading, 322 
  
  
 summary of important points, 321 
  
  
 syntax, 295–298 
  
 sents( ) function, 41
  
 General Index | 475",NA
T ,"\t tab character in regular expressions, 111 T9 
 system, entering text on mobile phones, 99 
 tabs 
  
  
 avoiding in code indentation, 138 
  
  
 matching in regular expressions, 109 
  
 tag patterns, 266 
  
  
 matching, precedence in, 267 
  
 tagging, 179–219 
  
  
 adjectives and adverbs, 186 
  
  
 combining taggers, 205 
  
  
 default tagger, 198 
  
  
 evaluating tagger performance, 201 
  
  
 exploring tagged corpora, 187–189 
  
  
 lookup tagger, 200–201 
  
  
 mapping words to tags using Python 
  
  
  
 dictionaries, 189–198 
  
  
 nouns, 184 
  
  
 part-of-speech (POS) tagging, 229 
  
  
 performance limitations, 206 
  
  
 reading tagged corpora, 181 
  
  
 regular expression tagger, 199 
  
  
 representing tagged tokens, 181 
  
  
 resources for further reading, 214 
  
  
 across sentence boundaries, 208 
  
  
 separating training and testing data, 203 
  
 simplified part-of-speech tagset, 183 
  
  
 storing taggers, 206 
  
  
 transformation-based, 208–210 
  
  
 unigram tagging, 202 
  
  
 unknown words, 206 
  
  
 unsimplified POS tags, 187 
  
  
 using POS (part-of-speech) tagger, 179 
  
 verbs, 185 
  
 tags 
  
  
 in feature structures, 340 
  
  
 IOB tags representing chunk structures, 
  
  
 269 
  
  
 XML, 425 
  
 tagsets, 179 
  
  
 morphosyntactic information in POS 
  
  
  
 tagsets, 212 
  
  
 simplified POS tagset, 183
  
 terms (first-order logic), 372 
  
 test sets, 44, 223 
  
  
 choosing for classification models, 238 
 testing classifier for document classification, 
  
  
  
 228 
  
 text, 1 
  
  
 computing statistics from, 16–22 
  
  
 counting vocabulary, 7–10 
  
  
 entering on mobile phones (T9 system), 
 99 as lists of words, 10–16 
  
  
 searching, 4–7 
  
  
  
 examining common contexts, 5 
  
 text alignment, 30 
  
 text editor, creating programs with, 56 
  
 textonyms, 99 
  
 textual entailment, 32 
  
 textwrap module, 120 
  
 theorem proving in first order logic, 375 
  
 timeit module, 164 
  
 TIMIT Corpus, 407–412 
  
 tokenization, 80 
  
  
 chunking and, 264 
  
  
 in information retrieval, 263 
  
  
 issues with, 111 
  
  
 list produced from tokenizing string, 86 
  
 regular expressions for, 109–112 
  
  
 representing tagged tokens, 181 
  
  
 segmentation and, 112 
  
  
 with Unicode strings as input and output, 
  
  
  
  
 97 
  
 tokenized text, searching, 105 
  
 tokens, 8 
  
 Toolbox, 66, 412, 431–435 
  
  
 accessing data from XML, using 
  
  
  
  
  
 ElementTree, 429 
  
  
 adding field to each entry, 431 
  
  
 resources for further reading, 438 
  
  
 validating lexicon, 432–435 
  
 tools for creation, publication, and use of 
  
  
  
  
 linguistic data, 421 
  
 top-down approach to dynamic 
 programming, 
   
  
 167 
  
 top-down parsing, 304 
  
 total likelihood, 251 
  
 training 
  
  
 classifier, 223 
  
  
 classifier for document classification, 228 
  
 classifier-based chunkers, 274–278 
  
  
 taggers, 203
  
 General Index | 477",NA
U ,"unary predicate, 372 
  
 unbounded dependency constructions, 349–
  
  
 353 
  
  
 defined, 350 
  
 underspecified, 333 
  
 Unicode, 93–97 
  
  
 decoding and encoding, 94 
  
  
 definition and description of, 94 
  
  
 extracting gfrom files, 94 
  
  
 resources for further information, 122 
  
 using your local encoding in Python, 97 
 unicodedata module, 96 
  
 unification, 342–344 
  
 unigram taggers 
  
  
 confusion matrix for, 240 
  
  
 noun phrase chunking with, 272 
  
 unigram tagging, 202 
  
  
 lookup tagger (example), 200 
  
  
 separating training and test data, 203
  
 unique beginners, 69 
  
 Universal Feed Parser, 83 
  
 universal quantifier, 374 
  
 unknown words, tagging, 206 
  
 updating dictionary incrementally, 195 
  
 US Presidential Inaugural Addresses 
 Corpus, 
  
 45 
  
 user input, capturing, 85",NA
V ,"valencies, 313 
  
 validity of arguments, 369 
  
 validity of XML documents, 426 
  
 valuation, 377 
  
  
 examining quantifier scope ambiguity, 
 381 
  
 Mace4 model converted to, 384 
  
 valuation function, 377 
  
 values, 191 
  
  
 complex, 196 
  
 variables 
  
  
 arguments of predicates in first-order 
 logic, 
  
  
 373 
  
  
 assignment, 378 
  
  
 bound by quantifiers in first-order logic, 
  
  
 373 
  
  
 defining, 14 
  
  
 local, 58 
  
  
 naming, 15 
  
  
 relabeling bound variables, 389 
  
  
 satisfaction of, using to interpret 
 quantified 
   
 formulas, 380 
  
  
 scope of, 145 
  
 verb phrase (VP), 297 
  
 verbs 
  
  
 agreement paradigm for English regular 
  
  
 verbs, 329 
  
  
 auxiliary, 336 
  
  
 auxiliary verbs and inversion of subject 
 and 
  
  
 verb, 348 
  
  
 categorizing and tagging, 185 
  
  
 examining for dependency grammar, 312 
  
 head of sentence and dependencies, 310 
  
 present participle, 211 
  
  
 transitive, 391–394",NA
W ,"\W non-word characters in Python, 110, 111 
 \w word characters in Python, 110, 111
  
 478 | General Index",NA
Z ,"zero counts (naive Bayes classifier), 
 249 zero projection, 347
  
  
 probabilistic context-free grammar (PCFG), 
  
  
  
  
 320 
  
 well-formed (XML), 425 
  
 well-formed formulas, 368 
  
 well-formed substring tables (WFST), 307–
  
  
  
 310 
  
 whitespace 
  
  
 regular expression characters for, 109 
  
  
 tokenizing text on, 109 
  
 wildcard symbol (.), 98 
  
 windowdiff scorer, 414 
  
 word classes, 179 
  
 word comparison operators, 23 
  
 word occurrence, counting in text, 8 
  
 word offset, 45 
  
 word processor files, obtaining data from, 417 
  
 word segmentation, 113–116 
  
 word sense disambiguation, 28 
  
 word sequences, 7 
  
 wordlist corpora, 60–63 
  
 WordNet, 67–73 
  
  
 concept hierarchy, 69 
  
  
 lemmatizer, 108 
  
  
 more lexical relations, 70 
  
  
 semantic similarity, 71 
  
  
 visualization of hypernym hierarchy using 
  
  
  
  
 Matplotlib and NetworkX, 170 
  
 Words Corpus, 60 
  
 words( ) function, 40 
  
 wrapping text, 120",NA
X ,"XML, 425–431 
  
  
 ElementTree interface, 427–429 
  
  
 formatting entries, 430 
  
  
 representation of lexical entry from 
 chunk 
  
  
  
 parsing Toolbox record, 434 
  
  
 resources for further reading, 438 
  
  
 role of, in using to represent linguistic 
  
  
  
 structures, 426 
  
  
 using ElementTree to access Toolbox 
 data, 
  
  
  
 429 
  
  
 using for linguistic structures, 425 
  
  
 validity of documents, 426
  
 General Index | 479",NA
About the Authors,"Steven Bird
  is Associate Professor in the Department of Computer Science and 
 Soft-ware Engineering at the University of Melbourne, and Senior Research 
 Associate in the Linguistic Data Consortium at the University of Pennsylvania. He 
 completed a Ph.D. on computational phonology at the University of Edinburgh in 
 1990, supervised by Ewan Klein. He later moved to Cameroon to conduct linguistic 
 fieldwork on the Grass-fields Bantu languages under the auspices of the Summer 
 Institute of Linguistics. More recently, he spent several years as Associate Director 
 of the Linguistic Data Consortium, where he led an R&D team to create models and 
 tools for large databases of annotated text. At Melbourne University, he established 
 a language technology research group and has taught at all levels of the 
 undergraduate computer science curriculum. In 2009, Steven is President of the 
 Association for Computational Linguistics.
  
 Ewan Klein
  is Professor of Language Technology in the School of Informatics at the 
 University of Edinburgh. He completed a Ph.D. on formal semantics at the 
 University of Cambridge in 1978. After some years working at the Universities of 
 Sussex and Newcastle upon Tyne, Ewan took up a teaching position at Edinburgh. 
 He was involved in the establishment of Edinburgh’s Language Technology Group 
 in 1993, and has been closely associated with it ever since. From 2000 to 2002, he 
 took leave from the University to act as Research Manager for the Edinburgh-based 
 Natural Language Re-search Group of Edify Corporation, Santa Clara, and was 
 responsible for spoken dia-logue processing. Ewan is a past President of the 
 European Chapter of the Association for Computational Linguistics and was a 
 founding member and Coordinator of the European Network of Excellence in 
 Human Language Technologies (ELSNET).
  
 Edward Loper
  has recently completed a Ph.D. on machine learning for natural lan-
 guage processing at the University of Pennsylvania. Edward was a student in 
 Steven’s graduate course on computational linguistics in the fall of 2000, and went 
 on to be a Teacher’s Assistant and share in the development of NLTK. In addition to 
 NLTK, he has helped develop two packages for documenting and testing Python 
 software, 
 epydoc
  and 
 doctest
 .",NA
Colophon,"The animal on the cover of 
 Natural Language Processing with Python 
 is a right 
 whale, the rarest of all large whales. It is identifiable by its enormous head, which 
 can measure up to one-third of its total body length. It lives in temperate and cool 
 seas in both hemispheres at the surface of the ocean. It’s believed that the right 
 whale may have gotten its name from whalers who thought that it was the “right” 
 whale to kill for oil. Even though it has been protected since the 1930s, the right 
 whale is still the most endangered of all the great whales.
  
 The large and bulky right whale is easily distinguished from other whales by the 
 calluses on its head. It has a broad back without a dorsal fin and a long arching 
 mouth that",NA
