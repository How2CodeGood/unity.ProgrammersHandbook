Larger Text,Smaller Text,Symbol
Python for Data Analysis,NA,NA
Wes McKinney,"Beijing
  •
  Cambridge
  •
  Farnham
  •
  Köln
  •
  Sebastopol
  •
  Tokyo
  
 www.it-ebooks.info",NA
Table of Contents,"Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . .  xi
  
 1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . . 1 
  
 What Is This Book About? 
  
  
 1 
  
 Why Python for Data Analysis? 
  
  
 2 
  
  
 Python as Glue 
  
  
 2 
  
  
 Solving the “Two-Language” Problem 
  
  
  
 2 
  
  
 Why Not Python? 
  
  
 3 
  
 Essential Python Libraries 
  
  
 3 
  
  
 NumPy 
  
  
 4 
  
  
 pandas 
  
  
  
 4 
  
  
 matplotlib 
  
  
 5 
  
  
 IPython 
  
  
 5 
  
  
 SciPy 
  
  
 6 
  
 Installation and Setup 
  
  
  
 6 
  
  
 Windows 
  
  
 7 
  
  
 Apple OS X 
  
  
 9 
  
  
 GNU/Linux 
  
 10 
  
  
 Python 2 and Python 3 
  
  
 11 
  
  
 Integrated Development Environments (IDEs) 
  
 11 
  
 Community and Conferences 
  
 12 
  
 Navigating This Book 
  
 12 
  
  
 Code Examples 
  
  
 13 
  
  
 Data for Examples 
  
 13 
  
  
 Import Conventions 
  
 13 
  
  
 Jargon 
  
 13 
  
 Acknowledgements 
  
 14
  
 2. Introductory Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . . . . . 17",NA
Preface,"The scientific Python ecosystem of open source libraries has grown substantially 
 over the last 10 years. By late 2011, I had long felt that the lack of centralized 
 learning resources for data analysis and statistical applications was a stumbling 
 block for new Python programmers engaged in such work. Key projects for data 
 analysis (especially NumPy, IPython, matplotlib, and pandas) had also matured 
 enough that a book written about them would likely not go out-of-date very 
 quickly. Thus, I mustered the nerve to embark on this writing project. This is the 
 book that I wish existed when I started using Python for data analysis in 2007. I 
 hope you find it useful and are able to apply these tools productively in your work.",NA
Conventions Used in This Book,"The following typographical conventions are used in this book:
  
 Italic 
  
  
 Indicates new terms, URLs, email addresses, filenames, and file extensions.
  
 Constant width 
  
 Used for program listings, as well as within paragraphs to refer to program 
 elements such as variable or function names, databases, data types, 
 environment variables, statements, and keywords.
  
 Constant width bold 
  
  
 Shows commands or other text that should be typed literally by the 
 user.
  
 Constant width italic 
  
 Shows text that should be replaced with user-supplied values or by values 
 deter-mined by context.
  
  
 This icon signifies a tip, suggestion, or general note.
  
 xi",NA
Using Code Examples,"This book is here to help you get your job done. In general, you may use the code in 
 this book in your programs and documentation. You do not need to contact us for 
 permission unless you’re reproducing a significant portion of the code. For 
 example, writing a program that uses several chunks of code from this book does 
 not require permission. Selling or distributing a CD-ROM of examples from O’Reilly 
 books does require permission. Answering a question by citing this book and 
 quoting example code does not require permission. Incorporating a significant 
 amount of example code from this book into your product’s documentation does 
 require permission.
  
 We appreciate, but do not require, attribution. An attribution usually includes the 
 title, author, publisher, and ISBN. For example: “
 Python for Data Analysis
  by William 
 Wes-ley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-
 31979-3.”
  
 If you feel your use of code examples falls outside fair use or the permission given 
 above, feel free to contact us at 
 permissions@oreilly.com
 .",NA
Safari® Books Online,"Safari Books Online (
 www.safaribooksonline.com
 ) is an on-demand 
 digital library that delivers expert 
 content
  in both book and video form 
 from the
  
 world’s leading authors in technology and business.
  
 Technology professionals, software developers, web designers, and business and 
 cre-ative professionals use Safari Books Online as their primary resource for 
 research, problem solving, learning, and certification training.
  
 Safari Books Online offers a range of 
 product mixes
  and pricing programs for 
 organi-zations
 , 
 government agencies
 , and 
 individuals
 . Subscribers have access to 
 thousands of books, training videos, and prepublication manuscripts in one fully 
 searchable da-tabase from publishers like O’Reilly Media, Prentice Hall 
 Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit 
 Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, 
 IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, 
 McGraw-Hill, Jones & Bartlett, Course Tech-nology, and dozens 
 more
 . For more 
 information about Safari Books Online, please visit us 
 online
 .
  
 xii | Preface",NA
How to Contact Us,"Please address comments and questions concerning this book to the publisher:
  
 O’Reilly Media, Inc.
  
 1005 Gravenstein Highway North 
  
 Sebastopol, CA 95472 
  
 800-998-9938 (in the United States or 
 Canada) 707-829-0515 (international or 
 local) 
  
 707-829-0104 (fax)
  
 We have a web page for this book, where we list errata, examples, and any 
 additional information. You can access this page at 
 http://oreil.ly/python_for_data_analysis
 .
  
 To comment or ask technical questions about this book, send email to 
 bookquestions@oreilly.com
 .
  
 For more information about our books, courses, conferences, and news, see our 
 website at 
 http://www.oreilly.com
 .
  
 Find us on Facebook: 
 http://facebook.com/oreilly
  
 Follow us on Twitter: 
 http://twitter.com/oreillymedia
  
 Watch us on YouTube: 
 http://www.youtube.com/oreillymedia
  
 Preface | xiii",NA
CHAPTER 1,NA,NA
Preliminaries,NA,NA
What Is This Book About?,"This book is concerned with the nuts and bolts of manipulating, processing, 
 cleaning, and crunching data in Python. It is also a practical, modern introduction to 
 scientific computing in Python, tailored for data-intensive applications. This is a 
 book about the parts of the Python language and libraries you’ll need to effectively 
 solve a broad set of data analysis problems. This book is 
 not
  an exposition on 
 analytical methods using Python as the implementation language.
  
 When I say “data”, what am I referring to exactly? The primary focus is on 
 structured data
 , a deliberately vague term that encompasses many different 
 common forms of data, such as
  
 • Multidimensional arrays (matrices)
  
 • Tabular or spreadsheet-like data in which each column may be a different type 
 (string, numeric, date, or otherwise). This includes most kinds of data 
 commonly stored in relational databases or tab- or comma-delimited text files
  
 • Multiple tables of data interrelated by key columns (what would be primary or 
  
 foreign keys for a SQL user)
  
 • Evenly or unevenly spaced time series
  
 This is by no means a complete list. Even though it may not always be obvious, a 
 large percentage of data sets can be transformed into a structured form that is 
 more suitable for analysis and modeling. If not, it may be possible to extract 
 features from a data set into a structured form. As an example, a collection of news 
 articles could be processed into a word frequency table which could then be used to 
 perform sentiment analysis.
  
 Most users of spreadsheet programs like Microsoft Excel, perhaps the most widely 
 used data analysis tool in the world, will not be strangers to these kinds of data.
  
 1",NA
Why Python for Data Analysis?,"For many people (myself among them), the Python language is easy to fall in love 
 with. Since its first appearance in 1991, Python has become one of the most popular 
 dynamic, programming languages, along with Perl, Ruby, and others. Python and 
 Ruby have become especially popular in recent years for building websites using 
 their numerous web frameworks, like Rails (Ruby) and Django (Python). Such 
 languages are often called 
 scripting
  languages as they can be used to write quick-
 and-dirty small programs, or 
 scripts
 . I don’t like the term “scripting language” as it 
 carries a connotation that they cannot be used for building mission-critical 
 software. Among interpreted languages Python is distinguished by its large and 
 active 
 scientific computing
  community. Adop-tion of Python for scientific computing 
 in both industry applications and academic research has increased significantly 
 since the early 2000s.
  
 For data analysis and interactive, exploratory computing and data visualization, 
 Python will inevitably draw comparisons with the many other domain-specific 
 open source and commercial programming languages and tools in wide use, such as 
 R, MATLAB, SAS, Stata, and others. In recent years, Python’s improved library 
 support (primarily pandas) has made it a strong alternative for data manipulation 
 tasks. Combined with Python’s strength in general purpose programming, it is an 
 excellent choice as a single language for building data-centric applications.",NA
Python as Glue,"Part of Python’s success as a scientific computing platform is the ease of integrating 
 C, C++, and FORTRAN code. Most modern computing environments share a similar 
 set of legacy FORTRAN and C libraries for doing linear algebra, optimization, 
 integration, fast fourier transforms, and other such algorithms. The same story has 
 held true for many companies and national labs that have used Python to glue 
 together 30 years’worth of legacy software.
  
 Most programs consist of small portions of code where most of the time is spent, 
 with large amounts of “glue code” that doesn’t run often. In many cases, the 
 execution time of the glue code is insignificant; effort is most fruitfully invested in 
 optimizing the computational bottlenecks, sometimes by moving the code to a 
 lower-level language like C.
  
 In the last few years, the Cython project (
 http://cython.org
 ) has become one of the 
 preferred ways of both creating fast compiled extensions for Python and also 
 interfacing with C and C++ code.",NA
Solving the “Two-Language” Problem,"In many organizations, it is common to research, prototype, and test new ideas 
 using a more domain-specific computing language like MATLAB or R then later port 
 those
  
 2 | Chapter 1:Preliminaries",NA
Why Not Python?,"While Python is an excellent environment for building computationally-intensive 
 sci-entific applications and building most kinds of general purpose systems, there 
 are a number of uses for which Python may be less suitable.
  
 As Python is an interpreted programming language, in general most Python code 
 will run substantially slower than code written in a compiled language like Java or 
 C++. As 
 programmer time
  is typically more valuable than 
 CPU time
 , many are happy 
 to make this tradeoff. However, in an application with very low latency 
 requirements (for ex-ample, a high frequency trading system), the time spent 
 programming in a lower-level, lower-productivity language like C++ to achieve the 
 maximum possible performance might be time well spent.
  
 Python is not an ideal language for highly concurrent, multithreaded applications, 
 par-ticularly applications with many CPU-bound threads. The reason for this is that 
 it has what is known as the 
 global interpreter lock
  (GIL), a mechanism which 
 prevents the interpreter from executing more than one Python bytecode 
 instruction at a time. The technical reasons for why the GIL exists are beyond the 
 scope of this book, but as of this writing it does not seem likely that the GIL will 
 disappear anytime soon. While it is true that in many big data processing 
 applications, a cluster of computers may be required to process a data set in a 
 reasonable amount of time, there are still situations where a single-process, 
 multithreaded system is desirable.
  
 This is not to say that Python cannot execute truly multithreaded, parallel code; 
 that code just cannot be executed in a single Python process. As an example, the 
 Cython project features easy integration with OpenMP, a C framework for parallel 
 computing, in order to to parallelize loops and thus significantly speed up 
 numerical algorithms.",NA
Essential Python Libraries,"For those who are less familiar with the scientific Python ecosystem and the 
 libraries used throughout the book, I present the following overview of each library.
  
 Essential Python Libraries | 3",NA
NumPy,"NumPy, short for Numerical Python, is the foundational package for scientific com-
 puting in Python. The majority of this book will be based on NumPy and libraries 
 built on top of NumPy. It provides, among other things:
  
 • A fast and efficient multidimensional array object 
 ndarray
  
 • Functions for performing element-wise computations with arrays or 
 mathematical 
  
 operations between arrays
  
 • Tools for reading and writing array-based data sets to disk
  
 • Linear algebra operations, Fourier transform, and random number generation
  
 • Tools for integrating connecting C, C++, and Fortran code to Python
  
 Beyond the fast array-processing capabilities that NumPy adds to Python, one of its 
 primary purposes with regards to data analysis is as the primary container for data 
 to be passed between algorithms. For numerical data, NumPy arrays are a much 
 more efficient way of storing and manipulating data than the other built-in Python 
 data structures. Also, libraries written in a lower-level language, such as C or 
 Fortran, can operate on the data stored in a NumPy array without copying any data.",NA
pandas,"pandas provides rich data structures and functions designed to make working with 
 structured data fast, easy, and expressive. It is, as you will see, one of the critical in-
 gredients enabling Python to be a powerful and productive data analysis 
 environment. The primary object in pandas that will be used in this book is the 
 DataFrame
 , a two-dimensional tabular, column-oriented data structure with both 
 row and column labels:
  
 >>> frame
  
  
  total_bill  tip   sex     smoker  day  time    size 1   16.99       1.01  
 Female  No      Sun  Dinner  2 2   10.34       1.66  Male    No      Sun  
 Dinner  3 3   21.01       3.5   Male    No      Sun  Dinner  3 4   23.68       
 3.31  Male    No      Sun  Dinner  2 5   24.59       3.61  Female  No      
 Sun  Dinner  4 6   25.29       4.71  Male    No      Sun  Dinner  4 7   
 8.77        2     Male    No      Sun  Dinner  2 8   26.88       3.12  Male    
 No      Sun  Dinner  4 9   15.04       1.96  Male    No      Sun  Dinner  
 2 10  14.78       3.23  Male    No      Sun  Dinner  2
  
 pandas combines the high performance array-computing features of NumPy with 
 the flexible data manipulation capabilities of spreadsheets and relational databases 
 (such as SQL). It provides sophisticated indexing functionality to make it easy to 
 reshape, slice and dice, perform aggregations, and select subsets of data. pandas is 
 the primary tool that we will use in this book.
  
 4 | Chapter 1:Preliminaries
  
 www.it-ebooks.info",NA
matplotlib,"matplotlib is the most popular Python library for producing plots and other 2D data 
 visualizations. It was originally created by John D. Hunter (JDH) and is now 
 maintained by a large team of developers. It is well-suited for creating plots 
 suitable for publication. It integrates well with IPython (see below), thus providing 
 a comfortable interactive environment for plotting and exploring data. The plots 
 are also 
 interactive
 ; you can zoom in on a section of the plot and pan around the 
 plot using the toolbar in the plot window.",NA
IPython,"IPython is the component in the standard scientific Python toolset that ties 
 everything together. It provides a robust and productive environment for 
 interactive and explor-atory computing. It is an enhanced Python shell designed to 
 accelerate the writing, testing, and debugging of Python code. It is particularly 
 useful for interactively working with data and visualizing data with matplotlib. 
 IPython is usually involved with the majority of my Python work, including 
 running, debugging, and testing code.
  
 Aside from the standard terminal-based IPython shell, the project also provides
  
 • A Mathematica-like HTML notebook for connecting to IPython through a web 
  
 browser (more on this later).
  
 • A Qt framework-based GUI console with inline plotting, multiline editing, and 
  
 syntax highlighting
  
 • An infrastructure for interactive parallel and distributed computing
  
 I will devote a chapter to IPython and how to get the most out of its features. I 
 strongly recommend using it while working through this book.
  
 Essential Python Libraries | 5",NA
SciPy,"SciPy is a collection of packages addressing a number of different standard problem 
 domains in scientific computing. Here is a sampling of the packages included:
  
 •
  scipy.integrate
 : numerical integration routines and differential equation solvers
  
 •
  scipy.linalg
 : linear algebra routines and matrix decompositions extending be-
  
 yond those provided in 
 numpy.linalg
 .
  
 •
  scipy.optimize
 : function optimizers (minimizers) and root finding algorithms
  
 •
  scipy.signal
 : signal processing tools
  
 •
  scipy.sparse
 : sparse matrices and sparse linear system solvers
  
 •
  scipy.special
 : wrapper around SPECFUN, a Fortran library implementing many 
  
 common mathematical functions, such as the gamma function
  
 •
  scipy.stats
 : standard continuous and discrete probability distributions (density 
 functions, samplers, continuous distribution functions), various statistical tests, 
 and more descriptive statistics
  
 •
  scipy.weave
 : tool for using inline C++ code to accelerate array computations
  
 Together NumPy and SciPy form a reasonably complete computational replacement 
 for much of MATLAB along with some of its add-on toolboxes.",NA
Installation and Setup,"Since everyone uses Python for different applications, there is no single solution for 
 setting up Python and required add-on packages. Many readers will not have a 
 complete scientific Python environment suitable for following along with this book, 
 so here I will give detailed instructions to get set up on each operating system. I 
 recommend using one of the following base Python distributions:
  
 • Enthought Python Distribution: a scientific-oriented Python distribution from 
 En-thought (
 http://www.enthought.com
 ). This includes EPDFree, a free base 
 scientific distribution (with NumPy, SciPy, matplotlib, Chaco, and IPython) and 
 EPD Full, a comprehensive suite of more than 100 scientific packages across 
 many domains. EPD Full is free for academic use but has an annual 
 subscription for non-academic users.
  
 • Python(x,y) (
 http://pythonxy.googlecode.com
 ): A free scientific-oriented Python 
  
 distribution for Windows.
  
 I will be using EPDFree for the installation guides, though you are welcome to take 
 another approach depending on your needs. At the time of this writing, EPD 
 includes Python 2.7, though this might change at some point in the future. After 
 installing, you will have the following packages installed and importable:
  
 6 | Chapter 1:Preliminaries
  
 www.it-ebooks.info",NA
Windows,"To get started on Windows, download the EPDFree installer from 
 http://www.en 
 thought.com
 , which should be an MSI installer named like 
 epd_free-7.3-1-win-x86.msi
 . 
 Run the installer and accept the default installation location 
 C:\Python27
 . If you had 
 previously installed Python in this location, you may want to delete it manually first 
 (or using Add/Remove Programs).
  
 Next, you need to verify that Python has been successfully added to the system path 
 and that there are no conflicts with any prior-installed Python versions. First, open 
 a command prompt by going to the Start Menu and starting the Command Prompt 
 ap-plication, also known as 
 cmd.exe
 . Try starting the Python interpreter by typing 
 python
 . You should see a message that matches the version of EPDFree you 
 installed:
  
 C:�sers\Wes>python 
  
 Python 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 14:30:37) on win32 Type 
 ""credits"", ""demo"" or ""enthought"" for more information.
  
 >>>
  
 Installation and Setup | 7",NA
Apple OS X,"To get started on OS X, you must first install Xcode, which includes Apple’s suite of 
 software development tools. The necessary component for our purposes is the gcc 
 C and C++ compiler suite. The Xcode installer can be found on the OS X install DVD 
 that came with your computer or downloaded from Apple directly.
  
 Once you’ve installed Xcode, launch the terminal (Terminal.app) by navigating to 
 Applications > Utilities
 . Type 
 gcc
  and press enter. You should hopefully see some-
 thing like:
  
 $ gcc 
  
 i686-apple-darwin10-gcc-4.2.1: no input files
  
 Now you need to install EPDFree. Download the installer which should be a disk 
 image named something like 
 epd_free-7.3-1-macosx-i386.dmg
 . Double-click the 
 .dmg
  
 file to mount it, then double-click the 
 .mpkg
  file inside to run the installer.
  
 When the installer runs, it automatically appends the EPDFree executable path to 
 your 
 .bash_profile
  file. This is located at 
 /Users/your_uname/.bash_profile
 :
  
 # Setting PATH for EPD_free-7.3-1 
  
 PATH=""/Library/Frameworks/Python.framework/Versions/Current/bin:${PATH}"" 
 export PATH
  
 Should you encounter any problems in the following steps, you’ll want to inspect 
 your 
 .bash_profile
  and potentially add the above directory to your path.
  
 Now, it’s time to install pandas. Execute this command in the terminal:
  
 $ sudo easy_install pandas 
  
 Searching for pandas 
  
 Reading http://pypi.python.org/simple/pandas/ 
  
 Reading http://pandas.pydata.org 
  
 Reading http://pandas.sourceforge.net 
  
 Best match: pandas 0.9.0 
  
 Downloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip 
 Processing pandas-0.9.0.zip 
  
 Writing /tmp/easy_install-H5mIX6/pandas-0.9.0/setup.cfg 
  
 Running pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-H5mIX6/ pandas-
 0.9.0/egg-dist-tmp-RhLG0z 
  
 Adding pandas 0.9.0 to easy-install.pth file
  
 Installed /Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/ site-
 packages/pandas-0.9.0-py2.7-macosx-10.5-i386.egg 
  
 Processing dependencies for pandas 
  
 Finished processing dependencies for pandas
  
 To verify everything is working, launch IPython in Pylab mode and test importing 
 pan-das then making a plot interactively:
  
 Installation and Setup | 9
  
 www.it-ebooks.info",NA
GNU/Linux,"Some, but not all, Linux distributions include sufficiently up-to-date 
 versions of all the required Python packages and can be installed 
 using the built-in package management tool like 
 apt
 . I detail setup 
 using EPD-Free as it's easily reproducible across distributions.
  
 Linux details will vary a bit depending on your Linux flavor, but here I give details 
 for Debian-based GNU/Linux systems like Ubuntu and Mint. Setup is similar to OS X 
 with the exception of how EPDFree is installed. The installer is a shell script that 
 must be executed in the terminal. Depending on whether you have a 32-bit or 64-
 bit system, you will either need to install the 
 x86
  (32-bit) or 
 x86_64
  (64-bit) 
 installer. You will then have a file named something similar to 
 epd_free-7.3-1-rh5-
 x86_64.sh
 . To install it, execute this script with bash:
  
 $ bash epd_free-7.3-1-rh5-x86_64.sh
  
 After accepting the license, you will be presented with a choice of where to put the 
 EPDFree files. I recommend installing the files in your home directory, say 
 /home/wesm/ epd
  (substituting your own username for 
 wesm
 ).
  
 Once the installer has finished, you need to add EPDFree’s 
 bin
  directory to your 
 $PATH
  variable. If you are using the bash shell (the default in Ubuntu, for example), 
 this means adding the following path addition in your 
 .bashrc
 :
  
 export PATH=/home/wesm/epd/bin:$PATH
  
 Obviously, substitute the installation directory you used for 
 /home/wesm/epd/
 . 
 After doing this you can either start a new terminal process or execute your 
 .bashrc
  
 again with 
 source ~/.bashrc
 .
  
 10 | Chapter 1:Preliminaries
  
 www.it-ebooks.info",NA
Python 2 and Python 3,"The Python community is currently undergoing a drawn-out transition from the 
 Python 2 series of interpreters to the Python 3 series. Until the appearance of 
 Python 3.0, all Python code was backwards compatible. The community decided 
 that in order to move the language forward, certain backwards incompatible 
 changes were necessary.
  
 I am writing this book with Python 2.7 as its basis, as the majority of the scientific 
 Python community has not yet transitioned to Python 3. The good news is that, 
 with a few exceptions, you should have no trouble following along with the book if 
 you happen to be using Python 3.2.",NA
Integrated Development Environments (IDEs),"When asked about my standard development environment, I almost always say 
 “IPy-thon plus a text editor”. I typically write a program and iteratively test and 
 debug each piece of it in IPython. It is also useful to be able to play around with 
 data interactively and visually verify that a particular set of data manipulations are 
 doing the right thing. Libraries like pandas and NumPy are designed to be easy-to-
 use in the shell.
  
 However, some will still prefer to work in an IDE instead of a text editor. They do 
 provide many nice “code intelligence” features like completion or quickly pulling up 
 the documentation associated with functions and classes. Here are some that you 
 can explore:
  
 • Eclipse with PyDev Plugin
  
 • Python Tools for Visual Studio (for Windows users)
  
 • PyCharm
  
 • Spyder
  
 • Komodo IDE
  
 Installation and Setup | 11",NA
Community and Conferences,"Outside of an Internet search, the scientific Python mailing lists are generally 
 helpful and responsive to questions. Some ones to take a look at are:
  
 • pydata: a Google Group list for questions related to Python for data analysis and 
  
 pandas
  
 • pystatsmodels: for statsmodels or pandas-related questions
  
 • numpy-discussion: for NumPy-related questions
  
 • scipy-user: for general SciPy or scientific Python questions
  
 I deliberately did not post URLs for these in case they change. They can be easily 
 located via Internet search.
  
 Each year many conferences are held all over the world for Python programmers. 
 PyCon and EuroPython are the two main general Python conferences in the United 
 States and Europe, respectively. SciPy and EuroSciPy are scientific-oriented Python 
 conferences where you will likely find many “birds of a feather” if you become more 
 involved with using Python for data analysis after reading this book.",NA
Navigating This Book,"If you have never programmed in Python before, you may actually want to start at 
 the 
 end
  of the book, where I have placed a condensed tutorial on Python syntax, 
 language features, and built-in data structures like tuples, lists, and dicts. These 
 things are con-sidered prerequisite knowledge for the remainder of the book.
  
 The book starts by introducing you to the IPython environment. Next, I give a short 
 introduction to the key features of NumPy, leaving more advanced NumPy use for 
 another chapter at the end of the book. Then, I introduce pandas and devote the 
 rest of the book to data analysis topics applying pandas, NumPy, and matplotlib (for 
 vis-ualization). I have structured the material in the most incremental way possible, 
 though there is occasionally some minor cross-over between chapters.
  
 Data files and related material for each chapter are hosted as a git repository on 
 GitHub:
  
 http://github.com/pydata/pydata-book
  
 I encourage you to download the data and use it to replicate the book’s code 
 examples and experiment with the tools presented in each chapter. I will happily 
 accept contri-butions, scripts, IPython notebooks, or any other materials you wish 
 to contribute to the book's repository for all to enjoy.
  
 12 | Chapter 1:Preliminaries",NA
Code Examples,"Most of the code examples in the book are shown with input and output as it would 
 appear executed in the IPython shell.
  
 In [5]: code 
  
 Out[5]: output
  
 At times, for clarity, multiple code examples will be shown side by side. These 
 should be read left to right and executed separately.
  
 In [5]: code         In [6]: code2 
  
 Out[5]: output       Out[6]: output2",NA
Data for Examples,"Data sets for the examples in each chapter are hosted in a repository on GitHub: 
 http: //github.com/pydata/pydata-book
 . You can download this data either by 
 using the git revision control command-line program or by downloading a zip file of 
 the repository from the website.
  
 I have made every effort to ensure that it contains everything necessary to 
 reproduce the examples, but I may have made some mistakes or omissions. If so, 
 please send me an e-mail: 
 wesmckinn@gmail.com
 .",NA
Import Conventions,"The Python community has adopted a number of naming conventions for 
 commonly-used modules:
  
 import numpy as np 
  
 import pandas as pd 
  
 import matplotlib.pyplot as plt
  
 This means that when you see 
 np.arange
 , this is a reference to the 
 arange
  function in 
 NumPy. This is done as it’s considered bad practice in Python software 
 development to import everything (
 from numpy import *
 ) from a large package like 
 NumPy.",NA
Jargon,"I’ll use some terms common both to programming and data science that you may 
 not be familiar with. Thus, here are some brief definitions:
  
 Munge/Munging/Wrangling 
  
 Describes the overall process of manipulating unstructured and/or messy data 
 into a structured or clean form. The word has snuck its way into the jargon of 
 many modern day data hackers. Munge rhymes with “lunge”.
  
 Navigating This Book | 13",NA
Acknowledgements,"It would have been difficult for me to write this book without the support of a large 
 number of people.
  
 On the O’Reilly staff, I’m very grateful for my editors Meghan Blanchette and Julie 
 Steele who guided me through the process. Mike Loukides also worked with me in 
 the proposal stages and helped make the book a reality.
  
 I received a wealth of technical review from a large cast of characters. In particular, 
 Martin Blais and Hugh White were incredibly helpful in improving the book’s exam-
 ples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-
 nando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, 
 and Stéfan van der Walt each reviewed one or more chapters, providing pointed 
 feedback from many different perspectives.
  
 I got many great ideas for examples and data sets from friends and colleagues in the 
 data community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow, 
 Kristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.
  
 I am of course indebted to the many leaders in the open source scientific Python 
 com-munity who’ve built the foundation for my development work and gave 
 encouragement while I was writing this book: the IPython core team (Fernando 
 Pérez, Brian Granger, Min Ragan-Kelly, Thomas Kluyver, and others), John Hunter, 
 Skipper Seabold, Travis Oliphant, Peter Wang, Eric Jones, Robert Kern, Josef 
 Perktold, Francesc Alted, Chris Fonnesbeck, and too many others to mention. 
 Several other people provided a great deal of support, ideas, and encouragement 
 along the way: Drew Conway, Sean Taylor, Giuseppe Paleologo, Jared Lander, David 
 Epstein, John Krowas, Joshua Bloom, Den Pilsworth, John Myles-White, and many 
 others I’ve forgotten.
  
 I’d also like to thank a number of people from my formative years. First, my former 
 AQR colleagues who’ve cheered me on in my pandas work over the years: Alex 
 Reyf-man, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni 
 Israelov, Michael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. 
 Lastly, my academic advisors Haynes Miller (MIT) and Mike West (Duke).
  
 On the personal side, Casey Dinkin provided invaluable day-to-day support during 
 the writing process, tolerating my highs and lows as I hacked together the final 
 draft on
  
 14 | Chapter 1:Preliminaries",NA
CHAPTER 2,NA,NA
Introductory Examples,"This book teaches you the Python tools to work productively with data. While 
 readers may have many different end goals for their work, the tasks required 
 generally fall into a number of different broad groups:
  
 Interacting with the outside world 
  
  
 Reading and writing with a variety of file formats and databases.
  
 Preparation 
  
 Cleaning, munging, combining, normalizing, reshaping, slicing and dicing, and 
 transforming data for analysis.
  
 Transformation 
  
 Applying mathematical and statistical operations to groups of data sets to 
 derive new data sets. For example, aggregating a large table by group variables.
  
 Modeling and computation 
  
 Connecting your data to statistical models, machine learning algorithms, or 
 other computational tools
  
 Presentation 
  
  
 Creating interactive or static graphical visualizations or textual 
 summaries
  
 In this chapter I will show you a few data sets and some things we can do with 
 them. These examples are just intended to pique your interest and thus will only be 
 explained at a high level. Don’t worry if you have no experience with any of these 
 tools; they will be discussed in great detail throughout the rest of the book. In the 
 code examples you’ll see input and output prompts like 
 In [15]:
 ; these are from the 
 IPython shell.",NA
1.usa.gov data from bit.ly,"In 2011, URL shortening service bit.ly partnered with the United States government 
 website 
 usa.gov
  to provide a feed of anonymous data gathered from users who 
 shorten links ending with 
 .gov
  or 
 .mil
 . As of this writing, in addition to providing a 
 live feed, hourly snapshots are available as downloadable text files.
 1
  
 17",NA
Counting Time Zones in Pure Python,"Suppose we were interested in the most often-occurring time zones in the data set 
 (the 
 tz
  field). There are many ways we could do this. First, let’s extract a list of time 
 zones again using a list comprehension:
  
 In [25]: time_zones = [rec['tz'] for rec in records]
  
 ---------------------------------------------------------------------------KeyError                                  
 Traceback (most recent call last) /home/wesm/book_scripts/whetting/<ipython> in 
 <module>()
  
 ----> 1 time_zones = [rec['tz'] for rec in records]
  
 KeyError: 'tz'
  
 Oops! Turns out that not all of the records have a time zone field. This is easy to 
 handle as we can add the check 
 if 'tz' in rec
  at the end of the list comprehension:
  
 In [26]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
  
 In [27]: time_zones[:10] 
  
 Out[27]: 
  
 [u'America/New_York',
  
  u'America/Denver',
  
  u'America/New_York',
  
  u'America/Sao_Paulo',
  
  u'America/New_York',
  
  u'America/New_York',
  
  u'Europe/Warsaw',
  
  u'',
  
  u'',
  
  u'']
  
 Just looking at the first 10 time zones we see that some of them are unknown 
 (empty). You can filter these out also but I’ll leave them in for now. Now, to produce 
 counts by time zone I’ll show two approaches: the harder way (using just the 
 Python standard library) and the easier way (using pandas). One way to do the 
 counting is to use a dict to store counts while we iterate through the time zones:
  
 def get_counts(sequence):
  
  
  counts = {}
  
 1.usa.gov data from bit.ly | 19
  
 www.it-ebooks.info",NA
Counting Time Zones with pandas,"The main pandas data structure is the 
 DataFrame
 , which you can think of as repre-
 senting a table or spreadsheet of data. Creating a DataFrame from the original set of 
 records is simple:
  
 In [289]: from pandas import DataFrame, Series
  
 In [290]: import pandas as pd
  
 In [291]: frame = DataFrame(records)
  
 In [292]: frame 
  
 Out[292]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 Int64Index: 3560 entries, 0 to 3559 
  
 Data columns: 
  
 _heartbeat_    120  non-null values 
  
 a              3440  non-null values 
  
 al             3094  non-null values 
  
 c              2919  non-null values 
  
 cy             2919  non-null values 
  
 g              3440  non-null values 
  
 gr             2919  non-null values 
  
 h              3440  non-null values 
  
 hc             3440  non-null values 
  
 hh             3440  non-null values 
  
 kw             93  non-null values 
  
 l              3440  non-null values 
  
 ll             2919  non-null values 
  
 nk             3440  non-null values 
  
 r              3440  non-null values 
  
 t              3440  non-null values 
  
 tz             3440  non-null values
  
 1.usa.gov data from bit.ly | 21
  
 www.it-ebooks.info",NA
MovieLens 1M Data Set,"GroupLens Research (
 http://www.grouplens.org/node/73
 ) provides a number of 
 collec-tions of movie ratings data collected from users of MovieLens in the late 
 1990s and
  
 26 | Chapter 2:Introductory Examples",NA
Measuring rating disagreement,"Suppose you wanted to find the movies that are most divisive between male and 
 female viewers. One way is to add a column to 
 mean_ratings
  containing the 
 difference in means, then sort by that:
  
 In [352]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']
  
 Sorting by 
 'diff'
  gives us the movies with the greatest rating difference and which 
 were preferred by women:
  
 In [353]: sorted_by_diff = mean_ratings.sort_index(by='diff')
  
 In [354]: sorted_by_diff[:15] 
  
 Out[354]: 
  
 gender                                        F         M      diff Dirty Dancing (1987)                   
 3.790378  2.959596 -0.830782 Jumpin' Jack Flash (1986)              3.254717  
 2.578358 -0.676359 Grease (1978)                          3.975265  3.367041 -
 0.608224 Little Women (1994)                    3.870588  3.321739 -0.548849 Steel 
 Magnolias (1989)                 3.901734  3.365957 -0.535777 Anastasia (1997)                       
 3.800000  3.281609 -0.518391 Rocky Horror Picture Show, The (1975)  
 3.673016  3.160131 -0.512885 Color Purple, The (1985)               4.158192  
 3.659341 -0.498851 Age of Innocence, The (1993)           3.827068  3.339506 -
 0.487561 Free Willy (1993)                      2.921348  2.438776 -0.482573 French 
 Kiss (1995)                     3.535714  3.056962 -0.478752 Little Shop of Horrors, 
 The (1960)     3.650000  3.179688 -0.470312 Guys and Dolls (1955)                  
 4.051724  3.583333 -0.468391 Mary Poppins (1964)                    4.197740  
 3.730594 -0.467147 Patch Adams (1998)                     3.473282  3.008746 -
 0.464536
  
 30 | Chapter 2:Introductory Examples
  
 www.it-ebooks.info",NA
US Baby Names 1880-2010,"The United States Social Security Administration (SSA) has made available data on 
 the frequency of baby names from 1880 through the present. Hadley Wickham, an 
 author of several popular R packages, has often made use of this data set in 
 illustrating data manipulation in R.
  
 In [4]: names.head(10) 
  
 Out[4]:
  
  
  name sex  births  year 
  
 0       Mary   F    7065  1880 
  
 1       Anna   F    2604  1880 
  
 2       Emma   F    2003  1880 
  
 3  Elizabeth   F    1939  1880 
  
 4     Minnie   F    1746  1880 
  
 5   Margaret   F    1578  1880 
  
 6        Ida   F    1472  1880 
  
 7      Alice   F    1414  1880 
  
 8     Bertha   F    1320  1880 
  
 9      Sarah   F    1288  1880
  
 There are many things you might want to do with the data set:
  
 • Visualize the proportion of babies given a particular name (your own, or another 
  
 name) over time.
  
 • Determine the relative rank of a name.
  
 • Determine the most popular names in each year or the names with largest 
 increases 
  
 or decreases.
  
 • Analyze trends in names: vowels, consonants, length, overall diversity, changes 
 in 
  
 spelling, first and last letters
  
 • Analyze external sources of trends: biblical names, celebrities, demographic 
  
 changes
  
 Using the tools we’ve looked at so far, most of these kinds of analyses are very 
 straight-forward, so I will walk you through many of them. I encourage you to 
 download and explore the data yourself. If you find an interesting pattern in the 
 data, I would love to hear about it.
  
 As of this writing, the US Social Security Administration makes available data files, 
 one per year, containing the total number of births for each sex/name combination. 
 The raw archive of these files can be obtained here:
  
 http://www.ssa.gov/oact/babynames/limits.html
  
 In the event that this page has been moved by the time you’re reading this, it can 
 most likely be located again by Internet search. After downloading the “National 
 data” file 
 names.zip
  and unzipping it, you will have a directory containing a series of 
 files like 
 yob1880.txt
 . I use the UNIX 
 head
  command to look at the first 10 lines of 
 one of the files (on Windows, you can use the 
 more
  command or open it in a text 
 editor):
  
 32 | Chapter 2:Introductory Examples",NA
Analyzing Naming Trends,"With the full data set and Top 1,000 data set in hand, we can start analyzing various 
 naming trends of interest. Splitting the Top 1,000 names into the boy and girl 
 portions is easy to do first:
  
 In [383]: boys = top1000[top1000.sex == 'M']
  
 In [384]: girls = top1000[top1000.sex == 'F']
  
 Simple time series, like the number of Johns or Marys for each year can be plotted 
 but require a bit of munging to be a bit more useful. Let’s form a pivot table of the 
 total number of births by year and name:
  
 In [385]: total_births = top1000.pivot_table('births', rows='year', cols='name',
  
  .....:                                    
 aggfunc=sum)
  
 Now, this can be plotted for a handful of names using DataFrame’s 
 plot
  method:
  
 In [386]: total_births 
  
 Out[386]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 Int64Index: 131 entries, 1880 to 2010 
  
 Columns: 6865 entries, Aaden to Zuri 
  
 dtypes: float64(6865)
  
 In [387]: subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]
  
 In [388]: subset.plot(subplots=True, figsize=(12, 10), grid=False,
  
  .....:             
 title=""Number of births per year"")
  
 36 | Chapter 2:Introductory Examples
  
 www.it-ebooks.info",NA
Conclusions and The Path Ahead,"The examples in this chapter are rather simple, but they’re here to give you a bit of 
 a flavor of what sorts of things you can expect in the upcoming chapters. The focus 
 of this book is on 
 tools
  as opposed to presenting more sophisticated analytical 
 methods. Mastering the techniques in this book will enable you to implement your 
 own analyses (assuming you know what you want to do!) in short order.
  
 Conclusions and The Path Ahead | 43",NA
CHAPTER 3,NA,NA
IPython: An Interactive ,NA,NA
Computing and ,NA,NA
Development ,NA,NA
Environment,"Act without doing; work without effort. Think of the small as large and the few as 
 many. Confront the difficult while it is still easy; accomplish the great task by a 
 series of small acts.
  
 —Laozi
  
 People often ask me, “What is your Python development environment?” My answer 
 is almost always the same, “IPython and a text editor”. You may choose to 
 substitute an Integrated Development Environment (IDE) for a text editor in order 
 to take advantage of more advanced graphical tools and code completion 
 capabilities. Even if so, I strongly recommend making IPython an important part of 
 your workflow. Some IDEs even provide IPython integration, so it’s possible to get 
 the best of both worlds.
  
 The 
 IPython
  project began in 2001 as Fernando Pérez’s side project to make a 
 better interactive Python interpreter. In the subsequent 11 years it has grown into 
 what’s widely considered one of the most important tools in the modern scientific 
 Python computing stack. While it does not provide any computational or data 
 analytical tools by itself, IPython is designed from the ground up to maximize your 
 productivity in both interactive computing and software development. It 
 encourages an 
 execute-explore 
 workflow instead of the typical 
 edit-compile-run
  
 workflow of many other programming languages. It also provides very tight 
 integration with the operating system’s shell and file system. Since much of data 
 analysis coding involves exploration, trial and error, and iteration, IPython will, in 
 almost all cases, help you get the job done faster.
  
 Of course, the IPython project now encompasses a great deal more than just an en-
 hanced, interactive Python shell. It also includes a rich GUI console with inline 
 plotting, a web-based interactive notebook format, and a lightweight, fast parallel 
 computing engine. And, as with so many other tools designed for and by 
 programmers, it is highly customizable. I’ll discuss some of these features later in 
 the chapter.",NA
IPython Basics,"You can launch IPython on the command line just like launching the regular Python 
 interpreter except with the 
 ipython
  command:
  
 $ 
 ipython 
  
 Python 2.7.2 (default, May 27 2012, 21:26:12) 
  
 Type ""copyright"", ""credits"" or ""license"" for more information.
  
 IPython 0.12 -- An enhanced Interactive Python.
  
 ?         -> Introduction and overview of IPython's features.
  
 %quickref -> Quick reference.
  
 help      -> Python's own help system.
  
 object?   -> Details about 'object', use 'object??' for extra details.
  
 In [1]: a = 5
  
 In [2]: a 
  
 Out[2]: 5
  
 You can execute arbitrary Python statements by typing them in and pressing 
 <return>
 . When typing just a variable into IPython, it renders a string 
 representation of the object:
  
 In [542]: data = {i : randn() for i in range(7)}
  
 In [543]: data 
  
 Out[543]: 
  
 {0: 0.6900018528091594,
  
  1: 1.0015434424937888,
  
  2: -0.5030873913603446,
  
  3: -0.6222742250596455,
  
  4: -0.9211686080130108,
  
  5: -0.726213492660829,
  
  6: 0.2228955458351768}
  
 46 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Tab Completion,"On the surface, the IPython shell looks like a cosmetically slightly-different 
 interactive Python interpreter. Users of Mathematica may find the enumerated 
 input and output prompts familiar. One of the major improvements over the 
 standard Python shell is 
 tab completion
 , a feature common to most interactive data 
 analysis environments. While entering expressions in the shell, pressing 
 <Tab>
  will 
 search the namespace for any variables (objects, functions, etc.) matching the 
 characters you have typed so far:
  
 In [1]: an_apple = 27
  
 In [2]: an_example = 42
  
 In [3]: an
 <Tab> 
  
 an_apple    and         an_example  any
  
 In this example, note that IPython displayed both the two variables I defined as 
 well as the Python keyword 
 and
  and built-in function 
 any
 . Naturally, you can also 
 complete methods and attributes on any object after typing a period:
  
 In [3]: b = [1, 2, 3]
  
 In [4]: b.
 <Tab> 
  
 b.append   b.extend   b.insert   b.remove   b.sort b.count    
 b.index    b.pop      b.reverse
  
 The same goes for modules:
  
 In [1]: import datetime
  
 In [2]: datetime.
 <Tab> 
  
 datetime.date           datetime.MAXYEAR        datetime.timedelta 
 datetime.datetime       datetime.MINYEAR        datetime.tzinfo 
 datetime.datetime_CAPI  datetime.time
  
 IPython Basics | 47",NA
Introspection,"Using a question mark (
 ?
 ) before or after a variable will display some general 
 informa-tion about the object:
  
 In [545]: b?
  
 Type:       list 
  
 String Form:[1, 2, 3] 
  
 Length:     3 
  
 Docstring: 
  
 list() -> new empty list 
  
 list(iterable) -> new list initialized from iterable's items
  
 This is referred to as 
 object introspection
 . If the object is a function or instance 
 method, the docstring, if defined, will also be shown. Suppose we’d written the 
 following func-tion:
  
 def add_numbers(a, b):
  
  """"""
  
  Add two numbers together
  
  Returns
  
  -------
  
  the_sum : type of arguments
  
 48 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
The %run Command,"Any file can be run as a Python program inside the environment of your IPython 
 session using the 
 %run
  command. Suppose you had the following simple script 
 stored in 
 ipy thon_script_test.py
 :
  
 def f(x, y, z):
  
  
  return (x + y) / z
  
 a = 5
  
 IPython Basics | 49
  
 www.it-ebooks.info",NA
Executing Code from the Clipboard,"A quick-and-dirty way to execute code in IPython is via pasting from the clipboard. 
 This might seem fairly crude, but in practice it is very useful. For example, while de-
 veloping a complex or time-consuming application, you may wish to execute a 
 script piece by piece, pausing at each stage to examine the currently loaded data 
 and results. Or, you might find a code snippet on the Internet that you want to run 
 and play around with, but you’d rather not create a new 
 .py
  file for it.
  
 50 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Keyboard Shortcuts,"IPython has many keyboard shortcuts for navigating the prompt (which will be 
 familiar to users of the Emacs text editor or the UNIX bash shell) and interacting 
 with the shell’s command history (see later section). 
 Table 3-1
  summarizes some of 
 the most commonly used shortcuts. See 
 Figure 3-1
  for an illustration of a few of 
 these, such as cursor move-ment.
  
  
 Figure 3-1. Illustration of some of IPython’s keyboard shortcuts
  
 52 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
Exceptions and Tracebacks,"If an exception is raised while 
 %run
 -ing a script or executing any statement, 
 IPython will by default print a full call stack trace (traceback) with a few lines of 
 context around the position at each point in the stack.
  
 In [553]: %run ch03/ipython_bug.py
  
 ---------------------------------------------------------------------------AssertionError                            
 Traceback (most recent call last) 
 /home/wesm/code/ipython/IPython/utils/py3compat.pyc in execfile(fname, *where)
  
  
 176             else:
  
  
  177                 filename = fname
  
 --> 178             __builtin__.execfile(filename, *where) 
  
 book_scripts/ch03/ipython_bug.py in <module>()
  
  
  
  13     throws_an_exception()
  
  
  
  14 
  
 ---> 15 calling_things() 
  
 book_scripts/ch03/ipython_bug.py in calling_things()
  
  
  
  11 def calling_things():
  
  
  
  12     works_fine()
  
 ---> 13     throws_an_exception()
  
  
  
  14 
  
  
  
  15 calling_things() 
  
 book_scripts/ch03/ipython_bug.py in throws_an_exception()
  
  
   
  7     a = 5
  
  
   
  8     b = 6
  
 ----> 9     assert(a + b == 10)
  
  
  
  10 
  
  
  
  11 def calling_things(): 
  
 AssertionError:
  
 IPython Basics | 53
  
 www.it-ebooks.info",NA
Magic Commands,"IPython has many special commands, known as “magic” commands, which are de-
 signed to faciliate common tasks and enable you to easily control the behavior of 
 the IPython system. A magic command is any command prefixed by the the percent 
 symbol 
 %
 . For example, you can check the execution time of any Python statement, 
 such as a matrix multiplication, using the 
 %timeit
  magic function (which will be 
 discussed in more detail later):
  
 In [554]: a = np.random.randn(100, 100)
  
 In [555]: %timeit np.dot(a, a) 
  
 10000 loops, best of 3: 69.1 us per loop
  
 Magic commands can be viewed as command line programs to be run within the 
 IPy-thon system. Many of them have additional “command line” options, which can 
 all be viewed (as you might expect) using 
 ?
 :
  
 In [1]: %reset?
  
 Resets the namespace by removing all names defined by the user.
  
 Parameters---------- -f : force reset without asking for 
 confirmation.
  
  -s : 'Soft' reset: Only clears your namespace, leaving history intact. References to 
 objects may be kept. By default (without this option), we do a 'hard' reset, giving you 
 a new session and removing all references to objects from the current session.
  
 Examples
  
 --------
  
 In [6]: a = 1
  
 In [7]: a 
  
 Out[7]: 1
  
 In [8]: 'a' in _ip.user_ns 
  
 Out[8]: True
  
 In [9]: %reset -f
  
 In [1]: 'a' in _ip.user_ns 
  
 Out[1]: False
  
 54 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
Qt-based Rich GUI Console,"The IPython team has developed a Qt framework-based GUI console, designed to 
 wed the features of the terminal-only applications with the features provided by a 
 rich text widget, like embedded images, multiline editing, and syntax highlighting. If 
 you have either PyQt or PySide installed, the application can be launched with 
 inline plotting by running this on the command line:
  
 ipython qtconsole --pylab=inline
  
 The Qt console can launch multiple IPython processes in tabs, enabling you to 
 switch between tasks. It can also share a process with the IPython HTML Notebook 
 applica-tion, which I’ll highlight later.
  
 IPython Basics | 55
  
 www.it-ebooks.info",NA
Matplotlib Integration and Pylab Mode,"Part of why IPython is so widely used in scientific computing is that it is designed 
 as a companion to libraries like matplotlib and other GUI toolkits. Don’t worry if 
 you have never used matplotlib before; it will be discussed in much more detail 
 later in this book. If you create a matplotlib plot window in the regular Python shell, 
 you’ll be sad to find that the GUI event loop “takes control” of the Python session 
 until the plot window is closed. That won’t work for interactive data analysis and 
 visualization, so IPython has
  
 56 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Using the Command History,"IPython maintains a small on-disk database containing the text of each command 
 that you execute. This serves various purposes:
  
 • Searching, completing, and executing previously-executed commands with mini-
  
 mal typing
  
 • Persisting the command history between sessions.
  
 • Logging the input/output history to a file",NA
Searching and Reusing the Command History,"Being able to search and execute previous commands is, for many people, the most 
 useful feature. Since IPython encourages an iterative, interactive code development 
 workflow, you may often find yourself repeating the same commands, such as a 
 %run 
 command or some other code snippet. Suppose you had run:
  
 In[7]: %run first/second/third/data_script.py
  
 and then explored the results of the script (assuming it ran successfully), only to 
 find that you made an incorrect calculation. After figuring out the problem and 
 modifying 
 data_script.py
 , you can start typing a few letters of the 
 %run
  command 
 then press either the 
 <Ctrl-P>
  key combination or the 
 <up arrow>
  key. This will 
 search the command history for the first prior command matching the letters you 
 typed. Pressing either 
 <Ctrl-P>
  or 
 <up arrow>
  multiple times will continue to search 
 through the history. If you pass over the command you wish to execute, fear not. 
 You can move 
 forward 
 through the command history by pressing either 
 <Ctrl-N>
  or 
 <down arrow>
 . After doing this a few times you may start pressing these keys 
 without thinking!
  
 Using 
 <Ctrl-R>
  gives you the same partial incremental searching capability provided 
 by the 
 readline
  used in UNIX-style shells, such as the bash shell. On Windows, 
 read 
 line
  functionality is emulated by IPython. To use this, press 
 <Ctrl-R>
  then type a few 
 characters contained in the input line you want to search for:
  
 In [1]: a_command = foo(x, y, z)
  
 (reverse-i-search)`com': a_command = foo(x, y, z)
  
 Pressing 
 <Ctrl-R>
  will cycle through the history for each line matching the 
 characters you’ve typed.",NA
Input and Output Variables,"Forgetting to assign the result of a function call to a variable can be very annoying. 
 Fortunately, IPython stores references to 
 both
  the input (the text that you type) and 
 output (the object that is returned) in special variables. The previous two outputs 
 are stored in the 
 _
  (one underscore) and 
 __
  (two underscores) variables, 
 respectively:
  
 58 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Logging the Input and Output,"IPython is capable of logging the entire console session including input and output. 
 Logging is turned on by typing 
 %logstart
 :
  
 In [3]: %logstart 
  
 Activating auto-logging. Current session state plus future input saved.
  
 Filename       : ipython_log.py 
  
 Mode           : rotate 
  
 Output logging : False 
  
 Raw input log  : False
  
 Using the Command History | 59",NA
Interacting with the Operating System,"Another important feature of IPython is that it provides very strong integration 
 with the operating system shell. This means, among other things, that you can 
 perform most standard command line actions as you would in the Windows or 
 UNIX (Linux, OS X) shell without having to exit IPython. This includes executing 
 shell commands, changing directories, and storing the results of a command in a 
 Python object (list or string).
  
 There are also simple shell command aliasing and directory bookmarking features.
  
 See 
 Table 3-3
  for a summary of magic functions and syntax for calling shell 
 commands. I’ll briefly visit these features in the next few sections.
  
 Table 3-3. IPython system-related commands
  
 Command
  
 Description
  
 !cmd 
  
 output = !cmd args 
  
 %alias 
 alias_name cmd 
 %bookmark 
  
 %cd
 directory 
  
 %pwd 
  
 %pushd
 directory 
  
 %popd 
  
 %dirs 
  
 %dhist 
  
 %env
  
 Execute 
 cmd
  in the system shell 
  
 Run 
 cmd
  and store the stdout in 
 output 
  
 Define an alias for a system (shell) command 
  
 Utilize IPython’s directory bookmarking 
 system 
  
 Change system working directory to passed 
 directory 
  
 Return the current system working directory 
  
 Place current directory on stack and change 
 to target directory Change to directory 
 popped off the top of the stack 
  
 Return a list containing the current directory 
 stack 
  
 Print the history of visited directories 
  
 Return the system environment variables as a 
 dict",NA
Shell Commands and Aliases,"Starting a line in IPython with an exclamation point 
 !
 , or bang, tells IPython to 
 execute everything after the bang in the system shell. This means that you can 
 delete files (using 
 rm
  or 
 del
 , depending on your OS), change directories, or execute 
 any other process. It’s even possible to start processes that take control away from 
 IPython, even another Python interpreter:
  
 60 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Directory Bookmark System,"IPython has a simple directory bookmarking system to enable you to save aliases 
 for common directories so that you can jump around very easily. For example, I’m 
 an avid user of Dropbox, so I can define a bookmark to make it easy to change 
 directories to my Dropbox:
  
 In [6]: %bookmark db /home/wesm/Dropbox/
  
 Once I’ve done this, when I use the 
 %cd
  magic, I can use any bookmarks I’ve defined
  
 In [7]: cd db 
  
 (bookmark:db) -> /home/wesm/Dropbox/ 
  
 /home/wesm/Dropbox
  
 If a bookmark name conflicts with a directory name in your current working 
 directory, you can use the 
 -b
  flag to override and use the bookmark location. Using 
 the 
 -l
  option with 
 %bookmark
  lists all of your bookmarks:
  
 In [8]: %bookmark -l 
  
 Current bookmarks: 
  
 db -> /home/wesm/Dropbox/
  
 Bookmarks, unlike aliases, are automatically persisted between IPython sessions.",NA
Software Development Tools,"In addition to being a comfortable environment for interactive computing and data 
 exploration, IPython is well suited as a software development environment. In data 
 analysis applications, it’s important first to have 
 correct
  code. Fortunately, IPython 
 has closely integrated and enhanced the built-in Python 
 pdb
  debugger. Secondly 
 you want your code to be 
 fast
 . For this IPython has easy-to-use code timing and 
 profiling tools. I will give an overview of these tools in detail here.",NA
Interactive Debugger,"IPython’s debugger enhances 
 pdb
  with tab completion, syntax highlighting, and 
 context for each line in exception tracebacks. One of the best times to debug code is 
 right after an error has occurred. The 
 %debug
  command, when entered 
 immediately after an ex-ception, invokes the “post-mortem” debugger and drops 
 you into the stack frame where the exception was raised:
  
 In [2]: run ch03/ipython_bug.py
  
 ---------------------------------------------------------------------------AssertionError                            
 Traceback (most recent call last) /home/wesm/book_scripts/ch03/ipython_bug.py in 
 <module>()
  
  
  13     throws_an_exception()
  
  
  14
  
 ---> 15 calling_things()
  
 /home/wesm/book_scripts/ch03/ipython_bug.py in calling_things()
  
 62 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
Timing Code: %time and %timeit,"For larger-scale or longer-running data analysis applications, you may wish to 
 measure the execution time of various components or of individual statements or 
 function calls. You may want a report of which functions are taking up the most 
 time in a complex process. Fortunately, IPython enables you to get this information 
 very easily while you are developing and testing your code.
  
 Timing code by hand using the built-in 
 time
  module and its functions 
 time.clock
  and 
 time.time
  is often tedious and repetitive, as you must write the same uninteresting 
 boilerplate code:
  
 import time 
  
 start = time.time() 
  
 for i in range(iterations):
  
  
  # some code to run here 
  
 elapsed_per = (time.time() - start) / iterations
  
 Since this is such a common operation, IPython has two magic functions 
 %time
  and 
 %timeit
  to automate this process for you. 
 %time
  runs a statement once, reporting 
 the total execution time. Suppose we had a large list of strings and we wanted to 
 compare different methods of selecting all strings starting with a particular prefix. 
 Here is a simple list of 700,000 strings and two identical methods of selecting only 
 the ones that start with 
 'foo'
 :
  
 # a very large list of strings 
  
 strings = ['foo', 'foobar', 'baz', 'qux',
  
  
  'python', 'Guido Van Rossum'] * 100000
  
 method1 = [x for x in strings if x.startswith('foo')]
  
 method2 = [x for x in strings if x[:3] == 'foo']
  
 It looks like they should be about the same performance-wise, right? We can check 
 for sure using 
 %time
 :
  
 In [561]: %time method1 = [x for x in strings if x.startswith('foo')] CPU times: 
 user 0.19 s, sys: 0.00 s, total: 0.19 s 
  
 Wall time: 0.19 s
  
 In [562]: %time method2 = [x for x in strings if x[:3] == 'foo'] CPU times: 
 user 0.09 s, sys: 0.00 s, total: 0.09 s 
  
 Wall time: 0.09 s
  
 The 
 Wall time
  is the main number of interest. So, it looks like the first method takes 
 more than twice as long, but it’s not a very precise measurement. If you try 
 %time
 -
 ing those statements multiple times yourself, you’ll find that the results are 
 somewhat variable. To get a more precise measurement, use the 
 %timeit
  magic 
 function. Given an arbitrary statement, it has a heuristic to run a statement multiple 
 times to produce a fairly accurate average runtime.
  
 In [563]: %timeit [x for x in strings if x.startswith('foo')] 10 loops, best of 
 3: 159 ms per loop
  
 Software Development Tools | 67
  
 www.it-ebooks.info",NA
Basic Profiling: %prun and %run -p,"Profiling code is closely related to timing code, except it is concerned with 
 determining 
 where
  time is spent. The main Python profiling tool is the 
 cProfile
  
 module, which is not specific to IPython at all. 
 cProfile
  executes a program or any 
 arbitrary block of code while keeping track of how much time is spent in each 
 function.
  
 A common way to use 
 cProfile
  is on the command line, running an entire program 
 and outputting the aggregated time per function. Suppose we had a simple script 
 which does some linear algebra in a loop (computing the maximum absolute 
 eigenvalues of a series of 
 100 x 100
  matrices):
  
 import numpy as np 
  
 from numpy.linalg import eigvals
  
 def run_experiment(niter=100):
  
  
  K = 100
  
  
  results = []
  
  
  for _ in xrange(niter):
  
  
  
  mat = np.random.randn(K, K)
  
  
  
  max_eigenvalue = np.abs(eigvals(mat)).max()
  
  
  results.append(max_eigenvalue)
  
  
  return results 
  
 some_results = run_experiment() 
  
 print 'Largest one we saw: %s' % np.max(some_results)
  
 68 | Chapter 3:IPython: An Interactive Computing and Development Environment",NA
Profiling a Function Line-by-Line,"In some cases the information you obtain from 
 %prun
  (or another 
 cProfile
 -based 
 profile method) may not tell the whole story about a function’s execution time, or it 
 may be so complex that the results, aggregated by function name, are hard to 
 interpret. For this case, there is a small library called 
 line_profiler
  (obtainable via 
 PyPI or one of the package management tools). It contains an IPython extension 
 enabling a new magic function 
 %lprun
  that computes a line-by-line-profiling of one 
 or more functions. You can enable this extension by modifying your IPython 
 configuration (see the IPython documentation or the section on configuration later 
 in this chapter) to include the following line:
  
 # A list of dotted module names of IPython extensions to load. 
 c.TerminalIPythonApp.extensions = ['line_profiler']
  
 line_profiler
  can be used programmatically (see the full documentation), but it is 
 perhaps most powerful when used interactively in IPython. Suppose you had a 
 module 
 prof_mod
  with the following code doing some NumPy array operations:
  
 from numpy.random import randn
  
 def add_and_sum(x, y):
  
  added = x + y
  
  summed = added.sum(axis=1)
  
  return summed
  
 def call_function():
  
  x = randn(1000, 1000)
  
  y = randn(1000, 1000)
  
  return add_and_sum(x, y)
  
 If we wanted to understand the performance of the 
 add_and_sum
  function, 
 %prun
  
 gives us the following:
  
 In [569]: %run prof_mod
  
 In [570]: x = randn(3000, 3000)
  
 In [571]: y = randn(3000, 3000)
  
 In [572]: %prun add_and_sum(x, y)
  
   
  
  4 function calls in 0.049 seconds
  
  
  Ordered by: internal time
  
  
  ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  
  
  1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)
  
 70 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
IPython HTML Notebook,"Starting in 2011, the IPython team, led by Brian Granger, built a web 
 technology−based interactive computational document format that is commonly 
 known as the IPython Notebook. It has grown into a wonderful tool for interactive 
 computing and an ideal medium for reproducible research and teaching. I’ve used it 
 while writing most of the examples in the book; I encourage you to make use of it, 
 too.
  
 It has a JSON-based 
 .ipynb
  document format that enables easy sharing of code, 
 output, and figures. Recently in Python conferences, a popular approach for 
 demonstrations has been to use the notebook and post the 
 .ipynb
  files online 
 afterward for everyone to play with.
  
 The notebook application runs as a lightweight server process on the command line. 
 It can be started by running:
  
 $ ipython notebook --pylab=inline 
  
 [NotebookApp] Using existing profile dir: u'/home/wesm/.config/ipython/profile_default' 
 [NotebookApp] Serving notebooks from /home/wesm/book_scripts 
  
 [NotebookApp] The IPython Notebook is running at: http://127.0.0.1:8888/ 
  
 [NotebookApp] Use Control-C to stop this server and shut down all kernels.
  
 On most platforms, your primary web browser will automatically open up to the 
 note-book dashboard. In some cases you may have to navigate to the listed URL. 
 From there, you can create a new notebook and start exploring.
  
 Since you use the notebook inside a web browser, the server process can run 
 anywhere. You can even securely connect to notebooks running on cloud service 
 providers like Amazon EC2. As of this writing, a new project NotebookCloud 
 (
 http://notebookcloud .appspot.com
 ) makes it easy to launch notebooks on EC2.",NA
Tips for Productive Code Development ,NA,NA
Using IPython,"Writing code in a way that makes it easy to develop, debug, and ultimately 
 use
  
 inter-actively may be a paradigm shift for many users. There are procedural details 
 like code reloading that may require some adjustment as well as coding style 
 concerns.
  
 As such, most of this section is more of an art than a science and will require some 
 experimentation on your part to determine a way to write your Python code that is 
 effective and productive for you. Ultimately you want to structure your code in a 
 way that makes it easy to use iteratively and to be able to explore the results of 
 running a program or function as effortlessly as possible. I have found software 
 designed with",NA
Reloading Module Dependencies,"In Python, when you type 
 import some_lib
 , the code in 
 some_lib
  is executed and all 
 the variables, functions, and imports defined within are stored in the newly created 
 some_lib
  module namespace. The next time you type 
 import some_lib
 , you will get a 
 reference to the existing module namespace. The potential difficulty in interactive 
 code development in IPython comes when you, say, 
 %run
  a script that depends on 
 some other module where you may have made changes. Suppose I had the 
 following code in 
 test_script.py
 :
  
 import some_lib
  
 x = 5 
  
 y = [1, 2, 3, 4] 
  
 result = some_lib.get_answer(x, y)
  
 If you were to execute 
 %run test_script.py
  then modify 
 some_lib.py
 , the next time you 
 execute 
 %run test_script.py
  you will still get the 
 old version
  of 
 some_lib
  because of 
 Python’s “load-once” module system. This behavior differs from some other data 
 anal-ysis environments, like MATLAB, which automatically propagate code 
 changes.
 1
  To cope with this, you have a couple of options. The first way is to use 
 Python's built-in 
 reload
  function, altering 
 test_script.py
  to look like the following:
  
 import some_lib 
  
 reload(some_lib)
  
 x = 5 
  
 y = [1, 2, 3, 4] 
  
 result = some_lib.get_answer(x, y)
  
 This guarantees that you will get a fresh copy of 
 some_lib
  every time you run 
 test_script.py
 . Obviously, if the dependencies go deeper, it might be a bit tricky to be 
 inserting usages of 
 reload
  all over the place. For this problem, IPython has a special 
 dreload
  function (
 not
  a magic function) for “deep” (recursive) reloading of modules. 
 If I were to run 
 import some_lib
  then type 
 dreload(some_lib)
 , it will attempt to reload 
 some_lib
  as well as all of its dependencies. This will not work in all cases, 
 unfortunately, but when it does it beats having to restart IPython.",NA
Code Design Tips,"There’s no simple recipe for this, but here are some high-level principles I have 
 found effective in my own work.
  
 1. Since a module or package may be imported in many different places in a particular program, 
 Python caches a module’s code the first time it is imported rather than executing the code in the 
 module every time. Otherwise, modularity and good code organization could potentially cause 
 inefficiency in an application.
  
 74 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
Advanced IPython Features,NA,NA
Making Your Own Classes IPython-friendly,"IPython makes every effort to display a console-friendly string representation of 
 any object that you inspect. For many objects, like dicts, lists, and tuples, the built-in 
 pprint
  module is used to do the nice formatting. In user-defined classes, however, 
 you have to generate the desired string output yourself. Suppose we had the 
 following sim-ple class:
  
 class Message:
  
  
  def __init__(self, msg):
  
  
  
  self.msg = msg
  
 If you wrote this, you would be disappointed to discover that the default output for 
 your class isn’t very nice:
  
 In [576]: x = Message('I have a secret')
  
 In [577]: x 
  
 Out[577]: <__main__.Message instance at 0x60ebbd8>
  
 IPython takes the string returned by the 
 __repr__
  magic method (by doing 
 output = 
 repr(obj)
 ) and prints that to the console. Thus, we can add a simple 
 __repr__
  method 
 to the above class to get a more helpful output:
  
 class Message:
  
  
  def __init__(self, msg):
  
  
  
  self.msg = msg
  
  def __repr__(self):
  
  
  return 'Message: %s' % self.msg
  
 In [579]: x = Message('I have a secret')
  
 In [580]: x 
  
 Out[580]: Message: I have a secret
  
 76 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
Profiles and Configuration,"Most aspects of the appearance (colors, prompt, spacing between lines, etc.) and be-
 havior of the IPython shell are configurable through an extensive configuration 
 system.
  
 Here are some of the things you can do via configuration:
  
 • Change the color scheme
  
 • Change how the input and output prompts look, or remove the blank line after 
  
 Out
  and before the next 
 In
  prompt
  
 • Change how the input and output prompts look
  
 • Execute an arbitrary list of Python statements. These could be imports that you 
 use all the time or anything else you want to happen each time you launch 
 IPython
  
 • Enable IPython extensions, like the 
 %lprun
  magic in 
 line_profiler
  
 • Define your own magics or system aliases
  
 All of these configuration options are specified in a special 
 ipython_config.py
  file 
 which will be found in the 
 ~/.config/ipython/
  directory on UNIX-like systems and 
 %HOME %/.ipython/
  directory on Windows. Where your home directory is depends 
 on your system. Configuration is performed based on a particular 
 profile
 . When you 
 start IPy-thon normally, you load up, by default, the 
 default profile
 , stored in the 
 pro 
 file_default
  directory. Thus, on my Linux OS the full path to my default IPython 
 configuration file is:
  
 /home/wesm/.config/ipython/profile_default/ipython_config.py
  
 I’ll spare you the gory details of what’s in this file. Fortunately it has comments de-
 scribing what each configuration option is for, so I will leave it to the reader to 
 tinker and customize. One additional useful feature is that it’s possible to have 
 multiple pro-files
 . Suppose you wanted to have an alternate IPython configuration 
 tailored for a particular application or project. Creating a new profile is as simple is 
 typing something like
  
 ipython profile create secret_project
  
 Once you’ve done this, edit the config files in the newly-created 
 pro file_secret_project
  
 directory then launch IPython like so
  
 $ ipython --profile=secret_project 
  
 Python 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51) Type 
 ""copyright"", ""credits"" or ""license"" for more information.
  
 IPython 0.13 -- An enhanced Interactive Python.
  
 ?         -> Introduction and overview of IPython's features.
  
 %quickref -> Quick reference.
  
 help      -> Python's own help system.
  
 object?   -> Details about 'object', use 'object??' for extra details.
  
 IPython profile: secret_project
  
 Advanced IPython Features | 77",NA
Credits,"Parts of this chapter were derived from the wonderful documentation put together 
 by the IPython Development Team. I can’t thank them enough for all of their work 
 build-ing this amazing set of tools.
  
 78 | Chapter 3:IPython: An Interactive Computing and Development Environment
  
 www.it-ebooks.info",NA
CHAPTER 4,NA,NA
NumPy Basics: Arrays and ,NA,NA
Vectorize,NA,NA
d ,NA,NA
Computat,NA,NA
ion,"NumPy, short for Numerical Python, is the fundamental package required for high 
 performance scientific computing and data analysis. It is the foundation on which 
 nearly all of the higher-level tools in this book are built. Here are some of the things 
 it provides:
  
 •
  ndarray
 , a fast and space-efficient multidimensional array providing vectorized 
  
 arithmetic operations and sophisticated 
 broadcasting
  capabilities
  
 • Standard mathematical functions for fast operations on entire arrays of data 
  
 without having to write loops
  
 • Tools for reading / writing array data to disk and working with memory-mapped 
  
 files
  
 • Linear algebra, random number generation, and Fourier transform capabilities
  
 • Tools for integrating code written in C, C++, and Fortran
  
 The last bullet point is also one of the most important ones from an ecosystem 
 point of view. Because NumPy provides an easy-to-use C API, it is very easy to pass 
 data to external libraries written in a low-level language and also for external 
 libraries to return data to Python as NumPy arrays. This feature has made Python a 
 language of choice for wrapping legacy C/C++/Fortran codebases and giving them 
 a dynamic and easy-to-use interface.
  
 While NumPy by itself does not provide very much high-level data analytical func-
 tionality, having an understanding of NumPy arrays and array-oriented computing 
 will help you use tools like pandas much more effectively. If you’re new to Python 
 and just looking to get your hands dirty working with data using pandas, feel free to 
 give this chapter a skim. For more on advanced NumPy features like broadcasting, 
 see 
 Chap-ter 12
 .",NA
The NumPy ndarray: A Multidimensional ,NA,NA
Array Object,"One of the key features of NumPy is its N-dimensional array object, or ndarray, 
 which is a fast, flexible container for large data sets in Python. Arrays enable you to 
 perform mathematical operations on whole blocks of data using similar syntax to 
 the equivalent operations between scalar elements:
  
 In [8]: data 
  
 Out[8]: 
  
 array([[ 0.9526, -0.246 , -0.8856],
  
  
  [ 0.5639,  0.2379,  0.9104]])
  
 In [9]: data * 10                         In [10]: data + data 
  
 Out[9]:                                   Out[10]: 
  
 array([[ 9.5256, -2.4601, -8.8565],       array([[ 1.9051, -0.492 , -1.7713], 
  
  [ 5.6385,  
 2.3794,  9.104 ]])             [ 1.1277,  0.4759,  1.8208]])
  
 An ndarray is a generic multidimensional container for homogeneous data; that is, 
 all of the elements must be the same type. Every array has a 
 shape
 , a tuple 
 indicating the size of each dimension, and a 
 dtype
 , an object describing the 
 data type
  
 of the array:
  
 In [11]: data.shape 
  
 Out[11]: (2, 3)",NA
Creating ndarrays,"The easiest way to create an array is to use the 
 array
  function. This accepts any se-
 quence-like object (including other arrays) and produces a new NumPy array 
 contain-ing the passed data. For example, a list is a good candidate for conversion:
  
 In [13]: data1 = [6, 7.5, 8, 0, 1]
  
 In [14]: arr1 = np.array(data1)
  
 In [15]: arr1 
  
 Out[15]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])
  
 Nested sequences, like a list of equal-length lists, will be converted into a 
 multidimen-sional array:
  
 In [16]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]
  
 In [17]: arr2 = np.array(data2)
  
 In [18]: arr2 
  
 Out[18]: 
  
 array([[1, 2, 3, 4],
  
  
  [5, 6, 7, 8]])
  
 In [19]: arr2.ndim 
  
 Out[19]: 2
  
 In [20]: arr2.shape 
  
 Out[20]: (2, 4)
  
 Unless explicitly specified (more on this later), 
 np.array
  tries to infer a good data 
 type for the array that it creates. The data type is stored in a special 
 dtype
  object; for 
 example, in the above two examples we have:
  
 In [21]: arr1.dtype 
  
 Out[21]: dtype('float64')
  
 The NumPy ndarray: A Multidimensional Array Object | 81
  
 www.it-ebooks.info",NA
Data Types for ndarrays,"The 
 data type
  or 
 dtype
  is a special object containing the information the ndarray 
 needs to interpret a chunk of memory as a particular type of data:
  
 In [27]: arr1 = np.array([1, 2, 3], dtype=np.float64)
  
 In [28]: arr2 = np.array([1, 2, 3], dtype=np.int32)
  
 In [29]: arr1.dtype            In [30]: arr2.dtype Out[29]: 
 dtype('float64')      Out[30]: dtype('int32')
  
 Dtypes are part of what make NumPy so powerful and flexible. In most cases they 
 map directly onto an underlying machine representation, which makes it easy to 
 read and write binary streams of data to disk and also to connect to code written in 
 a low-level language like C or Fortran. The numerical dtypes are named the same 
 way: a type name, like 
 float
  or 
 int
 , followed by a number indicating the number of 
 bits per element. A standard double-precision floating point value (what’s used 
 under the hood in Python’s 
 float
  object) takes up 8 bytes or 64 bits. Thus, this type 
 is known in NumPy as 
 float64
 . See 
 Table 4-2
  for a full listing of NumPy’s supported 
 data types.
  
  
 Don’t worry about memorizing the NumPy dtypes, especially if 
 you’re a new user. It’s often only necessary to care about the 
 general 
 kind
  of data you’re dealing with, whether floating point, 
 complex, integer, boolean, string, or general Python object. When 
 you need more control
  
 over how data are stored in memory and on disk, especially large 
 data sets, it is good to know that you have control over the storage 
 type.
  
 Table 4-2. NumPy data types
  
 Type
  
 Type Code
  
 Description
  
 int8, uint8
  
 i1, u1
  
 Signed and unsigned 8-bit (1 byte) integer types
  
 int16, uint16
  
 i2, u2
  
 Signed and unsigned 16-bit integer types
  
 int32, uint32
  
 i4, u4
  
 Signed and unsigned 32-bit integer types
  
 int64, uint64
  
 i8, u8
  
 Signed and unsigned 32-bit integer types
  
 float16
  
 f2
  
 Half-precision floating point
  
 float32
  
 f4 or f
  
 Standard single-precision floating point. 
 Compatible with C float
  
 float64, float128
  
 f8 or d
  
 Standard double-precision floating point. 
 Compatible with C double
  
 and Python 
 float
  object
  
 The NumPy ndarray: A Multidimensional Array Object | 83",NA
Operations between Arrays and Scalars,"Arrays are important because they enable you to express batch operations on data 
 without writing any 
 for
  loops. This is usually called 
 vectorization
 . Any arithmetic 
 op-erations between equal-size arrays applies the operation elementwise:
  
 In [45]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])
  
 In [46]: arr 
  
 Out[46]: 
  
 array([[ 1.,  2.,  3.],
  
  
  [ 4.,  5.,  6.]])
  
 In [47]: arr * arr                 In [48]: arr - arr Out[47]:                           
 Out[48]: 
  
 array([[  1.,   4.,   9.],         array([[ 0.,  0.,  0.], 
  
  [ 16.,  25.,  36.]])               
 [ 0.,  0.,  0.]])
  
 Arithmetic operations with scalars are as you would expect, propagating the value 
 to each element:
  
 In [49]: 1 / arr                            In [50]: arr ** 0.5 
  
 Out[49]:                                    Out[50]: 
  
 array([[ 1.    ,  0.5   ,  0.3333],         array([[ 1.    ,  1.4142,  1.7321], 
  
  [ 0.25  ,  0.2   ,  0.1667]])               
 [ 2.    ,  2.2361,  2.4495]])
  
 The NumPy ndarray: A Multidimensional Array Object | 85
  
 www.it-ebooks.info",NA
Basic Indexing and Slicing ,"NumPy array indexing is a rich topic, as there are many ways you may want to 
 select a subset of your data or individual elements. One-dimensional arrays are 
 simple; on the surface they act similarly to Python lists: 
  
  
 In [51]: arr = np.arange(10) 
  
  
 In [52]: arr 
  
  
 Out[52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 
  
  
 In [53]: arr[5] 
  
  
 Out[53]: 5 
  
  
 In [54]: arr[5:8] 
  
  
 Out[54]: array([5, 6, 7]) 
  
  
 In [55]: arr[5:8] = 12 
  
  
 In [56]: arr 
  
  
 Out[56]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9]) 
  
 As you can see, if you assign a scalar value to a slice, as in 
 arr[5:8] = 12
 , the value is 
 propagated (or 
 broadcasted
  henceforth) to the entire selection. An important first 
 dis-tinction from lists is that array slices are 
 views
  on the original array. This means 
 that the data is not copied, and any modifications to the view will be reflected in the 
 source array: 
  
  
 In [57]: arr_slice = arr[5:8] 
  
  
 In [58]: arr_slice[1] = 12345 
  
  
 In [59]: arr 
  
  
 Out[59]: array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,     9]) 
  
 In [60]: arr_slice[:] = 64 
  
  
 In [61]: arr 
  
  
 Out[61]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9]) 
  
 If you are new to NumPy, you might be surprised by this, especially if they have 
 used other array programming languages which copy data more zealously. As 
 NumPy has been designed with large data use cases in mind, you could imagine 
 performance and memory problems if NumPy insisted on copying data left and 
 right.
  
 86 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation
  
 www.it-ebooks.info",NA
Boolean Indexing,"Let’s consider an example where we have some data in an array and an array of 
 names with duplicates. I’m going to use here the 
 randn
  function in 
 numpy.random
  to 
 generate some random normally distributed data:
  
 In [83]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
  
 In [84]: data = randn(7, 4)
  
 In [85]: names 
  
 Out[85]: 
  
 array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], 
  
  dtype='|S4')
  
 In [86]: data 
  
 Out[86]: 
  
 array([[-0.048 ,  0.5433, -0.2349,  1.2792],
  
  
  [-0.268 ,  0.5465,  0.0939, -2.0445],
  
  
  [-0.047 , -2.026 ,  0.7719,  0.3103],
  
  
  [ 2.1452,  0.8799, -0.0523,  0.0672],
  
  
  [-1.0023, -0.1698,  1.1503,  1.7289],
  
 The NumPy ndarray: A Multidimensional Array Object | 89
  
 www.it-ebooks.info",NA
Fancy Indexing,"Fancy indexing
  is a term adopted by NumPy to describe indexing using integer 
 arrays. Suppose we had a 8 × 4 array:
  
 In [100]: arr = np.empty((8, 4))
  
 In [101]: for i in range(8):
  
  
  .....:     arr[i] = i
  
 In [102]: arr 
  
 Out[102]: 
  
 array([[ 0.,  0.,  0.,  0.],
  
  
  [ 1.,  1.,  1.,  1.],
  
  
  [ 2.,  2.,  2.,  2.],
  
  
  [ 3.,  3.,  3.,  3.],
  
  
  [ 4.,  4.,  4.,  4.],
  
  
  [ 5.,  5.,  5.,  5.],
  
  
  [ 6.,  6.,  6.,  6.],
  
  
  [ 7.,  7.,  7.,  7.]])
  
 To select out a subset of the rows in a particular order, you can simply pass a list or 
 ndarray of integers specifying the desired order:
  
 In [103]: arr[[4, 3, 0, 6]] 
  
 Out[103]: 
  
 array([[ 4.,  4.,  4.,  4.],
  
  
  [ 3.,  3.,  3.,  3.],
  
  
  [ 0.,  0.,  0.,  0.],
  
  
  [ 6.,  6.,  6.,  6.]])
  
 Hopefully this code did what you expected! Using negative indices select rows from 
 the end:
  
 In [104]: arr[[-3, -5, -7]] 
  
 Out[104]: 
  
 array([[ 5.,  5.,  5.,  5.],
  
  
  [ 3.,  3.,  3.,  3.],
  
  
  [ 1.,  1.,  1.,  1.]])
  
 92 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation
  
 www.it-ebooks.info",NA
Transposing Arrays and Swapping Axes,"Transposing is a special form of reshaping which similarly returns a view on the 
 un-derlying data without copying anything. Arrays have the 
 transpose
  method and 
 also the special 
 T
  attribute:
  
 In [110]: arr = np.arange(15).reshape((3, 5))
  
 In [111]: arr                        In [112]: arr.T 
  
 The NumPy ndarray: A Multidimensional Array Object | 93",NA
Universal Functions: Fast Element-wise ,NA,NA
Array Functions,"A universal function, or 
 ufunc
 , is a function that performs elementwise operations 
 on data in ndarrays. You can think of them as fast vectorized wrappers for simple 
 functions that take one or more scalar values and produce one or more scalar 
 results.
  
 Many ufuncs are simple elementwise transformations, like 
 sqrt
  or 
 exp
 :
  
 In [120]: arr = np.arange(10)
  
 In [121]: np.sqrt(arr) 
  
 Out[121]: 
  
 array([ 0.    ,  1.    ,  1.4142,  1.7321,  2.    ,  2.2361,  2.4495,
  
  2.6458,  2.8284,  3.    ])
  
 In [122]: np.exp(arr) 
  
 Out[122]: 
  
 array([    1.    ,     2.7183,     7.3891,    20.0855,    54.5982,
  
  148.4132,   
 403.4288,  1096.6332,  2980.958 ,  8103.0839])
  
 These are referred to as 
 unary
  ufuncs. Others, such as 
 add
  or 
 maximum
 , take 2 arrays 
 (thus, 
 binary
  ufuncs) and return a single array as the result:
  
 In [123]: x = randn(8)
  
 In [124]: y = randn(8)
  
 In [125]: x 
  
 Out[125]: 
  
 array([ 0.0749,  0.0974,  0.2002, -0.2551,  0.4655,  0.9222,  0.446 ,
  
  -0.9337])
  
 In [126]: y 
  
 Out[126]: 
  
 array([ 0.267 , -1.1131, -0.3361,  0.6117, -1.2323,  0.4788,  0.4315,
  
  -0.7147])
  
 In [127]: np.maximum(x, y) # element-wise maximum 
  
 Out[127]: 
  
 array([ 0.267 ,  0.0974,  0.2002,  0.6117,  0.4655,  0.9222,  0.446 ,
  
  -0.7147])
  
 While not common, a ufunc can return multiple arrays. 
 modf
  is one example, a 
 vector-ized version of the built-in Python 
 divmod
 : it returns the fractional and 
 integral parts of a floating point array:
  
 In [128]: arr = randn(7) * 5
  
 In [129]: np.modf(arr) 
  
 Out[129]: 
  
 (array([-0.6808,  0.0636, -0.386 ,  0.1393, -0.8806,  0.9363, -0.883 ]), array([-2.,  4., -3.,  
 5., -3.,  3., -6.]))
  
 Universal Functions: Fast Element-wise Array Functions | 95
  
 www.it-ebooks.info",NA
Data Processing Using Arrays,"Using NumPy arrays enables you to express many kinds of data processing tasks as 
 concise array expressions that might otherwise require writing loops. This practice 
 of replacing explicit loops with array expressions is commonly referred to as 
 vectoriza-tion
 . In general, vectorized array operations will often be one or two (or 
 more) orders of magnitude faster than their pure Python equivalents, with the 
 biggest impact in any kind of numerical computations. Later, in 
 Chapter 12
 , I will 
 explain 
 broadcasting
 , a powerful method for vectorizing computations.
  
 As a simple example, suppose we wished to evaluate the function 
 sqrt(x^2 + y^2) 
 across a regular grid of values. The 
 np.meshgrid
  function takes two 1D arrays and 
 pro-duces two 2D matrices corresponding to all pairs of 
 (x, y)
  in the two arrays:
  
 In [130]: points = np.arange(-5, 5, 0.01) # 1000 equally spaced points
  
 In [131]: xs, ys = np.meshgrid(points, points)
  
 In [132]: ys 
  
 Out[132]: 
  
 array([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],
  
  [-4.99, -4.99, -4.99, 
 ..., -4.99, -4.99, -4.99],
  
  [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -
 4.98],
  
  ..., 
  
  
  [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],
  
  [ 4.98,  4.98,  4.98, 
 ...,  4.98,  4.98,  4.98],
  
  [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  
 4.99]])
  
 Now, evaluating the function is a simple matter of writing the same expression you 
 would write with two points:
  
 In [134]: import matplotlib.pyplot as plt
  
 In [135]: z = np.sqrt(xs ** 2 + ys ** 2)
  
 In [136]: z 
  
 Out[136]: 
  
 array([[ 7.0711,  7.064 ,  7.0569, ...,  7.0499,  7.0569,  7.064 ],
  
  [ 7.064 ,  
 7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569],
  
  [ 7.0569,  7.0499,  7.0428, ...,  
 7.0357,  7.0428,  7.0499],
  
  ..., 
  
  
  [ 7.0499,  7.0428,  7.0357, ...,  7.0286,  7.0357,  7.0428],
  
  [ 7.0569,  
 7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],
  
  [ 7.064 ,  7.0569,  7.0499, ...,  
 7.0428,  7.0499,  7.0569]])
  
 Data Processing Using Arrays | 97
  
 www.it-ebooks.info",NA
Expressing Conditional Logic as Array Operations,"The 
 numpy.where
  function is a vectorized version of the ternary expression 
 x if condi 
 tion else y
 . Suppose we had a boolean array and two arrays of values:
  
 In [140]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
  
 In [141]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
  
 In [142]: cond = np.array([True, False, True, True, False])
  
 Suppose we wanted to take a value from 
 xarr
  whenever the corresponding value in 
 cond
  is 
 True
  otherwise take the value from 
 yarr
 . A list comprehension doing this 
 might look like:
  
 In [143]: result = [(x if c else y)
  
  
  .....:           for x, y, c in zip(xarr, yarr, cond)]
  
 In [144]: result 
  
 Out[144]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]
  
 98 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation
  
 www.it-ebooks.info",NA
Mathematical and Statistical Methods,"A set of mathematical functions which compute statistics about an entire array or 
 about the data along an axis are accessible as array methods. Aggregations (often 
 called 
 reductions
 ) like 
 sum
 , 
 mean
 , and standard deviation 
 std
  can either be used by 
 calling the array instance method or using the top level NumPy function:
  
 In [151]: arr = np.random.randn(5, 4) # normally-distributed data
  
 In [152]: arr.mean() 
  
 Out[152]: 0.062814911084854597
  
 In [153]: np.mean(arr) 
  
 Out[153]: 0.062814911084854597
  
 In [154]: arr.sum() 
  
 Out[154]: 1.2562982216970919
  
 Functions like 
 mean
  and 
 sum
  take an optional 
 axis
  argument which computes the 
 statistic over the given axis, resulting in an array with one fewer dimension:
  
 In [155]: arr.mean(axis=1) 
  
 Out[155]: array([-1.2833,  0.2844,  0.6574,  0.6743, -0.0187])
  
 In [156]: arr.sum(0) 
  
 Out[156]: array([-3.1003, -1.6189,  1.4044,  4.5712])
  
 Other methods like 
 cumsum
  and 
 cumprod
  do not aggregate, instead producing an 
 array of the intermediate results:
  
 In [157]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
  
 In [158]: arr.cumsum(0)        In [159]: arr.cumprod(1) Out[158]:                      
 Out[159]: 
  
 array([[ 0,  1,  2],           array([[  0,   0,   0], 
  
  [ 3,  5,  7],                  
 [  3,  12,  60], 
  
  [ 9, 12, 15]])                 [  6,  42, 336]])
  
 See 
 Table 4-5
  for a full listing. We’ll see many examples of these methods in action 
 in later chapters.
  
 100 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation
  
 www.it-ebooks.info",NA
Methods for Boolean Arrays,"Boolean values are coerced to 1 (
 True
 ) and 0 (
 False
 ) in the above methods. Thus, 
 sum 
 is often used as a means of counting 
 True
  values in a boolean array:
  
 In [160]: arr = randn(100)
  
 In [161]: (arr > 0).sum() # Number of positive values 
 Out[161]: 44
  
 There are two additional methods, 
 any
  and 
 all
 , useful especially for boolean arrays. 
 any
  tests whether one or more values in an array is 
 True
 , while 
 all
  checks if every 
 value is 
 True
 :
  
 In [162]: bools = np.array([False, False, True, False])
  
 In [163]: bools.any() 
  
 Out[163]: True
  
 In [164]: bools.all() 
  
 Out[164]: False
  
 These methods also work with non-boolean arrays, where non-zero elements 
 evaluate to 
 True
 .",NA
Sorting,"Like Python’s built-in list type, NumPy arrays can be sorted in-place using the 
 sort 
 method:
  
 In [165]: arr = randn(8)
  
 In [166]: arr 
  
 Out[166]: 
  
 array([ 0.6903,  0.4678,  0.0968, -0.1349,  0.9879,  0.0185, -1.3147,
  
  -0.5425])
  
 In [167]: arr.sort()
  
 Data Processing Using Arrays | 101
  
 www.it-ebooks.info",NA
Unique and Other Set Logic,"NumPy has some basic set operations for one-dimensional ndarrays. Probably the 
 most commonly used one is 
 np.unique
 , which returns the sorted unique values in an 
 array:
  
 In [176]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
  
 In [177]: np.unique(names) 
  
 Out[177]: 
  
 102 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation",NA
File Input and Output with Arrays ,"NumPy is able to save and load data to and from disk either in text or binary format. 
 In later chapters you will learn about tools in pandas for reading tabular data into 
 memory.",NA
Storing Arrays on Disk in Binary Format ,"np.save
  and 
 np.load
  are the two workhorse functions for efficiently saving and 
 loading array data on disk. Arrays are saved by default in an uncompressed raw 
 binary format with file extension 
 .npy
 .
  
 In [183]: arr = np.arange(10) 
  
 In [184]: np.save('some_array', arr)
  
 File Input and Output with Arrays | 103
  
 www.it-ebooks.info",NA
Saving and Loading Text Files,"Loading text from files is a fairly standard task. The landscape of file reading and 
 writing functions in Python can be a bit confusing for a newcomer, so I will focus 
 mainly on the 
 read_csv
  and 
 read_table
  functions in pandas. It will at times be useful 
 to load data into vanilla NumPy arrays using 
 np.loadtxt
  or the more specialized 
 np.genfromtxt
 .
  
 These functions have many options allowing you to specify different delimiters, 
 con-verter functions for certain columns, skipping rows, and other things. Take a 
 simple case of a comma-separated file (CSV) like this:
  
 In [191]: !cat array_ex.txt 
  
 0.580052,0.186730,1.040717,1.134411 
  
 0.194163,-0.636917,-0.938659,0.124094
  
 -0.126410,0.268607,-0.695724,0.047428
  
 -1.484413,0.004176,-0.744203,0.005487 
  
 2.302869,0.200131,1.670238,-1.881090
  
 -0.193230,1.047233,0.482803,0.960334
  
 This can be loaded into a 2D array like so:
  
 In [192]: arr = np.loadtxt('array_ex.txt', delimiter=',')
  
 In [193]: arr 
  
 Out[193]: 
  
 array([[ 0.5801,  0.1867,  1.0407,  1.1344],
  
  
  [ 0.1942, -0.6369, -0.9387,  0.1241],
  
  
  [-0.1264,  0.2686, -0.6957,  0.0474],
  
  
  [-1.4844,  0.0042, -0.7442,  0.0055],
  
  
  [ 2.3029,  0.2001,  1.6702, -1.8811],
  
  
  [-0.1932,  1.0472,  0.4828,  0.9603]])
  
 np.savetxt
  performs the inverse operation: writing an array to a delimited text file. 
 genfromtxt
  is similar to 
 loadtxt
  but is geared for structured arrays and missing data 
 handling; see 
 Chapter 12
  for more on structured arrays.
  
 104 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation",NA
Linear Algebra,"Linear algebra, like matrix multiplication, decompositions, determinants, and other 
 square matrix math, is an important part of any array library. Unlike some 
 languages like MATLAB, multiplying two two-dimensional arrays with 
 *
  is an 
 element-wise product instead of a matrix dot product. As such, there is a function 
 dot
 , both an array method, and a function in the 
 numpy
  namespace, for matrix 
 multiplication:
  
 In [194]: x = np.array([[1., 2., 3.], [4., 5., 6.]])
  
 In [195]: y = np.array([[6., 23.], [-1, 7], [8, 9]])
  
 In [196]: x                   In [197]: y 
  
 Out[196]:                     Out[197]: 
  
 array([[ 1.,  2.,  3.],       array([[  6.,  23.], 
  
  [ 4.,  5.,  6.]])             
 [ -1.,   7.], 
  
  
  [  8.,   9.]])
  
 In [198]: x.dot(y)  # equivalently np.dot(x, y) Out[198]: 
  
 array([[  28.,   64.],
  
  
  [  67.,  181.]])
  
 A matrix product between a 2D array and a suitably sized 1D array results in a 1D 
 array:
  
 In [199]: np.dot(x, np.ones(3)) 
  
 Out[199]: array([  6.,  15.])
  
 numpy.linalg
  has a standard set of matrix decompositions and things like inverse 
 and determinant. These are implemented under the hood using the same industry-
 standard Fortran libraries used in other languages like MATLAB and R, such as like 
 BLAS, LA-PACK, or possibly (depending on your NumPy build) the Intel MKL:
  
 In [201]: from numpy.linalg import inv, qr
  
 In [202]: X = randn(5, 5)
  
 In [203]: mat = X.T.dot(X)
  
 In [204]: inv(mat) 
  
 Out[204]: 
  
 array([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911],
  
  [-
 0.1808,  0.5035,  0.1215,  0.6702,  0.0956],
  
  [-0.6878,  
 0.1215,  0.2904,  0.8081,  0.3049],
  
  [-2.8285,  0.6702,  0.8081,  
 3.4152,  1.1557],
  
  [-1.1911,  0.0956,  0.3049,  1.1557,  
 0.6051]])
  
 In [205]: mat.dot(inv(mat))
  
 Linear Algebra | 105",NA
Random Number Generation,"The 
 numpy.random
  module supplements the built-in Python 
 random
  with functions 
 for efficiently generating whole arrays of sample values from many kinds of 
 probability
  
 106 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation",NA
Example: Random Walks,"An illustrative application of utilizing array operations is in the simulation of 
 random walks. Let’s first consider a simple random walk starting at 0 with steps of 
 1 and -1 occurring with equal probability. A pure Python way to implement a single 
 random walk with 1,000 steps using the built-in 
 random
  module:
  
 import random 
  
 position = 0 
  
 walk = [position] 
  
 steps = 1000 
  
 for i in xrange(steps):
  
  
  step = 1 if random.randint(0, 1) else -1
  
  
  position += step
  
  
  walk.append(position)
  
 See 
 Figure 4-4
  for an example plot of the first 100 values on one of these random 
 walks.
  
  
 Figure 4-4. A simple random walk
  
 You might make the observation that 
 walk
  is simply the cumulative sum of the 
 random steps and could be evaluated as an array expression. Thus, I use the 
 np.random
  module to draw 1,000 coin flips at once, set these to 1 and -1, and 
 compute the cumulative sum:
  
 In [215]: nsteps = 1000
  
 In [216]: draws = np.random.randint(0, 2, size=nsteps)
  
 In [217]: steps = np.where(draws > 0, 1, -1)
  
 In [218]: walk = steps.cumsum()
  
 108 | Chapter 4:NumPy Basics: Arrays and Vectorized Computation",NA
Simulating Many Random Walks at Once,"If your goal was to simulate many random walks, say 5,000 of them, you can 
 generate all of the random walks with minor modifications to the above code. The 
 numpy.ran dom
  functions if passed a 2-tuple will generate a 2D array of draws, and 
 we can compute the cumulative sum across the rows to compute all 5,000 random 
 walks in one shot:
  
 In [222]: nwalks = 5000
  
 In [223]: nsteps = 1000
  
 In [224]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1
  
 In [225]: steps = np.where(draws > 0, 1, -1)
  
 In [226]: walks = steps.cumsum(1)
  
 In [227]: walks 
  
 Out[227]: 
  
 array([[  1,   0,   1, ...,   8,   7,   8],
  
  
  [  1,   0,  -1, ...,  34,  33,  32],
  
  
  [  1,   0,  -1, ...,   4,   5,   4],
  
  
  ..., 
  
  
  [  1,   2,   1, ...,  24,  25,  26],
  
  
  [  1,   2,   3, ...,  14,  13,  14],
  
  
  [ -1,  -2,  -3, ..., -24, -23, -22]])
  
 Now, we can compute the maximum and minimum values obtained over all of the 
 walks:
  
 In [228]: walks.max()        In [229]: walks.min() Out[228]: 
 138                Out[229]: -133
  
 Example: Random Walks | 109",NA
CHAPTER 5,NA,NA
Getting Started with pandas,"pandas will be the primary library of interest throughout much of the rest of the 
 book. It contains high-level data structures and manipulation tools designed to 
 make data analysis fast and easy in Python. pandas is built on top of NumPy and 
 makes it easy to use in NumPy-centric applications.
  
 As a bit of background, I started building pandas in early 2008 during my tenure at 
 AQR, a quantitative investment management firm. At the time, I had a distinct set of 
 requirements that were not well-addressed by any single tool at my disposal:
  
 • Data structures with labeled axes supporting automatic or explicit data 
 alignment. This prevents common errors resulting from misaligned data and 
 working with differently-indexed data coming from different sources.
  
 • Integrated time series functionality.
  
 • The same data structures handle both time series data and non-time series data.
  
 • Arithmetic operations and reductions (like summing across an axis) would pass 
  
 on the metadata (axis labels).
  
 • Flexible handling of missing data.
  
 • Merge and other relational operations found in popular database databases 
 (SQL-
  
 based, for example).
  
 I wanted to be able to do all of these things in one place, preferably in a language 
 well-suited to general purpose software development. Python was a good 
 candidate lan-guage for this, but at that time there was not an integrated set of data 
 structures and tools providing this functionality.
  
 Over the last four years, pandas has matured into a quite large library capable of 
 solving a much broader set of data handling problems than I ever anticipated, but it 
 has ex-panded in its scope without compromising the simplicity and ease-of-use 
 that I desired from the very beginning. I hope that after reading this book, you will 
 find it to be just as much of an indispensable tool as I do.
  
 Throughout the rest of the book, I use the following import conventions for pandas:
  
 111
  
 www.it-ebooks.info",NA
Introduction to pandas Data Structures,"To get started with pandas, you will need to get comfortable with its two 
 workhorse data structures: 
 Series
  and 
 DataFrame
 . While they are not a universal 
 solution for every problem, they provide a solid, easy-to-use basis for most 
 applications.",NA
Series,"A Series is a one-dimensional array-like object containing an array of data (of any 
 NumPy data type) and an associated array of data labels, called its 
 index
 . The 
 simplest Series is formed from only an array of data:
  
 In [4]: obj = Series([4, 7, -5, 3])
  
 In [5]: obj 
  
 Out[5]: 
  
 0    4 
  
 1    7 
  
 2   -5 
  
 3    3
  
 The string representation of a Series displayed interactively shows the index on the 
 left and the values on the right. Since we did not specify an index for the data, a 
 default one consisting of the integers 0 through N - 1 (where N is the length of the 
 data) is created. You can get the array representation and index object of the Series 
 via its values and index attributes, respectively:
  
 In [6]: obj.values 
  
 Out[6]: array([ 4,  7, -5,  3])
  
 In [7]: obj.index 
  
 Out[7]: Int64Index([0, 1, 2, 3])
  
 Often it will be desirable to create a Series with an index identifying each data 
 point:
  
 In [8]: obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
  
 In [9]: obj2 
  
 Out[9]: 
  
 d    4 
  
 b    7 
  
 a   -5 
  
 c    3
  
 112 | Chapter 5:Getting Started with pandas",NA
DataFrame,"A DataFrame represents a tabular, spreadsheet-like data structure containing an 
 or-dered collection of columns, each of which can be a different value type 
 (numeric, string, boolean, etc.). The DataFrame has both a row and column index; it 
 can be thought of as a dict of Series (one for all sharing the same index). Compared 
 with other such DataFrame-like structures you may have used before (like R’s 
 data.frame
 ), row-oriented and column-oriented operations in DataFrame are 
 treated roughly symmet-rically. Under the hood, the data is stored as one or more 
 two-dimensional blocks rather than a list, dict, or some other collection of one-
 dimensional arrays. The exact details of DataFrame’s internals are far outside the 
 scope of this book.
  
  
 While DataFrame stores the data internally in a two-dimensional 
 for-mat, you can easily represent much higher-dimensional data in a 
 tabular format using hierarchical indexing, a subject of a later 
 section and a key ingredient in many of the more advanced data-
 handling features in pan-
  
 das.
  
 Introduction to pandas Data Structures | 115",NA
Index Objects,"pandas’s Index objects are responsible for holding the axis labels and other 
 metadata (like the axis name or names). Any array or other sequence of labels used 
 when con-structing a Series or DataFrame is internally converted to an Index:
  
 In [68]: obj = Series(range(3), index=['a', 'b', 'c'])
  
 In [69]: index = obj.index
  
 In [70]: index 
  
 Out[70]: Index([a, b, c], dtype=object)
  
 In [71]: index[1:] 
  
 Out[71]: Index([b, c], dtype=object)
  
 Index objects are immutable and thus can’t be modified by the user:
  
 In [72]: index[1] = 'd'
  
 ---------------------------------------------------------------------------Exception                                 
 Traceback (most recent call last) <ipython-input-72-676fdeb26a68> in <module>()
  
 ----> 1 index[1] = 'd' 
  
 /Users/wesm/code/pandas/pandas/core/index.pyc in __setitem__(self, key, value)
  
  
 302     def __setitem__(self, key, value):
  
  
  303         """"""Disable the setting of values.""""""
  
 --> 304         raise Exception(str(self.__class__) + ' object is immutable')
  
  305 
  
  
  306     def __getitem__(self, key): 
  
 Exception: <class 'pandas.core.index.Index'> object is immutable
  
 120 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Essential Functionality,"In this section, I’ll walk you through the fundamental mechanics of interacting with 
 the data contained in a Series or DataFrame. Upcoming chapters will delve more 
 deeply into data analysis and manipulation topics using pandas. This book is not 
 intended to serve as exhaustive documentation for the pandas library; I instead 
 focus on the most important features, leaving the less common (that is, more 
 esoteric) things for you to explore on your own.",NA
Reindexing,"A critical method on pandas objects is 
 reindex
 , which means to create a new object 
 with the data 
 conformed
  to a new index. Consider a simple example from above:
  
 In [79]: obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])
  
 In [80]: obj 
  
 Out[80]: 
  
 d    4.5 
  
 b    7.2 
  
 a   -5.3 
  
 c    3.6
  
 Calling 
 reindex
  on this Series rearranges the data according to the new index, intro-
 ducing missing values if any index values were not already present:
  
 In [81]: obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])
  
 In [82]: obj2 
  
 Out[82]: 
  
 a   -5.3
  
 122 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Dropping entries from an axis,"Dropping one or more entries from an axis is easy if you have an index array or list 
 without those entries. As that can require a bit of munging and set logic, the 
 drop 
 method will return a new object with the indicated value or values deleted from an 
 axis:
  
 In [94]: obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])
  
 In [95]: new_obj = obj.drop('c')
  
 In [96]: new_obj 
  
 Out[96]: 
  
 a    0 
  
 b    1 
  
 d    3 
  
 e    4
  
 In [97]: obj.drop(['d', 'c']) 
  
 Out[97]: 
  
 a    0 
  
 b    1 
  
 e    4
  
 With DataFrame, index values can be deleted from either axis:
  
 In [98]: data = DataFrame(np.arange(16).reshape((4, 4)),
  
  ....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'], ....:                  
 columns=['one', 'two', 'three', 'four'])
  
 In [99]: data.drop(['Colorado', 'Ohio']) 
  
 Out[99]: 
  
  
  one  two  three  four 
  
 Utah        8    9     10    11 
  
 New York   12   13     14    15
  
 In [100]: data.drop('two', axis=1)      In [101]: data.drop(['two', 'four'], axis=1) Out[100]:                               
 Out[101]: 
  
  
  one  three  four                        one  three                        Ohio        0      2     3              Ohio        0      
 2 
  
 Colorado    4      6     7              Colorado    4      6 
  
 Utah        8     10    11              Utah        8     10 
  
 New York   12     14    15              New York   12     14",NA
"Indexing, selection, and filtering","Series indexing (
 obj[...]
 ) works analogously to NumPy array indexing, except you can 
 use the Series’s index values instead of only integers. Here are some examples this:
  
 In [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])
  
 In [103]: obj['b']          In [104]: obj[1] 
  
 Out[103]: 1.0               Out[104]: 1.0 
  
 In [105]: obj[2:4]          In [106]: obj[['b', 'a', 'd']] Out[105]:                   
 Out[106]: 
  
 Essential Functionality | 125",NA
Arithmetic and data alignment,"One of the most important pandas features is the behavior of arithmetic between 
 ob-jects with different indexes. When adding together objects, if any index pairs are 
 not the same, the respective index in the result will be the union of the index pairs. 
 Let’s look at a simple example:
  
 In [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])
  
 In [127]: s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])
  
 In [128]: s1        In [129]: s2 
  
 Out[128]:           Out[129]: 
  
 a    7.3            a   -2.1 
  
 c   -2.5            c    3.6 
  
 d    3.4            e   -1.5 
  
 128 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Function application and mapping,"NumPy ufuncs (element-wise array methods) work fine with pandas objects:
  
 In [158]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), .....:                   
 index=['Utah', 'Ohio', 'Texas', 'Oregon'])
  
 In [159]: frame                           In [160]: np.abs(frame) 
  
 Out[159]:                                 Out[160]: 
  
  
  b         d         e                     b         d         e Utah   -0.204708  0.478943 -
 0.519439      Utah    0.204708  0.478943  0.519439 Ohio   -0.555730  1.965781  1.393406      
 Ohio    0.555730  1.965781  1.393406 Texas   0.092908  0.281746  0.769023      Texas   
 0.092908  0.281746  0.769023 Oregon  1.246435  1.007189 -1.296221      Oregon  1.246435  
 1.007189  1.296221
  
 Another frequent operation is applying a function on 1D arrays to each column or 
 row. DataFrame’s 
 apply
  method does exactly this:
  
 In [161]: f = lambda x: x.max() - x.min()
  
 In [162]: frame.apply(f)        In [163]: frame.apply(f, axis=1) Out[162]:                       
 Out[163]: 
  
 b    1.802165                   Utah      0.998382 
  
 d    1.684034                   Ohio      2.521511 
  
 e    2.689627                   Texas     0.676115 
  
  
  Oregon    2.542656
  
 Many of the most common array statistics (like 
 sum
  and 
 mean
 ) are DataFrame 
 methods, so using 
 apply
  is not necessary.
  
 The function passed to 
 apply
  need not return a scalar value, it can also return a 
 Series with multiple values:
  
 In [164]: def f(x):
  
  
  .....:     return Series([x.min(), x.max()], index=['min', 'max'])
  
 In [165]: frame.apply(f)
  
 132 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Sorting and ranking,"Sorting a data set by some criterion is another important built-in operation. To sort 
 lexicographically by row or column index, use the 
 sort_index
  method, which returns 
 a new, sorted object:
  
 In [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])
  
 In [170]: obj.sort_index() 
  
 Out[170]: 
  
 a    1 
  
 b    2 
  
 c    3 
  
 d    0
  
 With a DataFrame, you can sort by index on either axis:
  
 In [171]: frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],
  
  .....:                   
 columns=['d', 'a', 'b', 'c'])
  
 In [172]: frame.sort_index()        In [173]: frame.sort_index(axis=1) Out[172]:                           
 Out[173]: 
  
  
  d  a  b  c                          a  b  c  d                 one    4  5  6  7                   three  1  2  3  
 0 
  
 three  0  1  2  3                   one    5  6  7  4
  
 Essential Functionality | 133",NA
Axis indexes with duplicate values,"Up until now all of the examples I’ve showed you have had unique axis labels (index 
 values). While many pandas functions (like 
 reindex
 ) require that the labels be 
 unique, it’s not mandatory. Let’s consider a small Series with duplicate indices:
  
 In [190]: obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])
  
 In [191]: obj 
  
 Out[191]: 
  
 a    0 
  
 a    1 
  
 b    2 
  
 b    3 
  
 c    4
  
 The index’s 
 is_unique
  property can tell you whether its values are unique or not:
  
 In [192]: obj.index.is_unique 
  
 Out[192]: False
  
 Data selection is one of the main things that behaves differently with duplicates. In-
 dexing a value with multiple entries returns a Series while single entries return a 
 scalar value:
  
 In [193]: obj['a']    In [194]: obj['c'] 
  
 Out[193]:             Out[194]: 4 
  
 a    0 
  
 a    1
  
 The same logic extends to indexing rows in a DataFrame:
  
 In [195]: df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])
  
 In [196]: df 
  
 Out[196]: 
  
  
  0         1         2 
  
 a  0.274992  0.228913  1.352917 
  
 a  0.886429 -2.001637 -0.371843 
  
 b  1.669025 -0.438570 -0.539741 
  
 b  0.476985  3.248944 -1.021228
  
 In [197]: df.ix['b'] 
  
 Out[197]: 
  
  
  0         1         2
  
 136 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Summarizing and Computing Descriptive ,NA,NA
Statistics,"pandas objects are equipped with a set of common mathematical and statistical 
 meth-ods. Most of these fall into the category of 
 reductions
  or 
 summary statistics
 , 
 methods that extract a single value (like the sum or mean) from a Series or a Series 
 of values from the rows or columns of a DataFrame. Compared with the equivalent 
 methods of vanilla NumPy arrays, they are all built from the ground up to exclude 
 missing data. Consider a small DataFrame:
  
 In [198]: df = DataFrame([[1.4, np.nan], [7.1, -4.5], .....:                 
 [np.nan, np.nan], [0.75, -1.3]], .....:                index=['a', 'b', 'c', 'd'],
  
  .....:                columns=['one', 'two'])
  
 In [199]: df 
  
 Out[199]: 
  
  
  one  two 
  
 a  1.40  NaN 
  
 b  7.10 -4.5 
  
 c   NaN  NaN 
  
 d  0.75 -1.3
  
 Calling DataFrame’s 
 sum
  method returns a Series containing column sums:
  
 In [200]: df.sum() 
  
 Out[200]: 
  
 one    9.25 
  
 two   -5.80
  
 Passing 
 axis=1
  sums over the rows instead:
  
 In [201]: df.sum(axis=1) 
  
 Out[201]: 
  
 a    1.40 
  
 b    2.60 
  
 c     NaN 
  
 d   -0.55
  
 NA values are excluded unless the entire slice (row or column in this case) is NA. 
 This can be disabled using the 
 skipna
  option:
  
 In [202]: df.mean(axis=1, skipna=False) 
  
 Out[202]: 
  
 a      NaN 
  
 b    1.300 
  
 c      NaN 
  
 d   -0.275
  
 See 
 Table 5-9
  for a list of common options for each reduction method options.
  
 Summarizing and Computing Descriptive Statistics | 137",NA
Correlation and Covariance,"Some summary statistics, like correlation and covariance, are computed from pairs 
 of arguments. Let’s consider some DataFrames of stock prices and volumes 
 obtained from Yahoo! Finance:
  
 import pandas.io.data as web
  
 all_data = {} 
  
 for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']:
  
  
  all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2000', '1/1/2010')
  
 price = DataFrame({tic: data['Adj Close']
  
  
  for tic, data in all_data.iteritems()}) volume = 
 DataFrame({tic: data['Volume']
  
  
  
  for tic, data in all_data.iteritems()})
  
 I now compute percent changes of the prices:
  
 In [209]: returns = price.pct_change()
  
 In [210]: returns.tail()
  
 Summarizing and Computing Descriptive Statistics | 139
  
 www.it-ebooks.info",NA
"Unique Values, Value Counts, and Membership","Another class of related methods extracts information about the values contained in 
 a one-dimensional Series. To illustrate these, consider this example:
  
 In [217]: obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])
  
 The first function is 
 unique
 , which gives you an array of the unique values in a 
 Series:
  
 In [218]: uniques = obj.unique()
  
 In [219]: uniques 
  
 Out[219]: array([c, a, d, b], dtype=object)
  
 The unique values are not necessarily returned in sorted order, but could be sorted 
 after the fact if needed (
 uniques.sort()
 ). Relatedly, 
 value_counts
  computes a Series 
 con-taining value frequencies:
  
 In [220]: obj.value_counts() 
  
 Out[220]: 
  
 c    3 
  
 a    3 
  
 b    2 
  
 d    1
  
 The Series is sorted by value in descending order as a convenience. 
 value_counts
  is 
 also available as a top-level pandas method that can be used with any array or 
 sequence:
  
 In [221]: pd.value_counts(obj.values, sort=False) 
 Out[221]: 
  
 a    3 
  
 b    2 
  
 c    3 
  
 d    1
  
 Lastly, 
 isin
  is responsible for vectorized set membership and can be very useful in 
 filtering a data set down to a subset of values in a Series or column in a DataFrame:
  
 In [222]: mask = obj.isin(['b', 'c'])
  
 In [223]: mask        In [224]: obj[mask] 
  
 Out[223]:             Out[224]: 
  
 0     True            0    c 
  
 1    False            5    b 
  
 2    False            6    b 
  
 3    False            7    c 
  
 4    False            8    c 
  
 5     True 
  
 6     True 
  
 Summarizing and Computing Descriptive Statistics | 141",NA
Handling Missing Data,"Missing data is common in most data analysis applications. One of the goals in de-
 signing pandas was to make working with missing data as painless as possible. For 
 example, all of the descriptive statistics on pandas objects exclude missing data as 
 you’ve seen earlier in the chapter.
  
 142 | Chapter 5:Getting Started with pandas
  
 www.it-ebooks.info",NA
Filtering Out Missing Data,"You have a number of options for filtering out missing data. While doing it by hand 
 is always an option, 
 dropna
  can be very helpful. On a Series, it returns the Series 
 with only the non-null data and index values:
  
 In [234]: from numpy import nan as NA
  
 In [235]: data = Series([1, NA, 3.5, NA, 7])
  
 In [236]: data.dropna() 
  
 Out[236]: 
  
 Handling Missing Data | 143
  
 www.it-ebooks.info",NA
Filling in Missing Data,"Rather than filtering out missing data (and potentially discarding other data along 
 with it), you may want to fill in the “holes” in any number of ways. For most 
 purposes, the 
 fillna
  method is the workhorse function to use. Calling 
 fillna
  with a 
 constant replaces missing values with that value:
  
 In [250]: df.fillna(0) 
  
 Out[250]: 
  
  
  0         1         2 
  
 0 -0.577087  0.000000  0.000000 
  
 1  0.523772  0.000000  0.000000 
  
 2 -0.713544  0.000000  0.000000 
  
 3 -1.860761  0.000000  0.560145 
  
 4 -1.265934  0.000000 -1.063512 
  
 5  0.332883 -2.359419 -0.199543 
  
 6 -1.541996 -0.970736 -1.307030
  
 Calling 
 fillna
  with a dict you can use a different fill value for each column:
  
 In [251]: df.fillna({1: 0.5, 3: -1}) 
  
 Out[251]: 
  
  
  0         1         2 
  
 0 -0.577087  0.500000       NaN 
  
 1  0.523772  0.500000       NaN 
  
 2 -0.713544  0.500000       NaN 
  
 3 -1.860761  0.500000  0.560145 
  
 4 -1.265934  0.500000 -1.063512 
  
 5  0.332883 -2.359419 -0.199543 
  
 6 -1.541996 -0.970736 -1.307030
  
 fillna
  returns a new object, but you can modify the existing object in place:
  
 # always returns a reference to the filled object In [252]: _ 
 = df.fillna(0, inplace=True)
  
 In [253]: df 
  
 Out[253]: 
  
  
  0         1         2 
  
 0 -0.577087  0.000000  0.000000 
  
 1  0.523772  0.000000  0.000000 
  
 2 -0.713544  0.000000  0.000000 
  
 3 -1.860761  0.000000  0.560145
  
 Handling Missing Data | 145
  
 www.it-ebooks.info",NA
Hierarchical Indexing,"Hierarchical indexing
  is an important feature of pandas enabling you to have 
 multiple (two or more) index 
 levels
  on an axis. Somewhat abstractly, it provides a 
 way for you to work with higher dimensional data in a lower dimensional form. 
 Let’s start with a simple example; create a Series with a list of lists or arrays as the 
 index:
  
 In [261]: data = Series(np.random.randn(10),
  
  .....:               index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'], .....:                      [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])
  
 In [262]: data 
  
 Out[262]: 
  
 a  1    0.670216
  
  
  2    0.852965
  
  
  3   -0.955869 
  
 b  1   -0.023493
  
  
  2   -2.304234
  
  
  3   -0.652469 
  
 c  1   -1.218302
  
  
  2   -1.332610 
  
 d  2    1.074623
  
  
  3    0.723642
  
 What you’re seeing is a prettified view of a Series with a 
 MultiIndex
  as its index. 
 The“gaps” in the index display mean “use the label directly above”:
  
 In [263]: data.index 
  
 Out[263]: 
  
 MultiIndex 
  
 [('a', 1) ('a', 2) ('a', 3) ('b', 1) ('b', 2) ('b', 3) ('c', 1) ('c', 2) ('d', 2) ('d', 3)]
  
 With a hierarchically-indexed object, so-called 
 partial
  indexing is possible, enabling 
 you to concisely select subsets of the data:
  
 In [264]: data['b'] 
  
 Out[264]: 
  
 1   -0.023493 
  
 2   -2.304234 
  
 3   -0.652469
  
 In [265]: data['b':'c']        In [266]: data.ix[['b', 'd']] Out[265]:                      
 Out[266]: 
  
 b  1   -0.023493               b  1   -0.023493 
  
  
  2   -2.304234                  2   -2.304234             
  
  3   -0.652469                  
 3   -0.652469             c  1   -1.218302               d  2    1.074623 
  
  
  2   -1.332610                  3    0.723642
  
 Selection is even possible in some cases from an “inner” level:
  
 In [267]: data[:, 2] 
  
 Out[267]: 
  
 a    0.852965
  
 Hierarchical Indexing | 147
  
 www.it-ebooks.info",NA
Reordering and Sorting Levels,"At times you will need to rearrange the order of the levels on an axis or sort the 
 data by the values in one specific level. The 
 swaplevel
  takes two level numbers or 
 names and returns a new object with the levels interchanged (but the data is 
 otherwise unaltered):
  
 In [276]: frame.swaplevel('key1', 'key2') 
  
 Out[276]: 
  
 state       Ohio       Colorado 
  
 color      Green  Red     Green 
  
 key2 key1 
  
 1    a         0    1         2 
  
 2    a         3    4         5 
  
 1    b         6    7         8 
  
 2    b         9   10        11
  
 sortlevel
 , on the other hand, sorts the data (stably) using only the values in a single 
 level. When swapping levels, it’s not uncommon to also use 
 sortlevel
  so that the 
 result is lexicographically sorted:
  
 In [277]: frame.sortlevel(1)           In [278]: frame.swaplevel(0, 1).sortlevel(0) Out[277]:                              
 Out[278]: 
  
 state       Ohio       Colorado        state       Ohio       Colorado 
  
 color      Green  Red     Green        color      Green  Red     Green 
  
 key1 key2                              key2 key1 
  
 a    1         0    1         2        1    a         0    1         2 
  
 b    1         6    7         8             b         6    7         8 
  
 a    2         3    4         5        2    a         3    4         5 
  
 b    2         9   10        11             b         9   10        11
  
 Hierarchical Indexing | 149",NA
Summary Statistics by Level,"Many descriptive and summary statistics on DataFrame and Series have a 
 level
  
 option in which you can specify the level you want to sum by on a particular axis. 
 Consider the above DataFrame; we can sum by level on either the rows or columns 
 like so:
  
 In [279]: frame.sum(level='key2') 
  
 Out[279]: 
  
 state   Ohio       Colorado 
  
 color  Green  Red     Green 
  
 key2 
  
 1          6    8        10 
  
 2         12   14        16
  
 In [280]: frame.sum(level='color', axis=1) 
  
 Out[280]: 
  
 color      Green  Red 
  
 key1 key2 
  
 a    1         2    1
  
  
  2         8    4 
  
 b    1        14    7
  
  
  2        20   10
  
 Under the hood, this utilizes pandas’s 
 groupby
  machinery which will be discussed in 
 more detail later in the book.",NA
Using a DataFrame’s Columns,"It’s not unusual to want to use one or more columns from a DataFrame as the row 
 index; alternatively, you may wish to move the row index into the DataFrame’s col-
 umns. Here’s an example DataFrame:
  
 In [281]: frame = DataFrame({'a': range(7), 'b': range(7, 0, -1),
  
  .....:                    'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'], .....:                    'd': [0, 1, 2, 0, 1, 2, 3]})
  
 In [282]: frame 
  
 Out[282]: 
  
  
  a  b    c  d 
  
 0  0  7  one  0 
  
 1  1  6  one  1 
  
 2  2  5  one  2 
  
 3  3  4  two  0 
  
 4  4  3  two  1 
  
 5  5  2  two  2 
  
 6  6  1  two  3
  
 150 | Chapter 5:Getting Started with pandas",NA
Other pandas Topics,Here are some additional topics that may be of use to you in your data travels.,NA
Integer Indexing,"Working with pandas objects indexed by integers is something that often trips up 
 new users due to some differences with indexing semantics on built-in Python data
  
 Other pandas Topics | 151",NA
Panel Data,"While not a major topic of this book, pandas has a Panel data structure, which you 
 can think of as a three-dimensional analogue of DataFrame. Much of the 
 development focus of pandas has been in tabular data manipulations as these are 
 easier to reason about,
  
 152 | Chapter 5:Getting Started with pandas",NA
CHAPTER 6,NA,NA
"Data Loading, Storage, ",NA,NA
and ,NA,NA
File ,NA,NA
Form,NA,NA
ats,"The tools in this book are of little use if you can’t easily import and export data in 
 Python. I’m going to be focused on input and output with pandas objects, though 
 there are of course numerous tools in other libraries to aid in this process. NumPy, 
 for ex-ample, features low-level but extremely fast binary data loading and storage, 
 including support for memory-mapped array. See 
 Chapter 12
  for more on those.
  
 Input and output typically falls into a few main categories: reading text files and 
 other more efficient on-disk formats, loading data from databases, and interacting 
 with net-work sources like web APIs.",NA
Reading and Writing Data in Text Format,"Python has become a beloved language for text and file munging due to its simple 
 syntax for interacting with files, intuitive data structures, and convenient features 
 like tuple packing and unpacking.
  
 pandas features a number of functions for reading tabular data as a DataFrame 
 object. 
 Table 6-1
  has a summary of all of them, though 
 read_csv
  and 
 read_table
  are 
 likely the ones you’ll use the most.
  
 Table 6-1. Parsing functions in pandas
  
 Function
  
 Description
  
 read_csv 
  
 read_table 
  
 read_fwf 
  
 read_clipboard
  
 Load delimited data from a file, URL, or file-like object. Use comma as 
 default delimiter 
  
 Load delimited data from a file, URL, or file-like object. Use tab (
 '\t'
 ) as 
 default delimiter 
  
 Read data in fixed-width column format (that is, no delimiters)",NA
Reading Text Files in Pieces,"When processing very large files or figuring out the right set of arguments to 
 correctly process a large file, you may only want to read in a small piece of a file or 
 iterate through smaller chunks of the file.
  
 In [871]: result = pd.read_csv('ch06/ex6.csv')
  
 In [872]: result 
  
 Out[872]: 
  
 160 | Chapter 6:Data Loading, Storage, and File Formats",NA
Writing Data Out to Text Format,"Data can also be exported to delimited format. Let’s consider one of the CSV files 
 read above:
  
 In [878]: data = pd.read_csv('ch06/ex5.csv')
  
 In [879]: data 
  
 Out[879]: 
  
  something  a   b   c   d message 
  
 0       one  1   2   3   4     NaN 
  
 1       two  5   6 NaN   8   world 
  
 2     three  9  10  11  12     foo
  
 Using DataFrame’s 
 to_csv
  method, we can write the data out to a comma-separated 
 file:
  
 In [880]: data.to_csv('ch06/out.csv')
  
 In [881]: !cat ch06/out.csv 
  
 ,something,a,b,c,d,message 
  
 0,one,1,2,3.0,4, 
  
 1,two,5,6,,8,world 
  
 2,three,9,10,11.0,12,foo
  
 Other delimiters can be used, of course (writing to 
 sys.stdout
  so it just prints the text 
 result):
  
 In [882]: data.to_csv(sys.stdout, sep='|') 
  
 |something|a|b|c|d|message 
  
 0|one|1|2|3.0|4| 
  
 1|two|5|6||8|world 
  
 2|three|9|10|11.0|12|foo
  
 Missing values appear as empty strings in the output. You might want to denote 
 them by some other sentinel value:
  
 In [883]: data.to_csv(sys.stdout, na_rep='NULL') 
 ,something,a,b,c,d,message 
  
 0,one,1,2,3.0,4,NULL 
  
 1,two,5,6,NULL,8,world 
  
 2,three,9,10,11.0,12,foo
  
 With no other options specified, both the row and column labels are written. Both 
 of these can be disabled:
  
 In [884]: data.to_csv(sys.stdout, index=False, header=False) 
 one,1,2,3.0,4, 
  
 two,5,6,,8,world 
  
 three,9,10,11.0,12,foo
  
 You can also write only a subset of the columns, and in an order of your choosing:
  
 162 | Chapter 6:Data Loading, Storage, and File Formats",NA
Manually Working with Delimited Formats,"Most forms of tabular data can be loaded from disk using functions like 
 pan 
 das.read_table
 . In some cases, however, some manual processing may be necessary. 
 It’s not uncommon to receive a file with one or more malformed lines that trip up 
 read_table
 . To illustrate the basic tools, consider a small CSV file:
  
 In [891]: !cat ch06/ex7.csv 
  
 ""a"",""b"",""c"" 
  
 ""1"",""2"",""3"" 
  
 ""1"",""2"",""3"",""4""
  
 For any file with a single-character delimiter, you can use Python’s built-in 
 csv
  
 module. To use it, pass any open file or file-like object to 
 csv.reader
 :
  
 Reading and Writing Data in Text Format | 163
  
 www.it-ebooks.info",NA
JSON Data,"JSON (short for JavaScript Object Notation) has become one of the standard formats 
 for sending data by HTTP request between web browsers and other applications. It 
 is a much more flexible data format than a tabular text form like CSV. Here is an 
 example:
  
 obj = """""" 
  
 {""name"": ""Wes"",
  
  ""places_lived"": [""United States"", ""Spain"", ""Germany""], ""pet"": null,
  
  ""siblings"": [{""name"": ""Scott"", ""age"": 25, ""pet"": ""Zuko""},
  
  {""name"": 
 ""Katie"", ""age"": 33, ""pet"": ""Cisco""}] } 
  
 """"""
  
 JSON is very nearly valid Python code with the exception of its null value 
 null
  and 
 some other nuances (such as disallowing trailing commas at the end of lists). The 
 basic types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. 
 All of the keys in an object must be strings. There are several Python libraries for 
 reading and writing JSON data. I’ll use 
 json
  here as it is built into the Python 
 standard library. To convert a JSON string to Python form, use 
 json.loads
 :
  
 In [899]: import json
  
 Reading and Writing Data in Text Format | 165",NA
XML and HTML: Web Scraping,"Python has many libraries for reading and writing data in the ubiquitous HTML and 
 XML formats. lxml (
 http://lxml.de
 ) is one that has consistently strong performance 
 in parsing very large files. lxml has multiple programmer interfaces; first I’ll show 
 using 
 lxml.html
  for HTML, then parse some XML using 
 lxml.objectify
 .
  
 Many websites make data available in HTML tables for viewing in a browser, but 
 not downloadable as an easily machine-readable format like JSON, HTML, or XML. I 
 no-ticed that this was the case with Yahoo! Finance’s stock options data. If you 
 aren’t familiar with this data; options are derivative contracts giving you the right 
 to buy (
 call
  option) or sell (
 put
  option) a company’s stock at some particular price 
 (the 
 strike
 ) between now and some fixed point in the future (the 
 expiry
 ). People 
 trade both 
 call
  and 
 put
  options across many strikes and expiries; this data can all be 
 found together in tables on Yahoo! Finance.
  
 166 | Chapter 6:Data Loading, Storage, and File Formats
  
 www.it-ebooks.info",NA
Binary Data Formats,"One of the easiest ways to store data efficiently in binary format is using Python’s 
 built-in 
 pickle
  serialization. Conveniently, pandas objects all have a 
 save
  method 
 which writes the data to disk as a pickle:
  
 In [933]: frame = pd.read_csv('ch06/ex1.csv')
  
 In [934]: frame 
  
 Out[934]: 
  
  
  a   b   c   d message 
  
 0  1   2   3   4   hello 
  
 1  5   6   7   8   world 
  
 2  9  10  11  12     foo
  
 In [935]: frame.save('ch06/frame_pickle')
  
 You read the data back into Python with 
 pandas.load
 , another pickle convenience 
 function:
  
 In [936]: pd.load('ch06/frame_pickle') 
  
 Out[936]: 
  
  
  a   b   c   d message 
  
 0  1   2   3   4   hello 
  
 1  5   6   7   8   world 
  
 2  9  10  11  12     foo
  
  
 pickle
  is only recommended as a short-term storage format. The 
 prob-lem is that it is hard to guarantee that the format will be stable 
 over time; an object pickled today may not unpickle with a later 
 version of a library.
  
 I have made every effort to ensure that this does not occur with 
 pandas, but at some point in the future it may be necessary to 
 “break” the pickle format.",NA
Using HDF5 Format,"There are a number of tools that facilitate efficiently reading and writing large 
 amounts of scientific data in binary format on disk. A popular industry-grade 
 library for this is HDF5, which is a C library with interfaces in many other languages 
 like Java, Python, and MATLAB. The “HDF” in HDF5 stands for 
 hierarchical data 
 format
 . Each HDF5 file contains an internal file system-like node structure enabling 
 you to store multiple datasets and supporting metadata. Compared with simpler 
 formats, HDF5 supports on-the-fly compression with a variety of compressors, 
 enabling data with repeated pat-terns to be stored more efficiently. For very large 
 datasets that don’t fit into memory, HDF5 is a good choice as you can efficiently 
 read and write small sections of much larger arrays.
  
 There are not one but two interfaces to the HDF5 library in Python, PyTables and 
 h5py, each of which takes a different approach to the problem. h5py provides a 
 direct, but high-level interface to the HDF5 API, while PyTables abstracts many of 
 the details of
  
 Binary Data Formats | 171",NA
Reading Microsoft Excel Files,"pandas also supports reading tabular data stored in Excel 2003 (and higher) files 
 using the 
 ExcelFile
  class. Interally 
 ExcelFile
  uses the xlrd and openpyxl packages, so 
 you may have to install them first. To use 
 ExcelFile
 , create an instance by passing a 
 path to an 
 xls
  or 
 xlsx
  file:
  
 xls_file = pd.ExcelFile('data.xls')
  
 Data stored in a sheet can then be read into DataFrame using 
 parse
 :
  
 table = xls_file.parse('Sheet1')
  
 172 | Chapter 6:Data Loading, Storage, and File Formats
  
 www.it-ebooks.info",NA
Interacting with HTML and Web APIs,"Many websites have public APIs providing data feeds via JSON or some other 
 format. There are a number of ways to access these APIs from Python; one easy-to-
 use method that I recommend is the requests package (
 http://docs.python-
 requests.org
 ). To search for the words “python pandas” on Twitter, we can make an 
 HTTP 
 GET
  request like so:
  
 In [944]: import requests
  
 In [945]: url = 'http://search.twitter.com/search.json?q=python%20pandas'
  
 In [946]: resp = requests.get(url)
  
 In [947]: resp 
  
 Out[947]: <Response [200]>
  
 The Response object’s 
 text
  attribute contains the content of the 
 GET
  query. Many 
 web APIs will return a JSON string that must be loaded into a Python object:
  
 In [948]: import json
  
 In [949]: data = json.loads(resp.text)
  
 In [950]: data.keys() 
  
 Out[950]: 
  
 [u'next_page',
  
  u'completed_in',
  
  u'max_id_str',
  
  u'since_id_str',
  
  u'refresh_url',
  
  u'results',
  
  u'since_id',
  
  u'results_per_page',
  
  u'query',
  
  u'max_id',
  
  u'page']
  
 The 
 results
  field in the response contains a list of tweets, each of which is 
 represented as a Python dict that looks like:
  
 {u'created_at': u'Mon, 25 Jun 2012 17:50:33 +0000',
  
  u'from_user': u'wesmckinn',
  
  u'from_user_id': 115494880,
  
  u'from_user_id_str': u'115494880',
  
  u'from_user_name': u'Wes McKinney',
  
  u'geo': None,
  
  u'id': 217313849177686018,
  
  u'id_str': u'217313849177686018',
  
  u'iso_language_code': u'pt',
  
  u'metadata': {u'result_type': u'recent'},
  
  u'source': u'<a href=""http://twitter.com/"">web</a>',
  
  u'text': u'Lunchtime pandas-fu http://t.co/SI70xZZQ #pydata', 
 u'to_user': None,
  
  u'to_user_id': 0,
  
 Interacting with HTML and Web APIs | 173",NA
Interacting with Databases,"In many applications data rarely comes from text files, that being a fairly inefficient 
 way to store large amounts of data. SQL-based relational databases (such as SQL 
 Server, PostgreSQL, and MySQL) are in wide use, and many alternative non-SQL 
 (so-called 
 NoSQL
 ) databases have become quite popular. The choice of database is 
 usually de-pendent on the performance, data integrity, and scalability needs of an 
 application.
  
 Loading data from SQL into a DataFrame is fairly straightforward, and pandas has 
 some functions to simplify the process. As an example, I’ll use an in-memory SQLite 
 database using Python’s built-in 
 sqlite3
  driver:
  
 import sqlite3
  
 query = """""" 
  
 CREATE TABLE test 
  
 (a VARCHAR(20), b VARCHAR(20),
  
  c REAL,        d INTEGER 
  
 );""""""
  
 174 | Chapter 6:Data Loading, Storage, and File Formats
  
 www.it-ebooks.info",NA
Storing and Loading Data in MongoDB,"NoSQL databases take many different forms. Some are simple dict-like key-value 
 stores like BerkeleyDB or Tokyo Cabinet, while others are document-based, with a 
 dict-like object being the basic unit of storage. I've chosen MongoDB 
 (
 http://mongodb.org
 ) for my example. I started a MongoDB instance locally on my 
 machine, and connect to it on the default port using 
 pymongo
 , the official driver for 
 MongoDB:
  
 import pymongo 
  
 con = pymongo.Connection('localhost', port=27017)
  
 Documents stored in MongoDB are found in collections inside databases. Each 
 running instance of the MongoDB server can have multiple databases, and each 
 database can have multiple collections. Suppose I wanted to store the Twitter API 
 data from earlier in the chapter. First, I can access the (currently empty) tweets 
 collection:
  
 tweets = con.db.tweets
  
 Then, I load the list of tweets and write each of them to the collection using 
 tweets.save
  (which writes the Python dict to MongoDB):
  
 import requests, json 
  
 url = 'http://search.twitter.com/search.json?q=python%20pandas' data = 
 json.loads(requests.get(url).text)
  
 for tweet in data['results']:
  
  
  tweets.save(tweet)
  
 Now, if I wanted to get all of my tweets (if any) from the collection, I can query the 
 collection with the following syntax:
  
 cursor = tweets.find({'from_user': 'wesmckinn'})
  
 The cursor returned is an iterator that yields each document as a dict. As above I 
 can convert this into a DataFrame, optionally extracting a subset of the data fields 
 in each tweet:
  
 tweet_fields = ['created_at', 'from_user', 'id', 'text'] result = 
 DataFrame(list(cursor), columns=tweet_fields)
  
 176 | Chapter 6:Data Loading, Storage, and File Formats",NA
CHAPTER 7,NA,NA
"Data Wrangling: Clean, ",NA,NA
"Transform, ",NA,NA
"Merge, ",NA,NA
Reshape,"Much of the programming work in data analysis and modeling is spent on data 
 prep-aration: loading, cleaning, transforming, and rearranging. Sometimes the way 
 that data is stored in files or databases is not the way you need it for a data 
 processing application. Many people choose to do ad hoc processing of data from 
 one form to another using a general purpose programming, like Python, Perl, R, or 
 Java, or UNIX text processing tools like sed or awk. Fortunately, pandas along with 
 the Python standard library pro-vide you with a high-level, flexible, and high-
 performance set of core manipulations and algorithms to enable you to wrangle 
 data into the right form without much trouble.
  
 If you identify a type of data manipulation that isn’t anywhere in this book or 
 elsewhere in the pandas library, feel free to suggest it on the mailing list or GitHub 
 site. Indeed, much of the design and implementation of pandas has been driven by 
 the needs of real world applications.",NA
Combining and Merging Data Sets,"Data contained in pandas objects can be combined together in a number of built-in 
 ways:
  
 •
  pandas.merge
  connects rows in DataFrames based on one or more keys. This will 
 be familiar to users of SQL or other relational databases, as it implements 
 database 
 join
  operations.
  
 •
  pandas.concat
  glues or stacks together objects along an axis.
  
 •
  combine_first
  instance method enables splicing together overlapping data to fill 
  
 in missing values in one object with values from another.
  
 I will address each of these and give a number of examples. They’ll be utilized in ex-
 amples throughout the rest of the book.",NA
Database-style DataFrame Merges,"Merge
  or 
 join
  operations combine data sets by linking rows using one or more 
 keys
 . 
 These operations are central to relational databases. The 
 merge
  function in pandas 
 is the main entry point for using these algorithms on your data.
  
 Let’s start with a simple example:
  
 In [15]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
  
  ....:                  'data1': 
 range(7)})
  
 In [16]: df2 = DataFrame({'key': ['a', 'b', 'd'],
  
  ....:                  
 'data2': range(3)})
  
 In [17]: df1        In [18]: df2 
  
 Out[17]:            Out[18]:
  
  
  data1 key           data2 key 
  
 0      0   b        0      0   a 
  
 1      1   b        1      1   b 
  
 2      2   a        2      2   d 
  
 3      3   c 
  
 4      4   a 
  
 5      5   a 
  
 6      6   b
  
 This is an example of a 
 many-to-one
  merge situation; the data in 
 df1
  has multiple 
 rows labeled 
 a
  and 
 b
 , whereas 
 df2
  has only one row for each value in the 
 key
  
 column. Calling 
 merge
  with these objects we obtain:
  
 In [19]: pd.merge(df1, df2) 
  
 Out[19]:
  
  
  data1 key  data2 
  
 0      2   a      0 
  
 1      4   a      0 
  
 2      5   a      0 
  
 3      0   b      1 
  
 4      1   b      1 
  
 5      6   b      1
  
 Note that I didn’t specify which column to join on. If not specified, 
 merge
  uses the 
 overlapping column names as the keys. It’s a good practice to specify explicitly, 
 though:
  
 In [20]: pd.merge(df1, df2, on='key') 
  
 Out[20]:
  
  
  data1 key  data2 
  
 0      2   a      0 
  
 1      4   a      0 
  
 2      5   a      0 
  
 3      0   b      1 
  
 4      1   b      1 
  
 5      6   b      1
  
 If the column names are different in each object, you can specify them separately:
  
 In [21]: df3 = DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
  
  ....:                  'data1': 
 range(7)})
  
 178 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape",NA
Merging on Index,"In some cases, the merge key or keys in a DataFrame will be found in its index. In 
 this case, you can pass 
 left_index=True
  or 
 right_index=True
  (or both) to indicate that 
 the index should be used as the merge key:
  
 In [36]: left1 = DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],
  
  ....:                   'value': 
 range(6)})
  
 In [37]: right1 = DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])
  
 In [38]: left1        In [39]: right1 
  
 Out[38]:              Out[39]:
  
  key  value             group_val 
  
 0   a      0          a        3.5 
  
 1   b      1          b        7.0 
  
 2   a      2 
  
 3   a      3 
  
 4   b      4 
  
 5   c      5
  
 In [40]: pd.merge(left1, right1, left_on='key', right_index=True) Out[40]:
  
  key  value  group_val 
  
 0   a      0        3.5 
  
 2   a      2        3.5 
  
 3   a      3        3.5 
  
 1   b      1        7.0 
  
 4   b      4        7.0
  
 Since the default merge method is to intersect the join keys, you can instead form 
 the union of them with an outer join:
  
 In [41]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer') Out[41]:
  
  key  value  group_val 
  
 0   a      0        3.5 
  
 2   a      2        3.5 
  
 3   a      3        3.5 
  
 1   b      1        7.0 
  
 4   b      4        7.0 
  
 5   c      5        NaN
  
 With hierarchically-indexed data, things are a bit more complicated:
  
 In [42]: lefth = DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], ....:                    
 'key2': [2000, 2001, 2002, 2001, 2002],
  
  ....:                    'data': np.arange(5.)})
  
 In [43]: righth = DataFrame(np.arange(12).reshape((6, 2)),
  
  ....:                    index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], ....:                           [2001, 2000, 
 2000, 2000, 2001, 2002]],
  
  ....:                    columns=['event1', 'event2'])
  
 In [44]: lefth               In [45]: righth 
  
 Out[44]:                     Out[45]:
  
 182 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Concatenating Along an Axis,"Another kind of data combination operation is alternatively referred to as 
 concatena-tion, binding, or stacking. NumPy has a 
 concatenate
  function for doing 
 this with raw NumPy arrays:
  
 In [58]: arr = np.arange(12).reshape((3, 4))
  
 In [59]: arr 
  
 Out[59]: 
  
 array([[ 0,  1,  2,  3],
  
  
  [ 4,  5,  6,  7],
  
  
  [ 8,  9, 10, 11]])
  
 In [60]: np.concatenate([arr, arr], axis=1) 
  
 Out[60]: 
  
 array([[ 0,  1,  2,  3,  0,  1,  2,  3],
  
  
  [ 4,  5,  6,  7,  4,  5,  6,  7],
  
  
  [ 8,  9, 10, 11,  8,  9, 10, 11]])
  
 In the context of pandas objects such as Series and DataFrame, having labeled axes 
 enable you to further generalize array concatenation. In particular, you have a 
 number of additional things to think about:
  
 • If the objects are indexed differently on the other axes, should the collection of 
  
 axes be unioned or intersected?
  
 • Do the groups need to be identifiable in the resulting object?
  
 • Does the concatenation axis matter at all?
  
 The 
 concat
  function in pandas provides a consistent way to address each of these 
 con-cerns. I’ll give a number of examples to illustrate how it works. Suppose we 
 have three Series with no index overlap:
  
 In [61]: s1 = Series([0, 1], index=['a', 'b'])
  
 In [62]: s2 = Series([2, 3, 4], index=['c', 'd', 'e'])
  
 In [63]: s3 = Series([5, 6], index=['f', 'g'])
  
 Calling 
 concat
  with these object in a list glues together the values and indexes:
  
 In [64]: pd.concat([s1, s2, s3]) 
  
 Out[64]: 
  
 a    0 
  
 b    1 
  
 c    2 
  
 d    3 
  
 e    4 
  
 f    5 
  
 g    6
  
 Combining and Merging Data Sets | 185
  
 www.it-ebooks.info",NA
Combining Data with Overlap,"Another data combination situation can’t be expressed as either a merge or 
 concate-nation operation. You may have two datasets whose indexes overlap in full 
 or part. As a motivating example, consider NumPy’s 
 where
  function, which 
 expressed a vectorized if-else:
  
 188 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Reshaping and Pivoting,"There are a number of fundamental operations for rearranging tabular data. These 
 are alternatingly referred to as 
 reshape
  or 
 pivot
  operations.
  
 Reshaping and Pivoting | 189
  
 www.it-ebooks.info",NA
Reshaping with Hierarchical Indexing,"Hierarchical indexing provides a consistent way to rearrange data in a DataFrame. 
 There are two primary actions:
  
 •
  stack
 : this “rotates” or pivots from the columns in the data to the rows
  
 •
  unstack
 : this pivots from the rows into the columns
  
 I’ll illustrate these operations through a series of examples. Consider a small 
 DataFrame with string arrays as row and column indexes:
  
 In [94]: data = DataFrame(np.arange(6).reshape((2, 3)),
  
  ....:                  index=pd.Index(['Ohio', 'Colorado'], name='state'), ....:                  
 columns=pd.Index(['one', 'two', 'three'], name='number'))
  
 In [95]: data 
  
 Out[95]: 
  
 number    one  two  three 
  
 state 
  
 Ohio        0    1      2 
  
 Colorado    3    4      5
  
 Using the 
 stack
  method on this data pivots the columns into the rows, producing a 
 Series:
  
 In [96]: result = data.stack()
  
 In [97]: result 
  
 Out[97]: 
  
 state     number 
  
 Ohio      one       0
  
  
  two       1
  
  
  three     2 
  
 Colorado  one       3
  
  
  two       4
  
  
  three     5
  
 From a hierarchically-indexed Series, you can rearrange the data back into a 
 DataFrame with 
 unstack
 :
  
 In [98]: result.unstack() 
  
 Out[98]: 
  
 number    one  two  three 
  
 state 
  
 Ohio        0    1      2 
  
 Colorado    3    4      5
  
 By default the innermost level is unstacked (same with 
 stack
 ). You can unstack a dif-
 ferent level by passing a level number or name:
  
 In [99]: result.unstack(0)        In [100]: result.unstack('state') Out[99]:                          
 Out[100]: 
  
 state   Ohio  Colorado            state   Ohio  Colorado 
  
 number                            number 
  
 one        0         3            one        0         3
  
 190 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Pivoting “long” to “wide” Format,"A common way to store multiple time series in databases and CSV is in so-called 
 long 
 or 
 stacked
  format:
  
 In [116]: ldata[:10] 
  
 Out[116]:
  
  
  date     item     value 
  
 0 1959-03-31 00:00:00  realgdp  2710.349 
  
 1 1959-03-31 00:00:00     infl     0.000 
  
 2 1959-03-31 00:00:00    unemp     5.800 
  
 3 1959-06-30 00:00:00  realgdp  2778.801 
  
 4 1959-06-30 00:00:00     infl     2.340 
  
 5 1959-06-30 00:00:00    unemp     5.100 
  
 6 1959-09-30 00:00:00  realgdp  2775.488 
  
 7 1959-09-30 00:00:00     infl     2.740 
  
 8 1959-09-30 00:00:00    unemp     5.300 
  
 9 1959-12-31 00:00:00  realgdp  2785.204
  
 Data is frequently stored this way in relational databases like MySQL as a fixed 
 schema (column names and data types) allows the number of distinct values in the 
 item
  column to increase or decrease as data is added or deleted in the table. In the 
 above example 
 date
  and 
 item
  would usually be the primary keys (in relational 
 database parlance), offering both relational integrity and easier joins and 
 programmatic queries in many cases. The downside, of course, is that the data may 
 not be easy to work with in long format; you might prefer to have a DataFrame 
 containing one column per distinct 
 item
  value indexed by timestamps in the 
 date
  
 column. DataFrame’s 
 pivot
  method per-forms exactly this transformation:
  
 In [117]: pivoted = ldata.pivot('date', 'item', 'value')
  
 In [118]: pivoted.head() 
  
 Out[118]: 
  
 item        infl   realgdp  unemp 
  
 date 
  
 1959-03-31  0.00  2710.349    5.8 
  
 1959-06-30  2.34  2778.801    5.1 
  
 1959-09-30  2.74  2775.488    5.3 
  
 1959-12-31  0.27  2785.204    5.6 
  
 1960-03-31  2.31  2847.699    5.2
  
 The first two values passed are the columns to be used as the row and column 
 index, and finally an optional value column to fill the DataFrame. Suppose you had 
 two value columns that you wanted to reshape simultaneously:
  
 In [119]: ldata['value2'] = np.random.randn(len(ldata))
  
 In [120]: ldata[:10] 
  
 Out[120]:
  
 192 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Data Transformation,"So far in this chapter we’ve been concerned with rearranging data. Filtering, 
 cleaning, and other tranformations are another class of important operations.",NA
Removing Duplicates,"Duplicate rows may be found in a DataFrame for any number of reasons. Here is an 
 example:
  
 In [126]: data = DataFrame({'k1': ['one'] * 3 + ['two'] * 4, .....:                   
 'k2': [1, 1, 2, 3, 3, 4, 4]})
  
 In [127]: data 
  
 Out[127]:
  
  
  k1  k2 
  
 0  one   1 
  
 1  one   1 
  
 2  one   2 
  
 3  two   3 
  
 4  two   3 
  
 5  two   4 
  
 6  two   4
  
 The DataFrame method 
 duplicated
  returns a boolean Series indicating whether each 
 row is a duplicate or not:
  
 In [128]: data.duplicated() 
  
 Out[128]: 
  
 0    False 
  
 1     True 
  
 2    False 
  
 3    False 
  
 4     True 
  
 5    False 
  
 6     True
  
 Relatedly, 
 drop_duplicates
  returns a DataFrame where the 
 duplicated
  array is 
 True
 :
  
 In [129]: data.drop_duplicates() 
  
 Out[129]:
  
  
  k1  k2 
  
 0  one   1 
  
 2  one   2 
  
 3  two   3 
  
 5  two   4
  
 Both of these methods by default consider all of the columns; alternatively you can 
 specify any subset of them to detect duplicates. Suppose we had an additional 
 column of values and wanted to filter duplicates only based on the 
 'k1'
  column:
  
 In [130]: data['v1'] = range(7)
  
 In [131]: data.drop_duplicates(['k1'])
  
 194 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Transforming Data Using a Function or Mapping,"For many data sets, you may wish to perform some transformation based on the 
 values in an array, Series, or column in a DataFrame. Consider the following 
 hypothetical data collected about some kinds of meat:
  
 In [133]: data = DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', .....:                            
 'corned beef', 'Bacon', 'pastrami', 'honey ham', .....:                            'nova lox'],
  
  .....:                   'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
  
 In [134]: data 
  
 Out[134]:
  
  
  food  ounces 
  
 0        bacon     4.0 
  
 1  pulled pork     3.0 
  
 2        bacon    12.0 
  
 3     Pastrami     6.0 
  
 4  corned beef     7.5 
  
 5        Bacon     8.0 
  
 6     pastrami     3.0 
  
 7    honey ham     5.0 
  
 8     nova lox     6.0
  
 Suppose you wanted to add a column indicating the type of animal that each food 
 came from. Let’s write down a mapping of each distinct meat type to the kind of 
 animal:
  
 meat_to_animal = {
  
  'bacon': 'pig',
  
  'pulled pork': 'pig',
  
  'pastrami': 'cow',
  
  'corned beef': 'cow',
  
  'honey ham': 'pig',
  
  'nova lox': 'salmon' 
  
 }
  
 Data Transformation | 195
  
 www.it-ebooks.info",NA
Replacing Values,"Filling in missing data with the 
 fillna
  method can be thought of as a special case of 
 more general value replacement. While 
 map
 , as you’ve seen above, can be used to 
 modify a subset of values in an object, 
 replace
  provides a simpler and more flexible 
 way to do so. Let’s consider this Series:
  
 In [139]: data = Series([1., -999., 2., -999., -1000., 3.])
  
 In [140]: data 
  
 Out[140]: 
  
 0       1 
  
 1    -999 
  
 2       2 
  
 3    -999 
  
 4   -1000 
  
 5       3
  
 196 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape",NA
Renaming Axis Indexes,"Like values in a Series, axis labels can be similarly transformed by a function or 
 mapping of some form to produce new, differently labeled objects. The axes can 
 also be modified in place without creating a new data structure. Here’s a simple 
 example:
  
 In [145]: data = DataFrame(np.arange(12).reshape((3, 4)),
  
  .....:                  index=['Ohio', 'Colorado', 'New York'], .....:                  
 columns=['one', 'two', 'three', 'four'])
  
 Data Transformation | 197
  
 www.it-ebooks.info",NA
Discretization and Binning,"Continuous data is often discretized or otherwised separated into “bins” for 
 analysis. Suppose you have data about a group of people in a study, and you want to 
 group them into discrete age buckets:
  
 In [153]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
  
 Let’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. 
 To do so, you have to use 
 cut
 , a function in pandas:
  
 In [154]: bins = [18, 25, 35, 60, 100]
  
 In [155]: cats = pd.cut(ages, bins)
  
 In [156]: cats 
  
 Out[156]: 
  
 Categorical: 
  
 array([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],
  
  
  (35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object) Levels (4): 
 Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)
  
 The object pandas returns is a special 
 Categorical
  object. You can treat it like an 
 array of strings indicating the bin name; internally it contains a 
 levels
  array 
 indicating the distinct category names along with a labeling for the 
 ages
  data in the 
 labels
  attribute:
  
 In [157]: cats.labels 
  
 Out[157]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1])
  
 In [158]: cats.levels 
  
 Out[158]: Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)
  
 In [159]: pd.value_counts(cats) 
  
 Out[159]: 
  
 (18, 25]     5 
  
 (35, 60]     3 
  
 (25, 35]     3 
  
 (60, 100]    1
  
 Consistent with mathematical notation for intervals, a parenthesis means that the 
 side is 
 open
  while the square bracket means it is 
 closed
  (inclusive). Which side is 
 closed can be changed by passing 
 right=False
 :
  
 In [160]: pd.cut(ages, [18, 26, 36, 61, 100], right=False) 
  
 Out[160]: 
  
 Categorical: 
  
 array([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),
  
  
  [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object) Levels (4): 
 Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)
  
 You can also pass your own bin names by passing a list or array to the 
 labels
  option:
  
 In [161]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
  
 In [162]: pd.cut(ages, bins, labels=group_names) 
 Out[162]:
  
 Data Transformation | 199",NA
Detecting and Filtering Outliers,"Filtering or transforming outliers is largely a matter of applying array operations. 
 Con-sider a DataFrame with some normally distributed data:
  
 In [170]: np.random.seed(12345)
  
 In [171]: data = DataFrame(np.random.randn(1000, 4))
  
 In [172]: data.describe() 
  
 Out[172]:
  
  
  0            1            2            3 count  1000.000000  
 1000.000000  1000.000000  1000.000000 mean     -0.067684     
 0.067924     0.025598    -0.002298 std       0.998035     0.992106     
 1.006835     0.996794 min      -3.428254    -3.548824    -3.184377    
 -3.745356 25%      -0.774890    -0.591841    -0.641675    -0.644144 
 50%      -0.116401     0.101143     0.002073    -0.013611 75%       
 0.616366     0.780282     0.680391     0.654328 max       3.366626     
 2.653656     3.260383     3.927528
  
 Suppose you wanted to find values in one of the columns exceeding three in 
 magnitude:
  
 In [173]: col = data[3]
  
 In [174]: col[np.abs(col) > 3] 
  
 Out[174]: 
  
 97     3.927528 
  
 305   -3.399312 
  
 400   -3.745356 
  
 Name: 3
  
 To select all rows having a value exceeding 3 or -3, you can use the 
 any
  method on a 
 boolean DataFrame:
  
 In [175]: data[(np.abs(data) > 3).any(1)] 
  
 Out[175]:
  
  
  0         1         2         3 
  
 5   -0.539741  0.476985  3.248944 -1.021228 
  
 97  -0.774363  0.552936  0.106061  3.927528 
  
 102 -0.655054 -0.565230  3.176873  0.959533 
  
 305 -2.315555  0.457246 -0.025907 -3.399312 
  
 324  0.050188  1.951312  3.260383  0.963301 
  
 400  0.146326  0.508391 -0.196713 -3.745356 
  
 499 -0.293333 -0.242459 -3.056990  1.918403 
  
 523 -3.428254 -0.296336 -0.439938 -0.867165 
  
 586  0.275144  1.179227 -3.184377  1.369891 
  
 808 -0.362528 -3.548824  1.553205 -2.186301 
  
 900  3.366626 -2.372214  0.851010  1.332846
  
 Values can just as easily be set based on these criteria. Here is code to cap values 
 outside the interval -3 to 3:
  
 Data Transformation | 201",NA
Permutation and Random Sampling,"Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do 
 using the 
 numpy.random.permutation
  function. Calling 
 permutation
  with the length of 
 the axis you want to permute produces an array of integers indicating the new 
 ordering:
  
 In [178]: df = DataFrame(np.arange(5 * 4).reshape(5, 4))
  
 In [179]: sampler = np.random.permutation(5)
  
 In [180]: sampler 
  
 Out[180]: array([1, 0, 2, 3, 4])
  
 That array can then be used in 
 ix
 -based indexing or the 
 take
  function:
  
 In [181]: df             In [182]: df.take(sampler) Out[181]:                
 Out[182]:
  
  
  0   1   2   3            0   1   2   3 
  
 0   0   1   2   3        1   4   5   6   7 
  
 1   4   5   6   7        0   0   1   2   3 
  
 2   8   9  10  11        2   8   9  10  11 
  
 3  12  13  14  15        3  12  13  14  15 
  
 4  16  17  18  19        4  16  17  18  19
  
 To select a random subset without replacement, one way is to slice off the first 
 k
  
 ele-ments of the array returned by 
 permutation
 , where 
 k
  is the desired subset size. 
 There are much more efficient sampling-without-replacement algorithms, but this 
 is an easy strategy that uses readily available tools:
  
 In [183]: df.take(np.random.permutation(len(df))[:3]) 
 Out[183]:
  
  
  0   1   2   3 
  
 1   4   5   6   7 
  
 3  12  13  14  15 
  
 4  16  17  18  19
  
 To generate a sample 
 with
  replacement, the fastest way is to use 
 np.random.randint
  
 to draw random integers:
  
 202 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape",NA
Computing Indicator/Dummy Variables,"Another type of transformation for statistical modeling or machine learning 
 applica-tions is converting a categorical variable into a “dummy” or “indicator” 
 matrix. If a column in a DataFrame has 
 k
  distinct values, you would derive a matrix 
 or DataFrame containing 
 k
  columns containing all 1’s and 0’s. pandas has a 
 get_dummies
  function for doing this, though devising one yourself is not difficult. 
 Let’s return to an earlier ex-ample DataFrame:
  
 In [189]: df = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
  
  .....:                 
 'data1': range(6)})
  
 In [190]: pd.get_dummies(df['key']) 
  
 Out[190]:
  
  
  a  b  c 
  
 0  0  1  0 
  
 1  0  1  0 
  
 2  1  0  0 
  
 3  0  0  1 
  
 4  1  0  0 
  
 5  0  1  0
  
 In some cases, you may want to add a prefix to the columns in the indicator 
 DataFrame, which can then be merged with the other data. 
 get_dummies
  has a prefix 
 argument for doing just this:
  
 In [191]: dummies = pd.get_dummies(df['key'], prefix='key')
  
 In [192]: df_with_dummy = df[['data1']].join(dummies)
  
 In [193]: df_with_dummy 
  
 Out[193]:
  
  
  data1  key_a  key_b  key_c 
  
 0      0      0      1      0 
  
 1      1      0      1      0 
  
 2      2      1      0      0 
  
 3      3      0      0      1 
  
 4      4      1      0      0 
  
 5      5      0      1      0
  
 Data Transformation | 203
  
 www.it-ebooks.info",NA
String Manipulation,"Python has long been a popular data munging language in part due to its ease-of-
 use for string and text processing. Most text operations are made simple with the 
 string object’s built-in methods. For more complex pattern matching and text 
 manipulations, regular expressions may be needed. pandas adds to the mix by 
 enabling you to apply string and regular expressions concisely on whole arrays of 
 data, additionally handling the annoyance of missing data.
  
 String Manipulation | 205
  
 www.it-ebooks.info",NA
String Object Methods,"In many string munging and scripting applications, built-in string methods are suffi-
 cient. As an example, a comma-separated string can be broken into pieces with 
 split
 :
  
 In [208]: val = 'a,b,  guido'
  
 In [209]: val.split(',') 
  
 Out[209]: ['a', 'b', '  guido']
  
 split
  is often combined with 
 strip
  to trim whitespace (including newlines):
  
 In [210]: pieces = [x.strip() for x in val.split(',')]
  
 In [211]: pieces 
  
 Out[211]: ['a', 'b', 'guido']
  
 These substrings could be concatenated together with a two-colon delimiter using 
 ad-dition:
  
 In [212]: first, second, third = pieces
  
 In [213]: first + '::' + second + '::' + third Out[213]: 
 'a::b::guido'
  
 But, this isn’t a practical generic method. A faster and more Pythonic way is to pass 
 a list or tuple to the 
 join
  method on the string 
 '::'
 :
  
 In [214]: '::'.join(pieces) 
  
 Out[214]: 'a::b::guido'
  
 Other methods are concerned with locating substrings. Using Python’s 
 in
  keyword 
 is the best way to detect a substring, though 
 index
  and 
 find
  can also be used:
  
 In [215]: 'guido' in val 
  
 Out[215]: True
  
 In [216]: val.index(',')        In [217]: val.find(':') Out[216]: 1                     
 Out[217]: -1
  
 Note the difference between 
 find
  and 
 index
  is that 
 index
  raises an exception if the 
 string isn’t found (versus returning -1):
  
 In [218]: val.index(':')
  
 ---------------------------------------------------------------------------ValueError                                
 Traceback (most recent call last) <ipython-input-218-280f8b2856ce> in <module>()
  
 ----> 1 val.index(':') 
  
 ValueError: substring not found
  
 Relatedly, 
 count
  returns the number of occurrences of a particular substring:
  
 In [219]: val.count(',') 
  
 Out[219]: 2
  
 replace
  will substitute occurrences of one pattern for another. This is commonly 
 used to delete patterns, too, by passing an empty string:
  
 206 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Regular expressions,"Regular expressions
  provide a flexible way to search or match string patterns in 
 text. A single expression, commonly called a 
 regex
 , is a string formed according to 
 the regular expression language. Python’s built-in 
 re
  module is responsible for 
 applying regular expressions to strings; I’ll give a number of examples of its use 
 here.
  
  
 The art of writing regular expressions could be a chapter of its own 
 and thus is outside the book’s scope. There are many excellent 
 tutorials and references on the internet, such as Zed Shaw’s 
 Learn 
 Regex The Hard Way
  (
 http://regex.learncodethehardway.org/book/
 ).
  
 The 
 re
  module functions fall into three categories: pattern matching, substitution, 
 and splitting. Naturally these are all related; a regex describes a pattern to locate in 
 the text, which can then be used for many purposes. Let’s look at a simple example: 
 suppose I wanted to split a string with a variable number of whitespace characters 
 (tabs, spaces, and newlines). The regex describing one or more whitespace 
 characters is 
 \s+
 :
  
 String Manipulation | 207",NA
Vectorized string functions in pandas,"Cleaning up a messy data set for analysis often requires a lot of string munging and 
 regularization. To complicate matters, a column containing strings will sometimes 
 have missing data:
  
 In [244]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',
  
  .....:         
 'Rob': 'rob@gmail.com', 'Wes': np.nan}
  
 In [245]: data = Series(data)
  
 In [246]: data                  In [247]: data.isnull() Out[246]:                       
 Out[247]: 
  
 Dave     dave@google.com        Dave     False 
  
 Rob        rob@gmail.com        Rob      False 
  
 Steve    steve@gmail.com        Steve    False 
  
 Wes                  NaN        Wes       True
  
 210 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
Example: USDA Food Database,"The US Department of Agriculture makes available a database of food nutrient 
 infor-mation. Ashley Williams, an English hacker, has made available a version of 
 this 
 da-tabase 
 in 
 JSON 
 format 
 (
 http://ashleyw.co.uk/project/food-nutrient-
 database
 ). The re-cords look like this:
  
 {
  
  ""id"": 21441,
  
  ""description"": ""KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY, Wing, 
 meat and skin with breading"",
  
  ""tags"": [""KFC""],
  
  ""manufacturer"": ""Kentucky Fried Chicken"",
  
  ""group"": ""Fast Foods"",
  
  ""portions"": [
  
   
  {
  
    
  ""amount"": 1,
  
    
  ""unit"": ""wing, with skin"",
  
    
  ""grams"": 68.0
  
   
  },
  
 212 | Chapter 7:Data Wrangling: Clean, Transform, Merge, Reshape
  
 www.it-ebooks.info",NA
CHAPTER 8,NA,NA
Plotting and Visualization,"Making plots and static or interactive visualizations is one of the most important 
 tasks in data analysis. It may be a part of the exploratory process; for example, 
 helping iden-tify outliers, needed data transformations, or coming up with ideas for 
 models. For others, building an interactive visualization for the web using a toolkit 
 like d3.js (
 http: //d3js.org/
 ) may be the end goal. Python has many visualization 
 tools (see the end of this chapter), but I’ll be mainly focused on matplotlib 
 (
 http://matplotlib.sourceforge .net
 ).
  
 matplotlib is a (primarily 2D) desktop plotting package designed for creating 
 publica-tion-quality plots. The project was started by John Hunter in 2002 to enable 
 a MAT-LAB-like plotting interface in Python. He, Fernando Pérez (of IPython), and 
 others have collaborated for many years since then to make IPython combined with 
 matplotlib a very functional and productive environment for scientific computing. 
 When used in tandem with a GUI toolkit (for example, within IPython), matplotlib 
 has interactive features like zooming and panning. It supports many different GUI 
 backends on all operating systems and additionally can export graphics to all of the 
 common vector and raster graphics formats: PDF, SVG, JPG, PNG, BMP, GIF, etc. I 
 have used it to produce almost all of the graphics outside of diagrams in this book.
  
 matplotlib has a number of add-on toolkits, such as 
 mplot3d
  for 3D plots and 
 basemap 
 for mapping and projections. I will give an example using 
 basemap
  to plot 
 data on a map and to read 
 shapefiles
  at the end of the chapter.
  
 To follow along with the code examples in the chapter, make sure you have started 
 IPython in Pylab mode (
 ipython --pylab
 ) or enabled GUI event loop integration with 
 the 
 %gui
  magic.",NA
A Brief matplotlib API Primer,"There are several ways to interact with matplotlib. The most common is through 
 pylab mode
  in IPython by running 
 ipython --pylab
 . This launches IPython configured 
 to be able to support the matplotlib GUI backend of your choice (Tk, wxPython, 
 PyQt, Mac
  
 219",NA
Figures and Subplots,"Plots in matplotlib reside within a 
 Figure
  object. You can create a new figure with 
 plt.figure
 :
  
 In [13]: fig = plt.figure()
  
 220 | Chapter 8:Plotting and Visualization",NA
"Colors, Markers, and Line Styles","Matplotlib’s main 
 plot
  function accepts arrays of X and Y coordinates and optionally 
 a string abbreviation indicating color and line style. For example, to plot 
 x
  versus 
 y
  
 with green dashes, you would execute:
  
 ax.plot(x, y, 'g--')
  
 This way of specifying both color and linestyle in a string is provided as a 
 convenience; in practice if you were creating plots programmatically you might 
 prefer not to have to munge strings together to create plots with the desired style. 
 The same plot could also have been expressed more explicitly as:
  
 ax.plot(x, y, linestyle='--', color='g')
  
 There are a number of color abbreviations provided for commonly-used colors, but 
 any color on the spectrum can be used by specifying its RGB value (for example, 
 '#CECE CE'
 ). You can see the full set of linestyles by looking at the docstring for 
 plot
 .
  
 Line plots can additionally have 
 markers
  to highlight the actual data points. Since 
 mat-plotlib creates a continuous line plot, interpolating between points, it can 
 occasionally be unclear where the points lie. The marker can be part of the style 
 string, which must have color followed by marker type and line style (see 
 Figure 8-
 6
 ):
  
 In [28]: plt.plot(randn(30).cumsum(), 'ko--')
  
 224 | Chapter 8:Plotting and Visualization",NA
"Ticks, Labels, and Legends","For most kinds of plot decorations, there are two main ways to do things: using the 
 procedural 
 pyplot
  interface (which will be very familiar to MATLAB users) and the 
 more object-oriented native matplotlib API.
  
 The 
 pyplot
  interface, designed for interactive use, consists of methods like 
 xlim, 
 xticks,
  and 
 xticklabels
 . These control the plot range, tick locations, and tick labels, 
 respectively. They can be used in two ways:
  
 • Called with no arguments returns the current parameter value. For example 
  
 plt.xlim()
  returns the current X axis plotting range
  
 A Brief matplotlib API Primer | 225
  
 www.it-ebooks.info",NA
Annotations and Drawing on a Subplot,"In addition to the standard plot types, you may wish to draw your own plot 
 annotations, which could consist of text, arrows, or other shapes.
  
 228 | Chapter 8:Plotting and Visualization
  
 www.it-ebooks.info",NA
Saving Plots to File,"The active figure can be saved to file using 
 plt.savefig
 . This method is equivalent to 
 the figure object’s 
 savefig
  instance method. For example, to save an SVG version of a 
 figure, you need only type:
  
 plt.savefig('figpath.svg')
  
 The file type is inferred from the file extension. So if you used 
 .pdf
  instead you 
 would get a PDF. There are a couple of important options that I use frequently for 
 publishing graphics: 
 dpi
 , which controls the dots-per-inch resolution, and 
 bbox_inches
 , which can trim the whitespace around the actual figure. To get the 
 same plot as a PNG above with minimal whitespace around the plot and at 400 DPI, 
 you would do:
  
 plt.savefig('figpath.png', dpi=400, bbox_inches='tight')
  
 savefig
  doesn’t have to write to disk; it can also write to any file-like object, such as a 
 StringIO
 :
  
 from io import StringIO 
  
 buffer = StringIO() 
  
 plt.savefig(buffer) 
  
 plot_data = buffer.getvalue()
  
 For example, this is useful for serving dynamically-generated images over the web.
  
 Table 8-2. Figure.savefig options
  
 Argument
  
 Description
  
 fname
  
 dpi
  
 facecolor, edge
  
 String containing a filepath or a Python file-like object. The figure 
 format is inferred from the file extension, e.g. 
 .pdf
  for PDF or 
 .png
  for 
 PNG.
  
 The figure resolution in dots per inch; defaults to 100 out of the box but 
 can be configured
  
 The color of the figure background outside of the subplots. 
 'w'
  (white), 
 by default
  
 color
  
 format 
  
 bbox_inches
  
 The explicit file format to use (
 'png', 'pdf', 'svg', 'ps', 'eps', ...
 ) 
  
 The portion of the figure to save. If 
 'tight'
  is passed, will attempt to trim 
 the empty space around the figure",NA
matplotlib Configuration,"matplotlib comes configured with color schemes and defaults that are geared 
 primarily toward preparing figures for publication. Fortunately, nearly all of the 
 default behavior can be customized via an extensive set of global parameters 
 governing figure size, sub-plot spacing, colors, font sizes, grid styles, and so on. 
 There are two main ways to interact with the matplotlib configuration system. The 
 first is programmatically from Python using the 
 rc
  method. For example, to set the 
 global default figure size to be 10 x 10, you could enter:
  
 plt.rc('figure', figsize=(10, 10))
  
 A Brief matplotlib API Primer | 231",NA
Plotting Functions in pandas,"As you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot 
 from its base components: the data display (the type of plot: line, bar, box, scatter, 
 contour, etc.), legend, title, tick labels, and other annotations. Part of the reason for 
 this is that in many cases the data needed to make a complete plot is spread across 
 many objects. In pandas we have row labels, column labels, and possibly grouping 
 information. This means that many kinds of fully-formed plots that would 
 ordinarily require a lot of matplotlib code can be expressed in one or two concise 
 statements. Therefore, pandas has an increasing number of high-level plotting 
 methods for creating standard visual-izations that take advantage of how data is 
 organized in DataFrame objects.
  
  
 As of this writing, the plotting functionality in pandas is undergoing 
 quite a bit of work. As part of the 2012 Google Summer of Code pro-
 gram, a student is working full time to add features and to make the
  
 interface more consistent and usable. Thus, it’s possible that this 
 code may fall out-of-date faster than the other things in this book. 
 The online pandas documentation will be the best resource in that 
 event.",NA
Line Plots,"Series and DataFrame each have a 
 plot
  method for making many different plot 
 types. By default, they make line plots (see 
 Figure 8-13
 ):
  
 In [55]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))
  
 In [56]: s.plot()
  
 The Series object’s index is passed to matplotlib for plotting on the X axis, though 
 this can be disabled by passing 
 use_index=False
 . The X axis ticks and limits can be 
 adjusted using the 
 xticks
  and 
 xlim
  options, and Y axis respectively using 
 yticks
  and 
 ylim
 . See
  
 232 | Chapter 8:Plotting and Visualization",NA
Bar Plots,"Making bar plots instead of line plots is a simple as passing 
 kind='bar'
  (for vertical 
 bars) or 
 kind='barh'
  (for horizontal bars). In this case, the Series or DataFrame index 
 will be used as the X (
 bar
 ) or Y (
 barh
 ) ticks (see 
 Figure 8-15
 ):
  
 In [59]: fig, axes = plt.subplots(2, 1)
  
 In [60]: data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))
  
 In [61]: data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7) Out[61]: 
 <matplotlib.axes.AxesSubplot at 0x4ee7750>
  
 In [62]: data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)
  
  
 For more on the 
 plt.subplots
  function and matplotlib axes and figures, 
 see the later section in this chapter.
  
 With a DataFrame, bar plots group the values in each row together in a group in 
 bars, side by side, for each value. See 
 Figure 8-16
 :
  
 In [63]: df = DataFrame(np.random.rand(6, 4),
  
  ....:                index=['one', 'two', 'three', 'four', 'five', 'six'], ....:                
 columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))
  
 In [64]: df 
  
 Out[64]: 
  
 Genus         A         B         C         D 
  
 one    0.301686  0.156333  0.371943  0.270731 
  
 two    0.750589  0.525587  0.689429  0.358974 
  
 three  0.381504  0.667707  0.473772  0.632528 
  
 four   0.942408  0.180186  0.708284  0.641783 
  
 five   0.840278  0.909589  0.010041  0.653207 
  
 six    0.062854  0.589813  0.811318  0.060217
  
 In [65]: df.plot(kind='bar')
  
 Plotting Functions in pandas | 235
  
 www.it-ebooks.info",NA
Histograms and Density Plots,"A histogram, with which you may be well-acquainted, is a kind of bar plot that gives 
 a discretized display of value frequency. The data points are split into discrete, 
 evenly spaced bins, and the number of data points in each bin is plotted. Using the 
 tipping data from before, we can make a histogram of tip percentages of the total 
 bill using the 
 hist
  method on the Series (see 
 Figure 8-19
 ):
  
 In [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']
  
 In [77]: tips['tip_pct'].hist(bins=50)
  
 238 | Chapter 8:Plotting and Visualization
  
 www.it-ebooks.info",NA
Scatter Plots,"Scatter plots are a useful way of examining the relationship between two one-
 dimen-sional data series. matplotlib has a 
 scatter
  plotting method that is the 
 workhorse of
  
 Plotting Functions in pandas | 239",NA
Plotting Maps: Visualizing Haiti Earthquake ,NA,NA
Crisis Data,"Ushahidi is a non-profit software company that enables crowdsourcing of 
 information related to natural disasters and geopolitical events via text message. 
 Many of these data sets are then published on their 
 website
  for analysis and 
 visualization. I downloaded",NA
Python Visualization Tool Ecosystem,"As is common with open source, there are a plethora of options for creating 
 graphics in Python (too many to list). In addition to open source, there are 
 numerous commercial libraries with Python bindings.
  
 In this chapter and throughout the book, I have been primarily concerned with 
 mat-plotlib as it is the most widely used plotting tool in Python. While it’s an 
 important part of the scientific Python ecosystem, matplotlib has plenty of 
 shortcomings when it comes to the creation and display of statistical graphics. 
 MATLAB users will likely find matplotlib familiar, while R users (especially users of 
 the excellent 
 ggplot2
  and 
 trel lis
  packages) may be somewhat disappointed (at least 
 as of this writing). It is possible to make beautiful plots for display on the web in 
 matplotlib, but doing so often requires significant effort as the library is designed 
 for the printed page. Aesthetics aside, it is sufficient for most needs. In pandas, I, 
 along with the other developers, have sought to build a convenient user interface 
 that makes it easier to make most kinds of plots com-monplace in data analysis.
  
 There are a number of other visualization tools in wide use. I list a few of them here 
 and encourage you to explore the ecosystem.
  
 Python Visualization Tool Ecosystem | 247
  
 www.it-ebooks.info",NA
Chaco,"Chaco (
 http://code.enthought.com/chaco/
 ), developed by Enthought, is a plotting 
 tool-kit suitable both for static plotting and interactive visualizations. It is 
 especially 
 well-suited 
 for 
 expressing 
 complex 
 visualizations 
 with 
 data 
 interrelationships. Compared with matplotlib, Chaco has much better support for 
 interacting with plot elements and rendering is very fast, making it a good choice 
 for building interactive GUI applications.
  
  
 Figure 8-26. A Chaco example plot",NA
mayavi,"The mayavi project, developed by Prabhu Ramachandran, Gaël Varoquaux, and 
 others, is a 3D graphics toolkit built on the open source C++ graphics library VTK. 
 mayavi, like matplotlib, integrates with IPython so that it is easy to use 
 interactively. The plots can be panned, rotated, and zoomed using the mouse and 
 keyboard. I used mayavi to make one of the illustrations of broadcasting in 
 Chapter 
 12
 . While I don’t show any mayavi-using code here, there is plenty of 
 documentation and examples available on-line. In many cases, I believe it is a good 
 alternative to a technology like WebGL, though the graphics are harder to share in 
 interactive form.",NA
Other Packages,"Of course, there are numerous other visualization libraries and applications 
 available in Python: PyQwt, Veusz, gnuplot-py, biggles, and others. I have seen 
 PyQwt put to good use in GUI applications built using the Qt application framework 
 using PyQt. While many of these libraries continue to be under active development 
 (some of them
  
 248 | Chapter 8:Plotting and Visualization",NA
The Future of Visualization Tools?,"Visualizations built on web technologies (that is, JavaScript-based) appear to be the 
 inevitable future. Doubtlessly you have used many different kinds of static or 
 interactive visualizations built in Flash or JavaScript over the years. New toolkits 
 (such as d3.js and its numerous off-shoot projects) for building such displays are 
 appearing all the time. In contrast, development in non web-based visualization has 
 slowed significantly in recent years. This holds true of Python as well as other data 
 analysis and statistical computing environments like R.
  
 The development challenge, then, will be in building tighter integration between 
 data analysis and preparation tools, such as pandas, and the web browser. I am 
 hopeful that this will become a fruitful point of collaboration between Python and 
 non-Python users as well.
  
 Python Visualization Tool Ecosystem | 249
  
 www.it-ebooks.info",NA
CHAPTER 9,NA,NA
Data Aggregation and ,NA,NA
Group ,NA,NA
Operati,NA,NA
ons,"Categorizing a data set and applying a function to each group, whether an 
 aggregation or transformation, is often a critical component of a data analysis 
 workflow. After loading, merging, and preparing a data set, a familiar task is to 
 compute group statistics or possibly 
 pivot tables
  for reporting or visualization 
 purposes. pandas provides a flex-ible and high-performance 
 groupby
  facility, 
 enabling you to slice and dice, and sum-marize data sets in a natural way.
  
 One reason for the popularity of relational databases and SQL (which stands 
 for“structured query language”) is the ease with which data can be joined, filtered, 
 trans-formed, and aggregated. However, query languages like SQL are rather 
 limited in the kinds of group operations that can be performed. As you will see, 
 with the expressive-ness and power of Python and pandas, we can perform much 
 more complex grouped operations by utilizing any function that accepts a pandas 
 object or NumPy array. In this chapter, you will learn how to:
  
 • Split a pandas object into pieces using one or more keys (in the form of 
 functions, 
  
 arrays, or DataFrame column names)
  
 • Computing group summary statistics, like count, mean, or standard deviation, or 
  
 a user-defined function
  
 • Apply a varying set of functions to each column of a DataFrame
  
 • Apply within-group transformations or other manipulations, like normalization, 
  
 linear regression, rank, or subset selection
  
 • Compute pivot tables and cross-tabulations
  
 • Perform quantile analysis and other data-derived group analyses",NA
GroupBy Mechanics,"Hadley Wickham, an author of many popular packages for the R programming lan-
 guage, coined the term 
 split-apply-combine
  for talking about group operations, and I 
 think that’s a good description of the process. In the first stage of the process, data 
 contained in a pandas object, whether a Series, DataFrame, or otherwise, is 
 split
  
 into groups based on one or more 
 keys
  that you provide. The splitting is performed 
 on a particular axis of an object. For example, a DataFrame can be grouped on its 
 rows (
 axis=0
 ) or its columns (
 axis=1
 ). Once this is done, a function is 
 applied
  to each 
 group, producing a new value. Finally, the results of all those function applications 
 are 
 com-bined
  into a result object. The form of the resulting object will usually 
 depend on what’s being done to the data. See 
 Figure 9-1
  for a mockup of a simple 
 group aggregation.
  
  
 Figure 9-1. Illustration of a group aggregation
  
 Each grouping key can take many forms, and the keys do not have to be all of the 
 same type:
  
 • A list or array of values that is the same length as the axis being grouped
  
 • A value indicating a column name in a DataFrame
  
 252 | Chapter 9:Data Aggregation and Group Operations",NA
Iterating Over Groups,"The GroupBy object supports iteration, generating a sequence of 2-tuples 
 containing the group name along with the chunk of data. Consider the following 
 small example data set:
  
 In [27]: for name, group in df.groupby('key1'):
  
  ....:     
 print name
  
  
  ....:     print group
  
  
  ....: 
  
 a
  
   
  data1     data2 key1 key2 
  
 0 -0.204708  1.393406    a  one 
  
 1  0.478943  0.092908    a  two 
  
 4  1.965781  1.246435    a  one 
  
 b
  
   
  data1     data2 key1 key2 
  
 2 -0.519439  0.281746    b  one 
  
 3 -0.555730  0.769023    b  two
  
 In the case of multiple keys, the first element in the tuple will be a tuple of key 
 values:
  
 In [28]: for (k1, k2), group in df.groupby(['key1', 'key2']):
  
  ....:     print k1, 
 k2
  
  
  ....:     print group
  
  
  ....: 
  
 a one
  
   
  data1     data2 key1 key2 
  
 0 -0.204708  1.393406    a  one 
  
 4  1.965781  1.246435    a  one 
  
 a two
  
   
  data1     data2 key1 key2 
  
 1  0.478943  0.092908    a  two 
  
 b one
  
   
  data1     data2 key1 key2
  
 GroupBy Mechanics | 255",NA
Selecting a Column or Subset of Columns,"Indexing a GroupBy object created from a DataFrame with a column name or array 
 of column names has the effect of 
 selecting those columns
  for aggregation. This 
 means that:
  
 df.groupby('key1')['data1'] 
  
 df.groupby('key1')[['data2']]
  
 are syntactic sugar for:
  
 df['data1'].groupby(df['key1']) 
  
 df[['data2']].groupby(df['key1'])
  
 256 | Chapter 9:Data Aggregation and Group Operations",NA
Grouping with Dicts and Series,"Grouping information may exist in a form other than an array. Let’s consider 
 another example DataFrame:
  
 In [38]: people = DataFrame(np.random.randn(5, 5),
  
  ....:                    columns=['a', 'b', 'c', 'd', 'e'],
  
  ....:                    index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
  
 In [39]: people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA values
  
 In [40]: people 
  
 Out[40]: 
  
  
  a         b         c         d         e Joe     1.007189 -
 1.296221  0.274992  0.228913  1.352917 Steve   0.886429 -
 2.001637 -0.371843  1.669025 -0.438570 Wes    -0.539741       
 NaN       NaN -1.021228 -0.577087 Jim     0.124121  0.302614  
 0.523772  0.000940  1.343810 Travis -0.713544 -0.831154 -
 2.370232 -1.860761 -0.860757
  
 Now, suppose I have a group correspondence for the columns and want to sum 
 together the columns by group:
  
 In [41]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',
  
  ....:            'd': 'blue', 'e': 
 'red', 'f' : 'orange'}
  
 GroupBy Mechanics | 257",NA
Grouping with Functions,"Using Python functions in what can be fairly creative ways is a more abstract way 
 of defining a group mapping compared with a dict or Series. Any function passed as 
 a group key will be called once per index value, with the return values being used 
 as the group names. More concretely, consider the example DataFrame from the 
 previous section, which has people’s first names as index values. Suppose you 
 wanted to group by the length of the names; you could compute an array of string 
 lengths, but instead you can just pass the 
 len
  function:
  
 In [47]: people.groupby(len).sum() 
  
 Out[47]: 
  
  
  a         b         c         d         e 3  0.591569 -0.993608  
 0.798764 -0.791374  2.119639
  
 258 | Chapter 9:Data Aggregation and Group Operations",NA
Grouping by Index Levels,"A final convenience for hierarchically-indexed data sets is the ability to aggregate 
 using one of the levels of an axis index. To do this, pass the level number or name 
 using the 
 level
  keyword:
  
 In [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
  
  
  ....:                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])
  
 In [51]: hier_df = DataFrame(np.random.randn(4, 5), columns=columns)
  
 In [52]: hier_df 
  
 Out[52]: 
  
 cty          US                            JP 
  
 tenor         1         3         5         1         3 0      0.560145 -1.265934  
 0.119827 -1.063512  0.332883 1     -2.359419 -0.199543 -
 1.541996 -0.970736 -1.307030 2      0.286350  0.377984 -
 0.753887  0.331286  1.349742 3      0.069877  0.246674 -
 0.011862  1.004812  1.327195
  
 In [53]: hier_df.groupby(level='cty', axis=1).count() Out[53]: 
  
 cty  JP  US 
  
 0     2   3 
  
 1     2   3 
  
 2     2   3 
  
 3     2   3",NA
Data Aggregation,"By aggregation, I am generally referring to any data transformation that produces 
 scalar values from arrays. In the examples above I have used several of them, such 
 as 
 mean, count, min
  and 
 sum
 . You may wonder what is going on when you invoke 
 mean()
  on a GroupBy object. Many common aggregations, such as those found in 
 Table 9-1
 , have optimized implementations that compute the statistics on the 
 dataset 
 in place
 . How-ever, you are not limited to only this set of methods. You can 
 use aggregations of your
  
 Data Aggregation | 259",NA
Column-wise and Multiple Function Application,"As you’ve seen above, aggregating a Series or all of the columns of a DataFrame is a 
 matter of using 
 aggregate
  with the desired function or calling a method like 
 mean
  or 
 std
 . However, you may want to aggregate using a different function depending on 
 the column or multiple functions at once. Fortunately, this is straightforward to do, 
 which I’ll illustrate through a number of examples. First, I’ll group the 
 tips
  by 
 sex
  
 and 
 smoker
 :
  
 In [63]: grouped = tips.groupby(['sex', 'smoker'])
  
 Note that for descriptive statistics like those in 
 Table 9-1
 , you can pass the name of 
 the function as a string:
  
 In [64]: grouped_pct = grouped['tip_pct']
  
 In [65]: grouped_pct.agg('mean') 
  
 Out[65]: 
  
 sex     smoker 
  
 Female  No        0.156921
  
  
  Yes       0.182150 
  
 Male    No        0.160669
  
  
  Yes       0.152771 
  
 Name: tip_pct
  
 If you pass a list of functions or function names instead, you get back a DataFrame 
 with column names taken from the functions:
  
 In [66]: grouped_pct.agg(['mean', 'std', peak_to_peak]) Out[66]: 
  
  
  
  mean       std  peak_to_peak 
  
 sex    smoker 
  
 Female No      0.156921  0.036421      0.195876
  
  
  Yes     0.182150  0.071595      0.360233 
  
 Male   No      0.160669  0.041849      0.220186
  
  
  Yes     0.152771  0.090588      0.674707
  
 You don’t need to accept the names that GroupBy gives to the columns; notably 
 lambda
  functions have the name 
 '<lambda>'
  which make them hard to identify (you 
 can see for yourself by looking at a function’s 
 __name__
  attribute). As such, if you 
 pass a list of 
 (name, function)
  tuples, the first element of each tuple will be used as 
 the DataFrame column names (you can think of a list of 2-tuples as an ordered 
 mapping):
  
 In [67]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)]) Out[67]: 
  
  
  
  foo       bar 
  
 sex    smoker 
  
 Female No      0.156921  0.036421
  
  
  Yes     0.182150  0.071595
  
 262 | Chapter 9:Data Aggregation and Group Operations",NA
Returning Aggregated Data in “unindexed” Form,"In all of the examples up until now, the aggregated data comes back with an index, 
 potentially hierarchical, composed from the unique group key combinations 
 observed. Since this isn’t always desirable, you can disable this behavior in most 
 cases by passing 
 as_index=False
  to 
 groupby
 :
  
 In [76]: tips.groupby(['sex', 'smoker'], as_index=False).mean() Out[76]: 
  
  
  sex smoker  total_bill       tip      size   tip_pct 0  Female     No   
 18.105185  2.773519  2.592593  0.156921 1  Female    Yes   17.977879  
 2.931515  2.242424  0.182150 2    Male     No   19.791237  3.113402  
 2.711340  0.160669 3    Male    Yes   22.284500  3.051167  2.500000  
 0.152771
  
 Of course, it’s always possible to obtain the result in this format by calling 
 reset_index
  on the result.
  
  
 Using 
 groupby
  in this way is generally less flexible; results with hier-
 archical columns, for example, are not currently implemented as the 
 form of the result would have to be somewhat arbitrary.",NA
Group-wise Operations and ,NA,NA
Transformations,"Aggregation is only one kind of group operation. It is a special case in the more 
 general class of data transformations; that is, it accepts functions that reduce a one-
 dimensional array to a scalar value. In this section, I will introduce you to the 
 transform
  and 
 apply 
 methods, which will enable you to do many other kinds of 
 group operations.
  
 Suppose, instead, we wanted to add a column to a DataFrame containing group 
 means for each index. One way to do this is to aggregate, then merge:
  
 264 | Chapter 9:Data Aggregation and Group Operations",NA
Apply: General split-apply-combine,"Like 
 aggregate
 , 
 transform
  is a more specialized function having rigid requirements: 
 the passed function must either produce a scalar value to be broadcasted (like 
 np.mean
 ) or a transformed array of the same size. The most general purpose 
 GroupBy method is 
 apply
 , which is the subject of the rest of this section. As in 
 Figure 
 9-1
 , 
 apply
  splits the object being manipulated into pieces, invokes the passed 
 function on each piece, then attempts to concatenate the pieces together.
  
 Returning to the tipping data set above, suppose you wanted to select the top five 
 tip_pct
  values by group. First, it’s straightforward to write a function that selects the 
 rows with the largest values in a particular column:
  
 In [88]: def top(df, n=5, column='tip_pct'):
  
  
  ....:     return df.sort_index(by=column)[-n:]
  
 In [89]: top(tips, n=6) 
  
 Out[89]: 
  
  
  total_bill   tip     sex smoker  day    time  size   tip_pct 109       14.31  4.00  
 Female    Yes  Sat  Dinner     2  0.279525 183       23.17  6.50    Male    Yes  Sun  
 Dinner     4  0.280535 232       11.61  3.39    Male     No  Sat  Dinner     2  
 0.291990 67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733 178        
 9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667 172        7.25  5.15    Male    
 Yes  Sun  Dinner     2  0.710345
  
 Now, if we group by 
 smoker
 , say, and call 
 apply
  with this function, we get the 
 following:
  
 In [90]: tips.groupby('smoker').apply(top) 
  
 Out[90]: 
  
  
  total_bill   tip     sex smoker   day    time  size   tip_pct smoker 
  
 266 | Chapter 9:Data Aggregation and Group Operations
  
 www.it-ebooks.info",NA
Quantile and Bucket Analysis,"As you may recall from 
 Chapter 7
 , pandas has some tools, in particular 
 cut
  and 
 qcut
 , 
 for slicing data up into buckets with bins of your choosing or by sample quantiles. 
 Combining these functions with 
 groupby
 , it becomes very simple to perform bucket 
 or
  
 268 | Chapter 9:Data Aggregation and Group Operations
  
 www.it-ebooks.info",NA
Example: Filling Missing Values with Group-specific ,NA,NA
Values,"When cleaning up missing data, in some cases you will filter out data observations 
 using 
 dropna
 , but in others you may want to impute (fill in) the NA values using a 
 fixed value or some value derived from the data. 
 fillna
  is the right tool to use; for 
 example here I fill in NA values with the mean:
  
 In [105]: s = Series(np.random.randn(6))
  
 In [106]: s[::2] = np.nan
  
 In [107]: s 
  
 Out[107]: 
  
 0         NaN 
  
 1   -0.125921 
  
 2         NaN 
  
 3   -0.884475 
  
 4         NaN 
  
 5    0.227290
  
 In [108]: s.fillna(s.mean()) 
  
 Out[108]: 
  
 0   -0.261035 
  
 1   -0.125921 
  
 2   -0.261035 
  
 3   -0.884475 
  
 4   -0.261035 
  
 5    0.227290
  
 Suppose you need the fill value to vary by group. As you may guess, you need only 
 group the data and use 
 apply
  with a function that calls 
 fillna
  on each data chunk. 
 Here is some sample data on some US states divided into eastern and western 
 states:
  
 In [109]: states = ['Ohio', 'New York', 'Vermont', 'Florida',
  
  .....:           
 'Oregon', 'Nevada', 'California', 'Idaho']
  
 In [110]: group_key = ['East'] * 4 + ['West'] * 4
  
 In [111]: data = Series(np.random.randn(8), index=states)
  
 In [112]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan
  
 In [113]: data 
  
 Out[113]: 
  
 Ohio          0.922264 
  
 New York     -2.153545 
  
 Vermont            NaN 
  
 Florida      -0.375842 
  
 Oregon        0.329939 
  
 Nevada             NaN 
  
 California    1.105913 
  
 Idaho              NaN
  
 In [114]: data.groupby(group_key).mean() 
  
 Out[114]: 
  
 270 | Chapter 9:Data Aggregation and Group Operations",NA
Example: Random Sampling and Permutation,"Suppose you wanted to draw a random sample (with or without replacement) from 
 a large dataset for Monte Carlo simulation purposes or some other application. 
 There are a number of ways to perform the “draws”; some are much more efficient 
 than others. One way is to select the first 
 K
  elements of 
 np.random.permutation(N)
 , 
 where 
 N
  is the size of your complete dataset and 
 K
  the desired sample size. As a 
 more fun example, here’s a way to construct a deck of English-style playing cards:
  
 # Hearts, Spades, Clubs, Diamonds 
  
 suits = ['H', 'S', 'C', 'D'] 
  
 card_val = (range(1, 11) + [10] * 3) * 4 
  
 base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q'] cards = [] 
  
 for suit in ['H', 'S', 'C', 'D']:
  
  
  cards.extend(str(num) + suit for num in base_names)
  
 deck = Series(card_val, index=cards)
  
 Group-wise Operations and Transformations | 271
  
 www.it-ebooks.info",NA
Example: Group Weighted Average and Correlation,"Under the split-apply-combine paradigm of 
 groupby
 , operations between columns 
 in a DataFrame or two Series, such a group weighted average, become a routine 
 affair. As an example, take this dataset containing group keys, values, and some 
 weights:
  
 In [127]: df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], .....:                 'data': 
 np.random.randn(8),
  
  .....:                 'weights': np.random.rand(8)})
  
 In [128]: df 
  
 Out[128]: 
  
  category      data   weights 
  
 0        a  1.561587  0.957515 
  
 1        a  1.219984  0.347267 
  
 2        a -0.482239  0.581362 
  
 3        a  0.315667  0.217091 
  
 4        b -0.047852  0.894406 
  
 5        b -0.454145  0.918564 
  
 6        b -0.556774  0.277825 
  
 7        b  0.253321  0.955905
  
 The group weighted average by 
 category
  would then be:
  
 In [129]: grouped = df.groupby('category')
  
 In [130]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])
  
 In [131]: grouped.apply(get_wavg) 
  
 Out[131]: 
  
 category 
  
 a           0.811643 
  
 b          -0.122262
  
 As a less trivial example, consider a data set from Yahoo! Finance containing end of 
 day prices for a few stocks and the S&P 500 index (the 
 SPX
  ticker):
  
 In [132]: close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)
  
 In [133]: close_px 
  
 Out[133]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 DatetimeIndex: 2214 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00 Data 
 columns: 
  
 AAPL    2214  non-null values 
  
 MSFT    2214  non-null values 
  
 XOM     2214  non-null values 
  
 SPX     2214  non-null values 
  
 dtypes: float64(4)
  
 Group-wise Operations and Transformations | 273",NA
Example: Group-wise Linear Regression,"In the same vein as the previous example, you can use 
 groupby
  to perform more 
 complex group-wise statistical analysis, as long as the function returns a pandas 
 object or scalar value. For example, I can define the following 
 regress
  function 
 (using the 
 statsmo dels
  econometrics library) which executes an ordinary least 
 squares (OLS) regression on each chunk of data:
  
 274 | Chapter 9:Data Aggregation and Group Operations
  
 www.it-ebooks.info",NA
Pivot Tables and Cross-Tabulation,"A 
 pivot table
  is a data summarization tool frequently found in spreadsheet 
 programs and other data analysis software. It aggregates a table of data by one or 
 more keys, arranging the data in a rectangle with some of the group keys along the 
 rows and some along the columns. Pivot tables in Python with pandas are made 
 possible using the 
 groupby
  facility described in this chapter combined with reshape 
 operations utilizing hierarchical indexing. DataFrame has a 
 pivot_table
  method, and 
 additionally there is a top-level 
 pandas.pivot_table
  function. In addition to providing 
 a convenience inter-face to 
 groupby
 , 
 pivot_table
  also can add partial totals, also 
 known as 
 margins
 .
  
 Returning to the tipping data set, suppose I wanted to compute a table of group 
 means (the default 
 pivot_table
  aggregation type) arranged by 
 sex
  and 
 smoker
  on the 
 rows:
  
 In [142]: tips.pivot_table(rows=['sex', 'smoker']) Out[142]: 
  
  
  
  size       tip   tip_pct  total_bill sex    smoker 
  
 Female No      2.592593  2.773519  0.156921   18.105185
  
  Yes     
 2.242424  2.931515  0.182150   17.977879 Male   No      
 2.711340  3.113402  0.160669   19.791237
  
  Yes     2.500000  
 3.051167  0.152771   22.284500
  
 This could have been easily produced using 
 groupby
 . Now, suppose we want to 
 aggre-gate only 
 tip_pct
  and 
 size
 , and additionally group by 
 day
 . I’ll put 
 smoker
  in the 
 table columns and 
 day
  in the rows:
  
 In [143]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],
  
  .....:                  
 cols='smoker') 
  
 Out[143]: 
  
 Pivot Tables and Cross-Tabulation | 275
  
 www.it-ebooks.info",NA
Cross-Tabulations: Crosstab,"A cross-tabulation (or 
 crosstab
  for short) is a special case of a pivot table that 
 computes group frequencies. Here is a canonical example taken from the Wikipedia 
 page on cross-tabulation:
  
 In [150]: data 
  
 Out[150]: 
  
  
  Sample  Gender    Handedness 
  
 0       1  Female  Right-handed 
  
 1       2    Male   Left-handed 
  
 2       3  Female  Right-handed 
  
 3       4    Male  Right-handed 
  
 4       5    Male   Left-handed 
  
 5       6    Male  Right-handed 
  
 6       7  Female  Right-handed 
  
 7       8  Female   Left-handed 
  
 8       9    Male  Right-handed 
  
 9      10  Female  Right-handed
  
 As part of some survey analysis, we might want to summarize this data by gender 
 and handedness. You could use 
 pivot_table
  to do this, but the 
 pandas.crosstab
  
 function is very convenient:
  
 In [151]: pd.crosstab(data.Gender, data.Handedness, margins=True) 
 Out[151]: 
  
 Handedness  Left-handed  Right-handed  All 
  
 Gender 
  
 Female                1             4    5 
  
 Male                  2             3    5 
  
 All                   3             7   10
  
 Pivot Tables and Cross-Tabulation | 277
  
 www.it-ebooks.info",NA
Example: 2012 Federal Election ,NA,NA
Commission Database,"The US Federal Election Commission publishes data on contributions to political 
 cam-paigns. This includes contributor names, occupation and employer, address, 
 and con-tribution amount. An interesting dataset is from the 2012 US presidential 
 election (
 http://www.fec.gov/disclosurep/PDownload.do
 ). As of this writing (June 
 2012), the full dataset for all states is a 150 megabyte CSV file 
 P00000001-ALL.csv
 , 
 which can be loaded with 
 pandas.read_csv
 :
  
 In [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')
  
 In [14]: fec 
  
 Out[14]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 Int64Index: 1001731 entries, 0 to 1001730 
  
 Data columns: 
  
 cmte_id              1001731  non-null values 
  
 cand_id              1001731  non-null values 
  
 cand_nm              1001731  non-null values 
  
 contbr_nm            1001731  non-null values 
  
 contbr_city          1001716  non-null values 
  
 contbr_st            1001727  non-null values 
  
 contbr_zip           1001620  non-null values 
  
 contbr_employer      994314   non-null values 
  
 contbr_occupation    994433   non-null values 
  
 contb_receipt_amt    1001731  non-null values 
  
 contb_receipt_dt     1001731  non-null values 
  
 receipt_desc         14166    non-null values 
  
 memo_cd              92482    non-null values 
  
 memo_text            97770    non-null values 
  
 form_tp              1001731  non-null values 
  
 file_num             1001731  non-null values 
  
 dtypes: float64(1), int64(1), object(14)
  
 A sample record in the DataFrame looks like this:
  
 In [15]: fec.ix[123456] 
  
 Out[15]: 
  
 cmte_id                             C00431445
  
 278 | Chapter 9:Data Aggregation and Group Operations",NA
Donation Statistics by Occupation and Employer,"Donations by occupation is another oft-studied statistic. For example, lawyers 
 (attor-neys) tend to donate more money to Democrats, while business executives 
 tend to donate more to Republicans. You have no reason to believe me; you can see 
 for yourself in the data. First, the total number of donations by occupation is easy:
  
 280 | Chapter 9:Data Aggregation and Group Operations
  
 www.it-ebooks.info",NA
Bucketing Donation Amounts,"A useful way to analyze this data is to use the 
 cut
  function to discretize the 
 contributor amounts into buckets by contribution size:
  
 In [43]: bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])
  
 Example: 2012 Federal Election Commission Database | 283
  
 www.it-ebooks.info",NA
Donation Statistics by State,"Aggregating the data by candidate and state is a routine affair:
  
 In [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])
  
 In [54]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)
  
 In [55]: totals = totals[totals.sum(1) > 100000]
  
 In [56]: totals[:10] 
  
 Out[56]: 
  
 cand_nm    Obama, Barack  Romney, Mitt 
  
 contbr_st
  
 Example: 2012 Federal Election Commission Database | 285
  
 www.it-ebooks.info",NA
CHAPTER 10,NA,NA
Time Series,"Time series data is an important form of structured data in many different fields, 
 such as finance, economics, ecology, neuroscience, or physics. Anything that is 
 observed or measured at many points in time forms a time series. Many time series 
 are 
 fixed fre-quency
 , which is to say that data points occur at regular intervals 
 according to some rule, such as every 15 seconds, every 5 minutes, or once per 
 month. Time series can also be 
 irregular
  without a fixed unit or time or offset 
 between units. How you mark and refer to time series data depends on the 
 application and you may have one of the following:
  
 •
  Timestamps
 , specific instants in time
  
 • Fixed 
 periods
 , such as the month January 2007 or the full year 2010
  
 •
  Intervals
  of time, indicated by a start and end timestamp. Periods can be thought 
  
 of as special cases of intervals
  
 • Experiment or elapsed time; each timestamp is a measure of time relative to a 
 particular start time. For example, the diameter of a cookie baking each second 
 since being placed in the oven
  
 In this chapter, I am mainly concerned with time series in the first 3 categories, 
 though many of the techniques can be applied to experimental time series where 
 the index may be an integer or floating point number indicating elapsed time from 
 the start of the experiment. The simplest and most widely used kind of time series 
 are those indexed by timestamp.
  
 pandas provides a standard set of time series tools and data algorithms. With this, 
 you can efficiently work with very large time series and easily slice and dice, 
 aggregate, and resample irregular and fixed frequency time series. As you might 
 guess, many of these tools are especially useful for financial and economics 
 applications, but you could cer-tainly use them to analyze server log data, too.
  
 289
  
 www.it-ebooks.info",NA
Date and Time Data Types and Tools ,"The Python standard library includes data types for date and time data, as well as 
 calendar-related functionality. The 
 datetime
 , 
 time
 , and 
 calendar
  modules are the main 
 places to start. The 
 datetime.datetime
  type, or simply 
 datetime
 , is widely used: 
  
 In [317]: from datetime import datetime 
  
  
 In [318]: now = datetime.now() 
  
  
 In [319]: now 
  
  
 Out[319]: datetime.datetime(2012, 8, 4, 17, 9, 21, 832092) 
  
  
 In [320]: now.year, now.month, now.day 
  
  
 Out[320]: (2012, 8, 4) 
  
 datetime
  stores both the date and time down to the microsecond. 
 datetime.time delta
  
 represents the temporal difference between two 
 datetime
  objects: 
  
  
 In [321]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15) 
  
  
 In [322]: delta 
  
  
 Out[322]: datetime.timedelta(926, 56700) 
  
  
 In [323]: delta.days        In [324]: delta.seconds 
  
  
 Out[323]: 926               Out[324]: 56700 
  
 You can add (or subtract) a 
 timedelta
  or multiple thereof to a 
 datetime
  object to yield 
 a new shifted object: 
  
  
 In [325]: from datetime import timedelta 
  
  
 In [326]: start = datetime(2011, 1, 7) 
  
  
 In [327]: start + timedelta(12) 
  
  
 Out[327]: datetime.datetime(2011, 1, 19, 0, 0) 
  
  
 In [328]: start - 2 * timedelta(12) 
  
  
 Out[328]: datetime.datetime(2010, 12, 14, 0, 0) 
  
 The data types in the 
 datetime
  module are summarized in 
 Table 10-1
 . While this 
 chap-ter is mainly concerned with the data types in pandas and higher level time 
 series ma-nipulation, you will undoubtedly encounter the 
 datetime
 -based types in 
 many other places in Python the wild.
  
 290 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Converting between string and datetime,"datetime
  objects and pandas 
 Timestamp
  objects, which I’ll introduce later, can be for-
 matted as strings using 
 str
  or the 
 strftime
  method, passing a format specification:
  
 In [329]: stamp = datetime(2011, 1, 3)
  
 In [330]: str(stamp)                   In [331]: stamp.strftime('%Y-%m-%d') Out[330]: '2011-
 01-03 00:00:00'        Out[331]: '2011-01-03'
  
 See 
 Table 10-2
  for a complete list of the format codes. These same format codes can 
 be used to convert strings to dates using 
 datetime.strptime
 :
  
 In [332]: value = '2011-01-03'
  
 In [333]: datetime.strptime(value, '%Y-%m-%d') 
 Out[333]: datetime.datetime(2011, 1, 3, 0, 0)
  
 In [334]: datestrs = ['7/6/2011', '8/6/2011']
  
 In [335]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs] 
  
 Out[335]: [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]
  
 datetime.strptime
  is the best way to parse a date with a known format. However, it 
 can be a bit annoying to have to write a format spec each time, especially for 
 common date formats. In this case, you can use the 
 parser.parse
  method in the third 
 party 
 dateutil
  package:
  
 In [336]: from dateutil.parser import parse
  
 In [337]: parse('2011-01-03') 
  
 Out[337]: datetime.datetime(2011, 1, 3, 0, 0)
  
 dateutil
  is capable of parsing almost any human-intelligible date representation:
  
 In [338]: parse('Jan 31, 1997 10:45 PM') 
  
 Out[338]: datetime.datetime(1997, 1, 31, 22, 45)
  
 In international locales, day appearing before month is very common, so you can 
 pass 
 dayfirst=True
  to indicate this:
  
 In [339]: parse('6/12/2011', dayfirst=True) 
 Out[339]: datetime.datetime(2011, 12, 6, 0, 0)
  
 Date and Time Data Types and Tools | 291
  
 www.it-ebooks.info",NA
Time Series Basics,"The most basic kind of time series object in pandas is a Series indexed by 
 timestamps, which is often represented external to pandas as Python strings or 
 datetime
  objects:
  
 In [346]: from datetime import datetime
  
 In [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),
  
  .....:          
 datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]
  
 In [348]: ts = Series(np.random.randn(6), index=dates)
  
 In [349]: ts 
  
 Out[349]: 
  
 2011-01-02    0.690002 
  
 2011-01-05    1.001543 
  
 2011-01-07   -0.503087 
  
 2011-01-08   -0.622274
  
 Time Series Basics | 293",NA
"Indexing, Selection, Subsetting","TimeSeries is a subclass of Series and thus behaves in the same way with regard to 
 indexing and selecting data based on label:
  
 294 | Chapter 10:Time Series",NA
Time Series with Duplicate Indices,"In some applications, there may be multiple data observations falling on a 
 particular timestamp. Here is an example:
  
 In [371]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000',
  
  .....:                           
 '1/3/2000'])
  
 In [372]: dup_ts = Series(np.arange(5), index=dates)
  
 In [373]: dup_ts 
  
 Out[373]: 
  
 2000-01-01    0 
  
 2000-01-02    1 
  
 2000-01-02    2 
  
 2000-01-02    3 
  
 2000-01-03    4
  
 We can tell that the index is not unique by checking its 
 is_unique
  property:
  
 296 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
"Date Ranges, Frequencies, and Shifting","Generic time series in pandas are assumed to be irregular; that is, they have no 
 fixed frequency. For many applications this is sufficient. However, it’s often 
 desirable to work relative to a fixed frequency, such as daily, monthly, or every 15 
 minutes, even if that means introducing missing values into a time series. 
 Fortunately pandas has a full suite of standard time series frequencies and tools for 
 resampling, inferring frequencies, and generating fixed frequency date ranges. For 
 example, in the example time series, con-verting it to be fixed daily frequency can 
 be accomplished by calling 
 resample
 :
  
 In [380]: ts                  In [381]: ts.resample('D') Out[380]:                     
 Out[381]: 
  
 2011-01-02    0.690002        2011-01-02    0.690002 2011-01-05    
 1.001543        2011-01-03         NaN 2011-01-07   -0.503087        
 2011-01-04         NaN 2011-01-08   -0.622274        2011-01-05    
 1.001543 2011-01-10   -0.921169        2011-01-06         NaN 2011-
 01-12   -0.726213        2011-01-07   -0.503087 
  
  2011-01-08   -
 0.622274    
  
  2011-01-09         NaN    
  
  2011-
 01-10   -0.921169    
  
  2011-01-11         NaN    
  
  2011-
 01-12   -0.726213    
  
  Freq: D
  
 Conversion between frequencies or 
 resampling
  is a big enough topic to have its own 
 section later. Here I’ll show you how to use the base frequencies and multiples 
 thereof.
  
 Date Ranges, Frequencies, and Shifting | 297
  
 www.it-ebooks.info",NA
Generating Date Ranges,"While I used it previously without explanation, you may have guessed that 
 pan 
 das.date_range
  is responsible for generating a 
 DatetimeIndex
  with an indicated length 
 according to a particular frequency:
  
 In [382]: index = pd.date_range('4/1/2012', '6/1/2012')
  
 In [383]: index 
  
 Out[383]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> [2012-04-
 01 00:00:00, ..., 2012-06-01 00:00:00] Length: 62, Freq: 
 D, Timezone: None
  
 By default, 
 date_range
  generates daily timestamps. If you pass only a start or end 
 date, you must pass a number of periods to generate:
  
 In [384]: pd.date_range(start='4/1/2012', periods=20) 
 Out[384]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2012-04-01 00:00:00, ..., 2012-04-20 00:00:00] 
  
 Length: 20, Freq: D, Timezone: None
  
 In [385]: pd.date_range(end='6/1/2012', periods=20) 
 Out[385]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2012-05-13 00:00:00, ..., 2012-06-01 00:00:00] Length: 20, 
 Freq: D, Timezone: None
  
 The start and end dates define strict boundaries for the generated date index. For 
 ex-ample, if you wanted a date index containing the last business day of each 
 month, you would pass the 
 'BM'
  frequency (business end of month) and only dates 
 falling on or inside the date interval will be included:
  
 In [386]: pd.date_range('1/1/2000', '12/1/2000', freq='BM') Out[386]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2000-01-31 00:00:00, ..., 2000-11-30 00:00:00] 
  
 Length: 11, Freq: BM, Timezone: None
  
 date_range
  by default preserves the time (if any) of the start or end timestamp:
  
 In [387]: pd.date_range('5/2/2012 12:56:31', periods=5) 
 Out[387]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2012-05-02 12:56:31, ..., 2012-05-06 12:56:31] 
  
 Length: 5, Freq: D, Timezone: None
  
 Sometimes you will have start or end dates with time information but want to 
 generate a set of timestamps 
 normalized
  to midnight as a convention. To do this, 
 there is a 
 normalize
  option:
  
 In [388]: pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True) 
 Out[388]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'>
  
 298 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Frequencies and Date Offsets,"Frequencies in pandas are composed of a 
 base frequency
  and a multiplier. Base fre-
 quencies are typically referred to by a string alias, like 
 'M'
  for monthly or 
 'H'
  for 
 hourly. For each base frequency, there is an object defined generally referred to as 
 a 
 date off-set
 . For example, hourly frequency can be represented with the 
 Hour
  
 class:
  
 In [389]: from pandas.tseries.offsets import Hour, Minute
  
 In [390]: hour = Hour()
  
 In [391]: hour 
  
 Out[391]: <1 Hour>
  
 You can define a multiple of an offset by passing an integer:
  
 In [392]: four_hours = Hour(4)
  
 In [393]: four_hours 
  
 Out[393]: <4 Hours>
  
 In most applications, you would never need to explicitly create one of these objects, 
 instead using a string alias like 
 'H'
  or 
 '4H'
 . Putting an integer before the base 
 frequency creates a multiple:
  
 In [394]: pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h') Out[394]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2000-01-01 00:00:00, ..., 2000-01-03 20:00:00] 
  
 Length: 18, Freq: 4H, Timezone: None
  
 Many offsets can be combined together by addition:
  
 In [395]: Hour(2) + Minute(30) 
  
 Out[395]: <150 Minutes>
  
 Similarly, you can pass frequency strings like 
 '2h30min'
  which will effectively be 
 parsed to the same expression:
  
 In [396]: pd.date_range('1/1/2000', periods=10, freq='1h30min') 
 Out[396]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2000-01-01 00:00:00, ..., 2000-01-01 13:30:00] 
  
 Length: 10, Freq: 90T, Timezone: None
  
 Some frequencies describe points in time that are not evenly spaced. For example, 
 'M'
  (calendar month end) and 
 'BM'
  (last business/weekday of month) depend on the 
 number of days in a month and, in the latter case, whether the month ends on a 
 weekend or not. For lack of a better term, I call these 
 anchored
  offsets.
  
 See 
 Table 10-4
  for a listing of frequency codes and date offset classes available in 
 pandas.
  
 Date Ranges, Frequencies, and Shifting | 299",NA
Shifting (Leading and Lagging) Data,"“Shifting” refers to moving data backward and forward through time. Both Series 
 and DataFrame have a 
 shift
  method for doing naive shifts forward or backward, 
 leaving the index unmodified:
  
 In [399]: ts = Series(np.random.randn(4),
  
  
  .....:             index=pd.date_range('1/1/2000', periods=4, freq='M'))
  
 In [400]: ts                In [401]: ts.shift(2)       In [402]: ts.shift(-2) Out[400]:                   Out[401]:                   
 Out[402]: 
  
 2000-01-31    0.575283      2000-01-31         NaN      2000-01-31    1.814582 2000-02-29    
 0.304205      2000-02-29         NaN      2000-02-29    1.634858 2000-03-31    1.814582      
 2000-03-31    0.575283      2000-03-31         NaN 2000-04-30    1.634858      2000-04-30    
 0.304205      2000-04-30         NaN Freq: M                     Freq: M                     Freq: M
  
 A common use of 
 shift
  is computing percent changes in a time series or multiple 
 time series as DataFrame columns. This is expressed as
  
 ts / ts.shift(1) - 1
  
 Because naive shifts leave the index unmodified, some data is discarded. Thus if the 
 frequency is known, it can be passed to 
 shift
  to advance the timestamps instead of 
 simply the data:
  
 In [403]: ts.shift(2, freq='M') 
  
 Out[403]: 
  
 2000-03-31    0.575283 
  
 2000-04-30    0.304205 
  
 2000-05-31    1.814582 
  
 2000-06-30    1.634858 
  
 Freq: M
  
 Date Ranges, Frequencies, and Shifting | 301
  
 www.it-ebooks.info",NA
Time Zone Handling,"Working with time zones is generally considered one of the most unpleasant parts 
 of time series manipulation. In particular, daylight savings time (DST) transitions 
 are a common source of complication. As such, many time series users choose to 
 work with time series in 
 coordinated universal time
  or 
 UTC
 , which is the successor 
 to Greenwich Mean Time and is the current international standard. Time zones are 
 expressed as offsets from UTC; for example, New York is four hours behind UTC 
 during daylight savings time and 5 hours the rest of the year.
  
 In Python, time zone information comes from the 3rd party 
 pytz
  library, which 
 exposes the 
 Olson database
 , a compilation of world time zone information. This is 
 especially important for historical data because the DST transition dates (and even 
 UTC offsets) have been changed numerous times depending on the whims of local 
 governments. In the United States,the DST transition times have been changed 
 many times since 1900!
  
 For detailed information about 
 pytz
  library, you’ll need to look at that library’s 
 docu-mentation. As far as this book is concerned, pandas wraps 
 pytz
 ’s functionality 
 so you can ignore its API outside of the time zone names. Time zone names can be 
 found interactively and in the docs:
  
 In [418]: import pytz
  
 In [419]: pytz.common_timezones[-5:] 
  
 Out[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']
  
 To get a time zone object from 
 pytz
 , use 
 pytz.timezone
 :
  
 In [420]: tz = pytz.timezone('US/Eastern')
  
 In [421]: tz 
  
 Out[421]: <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>
  
 Methods in pandas will accept either time zone names or these objects. I 
 recommend just using the names.
  
 Time Zone Handling | 303",NA
Localization and Conversion,"By default, time series in pandas are 
 time zone naive
 . Consider the following time 
 series:
  
 rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D') ts = 
 Series(np.random.randn(len(rng)), index=rng)
  
 The index’s 
 tz
  field is 
 None
 :
  
 In [423]: print(ts.index.tz) 
  
 None
  
 Date ranges can be generated with a time zone set:
  
 In [424]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC') Out[424]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> 
  
 [2012-03-09 09:30:00, ..., 2012-03-18 09:30:00] 
  
 Length: 10, Freq: D, Timezone: UTC
  
 Conversion from naive to 
 localized
  is handled by the 
 tz_localize
  method:
  
 In [425]: ts_utc = ts.tz_localize('UTC')
  
 In [426]: ts_utc 
  
 Out[426]: 
  
 2012-03-09 09:30:00+00:00    0.414615 
  
 2012-03-10 09:30:00+00:00    0.427185 
  
 2012-03-11 09:30:00+00:00    1.172557 
  
 2012-03-12 09:30:00+00:00   -0.351572 
  
 2012-03-13 09:30:00+00:00    1.454593 
  
 2012-03-14 09:30:00+00:00    2.043319 
  
 Freq: D
  
 In [427]: ts_utc.index 
  
 Out[427]: 
  
 <class 'pandas.tseries.index.DatetimeIndex'> [2012-03-
 09 09:30:00, ..., 2012-03-14 09:30:00] Length: 6, Freq: 
 D, Timezone: UTC
  
 Once a time series has been localized to a particular time zone, it can be converted 
 to another time zone using 
 tz_convert
 :
  
 In [428]: ts_utc.tz_convert('US/Eastern') 
  
 Out[428]: 
  
 2012-03-09 04:30:00-05:00    0.414615 
  
 2012-03-10 04:30:00-05:00    0.427185 
  
 2012-03-11 05:30:00-04:00    1.172557 
  
 2012-03-12 05:30:00-04:00   -0.351572 
  
 2012-03-13 05:30:00-04:00    1.454593 
  
 2012-03-14 05:30:00-04:00    2.043319 
  
 Freq: D
  
 In the case of the above time series, which straddles a DST transition in the 
 US/Eastern time zone, we could localize to EST and convert to, say, UTC or Berlin 
 time:
  
 In [429]: ts_eastern = ts.tz_localize('US/Eastern')
  
 304 | Chapter 10:Time Series",NA
Operations with Time Zone−aware Timestamp ,NA,NA
Objects,"Similar to time series and date ranges, individual Timestamp objects similarly can 
 be localized from naive to time zone-aware and converted from one time zone to 
 another:
  
 In [433]: stamp = pd.Timestamp('2011-03-12 04:00')
  
 In [434]: stamp_utc = stamp.tz_localize('utc')
  
 In [435]: stamp_utc.tz_convert('US/Eastern') 
  
 Out[435]: <Timestamp: 2011-03-11 23:00:00-0500 EST, tz=US/Eastern>
  
 You can also pass a time zone when creating the Timestamp:
  
 In [436]: stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')
  
 In [437]: stamp_moscow 
  
 Out[437]: <Timestamp: 2011-03-12 04:00:00+0300 MSK, tz=Europe/Moscow>
  
 Time zone-aware Timestamp objects internally store a UTC timestamp value as 
 nano-seconds since the UNIX epoch (January 1, 1970); this UTC value is invariant 
 between time zone conversions:
  
 Time Zone Handling | 305",NA
Operations between Different Time Zones,"If two time series with different time zones are combined, the result will be UTC. 
 Since the timestamps are stored under the hood in UTC, this is a straightforward 
 operation and requires no conversion to happen:
  
 In [447]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')
  
 In [448]: ts = Series(np.random.randn(len(rng)), index=rng)
  
 In [449]: ts 
  
 Out[449]: 
  
 2012-03-07 09:30:00   -1.749309 
  
 2012-03-08 09:30:00   -0.387235 
  
 2012-03-09 09:30:00   -0.208074 
  
 2012-03-12 09:30:00   -1.221957 
  
 2012-03-13 09:30:00   -0.067460 
  
 2012-03-14 09:30:00    0.229005 
  
 2012-03-15 09:30:00   -0.576234 
  
 2012-03-16 09:30:00    0.816895 
  
 2012-03-19 09:30:00   -0.772192 
  
 2012-03-20 09:30:00   -1.333576 
  
 Freq: B
  
 In [450]: ts1 = ts[:7].tz_localize('Europe/London')
  
 306 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Periods and Period Arithmetic,"Periods
  represent time spans, like days, months, quarters, or years. The 
 Period
  class 
 represents this data type, requiring a string or integer and a frequency from the 
 above table:
  
 In [454]: p = pd.Period(2007, freq='A-DEC')
  
 In [455]: p 
  
 Out[455]: Period('2007', 'A-DEC')
  
 In this case, the 
 Period
  object represents the full timespan from January 1, 2007 to 
 December 31, 2007, inclusive. Conveniently, adding and subtracting integers from 
 pe-riods has the effect of shifting by their frequency:
  
 In [456]: p + 5                          In [457]: p - 2 
  
 Out[456]: Period('2012', 'A-DEC')        Out[457]: Period('2005', 'A-DEC')
  
 If two periods have the same frequency, their difference is the number of units 
 between them:
  
 In [458]: pd.Period('2014', freq='A-DEC') - p 
  
 Out[458]: 7
  
 Regular ranges of periods can be constructed using the 
 period_range
  function:
  
 In [459]: rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')
  
 In [460]: rng 
  
 Out[460]: 
  
 <class 'pandas.tseries.period.PeriodIndex'> 
  
 freq: M 
  
 [2000-01, ..., 2000-06] 
  
 length: 6
  
 The 
 PeriodIndex
  class stores a sequence of periods and can serve as an axis index in 
 any pandas data structure:
  
 In [461]: Series(np.random.randn(6), index=rng) 
 Out[461]: 
  
 2000-01   -0.309119 
  
 2000-02    0.028558 
  
 2000-03    1.129605 
  
 2000-04   -0.374173 
  
 2000-05   -0.011401
  
 Periods and Period Arithmetic | 307
  
 www.it-ebooks.info",NA
Period Frequency Conversion,"Periods and 
 PeriodIndex
  objects can be converted to another frequency using their 
 asfreq
  method. As an example, suppose we had an annual period and wanted to 
 convert it into a monthly period either at the start or end of the year. This is fairly 
 straightfor-ward:
  
 In [465]: p = pd.Period('2007', freq='A-DEC')
  
 In [466]: p.asfreq('M', how='start')      In [467]: p.asfreq('M', how='end') Out[466]: 
 Period('2007-01', 'M')         Out[467]: Period('2007-12', 'M')
  
 You can think of 
 Period('2007', 'A-DEC')
  as being a cursor pointing to a span of time, 
 subdivided by monthly periods. See 
 Figure 10-1
  for an illustration of this. For a 
 fiscal year
  ending on a month other than December, the monthly subperiods 
 belonging are different:
  
 In [468]: p = pd.Period('2007', freq='A-JUN')
  
 In [469]: p.asfreq('M', 'start')       In [470]: p.asfreq('M', 'end') Out[469]: 
 Period('2006-07', 'M')      Out[470]: Period('2007-07', 'M')
  
 When converting from high to low frequency, the superperiod will be determined 
 de-pending on where the subperiod “belongs”. For example, in 
 A-JUN
  frequency, the 
 month 
 Aug-2007
  is actually part of the 
 2008
  period:
  
 In [471]: p = pd.Period('2007-08', 'M')
  
 In [472]: p.asfreq('A-JUN') 
  
 Out[472]: Period('2008', 'A-JUN')
  
 Whole 
 PeriodIndex
  objects or TimeSeries can be similarly converted with the same 
 semantics:
  
 In [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')
  
 In [474]: ts = Series(np.random.randn(len(rng)), index=rng)
  
 In [475]: ts
  
 308 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Quarterly Period Frequencies,"Quarterly data is standard in accounting, finance, and other fields. Much quarterly 
 data is reported relative to a 
 fiscal year end
 , typically the last calendar or business 
 day of one of the 12 months of the year. As such, the period 
 2012Q4
  has a different 
 meaning de-pending on fiscal year end. pandas supports all 12 possible quarterly 
 frequencies as 
 Q-JAN
  through 
 Q-DEC
 :
  
 In [478]: p = pd.Period('2012Q4', freq='Q-JAN')
  
 In [479]: p 
  
 Out[479]: Period('2012Q4', 'Q-JAN')
  
 In the case of fiscal year ending in January, 
 2012Q4
  runs from November through 
 Jan-uary, which you can check by converting to daily frequency. See 
 Figure 10-2
  for 
 an illustration:
  
 In [480]: p.asfreq('D', 'start')          In [481]: p.asfreq('D', 'end') Out[480]: Period('2011-11-
 01', 'D')      Out[481]: Period('2012-01-31', 'D')
  
 Periods and Period Arithmetic | 309
  
 www.it-ebooks.info",NA
Converting Timestamps to Periods (and Back),"Series and DataFrame objects indexed by timestamps can be converted to periods 
 using the 
 to_period
  method:
  
 In [491]: rng = pd.date_range('1/1/2000', periods=3, freq='M')
  
 In [492]: ts = Series(randn(3), index=rng)
  
 In [493]: pts = ts.to_period()
  
 In [494]: ts                  In [495]: pts 
  
 Out[494]:                     Out[495]: 
  
 2000-01-31   -0.505124        2000-01   -0.505124 2000-02-
 29    2.954439        2000-02    2.954439 2000-03-31   -
 2.630247        2000-03   -2.630247 Freq: M                       
 Freq: M
  
 Since periods always refer to non-overlapping timespans, a timestamp can only 
 belong to a single period for a given frequency. While the frequency of the new 
 PeriodIndex
  is inferred from the timestamps by default, you can specify any 
 frequency you want. There is also no problem with having duplicate periods in the 
 result:
  
 In [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')
  
 In [497]: ts2 = Series(randn(6), index=rng)
  
 In [498]: ts2.to_period('M') 
  
 Out[498]: 
  
 2000-01   -0.352453 
  
 2000-01   -0.477808 
  
 2000-01    0.161594 
  
 2000-02    1.686833 
  
 2000-02    0.821965 
  
 2000-02   -0.667406 
  
 Freq: M
  
 To convert back to timestamps, use 
 to_timestamp
 :
  
 In [499]: pts = ts.to_period()
  
 In [500]: pts 
  
 Out[500]: 
  
 2000-01   -0.505124 
  
 2000-02    2.954439 
  
 2000-03   -2.630247 
  
 Freq: M
  
 In [501]: pts.to_timestamp(how='end') 
  
 Out[501]: 
  
 2000-01-31   -0.505124 
  
 2000-02-29    2.954439 
  
 2000-03-31   -2.630247 
  
 Freq: M
  
 Periods and Period Arithmetic | 311",NA
Creating a PeriodIndex from Arrays,"Fixed frequency data sets are sometimes stored with timespan information spread 
 across multiple columns. For example, in this macroeconomic data set, the year and 
 quarter are in different columns:
  
 In [502]: data = pd.read_csv('ch08/macrodata.csv')
  
 In [503]: data.year            In [504]: data.quarter Out[503]:                      
 Out[504]: 
  
 0    1959                      0    1 
  
 1    1959                      1    2 
  
 2    1959                      2    3 
  
 3    1959                      3    4 
  
 ...                            ... 
  
 199    2008                    199    4 
  
 200    2009                    200    1 
  
 201    2009                    201    2 
  
 202    2009                    202    3 
  
 Name: year, Length: 203        Name: quarter, Length: 203
  
 By passing these arrays to 
 PeriodIndex
  with a frequency, they can be combined to 
 form an index for the DataFrame:
  
 In [505]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')
  
 In [506]: index 
  
 Out[506]: 
  
 <class 'pandas.tseries.period.PeriodIndex'> 
  
 freq: Q-DEC 
  
 [1959Q1, ..., 2009Q3] 
  
 length: 203
  
 In [507]: data.index = index
  
 In [508]: data.infl 
  
 Out[508]: 
  
 1959Q1    0.00 
  
 1959Q2    2.34 
  
 1959Q3    2.74 
  
 1959Q4    0.27 
  
 ...
  
 2008Q4 
  
  
 -8.79 
  
 2009Q1 
  
  
  
 0.94 
  
 2009Q2 
  
  
  
 3.37 
  
 2009Q3 
  
  
  
 3.56 
  
 Freq: Q-DEC, Name: infl, Length: 203",NA
Resampling and Frequency Conversion,"Resampling
  refers to the process of converting a time series from one frequency to 
 another. Aggregating higher frequency data to lower frequency is called 
 downsam-
 pling
 , while converting lower frequency to higher frequency is called 
 upsampling
 . 
 Not
  
 312 | Chapter 10:Time Series",NA
Downsampling,"Aggregating data to a regular, lower frequency is a pretty normal time series task. 
 The data you’re aggregating doesn’t need to be fixed frequently; the desired 
 frequency de-fines 
 bin edges
  that are used to slice the time series into pieces to 
 aggregate. For example, to convert to monthly, 
 'M'
  or 
 'BM'
 , the data need to be 
 chopped up into one month intervals. Each interval is said to be 
 half-open
 ; a data 
 point can only belong to one interval, and the union of the intervals must make up 
 the whole time frame. There are a couple things to think about when using 
 resample
  
 to downsample data:
  
 • Which side of each interval is 
 closed
  
 • How to label each aggregated bin, either with the start of the interval or the end
  
 To illustrate, let’s look at some one-minute data:
  
 In [513]: rng = pd.date_range('1/1/2000', periods=12, freq='T')
  
 In [514]: ts = Series(np.arange(12), index=rng)
  
 In [515]: ts 
  
 Out[515]: 
  
 2000-01-01 00:00:00     0 
  
 2000-01-01 00:01:00     1 
  
 2000-01-01 00:02:00     2 
  
 2000-01-01 00:03:00     3 
  
 2000-01-01 00:04:00     4 
  
 2000-01-01 00:05:00     5 
  
 2000-01-01 00:06:00     6 
  
 2000-01-01 00:07:00     7 
  
 2000-01-01 00:08:00     8 
  
 2000-01-01 00:09:00     9 
  
 2000-01-01 00:10:00    10 
  
 2000-01-01 00:11:00    11 
  
 Freq: T
  
 Suppose you wanted to aggregate this data into five-minute chunks or 
 bars
  by 
 taking the sum of each group:
  
 In [516]: ts.resample('5min', how='sum') 
  
 Out[516]: 
  
 2000-01-01 00:00:00     0 
  
 2000-01-01 00:05:00    15 
  
 2000-01-01 00:10:00    40 
  
 2000-01-01 00:15:00    11 
  
 Freq: 5T
  
 314 | Chapter 10:Time Series",NA
Upsampling and Interpolation,"When converting from a low frequency to a higher frequency, no aggregation is 
 needed. Let’s consider a DataFrame with some weekly data:
  
 In [525]: frame = DataFrame(np.random.randn(2, 4),
  
  .....:                   index=pd.date_range('1/1/2000', periods=2, freq='W-WED'), .....:                   
 columns=['Colorado', 'Texas', 'New York', 'Ohio'])
  
 316 | Chapter 10:Time Series",NA
Resampling with Periods,"Resampling data indexed by periods is reasonably straightforward and works as 
 you would hope:
  
 In [532]: frame = DataFrame(np.random.randn(24, 4),
  
  .....:                   index=pd.period_range('1-2000', '12-2001', freq='M'), .....:                   
 columns=['Colorado', 'Texas', 'New York', 'Ohio'])
  
 In [533]: frame[:5] 
  
 Out[533]: 
  
  
  Colorado     Texas  New York      Ohio 2000-01  
 0.120837  1.076607  0.434200  0.056432 2000-02 -
 0.378890  0.047831  0.341626  1.567920 2000-03 -
 0.047619 -0.821825 -0.179330 -0.166675 2000-04  
 0.333219 -0.544615 -0.653635 -2.311026 2000-05  
 1.612270 -0.806614  0.557884  0.580201
  
 In [534]: annual_frame = frame.resample('A-DEC', how='mean')
  
 In [535]: annual_frame 
  
 Out[535]: 
  
  
  Colorado     Texas  New York      Ohio 
  
 2000  0.352070 -0.553642  0.196642 -0.094099 
  
 2001  0.158207  0.042967 -0.360755  0.184687
  
 Upsampling is more nuanced as you must make a decision about which end of the 
 timespan in the new frequency to place the values before resampling, just like the 
 asfreq
  method. The 
 convention
  argument defaults to 
 'end'
  but can also be 
 'start'
 :
  
 # Q-DEC: Quarterly, year ending in December 
  
 In [536]: annual_frame.resample('Q-DEC', fill_method='ffill') Out[536]: 
  
  
  Colorado     Texas  New York      Ohio 
  
 2000Q4  0.352070 -0.553642  0.196642 -0.094099 
  
 2001Q1  0.352070 -0.553642  0.196642 -0.094099 
  
 2001Q2  0.352070 -0.553642  0.196642 -0.094099 
  
 2001Q3  0.352070 -0.553642  0.196642 -0.094099 
  
 2001Q4  0.158207  0.042967 -0.360755  0.184687
  
 In [537]: annual_frame.resample('Q-DEC', fill_method='ffill', convention='start') Out[537]: 
  
  
  Colorado     Texas  New York      Ohio 
  
 2000Q1  0.352070 -0.553642  0.196642 -0.094099 
  
 2000Q2  0.352070 -0.553642  0.196642 -0.094099 
  
 2000Q3  0.352070 -0.553642  0.196642 -0.094099 
  
 2000Q4  0.352070 -0.553642  0.196642 -0.094099 
  
 2001Q1  0.158207  0.042967 -0.360755  0.184687
  
 318 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Time Series Plotting,"Plots with pandas time series have improved date formatting compared with 
 matplotlib out of the box. As an example, I downloaded some stock price data on a 
 few common US stock from Yahoo! Finance:
  
 In [539]: close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)
  
 In [540]: close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]
  
 In [541]: close_px = close_px.resample('B', fill_method='ffill')
  
 In [542]: close_px 
  
 Out[542]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 DatetimeIndex: 2292 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00 Freq: B 
  
 Data columns: 
  
 AAPL    2292  non-null values 
  
 MSFT    2292  non-null values 
  
 XOM     2292  non-null values 
  
 dtypes: float64(3)
  
 Calling 
 plot
  on one of the columns grenerates a simple plot, seen in 
 Figure 10-4
 .
  
 In [544]: close_px['AAPL'].plot()
  
 When called on a DataFrame, as you would expect, all of the time series are drawn 
 on a single subplot with a legend indicating which is which. I’ll plot only the year 
 2009 data so you can see how both months and years are formatted on the X axis; 
 see 
 Figure 10-5
 .
  
 In [546]: close_px.ix['2009'].plot()
  
 Time Series Plotting | 319
  
 www.it-ebooks.info",NA
Moving Window Functions,"A common class of array transformations intended for time series operations are 
 sta-tistics and other functions evaluated over a sliding window or with 
 exponentially de-
  
 320 | Chapter 10:Time Series",NA
Exponentially-weighted functions,"An alternative to using a static window size with equally-weighted observations is 
 to specify a constant 
 decay factor
  to give more weight to more recent observations. 
 In mathematical terms, if ma
 t
  is the moving average result at time 
 t
  and x is the time 
 series in question, each value in the result is computed as ma
 t
  = a * ma
 t - 1
  + (a - 1) * 
 x_
 t
 , where a is the decay factor. There are a couple of ways to specify the decay 
 factor, a popular one is using a 
 span
 , which makes the result comparable to a simple 
 moving window function with window size equal to the span.
  
 Since an exponentially-weighted statistic places more weight on more recent 
 observa-tions, it “adapts” faster to changes compared with the equal-weighted 
 version. Here’s an example comparing a 60-day moving average of Apple’s stock 
 price with an EW moving average with 
 span=60
  (see 
 Figure 10-11
 ):
  
 fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,
  
  
 figsize=(12, 7))
  
 aapl_px = close_px.AAPL['2005':'2009']
  
 ma60 = pd.rolling_mean(aapl_px, 60, min_periods=50) 
 ewma60 = pd.ewma(aapl_px, span=60)
  
 aapl_px.plot(style='k-', ax=axes[0]) 
  
 ma60.plot(style='k--', ax=axes[0]) 
  
 aapl_px.plot(style='k-', ax=axes[1]) 
  
 ewma60.plot(style='k--', ax=axes[1]) 
  
 axes[0].set_title('Simple MA') 
  
 axes[1].set_title('Exponentially-weighted MA')",NA
Binary Moving Window Functions,"Some statistical operators, like correlation and covariance, need to operate on two 
 time series. As an example, financial analysts are often interested in a stock’s 
 correlation to a benchmark index like the S&P 500. We can compute that by 
 computing the percent changes and using 
 rolling_corr
  (see 
 Figure 10-12
 ):
  
 In [570]: spx_rets = spx_px / spx_px.shift(1) - 1
  
 In [571]: returns = close_px.pct_change()
  
 In [572]: corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)
  
 In [573]: corr.plot()
  
 324 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
User-Defined Moving Window Functions,"The 
 rolling_apply
  function provides a means to apply an array function of your own 
 devising over a moving window. The only requirement is that the function produce 
 a single value (a reduction) from each piece of the array. For example, while we can 
 compute sample quantiles using 
 rolling_quantile
 , we might be interested in the per-
 centile rank of a particular value over the sample. The 
 scipy.stats.percentileof score
  
 function does just this:
  
 In [578]: from scipy.stats import percentileofscore
  
 In [579]: score_at_2percent = lambda x: percentileofscore(x, 0.02)
  
 In [580]: result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)
  
 In [581]: result.plot()
  
 326 | Chapter 10:Time Series
  
 www.it-ebooks.info",NA
Performance and Memory Usage Notes,"Timestamps and periods are represented as 64-bit integers using NumPy’s 
 date 
 time64
  dtype. This means that for each data point, there is an associated 8 bytes of 
 memory per timestamp. Thus, a time series with 1 million 
 float64
  data points has a 
 memory footprint of approximately 16 megabytes. Since pandas makes every effort 
 to share indexes among time series, creating views on existing time series do not 
 cause any more memory to be used. Additionally, indexes for lower frequencies 
 (daily and up) are stored in a central cache, so that any fixed-frequency index is a 
 view on the date cache. Thus, if you have a large collection of low-frequency time 
 series, the memory footprint of the indexes will not be as significant.
  
 Performance-wise, pandas has been highly optimized for data alignment operations 
 (the behind-the-scenes work of differently indexed 
 ts1 + ts2
 ) and resampling. Here 
 is an example of aggregating 10MM data points to OHLC:
  
 In [582]: rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')
  
 In [583]: ts = Series(np.random.randn(len(rng)), index=rng)
  
 In [584]: ts 
  
 Out[584]: 
  
 2000-01-01 00:00:00          -1.402235 
  
 2000-01-01 00:00:00.010000    2.424667 
  
 2000-01-01 00:00:00.020000   -1.956042 
  
 2000-01-01 00:00:00.030000   -0.897339 
  
 ...
  
 2000-01-02 03:46:39.960000    0.495530 
  
 2000-01-02 03:46:39.970000    0.574766 
  
 2000-01-02 03:46:39.980000    1.348374 
  
 2000-01-02 03:46:39.990000    0.665034 
  
 Freq: 10L, Length: 10000000
  
 In [585]: ts.resample('15min', how='ohlc') 
  
 Out[585]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 DatetimeIndex: 113 entries, 2000-01-01 00:00:00 to 2000-01-02 04:00:00 Freq: 
 15T 
  
 Data columns: 
  
 open     113  non-null values 
  
 high     113  non-null values 
  
 low      113  non-null values 
  
 close    113  non-null values 
  
 dtypes: float64(4)
  
 In [586]: %timeit ts.resample('15min', how='ohlc') 10 loops, 
 best of 3: 61.1 ms per loop
  
 The runtime may depend slightly on the relative size of the aggregated result; 
 higher frequency aggregates unsurprisingly take longer to compute:
  
 In [587]: rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')
  
 Performance and Memory Usage Notes | 327
  
 www.it-ebooks.info",NA
CHAPTER 11,NA,NA
Financial and ,NA,NA
Economi,NA,NA
c Data ,NA,NA
Applicati,NA,NA
ons,"The use of Python in the financial industry has been increasing rapidly since 2005, 
 led largely by the maturation of libraries (like NumPy and pandas) and the 
 availability of skilled Python programmers. Institutions have found that Python is 
 well-suited both as an interactive analysis environment as well as enabling robust 
 systems to be devel-oped often in a fraction of the time it would have taken in Java 
 or C++. Python is also an ideal glue layer; it is easy to build Python interfaces to 
 legacy libraries built in C or C++.
  
 While the field of financial analysis is broad enough to fill an entire book, I hope to 
 show you how the tools in this book can be applied to a number of specific 
 problems in finance. As with other research and analysis domains, too much 
 programming effort is often spent wrangling data rather than solving the core 
 modeling and research prob-lems. I personally got started building pandas in 2008 
 while grappling with inadequate data tools.
  
 In these examples, I’ll use the term 
 cross-section
  to refer to data at a fixed point in 
 time. For example, the closing prices of all the stocks in the S&P 500 index on a 
 particular date form a cross-section. Cross-sectional data at multiple points in time 
 over multiple data items (for example, prices together with volume) form a 
 panel
 . 
 Panel data can either be represented as a hierarchically-indexed DataFrame or 
 using the three-dimen-sional Panel pandas object.",NA
Data Munging Topics,"Many helpful data munging tools for financial applications are spread across the 
 earlier chapters. Here I’ll highlight a number of topics as they relate to this problem 
 domain.",NA
Time Series and Cross-Section Alignment,"One of the most time-consuming issues in working with financial data is the so-
 called 
 data alignment
  problem. Two related time series may have indexes that don’t 
 line up perfectly, or two DataFrame objects might have columns or row labels that 
 don’t match. Users of MATLAB, R, and other matrix-programming languages often 
 invest significant effort in wrangling data into perfectly aligned forms. In my 
 experience, having to align data by hand (and worse, having to verify that data is 
 aligned) is a far too rigid and tedious way to work. It is also rife with potential for 
 bugs due to combining misaligned data.
  
 pandas take an alternate approach by automatically aligning data in arithmetic 
 opera-tions. In practice, this grants immense freedom and enhances your 
 productivity. As an example, let’s consider a couple of DataFrames containing time 
 series of stock prices and volume:
  
 In [16]: prices 
  
 Out[16]: 
  
  
  AAPL    JNJ      SPX    XOM 
  
 2011-09-06  379.74  64.64  1165.24  71.15 
  
 2011-09-07  383.93  65.43  1198.62  73.65 
  
 2011-09-08  384.14  64.95  1185.90  72.82 
  
 2011-09-09  377.48  63.64  1154.23  71.01 
  
 2011-09-12  379.94  63.59  1162.27  71.84 
  
 2011-09-13  384.62  63.61  1172.87  71.65 
  
 2011-09-14  389.30  63.73  1188.68  72.64
  
 In [17]: volume 
  
 Out[17]: 
  
  
  AAPL       JNJ       XOM 
  
 2011-09-06  18173500  15848300  25416300 
  
 2011-09-07  12492000  10759700  23108400 
  
 2011-09-08  14839800  15551500  22434800 
  
 2011-09-09  20171900  17008200  27969100 
  
 2011-09-12  16697300  13448200  26205800
  
 Suppose you wanted to compute a volume-weighted average price using all 
 available data (and making the simplifying assumption that the volume data is a 
 subset of the price data). Since pandas aligns the data automatically in arithmetic 
 and excludes missing data in functions like 
 sum
 , we can express this concisely as:
  
 In [18]: prices * volume 
  
 Out[18]: 
  
  
  AAPL         JNJ  SPX         XOM 2011-09-06  
 6901204890  1024434112  NaN  1808369745 2011-09-07  
 4796053560   704007171  NaN  1701933660 2011-09-08  
 5700560772  1010069925  NaN  1633702136 2011-09-09  
 7614488812  1082401848  NaN  1986085791 2011-09-12  
 6343972162   855171038  NaN  1882624672 2011-09-13         
 NaN         NaN  NaN         NaN 2011-09-14         NaN         NaN  
 NaN         NaN
  
 In [19]: vwap = (prices * volume).sum() / volume.sum()
  
 330 | Chapter 11:Financial and Economic Data Applications
  
 www.it-ebooks.info",NA
Operations with Time Series of Different ,NA,NA
Frequencies,"Economic time series are often of annual, quarterly, monthly, daily, or some other 
 more specialized frequency. Some are completely irregular; for example, earnings 
 revisions for a stock may arrive at any time. The two main tools for frequency 
 conversion and realignment are the 
 resample
  and 
 reindex
  methods. 
 resample
  
 converts data to a fixed frequency while 
 reindex
  conforms data to a new index. Both 
 support optional inter-polation (such as forward filling) logic.
  
 Let’s consider a small weekly time series:
  
 In [28]: ts1 = Series(np.random.randn(3),
  
  
  ....:              index=pd.date_range('2012-6-13', periods=3, freq='W-WED'))
  
 In [29]: ts1 
  
 Out[29]: 
  
 2012-06-13   -1.124801 
  
 2012-06-20    0.469004 
  
 2012-06-27   -0.117439 
  
 Freq: W-WED
  
 If you resample this to business daily (Monday-Friday) frequency, you get holes on 
 the days where there is no data:
  
 In [30]: ts1.resample('B') 
  
 Out[30]: 
  
 2012-06-13   -1.124801 
  
 2012-06-14         NaN 
  
 2012-06-15         NaN 
  
 2012-06-18         NaN 
  
 2012-06-19         NaN 
  
 2012-06-20    0.469004 
  
 2012-06-21         NaN 
  
 2012-06-22         NaN 
  
 2012-06-25         NaN 
  
 2012-06-26         NaN 
  
 2012-06-27   -0.117439 
  
 Freq: B
  
 Of course, using 
 'ffill'
  as the 
 fill_method
  forward fills values in those gaps. This is a 
 common practice with lower frequency data as you compute a time series of values 
 on each timestamp having the latest valid or 
 “as of”
  value:
  
 In [31]: ts1.resample('B', fill_method='ffill') Out[31]: 
  
 2012-06-13   -1.124801 
  
 2012-06-14   -1.124801 
  
 2012-06-15   -1.124801 
  
 2012-06-18   -1.124801 
  
 2012-06-19   -1.124801 
  
 2012-06-20    0.469004 
  
 2012-06-21    0.469004 
  
 2012-06-22    0.469004 
  
 2012-06-25    0.469004 
  
 2012-06-26    0.469004
  
 332 | Chapter 11:Financial and Economic Data Applications",NA
Time of Day and “as of” Data Selection,"Suppose you have a long time series containing intraday market data and you want 
 to extract the prices at a particular time of day on each day of the data. What if the 
 data are irregular such that observations do not fall exactly on the desired time? In 
 practice this task can make for error-prone data munging if you are not careful. 
 Here is an example for illustration purposes:
  
 # Make an intraday date range and time series 
  
 In [44]: rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')
  
 # Make a 5-day series of 9:30-15:59 values
  
 334 | Chapter 11:Financial and Economic Data Applications
  
 www.it-ebooks.info",NA
Splicing Together Data Sources,"In 
 Chapter 7
 , I described a number of strategies for merging together two related 
 data sets. In a financial or economic context, there are a few widely occurring use 
 cases:
  
 • Switching from one data source (a time series or collection of time series) to 
 another 
  
 at a specific point in time
  
 • “Patching” missing values in a time series at the beginning, middle, or end using 
  
 another time series
  
 • Completely replacing the data for a subset of symbols (countries, asset tickers, 
 and 
  
 so on)
  
 In the first case, switching from one set of time series to another at a specific 
 instant, it is a matter of splicing together two TimeSeries or DataFrame objects 
 using 
 pandas.con cat
 :
  
 In [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),
  
  ....:                   columns=['a', 'b', 'c'],
  
  ....:                   index=pd.date_range('6/12/2012', periods=6))
  
 336 | Chapter 11:Financial and Economic Data Applications",NA
Return Indexes and Cumulative Returns,"In a financial context, 
 returns
  usually refer to percent changes in the price of an 
 asset. Let’s consider price data for Apple in 2011 and 2012:
  
 In [73]: import pandas.io.data as web
  
 In [74]: price = web.get_data_yahoo('AAPL', '2011-01-01')['Adj Close']
  
 In [75]: price[-5:] 
  
 Out[75]: 
  
 Date 
  
 2012-07-23    603.83 
  
 2012-07-24    600.92 
  
 2012-07-25    574.97 
  
 2012-07-26    574.88 
  
 2012-07-27    585.16 
  
 Name: Adj Close
  
 For Apple, which has no dividends, computing the cumulative percent return 
 between two points in time requires computing only the percent change in the 
 price:
  
 In [76]: price['2011-10-03'] / price['2011-3-01'] - 1 Out[76]: 
 0.072399874037388123
  
 338 | Chapter 11:Financial and Economic Data Applications",NA
Group Transforms and Analysis,"In 
 Chapter 9
 , you learned the basics of computing group statistics and applying 
 your own transformations to groups in a dataset.
  
 Let’s consider a collection of hypothetical stock portfolios. I first randomly generate 
 a broad 
 universe
  of 2000 tickers:
  
 import random; random.seed(0) 
  
 import string
  
 N = 1000 
  
 def rands(n):
  
  
  choices = string.ascii_uppercase
  
  
  return ''.join([random.choice(choices) for _ in xrange(n)]) tickers = 
 np.array([rands(5) for _ in xrange(N)])
  
 I then create a DataFrame containing 3 columns representing hypothetical, but 
 random portfolios for a subset of tickers:
  
 M = 500 
  
 df = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,
  
  'Value' : 
 np.random.randn(M) / 200 + 0.08,
  
  
  'ShortInterest' : np.random.randn(M) / 200 - 0.02},
  
  
 index=tickers[:M])
  
 Next, let’s create a random industry classification for the tickers. To keep things 
 simple, I’ll just keep it to 2 industries, storing the mapping in a Series:
  
 ind_names = np.array(['FINANCIAL', 'TECH']) 
  
 sampler = np.random.randint(0, len(ind_names), N) industries = 
 Series(ind_names[sampler], index=tickers,
  
  
 name='industry')
  
 Now we can group by 
 industries
  and carry out group aggregation and 
 transformations:
  
 In [90]: by_industry = df.groupby(industries)
  
 In [91]: by_industry.mean() 
  
 Out[91]: 
  
  
  Momentum  ShortInterest     Value
  
 340 | Chapter 11:Financial and Economic Data Applications",NA
Group Factor Exposures,"Factor analysis
  is a technique in quantitative portfolio management. Portfolio 
 holdings and performance (profit and less) are decomposed using one or more 
 factors
  (risk fac-tors are one example) represented as a portfolio of weights. For 
 example, a stock price’s co-movement with a benchmark (like S&P 500 index) is 
 known as its 
 beta
 , a common risk factor. Let’s consider a contrived example of a 
 portfolio constructed from 3 ran-domly-generated factors (usually called the 
 factor 
 loadings
 ) and some weights:
  
 from numpy.random import rand 
  
 fac1, fac2, fac3 = np.random.rand(3, 1000)
  
 ticker_subset = tickers.take(np.random.permutation(N)[:1000])
  
 # Weighted sum of factors plus noise 
  
 port = Series(0.7 * fac1 - 1.2 * fac2 + 0.3 * fac3 + rand(1000),
  
  
 index=ticker_subset) 
  
 factors = DataFrame({'f1': fac1, 'f2': fac2, 'f3': fac3},
  
  
  
  index=ticker_subset)
  
 Vector correlations between each factor and the portfolio may not indicate too 
 much:
  
 In [99]: factors.corrwith(port) 
  
 Out[99]: 
  
 f1    0.402377 
  
 f2   -0.680980 
  
 f3    0.168083
  
 The standard way to compute the factor exposures is by least squares regression; 
 using 
 pandas.ols
  with 
 factors
  as the explanatory variables we can compute exposures 
 over the entire set of tickers:
  
 In [100]: pd.ols(y=port, x=factors).beta 
  
 Out[100]: 
  
 f1           0.761789 
  
 f2          -1.208760 
  
 f3           0.289865 
  
 intercept    0.484477
  
 342 | Chapter 11:Financial and Economic Data Applications",NA
Decile and Quartile Analysis,"Analyzing data based on sample quantiles is another important tool for financial 
 ana-lysts. For example, the performance of a stock portfolio could be broken down 
 into quartiles (four equal-sized chunks) based on each stock’s price-to-earnings. 
 Using 
 pan das.qcut
  combined with 
 groupby
  makes quantile analysis reasonably 
 straightforward.
  
 As an example, let’s consider a simple trend following or 
 momentum
  strategy 
 trading the S&P 500 index via the SPY exchange-traded fund. You can download the 
 price history from Yahoo! Finance:
  
 In [105]: import pandas.io.data as web
  
 In [106]: data = web.get_data_yahoo('SPY', '2006-01-01')
  
 In [107]: data 
  
 Out[107]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 DatetimeIndex: 1655 entries, 2006-01-03 00:00:00 to 2012-07-27 00:00:00 Data 
 columns: 
  
 Open         1655  non-null values 
  
 High         1655  non-null values 
  
 Low          1655  non-null values 
  
 Close        1655  non-null values 
  
 Volume       1655  non-null values 
  
 Adj Close    1655  non-null values 
  
 dtypes: float64(5), int64(1)
  
 Now, we’ll compute daily returns and a function for transforming the returns into a 
 trend signal formed from a lagged moving sum:
  
 px = data['Adj Close'] 
  
 returns = px.pct_change()
  
 Group Transforms and Analysis | 343",NA
More Example Applications,Here is a small set of additional examples.,NA
Signal Frontier Analysis,"In this section, I’ll describe a simplified cross-sectional momentum portfolio and 
 show how you might explore a grid of model parameterizations. First, I’ll load 
 historical prices for a portfolio of financial and technology stocks:
  
 names = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', 'BAC', 'C'] def get_px(stock, 
 start, end):
  
  
  return web.get_data_yahoo(stock, start, end)['Adj Close'] 
  
 px = DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})
  
 We can easily plot the cumulative returns of each stock (see 
 Figure 11-2
 ):
  
 In [117]: px = px.asfreq('B').fillna(method='pad')
  
 In [118]: rets = px.pct_change()
  
 In [119]: ((1 + rets).cumprod() - 1).plot()
  
 For the portfolio construction, we’ll compute momentum over a certain lookback, 
 then rank in descending order and standardize:
  
 def calc_mom(price, lookback, lag):
  
  mom_ret = price.shift(lag).pct_change(lookback) ranks = 
 mom_ret.rank(axis=1, ascending=False) demeaned = 
 ranks - ranks.mean(axis=1)
  
  return demeaned / demeaned.std(axis=1)
  
 With this transform function in hand, we can set up a strategy backtesting function 
 that computes a portfolio for a particular lookback and holding period (days 
 between trading), returning the overall Sharpe ratio:
  
 compound = lambda x : (1 + x).prod() - 1 
  
 daily_sr = lambda x: x.mean() / x.std()
  
 More Example Applications | 345
  
 www.it-ebooks.info",NA
Future Contract Rolling,"A 
 future
  is an ubiquitous form of derivative contract; it is an agreement to take 
 delivery of a certain asset (such as oil, gold, or shares of the FTSE 100 index) on a 
 particular date. In practice, modeling and trading futures contracts on equities, 
 currencies,
  
 More Example Applications | 347",NA
Rolling Correlation and Linear Regression,"Dynamic models play an important role in financial modeling as they can be used to 
 simulate trading decisions over a historical period. Moving window and 
 exponentially-weighted time series functions are an example of tools that are used 
 for dynamic models.
  
 Correlation is one way to look at the co-movement between the changes in two 
 asset time series. pandas’s 
 rolling_corr
  function can be called with two return series 
 to compute the moving window correlation. First, I load some price series from 
 Yahoo!
  
 Finance and compute daily returns:
  
 aapl = web.get_data_yahoo('AAPL', '2000-01-01')['Adj Close'] msft = 
 web.get_data_yahoo('MSFT', '2000-01-01')['Adj Close']
  
 aapl_rets = aapl.pct_change() 
  
 msft_rets = msft.pct_change()
  
 Then, I compute and plot the one-year moving correlation (see 
 Figure 11-4
 ):
  
 In [140]: pd.rolling_corr(aapl_rets, msft_rets, 250).plot()
  
 One issue with correlation between two assets is that it does not capture 
 differences in volatility. Least-squares regression provides another means for 
 modeling the dynamic relationship between a variable and one or more other 
 predictor variables.
  
 In [142]: model = pd.ols(y=aapl_rets, x={'MSFT': msft_rets}, window=250)
  
 In [143]: model.beta 
  
 Out[143]: 
  
 <class 'pandas.core.frame.DataFrame'> 
  
 DatetimeIndex: 2913 entries, 2000-12-28 00:00:00 to 2012-07-27 00:00:00 Data 
 columns:
  
 350 | Chapter 11:Financial and Economic Data Applications",NA
CHAPTER 12,NA,NA
Advanced NumPy,NA,NA
ndarray Object Internals,"The NumPy ndarray provides a means to interpret a block of homogeneous data 
 (either contiguous or strided, more on this later) as a multidimensional array 
 object. As you’ve seen, the data type, or 
 dtype
 , determines how the data is 
 interpreted as being floating point, integer, boolean, or any of the other types we’ve 
 been looking at.
  
 Part of what makes ndarray powerful is that every array object is a 
 strided
  view on 
 a block of data. You might wonder, for example, how the array view 
 arr[::2, ::-1]
  does 
 not copy any data. Simply put, the ndarray is more than just a chunk of memory and 
 a dtype; it also has striding information which enables the array to move through 
 memory with varying step sizes. More precisely, the ndarray internally consists of 
 the following:
  
 • A 
 pointer to data
 , that is a block of system memory
  
 • The 
 data type
  or dtype
  
 • A tuple indicating the array’s 
 shape
 ; For example, a 10 by 5 array would have 
 shape 
  
 (10, 5)
  
 In [8]: np.ones((10, 5)).shape 
  
 Out[8]: (10, 5)
  
 • A tuple of 
 strides
 , integers indicating the number of bytes to “step” in order to 
 advance one element along a dimension; For example, a typical (C order, more 
 on this later) 3 x 4 x 5 array of 
 float64
  (8-byte) values has strides 
 (160, 40, 8)
  
 In [9]: np.ones((3, 4, 5), dtype=np.float64).strides Out[9]: 
 (160, 40, 8)
  
 While it is rare that a typical NumPy user would be interested in the array 
 strides, they are the critical ingredient in constructing copyless array views. 
 Strides can even be negative which enables an array to move 
 backward
  through 
 memory, which would be the case in a slice like 
 obj[::-1]
  or 
 obj[:, ::-1]
 .
  
 353",NA
NumPy dtype Hierarchy,"You may occasionally have code which needs to check whether an array contains 
 in-tegers, floating point numbers, strings, or Python objects. Because there are 
 many types of floating point numbers (
 float16
  through 
 float128
 ), checking that the 
 dtype is among a list of types would be very verbose. Fortunately, the dtypes have 
 superclasses such as 
 np.integer
  and 
 np.floating
  which can be used in conjunction 
 with the 
 np.issubd type
  function:
  
 In [10]: ints = np.ones(10, dtype=np.uint16)
  
 In [11]: floats = np.ones(10, dtype=np.float32)
  
 In [12]: np.issubdtype(ints.dtype, np.integer) Out[12]: 
 True
  
 In [13]: np.issubdtype(floats.dtype, np.floating) Out[13]: 
 True
  
 You can see all of the parent classes of a specific dtype by calling the type’s 
 mro
  
 method:
  
 In [14]: np.float64.mro() 
  
 Out[14]: 
  
 [numpy.float64,
  
  numpy.floating,
  
  numpy.inexact,
  
  numpy.number,
  
  numpy.generic,
  
  float,
  
  object]
  
 Most NumPy users will never have to know about this, but it occasionally comes in 
 handy. See 
 Figure 12-2
  for a graph of the dtype hierarchy and parent-subclass 
 relationships 
 1
 .
  
 1. Some of the dtypes have trailing underscores in their names. These are there to avoid variable name 
 conflicts between the NumPy-specific types and the Python built-in ones.
  
 354 | Chapter 12:Advanced NumPy",NA
Advanced Array Manipulation,"There are many ways to work with arrays beyond fancy indexing, slicing, and 
 boolean subsetting. While much of the heavy lifting for data analysis applications is 
 handled by higher level functions in pandas, you may at some point need to write a 
 data algorithm that is not found in one of the existing libraries.",NA
Reshaping Arrays,"Given what we know about NumPy arrays, it should come as little surprise that you 
 can convert an array from one shape to another without copying any data. To do 
 this, pass a tuple indicating the new shape to the 
 reshape
  array instance method. 
 For exam-ple, suppose we had a one-dimensional array of values that we wished to 
 rearrange into a matrix:
  
 In [15]: arr = np.arange(8)
  
 In [16]: arr 
  
 Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7])
  
 In [17]: arr.reshape((4, 2)) 
  
 Out[17]: 
  
 array([[0, 1],
  
  
  [2, 3],
  
  
  [4, 5],
  
  
  [6, 7]])
  
 A multidimensional array can also be reshaped:
  
 In [18]: arr.reshape((4, 2)).reshape((2, 4)) 
  
 Out[18]: 
  
 Advanced Array Manipulation | 355
  
 www.it-ebooks.info",NA
C versus Fortran Order,"Contrary to some other scientific computing environments like R and MATLAB, 
 NumPy gives you much more control and flexibility over the layout of your data in
  
 356 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
Concatenating and Splitting Arrays,"numpy.concatenate
  takes a sequence (tuple, list, etc.) of arrays and joins them 
 together in order along the input axis.
  
 In [32]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])
  
 In [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])
  
 In [34]: np.concatenate([arr1, arr2], axis=0) 
  
 Out[34]: 
  
 array([[ 1,  2,  3],
  
  
  [ 4,  5,  6],
  
 Advanced Array Manipulation | 357
  
 www.it-ebooks.info",NA
Repeating Elements: Tile and Repeat,"The need to replicate or repeat arrays is less common with NumPy 
 than it is with other popular array programming languages like 
 MATLAB. The main reason for this is that 
 broadcasting
  fulfills this 
 need better, which is the subject of the next section.
  
 The two main tools for repeating or replicating arrays to produce larger arrays are 
 the 
 repeat
  and 
 tile
  functions. 
 repeat
  replicates each element in an array some 
 number of times, producing a larger array:
  
 In [51]: arr = np.arange(3)
  
 In [52]: arr.repeat(3) 
  
 Out[52]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])
  
 By default, if you pass an integer, each element will be repeated that number of 
 times. If you pass an array of integers, each element can be repeated a different 
 number of times:
  
 In [53]: arr.repeat([2, 3, 4]) 
  
 Out[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])
  
 Multidimensional arrays can have their elements repeated along a particular axis.
  
 In [54]: arr = randn(2, 2)
  
 In [55]: arr                       In [56]: arr.repeat(2, axis=0) Out[55]:                           
 Out[56]: 
  
 array([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387], 
  
  [ 0.3626,  0.849 ]])               
 [ 0.7157, -0.6387],    
  
  
  [ 0.3626,  0.849 ],    
  
  
  [ 0.3626,  0.849 ]])
  
 Note that if no axis is passed, the array will be flattened first, which is likely not 
 what you want. Similarly you can pass an array of integers when repeating a 
 multidimen-sional array to repeat a given slice a different number of times:
  
 In [57]: arr.repeat([2, 3], axis=0) 
  
 Out[57]: 
  
 array([[ 0.7157, -0.6387],
  
  
  [ 0.7157, -0.6387],
  
  
  [ 0.3626,  0.849 ],
  
  
  [ 0.3626,  0.849 ],
  
  
  [ 0.3626,  0.849 ]])
  
 In [58]: arr.repeat([2, 3], axis=1) 
  
 Out[58]: 
  
 array([[ 0.7157,  0.7157, -0.6387, -0.6387, -0.6387],
  
  [ 
 0.3626,  0.3626,  0.849 ,  0.849 ,  0.849 ]])
  
 360 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
Fancy Indexing Equivalents: Take and Put,"As you may recall from 
 Chapter 4
 , one way to get and set subsets of arrays is by 
 fancy
  indexing using integer arrays:
  
 In [64]: arr = np.arange(10) * 100
  
 In [65]: inds = [7, 1, 2, 6]        In [66]: arr[inds] 
  
  
  Out[66]: array([700, 100, 200, 600])
  
 There are alternate ndarray methods that are useful in the special case of only 
 making a selection on a single axis:
  
 In [67]: arr.take(inds) 
  
 Out[67]: array([700, 100, 200, 600])
  
 In [68]: arr.put(inds, 42)
  
 In [69]: arr 
  
 Out[69]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])
  
 In [70]: arr.put(inds, [40, 41, 42, 43])
  
 Advanced Array Manipulation | 361
  
 www.it-ebooks.info",NA
Broadcasting,"Broadcasting
  describes how arithmetic works between arrays of different shapes. It 
 is a very powerful feature, but one that can be easily misunderstood, even by 
 experienced users. The simplest example of broadcasting occurs when combining a 
 scalar value with an array:
  
 In [80]: arr = np.arange(5)
  
 In [81]: arr                           In [82]: arr * 4 
  
 Out[81]: array([0, 1, 2, 3, 4])        Out[82]: array([ 0,  4,  8, 12, 16])
  
 362 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
The Broadcasting Ru,"Two arrays are compatible for broadcasting if for each 
 trailing dimension
  (that is, 
 start-ing from the end), the axis lengths match or if either of the lengths is 1. 
 Broadcasting is then performed over the missing and / or length 1 dimensions.
  
 Even as an experienced NumPy user, I often must stop to draw pictures and think 
 about the broadcasting rule. Consider the last example and suppose we wished 
 instead to subtract the mean value from each row. Since 
 arr.mean(0)
  has length 3, it 
 is compatible
  
 Broadcasting | 363",NA
Broadcasting Over Other Axes,"Broadcasting with higher dimensional arrays can seem even more mind-bending, 
 but it is really a matter of following the rules. If you don’t, you’ll get an error like 
 this:
  
 In [93]: arr - arr.mean(1)
  
 ---------------------------------------------------------------------------ValueError                                
 Traceback (most recent call last) <ipython-input-93-7b87b85a20b2> in <module>()
  
 364 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
Setting Array Values by Broadcasting,"The same broadcasting rule governing arithmetic operations also applies to setting 
 values via array indexing. In the simplest case, we can do things like:
  
 In [106]: arr = np.zeros((4, 3))
  
 In [107]: arr[:] = 5        In [108]: arr 
  
  
  Out[108]:               
  
  
 array([[ 5.,  5.,  5.], 
  
  
  [ 5.,  5.,  5.], 
  
  
  [ 5.,  5.,  5.], 
  
  
  [ 5.,  
 5.,  5.]])
  
 However, if we had a one-dimensional array of values we wanted to set into the 
 columns of the array, we can do that as long as the shape is compatible:
  
 In [109]: col = np.array([1.28, -0.42, 0.44, 1.6])
  
 In [110]: arr[:] = col[:, np.newaxis]       In [111]: arr 
  
  Out[111]:                      array([[ 1.28,  1.28,  1.28], 
  
  [-0.42, -0.42, -0.42], 
  
  [ 0.44,  
 0.44,  0.44], 
  
  [ 1.6 ,  1.6 ,  1.6 ]])
  
 In [112]: arr[:2] = [[-1.37], [0.509]]      In [113]: arr 
  
  Out[113]:                         array([[-1.37 , -1.37 , -1.37 ], 
  
  [ 0.509,  0.509,  0.509], 
  
  [ 
 0.44 ,  0.44 ,  0.44 ], 
  
  [ 1.6  ,  1.6  ,  1.6  ]])",NA
Advanced ufunc Usage,"While many NumPy users will only make use of the fast element-wise operations 
 pro-vided by the universal functions, there are a number of additional features that 
 occa-sionally can help you write more concise code without loops.
  
 Advanced ufunc Usage | 367
  
 www.it-ebooks.info",NA
ufunc Instance Methods,"Each of NumPy’s binary ufuncs has special methods for performing certain kinds of 
 special vectorized operations. These are summarized in 
 Table 12-2
 , but I’ll give a 
 few concrete examples to illustrate how they work.
  
 reduce
  takes a single array and aggregates its values, optionally along an axis, by 
 per-forming a sequence of binary operations. For example, an alternate way to sum 
 ele-ments in an array is to use 
 np.add.reduce
 :
  
 In [114]: arr = np.arange(10)
  
 In [115]: np.add.reduce(arr) 
  
 Out[115]: 45
  
 In [116]: arr.sum() 
  
 Out[116]: 45
  
 The starting value (0 for 
 add
 ) depends on the ufunc. If an axis is passed, the 
 reduction is performed along that axis. This allows you to answer certain kinds of 
 questions in a concise way. As a less trivial example, we can use 
 np.logical_and
  to 
 check whether the values in each row of an array are sorted:
  
 In [118]: arr = randn(5, 5)
  
 In [119]: arr[::2].sort(1) # sort a few rows
  
 In [120]: arr[:, :-1] < arr[:, 1:] 
  
 Out[120]: 
  
 array([[ True,  True,  True,  True],
  
  
  [False,  True, False, False],
  
  
  [ True,  True,  True,  True],
  
  
  [ True, False,  True,  True],
  
  
  [ True,  True,  True,  True]], dtype=bool)
  
 In [121]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1) Out[121]: array([ 
 True, False,  True, False,  True], dtype=bool)
  
 Of course, 
 logical_and.reduce
  is equivalent to the 
 all
  method.
  
 accumulate
  is related to 
 reduce
  like 
 cumsum
  is related to 
 sum
 . It produces an array of 
 the same size with the intermediate “accumulated” values:
  
 In [122]: arr = np.arange(15).reshape((3, 5))
  
 In [123]: np.add.accumulate(arr, axis=1) 
  
 Out[123]: 
  
 array([[ 0,  1,  3,  6, 10],
  
  
  [ 5, 11, 18, 26, 35],
  
  
  [10, 21, 33, 46, 60]])
  
 outer
  performs a pairwise cross-product between two arrays:
  
 In [124]: arr = np.arange(3).repeat([1, 2, 2])
  
 368 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
Custom ufuncs,"There are a couple facilities for creating your own functions with ufunc-like 
 semantics. 
 numpy.frompyfunc
  accepts a Python function along with a specification 
 for the number of inputs and outputs. For example, a simple function that adds 
 element-wise would be specified as:
  
 In [134]: def add_elements(x, y):
  
  
  .....:     return x + y
  
 In [135]: add_them = np.frompyfunc(add_elements, 2, 1)
  
 In [136]: add_them(np.arange(8), np.arange(8)) 
  
 Out[136]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)
  
 Functions created using 
 frompyfunc
  always return arrays of Python objects which 
 isn’t very convenient. Fortunately, there is an alternate, but slightly less featureful 
 function 
 numpy.vectorize
  that is a bit more intelligent about type inference:
  
 In [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])
  
 In [138]: add_them(np.arange(8), np.arange(8)) 
  
 Out[138]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])
  
 These functions provide a way to create ufunc-like functions, but they are very slow 
 because they require a Python function call to compute each element, which is a lot 
 slower than NumPy’s C-based ufunc loops:
  
 In [139]: arr = randn(10000)
  
 In [140]: %timeit add_them(arr, arr) 
  
 100 loops, best of 3: 2.12 ms per loop
  
 In [141]: %timeit np.add(arr, arr) 
  
 100000 loops, best of 3: 11.6 us per loop
  
 There are a number of projects under way in the scientific Python community to 
 make it easier to define new ufuncs whose performance is closer to that of the built-
 in ones.",NA
Structured and Record Arrays,"You may have noticed up until now that ndarray is a 
 homogeneous
  data container; 
 that is, it represents a block of memory in which each element takes up the same 
 number of bytes, determined by the dtype. On the surface, this would appear to not 
 allow you to represent heterogeneous or tabular-like data. A 
 structured
  array is an 
 ndarray in which each element can be thought of as representing a 
 struct
  in C 
 (hence the “struc-tured” name) or a row in a SQL table with multiple named fields:
  
 In [142]: dtype = [('x', np.float64), ('y', np.int32)]
  
 In [143]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)
  
 370 | Chapter 12:Advanced NumPy",NA
Nested dtypes and Multidimensional Fields,"When specifying a structured dtype, you can additionally pass a shape (as an int or 
 tuple):
  
 In [148]: dtype = [('x', np.int64, 3), ('y', np.int32)]
  
 In [149]: arr = np.zeros(4, dtype=dtype)
  
 In [150]: arr 
  
 Out[150]: 
  
 array([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], 
  
  dtype=[('x', 
 '<i8', (3,)), ('y', '<i4')])
  
 In this case, the 
 x
  field now refers to an array of length three for each record:
  
 In [151]: arr[0]['x'] 
  
 Out[151]: array([0, 0, 0])
  
 Conveniently, accessing 
 arr['x']
  then returns a two-dimensional array instead of a 
 one-dimensional array as in prior examples:
  
 In [152]: arr['x'] 
  
 Out[152]: 
  
 array([[0, 0, 0],
  
  
  [0, 0, 0],
  
  
  [0, 0, 0],
  
  
  [0, 0, 0]])
  
 This enables you to express more complicated, nested structures as a single block 
 of memory in an array. Though, since dtypes can be arbitrarily complex, why not 
 nested dtypes? Here is a simple example:
  
 Structured and Record Arrays | 371
  
 www.it-ebooks.info",NA
Why Use Structured Arrays?,"Compared with, say, a DataFrame from pandas, NumPy structured arrays are a 
 com-paratively low-level tool. They provide a means to interpreting a block of 
 memory as a tabular structure with arbitrarily complex nested columns. Since each 
 element in the array is represented in memory as a fixed number of bytes, 
 structured arrays provide a very fast and efficient way of writing data to and from 
 disk (including memory maps, more on this later), transporting it over the network, 
 and other such use.
  
 As another common use for structured arrays, writing data files as fixed length 
 record byte streams is a common way to serialize data in C and C++ code, which is 
 commonly found in legacy systems in industry. As long as the format of the file is 
 known (the size of each record and the order, byte size, and data type of each 
 element), the data can be read into memory using 
 np.fromfile
 . Specialized uses like 
 this are beyond the scope of this book, but it’s worth knowing that such things are 
 possible.",NA
Structured Array Manipulations: ,NA,NA
numpy.lib.recfunctions,"While there is not as much functionality available for structured arrays as for Data-
 Frames, the NumPy module 
 numpy.lib.recfunctions
  has some helpful tools for adding 
 and dropping fields or doing basic join-like operations. The thing to remember with 
 these tools is that it is typically necessary to create a new array to make any 
 modifica-tions to the dtype (like adding or dropping a column). These functions are 
 left to the interested reader to explore as I do not use them anywhere in this book.",NA
More About Sorting,"Like Python’s built-in list, the ndarray 
 sort
  instance method is an 
 in-place
  sort, 
 meaning that the array contents are rearranged without producing a new array:
  
 In [158]: arr = randn(6)
  
 In [159]: arr.sort()
  
 In [160]: arr 
  
 Out[160]: array([-1.082 ,  0.3759,  0.8014,  1.1397,  1.2888,  1.8413])
  
 When sorting arrays in-place, remember that if the array is a view on a different 
 ndarray, the original array will be modified:
  
 In [161]: arr = randn(3, 5)
  
 In [162]: arr 
  
 Out[162]: 
  
 array([[-0.3318, -1.4711,  0.8705, -0.0847, -1.1329],
  
  [-
 1.0111, -0.3436,  2.1714,  0.1234, -0.0189],
  
  [ 0.1773,  
 0.7424,  0.8548,  1.038 , -0.329 ]])
  
 In [163]: arr[:, 0].sort()  # Sort first column values in-place
  
 In [164]: arr 
  
 Out[164]: 
  
 array([[-1.0111, -1.4711,  0.8705, -0.0847, -1.1329],
  
  [-
 0.3318, -0.3436,  2.1714,  0.1234, -0.0189],
  
  [ 0.1773,  
 0.7424,  0.8548,  1.038 , -0.329 ]])
  
 On the other hand, 
 numpy.sort
  creates a new, sorted copy of an array. Otherwise it 
 accepts the same arguments (such as 
 kind
 , more on this below) as 
 ndarray.sort
 :
  
 In [165]: arr = randn(5)
  
 In [166]: arr 
  
 Out[166]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])
  
 In [167]: np.sort(arr) 
  
 Out[167]: array([-2.0051, -1.1181, -1.0614, -0.2415,  0.7379])
  
 In [168]: arr 
  
 Out[168]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])
  
 All of these sort methods take an axis argument for sorting the sections of data 
 along the passed axis independently:
  
 In [169]: arr = randn(3, 5)
  
 In [170]: arr 
  
 Out[170]: 
  
 array([[ 0.5955, -0.2682,  1.3389, -0.1872,  0.9111],
  
  [-
 0.3215,  1.0054, -0.5168,  1.1925, -0.1989],
  
  [ 0.3969, -
 1.7638,  0.6071, -0.2222, -0.2171]])
  
 More About Sorting | 373
  
 www.it-ebooks.info",NA
Indirect Sorts: argsort and lexsort,"In data analysis it’s very common to need to reorder data sets by one or more keys. 
 For example, a table of data about some students might need to be sorted by last 
 name then by first name. This is an example of an 
 indirect
  sort, and if you’ve read 
 the pandas-related chapters you have already seen many higher-level examples. 
 Given a key or keys (an array or values or multiple arrays of values), you wish to 
 obtain an array of integer 
 indices
  (I refer to them colloquially as 
 indexers
 ) that tells 
 you how to reorder the data to be in sorted order. The two main methods for this 
 are 
 argsort
  and 
 numpy.lexsort
 .
  
 As a trivial example:
  
 In [174]: values = np.array([5, 0, 1, 3, 2])
  
 In [175]: indexer = values.argsort()
  
 In [176]: indexer 
  
 Out[176]: array([1, 2, 4, 3, 0])
  
 In [177]: values[indexer] 
  
 Out[177]: array([0, 1, 2, 3, 5])
  
 As a less trivial example, this code reorders a 2D array by its first row:
  
 In [178]: arr = randn(3, 5)
  
 In [179]: arr[0] = values
  
 In [180]: arr 
  
 Out[180]: 
  
 array([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],
  
  [-0.3636, -0.1378,  2.1777, 
 -0.4728,  0.8356],
  
  [-0.2089,  0.2316,  0.728 , -1.3918,  
 1.9956]])
  
 374 | Chapter 12:Advanced NumPy",NA
Alternate Sort Algorithms,"A 
 stable
  sorting algorithm preserves the relative position of equal elements. This 
 can be especially important in indirect sorts where the relative ordering is 
 meaningful:
  
 In [186]: values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])
  
 In [187]: key = np.array([2, 2, 1, 1, 1])
  
 In [188]: indexer = key.argsort(kind='mergesort')
  
 In [189]: indexer 
  
 Out[189]: array([2, 3, 4, 0, 1])
  
 In [190]: values.take(indexer) 
  
 Out[190]: 
  
 array(['1:first', '1:second', '1:third', '2:first', '2:second'], 
  
  dtype='|S8')
  
 The only stable sort available is 
 mergesort
  which has guaranteed O(n log n) 
 performance (for complexity buffs), but its performance is on average worse than 
 the default
  
 More About Sorting | 375",NA
numpy.searchsorted: Finding elements in a Sorted ,NA,NA
Array ,"searchsorted
  is an array method that performs a binary search on a sorted array, re-
 turning the location in the array where the value would need to be inserted to 
 maintain sortedness: 
  
  
 In [191]: arr = np.array([0, 1, 7, 12, 15]) 
  
  
 In [192]: arr.searchsorted(9) 
  
  
 Out[192]: 3 
  
 As you might expect, you can also pass an array of values to get an array of indices 
 back: 
  
 In [193]: arr.searchsorted([0, 8, 11, 16]) 
  
  
 Out[193]: array([0, 3, 3, 5]) 
  
 You might have noticed that 
 searchsorted
  returned 
 0
  for the 
 0
  element. This is 
 because the default behavior is to return the index at the left side of a group of 
 equal values: 
  
 In [194]: arr = np.array([0, 0, 0, 1, 1, 1, 1]) 
  
  
 In [195]: arr.searchsorted([0, 1]) 
  
  
 Out[195]: array([0, 3]) 
  
  
 In [196]: arr.searchsorted([0, 1], side='right') 
  
  
 Out[196]: array([3, 7]) 
  
 As another application of 
 searchsorted
 , suppose we had an array of values between 0 
 and 10,000) and a separate array of “bucket edges” that we wanted to use to bin the 
 data: 
  
  
 In [197]: data = np.floor(np.random.uniform(0, 10000, size=50)) 
  
  
 In [198]: bins = np.array([0, 100, 1000, 5000, 10000]) 
  
  
 In [199]: data
  
 376 | Chapter 12:Advanced NumPy",NA
NumPy Matrix Class,"Compared with other languages for matrix operations and linear algebra, like MAT-
 LAB, Julia, and GAUSS, NumPy’s linear algebra syntax can often be quite verbose. 
 One reason is that matrix multiplication requires using 
 numpy.dot
 . Also NumPy’s 
 indexing semantics are different, which makes porting code to Python less 
 straightforward at times. Selecting a single row (e.g. 
 X[1, :]
 ) or column (e.g. 
 X[:, 1]
 ) 
 from a 2D array yields a 1D array compared with a 2D array as in, say, MATLAB.
  
 In [204]: X =  np.array([[ 8.82768214,  3.82222409, -1.14276475,  2.04411587], .....:                [ 
 3.82222409,  6.75272284,  0.83909108,  2.08293758], .....:                [-1.14276475,  
 0.83909108,  5.01690521,  0.79573241], .....:                [ 2.04411587,  2.08293758,  
 0.79573241,  6.24095859]])
  
 In [205]: X[:, 0]  # one-dimensional 
  
 Out[205]: array([ 8.8277,  3.8222, -1.1428,  2.0441])
  
 In [206]: y = X[:, :1]  # two-dimensional by slicing
  
 NumPy Matrix Class | 377
  
 www.it-ebooks.info",NA
Advanced Array Input and Output,"In 
 Chapter 4
 , I introduced you to 
 np.save
  and 
 np.load
  for storing arrays in binary 
 format on disk. There are a number of additional options to consider for more 
 sophisticated use. In particular, memory maps have the additional benefit of 
 enabling you to work with data sets that do not fit into RAM.",NA
Memory-mapped Files,"A 
 memory-mapped
  file is a method for treating potentially very large binary data on 
 disk as an in-memory array. NumPy implements a 
 memmap
  object that is ndarray-
 like, enabling small segments of a large file to be read and written without reading 
 the whole array into memory. Additionally, a 
 memmap
  has the same methods as an 
 in-memory array and thus can be substituted into many algorithms where an 
 ndarray would be expected.
  
 To create a new 
 memmap
 , use the function 
 np.memmap
  and pass a file path, dtype, 
 shape, and file mode:
  
 In [216]: mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))
  
 In [217]: mmap 
  
 Out[217]: 
  
 memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
  
  
  [ 0.,  0.,  0., ...,  0.,  0.,  0.],
  
  
  [ 0.,  0.,  0., ...,  0.,  0.,  0.],
  
  
  ..., 
  
  
  [ 0.,  0.,  0., ...,  0.,  0.,  0.],
  
  
  [ 0.,  0.,  0., ...,  0.,  0.,  0.],
  
  
  [ 0.,  0.,  0., ...,  0.,  0.,  0.]])
  
 Slicing a 
 memmap
  returns views on the data on disk:
  
 In [218]: section = mmap[:5]
  
 If you assign data to these, it will be buffered in memory (like a Python file object), 
 but can be written to disk by calling 
 flush
 :
  
 In [219]: section[:] = np.random.randn(5, 10000)
  
 In [220]: mmap.flush()
  
 In [221]: mmap 
  
 Out[221]: 
  
 memmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],
  
  [ 0.4898, -
 2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],
  
  [-0.6875,  1.6901, -0.7444, ..., -
 1.4218, -0.0509,  1.2224],
  
 Advanced Array Input and Output | 379",NA
HDF5 and Other Array Storage Options,"PyTables and h5py are two Python projects providing NumPy-friendly interfaces 
 for storing array data in the efficient and compressible HDF5 format (HDF stands 
 for 
 hierarchical data format
 ). You can safely store hundreds of gigabytes or even 
 terabytes of data in HDF5 format. The use of these libraries is unfortunately outside 
 the scope of the book.
  
 PyTables provides a rich facility for working with structured arrays with advanced 
 querying features and the ability to add column indexes to accelerate queries. This 
 is very similar to the table indexing capabilities provided by relational databases.",NA
Performance Tips,"Getting good performance out of code utilizing NumPy is often straightforward, as 
 array operations typically replace otherwise comparatively extremely slow pure 
 Python loops. Here is a brief list of some of the things to keep in mind:
  
 • Convert Python loops and conditional logic to array operations and boolean 
 array 
  
 operations
  
 • Use broadcasting whenever possible
  
 • Avoid copying data using array views (slicing)
  
 • Utilize ufuncs and ufunc methods
  
 380 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
The Importance of Contiguous Memory,"While the full extent of this topic is a bit outside the scope of this book, in some ap-
 plications the memory layout of an array can significantly affect the speed of 
 compu-tations. This is based partly on performance differences having to do with 
 the cache hierarchy of the CPU; operations accessing contiguous blocks of memory 
 (for example, summing the rows of a C order array) will generally be the fastest 
 because the memory subsystem will buffer the appropriate blocks of memory into 
 the ultrafast L1 or L2 CPU cache. Also, certain code paths inside NumPy’s C 
 codebase have been optimized for the contiguous case in which generic strided 
 memory access can be avoided.
  
 To say that an array’s memory layout is 
 contiguous
  means that the elements are 
 stored in memory in the order that they appear in the array with respect to Fortran 
 (column major) or C (row major) ordering. By default, NumPy arrays are created as 
 C-contigu-ous
  or just simply contiguous. A column major array, such as the 
 transpose of a C-contiguous array, is thus said to be Fortran-contiguous. These 
 properties can be ex-plicitly checked via the 
 flags
  attribute on the 
 ndarray
 :
  
 In [227]: arr_c = np.ones((1000, 1000), order='C')
  
 In [228]: arr_f = np.ones((1000, 1000), order='F')
  
 In [229]: arr_c.flags         In [230]: arr_f.flags Out[229]:                     
 Out[230]: 
  
  C_CONTIGUOUS : True           C_CONTIGUOUS : False 
 F_CONTIGUOUS : False          F_CONTIGUOUS : True  OWNDATA 
 : True                OWNDATA : True       WRITEABLE : True              
 WRITEABLE : True     ALIGNED : True                ALIGNED : True       
 UPDATEIFCOPY : False          UPDATEIFCOPY : False
  
 In [231]: arr_f.flags.f_contiguous 
  
 Out[231]: True
  
 In this example, summing the rows of these arrays should, in theory, be faster for 
 arr_c
  than 
 arr_f
  since the rows are contiguous in memory. Here I check for sure 
 using 
 %timeit
  in IPython:
  
 In [232]: %timeit arr_c.sum(1) 
  
 1000 loops, best of 3: 1.33 ms per loop
  
 In [233]: %timeit arr_f.sum(1) 
  
 100 loops, best of 3: 8.75 ms per loop
  
 Performance Tips | 381",NA
"Other Speed Options: Cython, f2py, C","In recent years, the Cython project ((
 http://cython.org
 ) has become the tool of 
 choice for many scientific Python programmers for implementing fast code that 
 may need to interact with C or C++ libraries, but without having to write pure C 
 code. You can think of Cython as Python with static types and the ability to 
 interleave functions im-plemented in C into Python-like code. For example, a simple 
 Cython function to sum the elements of a one-dimensional array might look like:
  
 from numpy cimport ndarray, float64_t
  
 def sum_elements(ndarray[float64_t] arr):
  
  cdef Py_ssize_t i, n = len(arr)
  
  cdef float64_t result = 0
  
  for i in range(n):
  
  
  result += arr[i]
  
  return result
  
 Cython takes this code, translates it to C, then compiles the generated C code to 
 create a Python extension. Cython is an attractive option for performance 
 computing because the code is only slightly more time-consuming to write than 
 pure Python code and it integrates closely with NumPy. A common workflow is to 
 get an algorithm working in Python, then translate it to Cython by adding type 
 declarations and a handful of other tweaks. For more, see the project 
 documentation.
  
 382 | Chapter 12:Advanced NumPy
  
 www.it-ebooks.info",NA
APPENDIX,NA,NA
Python Language Essentials,"Knowledge is a treasure, but practice is the key to it.
  
 —Thomas Fuller
  
 People often ask me about good resources for learning Python for data-centric 
 appli-cations. While there are many excellent Python language books, I am usually 
 hesitant to recommend some of them as they are intended for a general audience 
 rather than tailored for someone who wants to load in some data sets, do some 
 computations, and plot some of the results. There are actually a couple of books on 
 “scientific program-ming in Python”, but they are geared toward numerical 
 computing and engineering applications: solving differential equations, computing 
 integrals, doing Monte Carlo simulations, and various topics that are more 
 mathematically-oriented rather than be-ing about data analysis and statistics. As 
 this is a book about becoming proficient at working with data in Python, I think it is 
 valuable to spend some time highlighting the most important features of Python’s 
 built-in data structures and libraries from the per-spective of processing and 
 manipulating structured and unstructured data. As such, I will only present roughly 
 enough information to enable you to follow along with the rest of the book.
  
 This chapter is not intended to be an exhaustive introduction to the Python 
 language but rather a biased, no-frills overview of features which are used 
 repeatedly throughout this book. For new Python programmers, I recommend that 
 you 
 supplement 
 this 
 chap-ter 
 with 
 the 
 official 
 Python 
 tutorial 
 (
 http://docs.python.org
 ) and potentially one of the many excellent (and much 
 longer) books on general purpose Python programming. In my opinion, it is 
 not
  
 necessary to become proficient at building good software in Python to be able to 
 productively do data analysis. I encourage you to use IPython to experi-ment with 
 the code examples and to explore the documentation for the various types, 
 functions, and methods. Note that some of the code used in the examples may not 
 necessarily be fully-introduced at this point.
  
 Much of this book focuses on high performance array-based computing tools for 
 work-ing with large data sets. In order to use those tools you must often first do 
 some munging to corral messy data into a more nicely structured form. 
 Fortunately, Python is one of
  
 385",NA
The Python Interpreter,"Python is an 
 interpreted
  language. The Python interpreter runs a program by 
 executing one statement at a time. The standard interactive Python interpreter can 
 be invoked on the command line with the 
 python
  command:
  
 $ python 
  
 Python 2.7.2 (default, Oct  4 2011, 20:06:09) 
  
 [GCC 4.6.1] on linux2 
  
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
  
 >>> a = 5 
  
 >>> print a 
  
 5
  
 The 
 >>>
  you see is the 
 prompt
  where you’ll type expressions. To exit the Python 
 inter-preter and return to the command prompt, you can either type 
 exit()
  or press 
 Ctrl-D
 .
  
 Running Python programs is as simple as calling 
 python
  with a 
 .py
  file as its first 
 argu-ment. Suppose we had created 
 hello_world.py
  with these contents:
  
 print 'Hello world'
  
 This can be run from the terminal simply as:
  
 $ python hello_world.py 
  
 Hello world
  
 While many Python programmers execute all of their Python code in this way, 
 many 
 scientific
  Python programmers make use of IPython, an enhanced interactive 
 Python interpreter. 
 Chapter 3
  is dedicated to the IPython system. By using the 
 %run
  
 command, IPython executes the code in the specified file in the same process, 
 enabling you to explore the results interactively when it’s done.
  
 $ ipython 
  
 Python 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51) Type 
 ""copyright"", ""credits"" or ""license"" for more information.
  
 IPython 0.12 -- An enhanced Interactive Python.
  
 ?         -> Introduction and overview of IPython's features.
  
 %quickref -> Quick reference.
  
 help      -> Python's own help system.
  
 object?   -> Details about 'object', use 'object??' for extra details.
  
 In [1]: %run hello_world.py 
  
 Hello world
  
 In [2]:
  
 386 | Appendix:Python Language Essentials",NA
The Basics,NA,NA
Language Semantics,"The Python language design is distinguished by its emphasis on readability, 
 simplicity, and explicitness. Some people go so far as to liken it to “executable 
 pseudocode”.
  
 Indentation, not braces
  
 Python uses whitespace (tabs or spaces) to structure code instead of using braces 
 as in many other languages like R, C++, Java, and Perl. Take the for loop in the above 
 quicksort algorithm:
  
 for x in array:
  
  
  if x < pivot:
  
  
  
  less.append(x)
  
  
  else:
  
  
  
  greater.append(x)
  
 A colon denotes the start of an indented code block after which all of the code must 
 be indented by the same amount until the end of the block. In another language, 
 you might instead have something like:
  
 for x in array {
  
  
  
  if x < pivot {
  
  
  
  
  less.append(x)
  
  
  
  } else {
  
  
  
  
  greater.append(x)
  
  
  
  }
  
  
  }
  
 One major reason that whitespace matters is that it results in most Python code 
 looking cosmetically similar, which means less cognitive dissonance when you read 
 a piece of code that you didn’t write yourself (or wrote in a hurry a year ago!). In a 
 language without significant whitespace, you might stumble on some differently 
 formatted code like:
  
 for x in array
  
  
  {
  
  
  
  if x < pivot
  
  
  
  {
  
  
   
  less.append(x)
  
  
  
  }
  
  
  
  else
  
  
  
  {
  
  
   
  greater.append(x)
  
 The Basics | 387",NA
Scalar Types,"Python has a small set of built-in types for handling numerical data, strings, boolean 
 (
 True
  or 
 False
 ) values, and dates and time. See 
 Table A-2
  for a list of the main scalar 
 types. Date and time handling will be discussed separately as these are provided by 
 the 
 datetime
  module in the standard library.
  
 Table A-2. Standard Python Scalar Types
  
 Type
  
 Description
  
 None 
  
 str 
  
 unicode 
 float 
  
 bool 
  
 int 
  
 long
  
 The Python “null” value (only one instance of the 
 None
  object 
 exists) 
  
 String type. ASCII-valued only in Python 2.x and Unicode in 
 Python 3 
  
 Unicode string type 
  
 Double-precision (64-bit) floating point number. Note there is 
 no separate 
 double
  type.
  
 A 
 True
  or 
 False
  value 
  
 Signed integer with maximum value 
 determined by the platform.
  
 Arbitrary precision signed integer. Large 
 int
  values are 
 automatically converted to 
 long
 .
  
 Numeric types
  
 The primary Python types for numbers are 
 int
  and 
 float
 . The size of the integer 
 which can be stored as an 
 int
  is dependent on your platform (whether 32 or 64-bit), 
 but Python will transparently convert a very large integer to 
 long
 , which can store 
 arbitrarily large integers.
  
 In [279]: ival = 17239871
  
 In [280]: ival ** 6 
  
 Out[280]: 
 26254519291092456596965462913230729701102721L
  
 The Basics | 395",NA
Control Flow,"if, elif, and else
  
 The 
 if
  statement is one of the most well-known control flow statement types. It 
 checks a condition which, if 
 True
 , evaluates the code in the block that follows:
  
 if x < 0:
  
  
  print 'It's negative'
  
 400 | Appendix:Python Language Essentials
  
 www.it-ebooks.info",NA
Data Structures and Sequences,"Python’s data structures are simple, but powerful. Mastering their use is a critical 
 part of becoming a proficient Python programmer.
  
 Data Structures and Sequences | 405
  
 www.it-ebooks.info",NA
Tuple,"A tuple is a one-dimensional, fixed-length, 
 immutable
  sequence of Python objects. 
 The easiest way to create one is with a comma-separated sequence of values:
  
 In [356]: tup = 4, 5, 6
  
 In [357]: tup 
  
 Out[357]: (4, 5, 6)
  
 When defining tuples in more complicated expressions, it’s often necessary to 
 enclose the values in parentheses, as in this example of creating a tuple of tuples:
  
 In [358]: nested_tup = (4, 5, 6), (7, 8)
  
 In [359]: nested_tup 
  
 Out[359]: ((4, 5, 6), (7, 8))
  
 Any sequence or iterator can be converted to a tuple by invoking 
 tuple
 :
  
 In [360]: tuple([4, 0, 2]) 
  
 Out[360]: (4, 0, 2)
  
 In [361]: tup = tuple('string')
  
 In [362]: tup 
  
 Out[362]: ('s', 't', 'r', 'i', 'n', 'g')
  
 Elements can be accessed with square brackets 
 []
  as with most other sequence 
 types. Like C, C++, Java, and many other languages, sequences are 0-indexed in 
 Python:
  
 In [363]: tup[0] 
  
 Out[363]: 's'
  
 While the objects stored in a tuple may be mutable themselves, once created it’s not 
 possible to modify which object is stored in each slot:
  
 In [364]: tup = tuple(['foo', [1, 2], True])
  
 In [365]: tup[2] = False
  
 ---------------------------------------------------------------------------TypeError                                 
 Traceback (most recent call last) <ipython-input-365-c7308343b841> in <module>()
  
 ----> 1 tup[2] = False 
  
 TypeError: 'tuple' object does not support item assignment
  
 # however 
  
 In [366]: tup[1].append(3)
  
 In [367]: tup 
  
 Out[367]: ('foo', [1, 2, 3], True)
  
 Tuples can be concatenated using the 
 +
  operator to produce longer tuples:
  
 In [368]: (4, None, 'foo') + (6, 0) + ('bar',) Out[368]: 
 (4, None, 'foo', 6, 0, 'bar')
  
 406 | Appendix:Python Language Essentials",NA
List,"In contrast with tuples, lists are variable-length and their contents can be modified. 
 They can be defined using square brackets 
 []
  or using the 
 list
  type function:
  
 In [378]: a_list = [2, 3, 7, None]
  
 In [379]: tup = ('foo', 'bar', 'baz')
  
 In [380]: b_list = list(tup)        In [381]: b_list 
  
  
  Out[381]: ['foo', 'bar', 'baz']
  
 In [382]: b_list[1] = 'peekaboo'    In [383]: b_list 
  
  
  Out[383]: ['foo', 'peekaboo', 'baz']
  
 Lists and tuples are semantically similar as one-dimensional sequences of objects 
 and thus can be used interchangeably in many functions.
  
 Adding and removing elements
  
 Elements can be appended to the end of the list with the 
 append
  method:
  
 In [384]: b_list.append('dwarf')
  
 In [385]: b_list 
  
 Out[385]: ['foo', 'peekaboo', 'baz', 'dwarf']
  
 Using 
 insert
  you can insert an element at a specific location in the list:
  
 In [386]: b_list.insert(1, 'red')
  
 In [387]: b_list 
  
 Out[387]: ['foo', 'red', 'peekaboo', 'baz', 'dwarf']
  
  
 insert
  is computationally expensive compared with 
 append
  as 
 references to subsequent elements have to be shifted internally to 
 make room for the new element.
  
 The inverse operation to 
 insert
  is 
 pop
 , which removes and returns an element at a 
 particular index:
  
 In [388]: b_list.pop(2) 
  
 Out[388]: 'peekaboo'
  
 In [389]: b_list 
  
 Out[389]: ['foo', 'red', 'baz', 'dwarf']
  
 Elements can be removed by value using 
 remove
 , which locates the first such value 
 and removes it from the last:
  
 408 | Appendix:Python Language Essentials
  
 www.it-ebooks.info",NA
Built-in Sequence Functions,"Python has a handful of useful sequence functions that you should familiarize 
 yourself with and use at any opportunity.
  
 Data Structures and Sequences | 411",NA
Dict,"dict
  is likely the most important built-in Python data structure. A more common 
 name for it is 
 hash map
  or 
 associative array
 . It is a flexibly-sized collection of 
 key-
 value
  pairs, where 
 key
  and 
 value
  are Python objects. One way to create one is by 
 using curly braces 
 {}
  and using colons to separate keys and values:
  
 In [437]: empty_dict = {}
  
 In [438]: d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4]}
  
 Data Structures and Sequences | 413
  
 www.it-ebooks.info",NA
Set,"A set is an unordered collection of unique elements. You can think of them like 
 dicts, but keys only, no values. A set can be created in two ways: via the 
 set
  function 
 or using a 
 set literal
  with curly braces:
  
 In [465]: set([2, 2, 2, 1, 3, 3]) 
  
 Out[465]: set([1, 2, 3])
  
 416 | Appendix:Python Language Essentials",NA
"List, Set, and Dict Comprehensions","List comprehensions
  are one of the most-loved Python language features. They 
 allow you to concisely form a new list by filtering the elements of a collection and 
 transforming the elements passing the filter in one conscise expression. They take 
 the basic form:
  
 [
 expr
  for val in collection if 
 condition
 ]
  
 This is equivalent to the following 
 for
  loop:
  
 result = [] 
  
 for val in collection:
  
  
  if 
 condition
 :
  
  
  
  result.append(
 expr
 )
  
 The filter condition can be omitted, leaving only the expression. For example, given 
 a list of strings, we could filter out strings with length 2 or less and also convert 
 them to uppercase like this:
  
 In [477]: strings = ['a', 'as', 'bat', 'car', 'dove', 'python']
  
 In [478]: [x.upper() for x in strings if len(x) > 2] Out[478]: 
 ['BAT', 'CAR', 'DOVE', 'PYTHON']
  
 Set and dict comprehensions are a natural extension, producing sets and dicts in a 
 idiomatically similar way instead of lists. A dict comprehension looks like this:
  
 dict_comp = {
 key-expr
  : 
 value-expr
  for value in collection
  
  if 
 condition
 }
  
 A set comprehension looks like the equivalent list comprehension except with curly 
 braces instead of square brackets:
  
 set_comp = {
 expr
  for value in collection if 
 condition
 }
  
 Like list comprehensions, set and dict comprehensions are just syntactic sugar, but 
 they similarly can make code both easier to write and read. Consider the list of 
 strings above. Suppose we wanted a set containing just the lengths of the strings 
 contained in the collection; this could be easily computed using a set 
 comprehension:
  
 In [479]: unique_lengths = {len(x) for x in strings}
  
 In [480]: unique_lengths 
  
 Out[480]: set([1, 2, 3, 4, 6])
  
 As a simple dict comprehension example, we could create a lookup map of these 
 strings to their locations in the list:
  
 In [481]: loc_mapping = {val : index for index, val in enumerate(strings)}
  
 In [482]: loc_mapping 
  
 Out[482]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}
  
 Note that this dict could be equivalently constructed by:
  
 loc_mapping = dict((val, idx) for idx, val in enumerate(strings)}
  
 418 | Appendix:Python Language Essentials",NA
Functions,"Functions are the primary and most important method of code organization and 
 reuse in Python. There may not be such a thing as having too many functions. In 
 fact, I would argue that most programmers doing data analysis don’t write enough 
 functions! As you have likely inferred from prior examples, functions are declared 
 using the 
 def
  keyword and returned from using the 
 return
  keyword:
  
 def my_function(x, y, z=1.5):
  
  
  if z > 1:
  
  
  
  return z * (x + y)
  
  
  else:
  
  
  
  return z / (x + y)
  
 There is no issue with having multiple 
 return
  statements. If the end of a function is 
 reached without encountering a 
 return
  statement, 
 None
  is returned.
  
 Each function can have some number of 
 positional
  arguments and some number of 
 keyword
  arguments. Keyword arguments are most commonly used to specify 
 default values or optional arguments. In the above function, 
 x
  and 
 y
  are positional 
 arguments while 
 z
  is a keyword argument. This means that it can be called in either 
 of these equivalent ways:
  
 my_function(5, 6, z=0.7) 
  
 my_function(3.14, 7, 3.5)
  
 The main restriction on function arguments it that the keyword arguments 
 must
  
 follow the positional arguments (if any). You can specify keyword arguments in any 
 order; this frees you from having to remember which order the function arguments 
 were specified in and only what their names are.",NA
"Namespaces, Scope, and Local Functions","Functions can access variables in two different scopes: 
 global
  and 
 local
 . An 
 alternate and more descriptive name describing a variable scope in Python is a 
 namespace
 . Any variables that are assigned within a function by default are 
 assigned to the local name-space. The local namespace is created when the function 
 is called and immediately populated by the function’s arguments. After the function 
 is finished, the local name-space is destroyed (with some exceptions, see section on 
 closures below). Consider the following function:
  
 420 | Appendix:Python Language Essentials
  
 www.it-ebooks.info",NA
Returning Multiple Values,"When I first programmed in Python after having programmed in Java and C++, one 
 of my favorite features was the ability to return multiple values from a function. 
 Here’s a simple example:
  
 def f():
  
  a = 5
  
  b = 6
  
  c = 7
  
  return a, b, c
  
 a, b, c = f()
  
 In data analysis and other scientific applications, you will likely find yourself doing 
 this very often as many functions may have multiple outputs, whether those are 
 data struc-tures or other auxiliary data computed inside the function. If you think 
 about tuple packing and unpacking from earlier in this chapter, you may realize 
 that what’s hap-pening here is that the function is actually just returning 
 one
  object, 
 namely a tuple, which is then being unpacked into the result variables. In the above 
 example, we could have done instead:
  
 return_value = f()
  
 In this case, 
 return_value
  would be, as you may guess, a 3-tuple with the three 
 returned variables. A potentially attractive alternative to returning multiple values 
 like above might be to return a dict instead:
  
 def f():
  
  a = 5
  
  b = 6
  
  c = 7
  
  return {'a' : a, 'b' : b, 'c' : c}",NA
Functions Are Objects,"Since Python functions are objects, many constructs can be easily expressed that 
 are difficult to do in other languages. Suppose we were doing some data cleaning 
 and needed to apply a bunch of transformations to the following list of strings:
  
 states = ['   Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',
  
  'south   
 carolina##', 'West virginia?']
  
 Anyone who has ever worked with user-submitted survey data can expect messy 
 results like these. Lots of things need to happen to make this list of strings uniform 
 and ready for analysis: whitespace stripping, removing punctuation symbols, and 
 proper capital-ization. As a first pass, we might write some code like:
  
 import re  # Regular expression module
  
 def clean_strings(strings):
  
  
  result = []
  
 422 | Appendix:Python Language Essentials
  
 www.it-ebooks.info",NA
Anonymous (lambda) Functions,"Python has support for so-called 
 anonymous
  or 
 lambda
  functions, which are really 
 just simple functions consisting of a single statement, the result of which is the 
 return value. They are defined using the 
 lambda
  keyword, which has no meaning 
 other than “we are declaring an anonymous function.”
  
 def short_function(x):
  
  
  return x * 2
  
 equiv_anon = lambda x: x * 2
  
 I usually refer to these as lambda functions in the rest of the book. They are 
 especially convenient in data analysis because, as you’ll see, there are many cases 
 where data transformation functions will take functions as arguments. It’s often 
 less typing (and clearer) to pass a lambda function as opposed to writing a full-out 
 function declaration or even assigning the lambda function to a local variable. For 
 example, consider this silly example:
  
 def apply_to_list(some_list, f):
  
  
  return [f(x) for x in some_list]
  
 ints = [4, 0, 1, 5, 6] 
  
 apply_to_list(ints, lambda x: x * 2)
  
 You could also have written 
 [x * 2 for x in ints]
 , but here we were able to succintly 
 pass a custom operator to the 
 apply_to_list
  function.
  
 As another example, suppose you wanted to sort a collection of strings by the 
 number of distinct letters in each string:
  
 In [492]: strings = ['foo', 'card', 'bar', 'aaaa', 'abab']
  
 Here we could pass a lambda function to the list’s 
 sort
  method:
  
 In [493]: strings.sort(key=lambda x: len(set(list(x))))
  
 In [494]: strings 
  
 Out[494]: ['aaaa', 'foo', 'abab', 'bar', 'card']
  
  
 One reason lambda functions are called anonymous functions is that 
 the function object itself is never given a name attribute.
  
 424 | Appendix:Python Language Essentials
  
 www.it-ebooks.info",NA
Closures: Functions that Return Functions,"Closures are nothing to fear. They can actually be a very useful and powerful tool in 
 the right circumstance! In a nutshell, a closure is any 
 dynamically-generated
  
 function returned by another function. The key property is that the returned 
 function has access to the variables in the local namespace where it was created. 
 Here is a very simple example:
  
 def make_closure(a):
  
  
  def closure():
  
  
  
  print('I know the secret: %d' % a)
  
  
  return closure
  
 closure = make_closure(5)
  
 The difference between a closure and a regular Python function is that the closure 
 continues to have access to the namespace (the function) where it was created, 
 even though that function is done executing. So in the above case, the returned 
 closure will always print 
 I know the secret: 5
  whenever you call it. While it’s common 
 to create closures whose internal state (in this example, only the value of 
 a
 ) is 
 static, you can just as easily have a mutable object like a dict, set, or list that can be 
 modified. For example, here’s a function that returns a function that keeps track of 
 arguments it has been called with:
  
 def make_watcher():
  
  
  have_seen = {}
  
  def has_been_seen(x):
  
  
  if x in have_seen:
  
  
  
  return True
  
  
  else:
  
  
  
  have_seen[x] = True
  
  
  
  return False
  
  return has_been_seen
  
 Using this on a sequence of integers I obtain:
  
 In [496]: watcher = make_watcher()
  
 In [497]: vals = [5, 6, 1, 5, 1, 6, 3, 5]
  
 In [498]: [watcher(x) for x in vals] 
  
 Out[498]: [False, False, False, True, True, True, False, True]
  
 However, one technical limitation to keep in mind is that while you can mutate any 
 internal state objects (like adding key-value pairs to a dict), you cannot 
 bind
  
 variables in the enclosing function scope. One way to work around this is to modify 
 a dict or list rather than binding variables:
  
 def make_counter():
  
  count = [0]
  
  def counter():
  
 Functions | 425
  
 www.it-ebooks.info",NA
"Extended Call Syntax with *args, **kwargs","The way that function arguments work under the hood in Python is actually very 
 sim-ple. When you write 
 func(a, b, c, d=some, e=value)
 , the positional and keyword 
 arguments are actually packed up into a tuple and dict, respectively. So the internal 
 function receives a tuple 
 args
  and dict 
 kwargs
  and internally does the equivalent of:
  
 a, b, c = args 
  
 d = kwargs.get('d', d_default_value) 
  
 e = kwargs.get('e', e_default_value)
  
 This all happens nicely behind the scenes. Of course, it also does some error 
 checking and allows you to specify some of the positional arguments as keywords 
 also (even if they aren’t keyword in the function declaration!).
  
 def say_hello_then_call_f(f, *args, **kwargs): print 'args is', 
 args
  
  print 'kwargs is', kwargs
  
  print(""Hello! Now I'm going to call %s"" % f) return 
 f(*args, **kwargs)
  
 def g(x, y, z=1):
  
  
  return (x + y) / z
  
 Then if we call 
 g
  with 
 say_hello_then_call_f
  we get:
  
 426 | Appendix:Python Language Essentials",NA
Currying: Partial Argument Application,"Currying
  is a fun computer science term which means deriving new functions from 
 existing ones by 
 partial argument application
 . For example, suppose we had a 
 trivial function that adds two numbers together:
  
 def add_numbers(x, y):
  
  
  return x + y
  
 Using this function, we could derive a new function of one variable, 
 add_five
 , that 
 adds 5 to its argument:
  
 add_five = lambda y: add_numbers(5, y)
  
 The second argument to 
 add_numbers
  is said to be 
 curried
 . There’s nothing very 
 fancy here as we really only have defined a new function that calls an existing 
 function. The built-in 
 functools
  module can simplify this process using the 
 partial
  
 function:
  
 from functools import partial 
  
 add_five = partial(add_numbers, 5)
  
 When discussing pandas and time series data, we’ll use this technique to create 
 speci-alized functions for transforming data series
  
 # compute 60-day moving average of time series x ma60 = 
 lambda x: pandas.rolling_mean(x, 60)
  
 # Take the 60-day moving average of of all time series in data 
 data.apply(ma60)",NA
Generators,"Having a consistent way to iterate over sequences, like objects in a list or lines in a 
 file, is an important Python feature. This is accomplished by means of the 
 iterator 
 proto-col
 , a generic way to make objects iterable. For example, iterating over a dict 
 yields the dict keys:
  
 In [502]: some_dict = {'a': 1, 'b': 2, 'c': 3}
  
 In [503]: for key in some_dict:
  
  
  .....:     print key, 
  
 a c b
  
 When you write 
 for key in some_dict
 , the Python interpreter first attempts to create 
 an iterator out of 
 some_dict
 :
  
 In [504]: dict_iterator = iter(some_dict)
  
 Functions | 427",NA
Files and the operating system,"Most of this book uses high-level tools like 
 pandas.read_csv
  to read data files from 
 disk into Python data structures. However, it’s important to understand the basics 
 of how to work with files in Python. Fortunately, it’s very simple, which is part of 
 why Python is so popular for text and file munging.
  
 To open a file for reading or writing, use the built-in 
 open
  function with either a 
 relative or absolute file path:
  
 In [518]: path = 'ch13/segismundo.txt'
  
 In [519]: f = open(path)
  
 By default, the file is opened in read-only mode 
 'r'
 . We can then treat the file handle 
 f
  like a list and iterate over the lines like so
  
 for line in f:
  
  
  pass
  
 The lines come out of the file with the end-of-line (EOL) markers intact, so you’ll 
 often see code to get an EOL-free list of lines in a file like
  
 In [520]: lines = [x.rstrip() for x in open(path)]
  
 In [521]: lines
  
 430 | Appendix:Python Language Essentials",NA
Index,NA,NA
Symbols ,"! character, 60, 61, 64 
  
 != operator, 91 
  
 !cmd command, 60 
  
 ""two-language"" problem, 2–3 
  
 # (hash mark), 388 
  
 $PATH variable, 10 
  
 % character, 398 
  
 %a datetime format, 293 
  
 %A datetime format, 293 
  
 %alias magic function, 61 
  
 %automagic magic function, 55 
  
 %b datetime format, 293 
  
 %B datetime format, 293 
  
 %bookmark magic function, 60, 62 
 %c datetime format, 293 
  
 %cd magic function, 60 
  
 %cpaste magic function, 51–52, 55 
 %d datetime format, 292 
  
 %D datetime format, 293 
  
 %d format character, 398 
  
 %debug magic function, 54–55, 62 
 %dhist magic function, 60 
  
 %dirs magic function, 60 
  
 %env magic function, 60 
  
 %F datetime format, 293 
  
 %gui magic function, 57 
  
 %H datetime format, 292 
  
 %hist magic function, 55, 59 
  
 %I datetime format, 292 
  
 %logstart magic function, 60 
  
 %logstop magic function, 60 
  
 %lprun magic function, 70, 72 
  
 %m datetime format, 292
  
 %M datetime format, 292 
  
 %magic magic function, 55 
  
 %p datetime format, 293 
  
 %page magic function, 55 
  
 %paste magic function, 51, 55 
  
 %pdb magic function, 54, 63 
  
 %popd magic function, 60 
  
 %prun magic function, 55, 70 
  
 %pushd magic function, 60 
  
 %pwd magic function, 60 
  
 %quickref magic function, 55 
  
 %reset magic function, 55, 59 
  
 %run magic function, 49–50, 55, 386 
  
 %S datetime format, 292 
  
 %s format character, 398 
  
 %time magic function, 55, 67 
  
 %timeit magic function, 54, 67, 68 
  
 %U datetime format, 293 
  
 %w datetime format, 292 
  
 %W datetime format, 293 
  
 %who magic function, 55 
  
 %whos magic function, 55 
  
 %who_ls magic function, 55 
  
 %x datetime format, 293 
  
 %X datetime format, 293 
  
 %xdel magic function, 55, 59 
  
 %xmode magic function, 54 
  
 %Y datetime format, 292 
  
 %y datetime format, 292 
  
 %z datetime format, 293 
  
 & operator, 91 
  
 * operator, 105 
  
 + operator, 406, 409 
  
 2012 Federal Election Commission 
 database 
  
 example, 278–287
  
 We’d like to hear your suggestions for improving our indexes. Send email to 
 index@oreilly.com
 .
  
 433
  
 www.it-ebooks.info",NA
A ,"a file mode, 431 
  
 abs function, 96 
  
 accumulate method, 368 
  
 add method, 95, 130, 417 
  
 add_patch method, 229 
  
 add_subplot method, 221 
  
 aggfunc option, 277 
  
 aggregate method, 260, 262 
  
 aggregations, 100 
  
 algorithms for sorting, 375–376 
  
 alignment of data, 330–331 
  
 all method, 101, 368 
  
 alpha argument, 233 
  
 and keyword, 398, 401 
  
 annotating in matplotlib, 228–230 
  
 anonymous functions, 424 
  
 any method, 101, 110, 201 
  
 append method, 122, 408 
  
 apply method, 39, 132, 142, 266–268, 270 
 apt package management tool, 10 
  
 arange function, 82 
  
 arccos function, 96 
  
 arccosh function, 96 
  
 arcsin function, 96 
  
 arcsinh function, 96 
  
 arctan function, 96 
  
 arctanh function, 96 
  
 argmax method, 101 
  
 argmin method, 101, 139 
  
 argsort method, 135, 374 
  
 arithmetic, 128–132 
  
  
 operations between DataFrame and 
 Series, 
  
  
 130–132 
  
  
 with fill values, 129–130
  
  
 finding elements in sorted array, 376–
 377 
  
 in NumPy, 355–362 
  
  
  
 concatenating, 357–359 
  
  
  
 c_ object, 359 
  
  
  
 layout of in memory, 356–357 
  
  
  
 replicating, 360–361 
  
  
  
 reshaping, 355–356 
  
  
  
 r_ object, 359 
  
  
  
 saving to file, 379–380 
  
  
  
 splitting, 357–359 
  
  
  
 subsets for, 361–362 
  
  
 indexes for, 86–89 
  
  
 operations between, 85–86 
  
  
 setting values by broadcasting, 367 
  
  
 slicing, 86–89 
  
  
 sorting, 101–102 
  
  
 statistical methods for, 100 
  
  
 structured arrays, 370–372 
  
  
  
 benefits of, 372 
  
  
  
 mainpulating, 372 
  
  
  
 nested data types, 371–372 
  
  
 swapping axes in, 93–94 
  
  
 transposing, 93–94 
  
  
 unique function, 102–103 
  
  
 where function, 98–100 
  
 arrow function, 229 
  
 as keyword, 393 
  
 asarray function, 82, 379 
  
 asfreq method, 308, 318 
  
 asof method, 334–336 
  
 astype method, 84, 85 
  
 attributes 
  
  
 in Python, 391 
  
  
 starting with underscore, 48 
  
 average method, 136 
  
 ax argument, 233 
  
 axes
  
 434 | Index",NA
B ,"b file mode, 431 
  
 backslash (\), 397 
  
 bar plots, 235–238 
  
 Basemap object, 245, 246 
  
 .bashrc file, 10 
  
 .bash_profile file, 9 
  
 bbox_inches option, 231 
  
 benefits 
  
  
 of Python, 2–3 
  
  
  
 glue for code, 2 
  
  
  
 solving ""two-language"" problem with, 
 2–
   
  
 3 
  
  
 of structured arrays, 372 
  
 beta function, 107 
  
  
 defined, 342 
  
 between_time method, 335 
  
 bfill method, 123 
  
 bin edges, 314 
  
 binary data formats, 171–172 
  
  
 HDF5, 171–172 
  
  
 Microsoft Excel files, 172 
  
  
 storing arrays in, 103–104 
  
 binary moving window functions, 324–325 
 binary search of lists, 410 
  
 binary universal functions, 96 
  
 binding 
  
  
 defined, 390 
  
  
 variables, 425 
  
 binomial function, 107 
  
 bisect module, 410 
  
 bookmarking directories in IPython, 62 
  
 Boolean 
  
  
 arrays, 101 
  
  
 data type, 84, 398 
  
  
 indexing for arrays, 89–92 
  
 bottleneck library, 324 
  
 braces ({}), 413 
  
 brackets ([]), 406, 408 
  
 break keyword, 401",NA
C ,"calendar module, 290 
  
 casting, 84 
  
 cat method, 156, 212 
  
 Categorical object, 199 
  
 ceil function, 96 
  
 center method, 212 
  
 Chaco, 248 
  
 chisquare function, 107 
  
 chunksize argument, 160, 161 
  
 clearing screen shortcut, 53 
  
 clipboard, executing code from, 50–52 
  
 clock function, 67 
  
 close method, 220, 432 
  
 closures, 425–426 
  
 cmd.exe, 7 
  
 collections module, 416 
  
 colons, 387 
  
 cols option, 277 
  
 columns, grouping on, 256–257 
  
 column_stack function, 359 
  
 combinations function, 430 
  
 combine_first method, 177, 189 
  
 combining 
  
  
 data sources, 336–338 
  
  
 data sources, with overlap, 188–189 
  
  
 lists, 409 
  
 commands, 65 
  
  
 (see also magic commands) 
  
  
 debugger, 65 
  
  
 history in IPython, 58–60 
  
  
  
 input and output variables, 58–59 
  
  
  
 logging of, 59–60 
  
  
  
 reusing command history, 58 
  
  
 searching for, 53 
  
 comment argument, 160 
  
 comments in Python, 388 
  
 compile method, 208 
  
 complex128 data type, 84 
  
 complex256 data type, 84 
  
 complex64 data type, 84 
  
 concat function, 34, 177, 184, 185, 186, 267, 
  
  
  
 357, 359
  
 Index | 435
  
 www.it-ebooks.info",NA
D ,"data aggregation, 259–264 
  
  
 returning data in unindexed form, 264 
  
  
 using multiple functions, 262–264 
  
 data alignment, 128–132 
  
  
 arithmetic methods with fill values, 129–
  
  
  
 130 
  
  
 operations between DataFrame and 
 Series, 
  
  
  
 130–132 
  
 data munging, 329–340 
  
  
 asof method, 334–336 
  
  
 combining data, 336–338 
  
  
 for data alignment, 330–331 
  
  
 for specialized frequencies, 332–334 
  
 data structures for pandas, 112–121 
  
  
 DataFrame, 115–120 
  
  
 Index objects, 120–121 
  
  
 Panel, 152–154 
  
  
 Series, 112–115 
  
 data types 
  
  
 for arrays, 83–85 
  
  
 for ndarray, 83–85 
  
  
 for NumPy, 353–354 
  
  
  
 hierarchy of, 354 
  
  
 for Python, 395–400 
  
  
  
 boolean data type, 398 
  
  
  
 dates and times, 399–400 
  
  
  
 None data type, 399 
  
  
  
 numeric data types, 395–396 
  
  
  
 str data type, 396–398 
  
  
  
 type casting in, 399 
  
  
 for time series data, 290–293 
  
  
  
 converting between string and 
 datetime, 
   
  
  
 291–293 
  
  
 nested, 371–372 
  
 data wrangling 
  
  
 manipulating strings, 205–211 
  
  
  
 methods for, 206–207 
  
  
  
 vectorized string methods, 210–211 
  
  
 with regular expressions, 207–210 
  
  
 merging data, 177–189
  
 436 | Index
  
 www.it-ebooks.info",NA
E ,"edgecolo option, 231 
  
 edit-compile-run workflow, 45 
  
 eig function, 106 
  
 elif blocks (see if statements) 
  
 else block (see if statements) 
  
 empty function, 82, 83 
  
 empty namespace, 50 
  
 encoding argument, 160 
  
 endswith method, 207, 212 
  
 enumerate function, 412 
  
 environment variables, 8, 60 
  
 EPD (Enthought Python Distribution), 7–9 
 equal function, 96 
  
 escapechar option, 165 
  
 ewma function, 323 
  
 ewmcorr function, 323 
  
 ewmcov function, 323 
  
 ewmstd function, 323 
  
 ewmvar function, 323 
  
 ExcelFile class, 172 
  
 except block, 403 
  
 exceptions 
  
  
 automatically entering debugger after, 55 
  
 defined, 402 
  
  
 handling in Python, 402–404 
  
 exec keyword, 59 
  
 execute-explore workflow, 45 
  
 execution time 
  
  
 of code, 55 
  
  
 of single statement, 55 
  
 exit command, 386 
  
 exp function, 96 
  
 expanding window mean, 322 
  
 exponentially-weighted functions, 324 
  
 extend method, 409 
  
 extensible markup language (XML) files, 169–
  
  
 170 
  
 eye function, 83",NA
F ,"fabs function, 96 
  
 facecolor option, 231 
  
 factor analysis, 342–
 343 Factor object, 269 
  
 factors, 342 
  
 fancy indexing 
  
  
 defined, 361
  
  
 for arrays, 92–93 
  
 ffill method, 123 
  
 figsize argument, 234 
  
 Figure object, 220, 223 
  
 file input/output 
  
  
 binary data formats for, 171–172 
  
  
  
 HDF5, 171–172 
  
  
  
 Microsoft Excel files, 172 
  
  
 for arrays, 103–105 
  
  
  
 HDF5, 380 
  
  
  
 memory-mapped files, 379–380 
  
  
  
 saving and loading text files, 104–
 105 
   
 storing on disk in binary format, 
 103–
   
  
  
 104 
  
  
 in Python, 430–431 
  
  
 saving plot to file, 231 
  
  
 text files, 155–170 
  
  
  
 delimited formats, 163–165 
  
  
  
 HTML files, 166–170 
  
  
  
 JSON data, 165–166 
  
  
  
 lxml library, 166–170 
  
  
  
 reading in pieces, 160–162 
  
  
  
 writing to, 162–163 
  
  
  
 XML files, 169–170 
  
  
 with databases, 174–176 
  
  
 with Web APIs, 173–174 
  
 filling in missing data, 145–146, 270–271 
 fillna method, 22, 143, 145, 146, 196, 270, 
  
  
  
 317 
  
 fill_method argument, 313 
  
 fill_value option, 277 
  
 filtering 
  
  
 in pandas, 125–128 
  
  
 missing data, 143–144 
  
  
 outliers, 201–202 
  
 financial applications 
  
  
 cumulative returns, 338–340 
  
  
 data munging, 329–340 
  
  
  
 asof method, 334–336 
  
  
  
 combining data, 336–338 
  
  
  
 for data alignment, 330–331 
  
  
  
 for specialized frequencies, 332–334 
  
 future contract rolling, 347–350 
  
  
 grouping for, 340–345 
  
  
  
 factor analysis with, 342–343 
  
  
  
 quartile analysis, 343–345 
  
  
 linear regression, 350–351 
  
  
 return indexes, 338–340 
  
  
 rolling correlation, 350–351
  
 438 | Index
  
 www.it-ebooks.info",NA
G ,"gamma function, 107 
  
 gcc command, 9, 11 
  
 generators, 427–430 
  
  
 defined, 428 
  
  
 generator expressions, 429 
  
  
 itertools module for, 429–430 
  
 get method, 167, 172, 212, 415 
  
 getattr function, 391 
  
 get_chunk method, 162 
  
 get_dummies function, 203, 205 
  
 get_value method, 128 
  
 get_xlim method, 226 
  
 GIL (global interpreter lock), 3 
  
 global scope, 420, 421 
  
 glue for code 
  
  
 Python as, 2 
  
 .gov domain, 17 
  
 Granger, Brian, 72 
  
 graphics 
  
  
 Chaco, 248 
  
  
 mayavi, 248 
  
 greater function, 96 
  
 greater_equal function, 96 
  
 grid argument, 234 
  
 group keys, 268 
  
 groupby method, 39, 252–259, 297, 316, 
 343, 
   
  
 377, 429 
  
  
 iterating over groups, 255–256 
  
  
 on column, 256–257 
  
  
 on dict, 257–258 
  
  
 on levels, 259 
  
  
 resampling with, 316 
  
  
 using functions with, 258–259 
  
  
 with Series, 257–258 
  
 grouping 
  
  
 2012 Federal Election Commission 
 database 
  
  
  
  
 example, 
 278–287 
  
  
  
 bucketing donation amounts, 283–285 
  
  
 donation statistics by occupation and 
  
  
  
  
  
 employer, 280–283 
  
  
  
 donation statistics by state, 285–287 
  
 apply method, 266–268 
  
  
 data aggregation, 259–264 
  
  
  
 returning data in unindexed form, 264 
  
  
 using multiple functions, 262–264
  
 Index | 439
  
 www.it-ebooks.info",NA
H ,"Haiti earthquake crisis data example, 241–247",NA
I ,"icol method, 128, 152 
  
 IDEs (Integrated Development 
 Environments), 
   
 11, 52 
  
 idxmax method, 138 
  
 idxmin method, 138 
  
 if statements, 400–401, 415 
  
 ifilter function, 430 
  
 iget_value method, 152 
  
 ignore_index argument, 188 
  
 imap function, 430 
  
 import directive 
  
  
 in Python, 392–393 
  
  
 usage of in this book, 13 
  
 imshow function, 98 
  
 in keyword, 409 
  
 in-place sort, 373 
  
 in1d method, 103 
  
 indentation 
  
  
 in Python, 387–388 
  
  
 IndentationError event, 51
  
 half-open, 314 
  
 index method, 206, 207
  
 hasattr function, 391 
  
 Index objects data structure, 120–121
  
 hash mark (#), 388 
  
 indexes
  
 hashability, 416 
  
 defined, 112
  
 HDF5 (hierarchical data format), 171–172, 
  
 for arrays, 86–89
  
 380 
  
 for axis, 197–198
  
 HDFStore class, 171 
  
 header argument, 160 
  
 heapsort sorting method, 376 
  
 hierarchical data format (HDF5), 171–172,
  
 for TimeSeries class, 294–296 
 hierarchical indexing, 147–151 
  
 reshaping data with, 190–191 
  
 sorting levels, 149–150
  
 380 
  
 summary statistics by level, 150
  
 hierarchical indexing 
  
 with DataFrame columns, 150–151
  
 in pandas, 147–151 
  
 in pandas, 136
  
  
  
 sorting levels, 149–150 
  
  
  
 summary statistics by level, 150 
  
  
 with DataFrame columns, 150–
 151 
  
 reshaping data with, 190–191 
  
 hist method, 238 
  
 histograms, 238–239 
  
 history of commands, searching, 53 
  
 homogeneous data container, 370
  
  
 integer indexing, 151–
 152 
  
 merging data on, 182–
 184 index_col argument, 
 160 
  
 indirect sorts, 374–375, 374 
 input variables, 58–59 
  
 insert method, 122, 408 
  
 insort method, 410 
  
 int data type, 83, 395, 399
  
 how argument, 181, 313, 316 
  
 hsplit function, 359 
  
 hstack function, 358 
  
 HTML files, 166–170 
  
 HTML Notebook in IPython, 72 
  
 Hunter, John D., 5, 219 
  
 hyperbolic trigonometric functions, 96
  
 int16 data type, 84 
  
 int32 data type, 84 
  
 int64 data type, 84 
  
 Int64Index Index object, 121 
  
 int8 data type, 84 
  
 integer arrays, indexing using (see 
 fancy 
  
 indexing)
  
 440 | Index",NA
J ,"join method, 184, 206, 212 
  
 JSON (JavaScript Object Notation), 18, 165–
  
 166, 213",NA
K ,"KDE (kernel density estimate) plots, 
 239 keep_date_col argument, 160 
  
 kernels, 239 
  
 key-value pairs, 413 
  
 keyboard shortcuts, 53 
  
  
 for deleting text, 53 
  
  
 for IPython, 52 
  
 KeyboardInterrupt event, 50 
  
 keys 
  
  
 argument, 188 
  
  
 for dicts, 416 
  
  
 method, 414 
  
 keyword arguments, 389, 420 
  
 kind argument, 234, 314 
  
 kurt method, 139",NA
L ,"label argument, 233, 313, 315 
  
 lambda functions, 211, 262, 424 
  
 last method, 261 
  
 layout of arrays in memory, 356–
 357 left argument, 181 
  
 left_index argument, 181 
  
 left_on argument, 181 
  
 legends in matplotlib, 228
  
 Index | 441
  
 www.it-ebooks.info",NA
M ,"mad method, 139 
  
 magic methods, 48, 54–55 
  
 main function, 75 
  
 mainpulating structured arrays, 372 
  
 many-to-many merge, 179 
  
 many-to-one merge, 178 
  
 map method, 133, 195–196, 211, 280, 423 
 margins, 275 
  
 markers, 224 
  
 match method, 208–212 
  
 matplotlib, 5, 219–232 
  
  
 annotating in, 228–230 
  
  
 axis labels in, 226–227 
  
  
 configuring, 231–232 
  
  
 integrating with IPython, 56–57 
  
  
 legends in, 228 
  
  
 saving to file, 231 
  
  
 styling for, 224–225 
  
  
 subplots in, 220–224 
  
  
 ticks in, 226–227 
  
  
 title in, 226–227 
  
 matplotlibrc file, 232 
  
 matrix operations in NumPy, 377–379 
  
 max method, 101, 136, 139, 261, 428 
  
 maximum function, 95, 96 
  
 mayavi, 248 
  
 mean method, 100, 139, 253, 259, 261, 265 
 median method, 139, 261 
  
 memmap object, 379 
  
 memory, layout of arrays in, 356–357 
  
 memory-mapped files 
  
  
 defined, 379 
  
  
 saving arrays to file, 379–380 
  
 mergesort sorting method, 375, 376 
  
 merging data, 177–189
  
 442 | Index
  
 www.it-ebooks.info",NA
N ,"NA data type, 143 
  
 names argument, 160, 188 
  
 namespaces 
  
  
 defined, 420 
  
  
 in Python, 420–421 
  
 naming trends 
  
  
 in US baby names 1880-2010 example, 36–
  
  
  
 43 
  
  
  
 boy names that became girl names, 42–
  
  
  
  
 43 
  
  
  
 measuring increase in diversity, 37–40
  
  
 replicating, 360–361 
  
  
 reshaping, 355–356 
  
  
 r_ object, 359 
  
  
 saving to file, 379–380 
  
  
 splitting, 357–359 
  
  
 subsets for, 361–362 
  
 broadcasting, 362–367 
  
  
 over other axes, 364–367 
  
  
 setting array values by, 367 
  
 data processing using 
  
  
 where function, 98–100 
  
 data processing using arrays, 97–
 103
  
 revolution of last letter, 40–41
  
 Index | 443
  
 www.it-ebooks.info",NA
P ,"pad method, 212 
  
 pairs plot, 241 
  
 pandas, 4–5 
  
  
 arithmetic and data alignment, 128–132 
  
  
 arithmetic methods with fill values, 129–
  
  
  
 130 
  
  
  
 operations between DataFrame and 
  
  
  
 Series, 130–132 
  
  
 data structures for, 112–121 
  
  
  
 DataFrame, 115–120 
  
  
  
 Index objects, 120–121 
  
  
  
 Panel, 152–154 
  
  
  
 Series, 112–115 
  
  
 drop function, 125 
  
  
 filtering in, 125–128 
  
  
 handling missing data, 142–146 
  
  
  
 filling in, 145–146 
  
  
  
 filtering out, 143–144 
  
  
 hierarchical indexing in, 147–151 
  
  
  
 sorting levels, 149–150 
  
  
  
 summary statistics by level, 150 
  
  
  
 with DataFrame columns, 150–151 
  
  
 indexes in, 136",NA
O ,"object introspection, 48–49 
 object model, 388 
  
 object type, 84
  
 indexing options, 125–128 
  
 integer indexing, 151–152 
  
 NumPy universal functions with, 132–
 133 plotting with, 232 
  
  
 bar plots, 235–238
  
  
 objectify function, 166, 169 
 objs argument, 188
  
 density plots, 238–
 239 
  
 histograms, 238–239
  
 444 | Index",NA
Q ,"qcut method, 200, 201, 268, 269, 343 
 qr function, 106 
  
 Qt console for IPython, 55 
  
 quantile analysis, 268–269 
  
 quarterly periods, 309–310 
  
 quartile analysis, 343–345 
  
 question mark (?), 49 
  
 quicksort sorting method, 376 
  
 quotechar option, 164 
  
 quoting option, 164",NA
R ,"r file mode, 431 
  
 r+ file mode, 431 
  
 Ramachandran, Prabhu, 248 
  
 rand function, 107 
  
 randint function, 107, 202 
  
 randn function, 89, 107 
  
 random number generation, 106–107 
  
 random sampling with grouping, 271–272 
 random walks example, 108–110 
  
 range function, 82, 404–405 
  
 ranking data 
  
  
 defined, 135 
  
  
 in pandas, 133–135 
  
 ravel method, 356, 357 
  
 rc method, 231, 232 
  
 re module, 207 
  
 read method, 432 
  
 read-only mode, 431 
  
 reading 
  
  
 from databases, 174–176 
  
  
 from text files in pieces, 160–162 
  
 readline functionality, 58 
  
 readlines method, 432 
  
 readshapefile method, 246 
  
 read_clipboard function, 155 
  
 read_csv function, 104, 155, 161, 163, 261, 
  
  
 430 
  
 read_frame function, 175 
  
 read_fwf function, 155 
  
 read_table function, 104, 155, 158, 163 
 recfunctions module, 372 
  
 reduce method, 368, 369 
  
 reduceat method, 369 
  
 reductions, 137 
  
  
 (see also aggregations)
  
  
 defined, 137 
  
  
 in pandas, 137–142 
  
 references 
  
  
 defined, 389, 390 
  
  
 in Python, 389–390 
  
 regress function, 274 
  
 regular expressions (regex) 
  
  
 defined, 207 
  
  
 manipulating strings with, 207–210 
 reindex method, 122–124, 317, 332 
 reload function, 74 
  
 remove method, 408, 417 
  
 rename method, 198 
  
 renaming axis indexes, 197–198 
  
 repeat method, 212, 360 
  
 replace method, 196, 206, 212 
  
 replicating arrays, 360–361 
  
 resampling, 312–319, 332 
  
  
 defined, 312 
  
  
 OHLC (Open-High-Low-Close) 
  
  
  
 resampling, 316 
  
  
 upsampling, 316–317 
  
  
 with groupby method, 316 
  
  
 with periods, 318–319 
  
 reset_index function, 151 
  
 reshape method, 190–191, 355, 365 
 reshaping 
  
  
 arrays, 355–356 
  
  
 defined, 189 
  
  
 with hierarchical indexing, 190–191 
 resources, 12 
  
 return statements, 420 
  
 returns 
  
  
 cumulative returns, 338–340 
  
  
 defined, 338 
  
  
 return indexes, 338–340 
  
 reversed function, 413 
  
 rfind method, 207 
  
 right argument, 181 
  
 right_index argument, 181 
  
 right_on argument, 181 
  
 rint function, 96 
  
 rjust method, 207 
  
 rollback method, 302 
  
 rollforward method, 302 
  
 rolling, 348 
  
 rolling correlation, 350–351 
  
 rolling_apply function, 323, 326 
  
 rolling_corr function, 323, 350
  
 Index | 447
  
 www.it-ebooks.info",NA
S ,"save function, 103, 379 
  
 save method, 171, 176 
  
 savefig method, 231 
  
 savez function, 104 
  
 saving text files, 104–105 
  
 scatter method, 239 
  
 scatter plots, 239–241 
  
 scatter_matrix function, 241 
 Scientific Python base, 7 
  
 SciPy library, 6 
  
 scipy-user (mailing list), 12 
  
 scope, 420–421 
  
 screen, clearing, 53 
  
 scripting languages, 2 
  
 scripts, 2 
  
 search method, 208, 210 
  
 searchsorted method, 376 
  
 seed function, 107 
  
 seek method, 432 
  
 semantics, 387–395 
  
  
 attributes in, 391 
  
  
 comments in, 388
  
  
 “duck” typing, 392 
  
  
 functions in, 389 
  
  
 import directive, 392–393 
  
 indentation, 387–388 
  
  
 methods in, 389 
  
  
 mutable objects in, 394–395 
  
 object model, 388 
  
  
 operators for, 393
  
 setattr function, 391 
  
 setdefault method, 415 
  
 setdiff1d method, 103 
  
 sets/set comprehensions, 416–417 
 setxor1d method, 103 
  
 set_index function, 151 
  
 set_index method, 193 
  
 set_title method, 226 
  
 set_trace function, 65 
  
 set_value method, 128 
  
 set_xlabel method, 226 
  
 set_xlim method, 226 
  
 set_xticklabels method, 226 
  
 set_xticks method, 226 
  
 shapefiles, 246 
  
 shapes, 80, 353 
  
 sharex option, 223, 234 
  
 sharey option, 223, 234 
  
 shell commands in IPython, 60–61 
 shifting in time series data, 301–
 303 shortcuts, keyboard, 53 
  
  
 for deleting text, 53 
  
  
 for IPython, 52 
  
 shuffle function, 107 
  
 sign function, 96, 202 
  
 signal frontier analysis, 345–347 
  
 sin function, 96 
  
 sinh function, 96 
  
 size method, 255 
  
 skew method, 139 
  
 skipinitialspace option, 165
  
 448 | Index",NA
T ,"tab completion in IPython, 47–48 
  
 tabs, structuring code with, 387–388 
  
 take method, 202, 362 
  
 tan function, 96 
  
 tanh function, 96 
  
 tell method, 432 
  
 terminology, 13–14 
  
 ternary expressions, 405 
  
 text editors, integrating with IPython, 52 
  
 text files, 155–170 
  
  
 delimited formats, 163–165 
  
  
 HTML files, 166–170 
  
  
 JSON data, 165–166 
  
  
 lxml library, 166–170 
  
  
 reading in pieces, 160–162 
  
  
 saving and loading, 104–105 
  
  
 writing to, 162–163 
  
  
 XML files, 169–170 
  
 TextParser class, 160, 162, 168 
  
 text_content method, 167 
  
 thousands argument, 160 
  
 thresh argument, 144 
  
 ticks, 226–227 
  
 tile function, 360, 361 
  
 time series data 
  
  
 and performance, 327–328 
  
  
 data types for, 290–293 
  
  
  
 converting between string and datetime, 
  
  
  
 291–293 
  
  
 date ranges, 298 
  
  
 frequencies, 299–301 
  
  
  
 week of month dates, 301 
  
  
 moving window functions, 320–326 
  
  
  
 binary moving window functions, 324–
  
  
  
 325 
  
  
  
 exponentially-weighted functions, 324 
  
  
 user-defined, 326 
  
  
 periods, 307–312 
  
  
  
 converting timestamps to, 311 
  
  
  
 creating PeriodIndex from arrays, 312 
  
  
 frequency conversion for, 308 
  
  
  
 quarterly periods, 309–310 
  
  
 plotting, 319–320 
  
  
 resampling, 312–319 
  
  
  
 OHLC (Open-High-Low-Close) 
  
  
  
  
 resampling, 316
  
  
  
 upsampling, 316–317 
  
  
  
 with groupby method, 316 
  
  
  
 with periods, 318–319 
  
  
 shifting in, 301–303 
  
  
  
 with offsets, 302–303 
  
  
 time zones in, 303–306 
  
  
  
 localizing objects, 304–305 
  
  
  
 methods for time zone-aware 
 objects, 
  
   
 305–306 
  
  
 TimeSeries class, 293–297 
  
  
  
 duplicate indices with, 296–297 
  
  
  
 indexes for, 294–296 
  
  
  
 selecting data in, 294–296 
  
 timestamps 
  
  
 converting to periods, 311 
  
  
 defined, 289 
  
  
 using periods instead of, 333–334 
  
 timing code, 67–68 
  
 title in matplotlib, 226–227 
  
 top method, 267, 282 
  
 to_csv method, 162, 163 
  
 to_datetime method, 292 
  
 to_panel method, 154 
  
 to_period method, 311 
  
 trace function, 106 
  
 tracebacks, 53–54 
  
 transform method, 264–266 
  
 transforming data, 194–205 
  
  
 discretization, 199–201 
  
  
 dummy variables, 203–205 
  
  
 filtering outliers, 201–202 
  
  
 mapping, 195–196 
  
  
 permutation, 202 
  
  
 removing duplicates, 194–195 
  
  
 renaming axis indexes, 197–198 
  
  
 replacing values, 196–197 
  
 transpose method, 93, 94 
  
 transposing arrays, 93–94 
  
 trellis package, 247 
  
 trigonometric functions, 96 
  
 truncate method, 296 
  
 try/except block, 403, 404 
  
 tuples, 406–407 
  
  
 methods for, 407 
  
  
 unpacking, 407 
  
 type casting, 399 
  
 type command, 156 
  
 TypeError event, 84, 403 
  
 types, 388
  
 450 | Index
  
 www.it-ebooks.info",NA
U ,"U file mode, 431 
  
 uint16 data type, 84 
  
 uint32 data type, 84 
  
 uint64 data type, 84 
  
 uint8 data type, 84 
  
 unary functions, 95 
  
 underscore (_), 48, 58 
  
 unicode type, 19, 84, 395 
  
 uniform function, 107 
  
 union method, 103, 122, 204, 417 
  
 unique method, 102–103, 122, 141–142, 
 279 universal functions, 95–96, 367–370 
  
  
 custom, 370 
  
  
 in pandas, 132–133 
  
  
 instance methods for, 368–369 
  
 universal newline mode, 431 
  
 unpacking tuples, 407 
  
 unstack function, 148 
  
 update method, 337 
  
 upper method, 207, 212 
  
 upsampling, 312, 316–317 
  
 US baby names 1880-2010 example, 32–43 
  
 boy names that became girl names, 42–
 43 measuring increase in diversity, 37–40 
  
 revolution of last letter, 40–41 
  
 usa.gov data from bit.ly example, 17–26 
  
 USDA (US Department of Agriculture) food 
  
  
 database example, 212–217 
  
 use_index argument, 234 
  
 UTC (coordinated universal time), 303",NA
V ,"ValueError event, 402, 403 
  
 values method, 414 
  
 value_counts method, 141–142 
  
 var method, 101, 139, 261 
  
 variables, 55 
  
  
 (see also environment 
 variables) 
  
 deleting, 55 
  
  
 displaying, 55 
  
  
 in Python, 389–390 
  
 Varoquaux, Gaël, 248 
  
 vectorization, 85 
  
  
 defined, 97
  
 verify_integrity argument, 
 188 views, 86, 118 
  
 visualization tools 
  
  
 Chaco, 248 
  
  
 mayavi, 248 
  
 vsplit function, 359 
  
 vstack function, 358",NA
W ,"w file mode, 431 
  
 Wattenberg, Laura, 40 
  
 Web APIs, file input/output with, 173–174 
 week of month dates, 301 
  
 when expressions, 394 
  
 where function, 98–100, 188 
  
 while loops, 402 
  
 whitespace, structuring code with, 387–
 388 Wickham, Hadley, 252 
  
 Williams, Ashley, 212 
  
 Windows, setting up Python on, 7–9 
  
 working directory 
  
  
 changing to passed directory, 60 
  
  
 of current system, returning, 60 
  
 wrangling (see data wrangling) 
  
 write method, 431 
  
 write-only mode, 431 
  
 writelines method, 431 
  
 writer method, 165 
  
 writing 
  
  
 to databases, 174–176 
  
  
 to text files, 162–163",NA
X ,"Xcode, 9 
  
 xlim method, 225, 226 
  
 XML (extensible markup language) files, 
 169–
  
 170 
  
 xrange function, 404–405 
  
 xs method, 128 
  
 xticklabels method, 225",NA
Y ,"yield keyword, 428 
  
 ylim argument, 234 
  
 yticks argument, 
 234
  
 Index | 451
  
 www.it-ebooks.info",NA
Z ,"zeros function, 82 
  
 zip function, 412–413
  
 452 | Index
  
 www.it-ebooks.info",NA
About the Author,"Wes McKinney
  is a New York−based data hacker and entrepreneur. After finishing 
 his undergraduate degree in mathematics at MIT in 2007, he went on to do 
 quantitative finance work at AQR Capital Management in Greenwich, CT. Frustrated 
 by cumber-some data analysis tools, he learned Python and in 2008, started 
 building what would later become the pandas project. He's now an active member 
 of the scientific Python community and is an advocate for the use of Python in data 
 analysis, finance, and statistical computing applications.",NA
Colophon,"The animal on the cover of 
 Python for Data Analysis
  is a golden-tailed, or pen-tailed, 
 tree shrew (
 Ptilocercus lowii
 ). The golden-tailed tree shrew is the only one of its 
 species in the genus 
 Ptilocercus
  and family 
 Ptilocercidae
 ; all the other tree shrews 
 are of the family 
 Tupaiidae
 . Tree shrews are identified by their long tails and soft 
 red-brown fur. As nicknamed, the golden-tailed tree shrew has a tail that resembles 
 the feather on a quill pen. Tree shrews are omnivores, feeding primarily on insects, 
 fruit, seeds, and small vertebrates.
  
 Found predominantly in Indonesia, Malaysia, and Thailand, these wild mammals 
 are known for their chronic consumption of alcohol. Malaysian tree shrews were 
 found to spend several hours consuming the naturally fermented nectar of the 
 bertam palm, equalling about 10 to 12 glasses of wine with 3.8% alcohol content. 
 Despite this, no golden-tailed tree shrew has ever been intoxicated, thanks largely 
 to their impressive ethanol breakdown, which includes metabolizing the alcohol in 
 a way not used by humans. Also more impressive than any of their mammal 
 counterparts, including hu-mans? Brain to body mass ratio.
  
 Despite these mammals’ name, the golden-tailed shrew is not a true shrew, instead 
 more closely related to primates. Because of their close relation, tree shrews have 
 be-come an alternative to primates in medical experimentation for myopia, 
 psychosocial stress, and hepatitis.
  
 The cover image is from 
 Cassel’s Natural History
 . The cover font is Adobe ITC Gara-
 mond. The text font is Linotype Birka; the heading font is Adobe Myriad Condensed; 
 and the code font is LucasFont’s TheSansMonoCondensed.
  
 www.it-ebooks.info",NA
