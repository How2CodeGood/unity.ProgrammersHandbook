Larger Text,Smaller Text,Symbol
Programming with Unicode ,NA,NA
Documentation,NA,NA
Release 2011,NA,NA
Victor Stinner,"August 22, 2015",NA
 1,NA,NA
About this book,"The book is written in
  reStructuredText
  (reST) syntax and compiled by
  
 Sphinx
 . I started to write in the 25th September 2010.",NA
1.1 License,"This book is distributed under the
  CC BY-SA 3.0 license
 .",NA
1.2 Thanks to,"Reviewers: Alexander Belopolsky, Antoine Pitrou, Feth Arezki and Nelle Varoquaux, Natal Ngétal.",NA
1.3 Notations,"• 0bBBBBBBBB: 8 bit unsigned number written in binary, first digit is the most significant. 
  
 0b10000000 is 128.
  
 For 
 example,
  
 • 0xHHHH: number written in hexadecimal, e.g. 0xFFFF is 65535.
  
 • 0xHH 0xHH ...: byte sequence with bytes written in hexadecimal, e.g. 0xC3 0xA9 (2 bytes) is the char-
  
 acter é (U+00E9)
  encoded
  to UTF-8.
  
 • U+HHHH: Unicode character with its code point written in hexadecimal. For example, U+20AC is the 
 “euro 
  
 sign” character, code point 8,364. 
  
 Big code point are written with 
 more than 4 hexadecimal digits, e.g. 
  
 U+10FFFF is the biggest (unallocated) code point of
  Unicode 
 Character Set
  6.0: 1,114,111.
  
 • A—B: range including start and end. Examples:
  
   
 –
  0x00—0x7F is the range 0 through 127 (128 bytes)
  
   
 –
  U+0000—U+00FF is the range 0 through 255 (256 characters)
  
 • {U+HHHH, U+HHHH, ...}: a
  character string
 . For example, {U+0041, U+0042, U+0043} is the string “abc”
  
 (3 characters).
  
 1",NA
 2,NA,NA
Unicode nightmare,"Unicode
  is the nightmare of many developers (and users) for different, and sometimes good reasons.
  
 In the 1980’s, only few people read documents in languages other their mother tongue and English. A 
 computer supported only a small number of languages, the user configured his region to support languages of 
 close countries. Memories and disks were expensive, all applications were written to use
  byte strings
  using 8 
 bits encodings: one byte per character was a good compromise.
  
 Today with the Internet and the globalization, we all read and exchange documents from everywhere around 
 the world (even if we don’t understand everything). The problem is that documents rarely indicate their 
 language (encoding), and displaying a document with the wrong encoding leads to a well known problem:
  
 mojibake
 .
  
 It is difficult to get, or worse,
  guess the encoding
  of a document. Except for encodings of the UTF family 
 (coming from the Unicode standard), there is no reliable algorithm for that. We have to rely on statistics to 
 guess the most probable encoding, which is done by most Internet browsers.
  
 Unicode support
  by
  operating systems
 ,
  programming languages
  and
  libraries
  varies a lot. In general, the 
 support is basic or non-existent. Each operating system manages Unicode differently. For example,
  Windows
  
 stores
  filenames
  as Unicode, whereas UNIX and BSD operating systems use bytes.
  
 Mixing documents stored as bytes is possible, even if they use different encodings, but leads to
  mojibake
 . 
 Because libraries and programs do also ignore encode and decode
  warnings or errors
 , write a single character 
 with a diacritic (any non-
 ASCII
  character) is sometimes enough to get an error.
  
 Full Unicode support is complex because the Unicode charset is bigger than any other charset. For example,
  
 ISO 8859-1
  contains 256 code points including 191 characters, whereas Unicode version 6.0 contains
  248,966 
 assigned code points
 . The Unicode standard is larger than just a charset: it explains also how to display 
 characters (e.g. left-to-right for English and right-to-left for persian), how to
  normalize
  a
  character string
  (e.g. 
 precomposed characters versus the decomposed form), etc.
  
 This book explains how to sympathize with Unicode, and how you should modify your program to avoid most, 
 or all, issues related to encodings and Unicode.
  
 3",NA
 3,NA,NA
Definitions,NA,NA
3.1 Character ,NA,NA
3.2 Glyph ,NA,NA
3.3 Code point ,"A
  code point
  is an unsigned integer. The smallest code point is zero. Code points are usually written as 
 hexadecimal, e.g. “0x20AC” (8,364 in decimal).",NA
3.4 Character set (charset) ,"A
  character set
 , abbreviated
  charset
 , is a mapping between
  code points
  and
  characters
 . The mapping has a 
 fixed size. For example, most 7 bits encodings have 128 entries, and most 8 bits encodings have 256 entries. 
 The biggest charset is the
  Unicode Character Set
  6.0 with 1,114,112 entries.
  
 In some charsets, code points are not all contiguous. For example, the
  cp1252
  charset maps code points from 0 
 though 255, but it has only 251 entries: 0x81, 0x8D, 0x8F, 0x90 and 0x9D code points are not assigned.
  
 Examples of the
  ASCII
  charset: the digit five (“5”, U+0035) is assigned to the code point 0x35 (53 in decimal), 
 and the uppercase letter “A” (U+0041) to the code point 0x41 (65).
  
 The biggest code point depends on the size of the charset. For example, the biggest code point of the ASCII 
 charset is 127 (2
 7
 −
  1) 
  
 Charset examples:
  
 Charset
  
 Code point
  
 Character
  
 ASCII
  
 0x35
  
 5 (U+0035)
  
 ASCII
  
 0x41
  
 A (U+0041)
  
 ISO-8859-15
  
 0xA4
  
 C (U+20AC)
  
 Unicode Character Set
  
 0x20AC
  
 C (U+20AC)",NA
3.5 Character string ,"A
  character string
 , or “Unicode string”, is a string where each unit is a
  character
 . Depending on the 
 implementation, each character can be any Unicode character, or only characters in the range U+0000—
 U+FFFF, range called the
  Basic
  
 5",NA
3.12 Unicode: an Universa,"See also: 
  
 UCS-2
 ,
  UCS-4
 ,
  UTF-8
 ,
  UTF-16
 , and
  UTF-32
  
 encodings.
  
 8
  
  literally “unintelligible sequence of characters”. This issue is called 
 “”",NA
l Character Set (UCS),"encodings.
  
 Chapter 3. Definitions",NA
 4,NA,NA
Unicode,"Unicode is a character set.
  
 It is a superset of all the other character sets.
  
 In the version 6.0, Unicode has
  
 1,114,112 code points (the last code point is U+10FFFF). Unicode 1.0 was limited to 65,536 code points (the 
 last code point was U+FFFF), the range U+0000—U+FFFF called
  BMP
  (
 Basic Multilingual Plane
 ). I call the 
 range U+10000—U+10FFFF as
  non-BMP
  characters.",NA
4.1 Unicode Character Set,"The Unicode Character Set (UCS) contains 1,114,112 code points: U+0000—U+10FFFF. Characters and code 
 point ranges are grouped by
  categories
 . Only encodings of the UTF family are able to encode the UCS.",NA
4.2 Categories,"Unicode 6.0 has 7 character categories, and each category has subcategories:
  
  
 • Letter (L): lowercase (Ll), modifier (Lm), titlecase (Lt), uppercase (Lu), other (Lo)
  
  
 • Mark (M): spacing combining (Mc), enclosing (Me), non-spacing (Mn)
  
  
 • Number (N): decimal digit (Nd), letter (Nl), other (No)
  
  
 • Punctuation (P): connector (Pc), dash (Pd), initial quote (Pi), final quote (Pf), open (Ps), close (Pe), other 
 (Po)
  
 • Symbol (S): currency (Sc), modifier 
 (Sk), math (Sm), other (So)
  
  
 • Separator (Z): line (Zl), paragraph (Zp), space (Zs)
  
  
 • Other (C): control (Cc), format (Cf), not assigned (Cn), private use (Co),
  surrogate
  (Cs) 
  
 There are 3 ranges reserved for private use (Co subcategory): 
  
 U+E000—U+F8FF (6,400 code points), 
 U+F0000—U+FFFFD (65,534) and U+100000—U+10FFFD (65,534). Surrogates (Cs subcategory) use the 
 range U+D800—U+DFFF (2,048 code points).",NA
4.3 Statistics,"On a total of 1,114,112 possible code points, only 248,966 code points are assigned: 77.6% are not assigned. 
 Statistics excluding not assigned (Cn), private use (Co) and
  surrogate
  (Cs) subcategories:
  
  
 • Letter: 100,520 (91.8%)
  
 9",NA
4.4 Normalization,"Unicode standard explains how to decompose a character. For example, the precomposed character ç 
 (U+00C7, Latin capital letter C with cedilla) can be written as the sequence of two characters: {¸ (U+0327, 
 Combining cedilla), c (U+0043, Latin capital letter C)}. This decomposition can be useful to search a substring 
 in a text, e.g. remove diacritic is pratical for the user. The decomposed form is called Normal Form D (
 NFD
 ) 
 and the precomposed form is called Normal Form C (
 NFC
 ).
  
 Form
  
 String
  
 Unicode
  
 NFC
  
 ç
  
 U+00C7
  
 NFD
  
 ¸c
  
 {U+0327, U+0043}
  
 Unicode database contains also a compatibility layer: if a character cannot be rendered (no font contain the 
 requested character) or encoded to a specific encoding, Unicode proposes a
  replacment character sequence 
 which looks like the character
 , but may have a different meaning.
  
 For example, ij(U+0133, Latin small ligature ij) is replaced by the two characters {i (U+0069, Latin small letter 
 I), j (U+006A, Latin small letter J)}. ijcharacter
  cannot be encoded
  to
  ISO 8859-1
 , whereas ij characters can.
  
 Two extra normal forms use this compatibility layer:
  NFKD
  (decomposed) and
  NFKC
  (precomposed).
  
 Note: 
  
 The precomposed forms (NFC and NFKC) begin by a canonical decomposition before recomposing 
 pre-
  
 combined characters again.
  
 10
  
 Chapter 4. Unicode",NA
 5,NA,NA
Charsets and encodings,NA,NA
5.1 Encodings,"There are many encodings around the world. Before Unicode, each manufacturer invented its own encoding 
 to fit its client market and its usage. Most encodings are incompatible on at least one code, except some 
 exceptions. A document stored in
  ASCII
  can be read using
  ISO 8859-1
  or UTF-8, because ISO-8859-1 and UTF-8 
 are supersets of ASCII. Each encoding can have multiple aliases, examples:
  
  
 • ASCII: US-ASCII, ISO 646, ANSI_X3.4-1968, ...
  
 • ISO-8859-1: Latin-1, iso88591, ...
  
 • UTF-8: utf8, UTF_8, ...
  
 Unicode
  is a charset and it requires a encoding. Only encodings of the UTF family are able to encode and 
 decode all Unicode code points. Other encodings only support a subset of Unicode codespace. For example, 
 ISO-8859-1 are the first 256 Unicode code points (U+0000—U+00FF).
  
 This book presents the following encodings:
  ASCII
 ,
  cp1252
 ,
  GBK
 ,
  ISO 8859-1
 ,
  ISO 8859-15
 ,
  JIS
 ,
  UCS-2
 ,
  UCS-4
 , 
 UTF-8
 ,
  UTF-16
  and
  UTF-32
 .",NA
5.2 Popularity,"The three most common encodings are, in chronological order of their creation:
  ASCII
  (1968),
  ISO 8859-1
  
 (1987) and 
 UTF-8
  (1996).
  
 Google posted an interesting graph of the usage of different encodings on the web:
  Unicode nearing 50% of 
 the web 
 (Mark Davis, january 2010). Because Google crawls a huge part of the web, these numbers should be 
 reliable. In 2001, the most used encodings were:
  
  
 • 1st (56%):
  ASCII
  
  
 • 2nd (23%): Western Europe encodings (
 ISO 8859-1
 ,
  ISO 8859-15
  and
  cp1252
 )
  
  
 • 3rd (8%): Chinese encodings (
 GB2312
 , ...)
  
  
 • and then come Korean (EUC-KR), Cyrillic (cp1251, KOI8-R, ...), East Europe (cp1250, ISO-8859-2), Arabic 
  
  
 (cp1256, ISO-8859-6), etc.
  
 • (UTF-8 was not used on the web in 2001) 
  
 In december 2007, for the first time:
  UTF-8
  becomes the most used encoding (near 25%). In january 2010, 
 UTF-8 was close to 50%, and ASCII and Western Europe encodings were near 20%. The usage of the other 
 encodings don’t change.
  
 11",NA
5.3 Encodings performances,"Complexity of getting the n
 th
 character in a string, and of getting the length in character of a 
 string:•",NA
 6,NA,NA
Historical charsets and encodings,"Between 1950 and 2000, each manufacturer and each operating system created its own 8 bits encoding. The 
 problem was that 8 bits (256 code points) are not enough to store any character, and so the encoding tries to 
 fit the user’s language. Most 8 bits encodings are able to encode multiple languages, usually geograpically 
 close (e.g. ISO-8859-1 is intented for Western Europe).
  
 It was difficult to exchange documents of different languages, because if a document was encoded to an 
 encoding different than the user encoding, it leaded to
  mojibake
 .",NA
6.1 ASCII,"ASCII encoding is supported by all applications. A document encoded in ASCII can be read decoded by any 
 other encoding. This is explained by the fact that all 7 and 8 bits encodings are superset of ASCII, to be 
 compatible with ASCII. Except
  JIS X 0201
  encoding: 0x5C is decoded to the yen sign (U+00A5, ¥) instead of a 
 backslash (U+005C, \).
  
 ASCII is the smallest encoding, it only contains 128 codes including 95 printable characters (letters, digits, 
 punctuation signs and some other various characters) and 33 control codes. Control codes are used to control 
 the terminal. For example, the “line feed” (code point 10, usually written ""\n"") marks the end of a line. There 
 are some special control code. For example, the “bell” (code point 7, written ""\b"") sent to ring a bell.
  
  
 -0
  
 -1
  
 -2
  
 -3
  
 -4
  
 -5
  
 -6
  
 -7
  
 -8
  
 -9
  
 -a
  
 -b
  
 -c
  
 -d
  
 -e
  
 -f
  
 0-
  
 NUL
   
  
  
  
  
  
 BEL
   
 TAB
  
 LF
   
  
 CR
   
  
 1-
   
  
  
  
  
  
  
  
  
  
  
 ESC
   
  
  
  
 2-
   
 !
  
 “
  
 #
  
 $
  
 %
  
 &
  
 ‘
  
 (
  
 )
  
 *
  
 +
  
 ,
  
 -
  
 .
  
 /
  
 3-
  
 0
  
 1
  
 2
  
 3
  
 4
  
 5
  
 6
  
 7
  
 8
  
 9
  
 :
  
 ;
  
 <
  
 =
  
 >
  
 ?
  
 4-
  
 @
  
 A
  
 B
  
 C
  
 D
  
 E
  
 F
  
 G
  
 H
  
 I
  
 J
  
 K
  
 L
  
 M
  
 N
  
 O
  
 5-
  
 P
  
 Q
  
 R
  
 S
  
 T
  
 U
  
 V
  
 W
  
 X
  
 Y
  
 Z
  
 [
  
 \
  
 ]
  
 ^
  
 _
  
 6-
  
 ‘
  
 a
  
 b
  
 c
  
 d
  
 e
  
 f
  
 g
  
 h
  
 i
  
 j
  
 k
  
 l
  
 m
  
 n
  
 o
  
 7-
  
 p
  
 q
  
 r
  
 s
  
 t
  
 u
  
 v
  
 w
  
 x
  
 y
  
 z
  
 {
  
 |
  
 }
  
 ~
  
 DEL
  
 0x00—0x1F and 0x7F are control codes:
  
 • NUL (0x00): nul character (U+0000, ""\0"")
  
 • BEL (0x07): sent to ring a bell (U+0007, ""\b"")
  
 • TAB (0x09): horizontal tabulation (U+0009, ""\t"")
  
 • LF (0x0A): line feed (U+000A, ""\n"")
  
 • CR (0x0D): carriage return (U+000D, ""\r"")
  
 15",NA
6.2 ISO 8859 family,"Year
  
 Norm
  
 Description
  
 Variant
  
 1987
  
 ISO 8859-1
  
 Western European: 
  
 Ger-
  
 man, French, Italian, ...
  
 cp1252
  
 1987
  
 ISO 8859-2
  
 Central European: 
  
 Croat-
  
 ian, Polish, Czech, ...
  
 cp1250
  
 1988
  
 ISO 8859-3
  
 South European: 
 and Esperanto
  
 Turkish
  
  
 •
  
 1988
  
 ISO 8859-4
  
 North European -
  
  
 1988
  
 ISO 8859-5
  
 Latin/Cyrillic: 
  
 nian, Russian, ...
  
 Macado-
  
  
 KOI family
  
 1987
  
 ISO 8859-6
  
 Latin/Arabic: 
  
 Arabic lan-
  
 guage characters
  
 cp1256
  
 1987
  
 ISO 8859-7
  
 Latin/Greek: modern 
 greek language
  
 cp1253
  
 1988
  
 ISO 8859-8
  
 Latin/Hebrew: modern He-
 brew alphabet
  
 cp1255
  
 1989
  
 ISO 8859-9
  
 Turkish: Largely the same 
 as ISO 8859-1
  
 cp1254
  
 1992
  
 ISO 8859-10
  
 Nordic: a rearrangement of 
 Latin-4
  
 •
  
 2001
  
 ISO 8859-11
  
 Latin/Thai: Thai language
  
 TIS 620, cp874
  
 1998
  
 ISO 8859-13
  
 Baltic Rim: 
 guages
  
 Baltic lan-
  
  
 cp1257
  
 1998
  
 ISO 8859-14
  
 Celtic: Gaelic, Breton
  
 •
  
 1999
  
 ISO 8859-15
  
 Revision of 8859-1: 
 sign
  
 euro
  
  
 cp1252
  
 2001
  
 ISO 8859-16
  
 South-Eastern European
  
 •
  
  
 Note:
  ISO 8859-12 doesn’t 
 exist.
  
 6.2.1 ISO 8859-1
  
 ISO/CEI 8859-1, also known 
 as 
  
 latin letters with diacritics and 
 32
  
 t.
  
 “Latin-1” or “ISO-8859-1”, is a superset of
  ASCII
 : it adds 128 code points, 
 mostly
  
  control codes. It is used in the USA and in Western Europe.
  
 Chapter 6. Historical charsets and encodings",NA
6.3 CJK: asian encodings,"6.3.1 Chinese encodings
  
 GBK is a family of Chinese charsets using multibyte 
 encodings:
  
 • GB 2312 (1980): includes 6,763 Chinese 
 characters
  
 • GBK (1993) (
 code page
  936)
  
  
 • GB 18030 (2005, last revision in 2006)
  
  
 • HZ (1989) (HG-GZ-2312) 
  
 Other encodings: Big5 (, Big Five Encoding, 1984), cp950.
  
 6.3.2 Japanese encodings
  
 JIS is a family of Japanese encodings:
  
 • JIS X 0201 (1969): all code points are encoded to 1 
 byte• 16 bits:
  
 18
  
 Chapter 6. Historical charsets and encodings",NA
6.4 Cyrillic,"KOI family, “ ”:
  
  
 • KOI-7: oldest KOI encoding (ASCII + some characters)
  
 6.4. Cyrillic
  
 19",NA
 7,NA,NA
Unicode encodings,NA,NA
7.1 UTF-8,"UTF-8 is a multibyte encoding able to encode the whole Unicode charset. An encoded character takes 
 between 1 and 4 bytes. UTF-8 encoding supports longer byte sequences, up to 6 bytes, but the biggest code 
 point of Unicode 6.0 (U+10FFFF) only takes 4 bytes.
  
 It is possible to be sure that a
  byte string
  is encoded to UTF-8, because UTF-8 adds markers to each byte. For 
 the first byte of a multibyte character, bit 7 and bit 6 are set (0b11xxxxxx); the next bytes have bit 7 set and 
 bit 6 unset (0b10xxxxxx).
  
 Another cool feature of UTF-8 is that it has no endianness (it can be read in big or little endian order, it does 
 not matter). Another advantage of UTF-8 is that most
  C
  bytes functions are compatible with UTF-8 encoded 
 strings (e.g. strcat() or
  printf()
 ), whereas they fail with UTF-16 and UTF-32 encoded strings because these 
 encodings encode small codes with nul bytes.
  
 The problem with UTF-8, if you compare it to
  ASCII
  or
  ISO 8859-1
 , is that it is a multibyte encoding: you 
 cannot access a character by its character index directly, you have to iterate on each character because each 
 character may have a different length in bytes. If getting a character by its index is a common operation in 
 your program, use a 
 character string
  instead of a
  UTF-8 encoded string
 .
  
 See also:
  
 Non-strict UTF-8 decoder
  and
  Is UTF-8?
 .",NA
"7.2 UCS-2, UCS-4, UTF-16 and UTF-32","UCS-2
  and
  UCS-4
  encodings
  encode
  each code point to exactly one unit of, respectivelly, 16 and 32 bits. UCS-4 
 is able to encode all Unicode 6.0 code points, whereas UCS-2 is limited to
  BMP
  characters. These encodings 
 are practical because the length in units is the number of characters.
  
 UTF-16
  and
  UTF-32
  encodings use, respectivelly, 16 and 32 bits units. UTF-16 encodes code points bigger 
 than U+FFFF using two units: a
  surrogate pair
 . UCS-2 can be
  decoded
  from UTF-16. UTF-32 is also supposed to 
 use more than one unit for big code points, but in practical, it only requires one unit to store all code points of 
 Unicode 6.0. That’s why UTF-32 and UCS-4 are the same encoding.
  
 Encoding
  
 Word size
  
 Unicode support
  
 UCS-2
  
 16 bits
  
 BMP only
  
 UTF-16
  
 16 bits
  
 Full
  
 UCS-4
  
 32 bits
  
 Full
  
 UTF-32
  
 32 bits
  
 Full
  
 21",NA
7.3 UTF-7,"The UTF-7 encoding is similar to the
  UTF-8 encoding
 , except that it uses 7 bits units instead of 8 bits units. It is 
 used for example in emails with server which are not “8 bits clean”.",NA
7.4 Byte order marks (BOM),"UTF-16
  and
  UTF-32
  use units bigger than 8 bits, and so hit endian issue. A single unit can be stored in the big 
 endian (most significant bits first) or little endian (less significant bits first). BOM are short byte sequences to 
 indicate the encoding and the endian. It’s the U+FEFF code point encoded to the UTF encodings.
  
 Unicode defines 6 different BOM:
  
 BOM
  
 Encoding
  
 Endian
  
 0x2B 0x2F 0x76 0x38 0x2D (5 bytes)
  
 UTF-7
  
 endianless
  
 0xEF 0xBB 0xBF (3)
  
 UTF-8
  
 endianless
  
 0xFF 0xFE (2)
  
 UTF-16-LE
  
 little endian
  
 0xFE 0xFF (2)
  
 UTF-16-BE
  
 big endian
  
 0xFF 0xFE 0x00 0x00 (4)
  
 UTF-32-LE
  
 little endian
  
 0x00 0x00 0xFE 0xFF (4)
  
 UTF-32-BE
  
 big endian
  
 UTF-32-LE BOMs starts with UTF-16-LE BOM.
  
 “UTF-16” and “UTF-32” encoding names are imprecise: depending of the context, format or protocol, it means 
 UTF-16 and UTF-32 with BOM markers, or UTF-16 and UTF-32 in the host endian without BOM. On Windows, 
 “UTF-16”usually means UTF-16-LE.
  
 Some Windows applications, like notepad.exe, use UTF-8 BOM, whereas many applications are unable to 
 detect the BOM, and so the BOM causes troubles. UTF-8 BOM should not be used for better interoperability.",NA
7.5 UTF-16 surrogate pairs,"Surrogates are characters in the Unicode range U+D800—U+DFFF (2,048 code points): it is also the
  Unicode 
 category
 “surrogate” (Cs). The range is composed of two parts:
  
 • U+D800—U+DBFF (1,024 code points): high surrogates
  
 • U+DC00—U+DFFF (1,024 code points): low surrogates
  
 In
  UTF-16
 , characters in ranges U+0000—U+D7FF and U+E000—U+FFFD are stored as a single 16 bits unit.
  
 Non-BMP
  characters (range U+10000—U+10FFFF) are stored as “surrogate pairs”, two 16 bits units: an high 
 surrogate (in range U+D800—U+DBFF) followed by a low surrogate (in range U+DC00—U+DFFF). A lone 
 surrogate character is invalid in UTF-16, surrogate characters are always written as pairs (high followed by 
 low).
  
 Examples of surrogate pairs:
  
 22
  
 Chapter 7. Unicode encodings",NA
 8,NA,NA
How to guess the encoding of a document?,"Only
  ASCII
 ,
  UTF-8
  and encodings using a
  BOM
  (
 UTF-7
  with BOM, UTF-8 with BOM,
  UTF-16
 , and
  UTF-32
 ) have 
 reliable algorithms to get the encoding of a document. For all other encodings, you have to trust heuristics 
 based on statistics.",NA
8.1 Is ASCII?,"Check if a document is encoded to
  ASCII
  is simple: test if the bit 7 of all bytes is unset (0b0xxxxxxx).
  
 Example in
  C
 :
  
 int
  isASCII
 (
 const
  char
  *
 data,
  size_t
  size) 
  
 { 
  
  
 const
  unsigned char
  *
 str
  =
  (
 const
  unsigned char
 *
 )data; 
  
 const
  unsigned 
 char
  *
 end
  =
  str
  +
  size; 
  
  
 for
  (; str
  !=
  end; str
 ++
 ) { 
  
  
  
 if
  (
 *
 str
  &
  0x80
 ) 
  
  
  
  
 return
  0
 ; 
  
  
 } 
  
  
 return
  1
 ; 
  
 }
  
 In
  Python
 , the ASCII decoder can be used:
  
 def
  isASCII
 (data): 
  
  
 try
 : 
  
  
  
 data
 .
 decode(
 'ASCII'
 ) 
  
  
 except
  UnicodeDecodeError
 : 
  
  
  
 return
  False 
  
  
 else
 : 
  
  
  
 return
  True
  
  
 Note: 
  
 Only use the Python function on short strings because it decodes the whole string into memory. For 
 long
  
 strings, it is better to use the algorithm of the C function because it doesn’t allocate any memory.",NA
8.2 Check for BOM markers,"If the string begins with a
  BOM
 , the encoding can be extracted from the BOM. But there is a problem with
  UTF-
 16-BE 
 and
  UTF-32-LE
 : UTF-32-LE BOM starts with the UTF-16-LE BOM.
  
 25",NA
8.3 Is UTF-8?,"UTF-8
  encoding adds markers to each bytes and so it’s possible to write a reliable algorithm to check if a
  byte 
 string 
 is encoded to UTF-8.
  
 26
  
 Chapter 8. How to guess the encoding of a document?",NA
8.4 Libraries,"PHP
  has a builtin function to detect the encoding of a
  byte string
 : mb_detect_encoding().
  
 •
  chardet
 :
  Python
  version of the “chardet” algorithm implemented in Mozilla
  
 •
  UTRAC
 : command line program (written in
  C
 ) to recognize the encoding of an input file and its end-of-
 line 
  
 type
  
 •
  charguess
 : Ruby library to guess the charset of a document
  
 28
  
 Chapter 8. How to guess the encoding of a document?",NA
 9,NA,NA
Good practices,NA,NA
9.1 Rules,"To limit or avoid issues with Unicode, try to follow these rules:
  
 •
  decode
  all bytes data as early as possible: keyboard strokes, files, data received from the network, ...•
  
 encode
  back Unicode to bytes as late as possible: write text to a file, log a message, send data to the 
 network, ...
  
 • always store and manipulate text as
  character strings
  
 • if you have to encode text and you can choose the encoding: prefer the
  UTF-8
  encoding. It is able to 
 encode 
  
 all Unicode 6.0 characters (including
  non-BMP characters
 ), has no endian issue, is well 
 supported by most 
  
 programs, and its good compromise is size.",NA
9.2 Unicode support levels,"There are different levels of Unicode support:
  
  
 •
  don’t
  support Unicode: only work correctly if all inputs and outputs are encoded to the same encoding, 
 usually 
   
 the
  locale encoding
 , use
  byte strings
 .
  
 •
  basic
  Unicode support: decode inputs and encode outputs using the correct encodings, usually only 
 support 
 BMP
  characters. Use
  Unicode strings
 , or
  byte strings
  with the locale encoding or, better, an 
 encoding of the UTF family (e.g.
  UTF-8
 ).
  
 •
  full
  Unicode support: have access to the Unicode database,
  normalize text
 , render correctly bidirectional 
 texts 
  
 and characters with diacritics.
  
 These levels should help you to estimate the status of the Unicode support of your project. Basic support is 
 enough if all of your users speak the same language or live in close countries. Basic Unicode support usually 
 means excellent support of Western Europe languages. Full Unicode support is required to support Asian 
 languages.
  
 By default, the
  C
 ,
  C++
  and
  PHP5
  languages have basic Unicode support. For the C and C++ languages, you can 
 have basic or full Unicode support using a third-party library like
  glib
 ,
  Qt
  or
  ICU
 . With PHP5, you can have 
 basic Unicode support using “mb_” functions.
  
 By default, the
  Python 2
  language doesn’t support Unicode. You can have basic Unicode support if you store 
 text into the unicode type and take care of input and output encodings. For
  Python 3
 , the situation is different: 
 it has direct basic Unicode support by using the wide character API on Windows and by taking care of input 
 and output encodings for you (e.g. decode command line arguments and environment variables). The 
 unicodedata module is a first step for a full Unicode support.",NA
9.3 Test the Unicode support of a program,"Tests to evaluate the Unicode support of a program:
  
  
 • Write non-ASCII characters (e.g. é, U+00E9) in all input fields: if the program fails with an error, it has no 
  
  
 Unicode support.
  
 • Write characters not encodable to the
  locale encoding
  (e.g. Ł, U+0141) in all input fields: if the program 
 fails 
  
 with an error, it probably has basic Unicode support.
  
 • To test if a program is fully Unicode compliant, write text mixing different languages in different 
 directions and characters with diacritics, especially in Persian characters. Try also
  decomposed 
 characters
 , for example: {e, U+0301} (decomposed form of é, U+00E9).
  
 See also: 
  
 Wikipedia article to
  test the Unicode support of your web browser
 .
  UTF-8 encoded sample plain-text file
  
 (Markus Kuhn, 2002).",NA
9.4 Get the encoding of your inputs,"Console:
  
  
 • Windows:
  GetConsoleCP()
  for stdin and
  GetConsoleOutputCP()
  for stdout and stderr
  
  
 • Other OSes: use the
  locale encoding 
  
 File formats:
  
  
 • XML: the encoding can be specified in the <?xml ...?> header, use
  UTF-8
  if the encoding is not specified. 
  
  
 For example, <?xml version=""1.0"" encoding=""iso-8859-1""?>.
  
 • HTML:
  
 the
  
 encoding
  
 can
  
 be
  
 specified
  
 in
  
 a
  
 “Content
  
 type”
  
 HTTP
  
 header,
  
 e.g.
  
 <meta 
 If it is
  
 http-equiv=""content-type"" content=""text/html; charset=ISO-8859-1"">. not, you have to guess 
 the encoding.
  
 Filesystem (filenames):
  
  
 •
  Windows
  stores filenames as Unicode. It provides a bytes compatibily layer using the
  ANSI code page
  for 
  
  
 applications using
  byte strings
 .
  
 •
  Mac OS X
  encodes filenames to
  UTF-8
  and
  normalize
  see to a variant of the Normal Form D.
  
 • Other OSes: use the
  locale encoding 
  
 See also: 
  
 How to guess the encoding of a 
 document?",NA
9.5 Switch from byte strings to character strings ,"Use 
 character strings, instead of byte strings, to avoid
  mojibake issues
 .
  
 9.5. Switch from byte strings to character strings
  
 31",NA
 10,NA,NA
Operating systems,NA,NA
10.1 Windows,"Since Windows 2000, Windows offers a nice Unicode API and supports
  non-BMP characters
 . It uses
  Unicode 
 strings 
 implemented as
  wchar_t*
  strings (LPWSTR).
  wchar_t
  is 16 bits long on Windows and so it uses
  UTF-16
 :
  
 non-BMP
  characters are stored as two
  wchar_t
  (a
  surrogate pair
 ), and the length of a string is the number of 
 UTF-16 units and not the number of characters.
  
 Windows 95, 98 an Me had also Unicode strings, but were limited to
  BMP characters
 : they used
  UCS-2
  instead 
 of UTF-16.
  
 10.1.1 Code pages
  
 A Windows application has two encodings, called code pages (abbreviated “cp”): ANSI and OEM code pages. 
 The ANSI code page, CP_ACP, is used for the ANSI version of the
  Windows API
  to decode
  byte strings
  to
  
 character strings
  and has a number between 874 and 1258. The OEM code page or “IBM PC” code page, 
 CP_OEMCP, comes from MS-DOS, is used for the
  Windows console
 , contains glyphs to create text interfaces 
 (draw boxes) and has a number between 437 and 874. Example of a French setup: ANSI is
  cp1252
  and OEM is 
 cp850.
  
 There are code page constants:
  
  
 • CP_ACP: Windows ANSI code page
  
  
 • CP_MACCP: Macintosh code page
  
  
 • CP_OEMCP: ANSI code page of the current process
  
 • 
 CP_SYMBOL (42): Symbol code page
  
  
 • CP_THREAD_ACP: ANSI code page of the current thread
  
 • CP_UTF7 (65000):
  UTF-7
  
  
 • CP_UTF8 (65001):
  UTF-8 
  
 Functions.
  
 UINT
  GetACP
 () 
  
  
 Get the ANSI code page 
 number.
  
 UINT
  GetOEMCP
 () 
  
  
 Get the OEM code page 
 number.
  
 BOOL
  SetThreadLocale
 (LCID
  locale
 ) 
  
  
 Set the locale. It can be used to change the ANSI code page of current thread 
 (CP_THREAD_ACP).",NA
10.2 Mac OS X,"Mac OS X uses
  UTF-8
  for the filenames. If a filename is an invalid UTF-8 byte string, Mac OS X
  returns an error
 . 
 The filenames are
  decomposed
  to an incompatible variant of the Normal Form D (NFD). Extract of the
  
 Technical Q&A QA1173
 : “For example, HFS Plus uses a variant of Normal Form D in which U+2000 through 
 U+2FFF, U+F900 through U+FAFF, and U+2F800 through U+2FAFF are not decomposed.”",NA
10.3 Locales,"To support different languages and encodings, UNIX and BSD operating systems have “locales”. Locales are 
 process-wide: if a thread or a library change the locale, the whole process is impacted.
  
 10.3.1 Locale categories
  
 Locale categories:
  
  
 • LC_COLLATE: compare and sort strings
  
  
 • LC_CTYPE: decode
  byte strings
  and encode
  character strings
  
  
 • LC_MESSAGES: language of messages
  
  
 • LC_MONETARY: monetary formatting
  
  
 • LC_NUMERIC: number formatting (e.g. thousands separator)
  
  
 • LC_TIME: time and date formatting 
  
 LC_ALL is a special category: if you set a locale using this category, it sets the locale for all 
 categories.
  
 Each category has its own environment variable with the same name. For example, LC_MESSAGES=C displays 
 error messages in English. To get the value of a locale category, LC_ALL, LC_xxx (e.g. LC_CTYPE) or LANG 
 environment variables are checked: use the first non empty variable. If all variables are unset, fallback to the 
 C locale.
  
 Note:
  The gettext library reads LANGUAGE, LC_ALL and LANG environment variables (and some others) to 
 get the user language. The LANGUAGE variable is specific to gettext and is not related to locales.",NA
10.4 Filesystems (filenames),"10.4.1 CD-ROM and DVD 
  
 CD-ROM uses the ISO 9660 filesystem which stores filenames as
  byte strings
 . This filesystem is very 
 restrictive: only A-Z, 0-9, _ and ”.” are allowed. Microsoft has developed the Joliet extension: store filenames as
  
 UCS-2
 , up to 64 characters (
 BMP
  only). It was first supported by Windows 95. Today, all operating systems are 
 able to read it. UDF (Universal Disk Format) is the filesystem of DVD: it stores filenames as character strings.
  
 10.4.2 Microsoft: FAT and NTFS filesystems 
  
 MS-DOS uses the FAT filesystems (FAT 12, FAT 16, FAT 32): filenames are stored as
  byte strings
 . Filenames 
 are limited to 8+3 characters (8 for the name, 3 for the extension) and displayed differently depending on the
  
 code page 
 (
 mojibake issue
 ).
  
 Microsoft extended its FAT filesystem in Windows 95: the Virtual FAT (VFAT) supports “long filenames”, 
 filenames are stored as
  UCS-2
 , up to 255 characters (BMP only). Starting at Windows 2000,
  non-BMP 
 characters
  can be used: 
 UTF-16
  replaces UCS-2 and the limit is now 255 UTF-16 units.
  
 The NTFS filesystem stores filenames using UTF-16 encoding.
  
 10.4.3 Apple: HFS and HFS+ filesystems 
  
 HFS stores filenames as byte strings.
  
 HFS+ stores filenames as
  UTF-16
 : the maximum length is 255 UTF-16 units.
  
 10.4.4 Others 
  
 JFS and ZFS also use Unicode.
  
 The ext family (ext2, ext3, ext4) store filenames as byte strings.
  
 10.4. Filesystems (filenames)
  
 39",NA
 11,NA,NA
Programming languages,NA,NA
11.1 C language,"The C language is a low level language, close to the hardware. It has a builtin
  character string
  type (
 wchar_t*
 ), 
 but only few libraries support this type. It is usually used as the first “layer” between the kernel (system calls, 
 e.g. open a file) and applications, higher level libraries and other programming languages. This first layer uses 
 the same type as the kernel: except
  Windows
 , all kernels use
  byte strings
 .
  
 There are higher level libraries, like
  glib
  or
  Qt
 , offering a Unicode API, even if the underlying kernel uses byte 
 strings. Such libraries use a codec to
  encode
  data to the kernel and to
  decode
  data from the kernel. The codec 
 is usually the current
  locale encoding
 .
  
 Because there is no Unicode standard library, most third-party libraries chose the simple solution: use
  byte 
 strings
 . For example, the OpenSSL library, an open source cryptography toolkit, expects
  filenames
  as byte 
 strings. On Windows, you have to encode Unicode filenames to the current
  ANSI code page
 , which is a small 
 subset of the Unicode charset.
  
 11.1.1 Byte API (char)
  
 char
  
 For historical reasons,
  char
  is the C type for a character (“char” as “character”). In pratical, it’s only true 
 for 7 and 8 bits encodings like
  ASCII
  or
  ISO 8859-1
 . With multibyte encodings, a
  char
  is only one byte. 
 For example, the character “é” (U+00E9) is encoded as two bytes (0xC3 0xA9) in
  UTF-8
 .
  
 char
  is a 8 bits integer, it is signed or not depending on the operating system and the compiler. On 
 Linux, the GNU compiler (gcc) uses a signed type for Intel CPU. It defines __CHAR_UNSIGNED__ if
  char
  
 type is unsigned. Check if the CHAR_MAX constant from <limits.h> is equal to 255 to check if
  char
  is 
 unsigned.
  
 A literal byte is written between apostrophes, e.g. ’a’. Some control characters can be written with an 
 back-slash plus a letter (e.g. ’\n’ = 10). It’s also possible to write the value in octal (e.g. ’\033’ = 27) or 
 hexadecimal (e.g. ’\x20’ = 32). An apostrophe can be written ’\\u2019’ or ’\x27’. A backslash is written’\\’.
  
 <ctype.h> contains functions to manipulate bytes, like toupper() or isprint().
  
 11.1.2 Byte string API (char*)
  
 char* 
  
 char*
  is a a
  byte string
 . This type is used in many places in the C standard library. For example, fopen() 
 uses
  char*
  for the filename.
  
 41",NA
11.2 C++,"• std::wstring:
  character string
  using the
  wchar_t
  type, Unicode version of std::string (
 byte string
 )
  
 • std::wcin, std::wcout and std::wcerr: standard input, output and error output; Unicode version of 
  
 std::cin, std::cout and std::cerr
  
 • std::wostringstream: character stream buffer; Unicode version of std::ostringstream.
  
 To initialize the
  locales
 , equivalent to setlocale(LC_ALL, """"), use:
  
 #include <locale> 
  
 std::locale::global(std::locale(""""));
  
 If you use also C and C++ functions (e.g.
  printf()
  and std::cout) to access the standard streams, you may have 
 issues with
  non-ASCII
  characters. To avoid these issues, you can disable the automatic synchronization 
 between C (std*) and C++ (std::c*) streams using:
  
 #include <iostream> 
  
 std::ios_base::sync_with_stdio(false);
  
  
 Note: 
  
 Use typedef basic_ostringstream<wchar_t> wostringstream; if wostringstream is not
  
 available.",NA
11.3 Python,"Python supports Unicode since its version 2.0 released in october 2000.
  Byte
  and
  Unicode
  strings store their 
 length, so it’s possible to embed nul byte/character.
  
 Python can be compiled in two modes: narrow (
 UTF-16
 ) and wide (
 UCS-4
 ). sys.maxunicode constant is 0xFFFF 
 in narrow build, and 0x10FFFF in wide build. Python is compiled in narrow mode on Windows, because
  
 wchar_t
  
 11.2. C++
  
 43",NA
11.4 PHP,"In PHP 5, a literal string (e.g. ""abc"") is a
  byte string
 . PHP has no
  character string
  type, only a “string” type 
 which is a
  byte string
 .
  
 PHP have “multibyte” functions to manipulate byte strings using their encoding. These functions have an 
 optional encoding argument. If the encoding is not specified, PHP uses the default encoding (called “internal 
 encoding”).
  
 Some multibyte functions:
  
  
 • mb_internal_encoding(): get or set the internal encoding
  
  
 • mb_substitute_character(): change how to
  handle unencodable characters
 :
  
  
  
 –
  ""none"":
  ignore
  unencodable characters
  
  
  
 –
  ""long"":
  escape as hexadecimal
  value, e.g. ""U+E9"" or ""JIS+7E7E""
  
  
  
 –
  ""entity"":
  escape as HTML entities
 , e.g. ""&#xE9;""
  
  
 • mb_convert_encoding():
  decode
  from an encoding and
  encode
  to another encoding
  
  
 • mb_ereg(): search a pattern using a regular expression
  
  
 • mb_strlen(): get the length in characters
  
  
 • mb_detect_encoding():
  guess the encoding
  of a
  byte string 
  
 Perl compatible regular expressions (PCRE) have an u flag (“PCRE8”) to process byte strings as UTF-8 
 encoded strings.
  
 PHP includes also a binding of the
  iconv
  library.
  
 • iconv():
  decode
  a
  byte string
  from an encoding and
  encode
  to another encoding, you can use //IGNORE or 
  
 //TRANSLIT suffix to choose the
  error handler
  
 • iconv_mime_decode(): decode a MIME header field
  
 11.4. PHP
  
 47",NA
11.5 Perl,"Write a character using its code point written in hexadecimal:
  
  
 • chr(0x1F4A9)
  
  
 • ""\x{2639}""
  
  
 • ""\N{U+A0}"" 
  
 Using use charnames qw( :full );, you can use a Unicode character in a string using ""\N{name}"" syntax.
  
 Example:
  
 say
  ""\N{long s} \N{ae} \N{Omega} \N{omega} \N{UPWARDS ARROW}""
  
 Declare that filehandles opened within this lexical scope but not elsewhere are in UTF-8, until and unless you 
 say otherwise. The :std adds in STDIN, STDOUT, and STDERR. This critical step implicitly decodes incoming 
 data and encodes outgoing data as UTF-8:
  
 use
  open
  qw( :encoding(UTF-8) :std )
 ;
  
 If PERL_UNICODE environment variable is set to AS, the following data will use UTF-8:
  
  
 • @ARGV
  
  
 • STDIN, STDOUT, STDERR 
  
 If you have a DATA handle, you must explicitly set its encoding. If you want this to be UTF-8, then 
 say:
  
 binmode
 (DATA,
  "":encoding(UTF-8)""
 );
  
 Misc:
  
 use
  feature
  qw< unicode_strings >
 ; 
  
 use
  Unicode::
 Normalize
  qw< NFD NFC >
 ; 
  
 use
  Encode
  qw< encode decode >
 ; 
  
 @ARGV
  =
  map
  { decode(
 ""UTF-8""
 ,
  $_
 ) }
  @ARGV
 ; 
  
 open
 (OUTPUT,
  ""> :raw :encoding(UTF-16LE) :crlf""
 ,
  $filename
 );
  
 Misc:
  
  
 • Encode
  
  
 • Unicode::Normalize
  
  
 • Unicode::Collate
  
  
 • Unicode::Collate::Locale
  
  
 • Unicode::UCD
  
  
 • DBM_Filter::utf8 
  
 History:
  
  
 • Perl 5.6 (2000): initial Unicode support, support
  character strings
  
  
 • Perl 5.8 (2002): regex supports Unicode
  
  
 • use “use utf8;” pragma to specify that your Perl script is encoded to
  UTF-8
  
 48
  
 Chapter 11. Programming languages",NA
11.6 Java,"char is a character able to store Unicode
  BMP
  only characters (U+0000—U+FFFF), whereas Character is a 
 character able to store any Unicode character (U+0000—U+10FFFF). Character methods:
  
  
 • .getType(ch): get the
  category
  of a character
  
  
 • .isWhitespace(ch): test if a character is a whitespace according to Java
  
  
 • .toUpperCase(ch): convert to uppercase 
  
 String is a
  character string
  implemented using a char array and
  UTF-16
 . String methods:
  
  
 • String(bytes, encoding):
  decode
  a
  byte string
  from the specified encoding. The decoder is
  strict
 : 
  
  
 throw a CharsetDecoder exception if a
  byte sequence cannot be decoded
 .
  
 • .getBytes(encoding):
  encode
  to the specified encoding, throw a CharsetEncoder exception if a 
  
 character
  cannot be encoded
 .
  
 • .length(): get the length in UTF-16 units.
  
 As
  Python
  compiled in narrow mode,
  non-BMP
  characters are stored as
  UTF-16 surrogate pairs
  and the length 
 of a string is the number of UTF-16 units, not the number of Unicode characters.
  
 Java, as the Tcl language, uses a variant of
  UTF-8
  which encodes the nul character (U+0000) as the
  overlong 
 byte sequence
  0xC0 0x80, instead of 0x00. So it is possible to use
  C
  functions like strlen() on
  byte string
  with 
 embeded nul characters.",NA
11.7 Go and D,"The Go and D languages use
  UTF-8
  as internal encoding to store
  Unicode strings
 .
  
 11.6. Java
  
 49",NA
 12,NA,NA
Libraries,"Programming languages have no or basic support of Unicode. Libraries are required to get a full support of 
 Unicode on all platforms.",NA
12.1 Qt library,"Qt is a big
  C++
  library covering different topics, but it is typically used to create graphical interfaces. It is 
 distributed under the
  GNU LGPL license
  (version 2.1), and is also available under a commercial license.
  
 12.1.1 Character and string classes
  
 QChar is a Unicode character, only able to store
  BMP characters
 . It is implemented using a 16 bits unsigned 
 number.
  
 Interesting QChar methods:
  
  
 • isSpace(): True if the
  character category
  is separator (Zl, Zp or Zs)
  
  
 • toUpper(): convert to upper case 
  
 QString is a
  character string
  implemented as an array of QChar using
  UTF-16
 . A
  Non-BMP character
  is stored 
 as two QChar (a
  surrogate pair
 ). Interesting QString methods:
  
  
 • toAscii(), fromAscii(): encode to/decode from
  ASCII
  
  
 • toLatin1(), fromLatin1(): encode to/decode from
  ISO 8859-1
  
  
 • utf16(), fromUtf16(): encode to/decode to
  UTF-16
  (in the host endian)
  
  
 • normalized():
  normalize
  to NFC, NFD, NFKC or NFKD 
  
 Qt
  decodes
  literal byte strings from
  ISO 8859-1
  using the QLatin1String class, a thin wrapper to
  char*
 . 
 QLatin1String is a character string storing each character as a single byte. It is possible because it only sup-
 ports characters in U+0000—U+00FF range. QLatin1String cannot be used to manipulate text, it has a smaller 
 API than QString. For example, it is not possible to concatenate two QLatin1String strings.
  
 12.1.2 Codec
  
 QTextCodec.codecForLocale() gets the locale encoding codec:
  
  
 • Windows:
  ANSI code page
  
  
 • Otherwise: the
  locale encoding
 . Try nl_langinfo(CODESET), or LC_ALL, LC_CTYPE, LANG environ-
  
  
 ment variables. If no one gives any useful information, fallback to
  ISO 8859-1
 .
  
 51",NA
12.2 The glib library,"The
  glib library
  is a great
  C
  library distributed under the
  GNU LGPL license
  (version 2.1).
  
 12.2.1 Character strings
  
 The gunichar type is a character. It is able to store any Unicode 6.0 character (U+0000—U+10FFFF).
  
 The glib library has no
  character string
  type. It uses
  byte strings
  using the gchar* type, but most functions use 
 UTF-8
  encoded strings.
  
 12.2.2 Codec functions
  
 • g_convert():
  decode
  from an encoding and
  encode
  to another encoding with the
  iconv library
 . 
  
 Use 
  
 g_convert_with_fallback() to choose
  how to handle undecodable bytes
  and
  unencodable characters
 .• 
 g_locale_from_utf8() / g_locale_to_utf8(): encode to/decode from the current
  locale encoding
 .
  
 • g_get_charset(): get the locale encoding
  
  
 –
  Windows: current
  ANSI code page
  
  
 –
  OS/2: current code page (call DosQueryCp())
  
  
 –
  other: try nl_langinfo(CODESET), or LC_ALL, LC_CTYPE or LANG environment variables• 
 g_utf8_get_char(): get the first character of an UTF-8 string as gunichar
  
 12.2.3 Filename functions
  
 • g_filename_from_utf8() / g_filename_to_utf8():
  encode
 /
 decode
  a filename to/from UTF-8• 
 g_filename_display_name(): human readable version of a filename. Try to decode the filename from 
  
 each encoding of g_get_filename_charsets() encoding list. If all decoding failed, decode the filename 
  
 from
  UTF-8
  and
  replace undecodable bytes
  by (U+FFFD).
  
 52
  
 Chapter 12. Libraries",NA
12.3 iconv library,"libiconv
  is a library to encode and decode text in different encodings. It is distributed under the
  GNU LGPL 
 license
 . It supports a lot of encodings including rare and old encodings.
  
 By default, libiconv is
  strict
 : an
  unencodable character
  raise an error. You can
  ignore
  these characters by 
 adding the //IGNORE suffix to the encoding name. There is also the //TRANSLIT suffix to
  replace unencodable 
 characters 
 by similarly looking characters.
  
 PHP
  has a builtin binding of iconv.",NA
12.4 ICU libraries,"International Components for Unicode
  (ICU) is a mature, widely used set of
  C
 ,
  C++
  and
  Java
  libraries 
 providing Unicode and Globalization support for software applications. ICU is an open source project 
 distributed under the
  MIT license
 .",NA
12.5 libunistring,"libunistring
  provides functions for manipulating Unicode strings and for manipulating C strings according to 
 the Unicode standard. It is distributed under the
  GNU LGPL license
  version 3.
  
 12.3. iconv library
  
 53",NA
 13,NA,NA
Unicode issues,NA,NA
13.1 Security vulnerabilities,"13.1.1 Special characters
  
 Fullwidth (U+FF01—U+FF60) and halfwidth (U+FF61—U+FFEE) characters has been used in 2007 to bypass 
 secu-rity checks. Examples with the
  Unicode normalization
 :
  
  
 • U+FF0E is normalized to . (U+002E) in NFKC
  
  
 • U+FF0F is normalized to / (U+002F) in NFKC 
  
 Some important characters have also “alternatives” in Unicode:
  
  
 • Windows directory separator, \ (U+005C): U+20E5, U+FF3C
  
  
 • UNIX directory separator, / (U+002F): U+2215, U+FF0F
  
  
 • Parent directory, .. (U+002E, U+002E): U+FF0E 
  
 For more information, read
  GS07-01 Full-Width and Half-Width Unicode Encoding IDS/IPS/WAF Bypass 
 Vulnera-bility
  (GamaTEAM, april 2007).
  
 13.1.2 Non-strict UTF-8 decoder: overlong byte sequences and surrogates
  
 An
  UTF-8 decoder
  have to reject overlong byte sequences, or an attacker can use them to bypass security 
 checks (e.g. check rejecting string containing nul bytes, 0x00). For example, 0xC0 0x80 byte sequence must 
 raise an error and not be decoded as U+0000, and ”.” (U+002E) can be encoded to 0xC0 0xAE (two bytes 
 instead of one) to bypass directory traversal checks.
  
 Surrogates characters
  are also invalid in UTF-8: characters in U+D800—U+DFFF have to be rejected. See the 
 table 3-7 in the
  Conformance chapter of the Unicode standard
  (december 2009); and the section 3 (UTF-8 
 definition) of 
 UTF-8, a transformation format of ISO 10646
  (RFC 3629, november 2003).
  
 The libxml2 library had such vulnerability until january 2008:
  CVE-2007-6284
 .
  
 Some PHP functions use a strict UTF-8 decoder (e.g. mb_convert_encoding()), some other don’t. For example, 
 utf8_decode() and mb_strlen() accept 0xC0 0x80 in PHP 5.3.2. The UTF-8 decoder of Python 3 is strict, 
 whereas the UTF-8 decoder of Python 2 accepts surrogates (to keep the backward compatibility). In Python 3, 
 the error handler surrogatepass can be used to encode and decode surrogates.
  
 Note:
  The
  Java
  and Tcl languages uses a variant of
  UTF-8
  which encodes the nul character (U+0000) as the 
 overlong byte sequence 0xC0 0x80, instead of 0x00, for practical reasons.
  
 55",NA
 14,NA,NA
See also,"•
  UTF-8 and Unicode FAQ for Unix/Linux
  by Markus Kuhn, first version in june 1999, last edit in may 2009
  
 57",NA
Symbols,"nl_langinfo (C function),
  38
  
  
 _wfopen (C function),
  36 
 _wfstat (C function),
  36 
 _wopen (C function),
  36",NA
A,"ASCII,
  15",NA
B,"BMP,
  8 
  
 BOM,
  22",NA
P ,"printf (C function),
  42",NA
S ,"setlocale (C function),
  38 
  
 SetThreadLocale (C function),
  
 33 
 Surrogate pair,
  22",NA
U,NA,NA
C ,"char (C type),
  41 
  
 cp1252,
  17",NA
G ,"GBK,
  18 
  
 GetACP (C function),
  33 
  
 GetConsoleCP (C function),
  36 
  
 GetConsoleOutputCP (C function),
  36 
 GetOEMCP (C function),
  33",NA
I ,"ISO-8859-1,
  16 
  
 ISO-8859-15,
  18",NA
J ,"JIS,
  18",NA
M,"UCS-2,
  21 
  
 UCS-4,
  21 
  
 Unicode,
  8 
  
 UTF-16,
  21 
  
 UTF-32,
  21 
  
 UTF-7,
  22 
  
 UTF-8,
  21",NA
W,"wchar_t (C type),
  42 
  
 wcstombs (C function),
  38 
  
 WideCharToMultiByte (C function),
  
 34 
 wprintf (C function),
  42 
  
 WriteConsoleW (C function),
  36
  
 mbstowcs (C function),
  38 
  
 Mojibake,
  7 
  
 MultiByteToWideChar (C function),
  
 34",NA
N,"NFC,
  10 
  
 NFD,
  10 
  
 NFKC,
  10 
  
 NFKD,
  10
  
 59",NA
