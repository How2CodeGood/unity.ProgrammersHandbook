Larger Text,Smaller Text,Symbol
Programmi,NA,NA
ng ,NA,NA
Persistent ,NA,NA
Memory,NA,NA
A Comprehensive Guide for ,NA,NA
Developers,NA,NA
—,NA,NA
Steve Scargall,NA,NA
Programming ,NA,NA
Persistent ,NA,NA
Memory,NA,NA
A Comprehensive Guide for ,NA,NA
Developers,NA,NA
Steve Scargall,NA,NA
Table of Contents,"About the Author  xiii
  
 About the Technical Reviewer xv
  
 About the Contributors xvii
  
 Acknowledgments xxi
  
 Preface xxiii
  
 Chapter 1: Introduction to Persistent Memory Programming  1 
 A High-Level 
 Example Program  2 What’s Different?  5 The Performance Difference 6 Program 
 Complexity  7 How Does libpmemkv Work?  8 What’s Next?  9 Summary 9
  
 Chapter 2: Persistent Memory Architecture  11 
 Persistent Memory Characteristics  
 12 Platform Support for Persistent Memory  13 Cache Hierarchy  14 Power-Fail 
 Protected Domains  16 The Need for Flushing, Ordering, and Fencing  19 Data 
 Visibility  23 Intel Machine Instructions for Persistent Memory  24 Detecting Platform 
 Capabilities  25
  
 iii",NA
About the Author,"Steve Scargall 
 is a persistent memory software and cloud architect at Intel 
  
 Corporation. As a technology evangelist, he supports the enabling and development 
 effort to integrate persistent memory technology into software stacks, applications, and 
 hardware architectures. This includes working with independent software vendors 
 (ISVs) on both proprietary and open source development, original equipment 
 manufacturers (OEMs), and cloud service providers (CSPs).
  
 Steve holds a Bachelor of Science in computer science and cybernetics from the 
 University of Reading, UK, where he studied neural networks, AI, and robotics. He has 
 over 19 years’ experience providing performance analysis on x86 architecture and 
 SPARC for Solaris Kernel, ZFS, and UFS file system. He performed DTrace debugging in 
 enterprise and cloud environments during his tenures at Sun Microsystems and 
 Oracle.
  
 xiii",NA
About the Technical Reviewer,"Andy Rudoff 
 is a principal engineer at Intel Corporation, focusing on non-volatile 
 memory programming. He is a contributor to the SNIA NVM Programming Technical 
 Work Group. His more than 30 years’ industry experience includes design and 
  
 development work in operating systems, file systems, networking, and fault 
 management at companies large and small, including Sun Microsystems and VMware. 
 Andy has taught various operating systems classes over the years and is a coauthor of 
 the popular 
 UNIX Network Programming
  textbook.
  
 xv",NA
About the Contributors,"Piotr Balcer 
 is a software engineer at Intel Corporation with many years’ experience 
 working on storage-related technologies. He holds a Bachelor of Science in engineering 
 from the Gdańsk University of Technology, Poland, where he studied system software 
 engineering. Piotr has been working on the software ecosystem for next-generation 
 persistent memory since 2014.
  
 Eduardo Berrocal 
 joined Intel Corporation as a cloud software engineer in 2017 after 
 receiving his PhD in computer science from the Illinois Institute of Technology. His 
 doctoral research focused on data analytics and fault tolerance for high-performance 
 computing. Past experience includes working as an intern at Bell Labs (Nokia), a 
 research aid at Argonne National Laboratory, a scientific programmer and web 
 developer at the University of Chicago, and an intern in the CESVIMA laboratory in Spain.
  
 Adam Borowski 
 is a software engineer at Intel Corporation, hailing from the 
  
 University of Warsaw, Poland. He is a Debian developer and has made many open 
 source contributions over the past two decades. Adam is currently working on 
 persistent memory stacks, both on upstream code and integrating it with 
 downstream distributions.
  
 Igor Chorazewicz 
 is a software engineer at Intel Corporation. His main focus is on 
 persistent memory data structures and enabling C++ applications for persistent 
 memory. Igor holds a Bachelor of Science in engineering from the Gdańsk University of 
 Technology, Poland.
  
 Adam Czapski 
 is a technical writer at Intel Corporation. He writes technical 
  
 documentation in the Data Center Group and is currently working in the persistent 
 memory department. Adam holds a Bachelor of Arts in English philology and a master’s 
 degree in natural language processing from the Gdańsk University of Technology, 
 Poland.
  
 Steve Dohrmann 
 is a software engineer at Intel Corporation. He has worked on a 
 variety of projects over the past 20 years, including media frameworks, mobile agent 
 software, secure collaboration software, and parallel programming language 
 implementation. He is currently working on enabling the use of persistent memory in 
 Java*.",NA
Acknowledgments,"First and foremost, I would like to thank Ken Gibson for masterminding this book idea 
 and for gifting me the pleasure of writing and managing it. Your support, guidance, and 
 contributions have been instrumental in delivering a high-quality product.
  
 If the Vulcan mind-meld or 
 The Matrix
  Headjack were possible, I could have cloned 
 Andy Rudoff’s mind and allowed him to work on his daily activities. Instead, Andy’s 
 infinite knowledge of persistent memory had to be tapped through good old verbal 
 communication and e-mail. I sincerely thank you for devoting so much time to me and 
 this project. The results read for themselves.
  
  
 Debbie Graham was instrumental in helping me manage this colossal project. Her 
 dedication and support helped drive the project to an on-time completion.
  
 To my friends and colleagues at Intel who contributed content, supported 
  
 discussions, helped with decision-making, and reviewed drafts during the book-writing 
 process. These are the real heroes. Without your heavily invested time and support, this 
 book would have taken considerably longer to complete. It is a much better product as a 
 result of the collaborative effort. A huge thanks to all of you.
  
 I'd like to express my sincerest gratitude and appreciation to the people at Apress, 
 without whom this book could not have been published. From the initial contact and 
 outline discussions through the entire publishing process to this final polished product, 
 the Apress team delivered continuous support and assistance. Many thanks to Susan, 
 Jessica, and Rita. It was a real pleasure working with you.
  
 xxi",NA
Preface,NA,NA
 About This Book,"Persistent memory is often referred to as non-volatile memory (NVM) or storage class 
 memory (SCM). In this book, we purposefully use 
 persistent memory
  as an all- 
 encompassing term to represent all the current and future memory technologies that fall 
 under this umbrella. This book introduces the persistent memory technology and 
 provides answers to key questions. For software developers, those questions include: 
 What is persistent memory? How do I use it? What APIs and libraries are available? 
 What benefits can it provide for my application? What new programming methods do I 
 need to learn? How do I design applications to use persistent memory? Where can I find 
 information, documentation, and help?
  
 System and cloud architects will be provided with answers to questions such as: 
 What is persistent memory? How does it work? How is it different than DRAM or 
 SSD/ NVMe storage devices? What are the hardware and operating system 
 requirements? 
  
 What applications need or could benefit from persistent memory? Can my existing 
 applications use persistent memory without being modified?
  
 Persistent memory is not a plug-and-play technology for software applications. 
  
 Although it may look and feel like traditional DRAM memory, applications need to be 
 modified to fully utilize the persistence feature of persistent memory. That is not to say 
 that applications cannot run unmodified on systems with persistent memory installed, 
 they can, but they will not see the full potential of what persistent memory offers 
 without code modification.
  
 Thankfully, server and operating system vendors collaborated very early in the 
 design phase and already have products available on the market. Linux and Microsoft 
 Windows already provide native support for persistent memory technologies. Many 
 popular virtualization technologies also support persistent memory.
  
 For ISVs and the developer community at large, the journey is just beginning. Some 
 software has already been modified and is available on the market. However, it will take 
 time for the enterprise and cloud computing industries to adopt and make the hardware 
 available to the general marketplace. ISVs and software developers need time to 
 understand what changes to existing applications are required and implement them.
  
 xxiii",NA
 Book Structure,"This book has 19 chapters, each one focusing on a different topic. The book has three main 
 sections. Chapters 
 1
 -
 4
  provide an introduction to persistent memory architecture, 
 hardware, and operating system support. Chapters 
 5
 -
 16
  allow developers to understand 
 the PMDK libraries and how to use them in applications. Finally, Chapters 
 17
 -
 19
  provide 
 information on advanced topics such as RAS and replication of data using RDMA.
  
 xxiv",NA
 Intended Audience,"This book has been written for experienced application developers in mind. We intend 
 the content to be useful to a wider readership such as system administrators and 
 architects, students, lecturers, and academic research fellows to name but a few. System 
 designers, kernel developers, and anyone with a vested or passing interest in this 
 emerging technology will find something useful within this book.
  
 Every reader will learn what persistent memory is, how it works, and how 
 operating systems and applications can utilize it. Provisioning and managing persistent 
 memory are vendor specific, so we include some resources in the Appendix sections to 
 avoid overcomplicating the main chapter content.
  
 Application developers will learn, by example, how to integrate persistent memory 
 in to existing or new applications. We use examples extensively throughout this book 
 using a variety of libraries available within the Persistent Memory Development Kit 
 (PMDK). Example code is provided in a variety of programming languages such as C, C++, 
 JavaScript, and others. We want developers to feel comfortable using these libraries in 
 their own projects. The book provides extensive links to resources where you can find 
 help and information.
  
 System administrators and architects of Cloud, high-performance computing, and 
 enterprise environments can use most of the content of this book to 
  
 understand persistent memory features and benefits to support applications and 
 developers. Imagine being able to deploy more virtual machines per physical server or 
 provide applications with this new memory/storage tier such that they can keep more 
 data closer to the CPU or restart in a fraction of the time they could before while keeping 
 a warm cache of data. 
  
 Students, lecturers, and academic research fellows will also benefit from many 
 chapters within this book. Computer science classes can learn about the hardware, 
 operating system features, and programming techniques. Lecturers are free use the 
 content in student classes or to form the basis of research projects such as new 
 persistent memory file systems, algorithms, or caching implementations.
  
 xxvii",NA
 A Future Reference,"The book content has been written to provide value for many years. Industry 
  
 specification such as ACPI, UEFI, and the SNIA non-volatile programming model will, 
 unless otherwise stated by the specification, remain backward compatible as new 
 versions are released. If new form factors are introduced, the approach to 
 programming remains the same. We do not limit ourselves to one specific persistent 
 memory vendor or implementation. In places where it is necessary to describe vendor-
 specific features or implementations, we specifically call this out as it may change 
 between vendors or between product generations. We encourage you to read the 
 vendor documentation for the persistent memory product to learn more.
  
 Developers using the Persistent Memory Development Kit (PMDK) will retain a 
 stable API interface. PMDK will deliver new features and performance improvements 
 with each major release. It will evolve with new persistent memory products, CPU 
 instructions, platform designs, industry specifications, and operating system feature 
 support.",NA
 Source Code Examples,"Concepts and source code samples within this book adhere to the vendor neutral SNIA 
 non-volatile memory programming model. 
 SNIA
  which is the Storage 
  
 Networking Industry Association is a non-profit global organization dedicated to 
 developing standards and education programs to advance storage and information 
 technology. The model was designed, developed, and is maintained by the 
 SNIA NVM 
 Technical Working Group (TWG
 ) which includes many leading operating system, 
 hardware, and server vendors. You can join this group or find information at 
 https:// 
 www.snia.org/forums/sssi/nvmp
 .
  
 xxviii",NA
 Book Conventions,"This book uses several conventions to draw your attention to specific pieces of 
 information. The convention used depends on the type of information displayed.",NA
 Computer Commands,"Commands, programming library, and API function references may be presented in line 
 with the paragraph text using a monospaced font. For example: 
  
  
 To illustrate how persistent memory is used, let’s start with a sample program 
 demonstrating the key-value store provided by a library called libpmemkv.
  
 xxix",NA
 Computer Terminal Output,"Computer terminal output is usually taken directly from a computer terminal presented 
 in a monospaced font such as the following example demonstrating cloning the 
 Persistent Memory Development Kit (PMDK) from the GitHub project:
  
 $ git clone https://github.com/pmem/pmdk 
 Cloning into 'pmdk'...
  
 remote: Enumerating objects: 12, done.
  
 remote: Counting objects: 100% (12/12), done.
  
 remote: Compressing objects: 100% (10/10), done.
  
 remote: Total 100169 (delta 2), reused 7 (delta 2), pack-reused 100157 Receiving 
 objects: 100% (100169/100169), 34.71 MiB | 4.85 MiB/s, done. Resolving deltas: 
 100% (83447/83447), done.",NA
 Source Code,"Source code examples taken from the accompanying GitHub repository are shown with 
 relevant line numbers in a monospaced font. Below each code listing is a reference to 
 the line number or line number range with a brief description. Code comments use 
 language native styling. Most languages use the same syntax. Single line comments will 
 use // and block/multiline comments should use /*..*/. An example is shown in Listing 
 1
 .
  
 Listing 1.
  A sample program using libpmemkv
  
  37  #include <iostream>
  
  38  #include ""libpmemkv.h""
  
  39  
  
  40  using namespace pmemkv;
  
  41  
  
  42  /*
  
  43   * kvprint -- print a single key-value pair
  
  44   */
  
  45  void kvprint(const string& k, const string& v) {
  
  46      std::cout << ""key: "" << k << "", value: "" << v << ""\n""; 47  }
  
 xxx",NA
 Notes,"We use a standard format for notes, cautions, and tips when we want to direct 
 your attention to an important point, for example.",NA
Note,NA,NA
" notes are tips, shortcuts, or alternative approaches to the current ",NA,NA
discussion topic. ignoring a note should have no negative ,NA,NA
"consequences, but you might miss out on a nugget of information that ",NA,NA
makes your life easier.,xxxi,NA
CHAPTER 1,NA,NA
Introduction to ,NA,NA
Persistent Memory ,NA,NA
Programming,"This book describes programming techniques for writing applications that use 
 persistent memory. It is written for experienced software developers, but we assume no 
 previous experience using persistent memory. We provide many code examples in a 
 variety of programming languages. Most programmers will understand these examples, 
 even if they have not previously used the specific language.",NA
Note,NA,NA
 All code examples are available on a GitHub repository ,NA,NA
(,NA,NA
https:// github.com/Apress/programming-persistent-memory,NA,NA
"), ",NA,NA
along with instructions for building and running it.,"Additional documentation for persistent memory, example programs, tutorials, and 
 details on the Persistent Memory Development Kit (PMDK), which is used heavily in this 
 book, can be found on 
 http://pmem.io
 .
  
 The persistent memory products on the market can be used in various ways, and 
 many of these ways are transparent to applications. For example, all persistent memory 
 products we encountered support the storage interfaces and standard file API’s just like 
 any solid-state disk (SSD). Accessing data on an SSD is simple and well-understood, so 
 we consider these use cases outside the scope of this book. Instead, we concentrate on 
 memory-style access, where applications manage byte-addressable data structures that 
 reside in persistent memory. Some use cases we describe are 
 volatile
 , using the 
 persistent memory only for its capacity and ignoring the fact it is persistent. However, 
 most of this book is dedicated to the 
 persistent
  use cases, where data structures placed in 
 persistent memory are expected to survive crashes and power failures, and the 
 techniques 
  
 described in this book keep those data structures consistent across those events.",NA
 A High-Level Example Program,"To illustrate how persistent memory is used, we start with a sample program 
  
 demonstrating the key-value store provided by a library called libpmemkv. Listing 
 1-1 
 shows a full C++ program that stores three key-value pairs in persistent memory and 
 then iterates through the key-value store, printing all the pairs. This example may seem 
 trivial, but there are several interesting components at work here. Descriptions below 
 the listing show what the program does.
  
 Listing 1-1.
  A sample program using libpmemkv
  
  
  37  #include <iostream>
  
  
  38  #include <cassert>
  
  
  39  #include <libpmemkv.hpp>
  
  
  40
  
  
  41  using namespace pmem::kv;
  
  
  42  using std::cerr;
  
  
  43  using std::cout;
  
  
  44  using std::endl;
  
  
  45  using std::string;
  
  
  46
  
  
  47  /*
  
  
  48   * for this example, create a 1 Gig file
  
  49   * called 
 ""/daxfs/kvfile""
  
  
  50   */
  
  
  51  auto PATH = ""/daxfs/kvfile"";
  
  
  52  const uint64_t SIZE = 1024 * 1024 * 1024;
  
  53
  
  
  54  /*
  
  
  55   * kvprint -- print a single key-value pair
  
  56   */
  
  
  57  int kvprint(string_view k, string_view v) {
  
  58      
 cout << ""key: ""    << k.data() <<
  
  
  59          "" value: "" << v.data() << endl;
  
  60      return 0;
  
  
  61  }
  
  
  62 
  
 2",NA
 What’s Different?,"A wide variety of key-value libraries are available in practically every programming 
 language. The persistent memory example in Listing 
 1-1
  is different because the key- 
 value store itself resides in persistent memory. For comparison, Figure 
 1-1
  shows how a 
 key-value store using traditional storage is laid out.
  
  
 Figure 1-1. 
 A key-value store on traditional storage
  
 When the application in Figure 
 1-1
  wants to fetch a value from the key-value store, a 
 buffer must be allocated in memory to hold the result. This is because the values are 
 kept on block storage, which cannot be addressed directly by the application. The only 
 way to access a value is to bring it into memory, and the only way to do that is to read 
 full blocks from the storage device, which can only be accessed via block I/O. Now 
 consider Figure 
 1-2
 , where the key-value store resides in persistent memory like our 
 sample code.
  
 5",NA
 The Performance Difference,"Moving a data structure from storage to persistent memory does not just mean smaller 
 I/O sizes are supported; there is a fundamental performance difference. To illustrate 
 this, Figure 
 1-3
  shows a hierarchy of latency among the different types of media where 
 data can reside at any given time in a program.
  
 6",NA
 Program Complexity,"Perhaps the most important point of our example is that the programmer still uses the 
 familiar get/put interfaces normally associated with key-value stores. The fact that the 
 data structures are in persistent memory is abstracted away by the high-level API 
 provided by libpmemkv. This principle of using the highest level of abstraction possible, 
 as long as it meets the application’s needs, will be a recurring theme throughout this 
 book. We start by introducing very high-level APIs; later chapters delve into the lower- 
 level details for programmers who need them. At the lowest level, programming directly 
 to raw persistent memory requires detailed knowledge of things like hardware 
 atomicity, cache flushing, and transactions. High-level libraries like libpmemkv abstract 
 away all that complexity and provide much simpler, less error-prone interfaces.
  
 7",NA
 How Does libpmemkv Work?,"All the complexity hidden by high-level libraries like libpmemkv are described more 
 fully in later chapters, but let’s look at the building blocks used to construct a library like 
 this. Figure 
 1-4
  shows the full software stack involved when an application uses 
 libpmemkv.
  
  
 Figure 1-4. 
 The software stack when using 
 libpmemkv
  
 Starting from the bottom of Figure 
 1-4
  and working upward are these components:
  
 • The 
 persistent memory
  hardware, typically connected to the system 
 memory bus and accessed using common memory load/store 
 operations.
  
 • A 
 pmem-aware file system
 , which is a kernel module that exposes 
 persistent memory to applications as files. Those files can be memory 
 mapped to give applications direct access (abbreviated as DAX). This 
 method of exposing persistent memory was published by SNIA (Storage 
 Networking Industry Association) and is described in detail in Chapter 
 3
 .
  
 • The libpmem library is part of the PMDK. This library abstracts 
 away some of the low-level hardware details like cache flushing 
 instructions.
  
 8",NA
 What’s Next?,"Chapters 
 1
  through 
 3
  provide the essential background that programmers need to know 
 to start persistent memory programming. The stage is now set with a simple example; 
 the next two chapters provide details about persistent memory at the hardware and 
 operating system levels. The later and more advanced chapters provide much more 
 detail for those interested.
  
 Because the immediate goal is to get you programming quickly, we recommend 
 reading Chapters 
 2
  and 
 3
  to gain the essential background and then dive into Chapter 
 4 
 where we start to show more detailed persistent memory programming examples.",NA
 Summary,"This chapter shows how high-level APIs like libpmemkv can be used for persistent 
 memory programming, hiding complex details of persistent memory from the 
 application developer. Using persistent memory can allow finer-grained access and 
 higher performance than block-based storage. We recommend using the highest-level, 
 simplest APIs possible and only introducing the complexity of lower-level persistent 
 memory programming as necessary.
  
 9",NA
CHAPTER 2,NA,NA
Persistent ,NA,NA
Memory ,NA,NA
Architecture,"This chapter provides an overview of the persistent memory architecture while focusing 
 on the hardware to emphasize requirements and decisions that developers need to 
 know.
  
 Applications that are designed to recognize the presence of persistent memory in 
 a system can run much faster than using other storage devices because data does not 
 have to transfer back and forth between the CPU and slower storage devices. Because 
 applications that only use persistent memory may be slower than dynamic random- 
 access memory (DRAM), they should decide what data resides in DRAM, persistent 
 memory, and storage.
  
 The capacity of persistent memory is expected to be many times larger than DRAM; 
 thus, the volume of data that applications can potentially store and process in place is 
 also much larger. This significantly reduces the number of disk I/Os, which improves 
 performance and reduces wear on the storage media.
  
 On systems without persistent memory, large datasets that cannot fit into DRAM 
 must be processed in segments or streamed. This introduces processing delays as the 
 application stalls waiting for data to be paged from disk or streamed from the network.
  
 If the working dataset size fits within the capacity of persistent memory and DRAM, 
 applications can perform in-memory processing without needing to checkpoint or page 
 data to or from storage. This significantly improves performance.",NA
 Persistent Memory Characteristics,"As with every new technology, there are always new things to consider. 
 Persistent memory is no exception. Consider these characteristics when 
 architecting and developing solutions:
  
 • Performance (throughput, latency, and bandwidth) of persistent 
 memory is much better than NAND but potentially slower than 
 DRAM.
  
 • Persistent memory is durable unlike DRAM. Its endurance is usually 
 orders of magnitude better than NAND and should exceed the lifetime 
 of the server without wearing out.
  
 • Persistent memory module capacities can be much larger than 
 DRAM DIMMs and can coexist on the same memory channels.
  
 • Persistent memory-enabled applications can update data in place 
 without needing to serialize/deserialize the data.
  
 • Persistent memory is byte addressable like memory. Applications 
 can update only the data needed without any read-modify-write 
 overhead.
  
 • Data is CPU cache coherent.
  
 • Persistent memory provides direct memory access (DMA) and 
 remote DMA (RDMA) operations.
  
 • Data written to persistent memory is not lost when power is removed. 
  
 • After permission checks are completed, data located on persistent 
 memory is directly accessible from user space. No kernel code, file 
 system page caches, or interrupts are in the data path.
  
 12",NA
 Platform Support for Persistent Memory,"Platform vendors such as Intel, AMD, ARM, and others will decide how persistent 
 memory should be implemented at the lowest hardware levels. We try to provide a 
 vendor-agnostic perspective and only occasionally call out platform-specific details.
  
 For systems with persistent memory, failure atomicity guarantees that systems can 
 always recover to a consistent state following a power or system failure. Failure 
 atomicity for applications can be achieved using logging, flushing, and memory store 
 barriers that order such operations. Logging, either undo or redo, ensures atomicity 
 when a failure interrupts the last atomic operation from completion. Cache flushing 
 ensures that data held within volatile caches reach the persistence domain so it will not 
 be lost if a sudden failure occurs. Memory store barriers, such as an SFENCE operation 
 on the x86 architecture, help prevent potential reordering in the memory hierarchy, as 
 caches and memory controllers may reorder memory operations. For example, a barrier 
 ensures that the undo log copy of the data gets persisted onto the persistent memory 
 before the actual data is modified in place. This guarantees that the last atomic 
 operation can be rolled back should a failure occur. However, it is nontrivial to add such 
 failure atomicity in user applications with low-level operations such as write logging, 
 cache flushing, and barriers. The Persistent Memory Development Kit (PMDK) was 
 developed to isolate developers from having to re-implement the hardware intricacies.
  
  
 Failure atomicity should be a familiar concept, since most file systems implement 
 and perform journaling and flushing of their metadata to storage devices.
  
 13",NA
 Cache Hierarchy,"We use load and store operations to read and write to persistent memory rather than 
 using block-based I/O to read and write to traditional storage. We suggest reading the 
 CPU architecture documentation for an in-depth description because each successive 
 CPU generation may introduce new features, methods, and optimizations.
  
 Using the Intel architecture as an example, a CPU cache typically has three distinct 
 levels: L1, L2, and L3. The hierarchy makes references to the distance from the CPU 
 core, its speed, and size of the cache. The L1 cache is closest to the CPU. It is extremely 
 fast but very small. L2 and L3 caches are increasingly larger in capacity, but they are 
 relatively slower. Figure 
 2-1
  shows a typical CPU microarchitecture with three levels 
 of CPU cache and a memory controller with three memory channels. Each memory 
 channel has a single DRAM and persistent memory attached. On platforms where the 
 CPU caches are not contained within the power-fail protected domain, any modified 
 data within the CPU caches that has not been flushed to persistent memory will be lost 
 when the system loses power or crashes.  Platforms that do include CPU caches in the 
 power-fail protected domain will ensure modified data within the CPU caches are 
 flushed to the persistent memory should the system crash or loses power. We describe 
 these requirements and features in the upcoming “Power-Fail Protected Domains” 
 section. 
  
 14",NA
 Power-Fail Protected Domains,"A computer system may include one or more CPUs, volatile or persistent memory 
 modules, and non-volatile storage devices such as SSDs or HDDs.
  
 System platform hardware supports the concept of a 
 persistence domain
 , also called 
 power-fail protected domains
 . Depending on the platform, a persistence domain may 
 include the persistent memory controller and write queues, memory controller write 
 queues, and CPU caches. Once data has reached the persistence domain, it may be 
 recoverable during a process that results from a system restart. That is, if data is located 
 within hardware write queues or buffers protected by power failure, domain 
 applications should assume it is persistent. For example, if a power failure occurs, the 
 data will be flushed",NA
" The Need for Flushing, Ordering, and ",NA,NA
Fencing,"Except for WBINVD, which is a kernel-mode-only operation, the machine instructions in 
 Table 
 2-1
  (in the “Intel Machine Instructions for Persistent Memory” section) are 
 supported in user space by Intel and AMD CPUs. Intel adopted the SNIA NVM 
 programming model for working with persistent memory. This model allows for direct 
 access (DAX) using byte-addressable operations (i.e., load/store). However, the 
 persistence of the data in the cache is not guaranteed until it has entered the persistence 
 domain. The x86 architecture provides a set of instructions for flushing cache lines in a 
 more optimized way. In addition to existing x86 instructions, such as non-temporal 
 stores, CLFLUSH, and WBINVD, two new instructions were added: CLFLUSHOPT and 
 CLWB. Both new instructions must be followed by an SFENCE to ensure all flushes are 
 completed before continuing. Flushing a cache line using CLWB, CLFLUSHOPT, or 
 CLFLUSH and using non-temporal stores are all supported from user space. You can find 
 details for each machine instruction in the software developer manuals for the 
 architecture. 
  
 On Intel platforms, for example, this information can be found in the Intel 64 and 32 
 Architectures Software Developer Manuals (
 https://software.intel.com/en-us/ 
 articles/intel-sdm
 ).
  
 Non-temporal stores imply that the data being written is not going to be read again 
 soon, so we bypass the CPU caches. That is, there is no 
 temporal locality
 , so there is no 
 benefit to keeping the data in the processor’s cache(s), and there may be a penalty if the 
 stored data displaces other useful data from the cache(s).
  
 Flushing to persistent memory directly from user space negates calling into the 
 kernel, which makes it highly efficient. The feature is documented in the SNIA persistent 
 memory programming model specification as an 
 optimized flush
 . The specification",NA
 Data Visibility,"When data is visible to other processes or threads, and when it is safe in the persistence 
 domain, is critical to understand when using persistent memory in applications. In the 
 Figure 
 2-2
  and 
 2-3
  examples, updates made to data in the CPU caches could become 
 visible to other processes or threads. Visibility and persistence are often not the same 
 thing, and changes made to persistent memory are often visible to other running threads 
 in the system before they are persistent. Visibility works the same way as it does for 
 normal DRAM, described by the memory model ordering and visibility rules for a given 
 platform (for example, see the Intel Software Development Manual for the visibility rules 
 for Intel platforms). Persistence of changes is achieved in one of three ways: either by 
 calling the standard storage API for persistence (msync on Linux or FlushFileBuffers on 
 Windows), by using optimized flush when supported, or by achieving visibility on a 
 platform where the CPU caches are considered persistent. This is one reason we use 
 flushing and fencing operations.
  
 A pseudo C code example may look like this:
  
 open()   // Open a file on a file system ...
  
 mmap()   // Memory map the file 
  
 ...
  
 strcpy() // Execute a store operation 
  
 ...      // Data is globally visible 
  
 msync()  // Data is now persistent 
  
 Developing for persistent memory follows this decades-old model. 
  
 23",NA
 Intel Machine Instructions for Persistent ,NA,NA
Memory,"Applicable to Intel- and AMD-based ADR platforms, executing an Intel 64 and 32 
 architecture store instruction is not enough to make data persistent since the data may 
 be sitting in the CPU caches indefinitely and could be lost by a power failure. Additional 
 cache flush actions are required to make the stores persistent. Importantly, these non- 
 privileged cache flush operations can be called from user space, meaning applications 
 decide when and where to fence and flush data. Table 
 2-1
  summarizes each of these 
 instructions. For more detailed information, the Intel 64 and 32 Architectures Software 
 Developer Manuals are online at 
 https://software.intel.com/en-us/articles/intel-sdm
 .
  
  
 Developers should primarily focus on CLWB and Non-Temporal Stores if available 
 and fall back to the others as necessary. Table 
 2-1
  lists other opcodes for completeness.
  
 Table 2-1. 
 Intel architecture instructions for persistent memory
  
 OPCODE
  
 Description
  
 CLFLUSH
  
 this instruction, supported in many generations of Cpu, flushes a single 
  
 cache line. historically, this instruction is serialized, causing 
 multiple CLFLush instructions to execute one after the other, 
 without any concurrency.
  
 CLFLUSHOPT 
  
 this instruction, newly introduced for persistent memory support, is 
 like 
 (followed by an 
  
 CLFLUSH but without the serialization. to flush a range, the 
 software executes a 
 SFENCE) 
  
 CLFLUSHOPT instruction for each 64-byte cache line in 
 the range, followed by a 
  
 single SFENCE instruction to ensure the flushes are 
 complete before continuing. 
  
 CLFLUSHOPT is optimized, hence the name, to allow some 
 concurrency when executing multiple CLFLUSHOPT instructions 
 back-to-back.
  
 CLWB (followed by 
 the effect of cache line writeback (CLWB) is the same as 
 CLFLUSHOPT except 
 an SFENCE) 
  
 that the cache line may remain valid in the cache 
 but is no longer dirty since it 
  
 was flushed. this makes it more likely to get a cache hit 
 on this line if the data 
  
 is accessed again later.
  
 Non-temporal 
  
 this feature has existed for a while in x86 Cpus. these stores are 
 “write 
 stores (followed 
  
 combining” and bypass the Cpu cache; using them does not 
 require a flush. a 
 by an SFENCE) 
  
 final SFENCE instruction is still required to 
 ensure the stores have reached the 
  
 persistence domain.",NA
 Detecting Platform Capabilities,"Server platform, CPU, and persistent memory features and capabilities are exposed to 
 the operating system through the BIOS and ACPI that can be queried by applications. 
 Applications should not assume they are running on hardware with all the optimizations 
 available. Even if the physical hardware supports it, virtualization technologies may or 
 may not expose those features to the guests, or your operating system may or may not 
 implement them. As such, we encourage developers to use libraries, such as those in the 
 PMDK, that perform the required feature checks or implement the checks within the 
 application code base.",NA
 Application Startup and Recovery,"In addition to detecting platform features, applications should verify whether the 
 platform was previously stopped and restarted gracefully or ungracefully. Figure 
 2-
 6 
 shows the checks performed by the Persistent Memory Development Kit. 
  
 Some persistent memory devices, such as Intel Optane DC persistent memory, 
 provide SMART counters that can be queried to check the health and status. Several 
 libraries such as libpmemobj query the BIOS, ACPI, OS, and persistent memory module 
 information then perform the necessary validation steps to decide which flush operation 
 is most optimal to use.
  
 We described earlier that if a system loses power, there should be enough stored 
 energy within the power supplies and platform to successfully flush the contents of the 
 memory controller’s WPQ and the write buffers on the persistent memory devices. 
  
 Data will be considered consistent upon successful completion. If this process fails, due 
 to exhausting all the stored energy before all the data was successfully flushed, the 
 persistent memory modules will report a 
 dirty shutdown
 . A dirty shutdown indicates 
 that data on the device may be inconsistent. This may or may not result in needing to 
 restore the data from backups. You can find more information on this process – and 
 what errors and signals are sent – in the RAS (reliability, availability, serviceability) 
 documentation for your platform and the persistent memory device. Chapter 
 17
  also 
 discusses this further.
  
 Assuming no dirty shutdown is indicated, the application should check to see if 
 the persistent memory media is reporting any known poison blocks (see Figure 
 2-
 6
 ). Poisoned blocks are areas on the physical media that are known to be bad.
  
 27",NA
 What’s Next?,"Chapter 
 3
  continues to provide foundational information from the perspective of the 
 kernel and user spaces. We describe how operating systems such as Linux and Windows 
 have adopted and implemented the SNIA non-volatile programming model that defines 
 recommended behavior between various user space and operating system kernel 
 components supporting persistent memory. Later chapters build on the foundations 
 provided in Chapters 
 1
  through 
 3
 .",NA
 Summary,"This chapter defines persistent memory and its characteristics, recaps how CPU caches 
 work, and describes why it is crucial for applications directly accessing persistent 
 memory to assume responsibility for flushing CPU caches. We focus primarily on 
 hardware implementations. User libraries, such as those delivered with the PMDK, 
 assume the responsibilities for architecture and hardware-specific operations and allow 
 developers to use simple APIs to implement them. Later chapters describe the PMDK 
 libraries in more detail and show how to use them in your application.
  
 29",NA
CHAPTER 3,NA,NA
Operating System ,NA,NA
Support for Persistent ,NA,NA
Memory,"This chapter describes how operating systems manage persistent memory as a platform 
 resource and describes the options they provide for applications to use persistent 
 memory. We first compare memory and storage in popular computer architectures and 
 then describe how operating systems have been extended for persistent memory.",NA
 Operating System Support for Memory and ,NA,NA
Storage,"Figure 
 3-1
  shows a simplified view of how operating systems manage storage and volatile 
 memory. As shown, the volatile main memory is attached directly to the CPU through a 
 memory bus. The operating system manages the mapping of memory regions directly into 
 the application’s visible memory address space. Storage, which usually operates at speeds 
 much slower than the CPU, is attached through an I/O controller. The operating system 
 handles access to the storage through device driver modules loaded into the operating 
 system’s I/O subsystem.",NA
 Persistent Memory As Block Storage,"The first operating system extension for persistent memory is the ability to detect 
 the existence of persistent memory modules and load a device driver into the 
 operating system’s I/O subsystem as shown in Figure 
 3-2
 . This NVDIMM driver 
 serves two important functions. First, it provides an interface for management and 
 system administrator utilities to configure and monitor the state of the persistent 
 memory hardware. Second, it functions similarly to the storage device drivers.
  
  
 Figure 3-2. 
 Persistent memory as block storage
  
 The NVDIMM driver presents persistent memory to applications and operating 
 system modules as a fast block storage device. This means applications, file systems, 
 volume managers, and other storage middleware layers can use persistent memory 
 the same way they use storage today, without modifications.
  
 33",NA
 Persistent Memory-Aware File Systems,"The next extension to the operating system is to make the file system aware of and be 
 optimized for persistent memory. File systems that have been extended for persistent 
 memory include Linux ext4 and XFS, and Microsoft Windows NTFS. As shown in Figure 
 3-3
 , these file systems can either use the block driver in the I/O subsystem (as described 
 in the previous section) or bypass the I/O subsystem to directly use persistent memory 
 as byte-addressable load/store memory as the fastest and shortest path to data stored in 
 persistent memory. In addition to eliminating the I/O operation, this path enables small 
 data writes to be executed faster than traditional block storage devices that require the 
 file system to read the device’s native block size, modify the block, and then write the full 
 block back to the device.
  
 34",NA
 Memory-Mapped Files,"Before describing the next operating system option for using persistent memory, this 
 section reviews memory-mapped files in Linux and Windows. When memory mapping 
 a file, the operating system adds a range to the application’s virtual address space 
 which corresponds to a range of the file, paging file data into physical memory as 
 required. This allows an application to access and modify file data as byte-addressable 
 in-memory data structures. This has the potential to improve performance and 
 simplify application development, especially for applications that make frequent, small 
 updates to file data.
  
 Applications memory map a file by first opening the file, then passing the resulting 
 file handle as a parameter to the mmap() system call in Linux or to MapViewOfFile() in 
 Windows. Both return a pointer to the in-memory copy of a portion of the file. Listing 
 3-
 1 
 shows an example of Linux C code that memory maps a file, writes data into the file by 
 accessing it like memory, and then uses the msync system call to perform the I/O 
  
 35",NA
 Persistent Memory Direct Access (DAX),"The persistent memory direct access feature in operating systems, referred to as DAX 
 in Linux and Windows, uses the memory-mapped file interfaces described in the 
 previous section but takes advantage of persistent memory’s native ability to both 
 store data and to be used as memory. Persistent memory can be natively mapped as 
 application memory, eliminating the need for the operating system to cache files in 
 volatile main memory.
  
 To use DAX, the system administrator creates a file system on the persistent memory 
 module and mounts that file system into the operating system’s file system tree. For 
 Linux users, persistent memory devices will appear as /dev/pmem* device special files. 
 To show the persistent memory physical devices, system administrators can use the 
 ndctl and ipmctl utilities shown in Listings 
 3-3
  and 
 3-4
 .
  
 43",NA
 Summary,"Figure 
 3-6
  shows the complete view of the operating system support that this chapter 
 describes. As we discussed, an application can use persistent memory as a fast SSD, 
 more directly through a persistent memory-aware file system, or mapped directly into 
 the application’s memory space with the DAX option. DAX leverages operating system 
 services for memory-mapped files but takes advantage of the server hardware’s ability 
 to map persistent memory directly into the application’s address space. This avoids the 
 need to move data between main memory and storage. The next few chapters describe 
 considerations for working with data directly in persistent memory and then discuss the 
 APIs for simplifying development.
  
 53",NA
CHAPTER 4,NA,NA
Fundamental ,NA,NA
Concepts of ,NA,NA
Persistent Memory ,NA,NA
Programming,"In Chapter 
 3
 , you saw how operating systems expose persistent memory to applications 
 as memory-mapped files. This chapter builds on this fundamental model and examines 
 the programming challenges that arise. Understanding these challenges is an essential 
 part of persistent memory programming, especially when designing a strategy for 
 recovery after application interruption due to issues like crashes and power failures. 
 However, do not let these challenges deter you from persistent memory programming! 
  
 Chapter 
 5
  describes how to leverage existing solutions to save you programming 
 time and reduce complexity.",NA
 What’s Different?,"Application developers typically think in terms of 
 memory-resident
  data structures and 
 storage-resident
  data structures. For data center applications, developers are careful to 
 maintain consistent data structures on storage, even in the face of a system crash. This 
 problem is commonly solved using logging techniques such as 
 write-ahead logging
 , 
 where changes are first written to a log and then flushed to persistent storage. If the 
 data modification process is interrupted, the application has enough information in the 
 log to finish the operation on restart. Techniques like this have been around for many 
 years; however, correct implementations are challenging to develop and time-
 consuming to maintain. Developers often rely on a combination of databases, libraries, 
 and modern file systems to provide consistency. Even so, it is ultimately the application 
 developer’s",NA
 Atomic Updates,"Each platform supporting persistent memory will have a set of 
 native
  memory 
 operations that are atomic. On Intel hardware, the atomic persistent store is 8 bytes. 
  
 Thus, if the program or system crashes while an aligned 8-byte store to persistent 
 memory is in-flight, on recovery those 8 bytes will either contain the old contents or 
 the new contents. The Intel processor has instructions that store more than 8 bytes, 
 but those are not failure atomic, so they can be 
 torn
  by events like a power failure. 
  
 56",NA
 Transactions,"Combining multiple operations into a single atomic operation is usually referred to as 
 a 
 transaction
 . In the database world, the acronym ACID describes the properties of a 
 transaction: 
 atomicity, consistency, isolation, and durability
 .",NA
 Atomicity,"As described earlier, atomicity is when multiple operations are composed into a single 
 atomic action that either happens entirely or does not happen at all, even in the face of 
 system failure. For persistent memory, the most common techniques used are
  
 • Redo logging, where the full change is first written to a log, so during 
 recovery, it can be 
 rolled forward
  if interrupted.
  
 • Undo logging, where information is logged that allows a partially 
 done change to be 
 rolled back
  during recovery.
  
 • Atomic pointer updates, where a change is made active by updating a 
 single pointer atomically, usually changing it from pointing to old data 
 to new data.
  
 The preceding list is not exhaustive, and it ignores the details that can get relatively 
 complex. One common consideration is that transactions often include memory 
 allocation/deallocation. For example, a transaction that adds a node to a tree data 
 structure usually includes the allocation of the new node. If the transaction is rolled 
 back, the memory must be freed to prevent a memory leak. Now imagine a transaction 
 that performs multiple persistent memory allocations and free operations, all of which 
 must be part of the same atomic operation. The implementation of this transaction is 
 clearly more complex than just writing the new value to a log or updating a single 
 pointer.
  
 57",NA
 Consistency,"Consistency means that a transaction can only move a data structure from one valid state 
 to another. For persistent memory, programmers usually find that the locking they use 
 to make updates thread-safe often indicates consistency points as well. If it is not valid 
 for a thread to see an intermediate state, locking prevents it from happening, and when it 
 is safe to drop the lock, that is because it is safe for another thread to observe the current 
 state of the data structure.",NA
 Isolation,"Multithreaded (concurrent) execution is commonplace in modern applications. When 
 making transactional updates, the isolation is what allows the concurrent updates to 
 have the same effect as if they were executed sequentially. At runtime, isolation for 
 persistent memory updates is typically achieved by locking. Since the memory is 
 persistent, the isolation must be considered for transactions that were in-flight when 
 the application was interrupted. Persistent memory programmers typically detect this 
 situation on restart and roll partially done transactions forward or backward 
 appropriately before allowing general-purpose threads access to the data structures.",NA
 Durability,"A transaction is considered durable if it is on persistent media when it is complete. Even 
 if the system loses power or crashes at that point, the transaction remains completed. As 
 described in Chapter 
 2
 , this usually means the changes must be flushed from the CPU 
 caches. This can be done using standard APIs, such as the Linux msync() call, or 
 platform-specific instructions such as Intel’s CLWB. When implementing transactions on 
 persistent memory, pay careful attention to ensure that log entries are flushed to 
 persistence before changes are started and flush changes to persistence before a 
 transaction is considered complete.
  
 Another aspect of the durable property is the ability to find the persistent 
  
 information again when an application starts up. This is so fundamental to how storage 
 works that we take it for granted. Metadata such as file names and directory names are 
 used to find the durable state of an application on storage. For persistent memory, the 
 same is true due to the programming model described in Chapter 
 3
 , where persistent 
 memory is accessed by first opening a file on a direct access (DAX) file system and then 
 memory mapping that file. However, a memory-mapped file is just a range of raw data; 
  
 58",NA
 Flushing Is Not Transactional,"It is important to separate the ideas of flushing to persistence from transactional 
  
 updates. Flushing changes to storage using calls like msync() or fsync() on Linux and 
 FlushFileBuffers() on Windows have never provided transactional updates. Applications 
 assume the responsibility for maintaining consistent storage data structures in addition 
 to flushing changes to storage. With persistent memory, the same is true. In Chapter 
 3
 , a 
 simple program stored a string to persistent memory and then flushed it to make sure 
 the change was persistent. But that code was not transactional, and in the face of failure, 
 the change could be in just about any state – from completely lost to partially lost to fully 
 completed.
  
  
 A fundamental property of caches is that they hold data temporarily for 
  
 performance, but they do not typically hold data until a transaction is ready to commit. 
  
 Normal system activity can cause cache pressure and evict data at any time and in any 
 order. If the examples in Chapter 
 3
  were interrupted by power failure, it is possible for 
 any part of the string being stored to be lost and any part to be persistent, in any order. 
 It is important to think of the cache flush operation as 
 flush anything that hasn’t already 
 been flushed
  and not as 
 flush all my changes now
 .
  
 Finally, we showed a decision tree in Chapter 
 2
  (Figure 
 2-5
 ) where an application 
 can determine at startup that no cache flushing is required for persistent memory. This 
 can be the case on platforms where the CPU cache is flushed automatically on power 
 failure, for example. Even on platforms where flush instructions are not needed, 
 transactions are still required to keep data structures consistent in the face of failure.",NA
 Start-Time Responsibilities,"In Chapter 
 2
  (Figures 
 2-5
  and 
 2-6
 ), we showed flowcharts outlining the application’s 
 responsibilities when using persistent memory. These responsibilities included 
 detecting platform details, available instructions, media failures, and so on. For storage, 
 these types of things happen in the storage stack in the operating system. Persistent 
  
 59",NA
 Tuning for Hardware Configurations,"When storing a large data structure to persistent memory, there are several ways to 
 copy the data and make it persistent. You can either copy the data using the common 
 store operations and then flush the caches (if required) or use special instructions like 
 Intel’s non-temporal store instructions that bypass the CPU caches. Another 
 consideration is that persistent memory write performance may be slower than writing 
 to normal memory, so you may want to take steps to store to persistent memory as 
 efficiently as possible, by combining multiple small writes into larger changes before 
 storing them to persistent memory. The optimal write size for persistent memory will 
 depend on both the platform it is plugged into and the persistent memory product itself. 
 These examples show that different platforms will have different characteristics when 
 using persistent memory, and any production-quality application will be tuned to 
 perform best on the intended target platforms. Naturally, one way to help with this 
 tuning work is to leverage libraries or middleware that has already been tuned and 
 validated.",NA
 Summary,"This chapter provides an overview of the fundamental concepts of persistent memory 
 programming. When developing an application that uses persistent memory, you must 
 carefully consider several areas:
  
 • Atomic updates.
  
 • Flushing is not transactional.
  
 60",NA
CHAPTER 5,NA,NA
Introducing the ,NA,NA
Persistent Memory ,NA,NA
Development Kit,"Previous chapters introduced the unique properties of persistent memory that make it 
 special, and you are correct in thinking that writing software for such a novel technology 
 is complicated. Anyone who has researched or developed code for persistent memory 
 can testify to this. To make your job easier, Intel created the Persistent Memory 
  
 Development Kit (PMDK). The team of PMDK developers envisioned it to be the 
 standard library for all things persistent memory that would provide solutions to the 
 common challenges of persistent memory programming.",NA
 Background,"The PMDK has evolved to become a large collection of open source libraries and tools 
 for application developers and system administrators to simplify managing and 
 accessing persistent memory devices. It was developed alongside evolving support for 
 persistent memory in operating systems, which ensures the libraries take advantage of 
 all the features exposed through the operating system interfaces.
  
 The PMDK libraries build on the SNIA NVM programming model (described in 
 Chapter 
 3
 ). They extend it to varying degrees, some by simply wrapping around the 
 primitives exposed by the operating system with easy-to-use functions and others by 
 providing complex data structures and algorithms for use with persistent memory. 
 This means you are responsible for making an informed decision about which level of 
 abstraction is the best for your use case.",NA
 Choosing the Right Semantics,"With so many libraries available within the PMDK, it is important to carefully consider 
 your options. The PMDK offers two library categories:
  
  1. 
 Volatile
  libraries are for use cases that only wish to exploit the 
  
 capacity of persistent memory.
  
  2. 
 Persistent
  libraries are for use in software that wishes to 
  
 implement fail-safe persistent memory algorithms.
  
 While you are deciding how to best solve a problem, carefully consider which 
 category it fits into. The challenges that fail-safe persistent programs present are 
 significantly different from volatile ones. Choosing the right approach upfront will 
 minimize the risk of having to rewrite any code.
  
  
 You may decide to use libraries from both categories for different parts of the 
 application, depending on feature and functional requirements.
  
 64",NA
 Volatile Libraries,"Volatile libraries are typically simpler to use because they can fall back to dynamic 
 random-access memory (DRAM) when persistent memory is not available. This 
 provides a more straightforward implementation. Depending on the workload, they may 
 also have lower overall overhead compared to similar persistent libraries because they 
 do not need to ensure consistency of data in the presence of failures.
  
 This section explores the available libraries for volatile use cases in 
 applications, including what the library is and when to use it. The libraries may 
 have overlapping situation use cases.",NA
 libmemkind,"What is it?
  
 The memkind library, called libmemkind, is a user-extensible heap manager built on 
 top of jemalloc. It enables control of memory characteristics and partitioning of the heap 
 between different kinds of memory. The kinds of memory are defined by operating 
 system memory policies that have been applied to virtual address ranges. Memory 
 characteristics supported by memkind without user extension include control of 
 nonuniform memory access (NUMA) and page size features. The jemalloc nonstandard 
 interface has been extended to enable specialized kinds to make requests for virtual 
 memory from the operating system through the memkind partition interface. Through 
 the other memkind interfaces, you can control and extend memory partition features 
 and allocate memory while selecting enabled features. The memkind interface allows 
 you to create and control file-backed memory from persistent memory with PMEM kind.
  
  
  Chapter 
 10
  describes this library in more detail. You can download memkind and 
 read the architecture specification and API documentation at 
 http://memkind.github.
  
 io/memkind/
 . memkind is an open source project on GitHub at 
 https://github.com/ 
 memkind/memkind
 .
  
 When to use it?
  
 Choose libmemkind when you want to manually move select memory objects to 
 persistent memory in a volatile application while retaining the traditional programming 
 model. The memkind library provides familiar malloc() and free() semantics. This is the 
 recommended memory allocator for most volatile use cases of persistent memory.
  
 65",NA
 libvmemcache,"What is it?
  
 libvmemcache is an embeddable and lightweight in-memory caching solution that 
 takes full advantage of large-capacity memory, such as persistent memory with direct 
 memory access (DAX), through memory mapping in an efficient and scalable way. 
  
 libvmemcache has unique characteristics:
  
 • An extent-based memory allocator sidesteps the fragmentation problem 
 that affects most in-memory databases and allows the cache to achieve 
 very high space utilization for most workloads.
  
 • The buffered least recently used (LRU) algorithm combines a 
 traditional LRU doubly linked list with a non-blocking ring buffer to 
 deliver high degrees of scalability on modern multicore CPUs.
  
 • The critnib indexing structure delivers high performance while 
 being very space efficient.
  
 The cache is tuned to work optimally with relatively large value sizes. The smallest 
 possible size is 256 bytes, but libvmemcache works best if the expected value sizes are 
 above 1 kilobyte.
  
  
 Chapter 
 10
  describes this library in more detail. libvmemcache is an open source 
 project on GitHub at 
 https://github.com/pmem/vmemcache
 .
  
 66",NA
 libvmem,"What is it?
  
 libvmem is a deprecated predecessor to libmemkind. It is a jemalloc-derived 
 memory allocator, with both metadata and objects allocations placed in file-based 
 mapping. The libvmem library is an open source project available from 
 https://pmem. io/pmdk/libvmem/
 .
  
 When to use it?
  
 Use libvmem only if you have an existing application that uses libvmem or if you 
 need to have multiple completely separate heaps of memory. Otherwise, consider using 
 libmemkind.",NA
 Persistent Libraries,"Persistent libraries help applications maintain data structure consistency in the 
 presence of failures. In contrast to the previously described volatile libraries, these 
 provide new semantics and take full advantage of the unique possibilities enabled by 
 persistent memory.",NA
 libpmem,"What is it?
  
 libpmem is a low-level C library that provides basic abstraction over the primitives 
 exposed by the operating system. It automatically detects features available in the 
 platform and chooses the right durability semantics and memory transfer (memcpy()) 
 methods optimized for persistent memory. Most applications will need at least parts of 
 this library.
  
 67",NA
 libpmemobj,"What is it?
  
 libpmemobj is a C library that provides a transactional object store, with a manual 
 dynamic memory allocator, transactions, and general facilities for persistent memory 
 programming. This library solves many of the commonly encountered algorithmic and 
 data structure problems when programming for persistent memory. Chapter 
 7
  describes 
 this library in detail.
  
 When to use it?
  
 Use libpmemobj when the programming language of choice is C and when you need 
 flexibility in terms of data structures design but can use a general-purpose memory 
 allocator and transactions.",NA
 libpmemobj-cpp,"What is it?
  
 libpmemobj-cpp, also known as libpmemobj++, is a C++ header-only library that 
 uses the metaprogramming features of C++ to provide a simpler, less error-prone 
 interface to libpmemobj. It enables rapid development of persistent memory 
 applications by reusing many concepts C++ programmers are already familiar with, such 
 as smart pointers and closure-based transactions.
  
 This library also ships with custom-made, STL-compatible data structures and 
 containers, so that application developers do not have to reinvent the basic algorithms 
 for persistent memory.
  
 68",NA
 libpmemkv,"What is it?
  
 libpmemkv is a generic embedded local key-value store optimized for persistent 
 memory. It is easy to use and ships with many different language integrations, including 
 C, C++, and JavaScript.
  
 This library has a pluggable back end for different storage engines. Thus, it can 
 be used as a volatile library, although it was originally designed primarily to 
 support persistent use cases.
  
 Chapter 
 9
  describes this library in detail.
  
 When to use it?
  
 This library is the recommended starting point into the world of persistent 
 memory programming because it is approachable and has a simple interface. Use it 
 when complex and custom data structures are not needed and a generic key-value 
 store interface is enough to solve the current problem.",NA
 libpmemlog,"What is it?
  
  
 libpmemlog is a C library that implements a persistent memory append-only log file 
 with power fail-safe operations.
  
 When to use it?
  
  
 Use libpmemlog when your use case exactly fits into the provided log API; otherwise, 
 a more generic library such as libpmemobj or libpmemobj-cpp might be more useful.",NA
 libpmemblk,"What is it?
  
  
 libpmemblk is a C library for managing fixed-size arrays of blocks. It provides fail-
 safe interfaces to update the blocks through buffer-based functions.
  
 69",NA
 Tools and Command Utilities ,"PMDK comes with a wide variety of tools and utilities to assist in the development and 
 deployment of persistent memory applications.",NA
 ,NA,NA
pmempo,NA,NA
ol ,"What is it?
  
  
 The pmempool utility is a tool for managing and offline analysis of persistent 
 memory pools. Its variety of functionalities, useful throughout the entire life cycle of an 
 application, include
  
  
  
 • 
  
 Obtaining information and statistics from a memory pool 
  
  
   
 Checking a memory pool’s consistency and repairing it if possible• 
  
  
  
 • 
  
  
 Creating memory pools
  
  
 • 
  
 Removing/deleting a previously created 
 memory pool
  
  
  
 • 
  
 Updating internal metadata to the latest layout version 
  
  
   
 Synchronizing replicas within a poolset• 
  
  
  
 • 
  
  
 Modifying internal data structures within a poolset
  
  
 • 
  
 Enabling 
 or disabling pool and poolset features 
  
 When to use it?
  
  
 Use pmempool whenever you are creating persistent memory pools for 
 applications using any of the persistent libraries from PMDK.",NA
 ,NA,NA
pmemchec,NA,NA
k ,"What is it?
  
 The pmemcheck utility is a Valgrind-based tool for dynamic runtime analysis 
 of common persistent memory errors, such as a missing flush or incorrect use of 
 transactions. Chapter 
 12
  describes this utility in detail.
  
 70",NA
 pmreorder,"What is it?
  
 The pmreorder utility helps detect data structure consistency problems of persistent 
 applications in the presence of failures. It does this by first recording and then replaying 
 the persistent state of the application while verifying consistency of the application’s 
 data structures at any possible intermediate state. Chapter 
 12
  describes this utility in 
 detail.
  
 When to use it?
  
 Just like pmemcheck, pmreorder is an essential tool for finding hard-to-debug 
 persistent problems and should be integrated into the development and testing cycle of 
 any persistent memory application.",NA
 Summary,"This chapter provides a brief listing of the libraries and tools available in PMDK and 
 when to use them. You now have enough information to know what is possible. 
 Throughout the rest of this book, you will learn how to create software using these 
 libraries and tools.
  
  
 The next chapter introduces libpmem and describes how to use it to create 
 simple persistent applications.
  
 71",NA
CHAPTER 6,NA,NA
libpmem: Low-,NA,NA
Level Persistent ,NA,NA
Memory Support,"This chapter introduces libpmem, one of the smallest libraries in PMDK. This C library is 
 very low level, dealing with things like CPU instructions related to persistent memory, 
 optimal ways to copy data to persistence, and file mapping. Programmers who only 
 want completely raw access to persistent memory, without libraries to provide 
 allocators or transactions, will likely want to use libpmem as a basis for their 
 development.
  
  
 The code in libpmem that detects the available CPU instructions, for example, is a 
 mundane boilerplate code that you do not want to invent repeatedly in applications. 
  
 Leveraging this small amount of code from libpmem will save time, and you get 
 the benefit of fully tested and tuned code in the library.
  
 For most programmers, libpmem is too low level, and you can safely skim this 
 chapter quickly (or skip it altogether) and move on to the higher-level, friendlier 
 libraries available in PMDK. All the PMDK libraries that deal with persistence, such as 
 libpmemobj, are built on top of libpmem to meet their low-level needs.
  
 Like all PMDK libraries, online man pages are available. For libpmem, they are at 
 http://pmem.io/pmdk/libpmem/
 . This site includes links to the man pages for both the 
 Linux and Windows version. Although the goal of the PMDK project was to make the 
 interfaces similar across operating systems, some small differences appear as necessary. 
  
 The C code examples used in this chapter build and run on both Linux and Windows.
  
 © The Author(s) 2020 
  
 73
  
 S. Scargall, 
 Programming Persistent Memory
 , 
 https://doi.org/10.1007/978-1-4842-4932-1_6",NA
 Using the Library,"To use libpmem, start by including the appropriate header, as shown in Listing 
 6-1
 .
  
 Listing 6-1.
  Including the libpmem headers
  
  32
  
  33  /*
  
  34   * simple_copy.c
  
  35   *
  
  36   * usage: simple_copy src-file dst-file
  
  37   *
  
  38   * Reads 4KiB from src-file and writes it to dst-file.
  
  39   */
  
  40
  
  41  #include <sys/types.h>
  
  42  #include <sys/stat.h>
  
  43  #include <fcntl.h>
  
  44  #include <stdio.h>
  
  45  #include <errno.h>
  
  46  #include <stdlib.h>
  
  47  #ifndef _WIN32
  
  48  #include <unistd.h>
  
  49  #else
  
  50  #include <io.h>
  
  51  #endif
  
  52  #include <string.h>
  
  53  #include <libpmem.h>
  
 74",NA
 Mapping a File,"The libpmem library contains some convenience functions for memory mapping files. 
 Of course, your application can call mmap() on Linux or MapViewOfFile() on Windows 
 directly, but using libpmem has some advantages:
  
 • libpmem knows the correct arguments to the operating system 
 mapping calls. For example, on Linux, it is not safe to flush changes to 
 persistent memory using the CPU instructions directly unless the 
 mapping is created with the MAP_SYNC flag to mmap().
  
 • libpmem detects if the mapping is actually persistent memory and if 
 using the CPU instructions directly for flushing is safe.
  
  
 Listing 
 6-2
  shows how to memory map a file on a persistent memory-aware file 
 system into the application.
  
 Listing 6-2.
  Mapping a persistent memory file
  
  80      /* create a pmem file and memory map it */
  
  81      if ((pmemaddr = pmem_map_file(argv[2], BUF_LEN, 82              
 PMEM_FILE_CREATE|PMEM_FILE_EXCL,
  
  83              0666, &mapped_len, &is_pmem)) == NULL) { 84          
 perror(""pmem_map_file"");
  
  85          exit(1);
  
  86      }
  
 As part of the persistent memory detection mentioned earlier, the flag is_pmem is 
 returned by pmem_map_file. It is the caller’s responsibility to use this flag to determine 
 how to flush changes to persistence. When making a range of memory persistent, the 
 caller can use the optimal flush provided by libpmem, pmem_persist, only if the 
 is_pmem flag is set. This is illustrated in the man page example excerpt in Listing 
 6-3
 .
  
 75",NA
 Copying to Persistent Memory,"There are several interfaces in libpmem for optimally copying or zeroing ranges of 
 persistent memory. The simplest interface shown in Listing 
 6-4
  is used to copy the block 
 of data from the source file to the persistent memory in the destination file and flush it 
 to persistence.
  
 Listing 6-4.
  simple_copy.c: Copying to persistent memory
  
  
  88      /* read up to BUF_LEN from srcfd */
  
  
  89      if ((cc = read(srcfd, buf, BUF_LEN)) < 0) {
  
  90          
 pmem_unmap(pmemaddr, mapped_len);
  
  
  91          perror(""read"");
  
  
  92          exit(1);
  
  
  93      }
  
  
  94
  
  
  95      /* write it to the pmem */
  
  
  96      if (is_pmem) {
  
  
  97          pmem_memcpy_persist(pmemaddr, buf, cc);
  
  98      
 } else {
  
  
  99          memcpy(pmemaddr, buf, cc);
  
  100          pmem_msync(pmemaddr, cc);
  
  101      }
  
 76",NA
 Separating the Flush Steps,"Flushing to persistence involves two steps:
  
  1. Flush the CPU caches or bypass them entirely as explained in the 
 previous example.
  
  2. Wait for any hardware buffers to drain, to ensure writes have 
 reached the media.
  
 These steps are performed together when pmem_persist() is called, or they can be 
 called individually by calling pmem_flush() for the first step and pmem_drain() for the 
 second. Note that either of these steps may be unnecessary on a given platform, and 
 the library knows how to check for that and do what is correct. For example, on Intel 
 platforms, pmem_drain is an empty function.
  
 When does it make sense to break flushing into steps? The example in Listing 
 6-5 
 illustrates one reason you might want to do this. Since the example copies data using 
 multiple calls to memcpy(), it uses the version of libpmem copy 
 (pmem_memcpy_nodrain()) that only performs the flush, postponing the final drain step 
 to the end. This works because, unlike the flush step, the drain step does not take an 
 address range; it is a system-wide drain operation so can happen at the end of the loop 
 that copies individual blocks of data.
  
 Listing 6-5.
  full_copy.c: Separating the flush steps
  
  58  /*
  
  59   * do_copy_to_pmem
  
  60   */
  
  61  static void
  
  62  do_copy_to_pmem(char *pmemaddr, int srcfd, off_t len)
  
 77",NA
 Summary,"This chapter demonstrated some of the fairly small set of APIs provided by libpmem. 
  
 This library does not track what changed for you, does not provide power fail-safe 
 transactions, and does not provide an allocator. Libraries like libpmemobj (described 
 in the next chapter) provide all those tasks and use libpmem internally for simple 
 flushing and copying.
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 79",NA
CHAPTER 7,NA,NA
libpmemobj: A Native ,NA,NA
Transactional Object ,NA,NA
Store,"In the previous chapter, we described libpmem, the low-level persistent memory library 
 that provides you with an easy way to directly access persistent memory. libpmem is a 
 small, lightweight, and feature-limited library that is designed for software that tracks 
 every store to pmem and needs to flush those changes to persistence. It excels at what it 
 does. However, most developers will find higher-level libraries within the Persistent 
 Memory Development Kit (PMDK), like libpmemobj, to be much more convenient. 
  
 This chapter describes libpmemobj, which builds upon libpmem and turns 
 persistent memory-mapped files into a flexible object store. It supports transactions, 
 memory management, locking, lists, and several other features.",NA
 What is libpmemobj?,"The libpmemobj library provides a transactional object store in persistent memory for 
 applications that require transactions and persistent memory management using direct 
 access (DAX) to the memory. Briefly recapping our DAX discussion in Chapter 
 3
 , DAX 
 allows applications to memory map files on a persistent memory-aware file system to 
 provide direct load/store operations without paging blocks from a block storage device. 
 It bypasses the kernel, avoids context switches and interrupts, and allows applications 
 to read and write directly to the byte-addressable persistent storage.",NA
 Why not malloc( )?,"Using libpmem seems simple. You need to flush anything you have written and use 
 discipline when ordering such that data needs to be persisted before any pointers to it 
 go live.
  
 If only persistent memory programming were so simple. Apart from some specific 
 patterns that can be done in a simpler way, such as append-only records that can be 
 efficiently handled by libpmemlog, any new piece of data needs to have its memory 
 allocated. When and how should the allocator mark the memory as in use? Should the 
 allocator mark the memory as allocated before writing data or after? Neither approach 
 works for these reasons:
  
 • If the allocator marks the memory as allocated before the data is 
 written, a power outage during the write can cause torn updates and a 
 so-called “persistent leak.”
  
 • If the allocator writes the data, then marks it as allocated, a power 
 outage that occurs between the write completing and the allocator 
 marking it as allocated can overwrite the data when the application 
 restarts since the allocator believes the block is available.
  
 Another problem is that a significant number of data structures include cyclical 
 references and thus do not form a tree. They could be implemented as a tree, but this 
 approach is usually harder to implement.
  
 Byte-addressable memory guarantees atomicity of only a single write. For current 
 processors, that is generally one 64-bit word (8-bytes) that should be aligned, but this 
 is not a requirement in practice.
  
 All of the preceding problems could be solved if multiple writes occurred 
  
 simultaneously. In the event of a power failure, any incomplete writes should either be 
 replayed as though the power failure never happened or discarded as though the write 
 never occurred. Applications solve this in different ways using atomic operations, 
 transactions, redo/undo logging, etc. Using libpmemobj can solve those problems 
 because it uses atomic transactions and redo/undo logs.
  
 82",NA
 Grouping Operations,"With the exception of modifying a single scalar value that fits within the processor’s 
 word, a series of data modifications must be grouped together and accompanied by a 
 means of detecting an interruption before completion.",NA
 Memory Pools,"Memory-mapped files are at the core of the persistent memory programming model. 
 The libpmemobj library provides a convenient API to easily manage pool creation 
 and access, avoiding the complexity of directly mapping and synchronizing data. 
 PMDK also provides a pmempool utility to administer memory pools from the 
 command line. Memory pools reside on DAX-mounted file systems.",NA
 Creating Memory Pools,"Use the pmempool utility to create persistent memory pools for use with applications. 
 Several pool types can be created including pmemblk, pmemlog, and pmemobj. When 
 using libpmemobj in applications, you want to create a pool of type obj (pmemobj). 
 Refer to the pmempool-create(1) man page for all available commands and options. The 
 following examples are for reference:
  
 Example 1.
  Create a libpmemobj (obj) type pool of minimum allowed size and 
 layout called “my_layout” in the mounted file system /mnt/pmemfs0/
  
 $ pmempool create --layout my_layout obj /mnt/pmemfs0/pool.obj
  
 Example 2.
  Create a libpmemobj (obj) pool of 20GiB and layout called “my_ 
 layout” in the mounted file system /mnt/pmemfs0/
  
 $ pmempool create --layout my_layout –-size 20G obj \ 
 /mnt/pmemfs0/pool.obj
  
 83",NA
 Pool Object Pointer (POP) and the Root Object,"Due to the address space layout randomization (ASLR) feature used by most operating 
 systems, the location of the pool – once memory mapped into the application address 
 space – can differ between executions and system reboots. Without a way to access the 
 data within the pool, you would find it challenging to locate the data within a pool. 
 PMDK-based pools have a small amount of metadata to solve this problem.
  
 Every pmemobj (obj) type pool has a root object. This root object is necessary 
 because it is used as an entry point from which to find all the other objects created in a 
 pool, that is, user data. An application will locate the root object using a special object 
 called pool object pointer (POP). The POP object resides in volatile memory and is 
 created with every program invocation. It keeps track of metadata related to the pool, 
 such as the offset to the root object inside the pool. Figure 
 7-1
  depicts the POP and 
 memory pool layout.
  
 87",NA
 Opening and Reading from Memory Pools,"You create a pool using pmemobj_create(), and you open an existing pool using 
 pmemobj_open(). Both functions return a PMEMobjpool *pop pointer. The pwriter 
 example in Listing 
 7-1
  shows how to create a pool and write a string to it. Listing 
 7-
 2 
 shows how to open the same pool to read and display the string.
  
 Listing 7-2.
  preader.c – An example showing how to open a pool and access the 
 root object and data
  
  33  /*
  
  34   * preader.c -  Read a string from a 35   *              
 persistent memory pool 36   */
  
  37
  
  38  #include <stdio.h>
  
  39  #include <string.h>
  
  40  #include <libpmemobj.h>
  
  41
  
 88",NA
 Memory Poolsets,"The capacity of multiple pools can be combined into a 
 poolset
 . Besides providing a 
 way to increase the available space, a poolset can be used to span multiple persistent 
 memory devices and provide both local and remote replication.
  
 You open a poolset the same way as a single pool using pmemobj_open(). (At the time 
 of publication, pmemobj_create() and the pmempool utility cannot create poolsets. 
  
 Enhancement requests exist for these features.) Although creating poolsets requires 
 manual administration, poolset management can be automated via libpmempool or the 
 pmempool utility; full details appear in the poolset(5) man page.",NA
 Concatenated Poolsets,"Individual pools can be concatenated using pools on a single or multiple file systems. 
 Concatenation only works with the same pool type: block, object, or log pools. Listing 
 7- 
 3 
 shows an example “myconcatpool.set” poolset file that concatenates three smaller 
 pools into a larger pool. For illustrative purposes, each pool is a different size and 
 located on different file systems. An application using this poolset would see a single 
 700GiB memory pool.
  
 90",NA
Note,NA,NA
" Data will be preserved if it exists in /mountpoint0/myfile.part0, ",NA,NA
but any data in /mountpoint0/myfile.part1 or ,NA,NA
/mountpoint0/myfile.part2 will be lost. We recommend that you only ,NA,NA
add new and empty pools to a poolset.,NA,NA
 Replica Poolsets,"Besides combining multiple pools to provide more space, a poolset can also maintain 
 multiple copies of the same data to increase resiliency. Data can be replicated to another 
 poolset on a different file of the local host and a poolset on a remote host.
  
 Listing 
 7-4
  shows a poolset file called “myreplicatedpool.set” that will replicate 
 local writes into the /mnt/pmem0/pool1 pool to another local pool, 
 /mnt/pmem1/pool1, on a different file system, and to a remote-objpool.set poolset on 
 a remote host called example.com.
  
 Listing 7-4.
  myreplicatedpool.set – An example demonstrating how to replicate 
 local data locally and remote host
  
 PMEMPOOLSET 
  
 256G 
 /mnt/pmem0/pool1
  
 REPLICA 
  
 256G 
 /mnt/pmem1/pool1
  
 REPLICA user@example.com remote-objpool.set
  
  
 The librpmem library, a remote persistent memory support library, underpins 
 this feature. Chapter 
 18
  discusses librpmem and replica pools in more detail.",NA
 Managing Memory Pools and Poolsets,"The pmempool utility has several features that developers and system administrators 
 may find useful. We do not present their details here because each command has a 
 detailed man page:
  
 • 
 pmempool info
  prints information and statistics in human-readable 
 format about the specified pool.
  
 • 
 pmempool check
  checks the pool’s consistency and repairs pool if it is 
 not consistent.
  
 • 
 pmempool create
  creates a pool of specified type with additional 
 properties specific for this type of pool.
  
 • 
 pmempool dump
  dumps usable data from a pool in hexadecimal or 
 binary format.
  
 • 
 pmempool rm
  removes pool file or all pool files listed in pool set 
 configuration file.
  
 • 
 pmempool convert
  updates the pool to the latest available layout 
 version.
  
 • 
 pmempool sync
  synchronizes replicas within a poolset.
  
 • 
 pmempool transform
  modifies the internal structure of a poolset.
  
 • 
 pmempool feature
  toggles or queries a poolset’s features.",NA
 Typed Object Identifiers (TOIDs),"When we write data to a persistent memory pool or device, we commit it at a physical 
 address. With the ASLR feature of operating systems, when applications open a pool and 
 memory map it into the address space, the virtual address will change each time. For this 
 reason, a type of handle (pointer) that does not change is needed; this handle is called an 
 OID (object identifier). Internally, it is a pair of the pool or poolset unique identifier 
 (UUID) and an offset within the pool or poolset. The OID can be translated back and forth 
 between its persistent form and pointers that are fit for direct use by this particular 
 instance of your program.
  
 92",NA
 Allocating Memory,"Using malloc() to allocate memory is quite normal to C developers and those who use 
 languages that do not fully handle automatic memory allocation and deallocation. For 
 persistent memory, you can use pmemobj_alloc(), pmemobj_reserve(), or pmemobj_ 
 xreserve() to reserve memory for a transient object and use it the same way you would 
 use malloc(). We recommend that you free allocated memory using pmemobj_free() or 
 POBJ_FREE() when the application no longer requires it to avoid a runtime memory leak. 
 Because these are volatile memory allocations, they will not cause a persistent leak after 
 a crash or graceful application exit.
  
 93",NA
 Persisting Data,"The typical intent of using persistent memory is to save data persistently. For this, 
 you need to use one of three APIs that libpmemobj provides:
  
 • Atomic operations
  
 • Reserve/publish
  
 • Transactional",NA
 Atomic Operations,"The pmemobj_alloc() and its variants shown below are easy to use, but they are limited 
 in features, so additional coding is required by the developer:
  
 int pmemobj_alloc(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 size_t size, uint64_t type_num, pmemobj_constr
  
  
 constructor, void *arg); 
  
 int pmemobj_zalloc(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 size_t size, uint64_t type_num); 
  
 void pmemobj_free(PMEMoid *oidp); 
  
 int pmemobj_realloc(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 size_t size, uint64_t type_num); 
  
 int pmemobj_zrealloc(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 size_t size, uint64_t type_num); 
  
 int pmemobj_strdup(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 const char *s, uint64_t type_num); 
  
 int pmemobj_wcsdup(PMEMobjpool *pop, PMEMoid *oidp,
  
  
 const wchar_t *s, uint64_t type_num);
  
 The TOID-based wrappers for most of these functions include: 
  
 POBJ_NEW(PMEMobjpool *pop, TOID *oidp, TYPE,
  
  
  pmemobj_constr constructor, void *arg) 
  
 POBJ_ALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size,
  
  
 pmemobj_constr constructor, void *arg) 
  
 POBJ_ZNEW(PMEMobjpool *pop, TOID *oidp, TYPE) 
  
 POBJ_ZALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size)
  
 94",NA
 Reserve/Publish API,"The atomic allocation API will not help if
  
 • There is more than one reference to the object that needs to be 
 updated
  
 • There are multiple scalars that need to be updated
  
 For example, if your program needs to subtract money from account A and add it 
 to account B, both operations must be done together. This can be done via the reserve/ 
 publish API.
  
 To use it, you specify any number of operations to be done. The operations may be 
 setting a scalar 64-bit value using pmemobj_set_value(), freeing an object with 
 pmemobj_ defer_free(), or allocating it using pmemobj_reserve(). Of these, only the 
 allocation happens immediately, letting you do any initialization of the newly reserved 
 object. Modifications will not become persistent until pmemobj_publish() is called.
  
 Functions provided by libpmemobj related to the reserve/publish feature are
  
 PMEMoid pmemobj_reserve(PMEMobjpool *pop,
  
  
  struct pobj_action *act, size_t size, uint64_t type_num); void 
 pmemobj_defer_free(PMEMobjpool *pop, PMEMoid oid,
  
 97",NA
 Transactional API,"The reserve/publish API is fast, but it does not allow reading data you have just 
 written. In such cases, you can use the transactional API.
  
 The first time a variable is written, it must be explicitly added to the transaction. This 
 can be done via pmemobj_tx_add_range() or its variants (xadd, _direct). Convenient 
 macros such as TX_ADD() or TX_SET() can perform the same operation. The transaction- 
 based functions and macros provided by libpmemobj include
  
 100",NA
 Optional Flags,"Many of the functions discussed for the atomic, reserve/publish, and transactional APIs 
 have a variant with a ""flags"" argument that accepts these values:
  
 • POBJ_XALLOC_ZERO zeroes the object allocated.
  
 • POBJ_XALLOC_NO_FLUSH suppresses automatic flushing. It is expected 
 that you flush the data in some way; otherwise, it may not be durable in 
 case of an unexpected power loss.",NA
 Persisting Data Summary,"The atomic, reserve/publish, and transactional APIs have different strengths:
  
 • Atomic allocations are the simplest and fastest, but their use is 
 limited to allocating and initializing wholly new blocks.
  
 • The reserve/publish API can be as fast as atomic allocations when all 
 operations involve either allocating or deallocating whole objects or 
 modifying scalar values. However, being able to read the data you have 
 just written may be desirable.
  
 104",NA
 Guarantees of libpmemobj's APIs,"The transactional, atomic allocation, and reserve/publish APIs within libpmemobj all 
 provide fail-safe atomicity and consistency.
  
 The transactional API ensures the durability of any modifications of memory for an 
 object that has been added to the transaction. An exception is when the POBJ_X***_ 
 NO_FLUSH flag is used, in which case the application is responsible for either flushing 
 that memory range itself or using the memcpy-like functions from libpmemobj. The no-
 flush flag does not provide any isolation between threads, meaning partial writes are 
 immediately visible to other threads.
  
 The atomic allocation API requires that applications flush the writes done by the 
 object’s constructor. This ensures durability if the operation succeeded. It is the only API 
 that provides full isolation between threads.
  
 The reserve/publish API requires explicit flushes of writes to memory blocks 
 allocated via pmemobj_reserve() that will flush writes done via pmemobj_set_value(). 
 There is no isolation between threads, although no modifications go live until 
 pmemobj_ publish() starts, allowing you to take explicit locks for just the publishing 
 stage.
  
 Using terms known from databases, the isolation levels provided are
  
 • Transactional API: READ_UNCOMMITTED
  
 • Atomic allocations API: READ_COMMITTED
  
 • Reserve/publish API: READ_COMMITTED until publishing starts, then 
 READ_UNCOMMITTED
  
 105",NA
 Managing Library Behavior,"The pmemobj_set_funcs() function allows an application to override memory allocation 
 calls used internally by libpmemobj. Passing in NULL for any of the handlers will cause 
 the libpmemobj default function to be used. The library does not make heavy use of the 
 system malloc() functions, but it does allocate approximately 4–8 kilobytes for each 
 memory pool in use.
  
 By default, libpmemobj supports up to 1024 parallel transactions/allocations. For 
 debugging purposes, it is possible to decrease this value by setting the 
 PMEMOBJ_NLANES shell environment variable to the desired limit. For example, at the 
 shell prompt, run ""export PMEMOBJ_NLANES=512"" then run the application:
  
 $ export 
 PMEMOBJ_NLANES=512 
  
 $ ./my_app
  
 To return to the default behavior, unset PMEMOBJ_NLANES using
  
 $ unset PMEMOBJ_NLANES",NA
 Debugging and Error Handling,"If an error is detected during the call to a libpmemobj function, the application 
  
 may retrieve an error message describing the reason for the failure from pmemobj_ 
 errormsg(). This function returns a pointer to a static buffer containing the last error 
 message logged for the current thread. If errno was set, the error message may include a 
 description of the corresponding error code as returned by strerror(3). The error 
 message buffer is thread local; errors encountered in one thread do not affect its value in 
 other threads. The buffer is never cleared by any library function; its content is 
 significant only when the return value of the immediately preceding call to a libpmemobj 
 function indicated an error, or if errno was set. The application must not modify or free 
 the error message string, but it may be modified by subsequent calls to other library 
 functions.
  
 Two versions of libpmemobj are typically available on a development system. The 
 non-debug version is optimized for performance and used when a program is linked 
 using the -lpmemobj option. This library skips checks that impact performance, never 
 logs any trace information, and does not perform any runtime assertions.
  
 106",NA
 Summary,"This chapter describes the libpmemobj library, which is designed to simplify persistent 
 memory programming. By providing APIs that deliver atomic operations, transactions, 
 and reserve/publish features, it makes creating applications less error prone while 
 delivering guarantees for data integrity.
  
 108",NA
CHAPTER 8,NA,NA
libpmemobj-cpp: ,NA,NA
The Adaptable ,NA,NA
Language - C++ and ,NA,NA
Persistent ,NA,NA
Memory,NA,NA
 Introduction,"The Persistent Memory Development Kit (PMDK) includes several separate libraries; 
 each is designed with a specific use in mind. The most flexible and powerful one is 
 libpmemobj. It complies with the persistent memory programming model without 
 modifying the compiler. Intended for developers of low-level system software and 
 language creators, the libpmemobj library provides allocators, transactions, and a way 
 to automatically manipulate objects. Because it does not modify the compiler, its API is 
 verbose and macro heavy.
  
 To make persistent memory programming easier and less error prone, higher- level 
 language bindings for libpmemobj were created and included in PMDK. The C++ 
 language was chosen to create new and friendly API to libpmemobj called libpmemobj- 
 cpp, which is also referred to as libpmemobj++. C++ is versatile, feature rich, has a large 
 developer base, and it is constantly being improved with updates to the C++ 
 programming standard.
  
 The main goal for the libpmemobj-cpp bindings design was to focus modifications to 
 volatile programs on data structures and not on the code. In other words, libpmemobj- 
 cpp bindings are for developers, who want to modify volatile applications, provided 
 with a convenient API for modifying structures and classes with only slight 
 modifications to functions.",NA
 Metaprogramming to the Rescue,"Metaprogramming is a technique in which computer programs have the ability to treat 
 other programs as their data. It means that a program can be designed to read, generate, 
 analyze or transform other programs, and even modify itself while running. In some 
 cases, this allows programmers to minimize the number of lines of code to express a 
 solution, in turn reducing development time. It also allows programs greater flexibility 
 to efficiently handle new situations without recompilation.
  
 For the libpmemobj-cpp library, considerable effort was put into encapsulating the 
 PMEMoids (persistent memory object IDs) with a type-safe container. Instead of a 
 sophisticated set of macros for providing type safety, templates and metaprogramming 
 are used. This significantly simplifies the native C libpmemobj API.",NA
 Persistent Pointers,"The persistent memory programming model created by the Storage Networking 
 Industry Association (SNIA) is based on memory-mapped files. PMDK uses this model 
 for its architecture and design implementation. We discussed the SNIA programming 
 model in Chapter 
 3
 .
  
 Most operating systems implement address space layout randomization (ASLR). 
  
 ASLR is a computer security technique involved in preventing exploitation of memory 
 corruption vulnerabilities. To prevent an attacker from reliably jumping to, for example, 
 a particular exploited function in memory, ASLR randomly arranges the address space 
 positions of key data areas of a process, including the base of the executable and the 
 positions of the stack, heap, and libraries. Because of ASLR, files can be mapped at 
 different addresses of the process address space each time the application executes. 
  
 As a result, traditional pointers that store absolute addresses cannot be used. Upon 
 each execution, a traditional pointer might point to uninitialized memory for which 
  
 112",NA
 Transactions,"Being able to modify more than 8 bytes of storage at a time atomically is imperative for 
 most nontrivial algorithms one might want to use in persistent memory. Commonly, a 
 single logical operation requires multiple stores. For example, an insert into a simple list- 
 based queue requires two separate stores: a tail pointer and the next pointer of the last 
 element. To enable developers to modify larger amounts of data atomically, with respect 
 to power-fail interruptions, the PMDK library provides transaction support in some of its 
 libraries. The C++ language bindings wrap these transactions into two concepts: one, 
 based on the resource acquisition is initialization (RAII) idiom and the other based on a 
 callable std::function object. Additionally, because of some C++ standard issues, the 
 scoped transactions come in two flavors: manual and automatic. In this chapter we only 
 describe the approach with std::function object. For information about RAII- based 
 transactions, refer to libpmemobj-cpp documentation (
 https://pmem.io/pmdk/ 
 cpp_obj/
 ).
  
 The method which uses std::function is declared as
  
 void pmem::obj::transaction::run(pool_base &pop,
  
  
 std::function<void ()> tx, Locks&... locks)
  
 113",NA
 Snapshotting,"The C library requires manual snapshots before modifying data in a transaction. The C++ 
 bindings do all of the snapshotting automatically, to reduce the probability of 
 programmer error. The pmem::obj::p template wrapper class is the basic building block 
 for this mechanism. It is designed to work with basic types and not compound types such 
 as classes or PODs (
 Plain Old Data
 , structures with fields only and without any object-
 oriented features). This is because it does not define operator->() and there is no 
 possibility to implement operator.(). The implementation of pmem::obj::p is based on 
 the operator=(). Each time the assignment operator is called, the value wrapped by p will 
 be changed, and the library needs to snapshot the old value. In addition to snapshotting, 
 the p<> template ensures the variable is persisted correctly, flushing data if necessary. 
 Listing 
 8-2
  provides an example of using the p<> template.
  
 Listing 8-2.
  Using the p<> template to persist values correctly
  
  39    struct bad_example {
  
  40        int some_int;
  
  41        float some_float;
  
  42    };
  
  43
  
  44    struct good_example {
  
  45        pmem::obj::p<int> pint;
  
  46        pmem::obj::p<float> pfloat;
  
  47    };
  
  48
  
  49    struct root {
  
  50        bad_example bad;
  
  51        good_example good;
  
  52    };
  
  53
  
  54    int main(int argc, char *argv[]) {
  
  55        auto pop = pmem::obj::pool<root>::open(""/daxfs/file"", ""p""); 56
  
  57        auto r = pop.root();
  
  58
  
 115",NA
 Allocating,"As with std::shared_ptr, the pmem::obj::persistent_ptr comes with a set of allocating 
 and deallocating functions. This helps allocate memory and create objects, as well as 
 destroy and deallocate the memory. This is especially important in the case of 
 persistent 
  
 116",NA
 C++ Standard limitations,"The C++ language restrictions and persistent memory programming paradigm imply 
 serious restrictions on objects which may be stored on persistent memory. Applications 
 can access persistent memory with memory-mapped files to take advantage of its byte 
 addressability thanks to libpmemobj and SNIA programming model. No serialization 
 takes place here, so applications must be able to read and modify directly from the 
 persistent memory media even after the application was closed and reopened or after a 
 power failure event.
  
 118",NA
 An Object’s Lifetime,"The lifetime of an object is described in the [basic.life] section of the C++ standard 
 (
 https://isocpp.org/std/the-standard
 ):
  
 The lifetime of an object or reference is a runtime property of the object or 
 reference. A variable is said to have vacuous initialization if it is default- 
 initialized and, if it is of class type or a (possibly multi-dimensional) array 
 thereof, that class type has a trivial default constructor. The lifetime of an 
 object of type T begins when:
  
 (1.1) storage with the proper alignment and size for type T is obtained, and
  
 (1.2) its initialization (if any) is complete (including vacuous initializa-tion) 
 ([dcl.init]), except that if the object is a union member or subobject thereof, 
 its lifetime only begins if that union member is the initialized mem-ber in the 
 union ([dcl.init.aggr], [class.base.init]), or as described in [class.
  
 union]. The lifetime of an object of type T ends when:
  
 (1.3) if T is a non-class type, the object is destroyed, or
  
 (1.4) if T is a class type, the destructor call starts, or
  
 (1.5) the storage which the object occupies is released, or is reused by an 
 object that is not nested within o ([intro.object]).
  
 The standard states that properties ascribed to objects apply for a given object only 
 during its lifetime. In this context, the persistent memory programming problem is 
 similar to transmitting data over a network, where the C++ application is given an 
 array of bytes but might be able to recognize the type of object sent. However, the 
 object was not constructed in this application, so using it would result in undefined 
 behavior. 
  
 119",NA
 Trivial Types,"Transactions are the heart of libpmemobj. That is why libpmemobj-cpp was 
 implemented with utmost care while designing the C++ versions so they are as easy to 
 use as possible. Developers do not have to know the implementation details and do not 
 have to worry about snapshotting modified data to make undo log–based transaction 
 works. A special semi-transparent template property class has been implemented to 
 automatically add variable modifications to the transaction undo log, which is described 
 in the “Snapshotting” section.
  
 But what does snapshotting data mean? The answer is very simple, but the 
  
 consequences for C++ are not. libpmemobj implements snapshotting by copying data of 
 given length from a specified address to another address using memcpy(). If a 
 transaction aborts or a system power loss occurs, the data will be written from the undo 
 log when the memory pool is reopened. Consider a definition of the following C++ object, 
 presented in Listing 
 8-4
 , and think about the consequences that a memcpy() has on it.
  
 Listing 8-4.
  An example showing an unsafe memcpy() on an object
  
  35    class nonTriviallyCopyable {
  
  36    private:
  
  37        int* i;
  
  38    public:
  
  39        nonTriviallyCopyable (const nonTriviallyCopyable & from) 40        {
  
  41            /* perform non-trivial copying routine */
  
  42            i = new int(*from.i);
  
  43        }
  
  44    };
  
 120",NA
 Object Layout,"Object representation, also referred to as the 
 layout
 , might differ between compilers, 
 compiler flags, and application binary interface (ABI). The compiler may do some layout-
 related optimizations and is free to shuffle order of members with same specifier type – 
 for example, public then protected, then public again. Another problem related to 
 unknown object layout is connected to polymorphic types. Currently there is no reliable 
 and portable way to implement vtable rebuilding after reopening the memory pool, so 
 polymorphic objects cannot be supported with persistent memory.
  
 If we want to store objects on persistent memory using memory-mapped files and to 
 follow the SNIA NVM programming model, we must ensure that the following casting 
 will be always valid:
  
 someType A = *reinterpret_cast<someType*>(mmap(...));
  
 The bit representation of a stored object type must be always the same, and our 
 application should be able to retrieve the stored object from the memory-mapped file 
 without serialization.
  
 It is possible to ensure that specific types satisfy the aforementioned requirements. 
  
 C++11 provides another type trait called std::is_standard_layout. The standard mentions 
 that it is useful for communicating with other languages, such as for creating language 
 bindings to native C++ libraries as an example, and that's why a standard- layout class 
 has the same memory layout of the equivalent C struct or union. A general rule is that 
 standard-layout classes must have all non-static data members with the same access 
 control. We mentioned this at the beginning of this section – that a C++ compliant 
 compiler is free to shuffle access ranges of the same class definition.
  
 When using inheritance, only one class in the whole inheritance tree can have non- 
 static data members, and the first non-static data member cannot be of a base class type 
 because this could break aliasing rules. Otherwise, it is not a standard-layout class.
  
 122",NA
 Pointers,"In previous sections, we quoted parts of the C++ standard. We were describing the limits 
 of types which were safe to snapshot and copy and which we can binary-cast without 
 thinking of fixed layout. But what about pointers? How do we deal with them in our 
 objects as we come to grips with the persistent memory programming model? Consider 
 the code snippet presented in Listing 
 8-5
  which provides an example of a class that uses 
 a volatile pointer as a class member.
  
 123",NA
 Limitations Summary,"C++11 provides several very useful type traits for persistent memory 
 programming. These are
  
 • template <typename T>
  
 struct std::is_pod;
  
 • template <typename T>
  
 struct std::is_trivial;
  
 • template <typename T>
  
 struct std::is_trivially_copyable;
  
 • template <typename T>
  
 struct std::is_standard_layout;
  
  
 They are correlated with each other. The most general and restrictive is the 
 definition of a POD type shown in Figure 
 8-1
 .
  
 125",NA
 Persistence Simplified,"Consider a simple queue implementation, presented in Listing 
 8-6
 , which stores 
 elements in volatile DRAM.
  
 Listing 8-6.
  An implementation of a volatile queue
  
  33    #include <cstdio>
  
  34    #include <cstdlib>
  
  35    #include <iostream>
  
  36    #include <string>
  
  37
  
 126",NA
 The Ecosystem,"The overall goal for the libpmemobj C++ bindings was to create a friendly and less 
 error-prone API for persistent memory programming. Even with persistent memory 
 pool allocators, a convenient interface for creating and managing transactions, auto- 
 snapshotting class templates and smart persistent pointers, and designing 
  
 133",NA
 Persistent Containers,"The C++ standard library containers collection is something that persistent memory 
 programmers may want to use. Containers manage the lifetime of held objects 
 through allocation/creation and deallocation/destruction with the use of allocators. 
 Implementing custom persistent allocator for C++ STL (Standard Template Library) 
 containers has two main downsides:
  
 • Implementation details:
  
 • STL containers do not use algorithms optimal for a persistent 
 memory programming point of view.
  
 • Persistent memory containers should have durability and 
 consistency properties, while not every STL method guarantees 
 strong exception safety.
  
 • Persistent memory containers should be designed with an 
 awareness of fragmentation limitations.
  
 • Memory layout:
  
 • The STL does not guarantee that the container layout will remain 
 unchanged in new library versions.
  
 Due to these obstacles, the libpmemobj-cpp contains the set of custom, 
  
 implemented-from-scratch, containers with optimized on-media layouts and 
 algorithms to fully exploit the potential and features of persistent memory. These 
 methods guarantee atomicity, consistency, and durability. Besides specific internal 
 implementation details, libpmemobj-cpp persistent memory containers have a well- 
 known STL-like interface, and they work with STL algorithms.
  
 134",NA
 Examples of Persistent Containers,"Since the main goal for the libpmemobj-cpp design is to focus modifications to volatile 
 programs on data structures and not on the code, the use of libpmemobj-cpp persistent 
 containers is almost the same as for their STL counterparts. Listing 
 8-11
  shows a 
 persistent vector example to showcase this.
  
 Listing 8-11.
  Allocating a vector transactionally using persistent containers
  
  33    #include <libpmemobj++/make_persistent.hpp>
  
  34    #include <libpmemobj++/transaction.hpp>
  
  35    #include <libpmemobj++/persistent_ptr.hpp>
  
  36    #include <libpmemobj++/pool.hpp>
  
  37    #include ""libpmemobj++/vector.hpp""
  
  38
  
  39    using vector_type = pmem::obj::experimental::vector<int>; 40
  
  41    struct root {
  
  42            pmem::obj::persistent_ptr<vector_type> vec_p; 43    };
  
  44
  
  
  ...
  
  63
  
  64        /* creating pmem::obj::vector in transaction */
  
  65        pmem::obj::transaction::run(pool, [&] {
  
  66             root->vec_p = pmem::obj::make_persistent<vector_type> 
  
 (/* 
 optional constructor arguments */);
  
  67        });
  
  68
  
  69        vector_type &pvector = *(root->vec_p);
  
 Listing 
 8-11
  shows that a pmem::obj::vector must be created and allocated in 
 persistent memory using transaction to avoid an exception being thrown. The vector 
 type constructor may construct an object by internally opening another transaction. 
 In this case, an inner transaction will be flattened to an outer one. The interface and 
 semantics of pmem::obj::vector are similar to that of std::vector, as Listing 
 8-12 
 demonstrates.
  
 135",NA
 Summary,"This chapter describes the libpmemobj-cpp library. It makes creating applications 
 less error prone, and its similarity to standard C++ API makes it easier to modify 
 existing volatile programs to use persistent memory. We also list the limitations of 
 this library and the problems you must consider during development.
  
 138",NA
CHAPTER 9,NA,NA
pmemkv: A Persistent ,NA,NA
In- Memory Key-,NA,NA
Value Store,"Programming persistent memory is not easy. In several chapters we have described that 
 applications that take advantage of persistent memory must take responsibility for 
 atomicity of operations and consistency of data structures. PMDK libraries like 
 libpmemobj are designed with flexibility and simplicity in mind. Usually, these are 
 conflicting requirements, and one has to be sacrificed for the sake of the other. The truth 
 is that in most cases, an API’s flexibility increases its complexity.
  
 In the current cloud computing ecosystem, there is an unpredictable demand for 
 data. Consumers expect web services to provide data with predicable low-latency 
 reliability. Persistent memory’s byte addressability and huge capacity characteristics 
 make this technology a perfect fit for the broadly defined cloud environment.
  
 Today, as greater numbers of devices with greater levels of intelligence are 
 connected to various networks, businesses and consumers are finding the cloud to be 
 an increasingly attractive option that enables fast, ubiquitous access to their data. 
  
 Increasingly, consumers are fine with lower storage capacity on endpoint devices in 
 favor of using the cloud. By 2020, IDC predicts that more bytes will be stored in the 
 public cloud than in consumer devices (Figure 
 9-1
 ).
  
  
 Figure 9-1. 
 Where is data stored? Source: IDC White Paper – #US44413318",NA
 pmemkv Architecture,"There are many key-value data stores available on the market. They have different 
 features and licenses and their APIs are targeting different use cases. However, their 
 core API remains the same. All of them provide methods like put, get, remove, exists, 
 open, and close. At the time we published this book, the most popular key-value data 
 store is Redis. It is available in open source (
 https://redis.io/
 ) and enterprise (
 https:// 
 redislabs.com
 ) versions. DB-Engines (
 https://db-engines.com
 ) shows that Redis has a 
 significantly higher rank than any of its competitors in this sector.
  
  
 Figure 9-3. 
 DB-Engines ranking of key-value stores (July 2019). Scoring method: 
 https://db-engines.com/en/ranking_definition
 . Source: 
 https://db-
 engines.com/en/ranking/key-value+store
  
 Pmemkv was created as a separate project not only to complement PMDK’s set of 
 libraries with cloud-native support but also to provide a key-value API built for 
 persistent memory. One of the main goals for pmemkv developers was to create friendly 
 environment for open source community to develop new engines with the help of PMDK 
 and to integrate it with other programming languages. Pmemkv uses the same BSD 3-
 Clause permissive license as PMDK. The native API of pmemkv is C and C++. Other 
 programming language bindings are available such as JavaScript, Java, and Ruby. 
 Additional languages can easily be added.
  
 143",NA
 A Phonebook Example,"Listing 
 9-2
  shows a simple phonebook example implemented using the pmemkv C++ 
 API v0.9. One of the main intentions of pmemkv is to provide a familiar API similar to 
 the other key-value stores. This makes it very intuitive and easy to use. We will reuse 
 the config_setup() function from Listing 
 9-1
 .
  
 Listing 9-2.
  A simple phonebook example using the pmemkv C++ API
  
  37    #include <iostream>
  
  38    #include <cassert>
  
  39    #include <libpmemkv.hpp>
  
  40    #include <string>
  
  41    #include ""pmemkv_config.h""
  
  42
  
  43    using namespace pmem::kv;
  
  44
  
  45    auto PATH = ""/daxfs/kvfile"";
  
  46    const uint64_t FORCE_CREATE = 1;
  
  47    const uint64_t SIZE = 1024 ∗ 1024 ∗ 1024; // 1 Gig
  
  48
  
  49    int main() {
  
  50        // Prepare config for pmemkv database
  
  51        pmemkv_config ∗cfg = config_setup(PATH, FORCE_CREATE, SIZE); 52        
 assert(cfg != nullptr);
  
  53
  
  54        // Create a key-value store using the ""cmap"" engine.
  
  55        db kv;
  
  56
  
  57        if (kv.open(""cmap"", config(cfg)) != status::OK) { 58            
 std::cerr << db::errormsg() << std::endl;
  
 147",NA
 Bringing Persistent Memory Closer to the ,NA,NA
Cloud,"We will rewrite the phonebook example using the JavaScript language bindings. There 
 are several language bindings available for pmemkv – JavaScript, Java, Ruby, and Python. 
  
 However, not all provide the same API functionally equivalent to the native C and C++ 
 counterparts. Listing 
 9-3
  shows an implementation of the phonebook application 
 written using JavaScript language bindings API.
  
 Listing 9-3.
  A simple phonebook example written using the JavaScript bindings 
 for pmemkv v0.8
  
  1    const Database = require('./lib/all');
  
  2
  
  3    function assert(condition) {
  
  4        if (!condition) throw new Error('Assert failed');
  
  5    }
  
  6
  
  7    console.log('Create a key-value store using the ""cmap"" engine'); 8     const db = 
 new Database('cmap', '{""path"":""/daxfs/ 
  
  
 kvfile"",""size"":1073741824, ""force_create"":1}');
  
  9
  
  10    console.log('Add 2 entries with name and phone number'); 11    
 db.put('John', '123-456-789');
  
  12    db.put('Kate', '987-654-321');
  
  13
  
 151",NA
 Summary,"In this chapter, we have shown how a familiar key-value data store is an easy way for the 
 broader cloud software developer audience to use persistent memory and directly 
 access the data in place. The modular design, flexible engine API, and integration with 
 many of the most popular cloud programming languages make pmemkv an intuitive 
 choice for cloud-native software developers. As an open source and lightweight library, it 
 can easily be integrated into existing applications to immediately start taking advantage 
 of persistent memory.
  
 Some of the pmemkv engines are implemented using libpmemobj-cpp that we 
 described in Chapter 
 8
 . The implementation of such engines provides real-world 
 examples for developers to understand how to use PMDK (and related libraries) in 
 applications.
  
 152",NA
CHAPTER 10,NA,NA
Volatile Use of ,NA,NA
Persistent Memory,NA,NA
 Introduction,"This chapter discusses how applications that require a large quantity of volatile memory 
 can leverage high-capacity persistent memory as a complementary solution to dynamic 
 random-access memory (DRAM).
  
 Applications that work with large data sets, like in-memory databases, caching 
 systems, and scientific simulations, are often limited by the amount of volatile memory 
 capacity available in the system or the cost of the DRAM required to load a complete 
 data set. Persistent memory provides a high capacity memory tier to solve these 
 memory-hungry application problems. 
  
 In the memory-storage hierarchy (described in Chapter 
 1
 ), data is stored in tiers 
 with frequently accessed data placed in DRAM for low-latency access, and less 
 frequently accessed data is placed in larger capacity, higher latency storage devices. 
 Examples of such solutions include Redis on Flash (
 https://redislabs.com/redis-
 enterprise/ technology/redis-on-flash/
 ) and Extstore for Memcached 
 (
 https://memcached.org/ blog/extstore-cloud/
 ).
  
 For memory-hungy applications that do not require persistence, using the larger 
 capacity persistent memory as volatile memory provides new opportunities and 
 solutions.
  
  
 Using persistent memory as a volatile memory solution is advantageous when an 
 application: 
  
 • Has control over data placement between DRAM and other storage 
 tiers within the system
  
 • 
  
 Does not need to persist data
  
 155
  
 © The Author(s) 2020 
  
 S. Scargall, 
 Programming Persistent Memory
 , 
 https://doi.org/10.1007/978-1-4842-4932-1_10",NA
 Background,"Applications manage different kinds of data structures such as user data, key-value 
 stores, metadata, and working buffers. Architecting a solution that uses tiered memory 
 and storage may enhance application performance, for example, placing objects that 
 are accessed frequently and require low-latency access in DRAM while storing objects 
 that require larger allocations that are not as latency-sensitive on persistent memory. 
 Traditional storage devices are used to provide persistence.",NA
 Memory Allocation,"As described in Chapters 
 1
  through 
 3
 , persistent memory is exposed to the application 
 using memory-mapped files on a persistent memory-aware file system that provides 
 direct access to the application. Since malloc() and free() do not operate on different 
 types of memory or memory-mapped files, an interface is needed that provides malloc() 
 and free() semantics for multiple memory types. This interface is implemented as the 
 memkind library (
 http://memkind.github.io/memkind/
 ).",NA
 How it Works,"The memkind library is a user-extensible heap manager built on top of jemalloc
 ,
  which 
 enables partitioning of the heap between multiple 
 kinds
  of memory. Memkind was 
 created to support different kinds of memory when high bandwidth memory (HBM) was 
 introduced. A PMEM 
 kind
  was introduced to support persistent memory.
  
 Different “kinds” of memory are defined by the operating system memory policies 
 that are applied to virtual address ranges. Memory characteristics supported by 
  
 memkind without user extension include the control of non-uniform memory access 
 (NUMA) and page sizes. Figure 
 10-1
  shows an overview of libmemkind components and 
 hardware support.
  
 156",NA
 Supported “Kinds” of Memory,"The dynamic PMEM kind is best used with memory-addressable persistent storage 
 through a DAX-enabled file system that supports load/store operations that are 
  
 not paged via the system page cache. For the PMEM kind, the memkind library supports 
 the traditional malloc/free-like interfaces on a memory-mapped file. When an 
 application calls memkind_create_kind() with PMEM, a temporary file (tmpfile(3)) is 
 created on a mounted DAX file system and is memory-mapped into the application’s 
 virtual address space. This temporary file is deleted automatically when the program 
 terminates, giving the perception of volatility. 
  
  
 Figure 
 10-2
  shows memory mappings from two memory sources: DRAM 
 (MEMKIND_DEFAULT) and persistent memory (PMEM_KIND).
  
  
 For allocations from DRAM, rather than using the common malloc(), the 
  
 application can call memkind_malloc() with the 
 kind
  argument set to 
 MEMKIND_DEFAULT. 
  
 MEMKIND_DEFAULT is a static kind that uses the operating system’s default page 
 size for allocations. Refer to the memkind documentation for large and huge page 
 support.",NA
 The memkind API,"The memkind API functions related to persistent memory programming are shown in 
 Listing 
 10-1
  and described in this section. The complete memkind API is available in the 
 memkind man pages (
 http://memkind.github.io/memkind/man_pages/memkind.html
 ).
  
 Listing 10-1.
  Persistent memory-related memkind API functions
  
 KIND CREATION MANAGEMENT: 
  
 int memkind_create_pmem(const char *dir, size_t max_size, memkind_t *kind); int 
 memkind_create_pmem_with_config(struct memkind_config *cfg, memkind_t *kind); 
  
 memkind_t memkind_detect_kind(void *ptr); 
  
 int memkind_destroy_kind(memkind_t 
 kind
 );
  
 KIND HEAP MANAGEMENT: 
  
 void *memkind_malloc(memkind_t kind, size_t size); 
  
 void *memkind_calloc(memkind_t kind, size_t num, size_t size); void 
 *memkind_realloc(memkind_t kind, void *ptr, size_t size); void 
 memkind_free(memkind_t kind, void *ptr); 
  
 size_t memkind_malloc_usable_size(memkind_t kind, void *ptr); 
 memkind_t memkind_detect_kind(void *ptr);
  
 KIND CONFIGURATION MANAGEMENT: 
  
 struct memkind_config *memkind_config_new(); 
  
 void memkind_config_delete(struct memkind_config *cfg); 
  
 void memkind_config_set_path(struct memkind_config *cfg, const char *pmem_dir); 
  
 void memkind_config_set_size(struct memkind_config *cfg, size_t pmem_size); void 
 memkind_config_set_memory_usage_policy(struct memkind_config *cfg, 
 memkind_mem_usage_policy policy);",NA
 Kind Management API,"The memkind library supports a plug-in architecture to incorporate new memory kinds, 
 which are referred to as dynamic kinds. The memkind library provides the API to create 
 and manage the heap for the dynamic kinds.
  
 159",NA
 Kind Creation,"Use the memkind_create_pmem() function to create a PMEM 
 kind
  of memory from a file-
 backed source. This file is created as a tmpfile(3) in a specified directory (PMEM_DIR) 
 and is unlinked, so the file name is not listed under the directory. The temporary file is 
 automatically removed when the program terminates.
  
 Use memkind_create_pmem() to create a fixed or dynamic heap size depending on 
 the application requirement. Additionally, configurations can be created and supplied 
 rather than passing in configuration options to the *_create_* function.",NA
Creating a Fixed-Size Heap,"Applications that require a fixed amount of memory can specify a nonzero value for the 
 PMEM_MAX_SIZE argument to memkind_create_pmem(), shown below. This defines the 
 size of the memory pool to be created for the specified kind of memory. The value of 
 PMEM_MAX_SIZE should be less than the available capacity of the file system specified in 
 PMEM_DIR to avoid ENOMEM or ENOSPC errors. An internal data structure struct 
 memkind is populated internally by the library and used by the memory management 
 functions.
  
 int memkind_create_pmem(PMEM_DIR, PMEM_MAX_SIZE, &pmem_kind)
  
 The arguments to memkind_create_pmem() are
  
 • PMEM_DIR is the directory where the temp file is created.
  
 • PMEM_MAX_SIZE is the size, in bytes, of the memory region to be 
 passed to jemalloc.
  
 • &pmem_kind is the address of a memkind data structure.
  
  
 If successful, memkind_create_pmem() returns zero. On failure, an error 
 number is returned that memkind_error_message() can convert to an error message 
 string. 
  
 Listing 
 10-2
  shows how a 32MiB PMEM kind is created on a /daxfs file system. Included 
 in this listing is the definition of memkind_fatal() to print a memkind error message and 
 exit. 
  
 The rest of the examples in this chapter assume this routine is defined as shown below. 
  
 Listing 10-2.
  Creating a 32MiB PMEM kind",NA
Creating a Variable Size Heap,"When PMEM_MAX_SIZE is set to zero, as shown below, allocations are satisfied as long 
 as the temporary file can grow. The maximum heap size growth is limited by the 
 capacity of the file system mounted under the PMEM_DIR argument.
  
 memkind_create_pmem(PMEM_DIR, 0, &pmem_kind)
  
 The arguments to memkind_create_pmem() are: 
  
 • PMEM_DIR is the directory where the temp file is created.
  
 • PMEM_MAX_SIZE is 0.
  
 • &pmem_kind is the address of a memkind data structure.
  
 If the PMEM kind is created successfully, memkind_create_pmem() returns zero. 
 On failure, memkind_error_message() can be used to convert an error number 
 returned by memkind_create_pmem() to an error message string, as shown in the 
 memkind_fatal() routine in Listing 
 10-2
 .
  
 Listing 
 10-4
  shows how to create a PMEM kind with variable size.
  
 Listing 10-4.
  Creating a PMEM kind with variable size
  
 struct memkind *pmem_kind; 
  
 int err; 
  
 err = memkind_create_pmem(""/daxfs"",0,&pmem_kind); if 
 (err) {
  
  
  memkind_fatal(err); 
  
 }",NA
 Detecting the Memory Kind,"Memkind supports both automatic detection of the kind as well as a function to detect 
 the kind associated with a memory referenced by a pointer.",NA
Automatic Kind Detection,"Automatically detecting the kind of memory is supported to simplify code changes when 
 using libmemkind. Thus, the memkind library will automatically retrieve the 
 kind
  of 
 memory pool the allocation was made from, so the heap management functions listed in 
 Table 
 10-1
  can be called without specifying the kind.
  
 162",NA
Memory Kind Detection ,"Memkind also provides the memkind_detect_kind() function, shown below, to query 
 and return the kind of memory referenced by the pointer passed into the function. 
  
 If the input pointer argument is NULL, the function returns NULL. The input pointer 
 argument passed into memkind_detect_kind() must have been returned by a previous 
 call to memkind_malloc(), memkind_calloc(), memkind_realloc(), or memkind_posix_ 
 memalign().
  
 memkind_t memkind_detect_kind(void *ptr)
  
  
 Similar to the automatic detection approach, this function has nontrivial 
 performance overhead. Listing 
 10-5
  shows how to detect the kind type.
  
 Listing 10-5.
  pmem_detect_kind.c – how to automatically detect the ‘kind’ type
  
  73  err = memkind_create_pmem(path, 0, &pmem_kind); 74  
 if (err) {
  
  75      memkind_fatal(err);
  
  76  }
  
  77
  
 163",NA
 Destroying Kind Objects,"Use the memkind_destroy_kind() function, shown below, to delete the kind object that 
 was previously created using the memkind_create_pmem() or memkind_create_pmem_ 
 with_config() function.
  
 int memkind_destroy_kind(memkind_t kind);
  
  
 Using the same pmem_detect_kind.c code from Listing 
 10-5
 , Listing 
 10-6
  shows how 
 the kind is destroyed before the program exits.
  
 Listing 10-6.
  Destroying a kind object
  
  89     err = memkind_destroy_kind(pmem_kind); 90     
 if (err) {
  
  91         memkind_fatal(err);
  
  92     }
  
  
 When the kind returned by memkind_create_pmem() or 
 memkind_create_pmem_with_ config() is successfully destroyed, all the allocated 
 memory for the kind object is freed.",NA
 Heap Management API,"The heap management functions described in this section have an interface modeled on 
 the ISO C standard API, with an additional “kind” parameter to specify the memory type 
 used for allocation.
  
 164",NA
 Allocating Memory,"The memkind library provides memkind_malloc(), memkind_calloc(), and 
 memkind_ realloc() functions for allocating memory, defined as follows:
  
 void *memkind_malloc(memkind_t kind, size_t size); 
  
 void *memkind_calloc(memkind_t kind, size_t num, size_t size); void 
 *memkind_realloc(memkind_t kind, void *ptr, size_t size);
  
 memkind_malloc() allocates size bytes of uninitialized memory of the specified kind. 
 The allocated space is suitably aligned (after possible pointer coercion) for storage of 
 any object type. If size is 0, then memkind_malloc() returns NULL.
  
 memkind_calloc() allocates space for num objects, each is size bytes in length. The 
 result is identical to calling memkind_malloc() with an argument of num * size. The 
 exception is that the allocated memory is explicitly initialized to zero bytes. If num or 
 size is 0, then memkind_calloc() returns NULL.
  
 memkind_realloc() changes the size of the previously allocated memory 
  
 referenced by ptr to size bytes of the specified kind. The contents of the memory 
 remain unchanged, up to the lesser of the new and old sizes. If the new size is larger, 
 the contents of the newly allocated portion of the memory are undefined. If successful, 
 the memory referenced by ptr is freed, and a pointer to the newly allocated memory is 
 returned.
  
 The code example in Listing 
 10-7
  shows how to allocate memory from DRAM and 
 persistent memory (pmem_kind) using memkind_malloc(). Rather than using the 
 common C library malloc() for DRAM and memkind_malloc() for persistent memory, 
 we recommend using a single library to simplify the code.
  
 Listing 10-7.
  An example of allocating memory from both DRAM and persistent 
 memory
  
 /*
  
  * Allocates 100 bytes using appropriate ""kind"" * of 
 volatile memory
  
  */
  
 165",NA
 Freeing Allocated Memory,"To avoid memory leaks, allocated memory can be freed using the memkind_free() 
 function, defined as: 
  
 void memkind_free(memkind_t 
 kind
 , void *
 ptr
 );
  
 memkind_free() causes the allocated memory referenced by ptr to be made available 
 for future allocations. This pointer must be returned by a previous call to 
 memkind_malloc(), memkind_calloc(), memkind_realloc(), or memkind_posix_ 
 memalign(). Otherwise, if memkind_free(kind, ptr) was previously called, undefined 
 behavior occurs. If ptr is NULL, no operation is performed. In cases where the kind is 
 unknown in the context of the call to memkind_free(), NULL can be given as the kind 
 specified to memkind_free(), but this will require an internal lookup for the correct kind. 
  
 Always specify the correct kind because the lookup for kind could result in a serious 
 performance penalty.
  
  
 Listing 
 10-8
  shows four examples of memkind_free() being used. The first two 
 specify the kind, and the second two use NULL to detect the kind automatically.
  
 Listing 10-8.
  Examples of memkind_free() usage
  
 /* Free the memory by specifying the kind */ 
 memkind_free(MEMKIND_DEFAULT, dstring); 
  
 memkind_free(PMEM_KIND, pstring);
  
 /* Free the memory using automatic kind detection */ 
 memkind_free(NULL, dstring); 
  
 memkind_free(NULL, pstring);
  
 166",NA
 Kind Configuration Management,"You can also create a heap with a specific configuration using the function memkind_ 
 create_pmem_with_config(). This function requires completing a memkind_config 
 structure with optional parameters such as size, path to file, and memory usage policy.",NA
 Memory Usage Policy,"In jemalloc, a runtime option called dirty_decay_ms determines how fast it returns 
 unused memory back to the operating system. A shorter decay time purges unused 
 memory pages faster, but the purging costs CPU cycles. Trade-offs between memory and 
 CPU cycles needed for this operation should be carefully thought out before using this 
 parameter.
  
 The memkind library supports two policies related to this feature:
  
  1. MEMKIND_MEM_USAGE_POLICY_DEFAULT
  
  2. MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE
  
  
 The minimum and maximum values for dirty_decay_ms using the 
 MEMKIND_MEM_ USAGE_POLICY_DEFAULT are 0ms to 10,000ms for arenas assigned 
 to a PMEM kind. 
  
 Setting MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE sets shorter decay times to 
 purge unused memory faster, reducing memory usage. To define the memory usage 
 policy, use memkind_config_set_memory_usage_policy(), shown below:
  
 void memkind_config_set_memory_usage_policy (struct memkind_config *cfg, 
 memkind_mem_usage_policy policy );
  
 • MEMKIND_MEM_USAGE_POLICY_DEFAULT is the default memory 
 usage policy.
  
 • MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE allows changing 
 the dirty_decay_ms parameter.
  
  
 Listing 
 10-9
  shows how to use memkind_config_set_memory_usage_policy() with a 
 custom configuration.
  
 167",NA
 Additional memkind Code Examples,"The memkind source tree contains many additional code examples, available on GitHub 
 at 
 https://github.com/memkind/memkind/tree/master/examples
 .",NA
 C++ Allocator for PMEM Kind,"A new pmem::allocator class template is created to support allocations from persistent 
 memory, which conforms to C++11 allocator requirements. It can be used with C++ 
 compliant data structures from:
  
 • Standard Template Library (STL)
  
 • Intel
 ®
  Threading Building Blocks (Intel
 ®
  TBB) library
  
 168",NA
 pmem::allocator methods,"pmem::allocator(const char *dir, size_t max_size); 
  
 pmem::allocator(const std::string& dir, size_t max_size) ; 
  
 template <typename U> pmem::allocator<T>::allocator(const 
  
 pmem::allocator<U>&); 
  
 template <typename U> pmem::allocator(allocator<U>&& other); 
  
 pmem::allocator<T>::~allocator(); 
  
 T* pmem::allocator<T>::allocate(std::size_t n) const; 
  
 void pmem::allocator<T>::deallocate(T* p, std::size_t n) const ; 
  
 template <class U, class... Args> void pmem::allocator<T>::construct(U* p, Args... args) 
 const; 
  
 void pmem::allocator<T>::destroy(T* p) const;
  
  
 For more information about the pmem::allocator class template, refer to the pmem 
 allocator(3) man page.",NA
 Nested Containers,"Multilevel containers such as a vector of lists, tuples, maps, strings, and so on pose 
 challenges in handling the nested objects.
  
  
 Imagine you need to create a vector of strings and store it in persistent memory. The 
 challenges – and their solutions – for this task include: 
  
  1. Challenge: The std::string cannot be used for this purpose because it is 
 an alias of the std::basic_string. The std::allocator requires a new 
 alias that uses pmem:allocator. 
  
 Solution: A new alias called pmem_string is defined as a typedef 
 of std::basic_string when created with pmem::allocator.
  
 169",NA
 C++ Examples,"This section presents several full-code examples demonstrating the use of 
 libmemkind using C and C++.",NA
 Using the pmem::allocator,"As mentioned earlier, you can use pmem::allocator with any STL-like data structure. 
 The code sample in Listing 
 10-10
  includes a pmem_allocator.h header file to use 
 pmem::allocator.
  
 Listing 10-10.
  pmem_allocator.cpp: using pmem::allocator with std:vector
  
  37  #include <pmem_allocator.h>
  
  38  #include <vector>
  
  39  #include <cassert>
  
  40
  
  41  int main(int argc, char *argv[]) {
  
  42      const size_t pmem_max_size = 64 * 1024 * 1024; //64 MB 43      
 const std::string pmem_dir(""/daxfs"");
  
  44
  
  45      // Create allocator object
  
  46      libmemkind::pmem::allocator<int>
  
  47          alc(pmem_dir, pmem_max_size);
  
  48
  
 170",NA
 Creating a Vector of Strings,"Listing 
 10-11
  shows how to create a vector of strings that resides in persistent 
 memory. We define pmem_string as a typedef of std::basic_string with 
 pmem::allocator. In this example, std::scoped_allocator_adaptor allows the vector to 
 propagate the pmem::allocator instance to all pmem_string objects stored in the vector 
 object.
  
 Listing 10-11.
  vector_of_strings.cpp: creating a vector of strings
  
  37  #include <pmem_allocator.h>
  
  38  #include <vector>
  
  39  #include <string>
  
  40  #include <scoped_allocator>
  
  41  #include <cassert>
  
 171",NA
 Expanding Volatile Memory ,NA,NA
Using Persistent Memory,"Persistent memory is treated by the kernel as a device. In a typical use-case, a persistent 
 memory-aware file system is created and mounted with the 
 –o dax
  option, and files are 
 memory-mapped into the virtual address space of a process to give the application direct 
 load/store access to persistent memory regions.
  
 A new feature was added to the Linux kernel v5.1 such that persistent memory can 
 be used more broadly as volatile memory. This is done by binding a persistent memory 
 device to the kernel, and the kernel manages it as an extension to DRAM. Since persistent 
 memory has different characteristics than DRAM, memory provided by this device is 
 visible as a separate NUMA node on its corresponding socket.
  
 To use the MEMKIND_DAX_KMEM kind, you need pmem to be available using 
 device 
 DAX
 , which exposes pmem as devices with names like /dev/dax*. If you have an existing 
 dax device and want to migrate the device model type to use DEV_DAX_KMEM, use:
  
 $ sudo daxctl migrate-device-model
  
  
 To create a new dax device using all available capacity on the first available region 
 (NUMA node), use:
  
 $ sudo ndctl create-namespace --mode=devdax --map=mem
  
 To create a new dax device specifying the region and capacity, use:
  
 $ sudo ndctl create-namespace --mode=devdax --map=mem --region=region0 --
 size=32g
  
 To display a list of namespaces, use:
  
 $ ndctl list
  
 If you have already created a namespace in another mode, such as the default fsdax, 
 you can reconfigure the device using the following where namespace0.0 is the existing 
 namespace you want to reconfigure:
  
 $ sudo ndctl create-namespace --mode=devdax --map=mem --force -e namespace0.0
  
  
 For more details about creating new namespace read 
 https://docs.pmem.io/ 
 ndctl-users-guide/managing-namespaces#creating-namespaces
 .
  
 173",NA
 libvmemcache: An Efficient Volatile ,NA,NA
Key-Value Cache for Large-Capacity ,NA,NA
Persistent Memory,"Some existing in-memory databases (IMDB) rely on manual dynamic memory allocations 
 (malloc, jemalloc, tcmalloc), which can exhibit external and internal memory 
 fragmentation when run for a long period of time, leaving large amounts of memory un- 
 allocatable. Internal and external fragmentation is briefly explained as follows:
  
 • 
 Internal fragmentation
  occurs when more memory is allocated than is 
 required, and the unused memory is contained within the allocated 
 region. For example, if the requested allocation size is 200 bytes, a 
 chunk of 256 bytes is allocated.
  
 • 
 External fragmentation
  occurs when variable memory sizes are 
 allocated dynamically, resulting in a failure to allocate a contiguous 
 chunk of memory, although the requested chunk of memory remains 
 available in the system. This problem is more pronounced when large 
 capacities of persistent memory are being used as volatile memory. 
  
 Applications with substantially long runtimes need to solve this 
 problem, especially if the allocated sizes have considerable variation. 
  
 Applications and runtime environments handle this problem in 
 different ways, for example:
  
 • Java and .NET use compacting garbage collection
  
 • Redis and Apache Ignite* use defragmentation algorithms
  
 • Memcached uses a slab allocator
  
 Each of the above allocator mechanisms has pros and cons. Garbage collection and 
 defragmentation algorithms require processing to occur on the heap to free unused 
 allocations or move data to create contiguous space. Slab allocators usually define a fixed 
 set of different sized buckets at initialization without knowing how many of each bucket 
  
 177",NA
 libvmemcache Overview,"libvmemcache is an embeddable and lightweight in-memory caching solution with a 
 key-value store at its core. It is designed to take full advantage of large-capacity 
 memory, such as persistent memory, efficiently using memory mapping in a scalable 
 way. It is optimized for use with memory-addressable persistent storage through a 
 DAX-enabled file system that supports load/store operations. libvmemcache has these 
 unique characteristics:
  
 • The extent-based memory allocator sidesteps the fragmentation 
 problem that affects most in-memory databases, and it allows the 
 cache to achieve very high space utilization for most workloads.
  
 • Buffered LRU (least recently used) combines a traditional LRU 
 doubly linked list with a non-blocking ring buffer to deliver high 
 scalability on modern multicore CPUs.
  
 • A unique indexing critnib data structure delivers high performance and 
 is very space efficient.
  
 The cache for libvmemcache is tuned to work optimally with relatively large 
 value sizes. While the smallest possible size is 256 bytes, libvmemcache performs 
 best if the expected value sizes are above 1 kilobyte.
  
 libvmemcache has more control over the allocation because it implements a custom 
 memory-allocation scheme using an extents-based approach (like that of file system 
 extents). libvmemcache can, therefore, concatenate and achieve substantial space 
 efficiency. Additionally, because it is a cache, it can evict data to allocate new entries in a 
 worst-case scenario. libvmemcache will 
 always
  allocate exactly as much memory as it 
 freed, minus metadata overhead. This is not true for caches based on common memory 
 allocators such as memkind. libvmemcache is designed to work with terabyte-sized in-
 memory workloads, with very high space utilization.
  
 178",NA
 libvmemcache Design ,"libvmemcache has two main design aspects:
  
  
  1. Allocator design to improve/resolve fragmentation issues
  
  2. A scalable and efficient LRU policy",NA
 Extent-Based Allocator,"libvmemcache can solve fragmentation issues when working with terabyte-sized in- 
 memory workloads and provide high space utilization. Figure 
 10-5
  shows a workload 
 example that creates many small objects, and over time, the allocator stops due to 
 fragmentation.
  
 180",NA
 Scalable Replacement Policy,"An LRU cache is traditionally implemented as a doubly linked list. When an item is 
 retrieved from this list, it gets moved from the middle to the front of the list, so it is not 
 evicted. In a multithreaded environment, multiple threads may contend with the front 
 element, all trying to move elements being retrieved to the front. Therefore, the front 
 element is always locked (along with other locks) before moving the element being 
 retrieved, which results in lock contention. This method is not scalable and is inefficient.
  
 A buffer-based LRU policy creates a scalable and efficient replacement policy. A non- 
 blocking ring buffer is placed in front of the LRU linked list to track the elements being 
 retrieved. When an element is retrieved, it is added to this buffer, and only when the 
 buffer is full (or the element is being evicted), the linked list is locked, and the elements 
 in that buffer are processed and moved to the front of the list. This method preserves the 
 LRU policy and provides a scalable LRU mechanism with minimal performance impact. 
 Figure 
 10-7
  shows a ring buffer-based design for the LRU algorithm.
  
  
 Figure 10-7. 
 A ring buffer-based LRU design
  
 182",NA
 Using libvmemcache,"Table 
 10-3
  lists the basic functions that libvmemcache provides. For a complete list, 
  
 see the libvmemcache man pages (
 https://pmem.io/vmemcache/manpages/master/
  
 vmemcache.3.html
 ).
  
 Table 10-3. 
 The libvmemcache functions
  
 Function Name
  
 Description
  
 vmemcache_new
  
 Creates an empty unconfigured vmemcache instance with 
 default 
  
 values: eviction_policy=VMeMCaChe_replaCeMent_lrU
  
 extent_size = VMeMCahe_Min_eXtent
  
 VMeMCaChe_Min_pool
  
 vmemcache_add 
  
 associates the cache with a path.
  
 vmemcache_set_size 
  
 sets the size of the cache.
  
 vmemcache_set_extent_size 
  
 sets the block size of the cache (256 bytes minimum).
  
 vmemcache_set_eviction_policy 
  
 sets the eviction policy:
  
 1. VMeMCaChe_replaCeMent_none
  
 2. VMeMCaChe_replaCeMent_lrU
  
 vmemcache_add 
  
 associates the cache with a given path on a DaX-enabled 
 file 
  
 system or non-DaX-enabled file system.
  
 vmemcache_delete 
  
 frees any structures associated with the cache.
  
 vmemcache_get 
  
 searches for an entry with the given key, and if found, the 
 entry’s 
  
 value is copied to vbuf.
  
 vmemcache_put 
  
 inserts the given key-value pair into the cache.
  
 vmemcache_evict 
  
 removes the given key from the cache.
  
 vmemcache_callback_on_evict 
  
 Called when an entry is being removed from the cache.
  
 vmemcache_callback_on_miss 
  
 Called when a get query fails to provide an opportunity to 
 insert 
  
 the missing key.",NA
 Summary,"This chapter showed how persistent memory’s large capacity can be used to hold 
 volatile application data. Applications can choose to allocate and access data from DRAM 
 or persistent memory or both.
  
  
 memkind is a very flexible and easy-to-use library with semantics that are similar 
 to the libc malloc/free APIs that developers frequently use.
  
 libvmemcache is an embeddable and lightweight in-memory caching solution that 
 allows applications to efficiently use persistent memory’s large capacity in a scalable 
 way. libvmemcache is an open source project available on GitHub at 
 https://github. 
 com/pmem/vmemcache
 .
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will 
 need to obtain permission directly from the copyright holder.
  
 186",NA
CHAPTER 11,NA,NA
Designing Data ,NA,NA
Structures for ,NA,NA
Persistent Memory,"Taking advantage of the unique characteristics of persistent memory, such as byte 
 addressability, persistence, and update in place, allows us to build data structures that 
 are much faster than any data structure requiring serialization or flushing to a disk. 
 However, this comes at a cost. Algorithms must be carefully designed to properly persist 
 data by flushing CPU caches or using non-temporal stores and memory barriers to 
 maintain data consistency. This chapter describes how to design such data structures 
 and algorithms and shows what properties they should have.",NA
 Contiguous Data Structures and ,NA,NA
Fragmentation,"Fragmentation is one of the most critical factors to consider when designing a data 
 structure for persistent memory due to the length of heap life. A persistent heap can 
 live for years with different versions of an application. In volatile use cases, the heap is 
 destroyed when the application exits. The life of the heap is usually measured in hours, 
 days, or weeks.
  
 Using file-backed pages for memory allocation makes it difficult to take advantage of 
 the operating system–provided mechanisms for minimizing fragmentation, such as 
 presenting discontinuous physical memory as a contiguous virtual region. It is possible 
 to manually manage virtual memory at a low granularity, producing a page-level 
 defragmentation mechanism for objects in user space. But this mechanism could lead to 
 complete fragmentation of physical memory and an inability to take advantage of huge 
 pages. This can cause an increased number of translation lookaside buffer (TLB) misses, 
 which significantly slows down the entire application. To make effective use of",NA
 Internal and External Fragmentation,"Internal fragmentation refers to space that is overprovisioned inside allocated blocks. An 
 allocator always returns memory in fixed-sized 
 chunks
  or 
 buckets
 . The allocator must 
 determine what size each bucket is and how many different sized buckets it provides. If 
 the size of the memory allocation request does not exactly match a predefined bucket 
 size, the allocator will return a larger memory bucket. For example, if the application 
 requests a memory allocation of 200KiB, but the allocator has bucket sizes of 128KiB 
 and 256KiB, the request is allocated from an available 256KiB bucket. The allocator must 
 usually return a memory chunk with a size divisible by 16 due to its internal alignment 
 requirements.
  
 External fragmentation occurs when free memory is scattered in small blocks. 
  
 For example, imagine using up the entire memory with 4KiB allocations. If we then free 
 every other allocation, we have half of the memory available; however, we cannot 
 allocate more than 4KiB at once because that is the maximum size of any contiguous free 
 space. Figure 
 11-1
  illustrates this fragmentation, where the red cells represent allocated 
 space and the white cells represent free space.
  
  
 Figure 11-1. 
 External fragmentation
  
  
 When storing a sequence of elements in persistent memory, several possible data 
 structures can be used:
  
 • Linked list: Each node is allocated from persistent memory.
  
 • Dynamic array (vector): A data structure that pre-allocates memory in 
 bigger chunks. If there is no free space for new elements, it allocates a 
 new array with bigger capacity and moves all elements from the old 
 array to the new one.
  
 • Segment vector: A list of fixed-size arrays. If there is no free space left in 
 any segment, a new one is allocated.
  
 188",NA
 Atomicity and Consistency,"Guaranteeing consistency requires the proper ordering of stores and making sure data 
 is stored persistently. To make an atomic store bigger than 8 bytes, you must use some 
 additional mechanisms. This section describes several mechanisms and discusses their 
 memory and time overheads. For the time overhead, the focus is on analyzing the 
 number of flushes and memory barriers used because they have the biggest impact on 
 performance.",NA
 Transactions,"One way to guarantee atomicity and consistency is to use transactions (described in 
 detail in Chapter 
 7
 ). Here we focus on how to design a data structure to use transactions 
 efficiently. An example data structure that uses transactions is described in the “Sorted 
 Array with Versioning” section later in this chapter.
  
  
 Transactions are the simplest solution for guaranteeing consistency. While using 
 transactions can easily make most operations atomic, two items must be kept in mind. 
  
 First, transactions that use logging always introduce memory and time overheads. 
 Second, in the case of undo logging, the memory overhead is proportional to the size of 
 data you modify, while the time overhead depends on the number of snapshots. Each 
 snapshot must be persisted prior to the modification of snapshotted data.
  
 1
  Using the libpmemobj allocator, it is also possible to easily lower internal fragmentation by 
 using allocation classes (see Chapter 
 7
 ).
  
 189",NA
 Copy-on-Write and Versioning,"Another way to maintain consistency is the copy-on-write (CoW) technique. In this 
 approach, every modification creates a new version at a new location whenever you 
 want to modify some part of a persistent data structure. For example, a node in a linked 
 list can use the CoW approach as described in the following:
  
  1. Create a copy of the element in the list. If a copy is dynamically 
 allocated in persistent memory, you should also save the pointer 
 in persistent memory to avoid a memory leak. If you fail to do 
 that and the application crashes after the allocation, then on the 
 application restart, newly allocated memory will be unreachable.
  
  2. Modify the copy and persist the changes.
  
  3. Atomically change the original element with the copy and persist the 
 changes, then free the original node if needed. After this step 
 successfully completes, the element is updated and is in a 
 consistent state. If a crash occurs before this step, the original 
 element is untouched.
  
 192",NA
 Selective Persistence,"Persistent memory is faster than disk storage but potentially slower than DRAM. Hybrid 
 data structures, where some parts are stored in DRAM and some parts are in persistent 
 memory, can be implemented to accelerate performance. Caching previously computed 
 values or frequently accessed parts of a data structure in DRAM can improve access 
 latency and improve overall performance.
  
 Data does not always need to be stored in persistent memory. Instead, it can be 
 rebuilt during the restart of an application to provide a performance improvement 
 during runtime given that it accesses data from DRAM and does not require 
  
 transactions. An example of this approach appears in “Hash Table with Transactions and 
 Selective Persistence.”",NA
 Example Data Structures,"This section presents several data structure examples that were designed using the 
 previously described methods for guaranteeing consistency. The code is written in C++ 
 and uses libpmemobj-cpp. See Chapter 
 8
  for more information about this library.
  
 193",NA
 Hash Table with Transactions,"We present an example of a hash table implemented using transactions and containers 
 using libpmemobj-cpp.
  
 As a quick primer to some, and a refresher to other readers, a hash table is a data 
 structure that maps keys to values and guarantees O(1) lookup time. It is usually 
 implemented as an array of buckets (a bucket is a data structure that can hold one or 
 more key-value pairs). When inserting a new element to the hash table, a hash function 
 is applied to the element’s key. The resulting value is treated as an index of a bucket to 
 which the element is inserted. It is possible that the result of the hash function for 
 different keys will be the same; this is called a 
 collision
 . One method for resolving 
 collisions is to use separate chaining. This approach stores multiple key-value pairs in 
 one bucket; the example in Listing 
 11-4
  uses this method.
  
 For simplicity, the hash table in Listing 
 11-4
  only provides the const Value& get(const 
 std::string &key) and void put(const std::string &key, const Value &value) methods. It 
 also has a fixed number of buckets. Extending this data structure to support the remove 
 operation and to have a dynamic number of buckets is left as an exercise to you.
  
 Listing 11-4.
  Implementation of a hash table using transactions
  
 38   #include <functional> 
  
 39   #include <libpmemobj++/p.hpp> 
  
 40   #include <libpmemobj++/persistent_ptr.hpp> 41   
 #include <libpmemobj++/pext.hpp> 
  
 42   #include <libpmemobj++/pool.hpp> 
  
 43   #include <libpmemobj++/transaction.hpp> 44   
 #include <libpmemobj++/utils.hpp> 
  
 45   #include <stdexcept> 
  
 46   #include <string> 
  
 47 
  
 48   #include ""libpmemobj++/array.hpp"" 
  
 49   #include ""libpmemobj++/string.hpp"" 
  
 50   #include ""libpmemobj++/vector.hpp"" 
  
 51
  
 194",NA
 Hash Table with Transactions and Selective Persistence,"This example shows how to modify a persistent data structure (hash table) by moving 
 some data out of persistent memory. The data structure presented in Listing 
 11-5
  is a 
 modified version of the hash table in Listing 
 11-4
  and contains the implementation of 
 this hash table design. Here we store only the vector of keys and vector of values in 
 persistent memory. On application startup, we build the buckets and store them in 
 volatile memory for faster processing during runtime. The most noticeable performance 
 gain would be in the get() method.
  
 197",NA
 Sorted Array with Versioning,"This section presents an overview of an algorithm for inserting elements into a sorted 
 array and preserving the order of elements. This algorithm guarantees data 
 consistency using the versioning technique.
  
 First, we describe the layout of our sorted array. Figure 
 11-2
  and Listing 
 11-6
  show 
 that there are two arrays of elements and two size fields. Additionally, one current field 
 stores information about which array and size variable is currently used.
  
  
 Figure 11-2. 
 Sorted array layout
  
 202",NA
 Summary,"This chapter shows how to design data structures for persistent memory, considering 
 its characteristics and capabilities. We discuss fragmentation and why it is problematic 
 in the case of persistent memory. We also present a few different methods of 
 guaranteeing data consistency; using transactions is the simplest and least error-prone 
 method. Other approaches, such as copy-on-write or versioning, can perform better, but 
 they are significantly more difficult to implement correctly.
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 206",NA
CHAPTER 12,NA,NA
Debugging ,NA,NA
Persistent Memory ,NA,NA
Applications,"Persistent memory programming introduces new opportunities that allow developers to 
 directly persist data structures without serialization and to access them in place without 
 involving classic block I/O. As a result, you can merge your data models and avoid the 
 classic split between data in memory – which is volatile, fast, and byte addressable – 
 with data on traditional storage devices, which is non-volatile but slower.
  
 Persistent memory programming also brings challenges. Recall our discussion about 
 power-fail protected persistence domains in Chapter 
 2
 : When a process or system 
 crashes on an Asynchronous DRAM Refresh (ADR)-enabled platform, data residing in the 
 CPU caches that has not yet been flushed, is lost. This is not a problem with volatile 
 memory because all the memory hierarchy is volatile. With persistent memory, however, 
 a crash can cause permanent data corruption. How often must you flush data? Flushing 
 too frequently yields suboptimal performance, and not flushing often enough leaves the 
 potential for data loss or corruption.
  
 Chapter 
 11
  described several approaches to designing data structures and using 
 methods such as copy-on-write, versioning, and transactions to maintain data integrity. 
  
 Many libraries within the Persistent Memory Development Kit (PMDK) provide 
 transactional updates of data structures and variables. These libraries provide optimal 
 CPU cache flushing, when required by the platform, at precisely the right time, so you 
 can program without concern about the hardware intricacies.
  
 This programming paradigm introduces new dimensions related to errors and 
 performance issues that programmers need to be aware of. The PMDK libraries reduce 
 errors in persistent memory programming, but they cannot eliminate them. This chapter",NA
 pmemcheck for Valgrind,"pmemcheck is a Valgrind (
 http://www.valgrind.org/
 ) tool developed by Intel. It is very 
 similar to memcheck, which is the default tool in Valgrind to discover memory-related 
 bugs but adapted for persistent memory. Valgrind is an instrumentation framework for 
 building dynamic analysis tools. Some Valgrind tools can automatically detect many 
 memory management and threading bugs and profile your programs in detail. You can 
 also use Valgrind to build new tools.
  
 To run pmemcheck, you need a modified version of Valgrind supporting the new 
 CLFLUSHOPT and CLWB flushing instructions. The persistent memory version of 
 Valgrind includes the pmemcheck tool and is available from 
 https://github.com/pmem/valgrind
 . 
  
 Refer to the README.md within the GitHub project for installation instructions.
  
 All the libraries in PMDK are already instrumented with pmemcheck. If you use 
 PMDK for persistent memory programming, you will be able to easily check your code 
 with pmemcheck without any code modification.
  
  
 Before we discuss the pmemcheck details, the following two sections demonstrate 
 how it identifies errors in an out-of-bounds and a memory leak example.",NA
 Stack Overflow Example,"An out-of-bounds scenario is a stack/buffer overflow bug, where data is written or 
 read beyond the capacity of the stack or array. Consider the small code snippet in 
 Listing 
 12-1
 .
  
 Listing 12-1.
  stackoverflow.c: Example of an out-of-bound bug
  
  32  #include <stdlib.h>
  
  33
  
  34  int main() {
  
  35          int *stack = malloc(100 * sizeof(int)); 36          
 stack[100] = 1234;
  
 208",NA
 Memory Leak Example,"Memory leaks are another common issue. Consider the code in Listing 
 12-3
 .
  
 Listing 12-3.
  leak.c: Example of a memory leak
  
  32  #include <stdlib.h>
  
  33
  
  34  void func(void) {
  
 209",NA
 Intel Inspector – Persistence Inspector,"Intel Inspector – Persistence Inspector is a runtime tool that developers use to detect 
 programming errors in persistent memory programs. In addition to cache flush misses, 
 this tool detects
  
 210",NA
 Stack Overflow Example,"The Listing 
 12-5
  example demonstrates how to use the command-line interface to 
 perform the analysis and collect the data and then switches to the GUI to examine 
 the results in detail. To collect the data, we use the inspxe-cl utility with the –c=mi2 
 collection option for detecting memory problems.
  
 Listing 12-5.
  Running Intel Inspector with code Listing 
 12-1
  
 $ inspxe-cl -c=mi2 -- ./stackoverflow
  
 1 new problem(s) found
  
  
  1 Invalid memory access problem(s) detected
  
 Intel Inspector creates a new directory with the data and analysis results, and prints 
 a summary of findings to the terminal. For the stackoverflow app, it detected one invalid 
 memory access.
  
 After launching the GUI using inspxe-gui, we open the results collection through the 
 File 
 ➤
  Open 
 ➤
  Result
  menu and navigate to the directory created by inspxe-cli. The 
 directory will be named r000mi2 if it is the first run. Within the directory is a file named 
 r000mi2.inspxe. Once opened and processed, the GUI presents the data shown in Figure 
 12-1
 .
  
 211",NA
 Memory Leak Example,"The Listing 
 12-6
  example runs Intel Inspector using the leak.c code from Listing 
 12-2 
 and uses the same arguments from the stackoverflow program to detect memory issues.
  
 Listing 12-6.
  Running Intel Inspector with code Listing 
 12-2
  
 $ inspxe-cl -c=mi2 -- ./leak
  
 1 new problem(s) found
  
  
  1 Memory leak problem(s) detected
  
 212",NA
 Common Persistent Memory ,NA,NA
Programming Problems,"This section reviews several coding and performance problems you are likely to 
 encounter, how to catch them using the pmemcheck and Intel Inspector tools, and how 
 to resolve the issues.
  
 The tools we use highlight deliberately added issues in our code that can cause bugs, 
 data corruption, or other problems. For pmemcheck, we show how to bypass data 
 sections that should not be checked by the tool and use macros to assist the tool in better 
 understanding our intent.",NA
 Nonpersistent Stores,"Nonpersistent stores refer to data written to persistent memory but not flushed 
 explicitly. 
  
 It is understood that if the program writes to persistent memory, it wishes for those 
 writes to be persistent. If the program ends without explicitly flushing writes, there is 
 an open possibility for data corruption. When a program exits gracefully, all the 
 pending writes in the CPU caches are flushed automatically. However, if the program 
 were to crash unexpectedly, writes still residing in the CPU caches could be lost.
  
  
 Consider the code in Listing 
 12-7
  that writes data to a persistent memory device 
 mounted to /mnt/pmem without flushing the data.
  
 Listing 12-7.
  Example of writing to persistent memory without flushing
  
  32  #include <stdio.h>
  
  33  #include <sys/mman.h>
  
  34  #include <fcntl.h>
  
  35
  
  36  int main(int argc, char *argv[]) {
  
  37      int fd, *data;
  
  38      fd = open(""/mnt/pmem/file"", O_CREAT|O_RDWR, 0666); 39      
 posix_fallocate(fd, 0, sizeof(int));
  
  40      data = (int *) mmap(NULL, sizeof(int), PROT_READ | 41                      
 PROT_WRITE, MAP_SHARED_VALIDATE | 42                      MAP_SYNC, 
 fd, 0);
  
 214",NA
 Stores Not Added into a Transaction,"When working within a transaction block, it is assumed that all the modified persistent 
 memory addresses were added to it at the beginning, which also implies that their 
 previous values are copied to an undo log. This allows the transaction to implicitly flush 
 added memory addresses at the end of the block or roll back to the old values in the 
 event of an unexpected failure. A modification within a transaction to an address that is 
 not added to the transaction is a bug that you must be aware of.
  
 Consider the code in Listing 
 12-21
  that uses the libpmemobj library from PMDK. It 
 shows an example of writing within a transaction using a memory address that is not 
 explicitly tracked by the transaction.
  
 Listing 12-21.
  Example of writing within a transaction with a memory address 
 not added to the transaction
  
  33  #include <libpmemobj.h>
  
  34
  
  35  struct my_root {
  
  36      int value;
  
  37      int is_odd;
  
  38  };
  
  39
  
  40  // registering type 'my_root' in the layout 41  
 POBJ_LAYOUT_BEGIN(example);
  
  42  POBJ_LAYOUT_ROOT(example, struct my_root); 43  
 POBJ_LAYOUT_END(example);
  
  44
  
 228",NA
Note,NA,NA
" For a refresh on the definitions of a layout, root object, or macros ",NA,NA
used in listing ,NA,NA
12-21,NA,NA
", see Chapter ",NA,NA
7,NA,NA
 where we introduce libpmemobj.,"In lines 35-38, we create a my_root data structure, which has two integer members: 
 value and is_odd. These integers are modified inside a transaction (lines 52-61), setting 
 value=4 and is_odd=0. On line 57, we are only adding the value variable to the 
 transaction, leaving is_odd out. Given that persistent memory is not natively supported 
 in C, there is no way for the compiler to warn you about this. The compiler cannot 
 distinguish between pointers to volatile memory vs. those to persistent memory. 
  
 Listing 
 12-22
  shows the response from running the code through pmemcheck.
  
 229",NA
 Memory Added to Two Different Transactions,"In the case where one program can work with multiple transactions simultaneously, 
 adding the same memory object to multiple transactions can potentially corrupt data. 
 This can occur in PMDK, for example, where the library maintains a different transaction 
 per thread. If two threads write to the same object within different transactions, after an 
 application crash, a thread might overwrite modifications made by another thread in a 
 different transaction. In database systems, this problem is known as 
 dirty reads.
  Dirty 
 reads violate the isolation requirement of the ACID (atomicity, consistency, isolation, 
 durability) properties, as shown in Figure 
 12-5
 .
  
 233",NA
 Memory Overwrites,"When multiple modifications to the same persistent memory location occur before the 
 location is made persistent (that is, flushed), a memory overwrite occurs. This is a 
 potential data corruption source if a program crashes because the final value of the 
 persistent variable can be any of the values written between the last flush and the crash. 
  
 It is important to know that this may not be an issue if it is in the code by design. 
 We recommend using volatile variables for short-lived data and only write to 
 persistent variables when you want to persist data.
  
  
 Consider the code in Listing 
 12-31
 , which writes twice to the data variable inside the 
 main() function (lines 62 and 63) before we call flush() on line 64.
  
 Listing 12-31.
  Example of persistent memory overwriting – variable data – 
 before flushing
  
  33  #include <emmintrin.h>
  
  34  #include <stdint.h>
  
  35  #include <stdio.h>
  
  36  #include <sys/mman.h>
  
  37  #include <fcntl.h>
  
  38  #include <valgrind/pmemcheck.h>
  
  39
  
  40  void flush(const void *addr, size_t len) {
  
  41      uintptr_t flush_align = 64, uptr;
  
  42      for (uptr = (uintptr_t)addr & ~(flush_align - 1); 43              uptr 
 < (uintptr_t)addr + len;
  
  44              uptr += flush_align)
  
  45          _mm_clflush((char *)uptr);
  
  46  }
  
  47
  
  48  int main(int argc, char *argv[]) {
  
  49      int fd, *data;
  
  50
  
  51      fd = open(""/mnt/pmem/file"", O_CREAT|O_RDWR, 0666); 52      
 posix_fallocate(fd, 0, sizeof(int));
  
  53
  
 240",NA
 Unnecessary Flushes,"Flushing should be done carefully. Detecting unnecessary flushes, such as redundant 
 ones, can help improve code performance. The code in Listing 
 12-33
  shows a redundant 
 call to the flush() function on line 64.
  
 Listing 12-33.
  Example of redundant flushing of a persistent memory variable
  
  33  #include <emmintrin.h>
  
  34  #include <stdint.h>
  
  35  #include <stdio.h>
  
  36  #include <sys/mman.h>
  
  37  #include <fcntl.h>
  
  38  #include <valgrind/pmemcheck.h>
  
  39
  
  40  void flush(const void *addr, size_t len) {
  
  41      uintptr_t flush_align = 64, uptr;
  
  42      for (uptr = (uintptr_t)addr & ~(flush_align - 1); 43              uptr 
 < (uintptr_t)addr + len;
  
  44              uptr += flush_align)
  
  45          _mm_clflush((char *)uptr);
  
  46  }
  
  47
  
  48  int main(int argc, char *argv[]) {
  
  49      int fd, *data;
  
  50
  
 242",NA
 Out-of-Order Writes,"When developing software for persistent memory, remember that even if a cache line is 
 not explicitly flushed, that does not mean the data is still in the CPU caches. For example, 
 the CPU could have evicted it due to cache pressure or other reasons. Furthermore, the 
 same way that writes that are not flushed properly may produce bugs in the event of an 
 unexpected application crash, so do automatically evicted dirty cache lines if they violate 
 some expected order of writes that the applications rely on.
  
 To better understand this problem, explore how flushing works in the x86_64 
 and AMD64 architectures. From the user space, we can issue any of the following 
 instructions to ensure our writes reach the persistent media:
  
 • CLFLUSH
  
 • CLFLUSHOPT (needs SFENCE)
  
 • CLWB (needs SFENCE)
  
 • Non-temporal stores (needs SFENCE)
  
 The only instruction that ensures each flush is issued in order is CLFUSH because 
 each CLFLUSH instruction always does an implicit fence instruction (SFENCE). The other 
 instructions are asynchronous and can be issued in parallel and in any order. The CPU 
 can only guarantee that all flushes issued since the previous SFENCE have completed 
 when a new SFENCE instruction is explicitly executed. Think of SFENCE instructions as 
 synchronization points (see Figure 
 12-6
 ). For more information about these instructions, 
 refer to the 
 Intel software developer manuals
  and the 
 AMD software developer manuals
 .
  
 247",NA
 Summary,"This chapter provided an introduction to each tool and described how to use them. 
 Catching issues early in the development cycle can save countless hours of debugging 
 complex code later on. This chapter introduced three valuable tools – Persistence 
 Inspector, pmemcheck, and pmreorder – that persistent memory programmers will 
 want to integrate into their development and testing cycles to detect issues. We 
 demonstrated how useful these tools are at detecting many different types of common 
 programming errors.
  
 The Persistent Memory Development Kit (PMDK) uses the tools described here to 
 ensure each release is fully validated before it is shipped. The tools are tightly integrated 
 into the PMDK continuous integration (CI) development cycle, so you can quickly catch 
 and fix issues.
  
 259",NA
CHAPTER 13,NA,NA
Enabling ,NA,NA
Persistence Using ,NA,NA
a Real-World ,NA,NA
Application,"This chapter turns the theory from Chapter 
 4
  (and other chapters) into practice. 
  
 We show how an application can take advantage of persistent memory by building a 
 persistent memory-aware database storage engine. We use MariaDB (
 https:// 
 mariadb.org/
 ), a popular open source database, as it provides a pluggable storage 
 engine model. The completed storage engine is not intended for production use and 
 does not implement all the features a production quality storage engine should. We 
 implement only the basic functionality to demonstrate how to begin persistent memory 
 programming using a well known database. The intent is to provide you with a more 
 hands-on approach for persistent memory programming so you may enable persistent 
 memory features and functionality within your own application. Our storage engine is 
 left as an optional exercise for you to complete. Doing so would create a new persistent 
 memory storage engine for MariaDB, MySQL, Percona Server, and other derivatives. You 
 may also choose to modify an existing MySQL database storage engine to add persistent 
 memory features, or perhaps choose a different database entirely. 
  
 We assume that you are familiar with the preceding chapters that covered the 
 fundamentals of the persistent memory programming model and Persistent Memory 
 Development Kit (PMDK). In this chapter, we implement our storage engine using C++ 
 and libpmemobj-cpp from Chapter 
 8
 . If you are not a C++ developer, you will still find 
 this information helpful because the fundamentals apply to other languages and 
 applications.
  
 The complete source code for the persistent memory-aware database storage engine 
 can be found on GitHub at 
 https://github.com/pmem/pmdk-examples/tree/master/ 
 pmem-mariadb
 .",NA
 The Database Example,"A tremendous number of existing applications can be categorized in many ways. For the 
 purpose of this chapter, we explore applications from the common components 
 perspective, including an interface, a business layer, and a store. The interface interacts 
 with the user, the business layer is a tier where the application’s logic is implemented, 
 and the store is where data is kept and processed by the application.
  
 With so many applications available today, choosing one to include in this book that 
 would satisfy all or most of our requirements was difficult. We chose to use a database 
 as an example because a unified way of accessing data is a common denominator for 
 many applications.",NA
 Different Persistent Memory ,NA,NA
Enablement Approaches,"The main advantages of persistent memory include: 
  
 • It provides access latencies that are lower than flash SSDs.
  
 • It has higher throughput than NAND storage devices.
  
 • Real-time access to data allows ultrafast access to large datasets.
  
 • Data persists in memory after a power interruption.
  
  
 Persistent memory can be used in a variety of ways to deliver lower latency for 
 many applications:
  
 • 
 In-memory databases:
  In-memory databases can leverage 
  
 persistent memory’s larger capacities and significantly reduce restart 
 times. Once the database memory maps the index, tables, and other files, 
 the data is immediately accessible. This avoids lengthy startup times 
 where the data is traditionally read from disk and paged in to memory 
 before it can be accessed or processed. 
  
 • 
 Fraud detection:
  Financial institutions and insurance companies can 
 perform real-time data analytics on millions of records to detect 
 fraudulent transactions.
  
 • 
 Cyber threat analysis:
  Companies can quickly detect and defend 
 against increasing cyber threats.
  
 262",NA
 Developing a Persistent Memory-Aware ,NA,NA
MariaDB* Storage Engine,"The storage engine developed here is not production quality and does not implement 
 all the functionality expected by most database administrators. To demonstrate the 
 concepts described earlier, we kept the example simple, implementing table create(), 
 open(), and close() operations and INSERT, UPDATE, DELETE, and SELECT SQL 
  
 operations. Because the storage engine capabilities are quite limited without indexing, 
 we include a simple indexing system using volatile memory to provide faster access to 
 the data residing in persistent memory.
  
 Although MariaDB has many storage engines to which we could add persistent 
 memory, we are building a new storage engine from scratch in this chapter. To learn 
 more about the MariaDB storage engine API and how storage engines work, we 
 suggest reading the MariaDB “Storage Engine Development” documentation (
 https:// 
 mariadb.com/kb/en/library/storage-engines-storage-engine-development/
 ). Since 
 MariaDB is based on MySQL, you can also refer to the MySQL “Writing a Custom 
  
 263",NA
 Understanding the Storage Layer,"MariaDB provides a pluggable architecture for storage engines that makes it easier to 
 develop and deploy new storage engines. A pluggable storage engine architecture also 
 makes it possible to create new storage engines and add them to a running MariaDB 
 server without recompiling the server itself. The storage engine provides data storage 
 and index management for MariaDB. The MariaDB server communicates with the 
 storage engines through a well-defined API.
  
 In our code, we implement a prototype of a pluggable persistent memory–enabled 
 storage engine for MariaDB using the libpmemobj library from the Persistent Memory 
 Development Kit (PMDK).
  
  
 Figure 13-1. 
 MariaDB storage engine architecture diagram for persistent memory
  
 Figure 
 13-1
  shows how the storage engine communicates with libpmemobj to 
 manage the data stored in persistent memory. The library is used to turn a 
 persistent memory pool into a flexible object store.
  
 264",NA
 Creating a Storage Engine Class,"The implementation of the storage engine described here is single-threaded to support a 
 single session, a single user, and single table requests. A multi-threaded implementation 
 would detract from the focus of this chapter. Chapter 
 14
  discussed concurrency in more 
 detail. The MariaDB server communicates with storage engines through a well-defined 
 handler interface that includes a handlerton, which is a singleton handler that is 
 connected to a table handler. The handlerton defines the storage engine and contains 
 pointers to the methods that apply to the persistent memory storage engine.
  
  
 The first method the storage engine needs to support is to enable the call for a new 
 handler instance, shown in Listing 
 13-1
 .
  
 Listing 13-1.
  ha_pmdk.cc – Creating a new handler instance
  
 117  static handler *pmdk_create_handler(handlerton *hton, 118                                       
 TABLE_SHARE *table, 119                                       MEM_ROOT *mem_root); 
 120 
  
 121  handlerton *pmdk_hton;
  
 When a handler instance is created, the MariaDB server sends commands to the 
 handler to perform data storage and retrieve tasks such as opening a table, manipulating 
 rows, managing indexes, and transactions. When a handler is instantiated, the first 
 required operation is the opening of a table. Since the storage engine is a single user and 
 single-threaded implementation, only one handler instance is created.
  
  
 Various handler methods are also implemented; they apply to the storage engine as 
 a whole, as opposed to methods like create() and open() that work on a per-table basis. 
  
 Some examples of such methods include transaction methods to handle commits and 
 rollbacks, shown in Listing 
 13-2
 .
  
 Listing 13-2.
  ha_pmdk.cc – Handler methods including transactions, rollback, etc
  
 209  static int pmdk_init_func(void *p) 210  { 
  
 ...
  
 213    pmdk_hton= (handlerton *)p; 
  
 214    pmdk_hton->state=   SHOW_OPTION_YES; 215    
 pmdk_hton->create=  pmdk_create_handler;
  
 265",NA
 Creating a Database Table,"The create() method is used to create the table. This method creates all necessary files 
 in persistent memory using libpmemobj. As shown in Listing 
 13-4
 , we create a new 
 pmemobj type pool for each table using the pmemobj_create() method; this method 
 creates a transactional object store with the given total poolsize. The table is created in 
 the form of an .obj extension.
  
 Listing 13-4.
  Creating a table method
  
 1247  int ha_pmdk::create(const char *name, TABLE *table_arg, 1248                         
 HA_CREATE_INFO *create_info) 1249  { 
  
 1250
  
 266",NA
 Opening a Database Table,"Before any read or write operations are performed on a table, the MariaDB server calls 
 the open()method to open the data and index tables. This method opens all the named 
 tables associated with the persistent memory storage engine at the time the storage 
 engine starts. A new table class variable, objtab, was added to hold the PMEMobjpool. 
 The names for the tables to be opened are provided by the MariaDB server. The index 
 container in volatile memory is populated using the open() function call at the time of 
 server start using the loadIndexTableFromPersistentMemory() function.
  
 The pmemobj_open() function from libpmemobj is used to open an existing 
 object store memory pool (see Listing 
 13-5
 ). The table is also opened at the time of a 
 table creation if any read/write action is triggered.
  
 Listing 13-5.
  ha_pmdk.cc – Opening a database table
  
 290  int ha_pmdk::open(const char *name, int mode, uint test_if_locked) 291  { 
  
 ...
  
 267",NA
 Closing a Database Table,"When the server is finished working with a table, it calls the closeTable() method to 
 close the file using pmemobj_close() and release any other resources (see Listing 
 13-6
 ). 
 The pmemobj_close() function closes the memory pool indicated by objtab and deletes 
 the memory pool handle.
  
 Listing 13-6.
  ha_pmdk.cc – Closing a database table
  
 376  int ha_pmdk::close(void) 
  
 377  { 
  
 378    DBUG_ENTER(""ha_pmdk::close""); 
  
 379    DBUG_PRINT(""info"", (""close"")); 
  
 380 
  
 381    pmemobj_close(objtab); 
  
 382    objtab = NULL; 
  
 383 
  
 384    DBUG_RETURN(0); 
  
 385  }",NA
 INSERT Operation,"The INSERT operation is implemented in the write_row() method, shown in Listing 
 13- 
 7
 . During an INSERT, the row objects are maintained in a singly linked list. If the table is 
 indexed, the index table container in volatile memory is updated with the new 
  
 268",NA
 UPDATE Operation,"The server executes UPDATE statements by performing a rnd_init() or index_init() 
 table scan until it locates a row matching the key value in the WHERE clause of the 
 UPDATE statement before calling the update_row() method. If the table is an indexed 
 table, the",NA
 DELETE Operation,"The DELETE operation is implemented using the delete_row() method. Three different 
 scenarios should be considered:
  
 • Deleting an indexed value from the indexed table
  
 • Deleting a non-indexed value from the indexed table
  
 • Deleting a field from the non-indexed table
  
 For each scenario, different functions are called. When the operation is successful, 
 the entry is removed from both the index (if the table is an indexed table) and 
 persistent memory. Listing 
 13-11
  shows the logic to implement the three scenarios.
  
 Listing 13-11.
  ha_pmdk.cc – Updating existing row data
  
 594  int ha_pmdk::delete_row(const uchar *buf) 595  
 { 
  
 ...
  
 602    // Delete the field from non indexed table 
  
 603    if (active_index == 64 && table->s->keys ==0 ) { 
  
 604      if (current) 
  
 605        deleteNodeFromSLL(); 
  
 606     } else if (active_index == 64 && table->s->keys !=0 ) { // Delete 
  
 non 
 indexed column field from indexed table 
  
 607      if (current) { 
  
 608        deleteRowFromAllIndexedColumns(current); 
  
 609        deleteNodeFromSLL(); 
  
 610      } 
  
 611    } else { // Delete indexed column field from indexed table 
  
 612    database *db = database::getInstance(); 
  
 613    table_ *tab; 
  
 614    key *k; 
  
 615    KEY_PART_INFO *key_part = table->key_info[active_index].key_part; 616    if 
 (db->getTable(table->s->table_name.str, &tab)) { 
  
 617        if (tab->getKeys(key_part->field->field_name.str, &k)) { 618          rowItr 
 currNode = k->getCurrent(); 
  
 619          rowItr prevNode = std::prev(currNode);
  
 272",NA
 SELECT Operation,"SELECT is an important operation that is required by several methods. Many methods 
 that are implemented for the SELECT operation are also called from other methods. The 
 rnd_init() method is used to prepare for a table scan for non-indexed tables, resetting 
 counters and pointers to the start of the table. If the table is an indexed table, the 
 MariaDB server calls the index_init() method. As shown in Listing 
 13-14
 , the pointers are 
 initialized.
  
 Listing 13-14.
  ha_pmdk.cc – rnd_init() is called when the system wants the 
 storage engine to do a table scan
  
 869  int ha_pmdk::rnd_init(bool scan) 
  
 870  { 
  
 ...
  
 874    current=prev=NULL; 
  
 875    iter = proot->rows; 
  
 876    DBUG_RETURN(0); 
  
 877  }
  
 When the table is initialized, the MariaDB server calls the rnd_next(), index_first(), or 
 index_read_map() method, depending on whether the table is indexed or not. These 
 methods populate the buffer with data from the current object and updates the iterator 
 to the next value. The methods are called once for every row to be scanned.
  
 Listing 
 13-15
  shows how the buffer passed to the function is populated with the 
 contents of the table row in the internal MariaDB format. If there are no more objects 
 to read, the return value must be HA_ERR_END_OF_FILE.
  
 Listing 13-15.
  ha_pmdk.cc – rnd_init() is called when the system wants the 
 storage engine to do a table scan
  
 902  int ha_pmdk::rnd_next(uchar *buf) 903  
 { 
  
 ...
  
 910    memcpy(buf, iter->buf, table->s->reclength); 911    if 
 (current != NULL) { 
  
 912      prev = current; 
  
 913    }
  
 275",NA
 Summary,"This chapter provided a walk-through using libpmemobj from the PMDK to create a 
 persistent memory-aware storage engine for the popular open source MariaDB 
 database. Using persistent memory in an application can provide continuity in the event 
 of an unplanned system shutdown along with improved performance gained by storing 
 your data close to the CPU where you can access it at the speed of the memory bus. 
 While database engines commonly use in-memory caches for performance, which take 
 time to warm up, persistent memory offers an immediately warm cache upon 
 application startup.
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 276",NA
CHAPTER 14,NA,NA
Concurrency ,NA,NA
and Persistent ,NA,NA
Memory,"This chapter discusses what you need to know when building multithreaded 
 applications for persistent memory. We assume you already have experience with 
 multithreaded programming and are familiar with basic concepts such as mutexes, 
 critical section, deadlocks, atomic operations, and so on.
  
 The first section of this chapter highlights common practical solutions for building 
 multithreaded applications for persistent memory. We describe the limitation of the 
 Persistent Memory Development Kit (PMDK) transactional libraries, such as 
 libpmemobj and libpmemobj-cpp, for concurrent execution. We demonstrate simple 
 examples that are correct for volatile memory but cause data inconsistency issues on 
 persistent memory in situations where the transaction aborts or the process crashes. 
 We also discuss why regular mutexes cannot be placed as is on persistent memory and 
 introduce the persistent deadlock term. Finally, we describe the challenges of building 
 lock-free algorithms for persistent memory and continue our discussion of visibility vs. 
 persistency from previous chapters.
  
 The second section demonstrates our approach to designing concurrent data 
 structures for persistent memory. At the time of publication, we have two concurrent 
 associative C++ data structures developed for persistent memory - a concurrent hash 
 map and a concurrent map. More will be added over time. We discuss both 
 implementations within this chapter. 
  
 All code samples are implemented in C++ using the libpmemobj-cpp library 
 described in Chapter 
 8
 . In this chapter, we usually refer to libpmemobj because it 
 implements the features and libpmemobj-cpp is only a C++ extension wrapper for 
 it. The concepts are general and can apply to any programming language.",NA
 Transactions and Multithreading,"In computer science, ACID (atomicity, consistency, isolation, and durability) is a set of 
 properties of transactions intended to guarantee data validity and consistency in case 
 of errors, power failures, and abnormal termination of a process. Chapter 
 7
  
 introduced PMDK transactions and their ACID properties. This chapter focuses on the 
 relevancy of multithreaded programs for persistent memory. Looking forward, 
 Chapter 
 16
  will provide some insights into the internals of libpmemobj transactions.
  
 The small program in Listing 
 14-1
  shows that the counter stored within the root 
 object is incremented concurrently by multiple threads. The program opens the 
 persistent memory pool and prints the value of counter. It then runs ten threads, each 
 of which calls the increment() function. Once all the threads complete successfully, the 
 program prints the final value of counter.
  
 Listing 14-1.
  Example to demonstrate that PMDK transactions do not 
 automatically support isolation
  
 41  using namespace std; 
  
 42  namespace pobj = pmem::obj; 
  
 43 
  
 44  struct root { 
  
 45      pobj::p<int> counter; 
  
 46  }; 
  
 47 
  
 48  using pop_type = pobj::pool<root>; 
  
 49 
  
 50  void increment(pop_type &pop) { 
  
 51      auto proot = pop.root(); 
  
 52      pobj::transaction::run(pop, [&] { 
  
 53          proot->counter.get_rw() += 1; 
  
 54      }); 
  
 55  } 
  
 56 
  
 57  int main(int argc, char *argv[]) { 
  
 58      pop_type pop = 
  
 59          pop_type::open(""/pmemfs/file"", ""COUNTER_INC""); 60 
  
 278",NA
 Mutexes on Persistent Memory,"Our previous examples used pmem::obj::mutex as a type for the mtx member in our root 
 data structure instead of the regular std::mutex provided by Standard Template Library. 
  
 The mtx object is a member of the root object that resides in persistent memory. The 
 std::mutex type cannot be used on persistent memory because it may cause persistent 
 deadlock.
  
 A persistent deadlock happens if an application crash occurs while holding a mutex. 
  
 When the program starts, if it does not release or reinitialize the mutex at startup, 
 threads that try to acquire it will wait forever. To avoid such situations, libpmemobj 
 provides synchronization primitives that reside in persistent memory. The main feature 
 of synchronization primitives is that they are automatically reinitialized every time the 
 persistent object store pool is open.
  
  
 For C++ developers, the libpmemobj-cpp library provides C++11-like 
 synchronization primitives shown in Table 
 14-1
 .
  
 282",NA
 Atomic Operations and Persistent Memory,"Atomic operations cannot be used inside PMDK transactions for the reason described in 
 Figure 
 14-1
 . Changes made by atomic operations inside a transaction become visible to 
 other concurrent threads before the transaction is committed. It forces data 
 inconsistency issues in cases of abnormal program termination or transaction aborts. 
 Consider lock-free algorithms where concurrency is achieved by atomically updating the 
 state in memory.",NA
 Lock-Free Algorithms and Persistent Memory,"It is intuitive to think that lock-free algorithms are naturally fit for persistent memory. In 
 lock-free algorithms, thread-safety is achieved by atomic transitions between consistent 
 states, and this is exactly what we need to support data consistency in persistent 
 memory. But this assumption is not always correct.
  
 285",NA
 Concurrent Data Structures for Persistent ,NA,NA
Memory,"This section describes two concurrent data structures available in the libpmemobj-cpp 
 library: pmem::obj::concurrent_map and pmem::obj::concurrent_hash_map. Both are 
 associative data structures composed of a collection of key and value pairs, such that 
 each possible key appears at most once in the collection. The main difference between 
 them is that the concurrent hash map is unordered, while the concurrent map is ordered 
 by keys.
  
 286",NA
 Concurrent Ordered Map,"The implementation of the concurrent ordered map for persistent memory 
  
 (pmem::obj::concurrent_map) is based on a concurrent skip list data structure. Intel 
 TBB supplies tbb::concurrent_map, which is designed for volatile memory that we use 
 as a baseline for a port to persistent memory. The concurrent skip list data structure 
 can be implemented as a lock-free algorithm. But Intel chose a provably correct 
  
 1
  Intel Threading Building Blocks library (
 https://github.com/intel/tbb
 ).
  
 2
  Michael Voss, Rafael Asenjo, James Reinders. C++ Parallel Programming with Threading Building 
 Blocks; Apress, 2019; ISBN-13 (electronic): 978-1-4842-4398-5; 
 https://www.apress.com/gp/ 
 book/9781484243978
 .
  
 287",NA
 Find Operation,"Because the find operation is non-modifying, it does not have to deal with data 
  
 consistency issues. The lookup operation for the target element always begins from the 
 topmost layer. The algorithm proceeds horizontally until the next element is greater or 
 equal to the target. Then it drops down vertically to the next lower list if it cannot 
 proceed on the current level. Figure 
 14-3
  illustrates how the find operation works for 
 the element with key=9. The search starts from the highest level and immediately goes 
 from dummy head node to the node with key=4, skipping nodes with keys 1, 2, 3. On the 
 node with key=4, the search is dropped two layers down and goes to the node with 
 key=8. 
  
 Then it drops one more layer down and proceeds to the desired node with key=9.
  
 3
  M. Herlihy, Y. Lev, V. Luchangco, N. Shavit. A provably correct scalable concurrent skip list. In 
 OPODIS ‘06: Proceedings of the 10th International Conference On Principles Of Distributed 
 Systems, 2006; 
 https://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/OPODIS2006-BA. pdf
 .
  
 288",NA
 Insert Operation,"The insert operation, shown in Figure 
 14-4
 , employs fine-grained locking schema for 
 thread-safety and consists of the following basic steps to insert a new node with 
 key=7 into the list:
  
  1. Allocate the new node with randomly generated height.
  
  2. Find a position to insert the new node. We must find the 
 predecessor and successor nodes on each level.
  
  3. Acquire locks for each predecessor node and check that the successor 
 nodes have not been changed. If successor nodes have changed, 
 the algorithm returns to step 2.
  
  4. Insert the new node to all layers starting from the bottom one. Since 
 the 
 find
  operation is lock-free, we must update pointers on each 
 level atomically using store- with- release memory semantics.
  
 289",NA
 Erase Operation,"The implementation of the erase operation for pmem::obj::concurrent_map is not 
 thread-safe. This method cannot be called concurrently with other methods of the 
 concurrent ordered map because this is a memory reclamation problem that is hard to 
 solve in C++ without a garbage collector. There is a way to logically extract a node from 
 a skip list in a thread-safe manner, but it is not trivial to detect when it is safe to delete 
 the removed node because other threads may still have access to the node. There are 
 possible solutions, such as hazard pointers, but these can impact the performance of the 
 find and insert operations.",NA
 Concurrent Hash Map,"The concurrent hash map designed for persistent memory is based on tbb::concurrent_ 
 hash_map that exists in the Intel TBB. The implementation is based on a concurrent 
 hash table algorithm where elements assigned to buckets based on a hash code are 
 calculated from a key. In addition to concurrent find, insert, and erase operations, the 
 algorithm employs concurrent resizing and on-demand per-bucket rehashing.
 4 
  
  
 Figure 
 14-6
  illustrates the basic idea of the concurrent hash table. The hash table 
 consists of an array of buckets, and each bucket consists of a list of nodes and a read- 
 write lock to control concurrent access by multiple threads.
  
 4
  Anton Malakhov. Per-bucket concurrent rehashing algorithms, 2015, 
 arXiv:1509.02235v1; 
 https://arxiv.org/ftp/arxiv/papers/1509/1509.02235.pdf
 .
  
 291",NA
 Find Operation,"The find operation is a read-only event that does not change the hash map state. 
  
 Therefore, data consistency is maintained while performing a find request. The find 
 operation works by first calculating the hash value for a target key and acquires read 
 lock for the corresponding bucket. The read lock guarantees that there is no concurrent 
 modifications to the bucket while we are reading it. Inside the bucket, the find operation 
 performs a linear search through the list of nodes.",NA
 Insert Operation,"The insert method of the concurrent hash map uses the same technique to support 
 data consistency as the concurrent skip list data structure. The operation consists of 
 the following steps:
  
  1. Allocate the new node, and assign a pointer to the new node to 
  
 persistent thread-local storage.
  
  2. Calculate the hash value of the new node, and find the 
  
 corresponding bucket.
  
 292",NA
 Erase Operation,"Although the erase operation is similar to an insert (the opposite action), its 
 implementation is even simpler than the insert. The erase implementation acquires 
 the write lock for the required bucket and, using a transaction, removes the 
 corresponding node from the list of nodes within that bucket.",NA
 Summary,"Although building an application for persistent memory is a challenging task, it is more 
 difficult when you need to create a multithreaded application for persistent memory. 
  
 You need to handle data consistency in a multithreaded environment when multiple 
 threads can update the same data in persistent memory.
  
  
 If you develop concurrent applications, we encourage you to use existing libraries 
 that provide concurrent data structures designed to store data in persistent memory. 
  
 You should develop custom algorithms only if the generic ones do not fit your needs. 
  
 See the implementations of concurrent cmap and csmap engines in pmemkv, 
 described in Chapter 
 9
 , which are implemented using 
 pmem::obj::concurrent_hash_map and pmem::obj::concurrent_map, respectively.
  
 If you need to develop a custom multithreaded algorithm, be aware of the limitation 
 PMDK transactions have for concurrent execution. This chapter shows that transactions 
 do not automatically provide isolation out of the box. Changes made inside one 
 transaction become visible to other concurrent transactions before they are committed. 
 You will need to implement additional synchronization if it is required by an algorithm. 
  
 We also explain that atomic operations cannot be used inside a transaction while 
 building lock-free algorithms without transactions. This is a very complicated task if 
 your platform does not support eADR.
  
 293",NA
CHAPTER 15,NA,NA
Profiling and ,NA,NA
Performance,NA,NA
 Introduction,"This chapter first discusses the general concepts for analyzing memory and storage 
 performance and how to identify opportunities for using persistent memory for both 
 high-performance persistent storage and high-capacity volatile memory. We then 
 describe the tools and techniques that can help you optimize your code to achieve the 
 best performance.
  
 Performance analysis requires tools to collect specific data and metrics about 
 application, system, and hardware performance. In this chapter, we describe how to 
 collect this data using Intel VTune Profiler. Many other data collection options are 
 available; the techniques we describe are relevant regardless of how the data is 
 collected.",NA
 Performance Analysis Concepts,"Most concepts for performance analysis of persistent memory are similar to those 
 already established for performance analysis of shared memory programs or storage 
 bottlenecks. This section outlines several important performance considerations you 
 should understand to profile and optimize persistent memory performance and defines 
 the terms and situations we use in this chapter.",NA
 Compute-Bound vs. Memory-Bound,"Performance optimization largely involves identifying the current performance 
 bottleneck and improving it. The performance of compute-bound workloads is generally 
 limited by the number of instructions the CPU can process per cycle. For example, an 
 application doing a large number of calculations on very compact data without many 
 dependencies is usually 
 compute-bound
 . This type of workload would run faster if the",NA
 Memory Latency vs. Memory Capacity,"This concept is essential when discussing persistent memory. For this discussion, we 
 assume that DRAM access latencies are lower than persistent memory and that the 
 persistent memory capacity within the system is larger than DRAM. Workloads bound by 
 memory capacity can benefit from adding persistent memory in a volatile mode, while 
 workloads that are bound by memory latency are less likely to benefit.",NA
 Read vs. Write Performance,"While each persistent memory technology is unique, it is important to understand that 
 there is usually a difference in the performance of reads (loads) vs. writes (stores). 
 Different media types exhibit varying degrees of asymmetric read-write performance 
 characteristics, where reads are generally much faster than writes. Therefore, 
  
 understanding the mix of loads and stores in an application workload is important for 
 understanding and optimizing performance.",NA
 Memory Access Patterns,"A memory access pattern is the pattern with which a system or application reads and 
 writes to or from the memory. Memory hardware usually relies on temporal locality 
 (accessing recently used data) and spatial locality (accessing contiguous memory 
 addresses) for best performance. This is often achieved through some structure of 
 fast internal caches and intelligent prefetchers. The access pattern and level of 
 locality can drastically affect cache performance and can also have implications on 
 parallelism and distributions of workloads within shared memory systems. Cache 
 coherency can 
  
 296",NA
 I/O Storage Bound Workloads,"A program is I/O bound if it would go faster if the I/O subsystem were faster. We are 
 primarily interested in the block-based disk I/O subsystem here, but it could also include 
 other subsystems such as the network. An I/O bound state is undesirable because it 
 means that the CPU must stall its operation while waiting for data to be loaded or 
 unloaded from main memory or storage. Depending on where the data is and the latency 
 of the storage device, this can invoke a voluntary context switching of the current 
 application thread with another. A voluntary context switch occurs when a thread blocks 
 because it requires a resource that is not immediately available or takes a long time to 
 respond. With faster computation speed being the primary goal of each successive 
 computer generation, there is a strong imperative to avoid I/O bound states. Eliminating 
 them can often yield a more economic improvement in performance than upgrading the 
 CPU or memory.",NA
 Determining the Suitability of ,NA,NA
Workloads for Persistent Memory,"Persistent memory technologies may not solve every workload performance problem. 
  
 You should understand the workload and platform on which it is currently running 
 when considering persistent memory. As a simple example, consider a compute- 
 intensive workload that relies heavily on floating-point arithmetic. The performance of 
 this application is likely limited by the floating-point unit in the CPU and not any part of 
 the memory subsystem. In that case, adding persistent memory to the platform will 
 likely have little impact on this application’s performance. Now consider an application 
  
 297",NA
 Volatile Use Cases,"Chapter 
 10
  described several libraries and use cases where applications can take 
 advantage of the performance and capacity of persistent memory to store non-volatile 
 data. For volatile use cases, persistent memory will act as an additional memory tier for 
 the platform. It may be transparent to the application, such as using Memory Mode 
 supported by Intel Optane DC persistent memory, or applications can make code 
 changes to perform volatile memory allocations using libraries such as libmemkind. In 
 both cases, memory-capacity bound workloads will benefit from adding persistent 
 memory to the platform. Application performance can dramatically improve if its 
 working dataset can fit into memory and avoid paging to disk.",NA
 Identifying Workloads That Are Memory-Capacity ,NA,NA
Bound,"To determine if a workload is memory-capacity bound, you must determine the 
  
 “memory footprint” of the application. The memory footprint is the high watermark of 
 memory concurrently allocated during the application’s life cycle. Since physical 
 memory is a finite resource, you should consider the fact that the operating system and 
 other processes also consume memory. If the footprint of the operating system and all 
 memory consumers on the system are approaching or exceeding the available DRAM 
 capacity on the platform, you can assume that the application would benefit from 
 additional memory because it cannot fit all its data in DRAM. Many tools and techniques 
 can be used to determine memory footprint. VTune Profiler includes two different ways 
  
 298",NA
 Identifying the Hot Working Set Size of a Workload,"Persistent memory usually has different characteristics than DRAM; therefore, you 
 should make intelligent decisions about where data will reside. We will assume that 
 accessing data from persistent memory has higher latency than DRAM. Given the choice 
 between accessing data in DRAM and persistent memory, we would always choose 
 DRAM for performance. However, the premise of adding persistent memory in a volatile 
 configuration assumes there is not enough DRAM to fit all the data. You need to 
 understand how your workload accesses data to make choices about persistent memory 
 configuration.
  
 The working set size (WSS) is how much memory an application needs to keep 
 working. For example, if an application has 50GiB of main memory allocated and page 
 mapped, but it is only accessing 20MiB each second to perform its job, we can say that 
 the working set size is 50GiB and the “hot” data is 20MiB. It is useful to know this for 
 capacity planning and scalability analysis. The “hot working set” is the set of objects 
 accessed frequently by an application, and the “hot working set size” is the total size of 
 those objects allocated at any given time.
  
 Determining the size of the working set and hot working set is not as straightforward 
 as determining memory footprint. Most applications will have a wide range of objects 
 with varying degrees of “hotness,” and there will not be a clear line delineating which 
 objects are hot and which are not. You must interpret this information and determine 
 the hot working set size.
  
 VTune Profiler has a Memory Access analysis feature that can help determine the hot 
 and working set sizes of an application (select the “Analyze dynamic memory objects” 
 option before data collection begins). Once enough data has been collected, VTune 
 Profiler will process the data and produce a report. In the bottom-up view within the 
 GUI, a grid lists each memory object that was allocated by the application.
  
 300",NA
 Use Cases Requiring Persistence,"Use cases that take advantage of persistent memory for persistence, as opposed to the 
 volatile use cases previously described, are generally replacing slower storage devices 
 with persistent memory. Determining the suitability of a workload for this use case is 
 straightforward. If application performance is limited by storage accesses (disks, SSDs, 
 etc.), then using a faster storage solution like persistent memory could help. There are 
 several ways to identify storage bottlenecks in an application. Open source tools like 
 dstat or iostat give a high-level overview of disk activity, and tools such as VTune 
 Profiler provide a more detailed analysis.
  
 301",NA
 Performance Analysis of Workloads ,NA,NA
Using Persistent Memory,"Optimizing a workload on a system with persistent memory follows the principles 
 similar to those of optimizing a workload performance on a DRAM-only system. 
  
 The additional factors to keep in mind are: 
  
 302",NA
 Characterizing the Workload,"The performance of a workload on a persistent memory system depends on a 
 combination of the workload characteristics and the underlying hardware. The key 
 metrics to understand the workload characteristics are: 
  
 • Persistent memory bandwidth
  
 • Persistent memory read/write ratio
  
 • Paging to and from traditional storage
  
 • Working set size and footprint of the workload
  
 • Nonuniform Memory Architecture (NUMA) characteristics
  
 • Near-memory cache behavior in Memory Mode (specific to Intel 
 Optane DC persistent memory)",NA
 Memory Bandwidth and Latency,"Persistent memory, like DRAM, has limited bandwidth. When it becomes saturated, it 
 can quickly bottleneck application performance. Bandwidth limits will vary depending 
 on the platform. You can calculate the peak bandwidth of your platform using hardware 
 specifications or a memory benchmarking application.
  
 303",NA
 Persistent Memory Read-Write Ratio,"As described in “Performance Analysis Concepts,” the ratio of read and write traffic to 
 the persistent memory plays a major role in the overall performance of a workload. If 
 the ratio of persistent memory write bandwidth to read bandwidth is high, there is a 
 good chance the persistent memory write latency is impacting performance. Using 
 the Platform Profiler feature in VTune Profiler is one way to collect this information. 
  
 Figure 
 15-7
  shows the ratio of read traffic vs. all traffic to persistent memory. 
 This number should be close to 1.0 for best performance.
  
  
 Figure 15-7. 
 Read traffic ratio from VTune Profiler’s Platform Profiler analysis",NA
 Working Set Size and Memory Footprint,"As described in “Determining the Suitability of Workloads for Persistent Memory,” the 
 working set size and memory footprint of the application are important characteristics 
 to understand once a workload is running on a system with persistent memory. Metrics 
 can be collected using the tools and processes previously described.",NA
 Non-Uniform Memory Architecture (NUMA) ,NA,NA
Behavior,"Multi-socket platforms typically have persistent memory attached to each socket. 
 Accesses to persistent memory from a thread on one socket to another will incur longer 
 latencies. These “remote” accesses are some of the NUMA behaviors that can impact 
 performance. Multiple metrics can be collected to determine how much NUMA activity is 
 occurring in a workload. On Intel platforms, data moves between sockets through the 
 socket interconnect called the QuickPath Interconnect (QPI) or Ultra Path Interconnect 
 (UPI). High interconnect bandwidth may indicate NUMA-related performance issues. In 
 addition to interconnect bandwidth, some hardware provides counters to track local and 
 remote accesses to persistent memory.
  
 305",NA
 Tuning the Hardware,"The memory configuration of a system is a significant factor in determining the system’s 
 performance. The workload performance depends on a combination of workload 
 characteristics and the memory configuration. There is no single configuration that 
 provides the best value for all workloads. These factors make it important to tune the 
 hardware with respect to workload characteristics and get the maximum value out of the 
 system.",NA
 Addressable Memory Capacity,"The combined capacity of DRAM and persistent memory determines the total 
 addressable memory available on the system. You should tune the size of persistent 
 memory to accommodate the workload’s footprint.
  
 The capacity of DRAM available on the system should be large enough to 
  
 accommodate the workload’s hot working set size. A large amount of volatile traffic 
 going to persistent memory while DRAM is fully utilized is a good indicator that the 
 workload can benefit from additional DRAM size.
  
 306",NA
 Bandwidth Requirements,"The maximum available persistent memory bandwidth depends on the number of 
 channels populated with a persistent memory module. A fully populated system works 
 well for a workload with a high bandwidth requirement. Partially populated systems 
 can be used for workloads that are not as memory latency sensitive. Refer to the server 
 documentation for population guidelines.",NA
 BIOS Options,"With the introduction of persistent memory into server platforms, many features and 
 options have been added to the BIOS that provide additional tuning capabilities. The 
 options and features available within the BIOS vary for each server vendor and 
 persistent memory product. Refer to the server BIOS documentation for all the options 
 available; most share common options, including: 
  
 • Ability to change power levels to balance power consumption and 
 performance. More power delivered to persistent memory can 
 increase performance 
  
 • Enable or disable persistent memory–specific features 
  
 • Tune latency or bandwidth characteristics of persistent memory",NA
 Optimizing the Software for Persistent Memory,"There are many ways to optimize applications to use persistent memory efficiently and 
 effectively. Each application will benefit in different ways and will need to have code 
 modified accordingly. This section describes some of the optimization methods.",NA
 Guided Data Placement,"Guided data placement is the most common avenue for optimizing volatile workloads 
 on a persistent memory system. Application developers can choose to allocate a data 
 structure or object in DRAM or persistent memory. It is important to choose accurately 
 because allocating incorrectly could impact application performance. This allocation is 
 usually handled via specific APIs, for example, the allocation APIs available in the 
 Persistent Memory Development Kit (PMDK) and memkind library.
  
 307",NA
 Memory Access Optimization,"The common techniques for optimizing cache performance on DRAM-only platforms 
 also apply to persistent memory platforms. Concepts like cache-miss penalties and 
 spatial/temporal data locality are important for performance. Many tools can collect 
 performance data for caches and memory. VTune Profiler has predefined metrics for 
 each level of the memory hierarchy, including Intel Optane DC persistent memory 
 shown in Figure 
 15-9
 .
  
 308",NA
 NUMA Optimizations,"NUMA-related performance issues were described in the “Characterizing the Workload” 
 section; we discuss NUMA in more detail in Chapter 
 19
 . If you identify performance 
 issues related to NUMA memory accesses, two things should be considered: data 
 allocation vs. first access, and thread migration.",NA
Data Allocation vs. First Access,"Data allocation is the process of allocating or reserving some amount of virtual address 
 space for an object. The virtual address space for a process is the set of virtual memory 
 addresses that it can use. The address space for each process is private and cannot be 
 accessed by other processes unless it is shared. A virtual address does not represent 
 the actual physical location of an object in memory. Instead, the system maintains a 
 multilayered page table, which is an internal data structure used to translate virtual 
 addresses into their corresponding physical addresses. Each time an application thread 
  
 309",NA
Thread Migration,"Thread migration, which is the movement of software threads across sockets by the 
 operating system scheduler, is the most common cause of NUMA issues. Once objects are 
 allocated in memory, accessing them from another physical CPU from which they were 
 originally allocated incurs a latency penalty. Even though you may allocate your data on 
 a socket where the accessing thread is currently running, unless you have specific 
 affinity bindings or other safeguards, the thread may move to any other core or socket in 
 the future. You can track thread migration by identifying which cores threads are 
 running on and which sockets those cores belong to. Figure 
 15-10
  shows an example of 
 this analysis from VTune Profiler.
  
  
 Figure 15-10. 
 VTune Profiler identifying thread migration across cores and 
 sockets (packages)
  
 310",NA
 Large and Huge Pages,"The default memory page size in most operating systems is 4 kilobytes (KiB). Operating 
 systems provide many different page sizes for different application workloads and 
 requirements. In Linux, a 
 Large Page
  is 2 megabytes (MiB), and a 
 Huge Page
  is 1 gigabyte 
 (GiB). The larger page sizes can be beneficial to workload performance on persistent 
 memory in certain scenarios.
  
 For applications with a large addressable memory requirement, the size of the page 
 table being maintained by the operating system for virtual to physical address 
 translation grows significantly larger in size. The translation lookaside buffer (TLB) is a 
 small cache to make virtual-to-physical address translations faster. The efficiency of TLB 
 goes down when the number of page entries increases in the page table. Chapter 
 19
  
 describes this in more detail.
  
 Persistent memory systems that are meant for applications with a large memory 
 requirement will likely encounter the problem of large page tables and inefficient TLB 
 usage. Using large page sizes in this scenario helps reduce the number of entries in the 
 page table. The main trade-offs when using large page sizes is a higher overhead for 
 each allocation and memory fragmentation. You must be aware of the application 
 behavior before using large pages on persistent memory. An application doing frequent 
 allocation/deallocation may not be a good fit for large page optimization. The memory 
 fragmentation issue is somewhat abated by the large address space available on the 
 persistent memory systems.",NA
 Summary,"Profiling and performance optimization techniques for persistent memory systems are 
 similar to those techniques used on systems without persistent memory. This chapter 
 outlined some important concepts for understanding performance. It also provides 
 guidance for characterizing an existing application without persistent memory and 
 understanding whether it is suitable for persistent memory. Finally, it presents 
  
 311",NA
CHAPTER 16,NA,NA
PMDK Internals: ,NA,NA
Important ,NA,NA
Algorithms and ,NA,NA
Data Structures,"Chapters 
 5
  through 
 10
  describe most of the libraries contained within the 
 Persistent Memory Development Kit (PMDK) and how to use them.
  
 This chapter introduces the fundamental algorithms and data structures on which 
 libpmemobj is built. After we first describe the overall architecture of the library, we 
 discuss the individual components and the interaction between them that makes 
 libpmemobj a cohesive system.",NA
 A Pool of Persistent Memory: High-,NA,NA
Level Architecture Overview,"Figure 
 16-1
  shows that libpmemobj comprises many isolated components that build 
 on top of each other to provide a transactional object store.
  
  
 Figure 16-1. 
 The modules of the libpmemobj architecture",NA
 The Uncertainty of Memory Mapping: ,NA,NA
Persistent Memory Object Identifier,"A key concept that is important for any persistent memory application is how to 
 represent the relative position of an object within a pool of memory, and even beyond 
 it. That is, how do you implement pointers? You could rely on normal pointers, which 
  
 315",NA
 Persistent Thread Local Storage: Using ,NA,NA
Lanes,"Very early in the development of PMDK, we found that persistent memory 
  
 programming closely resembles multithreaded programming because it requires 
 restricting visibility of memory changes – either through locking or transactions – to 
 other threads or instances of the program. But that is not the only similarity. The other 
 similarity, which we discuss in this section, is how sometimes low-level code",NA
 Ensuring Power-Fail Atomicity: Redo ,NA,NA
and Undo Logging,"The two fundamental concepts libpmemobj uses to ensure power-fail safety are redo 
 and undo logging. Redo logs are used to ensure atomicity of memory allocations, while 
 undo logs are used to implement transactional snapshots. Before we discuss the many 
 different possible implementation approaches, this section describes the basic ideas.",NA
 Transaction Redo Logging,"Redo logging is a method by which a group of memory modifications that need to be 
 done atomically are stored in a log and deferred until all modifications in the group are 
 persistently stored. Once completed, the log is marked as complete, and the memory 
 modifications are processed (applied); the log can then be discarded. If the processing is 
 interrupted before it finishes, the logging is repeated until successful. Figure 
 16-4
  shows 
 the four phases of transaction redo logging.
  
  
 Figure 16-4. 
 The phases of a transaction redo log
  
 320",NA
 Transaction Undo Logging,"Undo logging is a method by which each memory region of a group (undo transaction) 
 that needs to be modified atomically is snapshotted into a log prior to the modification. 
  
 Once all memory modifications are complete, the log is discarded. If the transaction 
 is interrupted, the modifications in the log are rolled back to their original state. 
 Figure 
 16-5
  shows the three phases of the transaction undo logging.
  
  
 Figure 16-5. 
 Phases of a transaction undo log
  
 This type of log can have lower performance characteristics compared with the redo 
 log approach because it requires a barrier for every snapshot that needs to be made, and 
 the snapshotting itself must be fail-safe atomic, which presents its own challenges. An 
 undo log benefit is that the changes are visible immediately, allowing for a natural 
 programming model.
  
 The important observation here is that redo and undo logging are complimentary. 
 Use redo logging for performance-critical code and where deferred modifications are 
 not a problem; use undo logging where ease of use is important. This observation led 
 to the current design of libpmemobj where a single transaction takes advantage of 
 both algorithms.
  
 321",NA
 libpmemobj Unified Logging,"Both redo and undo logging in libpmemobj share the same internal interface and data 
 structure, which is called a unified log (or ulog for short). This is because redo and undo 
 logging only differ in the execution order of the log phases, or more precisely, when the 
 log is applied on commit or recovery. In practice, however, there are performance 
 considerations that require specialization in certain parts of the algorithm.
  
  
 The ulog data structure contains one cache line header with metadata and a variable 
 length array of data bytes. The header consists of: 
  
 • A checksum for both the header and data, used only for redo logs
  
 • A monotonically increasing generation number of a transaction in 
 the log, used only for undo logs
  
 • The total length in bytes of the data array
  
 • An offset of the next log in the group
  
 The last field is used to create a singly linked list of all logs that participate in a single 
 transaction. This is because it is impossible to predict the total required size of the log at 
 the beginning of the transaction, so the library cannot allocate a log structure that is the 
 exact required length ahead of time. Instead, the logs are allocated on demand and 
 atomically linked into a list.
  
 The unified log supports two ways of fail-safe inserting of entries:
  
  1. 
 Bulk insert
  takes an array of log entries, prepares the header of the 
 log, and creates a checksum of both the header and data. Once done, 
 a non-temporal copy, followed by a fence, is performed to store this 
 structure into persistent memory. This is the way in which a group 
 of deferred memory modifications forms a redo log with only one 
 additional barrier at the end of the transaction. 
  
 In this case, the checksum in the header is used to verify the 
 consistency of the entire log. If that checksum doesn’t match, the 
 log is skipped during recovery.
  
  2. 
 Buffer insert
  takes only a single entry, checksums it together with 
 the current generation number, and stores it in persistent 
 memory through non-temporal stores followed by a fence. This 
 method is used to create undo logs when snapshotting. Undo logs 
  
 322",NA
 Persistent Allocations: The ,NA,NA
Interface of a Transactional ,NA,NA
Persistent Allocator,"The internal allocator interface in libpmemobj is far more complex than a typical volatile 
 dynamic memory allocator. First, it must ensure fail-safety of all its operations and 
 cannot allow for any memory to become unreachable due to interruptions. Second, it 
 must be transactional so that multiple operations on the heap can be done atomically 
 alongside other modifications. And lastly, it must operate on the pool state, allocating 
 memory from specific files instead of relying on the anonymous virtual memory 
 provided by the operating system. All these factors contribute to an internal API that 
 hardly resembles the standard malloc() and free(), shown in Listing 
 16-1
 .
  
 Listing 16-1.
  The core persistent memory allocator interface that splits heap 
 operations into two distinct steps
  
 int palloc_reserve(struct palloc_heap *heap, size_t size,..., 
  
  
 struct pobj_action *act); 
  
 void palloc_publish(struct palloc_heap *heap,
  
  
  struct pobj_action *actv, size_t actvcnt,
  
  
  struct operation_context *ctx);
  
 323",NA
 Persistent Memory Heap Management: ,NA,NA
Allocator Design for Persistent Memory,"The previous section described the interface for the memory allocation used internally in 
 libpmemobj, but that was only the tip of the allocator iceberg. Before diving deeper into 
 this topic, we briefly describe the principles behind normal volatile allocators so you can 
 understand how persistent memory impacts the status quo.
  
 Traditional allocators for volatile memory are responsible for efficient – in both time 
 and space – management of operating system–provided memory pages. Precisely how 
 this should be done for the generic case is an active research area of computer science; 
 many different techniques can be used. All of them try to exploit the regularities in 
 allocation and deallocation patterns to minimize heap fragmentation.
  
  
 Most commonly used general-purpose memory allocators settled on an algorithm 
 that we refer to as “segregated fit with page reuse and thread caching.”
  
 324",NA
 ACID Transactions: Efficient Low-Level ,NA,NA
Persistent Transactions,"The four components we just described – lanes, redo logs, undo logs, and the 
 transactional memory allocator – form the basis of libpmemobj's implementation of 
 ACID transactions that we defined in Chapter 
 4
 .
  
 A transaction’s persistent state consists of three logs. First is an undo log, which 
 contains snapshots of user data. Second is an external redo log, which contains 
  
 allocations and deallocations performed by the user. Third is an internal redo log, which 
 is used to perform atomic metadata allocations and deallocations. This is technically not 
  
 328",NA
 Lazy Reinitialization of Variables: ,NA,NA
Storing the Volatile State on ,NA,NA
Persistent Memory,"While developing software for persistent memory, it is often useful to store the runtime 
 (volatile) state inside of persistent memory locations. Keeping that state consistent, 
 however, is extremely difficult, especially in multithreaded applications.
  
 The problem is the initialization of the runtime state. One solution is to simply iterate 
 over all objects at the start of the application and initialize the volatile variables then, but 
 that might significantly contribute to startup time of applications with large persistent 
 pools. The other solution is to lazily reinitialize the variables on access, which is what 
 libpmemobj does for its built-in locks. The library also exposes this mechanism through 
 an API for use with custom algorithms.
  
 Lazy reinitialization of the volatile state is implemented using a lock-free algorithm 
 that relies on a generation number stored alongside each volatile variable on persistent 
 memory and inside the pool header. The pool header resident copy is increased by two 
 every time a pool is opened. This means that a valid generation number is always even. 
 When a volatile variable is accessed, its generation number is checked against the one 
 stored in the pool header. If they match, it means that the object can be used and is 
 simply returned to the application; otherwise, the object needs to be initialized before 
 returning to ensure the initialization is thread-safe and is performed exactly once in a 
 single instance of the application.
  
 The naive implementation could use a double-checked locking, where a thread 
 would try to acquire a lock prior to initialization and verify again if the generation 
 numbers match. If they still do not match, initialize the object, and increase the number. 
  
 To avoid the overhead that comes with using locks, the actual implementation first 
 uses a compare-and-swap to set the generation number to a value that is equal to the 
 generation number of the pool minus one, which is an odd number that indicates an 
 initialization operation is in progress. If this compare-and-swap were to fail, the 
  
 330",NA
 Summary,"This chapter described the architecture and inner workings of libpmemobj. We also 
 discuss the reasons for the choices that were made during the design and 
 implementation of libpmemobj. With this knowledge, you can accurately reason about 
 the semantics and performance characteristics of code written using this library.
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License 
 (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will need 
 to obtain permission directly from the copyright holder.
  
 331",NA
CHAPTER 17,NA,NA
"Reliability, ",NA,NA
"Availability, and ",NA,NA
Serviceability (RAS),"This chapter describes the high-level architecture of reliability, availability, and 
 serviceability (RAS) features designed for persistent memory. Persistent memory RAS 
 features were designed to support the unique error-handling strategy required for an 
 application when persistent memory is used. Error handling is an important part of the 
 program’s overall reliability, which directly affects the availability of applications. The 
 error-handling strategy for applications impacts what percentage of the expected time 
 the application is available to do its job.
  
 Persistent memory vendors and platform vendors will both decide which RAS 
 features and how they will be implemented at the lowest hardware levels. Some 
 common RAS features were designed and documented in the ACPI specification, which 
 is maintained and owned by the UEFI Forum (
 https://uefi.org/
 ). In this chapter, we try 
 to attain a general perspective of these ACPI-defined RAS features and call out vendor-
 specific details if warranted.",NA
 Dealing with Uncorrectable Errors,"The main memory of a server is protected using error correcting codes (ECC). This is a 
 common hardware feature that can automatically correct many memory errors that 
 happen due to transient hardware issues, such as power spikes, soft media errors, and 
 so on. If an error is severe enough, it will corrupt enough bits that ECC cannot correct; 
 the result is called an uncorrectable error (UE).
  
  
 Uncorrectable errors in persistent memory require special RAS handling that differs 
 from how a platform may traditionally handle volatile memory uncorrectable errors.",NA
 Consumed Uncorrectable Error Handling ,"When an uncorrectable error is detected on a requested memory address, data 
 poisoning is used to inform the CPU that the data requested has an uncorrectable error. 
 When the hardware detects an uncorrectable memory error, it routes a poison bit 
 along with the data to the CPU. For the Intel architecture, when the CPU detects this 
 poison bit, it sends a processor interrupt signal to the operating system to notify it of 
 this error. This signal is called a machine check exception (MCE). The operating system 
 can then",NA
 Unconsumed Uncorrectable Error Handling,"RAS features are defined to inform software of uncorrectable errors that have been 
 discovered on the persistent memory media but have not yet been consumed by 
 software. The goal of this feature is to allow the operating system to opportunistically 
 offline or clear pages with known uncorrectable errors before they can be used by an 
 application. If the address of the uncorrectable error is already in use by an application, 
 the operating system may also choose to notify it of the unconsumed uncorrectable error 
 or wait until the application consumes the error. The operating system may choose to 
 wait on the chance that the application never tries to access the affected page and later 
 return the page to the operating system for recycling. At this time, the operating system 
 would clear or offline the uncorrectable error.
  
 336",NA
 Patrol Scrub,"Patrol scrub (also known as memory scrubbing) is a long-standing RAS feature for 
 volatile memory that can also be extended to persistent memory. It is an excellent 
 example of how a platform can discover uncorrectable errors in the background 
 during normal operation.
  
 Patrol scrubbing is done using a hardware engine, on either the platform or on the 
 memory device, which generates requests to memory addresses on the memory device. 
 The engine generates memory requests at a predefined frequency. Given enough time, it 
 will eventually access every memory address. The frequency in which patrol scrub 
 generates requests produces no noticeable impact on the memory device’s quality of 
 service.
  
 337",NA
 Unconsumed Uncorrectable Memory-Error Persistent ,NA,NA
Memory Root-Device Notification,"The ACPI specification describes a method for hardware to notify software of 
  
 unconsumed uncorrectable errors called the Unconsumed Uncorrectable Memory- 
 Error Persistent Memory Root-Device Notification. Using the ACPI-defined framework, 
 the operating system can subscribe to be notified by the platform whenever an 
  
 uncorrectable memory error is detected. It is the platform’s responsibility to receive 
 notification from persistent memory devices that an uncorrectable error has been 
 detected and take appropriate action to generate a persistent memory root-device 
 notification. Upon receipt of root-device notification, the operating system can then use 
 existing ACPI methods, such as Address Range Scrub (ARS), to discover the address of 
 the newly created uncorrectable memory error and take appropriate actions.",NA
 Address Range Scrub,"ARS is a device-specific method (_DSM) defined in the ACPI specification. Privileged 
 software can call an ACPI _DSM such as ARS at runtime to retrieve or scan for the 
 locations of uncorrectable memory errors for all persistent memory in the platform. 
 Because ARS is implemented by the platform, each vendor may implement some of the 
 functionality differently.
  
 An ARS accepts a given system address range from the caller and, like patrol scrub, 
 inspects each memory address in that range for memory errors. When ARS completes, 
 the caller is given a list of memory addresses in the given range that contains memory 
 errors. Inspection of each memory address may be handled by persistent memory 
 hardware or by the platform itself. Unlike a patrol scrub, ARS inspects each memory 
 address at a very high frequency. This increased frequency of the scrub may impact the 
 quality of service for the persistent memory hardware. Thus, ARS can optionally be 
 invoked by the caller to return the results of the previous ARS, sometimes referred to as 
 a short ARS.
  
 338",NA
 Clearing Uncorrectable Errors,"Uncorrectable errors for persistent memory will survive power loss and may require 
 special handling to clear corrupted data from the memory address. When an 
 uncorrectable error is cleared, the data at the requested memory address is modified, 
 and the error is cleared. Because hardware cannot silently modify application data, 
 clearing uncorrectable errors is the software’s responsibility. Clearing uncorrectable 
 errors is optional, and some operating systems may choose to only offline memory pages 
 that contain memory errors instead of recycling memory pages that contain 
 uncorrectable errors. In some operating systems, privileged applications may have 
 access to clear uncorrectable errors. Nevertheless, an operating system is not required to 
 provide this access.
  
 The ACPI specification defines a Clear Uncorrectable Error DSM for operating 
 systems to instruct the platform to clear the uncorrectable errors. While persistent 
 memory programming is byte addressable, clearing uncorrectable errors is not. Different 
 vendor implementations of persistent memory may specify the alignment and size of the 
 memory unit that is to be cleared by a Clear Uncorrectable Error. Any internal platform 
 or operating system list of memory errors should also be updated upon successful 
 executing of the Clear Uncorrectable Error DSM command.",NA
 Device Health,"System administrators may wish to act and mitigate any device health issues before they 
 begin to affect the availability of applications using persistent memory. To that end, 
 operating systems or management applications will want to discover an accurate picture 
 of persistent memory device health to correctly determine the reliability of the 
 persistent memory.",NA
" ACPI-Defined Health Functions (_NCH, _NBS)","The ACPI specification includes two vendor-agnostic methods for operating systems and 
 management software to call for determining the health of a persistent memory device.
  
 Get NVDIMM Current Health Information (_NCH) can be called by the operating 
 systems at boot time to get the current health of the persistent memory device and 
 take appropriate action. The values reported by _NCH can change during runtime and 
 should be monitored for changes. _NCH contains health information that shows if
  
 • The persistent memory requires maintenance
  
 • The persistent memory device performance is degraded
  
 • The operating system can assume write persistency loss on 
 subsequent power events
  
 • The operating system can assume all data will be lost on subsequent 
 power events
  
 Get NVDIMM Boot Status (_NBS) allows operating systems a vendor-agnostic method 
 to discover persistent memory health status that does not change during runtime. The 
 most significant attribute reported by _NBS is Data Loss Count (DLC). Data Loss Count is 
 expected to be used by applications and operating systems to help identify the rare case 
 where a persistent memory dirty shutdown has occurred. See “Unsafe/Dirty Shutdown” 
 later in this chapter for more information on how to properly use this attribute.",NA
 Vendor-Specific Device Health (_DSMs),"Many vendors may want to add further health attributes beyond what exists in _NBS 
 and _NCH. Vendors are free to design their own ACPI persistent memory device-
 specific methods (_DSM) to be called by the operating system and privileged 
 applications. 
  
 Although vendors implement persistent memory health discovery differently, a few 
 common health attributes are likely to exist to determine if a persistent memory device 
 requires service. These health attributes may include information such as an overall 
 health summary of the persistent memory, current persistent memory temperature, 
 persistent media error counts, and total device lifetime utilization. Many operating 
 systems, such as Linux, include support to retrieve and report the vendor-unique health 
 statistics through tools such as ndctl. The Intel persistent memory _DSM interface 
 document can be found under the “Related Specification” section of 
 https://docs.pmem.io/
 .",NA
 ACPI NFIT Health Event Notification,"Due to the potential loss of quality of service, operating systems and privileged 
  
 applications may not want to actively poll persistent memory devices to retrieve device 
 health. Thus, the ACPI specification has defined a passive notification method to allow 
 the persistent memory device to notify when a significant change in device health has 
 occurred. Persistent memory device vendors and platform BIOS vendors decide which 
 device health changes are significant enough to trigger an NVDIMM Firmware Interface 
 Table (NFIT) health event notification. Upon receipt of an NFIT health event, a 
 notification to the operating system is expected to call an _NCH or a _DSM attached to the 
 persistent memory device and take appropriate action based on the data returned.",NA
 Unsafe/Dirty Shutdown,"An unsafe or dirty shutdown on persistent memory means that the persistent memory 
 device power-down sequence or platform power-down sequence may have failed to 
 write all in-flight data from the system’s persistence domain to persistent media. 
  
 (Chapter 
 2
  describes persistence domains.) A dirty shutdown is expected to be a 
 very rare event, but they can happen due to a variety of reasons such as physical 
 hardware issues, power spikes, thermal events, and so on.
  
 A persistent memory device does not know if any application data was lost as a 
 result of the incomplete power-down sequence. It can only detect if a series of events 
 occurred in which data may have been lost. In the best-case scenario, there might not 
 have been any applications that were in the process of writing data when the dirty 
 shutdown occurred.
  
 The RAS mechanism described here requires the platform BIOS and persistent 
 memory vendor to maintain a persistent rolling counter that is incremented anytime a 
 dirty shutdown is detected. The ACPI specification refers to such a mechanism as the 
 Data Loss Count (DLC) that can be returned as part of the Get NVDIMM Boot 
 Status(_NBS) persistent memory device method.
  
 Referring to the output from ndctl in Listing 
 17-1
 , the ""shutdown_count"" is reported 
 in the health information. Similarly, the output from ipmctl in Listing 
 17-2
  reports 
 ""LatchedDirtyShutdownCount"" as the dirty shutdown counter. For both outputs, a value 
 of 1 means no issues were detected.
  
 343",NA
 Application Utilization of Data Loss Count (DLC),"Applications may want to use the DLC counter provided by _NBS to detect if possible 
 data loss occurred while saving data from the system’s persistence domain to the 
 persistent media. If such a loss can be detected, applications can perform data recovery 
 or rollback using application-specific features.
  
  
 The application’s responsibilities and possible implementation suggestions for 
 applications are outlined as follows:
  
  1. Application first creates its initial metadata and stores it in a 
  
 persistent memory file:
  
 a. Application retrieves current DLC via operating system–specific 
 means for the physical persistent memory that make up the 
 logical volume the applications metadata resides on.
  
 b. Application calculates the current Logical Data Loss Count (LDLC) 
 as the sum of the DLC for all physical persistent memory that 
 make up the logical volume the applications metadata resides on.
  
 c. Application stores the current LDLC in its metadata file and 
 ensures that the update of the LDLC has been flushed to the 
 system’s persistence domain. This is done by using a flush that 
 forces the write data all the way to the persistent memory 
 power-fail safe domain. (Chapter 
 2
  contains more information 
 about flushing data to the persistence domain.)
  
 d. Application determines GUID or UUID for the logical volume the 
 applications metadata resides on, stores this in its metadata file, 
 and ensures the update of the GUID/UUID to the persistence 
 domain. This is used by the application to later identify if the 
 metadata file has been moved to another logical volume, where 
 the current DLC is no longer valid.
  
 e. Application creates and sets a “clean” flag in its metadata file and 
 ensures the update of the clean flag to the persistence domain. 
 This is used by the application to determine if the application was 
 actively writing data to persistence during dirty shutdown.
  
 344",NA
 Summary,"This chapter describes some of the RAS features that are available to persistent memory 
 devices and that are relevant to persistent memory applications. It should have given 
 you a deeper understanding of uncorrectable errors and how applications can respond 
 to them, how operating systems can detect health status changes to improve the 
 availability of applications, and how applications can best detect dirty shutdowns and 
 use the data loss counter.
  
  
 Open Access
  This chapter is licensed under the terms of the Creative 
 Commons Attribution 4.0 International License (
 http://creativecommons.
  
 org/licenses/by/4.0/
 ), which permits use, sharing, adaptation, distribution and 
 reproduction in any medium or format, as long as you give appropriate credit to the 
 original author(s) and the source, provide a link to the Creative Commons license and 
 indicate if changes were made.
  
 The images or other third party material in this chapter are included in the chapter’s 
 Creative Commons license, unless indicated otherwise in a credit line to the material. If 
 material is not included in the chapter’s Creative Commons license and your intended 
 use is not permitted by statutory regulation or exceeds the permitted use, you will 
 need to obtain permission directly from the copyright holder.
  
 346",NA
CHAPTER 18,NA,NA
Remote ,NA,NA
Persistent ,NA,NA
Memory,"This chapter provides an overview of how persistent memory – and the programming 
 concepts that were introduced in this book – can be used to access persistent memory 
 located in remote servers connected via a network. A combination of TCP/IP or RDMA 
 network hardware and software running on the servers containing persistent memory 
 provide direct remote access to persistent memory.
  
 Having remote direct memory access via a high-performance network connection is 
 a critical use case for most cloud deployments of persistent memory. Typically, in high- 
 availability or highly redundant use cases, data written locally to persistent memory is 
 not considered reliable until it has been replicated to two or more remote persistent 
 memory devices on separate remote servers. We describe this push model design later 
 in this chapter.
  
 While it is certainly possible to use existing TCP/IP networking infrastructures to 
 remotely access the persistent memory, this chapter focuses on the use of remote direct 
 memory access (RDMA). Direct memory access (DMA) allows data movement on a 
 platform to be off-loaded to a hardware DMA engine that moves that data on behalf of 
 the CPU, freeing it to do other important tasks during the data move. RDMA applies the 
 same concept and enables data movement between remote servers to occur without the 
 CPU on either server having to be directly involved.
  
 This chapter’s content and the PMDK librpmem remote persistent memory library 
 that is discussed assume the use of RDMA, but the concepts discussed here can apply to 
 other networking interconnects and protocols.",NA
 RDMA Networking Protocols,"Examples of popular RDMA networking protocols used throughout the cloud and 
 enterprise data centers include: 
  
 • InfiniBand is an I/O architecture and high-performance specification 
 for data transmission between high-speed, low-latency, and highly 
 scalable CPUs, processors, and storage.
  
 • RoCE (RDMA over Converged Ethernet) is a network protocol that 
 allows RDMA over an Ethernet network.
  
 • iWARP (Internet Wide Area RDMA Protocol) is a networking protocol 
 that implements RDMA for efficient data transfer over Internet Protocol 
 networks.
  
  
 All three protocols support high-performance data movement to and from persistent 
 memory using RDMA.
  
 348",NA
 Goals of the Initial Remote Persistent ,NA,NA
Memory Architecture,"The goal of the first remote persistent memory implementation was based on minimal 
 changes – or ideally, no changes – to the current RDMA hardware and software stacks 
 used with volatile memory. From a network hardware, middleware, and software 
 architecture standpoint, writing to remote volatile memory is identical to writing to 
 remote persistent memory. The knowledge that a specific memory-mapped file is backed 
 by persistent memory vs. volatile memory is entirely the responsibility of the application 
 to maintain. None of the lower layers in the networking stack are aware of the fact that 
 the write is to a persistent memory region or volatile memory. The responsibility of 
 knowing which write persistence method to use for a given target connection, and 
 making those remote writes persistent, falls to the application.",NA
 Guaranteeing Remote Persistence,"Until this chapter, much of the book focuses on the use and programming of persistent 
 memory on the local machine. You are now aware of some of the challenges of using 
 persistent memory, the persistence domain, and the need to understand and use a 
 flushing mechanism to ensure the data is persistent. These same programming concepts 
 and challenges apply to remote persistent memory with the additional constraints of 
 making it work within the existing network protocol and network latency.
  
 The SNIA NVM programming model (described in Chapter 
 3
 ) requires applications to 
 flush data that has been written to persistent memory to guarantee that the written data 
 made it into the persistence domain. This same requirement applies to writing to remote 
 persistent memory. After the RDMA Write or Send operation has moved the data 
  
 351",NA
 General-Purpose Remote Replication Method,"The general-purpose remote replication method (GPRRM), also referred to as the 
 general-purpose server persistency method (GPSPM), relies on the initiator RDMA 
 application to maintain a list of virtual addresses on the remote target system that have 
 been written to with previous RDMA Write requests. When all remote writes to 
 persistent memory are issued, the application issues an RDMA Send request from the 
 initiator NIC to the target NIC. The RDMA Send request contains a list of virtual starting 
 addresses and lengths that the target system will consume when the application 
 software running on the target node interrupts the system to process the send request. 
 The application walks the list of regions, flushing each cache line in the requested region 
 to the persistent memory using an optimized flush machine instruction (CLWB, 
 CLFLUSHOPT, etc.). When complete, an SFENCE machine instruction is required to fence 
 those previous writes and force them to complete before handling additional writes. The 
 application on the target system then issues an RDMA Send request back to interrupt the 
 initiator software of the completed flush operations. This is an indicator to the 
 application that the previous writes were made persistent.
  
  
 Figure 
 18-2
  outlines the general-purpose remote replication method sequence of 
 operation.
  
 353",NA
 How Does the General-Purpose Remote Replication ,NA,NA
Method Make Data Persistent?,"After the RDMA Write or any number of writes have been sent, the write data will 
 either be in the L3 CPU cache (due to the default allocating writes) or persistent 
 memory (assuming it does not all fit in L3) with potentially some write data still 
 pending in NIC internal buffers. An RDMA Send request, by definition, will force 
 previous writes to be pushed out of the NIC to the target L3 CPU cache and interrupt 
 the target CPU. At this point, all previously issued RDMA Writes to persistent memory 
 are now in L3 or persistent memory. The RDMA Send request contains a list of cache 
 lines that the initiator is requesting the target system to flush to its persistence domain. 
 The target 
  
 354",NA
 Performance Implications of the General-Purpose ,NA,NA
Remote Replication Method,"The general-purpose remote replication method requires that RDMA of the initiator 
 software follows a number of RDMA Write(s) with an RDMA Send. After the target NIC 
 finishes flushing the requested regions, an RDMA Send from the target goes back to the 
 initiator to affirm that the initiator application can consider those writes persistent. 
  
 This additional send/receive/send/receive messaging has an effect on latency and 
 throughput to make the writes persistent and has 50% higher latency than the 
 appliance remote replication method. The extra messaging has an effect on overall 
 bandwidth and scalability of all the RDMA connections running on those NICs.
  
 Also, if the size of the RDMA Write that needs to be made persistent is small, the 
 efficiency of the connection drops dramatically as the extra messaging overhead 
 becomes a significant component of the overall latency. Additionally, the target 
  
 node CPU and caches are consumed for that operation. The same data is essentially 
 transmitted twice: once from NIC (via PCIe) to the CPU L3 cache and then from the CPU 
 L3 cache to the memory controller (iMC).",NA
 Appliance Remote Replication Method,"Users of persistent memory on an Intel platform can use non-allocating write flows by 
 enabling the feature on the specific PCI root complex where incoming writes from the 
 NIC will enter into the CPU’s internal fabric and out to the persistent memory. Using the 
 non-allocating write flow, the incoming RDMA Writes will bypass CPU caches and go 
 directly to the persistence domain. This means that writes do not need to be flushed to 
 the persistence domain by the target system CPU.
  
 The I/O pipeline still needs to be flushed to the persistence domain. This is more 
 efficiently accomplished by issuing a small RDMA Read to any memory address on the 
 same RDMA connection as the RDMA Writes; the memory address does not need to be 
 one that was written or is persistent. The RDMA specification clearly states that an 
 RDMA Read will force the previous RDMA Writes to complete first. This ordering rule is 
  
 355",NA
 How Does the Appliance Remote Replication Method ,NA,NA
Make Data Persistent?,"The combination of bypassing CPU caches on the target system for the inbound RDMA 
 Writes to persistent memory with the ordering semantics of the RDMA and PCIe 
 protocols results in an efficient mechanism to make data persistent. Since the RDMA 
 Read to persistent memory will force previous writes first to persistent memory and the 
 persistence domain, the RDMA Read completion that comes back after those writes are 
 complete is the initiator application’s acknowledgment that those writes are now 
 durable.
  
 Chapter 
 2
  defines the persistence domain in depth, including how the platform 
 ensures that all writes get to the media from the persistence domain in the event of a 
 power loss.",NA
 Performance Implications of the Appliance Remote ,NA,NA
Replication Method,"This single extra round trip using an RDMA Read is roughly 50% lower latency than the 
 general-purpose server persistency method, which requires two round-trip messages 
 before the writes can be declared durable. As with the first method, as the size of the 
 writes to be made durable gets smaller, the RDMA Read round-trip overhead becomes a 
 significant component of the overall latency.",NA
 General Software Architecture,"The software stack for the use of remote persistent memory typically uses the same 
 memory-mapped files discussed in Chapter 
 3
 . Persistent memory is presented to the 
 RDMA application as a memory-mapped file. The application registers the persistent 
 memory with the local NIC on both ends of the connection, and the resulting registry key 
 is shared with the initiator application for use in the RDMA Read and Write requests. 
  
 This is the identical process required for using traditional volatile DRAM with RDMA.
  
  
 A layering of kernel and application-level software components is typically used to 
 allow an application to make use of both persistent memory and an RDMA connection. 
  
 The IBTA defines verbs interfaces that are typically implemented by the kernel drivers 
 for the NIC and the middleware software application library. Additional libraries may be 
 layered above the verbs layer to provide generic RDMA services via a common API- and 
 NIC-specific provider that implements the library.
  
 357",NA
 librpmem Architecture and Its Use in ,NA,NA
Replication,"PMDK implements both the general-purpose remote replication method and the 
 appliance remote replication method in the librpmem library. As of PMDK v1.7, the 
 librpmem library implements the synchronous and asynchronous replication of local 
 writes to persistent memory on remote systems. librpmem is a low-level library, like 
 libpmem, which allows other libraries to use its replication features.",NA
 Configuring Remote Replication Using Poolsets,"You are probably already familiar with using poolsets (introduced in Chapter 
 7
 ) 
 libpmemobj to initialize remote replication, which requires two such poolset files. The 
 file used on the initiator side by the libpmemobj-enabled application must describe the 
 local memory pool and point to poolset configuration file on the target node, whereas 
 the poolset file on the target node must describe the memory pool shared by the target 
 system.
  
  
 Listing 
 18-1
  shows a poolset file that will allow replicating local writes to 
 the “remotepool.set” on a remote host.
  
 Listing 18-1.
  poolwithremotereplica.set – An example of replicating local data to 
 a remote host
  
 PMEMPOOLSET 
  
 256G 
 /mnt/pmem0/pool1
  
 REPLICA user@example.com remotepool.set
  
 Listing 
 18-2
  shows a poolset file that describes the memory-mapped files shared for 
 the remote access. In many ways, a remote poolset file is the same as the regular poolset 
 file, but it must fulfill additional requirements:
  
 • Exist in a poolset directory specified in the rpmemd configuration file
  
 • Should be uniquely identified by its name, which an rpmem-enabled 
 application has to use to replicate to the specified memory pool
  
 • Cannot define any additional replicas, local or remote
  
 Listing 18-2.
  remotereplica.set – An example of how to describe the memory 
 pool on the remote host
  
 PMEMPOOLSET 
  
 256G 
 /mnt/pmem1/pool2",NA
 Performance Considerations,"Once persistent memory is accessible via a remote network connection, significantly 
 lower latency can be achieved compared with writing to a remote SSD or legacy block 
 storage device. This is because the RDMA hardware is writing the remote write data",NA
 Remote Replication Error Handling,"librpmem replication failures will occur for either a lost socket connection or a lost 
 RDMA connection. Any error status returned from rpmem_persist(), rpmem_flush(), 
 and rpmem_drain() is typically treated as an unrecoverable failure. The libpmemobj 
 user of librpmem API should treat this as a lost socket or RDMA condition and should 
 wait for all remaining librpmem API calls to complete, call rpmem_close() to close the 
 connection and clean up the stack, and then force the application to exit. When the 
 application restarts, the files will be reopened on both ends, and libpmemobj will check 
 only the file metadata. We recommend you do not proceed before synchronizing local 
 and remote memory pools with the pmempool-sync(1) command.",NA
 Say Hello to the Replicated World,"The beauty of the libpmemobj remote replication is that it does not require any changes 
 to the existing libpmemobj application. If you take any libpmemobj application and 
 provide it with the poolset file configured to use the remote replica, it will simply start 
 replicating. No coding required.
  
  
 To illustrate how to replicate persistent memory, we look at a Hello World type 
 program demonstrating the replication process directly using the librpmem 
 library. 
  
 Listing 
 18-3
  shows a part of the C program that writes the “Hello world” message to 
 remote memory. If it discovers that the message in English is already there, it translates 
 it to Spanish and writes it back to remote memory. We walk through the lines of the 
 program at the end of the listing.
  
 364",NA
 Execution Example,"The Hello World application produces the output shown in Listing 
 18-5
 .
  
 Listing 18-5.
  An output from the Hello World application for librpmem
  
 [user@initiator]$ ./hello 
  
 Hello world!
  
 [user@initiator]$ ./hello
  
 ¡Hola Mundo!
  
  
 Listing 
 18-6
  shows the contents of the target persistent memory pool where we 
 see the “Hola Mundo” string.
  
 Listing 18-6.
  The ¡Hola Mundo! snooped on the replication target
  
 [user@target]$ hexdump –s 4096 –C /mnt/pmem1/pool2 
  
 00001000  01 00 00 00 c2 a1 48 6f  6c 61 20 4d 75 6e 64 6f  |......Hola Mundo| 
  
 00001010  21 00 00 00 00 00 00 00  00 00 00 00 00 00 00 
  
 00  |!...............|
  
 369",NA
 Summary,"It is important to know that neither the general-purpose remote replication method 
 nor the appliance remote replication method is ideal because vendor-specific platform 
 features are required to use non-allocating writes, adding the complication of effecting 
 performance on an entire PCI root complex. Conversely, flushing remote writes using 
 allocating writes requires a painful interrupt of the target system to intercept an 
 RDMA Send request and flush the list of regions contained within the send buffer. 
 Waking the remote node is extremely painful in a cloud environment because there 
 are hundreds or thousands of inbound RDMA requests from many different 
 connections; avoid this if possible.
  
 There are cloud service providers using these two methods today and getting 
 phenomenal performance results. If the persistent memory is used as a replacement for 
 a remotely accessed SSD, huge reductions in latency can be achieved.
  
 As the first iteration of remote persistence support, we focused on application/ 
 library changes to implement these high-level persistence methods, without hardware, 
 firmware, driver, or protocol changes. At the time of publication, IBTA and IETF drafts 
 for a new wire protocol extension for persistent memory is nearing completion. This 
 will provide native hardware support for RDMA to persistent memory and allow 
 hardware entities to route each I/ O to its destination memory device without the need 
 to change allocating write mode and without the potential to adversely affect 
 performance on collateral devices connected to the same root port. See Appendix E for 
 more details on the new extensions to RDMA, specifically for remote persistence.
  
 RDMA protocol extensions are only one step into further remote persistent memory 
 development. Several other areas of improvement are already identified and shall be 
 addressed to the remote persistent memory users community, including atomicity of 
 remote operations, advanced error handling (including RAS), dynamic configuration of 
 remote persistent memory and custom setup, and real 0% CPU utilization on remote/ 
 target replication side.
  
 370",NA
CHAPTER 19,NA,NA
Advanced Topics,"This chapter covers several topics that we briefly described earlier in the book but did 
 not expand upon as it would have distracted from the focus points. The in-depth details 
 on these topics are here for your reference.",NA
 Nonuniform Memory Access (NUMA),"NUMA is a computer memory design used in multiprocessing where the memory access 
 time depends on the memory location relative to the processor. NUMA is used in a 
 symmetric multiprocessing (SMP) system. An SMP system is a “tightly coupled and share 
 everything” system in which multiple processors working under a single operating 
 system can access each other’s memory over a common bus or “interconnect” path. With 
 NUMA, a processor can access its own local memory faster than nonlocal memory 
 (memory that is local to another processor or memory shared between processors). The 
 benefits of NUMA are limited to particular workloads, notably on servers where the data 
 is often associated strongly with certain tasks or users.
  
 CPU memory access is always fastest when the CPU can access its local memory. 
 Typically, the CPU socket and the closest memory banks define a NUMA node. Whenever 
 a CPU needs to access the memory of another NUMA node, it cannot access it directly but 
 is required to access it through the CPU owning the memory. Figure 
  19- 1
  shows a two-
 socket system with DRAM and persistent memory represented as “memory.”
  
 © The Author(s) 2020 
  
 373
  
 S. Scargall, 
 Programming Persistent Memory
 , 
 https://doi.org/10.1007/978-1-4842-4932-1_19",NA
 NUMACTL Linux Utility,"On Linux we can use the numactl utility to display the NUMA hardware configuration 
 and control which cores and threads application processes can run. The libnuma library 
 included in the numactl package offers a simple programming interface to the NUMA 
 policy supported by the kernel. It is useful for more fine-grained tuning than the 
 numactl utility. Further information is available in the numa(7) man page.
  
 374",NA
 NDCTL Linux Utility,"The ndctl utility is used to create persistent memory capacity for the operating system, 
 called namespaces, as well as enumerating, enabling, and disabling the dimms, regions, 
 and namespaces. Using the –v (verbose) option shows what NUMA node (numa_node) 
 persistent memory DIMMS (-D), regions (-R), and namespaces (-N) belong to. Listing 
 19-
 1
  shows the region and namespaces for a two-socket system. We can correlate the 
 numa_node with the corresponding NUMA node shown by the numactl command.
  
 Listing 19-1.
  Region and namespaces for a two-socket system
  
 # ndctl list -Rv 
  
 {
  
  
  ""regions"":[
  
   
  {
  
    
  ""dev"":""region1"",
  
    
  ""size"":1623497637888,
  
    
  ""available_size"":0,
  
    
  ""max_available_extent"":0,
  
    
  ""type"":""pmem"",
  
    
  ""numa_node"":1,
  
 376",NA
 Intel Memory Latency Checker Utility,"To get absolute latency numbers between NUMA nodes on Intel systems, you can use 
 the Intel Memory Latency Checker (Intel MLC), available from 
 https://software. 
 intel.com/en-us/articles/intel-memory-latency-checker
 .
  
 Intel MLC provides several modes specified through command-line arguments:
  
 • --latency_matrix prints a matrix of local and cross-socket memory 
 latencies.
  
 • --bandwidth_matrix prints a matrix of local and cross-socket 
 memory bandwidths.
  
 • --peak_injection_bandwidth prints peak memory bandwidths of the 
 platform for various read-write ratios.
  
 • --idle_latency prints the idle memory latency of the platform.
  
 • --loaded_latency prints the loaded memory latency of the platform.
  
 • --c2c_latency prints the cache-to-cache data transfer latency of the 
 platform.
  
 Executing mlc or mlc_avx512 with no arguments runs all the modes in sequence 
 using the default parameters and values for each test and writes the results to the 
 terminal. The following example shows running just the latency matrix on a two-socket 
 Intel system.
  
 # ./mlc_avx512 --latency_matrix -e -r 
  
 Intel(R) Memory Latency Checker - v3.6 
  
 Command line parameters: --latency_matrix -e -r
  
 378",NA
 NUMASTAT Utility,"The numastat utility on Linux shows per NUMA node memory statistics for processors 
 and the operating system. With no command options or arguments, it displays NUMA 
 hit and miss system statistics from the kernel memory allocator. The default numastat 
 statistics shows per-node numbers, in units of pages of memory, for example:
  
 $ sudo numastat
  
  
  node0           node1 
 numa_hit                 8718076         7881244 numa_miss                      
 0               0 numa_foreign                   0               0 
 interleave_hit             40135           40160 local_node               
 8642532         2806430 other_node                 75544         
 5074814
  
 • numa_hit is memory successfully allocated on this node as intended.
  
 • numa_miss is memory allocated on this node despite the process 
 preferring some different node. Each numa_miss has a numa_foreign on 
 another node.
  
 • numa_foreign is memory intended for this node but is actually allocated 
 on a different node. Each numa_foreign has a numa_miss on another node.
  
 • interleave_hit is interleaved memory successfully allocated on this node 
 as intended.
  
 • local_node is memory allocated on this node while a process was 
 running on it.
  
 • other_node is memory allocated on this node while a process was 
 running on another node.
  
 380",NA
 Intel VTune Profiler – Platform Profiler,"On Intel systems, you can use the Intel VTune Profiler - Platform Profiler, previously 
 called VTune Amplifier, (discussed in Chapter 
 15
 ) to show CPU and memory statistics, 
 including hit and miss rates of CPU caches and data accesses to DDR and persistent 
 memory. It can also depict the system’s configuration to show what memory devices are 
 physically located on which CPU.",NA
 IPMCTL Utility,"Persistent memory vendor- and server-specific utilities can also be used to show DDR 
 and persistent memory device topology to help identify what devices are associated 
 with which CPU sockets. For example, the ipmctl show –topology command displays 
 the DDR and persistent memory (non-volatile) devices with their physical memory slot 
 location (see Figure 
 19-2
 ), if that data is available.
  
 Figure 19-2. 
 Topology report from the ipmctl show –topology command
  
 381",NA
 BIOS Tuning Options,"The BIOS contains many tuning options that change the behavior of CPU, memory, 
 persistent memory, and NUMA. The location and name may vary between server 
 platform types, server vendors, persistent memory vendors, or BIOS versions. 
 However, most applicable tunable options can usually be found in the Advanced menu 
 under Memory Configuration and Processor Configuration. Refer to your system BIOS 
 user manual for descriptions of each available option. You may want to test several 
 BIOS options with the application(s) to understand which options bring the most value.",NA
 Automatic NUMA Balancing,"Physical limitations to hardware are encountered when many CPUs and a lot of memory 
 are required. The important limitation is the limited communication bandwidth between 
 the CPUs and the memory. The NUMA architecture modification addresses this issue. An 
 application generally performs best when the threads of its processes are accessing 
 memory on the same NUMA node as the threads are scheduled. Automatic NUMA 
 balancing moves tasks (which can be threads or processes) closer to the memory they 
 are accessing. It also moves application data to memory closer to the tasks that reference 
 it. The kernel does this automatically when automatic NUMA balancing is active. Most 
 operating systems implement this feature. This section discusses the feature on Linux; 
 refer to your Linux distribution documentation for specific options as they may vary.
  
 Automatic NUMA balancing is enabled by default in most Linux distributions and 
 will automatically activate at boot time when the operating system detects it is running 
 on hardware with NUMA properties. To determine if the feature is enabled, use the 
 following command:
  
 $ sudo cat /proc/sys/kernel/numa_balancing
  
  
 A value of 1 (true) indicates the feature is enabled, whereas a value of 0 
 (zero/false) means it is disabled.
  
 382",NA
 Using Volume Managers with Persistent ,NA,NA
Memory,"We can provision persistent memory as a block device on which a file system can be 
 created. Applications can access persistent memory using standard file APIs or memory 
 map a file from the file system and access the persistent memory directly through load/ 
 store operations. The accessibility options are described in Chapters 
 2
  and 
 3
 .
  
 The main advantages of volume managers are increased abstraction, flexibility, and 
 control. Logical volumes can have meaningful names like “databases” or “web.” Volumes 
 can be resized dynamically as space requirements change and migrated between 
 physical devices within the volume group on a running system.
  
 On NUMA systems, there is a locality factor between the CPU and the DRR and 
 persistent memory that is directly attached to it. Accessing memory on a different CPU 
 across the interconnect incurs a small latency penalty. Latency-sensitive applications, 
 such as databases, understand this and coordinate their threads to run on the same 
 socket as the memory they are accessing.
  
  
 Compared with SSD or NVMe capacity, persistent memory is relatively small. If 
 your application requires a single file system that consumes all persistent memory on 
  
 383",NA
 The mmap( ) MAP_SYNC Flag,"Introduced in the Linux kernel v4.15, the MAP_SYNC flag ensures that any needed file 
 system metadata writes are completed before a process is allowed to modify directly 
 mapped data. The MAP_SYNC flag was added to the mmap() system call to request the 
 synchronous behavior; in particular, the guarantee provided by this flag is
  
 While a block is writeably mapped into page tables of this mapping, it is 
 guaranteed to be visible in the file at that offset also after a crash.
  
 This means the file system will not silently relocate the block, and it will ensure that the 
 file’s metadata is in a consistent state so that the blocks in question will be present after 
 a crash. This is done by ensuring that any needed metadata writes were done before the 
 process is allowed to write pages affected by that metadata.
  
 When a persistent memory region is mapped using MAP_SYNC, the memory 
 management code will check to see whether there are metadata writes pending for the 
 affected file. However, it will not actually flush those writes out. Instead, the pages are 
 mapped read only with a special flag, forcing a page fault when the process first 
 attempts to perform a write to one of those pages. The fault handler will then 
 synchronously flush out any dirty metadata, set the page permissions to allow the write, 
 and return. At that point, the process can write the page safely, since all the necessary 
 metadata changes have already made it to persistent storage.
  
 The result is a relatively simple mechanism that will perform far better than the 
 currently available alternative of manually calling fsync() before each write to persistent 
 memory. The additional IO from fsync() can potentially cause the process to block in 
 what was supposed to be a simple memory write, introducing latency that may be 
 unexpected and unwanted.
  
 385",NA
 Summary,"In this chapter, we presented some of the more advanced topics for persistent memory 
  
 including page size considerations on large memory systems, NUMA awareness and 
  
 how it affects application performance, how to use volume managers to create DAX file 
  
 systems that span multiple NUMA nodes, and the MAP_SYNC flag for mmap(). Additional 
  
 topics such as BIOS tuning were intentionally left out of this book as it is vendor and 
  
 product specific. Performance and benchmarking of persistent memory products are left 
  
 to external resources as there are too many tools – vdbench, sysbench, fio, etc. – and too 
  
 many options for each one, to cover in this book.
  
 386",NA
 APPENDIX A,NA,NA
How to Install ,NA,NA
NDCTL and ,NA,NA
DAXCTL on Linux,"The ndctl utility is used to manage the libnvdimm (non-volatile memory device) 
 subsystem in the Linux kernel and to administer namespaces. The daxctl utility provides 
 enumeration and provisioning commands for any device-dax namespaces you create. 
  
 daxctl is only required if you work directly with device-dax namespaces. We presented a 
 use-case for the ‘system-ram’ dax type in Chapter 
 10
 , that can use persistent memory 
 capacity to dynamically extend the usable volatile memory capacity in Linux. Chapter 
 10 
 also showed how libmemkind can use device dax namespaces for volatile memory in 
 addition to using DRAM. The default, and recommended, namespace for most developers 
 is filesystem-dax (fsdax). Both Linux-only utilities - ndctl and daxctl - are open source 
 and are intended to be persistent memory vendor neutral. Microsoft Windows has 
 integrated graphical utilities and PowerShell Commandlets to administer persistent 
 memory.
  
 libndctl and libdaxctl are required for several Persistent Memory Development 
 Kit (PMDK) features if compiling from source. If ndctl is not available, the PMDK may 
 not build all components and features, but it will still successfully compile and install. 
 In this appendix, we describe how to install ndctl and daxctl using the Linux package 
 repository only. To compile ndctl from source code, refer to the README on the ndctl 
 GitHub repository (
 https://github.com/pmem/ndctl
 ) or 
 https://docs.pmem.io
 .",NA
 Prerequisites,"Installing ndctl and daxctl using packages automatically installs any missing 
  
 dependency packages on the system. A full list of dependencies is usually listed when 
 installing the package. You can query the package repository to list dependencies or use 
 an online package took such as 
 https://pkgs.org
  to find the package for your operating",NA
 Installing NDCTL and DAXCTL Using ,NA,NA
the Linux Distribution Package ,NA,NA
Repository,"The ndctl and daxctl utilities are delivered as runtime binaries with the option to 
 install development header files which can be used to integrate their features in to 
 your application or when compiling PMDK from source code. To create debug binaries, 
 you need to compile ndctl and daxctl from source code. Refer to the README on the 
 project page 
 https://github.com/pmem/ndctl
  or 
 https://docs.pmem.io
  for detailed 
 instructions.
  
 390",NA
 Searching for Packages Within a Package ,NA,NA
Repository,"The default package manager utility for your operating system will allow you to query 
 the package repository using regular expressions to identify packages to install. Table 
 A-
 1 
 shows how to search the package repository using the command-line utility for several 
 distributions. If you prefer to use a GUI, feel free to use your favorite desktop utility to 
 perform the same search and install operations described here.
  
 Table A-1. 
 Searching for ndctl and daxctl packages in 
 different Linux distributions
  
 Operating System
  
 Command
  
  
 Fedora 21 or Earlier
  
 Fedora 22 or Later
  
 RHEL AND CENTOS
  
 SLES AND OPENSUSE
  
 CANONICAL/Ubuntu
  
 $ yum search ndctl 
  
 $ yum search daxctl
  
 $ dnf search ndctl 
  
 $ dnf search daxctl
  
 $ yum search ndctl 
  
 $ yum search daxctl
  
 $ zipper search ndctl $ 
 zipper search daxctl
  
 $ aptitude search ndctl $ apt-
 cache search ndctl $ apt 
 search ndctl 
  
 $ aptitude search daxctl $ 
 apt-cache search daxctl $ apt 
 search daxctl
  
 Additionally, you can use an online package search tools such as 
 https://pkgs.org 
 that allow you to search for packages across multiple distros. Figure 
 A-2
  shows the 
 results for many distros when searching for “libpmem.”
  
 391",NA
 Installing NDCTL and DAXCTL from the ,NA,NA
Package Repository,"Instructions for some popular Linux distributions follow. Skip to the section for your 
 operating system. If your operating system is not listed here, it may share the same 
 package family as one listed here so you can use the same instructions. Should your 
 operating system not meet either criteria, see the ndctl project home page 
 https:// 
 github.com/pmem/ndctl
  or 
 https://docs.pmem.io
  for installation instructions.
  
 392",NA
Note,NA,NA
 the version of the ndctl and daxctl available with your operating ,NA,NA
system may not match the most current project release. if you require ,NA,NA
"a newer release than your operating system delivers, consider ",NA,NA
compiling the projects from the source code. we do not describe ,NA,NA
compiling and installing from the source code in this book. instructions ,NA,NA
can be found on ,NA,NA
https://docs.pmem.io/getting-started- ,NA,NA
guide/installing-ndctl#installing-ndctl-from-source-on- linux,NA,NA
 and ,NA,NA
https://github.com/pmem/ndctl,NA,NA
.,NA,NA
 Installing PMDK on Fedora 22 or Later ,"To install individual packages, you can execute 
  
 $ sudo dnf install <package> 
  
  
 For example, to install just the ndctl runtime utility and library, use 
 $ sudo dnf install ndctl 
  
  
 To install all packages, use 
  
 Runtime: 
  
 $ sudo dnf install ndctl daxctl 
  
 Development library: 
  
 $ sudo dnf install ndctl-devel",NA
 Installing PMDK on RHEL and CentOS 7.5 or ,NA,NA
Later ,"To install individual packages, you can execute 
  
 $ sudo yum install <package> 
  
  
 For example, to install just the ndctl runtime utility and library, use 
 $ sudo yum install ndctl
  
 393",NA
 Installing PMDK on SLES 12 and OpenSUSE ,NA,NA
or Later ,"To install individual packages, you can execute 
  
 $ sudo zypper install <package> 
  
  
 For example, to install just the ndctl runtime utility and library, 
 use $ sudo zypper install ndctl 
  
  
 To install all packages, use 
  
 All Runtime: 
  
 $ zypper install ndctl daxctl 
  
 All Development: 
  
 $ zypper install libndctl-devel",NA
 Installing PMDK on Ubuntu 18.04 or Later ,"To install individual packages, you can execute 
  
 $ sudo zypper install <package> 
  
  
 For example, to install just the ndctl runtime utility and library, 
 use $ sudo zypper install ndctl 
  
  
 To install all packages, use 
  
 All Runtime: 
  
 $ sudo apt-get install ndctl daxctl 
  
 All Development: 
  
 $ sudo apt-get install libndctl-dev
  
 394",NA
 APPENDIX B,NA,NA
How to Install the ,NA,NA
Persistent Memory ,NA,NA
Development Kit ,NA,NA
(PMDK),"The Persistent Memory Development Kit (PMDK) is available on supported operating 
 systems in package and source code formats. Some features of the PMDK require 
 additional packages. We describe instructions for Linux and Windows.",NA
 PMDK Prerequisites,"In this appendix, we describe installing the PMDK libraries using the packages available 
 in your operating system package repository. To enable all PMDK features, such as 
 advanced reliability, accessibility, and serviceability (RAS), PMDK requires libndctl and 
 libdaxctl. Package dependencies automatically install these requirements. If you are 
 building and installing using the source code, you should install NDCTL first using the 
 instructions provided in Appendix C.",NA
 Installing PMDK Using the Linux ,NA,NA
Distribution Package Repository,"The PMDK is a collection of different libraries; each one provides different functionality. 
 This provides greater flexibility for developers as only the required runtime or header 
 files need to be installed without installing unnecessary libraries.",NA
 Package Naming Convention,"Libraries are available in runtime, development header files (∗-devel), and debug (∗-
 debug) versions. Table 
 B-1
  shows the runtime (libpmem), debug (libpmem-debug), 
 and development and header files (libpmem-devel) for Fedora. Package names may 
 differ between Linux distributions. We provide instructions for some of the common 
 Linux distributions later in this section.
  
 Table B-1. 
 Example runtime, debug, and development package naming 
 convention
  
 Library
  
 Description
  
 LIBPMEM 
  
 LIBPMEM-
 DEBUG 
 LIBPMEM-
 DEVEL
  
 Low-level persistent memory support library 
  
 Debug variant of the libpmem low-level persistent 
 memory library Development files for the low-level 
 persistent memory library",NA
 Searching for Packages Within a Package ,NA,NA
Repository,"Table 
 B-2
  shows the list of available libraries as of PMDK v1.6. For an up-to-date list, see 
 https://pmem.io/pmdk
 .
  
 Table B-2. 
 PMDK libraries as of PMDK v1.6
  
 Library
  
 Description
  
 LIBPMEM 
  
 LIBRPMEM 
  
 LIBPMEMBLK 
 LIBPMEMCTO 
 LIBPMEMLOG 
 LIBPMEMOBJ 
 LIBPMEMPO
 OL 
 PMEMPOOL
  
 Low-level persistent memory support library 
  
 Remote Access to persistent memory library 
  
 Persistent Memory Resident Array of Blocks library 
  
 Close-to-Open Persistence library (Deprecated in 
 PMDK v1.5) Persistent Memory Resident Log File 
 library 
  
 Persistent Memory Transactional Object Store 
 library 
  
 Persistent Memory pool management library 
  
 Utilities for Persistent Memory
  
 396",NA
 Installing PMDK Libraries from the Package ,NA,NA
Repository,"Instructions for some popular Linux distributions follow. Skip to the section for your 
 operating system. If your operating system is not listed here, it may share the same 
 package family as one listed here so you can use the same instructions. Should your 
 operating system not meet either criteria, see 
 https://docs.pmem.io
  for installation 
 instructions and the PMDK project home page (
 https://github.com/pmem/pmdk
 ) to 
 see the most recent instructions.
  
 398",NA
Note,NA,NA
 The version of the PMDK libraries available with your operating ,NA,NA
system may not match the most current PMDK release. if you require a ,NA,NA
"newer release than your operating system delivers, consider compiling ",NA,NA
PMDK from the source code. we do not describe compiling and installing ,NA,NA
PMDK from the source code in this book. ,NA,NA
instructions can be found on ,NA,NA
https://docs.pmem.io/getting-started-,NA,NA
guide/installing-pmdk/compiling-pmdk-from-source,NA,NA
 and ,NA,NA
https:// ,NA,NA
github.com/pmem/pmdk,NA,NA
.,NA,NA
 Installing PMDK on Fedora 22 or Later,"To install individual libraries, you can execute
  
 $ sudo dnf install <library>
  
 For example, to install just the libpmem runtime library, use
  
 $ sudo dnf install libpmem
  
 To install all packages, use
  
 All Runtime: 
  
 $ sudo dnf install libpmem librpmem libpmemblk libpmemlog/
  
  libpmemobj libpmempool pmempool
  
 All Development: 
  
 $ sudo dnf install libpmem-devel librpmem-devel \
  
  
  libpmemblk-devel libpmemlog-devel libpmemobj-devel \
  
  
 libpmemobj++-devel libpmempool-devel
  
 All Debug: 
  
 $ sudo dnf install libpmem-debug librpmem-debug \
  
  
  libpmemblk-debug libpmemlog-debug libpmemobj-debug \
  
  libpmempool-debug",NA
 Installing PMDK on RHEL and CentOS 7.5 or Later,"To install individual libraries, you can execute
  
 $ sudo yum install <library>
  
 For example, to install just the libpmem runtime library, use
  
 $ sudo yum install libpmem
  
 To install all packages, use
  
 All Runtime: 
  
 $ sudo yum install libpmem librpmem libpmemblk libpmemlog \
  
  
 libpmemobj libpmempool pmempool
  
 All Development: 
  
 $ sudo yum install libpmem-devel librpmem-devel \
  
  
  libpmemblk-devel libpmemlog-devel libpmemobj-devel \
  
  libpmemobj++-devel libpmempool-devel
  
 All Debug: 
  
 $ sudo yum install libpmem-debug librpmem-debug \
  
  
  libpmemblk-debug libpmemlog-debug libpmemobj-debug \
  
  libpmempool-debug",NA
 Installing PMDK on SLES 12 and OpenSUSE or Later,"To install individual libraries, you can execute
  
 $ sudo zypper install <library>
  
 For example, to install just the libpmem runtime library, use
  
 $ sudo zypper install libpmem
  
 To install all packages, use
  
 All Runtime: 
  
 $ sudo zypper install libpmem librpmem libpmemblk libpmemlog \
  
  
 libpmemobj libpmempool pmempool
  
 400",NA
 Installing PMDK on Ubuntu 18.04 or Later,"To install individual libraries, you can execute
  
 $ sudo zypper install <library>
  
 For example, to install just the libpmem runtime library, use
  
 $ sudo zypper install libpmem
  
 To install all packages, use
  
 All Runtime: 
  
 $ sudo apt-get install libpmem1 librpmem1 libpmemblk1 \
  
  
 libpmemlog1 libpmemobj1 libpmempool1
  
 All Development: 
  
 $ sudo apt-get install libpmem-dev librpmem-dev \
  
  
 libpmemblk-dev libpmemlog-dev libpmemobj-dev \
  
  
 libpmempool-dev libpmempool-dev
  
 All Debug: 
  
 $ sudo apt-get install libpmem1-debug \
  
  
  librpmem1-debug libpmemblk1-debug \
  
  
  libpmemlog1-debug libpmemobj1-debug libpmempool1-debug
  
 401",NA
 Installing PMDK on Microsoft Windows,"The recommended and easiest way to install PMDK on Windows is to use Microsoft 
 vcpkg. 
  
 Vcpkg is an open source tool and ecosystem created for library management. To build 
 PMDK from source that can be used in a different packaging or development solution, 
 see the README on 
 https://github.com/pmem/pmdk
  or 
 https://docs.pmem.io
 .
  
 To install the latest PMDK release and link it to your Visual Studio solution, you first 
 need to clone and set up vcpkg on your machine as described on the vcpkg GitHub page 
 (
 https://github.com/Microsoft/vcpkg
 ).
  
 In brief:
  
 > git clone https://github.com/Microsoft/vcpkg > cd 
 vcpkg 
  
 > .\bootstrap-vcpkg.bat 
  
 > .\vcpkg integrate install 
  
 > .\vcpkg install pmdk:x64-windows",NA
Note,NA,NA
 The last command can take a while as PMDK builds and installs.,"After successful completion of all of the preceding steps, the libraries are ready to 
 be used in Visual Studio with no additional configuration is required. Just open Visual 
 Studio with your existing project or create a new one (remember to use platform 
 x64
 ) and then include headers to project as you always do.
  
 402",NA
 APPENDIX C,NA,NA
How to Install ,NA,NA
IPMCTL on Linux ,NA,NA
and Windows,"The ipmctl utility is used to configure and manage Intel Optane DC persistent memory 
 modules (DCPMM). This is a vendor-specific utility available for Linux and Windows. It 
 supports functionality to:
  
  
  
 • 
  
 Discover DCPMMs on the platform
  
  
  
 • 
  
 Provision the platform memory configuration
  
  
  
 • 
  
  
 View and update the firmware on DCPMMs
  
  
 • 
  
 Configure data-at-
 rest security on DCPMMs
  
  
  
 • 
  
 Monitor DCPMM health
  
  
  
 • 
  
 Track performance of DCPMMs
  
  
  
 • 
  
  
 Debug and troubleshoot DCPMMs 
  
 ipmctl refers to the following interface 
 components:
  
  
  
 • 
  
 libipmctl: An application programming interface (API) library for 
  
  
   
 managing PMMs
  
  
  
 • 
  
 ipmctl: A command-line interface (CLI) application for configuring 
  
  
   
 and managing PMMs from the command line
  
  
  
 • 
  
 ipmctl-monitor: A monitor daemon/system service for monitoring 
  
  
   
 the health and status of PMMs",NA
 IPMCTL Linux Prerequisites,ipmctl requires libsafec as a dependency.,NA
 libsafec,"libsafec is available as a package in the Fedora package repository. For other Linux 
 distributions, it is available as a separate downloadable package for local 
 installation:
  
 • RHEL/CentOS EPEL 7 packages can be found at 
  
 https://copr.fedorainfracloud.org/coprs/jhli/safeclib/
 .
  
 • OpenSUSE/SLES packages can be found at 
  
 https://build.opensuse.org/package/show/home:jhli/safeclib
 .
  
 • Ubuntu packages can be found at 
  
 https://launchpad.net/~jhli/+archive/ubuntu/libsafec
 .
  
 Alternately, when compiling ipmctl from source code, use the -DSAFECLIB_SRC_ 
 DOWNLOAD_AND_STATIC_LINK=ON option to download sources and statically link 
 to safeclib.",NA
 IPMCTL Linux Packages,"As a vendor-specific utility, it is not included in most Linux distribution package 
 repositories other than Fedora. EPEL7 packages can be found at 
 https://copr. 
 fedorainfracloud.org/coprs/jhli/ipmctl
 . OpenSUSE and SLES packages can be found 
 at 
 https://build.opensuse.org/package/show/home:jhli/ipmctl
 .",NA
 IPMCTL for Microsoft Windows,"The latest Windows EXE binary for ipmctl can be downloaded from the “Releases” 
 section of the GitHub project page (
 https://github.com/intel/ipmctl/releases
 ) as 
 shown in Figure 
 C-1
 .
  
 404",NA
 Using ipmctl,"The ipmctl utility provides system administrators with the ability to configure 
  
 Intel Optane DC persistent memory modules which can then be used by Windows 
 PowerShellCmdlets or ndctl on Linux to create namespaces on which file systems can 
 be created. Applications can then create persistent memory pools and memory map 
 them to get direct access to the persistent memory. Detailed information about the 
 modules can also be extracted to help with errors or debugging.
  
  
 ipmctl has a rich set of commands and options that can be displayed by running 
 ipmctl without any command verb, as shown in Listing 
 C-1
 .
  
 405",NA
 APPENDIX D,NA,NA
Java for ,NA,NA
Persistent ,NA,NA
Memory,"Java is one of the most popular programming languages available because it is fast, 
 secure, and reliable. There are lots of applications and web sites implemented in Java. It 
 is cross-platform and supports multi-CPU architectures from laptops to datacenters, 
 game consoles to scientific supercomputers, cell phones to the Internet, and CD/DVD 
 players to automotive. Java is everywhere!
  
 At the time of writing this book, Java did not natively support storing data 
 persistently on persistent memory, and there were no Java bindings for the Persistent 
 Memory Development Kit (PMDK), so we decided Java was not worthy of a dedicated 
 chapter. 
  
 We didn’t want to leave Java out of this book given its popularity among developers, 
 so we decided to include information about Java in this appendix.
  
 In this appendix, we describe the features that have already been integrated in to 
 Oracle’s Java Development Kit
  (JDK) [
 https://www.oracle.com/java/
 ] and 
 OpenJDK 
 [
 https://openjdk.java.net/
 ]. We also provide information about proposed persistent 
 memory functionality in Java as well as two external Java libraries in development.",NA
 Volatile Use of Persistent Memory,"Java does support persistent memory for volatile use cases on systems that have 
 heterogeneous memory architectures. That is a system with DRAM, persistent memory, 
 and non-volatile storage such as SSD or NVMe drives.",NA
 Heap Allocation on Alternative Memory Devices,"Both Oracle JDK v10 and OpenJDK v10 implemented 
 JEP 316: Heap allocation on 
 alternative memory devices
  [
 http://openjdk.java.net/jeps/316
 ]. The goal of this 
 feature is to enable the HotSpot VM to allocate the Java object heap on an alternative 
 memory device, such as persistent memory, specified by the user.
  
 As described in Chapter 
 3
 , Linux and Windows can expose persistent memory 
 through the file system. Examples are NTFS and XFS or ext4. Memory-mapped files 
 on these direct access (DAX) file systems bypass the page cache and provide a 
 direct mapping of virtual memory to the physical memory on the device.
  
 To allocate the Java heap using memory-mapped files on a DAX file system, Java 
 added a new runtime option, -XX:AllocateHeapAt=<path>. This option takes a path to 
 the DAX file system and uses memory mapping to allocate the object heap on the 
 memory device. Using this option enables the HotSpot VM to allocate the Java object 
 heap on an alternative memory device, such as persistent memory, specified by the user. 
  
 The feature does not intend to share a non-volatile region between multiple running 
 JVMs or reuse the same region for further invocations of the JVM.
  
  
 Figure 
 D-1
  shows the architecture of this new heap allocation method using both 
 DRAM and persistent memory backed virtual memory.
  
  
 Figure D-1. 
 Java heap memory allocated from DRAM and persistent memory 
 using the “-XX:AllocateHeapAt=<path>” option
  
 The Java heap is allocated only from persistent memory. The mapping to DRAM is 
 shown to emphasize that non-heap components like code cache, gc bookkeeping, and 
 so on, are allocated from DRAM.
  
 412",NA
 Partial Heap Allocation on Alternative Memory Devices,"HotSpot JVM 12.0.1 introduced a feature to allocate old generation of Java heap on an 
 alternative memory device, such as persistent memory, specified by the user.
  
 The feature in G1 and parallel GC allows them to allocate part of heap memory in 
 persistent memory to be used exclusively for old generation objects. The rest of the heap 
 is mapped to DRAM, and young generation objects are always placed here.
  
 Operating systems expose persistent memory devices through the file system, so the 
 underlying media can be accessed directly, or direct access (DAX). File systems that 
 support DAX include NTFS on Microsoft Windows and ext4 and XFS on Linux. Memory- 
 mapped files in these file systems bypass the file cache and provide a direct mapping of 
 virtual memory to the physical memory on the device. The specification of a path to a 
 DAX mounted file system uses the flag -XX:AllocateOldGenAt=<path> which enables this 
 feature. There are no additional flags to enable this feature.
  
 When enabled, young generation objects are placed in DRAM only, while old 
 generation objects are always allocated in persistent memory. At any given point, the 
 garbage collector guarantees that the total memory committed in DRAM and persistent 
 memory is always less than the size of the heap as specified by -Xmx.
  
 When enabled, the JVM also limits the maximum size of the young generation based 
 on available DRAM, although it is recommended that users set the maximum size of the 
 young generation explicitly.
  
 For example, if the JVM is executed with -Xmx756g on a system with 32GB DRAM 
 and 1024GB persistent memory, the garbage collector will limit the young generation 
 size based on the following rules:
  
 • No -XX:MaxNewSize or -Xmn is specified: The maximum young 
 generation size is set to 80% of available memory (25.6GB).
  
 414",NA
 Non-volatile Mapped Byte Buffers,"JEP 352: Non-Volatile Mapped Byte Buffers
  [
 https://openjdk.java.net/jeps/352
 ] adds a 
 new JDK-specific file mapping mode so that the FileChannel API can be used to create 
 MappedByteBuffer instances that refer to persistent memory. The feature should be 
 available in Java 14 when it is released, which is after the publication of this book.
  
 This JEP proposes to upgrade MappedByteBuffer to support access to persistent 
 memory. The only API change required is a new enumeration employed by FileChannel 
 clients to request mapping of a file located on a DAX file system rather than a 
  
 conventional, file storage system. Recent changes to the MappedByteBufer API mean 
 that it supports all the behaviors needed to allow direct memory updates and provide 
 the durability guarantees needed for higher level, Java client libraries to implement 
 persistent data types (e.g., block file systems, journaled logs, persistent objects, etc.). The 
 implementations of FileChannel and MappedByteBuffer need revising to be aware of this 
 new backing type for the mapped file.
  
 The primary goal of this JEP is to ensure that clients can access and update persistent 
 memory from a Java program efficiently and coherently. A key element of this goal is to 
 ensure that individual writes (or small groups of contiguous writes) to a buffer region 
 can be committed with minimal overhead, that is, to ensure that any changes which 
 might still be in cache are written back to memory.
  
 415",NA
 Persistent Collections for Java (PCJ),"The Persistent Collections for Java library (PCJ) is an open source Java library being 
 developed by Intel for persistent memory programming. More information on PCJ, 
 including source code and sample code, is available on GitHub at 
 https://github.com/ 
 pmem/pcj
 .
  
 At the time of writing this book, the PCJ library was still defined as a “pilot” project 
 and still in an experimental state. It is being made available now in the hope it is useful in 
 exploring the retrofit of existing Java code to use persistent memory as well as exploring 
 persistent Java programming in general.
  
 The library offers a range of thread-safe persistent collection classes including 
 arrays, lists, and maps. It also offers persistent support for things like strings and 
 primitive integer and floating-point types. Developers can define their own persistent 
 classes as well.
  
 Instances of these persistent classes behave much like regular Java objects, but 
 their fields are stored in persistent memory. Like regular Java objects, their lifetime is 
 reachability-based; they are automatically garbage collected if there are no outstanding 
  
 416",NA
 Using PCJ in Java Applications,"To import this library into an existing Java application, include the project’s target/ 
 classes directory in your Java classpath and the project’s target/cppbuild directory in 
 your java.library.path. For example:
  
 $ javac -cp .:<path>/pcj/target/classes <source> 
  
 $ java -cp .:<path>/pcj/target/classes \
  
  
  -Djava.library.path=<path>/pcj/target/cppbuild <class>
  
 There are several ways to use the PCJ library:
  
  1. Use instances of built-in persistent classes in your applications.
  
  2. Extend built-in persistent classes with new methods.
  
  3. Declare new persistent classes or extend built-in classes with 
 methods and persistent fields.
  
 PCJ source code examples can be found in the resources listed in the following:
  
 • Introduction to Persistent Collections for Java – 
 https://github. 
 com/pmem/pcj/blob/master/Introduction.txt
  
 • Code Sample: Introduction to Java∗ API for Persistent Memory 
 Programming – 
 https://software.intel.com/en-us/articles/ code-
 sample-introduction-to-java-api-for-persistent-memory-
 programming
  
 417",NA
 Low-Level Persistent Library (LLPL),"The Low-Level Persistence Library (LLPL) is an open source Java library being 
 developed by Intel for persistent memory programming. By providing Java access to 
 persistent memory at a memory block level, LLPL gives developers a foundation for 
 building custom abstractions or retrofitting existing code. More information on LLPL, 
 including source code, sample code, and javadocs, is available on GitHub at 
 https:// 
 github.com/pmem/llpl
 .
  
 The library offers management of heaps of persistent memory and manual allocation 
 and deallocation of blocks of persistent memory within a heap. A Java persistent memory 
 block class provides methods to read and write Java integer types within a block as well 
 as copy bytes from block to block and between blocks and (volatile) Java byte arrays.
  
 Several different kinds of heaps and corresponding memory blocks are available 
 to aid in implementing different data consistency schemes. Examples of such 
 implementable schemes:
  
 • Transactional: Data in memory is usable after a crash or power failure
  
 • Persistent: Data in memory is usable after a controlled process exit
  
 • Volatile: Persistent memory used for its large capacity, data is not 
 needed after exit.
  
 Mixed data consistency schemes are also implementable. For example, transactional 
 writes for critical data and either persistent or volatile writes for less critical data (e.g., 
 statistics or caches).
  
 LLPL uses the libpmemobj library from the Persistent Memory Development Kit 
 (PMDK) which we discussed in Chapter 
 7
 . For additional information on PMDK, please 
 visit 
 https://pmem.io/
  and 
 https://github.com/pmem/pmdk
 .
  
 418",NA
 Using LLPL in Java Applications,"To use LLPL with your Java application, you need to have PMDK and LLPL installed 
 on your system. To compile the Java classes, you need to specify the LLPL class path. 
  
 Assuming you have LLPL installed on your home directory, do the following:
  
 $ javac -cp .:/home/<username>/llpl/target/classes LlplTest.java
  
 After that, you should see the generated ∗.class file. To run the main() method inside 
 your class, you need to again pass the LLPL class path. You also need to set the 
 java.library.path environment variable to the location of the compiled native library 
 used as a bridge between LLPL and PMDK:
  
 $ java -cp .:/.../llpl/target/classes \
  
 -Djava.library.path=/.../llpl/target/cppbuild LlplTest
  
 PCJ source code examples can be found in the resources listed in the following:
  
 • Code Sample: Introducing the Low-Level Persistent Library (LLPL) for 
 Java∗ – 
 https://software.intel.com/en-us/articles/ 
  
 introducing-the-low-level-persistent-library-llpl-for-java
  
 • Code Sample: Create a “Hello World” Program Using the Low-Level 
 Persistence Library (LLPL) for Java∗ – 
 https://software.intel.
  
 com/en-us/articles/code-sample-create-a-hello-world-
 program-using-the-low-level-persistence-library-llpl- for-java
  
 • Enabling Persistent Memory Use in Java – 
 https://www.snia. 
 org/sites/default/files/PM-Summit/2019/presentations/05-
 PMSummit19-Dohrmann.pdf",NA
 Summary,"At the time of writing this book, native support for persistent memory in Java is an 
 ongoing effort. Current features are mostly volatile, meaning the data is not persisted 
 once the app exits. We have described several features that have been integrated and 
 shown two libraries – LLPL and PCJ – that provide additional functionality for Java 
 applications.
  
 419",NA
 APPENDIX E,NA,NA
The Future of ,NA,NA
Remote Persistent ,NA,NA
Memory ,NA,NA
Replication,"As discussed in Chapter 
 18
 , the general purpose and appliance remote persistent 
 memory methods are simple high-level upper-layer-protocol (ULP) changes. These 
 methods add a secondary RDMA Send or RDMA Read after a number of RDMA Writes to 
 remote persistent memory. One of the pain points with these implementations is the 
 Intel-specific platform feature, allocating writes, which, by default, pushes inbound PCIe 
 Write data from the NIC directly into the lowest-level CPU cache, speeding the local 
 software access to that newly written data. For persistent memory, it is desirable to turn 
 off allocating writes to persistent memory, elevating the need to flush the CPU cache to 
 guarantee persistence. However, the platform limitations on the control over allocating 
 writes only imprecise control over the behavior of writes for an entire PCIe Root 
 complex. All devices connected to a given root complex will behave the same way. The 
 implications to other software running on the system can be difficult to determine if 
 access to the write data is delayed by bypassing caches. These are contradictory 
 requirements since allocating writes should be disabled for writes to persistent 
 memory, but for writes to volatile memory, allocating writes should be enabled.
  
 To make this per IO steering possible, the networking hardware and software 
 need to have native support for persistent memory. If the networking stack is aware 
 of the persistent memory regions, it can select whether the write is steered toward 
 the persistent memory subsystem or the volatile memory subsystem on a per IO 
 basis, completely removing the need to change global PCIe Root complex allocating-
 write settings.",NA
 Glossary,"Term
  
 Definition
  
 3D XPoint
  
 3D Xpoint is a non-volatile memory (NVM) technology developed jointly by 
 Intel and 
  
 Micron Technology.
  
 ACPI 
  
 The Advanced Configuration and Power Interface is used by BIOS to expose 
 platform 
  
 capabilities.
  
 ADR 
  
 Asynchronous DRAM Refresh is a feature supported on Intel that triggers a 
 flush of 
  
 write pending queues in the memory controller on power failure. Note that 
 ADR does 
  
 not flush the processor cache.
  
 AMD 
  
 Advanced Micro Devices 
 https://www.amd.com
  
 BIOS 
  
 Basic Input/Output System refers to the firmware used to initialize a 
 server.
  
 CPU 
  
 Central processing unit
  
 DCPM 
  
 Intel Optane DC persistent memory
  
 DCPMM 
  
 Intel Optane DC persistent memory module(s)
  
 DDR 
  
 Double Data Rate is an advanced version of SDRAM, a type of computer 
 memory.
  
 DDIO 
  
 Direct Data IO.  Intel DDIO makes the processor cache the primary 
 destination and 
  
 source of I/O data rather than main memory. By avoiding system memory, 
 Intel DDIO 
  
 reduces latency, increases system I/O bandwidth, and reduces power 
 consumption 
  
 due to memory reads and writes.
  
 DRAM 
  
 Dynamic random-access memory
  
 eADR 
  
 Enhanced Asynchronous DRAM Refresh, a superset of ADR that also 
 flushes the CPU 
  
 caches on power failure.",NA
Index ,NA,NA
A ,NA,NA
C,"ACPI specification, 
 28 
  
 Address range scrub (ARS), 
 338 
  
 Address space layout randomization 
  
 (ASLR), 
 87,112,316 
  
 Appliance remote replication 
  
  
 method, 
 355,357
  
 C++ Standard limitations 
  
 object layout, 
 122,123 
  
 object lifetime, 
 119,120 
  
 vs
 . persistent memory, 
 125,126 
 pointers, 
 123–125 
  
 trivial types, 
 120–122
  
 Application binary interface (ABI), 
 122 
  
 type traits, 
 125
  
 Application startup and recovery 
  
 Cache flush operation (CLWB), 
 24,59,286
  
 ACPI specification, 
 28 
  
 Cache hierarchy
  
 ARS, 
 29 
  
 CPU
  
 dirty shutdown, 
 27 
  
 cache hit, 
 15
  
 flow, 
 27,28 
  
 cache miss, 
 16
  
 infinite loop, 
 28 
  
 levels, 
 14,15
  
 libpmem library, 
 27 
  
 libpmemobj query, 
 27 
 PMDK, 
 29
  
  
 and memory controller, 
 14,15 
  
 non-volatile storage devices, 
 16 
 Cache thrashing, 
 374
  
 RAS, 
 27 
  
 Chunks/buckets, 
 188
  
 Asynchronous DRAM 
  
  
 Refresh (ADR), 
 17,207 
  
 Atomicity, consistency, isolation, and 
  
 durability (ACID), 
 278
  
 CLFLUSHOPT, 
 18,19,24,208,247,353 
 close() method, 
 151 
  
 closeTable() method, 
 268 
  
 CLWB flushing instructions, 
 208
  
 Atomic operations, 
 285,286 
  
 cmap engine, 
 4
  
 Concurrent data structures",NA
B,"definition, 
 287
  
 429
  
 erase operation, 
 293
  
 Block Translation Table (BTT) 
  
 find operation, 
 292
  
 driver, 
 34
  
 hash map, 
 291,292
  
 Buffer-based LRU design, 
 182
  
 insert operation, 
 292,293
  
 © The Author(s) 2020 
  
 S. Scargall, 
 Programming Persistent Memory
 , 
 https://doi.org/10.1007/978-1-4842-4932-1",NA
D,"Data at rest, 
 17 
  
 Data in-flight, 
 17 
  
 Data Loss Count (DLC), 
 342–346 
  
 Data structure 
  
  
 hash table and transactions, 
 194 
  
 persistence, 
 197,200–202 
  
  
 sorted array, versioning, 
 202–206 
 Data visibility, 
 23 
  
 DAX-enabled file system, 
 179,184 
  
 DB-Engines, 
 143 
  
 deleteNodeFromSLL(), 
 273 
  
 deleteRowFromAllIndexedColumns() 
  
  
 function, 
 273 
  
 delete_row() method, 
 272",NA
E,"Ecosystem, persistent containers 
  
 begin() and end(), 
 138 
  
  
 implementation, 
 134 
  
  
 iterating, 
 136,138 
  
  
 memory layout, 
 134 
  
  
 snapshots, 
 138 
  
  
 std::vector, 
 135,136 
  
  
 vector, 
 135 
  
 Enhanced Asynchronous DRAM 
  
 Refresh (eADR), 
 18 
  
 Error correcting codes (ECC), 
 333 
 errormsg() method, 
 150 
  
 exists() method, 
 150 
  
 External fragmentation, 
 177,188",NA
F,"Fence 
  
  
 code, 
 21,22 
  
  
 libpmem library, 
 23 
  
  
 PMDK, 
 22 
  
  
 pseudocode, 
 21 
  
  
 SFENCE instructions, 
 23 
 flush() function, 
 217,242 
 Flushing 
  
  
 msync(), 
 20 
  
  
 non-temporal stores, 
 19 
  
 optimized flush, 
 19 
  
  
 temporal locality, 
 19 
  
 Fragmentation, 
 187 
  
 func() function, 
 237
  
 Direct access (DAX), 
 19,66 
 Direct Data IO (DDIO), 
 352",NA
G,"Direct memory access (DMA), 
 12,347 
 Dirty reads, 
 233 
  
 Dynamic random-access memory 
  
 (DRAM), 
 11,155 
  
 430
  
 General-purpose remote replication 
  
 method (GPRRM) 
  
  
 performance implications, 
 355 
  
 persistent data, 
 354,355",NA
J,"Java, 
 411 
  
  
 heap memory allocation, 
 412–414 
  
 LLPL, 
 418–419 
  
  
 non-volatile mapped byte buffers, 
  
  
 415–416
  
 partial heap allocation, 
 414–415",NA
H,"PCJ, 
 416–418 
  
 Java Development Kit (JDK), 
 411
  
 Heap management API
  
 allocating memory, 
 165,166 
 freeing allocating memory, 
 166",NA
K,"High bandwidth memory (HBM), 
 156 
 High-performance appliance remote 
 replication method, 
 352
  
 key-value pairs, 
 4 
  
 Key-value store, 
 142 
  
  
 persistent memory, 
 5,6 
  
 storage, 
 6",NA
I ,"increment() function, 
 278,281
  
 traditional storage, 
 5 
  
 Kind configuration management, 
 167 
 kvprint(), 
 4,6
  
 index_init() method, 
 275
  
 InfiniBand, 
 348 
  
 In-memory databases (IMDB), 
 177 
 Intel Inspector, 
 212 
  
 Intel Inspector–Persistence",NA
L ,"libmemkind 
 vs.
  libvmemcache, 
 180 
 libpmem, 
 50
  
  
 Inspector, 
 210 
  
 Intel machine instructions, 
 24,25 
 Intel Memory Latency Checker 
  
 C code examples, 
 73 
  
 copying data, 
 76,77 
  
 CPU instructions, 
 73
  
 (Intel MLC), 
 304 
  
 flushing, 
 77,78
  
 Intel Threading Building Blocks 
  
 header, 
 74,75
  
 (Intel TBB), 
 168 
  
 memory mapping files, 
 75
  
 Internal fragmentation, 
 177,188 
  
 libpmemblk, 
 69
  
 Internet of Things (IoT), 
 263 
  
 Internet Wide Area RDMA Protocol 
  
 libpmemkv library, 
 2–4,69 
  
 components, 
 8,9
  
 (iWARP), 
 348 
  
 software stack, 
 8
  
 431",NA
M,"Machine check exception (MCE), 
 334 
 main() function, 
 213,234 
  
 MAP_SYNC flag, 
 385 
  
 MariaDB∗ storage engine 
  
  
 architecture, 
 264 
  
  
 creation 
  
  
  
 database table, 
 266,267 
  
  
  
 database table, closing, 
 268 
  
  
 database table, 
  
  
  
  
 opening, 
 267,268 
  
  
  
 data structure, 
 266 
  
  
  
 DELETE operation, 
 272–274 
  
  
 handle commits and 
  
  
  
  
 rollbacks, 
 265,266 
  
  
  
 INSERT operation, 
 268–270 
  
  
 new handler instance, 
 265 
  
  
  
 SELECT operation, 
 275,276 
  
  
 UPDATE operation, 
 270,271 
  
 storage layer, 
 264
  
  
 capacity, 
 11 
  
  
 characteristics, 
 12,13 
  
  
 kinds of, 
 158 
  
  
 leaked object, 
 213 
  
  
 leaks, 
 209 
  
 Memory management unit (MMU), 
 49 
 Metaprogramming 
  
  
 allocating, 
 116–118 
  
  
 definition, 
 112 
  
  
 persistent pointers, 
 112,113 
  
  
 snapshots, 
 115,116 
  
  
 transactions, 
 113,114 
  
 mtx object, 
 282 
  
 Multiple Device Driver (mdadm), 
 384 
 Mutexes 
  
  
 libpmemobj library, 
 283 
  
  
 main() function, 
 285 
  
  
 std::mutex, 
 282 
  
  
 synchronization primitives, 
 282–
 284
  
 memkind API functions, 
 159 
  
  
 fixed-size heap creation, 
 160,161",NA
N,"kind creation, 
 160 
  
  
 kind detection, 
 162,163 
  
  
 memory kind detection API, 
 163 
  
 variable size heap creation, 
 162 
 memkind_config structure, 
 161 
  
 memkind_create_pmem() 
  
 ndctl and daxctl, installation 
  
  
 Linux distribution package 
 repository 
  
  
 PMDK on 
 Fedora 22, 
 393 
  
  
  
 PMDK on RHEL and 
  
  
  
  
 CentOS, 
 393–394 
  
  
  
 PMDK on SLES 12 and 
  
 function, 
 160,169 
  
 OpenSUSE, 
 394
  
 memkind_create_pmem_with_config() 
  
 function, 
 161,164 
  
 memkind_destroy_kind() 
  
  
 PMDK on Ubuntu 18.04, 
 394 
  
 searching, packages, 
 391–392 
 prerequisites, 
 389–390
  
 function, 
 164 
  
 ndctl utility, 
 376
  
 memkind_detect_kind() function, 
 163 
 memkind_free() function, 
 166 
  
 memkind library, 
 156,157 
  
 memkind_realloc() functions, 
 165
  
 Network interface controller (NIC), 
 349 
 Non-maskable interrupt (NMI), 
 18 
  
 Nonuniform memory access 
  
 (NUMA), 
 65,156
  
 Memory 
  
 automatic balancing, 
 382,383",NA
P ,"paintball_init() function, 
 97 
 Patrol scrub, 
 337
  
  
 numastat utility, 
 380 
  
 NVDIMM driver, 
 33 
  
 NVDIMM Firmware Interface Table 
  
 Performance difference, 
 6 
 Persistent libraries, 
 64 
  
 Persistent memory, 
 173
  
 (NFIT), 
 343 
  
 advantages, 
 262
  
 application, 
 176",NA
O,"Object identifier (OID), 
 314 
  
 open()method, 
 267 
  
 Operating systems 
  
  
 memory direct access (DAX) 
  
  
  
 benefits, 
 49 
  
  
  
 I/O-accessed storage, 
 50 
  
  
  
 libpmem, 
 50 
  
  
  
 Linux, locating, 
 48 
  
  
  
 physical devices, regions, and 
  
  
  
 namespaces, 
  
  
 data structures, 
 188,189 
  
  
 selective, 
 193 
  
  
 snapshotting 
  
  
  
   
 performance, 
 190,191 
  
  
 uses, 
 262,263 
  
 Persistent Memory Development Kit 
  
  
 (PMDK), 
 1,13,111,207,261,307 
  
 persistent libraries, 
 63 
  
  
  
 libpmem, 
 67,68 
  
  
  
 libpmemblk, 
 69 
  
  
  
 libpmemkv, 
 69 
  
  
  
 libpmemlog, 
 69
  
  
 434
  
 displaying, 
 44–46,48 
  
 libpmemobj, 
 68
  
 pmem_map_file function, 
 52 
  
 libpmemobj-cpp, 
 68",NA
Q,"Queue implementation, 
 126,128 
  
 QuickPath Interconnect (QPI)/Ultra Path 
  
 Interconnect (UPI), 
 305
  
 Reliability, availability, 
  
  
 serviceability (RAS), 
 27 
 Remote direct memory access 
  
 (RDMA), 
 12,347,348 
  
  
 software architecture, 
 357,358",NA
R,"RDMA networking protocols 
 commands, 
 350 
  
 NIC, 
 349 
  
 RDMA Read, 
 350
  
 remote_open routine, 
 368,369 
  
 remove() method, 
 150 
  
 Resource acquisition is initialization 
 (RAII), 
 113 
  
 rpmem_drain(), 
 363 
  
 rpmem_flush(), 
 363
  
 RDMA Send (and Receive), 
 350,351
  
  
 RDMA Write, 
 350 
  
 RDMA over Converged Ethernet 
  
  
 (RoCE), 
 348 
  
 Redis, 
 143 
  
 Redo logging, 
 320",NA
S,"show() method, 
 131 
  
 Single instruction, multiple data (SIMD) 
 processing, 
 190
  
  
 Reliability, Availability, and 
  
  
 Serviceability (RAS) 
  
 device health
  
 Single linked list (SLL), 
 266 
  
 Snapshotting optimization, 
 196 
 SNIA NVM programming 
  
 ACPI NFIT, 
 343 
  
 model, 
 351
  
 ACPI specification, 
 342 
  
 unsafe/dirty shutdown, 
 343 
 using ndctl to query, 
 340,341 
 vendors, 
 342
  
 Solid-state disk (SSD), 
 1,156 
 Stack/buffer overflow bug, 
 208 
 Stackoverflow app, 
 211 
  
 Standard Template Library 
  
 ECC 
  
  
 (STL), 
 168,282,287 
  
 inifinite loop, 
 334 
  
 std::is_standard_layout, 
 122,123 
  
 MCE, 
 334 
  
 Storage and Networking Industry 
  
  
 using Linux, 
 335,336 
  
 unconsumed uncorrectable error 
  
 Association (SNIA), 
 33,112 
 symmetric multiprocessing (SMP) 
  
 handling 
  
 system, 
 373
  
 ARS, 
 338
  
 clearing errors, 
 339 
  
 memory root-device notification, 
 338",NA
T,"petrol scrub, 
 337 
  
 runtime, 
 337
  
 Thread migration, 
 310 
  
 Transactions and multithreading
  
 unsafe/dirty shutdown, 
  
 counter, 
 281
  
 DLC counter, 
 344,345
  
 illustrative execution, 
 281",NA
V,"Valgrind tools, 
 208,209 
  
 Vector of strings, 
 171 
  
 Versioning, 
 193 
  
 vmemcache_add() function, 
 185 
 vmemcache_get() function, 
 185 
 Volatile libraries, 
 64 
  
 Volume managers 
  
  
 advantages, 
 383 
  
  
 Linux architecture, 
 384 
  
  
 mdadm, 
 384",NA
U,"Undo logging, 
 321 
  
 Uninterruptable power supply 
  
 (UPS), 
 18
  
 NUMA systems, 
 383",NA
"W, X, Y, Z ","Write pending queue (WPQ), 
 18
  
  
 update_row() method, 
 270
  
 438
  
 write_row() method, 
 268",NA
