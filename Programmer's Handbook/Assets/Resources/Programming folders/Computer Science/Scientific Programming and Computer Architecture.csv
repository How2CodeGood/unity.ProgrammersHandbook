Larger Text,Smaller Text,Symbol
Table of Contents,"Chapter:Preface 
  
 Chapter 1:C/C++: Review 
  
  
 Section 1.1:An example: The Aitken transformation 
  
  
  
 Subsection 1.1.1:Leibniz series and the logarithmic series 
  
  
 Subsection 1.1.2:Modular organization of sources 
  
  
 Section 1.2:C review 
  
  
  
 Subsection 1.2.1:Header files 
  
  
  
 Subsection 1.2.2:Arrays and pointers 
  
  
  
 Subsection 1.2.3:The Aitken iteration using arrays and pointers 
  
  
 Subsection 1.2.4:Declarations and definitions 
  
  
  
 Subsection 1.2.5:Function calls and the compilation process 
  
 Section 1.3:C++ review 
  
  
  
 Subsection 1.3.1:The 
 Vector
  class
  
  
  
 Subsection 1.3.2:Aitken transformation in C++ 
  
  
 Section 1.4:A little Fortran 
  
  
 Section 1.5:References 
  
 Chapter 2:C/C++: Libraries and Makefiles 
  
  
 Section 2.1:Mixed-language programming 
  
  
  
 Subsection 2.1.1:Transmutation of names from source to object files 
  
  
 Subsection 2.1.2:Linking Fortran programs with C and C++ 
  
  
 Section 2.2:Using BLAS and LAPACK libraries 
  
  
  
 Subsection 2.2.1:Arrays, matrices, and leading dimensions 
  
  
  
 Subsection 2.2.2:BLAS and LAPACK 
  
  
  
 Subsection 2.2.3:C++ class interface to BLAS/LAPACK 
  
  
 Section 2.3:Building programs using GNU Make
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
Preface,"It is a common experience that minor changes to C/C++ programs can make a big difference to their speed. 
 Although all programmers who opt for C/C++ do so at least partly, and much of the time mainly, because programs 
 in these languages can be fast, writing fast programs in these languages is not so straightforward. Well-optimized 
 programs in C/C++ can be even 10 or more times faster than programs that are not well optimized.
  
 At the heart of this book is the following question: what makes computer programs fast or slow?
  
 Programming languages provide a level of abstraction that makes computers look simpler than they are. As soon as 
 we ask this question about program speed, we have to get behind the abstractions and understand how a computer 
 really works and how programming constructs map to different parts of the computer’s architecture. Although there 
 is much that can be understood, the modern computer is such a complicated device that this basic question cannot 
 be answered perfectly.
  
  
 Writing fast programs is the major theme of this book, but it is not the only theme. The other theme is 
 modularity of programs. Structuring programs so that their structure explains what they do is a valuable principle.
  
 Computer programs are organized into a series of functions to serve the purpose of that principle. Yet when 
 computer programs become large, merely dividing a program into functions becomes highly inadequate. It becomes 
 necessary to organize the computer program into distinct sources and the sources into a source tree. The entire 
 source tree can be made available as a library. We pay heed to program structure throughout this book.
  
 Most books on computer programming are written at the same level of abstraction as the programming 
 language they utilize or explain. If we want to understand program speed, we have to understand the different parts 
 of a computer, and such an approach is not feasible. It is inevitable that choices have to be made regarding the type 
 of computer system that is studied.
  
 The big choices in this book are to opt for the x86 line of computers backed by Intel and AMD corporations, 
 and for the Linux operating system. Nearly 100% of the computers in use today as servers, desktops, or laptops are 
 x86 based, and the x86 line has been dominant for more than 30 years. So a great deal is not lost. The choice of the 
 operating system does not have such a great impact on program speed. We pick Linux because it is the preferred
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1C/C++: Review,"A computer program is a sequence of instructions to a machine. In this chapter and the next, we emphasize that it is 
 a sequence of instructions that 
 builds on
   other computer programs and that in turn can be 
 built on
 . Codes that exist 
 in isolation are often limited to quite trivial tasks and can hardly be considered computer programs.
  
 This chapter is a review of C/C++. The C programming language is the most fundamental of all 
  
 programming languages. Computing machines come in great variety and are put together using many parts. The 
 computer’s parts, consisting of the processor at the center, and with memory, hard disk, network interfaces, 
 graphics devices, and other peripherals connected to it, are very different from each other. It would be an almost 
 impossible task for any single programmer to deliver instructions to such a complicated machine. The C 
 programming language is a major part of the setup to give the programmer a uniform view of computing machines. 
 No modern computing device can exist and be useful without C.
  
 Much of the time when programs are written, the programmer is not at all aware of the many parts of the 
 computer. Indeed, the programmer may not even be aware that there is a processor. It is more natural to think in
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1.1An example: The Aitken transformation,"The Aitken transformation maps a sequence of numbers to another sequence of numbers. It serves as the vehicle to 
 introduce aspects of C, C++, and Fortran in this chapter. It is also interesting in its own right.
  
 The Aitken transformation is given by the following formula:
  
 t
 n
  − 1
 =
 s
 n
  − 1
  − 
  
 (
 s
 n
  − 
 s
 n
  − 1
 )
 2 
 .
  
 s
 n
 +1
  − 2
 s
 n
 +
 s
 n
  − 1
  
 (1.1)
  
 It transforms a sequence 
 s
 0
 ,
 s
 1
 ,…,
 s
 N
  into a new sequence 
 t
 0
 ,
 t
 1
 ,…,
 t
 N
  − 2
 , which has two fewer terms. 
  
 [1]
  
 The idea behind the Aitken transformation is as follows. If the 
 s
 n
  sequence is of the form 
  
 s
 n
 =
 S
 +
 aλ
 n
 , all terms in
  
 the 
 t
 n
  sequence are equal to 
  
 S
 . It is useful for speeding up the convergence of a number of sequences, even those
  
 that do not directly fit the 
 S
 +
 aλ
 n
  pattern. Section 
 1.1.1↓
  illustrates the dramatic power of the Aitken iteration on a
  
 couple of examples---the Leibniz series and the logarithmic series. To be sure, these examples are chosen carefully.
  
 This section begins to make the point that it is generally advantageous to split a program into multiple 
 sources. We could use a single source file to code the Aitken iteration and apply it to the Leibniz series as well as 
 the logarithmic series. Such a program would work just as well to begin with. A few days later, we may want to 
 apply the Aitken iteration to another example. If we also throw that example into the same source file, the source 
 file will become a little more unwieldy. A few months later, we may want to use the Aitken iteration as part of a 
 large project. If we insist on using a single source file, there are two equally unpleasant alternatives: copy the whole
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1.1.1Leibniz series and the logarithmic series,"The Leibniz series
 [2]
  is given by
  
 π
 =4 − 4 
 3
  
 +4 − 4
  
 5 
  
 7
  
 +4 − 4
  
 9 
  
 11
  
 +
 ⋯
  
  
 This series, whose terms alternate in sign and diminish in magnitude monotonically, will be a recurring example. 
 So we will begin by looking at it carefully. Let 
 S
 n
  be the sum of the first 
  
 n
  terms. As shown in figure 
  
 1.1↓
 , the
  
 partial sums 
 S
 n
  are alternately above and below 
  
 π
 . Further, the convergence is slow. In fact,",NA
|,"π
  − 
 S
 n",NA
|,">2 ⁄ (2
 n
 +1), 
 which implies that the first million terms of the Leibniz series can give only slightly more than six digits of 
 π
  after
  
  
 the decimal point.
  
  
  
  
 Figure 1.1Convergence of the Leibniz series to 
 π
  (dashed line).
  
  
  
 If we look at figure 
 1.1↑
 , we may notice that although the convergence to the limit is slow, the partial sums
  
 seem to follow a certain trend as they approach 
 π
 . The partial sums are alternately above and below, and it seems as 
 if we can fit a smooth curve through the iterates. The Aitken iteration guesses this trend quite well and speeds up 
 the convergence of the Leibniz series.
  
  
 Table 
 1.1↓
  shows Aitken’s transformation 
  
 (1.1)↑
  applied repeatedly to the first 13 partial sums of the 
 Leibniz series. After each application, we have a sequence with two fewer numbers, and at the end of the sixth 
 application of the Aitken transformation, we have just one number that equals 
 π
  to 10 digits of accuracy. Because 
 none of the 13 partial sums gives even the first digit after the decimal point, it seems astonishing that an answer
  
 with 10 digits of accuracy can be produced from those numbers.
  
  
  
  
 4.0000000000
  
 3.1666666667
  
 ⋯
  
 3.1415926540
  
 3.1415926536
  
 2.6666666667
  
 3.1333333333
  
  
 3.1415926535
  
  
 3.4666666667
  
 3.1452380952
  
  
 3.1415926536
  
  
 2.8952380952
  
 3.1396825397
  
  
  
  
 3.3396825397
  
 3.1427128427
  
  
  
  
  
  
  
  
  
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1.1.2Modular organization of sources,"Before we delve into the syntax of C/C++, let us look at how to structure a program for the Aitken iteration. In 
 particular, we look at how to split the program between source files.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
|,"π
  − 
 S
 n",NA
|,">2 ⁄ (2
 n
 +1), showing that the Leibniz series converges slowly.
  
 π
  (not 
 π
  ⁄ 4), prove that
  
 Exercise:
  To understand the Aitken iteration, it is helpful to look at singularities. Prove that the function arctan
 z 
 has singularities at 
 z
 =±
 i
  in the complex plane. Determine the type of the singularities.",NA
1.2C review,"The C programming language is concise. In this review, we go over a few features of C, emphasizing points that 
 introductory classes often miss. Thus, we discuss header files, arrays, and pointers, with emphasis on the distinction 
 between lvalues and rvalues, as well as the distinction between declarations and definitions. We go over the 
 compilation and linking process while adding more detail to the picture in figure 
 1.2↑
 . Although C is concise, it 
 demands precision in thinking. The emphasis in this review is toward greater precision. Table 
 1.3↓
  shows two of
  
 the best books to learn C and C++.
 [8]
  
  
  
  
 C programming
  
 B.W. Kernighan and D.M. Ritchie, The C 
 programming language, 2nd ed.
  
 C++ programming
  
 B. Stroustrup, The C++ programming language, 3rd 
 ed.
  
 Table 1.3Books on C and C++ written by their inventors.",NA
1.2.1Header files,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
1.2.2Arrays and pointers,"Arrays and pointers are the heart of the C language. An array is a sequence of locations in memory. A pointer-type 
 variable is a variable that holds the address of a memory location. The word pointer can be used for either such a 
 variable or an expression that evaluates to an address. The two concepts may appear different, but the C language 
 blurs the distinction between them. In C, arrays and pointers are almost interchangeable.
  
 There is a very good reason for blurring the distinction between arrays and pointers. Suppose we want to 
 pass a long sequence, occupying a great deal of memory, as an argument to a function. It would be wasteful to 
 allocate new memory and copy the sequence entry by entry at every function call. In C, arrays are passed as 
 pointers.
  
 The key idea in almost identifying arrays with pointers is as follows. A sequence of data items in memory 
 can be specified using three pieces of information: the address of the first item, the size in bytes of each item, and 
 the number of entries in the sequence. In C, a pointer holding an address is specified as a pointer of a certain type. 
 The size of each item is inferred from type information. For example, in an array of 
 double
 s, the size of each item
  
 is 8 bytes and in an array of 
 int
 s the size of each item is 4 bytes (on GNU/Linux). Thus, from merely knowing a
  
 pointer, we can infer the first two pieces of information: the address of the first item in memory and the size of each 
 item in bytes. The last piece of information, namely, the length of the array, is often tagged along separately. Thus, 
 arrays and pointers may be identified, and arrays may be passed to functions efficiently as pointers with the length 
 of the array tagged along as an extra parameter.
  
 How this idea plays out in practice, we will examine presently.
  
 Arrays, pointers, lvalues, and rvalues
  
  
  
  
 Figure 1.3A schematic and simplified view of computer memory. The symbol 
 x
  is of integer type, whereas the 
 symbol 
 a
  is a pointer. Addresses are shown on top.
  
  
  
 The C language takes a certain view of computer memory, and arrays and pointers are both best understood in
  
 terms of computer memory. Figure 
 1.3↑
  is a schematic view of a portion of the computer memory (in hardware, this 
 would be DRAM in most circumstances). Names that we introduce into our code---whether they correspond to 
 variables of basic types such as 
 char
  or 
 int
  or 
 double
 , or to arrays, or to pointers---will all ultimately correspond 
 to locations in computer memory.
 [9]
  To introduce a name of a variable of basic type, we may use a definition such 
 as 
 int x;
  to introduce a name of an array, we may use a definition such as 
 int list[3];
  to introduce a name of
  
 a pointer, we may use a definition such as 
 int *a.
  
 In C semantics, an expression may evaluate to a value that is the name of a memory location. Such a value is 
 called an 
 lvalue
 . The “l” refers to the fact that such values may occur on the left-hand side of an assignment. In 
 contrast, an expression may evaluate to a value that may be used to fill a memory location but is not necessarily the 
 name of any location in memory. Such a value is called an 
 rvalue
 . The “r” here refers to the possible occurrence of 
 such a value on the right-hand side of an assignment statement. The concept of rvalues and lvalues is useful for 
 understanding arrays and pointers.
  
  
 The distinction between lvalues and rvalues arises fundamentally because of the assignment statement. In an 
 assignment, what occurs on the left is the name of a memory location or an lvalue. What occurs on the right is a
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1.2.3The Aitken iteration using arrays and pointers,"As already noted, arrays and pointers are almost equivalent in C. The principal advantage of thinking of arrays in 
 this way arises in passing arrays as arguments to functions. Here we use the Aitken example to illustrate how arrays
  
 may be passed as pointers.
  
 The file 
 aitken.c
  begins with two directives
  
  :
  
  
  
 #include <assert.h> 
 #include ""aitken.h""",NA
1.2.4Declarations and definitions,"Names introduced into a C program are for the most part names of either functions or variables. The names can be 
 introduced as either declarations or definitions.
  
 Suppose a variable name is introduced using 
 int x
 . When the compiler encounters that statement, it sets 
 aside a location in memory for an 
 int
  and makes 
 x
  the name of that memory location. This statement is a variable 
 definition, not merely a declaration, because it sets aside memory for 
 x
 .
  
 A declaration gives type information about a variable that is expected to be defined elsewhere. An example 
 of a variable declaration is a statement such as 
 extern int x
 . When the compiler encounters such a statement, it 
 notes that 
 x
  is a variable of type 
 int
  that is expected to be defined in some other source file. It does not set aside 
 any location in memory. If it later encounters a statement such as 
 x=x+2
 , it does not complain. However, it cannot 
 generate complete machine instructions to carry out that statement because it has no idea where 
 x
  is defined and 
 what address it corresponds to. That information has to be supplied by the linker later.
  
 Both the lines in the header file 
 aitken.h 
 are function declarations not definitions. When the compiler
  
 encounters a declaration such as
  
  
  
 void aitken(double *seq1, double *seq2, int len);
  
  
  
 it notes that 
 aitken()
  is the name of a function that takes three arguments, the first two of which are of type
  
 double *
  and the last of which is an 
  
 int
 , and returns nothing (
 void
 ). We can omit 
 seq1
 , 
 seq2
 , and 
 len
  from the
  
 declaration. Such an omission would make the declaration difficult to read and understand for us, but it makes no 
 difference as far as the compiler is concerned. The compiler does nothing more than note the types of the arguments 
 (or parameters).
  
 When the compiler later encounters a function call such as 
 aitken(s, t, n)
 , it first checks that 
 s
  and 
 t 
 are 
 of type 
 double *
  and 
 n
  is of type 
 int
 . If the check succeeds, the compiler will generate instructions to set up the 
 arguments and pass control to the function 
 aitken()
 . If the function 
 aitken() 
 is defined in some other source file, 
 which it may well be, the compiler has no idea where in memory the code for 
 aitken()
  is located. So it cannot 
 generate complete machine instructions for passing control. That job is the linker’s responsibility.
  
 A function definition such as
  
  
  
 void aitken(double *seq1, double *seq2, int len){ 
 ...",NA
1.2.5Function calls and the compilation process,"Here we take a look at the mechanism of function calls and the compilation process. Much of the discussion is 
 centered around the file 
 leibniz.c
 , which uses the functions defined by 
 aitken.c
  to extrapolate the partial sums
  
 of the Leibniz series and produces data corresponding to table 
 1.1↑
 .
  
 The source file 
 leibniz.c
  begins with two directives:
  
  
  
 #include ""aitken.h"" 
 #include <stdio.h>
  
  
  
 The first directive includes 
 aitken.h
  to interface to functions defined in 
 aitken.c
 . The second directive
  
 includes 
 stdio.h
  to interface to printing functions defined in the 
 stdio
  (standard input/output) library.
  
 The 
 leibniz()
  function, which generates partial sums of the Leibniz series, is defined below without
  
 comment.
  
  
  
 //partial sums of 4(1-1/3+1/5-1/7+1/9-...) 
 void leibniz(double* seq, int len){
  
  
  int i;
  
  
  for(i=0; i < len; i++)
  
  
  if(i==0)
  
   
  
  seq[i] = 4.0;
  
  
  else if(i%2==1)
  
   
  
  seq[i] = seq[i-1] - 4.0/(2.0*i+1);
  
  else
  
   
  
  seq[i] = seq[i-1] + 4.0/(2.0*i+1); 
 }
  
  
 The 
 printseq()
  function prints a sequence using the 
  
 printf()
  function defined in the 
 stdio
  library.
  
  
  
 void printseq(double* seq, int len){
  
  int i;
  
  
  printf(""\n \n"");
  
  
  for(i=0; i < len; i++) 
  
   
 printf(""%-.10f\n"",seq[i]); }
  
  
  
 The logic used by the 
 main()
  function for generating the data shown in table 
 1.1↑
  is similar to that of
  
 aitkenExtrapolate()
 . In a C or C++ program, the function named 
 main()
  is the first to gain control when a
  
 program is run.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12
  
 int main(){
  
  
  const int len = 13;
  
  
  double seq1[len];
  
  
  double seq2[len];
  
  
  int n, i, j;
  
  
  leibniz(seq1, len);
  
  
  n = len/2;
  
  
  if(len%2==0) 
  
   
 n--;
  
  
  for(i=0; i < n; i++){ 
  
   
 printseq(seq1,len-2*i); 
   
 aitken(seq1, seq2, len-
  
 2*i);
  
 13
  
 for(j=0; j < len-2*",NA
1.3C++ review,"The C language has a simple philosophy. Its aim is to offer a uniform view of the computer, especially computer 
 memory, to the programmer. C has been so successful that nearly every object that can be called a computer, 
 ranging from supercomputers to routers to embedded and mobile devices, is equipped with a C compiler or a cross-
 compiler. C is the best vehicle for highly optimized programs.
  
 Because C seeks to be close to the machine, it is a low-level language. There is often a considerable distance 
 between concepts that are native to a problem domain and their expression as C programs. High-level languages 
 provide constructs and syntax that bring the program much closer to ideas and concepts that are native to the 
 problem domain.
  
 The C++ language is something of a compromise to provide the facilities of high-level languages without 
 sacrificing the speed of C. Despite its name, it is not an incremental extension of C. It is a colossal expansion of C 
 syntax. It does not have the seamless nature of truly high-level languages such as Python. Classes in Python use 
 very little syntax and fit cleanly within the highly modular architecture of Python programs. However, languages 
 such as Python are much slower than C++.
  
 Although the C++ language is a compromise, or perhaps because of being a compromise, it has found a great 
 range of uses. On the one hand, C++ has all of C inside it. On the other hand, it provides many mechanisms for 
 capturing concepts and ideas more precisely. Its downside is its complexity. Although clear and careful thinking are 
 essential to all programming, a failure in this respect has particularly acute consequences in C++.
  
 Because our focus is on program speed, the part of C++ we use is quite small. Narrowly defined and flat (as 
 opposed to hierarchical) classes, references, occasional function name overloading, and the ability to define 
 variables in the middle of programs is an almost complete list of the C++ features we use. The classes we define are 
 no more than C 
 struct
 s endowed with functions. C features such as 
 enum
 , 
 struct
 , 
 typedef
 , and 
 static
  can be
  
 quite powerful for representing concepts and ideas when used judiciously.
  
  
 Classes in C++ are a mechanism for representing concepts and endowing them with functionality that makes 
 them easy to use. Classes can be general or narrow. The 
 Vector
  class studied in this section is an example of a
  
 general class. It can be made even more general. The 
 Vector
  class assumes that each entry of a vector is a 
 double
 .
  
 Using templates, one may define a class that allows each entry to be a 
 double
  or a 
 float
  or an 
 int
  or even some",NA
1.3.1The ,Vector,NA
 class,"The C++ language can be used in many different ways. Using general classes, one may make C++ look like easy to 
 use languages such as Python or Matlab without incurring the enormous cost of such interpreted languages. Our 
 interest is in fast programs and, even more so, in understanding what makes programs fast or slow. The C++ style 
 we adopt is quite close to C.
  
 Nevertheless, we begin with a general type of class, namely, the 
 Vector
  class. This class helps us review a
  
 few of the features of C++ and is used to implement the Aitken iteration later. In a later chapter, we criticize the use 
 of this class and show it to be slow.
  
 Header file with class definition 
  
 The
  Vector
  class is defined in the header file 
  
 Vector.hh
 . The C++ class consists of data members and member
  
 functions. With respect to computer memory, a class object is a collection of data items. The data items could be of 
 basic types such as 
 double
  or 
 char
 , or pointers. The data items may also be other class objects or C structures. The
  
 member functions provide various means to manipulate the class object or, equivalently, the package of data items 
 that constitutes the class object.
  
  
 It is typical for header files to give only part of the definition of the class. Many of the member functions are 
 typically defined in a separate source. Here the entire class definition is in the header file 
 Vector.hh
 , which makes
  
 the header file a bit long. We present the contents of the header file in stages, gradually unveiling features of C++.
  
 The skeleton of the header file is listed below.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8
  
 #ifndef MYVECTOR 
 #define MYVECTOR 
 #include <cassert> 
  
 #include <cmath> 
  
 #include <cstdlib> 
  
 #include <cstring> 
  
 #include <iostream> 
 #include <fstream>
  
  
 9 
  
 10
  
 using namespace std;
  
  
 11 
  
 12 
  
 13 
  
 14 
  
 15 
  
 16 
  
 17
  
 class Vector{ 
 private:
  
  
  ...
  
 public:
  
  ...
  
 };
  
  
 18 
  
 19
  
 #endif
  
     
  
  
 Lines 1, 2, and 19 ensure that the header file expands to the source code in between if and only if the macro
  
 variable 
 MYVECTOR
  is defined. Here 
  
 MYVECTOR
  is not good nomenclature, as it may be inadvertently reused by some
  
 other header, subverting our attempt to ensure conditional inclusion of header files.
  
  
 It is typical for C/C++ sources to begin by including a number of header files to interface to other source files 
 or libraries. C programs include the header 
 assert.h
  to use the 
 assert()
  macro (see section 
  
 1.2.3↑
 ). C++",NA
1.3.2Aitken transformation in C++,"The source files 
 Aitken.cpp
 , 
 Leibniz.cpp
 , and 
 Logseries.cpp
  contain the C++ implementation of the Aitken
  
 transformation and its application to the Leibniz and log series. The listing below is of the C++ header file
  
 Aitken.hh
 .
  
  
  
  
 1
  
 #ifndef AitkenAugust09DVjli",NA
√,"1,",NA
√,"2,…,",NA
√,"100. Write a C/C++ program that partitions the numbers
  
 into two sets such that the difference of the sums of the two sets is as small in magnitude as possible. Does your 
 program work for 1000 numbers or for 10
 6
  numbers?
 [18]
  
 Exercise:
  Write a C/C++ program that will open a file and print the last 
 n
  lines of the file. The name of the file 
 and 
 n
  are inputs to the program.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
{,"0,1,…,59",NA
},".
  
 There are exactly 144 sexagesimal numbers with leading digit 
  
 a
 n
  − 1
 =1, 
 n
  ≤ 6, and a terminating sexagesimal
  
 expansion for their reciprocals. Determine all 144 such numbers and their sexagesimal reciprocals. For
  
 n
 =6,7,…,20, determine the number of sexagesimal numbers with 
 a
 n
  − 1
  ≠ 0 and a terminating reciprocal.
 [19]
  
 Exercise:
  C 
 struct
 s are like C++ classes with public data members but no function members. In the following
  
 struct
  
  
 struct node{
  
  double x;
  
  struct node *next; }
  
  
  each node points to the next to form a linked list (see figure 
 1.6↑
 ). The 
 next
  
  
 field of the last node is assumed to be 
 NULL
 . Write a function that takes a pointer to the first node in a linked list
  
 and reverses the linked list.
  
 Exercise:
  The definition of 
 Aitken()
  given here overwrites the input sequence. Give an implementation that
  
 does not overwrite the input sequence and that corresponds to the declaration on line 6 of 
 Aitken.hh
  listed on page
  
 1↑
 .
  
 Exercise:
  Time C++ (page 
 1↑
 ) and C (page 
 1↑
 ) implementations of the Aitken transformation and compare.",NA
1.4A little Fortran,"In this section, we show a bit of Fortran syntax. The syntax is deliberately Fortran 77 and not the newer varieties.
  
 When the need to use old Fortran codes arises, it is often Fortran of this variety. We do not recommend 
 programming in Fortran.
 [20]
  The language is rigid and does not allow for dynamic data structures such as linked 
 lists, trees, and graphs in its 77 version. Such data structures are increasingly used in scientific computing and are 
 indispensable to computer science.
  
 The core data structure in Fortran is the array. In our opinion, Fortran does not do a good job here. In 
 Fortran, the array is thought of as a variable name, the length of the sequence, and the type of each item, which 
 determines the size of each item in bytes. The variable name is actually a pointer (an address), but only covertly and 
 not explicitly as in C. Fortran does not allow pointers in any generality. The Fortran array is an abstraction that 
 strives to be close to what happens on the machine. However, it is an awkward abstraction because the notion of 
 pointers is not thrown away but adopted covertly in a highly restricted form.
  
  
 Unlike C/C++, the Fortran language does not provide access to machine capabilities. For some of the more 
 sophisticated optimizations, the Fortran language is inadequate.
  
  
 There is a belief that Fortran is faster than C/C++. This belief is a complete myth, being no more than an 
 indication of the proficiency of those who believe in it, and will be completely dispelled later.
  
  
 Part of the Fortran code for applying the Aitken iteration to the logarithmic series follow. The listing of the 
 function 
 partialsum(x,n)
  follows.
 [21]
  It returns the partial sum of the first 
 n
  terms of the Taylor series of
  
 log(1+
 x
 ).
  
  
  
  double precision function partialSum(x, n) 
 double precision x
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
1.5References,NA,NA
Bibliography,"[1] B. Kernighan, D. Ritchie : 
 The C Programming Language
  
 . Prentice-Hall , 1988 .
  
 [2] B. Stroustrup : 
 The C++ Programming Language
  
 . Addison-Wesley , 1997 .
  
 [3] C. Brezinski, M. Redivo Zaglia
  
 : 
 Extrapolation Methods: Theory and Practice
  
 . North Holland , 1991 .
  
 [4] D. Kozen : 
 Automata and Computability
  
 . Springer-Verlag , 1997 .
  
 [5] D. Ritchie : 
 The development of the C language
  
  in 
 History of Programming Languages II
  (T.J. Bergin and R.G.
  
  
 Gibson, ed.). Addison-Wesley , 1996 .
  
 [6] D.E. Knuth : 
 Selected Papers on Computer Science
  
 . Cambridge University Press, 1996 .
  
 [7] D.E. Knuth : 
 The Art of Computer Programming
  
 . Addison-Wesley , 1998 .
  
 [8] G.A. Baker, P. Graves-Morris
  
 : 
 Pade
 ́
  Approximants
 . Cambridge University Press, 1996 .
  
 [9] J.M. Borwein, P.B. Borwein : 
 Pi and the AGM: A Study in Analytic Number Theory and Computational
  
 Complexity
 . Wiley-Interscience , 1998 .
  
 [10] K.V. Sarma : 
 A History of the Kerala School of Hindu Astronomy
  
 . Vishveshvaranand Institute
  
 , 1992 .
  
 [11] Morris Kline: 
 Mathematical Thought from Ancient to Modern Times
  
 . Oxford University Press, 1990 .
  
 [12] R.S. Westfall : 
 Never at Rest: A Biography of Isaac Newton
  
 . Cambridge University Press, 1980 .",NA
2C/C++: Libraries and Makefiles,"Splitting a program into several source and header files, as in the previous chapter, is essential but not sufficiently 
 powerful in itself to capture the conceptual relationships of many programs. When the interdependence between the 
 modules is complex, it is no longer adequate to put all the source files in a single directory. The source files must be 
 organized into directories and subdirectories to bring greater order and clarity.
  
 There are two powerful ideas for bringing greater modularity into C/C++ programs, and both of them will be 
 introduced in this chapter. The first idea is to combine object files into libraries, and the second idea is to organize 
 program sources into a source tree.
  
 In outline, a C/C++ program is built as follows. There are program sources to begin with. These are turned 
 into object files, which mainly consist of machine instructions, by the compiler. The linker eliminates unresolved 
 external references and merges the object files to produce an executable. The two ideas for bringing greater 
 modularity occur at different points in this process.
  
 The organization of sources into a tree precedes both compilation and linking. The solution to most problems 
 naturally breaks up into several components. For example, an image-processing program may be broken up into 
 modules for handling different image formats, modules for displaying images, modules for image transformations, 
 modules for image enhancements, modules for combining images, and so on. If the sources for each of these 
 functions is put in separate directories, the directories become modules, and the sources are now submodules within 
 these directories. Although an overly deep hierarchy can cause complications and must be used only for truly 
 complex programs, one can easily imagine directories within directories so that the source files are grouped into 
 modules, and these modules are grouped into higher level modules, and so on in a tree-like hierarchy.
  
  
 In contrast, libraries follow compilation and precede linking. The linking model is always flat. It does not 
 matter how or if the sources are arranged in a tree. The linker takes in a flat list of object files and smashes them
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.1Mixed-language programming,"Compilers translate 
 .c
  (C), 
 .cpp
  (C++), and 
 .f
  (Fortran) sources into 
  
 .o
  object files. The 
 .o
  object files are
  
 mainly a sequence of machine instructions. If the source file calls functions defined externally, which is the typical 
 scenario, there will be unresolved names in the corresponding object file. In section 
 2.1.1↓
 , we look at the map from 
 sources to object files as a precursor to mixed-language programming.
  
  
 The manner in which for-loops and other constructs map to machine instructions is the topic of the next 
 chapter. In this section, we only look at the map from globally visible names in the sources to names in the object
  
 file. The 
 aitken.c
  source looks as follows:
  
  
  
 #include <assert.h> 
  
 #include ""aitken.h"" 
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.1.1Transmutation of names from source to object files,"A 
 .o
  object file is mostly a collection of machine instructions that translate the corresponding source into
  
 machine-intelligible language. If the source has a statement such as 
 a=b+c
 , for example, the names 
  
 a
 , 
 b
 , 
 c
  typically
  
 disappear from the object file. The compiler decides the memory locations or registers that these variable names 
 map to. What is found in the object file is simply an add instruction of some type with operands being either 
 memory locations or registers.
  
 Not all names present in the source disappear, however. Those names present in the source that survive in 
 the object file are some of the most important. These are, typically, names of functions that may be called from 
 other object files or names of functions defined in external object files that are called from this one. Variable names 
 also may have global scope.
  
 The names present in the object file are needed to resolve undefined references during linking of object files. 
 The names may not be exactly the same as in the original source. Compilers may alter names before mapping them 
 from source to object files. In C, the names are not altered at all. In C++, the names must necessarily be altered to 
 support the overloading facility that allows the same name for multiple functions. In Fortran, too, the names are 
 altered, although the only reason here seems to be to maintain compatibility with earlier conventions.
  
 The GNU/Linux command 
 nm
 [23]
  lists the names present in an object file. To examine the object file
  
 aitken.o
 , we use the command 
  
 nm aitken.o
 . A part of the output of that command follows.
  
  
  
  
 1 
  
 2 
  
 3
  
  U _intel_fast_memcpy 
  
 0000000000000000 T aitken 
  
 0000000000000070 T aitkenExtrapolate
  
  
  
  
 The second and third lines verify that names are unchanged when a C source is transformed to an object file.
  
 This object file was produced using Intel’s 
 icc
 . The first line refers to a function call inserted by the compiler that
  
 was not present in our source. That name is undefined, but the linker will supply the appropriate definition. Any 
 compiler may insert function names during optimization.
  
 The function names are preceded by an address that is 16 hexadecimal digits long and by the letter 
 T
 . The
  
 letter is 
 T
  because both functions reside in the text area of the object code. The letter would be 
 C
  for the name of a
  
 Fortran common block or a C global variable defined outside the scope of any function. The hexadecimal addresses 
 indicate that code for 
 aitken()
  begins at 0 and for 
 aitkenExtrapolate()
  at 70 (hexadecimal). These addresses
  
 will be shifted by the linker when it merges several object files into a single executable.
  
 A partial listing of the output of 
 nm leibniz.o
  is included to make one more point about the transmutation",NA
2.1.2Linking Fortran programs with C and C++,"In scientific computing, C and C++ functions may need to call Fortran routines. Scientific software from earlier 
 generations tends to be in Fortran 77.
  
 To use Fortran functions within C or C++ programs, the naming used for the Fortran functions in C or C++ 
 has to be cognizant of the way the names in the source files are altered in the object file. We want the names to 
 agree in the object files because it is the object files and not the source files that get linked against each other. If the 
 naming is right, the linker takes care of resolving the function calls.
  
 Let us implement the repeated application of Aitken transformations to partial sums of the Leibniz series by
  
 mixing Fortran and C programs. Part of the output of 
 nm aitkenf.o
  is given below.
  
  
  
  
  U _intel_fast_memcpy 
  
 0000000000000000 T aitken_ 
  
 0000000000000220 T aitkenextrapolate_
  
  U for_write_seq_fmt
  
  
  U for_write_seq_lis 
  
 0000000000000150 T printseq_
  
  
  
 We will write a C program that calls the functions defined in 
 aitkenf.o
  to extrapolate the Leibniz series to
  
 illustrate the nature of mixed-language programming.
  
 The C code includes the following declarations near its beginning.
  
  
  
 extern void aitken_(double *seq1, double *seq2,  
 int *len); 
  
 extern void printseq_(double *seq, int *len); 
 extern double aitkenextrapolate_(double *seq1, 
  
  
  double* seq2, int * len);
  
  
  
 The 
 extern
  keyword indicates that the three function names that are declared must be found in some other
  
 object file. The underscore is appended to the names to follow the convention of the Fortran compiler. This 
 convention is common among Fortran compilers but may not be universal. The three arguments to 
 aitken_
  have
  
 types 
 double *
 , 
 double *
 , and 
 int *
 . The first few lines of the definition of 
 aitken()
  in the Fortran source are
  
 as follows.
  
  
  
  subroutine aitken(seq1, seq2, len) 
 double precision seq1(*), seq2(*) 
 integer len
  
  
  
 The first argument of 
 aitken()
  must be an array of double-precision numbers; once the function is called,
  
 seq1
  becomes another name for that array. The first argument is nothing other than a pointer to a double, although",NA
2.2Using BLAS and LAPACK libraries,"The basic concepts of linear algebra are matrices and vectors. Many problems in science, such as the numerical 
 solution of partial differential equations and numerical optimization, reduce to problems in numerical linear algebra. 
 BLAS and LAPACK are widely used numerical linear algebra libraries.
 [24] 
  
  
 The BLAS library is split into three levels. Functions for vector operations such as dot products are included 
 in the first level, for matrix-vector operations in the second level, and for matrix-matrix operations such as matrix 
 multiplication in the third level. The split into three levels is conceptual and reflects the historical order in which the 
 interfaces for the BLAS functions were specified. Implementations of BLAS and LAPACK such as MKL and 
 ACML, which are supported by Intel and AMD, respectively, bundle all three levels of BLAS as well as LAPACK 
 into the same library.
  
 The specifications of the BLAS functions have been frozen for nearly three decades. However, LAPACK 
 evolves from time to time to include new algorithms. LAPACK is built on top of BLAS. Functions for solving 
 systems of matrices, solving linear least squares problems, finding eigenvalues, and finding singular values are 
 found in LAPACK.
  
 Although the BLAS specifications have been frozen for decades, implementations of BLAS have to 
 constantly respond to the rapid changes in computer architecture. A good implementation of matrix multiplication 
 in 1990 looks nothing like a good implementation of matrix multiplication in 2015. In the intervening decades, 
 computer architecture has advanced to include instruction pipelines, instruction-level parallelism, multiple levels of 
 cache memory, expanded register sets, out-of-order execution, and multiple processing cores. Good 
  
 implementations optimize BLAS for all these features of modern computers.
  
 The early specifications of BLAS were given in Fortran. However, Fortran does not provide adequate access 
 to features of computer architecture. One cannot see the source code of commercial BLAS implementations in 
 libraries such as MKL and ACML are coded. However, it is almost certain that BLAS functions are coded in C and 
 in assembly language native to the computer architecture that is targeted. C functions can easily mimic the 
 interfaces and calling conventions of Fortran subroutines.
  
 Many LAPACK functions were coded in Fortran using BLAS years ago. The hope was that architecture-
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
"2.2.1Arrays, matrices, and leading dimensions","We look at multidimensional arrays in C/C++ briefly, although it is nearly always better to work with one-
  
 dimensional arrays. A two-dimensional array in C can be defined as follows:
  
  
  
 double two_d[20][40];
  
 Here 
 two_d[][]
  can be thought of as a two-dimensional array with 
  
 20 rows and 40 columns. One-
  
 dimensional arrays are almost equivalent to pointers. However, two-dimensional arrays are not. 
  
  
 The array subscripting operator 
 []
  has left to right associativity. Therefore, the compiler parses our
  
 definition as
  
  
 double (two_d[20])[40];
  
  
 20, each entry of which is an array of 
  
 40 doubles. In memory, the 800
  
 In words, 
 two_d
  is an array of size 
  
 double
  locations are next to each other same as for a one-dimensional array of size 800. However, it is illegal to
  
 say
  
  
  
 double *p = two_d;
  
 The value 
 two_d
  is of type pointer to an array of 
  
 40 
 double
 s and not 
 double *
 . The following usage would
  
  
 be legal.
  
  
  
 double (*p)[40]; 
 p = two_d;
  
 Here 
 p
 , like 
 two_d
 , is a pointer to an array of 
  
 40 
 double
 s. The connection of multidimensional arrays in C
  
 to pointers is not straightforward, which is the principal reason to avoid multidimensional arrays in C/C++. 
 Legitimate uses of multidimensional arrays are rare but do exist.
  
 Suppose we want a matrix of dimension 20×40. We can simply say
  
  
  
 double a[800];
  
 It is a bad idea to allocate large data structures statically.
 [26]
  More generally, we can make room for an
  
  
 m
 ×
 n
  matrix as follows:
  
  
  
 double *a = (double *)malloc(m*n*sizeof(double));
  
  
  
 We must remember to say 
 free(a)
  when the memory is no longer needed.
  
 Here we come to the distinction between column-major and row-major storage. In column-major storage,
  
 the (
 i
 ,
 j
 )th entry of 
 a[]
  is accessed as
  
  
  
 a[i+j*m]
  
  
  
 Here 0 ≤ 
 i
 <
 m
  and 0 ≤ 
 j
 <
 n
 . The column-major format is used by Fortran, BLAS, and LAPACK. In row-
  
  
 major storage, the (
 i
 ,
 j
 )th entry of 
 a[]
  is accessed as
  
  
  
 a[i*n+j]",NA
2.2.2BLAS and LAPACK,"BLAS and LAPACK functions, to which we now turn, typically have long argument lists. For example, the Fortran
  
 interface to the BLAS function for multiplying a matrix and a vector has the following declaration in C:
  
  
  
 extern ""C"" void dgemv_(char *,int *,int *,double *,double *, 
 int *,double *,int *,double *,double *, 
  
  int *, int);
  
 This function, which has 12 arguments in total, implements the operation 
 y
  ← 
 αAx
 +
 βy
  or 
 y
  ← 
 αA
 T
 y
 +
 βy
 .
  
 The first argument, which is a character string, allows us to specify whether the matrix 
 A
  must be transposed or not. The 
 next 10 arguments allow us to specify the entries and dimensions of the matrix 
  
 A
  and of the vectors 
 x
  and 
 y
 , as 
 well as the scalars 
 α
  and 
 β
 .
  
 The last argument to 
 dgemv_() 
 is the only one that is not a pointer. Because all arguments to Fortran
  
 subroutines are passed by reference, we may expect all arguments in the C declaration of a Fortran subroutine to be",NA
2.2.3C++ class interface to BLAS/LAPACK,"The 
 Vector
  class of section 
 1.3.1↑
  is an attempt to capture the general concept of vectors. The 
 LU_Solve
  class of
  
 this section is narrowly defined. It does just one thing, which is to provide an easy interface to LAPACK’s LU 
 solver functions 
 dgetrf()
  and 
 dgetrs()
 .
  
 The class is defined (in the header file 
 lusolve.hh
 ) as follows:
  
  
  
 class LU_Solve{ 
  
 private:
  
  
  int dim;
  
  
  double *A;
  
  
  int *ipiv; 
  
 public:
  
  
  LU_Solve(double *a, int dimi);
  
  ~LU_Solve();
  
  
  void factorize();
  
  
  void solve(double *v); 
  
 };
  
  
  
 In the 
 Vector
  class, the member functions were defined within the class definition itself. In the 
 LU_Solve
  
 class, the member functions are declared as part of the class definition, but they are defined separately. The class 
 constructor 
 LU_Solve()
  takes the matrix to be solved as well as its dimension as arguments. The member function",NA
2.3Building programs using GNU Make,"The organization of source files into directories and subdirectories is the heart of modular programming. 
 Typically, several source files cooperate to do a task, and yet more source files are involved in bigger tasks. A 
 directory holds source files that are related or perform similar tasks. Directories may be organized further into 
 subdirectories in a source tree to reflect the structure of the program.
  
 The 
 make
  utility provides a method for building a program from its source tree. Each source file must be
  
 turned into an object file, and the object files must be linked together to form executables. Compiling and linking 
 become quite repetitive and error-prone if done from the command line. Makefiles offer a more systematic 
 approach to building programs.
  
 A build system such as 
 make
  is essential to C/C++ programming. The Makefiles hold valuable information
  
 about the structure of the program as a whole, which is absent from the source files. In Python, the correspondence 
 between modules and the directory hierarchy is wired into the language, but there is no such facility in C/C++. 
 Modular programming aims to organize and conquer. There can be no modular programming without organization 
 of program sources into a directory hierarchy. Well-thought-out source trees aid programming as much as
  
 structured definitions of functions and classes.
  
  
  
  
 Figure 2.2Directories and files in the source tree for this book. The entire source tree is found at 
  
 https://github.com/divakarvi/bk-spca 
  
 .
  
  
  
 We begin our discussion of GNU 
 make
  by looking at the source tree shown in figure 
 2.2↑
 . The
  
 makevars.mk
  file at the root of the source tree defines 
  
 make
  variables that are used in modules such as 
 utils
  and
  
 linking
 . These modules correspond to directories. Some modules have several submodules. For example, the
  
 linking
  module has submodules 
  
 aitken
 , 
 easy
 , 
 fft
 , 
 lib
 , and 
 linalg
 .
  
 Much of the discussion in this chapter pertains to these submodules of 
 linking
 . All the Aitken iteration
  
 programs are in the 
 aitken
  submodule. The programs we used to illustrate use of the BLAS and LAPACK
  
 libraries are in 
 linalg
 . We will discuss the use of shared and static libraries as well as the Fast Fourier Transform
  
 (FFT) later in this chapter. The programs and makefiles that will aid that discussion are in 
 lib
  and 
 fft
 .
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.3.1The ,utils/,NA
folder,"A testing program in the source 
 linking/linalg/test_lusolve.cpp
 , which uses functions defined in sources in 
 a different folder in the source tree 
 utils/
  (see the source tree in figure 
 2.2↑
 ), will be described here. Later in 
 section 
 2.3.7↓
 , this program is used to illustrate how GNU’s 
 make
  utility builds an executable from sources 
 scattered in different parts of the source tree.
  
 The modules in 
 utils/
  facilitate timing, generation of random numbers, gathering statistics, making tables, 
 and manipulation of 
 double
  arrays. All the modules in 
 utils/
  are used extensively. The modules are used for 
 testing, timing, and laying out data elsewhere in the source tree. The corresponding code is almost always omitted 
 from the text. We avoid mentioning the modules in 
 utils
  for the most part, but a brief discussion is given here.
  
 In more complicated settings, there will be many dependencies between the directories and subdirectories of 
 the source tree. In the source tree for this book, the directories are mostly self-contained. Most of the dependencies 
 are on utilities in 
 utils/
  and on modules for plotting and displaying data that are not shown in the source tree in 
 figure 
 2.2↑
 .
  
  
 The header file 
 utils.hh
  defines a macro called 
  
 assrt()
 . This macro, which is used frequently, is similar 
 to 
 assert()
 , which is defined in the C standard header file 
 assert.h
 . The only difference is that 
  
 assrt()
  always 
 checks its assertion, and not only if the preprocessor macro 
 DEBUG
  is defined. We find little use for debuggers. The 
 debugger is a blunt tool that works without an idea of the logical structure of the program. When programs are 
 compiled in debug mode, the memory layout of their data can change. Memory errors may not be reproduced 
 faithfully in debug mode.
  
 In addition to 
 assrt()
 , 
 utils.hh
  declares the following functions:
  
 void array_abs(double *v, int n); 
  
 double array_max(double *v, int n); 
  
 void array_show(double *v, int n, const char* mesg); 
 void array_diff(double *restrict v,",NA
"2.3.2Targets, prerequisites, and dependency graphs","Figure 2.3Makefile dependency graph.
  
  
  
 Dependencies are fundamental to 
 make
 . Figure 
 2.3↑
  shows that each object file depends on a single source
  
 and a header. Typically, the dependency is on multiple header files, unlike the simple situation shown in the figure. 
 Each executable in turn depends on multiple object files.
  
 The first purpose of a Makefile is to capture the dependency graph between headers, sources, object files, 
 and executables. Each object in the dependency graph is typically a file as in figure 
 2.3↑
 . All files that have 
 incoming edges in the dependency graph are targets. The incoming edges indicate that a target file must be built 
 using a set of some other files. Those other files are the prerequisites. The targets may reappear as prerequisites, as 
 is the case for all the object files in figure 
 2.3↑
 .
  
 Typically, 
 make
  assumes that a target file may not exist. The target file is considered out of date if its time
  
 stamp (accessed using 
 stat
  on GNU/Linux) is older than that of any of its prerequisites. If a target either does not
  
 exist or is out of date, 
 make
  takes it upon itself to create a file corresponding to the target.
  
 The question arises of how 
 make
  can create a new file corresponding to a target. To answer that question is
  
 the second purpose of a Makefile. The Makefile associates each target with a recipe, and the recipe is a shell 
 command invoked by the 
 make
  utility to build the target if the target is either absent or out of date.
  
 The executables 
 leibniz.exe
  and 
 logseries.exe
  are built using the following Makefile:
  
  
  
  
 1 
  
 2
  
 leibniz.exe: leibniz.o aitken.o 
  
  
 icc -o leibniz.exe leibniz.o aitken.o
  
  
 3 
  
 4 
  
 5
  
 logseries.exe: logseries.o aitken.o 
  
  
 icc  -o logseries.exe logseries.o aitken.o 
  
 -lm
  
 6 
  
 7 
  
 8
  
 aitken.o: aitken.c aitken.h 
  
 icc -fPIC -c aitken.c
  
  
 9 
  
 10 
  
 11
  
 leibniz.o: leibniz.c aitken.h 
  
 icc -c leibniz.c",NA
2.3.3Make variables in ,"makevars.mk
  
 Almost all Makefiles have 
 make
  variables. We use 
  
 makevars.mk
  at the root of the source tree to show how 
 make 
 variables are used (see figure 
 2.2↑
 ).
  
 The 
 makevars.mk
  file will serve us throughout this book. If has three sections. The first section defines
  
 variables.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10
  
 #########
  
 CPP 
  
  := icpc
  
 CFLAGS   := -xHost -O3 -prec-div -no-ftz -restrict \ 
 -Wshadow -MMD -MP 
  
 FFTWINC  := $(FFTW_INC) 
  
 MKLINC := -mkl 
  
 ######### 
  
 MKLLIBS := -mkl=sequential 
  
 MKLTHRD := -mkl=parallel 
  
 FFTWLIBS  :=  $(FFTW_LINK)
  
  
  
  
 In C/C++, a variable is a name for a location in memory. In 
 make
 , a variable is a string.
  
 The variable definitions from 
 CPP
  to 
 FFTWLIBS
  use 
 :=
  and not 
 =
  following the variable. The use of 
 := 
 implies that the variables are evaluated immediately during the first pass. We do not discuss the other type of 
 variable evaluation, which is called deferred evaluation.
  
  
 All characters in a line following the 
 #
  character, including that character, are ignored. Lines 1 and 7 begin 
 with the 
 #
  character and are therefore comment lines.
  
 The variable 
 CPP
  is set to 
 icpc
  (line 2). It is the name of the C++ compiler used later in 
 makevars.mk
 .
  
 The 
 CFLAGS
  variable (lines 3 and 4) stands for the options passed to the C++ compiler. The definition of 
 CFLAGS
  is split across two lines using the continuation character \. It merits careful scrutiny. The optimization level 
 is 
 -xHost -O3
 . The 
 -xHost
  flag ensures that the compiler generates instructions corresponding to the highest 
 capability of the machine. This flag is essential for our purposes.
  
  
 We do not bother with debug levels such as 
 -g
  or 
 -O0
 . The recommended optimization option in 
 icpc
  is 
 -
 fast
 . We do not use that option. It turns on 
  
 -ipo
  or interprocedural optimization, which we do not want. Other 
 dubious flags are also turned on by 
 -fast
 .
  
 By default the 
 icpc
  compiler may use a less precise but faster division for IEEE double-precision numbers, 
 according to the compiler’s manual. It is unclear whether the faster division is ever really faster or whether the flag 
 ever really has any effect. The 
 -prec-div 
 flag (line 3) forces conformance to IEEE arithmetic. The compiler 
 manual states that the flush-to-zero optimization is used for really small numbers that almost underflow. This is 
 another “optimization” of dubious value and uncertain meaning. It is turned off using 
 -no-ftz
  (line 3).
  
  
 The 
 -restrict
  option (line 3) enables 
  
 restrict
  qualified pointers, a C99 feature we find to be quite 
 valuable in the next chapter and later.
  
 C++ member functions may accidentally redefine a class variable, leading to runtime errors. For example, 
 state
  could be a data member that keeps track of the state of the class object, and a member function, which wants 
 to set it to 1, may say 
 int state=1
  instead of 
 state=1
 . The 
 -Wshadow
  option (line 4) tells the compiler to issue a 
 warning when variables defined in an outer scope are redefined in an inner scope.
  
  
 The 
 -MMD
  and 
 -MP
  options (line 4) to the 
  
 icpc
  compiler tell it to generate a 
  
 .d
  file listing all dependencies 
 of the source on header files. The way dependencies of C/C++ sources on header files is handled is discussed in",NA
2.3.4Pattern rules in ,"makevars.mk
  
 Makefile rules are made up of dependencies and recipes. The variables defined in the first section of 
 makevars.mk
 , 
 which we just discussed, are used to construct recipes. The recipes have a formulaic character. For example, if the 
 target is an object file to be built from a C++ source, the recipe generally invokes the C++ compiler specified by 
 CPP
  using the options listed in 
  
 CFLAGS
 . Pattern rules take advantage of the repetitive nature of recipes to simplify
  
 their specification.",NA
2.3.5Phony targets in ,"makevars.mk
  
 We will discuss recursive make, useful for building programs with object files in several subdirectories, shortly. 
 Recursive make relies on phony targets. Our first encounter with phony targets is in a simpler context. The third
  
 and last section of 
 makevars.mk
  is listed below.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5
  
 .PHONY: clean cleanxx 
  
 clean: 
  
  
 rm *.o; rm *.exe; rm a.out; 
  
 cleanxx: 
  
  
 rm *.o; rm *.a; rm *.so;  rm *.exe;  rm
  
 *.d
  
  
  
 The rule with target 
 .PHONY
  has 
 clean
  and 
 cleanxx
  as prerequisites (line 22). These are treated as phony
  
 targets. Ordinarily, 
 make
  expects to find a file with the same name as a target and checks the latest modification 
 time of that file to determine whether the target is out of date. For phony targets, 
 make
  does not look for a file of the 
 same name. Phony targets are always assumed to be out of date.
  
  
 In this example, saying 
 make clean
  will remove all object files and executables in the current directory (the 
 directory from which 
 make
  is invoked). Making the target 
 cleanxx
  removes certain other files in addition. The 
  
 .d 
 files (see below) used to capture dependencies of a C++ source on header files get removed with 
 cleanx
 x.",NA
2.3.6Recursive ,make,NA
 and ,.d,NA
 files,NA,NA
2.3.7Beyond recursive ,"make
  
 Modularity and simplicity are two virtues of recursive make. However, there are several problems with it.
 [33]
  It can 
 be slow for large projects because a new shell process is created every time 
 make
  is called recursively. It does not
  
 gel too well with parallel 
 make
  using the 
 -j
  option. It leads to needless compilation because all object files and
  
 libraries that may be externally needed are built during recursive 
 make
  and not just those that are actually needed. It
  
 must be said that these deficiencies are not fatal. Recursive 
 make
  is still used.
  
 There appears to be a fundamental tension between the two-pass structure of the 
 make
  utility and recursive
  
 invocation of 
 make
 . The first pass is supposed to build a dependency graph, for example, of object files on sources
  
 and executables on object files. The second pass is supposed to invoke recipes to update targets that are out of date 
 with respect to their prerequisites. The recursive invocation of 
 make
  happens during the second pass. The
  
 dependency graph of object files and sources in the external module is built only when its Makefile is invoked 
 recursively. The result is to splinter the dependency graph, leading to multiple first and second passes. 
  
 It 
 is possible to avoid recursive 
 make
  entirely by building a single dependency graph for all the sources, object 
 files, and executables in the project.
 [34]
  One way to do this is to include a 
  
 rules.mk
  file in each
  
 subdirectory or submodule. The 
 rules.mk 
 file contains pattern rules specific to the subdirectory as well as
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.3.8Building your own library,"Our discussion of GNU
  make
  is uncommonly detailed and for a reason. Much of the time spent on C/C++ syntax is 
 wasted without a fairly good knowledge of 
 make
 . There is no modular programming in C/C++ without the 
 make 
 utility or an equivalent build system. The programmer is limited to single source files or awkward collections of 
 source files in a single directory.
  
 We end our discussion of 
 make
  by showing how to build and link static and shared libraries. Libraries 
 provide a level of modularity beyond what is possible within a source tree. Any program that is linked against a 
 library in effect treats the external library as a module.
  
 2.2↑
 ) has 
 utils.cpp
 , which provides basic facilities 
 The 
 utils/
  subdirectory in the source tree (see figure 
  
 such as 
 verify_dir()
 . The 
 linking/aitken/
  subdirectory implements the Aitken iteration in the C source 
 aitken.c
 . 
 The source 
 fft_mkl.cpp
  in 
 linking/fft/
  provides an interface to part of MKL’s Fast Fourier 
  
 Transform (FFT) facilities. The FFT is the topic of the next section. The Makefile below is in 
 linking/lib
 /. It
  
 shows how to combine 
 utils.o
 , 
 aitken.o
 , and 
 fft_mkl.o
  and build a shared or static library.
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15
  
 include ../../makevars.mk 
  
 CFLAGS := $(CFLAGS) $(MKLINC) 
 ###### 
  
 MODS := ../../utils ../aitken ../fft 
  
 .PHONY: $(MODS) 
  
 $(MODS): 
  
  
 @echo 
  
  
 make --directory=$@ objl 
  
  
 @echo 
  
 ###### 
  
 test_lib.o: test_lib.cpp
  
 -include test_lib.d 
  
 ###### 
  
 libxmath.so: $(MODS) 
  
  
 icpc -shared -o $@ ../../utils/utils.o
  
 \
  
 16
  
  ../aitken/aitken.o
  
 \
  
 17 
  
 18 
  
 19
  
  
  
  ../fft/fft_mkl.o 
 libxmath.a: $(MODS) 
  
  
 ar rcs $@ ../../utils/utils.o
  
 \
  
 20
  
  ../aitken/aitken.o
  
 \
  
 21
  
 ######
  
  ../fft/fft_mkl.o
  
 22",NA
2.4The Fast Fourier Transform,"So far in this book, modular programming in C/C++ has been the focus. Organization of sources into a source tree, 
 Makefiles, and libraries is the basis of modular programming. The 
 make
  utility, or an equivalent build system, is
  
 indispensable to modular programming in C/C++.
  
 Among programming languages, the C/C++ framework is the best---and often by far---for writing fast 
 programs. C/C++ programs can be several hundred or even several thousand times faster than programs written in 
 interpreted languages. In this last section, we look at the speed of a few implementations of the Fast Fourier 
 Transform (FFT). Program speed is a major theme of the rest of this book.
  
 Program speed is influenced by many factors, including programming skill, compilers, the processor 
 hardware, and the memory system. Each layer of software and hardware is heavily designed. Program speed is a 
 discontinuous function of the design parameters. A small change in a program, or in the environment in which it 
 runs, can result in unpredictable changes in program speed. In this section, we look at a few FFT implementations 
 to gain an understanding of some of the factors that influence program speed. What does it mean to say that an 
 implementation is optimal? To what extent does programming skill affect program speed? These are some of the 
 questions we ask. We find already that programming skill has a great influence on program speed. Later chapters 
 set forth many of the concepts that must be understood to produce efficient implementations of scientific programs.
  
 A program’s speed depends on the hardware configuration. In this chapter, we stick to a single processor 
 core. In later chapters, we will see that even if a program is multithreaded or networked, understanding what 
 happens on a single core is a big part of the game.
  
 About 100% of the computers in use for scientific programming use the x86 architecture. Thus, this book too 
 adopts the x86 architecture. The x86 architecture evolves constantly. There are only a few particulars of the x86 
 architecture that will be of concern to us. These are discussed in the next chapter.
  
 For the most part, all that concerns us is the level of the instruction set, in particular, whether the instruction 
 set is SSE2, AVX, AVX2, or AVX-512 (see table 
 3.1↓
  of the next chapter). Thus, the machines we use will be 
 designated as 2.6 GHz SSE2 or 2.2 GHz AVX. The full names of the machines may be looked up from table 
 9.1↓
 of 
 the appendix. The SSE2 machines support 128-bit XMM registers, and the AVX/AVX2 machines support 256-bit 
 YMM registers.
  
 The clock signal that is fed into each processor core is the heart beat of the computer. The activities of the 
 processor are synchronized with the clock signal. The memory system and other parts of the computer must 
 accommodate themselves to the processor. If we measure program speed in cycles, we get a better sense of how 
 well the program is exploiting the hardware.
  
 In this book, program speeds are reported using cycles. We use measures such as flops (floating point 
 operations) per cycle for program speed and bytes per cycle for memory bandwidth. For some programs, we report 
 the number of cycles consumed directly. Measuring program speed in terms of cycles is somewhat unconventional.
  
 It is more typical to see GFlops (Gigaflops per second) for arithmetic speed and GB/s (Gigabytes per second) for 
 memory bandwidth. The second is a standard unit of time and its use is most appropriate when different hardware 
 configurations are being compared. Our concern, which is to write efficient programs on a given hardware 
 configuration, is quite different. Although we report timing measurements in cycles, they can be easily converted to 
 seconds using the frequency in GHz of the processor clock.
  
 The FFT is one of the most widely used algorithms in scientific computing and is fundamental to many 
 areas of science. It is an appropriate starting point for the discussion of the speed of scientific programs. In section 
 2.4.1↓
 , we introduce the FFT algorithm in outline. The purpose of this outline is to help understand the speed of 
 FFT implementations. The two major FFT implementations in the MKL and FFTW libraries are introduced in 
 sections 
 2.4.2↓
  and 
 2.4.3↓
 , respectively.
  
 Programs run in a complex environment and the complexity of the environment influences program speed in
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.4.1The FFT algorithm in outline,"The discrete Fourier transform of 
 f
 0
 ,…,
 f
 N
  − 1
  is defined as
  
 f̂
 k
 =1 
  
 N
  
 N
  − 
 1
 ⎲
  
 ⎳
  
 ω
  − 
 jk
 f
 j
 , for 
 k
 =0,…,
 N
  − 1.
  
 (2.1)
  
 j
 =0
  
 Here 
 ω
 =exp",NA
(,"2
 π",NA
√,"− 1 ⁄ 
 N",NA
),"is a primitive 
 N
 th root of unity. The 
 f̂
 k
  are linear functions of 
 f
 j
 . The discrete Fourier 
 transform from 
 f
 =",NA
(,"f
 0
 ,…,
 f
 N
  − 1",NA
),"to 
 f̂
 =",NA
(,"f̂
 0
 ,…,
 f̂
 N
  − 1",NA
),"can be written as 
  
 f̂
 =
 Mf
 , where 
 M
  is the 
 N
 ×
 N
  matrix whose
  
 (
 j
 ,
 k
 )th entry is 
 ω
  − 
 jk
  ⁄ 
 N
 .
  
  
 The inverse discrete Fourier transform is given by
  
 (2.2)
  
 f
 j
 =
  
 N
  − 
 1
 ⎲
  
 ⎳
  
 ω
 jk
 f̂
 k
 .
  
 k
 =0
  
 It too can be thought of as a matrix-vector product.
  
 The discrete Fourier transform and its inverse have an intimate connection to Fourier series. Suppose 
 f
 (
 x
 ) is
  
 a function with period 2
 π
 . Its Fourier coefficients are defined by
  
 c
 n
 =1 2
 π
  
 2
 π
  
 ⌠
 f
 (
 x
 )exp",NA
(,−,NA
√,"− 1
 nx",NA
),"dx
  for 
 n
 =0,±1,±2,…
  
 ⌡
  
 0
  
  
 If 
 f
 j
 =
 f",NA
(,"2
 πj
  ⁄ 
 N",NA
),", then 
 f̂
 k
  ≈ 
 c
 k
  for 0 ≤ 
 k
 <
 N
  ⁄ 2 and 
 f̂
 k
  ≈ 
 c
 k
  − 
 N
  for 
 N
  ⁄ 2<
 k
  ≤ 
 N
  − 1. If 
 N
  is even, 
  
 f̂
 N
  ⁄ 2
  ≈",NA
(,"c
 N
  ⁄ 2
 +
 c
  − 
 N
  ⁄ 2",NA
),"⁄ 2. Here 
 f
 (
 x
 ) is assumed to be a function that is integrable and sufficiently smooth.
  
  
 The FFT is a method to effect the discrete transform 
 (2.1)↑
  or its inverse 
 (2.2)↑
  using 
 O",NA
(,"N
 log
 2
 N",NA
),"arithmetic 
 operations, which is a considerable improvement over the 
 O",NA
(,"N
 2",NA
),"arithmetic operations required by direct matrix-
  
 vector multiplication.
 [38]
  The improvement in operation count is vital to making the FFT fast but is not the full 
 story. The implementation can make a difference of more than a factor of 10 to the program speed. We present the 
 structure of the power of 2 FFT but omit mathematical details.
  
  
 Suppose 
 N
 =2
 n
 . We assume the data to be 
  
 N
  complex numbers. The first step in the power of 2 FFT is to 
 separate the data into even and odd parts as follows:
  
 f
 0
 ,
 f
 2
 ,…,
 f
 N
  − 2
 and 
 f
 1
 ,
 f
 3
 ,…,
 f
 N
  − 1
 .
  
 An 
 N
  ⁄ 2 FFT is applied separately to the even and odd parts. The odd part is multiplied by the twiddle factors 
 1,
 ω
 ,…,
 ω
 N
  ⁄ 2 − 1
 . The FFT of size 
 N
  is generated by adding and subtracting corresponding points in the even and 
 odd parts.
  
  
 Because 
 N
  ⁄ 2 is also a power of 2, the FFTs of size 
 N
  ⁄ 2 are effected using the same strategy. Thus, the even 
 part and odd part are once again separated into even and odd parts to obtain four lists of numbers. Repeated
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.4.2FFT using MKL,"The header file 
 mkl_dfti.h
  declares the FFT functions implemented in the MKL library. The following class
  
 simplifies application of the FFT and its inverse to complex data:
  
  
 class fft_mkl{ 
  
 private: 
  
  
 int n; 
  
  
 DFTI_DESCRIPTOR_HANDLE handle; 
 public: 
  
  
 fft_mkl(int nin); 
  
  
 ~fft_mkl(); 
  
  
 void fwd(double *f){ 
  
  
  
 DftiComputeForward(handle, f); 
  
 } 
  
  
 void bwd(double *f){
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.4.3FFT using FFTW,"The FFTW library
 [39]
  has a much cleaner interface than MKL, although that may not be clear from the one-
 dimensional complex-to-complex case we deal with. The header file is 
 fftw3.h
 . The following tightly defined
  
 class offers a means to use FFTW functions for the FFT:
  
  
  
 class fft_fftw{ 
  
 private: 
  
  
 int n; 
  
  
 fftw_plan pf; 
  
  
 fftw_plan pb; 
  
 public: 
  
  
 fft_fftw(int nin); 
  
  
 ~fft_fftw(); 
  
  
 void fwd(double *f){ 
  
  
  
 assrt((long)f%16 == 0); 
  
  
  
 fftw_execute_dft(pf, (fftw_complex *)f, 
  
  
  
  
  (fftw_complex *)f); 
  
  
  
 for(int i=0; i < 2*n; i++) 
  
  
  
  
 f[i] /= n; 
  
  
 } 
  
  
 void bwd(double *f){ 
  
  
  
 assrt((long)f%16 == 0); 
  
  
  
 fftw_execute_dft(pb, (fftw_complex *)f, 
  
  
  
  
  (fftw_complex *)f); 
  
  
 } 
  
 };
  
  
  
 The sole task of the 
 fft_fftw
  class is to offer an easy interface to FFTW transforms for one-dimensional
  
 complex data. FFTW stores 
 fftw_plan
 s instead of a 
 handle
  as with MKL. There are different plans for forward 
 and backward transforms. The 
 fwd()
  and 
 bwd() 
 member functions are implemented within the class definition.
  
  
 Both the member functions use 
 assrt()
  (defined in 
 utils/utils.hh
 ) to verify that the pointer 
 f
  is 16-byte 
 aligned. A pointer is 16-byte aligned if it is divisible by 16 or, equivalently, if the last 
  
 4 bits are zero. Because 
 of the way the FFTW plans are set up, the pointers must be 16-byte aligned for correctness. FFTW recognizes that the 
 transforms are in place because the same pointer 
 f
  is used as the input and output argument to 
  
 fftw_execute_dft()
 .
  
 FFTW does not offer a facility for normalizing the forward transform. Therefore, the member function 
 fwd()
  normalizes explicitly using a for-loop. As we will see, this seemingly innocuous bit of code nearly halves 
 the program speed.
  
 The constructor and destructor for 
 fft_fftw
  are defined below.
  
  
  
 fft_fftw::fft_fftw(int nin) 
  
  
 :n(nin) 
  
 { 
  
  
 double *f = (double *)",NA
2.4.4Cycles and histograms,"How many cycles does a one-dimensional complex FFT of dimension 2
 10
 =1024 take? Program performance is 
 influenced by so many factors that the question is too simple to be answered. First, we have to say how the 
 measurement is taken and which implementation of the FFT is used. Here we assume the implementation to be 
 from the MKL library.
  
 The issue of measurement is more complicated. Suppose a single measurement is made. The cycle count is 
 likely to be atypically large. Suppose a great number of measurements are made but while applying the FFT to the 
 same data. This time the average or median cycle count is likely to be an underestimate. A great part of the expense 
 of the FFT is in reading data from memory. If the same data is repeatedly transformed, the data locations will be 
 cached near the processor core in cache memory. Caching reduces the expense of reading data.
  
 We measure FFTs of dimension 
 N
 =2
 10
  in a way that mimics what we consider to be a realistic scenario.
  
 We line up 10
 6
  problem instances in one long array of 2×2
 10
 ×10
 6
  double-precision numbers (the factor 
  
 2 at the 
 front accounts for complex data). This array is 16 GB. We successively apply the inverse FFT to each problem 
 instance and record 10
 6
  cycle counts. The median (or average) cycle count obtained is likely to be a fair estimate of
  
 the cost of an FFT of dimension 1024 in a large computation.
  
  
  
 (c)
  
  
  
  
 (a)
  
  
  
  
 (b)
  
  
 (d)
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
2.4.5Optimality of FFT implementations,"What is the fastest possible speed of an FFT implementation? The many system features that intrude into the 
 histograms hint that this question may not have a straightforward answer. However, a discussion is worthwhile. It 
 helps us understand what makes an FFT implementation efficient. The discussion is based on a 2.2 GHz AVX 
 machine.
 [41] 
  
  
 The power of 2 FFT performs 5
 N
 log
 2
 N
  double-precision floating point operations. Additions and 
 multiplications are in the ratio 3:2. Thanks to instruction-level parallelism and 256-bit YMM registers, a single 
 AVX processor core can complete four additions and four multiplications every cycle. If we consider arithmetic 
 operations alone, a theoretical lower bound is .75
 N
 log
 2
 N
  cycles.
  
  
 We would be justified in ignoring memory accesses if the number of arithmetic operation for each memory 
 access were large. The total number of bytes that must be accessed is 16
 N
  (16 bytes for each complex number). For 
 large 
 N
 , we indeed have 
  
 0.75log
 2
 N
 >16. However, the structure of the FFT does not allow the outer iterations 
 (upper levels in figure 
 2.5↑
 ) to be cached if 
 N
  is large. The inner iterations (lower in the figure) operate on small 
 packets of data, such as pairs or quartets. Caching can be effective for the inner iterations. As the size of the data 
 packets in the outer iterations becomes comparable to cache size, caching becomes less and less effective. The FFT 
 is caught between two opposing currents. On the one hand, large 
 N
  means more arithmetic operations per item of
  
 data. On the other hand, the data items accessed in the outer iterations cannot be cached as effectively.
  
  
  
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
√,"− 1 ⁄ 
 N
 ), prove that 
 ω
 k
  ≠ 1 and 
 ω
 kN
 =1 for 
 k
 =1,2,…,
 N
  − 1. Prove that 
 N
  − 1
  
 ⎲⎳
 ω
 kj
 =0
  
 j
 =0
  
 for 
 k
 =1,2,…,
 N
  − 1.
  
 Exercise:
  Use the previous exercise to argue that the matrix with (
 j
 ,
 k
 )th entry equal to 
 ω
 jk
  ⁄",NA
√,"N
  has 
 orthonormal columns. Conclude that the transforms 
 (2.1↑)
  and 
 (2.2↑)
  are inverses of each other.
  
 Exercise:
  The inverse discrete Fourier transform is defined by
  
 f
 j
 =
  
 N
  − 1
  
  
 ⎲
  
 ω
 jk
 f̂
 k
 ,",NA
√,"− 1 ⁄ 
 N
 ). Suppose 
 N
  is even and 
 N
 =2
 n
 . We may write 
 k
 =2ℓ +
 p
 , with 
 p
 =0 or 
 p
 =1, and
  
 decompose the sum as
  
 f
 j
 =
  
 n
  − 
 1
 ⎲
  
 ⎳
  
 1
  
 ⎲
 ⎳
  
 ω
 2
 j
 ℓ
 ω
 jp
 f̂
 2ℓ +
 p
  
 =
  
 ℓ =0
  
 p
 =0
  
 n
  − 
 1
 ⎲
  
 ⎳
  
  
 n
  − 
 1
 ω
 2
 j
 ℓ
 f̂
 2ℓ
 +
 ω
 j
 ⎲
  
 ⎳
  
 ω
 2
 k
 ℓ
 f̂
 2ℓ +1
 .
  
 ℓ =0
  
 ℓ =0
  
 Notice that 
 ω
 2
 =exp(2
 π",NA
√,"− 1 ⁄ 
 n
 ) and interpret the two summations above as inverse discrete Fourier transforms of 
 dimension 
 n
 . Explain how to reduce a transform of dimension 
 N
 =2
 n
  to two transforms of dimension 
  
 n
 .
  
 Exercise:
  Suppose 
 N
 =2
 n
 . The array 
 a[]
  of dimension 
 N
  may be indexed using bit sequences of length 
 n
  with the 
 index ranging from 0=00...0 to 
 N
  − 1=11...1. In the bit-reversed permutation, 
  
 a[j]
  and 
 a[k]
  are 
 interchanged if the bit sequence of length 
 n
  corresponding to 
 k
  is the reversal of the one corresponding to 
 j
 .
  
 Write a program to effect the bit-reversed permutation of an array of 
 double
 s in place.
  
 Assume that 
 a[]
  is an array of complex numbers, with each complex number represented using two 
 adjacent 
 double
 s. Write a program to effect the bit-reversed permutation of 
 a[]
  in place.
  
 Exercise:
  Let 
 f
 (
 x
 )=|sin(
 x
 )|, 
 x
 j
 =2
 πj
  ⁄ 
 N
 , and 
 f
 j
 =
 f
 (
 x
 j
 ). Graph the discrete Fourier transform of 
 f
 j
  with 
 N
 =10
 4
 . Repeat 
 with 
 f
 (
 x
 )=sin(sin(
 x
 )). What do you observe?
  
 Exercise:
  Let 
 N
 =1024 and initialize a complex array of size 
  
 N
  to 0. Apply MKL’s inverse FFT to the same
  
 array 10
 6
  times. Histogram the 
  
 10
 6
  cycle counts (you will need the 
 TimeStamp
  class described in the next chapter).
  
 If your machine has L1 data cache of at least 16 KB, you will observe something closer to the normal distribution 
 than the plots in figure 
 2.6↑
 . Why? Fit the normal law and calculate the mean and variance of the fit.
  
 Exercise:
  Tables 
 2.3↑
  and 
 2.4↑
  report the number of cycles consumed by an in-place, complex one-dimensional 
 FFT of dimension 
 N
 , the latter with data in cache and the former with data out of cache. For each value of 
 N
  in the 
 tables, find the bandwidth to memory realized in bytes/cycles as well as GB/s, under the assumption that all the extra 
 cycles for out-of-cache FFT are due to data access. Investigate the possibility that the in-cache numbers are 
 artificially low because the FFT operates on an array that is always zero.
  
 Exercise:
  Assuming versions 11 or 12 of the 
 icpc
  compiler, the member function 
 fwd()
  of the class 
 fft_fftw 
 may 
 be sped up as follows. Remove the for-loop for dividing the array 
 f[]
  by 
 n
 . Instead, call a function 
  
 scale_fwd(double *f, int n)
 . The function first calculates 
  
 double x = 1.0/n
  and then multiplies each of the 
 2
 n
 double 
 entries of 
 f[]
  by 
 x
 . Compile using 
 -fno-inline-function
 s. Recalculate table 
 2.5↑
  and show that the 
 forward transform with FFTW is now much faster.",NA
2.5References,NA,NA
Bibliography,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
3The Processor,"The 80286 processor of 1982 had a mere 134,000 transistors. The Pentium 4 of 2001 had 4.2 million transistors.
  
 Processor packages in the x86 line for sale in 2015 have several billion transistors packed into less than 
 0.25cm
 2
 .
 [45]
  These advances in processor technology are correlated with the rise of computer technology as a 
 whole.
  
 Early x86 computers, such as the 80286 or the Pentium 4, featured a single central processor. Modern 
 processor packages may have a dozen or so processors, and this number is constantly increasing. As a result of the 
 explosion in the number of transistors, the design of each processor has changed considerably. A great deal of 
 parallelism is built into modern processors, and this parallelism is active even when the thread of execution is 
 serial.
  
 In contrast to this rapid progress in hardware, the software environment of the scientific programmer is 
 nearly the same as what it was 15 years ago. In recent years, innovation in software has been driven by the Internet 
 and mobile gadgetry and largely sidesteps the great difficulties in programming modern hardware optimally. If a
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.1Overview of the x86 architecture,"This section is a basic introduction to the x86 instruction architecture, which has dominated since its introduction in 
 1978. We look at registers and a little bit of assembly programming. Our intention is to lay the foundation for the 
 next section, where we look at compilation and show how to tell whether a compiler has generated good code or
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.1.164-bit x86 architecture,"Figure 3.1Land coordinates of Intel’s Xeon 5400 processor and a photograph of the same processor. The land
  
 coordinates are used to identify pins, which are shown in the photo.
  
  
  
 Perhaps a good way to begin is by looking at a processor package. Figure 
 3.1↑
  shows what a processor
  
 package looks like from the outside.
 [49]
  There are pins to transfer addresses and data to and from memory, pins to 
 interrupt the processors, and so on. The processor package shown has only four processor cores. More recent 
 packages have many more cores, and it takes multiple pages to describe their land coordinates.
  
  
 When we write programs, we do so with an awareness of memory. Every variable name is ultimately the 
 name for a segment of memory. However, in most programming, we have no awareness of the registers at all.
  
 Registers are locations that reside on the chip that are capable of holding data or addresses. They are very special 
 locations because they are wired into the circuits for carrying out arithmetic and logical operations. When 
 arithmetic operations are executed by the processor, all the operands may be registers. To add two numbers in 
 memory, for instance, typically we have to first move one or both of them to registers, add the two registers, and 
 move the result from a register to a memory location. The addresses of the memory locations can also be held in the
  
 registers.
  
  
  
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.1.264-bit x86 assembly programming,"Each processor core fetches instructions from memory and executes them. These instructions are stored in memory 
 as a sequence of bytes. In assembly language, the bytes that encode machine instructions are replaced by 
 mnemonics. For example, an instruction to move a quad word, which is eight bytes in size, from register 
 r8
  to
  
 register 
 rax
  is coded as the following hex sequence 
 4C 89 C0
 . The first byte 
 4C
  is the so-called REX byte, the
  
 second byte 
 89
  is the opcode, and the third byte 
 C0
  encodes the fact that the source register is 
  
 r8
  and the destination
  
 is 
 rax
 .
 [51]
 In the 64-bit x86 architecture, opcodes can be one, two, or three bytes. A single instruction can be as
  
 many as 17 bytes.
  
 Unlike processors, we can’t just look at bits and make sense of them, which is why the assembly languages 
 provide mnemonics. In the GNU assembler, which is called GAS, the mnemonic for the instruction to move a quad 
 word from 
 r8
  to 
 rax
  is 
 movq %r8, %rax
 . To move a double word, which is four bytes stored in the lower halves
  
 of the 64-bit registers, the mnemonic is 
 movl %r8d, %eax
 . These mnemonics vary with the assembler. In the
  
 MASM assembler, the mnemonic for moving a quad word from 
 r8
  to 
 rax
  would be 
 MOVQ RAX R8
 ---notice that the
  
 registers are given in reverse order.
  
  
 There is no standardization in the world of x86 assembly languages. The documentation for the instruction set, 
 which include the mnemonics for various instructions, is published by AMD and Intel. However, the 
  
 mnemonics can change in a predictable fashion depending on the assembly language. For instance, a register referred 
 to as 
 R8
  in the AMD/Intel documentation becomes 
  
 %r8
  in GAS. One point is important to keep in mind. In
  
 the AMD/Intel manuals, the destination precedes the source. In GAS, the destination follows the source. We will 
 always use the GAS convention. On Linux computers, even the Intel compilers use the GNU assembler.
  
  
 The reader may find it a little puzzling that “double” (or “long”) is used for 32-bit operands and “quad” for 64-
 bit operands. A word in the original 8086 machine of 1978 was 16 bits. Thus, double words are 
  
 32 bits and quad 
 words are 64 bits, as a result of a choice made long ago.
  
 Getting started
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.1.3The Time Stamp Counter,"To do anything useful with assembly code, it helps to have a method to make it part of a C or C++ program. The 
 asm
  directive allows us to embed assembly code in a C or C++ program. The implementation of the 
 asm
  directive 
 varies with the compiler. AMD’s PGI compiler manual describes inline assembly in detail. The implementations of 
 inline assembly by Intel and GNU compilers seem to correspond to the PGI documentation.
  
 RDTSC
  or 
 rdtsc
  is an x86 machine instruction for reading the Time Stamp Counter. The behavior of the 
 Time Stamp Counter varies. Its basic function is to record the clock cycles. After the instruction is executed, the 
 number of clock cycles is saved as a 64-bit number using two 32-bit registers, namely, 
 eax
  and 
 edx
 , with the 
 higher bits in 
 edx
 .",NA
3.1.4Cache parameters and the CPUID instruction,"The registers on a processor are few in number. Large arrays and other large data structures cannot be stored in 
 their entirety using registers. They are typically stored in memory (DRAM). However, memory is outside the 
 processor chip. Although it takes only a few cycles to operate on registers, it takes a few hundred cycles to fetch a 
 word from memory. To hide the cost of the large number of cycles required to access a word from memory, some 
 memory words are 
 cached 
 on the chip itself. Cache memory is organized into several levels.
  
 We will take a closer look at cache memory in the next chapter. Here we will use the 
 cpuid
  instruction to 
 ask the processor to report information about its cache and thus give us a preliminary idea of cache memory. As we 
 look at assembly code and explore certain facets of the processor, it will help to have some idea of cache memory. 
 Caches influence the performance of nearly every program.
  
 The 
 cpuid
  instruction can be used to extract a variety of information about the processor. If you type 
 cat 
 /proc/cpuinfo
  on a Linux computer, the command returns information about the processor’s instruction set, 
 power consumption, caches, support for specific technologies, and clock speed. Much of that information is 
 obtained by the operating system using 
 cpuid
 . The 
 cpuid
  instruction has a manual of its own. The AMD and Intel 
 processors use different conventions for 
 cpuid
 .
  
 The inline assembly statement for extracting information about caches using the 
 cpuid
  instruction is given
  
 below. 
 eax
 , 
 ebx
 , and 
 ecx
  must be defined as variables of type 
 unsigned int
 , and 
 i
  must be an integer value.
  
  
  
 asm volatile(""cpuid""                //instruction 
  
 :""=a""(eax), ""=b""(ebx), ""=c""(ecx)//output list 
  
 :""a""(0x04), ""c""(i)              //input list 
  
 :""edx"");                        //clobber list 
  
  
  
 This inline assembly statement has more complicated syntax than the one we used to read the Time Stamp
  
 Counter. The instruction, which is given as 
 cpuid
 , is followed by three colons and not just one. We do not need to 
 pass any parameters to the 
 rdtsc
  instruction and all the data items created by the instruction are output to program 
 variables. In the case of the 
 cpuid
  instruction, we need to leave some data in the registers to tell the 
 cpuid 
 instruction what to do. In addition, not all the data items returned by the 
 cpuid
  instruction are output to program 
 variables. The syntax of this inline assembly statement is more complicated for these reasons.
  
  
 The three colons in this inline assembly statement divide the part inside the parentheses into four segments. 
 The first segment, which is before the first colon, gives the instruction to be executed. The second, third, and fourth 
 segments give the output list, the input list, and the so-called clobber list, respectively. In this case, the output list 
 specifies that the contents of the registers 
 %eax
 , 
 %ebx
 , and 
 %ecx
 , which are indicated by 
  
 =a
 , 
 =b
 , and 
 =c
 , 
 respectively, must be output to the program variables 
 eax
 , 
 ebx
 , and 
 ecx
 , respectively.
  
 The input list specifies that the register 
 %eax
  must be loaded with the hexadecimal number 
 04
 . This 
 hexadecimal code tells 
 cpuid
  to get information about cache. By loading other codes into 
 %eax
 , 
 cpuid
  can be 
 asked to return information about performance counters, power managment, processor name, and other features of 
 the processor. The input list loads the integer value 
 i
  into the 
 %ecx
  register. A processor typically has multiple 
 levels of cache, and an integer value is loaded into 
 %ecx
  to tell 
 cpuid
  which cache it must get information about.
  
  
 CPUID
  returns information in four registers: 
  
 %eax
 , 
 %ebx
 , 
 %ecx
 , and 
 %edx
 . Of these, only three are output to 
 program variables. We have elected not to output the 
 %edx
  register. Therefore, the clobber list, which is the segment 
 following the third and last colon, includes 
 %edx
  to tell the compiler that the instruction will write over that register. 
 That way the compiler knows it must not save information in 
 %edx 
 with plans of using it later. The registers 
 %eax
 , 
 %ebx
 , and 
 %ecx
  are also clobbered by the 
 cpuid
  instruction. However, these should not be in the clobber list because 
 the compiler can figure out that they are being clobbered from the input and output segments.
  
 A program that uses the 
 cpuid
  instruction to extract information about cache memory follows:",NA
3.2Compiler optimizations,"Anyone who seeks to write fast programs must begin by grasping what compilers do. C/C++ programs are turned 
 into machine instructions by the compiler. The speed of the program can vary by a factor of 10, or even more, 
 depending on the instruction stream the compiler generates. While generating machine instructions, compilers can 
 alter the structure of the program considerably.
  
  
 In sections 
 3.2.2↓
  through 
 3.2.5↓
 , we go over a number of compiler optimizations of loops and loop nests. 
 The two big lessons are to use the 
 restrict
  qualifier, where appropriate, and to present loops to the compiler in as
  
 simple a form as possible. What is meant by simple is a little vague and will become clear during the discussion.
  
 Compilers can be highly unpredictable. The only way to get a sense of whether the compiler has done as 
 well as expected is to look at the assembly code. Although writing assembly programs can be difficult, scanning the 
 assembly code to find inner loops and checking the instruction mix is much easier to do. Thus, our discussion of 
 compiler optimization is not so much about the optimizations themselves as it is about what to look for in the 
 assembly code. There can be no rational discussion of program speed without an idea of what the assembly code 
 looks like.
  
 A dramatic example of the unpredictability of compilers occurs in section 
 3.2.5↓
 . In that section, we look at 
 a few simple programs for multiplying matrices. The programs are nearly identical, but their speeds can be quite 
 different. That is not the dramatic part, however. The same programs are run on an SSE2 machine, an AVX 
 machine, and an AVX2 machine that is theoretically more than 4 times faster than the SSE2 machine (see table 
 9.1↓
  
 for the full names of the machines). Surprisingly, our simple matrix multiplication programs are faster on the SSE2 
 machine, which leaves us wondering whatever happened to the speedup of a factor of 4. The SSE2 instruction set 
 stabilized over a period of 10 years before it was superseded. The AVX2 instruction set is only a couple of years old 
 as of this writing. Although AVX2 programs can be 4 times faster, compiler technology has not yet caught up.
  
 In section 
 3.2.6↓
 , we look at a C++ program for multiplying matrices, in which matrices and vectors are 
 represented using classes. This program is slow because C++ constructs are used without an understanding of how 
 they map to machine instructions. C++ programs can be as fast as programs in any other language, or they can be 
 slow, depending on the skill and intention of the programmer.
  
  
 Compilation is a difficult task because compilers have to look at a program character by character, token by 
 token, and statement by statement until they have an internal representation of the whole program. The
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.2.1Preliminaries,"Cache effects in simple trials can give unrealistically good figures for program speed. Cache flush, which we 
 describe here, is one way to eliminate cache effects. We also review compiler options.
  
 The C++ classes 
 PyPlot
  for plotting, 
 StatVector
  for gathering simple statistics, and 
 Table
  for making
  
 tables are used extensively in the source code for this book. However, they appear only rarely in the text. All three 
 classes are described in the appendix.
  
 Cache flush 
  
 When programs are timed, one may inadvertently ignore the effects of caching and come up with unrealistically
  
 good numbers. Suppose we initialize three matrices to time algorithms for matrix-matrix multiplication.
  
  
  
  for(int i=0; i < dim; i++) //intialize a, b and c
  
  for(int j=0; j < dim; j++){
  
   
  a[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2;
   
  b[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2;
   
  c[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2; 
 }
  
  
  
 The C library function 
 rand()
  generates a pseudorandom integer between zero and 
 RAND_MAX
 . It is handy
  
 for timing and testing but may not be a well-tested random number generator.
 [54]
  When pseudorandom numbers 
 with specific properties, such as uniformity and independence, are needed, a good library of statistical functions 
 must be used instead. For our purposes here, 
 rand()
  is adequate, but the manner in which the matrices 
 a
 , 
 b
 , and 
 c
  
 are initialized means that they will all be in cache if 
 dim
  is small. If 
 dim=100
 , for example, the matrices will remain
  
 in the 
 L2
  cache of any of the processors of table 
  
 9.1↓
 , which is more than 0.2 MB. In a realistic program, even
  
 small matrices are unlikely to be in cache when they are needed because the program may touch a lot of data before 
 it starts operating on the small matrices. Thus, timing immediately after initialization can give unrealistically good 
 numbers.
  
 The matrices can be evicted from cache as follows:
  
  
  
  //clear a, b, c from cache 
  
  
 for(int i=0; i < dim; i++) 
  
  for(int j=0; j < dim; j++){
  
    
  _mm_clflush(a+i+j*dim);
    
  _mm_clflush(b+i+j*dim);
    
  _mm_clflush(c+i+j*dim); }
  
 The function 
 _mm_clflush()
  is an 
 intrinsic
 . It corresponds directly to the instruction 
 CLFLUSH
 . The effect
  
 of cache flush is to evict the entire cache line that corresponds to its argument. Its argument must be a pointer. All 
 transfers to and out of cache occur in blocks or lines. A cache line is typically 64 bytes. The declaration of 
  
 _mm_clflush()
  is made visible through the header file 
 ia64intrin.h
  by the 
 icpc
  compiler. After 
 CLFLUSH
 , the
  
 cache line is found in DRAM memory but not in any of the caches.
  
 The use of 
 CLFLUSH
  can introduce artifacts and give pessimistic timing figures. There is no such thing as a
  
 perfect timing protocol. A better way is to arrange inputs to the program in a long array, which is larger than the 
 size of the cache, and apply the program to the inputs in succession. For some programs, such as matrix 
 multiplication, such a precaution is not really necessary, and the use of 
 CLFLUSH
  may be much more convenient.
  
 Compiler options
  
 The other preliminary topic is the use of compiler options. We have recommended the options
  
  
  
 -xHost -O3 -prec-div -no-ftz -restrict",NA
3.2.2Loop unrolling,"A program to compute the 
 n
 th partial sum of the Leibniz series follows:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13
  
 //sum of first n terms of 4(1-1/3+1/5-1/7+1/9-...) 
 double leibniz(long int n){ 
  
  
 long int i; 
  
  
 double ans; 
  
  
 for(i=0; i < n; i++) 
  
  
  
 if(i==0) 
  
  
  
  
 ans = 4.0; 
  
  
  
 else if(i%2==1) 
  
  
  
  
 ans -=  4.0/(2.0*i+1); 
  
  
  
 else 
  
  
  
  
 ans +=  4.0/(2.0*i+1); 
  
  
 return ans; 
  
 }
  
  
 This function will be run for large values of 
 n
 , such as 
 n
 =10
 9
 or10
 10
 . With 
 n
 =10
 9
 , the partial sum is
  
  
 generated in a few seconds.
  
  
  
  
  
 Cycles
  
 Unoptimized code
  
 32
  
 With 
 -xHost -03
  optimization
  
 14
  
 With 
 -xHost -03
  optimization and loop unrolling
  
 7
  
 Table 3.2Number of cycles per term of the Leibniz series on a 3.6 GHz machine with AVX2 instructions (see 
 table 
 9.1↓
  for the full name of the machine).
  
  
  
 Normally, if a program is compiled with optimizations turned on, the resulting code can be two to three
  
 times faster. This speedup is not only because the compilers are clever, which they sometimes are, but also because 
 the unoptimized code can be long and roundabout. Table 
 3.2↑
  shows that turning on compiler optimization doubles 
 the speed of the code. Rewriting 
 leibniz()
  to enable loop unrolling, which is one of the most important 
 optimizations, doubles the speed once again. We will use the 
 leibniz()
  function and its variants to explain how 
 loop unrolling works.
  
 First, we will understand why unoptimized code is nearly always quite slow. The first few lines of the
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.2.3Loop fusion,"Because the speed of modern x86 computers relies on instruction-level parallelism and vector registers, it is often a 
 good idea to have a lot of parallelism in the innermost loops. If sets of instructions in the body of the innermost loop 
 are independent of each other, the processor is likely to execute them in parallel.
  
 Loop fusion addresses the situation where we have two distinct loops. It is assumed that the iterations of 
 each loop have dependencies that make loop unrolling ineffective. In such a scenario, merging the bodies of the 
 two loops may be the best way to produce a loop body that is amenable to instruction-level parallelism. The 
 transformation where distinct loop bodies are merged is called loop fusion.
  
 For a simple example, we consider the power series for sine and cosine.
 [58] 
  
  
 sin
 x
 =
 x 
 1! − 
 x
 3 
 3!+
 x
 5 
 5! − 
 ⋯
  
  
 cos
 x
 =1 − 
 x
 2 
 2!+
 x
 4 
 4! − 
 ⋯",NA
3.2.4Unroll and jam,"In this section, we will study a compiler optimization that applies to nested loops. So far we have considered loop 
 unrolling and loop fusion. In loop nests, it may be desirable to unroll the outer loop and fuse several copies of the 
 inner loop that result. That is the unroll and jam transformation.
  
 The following C++ program computes a sine table:",NA
3.2.5Loop interchange,"Loop interchange is the next compiler optimization for discussion, and matrix multiplication is the example we use
  
 to bring it out. In our discussion of matrix multiplication and loop interchanging, the following points will emerge:
  
 One should use 
 restrict
  pointers as far as possible. The use of 
  
 restrict
 -qualified pointers can speed up
  
 a program by more than a factor of 2.
  
 It is best to present loops to compilers in a simple and transparent form.
  
  
 Although AVX2-capable machines are faster than SSE2-capable machines, generating AVX2 code is a
  
 bigger struggle for a compiler. Therefore, much of the advertised advantage of an AVX2 machine over an 
 SSE2 machine may not be realized.
  
 Compilers are capricious, and no assumptions about the generated assembly code can be made without
  
 inspecting it.
  
 Optimizing for the instruction pipeline can yield far greater speedups on more recent processors than on
  
 earlier ones.
  
 Some of these points have come up already. Many of these points are much broader in scope than any particular 
 compiler optimization.
  
 Before we begin, we make some remarks about the peak capabilities of SSE2- and AVX2-capable machines. 
 As shown in table 
 3.1↑
 , SSE2 provides XMM registers wide enough to hold two 
 double
 s. The 
 mulpd
  (or 
 vmulpd
 ) 
 instruction applied to XMM registers carries out two multiplications in a single instruction. Similarly, the 
 addpd
  (or 
 vaddpd
 ) instruction carries out two additions in a single cycle. Typical SSE2 processors can simultaneously issue an 
 addpd
  and a 
 mulpd
  to separate execution units in the same cycle. Therefore, the peak capability of a single SSE2 
 processor core is 4 flops (floating points operations) per cycle.
  
 The AVX2 processor has YMM registers, which are twice as wide as XMM. In addition, it has the",NA
3.2.6C++ overhead,"multijk()
  
 dim
 =1000 
 (SSE2)
  
 dim
 =2000 
 (SSE2)
  
 dim
 =1000 
 (AVX2)
  
 dim
 =2000 
 (AVX2)
  
 Using 
 Matrix
  objects
  
 .10
  
 .07
  
 0.18
  
 0.15
  
 Table 3.4Floating point operations per cycle. Compare with table 
 3.3↑
 .
  
  
  
 Table 
 3.4↑
  shows performance data for two implementations of matrix multiplication in C++. The
  
 implementations of section 
 3.2.5↑
  are also in C++, but the syntax and style properly belong to C. They are C++ 
 functions mainly because C++ is an extension of C. The functions were coded using arrays and pointer arithmetic. 
 In contrast, the C++ function timed in table 
 3.4↑
  uses 
 Matrix
  class objects (see chapter 
 1↑
  for the definition of the 
 Vector
  class, the 
 Matrix
  class is similar).
 [61]
  
 If matrices are multiplied using objects of the class 
 Matrix
 , the C++ syntax mimics mathematical syntax
  
 quite closely as shown below.
  
  
  
 void multijk(Matrix& A, Matrix& B, Matrix& C){ 
 int l = A.getm(); 
  
 int m = A.getn(); 
  
 assrt(B.getm()==m); 
  
 int n = B.getn(); 
  
 assrt(C.getm()==l); 
  
 assrt(C.getn()==n);
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.2.7A little compiler theory,"The compiler’s view of a program is quite different from that of a human programmer. A human programmer has a 
 problem to be solved and an idea to solve that problem that is expressed as a computer program. What the compiler 
 sees is a sequence of statements that obey the syntactic rules of the programming language. The global view of what 
 the program does is lost.
  
  
 To generate good assembly, the compiler has to grasp which variables are being used heavily and other 
 global aspects of the program. Uncovering global information from a program, which is presented to the compiler 
 as a sequence of statements, is quite hard. In this section, we will consider some of the ideas on which optimizing 
 compilers rely.
 [62] 
  
  
 In a sequential program, the statements depend on one another. We assume that 
 x
 , 
 y
 , and 
 z
  are program
  
 variables. The following is an example:
  
  
  
 y=x; 
 ...
  
 z=x;
  
  
  
 Here the second statement has an input dependence on the first because both of them read the same variable
  
 x
 . The input dependence is also called Read After Read (RAR).
 [63]
  RAR is the mildest form of dependence and",NA
3.3Optimizing for the instruction pipeline,"Earlier, we found that MKL’s fast Fourier transform and matrix multiplication can be 10 times faster than ordinary 
 C/C++ programs. If the C++ program is written without care, the speedup can even be a factor of 100. What does 
 Intel’s MKL do to be so much faster than ordinary C/C++ programs?
  
 The biggest part of the answer to that question is optimizing for the instruction pipeline, which is the topic 
 of the present section. The compiler converts a C/C++ program into a stream of machine instructions. When the 
 program runs, this instruction stream is consumed by the processor. From a programmer’s point of view, the 
 instructions are executed one by one, but that is not the way processors consume instructions. The x86 processors 
 consumed instructions in that manner before 1990. If processors still worked that way, they would be slower by at 
 least a factor of 10.",NA
3.3.1Instruction pipelines,"General remarks 
  
 The automobile assembly line offers a useful point of comparison to clarify concepts. An automobile assembly line 
 may take 48 hours to assemble a single car. However, thanks to pipelining and multiple assembly lines, the factory 
 may produce a car every minute. For such a factory, the latency would be 48 hours and the bandwidth 1 car per 
 minute.
  
 The assembly of the car is broken down into a number of steps that are executed sequentially in the 
 assembly line. Crucially, these steps or pipeline stages are independent of each other. Therefore, car B can be 
 pushed onto the first stage as soon as car A completes the first stage and moves to the second. When car A moves 
 to the third stage and car B to the second, car C is pushed to the first stage. Ideally, the various stages of the 
 assembly line should take nearly the same time. The bandwidth of the assembly line is constrained by the slowest 
 stage. If the number of stages in the assembly line is increased, the bandwidth increases (assuming that the stages 
 take the same amount of time), even though the latency is unchanged.
  
 The analogy to car manufacturing omits many complications that arise in processor pipelines. Instructions 
 are not as independent as cars. If there is a RAW dependence between two instructions, the second instruction 
 cannot begin to execute until the first completes. Even ignoring dependencies, it takes a lot of design to make the 
 stages of the instruction pipeline relatively independent. For example, the instruction cache should be separate from 
 data cache if the instruction fetch stage is to be kept relatively independent of the stage where operands are read 
 from memory. Another source of complication comes from interrupts or exceptions raised by an external device or 
 the operating system. If the interrupt is of high enough priority, the entire pipeline must be abandoned to service the 
 interrupt and restored after the interrupt is serviced.
  
  
 Although RAW dependencies cannot be eliminated, modern processors eliminate WAW and WAR 
 dependencies on the fly. Those dependencies are eliminated using register renaming.
 [65]
  Suppose we have two
  
 instructions as follows:
  
  
  
 movq %r8 %rax 
 ...
  
 movq %r9 %rax
  
  
  
 Evidently, the second instruction has WAW dependence on the first because both instructions write into the
  
 %rax
  register. It would be illegal for the processor to execute the second instruction before the first because there
  
 may be some other instruction in the middle that reads from 
 %rax
  and therefore has RAW dependence on the first
  
 instruction. But suppose the processor dynamically renames the second 
 %rax
  to some other internal register.
  
 Suppose as well that the processor renames 
 %rax
  to the same internal register in all later instructions that read what
  
 the second instruction writes into 
 %rax
 . If the processor does such renaming of registers, it can go ahead and
  
 execute the second instruction before the first.
  
 Using register renaming, the processor can eliminate WAR and WAW dependencies and greatly increase 
 available parallelism in the instruction stream. Instructions can be scheduled out of order and even executed in 
 parallel.
  
  
 Because of the sophistication of the algorithms used by processors to execute instructions, it is a 
  
 misconception to think that processor performance is somehow proportional to clock speed. The 3.2 GHz Pentium 4 
 from 2004 used between 1.19 and 5.85 cycles per instruction for 
  
 10 programs in the SPEC CPU benchmark.
 [66] 
 The 2.66 GHz AMD Opteron of that time used fewer cycles per instruction by a factor of 1.27. As a result, the 2.66 
 GHz Opteron performed slightly better than the 3.2 GHz Pentium. The use of deep pipelines by the 3.2 GHz Pentium 
 to accommodate a higher clock rate resulted in more pipeline stalls and more cycles per instruction, thus",NA
3.3.2Chipsets,"So far we have mainly been looking at the processor core. We are about to go even more deeply into the processor. 
 So let us pause for a moment and look at the rest of the computer.
  
 The processors are central to the computer but are only one among the many components that make up a 
 computer or compute node. Some of the other components are DRAM memory, graphics processor, hard disk, solid 
 state storage, network adapter, keyboard, and monitor. To understand how the processor is connected to all the
  
 components, we have to look at chipsets. Chipsets are chips used to assemble computers from many components.",NA
3.3.3Peak floating point performance,"The SSE2 instruction set architecture (see table 
 3.1↑
 ) provides for 16 XMM registers on each processor. Each 
 XMM register is 128 bits wide and capable of holding two 
 double
 s. A single instruction that uses an XMM
  
 register as source and another XMM register as destination can carry out two additions, if the instruction is 
 addpd
 ,
  
 or two subtractions, if it is 
 subpd,
  or two multiplications, if it is 
 mulpd
 . The addition and multiplication
  
 instructions are dispatched using separate ports, which means that an addition and a multiplication instruction can 
 be issued as well as completed in the same cycle. Thus, the peak rate at which double-precision floating point 
 operations (flops) are executed by a single SSE2 processor is 4 flops per cycle.
  
 Table 
 3.3 on page 1↑
  showed us that the matrix multiplication routines of the MKL library achieve more 
 than 3.8 flops per cycle. However, to write such a program is no simple matter. It requires intimate knowledge of 
 the processor pipeline, which is shown in figure 
 3.4↑
 .
  
 Our objective is to understand matrix multiplication routines of the sort implemented by MKL. We will not 
 aim to match MKL’s performance. Such a thing would require us to write more assembly code than is 
  
 pedagogically desirable or appropriate. Knowledge of the processor pipeline is one aspect of optimizing matrix 
 multiplication. Memory hierarchy is equally important. Memory is the topic of the next chapter.
  
  
 In this section, we write a few programs that do nothing meaningful but that get close to the peak 
  
 performance of 4 flops per cycle, with the 
  
 4 flops comprised of two additions and two multiplications of double-
 precision floating point numbers. Although the programs are not required to be meaningful, getting close to peak 
 performance is not easy. This exercise requires us to understand many aspects of how instructions are decoded and 
 then dispatched to execution units using several ports. Instruction latencies and throughputs, register read stalls, and 
 register renaming are other aspects of instruction-level parallelism we encounter during the exercise. Where 
 appropriate, we shall look back to figure 
 3.4↑
  to make the discussion concrete, even though the SSE2/Nehalem 
 pipeline is too complicated for a schematic sketch of the type given in that figure to be either complete or totally 
 accurate.
  
 This section is a prelude to the discussion of matrix multiplication, which begins in the next section. We 
 understand matrix multiplication on modern processors in two steps. In the next section, we take the first step by 
 writing programs or microkernels
 [67]
  to multiply 4×
 n
  matrices with an 
 n
 ×4 matrices for values of 
 n
  such as 
 n
 =1,4,200. The microkernels with 
 n
 =50,100,200 approach peak performance if the matrices are assumed to be in 
 cache. The final stage, which is to use a microkernel as the building block of a program for multiplying large 
 matrices in DRAM memory, is one of the examples discussed in the next chapter.
  
 Understanding matrix multiplication on modern processors will take us more deeply into computer hardware 
 than is customary in textbooks on scientific computing or indeed even computer architecture. The programs are run 
 on a 2.6 GHz SSE2 processor and a 3.6 GHz AVX2 processor (see table 
 9.1↓
  for their full names; neither processor 
 has in-core acceleration of the clock). Although the programs are not optimized for the AVX2 pipeline, they are still 
 more than three times faster than compiled code.
  
 Optimization for the more recent AVX2 instruction set is dealt with in the exercises. The AVX-512
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
3.3.4Microkernel for matrix multiplication,"Let 
 A
  be an 
 l
 ×
 m
  matrix and 
 B
  an 
 m
 ×
 n
  matrix. The operation 
 C
 :=
 C
 +
 AB
 , where 
 C
  is a 
 l
 ×
 n
  matrix, requires 2
 lmn
  
 flops, half of which are additions and half of which are multiplications.
 [71]
  A single 
 addpd
  instruction
  
 performs two additions and a single 
 mulpd
  performs two multiplications. If 
  
 addpd
  and 
 mulpd
  instructions are
  
 issued nearly every cycle, the matrix multiplication can be completed in slightly more than 
 lmn
  ⁄ 2 cycles.
  
  
 We will abuse terminology slightly and refer to the operation 
 C
 :=
 C
 +
 AB
  as matrix multiplication. In the 
 special case where 
 C
  is initialized to zero, this operation coincides with matrix multiplication.
  
 In this section, we write programs that multiply matrices of dimensions 4×
 n
  and 
 n
 ×4 using slightly more 
 than 8
 n
  cycles. The microkernel with 
 n
 =200 is the basis of matrix multiplication routines given in the next chapter. 
 For 
 n
 =200, the desired cycle count is 1,600. Our gets to 1,840. With better optimization, microkernels that get 
 closer to the ideal cycle count can be written.
  
 The one we present has a particularly simple design and uses only a few instructions. Yet it brings out many 
 of the essential features of this type of programming. One of these is the tension between two constraints that must 
 be simultaneously satisfied by such s. On the one hand, to utilize multiple dispatch ports and execute instructions in 
 parallel, it is favorable to interleave segments of the instruction stream that are independent of each other. On the 
 other hand, too much independence would mean that instructions dispatched during the same cycle are more likely 
 to have unrelated operands. Register read stalls would result from too much independence in the instruction stream.
  
 We need the instructions to be independent so that they can be dispatched in parallel, and, at the same time, we 
 want each instruction to be reading an operand that was written to recently by another instruction or we want two 
 instructions scheduled during the same cycle to have common operands.
  
 Working with such constraints on the instruction stream will require us to get into many aspects of the 
 microarchitecture. We limit ourselves to a relatively simple microkernel, partly to keep the exposition tractable and 
 partly because many details of the microarchitecture are unknown to us. Figuring out the microarchitecture calls for 
 laborious and time-consuming experimentation. For the most part, we stick to those aspects of the 
  
 microarchitecture that have already been uncovered in the previous section. We have seen the 
 movaps
  instruction,
  
 used for storing, loading, and moving one register to another, as well as 
 addpd
  and 
 mulpd
 . Our microkernel uses
  
 these three instructions and 
 shufpd
  but no others. The instruction
  
  
  
 shufpd $1, %xmm0, %xmm0
  
  
  
 flips the upper and lower halves of 
 %xmm0
 , or of whichever XMM register that appears in place of 
 %xmm0
 ,",NA
(,"b
 0
  
 ∗
  
  
  
  
  
 b
 1
  
 ∗",NA
 ),"c
 1
  
 ↘
  c
 3
  
 ↘
  
 c
 4
  ↗
  c
 6
  
 ↗
  
  
 a
 1
 ∗
  
  
 c
 5
  
 ↘
  c
 7
  
 ↘
  
  
 Eight XMM registers are used to store 
 C
 , which is 4×4. The 
 c
 0
  register holds 
 c
 0
  and the entry that is 
  
 immediately southeast, the 
 c
 1
  register holds 
 c
 1
  and the entry that is immediately northeast, and so on. Loading the 
 C 
 matrix into registers and then storing the registers in the matrix becomes particularly convenient if the matrix is
  
  
 stored in “skew” order in memory. If the 2×2 matrix
  
 ⎛
  
 ⎜
  
 ⎝
  
 ab
  
 ⎞
  
 ⎟
  
 ⎠
  
 c d
  
  
 is skewed, it becomes
  
 m
 ×
 n
  blocks of size 2×2 is skewed.
  
 ⎛
  
 ⎜
  
 ⎝
  
 a c
  
 ⎞
 ⎟
 ⎠
  
 .
  
 db
  
 A matrix of dimension 2
 m
 ×2
 n
  is said to be skewed if each of the 
  
 Notice that if a skewed matrix is skewed twice, we get back the original matrix.
  
  
 As far as matrix multiplication is concerned, assuming 
 C
  to be stored in skew order is a minor point in terms 
 of performance but simplifies the exposition. We shall assume 
 C
  to be stored in skew order.
  
  
 The diagram indicates that the register 
 a
 0
  holds 
 a
 0
  and the entry immediately below it. The contents of the 
 registers 
 a
 1
 , 
 b
 0
 , and 
 b
 1
  follow from the diagram in the same way.
  
  
 We assume 
 C
  to be stored in a contiguous array of size 
  
 16 in column-major order but after skewing. We 
 assume 
 A
  and 
 B
  to be stored in contiguous arrays of size 4.
  
  
 If the XMM register holding 
 b
 0
  is multiplied into 
 a
 0
  using 
 mulpd
  and the result is added to 
 c
 0
  using 
 addpd, 
 we 
 have completed updating 
 c
 0
 . If 
 a
 0
  is flipped, using 
 shufpd
  to interchange its lower and upper half, and then multiplied 
 by 
 b
 0
  and added to 
 c
 1
 , we have updated 
  
 c
 1
 . The 
 c
  registers with an even subscript do not require flipping. 
 The ones with an odd subscript require flipping.
  
 Using the notation introduced so far, we give the method for implementing 
 C
 :=
 C
 +
 AB
  in a kind of 
 pseudocode. The assembly code given later corresponds closely to this pseudocode. There is some vagueness in the 
 way the 
 a
 i
  are assigned to registers, which is cleared up by the assembly code (see figure 
 3.7↓
 ).
  
 1. Load 
 c
 0
 ,…,
 c
 7
  from memory.
  
 2. Load 
 a
 0
  from memory.
  
 3. Load 
 b
 0
  from memory.
  
 4. Use 
 mulpd
  to replace 
 a
 0
  by 
 b
 0
 ∗
 a
 0
  (entrywise product of two XMM registers).
  
 5. Load 
 a
  .
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
(,"b
 n
 1
 b
 n
 2
 b
 n
 3
 b
 n
 4",NA
),"⎛
  
 ⎜
  
 ⎜
  
 ⎜
  
 ⎜
  
 ⎜
  
 ⎝
  
 a
 11
  
 ⎞
 ⎟
 ⎟
 ⎟
 ⎟
 ⎟
 ⎠",NA
(,"b
 11
 b
 12
 b
 13
 b
 14",NA
),"+
 ⋯
 +
  
 ⎛
 ⎜
 ⎜
 ⎜
 ⎜
 ⎜
 ⎝
  
 a
 1
 n
  
 a
 21
  
 a
 2
 n
  
 a
 31
  
 a
 3
 n
  
 4
 n
  double-precision numbers. The matrix 
 A
  
 a
 41
  
 a
 4
 n
  
 The matrices 
 A
  and 
 B
  are assumed to be stored in arrays of size 
  
 is assumed to be stored column after column. In contrast, the matrix 
 B
  is assumed to be stored row after row, making 
 it easier to access the columns and rows for each outer product. To form the 
 k
 th outer product, where 0 ≤ 
 k
 <
 n
 , we 
 may add 4
 k
  to the pointers 
 A
  and 
 B
  to advance to the 
 k
 th column of 
 A
  and the 
 k
 th row of 
 B
 . 
  
 To multiply 
 matrices of dimensions 4×
 n
  and 
 n
 ×4, we modify 
 asm4x1x4()
 . The middle block is replicated
  
 n
  times. In the second block, the memory references
  
  
  
 (%rdi), (%rsi), 16(%rdi), 16(%rsi), (%rdi), (%rdi), 16(%rdi) 
  
  
  
 are replaced by
  
  
 32(%rdi), 32(%rsi), 48(%rdi), 48(%rsi), 32(%rdi), 32(%rdi), 48(%rdi) 
  
  
 Adding 32 to the displacement corresponds to moving forward by 
  
 4 doubles because each double is 8 bytes.
  
 The third, fourth, and fifth replications are treated similarly by adding 64, 96, and 128 to the displacement fields.
  
 After the fifth replication, we add 160 to 
 %rdi
  and 
 %rsi
  using the instructions
  
  
  
 addq 
  
 $160, %rdi
  
 addq 
  
 $160, %rsi
  
  
  
 and repeat the first five replications. Note that 160 bytes equals 20 doubles. This design tries to balance
  
  
 competing requirements for shorter instructions and fewer instructions.
  
  
  
  
 n
  
 Flops per cycle",NA
3.4References,NA,NA
Bibliography,"[28] G.I. Toomre : 
 Ptolemy's Almagest
  
 . Princeton University Press
  
 , 1998 .
  
 [29] J.L. Hennessy, D.A. Patterson
  
 : 
 Computer Architecture: A Quantitative Approach
 . Morgan Kaufmann , 1990-
  
  
 2011.
  
 [30] K.V. Sarma : 
 A History of the Kerala School of Hindu Astronomy
  
 . Vishveshvaranand Institute
  
 , 1992 .
  
 [31] M. Kerrisk: 
 The Linux Programming Interface
  
 . No Starch Press
  
 , 2010 .
  
 [32] R. Allen, K. Kennedy : 
 Optimizing Compilers for Modern Architectures
  
 . Morgan Kaufmann, 2002 .
  
 [33] S. Goto, R. A. van de Geijn
  
 : “Anatomy of high performance matrix multiplication”, 
 ACM TOMS
  , pp. art:12 ,
  
 2008 .
  
 [34] T.H. Cormen, C.E. Lieserson, R.L. Rivest 
  
 : 
 Introduction to Algorithms
  . MIT Press, 2001 .",NA
4Memory,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
4.1DRAM and cache memory,"DRAM was invented by Intel in 1973. In DRAM, a single capacitor is used to store a single bit, and each capacitor 
 is equipped with an access transistor. DRAM technology has evolved over the years to the point where it is the 
 primary form of memory in almost all computing and mobile devices. For a schematic illustration of where DRAM 
 memory fits into the computer as a whole, see figure 
 3.6↑
 , where it is labeled as system memory.
  
 Section 
 4.1.1↓
  is an overview of DRAM technology. At the finest level, DRAM is an array of bits. Arrays of 
 bits are organized into banks, ranks, and channels. There is a memory controller on each processor package that 
 drives the memory channels. Most of the details of the memory controller are entirely hidden from the programmer 
 (as well as the operating system). Fortunately, one does not need knowledge of the hardware at the level of the 
 memory controllers or channels to write optimized programs. Thus, the principal purpose of the information in 
 section 
 4.1.1↓
  is to provide context. The information is not directly useful in writing actual programs. However, its
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.1.1DRAM memory,"By opening the cover of a computer and peering inside, we can look at memory plugged into the mother board. The 
 devices that are plugged in are called Dual Inline Memory Modules (DIMMs). DIMMs can be purchased to add
  
 more memory to the computer. Each DIMM is a package of several little chips. The little chips are DRAM devices.
  
  
 14 
  
 11",NA
4.1.2Cache memory,"An instruction such as 
 movq %rax, %rdx
 , which moves one register to another, takes a single cycle. However, a
  
 load instruction such as 
 movq (%rsi), %rdx
 , which moves a quad word from memory to a register, can take more
  
 than 100 cycles. An instruction that writes can take even longer. Computer processors cache frequently used parts 
 of DRAM memory so that they can execute instructions at the speed of the registers, although nearly half the
  
 instructions are loads and stores from the much slower DRAM devices.
 [75]
  
  
  
  
 Figure 4.5Layout of a single core of AMD Opteron (code named Barcelona) (layout based on Patterson and
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.1.3Physical memory and virtual memory,"Virtual memory is implemented by the operating system and the processor hardware working in concert. Its main 
 purpose is to prevent processes from interfering with each other’s memory. A single process will have only part of 
 the DRAM memory for itself. The part of the DRAM memory that is available to a specific process is determined 
 only when the process is loaded and can change as it runs.
  
  
 Virtual memory is an illusion that simplifies programming, compilation, and linking in addition to keeping 
 programs from interfering with each other. As the program runs, the memory addresses generated by a program are 
 automatically mapped to physical addresses in the DRAM memory. So, for example, if a program issues an 
  
  
 %rdi
  is a virtual address. During instruction execution, the instruction such as 
 movq %rax, (%rdi)
 , the 
 address in 
  
 page tables are consulted by the hardware to map that address to a physical address. A separate map is maintained 
 for each process to keep the processes from interfering with each other. The map is typically hierarchical and stored 
 in multiple page tables. Page tables are the essence of virtual memory.
  
 Virtual memory in action 
  
 Let us consider an instruction that generates the memory reference 
 (%rax, %rsi, 2)
 . The reference is to the
  
 memory location whose address is 
 rax+2*rsi
 . The registers 
 %rax
  and 
 %rsi
  are 64-bit. Therefore, a 64-bit virtual
  
 address is formed. Strictly speaking, the virtual address is the last 48 bits of the address. If we print a pointer in a C 
 program, the program prints 12 hex digits because a virtual address is 48 bits. A 48-bit virtual address should get us 
 past 2020 and can then be extended without changes to the instruction set architecture.
  
 How does the hardware look up an actual word in memory using a virtual address? The answer is 
 complicated. The first step on x86 computers is to form a 64-bit linear address by adding a segment register. This 
 step is trivial and we will ignore it.
  
  
 The next and far more important step is to map virtual addresses to physical addresses. Once a physical 
 address is formed, it may be used to look up DRAM or the caches.
  
  
 To map addresses from virtual to physical memory, virtual memory is partitioned into pages. A page is 
 typically 4,096 bytes (the command 
 getconf PAGESIZE
  may be used to find out the page size). Thus, in a virtual
  
 address of 48 bits, the first 36 bits constitute a page address, and the following 
  
 12 bits are the address within that 
 page. Correspondingly, DRAM memory is broken up into page frames, each of which is of the same size as a page. 
 Page tables map page addresses to page frame addresses.
  
 The manner in which page tables are set up does not concern us here. They are set up by the operating 
 system kernel and left in a place where the processor hardware can look them up.The translation look-aside buffer 
 (TLB) is a cache of the page tables. When an address such as 
 (%rax, %rsi, 2)
  is formed, the next step is to look
  
 up the TLB to convert it to a physical address. Each entry in the TLB maps exactly one page to a page frame. If
  
 there is a TLB miss, the processor looks up the page tables.
  
  
  
 Number of Sets
  
 Associativity
  
 Size
  
 Instruction TLB
  
 32/16/32
  
 4/8/4 way
  
 128 entries
  
 Data TLB
  
 16
  
 4 way
  
 64 entries",NA
4.1.4Latency to DRAM memory: First attempts,"In this section, we make our first attempts at measuring the access time to DRAM memory. All our attempts fail, 
 but we are led into certain aspects of the memory system that have a bearing on program performance.
  
 The organization of memory is such that if we attempt to investigate one part of it, we need to be aware of 
 the other parts as well. All the parts of the memory system are interrelated. Thus, to measure the access time to 
 DRAM memory, we need to know about the size of the cache. We begin by giving a basic picture of the memory 
 system as a whole. The measurement of latency brings to light some other parts of the memory system, and as we
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.1.5Latency to DRAM,"Our earlier attempts to measure latency to DRAM memory failed because we did not account for the overhead of 
 creating and accessing page table entries. The more careful program in this section breaks instruction-level 
 parallelism, ensures that none of the cache lines accessed is from L1, L2, or L3 cache, and accesses all of the 256 
 cache lines within four pages of memory (so that TLB misses are not a factor).
  
 To begin with, we look at the function 
 randomp()
 , which initializes the 
 n
  entries of the array 
 List[]
  to be a
  
 random permutation of the numbers 0,1,…,
 n
  − 1.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4
  
 void randomp(int *List, int n){ 
  
 for(int i=0; i < n; i++) 
  
  
  
 List[i] = i; 
  
  
 for(int i=0; i < n; i++)
  
 {
  
 5
  
 int j = rand()%
  
 (n-i)+i;
  
 6
  
 int temp =
  
 List[j];
  
 7
  
 List[j] =
  
 List[i];
  
 8
  
 }
  
 }
  
 List[i] = temp;
  
 9
  
 10
  
  
  
  
 On lines 2 and 3, the array 
 List[]
  is initialized to be the identity permutation 0,1,…,
 n
  − 1. The loop from
  
 lines 4 through 9 picks 
 j
  to be a random number from the set 
 i
 ,…,
 n
  − 1 and swaps 
  
 List[i]
  and 
 List[j]
  for each
  
 value of 
 i
  from 0 to 
 n
  − 1. The random number generator 
  
 rand()
  used on line 5 is convenient, being part of the
  
 standard C libraries, and sufficient for our purposes here. However, faster and more rigorously tested random 
 number generators are available.
  
 The program for measuring latency uses two methods for clearing the array used for measurement from
  
 cache memory. If the preprocessor variable 
 MEMWALK
  is undefined at the top of the file using
  
  
  
 #undef MEMWALK
  
  
  
 then it uses the 
 CLFLUSH
  instruction issued using an intrinsic. However, if 
 MEMWALK
  is defined at the top of
  
  
 the file, it uses the following function:
  
  
  
  
 1 
  
 2
  
 void dummy(double *a, int len){ 
  
 for(int i=0; i < len;
  
 i++)
  
 3
  
 a[i] = (i%77)*
  
 (i*1001);
  
 4
  
 }
  
  
  
  
 A large array 
 a[]
  is passed to this function, which writes something into every entry on line 3. The array
  
 a[]
  is not used for measuring latency. Its sole purpose is to occupy the cache completely and evict the array used 
 for measuring latency from the cache.
 [76]
  The function 
 dummy()
  must be defined in a separate compilation unit to
  
 prevent the compiler from eliminating the function call.
  
 The function 
 latency()
 , whose listing follows, touches all the cache lines in four pages of memory and
  
 prints an estimate of the latency to DRAM memory.",NA
4.2Optimizing memory access,"In this section, we look at optimization of memory access using three examples. The first example, in section 
 4.2.1↓
 , is to simply access a long array of numbers to sum or to copy. With this simple example, we learn what may 
 be the most important lesson related to memory access, which is to utilize each cache line as fully as possible.
  
 The examples in sections 
 4.2.2↓
  and 
 4.2.3↓
  are more involved. Although cache lines are the units of transfer 
 of data between DRAM and the caches, cache organization involves multiple levels and sets. In every memory 
 access, a virtual address must be translated to a physical address. This translation using the TLB and possibly the 
 page tables can be a source of considerable overhead.
  
 The chief technique in optimizing for multiple cache levels and the TLB is the same, namely, blocking. In 
 section 
 4.2.2↓
 , we study blocking using matrix transposition as an example. Section 
 4.2.3↓
  also illustrates blocking 
 in addition to the technique of streaming data from cache to reduce cache and TLB misses.
  
 Fortunately, much of the time we do not need to program in assembly when optimizing memory access.
  
 This is partly because the memory system is so complicated that overly refined optimizations do not make sense. 
 Another reason is that the memory system is more amenable to optimization than the instruction pipeline. For the 
 vast majority of nontrivial programs, speed is limited by memory access. Thus, the memory system’s greater 
 amenability to optimization is probably by design. Although it would be incorrect to assume that compilers 
 generate optimal instruction streams, the penalty for suboptimality is not as great.
  
 The design of the memory system consisting of DRAM, memory controllers, and caches is relatively stable 
 across platforms. Therefore, the techniques of optimization may be expected to be the same on graphics devices, 
 non-x86 platforms, and mobile devices.
  
 Our discussion of the memory system in the previous section began with aspects of hardware design and 
 virtual memory and concluded with a measurement of latency. In contrast, through much of this section, the 
 emphasis is on bandwidth to memory. In all the examples discussed in this section, the large latency to memory can 
 be hidden with little effort.
  
 Examples in which latency to memory can be hidden completely are characterized by parallelism in the
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.2.1Bandwidth to DRAM,"The most predictable and common pattern of memory access is to access a long line of data in sequence. Every 
 memory controller is likely to be optimized to handle that pattern efficiently. Thus, to determine bandwidth to 
 memory, we will access a long array in sequence.
  
 The following simple function returns the sum of an array of 
 double
 s:
  
  
  
 double sum(double *restrict a, long int n){ 
  
 double s = 0; 
  
  
 for(long int i=0; i < n; i++) 
  
  
  
 s += a[i]; 
  
  
 return s; 
  
 }
  
  
  
 The loop body uses XMM/YMM registers, as we may verify by inspecting the assembly code. The single
  
 statement in the loop has a loop-carried RAW dependency. Nevertheless, the simple structure of the loop helps the 
 compiler unroll the loop and introduce parallelism in the loop body. The entries of the array 
 a[]
  are read from
  
 memory in parallel. The additions do not introduce an overhead above the time it takes to read the array from 
 memory.
  
 The program was called with an array 
 a[]
  that was 8 GB. The array was initialized in the same sequence it
  
 is summed. It is important to initialize the array before the function 
 sum()
  is called. Pages of virtual memory are
  
 mapped to page frames of physical memory only at first access. If the array is not initialized at all, the mapping of 
 virtual memory to physical memory takes place when 
 sum()
  executes.
  
  
 The cache is too small to hold 8 GB data. So none of the load instructions will hit the cache. Every byte 
 accessed by 
 sum()
  must be loaded from memory.
  
 Because each 
 double
  is 8 bytes, if the function 
 sum()
  takes 
 c
  cycles to execute, we may take the bandwidth
  
 to memory to be 8
 n
  ⁄ 
 c
  bytes per cycle. The bandwidth to memory on a 2.66 GHz SSE2 machine (see table 
 9.1↓
  for 
 the full names of the machines) was measured to be 4.36 bytes per cycle or, equivalently, 11.6 GB/s. On a more 
 recent 2.20 GHz AVX machine, the bandwidth was 5.40 bytes per cycle or 
  
 11.9 GB/s. On a yet more recent 3.6 
 GHz AVX2 machine, the bandwidth was 4.85 bytes per cycle or 
  
 17.46 GB/s.
  
  
 We use strided memory accesses to lead up to what is perhaps the single most important item a programmer 
 should know about accessing memory. The following function sums entries of the array 
 a[]
 , beginning with the
  
 zeroth entry and in steps of length 
 stride
 :
  
  
  
 double sumstride(double *restrict a, long int n, 
  
  
 int stride){ 
  
  
 double s = 0; 
  
  
 for(long int i=0; i < n; i+=stride) 
  
  
  
 s += a[i]; 
  
  
 return s;",NA
4.2.2Matrix transpose,"The only cache parameter of significance for strided accesses studied in the last section was cache line size. 
 The sizes of the caches, their organization into sets, as well as the size of the TLB influence the performance of the 
 example studied in this section.
  
 The example we study here is out-of-place matrix transpose. It is always best to access data sequentially 
 with unit stride, but when a matrix is transposed to another matrix stored in the same column-major format, there is 
 no way to access both matrices with unit stride. In many problems of this type, it is useful to break up the data into 
 blocks.
  
 When a single array is accessed with a constant stride, a cache line brought into cache is used in a single go. 
 Once the array moves past a cache line, we do not return to it. In matrix transpose with blocking, we rely on a cache 
 line remaining in cache as we return to it repeatedly after working on other columns of the matrix block. This type 
 of cache usage is more delicate. We find that the performance of the matrix transpose depends in a nonmonotonic 
 manner on the size of the blocks. The performance degrades abruptly if the leading dimension of the matrix is 
 divisible by a high power of 2. Such effects, though disconcerting to the programmer, cannot be eliminated.
  
  
 The cache and TLB parameters of SSE2/AVX/AVX2 machines we use are given in tables 
 4.1↑
  and 
 4.2↑
 . On 
 all the machines, the L1 data cache is big enough to hold 4,000 
 double
 s and the L3 cache can hold more than a
  
 million double-precision numbers.
  
 Blocking 
  
 The function 
 easytrans()
  listed below uses a simple doubly nested loop to transpose the matrix 
 a[]
  to the matrix
  
 b
 []
 . The matrices are of dimension 
  
 m
 ×
 n
  and 
 n
 ×
 m
 . Both of them are assumed to be stored in column-major order
  
 with leading dimension (see section 
 2.2.1↑
 ) equal to the number of rows.
  
  
  
 void easytrans(double *restrict a, double *restrict b,
  
  
  int m, int n){ 
  
  
 for(int i=0; i < m; i++) 
  
  
  
  
 for(int j=0; j < n; j++) 
  
  
  
   
 b[j+i*n] = a[i+j*m]; 
  
 }
  
  
  
 This function is easy to write and easy for the compiler to analyze. The array references use indices that are",NA
4.2.3Optimized matrix multiplication,"Some of the most nettlesome issues in implementing matrix multiplication arise at the level of the processor pipeline. 
 In section 
 3.3.4↑
 , we wrote an assembly program for 4×200×4 matrix multiplication, which reached 
  
 3.5 
 flops per cycle, against the theoretical limit of 4.0 flops per cycle, assuming all the matrices to be in cache. Here we 
 assume the matrices to be in DRAM memory and not in cache, and we show how to optimize matrix multiplication.
  
  
 Suppose 
 A
 , 
 B
 , and 
 C
  are matrices of dimensions 
   
 ℓ ×
 m
 , 
 m
 ×
 n
 , and ℓ ×
 n
 , respectively. The matrices are 
 assumed to be in DRAM memory. The cost of the operation 
 C
 =
 C
 +
 AB
  is 2ℓ
 mn
  arithmetic operations, half of which are 
 additions and half of which are multiplications. If the cache were large enough, each of the matrices can be loaded into 
 cache and kept there as the matrix multiplication is performed. Loading the matrices into cache would take ℓ
 m
 +
 mn
 + ℓ
 n
  
 DRAM memory accesses. If 
  
 ℓ, 
 m
 , and 
 n
  are large, the number of arithmetic operations is 
 much greater than the number of memory accesses. We may expect the cost of the computation to be dominated by the 
 arithmetic operations, allowing us to approach the peak bandwidth of 4 flops per cycle on a single core of an SSE2 
 machine (the figures are 8 flops per cycle and 
  
 16 flops per cycles for AVX and AVX2 machines, 
  
 respectively).
  
 We want the matrices to be big so that the arithmetic operations are far more numerous than memory 
 accesses, but the catch is that the matrices will not fit into cache when they are too big. One way to overcome this 
 dilemma is to use block matrix multiplication. We can pick the block sizes to be small enough to fit into cache but 
 large enough that the cost of loading from memory is outweighed by the cost of arithmetic operations. Careful 
 blocking would indeed improve the simple programs of section 
 3.2.5↑
  but not enough to get anywhere close to 
 peak bandwidth. A much more powerful set of ideas
 [79]
  shows how a program for multiplying matrices can 
 approach the peak bandwidth for floating point arithmetic.
  
 In outline, the basic idea remains to multiply in blocks, but intermediate blocks are stored in scratch space 
 and in convenient formats to minimize TLB and cache misses. As far as possible, data is stored in a format that 
 enables sequential access with unit stride. The actual execution of this idea can make it look more complicated than 
 it is, but the idea is elegant as well as possibly applicable to many other problems.
  
 As shown in table 
 4.7↓
 , our implementation progresses systematically from the 4×200×4 microkernel 
 described in section 
 3.3.4↑
  to the multiplication of square matrices of dimension 9,000. We code a hierarchy of 
 matrix multiplication functions with a function corresponding to each row of the table. Square matrices of 
 dimension 9,000, which occur in the last row, are too large to fit into cache memory. Limiting ourselves to matrices
  
 of specific dimensions keeps the exposition tractable.
  
  
 Matrix Dimensions
  
 b/w
  
 4×200×4
  
 3.48
  
 4×200×12
  
 3.32",NA
(,"B
 1
  B
 2",NA
),".
  
 A
 2
  
 Recursion is used to transpose 
 A
 i
  to 
 B
 i
 . The case with more columns than rows is handled similarly. Write a 
 program that implements this algorithm.
  
 Exercise:
  Rewrite the matrix multiplication programs of section 
 3.2.5↑
  to use blocking. Compare program 
 speeds with and without blocking.
  
 Exercise:
  Rewrite matrix multiplication using one level of blocking as in the previous exercise. But this time
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.3Reading from and writing to disk,"The data in registers and DRAM memory disappears when the computer is powered off. In contrast, hard disk 
 storage is permanent. The hard disk is a collection of platters. Bits stored on circular tracks on either side of the 
 platters are turned on and off using a magnetic field. More than 100 billion bits can be packed into a single square 
 inch of a platter.
  
 File systems, implemented inside the operating system kernel, impose a logical structure on hard disk 
 storage and facilitate its use. Files and directories are stored on the hard disk. In everyday usage, files are read from 
 and written to with the understanding that the data is stored on a hard disk. Between the file as viewed inside a 
 C/C++ program and the hard disk, there are several layers of software. These layers of software provide 
  
 modularity, enabling the operating system kernel to handle several different file systems with a uniform interface 
 and greatly improve speed of access.
  
 The Linux kernel implements a number of optimizations to speed up access to the disk. The most important 
 optimization is to store a page cache. The page cache is a list of page frames that corresponds to data in the disk. 
 When a file is read, Linux will service the read using the page cache if possible. A read or write file operation has to 
 fall through a number of software layers before it reaches the disk. It begins as a system call. The 
 read()
  and
  
 write()
  system calls are issued by the C library functions 
 fread()
  and 
 fwrite()
 . The C library function may do
  
 some buffering of its own. The kernel will service the system calls using a page cache if possible. If not, it invokes 
 the file system to which the file belongs. There are software layers for combining, queuing, and scheduling requests 
 to read or write to the hard disk. The request is finally issued using a device driver.
 [81] 
  
  
 The Linux command 
 lspci -v
  may be used to find out the type of hard disk as well as the device driver
  
 that is in use. Although it is useful to understand that every disk access falls through layers of the file system within 
 the operating system kernel, knowing specific details about the type of hard disk and the device driver is of little use 
 in actual programming. There can be considerable variation in capacity as well as bandwidth of different hard disk 
 systems, but that does not affect programming technique.
  
  
 The C versus C++ discussion in section 
 4.3.1↓
  is on programming technique. The C++ language provides a 
 convenient interface to file input/output using 
 ifstream
  and 
 ofstream
  objects. Although less convenient, the no-
  
 nonsense 
 fread()
 , 
 fwrite()
  interface in C can be much faster, by as much as a factor of 100. In section 
 4.3.1↓
 ,
  
 we explain why there can be such a big difference in speed.
  
 In sections 
 4.3.2↓
  and 
 4.3.3↓
 , we investigate latency and bandwidth to hard disk. In both of these sections, 
 the page cache maintained by the Linux kernel plays a big role. The page cache is a cache of the hard disk 
 maintained in main memory. DRAM memory can be tens of GB in size, and the page cache can occupy a
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.3.1C versus C++,"With regard to the programming technique for reading and writing files, the simplest lesson is also the most 
 valuable. The C interface can be much faster than the C++ interface as we show and for reasons we explain. The 
 following functions use C++ 
 ifstream
  and 
 ofstream
  objects to read and write a 
 double
  array 
 v[]
  
 from or to a file of name 
 fname
 .
  
  
  
 void write_easy(double *v, long len, 
  
  
 const char *fname){ 
  
  
 ofstream ofile(fname); 
  
  
 ofile<<scientific; 
  
  
 ofile.precision(16); 
  
  
 for(long i=0; i < len; i++) 
  
  
  
 ofile<<v[i]<<endl; 
  
  
 ofile.close(); 
  
 }
  
 void read_easy(double *v, long len, const char *fname){ 
  
 ifstream ifile(fname); 
  
  
 for(long i=0; i < len; i++) 
  
  
  
 ifile>>v[i]; 
  
 }
  
  
  
 The C interface below can be more than 100 times faster.
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8
  
 #include <cstdio> 
  
 void write_direct(double *v, long len, 
  
  
  
  const char *fname){ 
  
  
 FILE *fptr; 
  
  
 fptr = fopen(fname, ""w""); 
  
  
 fwrite((void *)v, len, sizeof(double), fptr); 
  
 fclose(fptr); 
  
 }
  
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15 
  
 16
  
 void read_direct(double *v, long len, 
  
  
  
  const char *fname){ 
  
  
 FILE *fptr; 
  
  
 fptr = fopen(fname, ""r""); 
  
  
 fread((void *)v, len, sizeof(double), fptr); 
  
 fclose(fptr); 
  
 }
  
   
  
 The 
 FILE
  type as well as the functions 
  
 fopen()
 , 
 fwrite()
 , 
 fread()
 , and 
 fclose()
  are declared in the
  
 stdio.h
  header file included on line 1. On line 5, a file is opened for writing and on line 13 for reading.
  
 The library function 
 fwrite()
  (line 6) has a quite simple interface. Its first argument is a pointer to a
  
 location in memory. The second argument is the number of items to be written, and the third argument is the size of 
 each item in bytes. The final argument is a pointer to a file.
  
 The library function 
 fread()
  (line 14) has an identical interface. It returns the number of items read, which",NA
4.3.2Latency to disk,"The measurement of latency to hard disk brings up issues not unlike the ones encountered in measuring the latency 
 to DRAM memory. Hard disk is so slow relative to memory that the operating system kernel goes to great lengths 
 to cache file data in DRAM memory. The hard disk maintains a cache and attempts to predict future accesses. The 
 true latency to hard disk is not easily visible to simple computer programs.
  
 The plan for measuring latency is to create a number of files and access several different positions in several 
 different files to gather latency statistics. The plan does not work so easily. If 100 files each of 100 MB are used in 
 the measurement, the latencies for a 1 TB hard disk come out quite low and of the order of microseconds, not 
 milliseconds. That is because all of 10 GB can fit comfortably inside the page cache of a system with 16 GB of 
 memory.
  
 The size of page cache can run into several GB. The size of the page cache may be seen in 
 /proc/meminfo
  
 against the item labeled “Cached.” If we write to a file of size 1 GB on a computer with several GB of memory, the
  
 entire file ends up in the page cache. To clear the page cache, one may use the GNU/Linux command
  
  
  
 echo 1 > /proc/sys/vm/drop_caches
  
  
  
 One may look at 
 /proc/meminfo
  after this command to verify that the page cache has indeed been cleared.
  
 The 
 latency2disk()
  function defined below reads a single 
  
 double
  from a given position in a given file
  
 and returns it. The idea is to use 100 files each of 1 GB to make the total file size in play of the order of 100 GB, 
 and then read a double-precision number at a random location in a random file.
 [82]
  As a result, the actual
  
 measurement of latency makes a large number of calls to 
 latency2disk()
  and the code for which is now shown.
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15 
  
 16
  
 struct disk_latency{ 
  
  
 double fopen_cycles; 
  
  
 double fseek_cycles; 
  
  
 double fread_cycles; 
  
  
 double fclose_cycles; 
  
 }; 
  
 double latency2disk(const char *dir, int filenum, 
  
  
  long posn,struct disk_latency& lat){ 
  
 char fname[200]; 
  
  
 sprintf(fname, ""%s/file%d.dat"", dir, filenum); 
  
 TimeStamp clk; 
  
  
 FILE *fptr; 
  
  
 clk.tic(); 
  
  
 fptr = fopen(fname,""r""); 
  
  
 lat.fopen_cycles = clk.toc(); 
  
  
 clk.tic();",NA
4.3.3Bandwidth to disk,"The functions 
 write_direct()
  and 
 read_direct()
  defined earlier in this section illustrated the use of 
 fwrite()
  
 and 
 fread()
 . These functions read or write a single array of 
 double
 s. A single array cannot exceed the available
  
 DRAM, although it is desirable to work with files much larger than the size of DRAM memory when estimating 
 bandwidth to disk.
  
 The function 
 write_direct()
  is modified below to write to files that are much bigger than available
  
 memory.
  
  
  
 void write_direct(double *v, long len, 
  
  
  
  
  const char *fname){ 
  
  
 FILE *fptr; 
  
  
 fptr = fopen(fname, ""w""); 
  
  
 fwrite((void *)v, len, sizeof(double), fptr); 
  
 for(int i=1; i < FLUSH_COUNT; i++) 
  
  
 fwrite((void *)v, len, sizeof(double), 
  
  
  
  
  fptr); 
  
  
 fclose(fptr);",NA
4.4Page tables and virtual memory,"In this section, we make our first foray into the operating system kernel.
 [83]
  The cost of a single write to memory 
 by an instruction such as 
 movq %rax, (%rsi) 
 brings in many layers of complexity. It could be a cache hit or a
  
 cache miss. If the DRAM memory is accessed, the cost of the access depends on preceding and following memory 
 accesses. It depends on the manner in which the memory controllers operate the DRAM devices. It depends on the 
 parallelism in the instruction stream. It depends on the pressure on dispatch ports in the instruction pipeline among 
 other factors. On top of the layers of complexity engineered into hardware, the operating system introduces yet 
 more complexity.
  
 The operating system is a process manager and has the responsibility of laying down and enforcing the rules 
 that govern nearly all activity on the computer. It creates the environment under which all processes run. It goes 
 about its job so surreptitiously that its presence is ignored in most programming.
  
 However, the activities of the operating system introduce a cost. The map from virtual memory addresses 
 (generated by running programs) to physical DRAM memory is maintained by the operating system. A page is 
 typically 4,096 bytes of virtual memory, as may be verified using the GNU/Linux command 
 getconf PAGESIZE
 .
  
 The operating system creates page tables to map pages to page frames. A page frame is a 4,096-byte-long region of 
 DRAM memory. As explained in section 
 4.1.3↑
 , before accessing a word in DRAM memory, the processor uses 
 page tables to convert a virtual address to a physical address. The operating system is invoked if there is a page fault 
 or if the address is illegal.
  
 Even ignoring page faults, having to look up the page tables every time a memory location is accessed 
 introduces an overhead. At worst, as explained in section 
 4.1.3↑
 , each memory access can turn into two or more 
 memory accesses. The processor cores use a Translation Lookaside Buffer (TLB) to eliminate this overhead.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
4.4.1Partitioning the virtual address space,NA,NA
4.4.2Physical address space and page tables,"Program speed is influenced by the paging system in several ways. Typically, 40% of the instructions are loads and 
 stores, and each memory access involves the paging system. The TLB (translation look-aside buffer) is a shortcut 
 past the huge overheads of the paging system. If a memory word is found in L1 cache and its virtual address is 
 found in TLB, the latency of the memory access would be just 4 cycles. However, if there is a TLB miss, the 
 latency can go up to several hundred cycles. If there is a page fault, the latency of a single memory access can go up 
 to millions, even billions, of cycles. It is also true that the first access of a page in virtual memory always leads to a 
 page fault, and the page fault handler is responsible for allocating a page frame in physical memory.
  
 The paging system is responsible for protecting a program’s memory from other programs and preventing 
 programs from making illegal memory accesses. During a context switch, when the kernel evicts one process and 
 schedules another, the kernel may need to flush the TLB (using a privileged instruction) to protect the address 
 space of the processes. The TLB flush can be expensive. We will look at how the kernel and hardware work 
 together to implement the paging system to get a sense of the overheads of the paging system.
  
 Physical address space 
  
 Soon after the computer is powered on, the kernel starts running in real mode. In real mode, the kernel generates 
 physical addresses directly. Its first job is to load itself completely into physical memory. The kernel can call the 
 BIOS using the 
 int
  (interrupt) instruction to find out about other devices in the system, read information from
  
 tables stored in specific areas of memory, or talk to the devices directly using special instructions. Any code that 
 gets control of the computer in real mode can do almost anything it likes with the computer.
  
  
 As shown in figure 
 4.9↓
 , the kernel is not allowed to use the lowest physical addresses for itself. That region 
 of memory is used by BIOS and the hardware. The kernel loads itself in a low address region of physical memory.
  
 In addition to the physical memory used by initialized kernel data and code at startup, the kernel will need to 
 dynamically allocate memory as more threads and processes are created. The memory claimed dynamically by the 
 kernel is marked “dynamic kernel data” in figure 
 4.9↓
 .",NA
4.5References,NA,NA
Bibliography,": 
 Memory Systems: Cache, DRAM, Disk
  
 . Morgan Kaufmann, 2008 .
  
 [35] B. Jacob, S. Ng, D. Wang
  
 [36] D. Molka, D. Hackenberg, R. Shöne, M.S. Müller: “Memory performance and cache coherency effects on an 
 Intel Nehalem multiprocessor system”, 
 18th International Conference on Parallel Architectures and Compilation 
 Techniques
 , pp. 261-270 , 2009 .
  
 [37] D.P. Bovet, M. Cesati : 
 Understanding the Linux Kernel 
  
 . O'Reilly, 2005 .
  
 [38] J.L. Hennessy, D.A. Patterson 
  
 : 
 Computer Architecture: A Quantitative Approach
 . Morgan Kaufmann , 1990-
 2011.
  
 [39] M. Frigo, C.E. Leiserson, H. Prokop, S. Ramachandran : “Cache oblivious algorithms”, 
 Foundations of 
 Computer Science, 40th Annual Symposium
 , pp. 285-297, 1999 .
  
 [40] S. Goto, R. A. van de Geijn 
  
 : “Anatomy of high performance matrix multiplication”, 
 ACM TOMS
  , pp. art:12 ,
  
 2008 .
  
 [41] V. Bubka, P. Tuma : 
 Investigating cache parameters of x86 processors
  
  in 
 SPEC Benchmark Workshop 2009
  
 (D. Kaeli and K. Sachs
  
 , ed.). Springer-Verlag , 2009 .",NA
5Threads and Shared Memory,"Programming with threads is a paradigm of great range and utility that encompasses everything from cell phones to 
 web servers to supercomputers. The processor cores of today are clocked at around the same rate as the processor 
 cores of 2005. However, the number of processor cores on the same package continues to increase dramatically. In 
 addition, multiple processor packages can be connected using a fast interconnect. All the processors in the 
 interconnected processor packages share the same DRAM memory. Nodes with multiple processor cores are so 
 powerful that problems with a billion grid points can be tackled on a single node. Multithreaded programming is set 
 to remain a leading programming paradigm.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.1Introduction to OpenMP,"Section 
 5.1.1↓
  introduces OpenMP. 
  
 [85]
  Nearly all the syntax that most programmers will ever need is brought out 
 by parallelizing the summation of the Leibniz series in two different ways.
  
 OpenMP programs look much like sequential programs, and the syntax is easy to learn. The simplicity is 
 mostly illusory, however. Whenever concurrent programs share memory, as OpenMP programs do, the 
 programming model inevitably becomes very subtle. OpenMP syntax conceals much of this subtlety behind a 
 sophisticated memory model. Every OpenMP programmer would be well advised to understand this memory 
 model. The correctness of even the simplest OpenMP program relies on it. The memory model is discussed in 
 section 
 5.1.2↓
 .
  
 Section 
 5.1.3↓
  is about the overheads associated with OpenMP parallel regions and other constructs. When a 
 task is divided between threads using OpenMP, the division itself will incur overhead. If this overhead is too great 
 relative to the size of the task, the task may not be worth parallelizing. OpenMP parallelism works best at a coarse 
 level and for outer regions of the program. One may think of a program as being essentially sequential and split 
 certain tasks within that sequential flow between threads using OpenMP constructs. Where and whether such a split 
 makes sense or not is entirely determined by OpenMP overheads.
  
 In section 
 5.1.3↓
 , we describe the implementation of mutual exclusion on x86 machines. Mutual exclusion is 
 so fundamental to threaded programming that the x86 instruction set has supported it for many decades. Much of 
 the overhead of OpenMP constructs is incurred through instructions related to mutual exclusion.",NA
5.1.1OpenMP syntax,"OpenMP syntax is beguilingly simple. A simple function for computing the 
 n
 th partial sum of the Leibniz series
  
 follows:
  
  
  
 double leibniz(long int n){ 
  
  
 long int i; 
  
  
 double ans=4.0; 
  
  
 for(i=1; i < n; i=i+2){ 
  
  
  
 ans -= 4.0/(2.0*i+1); 
  
  
 ans += 4.0/(2.0*i+3); 
  
 } 
  
  
 return ans; 
  
 }
  
  
  
 As it is written, this function runs on a single processor core. We will rewrite it to run on multiple cores
  
 using OpenMP. By writing the OpenMP program in different ways, we expose much of the basic syntax of 
 OpenMP. There is a lot more syntax to OpenMP than we will discuss here, but much of it is hardly ever needed. 
 OpenMP constructs are embedded into C or C++ code. The GNU, Intel, and PGI compilers support OpenMP. With 
 icpc
 , the option 
 -openmp
  must be used during compilation and linking. For 
 gcc/g++
 , the corresponding option is
  
 -fopenmp
 .",NA
5.1.2Shared variables and OpenMP’s memory model,"Let us look again at the 
 critical
  construct.
  
  
  
 #pragma omp critical 
 ans += sum;
  
 As stated earlier, the 
 critical
  construct ensures that the statement 
  
 ans += sum
  is executed by only one
  
 thread at a time. That might seem sufficient to ensure the correctness of the program because each thread will 
 separately add its local sum to the final answer, but it is not.
  
 The compiler may decide to store the variable 
 ans
  in a register and add its local sum to a register when 
 ans
  
 += sum
  executes. The code for copying the register to the memory location of 
 ans
  may be inserted 
  
 after
  the critical
  
 block. In that case, we are back to the situation where the threads interfere with each other.
  
 Why is that not a problem? In answering this question, we arrive at the 
 flush
  feature, which is a vital part
  
 of the OpenMP memory model. When a thread has a shared variable such as 
 ans
 , it is allowed to keep a copy of
  
 that variable in a register. Indeed, when the compiler generates assembly instructions, it may keep multiple copies 
 of the same variable in memory and in the register file for its convenience. When an OpenMP 
 flush
  operation is
  
 executed, a thread must make all its writes visible to other threads even if the writes have only modified locally 
 stored copies of a shared variable. The temporary view of all shared variables must be synced with the global view 
 in shared memory.
  
 The OpenMP standard requires that a thread should implicitly carry out a 
 flush
  operation when it enters
  
 and leaves a critical or parallel construct, and at each barrier it encounters. Even the basic program for summing the 
 Leibniz series relies on this rule for its correctness. Each thread syncs its local copy of 
 ans
  with the global copy in
  
 shared memory when it enters the critical region, updates its local copy in the critical region, and syncs the updated 
 value with the global copy of 
 ans
  in shared memory before it exits the critical region.
  
 A more complex example 
  
 The OpenMP memory model is in play every time threads use a shared variable to communicate directly or 
 indirectly. For finding the partial sum of the Leibniz series, the threads communicate indirectly using a shared 
 variable to which each thread adds its part of the sum. The correctness of almost every OpenMP program depends 
 on 
 flush
  operations that are implicitly carried out at the beginning and end of parallel and critical constructs and at
  
 every barrier.
  
  
 In general, OpenMP allows the compiler to generate assembly for a parallel region or for chunks of a for-
 loop assigned to a thread, as if the code is sequential---except for implicit 
 flush
  statements. Of course, the
  
 compiler must also handle reduction variables correctly.
  
  
 The peculiar implications of the OpenMP memory model can be difficult to grasp with just a single example. 
 Therefore, we give another more complex example here. The variable 
 x
  is the only variable shared by the
  
 two threads created by the function 
 printstuff()
 , whose listing follows.
  
  
  
  
 1
  
 void printstuff(){",NA
5.1.3Overheads of OpenMP constructs,"OpenMP constructs are embedded into C/C++ programs to create teams of threads that work in parallel. The 
 parallel, barrier, and for constructs introduce overheads. Work may need to be assigned to threads, threads may 
 need to be created and destroyed, or synchronization and serialization may need to be implemented using system 
 calls to the operating system kernel. These activities consume cycles. If the parallelized task is too small, the 
 benefits of parallelization will be overwhelmed by the overheads. Effective programming requires knowledge of
  
 the overheads of OpenMP constructs.
  
  
  
  
 Computer
  
 Number of Cores/Threads
  
 min
  
 median
  
 max
  
 3.6 GHz AVX2
  
 2
  
 1176
  
 1332
  
 3.4×10
 6
  
 2.6 GHz SSE2
  
 12
  
 4888
  
 6188
  
 3.2×10
 6
  
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.2Optimizing OpenMP programs,"In OpenMP, program memory is often naturally split between threads. In an example such as matrix transpose, for 
 instance, certain blocks of the matrix are mainly handled by certain threads. When multiple threads occupy 
 processor cores across multiple packages, there is much to be gained by allocating memory for each thread in a 
 memory channel that is close to it.
  
 The way to do this is explained in section 
 5.2.1↓
 . At first one may suspect that memory is allocated during a 
 call to 
 malloc()
  or its equivalent. In fact, during 
 malloc()
 , the memory that gets allocated is typically virtual and
  
 not physical. It is only during first access, and the resulting page fault, that a page of virtual memory is mapped to a
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.2.1Near memory and far memory,"Figure 5.1Assignment of threads numbered 0 through 11 to processor cores with 
  
 compact
  and 
 scatter 
  
 affinities.
  
  
 To explain the distinction between near and far memory, we turn to figure 
 5.1↑
 . That figure shows a 
  
 12-core
  
 machine, with the processors divided into packages. All memory references to far memory, which is those page 
 frames that reside in a memory channel connected to the other processor package, have to go through an 
  
 interconnect. This interconnect is similar to a fast network channel and its use makes references to far memory more 
 expensive. For example, on a 2.6 GHz 12-core SSE2 machine, the latency to near memory is 
  
 180 cycles, 
 whereas the latency to far memory is 300 cycles. On another machine ( 
  
 2.2 GHz 16-core AVX) with two processor 
 packages, the latency to near memory is again around 180 cycles and the latency to far memory is again much 
 greater, being more than 350 cycles.
  
  
 Thus, it is advantageous if a page frame that is mostly used by a thread resides in near memory. If page 
 frames are in far memory, the speed of the program can degrade by more than a factor of two.
  
 The assignment of pages to page frames can be enforced through some kind of an initialization routine. An
  
 example follows:
  
  
 void init_manycore(double *list, long len, int nthreads){
  
 #pragma omp parallel for 
  
 \
  
 \
  
 num_threads(nthreads) 
  
 \
  
 default(none) 
  
 shared(list, len, nthreads)
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.2.2Bandwidth to DRAM memory,"The 
 sum_onecore()
  function listed below is the engine for measuring read bandwidth to memory.
  
  
  
 double sum_onecore(double *list, int n){ 
  
 double ans = 0; 
  
 #pragme vector always 
  
  
 for(int i=0; i < n; i++) 
  
  
  
 ans += list[i]; 
  
  
 return ans; 
  
 }
  
  
  
 The 
 icpc
  compiler generates excellent code for this function. It unrolls the loop and generates packed 
 double instructions so that the cost of adding is irrelevant. The 
 pragma
  directive gives a strong suggestion to 
  
 icpc
  
 to use XMM/YMM/ZMM registers and packed double instructions. Other compilers would ignore this directive. 
  
 The 
 sum_onecore()
  function is called by each thread created by the function 
 sum_manycore()
 , whose",NA
5.2.3Matrix transpose,"The function 
 blocktransx()
  defined below is similar to the one defined in chapter 
 4↑
 . This definition
  
 differs by allowing the matrix stored in the array 
 b[]
  to have a leading dimension greater than the number of rows.
  
  
  
 void blocktransx(double *restrict a, double *restrict b, 
       
  int ldb, int m, int n){
  
  assert((m%B==0)&&(n%B==0));
  
  for(int i=0; i < m; i+=B)
  
   
  for(int j=0; j < n; j+=B)
  
   
  for(int ii=0; ii < B; ii++)
  
     
  for(int jj=0; jj < B; jj++)
  
     
  b[j+jj+(i+ii)*ldb] = a[i+ii+(j+jj)*m]; 
  
 }
  
  
  
 The function listed below is a multithreaded implementation of the matrix transpose, which uses
  
  
 blocktransx()
 .
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5
  
 void blocktrans(double *restrict a, double *restrict b, 
  
  
 int m, int n, int nthreads){ 
  
  
 assert(m%B==0); 
  
  
 assert(n%(nthreads*B)==0); 
  
 #pragma omp parallel
  
 \
  
 6
  
 num_threads(nthreads)
  
 \
  
 7
  
 default(none)
  
 \
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14
  
 shared(a, b, m, n, nthreads) 
  
 { 
  
  
 int tid = omp_get_thread_num(); 
  
  
 int nn = n/nthreads; 
  
  
 int nfst = tid*nn; 
  
  
 int ldb = n; 
  
  
 blocktransx(a+m*nfst, b+nfst, ldb, m,
  
 nn); 
  
 15
  
 }
  
 }
  
 16
  
  
  
 The function 
 blocktrans()
  assumes that the array 
  
 a[]
  stores an 
 m
 ×
 n
  matrix (with leading dimension 
 m
 )
  
 and saves its transpose in 
 b[]
 . The transposed 
 n
 ×
 m
  matrix is stored in 
 b[]
  using leading dimension 
  
 n
 . The threads
  
 split the columns of 
 a[]
  equally between themselves on lines 11 and 12. The matrix transposed by each thread has
  
 dimension 
 m
 ×
 nn
 , where 
 nn
  is defined on line 11. The function call on line 14 shifts the first entry of 
 a[]
  by 
 nfst
  
 columns and the first entry of 
 b[]
  by 
 nfst
  to isolate the submatrix that the thread works on.
  
  
 Table 
 5.4↓
  shows that the bandwidth realized for varying number of threads and varying block sizes 
 B
 . The 
 matrix transposed was square with dimension as close to 40,000 as possible subject to the divisibility conditions 
 assumed on lines 3 and 4. On both the 12-core SSE2 machine and the 
  
 16-core AVX machine, the bandwidth 
 realized in transposing is nearly 80% of the bandwidth for copying, as may be verified by comparing with table 
 5.4↓
 . Although we had complaints about the compiled code for a single processor with a single thread, such
  
 complaints may not be justified here. Getting to 80% of the best possible in a matrix transpose is quite good.
  
  
  
  
 Computer
  
 # of Threads/Cores
  
 B
  
 bw",NA
5.2.4Fast Fourier transform,"For a purely arithmetic program, such as summing the Leibniz series, the speed of the program increases linearly 
 with the number of cores, provided the number of terms summed is large enough to outweigh the overhead of 
 entering and leaving an OpenMP parallel region. Other tasks, such as transposing a matrix, are limited by 
 bandwidth to memory. The bandwidth to memory does not increase linearly with the number of cores employed. 
 Many problems in scientific computing---finite differences or finite elements on a 3D mesh to give two examples---
 fall in the latter category.
  
 Dense linear algebra problems, such as matrix multiplication, are limited by bandwidth to memory if 
 implemented in a straightforward way. In chapter 
 3↑
 , we showed how to write a microkernel for matrix 
  
 multiplication that takes advantage of parallelism in the instruction set. Chapter 
 4↑
  showed how to cleverly hide the 
 cost of accessing memory. A multithreaded matrix multiplication must account for cache memory slightly 
 differently because the L3 cache is common to all the processor cores on the same package. Yet linear speedup with 
 increasing processor cores can be achieved without drastic rethinking.
  
 Hiding the cost of memory accesses and making efficient use of processor resources are essential for
  
 optimized FFT routines as well. The inverse Discrete Fourier Transform (DFT) of the sequence 
 a
 0
 ,…,
 a
 N
  − 1
  is 
 given by
  
 b
 j
 =
  
 N
  − 
 1
 ⎲
  
 ⎳
  
 ω
 jk
 a
 k
  
 k
 =0
  
 where 
 ω
 =exp(2
 πi
  ⁄ 
 N
 ) is a primitive 
 N
 th root of unity. This transformation is sometimes called the DFT, but we 
 prefer to call it the inverse DFT to maintain an analogy with Fourier analysis. If the 
 a
 j
  are discrete Fourier 
 coefficients, we may think of 
 b
 j
  as equispaced samples in the time domain. The FFT is a fast algorithm for 
 computing the DFT or its inverse.
  
  
 In chapter 
 2↑
 , we discussed the FFTW and MKL libraries and their interfaces to the 1D inverse DFT. Here 
 we return to that topic to make a few points about the FFT and its implementation.
  
  
 Table 
 5.5↓
  shows the number of cycles used by the FFT for various 
  
  
 N
  as a function of the number of 
 threads. The speedups are quite good and reach 75% of linear speedup for some of the values of 
 N
 . The numbers in 
 the table were obtained by lining up many blocks of 
 N
  complex numbers (or 
  
 2
 N
  double-precision numbers) in 16 
 GB of memory. The number of blocks is approximately 10
 9
  ⁄ 
 N
 , and the FFT of each block was computed. The
  
 blocks were split between threads using OpenMP. Each thread applied MKL’s 1D FFT to its share of the blocks.",NA
5.3Introduction to Pthreads,"So far our discussion of threads has been limited to OpenMP. OpenMP is a limited programming model. It applies 
 mainly to those situations where the data is static and the access patterns are regular. Pthreads are a more powerful 
 programming model.
  
  
 The Pthread interface for creating and running threads is supported by the Linux kernel. In Linux, each 
 thread is a separate process. Thread creation is the same as process creation. The distinguishing feature of a group
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.3.1Pthreads,"We begin our discussion of Pthreads with a function that could be in a simple sequential program. The function 
 print_message()
  uses basic C syntax and has no Pthread construct in it. Indeed, it is compiled into assembly
  
 instructions as if it were a single threaded program---an important point that has already come up in our discussion
  
 of OpenMP. Each Pthread will later take control by executing this program.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15 
  
 16
  
 void *print_message(void *arg){ 
  
  
 char *s = (char *)arg; 
  
  
 char ss[400]; 
  
  
 int l = strlen(s); 
  
  
 for(int i=0; i < l; i++){ 
  
  
  
 ss[2*i] = s[i]; 
  
  
  
 ss[2*i+1] = (s[i]==’ ’)?’ ’:’_’; 
  
 } 
  
  
 ss[2*l] =’\0’; 
  
  
 printf(""%s"",s); 
  
  
 printf(""\n""); 
  
  
 printf(""%s"", ss); 
  
  
 printf(""\n""); 
  
  
 printf(""ss = %p \n\n"", (void *)ss); 
  
 return NULL; 
  
 }
  
  
  
  
 The only sign that 
 print_message()
  may have something to do with Pthreads occurs in its first line. It is
  
 declared to be a function that takes a single argument of type 
 void *
  and returns a single value also of type 
  
 void
  
 *
 . Whatever arguments we want to send to a Pthread must be packed into a region of memory and sent to the",NA
5.3.2Overhead of thread creation,"To find the cost of creating and destroying Pthreads, we use the following simple function, which each thread will
  
 execute:
  
  
  
 void *addone(void *arg){ 
 long *p = (long *)(arg); 
 *p += 1; 
  
 return NULL;",NA
5.3.3Parallel regions using Pthreads,"This section begins with a simple OpenMP program. The OpenMP program alternates between two parallel 
 regions. In the first parallel region, every thread runs a function called 
 addone()
 . In the second parallel region,
  
 every thread runs a function called 
 addtwo()
 .
  
 Later in the section, the parallel regions are implemented using Pthreads. The first implementation is plain 
 C, except for creating and launching Pthreads. The second and third implementations use spinlocks and mutexes, 
 respectively. The final implementation uses conditional variables.
  
  
 The basic idea in all four implementations is as follows. If the number of threads is 
 n
 , including the master 
 thread, the master thread begins by creating 
 n
  − 1 workers. The worker threads do not exit when their job is done",NA
5.4Program memory,"In the virtual memory setup described in section 
 4.4↑
 , every process has its own page tables to map virtual memory 
 to physical memory, and every instruction with a memory reference invokes this map from virtual to physical 
 memory. The virtual memory system allows many distinct processes to coexist on the same computer. Threads are a 
 group of processes that share the same map from virtual to physical memory. In addition, each thread may define its 
 own local variables in an area that is ordinarily invisible to other threads. Thus, the concept of threads is tied to the 
 memory system, and understanding program memory facilitates understanding threads.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.4.1An easy system call,"The operating system kernel offers its services to user programs through system calls. There are system calls for 
 dealing with every single part of the computing system. There are system calls related to the file system, networks, 
 memory management, and process creation and scheduling. Every 
 printf()
  or 
 malloc()
  begins life in the C
  
 library but finds its way into the operating system kernel through a system call.
  
 System call definition and invocation 
  
 The system call defined in this section gives us a peek into the Linux kernel. The system call sets a global flag. The 
 global flag is used to turn print statements on and off elsewhere in the kernel.
  
 The definition of the system call is placed at the end of the file 
 kernel/sys.c
  in the Linux source tree
  
 (Version 3.6).
 [92]
  
  
 1 
  
 2
  
 int dv_print_flag=0; 
  
 EXPORT_SYMBOL(dv_print_flag);
  
  
 3 
  
 4 
  
 5 
  
 6 
  
 7
  
 asmlinkage long sys_set_dvflag(int flag){ 
  
 dv_print_flag = flag; 
  
  
 return 77; 
  
 }",NA
5.4.2Stacks,"The stack is a collection of objects that supports two operations: push and pop. If an object is pushed on the stack, it 
 lands right at the top. If another object is pushed, it goes on top above the last object that was pushed. The objects of 
 a stack are ordered with the most recently pushed object on top. The pop operation removes the topmost object from 
 the stack.
  
 In terms of its representation in memory, the stack is a simple data structure. To represent a stack of 
 int
 s,
  
 for example, we may simply use an array of type 
 int
  and another variable to keep track of the size of the stack.
  
 Every pop operation decreases the size of the stack by 1, and every push operation increases the size of the stack by 
 1.
  
 In terms of the way data is represented or accessed, there is nothing new to stacks. The novelty is in the 
 abstract view of a collection of objects that constitute a stack. As with a stack of plates, we can insert and remove 
 only at the top.
  
 Towers of Hanoi
  
  
  
  
 Figure 5.2Towers of Hanoi. The discs must be moved from peg 0 to peg 1 without ever place a bigger disc on 
 top of a smaller disc.
  
  
  
 Stacks are useful for implementing recursion and function calls. The Towers of Hanoi, a classic example,
  
 illustrates the usefulness of stacks.
  
 The problem is to move 
 n
  disks from peg 0 to peg 1. In a single move, we may move the topmost disc of one 
 peg to another peg, but a move is not allowed to place a bigger disc over a smaller disc. To begin with, the 
 n
  discs on 
 peg 0 are ordered with the biggest disc at the bottom of the smallest at the top---to form a tower (see figure 
 5.2↑
 ).
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
5.4.3Segmentation faults and memory errors,"The use of pointers exposes the programmer to errors that corrupt memory. Memory errors can befuddle even 
 expert programmers. An erroneous program with memory errors may work on some systems but crash on others. 
 The point where the program crashes can be far away from the point where the error occurs. The program may 
 work for some inputs but not for others. In multithreaded programs, memory errors may be triggered by race 
 conditions that are hard to reproduce, making debugging difficult.
  
  
 In this section, we take a detailed look at segmentation faults and memory errors. Throughout this section, 
 we assume that the maximum size of the stack is at least 2
 23
  bytes or 8.389 MB. The examples can be easily
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
(,"i
 +10
 4",NA
),"2
 =242,644,667,000 to validate our explanation.
  
  
 In 
 run1()
 , 
 run2()
 , and 
 run3()
 , all the entries of 
 list[]
  were  ≥ 0. We may fill 
 list[]
  with negative 
 entries to examine scenarios that break the stack in the opposite direction, which is the direction the stack grows. If 
 the entries of 
 list[]
  are all negative and 
  
  ≤  − 1,000, for example, the local variables of 
 ff0()
  and the return",NA
5.5References,NA,NA
Bibliography,"[42] B. Chapman, G. Jost, R. van der Pas
  
 : 
 Using Open MP
  . MIT Press , 2008 .
  
 [43] C. van Loan : 
 Computational Frameworks for the Fast Fourier Transform
  
 . SIAM, 1992 .
  
 [44] D.P. Bovet, M. Cesati : 
 Understanding the Linux Kernel
  
 . O'Reilly, 2005 .
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6Special Topic: Networks and Message Passing,"For many, computer networks are synonymous with the Internet. The Internet is a means for rapid communication, 
 propagation of information and data, and delivery of services. Much of the activity is between clients and servers. 
 Peer-to-peer activity is less common.
  
 In scientific computing, networks of thousands of computers are deployed to solve large-scale problems.
  
 The coordination between the computers is much tighter than in the Internet. The subproblem tackled by each 
 computer typically has dependencies on other subproblems, which often implies that interprocess communication 
 must be deeply integrated into the computation. Accordingly, the architecture of high-performance computing 
 networks features much tighter integration than the Internet.
  
 The principal framework for programming high-performance networks is Message Passing Interface (MPI).
  
 MPI is a library that allows processes running concurrently on different nodes to communicate by sending and 
 receiving messages. Each computing node will have a few dozen (this number is growing rapidly) or so processor 
 cores and threads running on the same node share memory. However, threads on different nodes do not share 
 memory but can use the MPI library to communicate by sending and receiving messages. Ideally, the sending and 
 receiving of messages is done solely by the master thread on any given node.
  
 Many MPI programs were written years ago when each node had just a single core. Even now, when nodes 
 have dozens of processor cores, one may pretend that each processor core is a separate node and put a single MPI 
 process on each core. Then the MPI processes on the same node ignore that they share memory and communicate 
 using the MPI library. Although this practice is common, it is not a good one. It ignores the powerful market forces 
 that are putting more and more processor cores on the same node. Message passing is an inefficient way to 
 communicate when processors share memory.
  
 Normally, we compile sources into object files, build an executable, and then just run it at the command 
 line. The process of running an executable is a little different with MPI. The executable must be simultaneously 
 launched on multiple computers, and the MPI processes on the different computers must become aware of each 
 other before they can send and receive messages.
  
  
 Because the manner in which MPI processes start running is a little different, section 
 6.1↓
  begins by showing 
 how to initialize and run MPI. In section 
 6.2↓
 , we describe the architecture of high-performance networks.
  
 The particular architecture we discuss is the one most common today, and other high-performance networks are 
 built on similar ideas. The MPI standard and its wide adoption within scientific computing have provided powerful 
 impetus to innovation in this area. For more than two decades, the biggest scientific computations have been 
 performed using MPI.
  
 Section 
 6.3↓
  discusses a range of examples. Each of these is informed by the discussion of network 
 architecture in the earlier section. In many examples, memory optimizations studied in previous chapters are of 
 greater consequence than network optimizations. When optimizations such as overlapping processor activity with 
 network activity do make a difference, they do so only after memory accesses have been carefully dealt with. 
 Overlapping processor activity with network activity requires deep knowledge of the manner in which MPI library 
 functions map to the network’s architecture. One of the examples in section 
 6.3↓
  is a discussion of bandwidth to 
 disk from MPI programs.
  
  
 It is likely that the largest scientific computations in the world will be performed using MPI for several years 
 to come. The main competition to MPI is not from other networking libraries but from the increasing power of a
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.1MPI: Getting started,"Section 
 6.1.1↓
  explains how to compile, link, and run an MPI program. MPICH, MVAPICH2, and Open MPI are 
 the three major MPI implementations. The details of compilation, linking, and running vary between these 
 implementations as well as between different sites. However, the general picture is the same. The Open MPI-
 specific discussion in section 
 6.1.1↓
  may be altered to other implementations in a straightforward manner.
  
 It is typical to compile and link MPI programs using commands such as 
 mpiCC
  or 
 mpicxx
 . In such usage,
  
 MPI pretends to be an extension of C/C++. However, MPI is in fact a library, like many other libraries, although it 
 influences program structure in a quite radical way. The compilation and linking syntax exhibited in section 
 6.1.1↓
 does not use wrappers such as 
 mpiCC
  or 
 mpicxx
 . Instead, both compilation and linking use the C/C++ 
 compiler.
  
 This is a minor point, but our preference is to make a library look like a library.
  
 From the first MPI program in section 
 6.1.1↓
 , we run MPI with one process per node and not one process per 
 core. The much more common one process per core MPI programs must be discouraged. Such usage is directly in 
 opposition to powerful market forces that are increasing the number of cores on a single node. To make MPI 
 processes on the same node communicate using messages is to completely ignore the shared memory architecture of 
 each node. Because there is a single network card per node, the processes on the same node will contend for the 
 network card when passing messages to other nodes. The penalty for such negligence is likely to increase. In our 
 programs, the master thread on each node will use OpenMP to create a thread on each core. All the message passing 
 is handled by the master thread.
  
 Section 
 6.1.2↓
  introduces 
 MPI_Send()
  and 
 MPI_Recv()
 . These two function calls, together with variants to
  
 be introduced later, are the backbone of the MPI library. Both function calls block. That means 
 MPI_Send()
  does
  
 not return until the library verifies that whatever is sent has been received. Similarly, 
 MPI_Recv()
  does not return
  
 until whatever is expected to be received is completely received.
  
 The blocking semantics of 
 MPI_Send()
  and 
 MPI_Recv()
  is natural from the point of view of a user. Indeed,
  
 these calls are widely used. However, the way they map to network architecture creates inefficiencies. The use of 
 blocking calls wastes network bandwidth as we will see later. In section 
 6.1.2↓
 , we write a program called",NA
6.1.1Initializing MPI,"MPI
 [98]
  is the most commonly used library for solving scientific problems on networked computers. MPI calls itself 
 a fully featured library---with some justice. It has a lot of features. We will use only a thin sliver of MPI. As our 
 goal is to write programs informed by the underlying network architecture, many of MPI’s features are of little 
 concern to us. The features we use are the ones that are best optimized by MPI implementations.
  
 When an MPI run is set up, each node will have a process on it. For example, if the run is with 8 processes, 
 each of the 8 nodes in figure 
 6.1↓
  may have one process on it. The processes communicate by sending and 
 receiving messages. We begin by looking at how the processes are set up and turn to message passing later.
  
 Our first MPI program follows. This program assumes that the processor name is shorter than 199
  
 characters.
 [99]
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10
  
 #include <mpi.h> 
  
 int main(int argc, char **argv){ 
  
  
 MPI_Init(NULL, NULL); 
  
  
 int numprocs, rank; 
  
  
 MPI_Comm_size(MPI_COMM_WORLD, &numprocs); 
  
 MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
  
 char procname[200]; 
  
  
 int procnamelen; 
  
  
 MPI_Get_processor_name(procname, &procnamelen); 
  
 cout<<""proc name=""<<procname<<"" rank=""
  
 <<rank<<endl;
  
 11
  
 }
  
 MPI_Finalize();
  
 12
  
  
  
  
 On line 10, each process prints the name of the compute node it is running on. Thus, the purpose of this
  
 program is to print the names of the nodes that the processes run on. If the program is started up with 10 processes, 
 there will be 10 names printed.
  
 Calls to the MPI library occur on lines 3, 5, 6, 9, and 11. Each MPI function begins with the prefix 
 MPI
 . The
  
 declarations of the MPI library functions are found in 
 mpi.h
 , the header file included on line 1.
  
 The 
 MPI_Init()
  function on line 3 must be called before messages can be sent or received. Presumably, it
  
 initializes data structures essential to other MPI functions. Both its arguments are 
 NULL
 .
  
 On line 5, the function 
 MPI_Comm_size()
  is used to determine the size of an MPI communicator. MPI
  
 communicators are one of many MPI features we do not get into. The only communicator that occurs in this chapter 
 is 
 MPI_COMM_WORLD
 . All the MPI processes are members of this communicator. If the MPI run is started up
  
 with 10 processes, line 5 will set the value of the 
  
 int
  variable 
 numprocs
  to 10.
  
  
 Each of the processes has a rank, much as each OpenMP thread has a thread identifier. If the number of 
 processes is 10, the process ranks will go from 
  
 0 through 9. On line 6, the 
 int 
 variable 
 rank
  is set to be equal to
  
 the process rank by calling 
 MPI_Comm_rank()
 .
  
 MPI functions such as 
 MPI_Comm_size()
  and 
 MPI_Comm_rank()
  may be made to return error codes. Our
  
 aim being to understand the dependence of program performance on computer architecture, we follow our usual 
 custom and ignore error handling. Eliminating error handling saves considerable clutter in the program. By default, 
 MPI implementations abort when an error occurs. Although the default behavior can be changed, it is just fine by 
 us.
  
  
 On line 9, the processor name is read into a character string. The function used on that line is self-
 explanatory.",NA
6.1.2Unsafe communication in MPI,"All MPI processes execute the same program. If the 
 -bynode
  (or an equivalent) option is used, each process runs
  
 on a different node. The processes communicate by sending each other messages across the network. MPI syntax 
 allows a process to use either blocking or nonblocking function calls to send and receive. A blocking send or 
 receive waits until the operation is complete. Blocking communication is more vulnerable to deadlocks. Because 
 two processes have to participate in message passing, the processes may get stuck waiting for each other. Even 
 relatively simple programs can deadlock when blocking calls are used, as we show in this section.
  
 At the beginning, each MPI process has to determine its rank and the total number of processes. The
  
 following function is called for doing so:
  
  
  
 void mpi_initialize(int& rank, int& nprocs){ 
  
  
 MPI_Init(NULL, NULL); 
  
  
 MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
  
 MPI_Comm_size(MPI_COMM_WORLD, &nprocs); 
 }
  
  
  
 Using 
 mpi_initialize()
  saves us the trouble of remembering the syntax of three MPI library functions.
  
  
 Before making any calls to the MPI library, a program may execute the following code fragment:
  
  
  
 int rank; 
  
 int nprocs; 
  
 mpi_initialize(rank, nprocs);
  
  
  
 After this fragment, each process knows its rank and the total number of processes. Before exiting, each
  
 process must call 
 MPI_Finalize()
 . Each MPI process uses process rank to identify the process to which it sends a
  
 message or from which it receives a message.
  
 Our first example of sending and receiving messages is the function 
 unsafe()
  listed below.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8
  
 void unsafe(int numOFdoubles, int rank, int nprocs,
  
  
  double *sendbuf, double *recvbuf) 
  
 { 
  
  
 int tag = 0; 
  
  
 MPI_Status status; 
  
  
 MPI_Barrier(MPI_COMM_WORLD); 
  
  
 if(rank==0){ 
  
  
   
 MPI_Send(sendbuf, numOFdoubles,
  
 MPI_DOUBLE, 
  
 9 
  
 10
  
  
  nprocs-1, tag, MPI_COMM_WORLD); 
 MPI_Recv(recvbuf, numOFdoubles,
  
 MPI_DOUBLE, 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15
  
  
  
  nprocs-1, tag, MPI_COMM_WORLD,
  
  
  &status); 
  
 } 
  
 else if(rank==nprocs-1){ 
  
  
 MPI_Send(sendbuf, numOFdoubles,
  
 MPI_DOUBLE, 
  
 16 
  
 17
  
  
  0, tag, MPI_COMM_WORLD); 
 MPI_Recv(recvbuf, numOFdoubles,
  
 MPI_DOUBLE, 
  
 18
  
  0, tag, MPI_COMM_WORLD,
  
 &status);
  
 19
  
 }
  
 }
  
 20
  
  
  
  
 We will assume that the MPI program is started up with exactly two processes and that each process calls
  
 unsafe()
 . The two processes send a certain number of 
 double
 s to each other. The number of 
  
 double
 s transmitted
  
 is given by the first argument. It is assumed that 
 rank
  and 
 nprocs
  have been appropriately initialized by both",NA
6.2High-performance network architecture,"A network connects many computers together. Each computer is an independent entity. Thus, every communication 
 between two computers requires coordination. The coordination is much looser in the vast and decentralized 
 Internet than it is in high-performance networks used in scientific computing.
  
 Infiniband, which is an open standard, is a leading technology in high-performance networking. The 
 technology has stabilized over the past decade or so, and almost all large and even small computing clusters use 
 Infiniband or a proprietary variation. In this section, we review three principal features of Infiniband: network 
 topology, kernel-free message passing, and one-sided communication. These aspects of the network have an 
 influence on the way MPI programs are written, as shown in the next section.
  
 Network topology is the graph used to connect computers together. On the Internet, the graph is not regular 
 or structured, although it is far from being random (the Internet is a heavily designed network). In high-
 performance networks, the graphs are far more regular and structured.
  
 Every time information is transferred over the Internet, the message passes through the operating system 
 kernel at both the sender and the receiver. For example, when a send button is pressed on an email client, the email 
 is first copied to kernel buffers. It is transferred to the network card at the client site before traveling to the server. 
 The network card on the server again copies the email to kernel buffers before passing it along to some other server 
 or the receiving client.
  
 Kernel copies create considerable overhead and the Linux kernel goes to great lengths to avoid excessive 
 copying. In Infiniband, kernel copies can be avoided altogether. Information is copied directly between the 
 Infiniband network card and the DRAM memory owned by the client program.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.2.1Fat-tree network,"Figure 6.1Fat-tree with 8 nodes. Switches are shown as rectangles. The ideal fat-tree is a perfect binary tree 
 with the number of links between levels being approximately constant. The fat-tree is particularly easy to lay 
 out on the plane, as shown on the right-hand side.
  
  
  
 In many high-performance clusters, nodes are networked using fat-trees.
 [101]
  A defining property of fat-
  
 trees, shown in figure 
 6.1↑
 , is that the number of links between levels is roughly a constant. More links join 
 switches at higher levels than at lower levels. With a suitable switching algorithm and assuming that any two nodes 
 are equally likely to communicate, the load on each link is roughly the same. Of course, the switches at higher
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.2.2Infiniband network architecture,"Figure 6.3Multiple access network.
  
  
  
 Figure 
 6.3↑
  is a sketch of a multiple access network. The host computers connected to the shared cable can
  
 communicate directly with each other. In a switched network such as figure 
 6.2↑
 , the communication must pass 
 through a switch. Ethernet Local Area Networks (LANs) are in part multiple access networks.
  
 In both kinds of networks, data must change format when it travels from one computer to another. To begin 
 with, the transmitted data resides in a set of pages in the DRAM memory of the source computer. However, the 
 physical link does not handle data the way the memory system does. In modern networks, data travels across the 
 physical link in packets or frames.
  
 Each packet is made up of headers with information about the source and destination in addition to the 
 payload, which is the data to be transmitted. In packet switched networks, such as Infiniband or the Ethernet, data 
 in DRAM memory is converted to a sequence of packets by the source. The packets are transmitted across the 
 network. The packets are reassembled at the destination and stored once again in DRAM memory.
  
 The change in data format is fundamental to packet switched networks. Although the amount of data to be
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.3MPI examples,"In this section, we get more deeply into MPI syntax. Our account of MPI syntax is not extensive. However, every 
 bit of syntax introduced is related to computer architecture. MPI can make programming a lot more challenging. 
 Yet users take the trouble to write MPI programs to make programs faster and solve bigger problems. MPI syntax 
 can look a bit monochromatic, and by looking at MPI syntax (or semantics), one cannot gain any sense of how 
 effective MPI facilities may be in speeding up programs. The discussion of Infiniband network architecture in the 
 previous section will help us learn MPI with a double focus on syntax and its effectiveness.
  
 We introduce standard optimizations such as load balancing and overlapping processor activity with 
 network activity. At a conceptual level, these optimizations are no more than common sense. At a practical level, 
 implementing such optimizations effectively can be quite challenging.
  
 Section 
 6.3.1↓
  is about sending and receiving of messages. Three versions of send and receive are discussed. 
 In persistent send/receive, the sender and the receiver preallocate memory using the MPI library. All the sends and 
 receives are from or to the same preallocated memory. In nonblocking send/receive, the send/receive function 
 returns quickly, and it is up to the user to use functions such as 
 MPI_Wait()
  and 
 MPI_Test()
  to confirm that the
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.3.1Variants of MPI send and receive,"Function calls that send and receive messages are the heart of the MPI library. An MPI user who masters sending 
 and receive messages, but not much more, can get by quite well.
  
 Although the syntax for sending and receiving messages is simple (relative to other MPI syntax), the MPI 
 function calls do not map to RDMA directly. MPI send is issued by a process without explicitly accounting for the 
 matching receive on the remote process. Indeed, the remote process may use multiple receives to match a single 
 send. If MPI function calls are mapped directly to the network, the network will have to use channel semantics. 
 Channel semantics implies the need for wasteful buffering and flow control inside the MPI library. RDMA calls 
 completely bypass the kernel and do not require the MPI library to implement flow control. However, MPI send 
 and receive do not map to RDMA function calls directly.
  
  
 In this section, we describe several variants of send and receive in the MPI library. We begin with the 
 variant that maps to RDMA in the best possible manner. The blocking variants are in wide use, but they waste
  
 nearly half the network bandwidth, as we will see.
  
  
  
  
 Figure 6.6Data exchange between two nodes.
  
  
  
 Figure 
 6.6↑
  shows the communication pattern we will implement in this section. We consider the situation
  
 with just two hosts. Each host has a send and a receive buffer. Each host sends its send buffer to the other host. The 
 other host copies the received message to its receive buffer.
  
 This type of exchange communication between two hosts is a good place to start. Even if the network has 
 many active hosts, the communication between any two hosts is likely to fit this pattern. On the Internet, TCP/IP 
 connections between a client and a server involves a pattern of communication similar to that shown in figure 
 6.6↑
 .
  
 The network bandwidth is measured at each host as the number of bytes that are transferred into or out of 
 that host in a single cycle. This manner of measuring network bandwidth is in line with the fat-tree architecture of 
 the Infiniband network. The fat-tree is designed to realize good bidirectional bandwidth at each port of each switch.
  
 Persistent exchange 
  
 In persistent communication as described here, user processes secure memory using the MPI library (or even a 
 facility such as 
 malloc()
  or 
 new[]
 ---but that is not recommended). Send and receive request objects are initialized
  
 using memory secured from the MPI library. Sends and receives are initiated using the same request objects. The 
 termination of the data transfer is also verified using the request objects.
  
 The 
 Exchg
  C++ class, which will be described, is an interface for persistent exchange of data between two
  
 hosts. The usage of C++ confirms to earlier practice in this book. The C++ class is mainly a convenient interface 
 that reduces burden on the programmer’s memory. Class constructors and destructors are convenient for setting up 
 and tearing down persistent communication between MPI processes.
  
 The 
 Exchg
  class is as follows:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5
  
 class Exchg{ 
  
 private:
  
  
  double *sendbuf;
  
  
  double *recvbuf;
  
  
  int bufsize;",NA
6.3.2Jacobi iteration,"The Jacobi iteration is a technique for solving linear systems that arise when certain partial differential equations 
 are discretized. Jacobi is an iterative method. The method is started off with an initial guess for the solution.
  
 Successive iterations improve the guess with each iteration, reducing the error by a factor greater than 1. The 
 iterations are stopped when the error is deemed to be low enough.
  
 Poisson’s equation 
 u
 xx
 +
 u
 yy
 =
 f
  is amenable to the Jacobi iteration once the equation is discretized using 
 finite differences or finite elements. The Jacobi iteration by itself is not a particularly effective method for solving 
 such differential equations. However, the Jacobi iteration can be supplemented by other ideas, such as multigrid, 
 and turned into a powerful method.
  
  
 The data in the Jacobi iteration is laid out as a matrix 
 a
 ij
 , 0 ≤ 
 i
 <
 m
  and 0 ≤ 
 j
 <
 n
 . The matrix is split between 
 MPI processes by column. Each MPI process gets a contiguous set of columns. The iteration is taken to be
  
 b
 ij
  ← 
 a
 i
  − 1,
 j
 +
 a
 i
 +1,
 j
 +
 a
 i
 ,
 j
  − 1
 +
 a
 i
 ,
 j
 +1 
 4
  
 .
  
 In this form, the iteration does not directly correspond to any partial differential equation. There is no right-hand 
 side vector corresponding to the source term in the partial differential equation. We have simplified the Jacobi
  
 iteration to focus better on certain aspects of its implementation. Nevertheless, we do have to specify boundary 
 conditions on the data 
 a
 ij
  at the boundary points 
 i
 =0 or 
 i
 =
 m
  − 1 or 
 j
 =0 or 
 j
 =
 n
  − 1. Our version of the Jacobi 
 iteration generates indices that fall outside the domain for boundary points 
 a
 ij
 . For example, 
 a
 i
  − 1,
 j
  has a negative
  
 row index if 
 i
 =0. We interpret all row indices modulo 
 m
  and all column indices module 
  
 n
 . Thus, row indices of
  
  − 1 and 
 m
  are interpreted as 
 m
  − 1 and 0, respectively, and column indices of 
  
  − 1 and 
 n
  are interpreted as 
 n
  − 1
  
 and 0, respectively. In differential equations jargon, we are assuming periodic boundary conditions here.
  
  
 This section illustrates how OpenMP constructs may be mixed with MPI message passing. There will be 
 only one MPI process on each node of a 12-core3.33 GHz SSE2 computer (see table 
 9.1↓
 ), but that MPI process
  
 will use OpenMP constructs to create multiple threads. The matrix 
 a
 ij
  is split across the compute nodes, and each 
 compute node will do a further split to assign part of the Jacobi iteration to each of its threads. As far as program 
 optimization is concerned, it will emerge that the MPI part is irrelevant. The program speed is determined by 
 optimizing the Jacobi iteration within a single node.
  
 The Jacobi class
  
 If the matrix 
 a
 ij
  is 
 m
 ×
 n
  globally, each MPI process out of a total of 
  
 P
  processes gets approximately 
 n
  ⁄ 
 P
  columns.
  
 The Jacobi iteration entails accessing entries in neighboring rows and columns. Thus, each process must exchange
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.3.3Matrix transpose,"So far in this chapter, we have considered data exchange between two MPI processes and cyclic transfer of data between 
 several MPI processes. Both of these are especially simple patterns of communication. In this section, we implement the 
 transposition of large matrices stored on several host computers connected by QDR Infiniband network. The number of 
 host computers ranges from 10 to 100 and the largest matrix handled has 
  
 5×10
 4
  rows and 
 5×10
 5
  columns. There is exactly one MPI process per host computer.
  
 In the matrix transpose, as we implement it, there is communication between every pair of host computers.
  
 Such a setting, where every possible line of communication is active, exercises additional features of the Infiniband 
 network and the MPI library implementation. The Infiniband architecture relies on the host channel adapter to 
 convert data stored in page frames in DRAM memory to Infiniband data packets, each of which is 2 KB or less on 
 the system we use. The MPI processes initiate RDMA read and write transactions by writing directly to queue pairs 
 that reside on the host channel adapter. When many lines of communication are active, there will be a large number 
 of queue pairs on each host channel adapter. The communication pattern tests the ability of the MPI library 
 implementation as well as the host channel adapter to manage a large number of queue pairs.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.3.4Collective communication,"The MPI programs we have written so far transfer data using versions of 
 MPI_Send()
  and 
 MPI_Recv()
 . Such
  
 function calls are made autonomously by individual processes, but their completion is subject to matching calls 
 made by other processes. The MPI library provides several function calls for collective communication. A 
 collective function call must be made by all processes in a group. The result of collective calls such as 
 Bcast
 ,
  
 Scatter
 , 
 Gather
 , and 
 Alltoall
  is data transfer involving all the processes.
  
 The only collective call we have encountered so far is 
 MPI_Barrier()
  in the function 
 unsafe()
  on page
  
 1↑
 . The barrier was inserted with the sole purpose of exhibiting MPI syntax. It had no effect on the function 
 unsafe()
 .
  
 In this section, we look at 
 MPI_Bcast()
 , 
 MPI_Scatter()
 , 
 MPI_Alltoall()
 , and 
 MPI_Reduce()
 . All these
  
 collective functions transfer data between participating processes and can, in principle, be implemented using send 
 and receive operations between processes. For broadcast, the send/receive implementation is too slow. 
  
  
 The all-to-all transfer of data effected by 
 MPI_Alltoall()
  can be implemented using send/receive.
  
 However, if we restrict ourselves to send/receive syntax introduced so far, the resulting implementation is not as 
 fast as the library function. In deriving an effective send/receive implementation, we encounter an aspect of 
 send/receive syntax of much value. Not surprisingly, this new syntax is of value because it could be related to a 
 facility provided by the Infiniband hardware.
  
 In typical MPI programs, the problem to be solved is split between several processes. Not infrequently, the 
 solution involves reduction operations such as finding the minimum, maximum, or sum of data scattered across 
 several computers and processes. The 
 MPI_Reduce()
  function may be implemented using send/receive operations,
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.3.5Parallel I/O in MPI,"In parallel I/O, a number of MPI processes simultaneously write to or read from the same file. Typical file systems 
 lock a file when it is being accessed by one process, which precludes other processes from accessing it. Parallel I/O 
 is possible only if the file system allows it.
  
 For parallel I/O to be meaningful, the file system must be capable of storing or 
 striping
  a single file across 
 multiple hard disks. If the entire file is stored on a single hard disk, accesses from multiple processes will have to 
 be serialized, and parallel I/O from multiple processes will not bring any great advantage.
  
 MPI syntax for parallel I/O is quite complicated. We step through the syntax once for writing and once for 
 reading a file in parallel. For both writing and reading, we exhibit and exercise MPI functionality for parallel I/O in 
 its simplest form. The function 
 write_mpi()
  defined below is called simultaneously by MPI processes to write to
  
 a file in parallel. The data to be written is the array of bytes 
 data[]
  of length 
 len
 , 
 data[]
  and 
 len
  being the first
  
 two arguments to 
 write_mpi()
 . The other two arguments to 
 write_mpi()
  are 
 fname
 , which is the name of the
  
 file, and 
 disp
 . Because many processes may write to the same file, each process must specify its 
 view
  of the file. A
  
 process’s view of the file is the part of the file that is visible to it. The final argument 
 disp
  specifies the beginning
  
 of the view as an absolute displacement (in bytes) from the beginning of the file. It must be noted that both 
 len
  and 
 disp
  are of type 
 long
  and not 
 int
 . An 
 int
  can count up to 2
 31
  − 1, which is not enough for files that can go up to
  
 terabytes or petabytes.
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14 
  
 15 
  
 16 
  
 17 
  
 18 
  
 19 
  
 20 
  
 21 
  
 22 
  
 23 
  
 24 
  
 25 
  
 26 
  
 27 
  
 28 
  
 29 
  
 30 
  
 31 
  
 32
  
 void write_mpi(void *data, long len, 
  
  
  
  char *fname, long disp){ 
  
  
 MPI_File ofile; 
  
  
 MPI_File_open(MPI_COMM_WORLD, 
  
  
  
   
 fname, 
  
  
  
   
 MPI_MODE_CREATE|MPI_MODE_WRONLY, 
  
  
   
 MPI_INFO_NULL, 
  
  
  
   
 &ofile); 
  
  
 char datarep[200]; 
  
  
 sprintf(datarep, ""native""); 
  
  
 MPI_File_set_view(ofile, 
  
  
  
   
  
  disp, 
  
  
  
   
  
  MPI_BYTE,
  
  
  
   
  
  MPI_BYTE,
  
  
  
   
  
  datarep,
  
  
  
   
  
  MPI_INFO_NULL); 
  
  
 long offset = 0; 
  
  
 int maxcount = 2000*1000*1000; 
  
  
 while(len > 0){ 
  
  
  
  
 int count = (len<maxcount)?len:maxcount; 
  
  
  
  
 MPI_File_write_at(ofile,
  
  
  
   
   
  offset,
  
  
  
   
   
  data,
  
  
  
   
   
  count,
  
  
  
   
   
  MPI_BYTE,
  
  
  
   
   
  MPI_STATUS_IGNORE); 
  
  
  
  
 offset += count; 
  
  
  
  
 data = ((char *)data+count); 
  
  
  
  
 len -= count; 
  
  
 } 
  
  
 MPI_File_close(&ofile); 
  
 }
  
  
  
 On line 3, 
 ofile
  is defined to be of type 
  
 MPI_File
 . 
 MPI_File
  is defined as a pointer to a 
 struc
 t in 
 mpi.h
 .
  
 The 
 struct
  it points to will hold information about file attributes, file size, and file location.
  
 The 
 MPI_File_open()
  call, which spans lines 4 through 8, is a collective call. Accordingly, its first
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.4The Internet,"The main function of the Internet, as it exists today, is to connect people. There is no doubt that the Internet is one 
 of the most successful technologies. The Internet is not the result of a single act of invention. Even now its design is 
 evolving to accommodate new uses and technologies.
  
 Ever since the invention of the transistor, computing technology has been driven simultaneously by market 
 forces and the possibility of technological innovation. The market forces driving the Internet today are among the 
 most powerful. At the same time, a number of entities are well organized for innovation. The scope for new 
 technologies in this area appears considerable. One may struggle to get bandwidth of even 1 MB/s between 
 Michigan and Texas today, but that will surely change in the not too distant future.
  
 It would be folly for scientific programmers to ignore the Internet and think that MPI is the end of the world 
 in networking as far as scientific applications are concerned. The volume of investment in the Internet is far, far 
 greater, and so is the potential for technological innovation. The highly decentralized nature of the Internet gives it 
 powerful advantages over centralized technologies such as MPI and Infiniband.
  
 The Internet will intrude into scientific computing in more and more ways. The maintenance of databases is 
 one area where that has already happened. In many scientific applications, moving data from one place to another is 
 too expensive, leaving no choice but to coordinate data analysis over the Internet.
  
  
 In this section, we give an overview of the Internet at the level of the TCP/IP protocol. The TCP/IP protocol 
 is packetized. Indeed, it was one of the first packetized network protocols. In section 
 6.4.1↓
 , we outline the manner
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.4.1IP addresses,"To the average person, the Internet is a place for news, commerce, and entertainment. To the programmer, the 
 Internet is a software interface to produce and consume TCP/IP packets. The variety of information exchanged over",NA
6.4.2Send and receive,"Every network interface comes down to sending and receiving data. We use the following function for sending
  
 data:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5
  
 int block_send(int sockfd, void *buf, int len){ 
  
 int total_sent = 0; 
  
  
 int num_sends=0; 
  
  
 while(total_sent < len){ 
  
  
  
 int ns = send(sockfd, buf, len-
  
 total_sent, 0);
  
 6
  
 }
  
 buf = (char *)buf+ns;
  
 7
  
 total_sent += ns;
  
 8
  
 num_sends += 1;
  
 9
  
 }
  
 10
  
 return num_sends;
  
 11
  
  
  
 This function sends 
 len
  bytes out of the buffer 
  
 buf[]
 . Where the data gets sent to is determined by the
  
 socket file descriptor 
 sockfd
  (line 1). To send data in the form of TCP/IP packets, the Linux kernel needs the IP 
 address of the destination as well as the port numbers at the source and the destination. The sending and receiving 
 applications cannot be identified without the port number. Although the socket file descriptor 
 sockfd
  is a mere 
 int
 , the Linux kernel can use it to look up the IP address of the destination as well as the port numbers at the 
 source and the destination. The information is hidden from the application program, although there are ways to get 
 hold of it. The manner in which the socket file descriptors are set up is described later.
  
  
 The function 
 block_send()
  uses the Linux system call 
  
 send()
  (line 5). The first argument is the socket 
 file descriptor, the second argument is the buffer, and the third argument is the number of bytes to be sent. The final 
 argument is a flag, which is given as 0 on line 5. Other flag values such as 
  
 MSG_MORE
  may be used to alter socket 
 behavior. The 
 send()
  may not transmit all the data. It returns the number of bytes transmitted and  − 1 on error.",NA
6.4.3Server,"The server described here receives a sequence of double-precision numbers from its client and sends back their 
 partial sums. Before we get to partial sums, we must explain how the server listens for connections on a designated 
 port. Special port numbers are designated for major applications. For example, http uses port 80 and https uses port 
 443. On Linux, the list of port numbers in use can be found in the file 
 /etc/services
 . The port number for our
  
 series summing server is taken to be 28537.
  
  
  
 const char* PORTNUM=""28537"";
  
  
  
 To begin with, the server opens a socket to this port and marks the socket for listening.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11
  
 int bind2port(const char* portnum){ 
  
  
 struct addrinfo hint; 
  
  
 hint.ai_family = AF_UNSPEC; 
  
  
 hint.ai_socktype = SOCK_STREAM; 
  
  
 hint.ai_flags = AI_PASSIVE; 
  
  
 struct addrinfo *llist; 
  
  
 getaddrinfo(NULL, portnum, &hint, &llist); 
  
 int sock2port = socket(llist->ai_family,
  
  
  
  
  llist->ai_socktype,
  
  
  
  llist->ai_protocol); 
  
  
 bind(sock2port,llist->ai_addr,llist-
  
 >ai_addrlen);
  
 12
  
 }
  
 int backlog=10;
  
 13
  
 listen(sock2port, backlog);
  
 14
  
 int yes = 1;
  
 15
  
 setsockopt(sock2port, SOL_SOCKET, SO_REUSEADDR, 
  
 16
  
  &yes, sizeof(int));
  
 17
  
 return sock2port;
  
 18
  
  
  
  
 On line 5, the 
 ai_flags
  field of 
 hint
  is given as 
 AI_PASSIVE
  to indicate that the socket will accept
  
 connections from any Internet address. The 
 getaddrinfo()
  call on line 7 uses the hint to prepare another 
 struct
  
 addrinfo
  object 
 llist
 . The second argument to 
 getaddrinfo()
  is the service. It can be a character string such as
  
 “http” or “ssh” or it can be a character string representing a port number as in “28537.”",NA
6.4.4Client,"The client mirrors the server in some ways but differs in others. The client begins by establishing a connection with
  
 the server.
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8
  
 int connect2server(const char *server, 
  
  
  const char *portnum){ 
  
  
 struct addrinfo hint; 
  
  
 memset(&hint, 0, sizeof(hint)); 
  
  
 hint.ai_family = AF_UNSPEC; 
  
  
 hint.ai_socktype = SOCK_STREAM; 
  
 struct addrinfo *llist; 
  
  
 getaddrinfo(server, portnum, &hint,
  
 &llist);
  
 9
  
 int sock2server = socket(llist-
  
 >ai_family, 
  
 10
  
  llist-
  
 >ai_socktype,
  
 11
  
  llist-
  
 >ai_protocol);
  
 12
  
 }
  
 connect(sock2server, llist->ai_addr, 
  
 13
  
  llist->ai_addrlen);
  
 14
  
 freeaddrinfo(llist);
  
 15
  
 return sock2server;
  
 16
  
  
  
  
 The argument 
 server
  (line 1) is the name of machine on which the server is running. It could be
  
 login.univ.edu
 , for example. The hint specifies a TCP connection (line 6) but not the address family (line 5). The 
 call to 
 getaddrinfo()
  on line 7 gives the name of the server (as in the program for printing IP addresses) but also 
 specifies a port number. The specified port number is the second argument to this function. It should be the 
 character string “28537” to agree with our earlier convention. It opens a TCP socket on lines 9, 10, and 11. The 
 socket 
 sock2server
  is connected to the server on lines 12 and 13 using the system call 
 connect()
 . There is much 
 subtlety in the way connections are established and terminated over the Internet that is not discussed here.
  
  
 The system call 
 connect()
  is the client-side counterpart of 
  
 accept()
 . The second argument to 
 connect() 
 is of type 
 const struct sockaddr *
 . The 
 struct
  it points to (
 *llist
 ) has information about the IP address of the 
 server as well as the port number on which it is expected to be listening. That 
 struct
  of course is constructed by 
 getaddrinfo()
  (line 8). In general, 
  
 getaddrinfo()
  returns a linked list of 
 struct
 s. In principle, for robustness, 
 we must loop through entries of that linked list until a connection attempt succeeds with one of them.",NA
6.4.5Internet latency,"To find the network latency, we take the block size to be 100 double-precision numbers or 800 bytes. Such a block 
 fits comfortably within a single TCP/IP packet smaller than 1.5 KB (Ethernet MTU). Thus, the client in effect sends 
 a single TCP/IP packet and waits for the partial sums, which arrive in a single TCP/IP packet.
  
 All measurements were performed by running the server at the Texas Advanced Computing Center (TACC) 
 and the client at the University of Michigan (UM). The median time for send+recv was close to 55 milliseconds in 
 several trials. Therefore, we may take the round trip latency between UM and TACC to be 55 milliseconds.
  
 Understanding why the latency is 55 milliseconds is a fascinating problem. The distance from UM to TACC 
 is more than 1,000 miles. Therefore, the velocity of light by itself seems to account for more than 10 milliseconds 
 of the latency.
  
 In fact, things are much more complicated. The Linux command 
 traceroute
  shows that the packets take a 
 different route from UM to TACC and the way back.
 [116]
  A packet sent from UM to TACC travels from the 
 author’s desktop to a router located 2.3 miles almost immediately south (after several hops). The router 2.3 miles 
 south of the author’s desktop is managed by Merit Networks. From that router, it jumps to a router in Lubbock, 
 Texas, covering most of the distance from UM to TACC in a single hop. The router in Lubbock, Texas, is managed 
 by LEARN. The packet travels from Lubbock, Texas, to a TACC router in several quick hops. Much of the latency 
 in the journey from UM to TACC seems to be incurred at routers located 2.3 miles south of the author’s UM 
 desktop and at the router in Lubbock, Texas.
  
  
 On the way back, from TACC to UM, the packets take an entirely different route. The TACC servers send 
 the packet to a router in suburban Los Angeles. The routers in Los Angeles are managed by National LambdaRail.
  
 After several quick hops in Los Angeles, the packet makes a long trip to a router located 2.3 miles south of the 
 author’s desktop at UM. Although the postal address is the same, the IP address of this router is not the same as the 
 one that sent packets to Lubbock, Texas. After several quick hops, the packet lands on the author’s desktop. Much 
 of the latency during the return trip (excluding latency due to finite speed of light in optical fibers) seems to be 
 incurred at a router in Los Angeles and another router 2.3 miles south of the author’s desktop. It is difficult to be 
 certain without access to the routers.
  
 For a lot of Internet traffic, the return route is not the same as the forward route. If a company that owns a 
 router recognizes that a packet is destined for routers owned by some other company, it gets rid of the packet as 
 quickly as possible.
  
 The long round trip from UM to TACC to Los Angeles back to UM is almost 4,000 miles. Velocity of light 
 might account for almost 20 milliseconds of the observed latency. That still leaves around 35 milliseconds of 
 latency for us to explain.
  
 The routers buffer a lot of IP packets using linked lists. One may imagine a router that picks up a packet and 
 almost immediately routes it to the appropriate network interface. If routing were to behave in that manner, very 
 little latency would accumulate at the routers. However, it is impossible for a router that executes flow control and",NA
6.4.6Internet bandwidth,"Block Size
  
 1 May (Noon)
  
 3 May (2 pm)
  
 3 May (3 pm)
  
 80 KB
  
 0.8 MB/s
  
 1.6 MB/s
  
 0.8 MB/s
  
 800 KB
  
 0.9 MB/s
  
 0.9 MB/s
  
 0.9 MB/s
  
 8000 KB
  
 *
  
 1.9 MB/s
  
 2.5 MB/s
  
 Table 6.13Internet bandwidth between UM and TACC in 2014.
  
  
  
 Table 
 6.13↑
  shows the bandwidth recorded between UM and TACC on three different occasions. In each of the
  
 trials reported, 160 MB of data was sent from the client to the server. The server sent back 160 MB of data. The 
 client and server traded data in varying block sizes reported in the table. The bandwidths vary much more from trial 
 to trial than latencies.
  
 Host computers can inject packets into the network at great speed. If an application issues a 
 send()
  system
  
 call, the data to be sent must fall through the TCP layer, the IP layer, the ARP layer, and the device driver before it 
 reaches the network. The Linux TCP/IP stack is heavily optimized. One major optimization in Linux as well as 
 many other implementations is to avoid repeated copies. In fact, the data is copied just once from the application’s 
 user space to kernel buffers. The kernel buffers for TCP/IP packets are of the type 
 struct sk_buff
 . This 
 struct
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
6.5References,NA,NA
Bibliography,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
7Special Topic: The Xeon Phi Coprocessor,"The Top 500 organization
 [118]
  has published a list of the top supercomputers every year since 1993. This list has 
 been a huge contribution to scientific computing. The considerable profits in computer technology lie in 
 entertainment, everyday convenience, creating new enthusiasms, reinforcing preexisting ones, and commerce. The 
 profits from scientific computing are almost negligible. The wide publicity that Top 500 generates has ensured that 
 scientific computing is not ignored by major corporations.
  
 The Top 500 list ranks computers on the basis of flop (floating point operation) rate realized in solving large 
 linear systems using pivoted LU factorization (the LINPACK benchmark). The computation must be in double-
 precision. The flop rate has increased from 60 GFlops/s ( 6×10
 10
  flops/s) in 1993 to 34 PFlops/s ( 3.4×10
 16
 ) in 
 2015. A total of 500 computers are listed in the rankings.
  
 The relationship between scientific computing and mainstream computing is symbiotic, although the latter is 
 dominant. Many technologies pertaining to networks, visualization, and other topics have traveled from mainstream 
 to scientific computing. Technologies such as wide register sets were first exploited in scientific computing, helping 
 large computing clusters climb up the Top 500 list. Later these technologies have found applications in more 
 mainstream areas such as image processing and data analysis.
  
 Co-processors with peak ratings in excess of a tera-flop were first introduced to market by NVIDIA. Such 
 coprocessors are particularly efficacious for dense matrix computations and, in particular, for the LINPACK 
 benchmark used to rate the TOP 500 supercomputers. In 2015, as in some years before, the biggest supercomputers 
 have no choice but to use these coprocessors to hold their place in the TOP 500 list. Because the TOP 500 list is 
 widely publicized, so widely that the topmost positions are a source of national pride, the coprocessors have
  
 impressive marketing potential. Intel could not afford to ignore this marketing angle.
  
  
  
  
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
7.1Xeon Phi architecture,"In this section, we give an overview of the Xeon Phi’s architecture in three steps. The first step is to calculate the 
 peak floating point bandwidth of the Phi in section 
 7.1.1↓
  and compare it to that of its AVX host. The Phi turns out 
 to be better by a factor of 3. This is perhaps the most favorable comparison one can make for the coprocessor. 
 Applications in which that factor is approached are few.
  
 The second step in section 
 7.1.2↓
  is to introduce the thread picker. The thread picker is a hardware module 
 on each Phi core. During every cycle, it picks one of four threads. Section 
 7.1.2↓
  also introduces the ZMM registers 
 in the context of the thread picker. Section 
 7.1.2↓
  explains how to compile and run a Phi program in native mode.
  
 Compiling and running a Phi program in native mode differs very little from what is usual on mainline 
 computers. In fact, almost every OpenMP program of chapter 
 5↑
  can be run on the Phi with little effort and almost 
 no modifications. Thus, in section 
 7.1.3↓
 , we reuse the same programs to measure the Phi’s latency and bandwidth 
 to memory.
  
 The Phi’s bandwidth to memory is not even twice that of its AVX host. The copy bandwidth is better by a 
 factor of 1.72. This improvement in bandwidth comes at the expense of a latency that is nearly four times as high, 
 although on the positive side the Phi’s memory system is uniform with no distinction between near and far memory. 
 Co-processors such as the Phi and NVIDIA’s GPUs are worthless for programs that rely on dynamic data structures 
 such as linked lists because such programs are exposed to latency to memory. However, they can yield a speedup in 
 programs whose memory accesses are regular and predictable.",NA
7.1.1Peak floating point bandwidth,"Figure 
 7.1↑
  shows a typical setup for a Xeon Phi coprocessor. The host computer is a 2.7 GHz AVX machine (see 
 table 
 9.1↓
  for its full name). It has 16 cores, each equipped with 256-bit YMM registers, wide enough for four 
 double-precision numbers. Each core can issue a 
 vaddpd
  and a 
 vmulpd
  instruction simultaneously in each cycle.
  
 Therefore, the peak floating point bandwidth is:
  
 16 cores×8 flops/cycle/core×2.7 billion cycles/sec=345.6Gflops/sec.
  
 The host is quite powerful by itself.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
7.1.2A simple Phi program,"From here onward, we use Phi device and MIC device interchangeably. MIC is the acronym of Many Integrated 
 Cores and includes the microarchitecture of Phi-like devices. We begin by explaining why the right number of 
 threads on a MIC/Phi device is typically four times the number of cores.
  
 Thread pickers and ZMM registers
  
  
  
  
 Figure 7.2A preliminary view of the architecture of a Xeon Phi core.
  
  
  
 Processor cores of mainline computers run one thread at a time.
 [124]
  The operating system must be invoked
  
 to switch from one thread to another at a cost of around 10
 4
  cycles.
  
 In contrast, four threads can be scheduled to a single Phi core.
 [125]
  All four cores can remain active, and the 
 thread picker, which is a hardware module, switches between them in a single cycle (see figure 
 7.2↑
 ). During every 
 cycle, one of the four threads is picked to run. The thread picker prefers round-robin scheduling, where it cycles 
 between the four threads, and it never picks the same thread in two successive cycles. Therefore, one must create at 
 least two threads for every core to prevent alternate cycles from being wasted. Much of the time it is best to create 
 four threads for every core. On our Phi card, with 61 cores, the ideal number of threads is 244.
  
 7.2↑
 ). In fact, the number of registers in hardware is really 
 Each Phi core has 32 ZMM registers (see figure 
  
 32×4. Each of the four threads scheduled to a Phi core has its own register file so that the hardware can switch 
 between threads in a single cycle. This replication of registers is another reason to set the number of threads to be 
 four times the number of cores.
  
 In a thread or program, the registers may be referred to as ZMM0 through ZMM31. The Phi does not 
 support XMM or YMM registers. The ZMM registers must be used for floating point operations. Each ZMM 
 register may be thought of as a vector of eight double-precision numbers.
  
 Hello world
  
 Our first Phi program is no different from any OpenMP program.
  
  
  
 #include <stdio.h> 
  
 #include <omp.h>
  
 int main(){ 
  
 #pragma omp parallel",NA
7.1.3Xeon Phi memory system,NA,NA
7.2Offload,"The offload mode
 [127]
  has been a stable way to program the Phi device since the beginning. The “hello” program 
 has shown us that the syntax for writing Phi programs is virtually identical to the syntax for writing ordinary C/C++ 
 programs. In offload mode, the master thread of the main program is assumed to be on the host. However, the 
 program holds several segments that are meant to be outsourced to the Phi devices.
  
 In section 
 7.2.1↓
 , we write a simple function called 
  
 mic_init()
 . Much of the time, the only thing that
  
 needs to be changed when an OpenMP program is run on a Phi/MIC device is the number of threads. Typically, the 
 Phi has more cores, and the number of threads on the Phi is four times the number of cores. The 
 mic_init()
  
 function sets the number of threads correctly for the host as well as the Phi/MIC device once and for all. 
  
 Section 
 7.2.2↓
  introduces the 
  
 target(mic)
  compiler directive. This directive, which is currently
  
 implemented only by the Intel compilers, may be applied to function definitions as well as to globally defined 
 variables. Any function that is prefixed with 
 target(mic)
  will be compiled to run on Phi/MIC devices as well as
  
 the host.
  
 Section 
 7.2.3↓
  uses the familiar Leibniz example to show how the Phi/MIC device can communicate with 
 the host. The 
 offload
  compiler directive is supported by clauses such as 
 in
 , 
 out
 , 
 nocopy
 , and 
 inout
  to enable
  
 communication between the coprocessor and the host. Variables are automatically copied into the Phi when an 
 offload
  region is entered and are copied back when the region exits. Arrays are sent back and forth using clauses
  
 appended to the 
 offload
  directive.
  
 The 
 offload
  extension supports addressing an array 
 v[]
  on the host and its copy on the Phi/MIC device
  
 using identical syntax. If 
 v[i]
  refers to an entry on the host, 
 v[i]
  refers to its copy or mirror image on the Phi
  
 device. The 
 offload
  extension supports partial views of an array. For example, we may offload 
 v[offset:len]
  
 to copy the segment
  
  
  
 v[offset], v[offset+1], ...,v[offset+len-1]",NA
7.2.1Initializing to use the MIC device,"The host computer can use the function 
 mic_init()
  defined below to find out the number of MIC devices and set
  
 the default number of OpenMP threads. In all the offloading programs, it is assumed that 
 mic_init()
  is called at
  
 the beginning.
  
  
  
  
  
 1 
  
 #include <offload.h> 
  
  
  
 2 
  
 #include <omp.h> 
  
  
  
 3 
  
  
  
 4 
  
 int gl_host_nthreads = -1; 
  
  
  
 5 
  
  
 int gl_mic_nthreads = -1; 
  
  
 6 
  
  
  
 7 
  
 void mic_init(int &nmic){ 
  
  
  
 8 
   
 char *s = getenv(""OMP_NUM_THREADS""); 
  
  
 9 
   
 assrt(s != NULL); 
  
 10 
   
 gl_host_nthreads = atoi(s); 
  
 11 
   
 omp_set_num_threads(gl_host_nthreads); 12 
  
 13 
   
 nmic = _Offload_number_of_devices(); 
  
 14 
  
  
  
 s = getenv(""MIC_OMP_NUM_THREADS""); 15 
  
  
 assrt(s != NULL); 
  
 16 
   
 gl_mic_nthreads = atoi(s); 
  
 17 
   
 for(int i=0; i < nmic; i++) 
  
 18 
  
 omp_set_num_threads_target(TARGET_MIC, i, 
  
 19
  
 }
  
  gl_mic_nthreads);
  
 20
  
  
  
  
 On line 8, this program reads 
 OMP_NUM_THREADS
 . On line 11, the default number of OpenMP threads on the
  
 host is set to the value of this environment variable. The default number of threads on the host is globally visible 
 thanks to the definition on line 4.
  
 The function used on line 13 to find out the number of MIC devices is declared in 
 offload.h
  (which is
  
 included on line 1). The environment variable prefixed with 
 MIC
  is read on line 14, and the for-loop on lines 17 to
  
 19 sets the default number of OpenMP threads on every MIC device to the value of that environment variable. 
  
 Simply defining 
 OMP_NUM_THREADS
  and the same environment variable with the prefix 
 MIC
  is enough to set
  
 the default number of threads correctly. The function 
 mic_init()
  makes that setting explicit within the structure
  
 of the program.",NA
7.2.2The ,target(mic),NA
 declaration specification,"The 
 icpc
  compiler has the ability to compile a function written in C/C++ to produce assembly for both the host
  
 computer and MIC device.
  
 The function definition below is prefixed with 
 __declspec(target(mic))
 . It is defined in the compilation
  
 unit 
 leibniz_init.cpp
 .",NA
7.2.3Summing the Leibniz series,"We will discuss four different programs to sum the Leibniz series to compute 
 π
 . All four programs initialize the array 
 v[]
  to terms of the series 1 − 1 ⁄ 3+1 ⁄ 5 − 
 ⋯
  and then scale each term by 
  
 4
 1 ⁄ 3
  three times before summing 
 the series. Together the programs expose nearly all the offload syntax one needs to know. Work is offloaded using the 
 offload
  directive. Communication between the host and the Phi device is controlled using 
 in
 , 
 out
 , 
 inout
 , or 
 copy
  
 clauses.
  
  
 Each program works as follows. To begin with, the array 
 v[]
  is initialized with entries of the series 1 − 1 
 ⁄ 3+1 ⁄ 5 − 
 ⋯
  on the host using 
 leibniz_init()
  defined above. Subsequently, three calls are made to 
 hostmic_scale()
 , also defined above, and a single call to 
  
 hostmic_sum()
 , which too was defined above.
  
  
 The three calls to 
 hostmic_scale() 
 and the one call to 
  
 hostmic_sum()
  may be on either the host or Phi 
 device. Each call to 
 hostmic_scale()
  multiplies the terms of the series by 4
 1 ⁄ 3
 , and the effect of three calls is to 
 multiply by 4. The call to 
 hostmic_sum()
  will therefore return an approximation to 
  
 π
 .
  
  
  
 hostmic_scale()
  and the call to 
 hostmic_sum()
  are all offloaded to the 
 In 
 leibniz1()
 , the three calls to 
  
 Phi device. The syntax in this case is the simplest.
  
 The function 
 leibniz2()
  is similar, except for one big difference. When the master thread encounters a 
 typical offload region, such as the offload region in 
 leibniz1()
 , it blocks until the Phi device exits the offload 
 region. In 
 leibniz2()
 , we show syntax that allows the master thread to offload without blocking and then wait for 
 the offload region to complete. That type of syntax is essential to make the Phi coprocessor and the host work in 
 parallel.
  
 The function 
 leibniz3()
  exhibits facilities for communication between the host and the Phi coprocessor in 
 considerable detail. In this function, the first two scalings are done on the Phi device and the third scaling on the 
 host. The sum is then computed on the Phi device.
  
  
 Finally, the function 
 leibniz4()
  shows how multiple Phi devices may be used in parallel. In 
 offload 
 directives, syntax such as 
 v:length(n)
  is used to offload 
 v[0,...,len-1]
  to the coprocessor. Parallelism
  
 between Phi devices is facilitated by the syntax 
 v[offset:len]
 , which offloads
  
  
  
 v[offset, offset+1,...,offset+len-1]
  
  
  
 to the coprocessor.
  
 Offloading from the processor to the Phi
  
 The first of the four functions 
 leibniz1()
  is the simplest.
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7
  
 void leibniz1(){ 
  
  
  
 int nmic; 
  
  
  
 mic_init(nmic); 
  
  
  
 long n = 1l*1000*1000*800; 
  
  
  
 long nbytes = n*8;
  
  
  double* v = (double *)_mm_malloc(nbytes, 64); 
  
  
 leibniz_init(v, n);
  
  
 8
  
 double sum; 
  
 \
  
 9
  
 10
  
 #pragma offload target(mic:0) 
  
 11
  
 mandatory 
  
 \
  
 12
  
 in(v:length(n) align(64)) 
  
 13
  
 {
  
 14
  
 hostmic_scale(v, n);
  
 15
  
 hostmic_scale(v, n);
  
 16
  
 hostmic_scale(v, n);
  
 17
  
 sum = hostmic_sum(v, n);
  
 18
  
 }",NA
7.2.4Offload bandwidth,"If the Phi/MIC devices are used in offload mode, there is a significant cost to the data transfer between the host and 
 MIC devices. The PCIe bus, which connects the host with the MIC devices (via the chipset; see figure 
 7.1↑
 ), has 
 low bandwidth. Its bandwidth is more than an order of magnitude less than the bandwidth to memory of the MIC 
 devices or even the host. Data transfer between the host and the Xeon Phi coprocessors imposes serious limits on 
 the advantages as well as utility of the coprocessor model.
  
 To time the transfers between the host and the MIC devices, we use functions such as the following:
  
  
  
 void xfer_in(double *v, long n, int nmic){ 
  
 long fst[nmic+1]; 
  
  
 for(int i=0; i <= nmic; i++) 
  
  
  
 fst[i] = i*n/nmic;
  
  
 for(int mc=0; mc < nmic; mc++){ 
  
  
  
 long shft = fst[mc]; 
  
  
  
 long len = fst[mc+1] - fst[mc]; 
  
 #pragma offload target(mic:mc) 
  
  \ 
  
 in(v[shft:len]:align(64) alloc_if(0) free_if(0))\ 
  
 signal(1) 
  
  
  
 dummy(v+shft, len);
  
 }
  
  
 for(int mc=0; mc < nmic; mc++){ 
  
 #pragma offload_wait target(mic:mc) wait(1) 
  
 } 
  
 }
  
  
  
 This function transfers data into 
 nmic
  devices. Analogous functions transfer data out of the MIC devices or
  
  
 use the 
 inout
  clause to transfer data both into the MIC devices during entry to the offload region and out at exit.
  
  
  
  
  
 Cycles/Byte 
 (One MIC)
  
 Bandwidth 
 (One MIC)
  
 Cycles/Byte 
 (Two MICs)
  
 Bandwidth 
  
 (Two MICs)
  
 IN
  
 0.42
  
 6.48 GB/s
  
 0.40
  
 6.75 GB/s
  
 OUT
  
 0.42
  
 6.48 GB/s
  
 0.40
  
 6.75 GB/s
  
 INOUT
  
 0.81
  
 3.24 GB/s
  
 0.77
  
 3.51 GB/s
  
 Table 7.3Offload bandwidths in transferring data to one MIC device or simultaneously to two MIC devices. In 
 cycles/byte, the cycles are of the 2.7 GHz AVX host.",NA
7.3Two examples: FFT and matrix multiplication,"Section 
 7.3.1↓
  looks at the Fast Fourier Transform (FFT) implemented inside the MKL library. For small 
 n
 , the FFT 
 on the Phi/MIC device is nearly twice as fast as it is on the AVX host. For large 
 n
  such as 
 n
 =2
 26
 , the FFT does not 
 appear to be well optimized yet and is slower on the Phi/MIC device by more than a factor of 8. The fact that even 
 the FFT reaches only 15% of the peak floating point bandwidth shows how misleading that metric can be.
  
 Section 
 7.3.2↓
  is a look at the Phi instruction pipeline in the context of matrix multiplication. We write a 
 microkernel in assembly that reaches 50% of the peak floating point bandwidth. The discussion is carried far 
 enough to show that the principles of instruction pipeline optimization, explained in depth in chapter 
 3↑
 , apply to 
 Phi/MIC devices. Not all the techniques pertinent to matrix multiplication and developed in earlier chapters are 
 reviewed. To approach the peak bandwidth more closely, as the MKL library does, would require an application of 
 the full range of techniques.",NA
7.3.1FFT,"The FFT is a good basis for comparing the Phi against its AVX host. It is one of the most frequently employed 
 scientific algorithms. Unlike in the LINPACK benchmark employed to rate supercomputers, the cost of memory 
 accesses cannot be completely hidden.
  
 The same class 
 FFT
  was employed to time FFTs on the MIC device as well as the host.
  
 __declspec(target(mic)) 
  
 class FFT{ 
  
 private: 
  
  
 DFTI_DESCRIPTOR_HANDLE handle; 
 public: 
  
  
 FFT(int n, long count); 
  
  
 ~FFT(); 
  
  
 void fwd(double *f){ 
  
  
  
 DftiComputeForward(handle, f); 
  
 } 
  
  
 void bwd(double *f){ 
  
  
  
 DftiComputeBackward(handle, f); 
  
 } 
  
 };",NA
7.3.2Matrix multiplication,"Algorithms that are capable of approaching the theoretical peak are not many. These algorithms must involve a 
 great number of arithmetic operations relative to the data they access. In addition, their structure must permit hiding 
 the cost of multiple accesses of the same data item. Dense numerical linear algebra is the main source of such 
 algorithms.
  
 The number of arithmetic operations in 
 C
 =
 C
 +
 AB
 , where 
 A
 ,
 B
 ,
 C
  are 
 n
 ×
 n
  matrices, is 2
 n
 3
 . The number of 
 double-precision numbers accessed is only 3
 n
 2
 . For large 
 n
 , a great number of arithmetic operations are carried out 
 for every byte that is accessed. It is true that every entry of 
 A
 , 
 B
 , or 
 C
  is involved in 
 n
  operations, but we may hide the 
 cost of memory accesses, as shown in chapter 
 4↑
 .
  
  
 Even if the cost of memory accesses is ignored, writing a microkernel that achieves peak floating 
 performance, with all the data items in L1 cache, can be a formidable challenge. It requires knowledge of the 
 register set as well as the instruction pipeline. In this section, we look at a small part of the Xeon Phi’s instruction 
 set
 [129]
  and then write a microkernel that achieves 45% of the peak floating point throughput. Although this 
 microkernel falls short of the best possible by a factor of 2, it still gives a good understanding of the Xeon Phi’s 
 architecture, why it can be so fast, and why it can be so hard to write programs that approach its top speed. 
 Difficulty in approaching peak floating point throughput is characteristic of all modern architectures. Later we turn
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
8Special Topic: Graphics Coprocessor ,NA,NA
Programming Using CUDA,"Graphics libraries such as DirectX and OpenGL are widely used in the computer gaming industry. The graphics 
 libraries provide a number of functions for rendering, shading, texture manipulation, and similar tasks. Graphics 
 devices accelerate the execution of such library functions. If a suitable graphics card and driver are installed, these 
 functions are sent from the processor to the graphics card for faster execution.
  
 The graphics cards are powerful processors in their own right. A high-end graphics card may have more than 
 a billion transistors, same as a processor package. The instruction set and design of the graphics processor reflects 
 its principal function. The graphics processor prefers to think in terms of pixels, and the instruction set architecture 
 supports threading at a very fine level. If there is 1 GB of data to be handled, an OpenMP program may divide the 
 data between a dozen threads. For a graphics processor program, it is natural to split the data between 10
 4
  or 10
 5
  or 
 even more threads.
  
 Despite the focus on pixels and images, the capabilities of graphics processors are fairly general. Such is the 
 diversity and changeability of functions in DirectX and other libraries that it makes little sense for a graphics 
 processor to attempt to hardwire the functions directly. Instead the instruction set architecture is built up using 
 primitives, many of which are fairly general purpose. The graphics device is not flexible enough to run a web server 
 or word processor effectively, but may be used as a coprocessor in many scientific applications.
  
 As long as graphics devices and drivers are set up mainly to run graphics library functions, it is very hard to 
 use them for scientific computing. In 2007, NVIDIA introduced the Compute Unified Device Architecture (CUDA) 
 framework. CUDA added software layers to the device drivers and the GNU C/C++ compiler to greatly simplify 
 the task of programming graphics coprocessors for scientific use.
  
 The use of graphics coprocessors in scientific computation became an immediate sensation. Although there 
 was some hype, it is undeniable that graphics coprocessors were significantly faster for many important 
  
 applications. The innovative design of NVIDIA’s Tesla, Fermi, and now Kepler and Maxwell devices gives a hint 
 of what can be done with a billion transistors to go beyond the constraints of the x86 architecture and its reliance on 
 backward binary compatibility. Systems with NVIDIA coprocessors began to appear near the top of the Top 500 list 
 in 2010.
  
 Intel, the foremost champion of the x86 architecture, has closed the gap considerably to the extent that the 
 topmost supercomputer in 2015 uses Intel processors as well as coprocessors. One advantage enjoyed by the 
 CUDA framework is its compilation model, which is the topic of section 
 8.2↓
 . Graphics devices attain backward 
 compatibility using an intermediate assembly language that can be mapped to a variety of instruction sets. 
 Removing the constraint of backward binary compatibility creates greater room for rapid innovation in the design 
 of the graphics hardware. It may lower the cost of design as well.
  
 A disadvantage is that programming graphics coprocessors is harder, much harder, than programming 
 processors and likely to remain that way. In OpenMP, we may split the problem across threads or processes and 
 write much of the program as if it were a sequential program. It is true that subtleties of parallel programming can 
 never be eliminated entirely. Yet the difficulties of parallel programming may be localized to a considerable extent,
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
8.1Graphics coprocessor architecture,"The graphics device sits next to the processor node, as in figure 
 8.1↑
 . The K20 graphics device we use has 13 
 streaming multiprocessors. Each of these is roughly comparable to a processor core of the 16-core 2.7 GHz AVX 
 host (for the full name of this computer, see table 
 9.1↓
 ).
  
 Internally, a streaming multiprocessor is different from a processor core. In section 
 8.1.1↓
 , we begin by 
 examining the capability of the graphics device using a simple program. This program is run on the host and 
 involves none of the complexity of CUDA programming. This program may be used to detect a number of 
 available parameters, such as the major and minor revision number, clock rate, and available graphics memory.
  
  
 Sections 
 8.1.2↓
  and 
 8.1.3↓
  introduce utilities that simplify programming. These utilities, too, run on the host. 
 Section 
 8.1.2↓
  presents a simple class for moving data to and back from the graphics device. The timing class in 
 section 
 8.1.3↓
  uses CUDA events but has a 
  
 tic()/toc()
  interface similar to the 
 TimeStamp
  class.
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
8.1.1Graphics processor capability,"NVIDIA graphics processors come in a great variety. The number of devices that are CUDA enabled and are of 
 compute capability anywhere from 1.0 to 3.5 is more than 100. The first task is to find out exactly which graphics 
 processor is available and what its capability is.
  
 On a single computer system, several CUDA-enabled graphics processors may be available via the PCIe
  
 bus. With the following two lines, we can find out the number of CUDA-enabled graphics processors available:
  
  
 int count; 
  
 cudaGetDeviceCount(&count);
  
  
  
 CUDA programs look much like C programs. However, certain parts of the program run on the host, which
  
 is typically an x86 processor running either Linux or Windows, and certain other parts run on the device, which is the 
 graphics processor. The 
 cudaGetDeviceCount()
  function runs on the host. By invoking the operating system or some 
 other means, it finds out the number of devices of capability 1.0 or higher. A program using this function call can be 
 compiled using 
 nvcc
 , which is NVIDIA’s compiler driver. The compiler driver 
  
 nvcc
  is a sophisticated 
 wrapper around 
 gcc
 , and in this case all that 
  
 nvcc
  has to do is link the library in which 
 cudaGetDeviceCount()
  is 
 defined. No special option to 
 nvcc
  is necessary. This and other similar functions are declared in 
  
 cuda_runtime.h
 .
 [132]
  
 Once we know the device count, we can find out about each device.
  
  
  
  cudaDeviceProp prop;
  
  cudaGetDeviceProperties(&prop, i);
  
  
  
 Here 
 cudaDeviceProp
  is a structure type with fields corresponding to various properties of the device. The
  
 structure is defined in the 
 cuda_runtime.h
  header file. We do not have to bother to include the header file 
 explicitly in our C++ program because 
 nvcc
  will include the right header automatically. The 
  
 cudaGetDeviceProperties()
  function may talk to the operating system to find out the properties of device 
 number 
 i
 , where 
 i
  is the second argument in the function call. Alternatively, the information about device 
 properties may be left in some location during the installation of the 
 nvcc
  driver. In either case, no code is 
 generated that must run on the graphics device.
  
 The 
 prop
  variable, which is set up in this way, has information about the graphics device. The line
  
  
  
  cout<<""Device Name: ""<<prop.name<<endl;
  
  
  
 can be used to print the name of the device. Other fields of the structure 
 prop
  are
  
  
  
 clockRate 
  
 major 
  
 minor 
  
 totalGlobalMem 
  
 l2CacheSize 
  
 sharedMemPerBlock 
  
 regsPerBlock 
  
 maxThreadsPerMultiProcessor 
 warpSize 
  
 maxThreadsPerBlock 
  
 maxThreadsDim 
  
 maxGridSize 
  
 deviceOverlap
  
  
  
 Part of the information returned on our machine was as follows.
  
  
  
   
  Device Count: 1
  
   
  Device Name: Tesla K20m
  
  
  Clock rate in GHz: 0.7055
  
  Major revision number: 3",NA
8.1.2Host and device memory,"To set up any computation on the graphics device, data must be transferred from host to device memory. To 
 retrieve the result of a computation on the graphics device, data must be transferred in the other direction from
  
 device to host memory. We use the following C++ class to transfer between host and device memory:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9 
  
 10 
  
 11 
  
 12 
  
 13 
  
 14
  
 template<class ttype > class dhstmem{ 
 private: 
  
  
 ttype  *devicemem; 
  
  
 ttype  *hostmem; 
  
  
 int n; 
  
  
 cudaError_t errcode; 
  
 public: 
  
  
 dhstmem(long int nin){ 
  
  
  
 ...
  
 } 
  
 ~dhstmem(){ 
  
  
 ...
  
 } 
  
 ttype  *device(){return
  
 devicemem;}
  
 15
  
 };
  
 ttype  *host(){return hostmem;}
  
 16
  
 void device2host(){
  
 17
  
 ...
  
 18
  
 }
  
 19
  
 void host2device(){
  
 20
  
 ...
  
 21
  
 }
  
 22
  
  
  
  
 This class is templated. It can be used for data of type 
 double
 , 
 int
 , or some other type. The private section
  
 of the class is given in full on lines 3 to 6, but the definitions of some of the public member functions are not 
 shown. In the source, the definitions are included within the class itself but have been removed for better 
 readability. They will be given later.
  
 If we want memory equal to a million doubles, we can get that as follows:
  
  
  
 dhstmem<double> dhmem(1000*1000);
  
  
  
 The object 
 dhmem
  will hold pointers to a million doubles in device memory and a million doubles in host
  
  
 memory. The private field 
 dhmem.n
  will be set to a million. If we say
  
  
  
 double *hmem = dhmem.host();
  
  
  
 then 
 hmem
  becomes a pointer to the million doubles allocated in host memory. This is evident from line 15",NA
8.1.3Timing CUDA kernels,"The simplest programs have much to teach us as long as we time them carefully. Therefore, we begin by looking at 
 mechanisms to time kernels that run on the graphics coprocessor.
  
 We use the following class to time the graphics processor. The private data members of the class are shown
  
 in full. For the public member functions, the interface is shown, and the definitions will be given later.
  
  
  
 class hstTimer{ 
  
 private: 
  
  
 cudaEvent_t start, stop; 
 public: 
  
  
 hstTimer(){...}; 
  
  
 ~hstTimer(){...}; 
  
  
 void tic(){...}; 
  
  
 float toc(){...}; 
  
 };
  
  
  
 The private data members are two events named 
 start
  and 
 stop
 . The events will be “sent” to the graphics
  
 processor to record the time on the graphics processor and will be relayed back to the host.
  
 To time a piece of code, we can use the class as follows:
  
  
  
 hstTimer hclk; 
  
 hclk.tic(); 
  
 [code] 
  
 float tms = hclk.toc();
  
 The variable 
 tms
  gets set to the time elapsed between 
  
 tic()
  and 
 toc()
  in milliseconds.
  
 All member functions of 
 hstTimer
  are defined within the scope of the class. Therefore, the names of the
  
 member functions are not qualified with 
 hstTimer::
  in their definitions shown below. The constructor and the
  
 destructor are unremarkable.
  
 hstTimer(){ 
  
  
 cudaEventCreate(&start); 
  
  
 cudaEventCreate(&stop); 
  
 } 
  
 ~hstTimer(){ 
  
  
 cudaEventDestroy(start); 
  
  
 cudaEventDestroy(stop); 
  
 }
  
 The events 
 start
  and 
 stop
  are probably opaque pointers.
  
 [133]
  Therefore, it is natural that their addresses
  
 are passed during event creation and the pointers themselves are passed during event destruction. It is good practice 
 to check the error codes returned during event creation and destruction, although we have not done so here.
  
 The member functions 
 tic()
  and 
 toc()
  are defined as follows:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5 
  
 6 
  
 7 
  
 8 
  
 9
  
 void tic(){ 
  
  
 cudaEventRecord(start, 0); 
  
 } 
  
 float toc(){ 
  
  
 float time; 
  
  
 cudaEventRecord(stop, 0); 
  
  
 cudaEventSynchronize(start); 
  
  
 cudaEventSynchronize(stop); 
  
  
 cudaEventElapsedTime(&time, start,
  
 stop);
  
 10
  
 }
  
 return time;//milliseconds
  
 11
  
  
  
  
 The call to 
 cudaEventRecord()
  on line 2 has the effect of sending the event from the host to the graphics
  
 device in a streaming fashion. The second argument, which is zero on line 2, merits discussion. The host can talk to",NA
8.1.4Warps and thread blocks,"Figure 8.2The Kepler microarchitecture consists of 13 streaming multiprocessors and 
  
 6 memory 
 controllers.
  
  
  
 The K20’s design is so different from that of conventional processors that to program it we need to
  
 understand a few things about its architecture. A sketch of its microarchitecture is shown in figure 
 8.2↑
 . 
  
  
 To get a better sense of the microarchitecture, we return to the 
 cudaDeviceProp
  program of section 7.1 and
  
 show some more of its output.
  
  
  
   
  Total L2 cache (in bytes): 1310720 
 Shared memory per MP (in bytes): 49152
  
  Number of registers per MP: 65536 
 Max number of threads per MP: 2048
  
  
  
 The heart of the microarchitecture is the streaming multiprocessor, and there are 13 of these in Kepler. The
  
 K20 has 1.3 MB of L2 cache on chip, a small figure compared to the 20 MB cache of its 2.7 GHz AVX host. The 
 L2 cache is shared by the 13 multiprocessors.
  
 Each multiprocessor has 64 KB of on-chip memory for itself. This memory is split between L1 cache and 
 shared memory. The use of shared memory is under program control. On the K20, 48 KB is assigned to shared 
 memory by default, and only 16 KB is left for the L1 cache. The split between L1 cache and shared memory can be 
 changed by calling a function in the CUDA runtime library.
  
 The K20 coprocessor’s 13 streaming multiprocessors are comparable with the AVX host’s 16 processors.
  
 The K20 begins to look very different from its Sandy Bridge host if we note that each multiprocessor has as many as 
 64K=65,536 registers, each 
  
 32 bits wide. The AVX processor’s register file of sixteen 256-bit YMM registers 
 looks puny in comparison.
  
 A large register file is central to how the K20 and other graphics coprocessors work. A large number of 
 threads can be resident on the multiprocessor, and each thread gets its own subset of registers. Because the register 
 file is split between threads, the streaming multiprocessor can switch between threads with zero overhead.
  
  
 Each multiprocessor in the K20 can hold at most 2,048 threads. Therefore, each thread gets at least 64 
 registers.
  
 When Intel brought out the Xeon Phi to reclaim ground in supercomputing, the large number of registers of 
 the K20 and other graphics coprocessors was a key parameter the Xeon Phi matched. The Xeon Phi has 61 
 processor cores with four threads resident on each processor. The register file consisting of 32 512-bit ZMM 
 registers is replicated for each thread. So the total number of registers on the Phi is equivalent to 125K 32-bit 
 registers, which is approximately a sixth of the number of registers on the K20.
  
 A thread on a graphics device such as the K20 is different from a thread on an x86 machine or the Phi. To 
 get a sense of how threads work in the K20, we give the rest of the output of the 
 cudaDeviceProp
  program from",NA
8.2Introduction to CUDA,"Section 
 8.2.1↓
  uses the Leibniz example to introduce rudiments of CUDA programming. The first program to sum 
 the Leibniz series relies on help from the x86 processor. The program introduces syntax for dealing with thread 
 blocks and grids. Even for this simple program, one needs to ensure that sufficiently many thread blocks are created 
 and that each thread block has the right dimensions.
  
 The other program in section 
 8.2.1↓
  also sums the Leibniz series but without taking help from the x86 
 processors. In this program, each thread computes part of the sum, and all the threads add their portion to the global 
 result. For correctness, accesses of the global result by individual threads must be mutually exclusive. Mutual 
 exclusion is enforced using an atomic exchange instruction supported in CUDA. The program shows how to handle 
 warp divergence that arises during mutual exclusion.
  
 The program for summing the Leibniz series is faster on the K20 than on the Phi by a factor of 2.5. The Phi 
 is faster for some programs and the K20 for others. The factor of 2.5 must not be treated as a universal value. In the 
 case of the Leibniz series, the K20’s greater speed seems to be due to greater facility in handling divisions than x86 
 cores.
  
 The first program for summing the Leibniz series is perhaps not so much harder to code than the 
  
 corresponding OpenMP program, which runs on the Phi or any x86 machine. However, the second program, in 
 which the entire sum is found on the K20, is easily 100 times harder to code. In addition, the enforcement of mutual 
 exclusion implies that there is a cost associated with creating too many thread blocks. The ability to create many
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
8.2.1Summing the Leibniz series,"Threads are grouped into warps and warps into thread blocks. Thread blocks are arranged in a grid. The hierarchical 
 grouping of threads can never be forgotten in writing CUDA programs. Thus, it is useful to have a utility header
  
 file 
 const.hh
  that defines constants that other programs can reference.
  
 const int NWARP = 32; 
  
 const int THinBLK = 1024; 
 const int BLKinMP = 2; 
  
 const int NMP = 13;
  
 const int SQRTT = 32; 
  
 const int MAXTHMP = 2048; 
 const int GPUCLKMHZ = 706;
  
  
 NWARP
  is the number of threads in a warp.
  
 [134]
 THinBLK
  is the maximum number of threads in a block. In
  
 our programs, the number of threads in a block is always set to this value. 
 BLKinMP
  is the maximum number of
  
 blocks that can reside on a multiprocessor, assuming each thread block to have 1,024 threads. The number of 
 multiprocessors is 
 NMP
 . 
 SQRTT
  is the square root of the number of threads in a block. It is useful when working with
  
 two-dimensional thread blocks. The other two 
 const
 s are self-explanatory.
  
 The only parameter we vary when launching threads is the total number of blocks, which is denoted 
 NBLK
  
 later. This is typically at least equal to 
 BLKinMP*NMP
 , or 26, but can be much larger.
  
 Summing with help from the CPU
  
 Our first program for summing the Leibniz series
  
 +4 
 5
  
  − 4 
 7
  
 +
 ⋯
  
 π
 =4 − 4
  
 1 
  
 3
  
 on the K20 does not in fact sum the series entirely. It leaves part of the work to its AVX host. To begin with,
  
 we give the skeleton of the function that runs on the graphics device while omitting its body.
 [135]
  
  
  
  
 1 
  
 2 
  
 3 
  
 4 
  
 5
  
 __global__ void 
  
 __launch_bounds__(THinBLK, BLKinMP) 
 leibniz(long int n, double *result){
  
  ...
  
 }
  
  
  
  
 Evidently, the definition of the 
 leibniz
  function begins in a manner that is quite different from the C++
  
 function definitions we are used to. It looks different because this function runs on the graphics device and not the 
 x86 host.
  
 The 
 __global__
  keyword on line 1 is part of the C language extensions in the CUDA framework. The
  
 __global__
  qualifier indicates that the ensuing function definition is meant to run on the graphics device and
  
 something more. Not all functions that run on the graphics device are equal. Some of them can be called only from 
 functions that run on the graphics device, and some can be called only from functions that run on the host processor. 
 The 
 __global__
  qualifier introduces a device function that can be called from the host. Such functions
  
 are called kernels. Device functions that can be called only by kernels or other device functions are introduced 
 using 
 __device__
 . We encounter such functions later.",NA
8.2.2CUDA compilation,"So far we have made only brief remarks about compiling CUDA programs. The graphics device compilation model 
 is a little different from compiling for the host, and we will look at it in greater depth here.
  
  
 To begin with, we shall suppose that the entire Leibniz program discussed in the previous section is in one 
 file called 
 leibniz_all.cu
 . The compilation command in full, with \ being the line continuation character, is as
  
 follows:
  
  
  
  
 1 
  
 2 
  
 3 
  
 4
  
 nvcc -O3 \
  
 -prec-div=true -ftz=false -Drestrict=""__restrict__""\ 
 -arch=sm_35 \
  
 -Xptxas=-v -dc leibniz_all.cu 
  
  
  
  
 Here 
 nvcc
  (line 1) is the name of NVIDIA’s compiler driver, which is a wrapper around GNU’s gcc/g++.
  
 The 
 -O3
  option (line 1) sets the optimization level.
  
 The options on line 2 are more generic compiling options. Precise division is required, and the flush to zero 
 optimization is turned off. The keyword 
 restrict
  is defined as a macro that expands to 
 __restrict__
 . Some of 
 our utility programs that print tables and so on use the 
 restrict
  keyword, which is part of the C99 standard and 
 supported in C++ programs by Intel’s 
 icpc
  compiler. In GNU, a C++ program must use 
 __restrict__
  in place of 
 restrict
 .
  
 The 
 -arch
  option (line 3) specifies the compute capability as 3.5. To see what effect it has, we run the
  
 command
  
  
  
 cuobjdump leibniz_all.o
  
  
  
 on the object file. The output of this command, with some lines omitted, is
  
  
  
 Fatbin elf code: 
  
 ================ 
 arch = sm_35 
  
 code version = [1,7] 
  
 ...
  
 Fatbin ptx code: 
  
 ================ 
 arch = sm_35 
  
 code version = [3,2] 
  
 ...
  
  
  
 The object file is a fatbin (in CUDA terminology), including both binary elf code and ptx code. The binary
  
 code will only run on devices whose architecture exactly matches 
 sm_35
 , but the ptx code will run on any device of 
 compute capability 3.5 or higher.
  
 The PTX is an assembly-like intermediate language. It is in text not in binary format. The inner loop of the
  
 leibniz()
  kernel looks as follows in PTX:
  
  
  
 BB2_3: 
  
  
 .loc 1 13 1 
  
  
 cvt.rn.f64.s64 %fd6, %rd10; 
  
  
 fma.rn.f64 
  
 %fd7, %fd6, 0d4000000000000000,\
  
  
  0d3FF0000000000000; 
  
  
 mov.f64  %fd8, 0d4010000000000000; 
  
  
 .loc 3 3614 3 
  
  
 div.rn.f64 
  
 %fd9, %fd8, %fd7; 
  
  
 .loc 1 13 94 
  
  
 add.f64  %fd12, %fd12, %fd9; 
  
  
 .loc 1 12 17 
  
  
 add.s64  %rd10, %rd10, %rd3; 
  
  
 .loc 1 12 1 
  
  
 setp.lt.s64 
  
  
 %p2, %rd10, %rd6; 
  
 @%p2 bra 
  
 BB2_3;",NA
8.3Two examples,"In this section, we look at two more examples to better understand the CUDA programming model and the speed of",NA
8.3.1Bandwidth to memory,"The following functions are used to measure the K20’s bandwidth to memory:
  
  
  
 __global__ 
  
 __launch_bounds__(THinBLK, BLKinMP) 
  
 void add(double *list, int n, double *result){ 
  
  
 int tid = threadIdx.x + blockIdx.x*blockDim.x; 
  
 int stride = blockDim.x*gridDim.x; 
  
  
 double ans = 0; 
  
  
 for(int i=tid; i < n; i = i + stride) 
  
  
  
 ans += list[i]; 
  
  
 result[tid] = ans; 
  
 }
  
 __global__ 
  
 __launch_bounds__(THinBLK, BLKinMP) 
  
 void copy(double *list, int n, double *copy){ 
  
  
 int tid = threadIdx.x + blockIdx.x*blockDim.x; 
  
 int stride = blockDim.x*gridDim.x; 
  
  
 for(int i=tid; i < n; i = i + stride) 
  
  
  
 copy[i] = list[i]; 
  
 }
  
  
  
 Notice that data accesses from the threads are interleaved as in
  
 0,1,2,3,…,0,1,2,3,…,0,1,2,3,…
  
 and not blocked as in
  
 0,0,0,…,1,1,1,…,2,2,2,…,3,3,3,…
  
 Blocking memory accesses is the right thing to do on x86 processors. On the K20 and other graphics devices, 
 blocking will lose more than 90% of the bandwidth. Partly because instructions are executed warp by
  
 warp, memory accesses must be interleaved.
  
  
  
  
  
 AVX
  
 Phi
  
 K20",NA
8.3.2Matrix multiplication,"If 
 A
 , 
 B
 , and 
 C
  are 
 N
 ×
 N
  matrices, then 
 C
 =
 C
 +
 AB
  requires 2
 N
 3
  floating point operations (flops). Half the operations 
 are additions and half are multiplications. The amount of memory involved in matrix multiplication is 
 O",NA
(,"N
 2",NA
),", but 
 the number of flops is 
 O",NA
(,"N
 3",NA
),". If 
 N
  is large, there is a possibility that the cost of memory accesses can be hidden 
 almost completely, and the program’s speed is determined by the peak floating point bandwidth. On the K20, the 
 peak floating point bandwidth is 1.17 TFlops/s. Although the programs we describe do not approach that speed, we 
 compare them against a cuBLAS program that does.
  
 Global memory
  
 The first program for multiplying square matrices is as follows:
 [140]
  
  
  
 __global__ 
  
 __launch_bounds__(THinBLK, BLKinMP) 
  
 void mmult_gmem(double *A, double *B, double * C, 
  
  
 int N){ 
  
  
 int tidx = threadIdx.x; 
  
  
 int tidy = threadIdx.y; 
  
  
 int bidx = blockIdx.x; 
  
  
 int bidy = blockIdx.y; 
  
  
 int i = bidx*blockDim.x + tidx; 
  
  
 int j = bidy*blockDim.y + tidy; 
  
  
 for(int k = 0; k < N; k++) 
  
  
  
 C[i+j*N] += A[i+k*N]*B[k+j*N]; 
  
 }
  
  
  
 This kernel is assumed to be launched with two-dimensional thread blocks in a two-dimensional grid. The
  
 size of the thread block is assumed to 
 SQRTT x SQRTT
 . On the K20, we take 
  
 SQRTT
  to be 32=",NA
√,"1,024 because
  
 1,024 is the maximum number of threads in a block.
  
  
 The matrix dimension 
 N
  is assumed to be divisible by 
  
 SQRTT
 . The grid is assumed to be 
 N/SQRTT x 
 N/SQRTT
 . The total number of threads is thus equal to 
 N
 2
 . There is a natural map from threads to each entry of the
  
 N
 ×
 N
  matrix 
 C
 . The 
 mmult_gmem()
  kernel calculates the indices of 
  
 (
 i
 ,
 j
 ) of the entry the thread maps to and then
  
 updates the corresponding entry of 
 C[]
 . Thus, each entry of the matrix 
 C[]
  is given to a different thread for
  
 updating.
  
 The syntax for launching this kernel using two-dimensional thread blocks and grids is shown in the listing
  
 below. Multiple dimensions are specified using 
 dim3
  objects.
  
  
  
 double mmult(double *A, double *B, double *C, int N){ 
 assrt(SQRTT*SQRTT==THinBLK); 
  
 assrt(N%SQRTT==0);
  
 dim3 grid(N/SQRTT, N/SQRTT); 
 dim3 tblk(SQRTT, SQRTT);
  
 dhstmem<double> dhA(N*N); 
 dhstmem<double> dhB(N*N); 
 dhstmem<double> dhC(N*N);",NA
8.4References,NA,NA
Bibliography,"[58] D.B. Kirk, W.W. Hwu : 
 Programming Massively Parallel Processors
  
 . Morgan Kaufmann, 2010 .
  
 [59] J. Sanders, E. Kandrot
  
 : 
 CUDA by Example
  . Addison-Wesley , 2010 .
  
 [60] V. Volkov, J.W. Demmel : “Benchmarking GPUs to tune dense linear algebra
  
 ”, 
 Proceedings of the 2008
  
 ACM/IEEE Conference on Supercomputing
 , pp. 1-11 , 2008 .",NA
"9Machines Used, Plotting, Python, GIT, Cscope, ",NA,NA
and gcc,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
9.1Machines used,"Computer
  
 Instn
  
 Registers
  
 Microarchitecture
  
 Year
  
 Moniker
  
 Xeon 5650 (12 core)
  
 SSE2
  
 XMM
  
 Nehalem/Westmere
  
 2001/2010
  
 2.6 GHz SSE2
  
 Xeon 5680 (12 core)
  
 SSE2
  
 XMM
  
 Nehalem/Westmere
  
 2001/2010
  
 3.33 GHz SSE2
  
 E5-2660 (16 core)
  
 AVX
  
 YMM
  
 Sandy Bridge
  
 2011/2012
  
 2.2 GHz AVX
  
 E5-2680 (16 core)
  
 AVX
  
 YMM
  
 Sandy Bridge
  
 2011/2012
  
 2.7 GHz AVX
  
 Core i7-3770 (4 core)
  
 AVX
  
 YMM
  
 Sandy Bridge
  
 2011/2012
  
 3.4 GHz AVX
  
 Core i3-4350 (2 core)
  
 AVX2
  
 YMM
  
 Haswell
  
 2013/2014
  
 3.6 GHz AVX2
  
 Xeon Phi SE10P
  
 AVX-512
  
 ZMM
  
 Phi/MIC
  
 2013/2013
  
 Phi/MIC
  
 Table 9.1Machines used to run and time programs. The Xeon Phi, which is a coprocessor, uses a 1.09 GHz 
 clock. The second column gives the highest level of instruction set pertinent to this book. For the interpretation 
 and meaning of the instruction set, see table 
 3.1↑
  and the associated discussion. The second to last column gives 
 the year of the instruction set as well as the computer. The last column gives the name with which the machine 
 is referenced in the text. Much of the information in this table may be verified at 
 ark.intel.com
  .
  
  
  
 For cache parameters of the machines in table 
 9.1↑
 , see section 
 3.1.4↑
 .",NA
9.2Plotting in C/C++ and other preliminaries,"In this section, we describe C++ classes for plotting, gathering statistics, and making tables. These classes are used 
 throughout the book. However, in almost every instance, the code showing how these facilities are invoked is 
 suppressed.
  
 Plotting in C/C++ programs 
  
 Plots and pictures that show output and display program data can make programs less cryptic and clarify what is 
 going on. A picture can be worth a thousand lines of code.
  
 Plotting libraries are not part of the C/C++ languages for a good reason. Although plotting is very helpful, it 
 is too high level of an activity and far removed from the view of the machine that the C language offers. There are 
 many libraries, external to the language, that may be used to generate plots. Some of these libraries offer precise 
 control and many graphics capabilities. No single plotting library appears to be dominant. Some of the libraries are 
 extensive enough to overwhelm those who are not dedicated to computer graphics. In such a situation, it is difficult
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
9.3C/C++ versus Python versus MATLAB,How much faster C/C++ can be relative to interpreted languages such as Python and MATLAB is often not,NA
9.4GIT,https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49],NA
9.5Cscope,"The 
 cscope
  utility is invaluable for browsing source code. Using 
  
 cscope
 , one can easily find the definition of a
  
 function, all the places where it is called, and search for patterns inside the entire source tree. It is a search utility 
 that predates the era of Internet search by a few decades.
  
 Suppose the source for this book is saved in the GIT repository 
 bk-spca
 . To prepare 
 cscope
  database files,
  
 we may run the command
  
  
  
 cscope.py bk-spca
  
  
  
 in the parent directory of 
 bk-spca
 ; 
 cscope.py
  is the following Python script, which must be on the user’s",NA
9.6Compiling with gcc/g++,"The Makefiles in the GIT repository 
 bk-spca
  use Intel compilers for the most part. The switch to 
 gcc/g++
  is not
  
 too complicated. The following points must be kept in mind:
  
 To enable the 
 restrict
  qualifier, use the option 
  
 -Drestrict=""__restrict__""
 .
  
  
 To generate code for a specific instruction set such as AVX2, use an option such as 
 -mavx2
 .
  
  
 -openmp
  becomes 
 -fopenmp
 .
  
 Scientific and Engineering Computation 
  
 William Gropp and Ewing Lusk, editors; Janusz Kowalik, founding editor
  
 Data-Parallel Programming on MIMD Computers
 , Philip J. Hatcher and Michael J. Quinn, 1991 
  
 Enterprise Integration Modeling: Proceedings of the First International Conference
 , edited by Charles J.
  
 Petrie, Jr., 1992 
  
 The High Performance Fortran Handbook
 , Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L.
  
 Steele Jr., and Mary E. Zosel, 1994 
  
 PVM: A User’s Guide and Tutorial for Network Parallel Computing
 , Al Geist, Adam Beguelin, Jack Dongarra, 
 Weicheng Jiang, Robert Manchek, and Vaidyalingham S. Sunderam, 1994 
  
 Practical Parallel Programming
 , Gregory V. Wilson, 1995 
  
 Enabling Technologies for Petaflops Computing
 , Thomas Sterling, Paul Messina, and Paul H. Smith, 1995 
 An 
 Introduction to High-Performance Scientific Computing
 , Lloyd D. Fosdick, Elizabeth R. Jessup, Carolyn J.
  
 C. Schauble, and Gitta Domik, 1995 
  
 Parallel Programming Using C++
 , edited by Gregory V. Wilson and Paul Lu, 1996 
  
 Using PLAPACK: Parallel Linear Algebra Package
 , Robert A. van de Geijn, 1997 
  
 Fortran 95 Handbook
 , Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin, Brian T. Smith, and Jerrold L.
  
 Wagener, 1997 
  
 MPI—The Complete Reference: Volume 1
 , The MPI Core, Marc Snir, Steve Otto, Steven Huss-Lederman, 
 David Walker, and Jack Dongarra, 1998 
  
 MPI—The Complete Reference: Volume 2
 , The MPI-2 Extensions, William Gropp, Steven Huss-Lederman, 
 Andrew Lumsdaine, Ewing Lusk, Bill Nitzberg, William Saphir, and Marc Snir, 1998 
  
 A Programmer’s Guide to ZPL
 , Lawrence Snyder, 1999 
  
 How to Build a Beowulf
 , Thomas L. Sterling, John Salmon, Donald J. Becker, and Daniel F. Savarese, 1999 
 Using MPI-2: Advanced Features of the Message-Passing Interface
 , William Gropp, Ewing Lusk, and 
 Rajeev Thakur, 1999 
  
 Beowulf Cluster Computing with Windows
 , edited by Thomas Sterling, William Gropp, and Ewing Lusk, 2001 
 Beowulf Cluster Computing with Linux
 , 
 second edition
 , edited by Thomas Sterling, William Gropp, and Ewing 
 Lusk, 2003 
  
 Scalable Input/Output: Achieving System Balance
 , edited by Daniel A. Reed, 2003 
  
 Using OpenMP: Portable Shared Memory Parallel Programming
 , Barbara Chapman, Gabriele Jost, and Ruud 
 van der Pas, 2008 
  
 Quantum Computing without Magic: Devices
 , Zdzislaw Meglicki, 2008
  
 https://divakarvi.github.io/bk-spca/spca.html[20-1-2019 23:44:49]",NA
