Larger Text,Smaller Text,Symbol
Code: The Hidden Language of Computer,NA,NA
Hardware and Software,NA,NA
Charles Petzold,"Copyright © 2009
  
 Microsoft Press books are available through booksellers and distributors worldwide. For further information about 
 international editions, contact your local Microsoft Corporation office or contact Microsoft Press International directly at fax 
 (425) 936-7329. Visit our Web site at 
 mspress.microsoft.com
 . Send comments to 
 mspinput@microsoft.com
 .
  
 Macintosh is a registered trademark of Apple Computer, Inc. Microsoft, MS-DOS, and Windows are either registered 
 trademarks or trademarks of Microsoft Corporation in the United States and/or other countries. Other product and 
 company names mentioned herein may be the trademarks of their respective owners.
  
 Images of Charles Babbage, George Boole, Louis Braille, Herman Hollerith, Samuel Morse, and John von Neumann appear 
 courtesy of Corbis Images and were modified for this book by Joel Panchot. The January 1975 cover of 
 Popular Electronics
  is 
 reprinted by permission of Ziff-Davis and the Ziff family. All other illustrations in the book were produced by Joel Panchot.
  
 Unless otherwise noted, the example companies, organizations, products, people, and events depicted herein are fictitious. No 
 association with any real company, organization, product, person, or event is intended or should be inferred.",NA
SPECIAL OFFER: Upgrade this ebook with O’Reilly ,"Click here
  for more information on this offer!
  
 Please note that upgrade offers are not available from sample content.",NA
Preface to the Paperback Edition,"Code
  rattled around in my head for about a decade before I started writing it. As I was 
 contemplating 
 Code
  and then writing it, and even after the book was published, people would 
 ask me, ""What's the book about?""
  
 I was always reluctant to answer this question. I'd mumble something about ""a unique 
 journey through the evolution of the digital technologies that define the modern age"" and 
 hope that would be sufficient.
  
 But finally I had to admit it: ""
 Code
  is a book about how computers work.""
  
 As I feared, the reactions weren't favorable. ""Oh, I have a book like that,"" some people would 
 say, to which my immediate response was, ""No, no, no, you don't have a book like this one."" I 
 still think that's true. 
 Code
  is not like other how-computers-work books. It doesn't have big 
 color illustrations of disk drives with arrows showing how the data sweeps into the computer. 
 Code
  has no drawings of trains carrying a cargo of zeros and ones. Metaphors and similes are 
 wonderful literary devices but they do nothing but obscure the beauty of technology.
  
 The other comment I heard was, ""People don't want to know how computers work."" And this 
 I'm sure is true. I personally happen to enjoy learning how things work. But I also like to 
 choose which things I learn about and which I do not. I'd be hard pressed to explain how my 
 refrigerator works, for example.
  
 Yet I often hear people ask questions that reveal a need to know something about the inner 
 workings of personal computers. One such common question is, ""What's the difference 
 between storage and memory?""
  
 That's certainly a critical question. The marketing of personal computers is based on such 
 concepts. Even novice users are expected to know how many 
 megas
  of the one thing and 
 gigas
  
 of the other thing will be necessary for their particular applications. Novice users are also 
 expected to master the concept of the computer ""file"" and to visualize how files are loaded 
 from storage into memory and saved from memory back to storage.
  
 The storage-and-memory question is usually answered with an analogy: ""Memory is like the 
 surface of your desk and storage is like the filing cabinet."" That's not a bad answer as far as it 
 goes. But I find it quite unsatisfactory. It makes it sound as if computer architecture were 
 patterned after an office.
  
 The truth is that the distinction between memory and storage is an artificial one and exists 
 solely because we don't have a single storage medium that is both fast and vast as well as 
 nonvolatile. What we know today as ""von Neumann architecture""—the dominant computer 
 architecture for over 50 years—is a direct result of this technical deficiency.
  
 Here's another question that someone once asked me: ""Why can't you run Macintosh 
 programs under Windows?"" My mouth opened to begin an answer when I realized that it 
 involved many more technical issues than I'm sure my questioner was prepared to deal 
 with in one sitting.
  
 I want 
 Code
  to be a book that makes you understand these things, not in some abstract way, 
 but with a depth that just might even rival that of electrical engineers and programmers. I also 
 hope that you might recognize the computer to be one of the crowning achievements of",NA
Chapter 1. Best Friends,"code
  (kōd) …
  
 3.a. A system of signals used to represent letters or numbers in transmitting messages.
  
 b. A system of symbols, letters, or words given certain arbitrary meanings, used for transmitting messages requiring 
 secrecy or brevity.
  
 4. A system of symbols and rules used to represent instructions to a computer…
  
 —
 The American Heritage Dictionary of the English Language
  
 You're 10 years old. Your best friend lives across the street. In fact, the windows of your 
 bedrooms face each other. Every night, after your parents have declared bedtime at the 
 usual indecently early hour, you still need to exchange thoughts, observations, secrets, 
 gossip, jokes, and dreams. No one can blame you. After all, the impulse to communicate is 
 one of the most human of traits.
  
 While the lights are still on in your bedrooms, you and your best friend can wave to each 
 other from the windows and, using broad gestures and rudimentary body language, convey a 
 thought or two. But sophisticated transactions seem difficult. And once the parents have 
 decreed ""Lights out!"" the situation seems hopeless.
  
 How to communicate? The telephone perhaps? Do you have a telephone in your room at the 
 age of 10? Even so, wherever the phone is you'll be overheard. If your family personal 
 computer is hooked into a phone line, it might offer soundless help, but again, it's not in your 
 room.
  
 What you and your best friend 
 do
  own, however, are flashlights. Everyone knows that 
 flashlights were invented to let kids read books under the bed covers; flashlights also seem 
 perfect for the job of communicating after dark. They're certainly quiet enough, and the light 
 is highly directional and probably won't seep out under the bedroom door to alert your 
 suspicious folks.
  
 Can flashlights be made to speak? It's certainly worth a try. You learned how to write letters 
 and words on paper in first grade, so transferring that knowledge to the flashlight seems 
 reasonable. All you have to do is stand at your window and draw the letters with light. For an 
 O, you turn on the flashlight, sweep a circle in the air, and turn off the switch. For an I, you 
 make a vertical stroke. But, as you discover quickly, this method simply doesn't work. As you 
 watch your friend's flashlight making swoops and lines in the air, you find that it's too hard to 
 assemble the multiple strokes together in your head. These swirls and slashes of light are not 
 precise
  enough.
  
 You once saw a movie in which a couple of sailors signaled to each other across the sea with 
 blinking lights. In another movie, a spy wiggled a mirror to reflect the sunlight into a room 
 where another spy lay captive. Maybe that's the solution. So you first devise a simple 
 technique. Each letter of the alphabet corresponds to a series of flashlight blinks. An A is 1 
 blink, a B is 2 blinks, a C is 3 blinks, and so on to 26 blinks for Z. The word BAD is 2 blinks, 1 
 blink, and 4 blinks with little pauses between the letters so you won't mistake the 7 blinks 
 for a G. You'll pause a bit longer between words.",NA
Chapter 2. Codes and Combinations,"Morse code was invented by Samuel Finley Breese Morse (1791–1872), whom we shall meet 
 more properly later in this book. The invention of Morse code goes hand in hand with the 
 invention of the telegraph, which we'll also examine in more detail. Just as Morse code 
 provides a good introduction to the nature of codes, the telegraph provides a good 
 introduction to the hardware of the computer.
  
 Most people find Morse code easier to send than to receive. Even if you don't have 
 Morse code memorized, you can simply use this table, conveniently arranged in 
 alphabetical order:
  
  
 Receiving Morse code and translating it back into words is considerably harder and more 
 time consuming than sending because you must work backward to figure out the letter that 
 corresponds to a particular coded sequence of dots and dashes. For example, if you receive a 
 dash-dot-dash-dash, you have to scan through the table letter by letter before you finally 
 discover that the code is the letter Y.
  
 The problem is that we have a table that provides this translation:
  
 Alphabetical letter → Morse code dots and dashes
  
 But we 
 don't
  have a table that lets us go backward:
  
 Morse code dots and dashes → Alphabetical letter
  
 In the early stages of learning Morse code, such a table would certainly be convenient. But 
 it's not at all obvious how we could construct it. There's nothing in those dots and dashes 
 that we can put into alphabetical order.
  
 So let's forget about alphabetical order. Perhaps a better approach to organizing the codes 
 might be to group them depending on how many dots and dashes they have. For example, a 
 Morse code sequence that contains either one dot or one dash can represent only two letters, 
 which are E and T:",NA
Chapter 3. Braille and Binary Codes,"Samuel Morse wasn't the first person to successfully translate the letters of written language 
 to an interpretable code. Nor was he the first person to be remembered more as the name of 
 his code than as himself. That honor must go to a blind French teenager born some 18 years 
 after Samuel Morse but who made his mark much more precociously. Little is known of his 
 life, but what is known makes a compelling story.
  
 Louis Braille was born in 1809 in Coupvray, France, just 25 miles east of Paris. His father was 
 a harness maker. At the age of three—an age when young boys shouldn't be playing in their 
 fathers' workshops—he accidentally stuck a pointed tool in his eye. The wound became 
 infected, and the infection spread to his other eye, leaving him totally blind. Normally he 
 would have been doomed to a life of ignorance and poverty (as most blind people were in 
 those days), but young Louis's intelligence and desire to learn were soon recognized. Through 
 the intervention of the village priest and a schoolteacher, he first attended school in the 
 village with the other children and at the age of 10 was sent to the Royal Institution for Blind 
 Youth in Paris.
  
  
 One major obstacle in the education of the blind is, of course, their inability to read printed 
 books. Valentin Haüy (1745–1822), the founder of the Paris school, had invented a system of 
 raised letters on paper that could be read by touch. But this system was very difficult to use, 
 and only a few books had been produced using this method.
  
 The sighted Haüy was stuck in a paradigm. To him, an A was an A was an A, and the letter A 
 must look (or feel) like an A. (If given a flashlight to communicate, he might have tried 
 drawing letters in the air as we did before we discovered it didn't work very well.) Haüy 
 probably didn't realize that a type of code quite different from the printed alphabet might be 
 more appropriate for sightless people.
  
 The origins of an alternative type of code came from an unexpected source. Charles Barbier, a 
 captain of the French army, had by 1819 devised a system of writing he called 
 écriture 
 nocturne
 , or ""night writing."" This system used a pattern of raised dots and dashes on heavy 
 paper and was intended for use by soldiers in passing notes to each other in the dark when",NA
Chapter 4. Anatomy of a Flashlight,"Flashlights are useful for numerous tasks, of which reading under the covers and sending 
 coded messages are only the two most obvious. The common household flashlight can also 
 take center stage in an educational show-and-tell of the magical stuff known as electricity.
  
 Electricity is an amazing phenomenon, managing to be pervasively useful while remaining 
 largely mysterious, even to people who pretend to know how it works. But I'm afraid we must 
 wrestle with electricity anyway. Fortunately, we need to understand only a few basic concepts 
 to comprehend how it's used inside computers.
  
 The flashlight is certainly one of the simpler electrical appliances found in most homes. 
 Disassemble a typical flashlight, and you'll find it consists of a couple of batteries, a bulb, a 
 switch, some metal pieces, and a plastic case to hold everything together.
  
 You can make your own no-frills flashlight by disposing of everything except the batteries 
 and the lightbulb. You'll also need some short pieces of insulated wire (with the insulation 
 stripped from the ends) and enough hands to hold everything together.
  
  
 Notice the two loose ends of the wires at the right of the diagram. That's our switch. Assuming 
 that the batteries are good and the bulb isn't burned out, touching these loose ends together 
 will turn on the light.
  
 What we've constructed here is a simple electrical circuit, and the first thing to notice is that a 
 circuit 
 is a 
 circle
 . The lightbulb will be lit only if the path from the batteries to the wire to the 
 bulb to the switch and back to the batteries is continuous. Any break in this circuit will cause 
 the bulb to go out. The purpose of the switch is to control this process.
  
 The circular nature of the electrical circuit suggests that something is moving around the 
 circuit, perhaps like water flowing through pipes. The ""water and pipes"" analogy is quite 
 common in explanations of how electricity works, but eventually it breaks down, as all 
 analogies must.
  
 Electricity is like nothing else in this universe, and we must confront it on its own terms.",NA
Chapter 5. Seeing Around Corners,"You're twelve years old. One horrible day your best friend's family moves to another town. 
 You speak to your friend on the telephone now and then, but telephone conversations just 
 aren't the same as those late-night sessions with the flashlights blinking out Morse code. Your 
 second-best friend, who lives in the house next door to yours, eventually becomes your new 
 best friend. It's time to teach your new best friend some Morse code and get the late-night 
 flashlights blinking again.
  
 The problem is, your new best friend's bedroom window doesn't face your bedroom window. 
 The houses are side by side, but the bedroom windows face the same direction. Unless you 
 figure out a way to rig up a few mirrors outside, the flashlights are now inadequate for after-
 dark communication.
  
 Or are they?
  
 Maybe you have learned something about electricity by this time, so you decide to make 
 your own flashlights out of batteries, lightbulbs, switches, and wires. In the first experiment, 
 you wire up the batteries and switch in your bedroom. Two wires go out your window, 
 across a fence, and into your friend's bedroom, where they're connected to a lightbulb:
  
  
 Although I'm showing only one battery, you might actually be using two. In this and future 
 diagrams, this will be an off (or open) switch:
  
  
 and this will be the switch when it's on (or closed):
  
  
 The flashlight in this chapter works the same way as the one illustrated in the previous 
 chapter, although the wires connecting the components for this chapter's flashlight are a bit 
 longer. When you close the switch at your end, the light goes on at your friend's end:",NA
Chapter 6. Telegraphs and Relays,"Samuel Finley Breese Morse was born in 1791 in Charleston, Massachusetts, the town where 
 the Battle of Bunker Hill was fought and which is now the northeast part of Boston. In the year 
 of Morse's birth, the United States Constitution had been ratified just two years before and 
 George Washington was serving his first term as president. Catherine the Great ruled Russia. 
 Louis XVI and Marie Antoinette would lose their heads two years later in the French 
 Revolution. And in 1791, Mozart completed 
 The Magic Flute
 , his last opera, and died later that 
 year at the age of 35.
  
 Morse was educated at Yale and studied art in London. He became a successful portrait artist. 
 His painting 
 General Lafayette
  (1825) hangs in New York's City Hall. In 1836, he ran for mayor 
 of New York City on an independent ticket and received 5.7 percent of the vote. He was also 
 an early photography buff. Morse learned how to make daguerreotype photographs from 
 Louis Daguerre himself and made some of the first daguerreotypes in America. In 1840, he 
 taught the process to the 17-year-old Mathew Brady, who with his colleagues would be 
 responsible for creating the most memorable photographs of the Civil War, Abraham Lincoln, 
 and Samuel Morse himself.
  
  
 But these are just footnotes to an eclectic career. Samuel F. B. Morse is best known these days 
 for his invention of the telegraph and the code that bears his name.
  
 The instantaneous worldwide communication we've become accustomed to is a relatively 
 recent development. In the early 1800s, you could communicate instantly and you could 
 communicate over long distances, but you couldn't do both at the same time. Instantaneous 
 communication was limited to as far as your voice could carry (no amplification available) or 
 as far as the eye could see (aided perhaps by a telescope). Communication over longer 
 distances by letter took time and involved horses, trains, or ships.
  
 For decades prior to Morse's invention, many attempts were made to speed long-distance 
 communication. Technically simple methods employed a relay system of men standing on 
 hills waving flags in semaphore codes. Technically more complex solutions used large 
 structures with movable arms that did basically the same thing as men waving flags.",NA
Chapter 7. Our Ten Digits,"The idea that language is merely a code seems readily acceptable. Many of us at least 
 attempted to learn a foreign language in high school, so we're willing to acknowledge that the 
 animal we call a cat in English can also be a 
 gato
 , 
 chat
 , 
 Katze
 , 
 KOIIIKa
 , or 
 Kάττα
 .
  
 Numbers, however, seem less culturally malleable. Regardless of the language we speak and 
 the way we pronounce the numbers, just about everybody we're likely to come in contact 
 with on this planet writes them the same way:
  
  
 Isn't mathematics called ""the universal language"" for a reason?
  
 Numbers are certainly the most abstract codes we deal with on a regular basis. When we 
 see the number
  
  
 we don't immediately need to relate it to anything. We might visualize 3 apples or 3 of 
 something else, but we'd be just as comfortable learning from context that the number refers 
 to a child's birthday, a television channel, a hockey score, or the number of cups of flour in a 
 cake recipe. Because our numbers are so abstract to begin with, it's more difficult for us to 
 understand that this number of apples
  
  
 doesn't necessarily have to be denoted by the symbol
  
  
 Much of this chapter and the next will be devoted to persuading ourselves that this many 
 apples
  
  
 can also be indicated by writing
  
  
 Let's first dispense with the idea that there's something inherently special about the number 
 ten. That most civilizations have based their number systems around ten (or sometimes five) 
 isn't surprising.
  
 From the very beginning, people have used their fingers to count. Had our species developed 
 possessing eight or twelve fingers, our ways of counting would be a little different. It's no 
  
 coincidence that the word 
 digit
  can refer to fingers or toes as well as numbers or that the 
 words 
 five 
 and 
 fist
  have similar roots.",NA
Chapter 8. Alternatives to Ten,"Ten is an exceptionally important number to us humans. Ten is the number of fingers and 
 toes most of us have, and we certainly prefer to have all ten of each. Because our fingers are 
 convenient for counting, we humans have adapted an entire number system that's based on 
 the number 10.
  
  
 As I mentioned in the previous chapter, the number system that we use is called 
 base ten
 , or 
 decimal
 . The number system seems so natural to us that it's difficult at first to conceive of 
 alternatives. Indeed, when we see the number 
 10
  we can't help but think that this number 
 refers to this many ducks:
  
  
 But the only reason that the number 10 refers to this many ducks is that this many ducks is 
 the same as the number of fingers we have. If human beings had a different number of fingers, 
 the way we counted would be different, and 10 would mean something else. That same 
 number 10 could refer to this many ducks:
  
  
 or this many ducks:
  
  
 or even this many ducks:",NA
Chapter 9. Bit by Bit by Bit,"When Tony Orlando requested in a 1973 song that his beloved ""Tie a Yellow Ribbon Round 
 the Ole Oak Tree,"" he wasn't asking for elaborate explanations or extended discussion. He 
 didn't want any ifs, ands, or buts. Despite the complex feelings and emotional histories that 
 would have been at play in the real-life situation the song was based on, all the man really 
 wanted was a simple yes or no. He wanted a yellow ribbon tied around the tree to mean ""Yes, 
 even though you messed up big time and you've been in prison for three years, I still want you 
 back with me under my roof."" And he wanted the absence of a yellow ribbon to mean ""Don't 
 even 
 think
  about stopping here.""
  
 These are two clear-cut, mutually exclusive alternatives. Tony Orlando did 
 not
  sing, ""Tie 
 half of a yellow ribbon if you want to think about it for a while"" or ""Tie a blue ribbon if you 
 don't love me anymore but you'd still like to be friends."" Instead, he made it very, very 
 simple.
  
 Equally effective as the absence or presence of a yellow ribbon (but perhaps more awkward 
 to put into verse) would be a choice of traffic signs in the front yard: Perhaps ""Merge"" or 
 ""Wrong Way.""
  
 Or a sign hung on the door: ""Closed"" or ""Open.""
  
 Or a flashlight in the window, turned on or off.
  
 You can choose from lots of ways to say yes or no if that's all you need to say. You don't need 
 a sentence to say yes or no; you don't need a word, and you don't even need a letter. All you 
 need is a 
 bit
 , and by that I mean all you need is a 0 or a 1.
  
 As we discovered in the previous chapters, there's nothing really all that special about the 
 decimal number system that we normally use for counting. It's pretty clear that we base our 
 number system on ten because that's the number of fingers we have. We could just as 
 reasonably base our number system on eight (if we were cartoon characters) or four (if we 
 were lobsters) or even two (if we were dolphins).
  
 But there 
 is
  something special about the binary number system. What's special about binary 
 is that it's the 
 simplest
  number system possible. There are only two binary digits—0 and 1. If 
 we want something simpler than binary, we'll have to get rid of the 1, and then we'll be left 
 with just a 0. We can't do much of anything with just a 0.
  
 The word 
 bit
 , coined to mean 
 binary digit
 , is surely one of the loveliest words invented in 
  
 connection with computers. Of course, the word has the normal meaning, ""a small portion, 
 degree, or amount,"" and that normal meaning is perfect because a bit—one binary digit—is a 
 very small quantity indeed.
  
 Sometimes when a new word is invented, it also assumes a new meaning. That's certainly 
 true in this case. A 
 bit
  has a meaning beyond the 
 binary digits
  used by dolphins for counting. 
 In the computer age, the bit has come to be regarded as 
 the basic building block of 
 information
 .
  
 Now that's a bold statement, and of course, bits aren't the only things that convey information. 
 Letters and words and Morse code and Braille and decimal digits convey information as well. 
 The thing about the bit is that it conveys very 
 little
  information. A bit of information is the",NA
Chapter 10. Logic and Switches,"What is truth? Aristotle thought that logic had something to do with it. The collection of his 
 teachings known as the 
 Organon
  (which dates from the fourth century B.C.E.) is the earliest 
 extensive writing on the subject of logic. To the ancient Greeks, logic was a means of 
 analyzing language in the search for truth and thus was considered a form of philosophy. The 
 basis of Aristotle's logic was the 
 syllogism
 . The most famous syllogism (which isn't actually 
 found in the works of Aristotle) is
  
 All men are mortal;
  
 Socrates is a man;
  
 Hence, Socrates is mortal.
  
 In a syllogism, two premises are assumed to be correct, and from these a conclusion is 
 deduced.
  
 The mortality of Socrates might seem straightforward enough, but there are many varieties 
 of syllogisms. For example, consider the following two premises, proposed by the 
 nineteenth-century mathematician Charles Dodgson (also known as Lewis Carroll):
  
 All philosophers are logical;
  
 An illogical man is always obstinate.
  
 The conclusion isn't obvious at all. (It's ""Some obstinate persons are not philosophers."" 
 Notice the unexpected and disturbing appearance of the word ""some."")
  
 For over two thousand years, mathematicians wrestled with Aristotle's logic, attempting to 
 corral it using mathematical symbols and operators. Prior to the nineteenth century, the only 
 person to come close was Gottfried Wilhelm von Leibniz (1648–1716), who dabbled with 
 logic early in life but then went on to other interests (such as independently inventing 
 calculus at the same time as Isaac Newton).
  
 And then came George Boole.
  
 George Boole was born in England in 1815 to a world where the odds were certainly stacked 
 against him. Because he was the son of a shoe-maker and a former maid, Britain's rigid class 
 structure would normally have prevented Boole from achieving anything much different from 
 his ancestors. But aided by an inquisitive mind and his helpful father (who had strong 
 interests in science, mathematics, and literature), young George gave himself the type of 
 education normally the privilege of upper-class boys; his studies included Latin, Greek, and 
 mathematics. As a result of his early papers on 
  
 mathematics, in 1849 Boole was appointed the first Professor of Mathematics at Queen's 
 College, Cork, in Ireland.",NA
Chapter 11. Gates (Not Bill),"In some far-off distant time, when the twentieth century history of primitive computing is just 
 a murky memory, someone is likely to suppose that devices known as 
 logic gates
  were named 
 after the famous co-founder of Microsoft Corporation. Not quite. As we'll soon see, logic gates 
 bear a much greater resemblance to those ordinary gates through which pass water or people. 
 Logic gates perform simple tasks in logic by blocking or letting through the flow of electrical 
 current.
  
 You'll recall how in the last chapter you went into a pet shop and announced, ""I want a 
 male cat, neutered, either white or tan; or a female cat, neutered, any color but white; or I'll 
 take any cat you have as long as it's black."" This is summarized by the following Boolean 
 expression:
  
 (M x N x (W + T)) + (F x N x (1 – W)) + B
  
 and also by this circuit made up of switches and a lightbulb:
  
  
 Such a circuit is sometimes called a 
 network
 , except that nowadays that word is used 
 much more often to refer to connected computers rather than an assemblage of mere 
 switches.
  
 Although this circuit contains nothing that wasn't invented in the nineteenth century, nobody 
 in that century ever realized that Boolean expressions could be directly realized in electrical 
 circuits. This equivalence wasn't discovered until the 1930s, most notably by Claude Elwood 
 Shannon (born 1916), whose famous 1938 M.I.T. master's thesis was entitled ""A Symbolic 
 Analysis of Relay and Switching Circuits."" (Ten years later, Shannon's article ""The 
 Mathematical Theory of Communication"" was the first publication that used the word 
 bit
  to 
 mean 
 binary digit
 .)
  
 Prior to 1938, people knew that when you wired two switches in series, both switches had to 
 be closed for current to flow, and when you wired two switches in parallel, one or the other 
 had to be closed. But nobody had shown with Shannon's clarity and rigor that electrical 
 engineers could use all the tools of Boolean algebra to design circuits with switches. In",NA
Chapter 12. A Binary Adding Machine,"Addition is the most basic of arithmetic operations, so if we want to build a computer (and 
 that is my hidden agenda in this book), we must first know how to build something that adds 
 two numbers together. When you come right down to it, addition is just about the 
 only
  thing 
 that computers do. If we can build something that adds, we're well on our way to building 
 something that uses addition to also subtract, multiply, divide, calculate mortgage payments, 
 guide rockets to Mars, play chess, and foul up our phone bills.
  
 The adding machine that we'll build in this chapter will be big, clunky, slow, and noisy, at least 
 compared to the calculators and computers of modern life. What's most interesting is that 
 we're going to build this adding machine entirely out of simple electrical devices that we've 
 learned about in previous chapters—switches, lightbulbs, wires, a battery, and relays that 
 have been prewired into various logic gates. This adding machine will contain nothing that 
 wasn't invented at least 120 years ago. And what's really nice is that we don't have to actually 
 build anything in our living rooms; instead, we can build this adding machine on paper and in 
 our minds.
  
 This adding machine will work entirely with binary numbers and will lack some modern 
 amenities. You won't be able to use a keyboard to indicate the numbers you want to add; 
 instead you'll use a row of switches. Rather than a numeric display to show the results, this 
 adding machine will have a row of lightbulbs.
  
 But this machine will definitely add two numbers together, and it will do so in a way that's 
 very much like the way that computers add numbers.
  
 Adding binary numbers is a lot like adding decimal numbers. When you want to add two 
 decimal numbers such as 245 and 673, you break the problem into simpler steps. Each step 
 requires only that you add a pair of decimal digits. In this example, you begin with 5 plus 3. 
 The problem goes a lot faster if you memorized an addition table sometime during your life.
  
 The big difference between adding decimal and binary numbers is that you use a much 
 simpler table for binary numbers:
  
 + 
  
 0 
  
 1
  
 0
  
  
 0 
  
 1
  
 1
  
  
 1 
  
 10
  
 If you actually grew up with a community of whales and memorized this table in school, 
 you might have chanted aloud:
  
 0 plus 0 equals 0.
  
 0 plus 1 equals 1.
  
 1 plus 0 equals 1.
  
 1 plus 1 equals 0, carry the 1.
  
 You can rewrite the addition table with leading zeros so that each result is a 2-bit value:
  
 + 
  
 0
  
 1",NA
Chapter 13. But What About Subtraction?,"After you've convinced yourself that relays can indeed be wired together to add binary 
 numbers, you might ask, ""But what about subtraction?"" Rest assured that you're not making a 
 nuisance of yourself by asking questions like this; you're actually being quite perceptive. 
 Addition and subtraction 
  
 complement each other in some ways, but the mechanics of the two operations are different. 
 An addition marches consistently from the rightmost column of digits to the leftmost column. 
 Each carry from one column is added to the next column. We don't 
 carry
  in subtraction, 
 however; we 
 borrow
 , and that involves an intrinsically different mechanism—a messy back-
 and-forth kind of thing.
  
 For example, let's look at a typical borrow-laden subtraction problem:
  
  
 To do this, we start with the rightmost column. First we see that 6 is bigger than 3, so we 
 have to borrow 1 from the 5, and then subtract 6 from 13, which is 7. Then we have to 
 remember that we borrowed 1 from the 5, so it's really a 4, and this 4 is smaller than 7, so we 
 borrow 1 from the 2 and subtract 7 from 14, which is 7. Then we have to remember that we 
 borrowed 1 from the 2, so it's really a 1, and then we subtract 1 from it to get 0. Our answer 
 is 77:
  
  
 Now how are we ever going to persuade a bunch of logic gates to go through such perverse 
 logic?
  
 Well, we're not going to try. Instead, we're going to use a little trick that lets us subtract 
 without 
 borrowing. This will please Polonius (""Neither a borrower nor a lender be"") and 
 the rest of us as well. Moreover, examining subtraction in detail is useful because it directly 
 relates to the way in which binary codes are used for storing negative numbers in 
 computers.
  
 For this explanation, I need to refer to the two numbers being subtracted. Their proper 
 names are the 
 minuend
  and the 
 subtrahend
 . The subtrahend is subtracted from the minuend, 
 and the result is the 
 difference:
  
  
 To subtract without borrowing, you first subtract the subtrahend 
 not
  from the minuend but 
 from 999:",NA
Chapter 14. Feedback and Flip-Flops,"Everybody knows that electricity makes things move. A brief glance around the average home 
 reveals electric motors in appliances as diverse as clocks, fans, food processors, and compact 
 disc players. Electricity also makes the cones in loudspeakers vibrate, bringing forth sounds, 
 speech, and music from the stereo system and the television set. But perhaps the simplest and 
 most elegant way that electricity makes things move is illustrated by a class of devices that 
 are quickly disappearing as electronic counterparts replace them. I refer to the marvelously 
 retro electric buzzers and bells.
  
 Consider a relay wired this way with a switch and battery:
  
  
 If this looks a little odd to you, you're not imagining things. We haven't seen a relay wired 
 quite like this yet. Usually a relay is wired so that the input is separate from the output. Here 
 it's all one big circle. If you close the switch, a circuit is completed:
  
  
 The completed circuit causes the electromagnet to pull down the flexible contact:",NA
Chapter 15. Bytes and Hex,"The two improved adding machines of the last chapter illustrate clearly the concept of 
 data 
 paths
 .
  
 Throughout the circuitry, 8-bit values move from one component to another. Eight-bit values 
 are inputs to the adders, latches, and data selectors, and also outputs from these units. Eight-
 bit values are also defined by switches and displayed by lightbulbs. The data path in these 
 circuits is thus said to be 
 8 bits wide
 . But why 
 8
  bits? Why not 6 or 7 or 9 or 10?
  
 The simple answer is that these improved adding machines were based on the original adding 
 machine in 
 Chapter 12
 , which worked with 8-bit values. But there's really no reason why it 
 had to be built that way. Eight bits just seemed at the time to be a convenient amount—a nice 
 biteful
  of bits, if you will. And perhaps I was being just a little bit sneaky, for I now confess that 
 I knew all along (and perhaps you did as well) that 8 bits of data are known as a 
 byte
 .
  
 The word 
 byte
  originated at IBM, probably around 1956. The word had its origins in the 
 word 
 bite 
 but was spelled with a 
 y
  so that nobody would mistake the word for 
 bit
 . For a 
 while, a byte meant simply the number of bits in a particular data path. But by the mid-
 1960s, in connection with the development of IBM's System/360 (their large complex of 
 business computers), the word came to mean a group of 8 bits.
  
 As an 8-bit quantity, a byte can take on values from 00000000 through 11111111. These 
 values can represent positive integers from 0 through 255, or if two's complements are used 
 to represent negative numbers, they can represent both positive and negative integers in the 
 range –128 through 127. Or a particular byte can simply represent one of 2
 8
 , or 256, different 
 things.
  
 It turns out that 8 is, indeed, a nice bite size of bits. The byte is right, in more ways than one. 
 One reason that IBM gravitated toward 8-bit bytes was the ease in storing numbers in a 
 format known as BCD (which I'll describe in 
 Chapter 23
 ). But as we'll see in the chapters 
 ahead, quite by coincidence a byte is ideal for storing text because most written languages 
 around the world (with the exception of the ideographs used in Chinese, Japanese, and 
 Korean) can be represented with fewer than 256 characters. A byte is also ideal for 
 representing gray shades in black-and-white photographs because the human eye can 
 differentiate approximately 256 shades of gray. And where 1 byte is inadequate (for 
 representing, for example, the aforementioned ideographs of Chinese, Japanese, and Korean), 
 2 bytes—which allow the representation of 2
 16
 , or 65,536, things—usually works just fine.
  
 Half a byte—that is, 4 bits—is sometimes referred to as a 
 nibble
  (and is sometimes spelled 
 nybble
 ), but this word doesn't come up in conversation nearly as often as 
 byte
 .
  
 Because bytes show up a lot in the internals of computers, it's convenient to be able to refer 
 to their values in as succinct a manner as possible. The eight binary digits 10110110, for 
 example, are certainly explicit but hardly succinct.
  
 We could always refer to bytes by their decimal equivalents, of course, but that requires 
 converting from binary to decimal—not a particularly nasty calculation, but certainly a 
 nuisance. I showed one approach in 
 Chapter 8
  that's fairly straightforward. Because each",NA
Chapter 16. An Assemblage of Memory,"As we rouse ourselves from sleep every morning, memory fills in the blanks. We remember 
 where we are, what we did the day before, and what we plan to do today. These memories 
 might come in a rush or a dribble, and maybe after some minutes a few lapses might persist 
 (""Funny, I don't remember wearing my socks to bed""), but all in all we can usually reassemble 
 our lives and achieve enough continuity to commence living another day.
  
 Of course, human memory isn't very orderly. Try to remember something about high school 
 geometry, and you're likely to start thinking about the kid who sat in front of you or the day 
 there was a fire drill just as the teacher was about to explain what QED meant.
  
 Nor is human memory foolproof. Indeed, writing was probably invented specifically to 
 compensate for the failings of human memory. Perhaps last night you suddenly woke up at 
 3:00 A.M. with a great idea for a screenplay. You grabbed the pen and paper you keep by your 
 bed specifically for that purpose, and you wrote it down so you wouldn't forget. The next 
 morning you can read the brilliant idea and start work on the screenplay. (""Boy meets girl w. 
 car chase & explosions""? That's it?) Or maybe not.
  
 We 
 write
  and we later 
 read
 . We 
 save
  and we later 
 retrieve
 . We 
 store
  and we later 
 access
 . 
 The function of memory is to keep the information intact between those two events. 
 Anytime we store information, we're making use of different types of memory. Paper is a 
 good medium for storing textual information, and magnetic tape works well for music and 
 movies.
  
 Telegraph relays too—when assembled into logic gates and then flip-flops—can store 
 information. As we've seen, a flip-flop is capable of storing 1 bit. This isn't a whole lot of 
 information, but it's a start. For once we know how to store 1 bit, we can easily store 2, or 3, 
 or more.
  
 In 
 Chapter 14
 , we encountered the level-triggered D-type flip-flop, which is made out of an 
 inverter, two AND gates, and two NOR gates:
  
  
 When the Clock input is 1, the Q output is the same as the Data input. But when the Clock 
 input goes to 0, the Q output holds the last value of the Data input. Further changes to the 
 Data input don't affect the outputs until the Clock input goes to 1 again. The logic table of the 
 flip-flop is the following:
  
 Inputs
  
  Outputs
  
 D 
  
 Clk 
  
 Q 
  
 Q-bar",NA
Chapter 17. Automation,"The human species is often amazingly inventive and industrious but at the same time 
 profoundly lazy.
  
 It's very clear that we humans don't like to work. This aversion to work is so extreme—and 
 our ingenuity so acute—that we're eager to devote countless hours designing and building 
 devices that might shave a few minutes off our workday. Few fantasies tickle the human 
 pleasure center more than a vision of relaxing in a hammock watching some newfangled 
 contraption we just built mow the lawn.
  
 I'm afraid I won't be showing plans for an automatic lawn-mowing machine in these pages. 
 But in this chapter, through a progression of ever more sophisticated machines, I 
 will
  
 automate the process of adding and subtracting numbers. This hardly sounds earth-
 shattering, I know. But the final machine in this chapter will be so versatile that it will be able 
 to solve virtually any problem that makes use of addition and subtraction, and that includes a 
 great many problems indeed.
  
 Of course, with sophistication comes complexity, so some of this might be rough going. No one 
 will blame you if you skim over the excruciating details. At times, you might rebel and promise 
 that you'll never seek electrical or mechanical assistance for a math problem ever again. But 
 stick with me because by the end of this chapter we'll have invented a machine we can 
 legitimately call a 
 computer
 .
  
 The last adder we looked at was in 
 Chapter 14
 . That version included an 8-bit latch that 
 accumulated a running total entered on one set of eight switches:
  
  
 As you'll recall, an 8-bit latch uses flip-flops to store an 8-bit value. To use this device, you first 
 momentarily press the Clear switch to set the stored contents of the latch to all zeros. Then 
 you use the switches to enter your first number. The adder simply adds this number to the 
 zero output of the latch, so the result is the number you entered. Pressing the Add switch 
 stores that number in the latch and turns on some lightbulbs to display it. Now you set up the",NA
Chapter 18. From Abaci to Chips,"Throughout recorded history, people have invented numerous clever gadgets and machines 
 in a universal quest to make mathematical calculations just a little bit easier. While the human 
 species seemingly has an innate numerical ability, we also require frequent assistance. We can 
 often conceive of problems that we can't easily solve ourselves.
  
 The development of number systems can be seen as an early tool to help people keep track of 
 commodities and property. Many cultures, including the ancient Greeks and native 
 Americans, seem to have counted with the assistance also of pebbles or kernels of grain. In 
 Europe, this led to counting boards, and in the Middle East to the familiar frame-and-bead 
 abacus:
  
  
 Although commonly associated with Asian cultures, the abacus seems to have been 
 introduced to China by traders around 1200 CE.
  
 No one has ever really enjoyed multiplication and division, but few people have done 
 anything about it. The Scottish mathematician John Napier (1550–1617) was one of those 
 few. He invented logarithms for the specific purpose of simplifying these operations. The 
 product of two numbers is simply the sum of their logarithms. So if you need to multiply two 
 numbers, you look them up in a table of logarithms, add the numbers from the table, and then 
 use the table in reverse to find the actual product.
  
 The construction of tables of logarithms occupied some of the greatest minds of the 
 subsequent 400 years while others designed little gadgets to use in place of these tables. The 
 slide rule has a long history beginning with a logarithmic scale made by Edmund Gunter 
 (1581–1626) and refined by William Oughtred (1574–1660). The history of the slide rule 
 effectively ended in 1976, when the Keuffel & Esser Company presented its last 
 manufactured slide rule to the Smithsonian Institution in Washington D.C. The cause of 
 death was the hand-held calculator.
  
 Napier also invented another multiplication aid, which is composed of strips of numbers 
 usually inscribed on bone, horn, or ivory and hence referred to as 
 Napier's Bones
 . The earliest 
 mechanical calculator was a somewhat automated version of Napier's bones built around 
 1620 by Wilhelm Schickard (1592–1635). Other calculators based on interlocking wheels, 
 gears, and levers are almost as old. Two of the more significant builders of mechanical",NA
Chapter 19. Two Classic Microprocessors,"The microprocessor—a consolidation of all the components of a central processing unit 
 (CPU) of a computer on a single chip of silicon—was born in 1971. It was a modest 
 beginning: The first microprocessor, the Intel 4004, contained about 2300 transistors. Today, 
 nearly three decades later, microprocessors made for home computers are approaching the 
 10,000,000 transistor mark.
  
 Yet what the microprocessor actually does on a fundamental level has remained unchanged. 
 While those millions of additional transistors in today's chips might be doing interesting 
 things, in an initial exploration of the microprocessor they offer more distraction than 
 enlightenment. To obtain the clearest view of what a microprocessor does, let's look at the 
 first ready-for-prime-time 
  
 microprocessors.
  
 These microprocessors appeared in 1974, the year in which Intel introduced the 8080 
 (pronounced 
 eighty eighty
 ) in April and Motorola—a company that had been making 
 semiconductors and transistor-based products since the 1950s—introduced the 6800 
 (
 sixty-eight hundred
 ) in August.
  
 These weren't the only microprocessors available that year. Also in 1974, Texas Instruments 
 introduced the 4-bit TMS 1000, which was used in many calculators, toys, and appliances; and 
 National Semiconductor introduced the PACE, which was the first 16-bit microprocessor. In 
 retrospect, however, the 8080 and the 6800 were certainly the two most historically 
 significant chips.
  
 Intel set the initial price of the 8080 at $360, a sly dig at IBM's System/360, a large mainframe 
 system used by many large corporations that cost millions. (Today you can buy an 8080 chip 
 for $1.95.) It's not as if the 8080 is comparable to System/360 in any way, but within a few 
 years IBM itself would certainly be taking notice of these very small computers.
  
 The 8080 is an 8-bit microprocessor that contains about 6000 transistors, runs at a 2 MHz 
 clock speed, and addresses 64 kilobytes of memory. The 6800 (also selling these days for 
 $1.95) has about 4000 transistors and also addresses 64 KB of memory. The first 6800 ran at 
 1 MHz, but by 1977 Motorola introduced later versions running at 1.5 and 2 MHz.
  
 These chips are referred to as 
 single-chip microprocessors
  and less accurately as 
 computers on 
 a chip
 . The processor is only one part of the whole computer. In addition to the processor, a 
 computer at the very least requires some random access memory (RAM), some way for a 
 person to get information into the computer (an input device), some way for a person to get 
 information out of the computer (an output device), and several other chips that bind 
 everything together. But I'll describe these other components in greater detail in 
 Chapter 21
 .
  
 For now, let's look at the microprocessor itself. Often a description of a microprocessor is 
  
 accompanied by a block diagram that illustrates the internal components of the 
 microprocessor and how they're connected. But we had enough of that in 
 Chapter 17
 . Instead, 
 we'll get a sense of what's inside the processor by seeing how it interacts with the outside 
 world. In other words, we can think of the microprocessor as a black box whose internal 
 operations we don't need to study minutely in order to understand what it does. We can",NA
Chapter 20. ASCII and a Cast of Characters,"Digital computer memory stores only bits, so anything that we want to work with on the 
 computer must be stored in the form of bits. We've already seen how bits can represent 
 numbers and machine code. The next challenge must be text. After all, the great bulk of the 
 accumulated information of this world is in the form of text, and our libraries are full of 
 books and magazines and newspapers. Although we'd eventually like to use our computers 
 to store sounds and pictures and movies, text is a much easier place to begin.
  
 To represent text in digital form, we must develop some kind of system in which each letter 
 corresponds to a unique code. Numbers and punctuation also occur in text, so codes for these 
 must be developed as well. In short, we need codes for all 
 alphanumeric
  characters. Such a 
 system is sometimes known as a 
 coded character set
 , and the individual codes are known as 
 character codes
 .
  
 The first question must be: How many bits do we need for these codes? The answer isn't an 
 easy one!
  
 When we think about representing text using bits, let's not get too far ahead of ourselves. 
 We're accustomed to seeing text nicely formatted on the pages of a book or in the columns of a 
 magazine or newspaper. Paragraphs are neatly separated into lines of a consistent width. Yet 
 this formatting isn't essential to the text itself. When we read a short story in a magazine and 
 years later encounter that same story in a book, we don't think the story has changed just 
 because the text column is wider in the book than in the magazine.
  
 In other words, don't think about text as formatted into two-dimensional columns on the 
 printed page. Think of text instead as a one-dimensional stream of letters, numbers, and 
 punctuation marks, with perhaps an additional code to indicate the end of one paragraph 
 and the start of another.
  
 Again, if you read a story in a magazine and later see it in a book and the typeface is a little 
 different, is that a big deal? If the magazine version begins
  
 Call me Ishmael.
  
 and the book version begins
  
 Call me Ishmael.
  
 is that something we really want to be concerned with just yet? Probably not. Yes, the typeface 
 subtly affects the tone of the text, but the story hasn't been lost with the change of typeface. 
 The typeface can always be changed back. There's no harm done.
  
 Here's another way we're going to simplify the problem: Let's stick to plain vanilla text. No 
 italics, no boldface, no underlining, no colors, no outlined letters, no subscripts, no 
 superscripts. And no accent marks. No Å or é or ñ or ö. Just the naked Latin alphabet as it's 
 used in 99 percent of English.
  
 In our earlier studies of Morse code and Braille, we've already seen how the letters of the 
 alphabet can be represented in a binary form. Although these systems are fine for their 
 specific purposes, both have their failings when it comes to computers. Morse code, for",NA
Chapter 21. Get on the Bus,"The processor is certainly the most important component of a computer, but it's not the only 
  
 component. A computer also requires random access memory (RAM) that contains machine-
 code instructions for the processor to execute. The computer must also include some way for 
 those instructions to get into RAM (an input device) and some way for the results of the 
 program to be observed (an output device). As you'll also recall, RAM is volatile—it loses its 
 contents when the power is turned off. So another useful component of a computer is a long-
 term storage device that can retain code and data when the computer is turned off.
  
 All the integrated circuits that make up a complete computer must be mounted on circuit 
 boards. In some smaller machines, all the ICs can fit on a single board. But it's more usual for 
 the various components of the computer to be divided among two or more boards. These 
 boards communicate with each other by means of a 
 bus
 . A bus is simply a collection of digital 
 signals that are provided to every board in a computer. These signals fall into four categories:
  
 Address signals. These are signals generated by the microprocessor and used mostly to 
 address random access memory. But they're also used to address other devices attached 
 to the computer.
  
 Data Output signals. These also are signals provided by the microprocessor. They're used 
 to write data to RAM or to other devices. Be careful with the terms 
 input
  and 
 output
 . A data 
 output signal from the microprocessor becomes a data input signal to RAM and other 
 devices.
  
 Data Input signals. These are signals that are provided by other parts of the computer and 
 are read by the microprocessor. The data input signals most often originate in RAM output; 
 this is how the microprocessor reads the contents of memory. But other components also 
 provide data input signals to the microprocessor.
  
 Control signals. These are miscellaneous signals that usually correspond to the control 
 signals of the particular microprocessor around which the computer is built. Control 
 signals may originate in the microprocessor or from other devices to signal the 
 microprocessor. An example of a control signal is the signal used by the microprocessor to 
 indicate that it needs to write some data output into a particular memory address.
  
 In addition, the bus supplies power to the various boards that the computer comprises.
  
 One of the earliest popular busses for home computers was the S-100 bus, which was 
 introduced in 1975 in the first home computer, the MITS Altair. Although this bus was based 
 on the 8080 microprocessor, it was later adapted to other processors such as the 6800. An S-
 100 circuit board is 5.3 inches by 10 inches. One edge of the circuit board fits into a socket 
 that has 100 connectors (hence the name S-100).
  
 An S-100 computer contains a larger board called a 
 motherboard
  (or 
 main board
 ) that 
 contains a number of S-100 sockets (perhaps 12 of them) wired to one another. These 
 sockets are sometimes called 
 expansion slots
 . The S-100 circuit boards (also called 
 expansion boards
 ) fit into these sockets. The 8080 microprocessor and support chips 
 (some of which I mentioned in 
 Chapter 19
 ) occupy one S-100 board. Random access 
 memory occupies one or more other boards.",NA
Chapter 22. The Operating System,"We have, at long last, assembled—at least in our imaginations—what seems to be a complete 
 computer. This computer has a microprocessor, some random access memory, a keyboard, a 
 video display, and a disk drive. All the hardware is in place, and we eye with excitement the 
 on/off switch that will power it up and bring it to life. Perhaps this project has evoked in 
 your mind the labors of Victor Frankenstein as he assembled his monster, or Geppetto as he 
 built the wooden puppet that he will name Pinocchio.
  
 But still we're missing something, and it's neither the power of a lightning bolt nor the purity 
 of a wish upon a star. Go ahead: Turn on this new computer and tell me what you see.
  
 As the cathode-ray tube warms up, the screen displays an array of perfectly formed—but 
 totally random—ASCII characters. This is as we expect. Semiconductor memory loses its 
 contents when the power is off and begins in a random and unpredictable state when it first 
 gets power. Likewise, all the RAM that we've constructed for the microprocessor contains 
 random bytes. The microprocessor begins executing these random bytes as if they were 
 machine code. This won't cause anything 
 bad
  to happen—the computer won't blow up, for 
 instance—but it won't be very productive either.
  
 What we're missing here is software. When a microprocessor is first turned on or reset, it 
 begins executing machine code at a particular memory address. In the case of the Intel 8080, 
 that address is 0000h. In a properly designed computer, that memory address should contain 
 a machine-code instruction (most likely the first of many) when the computer is turned on.
  
 How does that machine-code instruction get there? The process of getting software into a 
 newly designed computer is possibly one of the most confusing aspects of the project. One 
 way to do it is with a control panel similar to the one in 
 Chapter 16
  used for writing bytes 
 into random access memory and later reading them:
  
  
 Unlike the earlier control panel, this one has a switch labeled Reset. The Reset switch is 
 connected to the Reset input of the microprocessor. As long as that switch is on, the 
 microprocessor doesn't do anything. When you turn off the switch, the microprocessor begins 
 executing machine code.
  
 To use this control panel, you turn the Reset switch on to reset the microprocessor and to stop 
 it from executing machine code. You turn on the Takeover switch to take over the address",NA
"Chapter 23. Fixed Point, Floating Point","Numbers are numbers, and in most of our daily lives we drift casually between whole 
 numbers, fractions, and percentages. We buy half a carton of eggs and pay 8 ¼ percent sales 
 tax with money earned getting time-and-a-half for working 2 ¾ hours overtime. Most people 
 are fairly comfortable—if not necessarily proficient—with numbers such as these. We can 
 even hear a statistic like ""the average American house-hold has 2.6 people"" without gasping in 
 horror at the widespread mutilation that must have occurred to achieve this.
  
 Yet this interchange between whole numbers and fractions isn't so casual when it comes to 
 computer memory. Yes, everything is stored in computers in the form of bits, which means 
 that everything is stored as binary numbers. But some kinds of numbers are definitely easier 
 to express in terms of bits than others.
  
 We began using bits to represent what mathematicians call the positive 
 whole numbers
  and 
 what computer programmers call the positive 
 integers
 . We've also seen how two's 
 complements allow us to represent 
 negative
  integers in a way that eases the addition of 
 positive and negative numbers. The table on the following page shows the range of positive 
 integers and two's-complement integers for 8, 16, and 32 bits of storage.
  
 Number of Bits
  
  Range of Positive Integers
  
  Range of Two's-Complement Integers
  
 8
  
 0 through 255
  
 –128 through 127
  
 16
  
 0 through 65,535
  
 –32,768 through 32,767
  
 32
  
 0 through 4,294,967,295
  
 –2,147,483,648 through 2,147,483,647
  
 But that's where we stopped. Beyond whole numbers, mathematicians also define 
 rational
  
 numbers as those numbers that can be represented as a 
 ratio
  of two whole numbers. This 
 ratio is also referred to as a 
 fraction
 . For example, ¾ is a rational number because it's the ratio 
 of 3 and 4. We can also write this number in 
 decimal fraction
 , or just 
 decimal
 , form: 0.75. When 
 we write it as a decimal, it
  
 really indicates a fraction, in this case 
  
 You'll recall from 
 Chapter 7
  that in a decimal number system, digits to the left of the decimal 
 point are multiples of integral powers of ten. Similarly, digits to the right of the decimal point 
 are multiples of 
 negative
  powers of ten. In 
 Chapter 7
 , I used the example 42,705.684, showing 
 first that it's equal to
  
 4 x 10,000 +
  
 2 x 1000 +
  
 7 x 100 +
  
 0 x 10 +
  
 5 x 1 +
  
 6 ÷ 10 +
  
 8 ÷ 100 +
  
 4 ÷ 1000",NA
Chapter 24. Languages High and Low,"Programming in machine code is like eating with a toothpick. The bites are so small and the 
 process so laborious that dinner takes forever. Likewise, the bytes of machine code perform 
 the tiniest and simplest of imaginable computing tasks—loading a number from memory into 
 the processor, adding it to another, storing the result back to memory—so that it's difficult to 
 imagine how they contribute to an entire meal.
  
 We have at least progressed from that primitive era at the beginning of 
 Chapter 22
 , in which 
 we were using switches on a control panel to enter binary data into memory. In that chapter, 
 we discovered how we could write simple programs that let us use the keyboard and the 
 video display to enter and examine hexadecimal bytes of machine code. This was certainly 
 better, but it's not the last word in improvements.
  
 As you know, the bytes of machine code are associated with certain short mnemonics, such as 
 MOV
 , 
 ADD
 , 
 CALL
 , and 
 HLT
 , that let us refer to the machine code in something vaguely 
 resembling English.
  
 These mnemonics are often written with operands that further indicate what the machine-
 code instruction does. For example, the 8080 machine-code byte 46h causes the 
 microprocessor to move into register B the byte stored at the memory address referenced 
 by the 16-bit value in the register pair HL. This is more concisely written as
  
 MOV B,[HL]
  
 Of course, it's much easier to write programs in assembly language than in machine code, 
 but the microprocessor can't understand assembly language. I've explained how you'd 
 write assembly-language programs on paper. Only when you thought you were ready to 
 run an assembly-language program on the microprocessor would you hand-assemble it, 
 which means that you'd convert the assembly-language statements to machine-code bytes 
 and enter them into memory.
  
 What's even better is for the computer to do this conversion for you. If you were running 
 the CP/M operating system on your 8080 computer, you'd already have all the tools you 
 need. Here's how it works.
  
 First you create a text file to contain your program written in assembly language. You can use 
 the CP/M program ED.COM for this job. This program is a text editor, which means that it 
 allows you to create and modify text files. Let's suppose you create a text file with the name 
 PROGRAM1.ASM. The ASM file type indicates that this file contains an assembly-language 
 program. The file might look something like this:
  
  
  ORG 0100h
  
  
  LXI DE, Text
  
  
  MVI C,9
  
  
  CALL 5
  
  
  RET 
  
 Text: DB 'Hello!$'
  
  
  END
  
 This file has a couple of statements we haven't seen before. The first one is an 
 ORG
  (for 
 origin
 ) statement. This statement does 
 not
  correspond to an 8080 instruction. Instead, it",NA
Chapter 25. The Graphical Revolution,"Readers of the September 10, 1945, issue of 
 Life
  magazine encountered mostly the usual 
 eclectic mix of articles and photographs: stories about the end of the Second World War, an 
 account of dancer Vaslav Nijinsky's life in Vienna, a photo essay on the United Auto Workers. 
 Also included in that issue was something unexpected: a provocative article by Vannevar 
 Bush (1890–1974) about the future of scientific research. Van Bush (as he was called) had 
 already made his mark in the history of computing by designing one of the most significant 
 analog computers—the differential analyzer—between 1927 and 1931 while an engineering 
 professor at MIT. At the time of the 
 Life
  article in 1945, Bush was serving as Director of the 
 Office of Scientific Research and Development, which had been responsible for coordinating 
 U.S. scientific activities during the war, including the Manhattan Project.
  
 Condensed somewhat from its first appearance two months earlier in 
 The Atlantic Monthly
 , 
 Bush's 
 Life
  article ""As We May Think"" described some hypothetical inventions of the future 
 ostensibly for the scientist and researcher who must deal with an ever-increasing number of 
 technical journals and articles. Bush saw microfilm as the solution and imagined a device he 
 called the 
 Memex
  to store books, articles, records, and pictures inside a desk. The Memex 
 also allowed the user to establish thematic connections among these works, according to the 
 associations normally made by the human mind. He even imagined a new professional group 
 of people who would forge these trails of association through massive bodies of information.
  
 Although articles about the delights of the future have been common throughout the 
 twentieth century, ""As We May Think"" is different. This isn't a story about household 
 laborsaving devices or futuristic transportation or robots. This is a story about 
 information
  
 and how new technology can help us successfully deal with it.
  
 Through the six and a half decades since the first relay calculators were built, computers have 
 become smaller, faster, and cheaper all at the same time. This trend has changed the very 
 nature of computing. As computers get cheaper, each person can have his or her own. As 
 computers get smaller and faster, software can become more sophisticated and the machines 
 can assume more and more work.
  
 One way in which this extra power and speed can be put to good use is in improving the most 
 crucial part of the computer system, which is the 
 user interface
 —the point at which human 
 and computer meet. People and computers are very different animals, and unfortunately it's 
 easier to persuade people to make adjustments to accommodate the peculiarities of 
 computers than the other way around.
  
 In the early days, digital computers weren't interactive at all. Some of them were 
 programmed using switches and cables, while others used punched paper tape or film. By the 
 1950s and 1960s (and even continuing into the 1970s), computers had evolved to the point 
 where 
 batch processing
  was the norm: Programs and data were punched on cards, which 
 were then read into computer memory. The program analyzed the data, drew some 
 conclusions, and printed the results on paper.
  
 The earliest interactive computers used teletypewriters. Setups such as the Dartmouth 
 time-sharing system (dating from the early 1960s) that I described in the preceding chapter 
 supported multiple teletypewriters that could be used at the same time. In such a system, a",NA
Appendix A. Acknowledgments ,"Code
  was conceived in 1987. It rattled around in my head for nearly a decade and was finally 
 committed to a Microsoft Word file between January 1996 and July 1999. I offer many thanks: 
 to the readers of early drafts of 
 Code
  who contributed comments, criticisms, and suggestions: 
 Sheryl Canter, Jan Eastlund, Peter Goldeman, Lynn Magalska, and Deirdre Sinnott; 
  
 to my agent, Claudette Moore of Moore Literary Agency, and to everyone at Microsoft Press 
 who helped make 
 Code
  a reality, particularly those whose names are listed on the copyright 
 page of this book and on the colophon, following the index; 
  
 to my mother, who never held me back; 
  
 to Little Cat, who shared my apartment with me from 1982 through May 1999, and who 
 inspired many cat references in my writing; 
  
 to Web sites such as Bibliofind (
 www.bibliofind.com
 ) and Advanced Book Exchange 
  
 (
 www.abebooks.com
 ) that offer convenient access to used books, and to the staff of the 
 Science, Industry, and Business Library (SIBL) branch of the New York Public Library 
 (
 www.nypl.org
 ); to my friends in the rooms, without whose support none of this would be 
 possible; 
  
 and again to Deirdre, my ideal reader and so much more.
  
 Charles Petzold 
  
 July 15, 1999",NA
Appendix B. Bibliography ,"An annotated bibliography for this book is available on the World Wide 
 Web site 
 www.charlespetzold.com/code
 .",NA
About the Author,"Charles Petzold wrote the classic Programming Windows®, which is currently in its fifth 
 edition and one of the best-known and widely used programming books of all time. He was 
 honored in 1994 with the Windows Pioneer Award, presented by Microsoft® founder Bill 
 Gates and Windows Magazine. He has been programming with Windows since first obtaining 
 a beta Windows 1.0 SDK in the spring of 1985, and he wrote the very first magazine article on 
 Windows programming in 1986. Charles is an MVP for Client Application Development and 
 the author of several other books including Code: The Hidden Language of Computer 
 Hardware and Software.",NA
Colophon ,"The manuscript for this book was prepared using Microsoft Word 2000. Pages were 
 composed using Adobe PageMaker 6.52, with text and display type in Sabon and math fonts 
 in Syntax. Composed pages were delivered to the printer as electronic prepress files.
  
 Dust Jacket and Cover Graphic Designer 
  
 Greg Hickman 
  
 Interior Book Design 
  
 Jimmie Young and Sally Slevin 
  
 Illustrator 
  
 Joel Panchot 
  
 Compositor 
  
 Elizabeth Hansford 
  
 Principal Proofreader/Copy Editor 
  
 Shawn Peck 
  
 Indexer 
  
 Liz Cunningham",NA
SPECIAL OFFER: Upgrade this ebook with O’Reilly ,"Upgrade this ebook today 
 for $4.99 at oreilly.com
  and get access to additional DRM-free 
 formats, including PDF and EPUB, along with free lifetime updates.",NA
