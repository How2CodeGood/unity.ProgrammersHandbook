Larger Text,Smaller Text,Symbol
BradlEy Efron ,NA,NA
trEvor hastiE,"elect
 Begi 
 theo
  
 – ind
 of inf
 logis
 jack
 k
  
 neu
 r 
  
 Carl
 o",NA
ComputEr agE ,NA,NA
statistiCal ,NA,NA
infErEnCE,"and 
 appr 
 algor
 book 
 direc",NA
"algorithms, EvidEnCE, and data sCiEnCE",NA,NA
Computer Age Statistical ,NA,NA
Inference ,"Algorithms, Evidence, and Data 
 Science
  
 Bradley Efron 
  
 Trevor 
 Hastie 
  
 Stanford University",NA
Contents,"Preface 
  
 xv
  
 Acknowledgment
 s 
  
 Notation
  
 xviii 
  
 xix
  
 1
  
 3 
  
 4 
  
 8 
  
 11
  
 1
 2 
  
 1
 4 
  
 1
 8 
  
 2
 0
  
 2
 2 
  
 2
 4 
  
 2
 8 
  
 3
 0 
  
 3
 3 
  
 3
 6
  
 3
 8 
  
 3
 8 
  
 4
 1 
  
 4
 5 
  
 4
 9 
  
 5
 1
  
 53
  
 Part I
  
 Classic Statistical Inference
  
 1
  
 Algorithms and Inference
  
 1.1
  
 A Regression Example
  
 1.2
  
 Hypothesis Testing
  
 1.3
  
 Notes
  
 2
  
 Frequentist Inference
  
 2.1
  
 Frequentism in Practice
  
 2.2
  
 Frequentist Optimality
  
 2.3
  
 Notes and Details
  
 3
  
 Bayesian Inference
  
 3.1
  
 Two Examples
  
 3.2
  
 Uninformative Prior Distributions
  
 3.3
  
 Flaws in Frequentist Inference
  
 3.4
  
 A Bayesian/Frequentist Comparison List
  
 3.5
  
 Notes and Details
  
 4
  
 Fisherian Inference and Maximum Likelihood Estimation
  
 4.1
  
 Likelihood and Maximum Likelihood
  
 4.2
  
 Fisher Information and the MLE
  
 4.3
  
 Conditional Inference
  
 4.4
  
 Permutation and Randomization
  
 4.5
  
 Notes and Details
  
 5
  
 Parametric Models and Exponential Families
  
 ix",NA
Preface,"Statistical inference is an unusually wide-ranging discipline, located 
 as it is at the triple-point of mathematics, empirical science, and 
 philosophy. The discipline can be said to date from 1763, with the 
 publication of Bayes’rule (representing the philosophical side of the 
 subject; the rule’s early ad-vocates considered it an argument for the 
 existence of God). The most re-cent quarter of this 250-year 
 history—from the 1950s to the present—is the “computer age” of 
 our book’s title, the time when computation, the tra-ditional 
 bottleneck of statistical applications, became faster and easier by a 
 factor of a million.
  
 The book is an examination of how statistics has evolved over the 
 past sixty years—an aerial view of a vast subject, but seen from the 
 height of a small plane, not a jetliner or satellite. The individual 
 chapters take up a se-ries of influential topics—generalized linear 
 models, survival analysis, the jackknife and bootstrap, false-
 discovery rates, empirical Bayes, MCMC, neural nets, and a dozen 
 more—describing for each the key methodologi-cal developments 
 and their inferential justification.
  
 Needless to say, the role of electronic computation is central to our 
 story. This doesn’t mean that every advance was computer-related. A 
 land bridge had opened to a new continent but not all were eager to 
 cross. Topics such as empirical Bayes and James–Stein estimation 
 could have emerged just as well under the constraints of mechanical 
 computation. Oth-ers, like the bootstrap and proportional hazards, 
 were pureborn children of the computer age. Almost all topics in 
 twenty-first-century statistics are now computer-dependent, but it 
 will take our small plane a while to reach the new millennium.
  
 Dictionary definitions of statistical inference tend to equate it with 
 the entire discipline. This has become less satisfactory in the “big 
 data” era of immense computer-based processing algorithms. Here 
 we will attempt, not always consistently, to separate the two aspects 
 of the statistical enterprise: algorithmic developments aimed at 
 specific problem areas, for instance
  
 xv",NA
Acknowledgments,"We are indebted to Cindy Kirby for her skillful work in the 
 preparation of this book, and Galit Shmueli for her helpful comments 
 on an earlier draft. At Cambridge University Press, a huge thank you 
 to Steven Holt for his ex-cellent copy editing, Clare Dennison for 
 guiding us through the production phase, and to Diana Gillooly, our 
 editor, for her unfailing support.
  
 Bradley Efron 
  
 Trevor Hastie 
  
 Department of 
 Statistics 
  
 Stanford University 
  
 May 2016
  
 xviii",NA
Part I ,NA,NA
Classic Statistical ,NA,NA
Inference,NA,NA
1,NA,NA
Algorithms and Inference,"Statistics is the science of learning from experience, particularly 
 experi-ence that arrives a little bit at a time: the successes and 
 failures of a new experimental drug, the uncertain measurements of 
 an asteroid’s path to-ward Earth. It may seem surprising that any one 
 theory can cover such an amorphous target as “learning from 
 experience.” In fact, there are
  two
  main statistical theories, 
 Bayesianism and frequentism, whose connections and disagreements 
 animate many of the succeeding chapters.
  
 First, however, we want to discuss a less philosophical, more 
 operational division of labor that applies to both theories: between 
 the
  algorithmic
  and 
 inferential
  aspects of statistical analysis. The 
 distinction begins with the most basic, and most popular, statistical 
 method, averaging. Suppose we have observed numbers x
 1
 ; x
 2
 ; : : : ; x
 n
  
 applying to some phenomenon of interest, perhaps the automobile 
 accident rates in the n D 50 states. The 
 mean
  
 Nx D X x
 i
 =n 
  
 (1.1)
  
 summarizes the results in a single number.
  
 How accurate is that number? The textbook answer is given in terms 
 of the
  standard error
 ,
  
 Here
  averaging
  (1.1) is the algorithm, while the standard error provides an bse D ""
  n 
 X .x
 i
   
 Nx/
 2
 ı.n.n 1// #
 1=2 
 : 
  
 (1.2)
  
 inference of the algorithm’s accuracy. It is a surprising, and crucial, 
 aspect of statistical theory that the same data that supplies an 
 estimate can also assess its accuracy.
 1
  
 1
  “Inference” concerns more than accuracy: speaking broadly, algorithms say what 
 the statistician does while inference says why he or she does it.
  
 3",NA
2,NA,NA
Frequentist Inference,"Before the computer age there was the calculator age, and before “big 
 data”there were small data sets, often a few hundred numbers or 
 fewer, labori-ously collected by individual scientists working under 
 restrictive experi-mental constraints. Precious data calls for 
 maximally efficient statistical analysis. A remarkably effective theory, 
 feasible for execution on mechan-ical desk calculators, was 
 developed beginning in 1900 by Pearson, Fisher, Neyman, Hotelling, 
 and others, and grew to dominate twentieth-century statistical 
 practice. The theory, now referred to as
  classical
 , relied almost 
 entirely on frequentist inferential ideas. This chapter sketches a 
 quick and simplified picture of frequentist inference, particularly as 
 employed in clas-sical applications.
  
 We begin with another example from Dr. Myers’ nephrology 
 laboratory: 211 kidney patients have had their
  glomerular filtration 
 rates
  measured, with the results shown in Figure 2.1;
  gfr
  is an 
 important indicator of kid-ney function, with low values suggesting 
 trouble. (It is a key component of 
 tot
  in Figure 1.1.) The mean and 
 standard error (1.1)–(1.2) are Nx D 54:25 and bse D 0:95, typically 
 reported as 54:25 ˙ 0:95I 
  
 (2.1)
  
 ˙0:95 denotes a frequentist inference for the accuracy of the estimate 
 Nx D 54:25, and suggests that we shouldn’t take the “.25” very 
 seriously, even the “4” being open to doubt. Where the inference 
 comes from and what exactly it means remains to be said.
  
 Statistical inference usually begins with the assumption that some 
 prob-ability model has produced the observed data x, in our case the 
 vector of n D 211
  gfr
  measurements x D .x
 1
 ; x
 2
 ; : : : ; x
 n
 /. Let X D .X
 1
 ; 
 X
 2
 ; : : : ; X
 n
 / indicate n independent draws from a probability 
 distribution F , writ-ten
  
 F ! X; 
  
 (2.2)
  
 12",NA
3,NA,NA
Bayesian Inference,"The human mind is an inference machine: “It’s getting windy, the sky 
 is darkening, I’d better bring my umbrella with me.” Unfortunately, 
 it’s not a very dependable machine, especially when weighing 
 complicated choices against past experience.
  Bayes’ theorem
  is a 
 surprisingly simple mathemat-ical guide to accurate inference. The 
 theorem (or “rule”), now 250 years old, marked the beginning of 
 statistical inference as a serious scientific sub-ject. It has waxed and 
 waned in influence over the centuries, now waxing again in the 
 service of computer-age applications.
  
 Bayesian inference, if not directly opposed to frequentism, is at 
 least or-thogonal. It reveals some worrisome flaws in the frequentist 
 point of view, while at the same time exposing itself to the criticism 
 of dangerous overuse. The struggle to combine the virtues of the two 
 philosophies has become more acute in an era of massively 
 complicated data sets. Much of what follows in succeeding chapters 
 concerns this struggle. Here we will review some basic Bayesian 
 ideas and the ways they impinge on frequentism.
  
 The fundamental unit of statistical inference both for frequentists 
 and for Bayesians is a
  family
  of probability densities
  
 F
  D˚f.x/I x 2
  X
  ;  2 
  
 I 
  
 (3.1)
  
 x, the observed data, is a point
 1
 in the
  sample space
  X
  , while the unob-
 served parameter is a point in the
  parameter space
 . The statistician 
 observes x from f.x/, and infers the value of. 
  
 Perhaps the most familiar case is the normal family
  
 f.x/ D
  
 1 
  
 p 2
  
 e
  1 2
 .x/
 2
  
 (3.2)
  
 1
  Both x and may be scalars, vectors, or more complicated objects. Other names for 
 the generic “x” and “” occur in specific situations, for instance x for x in Chapter 2. 
 We will also call
  F
  a “family of probability distributions.”
  
 22",NA
4,NA,NA
Fisherian Inference and ,NA,NA
Maximum Likelihood Estimation,"Sir Ronald Fisher was arguably the most influential anti-Bayesian of 
 all time, but that did not make him a conventional frequentist. His 
 key data-analytic methods—analysis of variance, significance testing, 
 and maxi-mum likelihood estimation—were almost always applied 
 frequentistically. Their Fisherian rationale, however, often drew on 
 ideas neither Bayesian nor frequentist in nature, or sometimes the 
 two in combination. Fisher’s work held a central place in twentieth-
 century applied statistics, and some of it, particularly maximum 
 likelihood estimation, has moved forcefully into computer-age 
 practice. This chapter’s brief review of Fisherian meth-odology 
 sketches parts of its unique philosophical structure, while concen-
 trating on those topics of greatest current importance.
  
 4.1 Likelihood and Maximum Likelihood
  
 Fisher’s seminal work on estimation focused on the likelihood 
 function, or more exactly its logarithm. For a family of probability 
 densities f.x/ (3.1), the
  log likelihood function
  is
  
 l
 x
 ./ D logff.x/g; 
  
 (4.1)
  
 the notation l
 x
 ./ emphasizing that the parameter vector is varying 
 while the observed data vector x is fixed. The
  maximum likelihood 
 esti-mate
  (MLE) is the value of in parameter space that maximizes 
 l
 x
 ./,
  
 MLE W O D arg max 
 2 
  
 fl
 x
 ./g: 
  
 (4.2)
  
 It can happen that O doesn’t exist or that there are multiple 
 maximizers, but here we will assume the usual case where O exists 
 uniquely. More careful references are provided in the endnotes.
  
 Definition (4.2) is extended to provide maximum likelihood estimates
  
 38",NA
5,NA,NA
Parametric Models and Exponential Families,"We have been reviewing classic approaches to statistical inference—
 fre-quentist, Bayesian, and Fisherian—with an eye toward examining 
 their strengths and limitations in modern applications. Putting 
 philosophical dif-ferences aside, there is a common methodological 
 theme in classical statis-tics: a strong preference for low-dimensional 
 parametric models; that is, for modeling data-analysis problems 
 using parametric families of probability densities (3.1),
  
 where the dimension of parameter is small, perhaps no greater than 5 
 F
  D˚f.x/I x 2
  X
  ;  2 
  
 ; 
  
 (5.1)
  
 or 10 or 20. The inverted nomenclature “nonparametric” suggests 
 the pre-dominance of classical parametric methods.
  
 Two words explain the classic preference for parametric models: 
 math-ematical tractability. In a world of sliderules and slow 
 mechanical arith-metic, mathematical formulation, by necessity, 
 becomes the computational tool of choice. Our new computation-rich 
 environment has unplugged the mathematical bottleneck, giving us a 
 more realistic, flexible, and far-reach-ing body of statistical 
 techniques. But the classic parametric families still play an important 
 role in computer-age statistics, often assembled as small parts of 
 larger methodologies (as with the generalized linear models of 
 Chapter 8). This chapter
 1
 presents a brief review of the most widely 
 used parametric models, ending with an overview of exponential 
 families, the great connecting thread of classical theory and a player 
 of continuing im-portance in computer-age applications.
  
 1
  This chapter covers a large amount of technical material for use later, and 
 may be reviewed lightly at first reading.
  
 53",NA
Part II ,NA,NA
Early Computer-Age ,NA,NA
Methods,NA,NA
6,NA,NA
Empirical Bayes,"The constraints of slow mechanical computation molded classical 
 statistics into a mathematically ingenious theory of sharply delimited 
 scope. Emerg-ing after the Second World War, electronic 
 computation loosened the com-putational stranglehold, allowing a 
 more expansive and useful statistical methodology.
  
 Some revolutions start slowly. The journals of the 1950s continued 
 to emphasize classical themes: pure mathematical development 
 typically cen-tered around the normal distribution. Change came 
 gradually, but by the 1990s a new statistical technology, computer 
 enabled, was firmly in place. Key developments from this period are 
 described in the next several chap-ters. The ideas, for the most part, 
 would not startle a pre-war statistician, but their computational 
 demands, factors of 100 or 1000 times those of classical methods, 
 would. More factors of a thousand lay ahead, as will be told in Part III, 
 the story of statistics in the twenty-first century.
  
 Empirical Bayes methodology, this chapter’s topic, has been a 
 particu-larly slow developer despite an early start in the 1940s. The 
 roadblock here was not so much the computational demands of the 
 theory as a lack of ap-propriate data sets. Modern scientific 
 equipment now provides ample grist for the empirical Bayes mill, as 
 will be illustrated later in the chapter, and more dramatically in 
 Chapters 15–21.
  
 6.1 Robbins’ Formula
  
 Table 6.1 shows one year of claims data for a European automobile 
 insur-ance company; 7840 of the 9461 policy holders made no claims 
 during the year, 1317 made a single claim, 239 made two claims 
 each, etc., with Ta-ble 6.1 continuing to the one person who made 
 seven claims. Of course the insurance company is concerned about 
 the claims each policy holder will make in the
  next
  year.
  
 Bayes’ formula seems promising here. We suppose that x
 k
 , the number
  
 75",NA
7,NA,NA
James–Stein Estimation and ,NA,NA
Ridge Regression,"If Fisher had lived in the era of “apps,” maximum likelihood 
 estimation might have made him a billionaire. Arguably the 
 twentieth century’s most influential piece of applied mathematics, 
 maximum likelihood continues to be a prime method of choice in the 
 statistician’s toolkit. Roughly speaking, maximum likelihood provides 
 nearly unbiased estimates of nearly mini-mum variance, and does so 
 in an automatic way.
  
 That being said, maximum likelihood estimation has shown itself 
 to be an inadequate and dangerous tool in many twenty-first-century 
 applica-tions. Again speaking roughly, unbiasedness can be an 
 unaffordable luxury when there are hundreds or thousands of 
 parameters to estimate at the same time.
  
 The James–Stein estimator made this point dramatically in 1961, 
 and made it in the context of just a few unknown parameters, not 
 hundreds or thousands. It begins the story of
  shrinkage estimation
 , in 
 which deliberate biases are introduced to improve overall 
 performance, at a possible danger to individual estimates. Chapters 7 
 and 21 will carry on the story in its modern implementations.
  
 7.1 The James–Stein Estimator
  
 Suppose we wish to estimate a single parameter from observation x 
 in the Bayesian situation
  
  N
  .M; A/ and xj
  N
  .; 1/; 
  
 (7.1)
  
 in which case has posterior distribution
  
 jx
  N
  .M C B.x M/; B/
  
 ŒB D A=.A C 1/ 
  
 (7.2)
  
 as given in (5.21) (where we take
 2
 D 1 for convenience). The Bayes estimator of,
  
 O
 Bayes
 D M C B.x M/; 
  
 (7.3)
  
 91",NA
9,NA,NA
Survival Analysis and the EM Algorithm,"Survival analysis had its roots in governmental and actuarial 
 statistics, spanning centuries of use in assessing life expectancies, 
 insurance rates, and annuities. In the 20 years between 1955 and 
 1975, survival analysis was adapted by statisticians for application to 
 biomedical studies. Three of the most popular post-war statistical 
 methodologies emerged during this period: the Kaplan–Meier 
 estimate, the log-rank test,
 1
 and Cox’s pro-portional hazards model, 
 the succession showing increased computational demands along 
 with increasingly sophisticated inferential justification. A connection 
 with one of Fisher’s ideas on maximum likelihood estimation leads in 
 the last section of this chapter to another statistical method that 
 has“gone platinum,” the EM algorithm.
  
 9.1 Life Tables and Hazard Rates
  
 An insurance company’s
  life table
  appears in Table 9.1, showing its 
 number of clients (that is, life insurance policy holders) by age, and 
 the number of deaths during the past year in each age group,
 2
 for 
 example five deaths among the 312 clients aged 59. The column 
 labeledOS is of great interest to the company’s actuaries, who have to 
 set rates for new policy holders. It is an estimate of survival 
 probability: probability 0.893 of a person aged 30 (the beginning of 
 the table) surviving past age 59, etc.OS is calculated according to an 
 ancient but ingenious algorithm.
  
 Let X represent a typical lifetime, so
  
 f
 i
  D PrfX D ig 
  
 (9.1)
  
 1
  Also known as the Mantel–Haenszel or Cochran–Mantel–Haenszel test.
  
 2
  The insurance company is fictitious but the deaths y are based on the true 2010 
 rates for US men, per Social Security Administration data.
  
 131",NA
10,NA,NA
The Jackknife and the Bootstrap,"A central element of frequentist inference is the
  standard error
 . An 
 algo-rithm has produced an estimate of a parameter of interest, for 
 instance the mean Nx D 0:752 for the 47
  ALL
  scores in the top panel 
 of Figure 1.4. How accurate is the estimate? In this case, formula (1.2) 
 for the standard deviation
 1
 of a sample mean gives estimated 
 standard error
  
 so one can’t take the third digit of Nx D 0:752 very seriously, and 
 even the 5 is dubious. bse D 0:040; 
  
  
 (10.1)
  
 Direct standard error formulas like (1.2) exist for various forms of 
 aver-aging, such as linear regression (7.34), and for hardly anything 
 else. Tay-lor series approximations (“device 2” of Section 2.1) extend 
 the formulas to smooth functions of averages, as in (8.30). Before 
 computers, applied statisticians needed to be Taylor series experts in 
 laboriously pursuing the accuracy of even moderately complicated 
 statistics.
  
 The jackknife (1957) was a first step toward a computation-based, 
 non-formulaic approach to standard errors. The bootstrap (1979) 
 went further toward automating a wide variety of inferential 
 calculations, including stan-dard errors. Besides sparing statisticians 
 the exhaustion of tedious routine calculations the jackknife and 
 bootstrap opened the door for more com-plicated estimation 
 algorithms, which could be pursued with the assurance that their 
 accuracy would be easily assessed. This chapter focuses on stan-dard 
 errors, with more adventurous bootstrap ideas deferred to Chapter 
 11. We end with a brief discussion of accuracy estimation for robust 
 statistics.
  
 1
  We will use the terms “standard error” and “standard deviation” interchangeably.",NA
11,NA,NA
Bootstrap Confidence Intervals,"The jackknife and the bootstrap represent a different use of modern 
 com-puter power: rather than extending classical methodology—
 from ordinary least squares to generalized linear models, for 
 example—they extend the reach of classical inference.
  
 Chapter 10 focused on standard errors. Here we will take up a 
 more am-bitious inferential goal, the bootstrap automation of 
 confidence intervals.
  
 The familiar
  standard intervals
  
 for approximate 95% coverage, are immensely useful in practice but 
 often O ˙ 1:96 bse; 
  
  
 (11.1)
  
 not very accurate. If we observeO D 10 from a Poisson modelO Poi./, 
 the standard 95% interval .3:8; 16:2/ (using bse D O
 1=2
 ) is a mediocre 
 ap-.5:1; 17:8/: (11.2)
  
 Standard intervals (11.1) are symmetric aroundO, this being their 
 main weakness. Poisson distributions grow more variable as 
 increases, which is why interval (11.2) extends farther to the right 
 ofO D 10 than to the left. Correctly capturing such effects in an 
 automatic way is the goal of bootstrap confidence interval theory.
  
 11.1 Neyman’s Construction for One-
 Parameter Problems
  
 The student score data of Table 3.1 comprised n D 22 pairs,
  
 x
 i
  D .m
 i
 ; v
 i
 /; i D 1; 2; : : : ; 22; 
  
 (11.3)
  
 1
  Using the Neyman construction of Section 11.1, as explained there; see also Table 
 11.2 in Section 11.4.
  
 181",NA
12,NA,NA
Cross-Validation and,NA,NA
 C,p,NA
 Estimates ,NA,NA
of Prediction Error,"Prediction has become a major branch of twenty-first-century 
 commerce. Questions of prediction arise naturally: how credit-
 worthy is a loan appli-cant? Is a new email message
  spam
 ? How 
 healthy is the kidney of a poten-tial donor? Two problems present 
 themselves: how to construct an effective prediction rule, and how to 
 estimate the accuracy of its predictions. In the language of Chapter 1, 
 the first problem is more algorithmic, the second more inferential. 
 Chapters 16–19, on
  machine learning
 , concern predic-tion rule 
 construction. Here we will focus on the second question: having 
 chosen a particular rule, how do we estimate its predictive accuracy?
  
 Two quite distinct approaches to prediction error assessment 
 developed in the 1970s. The first, depending on the classical 
 technique of cross-validation, was fully general and nonparametric. A 
 narrower (but more efficient) model-based approach was the 
 second, emerging in the form of Mallows’ C
 p
  estimate and the Akaike 
 information criterion (AIC). Both theories will be discussed here, 
 beginning with cross-validation, after a brief overview of prediction 
 rules.
  
 12.1 Prediction Rules
  
 Prediction problems typically begin with a
  training set
  d consisting of 
 N pairs .x
 i
 ; y
 i
 /,
  
 d D f.x
 i
 ; y
 i
 /; i D 1; 2; : : : ; Ng ; (12.1)
  
 where x
 i
  is a vector of p
  predictors
  and y
 i
  a real-valued
  response
 . On 
 the basis of the training set, a
  prediction rule
  r
 d
 .x/ is constructed such 
 that a prediction Oy is produced for any point x in the predictor’s 
 sample space
  X
  ,
  
 Oy D r
 d
 .x/
  
 for x 2
  X
  :
  
 (12.2)
  
 208",NA
13,NA,NA
Objective Bayes Inference and Markov ,NA,NA
Chain Monte Carlo,"From its very beginnings, Bayesian inference exerted a powerful 
 influence on statistical thinking. The notion of a single coherent 
 methodology em-ploying only the rules of probability to go from 
 assumption to conclusion was and is immensely attractive. For 200 
 years, however, two impediments stood between Bayesian theory’s 
 philosophical attraction and its practical application.
  
 1
  In the absence of relevant past experience, the choice of a prior 
 distribu-tion introduces an unwanted subjective element into 
 scientific inference. 
 2
  Bayes’ rule (3.5) looks simple enough, but 
 carrying out the numerical calculation of a posterior distribution 
 often involves intricate higher-dimensional integrals.
  
 The two impediments fit neatly into the dichotomy of Chapter 1, the 
 first being inferential and the second algorithmic.
 1 
  
 A renewed cycle of Bayesian enthusiasm took hold in the 1960s, at 
 first concerned mainly with coherent inference. Building on work by 
 Bruno de Finetti and L. J. Savage, a principled theory of
  subjective 
 probability
  was constructed: the Bayesian statistician, by the careful 
 elicitation of prior knowledge, utility, and belief, arrives at the 
 correct
  subjective prior dis-tribution
  for the problem at hand. 
 Subjective Bayesianism is particularly appropriate for individual 
 decision making, say for the business executive trying to choose the 
 best investment in the face of uncertain information.
  
 It is less appropriate for scientific inference, where the sometimes 
 skep-tical world of science puts a premium on objectivity. An answer 
 came from the school of
  objective Bayes inference
 . Following the 
 approach of Laplace and Jeffreys, as discussed in Section 3.2, their 
 goal was to fashion objec-tive, or “uninformative,” prior distributions 
 that in some sense were unbi-ased in their effects upon the data 
 analysis.
  
 1
  The exponential family material in this chapter provides technical support, but 
 is not required in detail for a general understanding of the main ideas.
  
 233",NA
14,NA,NA
Statistical Inference and Methodology in ,NA,NA
the Postwar Era,"The fundamentals of statistical inference—frequentist, Bayesian, 
 Fisherian—were set in place by the end of the first half of the 
 twentieth century, as discussed in Part I of this book. The postwar 
 era witnessed a massive ex-pansion of statistical methodology, 
 responding to the data-driven demands of modern scientific 
 technology. We are now at the end of Part II, “Early Computer-Age 
 Methods,” having surveyed the march of new statistical algorithms 
 and their inferential justification from the 1950s through the 1990s.
  
 This was a time of opportunity for the discipline of statistics, when 
 the speed of computation increased by a factor of a thousand, and 
 then another thousand. As we said before, a land bridge had opened 
 to a new continent, but not everyone was eager to cross. We saw a 
 mixed picture: the computer played a minor or negligible role in the 
 development of some influential topics such as empirical Bayes, but 
 was fundamental to others such as the bootstrap.
  
 Fifteen major topics were examined in Chapters 6 through 13. 
 What fol-lows is a short scorecard of their inferential affinities, 
 Bayesian, frequentist, or Fisherian, as well as an assessment of the 
 computer’s role in their devel-opment. None of this is very precise, 
 but the overall picture, illustrated in Figure 14.1, is evocative.
  
 Empirical Bayes
  
 Robbins’ original development of formula (6.5) was frequentistic, but 
 most statistical researchers were frequentists in the postwar era so 
 that could be expected. The obvious Bayesian component of 
 empirical Bayes arguments is balanced by their frequentist emphasis 
 on (nearly) unbiased estimation of Bayesian estimators, as well as 
 the restriction to using only current data for inference. Electronic 
 computation played hardly any role in the theory’s development (as 
 indicated by blue coloring in the figure). Of course mod-
  
 264",NA
Part III ,NA,NA
Twenty-First-Century ,NA,NA
Topics,NA,NA
15,NA,NA
Large-Scale Hypothesis Testing ,NA,NA
and False-Discovery Rates,"By the final decade of the twentieth century, electronic computation 
 fully dominated statistical practice. Almost all applications, classical 
 or other-wise, were now performed on a suite of computer 
 platforms: SAS, SPSS, Minitab, Matlab, S (later R), and others.
  
 The trend accelerates when we enter the twenty-first century, as 
 statis-tical methodology struggles, most often successfully, to keep 
 up with the vastly expanding pace of scientific data production. This 
 has been a two-way game of pursuit, with statistical algorithms 
 chasing ever larger data sets, while inferential analysis labors to 
 rationalize the algorithms.
  
 Part 
 III 
 of 
 our 
 book 
 concerns 
 topics 
 in 
 twenty-first-
 century
 1
 statistics. The word “topics” is intended to signal selections 
 made from a wide cat-alog of possibilities. Part II was able to review 
 a large portion (though certainly not all) of the important 
 developments during the postwar period. Now, deprived of the 
 advantage of hindsight, our survey will be more illus-trative than 
 definitive.
  
 For many statisticians,
  microarrays
  provided an introduction to 
 large-scale data analysis. These were revolutionary biomedical 
 devices that en-abled the assessment of individual activity for 
 thousands of genes at once—and, in doing so, raised the need to 
 carry out thousands of simultaneous hypothesis tests, done with the 
 prospect of finding only a few interesting genes among a haystack of 
 null cases. This chapter concerns large-scale hypothesis testing and 
 the
  false-discovery rate
 , the breakthrough in statis-tical inference it 
 elicited.
  
 1
  Actually what historians might call “the long twenty-first century” since we will 
 begin in 1995.
  
 271",NA
16,NA,NA
Sparse Modeling and the Lasso,"The amount of data we are faced with keeps growing. From around 
 the late 1990s we started to see
  wide
  data sets, where the number of 
 variables far exceeds the number of observations. This was largely 
 due to our in-creasing ability to measure a large amount of 
 information automatically. In genomics, for example, we can use a 
 high-throughput 
 experiment 
 to 
 auto-matically 
 measure 
 the 
 expression of tens of thousands of genes in a sam-ple in a short 
 amount of time. Similarly, sequencing equipment allows us to 
 genotype millions of SNPs (single-nucleotide polymorphisms) 
 cheaply and quickly. In document retrieval and modeling, we 
 represent a document by the presence or count of each word in the 
 dictionary. This easily leads to a feature vector with 20,000 
 components, one for each distinct vocabulary word, although most 
 would be zero for a small document. If we move to bi-grams or 
 higher, the feature space gets really large.
  
 In even more modest situations, we can be faced with hundreds of 
 vari-ables. If these variables are to be predictors in a regression or 
 logistic re-gression model, we probably do not want to use them all. 
 It is likely that a subset will do the job well, and including all the 
 redundant variables will degrade our fit. Hence we are often 
 interested in identifying a good subset of variables. Note also that in 
 these wide-data situations, even linear mod-els are over-
 parametrized, so some form of reduction or regularization is 
 essential.
  
 In this chapter we will discuss some of the popular methods for 
 model selection, starting with the time-tested and worthy forward-
 stepwise ap-proach. We then look at the lasso, a popular modern 
 method that does se-lection and shrinkage via convex optimization. 
 The LARs algorithm ties these two approaches together, and leads to 
 methods that can deliver paths of solutions.
  
 Finally, we discuss some connections with other modern big- and 
 wide-data approaches, and mention some extensions.
  
 298",NA
.,"
 
  
 16.2 The Lasso
  
 
 ^",NA
.,"
 
  
 305
  
 
  
  
 Figure 16.4
  An example with ˇ 2 R
 2
 to illustrate the difference 
 between ridge regression and the lasso. In both plots, the red 
 contours correspond to the squared-error loss function, with 
 the unrestricted least-squares estimateOˇ in the center. The 
 blue regions show the constraints, with the lasso on the left 
 and ridge on the right. The solution to the constrained 
 problem corresponds to the value of ˇ where the expanding 
 loss contours first touch the constraint region. Due to the 
 shape of the lasso constraint, this will often be at a corner (or 
 an edge more generally), as here, which means in this case 
 that the minimizing ˇ has ˇ
 1
  D 0. For the ridge constraint, this 
 is unlikely to happen.
  
 cept the constraint is kˇk
 2
  t; ridge regression bounds the quadratic `
 2 
 norm of the coefficient vector. It also has the effect of pulling the 
 coeffi-cients toward zero, in an apparently very similar way. Ridge 
 regression is discussed in Section 7.3.
 2
 Both the lasso and ridge 
 regression are shrinkage methods, in the spirit of the James–Stein 
 estimator of Chapter 7.
  
 A big difference, however, is that for the lasso, the solution 
 typically has many of the ˇ
 j
  equal to zero, while for ridge they are all 
 nonzero. Hence the lasso does variable selection and shrinkage, 
 while ridge only shrinks. Figure 16.4 illustrates this for ˇ 2 R
 2
 . In 
 higher dimensions, the `
 1
  norm has sharp edges and corners, which 
 correspond to coefficient estimates zero in ˇ.
  
 Since the constraint in the lasso treats all the coefficients equally, it 
 usu-ally makes sense for all the elements of x to be in the same units. 
 If not, we
  
 2
  Here we use the “bound” form of ridge regression, while in Section 7.3 we use 
 the“Lagrange” form. They are equivalent, in that for every “Lagrange” solution, 
 there is a corresponding bound solution.",NA
17,NA,NA
Random Forests and Boosting,"In the modern world we are often faced with enormous data sets, 
 both in terms of the number of observations n and in terms of the 
 number of variables p. This is of course good news—we have always 
 said the more data we have, the better predictive models we can 
 build. Well, we are there now—we have tons of data, and must figure 
 out how to use it.
  
 Although we can scale up our software to fit the collection of linear 
 and generalized linear models to these behemoths, they are often too 
 modest and can fall way short in terms of predictive power. A need 
 arose for some general purpose tools that could scale well to these 
 bigger problems, and exploit the large amount of data by fitting a 
 much richer class of functions, almost automatically. Random forests 
 and boosting are two relatively re-cent innovations that fit the bill, 
 and have become very popular as “out-the-box” learning algorithms 
 that enjoy good predictive performance. Random forests are 
 somewhat more automatic than boosting, but can also suffer a small 
 performance hit as a consequence.
  
 These two methods have something in common: they both 
 represent the fitted model by a sum of regression trees. We discuss 
 trees in some detail in Chapter 8. A single regression tree is typically 
 a rather
  weak
  prediction model; it is rather amazing that an 
 ensemble of trees leads to the state of the art in black-box predictors!
  
 We can broadly describe both these methods very simply.
  
 Random forest
  Grow many deep regression trees to randomized 
 versions of the training data, and average them. Here 
 “randomized” is a wide-ranging term, and includes bootstrap 
 sampling and/or subsampling of the observations, as well as 
 subsampling of the variables.
  
 Boosting
  Repeatedly grow shallow trees to the residuals, and hence 
 build up an additive model consisting of a sum of trees.
  
 The basic mechanism in random forests is variance reduction by 
 averag-ing. Each deep tree has a high variance, and the averaging 
 brings the vari-
  
 324",NA
18,NA,NA
Neural Networks and Deep Learning,"Something happened in the mid 1980s that shook up the applied 
 statistics community. Neural networks (NNs) were introduced, and 
 they marked a shift of predictive modeling towards computer science 
 and machine learn-ing. A neural network is a highly parametrized 
 model, inspired by the ar-chitecture of the human brain, that was 
 widely promoted as a
  universal approximator
 —a machine that with 
 enough data could learn any smooth predictive relationship.
  
 x
 1
  
 Input
  
 Hidden
  
 Output
  
 f
 (
 x
 )
  
 layer
  L
 1
  
 layer
  L
 2
  
 layer
  L
 3
  
  
 x
 2
  
 x
 3
  
 x
 4
  
 Figure 18.1
  Neural network diagram with a single hidden 
 layer. The hidden layer derives transformations of the 
 inputs—nonlinear transformations of linear combinations—
 which are then used to model the output.
  
 Figure 18.1 shows a simple example of a
  feed-forward
  neural network 
 diagram. There are four predictors or inputs x
 j
 , five hidden units a
 `
  D 
 g.w
 .1/ 
 The language associated with NNs is colorful: memory units or
  
 neurons 
 automatically learn new features from the data through a",NA
... ... ,NA,NA
...,NA,NA
+ +,"correspondingly sized subimage in each pane, and summed 
 across the p
 1
  panes. We used q D 2, and small values are 
 typical. This is repeated over all (overlapping) q q subimages 
 (with boundary padding), and hence produces an image of the 
 same dimension as one of the input panes. This is the 
 convolution operation. There are p
 2
  different versions of this 
 filter, and hence p
 2
  new panes are produced. Each of the p
 2
  
 filters has p
 1
 q
 2
 weights, which are learned via 
 backpropagation.
  
 is a q q matrix with q k, the convolved image is another k k matrix Qx 
 with elements Qx
 i; j
  DP
 q 
 achieve a full-sized k k output image). In our 
 application we used 2 2,
  
 but other sizes such as 3 3 are popular. It is most natural to represent 
 `D1 
 P
 q `
 0
 D1
 x
 iC`; jC`
 0
 f
 `; `
 0
  
 (with edge padding to
  
 the structure in terms of these images as in Figure 18.9, but they could all
  
 be vectorized into a massive network diagram as in Figures 18.1 and 18.3.
  
 However, the weights would have special sparse structure, with most being",NA
19,NA,NA
Support-Vector Machines and ,NA,NA
Kernel Methods,"While linear logistic regression has been the mainstay in biostatistics 
 and epidemiology, it has had a mixed reception in the machine-
 learning com-munity. There the goal is often classification accuracy, 
 rather than statistical inference. Logistic regression builds a classifier 
 in two steps: fit a condi-tional probability model for Pr.Y D 1jX D x/, 
 and then classify as a one ifb classifier directly.
  
 Another rather awkward issue with logistic regression is that it 
 fails if Pr.Y D 1jX D x/ 0:5. SVMs bypass the first step, and build 
 a
  
 the training data are linearly separable! What this means is that, in 
 the feature space, one can separate the two classes by a linear 
 boundary. In cases such as this, maximum likelihood fails and some 
 parameters march off to infinity. While this might have seemed an 
 unlikely scenario to the early users of logistic regression, it becomes 
 almost a certainty with mod-ern
  wide
  genomics data. When p n 
 (more features than observations), we can typically always find a 
 separating hyperplane. Finding an
  optimal separating hyperplane
  was 
 in fact the launching point for SVMs. As we will see, they have more 
 than this to offer, and in fact live comfortably alongside logistic 
 regression.
  
 SVMs pursued an age-old approach in statistics, of enriching the 
 feature space through nonlinear transformations and basis 
 expansions; a classical example being augmenting a linear regression 
 with interaction terms. A linear model in the enlarged space leads to 
 a nonlinear model in the ambient space. This is typically achieved via 
 the “kernel trick,” which allows the computations to be performed in 
 the n-dimensional space for an arbitrary number of predictors p. As 
 the field matured, it became clear that in fact this kernel trick 
 amounted to estimation in a reproducing-kernel Hilbert space.
  
 Finally, we contrast the kernel approach in SVMs with the 
 nonparame-teric regression techniques known as kernel smoothing.
  
 375",NA
20,NA,NA
Inference After Model Selection,"The classical theory of model selection focused on “F tests” 
 performed within Gaussian regression models. Inference after model 
 selection (for in-stance, assessing the accuracy of a fitted regression 
 curve) was typically done ignoring the model selection process. This 
 was a matter of neces-sity: the combination of discrete model 
 selection and continuous regression analysis was too awkward for 
 simple mathematical description. Electronic computation has opened 
 the door to a more honest analysis of estimation accuracy, one that 
 takes account of the variability induced by data-based model 
 selection.
  
 Figure 20.1 displays the
  cholesterol
  data, an example we will use 
 for illustration in what follows: cholestyramine, a proposed 
 cholesterol-lowering drug, was administered to n D 164 men for an 
 average of seven years each. The response variable d
 i
  was the ith 
 man’s decrease in choles-terol level over the course of the 
 experiment. Also measured was c
 i
 , his compliance or the proportion 
 of the intended dose actually taken, ranging from 1 for perfect 
 compliers to zero for the four men who took none at all. Here the 164 
 c
 i
  values have been transformed to approximately follow a standard 
 normal distribution,
  
 c
 i
  P
  N
  .0; 1/: 
  
 (20.1)
  
 We wish to predict cholesterol decrease from compliance. 
 Polynomial regression models, with d
 i
  a J th-order polynomial in c
 i
 , 
 were considered, for degrees J D 1; 2; 3; 4; 5, or 6. The C
 p
  criterion 
 (12.51) was applied and selected a cubic model, J D 3, as best. The 
 curve in Figure 20.1 is the OLS (ordinary least squares) cubic 
 regression curve fit to the cholesterol data set
  
 f.c
 i
 ; d
 i
 /; i D 1; 2; : : : ; 164g : (20.2)
  
 We are interested in answering the following question: how accurate is the
  
 394",NA
21,NA,NA
Empirical Bayes Estimation Strategies,"Classic statistical inference was focused on the analysis of individual 
 cases: a single estimate, a single hypothesis test. The interpretation 
 of direct evi-dence bearing on the case of interest—the number of 
 successes and failures of a new drug in a clinical trial as a familiar 
 example—dominated statistical practice.
  
 The story of modern statistics very much involves indirect 
 evidence,“learning from the experience of others” in the language of 
 Sections 7.4 and 15.3, carried out in both frequentist and Bayesian 
 settings. The computer-intensive prediction algorithms described in 
 Chapters 16–19 use regression theory, the frequentist’s favored 
 technique, to mine indirect evidence on a massive scale. False-discovery 
 rate theory, Chapter 15, collects indirect ev-idence for hypothesis testing 
 by means of Bayes’ theorem as implemented through empirical Bayes 
 estimation.
  
 Empirical Bayes methodology has been less studied than Bayesian 
 or frequentist theory. As with the James–Stein estimator (7.13), it can 
 seem to be little more than plugging obvious frequentist estimates 
 into Bayes esti-mation rules. This conceals a subtle and difficult task: 
 learning the equiva-lent of a Bayesian prior distribution from 
 ongoing statistical observations. Our final chapter concerns the 
 empirical Bayes learning process, both as an exercise in applied 
 deconvolution and as a relatively new form of statistical inference. 
 This puts us back where we began in Chapter 1, examining the two 
 faces of statistical analysis, the algorithmic and the inferential.
  
 21.1 Bayes Deconvolution
  
 A familiar formulation of empirical Bayes inference begins by 
 assuming that an unknown prior density g./, our object of interest, 
 has produced a random sample of real-valued variates ‚
 1
 ; ‚
 2
 ; : : : ; ‚
 N
 ,
  
 ‚
 i
  
 iid
  g./;
  
 i D 1; 2; : : : ; N:
  
 (21.1)
  
 421",NA
Epilogue,"Something important changed in the world of statistics in the new 
 millen-nium. Twentieth-century statistics, even after the heated 
 expansion of its late period, could still be contained within the classic 
 Bayesian–frequentist–Fisherian inferential triangle (Figure 14.1). This 
 is not so in the twenty-first century. Some of the topics discussed in 
 Part III—false-discovery rates, post-selection inference, empirical 
 Bayes modeling, the lasso—fit within the triangle but others seem to 
 have escaped, heading south from the fre-quentist corner, perhaps in 
 the direction of computer science.
  
 The escapees were the large-scale prediction algorithms of 
 Chapters 17–19: neural nets, deep learning, boosting, random 
 forests, and support-vector machines. Notably missing from their 
 development were parametric prob-ability models, the building 
 blocks of classical inference. Prediction algo-rithms are the media 
 stars of the big-data era. It is worth asking why they have taken 
 center stage and what it means for the future of the statistics 
 discipline.
  
 The
  why
  is easy enough: prediction is commercially valuable. 
 Modern equipment has enabled the collection of mountainous data 
 troves, which the “data miners” can then burrow into, extracting 
 valuable information. Moreover, prediction is the simplest use of 
 regression theory (Section 8.4). It can be carried out successfully 
 without probability models, perhaps with the assistance of 
 nonparametric analysis tools such as cross-validation, per-mutations, 
 and the bootstrap.
  
 A great amount of ingenuity and experimentation has gone into 
 the development of modern prediction algorithms, with statisticians 
 playing an important but not dominant role.
 1
 There is no shortage of 
 impressive success stories. In the absence of optimality criteria, 
 either frequentist or Bayesian, the prediction community grades 
 algorithmic excellence on per-
  
 1
  All papers mentioned in this section have their complete references in the 
 bibliography. Footnotes will identify papers not fully specified in the text.
  
 446",NA
References,"Abu-Mostafa, Y. 1995. Hints.
  Neural Computation
 ,
  7
 , 639–671.
  
 Achanta, R., and Hastie, T. 2015.
  Telugu OCR Framework using Deep Learning
 . 
 Tech. 
  
 rept. Statistics Department, Stanford University.
  
 Akaike, H. 1973. Information theory and an extension of the maximum likelihood 
 prin-ciple. Pages 267–281 of:
  Second International Symposium on Information 
 Theory (Tsahkadsor, 1971)
 . Akad´emiai Kiad´o, Budapest.
  
 Anderson, T. W. 2003.
  An Introduction to Multivariate Statistical Analysis
 . Third 
 edn. 
  
 Wiley Series in Probability and Statistics. Wiley-Interscience.
  
 Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., 
 Bouchard, N., and Bengio, Y. 2012.
  Theano: new features and speed 
 improvements
 . Deep Learning and Unsupervised Feature Learning NIPS 2012 
 Workshop.
  
 Becker, R., Chambers, J., and Wilks, A. 1988.
  The New S Language: A Programming 
 Environment for Data Analysis and Graphics
 . Pacific Grove, CA: Wadsworth and 
 Brooks/Cole.
  
 Bellhouse, D. R. 2004. The Reverend Thomas Bayes, FRS: A biography to 
 celebrate the tercentenary of his birth.
  Statist. Sci.
 ,
  19
 (1), 3–43. With 
 comments and a rejoinder by the author.
  
 Bengio, Y., Courville, A., and Vincent, P. 2013. Representation learning: a review 
 and new perspectives.
  IEEE Transactions on Pattern Analysis and Machine 
 Intelligence
 , 
 35
 (8), 1798–1828.
  
 Benjamini, Y., and Hochberg, Y. 1995. Controlling the false discovery rate: A 
 practical and powerful approach to multiple testing.
  J. Roy. Statist. Soc. Ser. B
 ,
  
 57
 (1), 289–300.
  
 Benjamini, Y., and Yekutieli, D. 2005. False discovery rate-adjusted multiple 
 confi-
  
 dence intervals for selected parameters.
  J. Amer. Statist. Assoc.
 ,
  
 100
 (469), 71–93. Berger, J. O. 2006. The case for objective Bayesian analysis.
  
 Bayesian Anal.
 ,
  1
 (3), 
  
 385–402 (electronic).
  
 Berger, J. O., and Pericchi, L. R. 1996. The intrinsic Bayes factor for model 
 selection 
  
 and prediction.
  J. Amer. Statist. Assoc.
 ,
  91
 (433), 109–122.
  
 Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., 
 Turian, J., Warde-Farley, D., and Bengio, Y. 2010 (June). Theano: a CPU and 
 GPU math expression compiler. In:
  Proceedings of the Python for Scientific 
 Computing Con-ference (SciPy)
 .
  
 Berk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. 2013. Valid post-selection 
  
 inference.
  Ann. Statist.
 ,
  41
 (2), 802–837.
  
 453",NA
Author Index,"Abu-Mostafa, Y. 372, 453 
  
 Achanta, R. 372, 453 
  
 Akaike, H. 231, 453 
  
 Anderson, T. W. 69, 453
  
 Bastien, F. 374, 453 
  
 Becker, R. 128, 453 
  
 Bellhouse, D. R. 36, 453 
  
 Bengio, Y. 372, 374, 453, 458 
 Benjamini, Y. 294, 418, 450, 
 453 Berger, J. O. 36, 261, 453 
  
 Bergeron, A. 374, 453 
  
 Bergstra, J. 374, 453 
  
 Berk, R. 323, 419, 453 
  
 Berkson, J. 128, 454 
  
 Bernardo, J. M. 261, 454 
  
 Bibby, J. M. 37, 69, 459 
  
 Bien, J. 322, 461 
  
 Birch, M. W. 128, 454 
  
 Bishop, C. 371, 454 
  
 Bloomston, M. 89, 457 
  
 Boos, D. D. 180, 454 
  
 Boser, B. 390, 454 
  
 Bouchard, N. 374, 453 
  
 Breiman, L. 129, 348, 451, 454 
 Breuleux, O. 374, 453 
  
 Brown, L. 323, 419, 453 
  
 B¨uhlmann, P. 323, 461 
  
 Buja, A. 323, 419, 453
  
 Carlin, B. P. 89, 261, 454 
  
 Carroll, R. J. 445, 460 
  
 Casella, G. 420, 459 
  
 Chambers, J. 128, 453 
  
 Chen, Z. 372, 459 
  
 Chia, D. 372, 459 
  
 Cho, C. S. 89, 457 
  
 Chopin, N. 261, 457 
  
 Cleveland, W. S. 11, 454 
  
 Cohen, A. 393, 458
  
 Corbet, A. 456 
  
 Cortes, C. 372, 458 
  
 Courville, A. 372, 453 
  
 Cover, T. M. 52, 454 
  
 Cox, D. R. 52, 128, 152, 153, 262, 454 
 Crowley, J. 153, 454 
  
 Cudkowicz, M. 349, 458
  
 de Finetti, B. 261, 454 
  
 Dembo, A. 52, 454 
  
 Dempster, A. P. 152, 
 454 
  
 Desjardins, G. 374, 453 
  
 Dezeure, R. 323, 461 
  
 Diaconis, P. 262, 454 
  
 DiCiccio, T. 204, 455 
  
 Donnelly, P. 261, 459 
  
 Donoho, D. L. 447, 455
  
 Edwards, A. W. F. 37, 455 
  
 Efron, B. 11, 20, 37, 51, 52, 69, 89, 
 90, 
  
 105, 106, 130, 152, 154, 
 177–179, 
  
 204, 206, 207, 231, 232, 
 262, 263, 
  
 267, 294–297, 322, 323, 
 348, 417, 
  
 419, 420, 444, 445, 455–
 457, 461 Ertaylan, G. 349, 458 
  
 Eskin, E. 393, 458
  
 Fang, L. 349, 458 
  
 Feldman, D. 417, 456 
  
 Fields, R. C. 89, 457 
  
 Finney, D. J. 262, 456 
  
 Fisher, R. A. 184, 204, 449, 456 
  
 Fithian, W. 323, 456, 458 
  
 Freund, Y. 348, 451, 456, 460 
  
 Friedman, J. 128, 129, 231, 321, 
 322, 
  
 348–350, 371, 454, 
 456, 457, 461
  
 Geisser, 
 S. 
 231, 
 457 
  
 Gerber, 
 M. 
 261, 
 457 
  
 Gholami, 
 S. 
 89, 
 457 
  
 Good, I. 88, 457
  
 463",NA
Subject Index,"abc method, 194, 204 
  
 Accelerated gradient descent, 359 
  
 Acceleration, 192, 206 
  
 Accuracy, 14 
  
  
 after model selection, 402–408 
  
 Accurate but not correct, 402 
  
 Activation function, 355, 361 
  
  
 leaky rectified linear, 362 
  
  
 rectified linear, 362 
  
  
 ReLU, 362 
  
  
 tanh, 362 
  
 Active set, 302, 308 
  
 adaboost
  algorithm, 341–345, 447 
 Adaboost.M1, 342 
  
 Adaptation, 404 
  
 Adaptive estimator, 404 
  
 Adaptive rate control, 359 
  
 Additive model, 324 
  
  
 adaptive, 346 
  
 Adjusted compliance, 404 
  
 Admixture modeling, 256–260 
  
 AIC,
  see
  Akaike information criterion 
 Akaike information criterion, 208, 
 218, 
  
 226, 231, 246, 267 
  
 Allele frequency, 257 
  
 American Statistical Association, 449 
 Ancillary, 44, 46, 139 
  
 Apparent error, 211, 213, 219 
  
 arcsin transformation, 95 
  
 Arthur Eddington, 447 
  
 Asymptotics, xvi, 119, 120 
  
 Autoencoder, 362–364
  
 Backfitting, 346 
  
 Backpropagation, 356–358 
  
 Bagged estimate, 404, 406 
  
 Bagging, 226, 327, 406, 408, 
 419 Balance equations, 256 
  
 Barycentric plot, 259
  
 Basis expansion, 375 
  
 Bayes 
  
  
 deconvolution, 421–424 
  
  
 factor, 244, 285 
  
  
 false-discovery rate, 279 
  
  
 posterior distribution, 254 
  
  
 posterior probability, 280 
  
  
 shrinkage, 212 
  
  
 t-statistic, 255 
  
  
 theorem, 22 
  
 Bayes–frequentist estimation, 412–
 417 Bayesian 
  
  
 inference, 22–37 
  
  
 information criterion, 246 
  
  
 lasso, 420 
  
  
 lasso prior, 415 
  
  
 model selection, 244 
  
  
 trees, 349 
  
 Bayesian information criterion, 267 
  
 Bayesianism, 3 
  
 BCa 
  
  
 accuracy and correctness, 205 
  
  
 confidence density, 202, 207, 237, 
 242, 
   
 243 
  
  
 interval, 202 
  
  
 method, 192 
  
 Benjamini and Hochberg, 276 
  
 Benjamini–Yekutieli, 400 
  
 Bernoulli, 338 
  
 Best-approximating linear subspace, 
 363 Best-subset selection, 299 
  
 Beta 
  
  
 distribution, 54, 239 
  
 BH
 q
 , 276 
  
 Bias, 14, 352 
  
 Bias-corrected, 330 
  
  
 and accelerated,
  see
  BCa method 
  
  
 confidence intervals, 190–191 
  
  
 percentile method, 190
  
 467",NA
