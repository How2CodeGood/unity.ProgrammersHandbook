Larger Text,Smaller Text,Symbol
Monographs in Computer Science,"Editors
  
 David Gries
  
 Fred B. Schneider",NA
Monographs in Computer Science,"Abadi and Cardelli, 
 A Theory of Objects 
  
 Benosman and Kang [editors], 
 Panoramic Vision: Sensors, Theory, and Applications 
 Bhanu, Lin, Krawiec, 
 Evolutionary Synthesis of Pattern Recognition Systems 
  
 Broy and Stølen, 
 Specification and Development of Interactive Systems: FOCUS on 
 Streams, Interfaces, and Refinement 
  
 Brzozowski and Seger, 
 Asynchronous Circuits 
  
 Burgin,
  Super-Recursive Algorithms 
  
 Cantone, Omodeo, and Policriti, 
 Set Theory for Computing: From Decision Procedures 
 to Declarative Programming with Sets 
  
 Castillo, Gutiérrez, and Hadi, 
 Expert Systems and Probabilistic Network Models 
 Downey and Fellows, 
 Parameterized Complexity 
  
 Feijen and van Gasteren, 
 On a Method of Multiprogramming 
  
 Grune and Jacobs, 
 Parsing Techniques: A Practical Guide, Second Edition 
  
 Herbert and Spärck Jones [editors], 
 Computer Systems: Theory, Technology, and 
 Applications 
  
 Leiss, 
 Language Equations 
  
 Levin, Heydon, Mann, and Yu, 
 Software Configuration Management Using VESTA 
 Mclver and Morgan [editors], 
 Programming Methodology 
  
 Mclver and Morgan [editors], 
 Abstraction, Refinement and Proof for Probabilistic 
 Systems 
  
 Misra, 
 A Discipline of Multiprogramming: Programming Theory for Distributed 
 Applications 
  
 Nielson [editor], 
 ML with Concurrency 
  
 Paton [editor], 
 Active Rules in Database Systems 
  
 Poernomo, Crossley, and Wirsing, 
 Adapting Proof-as-Programs: The Curry-Howard 
 Protocol 
  
 Selig,
  Geometrical Methods in Robotics 
  
 Selig,
  Geometric Fundamentals of Robotics, Second Edition 
  
 Shasha and Zhu, 
 High Performance Discovery in Time Series: Techniques and Case 
 Studies 
  
 Tonella and Potrich, 
 Reverse Engineering of Object Oriented Code",NA
Dick Grune ,NA,NA
Ceriel J.H. Jacobs ,NA,NA
Parsing Techniques ,NA,NA
A Practical Guide,NA,NA
Second Edition,NA,NA
Preface to the Second Edition,"As is fit, this second edition arose out of our readers’ demands to read about new 
 developments and our desire to write about them. Although parsing techniques is 
 not a fast moving field, it does move. When the first edition went to press in 1990, 
 there was only one tentative and fairly restrictive algorithm for linear-time 
 substring parsing. Now there are several powerful ones, covering all deterministic 
 languages; we describe them in Chapter 12. In 1990 Theorem 8.1 from a 1961 
 paper by Bar-Hillel, Perles, and Shamir lay gathering dust; in the last decade it has 
 been used to create new algorithms, and to obtain insight into existing ones. We 
 report on this in Chapter 13.
  
 More and more non-Chomsky systems are used, especially in linguistics. None 
 except two-level grammars had any prominence 20 years ago; we now describe six 
 of them in Chapter 15. Non-canonical parsers were considered oddities for a very 
 long time; now they are among the most powerful linear-time parsers we have; see 
 Chapter 10.
  
 Although still not very practical, marvelous algorithms for parallel parsing have 
 been designed that shed new light on the principles; see Chapter 14. In 1990 a gen-
 eralized LL parser was deemed impossible; now we describe two in Chapter 11.
  
 Traditionally, and unsurprisingly, parsers have been used for parsing; more re-
 cently they are also being used for code generation, data compression and logic 
 language implementation, as shown in Section 17.5. Enough. The reader can find 
 more developments in many places in the book and in the Annotated Bibliography 
 in Chapter 18.
  
 Kees van Reeuwijk has — only half in jest — called our book “a reservation 
 for endangered parsers”. We agree — partly; it is more than that — and we make 
 no apologies. Several algorithms in this book have very limited or just no practical 
 value. We have included them because we feel they embody interesting ideas and 
 offer food for thought; they might also grow and acquire practical value. But we 
 also include many algorithms that do have practical value but are sorely underused; 
 describing them here might raise their status in the world.",NA
Acknowledgments,"We thank Manuel E. Bermudez, Stuart Broad, Peter Bumbulis, Salvador Cavadini, 
 Carl Cerecke, Julia Dain, Akim Demaille, Matthew Estes, Wan Fokkink, Brian 
 Ford, Richard Frost, Clemens Grabmayer, Robert Grimm, Karin Harbusch, 
 Stephen Horne, Jaco Imthorn, Quinn Tyler Jackson, Adrian Johnstone, Michiel 
 Koens, Jaroslav Král, Olivier Lecarme, Lillian Lee, Olivier Lefevre, Joop Leo, 
 JianHua Li, Neil Mitchell, Peter Pepper, Wim Pijls, José F. Quesada, Kees van 
 Reeuwijk, Walter L. Ruzzo, Lothar Schmitz, Sylvain Schmitz, Thomas Schoebel-
 Theuer, Klaas Sikkel, Michael Sperberg-McQueen, Michal Žemliˇcka, Hans 
 Åberg, and many others, for helpful cor-respondence, comments on and errata to 
 the First Edition, and support for the Second Edition. In particular we want to 
 thank Kees van Reeuwijk and Sylvain Schmitz for their extensive “beta reading”, 
 which greatly helped the book — and us.
  
 We thank the Faculteit Exacte Wetenschappen of the Vrije Universiteit for the use 
 of their equipment.
  
 In a wider sense, we extend our thanks to the close to 1500 authors listed in the 
 (Web)Authors Index, who have been so kind as to invent scores of clever and 
 elegant algorithms and techniques for us to exhibit. Every page of this book leans 
 on them.",NA
Preface to the First Edition,"Parsing (syntactic analysis) is one of the best understood branches of computer sci-
 ence. Parsers are already being used extensively in a number of disciplines: in 
 com-puter science (for compiler construction, database interfaces, self-describing 
 data-bases, artificial intelligence), in linguistics (for text analysis, corpora analysis, 
 ma-chine translation, textual analysis of biblical texts), in document preparation 
 and con-version, in typesetting chemical formulae and in chromosome recognition, 
 to name a few; they can be used (and perhaps are) in a far larger number of 
 disciplines. It is therefore surprising that there is no book which collects the 
 knowledge about pars-ing and explains it to the non-specialist. Part of the reason 
 may be that parsing has a name for being “difficult”. In discussing the Amsterdam 
 Compiler Kit and in teach-ing compiler construction, it has, however, been our 
 experience that seemingly diffi-cult parsing techniques can be explained in simple 
 terms, given the right approach. The present book is the result of these 
 considerations.
  
 This book does not address a strictly uniform audience. On the contrary, while 
 writing this book, we have consistently tried to imagine giving a course on the 
 subject to a diffuse mixture of students and faculty members of assorted faculties, 
 sophis-ticated laymen, the avid readers of the science supplement of the large 
 newspapers, etc. Such a course was never given; a diverse audience like that would 
 be too uncoor-dinated to convene at regular intervals, which is why we wrote this 
 book, to be read, studied, perused or consulted wherever or whenever desired.
  
 Addressing such a varied audience has its own difficulties (and rewards). Al-
 though no explicit math was used, it could not be avoided that an amount of math-
 ematical thinking should pervade this book. Technical terms pertaining to parsing 
 have of course been explained in the book, but sometimes a term on the fringe of 
 the subject has been used without definition. Any reader who has ever attended a 
 lec-ture on a non-familiar subject knows the phenomenon. He skips the term, 
 assumes it refers to something reasonable and hopes it will not recur too often. And 
 then there will be passages where the reader will think we are elaborating the 
 obvious (this paragraph may be one such place). The reader may find solace in the 
 fact that he does not have to doodle his time away or stare out of the window until 
 the lecturer progresses.",NA
Contents,"Preface to the Second Edition
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 v
  
 Preface to the First Edition
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 xi
  
 1 Introduction
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 1
  
 1.1 
  
 Parsing as a Craft. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 2
  
 1.2 
  
 The Approach Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
  
 1.3 
  
 Outline of the Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
  
 1.4 
  
 The Annotated Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
  
 2 Grammars as a Generating Device
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
 5
  
 2.1 
  
 Languages as Infinite Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
  
 2.1.1 
  
 Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 5
  
 2.1.2 
  
 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 7
  
 2.1.3 
  
 Problems with Infinite Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 8
  
 2.1.4 
  
 Describing a Language through a Finite Recipe . . . . . . . . . . . 
  
 12
  
 2.2 
  
 Formal Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 14
  
 2.2.1 
  
 The Formalism of Formal Grammars . . . . . . . . . . . . . . . . . . . . 
  
 14
  
 2.2.2 
  
 Generating Sentences from a Formal Grammar . . . . . . . . . . . 
  
 15
  
 2.2.3 
  
 The Expressive Power of Formal Grammars . . . . . . . . . . . . . . 
  
 17
  
 2.3 
  
 The Chomsky Hierarchy of Grammars and Languages. . . . . . . . . . . . 19
  
 2.3.1 
  
 Type 1 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 19
  
 2.3.2 
  
 Type 2 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 23
  
 2.3.3 
  
 Type 3 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 30
  
 2.3.4 
  
 Type 4 Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 33
  
 2.3.5 
  
 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 34
  
 2.4 
  
 Actually Generating Sentences from a Grammar. . . . . . . . . . . . . . . . . 34
  
 2.4.1 
  
 The Phrase-Structure Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 34",NA
1,NA,NA
Introduction,"Parsing is the process of structuring a linear representation in accordance with a 
 given grammar. This definition has been kept abstract on purpose to allow as wide 
 an interpretation as possible. The “linear representation” may be a sentence, a 
 computer program, a knitting pattern, a sequence of geological strata, a piece of 
 music, actions in ritual behavior, in short any linear sequence in which the 
 preceding elements in some way restrict
 1
 the next element. For some of the 
 examples the grammar is well known, for some it is an object of research, and for 
 some our notion of a grammar is only just beginning to take shape.
  
 For each grammar, there are generally an infinite number of linear representa-
 tions (“sentences”) that can be structured with it. That is, a finite-size grammar can 
 supply structure to an infinite number of sentences. This is the main strength of the 
 grammar paradigm and indeed the main source of the importance of grammars: 
 they summarize succinctly the structure of an infinite number of objects of a 
 certain class.
  
 There are several reasons to perform this structuring process called parsing. 
 One reason derives from the fact that the obtained structure helps us to process the 
 object further. When we know that a certain segment of a sentence is the subject, 
 that in-formation helps in understanding or translating the sentence. Once the 
 structure of a document has been brought to the surface, it can be converted more 
 easily.
  
 A second reason is related to the fact that the grammar in a sense represents our 
 understanding of the observed sentences: the better a grammar we can give for the 
 movements of bees, the deeper our understanding is of them.
  
 A third lies in the completion of missing information that parsers, and 
 especially error-repairing parsers, can provide. Given a reasonable grammar of the 
 language, an error-repairing parser can suggest possible word classes for missing 
 or unknown words on clay tablets.
  
 The reverse problem — given a (large) set of sentences, find the/a grammar 
 which produces them — is called
  grammatical inference
 . Much less is known 
 about it than about parsing, but progress is being made. The subject would require 
 a complete",NA
2,NA,NA
Grammars as a Generating Device,"2.1 Languages as Infinite Sets
  
 In computer science as in everyday parlance, a “grammar” serves to “describe” 
 a“language”. If taken at face value, this correspondence, however, is misleading, 
 since the computer scientist and the naive speaker mean slightly different things by 
 the three terms. To establish our terminology and to demarcate the universe of 
 discourse, we shall examine the above terms, starting with the last one.
  
 2.1.1 Language
  
 To the larger part of mankind, language is first and foremost a means of communi-
 cation, to be used almost unconsciously, certainly so in the heat of a debate. Com-
 munication is brought about by sending messages, through air vibrations or 
 through written symbols. Upon a closer look the language messages (“utterances”) 
 fall apart into sentences, which are composed of words, which in turn consist of 
 symbol se-quences when written. Languages can differ on all three levels of 
 composition. The script can be slightly different, as between English and Irish, or 
 very different, as between English and Chinese. Words tend to differ greatly, and 
 even in closely re-lated languages people call
  un cheval
  or
  ein Pferd
 , that which is 
 known to others as 
 a horse
 . Differences in sentence structure are often 
 underestimated; even the closely related Dutch often has an almost Shakespearean 
 word order: “
 Ik geloof je niet
 ”, “
 I believe you not
 ”, and more distantly related 
 languages readily come up with con-structions like the Hungarian “
 Pénzem van
 ”, 
 “
 Money-my is
 ”, where the English say“
 I have money
 ”.
  
 The computer scientist takes a very abstracted view of all this. Yes, a language 
 has sentences, and these sentences possess structure; whether they communicate 
 something or not is not his concern, but information may possibly be derived from 
 their structure and then it is quite all right to call that information the “meaning”of 
 the sentence. And yes, sentences consist of words, which he calls “tokens”, each 
 possibly carrying a piece of information, which is its contribution to the meaning 
 of",NA
{,"descriptions of size 31 . . 
 . . . . . . . . . . . . . . . . . . . . .
  
 P",NA
{,"descriptions of size 32
  
 “
 the set of all positive integers
 ”
  
 Fig. 2.1.
  List of all descriptions of length 32 or less
  
 Two things should be pointed out here. The first is that just listing all 
 descriptions alphabetically, without reference to their lengths, would not do: there 
 are already infinitely many descriptions starting with an “a” and no description 
 starting with a higher letter could get a number on the list. The second is that there 
 is no need to actually do all this. It is just a thought experiment that allows us to 
 examine and draw conclusions about the behavior of a system in a situation which 
 we cannot possibly examine physically.
  
 Also, there will be many nonsensical descriptions on the list; it will turn out 
 that this is immaterial to the argument. The important thing is that all meaningful 
 descriptions are on the list, and the above argument ensures that.
  
 2.1.3.3 Languages are Infinite Bit-Strings
  
 We know that words (sentences) in a language are composed of a finite set of sym-
 bols; this set is called quite reasonably the “alphabet”. We will assume that the 
 sym-bols in the alphabet are ordered. Then the words in the language can be 
 ordered too. We shall indicate the alphabet by
  ©
 .
  
 Now the simplest language that uses alphabet
  ©
  is that which consists of all 
 words that can be made by combining letters from the alphabet. For the alphabet
  ©
  
 =
 {
 a, b
 } we get the language {
  , a, b, aa, ab, ba, bb, aaa,
  . . . }. We shall call this 
 language
  ©
 ∗
 , for reasons to be explained later; for the moment it is just a name.
  
 The set notation
  ©
 ∗
 above started with “ {
  , a,
 ”, a remarkable construction; the 
 first word in the language is the
  empty word
 , the word consisting of zero
  a
 s and 
 zero 
 b
 s. There is no reason to exclude it, but, if written down, it may easily be 
 overlooked, so we shall write it as
  
  (epsilon), regardless of the alphabet. So,
  ©
 ∗
 =
  
 {
  
 ,
  a, b, aa, ab, ba, bb, aaa,
  ... }. In some natural languages, forms of the present 
 tense of the verb“to be” are empty words, giving rise to sentences of the form “I 
 student”, meaning“I am a student.” Russian and Hebrew are examples of this.
  
 Since the symbols in the alphabet
  ©
  are ordered, we can list the words in the 
 language
  ©
 ∗
 , using the same technique as in the previous section: First, all words 
 of size zero, sorted; then all words of size one, sorted; and so on. This is actually 
 the order already used in our set notation for
  ©
 ∗
 .",NA
3,NA,NA
Introduction to Parsing,"To parse a string according to a grammar means to reconstruct the production tree 
 (or trees) that indicate how the given string can be produced from the given 
 grammar. It is significant in this respect that one of the first publications on parsing 
 (Greibach’s 1963 doctoral thesis [6]), was titled “Inverses of Phrase Structure 
 Generators”, where a phrase structure generator is to be understood as a system for 
 producing phrases from a phrase structure (actually context-free) grammar.
  
 Although production of a sentence based on a Type 0 or Type 1 grammar gives 
 rise to a production graph rather than a production tree, and consequently parsing 
 yields a parse graph, we shall concentrate on parsing using a Type 2, context-free 
 grammar, and the resulting parse trees. Occasionally we will touch upon parsing 
 with Type 0 or Type 1 grammars, as for example in Section 3.2, just to show that it 
 is
  a meaningful concept.
  
 3.1 The Parse Tree
  
 There are two important questions on reconstructing the production tree: why do 
 we need it; and how do we do it.
  
 The requirement to recover the production tree is not natural. After all, a 
 grammar is a condensed description of a set of strings, i.e., a language, and our 
 input string either belongs or does not belong to that language; no internal structure 
 or production path is involved. If we adhere to this formal view, the only 
 meaningful question we can ask is if a given string can be recognized according to 
 a grammar; any question as to how would be a sign of senseless, even morbid 
 curiosity. In practice, however, grammars have semantics attached to them: 
 specific semantics is attached to specific rules, and in order to determine the 
 semantics of a string we need to find out which rules were involved in its 
 production and how. In short, recognition is not enough, and we need to recover 
 the production tree to get the full benefit of the syntactic approach.",NA
4,NA,NA
General Non-Directional Parsing,"In this chapter we will present two general parsing methods, both non-directional: 
 Unger’s method and the CYK method. These methods are called non-directional 
 because they access the input in a seemingly arbitrary order. They require the 
 entire input to be in memory before parsing can start.
  
 Unger’s method is top-down; if the input belongs to the language at hand, it 
 must be derivable from the start symbol of the grammar, say
  S
 . Therefore, it must 
 be derivable from a right-hand side of the start symbol, say
  A
 1
 A
 2
  ···
 A
 m
 . This, in 
 turn, means that
  A
 1
  must derive a first part of the input,
  A
 2
  a second part, etc. If the 
 input sentence is
  t
 1
 t
 2
  ···
 t
 n
 , this demand can be depicted as follows:
  
 S
  
  
  
 A
 1
  
 ···
  
 ···
  
  
 A
 i
  
 ···
  
 ···
  
 A
 m
  
  
 t
 1
  
 t
 k
  
 t
 n
  
 Unger’s method tries to find a partition of the input that fits this demand. This is a 
 recursive problem: if a non-terminal
  A
 i
  is to derive a certain part of the input, there 
 must be a partition of this part that fits a right-hand side of
  A
 i
 . Ultimately, such a 
 right-hand side must consist of terminal symbols only, and these can easily be 
 matched with the current part of the input.
  
 The CYK method approaches the problem the other way around: it tries to find 
 occurrences of right-hand sides in the input; whenever it finds one, it makes a note 
 that the corresponding left-hand side derives this part of the input. Replacing the 
 occurrence of the right-hand side with the corresponding left-hand side results in 
 some sentential forms that derive the input. These sentential forms are again the",NA
5,NA,NA
Regular Grammars and Finite-State Automata,"Regular grammars, Type 3 grammars, are the simplest form of grammars that still 
 have generative power. They can describe concatenation (joining two strings to-
 gether) and repetition, and can specify alternatives, but they cannot express 
 nesting. Regular grammars are probably the best-understood part of formal 
 linguistics and almost all questions about them can be answered.
  
 5.1 Applications of Regular Grammars
  
 In spite of their simplicity there are many applications of regular grammars, of 
 which we will briefly mention the most important ones.
  
 5.1.1 Regular Languages in CF Parsing
  
 In some parsers for CF grammars, a subparser can be discerned which handles a 
 regular grammar. Such a subparser is based implicitly or explicitly on the follow-
 ing surprising phenomenon. Consider the sentential forms in leftmost or rightmost 
 derivations. Such sentential forms consist of a closed (finished) part, which 
 contains terminal symbols only and an open (unfinished) part which contains non-
 terminals as well. In leftmost derivations the open part starts at the leftmost non-
 terminal and extends to the right; in rightmost derivations the open part starts at the 
 rightmost non-terminal and extends to the left. See Figure 5.1 which uses sample 
 sentential forms from Section 2.4.3.
  
 d , N & N
  
  
 N , N & h
  
  
 Fig. 5.1.
  Open parts in leftmost and rightmost productions
  
 It can easily be shown that these open parts of the sentential form, which play 
 an important role in some CF parsing methods, can be described by a regular 
 grammar, and that that grammar follows from the CF grammar.",NA
6,NA,NA
General Directional Top-Down Parsing,"In this chapter, we will discuss top-down parsing methods that try to rederive the 
 input sentence by prediction. As explained in Section 3.2.1, we start with the start 
 symbol and try to produce the input from it; at any point in time, we have a 
 sentential form that represents our prediction of the rest of the input sentence. It is 
 convenient to draw the prediction right under the part of the input that it predicts, 
 with their left ends flush, as we did in Figure 3.5:
  
 rest of input
  
 prediction
  
 This sentential form consists of both terminals and non-terminals. If a terminal 
 sym-bol is in front, we match it with the current input symbol. If a non-terminal is 
 in front, we pick one of its right-hand sides and replace the non-terminal with this 
 right-hand side. This way, we all the time replace the leftmost non-terminal, and in 
 the end, if we succeed, we have imitated a leftmost derivation. Note that the 
 prediction part corresponds to the open part of the sentential form when doing 
 leftmost derivation, as discussed in Section 5.1.1.
  
 6.1 Imitating Leftmost Derivations
  
 Let us now illustrate such a rederiving process with an example. Consider the 
 gram-mar of Figure 6.1. This grammar produces all sentences with equal numbers 
 of
  a
 s
  
 S
 s 
  
 A
  
  
 B
  
 -
 -
 -
 >
 -
 -
 -
 >
 -
 -
 aB | bA 
  
 a | aS | bAA 
  
 b | bS | aBB
  
 Fig. 6.1.
  A grammar producing all sentences with equal numbers of
  a
 s and
  b
 s",NA
7,NA,NA
General Directional Bottom-Up Parsing,"As explained in Section 3.2.2, directional bottom-up parsing is conceptually very 
 simple. At all times we are in the possession of a sentential form that derives from 
 the input text through a series of leftmost reductions. These leftmost reductions 
 dur-ing parsing correspond to rightmost productions that produced the input text: 
 the first leftmost
  re
 duction corresponds to the last rightmost
  pro
 duction, the second 
 cor-responds to the one but last, etc.
  
 There is a cut somewhere in this sentential form which separates the already 
 reduced part (on the left) from the yet unexamined part (on the right). See Figure 
 7.1. The part on the left is called the “stack” and the part on the right “rest of 
 input”. The
  
 Stack Rest of input
  
  
 terminals 
  
    
 Cut 
  
 terminals 
  
  
 and 
  
    
  
  
 only 
  
 non-terminals
  
 t
 g
  N
 f
  t
 e
  t
 d
  N
 c
  N
 b
  t
 a
  t
 1
  t
 2
  t
 3
 ·
  
 ·
  
 partial parse 
  
 trees
  
 Fig. 7.1.
  The structure of a bottom-up parse
  
 latter contains terminal symbols only, since it is an unprocessed part of the original 
 sentence, while the stack contains a mixture of terminals and non-terminals, 
 resulting from recognized right-hand sides. We can complete the picture by 
 keeping the partial parse trees created by the reductions attached to their non-
 terminals. Now all the terminal symbols of the original input are still there; the 
 terminals in the stack are one part of them, another part is semi-hidden in the 
 partial parse trees and the rest is untouched in the rest of the input. No information 
 is lost, but structure has been added. When the bottom-up parser has reached the 
 situation where the rest of the",NA
8,NA,NA
Deterministic Top-Down Parsing,"In Chapter 6 we discussed two general top-down methods: one using breadth-first 
 search and one using depth-first search. These methods have in common the need 
 to search to find derivations, and thus are not efficient. In this chapter and the next 
 we will concentrate on parsers that do not have to search: there will always be only 
 one possibility to choose from. Parsers with this property are called
  deterministic
 . 
 Deter-ministic parsers have several advantages over non-deterministic ones: they 
 are much faster; they produce only one parse tree, so ambiguity is no longer a 
 problem; and this parse tree can be constructed on the fly rather than having to be 
 retrieved afterwards. But there is a penalty: the class of grammars that the 
 deterministic parsing methods are suitable for, while depending on the method 
 chosen, is more restricted than that of the grammars suitable for non-deterministic 
 parsing methods. In particular, only non-ambiguous grammars can be used.
  
 In this chapter we will focus on deterministic top-down methods. As has been 
 explained in Section 3.5.5, there is only one such method, this in contrast with the 
 deterministic bottom-up methods, which will be discussed in the next chapter. 
 From Chapters 3 and 6 we know that in a top-down parser we have a prediction for 
 the rest of the input, and that this prediction has either a terminal symbol in front, 
 in which case we “match”, or a non-terminal, in which case we “predict”.
  
 It is the predict step that, until now, has caused us so much trouble. The predict 
 step consists of replacing a non-terminal by one of its right-hand sides, and if we 
 have no means to decide which right-hand side to select, we have to try them all. 
 One restriction we could impose on the grammar, one that immediately comes to 
 mind, is limiting the number of alternatives for each non-terminal to one. Then we 
 would need no search, because no selection would be needed. However, such a 
 restriction is far too severe, as it would leave us only with languages that consist of 
 one word. So, limiting the number of right-hand sides per non-terminal to one is 
 not a solution.
  
 There are two sources of information that could help us in selecting the right 
 right-hand side. First there is the partial derivation as it has been constructed so far. 
 However, apart from the prediction this does not give us any information about the 
 rest of the input. The other source of information is the rest of the input. We will",NA
9,NA,NA
Deterministic Bottom-Up Parsing,"There is a great variety of deterministic bottom-up parsing methods. The first de-
 terministic parsers (Wolpe [110], Adams and Schlesinger [109]) were bottom-up 
 parsers and interest has only increased since. The full bibliography of this book on 
 its web site contains about 280 entries on deterministic bottom-up parsing against 
 some 85 on deterministic top-down parsing. These figures may not directly reflect 
 the rel-ative importance of the methods, but they are certainly indicative of the 
 fascination and complexity of the subject of this chapter.
  
 There are two families of deterministic bottom-up parsers:
  
 •Pure bottom-up parsers. This family comprises the precedence and bounded-
 (right)-context techniques, and are treated in Sections 9.1 to 9.3.
  
 •Bottom-up parsers with an additional top-down component. This family, which is 
 both more powerful and more complicated than the pure bottom-up parsers, 
 consists of the LR techniques and is treated in Sections 9.4 to 9.10.
  
 There are two main ways in which deterministic bottom-up methods are extended 
 to allow more grammars to be handled:
  
 •Remaining non-determinism is resolved by breadth-first search. This leads to 
 Generalized LR parsing, which is covered in Section 11.1.
  
 •The requirement that the bottom-up parser does the reductions in reverse right-
 most production order (see below and Section 3.4.3.2) is dropped. This leads to 
 non-canonical parsing, which is covered in Chapter 10.
  
 The proper setting for the subject at hand can best be obtained by summarizing a 
 number of relevant facts from previous chapters.
  
 •A rightmost production expands the rightmost non-terminal in a sentential form, 
 by replacing it by one of its right-hand sides, as explained in Section 2.4.3. A 
 sentence is then produced by repeated rightmost production until no non-terminal 
 remains. See Figure 9.1(
 a
 ), where the sentential forms are right-aligned to show 
 how the production process creeps to the left, where it terminates. The grammar 
 used is that of Figure 7.8.",NA
10,NA,NA
Non-Canonical Parsers,"Top-down parsers make their predictions in pre-order, in which the parent nodes 
 are identified
  before
  any of their children, and which imitates leftmost derivations 
 (see Section 6.1); bottom-up parsers perform their reductions in post-order, in 
 which the parent nodes are identified
  after
  all of their children have been 
 identified, and which imitates rightmost derivation (see the introduction in Chapter 
 7). These two orders of producing and visiting trees are called “canonical”, and so 
 are the parsing techniques that follow them.
  
 Non-canonical parsing methods take liberties with these traditional orders, and 
 sometimes postpone the decisions that would be required to create parse trees in 
 pure pre- or post-order. This allows them to use a larger set of grammars, but on 
 the other hand these methods create fragments of parse trees, which have to be 
 combined at later moments.
  
 Like their canonical counterparts, non-canonical methods can be classified as 
 top-down (Section 10.1) and bottom-up (Section 10.2) methods, based on whether 
 they primarily use pre-order or post-order. There are deterministic and general non-
 canonical methods. The deterministic methods allow parsing in linear-time; as with 
 LL and LR methods, they can be generalized by applying a limited breadth-first 
 search. Altogether the non-canonical methods form a large and diverse field that 
 has by no means been completely explored yet.
  
 Figure 10.1 shows the relation between non-canonical parsing and the corre-
 sponding non-canonical production process, as Figure 3.9 did for canonical 
 parsing. In this case just the node for
  q
  has been identified. Again the dotted line 
 represents the sentential form.
  
 The most important property of deterministic non-canonical parsing is that it al-
 lows a larger class of grammars to be used without modification while retaining 
 linear time requirements. Since it has simultaneously aspects of top-down and 
 bottom-up parsing it can also provide further insight in parsing; see, for example, 
 Demers [103], who describes a parsing technique on a gliding scale between LL(1) 
 and SLR(1).
  
 On the down side there is an increased complexity and difficulty, both for the 
 implementer and the user. Postponing decisions does not come free, so non-
 canonical parsing algorithms are more complex and require more ingenuity than 
 their canonical",NA
11,NA,NA
Generalized Deterministic Parsers,"Generalized deterministic parsers are general breadth-first context-free parsers that 
 gain efficiency by exploiting the methods and tables used by deterministic parsers, 
 even if these tables have conflicts (inadequate states) in them. Viewed 
 alternatively, generalized deterministic parsers are deterministic parsers extended 
 with a breadth-first search mechanism so they will be able to operate with tables 
 with some multiple, conflicting entries. The latter view is usually more to the point.
  
 Before going into the algorithms, we have to spend a few words on the question 
 what exactly constitutes a “generalized deterministic parser”. Usually the term is 
 taken to mean “a parser obtained by strengthening an almost-deterministic parser 
 by doing limited breadth-first search”, and initially the technique was applied only 
 to parsers with LR tables with just a few inadequate states. Later research has 
 shown that “generalized parsing” can also be used profitably with tables with large 
 amounts of inadequate states, and even without tables (actually with trivial tables; 
 see next paragraph). We will therefore cover under this heading any breadth-first 
 CF parser with some, even the weakest, table support.
  
 Trivial parse tables are interesting in themselves, since they are a low extreme 
 all other tables can be measured against: they form the bottom element if one 
 wishes to order parse tables in a lattice. The
  trivial bottom-up table
  has one state, 
 which says: both shift the next input token onto the top of the stack and reduce 
 with all rules whose right-hand sides match the top of the stack. The
  trivial top-
 down table 
 has one state, which says: either match the next input token to the top 
 of the stack or predict with all rules whose left-hand sides match the top of the 
 stack. Since states are used to make a distinction and since just one state cannot 
 make a distinction, one can also leave it out.
  
 As said above, the first generalized parsing algorithms that were designed were 
 based on almost-LR parse tables, and much more is known about generalized LR 
 parsing than about the other variants. We shall therefore treat generalized LR 
 parsing first, and then those based on other tables.
  
 Merrill [171] has shown that it is also possible to do generalized LR parsing by 
 strengthening an almost-deterministic LR parser by doing
  depth-first
  search.",NA
12,NA,NA
Substring Parsing,"In Chapter 2 grammars were explained as finite devices for generating infinite sets 
 of strings, and parsing was explained as the mechanism for determining whether a 
 given string — a sentence — belongs to the set of strings — the language — 
 generated by the grammar. As a bonus parsing also reveals the syntactic structure 
 of that sentence.
  
 But sometimes the sentence is not completely known. Parts of it may be 
 missing due to transmission errors, lost in milliseconds in an electronic network or 
 in mil-lennia on clay tablets. It may be syntactically almost but not quite correct, in 
 which case it will consist of a sequence of syntactically correct fragments, 
 “substrings”. Or we may have missed its beginning and picked up only somewhere 
 in the middle, in which case we obtain a “suffix” of a correct sentence. In each of 
 these cases we want to do the best syntax analysis possible; so it is useful to be able 
 to do substring and suffix recognition and/or parsing.
  
 In principle we have to distinguish between a substring — a section from a sen-
 tence — and a suffix — the tail of a sentence —, but in practice the difference is 
 not that big. Most suffix parsing algorithms are directional, which means that if the 
 suf-fix happens to be a substring, the algorithm just stops prematurely on end-of-
 input. In fact, all “substring parsers” mentioned in (Web)Section 18.2.3 are actually 
 suffix parsers.
  
 We do, however, have to distinguish between recognition and parsing. As usual 
 recognition is easier than parsing, and all known algorithms start by recognizing; 
 additional work is then needed to obtain a — necessarily incomplete — parse tree. 
 Suppose we try to parse the string
  -n)
  as a suffix with the grammar for arithmetic 
 expressions from Figure 9.23, which we repeat here in Figure 12.1. Then a 
 recognizer
  
 S
  
  
 E
  
  
 E
  
  
 T
  
  
 T
  
 -
 -
 -
 >
 -
 -
 -
 >
 -
 -
 -
 E 
  
 E - T 
  
 T 
  
 n 
  
 ( E )
  
 Fig. 12.1.
  The grammar for differences of numbers, copied from Figure 9.23",NA
13,NA,NA
Parsing as Intersection,"In 1961 Bar-Hillel, Perles and Shamir [219] proved that “the intersection of a 
 context-free language with a regular language is again a context-free language”. 
 On the face of it, this means that when we take the set of strings that constitute a 
 given CF language and remove from it all strings that do not occur in a given FS 
 language, we get a set of strings for which a CF grammar exists. Actually, it means 
 quite a bit more.
  
 It would be quite conceivable that the intersection of CF and FS were stronger 
 than CF. Consider the two CF languages
  L
 1
  =
  a
 n
 b
 n
 c
 m
 and
  L
 2
  =
  a
 m
 b
 n
 c
 n
 , whose CF 
 grammars are in Figure 13.1. When we take a string that occurs in both languages, 
 it
  
 L
 1s 
  
 A
  
  
 P
  
 -
 -
 -
 >
 -
 -
 -
 >
 -
 -
 -
 >
  
 A P 
  
 a A b |
  
  
 c P |
  
  
 L
 2s 
  
 C
  
  
 Q
  
 -
 -
 -
 >
 -
 -
 -
 >
 -
 -
 -
 >
  
 Q C 
  
 b C c |
  
  
 a Q |
  
  
 Fig. 13.1.
  CF grammars for
  a
 n
 b
 n
 c
 m
 and
  a
 m
 b
 n
 c
 n
  
 will have the form
  a
 p
 a
 q
 a
 r
 , where
  p
  =
  q
  because of
  L
 1
  and
  q
  =
  r
  because of
  L
 2
 . So 
 the intersection language consists of strings of the form
  a
 n
 b
 n
 c
 n
 , and we know that 
 that language is not context-free (Section 2.7.1). But Bar-Hillel et al.’s proof shows 
 that that cannot happen with a CF language and a FS language. On the other hand 
 one could well imagine that intersecting with a regular language would kill all CF 
 power of description; again Bar-Hillel’s proof shows that that is not the case.
  
 Sometimes proofs of such theorems are non-constructive: one shows, for exam-
 ple, that if the theorem were not true one could solve a problem of which it has 
 been proven that it is unsolvable, like full Type 0 parsing. Bar-Hillel et al.’s proof 
 is much better than that: it is constructive, and demonstrates how to obtain the CF 
 grammar of the intersection language, starting from the original CF grammar and 
 the FS au-",NA
14,NA,NA
Parallel Parsing,"There are two main reasons for doing parallel programming: the problem has in-
 herent parallelism, in which case the parallel programming comes naturally; and 
 the problem is inherently sequential but we need the speed-up promised by parallel 
 pro-cessing, in which case the parallel programming is often a struggle.
  
 Parsing does not fall in either of these categories. It has no obvious or inherent 
 parallelism and on present-day machines and using state-of-the-art techniques it is 
 already very fast. A 250-line module in a computer program and a 25-word 
 sentence in a natural language can both be parsed in a fraction of a second. In 
 parallel parsing processing is performed on multiple processors, which either have 
 a shared memory or are interconnected by means of a (high speed) network. Given 
 the communication overhead in the most common parallel systems, little speed-up 
 can be expected for the average parsing task.
  
 14.1 The Reasons for Parallel Parsing
  
 From a practical point of view, parallel parsing is interesting only for problems big 
 enough to require considerably more time than a fraction of a second on a single 
 processor. There are three ways in which a parsing problem can be this big: the 
 input is very long (millions of tokens); the grammar is very large (millions of 
 rules); or there are millions of inputs to be parsed. The last problem can be solved 
 trivially by distributing the inputs over multiple processors, where each processor 
 processes a different input and runs an ordinary, sequential, parser.
  
 Examples of very long inputs requiring parsing are hard to find. All very long 
 parsable sequences occurring in practice are likely to be regular: generating very 
 long CF sequences would require a place to store the nesting information during 
 sentence generation. The boundaries are not very clear, though, since a novel can 
 be con-sidered a very long parsable sequence of words. Upon a closer look, this 
 sequence, however, is either regular — just a list of chapters which are lists of 
 paragraphs which are lists of sentences, which in themselves are CF — or it is 
 context-sensitive — a coherent sequence of words, punctuation, etc., with strong 
 context dependencies, for",NA
15,NA,NA
Non-Chomsky Grammars and Their Parsers,"Just as the existence of non-stick pans points to user dissatisfaction with 
 “sticky”pans, the existence of non-Chomsky grammars points to user 
 dissatisfaction with the traditional Chomsky hierarchy. In both cases ease of use is 
 the issue.
  
 As we have seen in Section 2.3, the Chomsky hierarchy consists of five levels:
  
 •phrase structure (PS),
  
 •context-sensitive (CS),
  
 •context-free (CF),
  
 •regular (finite-state, FS) and
  
 •finite-choice (FC).
  
 Although each of the boundaries between the types is clear-cut, some boundaries 
 are more important than others. Two boundaries specifically stand out: that 
 between context-sensitive and context-free and that between regular (finite-state) 
 and finite-choice. The significance of the latter is trivial, being the difference 
 between produc-tive and non-productive, but the former is profound.
  
 The border between CS and CF is that between global correlation and local in-
 dependence. Once a non-terminal has been produced in a sentential form in a CF 
 grammar, its further development is independent of the rest of the sentential form, 
 but a non-terminal in a sentential form of a CS grammar has to look at its 
 neighbors on the left and on the right, to see what production rules are allowed for 
 it. The local production independence in CF grammars means that certain long-
 range correlations cannot be expressed by them. Such correlations are, however, 
 often very interesting, since they embody fundamental properties of the input text, 
 like the consistent use of variables in a program or the recurrence of a theme in a 
 musical composition.
  
 15.1 The Unsuitability of Context-Sensitive Grammars
  
 The obvious approach would be using a CS grammar to express the correlations (= 
 the context-sensitivity) but here we find our way obstructed by three practical",NA
16,NA,NA
Error Handling,"Until now, we have discussed parsing techniques while largely ignoring what hap-
 pens when the input contains errors. In practice, however, the input often contains 
 errors, the most common being typing errors and misconceptions, but we could 
 also be dealing with a grammar that only roughly, not precisely, describes the 
 input, for example in pattern matching. So the question arises how to deal with 
 errors. A con-siderable amount of research has been done on this subject, far too 
 much to discuss in one chapter. We will therefore limit our discussion to some of 
 the more well-known error handling methods, and not pretend to cover the field; 
 see (Web)Section 18.2.7 for references to more in-depth information.
  
 16.1 Detection versus Recovery versus Correction
  
 Usually, the least that is required of a parser is that it detects the occurrence of one 
 or more errors in the input, that is, we require
  error detection
 . The least informa-
 tive version of this is that the parser announces: “input contains syntax error(s)”. 
 We say that the input contains a
  syntax error
  when the input is not a sentence of 
 the language described by the grammar. All parsers discussed in the previous 
 chapters (except operator-precedence) are capable of detecting this situation 
 without exten-sive modification. However, there are few circumstances in which 
 this behavior is acceptable: when we have just typed a long sentence, or a complete 
 computer pro-gram, and the parser only tells us that there is a syntax error 
 somewhere, we will not be pleased at all, not only about the syntax error, but also 
 about the quality of the parser or lack thereof.
  
 The question as to where the error occurs is much more difficult to answer; in 
 fact it is almost impossible. Although some parsers have the “correct-prefix 
 property”, which means that they detect an error at the first symbol in the input that 
 results in a prefix that cannot start a sentence of the language, we cannot be sure 
 that this indeed is the place in which the error occurs. It could very well be that 
 there is an error somewhere before this symbol but that this is not a syntax error at 
 that point. Thus there is a difference in the perception of an error between the 
 parser and the user. In",NA
17,NA,NA
Practical Parser Writing and Usage,"Practical parsing is concerned almost exclusively with context-free (Type 2) and 
 regular (Type 3) grammars. Unrestricted (Type 0) and context-sensitive (Type 1) 
 grammars are hardly used since, first, they are user-unfriendly in that it is next to 
 impossible to construct a clear and readable Type 0 or Type 1 grammar and, sec-
 ond, all known parsers for them have exponential time requirements. Chapter 15 
 de-scribes a number of polynomial-time and even linear-time parsers for non-
 Chomsky systems, but few have seen practical application. For more experimental 
 results see (Web)Section 18.2.6.
  
 Regular grammars are used mainly to describe patterns that have to be found in 
 surrounding text. For this application a recognizer suffices. There is only one such 
 recognizer: the finite-state automaton described in Section 5.3. Actual parsing with 
 a regular grammar, when required, is generally done using techniques for CF gram-
 mars.
  
 In view of the above we shall restrict ourselves to CF grammars in the rest of 
 this chapter. We start with a comparative survey of the available parsing techniques 
 (Section 17.1). Parsers can be interpretive, table-driven or compiled; the techniques 
 are covered in Section 17.2.1. Section 17.3 presents a simple general context-free 
 parser in Java, both for experimentation purposes and to show the nitty-gritty 
 details of a complete parser. Parsers, both interpretive and compiled, must be 
 written in a programming language; the influence of the programming language 
 paradigm is discussed in Section 17.4. Finally, Section 17.5 exhibits a few unusual 
 applications of the pattern recognition inherent in parsing.
  
 17.1 A Comparative Survey
  
 17.1.1 Considerations
  
 The initial demands on a CF parsing technique are obvious: it should be general 
 (i.e., able to handle all CF grammars), it should be fast (i.e., have linear time 
 requirements) and preferably it should be easy to program. Practically the only way 
 to obtain linear",NA
18,NA,NA
Annotated Bibliography,"The purpose of this annotated bibliography is to supply the reader with more 
 material and with more detail than was possible in the preceding chapters, rather 
 than to just list the works referenced in the text. The annotations cover a 
 considerable number of subjects that have not been treated in the rest of the book.
  
 The printed version of this book includes only those literature references and 
 their summaries that are actually referred to in it. The full literature list with 
 summaries as far as available can be found on the web site of this book; it includes 
 its own authors index and subject index.
  
 This annotated bibliography differs in several respects from the habitual literature 
 list.
  
 •The annotated bibliography consists of four sections:
  
 –
  
 Main parsing material — papers about the main parsing techniques.
  
 –
  
 Further parsing material — papers about extensions of and refinements to 
 the 
  
 main parsing techniques, non-Chomsky systems, error recovery, etc.
  
 Parser writing and application — both in computer science and in natural–
  
 languages.
  
 –
  
 Support material — books and papers useful to the study of parsers.
  
 •The entries in each section have been grouped into more detailed categories; for 
 example, the main section contains categories for general CF parsing, LR parsing, 
 precedence parsing, etc. For details see the Table of Contents at the beginning of 
 this book.
  
 Most publications in parsing can easily be assigned a single category. Some that 
 span two categories have been placed in one, with a reference in the other.
  
 •The majority of the entries are annotated. This annotation is not a copy of the ab-
 stract provided with the paper (which generally says something about the results 
 obtained) but is rather the result of an attempt to summarize the technical content in 
 terms of what has been explained elsewhere in this book.
  
 •The entries are ordered chronologically rather than alphabetically. This arrange-
 ment has the advantage that it is much more meaningful than a single alphabetic 
 list, ordered on author names. Each section can be read as the history of research",NA
A,NA,NA
Hints and Solutions to Selected Problems,"Answer to Problem 2.1
 :
  The description is not really finite: it depends on all descriptions
  
 in the list, of which there are infinitely many.
  
 Answer to Problem 2.2
 :
  Any function that maps the integers onto unique integers will do 
 (an
  injective function
 ), for example
  n
 n
 or the
  n
 -th prime.
  
 Answer to Problem 2.3
 :
  Hint: use a special (non-terminal) marker for each block the tur-
  
 tle is located to the east of its starting point.
  
 Answer to Problem 2.4
 :
  The terminal production cannot contain
  S
 , and the only way to 
 get rid of it is to apply
  S--->aSQ
  k
  ≥
  0 times followed by
  S--->abc
 . This yields a 
 sentential form 
 a
 k
 abcQ
 k
 . The part
  bcQ
 k
 cannot produce any
  a
 s, so all the
  a
 s in the result 
 come from
  a
 k
 a
 , so all strings
  a
 n
 ···
  can be produced, for
  n
  =
  k
 +
 1
  ≥
  1. We now need to 
 prove that
  bcQ
 k
 produces exactly
  b
 n
 c
 n
 . The only way to get rid of the
  Q
  is through
  bQc-
 -->bbcc
 , which requires the
  Q
  to be adjacent to the
  b
 . The only way to get it there is 
 through repeated application of
  cQ--->Qc
 . This does not affect the number of each token. 
 After
  j
  application of
  bQc--->bbcc
  we have 
 bb
 j
 c
 j
 cQ
 k
  −
  j
 , and after
  k
  applications we 
 have
  bb
 k
 c
 k
 c
 , which equals
  b
 n
 c
 n
 .
  
 Answer to Problem 2.5
 :
  Assume the alphabet is {
 (
 ,
  [
 }. First create a nesting string of
  
 (
 ,
  )
 ,
  [
 , and
  ]
 , with a marker in the center and another marker at the end: 
 ([[...[([X])]...]])Z
 . Give the center marker the ability to “ignite” the
  )
  or
  ]
  on 
 its right:
  ([[...[([X]
 *
 )]...]])Z
 . Make the ignited parenthesis move right until it 
 bumps into the end marker:
  ([[...[([X)]...]])]
 *
 Z
 . Move it over the end marker 
 and turn it into its (normal) partner:
  ([[...[([X)]...]])]Z[
 . This moves the 
 character in the first position after the center marker to the first position after the end 
 marker; repeat for the next
  
 character:
  ([[...[([X]...]])]Z([
 . This reverses the string between the markers 
 while at the same time translating it:
  ([[...[([XZ([[...[([
 . Now the markers can 
 annihilate each other.
  
 Answer to Problem 2.11
 :
  We have to carve up
  a
 i
 b
 i
 into
  u
 ,
  v
 , and
  w
 , such that
  uv
 n
 w
  is 
 again in
  a
 i
 b
 i
 , for all
  n
 . The problem is to find
  v
 : 1.
  v
  cannot lie entirely in
  a
 i
 , since
  uv
 n
 w 
 would produce
  a
 i
 +
 n
 b
 i
 . 2.
  v
  cannot straddle the
  a
 -
 b
  boundary in
  a
 i
 b
 i
 , since even
  vv
  would 
 contain a
  b
  followed by an
  a
 . 3.
  v
  cannot lie entirely in
  b
 i
 either, for the same reason as 
 under 1.",NA
Author Index,"Page numbers in roman indicate pages referring to the author; page numbers in
  
 italic 
 refer to annotations of works by the author. Names and initials are indexed as 
 shown in the publications in question.
  
 Aasa, A, 272,
  596 
  
 Bouckaert, M, 223,
  579
 , 601
  
 Abney, S. P, 352,
  638 
  
 Adams, E. S, 263,
  593 
  
 Aguiar, M. Á. P,
  604 
  
 Aho, A. V, 162, 303, 316, 529,
  598, 599
 ,
  623
 ,
  630, 631
 , 
  
 639, 647 
  
 Akker, R. op den, 377, 378,
  608
 ,
  613 
  
 Al-Hussaini, A. M. M, 285,
  587 
  
 Alblas, H, 470, 613,
  613 
  
 Albro, D. M, 439,
  611 
  
 Alonso Pardo, M. A, 314, 390,
  603
 ,
  636 
  
 Anderson, S. O, 539, 540,
  625
 , 626,
  626 
  
 Anderson, T, 302, 303, 313,
  586 
  
 Augusteijn, L,
  632 
  
 Aycock, J, 217, 218, 318, 390, 551, 583,
  583
 , 603,
  603, 
  
 604
  
 Backhouse, R. C, 539, 540,
  600
 ,
  625, 626 
  
 Baker, T. P,
  586
 , 589 
  
 Bal, H. E,
  643 
  
 Ba´nko, M, 601,
  601 
  
 Bar-Hillel, Y, 425,
  611
 ,
  638 
  
 Bar-On, I, 447, 471,
  612 
  
 Barnard, D. T, 506, 585,
  585
 ,
  619 
  
 Bates, J, 409, 414, 415, 418, 447, 549, 573,
  610
 , 611, 629 
 Bauer, F. L, 584,
  594 
  
 Bell, J. R, 272,
  595 
  
 Bermudez, M. E, 311, 312, 315, 323, 333, 336, 338, 340, 
  
 588–590 
  
 Bertsch, E, 272, 299, 319, 401, 407–409, 414, 418–421, 
  
 546, 591,
  591
 ,
  595
 ,
  610
 , 628,
  628 
  
 Bhamidipaty, A, 552,
  633 
  
 Bhate, S, 14,
  642 
  
 Billington, D, 44,
  640 
  
 Billot, S, 91, 426, 582,
  601 
  
 Birman, A, 506, 510, 519,
  615
 , 622 
  
 Blank, G. D, 141, 637,
  637 
  
 Bod, R, 100,
  632
  
 Boyer, R. S, 162,
  598 
  
 Bratley, P, 141,
  636 
  
 Brent, R. P, 460, 462, 463,
  612 
 Brown, P. J, 523,
  626 
  
 Brüggemann-Klein, A, 141,
  599 
 Bryant, B. R, 484,
  618 
  
 Brzozowski, J. A, 148,
  597
 , 634 
 Bugge, E. H,
  626 
  
 Burke, M. G, 532, 590, 627,
  627
  
 Cabrero Souto, D,
  603 
  
 Carter, L. R, 552,
  631 
  
 Cerecke, C, 540,
  628 
  
 Chang, C.-H, 540, 587,
  587
 , 588, 626,
  626 
  
 Chang, J. H, 458,
  612 
  
 Chapman, N. P, 317,
  588 
  
 Charles, P, 314, 532,
  590
 ,
  627
 , 648 
  
 Cherry, L. L, 316,
  634 
  
 Chester, D, 352,
  637 
  
 Choe, K.-M, 318, 540, 587,
  587
 , 588,
  588
 , 590,
  590
 , 626, 
  
 626
 ,
  628 
  
 Chomsky, N, 14, 38,
  638 
  
 Clark, C, 550,
  633 
  
 Cleaveland, J. C, 479,
  617 
  
 Cocke, J, 77 
  
 Cohen, J, 192,
  581
 ,
  631 
  
 Cohen, R, 328, 333,
  586
 , 587, 606, 640 
  
 Colmerauer, A, 358, 359, 595,
  595
 ,
  606 
  
 Conway, M. E, 623,
  630 
  
 Conway, R. W, 543,
  623 
  
 Cook, S. A,
  639 
  
 Corasick, M. J, 162,
  598 
  
 Corchuelo, R, 540,
  628 
  
 Cordy, J. R, 506, 585,
  585
 ,
  619 
  
 Cormack, G. V, 276, 364, 409,
  607
 ,
  610
 ,
  627 
  
 Cormen, T. H, 76,
  643
  
 ˇCulik, II, K, 328, 333,
  586
 , 587, 606, 640
  
 Bolognesi, R, 391,
  602 
  
 De Vere, L,
  628",NA
Subject Index,"Page numbers in
  bold
  refer to pages where a definition of the indicated term can be
  
 found.
  
 l
 →
 ,
  38 
  
 *
 →
 ,
  38 
  
 r
 →
 ,
  38 
  
 *
 →
 ,
  38 
  
 *
 →
 ,
  38
 
 0
 , 12
  
 
 1
 , 12
  
 ∩
 ,
  52
  
 ∪
 ,
  52
  
 ¬
 ,
  52
 ↛
 , 607 
  
 /—, 607
  
 §-calculus, 160, 506,
  516
 , 622, 623 
  
 _
 ⋖
 ,
  273 
  
 .=
 ,
  267
  
 ⋗
 ,
  267
  
 ⋖
 ,
  267
  
 
 -LR, 591
  
 
 -LR(0),
  287
  
 
 -closure,
  148
  
 
 -free,
  27
 , 40
  
 
 -move,
  147
 , 293, 525
  
 
 -rule,
  27
 , 104, 147, 182, 221, 242, 
 295
 
 -transition,
  147
 , 163, 281 
  
 2-form, 497 
  
 2-form grammar,
  579 
  
 2DPDA, 423, 639 
  
 4-tuple,
  14
  
 A-BNF, 623 
  
 acceptable set, 627 
  
 acceptable-set,
  533 
  
 acceptable-set error recovery,
  533 
 accepting state, 142, 168 
  
 accessible non-terminal,
  51 
  
 ACTION table,
  284 
  
 ACTION/GOTO table,
  285 
  
 active edge,
  227 
  
 ad hoc error recovery,
  542
  
 adjacency restriction, 607 
  
 adjoining,
  494 
  
 affix,
  488
 , 619 
  
 affix grammar, 485,
  488
 , 615 
  
 agenda,
  228
 , 581 
  
 agent parsers,
  447 
  
 AGFL,
  490
 , 632 
  
 Aho and Corasick bibliographic search algorithm,
  162
 , 
  
 598 
  
 algemeines LR,
  592 
  
 ALGOL 60, 27, 514, 576, 594 
  
 ALGOL 68, 476, 615 
  
 alphabet,
  6
 , 578 
  
 alternative,
  15 
  
 ambiguity,
  63
 , 144, 316, 599, 630, 634 
  
 ambiguity rate, 578 
  
 ambiguity test, 63, 338 
  
 American Indian languages, 492 
  
 amusing, 619 
  
 analysis stack,
  170 
  
 analytic grammar, 506, 614 
  
 ancestors table, 602 
  
 AND-node,
  88 
  
 AND-OR tree,
  88 
  
 AND/OR form,
  135 
  
 angle brackets, 27, 619 
  
 ATN,
  56
 , 604, 620, 637 
  
 attribute,
  54
 , 254, 485, 549, 630, 631 
  
 attribute evaluation rule,
  485 
  
 attribute grammar, 485,
  485
 , 588, 615, 643 
  
 Augmented Transition Network,
  see
  ATN 
  
 auxiliary tree,
  493
  
 backreference,
  159 
  
 backslash, 508 
  
 backtracking,
  78
 , 176, 182, 201, 579, 600, 614 
 backtracking recursive descent, 576 
  
 Backus-Naur Form,
  see
  BNF 
  
 backward move,
  530
 , 624 
  
 backward/forward move,
  530
  
 adaptable grammars,
  see
  dynamic grammars 
  
 BC(
 h
 )DR(0), 609",NA
