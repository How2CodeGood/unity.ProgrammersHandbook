Larger Text,Smaller Text,Symbol
Contents at a Glance,"Foreword ��������������������������������������������������������������������������������������������������������������������������
 xvii
 About the Authors �������������������������������������������������������������������������������������������������������������
 �xix
 About the Technical Reviewer �������������������������������������������������������������������������������������������
 xxi
 Acknowledgments �����������������������������������������������������������������������������������������������������������
 xxiii
 Introduction ����������������������������������������������������������������������������������������������������������������������
 xxv
 Chapter 1: Getting Started with SQL Database
 ■
  
  ������������������������������������������������������������������
 1
 Chapter 2: Design Considerations
 ■
  
  �����������������������������������������������������������������������������������
 23
 Chapter 3: Security
 ■
  
  ����������������������������������������������������������������������������������������������������������
 45
 Chapter 4: Data Migration and Backup Strategies
 ■
  
  ����������������������������������������������������������
 67
 Chapter 5: Programming with SQL Database
 ■
  
  ������������������������������������������������������������������
 99
 Chapter 6: SQL Reporting
 ■
  
  ����������������������������������������������������������������������������������������������
 125
 Chapter 7: SQL Data Sync
 ■
  
  ����������������������������������������������������������������������������������������������
 143
 Chapter 8: Windows Azure and ASP�NET
 ■
  
  �����������������������������������������������������������������������
 165
 Chapter 9: Designing for High Performance
 ■
  
  ������������������������������������������������������������������
 183
 Chapter 10: Federations
 ■
  
  ������������������������������������������������������������������������������������������������
 207
 Chapter 11: Performance Tuning
 ■
  
  �����������������������������������������������������������������������������������
 219
 Chapter 12: Windows Azure Mobile Services
 ■
  
  ����������������������������������������������������������������
 241
 Appendix A: SQL Database Management Portal
 ■
  
  ������������������������������������������������������������
 257
 Appendix B: Windows Azure SQL Database Quick Reference
 ■
  
  ����������������������������������������
 275
 Index ���������������������������������������������������������������������������������������������������������������������������������
 283
 www.allitebooks.com",NA
Introduction,"Windows Azure SQL Database, formally known as SQL Azure, appeared on the scene about five years ago. At the 
 time, little was known about it, but Microsoft was beginning to talk quite a bit about the Azure platform. Most people 
 thought that SQL Azure was another NoSQL offering, while in reality it was, and is, nothing of the sort. At that time, 
 the largest database it could handle was 1GB, and no one was really taking it seriously. Since that time, Windows 
 Azure SQL Database has grown into an enterprise-ready PaaS (Platform as a Service) offering based on the proven 
 SQL Server technology. 
 Cloud computing is not hype anymore. Today, cloud-based solutions are becoming the norm rather than an 
 afterthought or sitting on the fringe. The benefits of the Windows Azure cloud platform, including Windows Azure 
 SQL Database, allow businesses to rapidly create and scale solutions with low acquisition costs, yet provide high 
 availability and interoperability. SQL developers and DBAs can use existing skills and knowledge to extend their 
 on-premises solutions and quicken cloud development time. This book covers the fundamental Windows Azure SQL 
 Database concepts, practices, and approaches your valuable data needs as it prepares for the journey to the cloud and 
 Windows Azure SQL Database.
 Because of the rapid pace at which Windows Azure SQL Database is updated, some of the services discussed in 
 this book are in Preview as we write and may change somewhat by the time you read this. However, we have  
 tried our best to bring you the most up-to-date information. Updated information can be found at our blogs, the 
 Windows Azure blog (
 http://blogs.msdn.com/b/windowsazure/
 ), and the all-important Windows Azure home page 
 (
 http://www.windowsazure.com/
 ) where you can find features, pricing, developer information, and much more.
 Our hope is that after reading this book you have a better understanding of, and appreciation for, Windows 
 Azure SQL Database. Whether you are just getting started with it or are a “seasoned veteran,” each chapter contains 
 scenarios and information that we hope you will find helpful and beneficial as you design and build Windows Azure 
 projects. There also is a plethora of source code that is used in chapters where examples are given.",NA
Who This Book Is For,"Pro SQL Database for Windows Azure, 2nd Edition
  is aimed at developers and database administrators who want 
 instant access to a fully-capable SQL Server database environment without the pain of sorting out and managing the 
 physical infrastructure.",NA
How This Book Is Structured,"Pro SQL Database for Windows Azure
  is designed to take you from knowing almost nothing at all about SQL Database 
 to being able to configure and deploy it for use by production applications. The book does assume a knowledge of 
 databases in general, and of SQL in general. From that base, the book takes you from the point of getting started 
 through performance tuning and the use of other Azure Data Services. 
 www.allitebooks.com",NA
Conventions,"Throughout the book, we’ve kept a consistent style for presenting SQL and results. Where a piece of code, a SQL 
 reserved word or fragment of SQL is presented in the text, it is presented in fixed-width Courier font, such as this 
 (working) example:
 select * from dual;
 Where we discuss the syntax and options of SQL commands, we’ve used a conversational style so you can quickly 
 reach an understanding of a command or technique. This means we haven’t duplicated large syntax diagrams that 
 better suit a reference manual. 
 www.allitebooks.com",NA
Downloading the Code,"The code for the examples shown in this book is available on the Apress web site, 
 www.apress.com
 . A link can be 
 found on the book’s information page under the Source Code/Downloads tab, located below the Related Titles 
 section of the page.",NA
Contacting the Authors,"Should you have any questions or comments—or even spot a mistake you think we should know about—you  
 can contact the authors at the following addresses:  
 scottkl@microsoft.com
  (Scott Klein), and  
  
 hroggero@BlueSyntax.onmicrosoft.com
  (Herve Roggero).  
 www.allitebooks.com",NA
Chapter 1,NA,NA
Getting Started with SQL Database,"Born only a few years ago, cloud computing is capturing the imagination of startups and large corporations alike. In 
 its simplest form, cloud computing is an evolution of traditional hosting models; as such, it isn’t necessarily a new 
 technology. Rather, it’s a new concept that offers new opportunities and challenges not found in existing business 
 models. Much as agile programming provided a new software development paradigm, cloud computing provides a 
 new delivery model for Internet-based solutions. And when it comes to relational data, Microsoft delivers the only 
 cloud database available today: Windows Azure SQL Database.",NA
Introduction to Cloud Computing,"Let’s begin with what cloud computing has to offer compared to traditional hosting services. The following 
 capabilities are generally expected from large cloud-computing providers:
 •�
 Automatic and unlimited scalability.
  The promise that if your service needs more resources, 
 more resources will be provisioned automatically or with limited effort. For example, if you 
 deploy a web service, and you experience a sudden surge in processing needs, your services 
 will automatically expand to additional servers to handle the temporary surge and contract to 
 fewer servers during off-peak activity.
 •�
 Unassisted deployment.
  The promise that if you need to deploy additional services or 
 databases, you don’t have to call anyone or open a service ticket. The cloud service provider 
 will give you the necessary tools to perform self-service.
 •�
 Built-in failover.
  The promise that if one of your servers fails, no one will ever notice. For 
 example, if the server on which your service is installed crashes, a new server immediately 
 takes over.
 •�
 Grow as you need; pay for what you use.
  The promise that you only pay for the resources you 
 use. For example, if your service experiences a sudden surge in processing needs for a day, but 
 it scales down to its usual usage for the rest of the month, you’re only charged marginally more 
 than usual for the temporary surge.
 Cloud providers deliver on those promises in different ways. For example, the promise for automated and 
 unlimited scalability comes in different flavors depending on the service being considered. A Web Service layer will 
 be easier to scale than a database layer. And scaling a Web Service layer with Amazon will be different than with 
 Microsoft. As a result, understanding how cloud providers implement these capabilities can be important in your 
 application design choices and support operations.
 The fact that each cloud provider implements its services differently has another, more subtle implication. 
 Switching cloud providers can be very difficult. If you design your application in a way that takes advantage of 
 Amazon-specific services, adapting your application for the Azure platform may be very difficult. As a result, you 
 www.allitebooks.com",NA
Who Is Doing What in the Cloud?,"Smaller companies, including startups, are building services that can run in the cloud, whereas larger companies are 
 investing in building cloud-enabled infrastructure. Some corporations are building consulting services and offering 
 to assist customers implement cloud-enabled solutions; others, like Microsoft, are investing in the core infrastructure 
 and services that make the cloud a reality.
 Microsoft has traditionally been a software provider, but the company has slowly moved closer to hardware 
 solutions over the years. In the late 1990s, Microsoft engaged with Unisys, HP, Dell, and other hardware manufacturers 
 to provide highly available Windows-based platforms (Windows Data Center Edition). At the same time, Microsoft 
 invested significant resources to build its Microsoft Systems Architecture (MSA). This program was designed to help 
 corporations plan, deploy, and manage Microsoft-based IT architecture. These initiatives, along with many others, 
 helped Microsoft develop strong knowledge capital around highly available and scalable architectures, which is a 
 prerequisite for building cloud computing platforms.
 Amazon entered the cloud computing space with its Elastic Compute Cloud (EC2) services in 2005. A few years 
 later, Google and IBM joined forces to enter this market, and Microsoft announced many of its cloud computing plans 
 during 2009, including the Azure platform. As part of its Azure platform, Microsoft delivered a very unique component 
 in its cloud computing offering: a transactional database called Windows Azure SQL Database (also called SQL 
 Database for simplicity, and previously called SQL Azure).",NA
Typical Cloud Services,"Generally speaking, cloud computing comes in one of three flavors:
 •�
 SaaS: software as a service.
  This delivery platform is usually in the form of web applications 
 that are made available on the Internet for a fee. This model has been around for a few years. 
 Microsoft Office 365 and Google Apps are examples of SaaS offerings.
 •�
 PaaS: platform as a service.
  This service offers a computing platform that facilitates the use 
 and deployment of other services and meets the general expectations of cloud computing, 
 such as scalability and pay-as-you-go. Windows Azure SQL Database and Amazon S3 (Simple 
 Storage Service) are examples of PaaS offerings.
 •�
 IaaS: infrastructure as a service.
  This offering provides the necessary infrastructure that 
 offers the scalability typically associated with cloud computing, such as Windows Azure and 
 Amazon EC2 (Elastic Compute), but falls short of delivering cloud services that applications 
 can use directly.
 SaaS, PaaS, and IaaS are considered the fundamental building blocks of cloud computing. Other acronyms are 
 being manufactured to depict new flavors of cloud computing, such as desktop as a service (DaaS), hardware as a 
 service (HaaS), and even research as a service (RaaS). Pretty soon, the entire alphabet will be consumed in describing 
 the many flavors of services that can be created in the cloud.
 More recently private cloud offerings are starting to emerge. A private cloud offers a key advantage over public 
 cloud offerings because it allows corporations to keep their data onsite. This allows certain companies to take 
 advantage of cloud computing without the risk associated with storing data on the Internet. However, private cloud 
 offerings offer fewer benefits than public cloud hosting in other areas. For example, the promise to pay for only what 
 you use no longer applies to private cloud offerings.
 www.allitebooks.com",NA
Discovering the Microsoft Azure Platform,"Let’s discover three major components of the Microsoft Azure platform: Windows Azure, Cloud Services, and 
 Windows Azure SQL Database. All three offer unique capabilities that provide a complete array of services needed  
 to build highly scalable and secure solutions:
 •�
 Windows Azure.
  A collection of virtual Microsoft operating systems that can run your web 
 applications and services in the cloud. For example, you can create a web service that converts 
 US dollars to Euros; then, you can deploy the service on Windows Azure Web Site and allow it 
 to scale as needed. Note that Windows Azure can run. NET applications and other platforms, 
 as well, including PHP.
 •�
 Cloud Services.
  A set of services that provide core capabilities such as federated identity for 
 access control, and a service bus for a messaging-based subscriber/publisher topology.
 •�
 SQL Database.
  Microsoft’s transactional database offering for cloud computing based on 
 Microsoft SQL Server 2012. For example, you can store your customer database in the cloud 
 using SQL Database and consume customer data using services deployed in Windows Azure.
 Microsoft also released, or will be releasing, additional services worth noting, including a Caching Service, High 
 Performance Computing (HPC) and Apache Hadoop for Azure. Additional services are likely to be released over time, 
 offering additional ways to leverage the promises of cloud computing.
 Figure 
 1-1
  shows a simplified corporate environment connecting to the Microsoft Azure platform and consuming 
 various Azure services. This diagram is overly simplified, but it conveys an important message: Microsoft Azure is 
 designed to extend a corporate environment securely for web applications, services, messaging, and data stores.
 Figure 1-1.
  Microsoft Azure platform overview",NA
Why Microsoft Azure?,"One fundamental question that’s frequently asked is, “Why?” Who’s interested in developing applications in Windows 
 Azure in the first place? To answer this question, let’s look at the evolution of web platforms.
 About 20 years ago, when the public Internet was all about bulletin board systems (BBBs), Gopher services, 
 and $500 9600-baud modems, the question was, “Will the Internet stick as a technology?” That question has been 
 answered, but many new concepts have grown since then, including web sites, hosting centers, and SaaS.
 This evolution relies on a common theme: 
 decoupling
 . BBSs decoupled public information from libraries; 
 web sites decoupled user interfaces from computers; hosting centers decoupled hardware from a company’s own 
 infrastructure; and SaaS decoupled complex applications from corporate computers.
 Cloud computing on Microsoft Azure is a natural evolution of computing flexibility in which the actual physical 
 storage and implementation details are decoupled from the software solution. For example, deploying services in 
 Windows Azure doesn’t require any knowledge of the machine running the service or any of the core services  
 (IIS version, operating system patches, and so on). You may never know which machine is running your software.  
 You connect to a Windows Azure server through logical names, and connecting to a SQL Database instance requires 
 an Internet address instead of an actual server name.
 www.allitebooks.com",NA
About Geographic Locations,"In order to provide high availability, Microsoft established regional data-center operations that allow customers to 
 select geographically dispersed services. When you create your Azure servers, you need to specify which geographic 
 location the servers should be provisioned in. This feature is called 
 Windows Azure geolocation
 . 
 Initially, it may be tempting to choose your company’s geographic location for improved performance. However, 
 if the availability of your Azure services is more important than response time, you may need to pick another location. 
 When selecting a geographic location, make sure to consider the following:
 •�
 Performance.
  When your data is closer to your users, network latency may be noticeably 
 lower, improving customer experience.
 •�
 Disaster recovery.
  If ensuring the availability of your cloud platform is important, you may 
 want to disperse your services and data across multiple regions.
 •�
 Legal factors.
  Consider the type of information that will be stored in the cloud, and ensure 
 that you aren’t bound by specific regulations and mandates that may prevent you from 
 selecting remote geographic locations.
 At the time of this writing, you can select from one of the following geographic locations, each of which is 
 supported by a regional data center:
 East Asia
 •�
 East US
 •�
 North Central US
 •�
 North Europe
 •�
 South Central US
 •�
 www.allitebooks.com",NA
Storing Data in Azure,"As you can imagine, cloud computing is all about storing data in a simple yet scalable manner. The Microsoft Azure 
 platform offers a variety of storage models that you can choose from. This section summarizes the four ways you can 
 store your data in Azure; three of these approaches are considered part of the Azure services.
 Figure 
 1-2
  provides an overview of the storage options and the available access methods. The set of storage 
 options provided by Windows Azure is referred to as 
 Windows Azure storage
 , which includes blobs, tables, and 
 queues. Windows Azure storage can be accessed directly from a corporate environment using HTTP/S calls, providing 
 a simple hook into the Microsoft Azure platform. In addition to using Windows Azure storage, consumers can make 
 requests directly to a SQL Database instance using ADO.NET or ODBC, because SQL Database supports the Tabular 
 Data Stream (TDS) protocol that SQL Server uses. As a result, applications and services connecting to a SQL Server 
 database can just as easily connect to SQL Database.
 Figure 1-2.
  Microsoft Azure data storage access",NA
SQL Database Primer,"As you’ve seen, SQL Database is a relational database engine based on SQL Server technology. It supports many of the 
 features of SQL Server including tables, primary keys, stored procedures, views, and much more. This section gives a 
 brief primer to get you started using SQL Database. You’ll see how to register for Azure, how to create a database and 
 then an account, and how to log in.",NA
Registering for Azure,"To register for Windows Azure, visit the Pricing page on the Windows Azure web site: 
 http://www.windowsazure.
 com/en-us/pricing/purchase-options/
 . Figure 
 1-3 
 shows some of the available options available at the time of this 
 writing.",NA
Creating a SQL Database Instance,"This first thing you need to do is to create a new SQL Database server. The name of the SQL Database server becomes 
 a fully qualified Internet address, and a logical name under which database instances are created. When the SQL 
 Database server is created, the master database is provisioned automatically. This database is read-only and contains 
 configuration and security information for your databases. You can then create your user databases. You can either 
 Figure 1-3.
  Choosing a Windows Azure plan",NA
Using the Windows Azure Management Portal,"One way to create a database is to do so from the Windows Azure Management Portal. Selecting the SQL Databases 
 tab in the Navigation pane (left side of the page) will list all of your existing SQL Database instances and the server 
 they are associated with. Creating a database can be accomplished in the portal in either of two ways. First, with 
 the list of database instances displayed, click the New button in the lower-left corner of the portal page in the lower 
 menu bar, and then select SQL Database 
 ➤
  Quick Create. Second, you can optionally select the Servers tab on the 
 top of the portal page (next to the Databases tab), select the appropriate server name from the list of servers, select 
 the Databases tab, and then click Add on the lower menu bar. Figure 
 1-4
  shows the management portal with a few 
 subscriptions and SQL Database instances created.
 Figure 1-4.
  SQL Database instances
 Creating a SQL Database instance via the Quick Create option lets you quickly create a database by specifying 
 the database name, and the subscription and server in which to create the new database instance. If you are 
 creating a new database in a subscription in which no server has been created, you will also be asked to provide an 
 administrator user name and password for the new server that will be provisioned. Creating a database through the 
 Quick Create option creates a 1GB Web Edition database instance.
 Creating a database through the Servers tab is a bit different, in that it brings up the New SQL Database-Custom 
 Create dialog box, as shown in Figure 
 1-5
 . In the Custom Create dialog, in addition to entering a database name, you 
 also have the option of selecting a database edition (Web or Business) and specifying the size of your database and its 
 database collation. Once you have entered the appropriate information in the Custom Create dialog, click OK.",NA
Using a T-SQL Command,"Creating a new database using a T-SQL command is straightforward. Because a database instance in SQL Database 
 is managed by Microsoft, only a few options are available to you. In addition, you must be connected to the master 
 database to create new databases.
 To create a new database using SQL Server Management Studio, log in using the administrator account (or any 
 user with the dbmanager role), and run the following T-SQL command:
 CREATE DATABASE TextDB (MAXSIZE = 10 GB)
 Figure 1-5.
  Creating a SQL Database instance",NA
Configuring the Firewall,"SQL Database implements a firewall on your behalf. That’s a benefit that helps protect your database. Indeed, the 
 default firewall rule is that 
 no one
  can connect to a newly created SQL Database server. You can use the management 
 portal to configure your firewall rules at any time and create databases even if no firewall rules are defined. Allowing 
 no connections by default is a good security practice, because it forces you to think through what IP addresses you 
 wish to allow in.
 Follow these steps to add an IP address (or IP range) for a computer that needs access to the SQL Database 
 server:
 1. 
 In the Windows Azure Management Portal, select the SQL Databases tab in the left 
 navigation bar.
 2. 
 Select the Servers tab above the List Items section.
 3. 
 Select the server name you want to add the firewall rule to.
 4. 
 Select the Configure tab on the top of the List Items section.
 5. 
 In the Allowed IP Addresses section, enter a rule name and the Start and End IP addresses 
 as shown in Figure 
 1-6
 . Click Save.
 Figure 1-6.
  Firewall settings 
 6. 
 Additionally, if you have Windows Azure services that need access to your SQL Database 
 server, select Yes for the Windows Azure Services option in the in the Allowed Services 
 section.",NA
Connecting with SQL Server Management Studio,"Follow these steps to connect to your SQL Database instance using SQL Server Management Studio:
 1. 
 You need to obtain the fully qualified server name of the SQL Database server. Figure 
 1-8
  
 shows the server information on the management portal. The fully qualified server name 
 is located in the Properties pane on the right.",NA
Creating Logins and Users,"With SQL Database, the process of creating logins and users is mostly identical to that in SQL Server, although certain 
 limitations apply. To create a new login, you must be connected to the master database. When you’re connected, you 
 create a login using the 
 CREATE LOGIN
  command. Then, you need to create a user account in the user database and 
 assign access rights to that account.",NA
Creating a New Login,"Connect to the master database using the administrator account (or any account with the loginmanager role granted), 
 and run the following command:
 CREATE LOGIN test WITH PASSWORD = 'T3stPwd001'
 Figure 1-11.
  Running a simple T-SQL command on a SQL Database instance",NA
Creating a New User,"You can now create a user account for your test login. To do so, connect to a user database using the administrator 
 account (you can also create a user in the master database if this login should be able to connect to it), and run the 
 following command:
 CREATE USER test FROM LOGIN test
 If you attempt to create a user without first creating the login account, you receive a message similar to the one 
 shown in Figure 
 1-15
 .
 Figure 1-14.
  Error when your password isn’t complex enough
 Figure 1-15.
  Error when creating a user without creating the login account first",NA
Assigning Access Rights,"So far, you’ve created the login account in the master database and the user account in the user database. But this user 
 account hasn’t been assigned any access rights.
 To allow the test account to have unlimited access to the selected user database, you need to assign the user to 
 the db_owner group :
 EXEC sp_addrolemember 'db_owner', 'test'
 At this point, you’re ready to use the test account to create tables, views, stored procedures, and more.
 ■
  
 Note
  in SQL Server, user accounts are automatically assigned to the public role. however, in SQL database the public 
 role can’t be assigned to user accounts for enhanced security. as a result, specific access rights must be granted in order 
 to use a user account.",NA
Understanding Billing for SQL Database,"SQL Database is a pay-as-you-go model, which includes a monthly fee based on the cumulative number and size of 
 your databases consumed daily, and a usage fee based on actual bandwidth usage. With SQL Database you pay for 
 what you use; so a 7GB database instance will be cheaper than an 8GB database instance. And as you might expect, 
 the cost per GB of space used goes down with larger database sizes. So it is cheaper to have one 100GB database 
 instance than two 50GB database instances. Also, as of this writing, when the consuming application of a SQL 
 Database instance is deployed as a Windows Azure application or service, and it belongs to the same geographic 
 region as the database, the bandwidth fee is waived.
 To view your current bandwidth consumption and the databases you’ve provisioned from a billing standpoint, 
 you can run the following commands:
 SELECT * FROM sys.database_usage        -- databases defined
 SELECT * FROM sys.bandwidth_usage       -- bandwidth
 The first statement returns the number of databases available per day of a specific type: Web or Business 
 edition. This information is used to calculate your monthly fee. The second statement shows a breakdown of hourly 
 consumption per database.
 Note that information stored in this database is available for a period of time, but is eventually purged by 
 Microsoft. You should be able to view up to three months of data in this table.
 Figure 
 1-16
  shows a sample output of the statement returning bandwidth consumption. This statement returns 
 the following information:",NA
Limitations in SQL Database,"As you’ve seen so far, creating databases and users requires manual scripting and switching database connections. 
 The fundamental differences between SQL Server and SQL Database lie in the basic design principles of cloud 
 computing, in which performance, ease of use, and scalability must be carefully balanced. The fact that user 
 databases can be located on different physical servers imposes natural limitations. In addition, designing applications 
 and services against SQL Database requires you to have a strong understanding of these limitations.
 Figure 1-16.
  Hourly bandwidth consumption",NA
Security,"Chapter 3 covers security in depth, but the following list summarizes important security considerations before you 
 deploy your SQL Database instances. From a security standpoint, you need to consider the following constraints:
 •�
 Encryption.
  Although SQL Database uses SSL for data transfers, it doesn’t support the data-
 encryption functions available in SQL Server. However, SQL Database provides support for 
 hashing functions.
 •�
 SSPI authentication.
  SQL Database only supports database logins. As a result, network logins 
 using Security Support Provider Interface (SSPI) aren’t supported.
 •�
 Connection constraints.
  In certain cases, the database connection is closed for one of the 
 following reasons:
 Excessive resource usage
 •�
 Long-running query
 •�
 Long-running single transaction
 •�
 Idle connection
 •�
 Failover due to server failure
 •�
 •�
 Disallowed user names.
  Certain user names can’t be created, for security reasons:
 sa
 •�
 admin
 •�
 administrator
 •�
 guest
 •�
 root
 •�
 •�
 Login name.
  In certain cases, you may need to append the server name to the login name 
 to correctly log in, in this format: [loginName]@[servername]. So, avoid using the arrobas 
 character (@) in login names.
 •�
 TCP port 1433.
  Only TCP Port 1433 is allowed. It isn’t possible to define another listening port 
 for SQL Database.",NA
Backups,"Backing up your SQL Database instance is somewhat different from backing up traditional SQL Server databases. You 
 can’t back up a SQL Database instances in the traditional sense, nor can you restore a SQL Server database in SQL 
 Database. You do, however, have the ability to create a transactionally consistent clone of a SQL Database instance 
 before you export/import your data. You can expect the following regarding backups:
 •�
 Backup/Restore operations.
  These operations aren’t available. In addition, you may not 
 attach or detach a SQL Database instance.
 •�
 Clone operations.
  You may create a clone of a SQL Database instance into another one using 
 the 
 CREATE DATABASE
  statement.
 •�
 Log files.
  You can’t access the database log files, nor can you create a log backup.",NA
Objects,"Certain objects available in SQL Server aren’t available in SQL Database. If your applications depend heavily on 
 these features, you may have difficulty using SQL Database, and you may need to rethink your application design to 
 accommodate these limitations. The following are some of the limitations that currently apply to SQL Database:
 •�
 CLR.
  The.NET CLR isn’t available in SQL Database. As a result, you can’t create extended 
 stored procedures or extended functions.
 •�
 System functions.
  SQL Database supports many system functions, including Aggregate 
 functions and Ranking functions. However, SQL Database doesn’t support RowSet functions, 
 including these:
 •�
 OPENQUERY
 •�
 OPENXML
 •�
 OPENROWSET
 •�
 OPENDATASOURCE
 •�
 System stored procedures.
  Only a small subset of system stored procedures are available in 
 SQL Database, in the following categories:
 Catalog stored procedures
 •�
 Database engine stored procedures
 •�
 Security stored procedures
 •�
 •�
 System tables.
  None of the system tables are available.
 •�
 System views.
  A subset of system views is available; you can access some of them from the 
 master database and others from user databases. The following are some of the system views 
 available (for a complete list, refer to the online MSDN library for SQL Database):
 •�
 sys.sql_logins
 •�
 sys.views
 •�
 sys.databases
 •�
 sys.columns
 •�
 sys.objects
 •�
 Heap tables.
  SQL Database doesn’t allow the use of heap tables. All tables must have a 
 clustered index.",NA
Miscellaneous,"In addition to the limitations outlined so far, additional components and options offered by SQL Server aren’t 
 available in SQL Database. For the most part, these limitations shouldn’t affect your application designs, but they’re 
 good to keep in mind:",NA
Drivers and Protocols,"You should also know that accessing SQL Database can only be performed using specific libraries. This may be 
 relevant if you don’t use ADO.NET in your programming stack. For example, older versions of Delphi can’t connect  
 to SQL Database. Here is a summary of the supported data libraries:
 •�
 TDS version 7.3 or higher.
  Any client using a TDS version prior to 7.3 isn’t supported.
 •�
 Drivers and libraries.
  The following drivers and libraries are allowed:
 .NET Framework Data Provider for SQL Server from.NET 3.5 SP1
 •�
 SQL Server 2008 Native Client ODBC driver or higher
 •�
 SQL Server 2008 driver for PHP version 1.1 or higher
 •�",NA
Summary,"This chapter focused on a fast-track overview of SQL Database by providing a high-level introduction to cloud 
 computing and how Microsoft is delivering its cloud offering. You also learned the major steps involved in creating an 
 Azure account and how to get started with SQL Database. You saw some important limitations of the SQL Database 
 platform, but keep in mind that Microsoft is releasing new versions of its cloud database every few months; as a result, 
 some of these limitations will be lifted over time.",NA
Chapter 2,NA,NA
Design Considerations,"In order to use cloud computing with the Azure platform for more than simple hosting, you must explore the vast 
 array of options available for designing solutions. In addition to understanding the design options presented in this 
 chapter, you need a strong grasp of cloud computing’s current shortcomings, which may affect your design choices.",NA
Design Factors,"Before reviewing various design patterns, let’s start with some opportunities and limitations that impact your design 
 choices. Keep in mind that although this book focuses primarily on SQL Database, many of the concepts in this 
 chapter apply to Azure development in general.",NA
Offsite Storage,"As introduced in Chapter 1, the Azure platform offers four distinct storage models: Blob, Table, Queue, and SQL 
 Database. Storing data in SQL Database is similar to storing data in SQL Server. All you need to do is issue T-SQL 
 statements and review some of the limitations of the syntax specific to SQL Database, and off you go!
 The ability to store data in SQL Database using T-SQL offers unique opportunities. In many cases, you can easily 
 extend or port certain types of applications in SQL Database with no (or limited) modifications. This portability allows 
 you either to implement solutions that directly depend on SQL Database for storage, or to use a local database while 
 using SQL Database transparently for additional storage requirements (such as reporting).
 However, keep in mind that you’re limited to the amount of data you can store in a single SQL Database instance. 
 At the moment, SQL Database supports two editions: Web (1GB or 5GB) and Business (from 10GB to 150GB). So, if 
 your application needs to store more than 150GB of data, or if your database can benefit from a multithreaded data 
 access layer, you need to consider splitting your data across multiple databases through a form of partitioning called a 
 shard
 . You’ll learn about shards later in this chapter and in more detail throughout this book.",NA
High Availability,"When designing applications, software developers and architects are usually concerned about high-availability 
 requirements. SQL Database uses a very elaborate topology that maximizes workload redistribution, transparency, 
 and recovery. Figure 
 2-1
  shows a high-level implementation of SQL Database that gives a hint about how advanced 
 the backend infrastructure must be.",NA
Performance,"The performance of applications you write can be affected by two things: throttling and how you design the 
 application. Microsoft has put 
 performance throttling
  in place to prevent one client’s applications from impacting 
 another. (It’s a good feature, not nearly so bad as it may sound.) Application design is something you control.",NA
Throttling,"SQL Database runs in a multitenant environment, which implies that your database instances share server resources 
 with databases from other companies. As a result, the SQL Database platform has implemented a throttling algorithm 
 that prevents large queries from affecting performance for other users. If your application issues a large query that 
 could potentially affect other databases, your database connection is terminated.
 Figure 2-1.
  SQL Database topology
 www.allitebooks.com",NA
Application Design Considerations,"When considering how to design your application to best take advantage of SQL Database, you need to evaluate the 
 following items:
 •�
 Database roundtrips.
  How many roundtrips are necessary to perform a specific function in 
 your application? More database roundtrips mean a slower application, especially when the 
 connection is made over an Internet link and is SSL encrypted.
 •�
 Caching.
  You can improve response time by caching resources on the client machine or 
 storing temporary data closer to the consumer.
 •�
 Property lazy loading.
  In addition to reducing roundtrips, it’s critical to load only the 
 data that’s absolutely necessary to perform the required functions. Lazy loading can help 
 significantly in this area.
 •�
 Asynchronous user interfaces.
  When waiting is unavoidable, providing a responsive user 
 interface can help. Multithreading can assist in providing more responsive applications.
 •�
 Shards.
  A 
 shard
  is a way of splitting your data across multiple databases in a manner that is as 
 transparent as possible to your application code, thus improving performance. Federation is 
 the sharding technology available in SQL Database.",NA
Data Synchronization,"There are two primary ways to synchronize data with SQL Database: the Microsoft Sync Framework and the SQL Data 
 Sync service. The Microsoft Sync Framework offers bidirectional data-synchronization capabilities between multiple 
 data stores, including databases. SQL Data Sync uses the Microsoft Sync Framework, which isn’t limited to database 
 synchronization; you can use the framework to synchronize files over different platforms and networks.
 Specifically, as it relates to SQL Database, you can use the Microsoft Sync Framework to provide an offline mode 
 for your applications by keeping a local database synchronized with a SQL Database instance. And because the 
 framework can synchronize data with multiple endpoints, you can design a shard, described in detail later, in which 
 all database instances keep their data in sync transparently.
 The SQL Data Sync service provides a simpler synchronization model between on-premise SQL Server 
 databases and SQL Database, or between SQL Database instances. Since this service runs in the cloud, it is ideal for 
 synchronizing cloud databases without having to install and configure a service on-premise.",NA
Direct vs. Serviced Connections,"You may also consider developing Azure services to keep the database connection to a local network, and send the 
 data back to the client using SOAP or REST messages. If your Azure services are deployed in the same region as your 
 SQL Database instances, the database connection is made from the same datacenter and performs much faster. 
 However, sending data back to the consumer using SOAP or REST may not necessarily improve performance; you’re 
 now sending back XML instead of raw data packets, which implies a larger bandwidth footprint. Finally, you may 
 consider writing stored procedures to keep some of the business logic as close to the data as possible.
 Figure 
 2-2
  shows the two different ways an application can retrieve data stored in a SQL Database instance.  
 A direct connection can be established to the database from the application, in which case the application issues 
 T-SQL statements to retrieve data. Alternatively, a serviced connection can be made by creating and deploying  
 custom SOAP or REST services on Windows Azure, which in turn communicate to the database. In this case, the 
 application requests data through web services deployed in Azure.
 Figure 2-2.
  Data connection options",NA
Pricing,"Pricing of a hosted environment isn’t usually considered a factor in standard application design. However, in the 
 case of cloud computing, including Azure, you need to keep in mind that your application’s performance and overall 
 design have a direct impact on your monthly costs.
 For example, you incur network and processing fees whenever you deploy and use Azure services. Although this 
 is true, at the time of this writing, the data traffic between a Windows Azure application or service and a SQL Database 
 instance is free within the same geographic location.
 Pricing may affect your short-term application design choices, but you should keep in mind that Microsoft may 
 change its pricing strategy at any time. As a result, although pricing is an important consideration especially for 
 projects on limited budget, long-term viability of a design should be more important than short-term financial gains.
 If you’re designing an application to live in the Azure world and you depend on this application to generate 
 revenue, you must ensure that your pricing model covers the resulting operational costs. For example, your 
 application should be designed from the ground up with billing capabilities in mind if you intend to charge for its use.
 Another factor related to pricing is that your SQL Database instance cost consists of a monthly fee and a usage 
 fee. The monthly fee is prorated, so if you create a database at 1pm and drop it at 2pm the same day, you’re charged 
 a fraction of the monthly fee, plus the usage fee. The usage fee is strictly limited to bandwidth consumption: CPU 
 utilization, I/O consumption, and your database’s memory footprint aren’t factors in the usage fee (see Figure 
 2-3
 ).  
 However, your database connection may be throttled if your database activity reaches specific thresholds, as 
 previously discussed.
 Figure 2-3.
  Pricing and resource throttling
 In summary, you can consider moving certain CPU-intensive activities (within reason) onto the SQL Database 
 instance without being charged. You may, for instance, perform complex joins that use large datasets in a stored 
 procedure and return a few summary rows to the consumer as a way to minimize your usage fee.",NA
Security,"It goes without saying that security may be a concern for certain types of applications; however, these concerns 
 are similar to those that companies face when using traditional hosting facilities. The question that comes to mind 
 when considering security in cloud computing is related to the lack of control over data privacy. In addition, certain 
 limitations may prevent certain kinds of monitoring, which automatically rules out the use of SQL Database for highly 
 sensitive applications unless the sensitive data is fully encrypted on the client side.
 As a result, encryption may become an important part of your design decision. And if you decide to encrypt your 
 data, where will the encryption take place? Although the connection link is encrypted between your application code 
 and SQL Database, and you can use hashing functions with SQL Database, the data itself isn’t encrypted when it’s 
 stored in SQL Database on disk. You may need to encrypt your data in your application code before sending it over the 
 public Internet so that it’s stored encrypted.
 Encryption is good for data privacy, but it comes with a couple of downsides: slower performance and difficulty 
 in searching for data. Heavy encryption can slow down an application, and it’s notoriously difficult to search for data 
 that is encrypted in a database.",NA
Review of Design Factors,"So far, you’re seen a few considerations that can impact your design choices. Table 
 2-1
  provides a summary. Some 
 of the considerations are related to opportunities that you may be able to take advantage of; others are limitations 
 imposed by the nature of cloud computing or specifically by the Azure platform.
 Table 2-1.
  Summary of design factors
 Opportunities
 Limitations
 Offsite storage
 Limited amount of storage
 Elastic cost
 Performance
 Instant provisioning
 Backups
 SQL Data Sync
 Security concerns
 High availability
 As you design applications, make sure you evaluate whether specific Azure limitations discussed in this book still 
 apply—the Azure platform is likely to change quickly in order to respond to customer demands.",NA
Design Patterns,"Let’s review the important design patterns that use SQL Database. Before designing your first cloud application, 
 you should read this section to become familiar with a few design options. Some of the advanced design patterns 
 explained in this chapter can also provide significant business value, although they’re more difficult to implement.
 Note that for simplicity, the diagrams in this section show only a direct connection to SQL Database. However, 
 almost all the patterns can be implemented using a serviced connection through Azure services.",NA
Direct Connection,"The 
 direct connection pattern
 , shown in Figure 
 2-4
 , is perhaps the simplest form of connectivity to a SQL Database 
 instance. The consumer can be either an application located in a corporation’s network or a Windows Azure service 
 connecting directly to the SQL Database instance.",NA
Smart Branching,"The 
 smart branching pattern
  (see Figure 
 2-5
 ) describes an application that contains sufficient logic to determine 
 whether the data it needs to load is located in the cloud or in a local database. The logic to make this determination is 
 either hardcoded in the application or driven from a configuration file. It may also be provided by a data access layer 
 (DAL) engine that contains logic that fetches data from either a local or a cloud database.
 Figure 2-4.
  Direct connection pattern
 Figure 2-5.
  Smart branching pattern",NA
Transparent Branching,"Whereas smart branching depends on the consumer (or one of its components) to determine whether data is local 
 or in the cloud, 
 transparent branching
  (see Figure 
 2-6
 ) removes this concern from the consumer. The consuming 
 application no longer depends on routing logic and becomes oblivious to the ultimate location of the data.
 Figure 2-6.
  Transparent branching pattern
 This pattern is best implemented by applications that are difficult to modify or for which the cost of 
 implementation is prohibitive. It can effectively be implemented in the form of extended stored procedures that have 
 the knowledge to fetch data from a cloud data source. In essence, this pattern implements a DAL at the database layer.",NA
Sharding n,"So far, you’ve seen patterns that implement a single connection at a time. In a 
 shard
  (see Figure 
 2-7
 ), multiple 
 databases can be accessed simultaneously in a read and/or write fashion and can be located in a mixed environment 
 (local and cloud). However, keep in mind that the total availability of your shard depends partially on the availability 
 of your local databases.",NA
Shard Concepts and Methods,"Before visiting the shard patterns, let’s analyze the various aspects of shard design. Some important concepts are 
 explained here:
 •�
 Decision rules.
  Logic that determines without a doubt which database contains the record(s) 
 of interest. For example, 
 if Country = US, then connect to SQL Database instance #1
 . Rules 
 can be static (hardcoded in C#, for example) or dynamic (stored in XML configuration files). 
 Static rules tend to limit the ability to grow the shard easily, because adding a new database is 
 likely to change the rules. Dynamic rules, on the other hand, may require the creation of a rule 
 engine. Not all shard libraries use decision rules.
 •�
 Round-robin.
  A method that changes the database endpoint for every new connection 
 (or other condition) in a consistent manner. For example, when accessing a group of five 
 databases in a round-robin manner, the first connection is made to database 1, the second to 
 database 2, and so on. Then, the sixth connection is made to database 1 again, and so forth. 
 Round-robin methods avoid the creation of decision engines and attempt to spread the data 
 and the load evenly across all databases involved in a shard.
 •�
 Horizontal partition.
  A collection of tables with similar schemas that represent an entire 
 dataset when concatenated. For example, sales records can be split by country, where each 
 country is stored in a separate table. You can create a horizontal partition by applying decision 
 rules or using a round-robin method. When using a round-robin method, no logic helps 
 identify which database contains the record of interest; so all databases must be searched.
 •�
 Vertical partition.
  A table schema split across multiple databases. As a result, a single record’s 
 columns are stored on multiple databases. Although this is considered a valid technique, 
 vertical partitioning isn’t explored in this book.
 •�
 Mirrors.
  An exact replica of a primary database (or a large portion of the primary database that 
 is of interest). Databases in a mirror configuration obtain their data at roughly the same time 
 using a synchronization mechanism like SQL Data Sync. For example, a mirror shard made of 
 two databases, each of which has the Sales table, has the same number of records in each table 
 at all times. Read operations are then simplified (no rules needed) because it doesn’t matter 
 which database you connect to; the Sales table contains the data you need in all the databases.",NA
Read-Only Shards,"Shards can be implemented in multiple ways. For example, you can create a read-only shard (ROS). Although the 
 shard is fed from a database that accepts read/write operations, its records are read-only for consumers.
 Figure 
 2-8
  shows an example of a shard topology that consists of a local SQL Server to store its data with read and 
 write access. The data is then replicated using the SQL Data Sync Framework (or other method) to the actual shards, 
 which are additional SQL Database instances in the cloud. The consuming application then connects to the shard (in 
 a SQL Database instance) to read the information as needed.
 Figure 2-8.
  Read-only shard topology
 In one scenario, the SQL Database instances each contain the exact same copy of the data (a mirror shard), so the 
 consumer can connect to one of the SQL Database instances (using a round-robin mechanism to spread the load, for 
 example). This is perhaps the simpler implementation because all the records are copied to all the databases in the 
 shard blindly. However, keep in mind that SQL Database doesn’t support distributed transactions; you may need to 
 have a compensating mechanism in case some transactions commit and others don’t.
 Another implementation of the ROS consists of synchronizing the data using horizontal partitioning. In a 
 horizontal partition, rules are applied to determine which database contains which data. For example, the SQL Data 
 Sync service can be implemented to partition the data for US sales to one SQL Database instance and European 
 sales to another. In this implementation, either the consumer knows about the horizontal partition and knows which 
 database to connect to (by applying decision rules based on customer input), or it connects to all databases in the 
 cloud by applying a 
 WHERE
  clause on the country if necessary, avoiding the cost of running the decision engine that 
 selects the correct database based on the established rules.
 www.allitebooks.com",NA
Read-Write Shards,"In a read-write shard (RWS), all databases are considered read/write. In this case, you don’t need to use a replication 
 topology that uses the SQL Data Sync Framework, because there is a single copy of each record within the shard. 
 Figure 
 2-9
  shows an RWS topology.
 Figure 2-9.
  Multimaster shard topology
 Although an RWS removes the complexity of synchronizing data between databases, the consumer is responsible 
 for directing all CRUD operations to the appropriate cloud database. This requires special considerations and 
 advanced development techniques to accomplish, as previously discussed, unless you use SQL Database Federations.",NA
Offloading,"In the offloading pattern, the primary consumer represents an existing onsite application with its own database; 
 but a subset of its data (or the entire database) is replicated to a cloud database using SQL Data Sync (or another 
 mechanism). The offloaded data can then be used by secondary consumers even if the primary database isn’t 
 accessible.
 You can implement the offloading pattern in two ways, as shown in Figure 
 2-10
 . The primary database can be 
 either the local SQL Server database or the cloud database. For example, a legacy application can use a local SQL 
 Server database for its core needs. SQL Data Sync is then used to copy relevant or summary data in a cloud database. 
 Finally, secondary consumers such as portable devices and PDAs can display live summary data by connecting to 
 the cloud for their data source. Note that if you use the SQL Data Sync service, you can choose to have bidirectional 
 synchronization of data.",NA
Aggregation,"In its simplest form, the aggregation pattern provides a mechanism to collect data from multiple data providers into a 
 SQL Database instance. The data providers can be geographically dispersed and not know about each other, but they 
 must share a common knowledge of the schema so that, when aggregated, the data is still meaningful.
 The aggregation patterns shown in Figure 
 2-11 
 use the direct connection pattern. You can use an aggregation 
 pattern to provide a common repository of information, such as demographic information or global warming metrics 
 collected from different countries. The key in this pattern is the ability to define a common schema that can be 
 used by all providers and understood by the consumers. Because SQL Database supports XML data types, you can 
 also store certain columns in XML, an option that provides a mechanism to store slightly different information per 
 customer.
 Figure 2-10.
  Offloading patterns",NA
Mirroring,"The mirror pattern, shown in Figure 
 2-12
 , is a variation of the offloading pattern where the secondary consumer can 
 be an external entity. In addition, this pattern implies that a two-way replication topology exists, so that changes in 
 either database are replicated back to the other database. This pattern allows a 
 shared nothing
  integration, in which 
 neither consumer has the authority to connect to the other consumer directly.",NA
Combining Patterns,"The previous design patterns provide the necessary basis to build systems with SQL Database. Some of these patterns 
 can be used as is, but you’re very likely to combine patterns to deliver improved solutions. This section describes 
 some useful combinations.",NA
Transparent Branching + RWS,"Figure 
 2-13
  shows the transparent branching and the read-write shard patterns combined. This pattern can be used 
 to offload, into the cloud, storage of historical data that an existing Enterprise Resource Planning (ERP) application 
 generates. In this example, the shard provides a way to ensure high throughput by using asynchronous round-robin 
 calls into SQL Database instances.",NA
Cascading Aggregation,"In cascading aggregation (see Figure 
 2-14
 ), the aggregation pattern is applied serially to generate a summary 
 database. The mechanism to copy (or move) data from one SQL Database instance to another must be accomplished 
 using a high-level process, such as a worker process in Windows Azure.",NA
Sample Design: Application SLA Monitoring,"To put a few of the patterns in perspective, let’s try an example. We’ll create a formal design around a system 
 that monitors application performance service-level agreements (SLAs). In this design, a company already has 
 a monitoring product that can audit activity in existing SQL Server databases at customer sites. Assume that the 
 company that makes this monitoring product wants to extend its services by offering a SQL Database storage 
 mechanism so it can monitor customers’ database SLAs centrally.",NA
Pre-Azure Application Architecture,"First, let’s look at the existing application-monitoring product. It contains a module that monitors one or more SQL 
 Servers in an enterprise and stores its results in another database located on the customer’s network.
 In this example, Company A has implemented the monitoring service against an existing ERP product to monitor 
 access security and overall SLA. The monitoring application performs the auditing based on live activity on the 
 internal SQL Server storing the ERP data. When certain statements take too long to execute, the monitoring service 
 receives an alert and stores an audit record in the local auditing database, as shown in Figure 
 2-15
 .
 Figure 2-14.
  Aggregation + MMS patterns",NA
Azure Implementation,"The monitoring provider has created an enhanced version of its monitoring system and includes an optional cloud 
 storage option, in which the monitoring service can forward performance events in a centrally located database in the 
 cloud. The monitoring provider has decided to implement an asynchronous smart branching pattern so that events 
 can be stored in a SQL Database instance.
 Figure 
 2-16
  shows the implementation architecture that lets the monitoring service store data in a cloud 
 database. Each monitoring service can now store SLA metrics in the cloud in addition to the local auditing database. 
 Finally, the local auditing database is an option that customers may choose not to install. To support this feature, 
 the monitoring provider has decided to implement a queuing mechanism in case the link to SQL Database becomes 
 unavailable.",NA
Other Considerations,"This chapter has introduced many important design factors to help you design a solution that uses SQL Database. Are 
 few more concepts are worth a glance, such as blob data stores, edge data caching, and data encryption.",NA
Blob Data Stores,"Blobs
  are files that can be stored in Windows Azure. What is interesting about blobs is that they can be easily accessed 
 through REST, there is no limit to the number of blobs that can be created, and each blob can contain as much as 
 200GB of data for block blobs and 1TB of data for page blobs. As a result, blobs can be used as a backup and transfer 
 mechanism between consumers.
 A system can dump SQL Database tables to files using the Bulk Copy Program (BCP), possibly compressing  
 and/or encrypting the files beforehand, and store the blobs in Windows Azure.",NA
Edge Data Caching,"The chapter briefly mentioned caching earlier, but you should remember that caching may yield the most important 
 performance gains in your application design. You can cache relatively static tables in memory, save them as blobs (or 
 a form of in-memory storage) so that other caching systems use the same cache, and create a mechanism to refresh 
 your data cache using queues in Azure.
 Figure 
 2-17
  shows an example of a design that creates a shared cache updated by two ERP systems. Each ERP 
 system uses the transparent branching pattern to update shared records in a SQL Database instance. At this point, 
 however, the edge caches aren’t aware of the change in data. At a specific interval (every 10 minutes, for example), 
 a worker process in Windows Azure picks up the changes and stores them in blobs. The worker may decide to apply 
 logic to the data and resolve conflicts, if any. Blobs are then created (or replaced) with the latest cache information 
 that should be loaded. The edge cache refreshes its internal data by loading the blobs at specific intervals (every  
 5 minutes, for example) and replaces its internal content with the latest cache. If all edge caches are configured to run 
 against a public atomic clock, all the caches are updated virtually at the same time.
 Figure 2-17.
  Shared edge data caching",NA
Data Encryption,"You can encrypt your data in two environments: onsite or in Windows Azure using a service. SQL Database, as 
 previously mentioned, doesn’t support encryption at this time (although hashing is supported). If you need to encrypt 
 Social Security numbers or phone numbers, you should consider where encryption makes sense.
 Generally speaking, unless your application runs in a public environment where your private keys can be at 
 risk, you should consider encrypting onsite before the data goes over the Internet. But if you need a way to decrypt in 
 Windows Azure, or you need to encrypt and decrypt data across consumers that don’t share keys, you probably need 
 to encrypt your data in Windows Azure before storing it in SQL Database.",NA
SaaS Applications and Federations,"Designing SaaS applications typically requires the use of multiple databases in order to scale both data storage and 
 performance. In order to build a solid SaaS solution, developers typically develop a custom framework that provides 
 the connection routing logic needed by an application to determine which database it should connect to.
 If you are building a SaaS application, you should investigate the use of Federations, a feature of SQL Database. 
 Federations help with scalability by distributing data across multiple databases by using administrative SQL 
 commands. I will review Federations in greater detail in Chapter 11, including how the feature works and some of its 
 current limitations.",NA
Summary,"This chapter reviewed many design concepts that are unique to distributed computing and for which cloud 
 computing is a natural playground. Remember that designing cloud applications can be as simple as connecting to a 
 SQL Database instance from your onsite applications or as complex as necessary (with distributed edge caches and 
 shards) for enterprise-wide systems.
 The chapter provided multiple parameters that you should consider when designing your application, each 
 of which can significantly impact your final design, such as performance, billing, security, storage options, and 
 throttling. You should consider creating two or more cloud-based designs and reviewing them with other designers 
 to discuss pros and cons before making your final selection. And if you have the time, build a proof-of-concept to 
 validate your assumptions and measure how effective the solution will be.",NA
Chapter 3,NA,NA
Security,"Compared to other systems in most corporations, database environments are probably the weakest point when it 
 comes to security, with a few exceptions such as the banking sector. The reason is that databases are considered 
 well within the boundaries of internal networks, which are considered secured and usually inaccessible directly 
 from the Internet.
 With the advent of SQL Database and most Database as a Service solutions, the focus on database security rises 
 all the way to the top for two primary reasons: you’re no longer in control of your data, and the data can be directly 
 accessed from the Internet. As a result, it becomes even more important to take advantage of all the capabilities of 
 SQL Database and understand its limitations.",NA
Overview,"Before diving into the specifics of SQL Database, let’s look at a general security framework to assess how Database 
 as a Service can impact you. The following discussion is based on the basic security principles encapsulated by 
 confidentiality, integrity, and availability (CIA). This is referred to as the 
 CIA triad
  and is one of the most accepted 
 forms of security categorization. SQL Database has different strengths and weaknesses than traditional SQL Server 
 installations, so it is important to review each area of the CIA triad to understand how to deal with its weaknesses and 
 to leverage its strengths.",NA
Confidentiality,"Confidentiality
  is the ability to ensure that data can be accessed only by authorized users. It’s about protecting your 
 data from prying eyes or from inadvertent leakage, and it’s achieved by using multiple technologies, including the 
 following:
 •�
 Encryption
 . Creates a 
 ciphertext
  (encrypted information) that can be decrypted through the 
 use of a shared key or a certificate.
 •�
 Hashing
 . Generates a ciphertext that can’t be decrypted (typically used for password storage).
 •�
 Access control
 . Controls access to data based on contextual information.
 •�
 Authentication
 . Controls who can access the database and which objects in the database a 
 user can access.
 •�
 Firewall
 . Uses technology to limit network connectivity to a list of known machines.
 SQL Database offers new features, such as a firewall (as previously discussed); however, it doesn’t yet support 
 data encryption natively (such as Transparent Data Encryption [TDE] and field-level encryption), which places more 
 emphasis on the other confidentiality techniques.
 www.allitebooks.com",NA
Integrity,"Data 
 integrity
  refers to the objective of ensuring that information is modified only by authorized users, and that 
 damage can be undone if necessary. Integrity of data can be compromised in multiple ways, such as a malicious 
 SQL Injection attack or the unintentional execution of a 
 TRUNCATE
  statement on a table, wiping out all the records. 
 Generally speaking, you can implement the following integrity measures in a database:
 •�
 Authorization
 . Controls who can change what data.
 •�
 Backup
 . Creates a transactionally consistent database snapshot from which data can be 
 recovered.
 •�
 Roles-based access
 . Provides the minimum access rights to different roles in a company, such 
 as developers and support.
 •�
 Auditing
 . Tracks database access and data changes to provide an audit trail for forensic 
 analysis.
 From an integrity standpoint, SQL Database doesn’t yet provide the same capabilities as SQL Server. SQL 
 Database does deliver strong authorization capabilities and role-based security, similar to SQL Server 2012. However, 
 traditional database backups and activity auditing aren’t available as of this writing. Microsoft is building new backup 
 mechanisms for SQL Database, above and beyond the BCP (Bulk Copy Program) operations available now. See 
 Chapter 4 for more information about how to back up your data in SQL Database.",NA
Availability,"Availability
  ensures service uptime so your data can be accessed when it’s needed. Designing highly available systems can 
 be very complex and requires advanced knowledge in multiple areas including disk configuration, system administration, 
 disaster-recovery locations, and more. The following are some of the technologies involved in high availability:
 •�
 Redundant disks
 . Allows you to recover from the loss of a disk spindle. Usually involves a 
 RAID configuration.
 •�
 Redundant networks
 . Allows you to survive the loss of multiple network components, such as 
 a network card or a router.
 •�
 Redundant services
 . Allows you to survive the interruption of services such as security and 
 databases. An example is the use of Microsoft Cluster Service.
 •�
 Redundant hardware
 . Allows you to survive the loss of machine hardware, such as a CPU or a 
 memory chip.
 •�
 Scalability
 . Delivers information at near constant speed under load.
 •�
 DoS prevention
 . Prevents successful denial of service (DoS) attacks that would otherwise 
 prevent data availability.",NA
Securing Your Data,"Let’s dive into some specifics and code examples to show how to secure your data in SQL Database. You may need 
 to secure specific columns in your database that contain sensitive information, such as Social Security numbers or 
 credit card numbers. Certain medical applications store patient data, which can fall under compliance review, and 
 as such may need to be encrypted as well. As hinted previously, not all security mechanisms are currently available, 
 so this section focuses on what SQL Database provides and on ways to mitigate the missing features. Regarding data 
 encryption, because SQL Database provides none, you’ll see how to implement your own security classes to simplify 
 data encryption in your projects.
 ■
  
 Note
  The examples that follow use a database script called 
 Security.sql
  and a Visual Studio 2008 project called 
 SQLAzureSecurity.sln
 . you can run the SQL script on your local SQL Server database if you don’t have a Windows 
 Azure account yet.
 This chapter uses a few classes and methods to demonstrate how to use encryption, hashing, and other 
 techniques. Figure 
 3-2
  shows the objects being used. The 
 Encryption
  class performs the actual encryption and 
 returns a 
 CipherText
  structure; the 
 UserProperties
  class uses extension methods from the 
 Extensions
  class and a 
 helper method in the 
 Util
  class. The 
 CDatabase
  class returns the database connection string.
 Figure 3-2.
  Object model used in the examples",NA
Encryption,"As mentioned previously, data encryption isn’t available. Why? Because SQL Database doesn’t support X.509 
 certificates. Certificates are necessary for many encryption-related features, such as Transparent Data Encryption 
 (TDE), column-level encryption, and certain T-SQL commands, such as 
 FOR ENCRYPTION
  and 
 SIGNBYCERT
 .
 However, SQL Database requires the use of SSL encryption for its communication. This means your sensitive 
 data is always transmitted safely between your clients and your SQL Database instance. There is nothing you need 
 to do to enable SSL encryption; it’s required and automatically enforced by SQL Database. If an application tries to 
 connect to SQL Database and the application doesn’t support SSL, the connection request fails.
 But SSL doesn’t encrypt data at rest; it only encrypts data in transit. How can you protect your data when it’s 
 stored in SQL Database? Because SQL Database doesn’t support encryption natively, you must encrypt and decrypt 
 your data in the application code.",NA
Hashing,"Hashing isn’t nearly as complicated as you’ve seen so far. And although you can store the values you’ve encrypted so 
 far in the database, in this example you hash all the columns of the rows (except the 
 ID
  value) to make sure they’re 
 unchanged. Why? The answer goes back to the integrity concern of the CIA triad discussed earlier. You want a way 
 to tell whether your data has been modified outside of your code. Encrypting your secret value makes it virtually 
 impossible to break the confidentiality aspect of the triad, but someone can still update the 
 PropertyName
  column—or, 
 worse, the 
 Value
  column. Hashing doesn’t prevent data from being modified, but you have a way to detect whether it 
 was changed without your authorization.",NA
Certificates,"As discussed previously, SQL Database doesn’t support X.509 certificates, although you can deploy X.509 certificates 
 in Windows Azure. Your client code (hosted either on your company’s network or in Windows Azure) can use 
 certificates to encrypt and decrypt values. The use of certificates implies that you’re encrypting using a public/private 
 key pair. The public key is used to encrypt data, and the private key is used to decrypt data.
 ■
  
 Note
  For more information on how to deploy X.509 certificates in Windows Azure, visit the MSDN blog  
 http://blogs.msdn.com/jnak
  and look at the January 2010 archive. The blog entry by Jim Nakashima contains 
 detailed instructions.
 You can easily create a self-signed certificate using the 
 MakeCert.exe
  utility which is a utility you can find in the 
 Windows SDK. To create a certificate on your machines, run the following command at a command line. You need to 
 execute this statement as an Administrator or the command will fail:
 makecert -ss root -pe -r -n ""CN=BlueSyntaxTest"" -sky Exchange -sr LocalMachine
 Here is a brief overview of the options used to create this certificate:
 •�
 -ss root
  stores the certificate in the root certificate store.
 •�
 -pe
  marks the private key exportable.
 •�
 -r
  creates a self-signed certificate (meaning that it wasn’t issued by a root certificate authority 
 (CA) like Thawte).
 •�
 -n ""CN=...""
  specifies the subject’s name of the certificate.
 •�
 -sky Exchange
  specifies that the certificate is used for encryption.
 •�
 -sr LocalMachine
  specifies that the certificate store location as 
 LocalMachine
 .
 www.allitebooks.com",NA
Access Control,"So far, you’ve spent a lot of time encrypting and hashing values for increased confidentiality and integrity. However, 
 another important aspect of the CIA triad is access control. This section reviews two subcategories of access control: 
 authentication (also referred to as AUTHN) and authorization (AUTHZ).",NA
Authentication (AUTHN),"AUTHN is a process that verifies you’re indeed who you say you are. In SQL Server, the AUTHN process is done 
 through one of two mechanisms: network credentials (which are handled through Kerberos authentication over the 
 Security Support Provider Interface [SSPI]) or SQL Server credentials. Connection strings must specify which AUTHN 
 is being used. And when you use SQL Server AUTHN, a password must be provided before attempting to connect, 
 either by a user at runtime or in a configuration file.
 Keep the following items in mind when you’re considering AUTHN with SQL Database:
 •�
 No network authentication
 . Because SQL Database isn’t on your network, network AUTHN 
 isn’t available. This further means you must use SQL AUTHN at all times and that you must 
 store passwords in your applications (in configuration files, preferably). You may want to store 
 your passwords encrypted. Although you can encrypt sections of your configuration files in 
 Windows using the 
 aspnet_regiis.exe
  utility, this option isn’t available in Windows Azure 
 using the default providers. However, you can encrypt sections of your web configuration file 
 using a custom configuration provider: PKCS12 (found on the MSDN Code Gallery). For more 
 information on how to use this custom provider, visit 
 http://tinyurl.com/9ta8m5u
 .
 •�
 Strong passwords
 . SQL Database requires the use of strong passwords. This option can’t be 
 disabled, which is a good thing. A strong password must be at least eight characters long; must 
 combine letters, numbers, and symbols; and can’t be a word found in a dictionary.
 •�
 Login name limitations
 . Certain login names aren’t available, such as sa, admin, and guest. 
 These logins can’t be created. You should also refrain from using the @ symbol in your login 
 names; this symbol is used to separate a user name from a machine name, which may be 
 needed at times.",NA
Authorization (AUTHZ),"Authorization gives you the ability to control who can perform which actions after being authenticated. It’s important 
 to define a good AUTHZ model early in your development cycle, because changing access-control strategy can be 
 relatively difficult.
 Generally speaking, a strong AUTHZ model defines which users can access which objects in the database. This 
 is typically performed in SQL Database and SQL Server by defining relationships between logins, users, schemas, 
 and rights.",NA
Creating Logins and Users,"A login account is used to manage authentication in SQL Database; it is a server-level entity. If you know a login 
 account with its password, you can log in. A user is a database entity and is used for access control. The user 
 determines which objects you can access and what actions you can perform. Each login can map to 0 or 1 user in each 
 database, including master.
 In SQL Database, you must be connected to the master database to manage your logins. The 
 CREATE LOGIN
  T-SQL 
 statement is partially supported. Also, remember that you must use a strong password when creating logins.
 SQL Database offers two new roles:
 •�
 LoginManager
 . Grants a user the ability to create new logins in the master database.
 •�
 DBManager
 . Grants a user the ability to create new databases from the master database.
 First let’s create a login account in the master database:
 CREATE LOGIN MyTestLogin WITH PASSWORD='MyT3stL0gin'
 GO
 You can optionally grant this login access to master. The following code shows how to give that login access to the 
 master database and grant the LoginManager role in the master database. If your intention is to only grant this login 
 access to the user database, you can skip this step.
 CREATE USER MyTestLoginUser FROM LOGIN MyTestLogin
 GO
 EXEC sp_addrolemember 'loginmanager', MyTestLoginUser
 GO
 In most cases you will want to create the login account in master and the user in the user database. The following 
 code shows how to create the user in a user database with read rights. You must first connect to the user database or 
 the statement below will fail.
 CREATE USER MyTestLoginUser FROM LOGIN MyTestLogin
 GO
 EXEC sp_addrolemember 'db_datareader', MyTestLoginUser
 GO
 Now the MyTestLogin account can connect to your user database.",NA
Schemas,"A 
 schema
  is a container that holds database objects; schemas reside inside a database. Schemas are part of the three-
 part naming convention of database objects; they’re considered namespaces. Each object in a schema must have a 
 unique name.
 By default, objects created are owned by the DBO schema when you connect with the dbo login. For example, the 
 CREATE TABLE
  statement showed previously for the UserProperties table uses DBO as the schema owner (
 schema_id
  is 
 always 1 for DBO). See Figure 
 3-6
 .
 Figure 3-6.
  Viewing an object’s schema ownership
 Right now, the new user MyTestLoginUser can’t read from this table. Attempting to issue a 
 SELECT
  statement 
 against UserProperties returns a 
 SELECT permission denied
  error. So, you have a choice: you can either give that user 
 account 
 SELECT
  permission on that table, assign the user to the appropriate role, or create a schema for that user and 
 assign the 
 SELECT
  permission to the schema.
 It’s usually much easier to manage access rights through roles instead of users directly. However, if you want finer 
 control over access rights, you should consider using schema-driven security. To do this properly, you need to change the 
 ownership of the UserProperties table to a new schema (other than DBO) and then assign access rights to the schema.
 To create a new schema, you must be connected to the desired user database where MyTestLoginUser has been 
 created. Then run the following statement:
 CREATE SCHEMA MyReadOnlySchema AUTHORIZATION DBO",NA
Schema Separation,"The schema security model presented previously is the foundation for implementing a multitenant model using schema 
 separation as a sharding mechanism. From a storage standpoint, a multitenant system is a collection of databases that 
 hosts multiple customers in such a way that each customer is isolated from a security and management standpoint.
 Although you can create a multitenant system by creating a database for each customer, you can also create a 
 schema container for each customer and co-locate customers in fewer databases. The schema security model discussed 
 in this chapter allows you to securely co-locate customers in the same database and keep strong security boundaries by 
 using different logins and users for each customer, where each user is only given access to a specific schema.
 In Figure 
 3-11
 , I am creating a new customer container in an existing database, called DB1. Because the database 
 stores other customers and my objective is to ensure strong security isolation, I first create a new login (L3) for my 
 customer in master (the customer is called C3 in this example).
 Figure 3-11.
  Creating a new schema container for a customer
 Then I connect to DB1 and create the schema container (S3), in which the customer data will be stored. Once the 
 schema container is created I can create all the necessary objects in S3, such as tables, views, and stored procedures.
 Next I create a user (U3) for the login account L3. I make sure to change the default schema of U3 from DBO to S3.",NA
SQL Database Firewall,"SQL Database comes with its own firewall, which you can configure directly from the SQL Database portal, as 
 previously covered in Chapter 1. You can also view and change firewall rules in T-SQL. Let’s take a quick look at the 
 available SQL statements.
 ■
  
 Note
  you need to be connected to the master database to view or change firewall rules. At least one connection rule 
 must be added to the firewall through the SQL Database portal before a connection can be made.
 To view the current firewall rules, execute this statement:
 SELECT * FROM sys.firewall_rules
 You can see that each rule has a name; the name is used as a unique key. The 
 sp_set_firewall_rule
  command 
 allows you to add a new rule.
 It may take a few minutes for the new rules to take effect. For example, the following statement adds a new rule 
 called 
 NewRule
 . Notice that the first parameter must be a Unicode string:
 sp_set_firewall_rule N'NewRule', '192.168.1.1', '192.168.1.10'
 To delete a rule, run the following statement:
 sp_delete_firewall_rule N'NewRule'",NA
Internal Firewalls,"Some organizations control network access with advanced rules for connecting to the Internet using internal firewalls. 
 Because connecting to SQL Database from your internal network requires opening port “TCP 1433 out,” if your 
 company uses an internal firewall you may need to request this port to be open. For security reasons your network 
 administrator may request the IP Range of SQL Database so that the port is only open for specific destinations. 
 Unfortunately, restricting the connection to an IP Range may become an issue at a later time because SQL Database 
 IP Ranges may change over time. As a result you should open the port to ANY IP destination.",NA
Compliance,"Although cloud computing creates new challenges for organizations from a risk-management standpoint, Microsoft’s 
 cloud data centers undergo multiple audits and assessments based on their local regulations. In order to facilitate its 
 compliance audits and assessment, Microsoft created the Operational Compliance team, which designed a common 
 compliance framework for its operations.
 www.allitebooks.com",NA
Summary,"Security in the cloud is a complex topic and involves careful analysis of your requirements and design options. This 
 chapter covered the basics of the CIA triad and classified security options in terms of confidentiality, integrity, and 
 availability.
 You also reviewed how to plan for strong encryption and hashing in your Visual Studio applications. Finally, keep 
 in mind that schema separation can be very useful and should be implemented early in your development cycles.
 By now, you should understand the many options available to you in order to secure you data in SQL Database 
 and be aware of some of the limitations of the SQL Database platform. Keep in mind, however, that some of those 
 limitations are likely to be either removed or mitigated at some point in the future as Microsoft provides additional 
 updates to its SQL Database platform.",NA
Chapter 4,NA,NA
Data Migration and Backup Strategies,"When companies talk about their research into or experiences with the Azure technology—specifically the SQL side 
 of Azure—two of their most frequent concerns (aside from security) are migrating local databases and data into the 
 cloud, and backup strategies. Until Azure came around, databases were housed locally (and they still are): they’re 
 contained within the walls of the company or in a data center. Moving to the Azure platform and SQL Azure means 
 moving all or a portion of your data into the cloud and storing it there.
 Chapter 3 talked at length about security concerns, considerations, and best practices for storing your precious 
 data in the cloud. Moving data into the cloud is a decision that you shouldn’t and can’t take lightly. But after you make 
 the decision to utilize SQL Azure, the question becomes, how do you get your data into the cloud? As nice as it would 
 be if moving your local database into SQL Azure was seamless, it isn’t as cut-and-dried as you may think. You do have 
 several viable options available; but you must consider things beyond just moving the data, such as costs from data 
 transfers.
 After your data is in the cloud, further questions arise regarding the backup strategies that are common with 
 local databases. In SQL Azure, gone are the concepts of backup devices and backing up and restoring databases. As 
 shocking as this may sound, remember that Microsoft is managing the hardware behind the scenes. For now, there are 
 no such things as drives, devices, and so on.
 In this chapter we will discuss the different migration tools, strategies, and concepts for moving your database 
 and data into the cloud. You’ll see examples illustrating how these tools are used. We’ll finish the chapter off by 
 spending a page or two on backup strategies and tools that help provide SQL Database backup capabilities.",NA
Migrating Databases and Data to SQL Azure,"So you want to move one or more of your applications and its databases to the cloud. It’s a noble idea. More than 
 likely, you’re in the same category as countless others who are looking into moving applications into the cloud: you 
 don’t want to start from scratch. You’d rather migrate an existing application to the cloud, but you aren’t sure about 
 the steps necessary to do so, or the technologies available to help in the process. This section discusses three tools 
 from Microsoft that come as part of SQL Server:
 The Import/Export Service
 •�
 The Generate and Publish Scripts Wizard
 •�
 The 
 •�
 bcp
  utility
 In addition to these three tools, we will also briefly mention a free utility found on CodePlex called the SQL Azure 
 Migration Wizard, which provides a wizard-driven interface to walk you through migrating your database and data to 
 SQL Azure.
 The examples in this chapter use SQL Server 2012, which is available from Microsoft’s MSDN site. These 
 examples also work with SQL Server 2008, although some the screens may be a bit different.",NA
The Import/Export Service,"In 2012 Microsoft released some great functionality for migrating an on-premises database to SQL Azure in the form 
 of the Import/Export service. To understand what this service is and how it works, you first need to know about the 
 Data-Tier Application Framework, more commonly known as DAC Fx.",NA
Data-Tier Application Framework,"DAC Fx was introduced in SQL Server 2008 R2 as part of the Application and Multi-Server Management services, 
 a way to manage your database environments efficiently and proactively. It was designed with scale in mind to 
 provide an effective way to manage the deployment of the database schema between environments, such as from 
 Development to Test, and Test to Production.
 DAC Fx is a set of tools, APIs, and services designed to improve the development, deployment and management 
 of SQL Server database schemas. In most environments, the DBA creates and maintains sets of T-SQL scripts that 
 create or alter database objects, such as tables and stored procedures. The issue is that in many instances the DBAs 
 need to maintain multiple sets of scripts; one for the initial creation and another for updates and modifications that 
 update the database from one version to another. Add on top of that multiple versions of those scripts, and you have 
 the makings of a complex maintenance scenario.
 This is where DAC Fx comes in. Instead of building and maintaining sets of T-SQL scripts, SQL Server 2008 R2 
 introduced the concept of a 
 BACPAC
 . A DAC, or Data-Tier Application, is an entity that contains all of the database 
 objects used by an application. Its purpose is to provide a single unit for creating, deploying, and managing objects 
 instead of managing each object individually.
 When a DAC is ready to be deployed, it is built into a DAC package, or 
 DACPAC
 . This package is a file that 
 contains the DAC definition, and is simply a zip file containing multiple XML files.
 However, these 
 DACPAC
 s only contain schema, not data. This is where the new version of the DAC Fx comes in. 
 Version 2.0 now supports data as well as schema,  dumping your table data right alongside your schema in a new file 
 format called 
 .bacpac
 . Using the DAC Fx, you can now extract a BACPAC from an existing database and deploy it to a 
 different SQL Server environment, including, wait for it . . . SQL Azure.
 Now, it should be noted that SQL Server 2012 comes with the DAC Fx version 2.0 and the ability to export to a 
 BACPAC. The next two sections are going to walk you through creating and deploying BACPACs using SQL Server 
 2012. You will see how the Import/Export service is used in the Windows Azure Management Portal to import and 
 export to and from a SQL Azure database.",NA
Deploying to SQL Azure via a BACPAC using SQL Server 2012,"For all those who have not upgraded to SQL Server 2012 the next session will address that, but for now, this section 
 will use SQL Server 2012. First you’ll see how to deploy an on-premises database directly to SQL Azure, and then how 
 to use the built-in import and export features to migrate using BACPACs from SQL Server 2012.
 Keep in mind that all of the examples in this section assume that your database is 
 Azure-ready
 , meaning that your 
 on-premises SQL Server database is ready to be migrated to SQL Azure. For example, if you were to download the full 
 AdventureWorks database, such as the SQL Server 2008 R2 version, and then try to walk through the examples in this 
 section, the examples in this section will fail due to objects that aren’t supported in Windows Azure SQL Database 
 (such as native encryption and Full Text Search). The section “Generate and Publish Scripts Wizard” in this chapter 
 explains how to make your database Azure-ready by taking an on-premise database and applying the appropriate 
 modifications and changes. More importantly, it also explains why those changes need to be made.",NA
The Import/Export Service,"Up until now, the focus has been on the client side; importing and exporting from SQL Server to a BACPAC. The 
 Import/Export service is designed to import and export directly between Windows Azure SQL Database and Windows 
 Azure BLOB storage. This service has been available in production since late January 2012 and is free of charge. No 
 sign-ups, no codes to enter. Just go use it. How? The Import/Export service is built directly into the Windows Azure 
 Management Portal via two buttons on the lower Ribbon bar: Import and Export.
 Import
 In this section, you will upload the BACPAC file  to BLOB storage and then import it  into an entirely new SQL 
 Database instance using the Import/Export service. The first step is to upload the 
 AWMini.bacpac
  to Windows Azure 
 BLOB storage. If you haven’t created a Windows Azure storage account, select the Storage tab in the left navigation 
 pane, and then select New 
 ➤
  Storage 
 ➤
  Quick Create from the lower New menu. Type in the name of the Storage 
 account into the URL field, select the Region and Subscription, and click Create Storage Account.
 With the storage account created, select the account name in the portal; then select the Containers tab. On 
 the lower menu bar, select Add Container to create a container in which to upload the BACPAC. In the New Blob 
 Container dialog, enter a name for the container (I called mine bacpac), and then click the check (OK) button.
 The next step is to upload the AWMini BACPAC into the storage account and container. Currently this option 
 is not available in the Windows Azure Management Portal, but there are several tools that can help you do this. On 
 CodePlex is a project called Azure Storage Explorer (
 http://azurestorageexplorer.codeplex.com/
 ), a utility for 
 viewing and managing Windows Azure storage (BLOBs, tables, and queues). RedGate also has a utility called Cloud 
 Storage Studio (
 www.cerebrata.com/Products/CloudStorageStudio/
 ), which provides a rich set of functionality for 
 managing your Windows Azure storage. There are a few more out on the Internet, but these two are the most popular. 
 Either of these two tools will allow you to quickly and easily upload the 
 AWMini.bacpac
  to your BLOB container.
 Once the BACPAC is uploaded into BLOB storage, log into the Windows Azure Management Portal at  
 https://manage.windowsazure.com
  with the Windows Live ID that is associated with your Windows Azure 
 subscription. Once logged in to the portal, select the SQL Databases node in the left navigation pane. If you have  
 any SQL Database instances, the lower section of the portal will display Import and Export buttons, as shown in 
 Figure 
 4-11
 . Otherwise, only an Import button will be visible.",NA
Generate and Publish Scripts Wizard,"The Generate and Publish Scripts Wizard is used to create T-SQL scripts for SQL Server databases and/or related 
 objects within the selected database. You have probably used this wizard, so this section doesn’t walk through it step 
 by step; instead, the section briefly highlights a few steps in the wizard and points out the options necessary to work 
 Figure 4-14.
  The Export Database dialog",NA
Starting the Wizard,"To start the Generate and Publish Scripts Wizard in SQL Server Management Studio (SSMS), open Object Explorer 
 and expand the Databases node. Select the AdventureWorks2012 database, right-click it, and then select Generate 
 Scripts from the context menu.
 On the wizard’s Introduction page for SQL Server 2012, you’re informed that you must follow four steps to 
 complete this wizard:
 1. 
 Select database objects.
 2. 
 Specify scripting or publishing objects.
 3. 
 Review selections.
 4. 
 Generate scripts.
 The following sections work through these steps.",NA
Choosing Target Objects,"To select your target database objects, follow these steps:
 1. 
 On the Introduction page of the Generate and Publish Scripts Wizard, click Next.
 2. 
 On the Choose Objects page (see Figure 
 4-15
 ), select the Select Specific Database Objects 
 option, because for the purposes of this example, you simply want to select a few objects to 
 migrate.",NA
Setting Advanced Options,"Clicking the Advanced button brings up the Advanced Scripting Options dialog shown in Figure 
 4-17
 . Follow these 
 steps to set the advanced options for this example’s script:
 1. 
 In the Advanced Scripting Options dialog, set the following options:
 •�
 Convert UDDTs to Base Types:
  True
 •�
 Script Extended Properties:
  False
 •�
 Script Logins:
  False
 •�
 Script USE DATABASE:
  False
 Figure 4-16.
  Scripting options",NA
Reviewing the Generated Script,"Open the file you created, and let’s take a quick look at the generated T-SQL. The following snippet from what you see 
 shows the creation of an XML Schema collection and a single table. To save space in the chapter, the majority of the 
 XML Schema Collection has been left out, along with creation of some of the constraints, but your script will show 
 the entire 
 CREATE
  statement. Also, except for the things you told the script-generation wizard to ignore, the following 
 T-SQL looks like all other object creation T-SQL you typically deal with on a daily basis:
 /****** Object:  Schema [Person]    Script Date: 4/22/2012 3:38:28 PM ******/
 CREATE SCHEMA [Person]
 GO
 /****** Object:  XmlSchemaCollection [Person].[IndividualSurveySchemaCollection]    Script Date: 
 4/22/2012 3:38:28 PM ******/
 CREATE XML SCHEMA COLLECTION [Person].[IndividualSurveySchemaCollection]
 AS N'<xsd:schema xmlns:xsd=""
 http://www.w3.org/2001/XMLSchema
 ""
 ...
 </xsd:schema>'
 GO
 /****** Object:  UserDefinedDataType [dbo].[Name]    Script Date: 4/22/2012 3:38:28 PM ******/
 CREATE TYPE [dbo].[Name] FROM [nvarchar](50) NULL
 GO
 /****** Object:  UserDefinedDataType [dbo].[NameStyle]    Script Date: 4/22/2012 3:38:28 PM ******/
 CREATE TYPE [dbo].[NameStyle] FROM [bit] NOT NULL
 GO
 /****** Object:  Table [Person].[Person]    Script Date: 4/22/2012 3:38:28 PM ******/
 SET ANSI_NULLS ON
 GO
 SET QUOTED_IDENTIFIER ON
 GO
 CREATE TABLE [Person].[Person](
      [BusinessEntityID] [int] NOT NULL,
      [PersonType] [nchar](2) NOT NULL,
      [NameStyle] [bit] NOT NULL,
      [Title] [nvarchar](8) NULL,
      [FirstName] [nvarchar](50) NOT NULL,
      [MiddleName] [nvarchar](50) NULL,
      [LastName] [nvarchar](50) NOT NULL,
      [Suffix] [nvarchar](10) NULL,
      [EmailPromotion] [int] NOT NULL,
      [AdditionalContactInfo] [xml](CONTENT [Person].[AdditionalContactInfoSchemaCollection]) NULL,",NA
Fixing the Script,"Because you selected to script for SQL Server 2012, the script includes some syntax and statements that aren’t 
 supported in SQL Azure. Figure 
 4-18
  shows some of the errors you will see if you try to run the script as generated.
 Figure 4-18.
  SQL Azure execution errors
 In this example, you scripted only a single table, a few UDTs, and an XML index. If you scripted more objects, you 
 would also see errors such as these:
 Keyword or statement option ‘pad_index’ is not supported in this version of SQL Server.
 •�
 Keyword or statement option ‘allow_row_locks’ is not supported in this version of SQL Server.
 •�
 Keyword or statement option ‘textimage_on’ is not supported in this version of SQL Server.
 •�
 ‘ROW GUID COLUMN’ is not supported in this version of SQL Server.
 •�
 ‘Filegroup reference and partitioning scheme’ is not supported in this version of SQL Server.
 •�",NA
Executing the Script Against an Azure Database,"You don’t have a SQL Azure database to run the script against, so let’s create one now:
 1. 
 Connect to your SQL Azure instance (refer to Chapter 1 for reference if needed), making 
 sure you’re connecting to the master database.
 2. 
 Open a new query window, and use the syntax discussed in Chapter 1 to create your SQL 
 Azure database. Name it AWMini, because this is the name the examples use throughout 
 this chapter.
 3. 
 Click over to the generated script. This query window is currently connected to your local 
 SQL instance, so you need to change it to your SQL Azure instance and the database 
 you just created. Right-click anywhere in the script, and select Connection 
 ➤
  Change 
 Connection from the context menu.",NA
The bcp Utility,"The 
 bcp
  utility provides bulk copying of data between instances of Microsoft SQL Server. This utility is installed with 
 SQL Server and requires no knowledge or understanding of T-SQL syntax. If you aren’t familiar with the 
 bcp
  utility, 
 don’t confuse or associate its functionality with that of the Import/Export Wizard in SQL Server. Although the 
 bcp
  
 documentation refers to what 
 bcp
  does as a “bulk copy,” be aware that you can’t 
 bcp
  data from a source into a destination 
 with a single statement. You must first 
 bcp
  the data out of the source; then, you can 
 bcp
  the data in to the destination.
 The 
 bcp
  utility is a great way to move data. Since it doesn’t do schema (that is, the schema must already exist), the 
 bcp
  utility is a very quick way of migrating data. For example, you need to refresh your SQL Database with data from 
 an on-premises database, the 
 bcp
  option is a great solution to do that.
 ■
  
 Note
  the 
 bcp
  utility is very flexible and powerful, and you can apply a lot of options to it. this section doesn’t go into 
 the entire range of 
 bcp
  options or dive deep into the many uses of the utility. You can find that information in the SQL Server 
 Books online or on the Microsoft MSDn web site at 
 http://msdn.microsoft.com/en-us/library/ms162802.aspx
 .
 This section describes how to use the 
 bcp
  utility to export data from a local database and import the data into 
 your SQL Azure database. It also discusses some things you should watch out for when using the 
 bcp
  utility for 
 SQL Azure.",NA
Invoking bcp,"The 
 bcp
  utility has no GUI; it’s a command prompt–driven utility. But don’t let that intimidate you, especially given 
 what you’re using it for. It’s very flexible and can seem a bit overwhelming, but it’s quite simple. The basic syntax for 
 the 
 bcp
  utility is as follows:
 bcp 
 table direction filename
  -
 servername
  -
 username
  -
 password
 where:
 •�
 table
  is the source or destination table based on the direction parameter.
 •�
 direction
  is 
 in
  or 
 out
 , depending on whether you’re copying data into the database or out of 
 the database.
 •�
 filename
  is the filename you’re copying data to or from.
 •�
 servername
  is the name of the server you’re copying data to or from.
 •�
 username
  is the username used to connect to either the local or SQL Azure database.
 •�
 password
  is the password associated with the username.
 Let’s get started by exporting the data from your source database.",NA
Exporting the Data,"Begin by copying data out of your local SQL instance. Open a command prompt, and type the command shown 
 in Figure 
 4-19
 . Enter your own values for the server name, and the target directory for the resulting 
 bcp
  file. In this 
 example I used the 
 -T
  parameter to tell the 
 bcp
  utility to connect to my local instance with a trusted connection using 
 integrated security.
 Figure 4-19.
  Using 
 bcp
 to export data
 Notice that in this example you’re using the 
 out
  keyword for the direction parameter. That’s because you’re 
 copying data 
 out
  of SQL Server.
 The 
 -n
  parameter performs the bulk-copy operation, using the native database data types of the data. The 
 -q
  
 parameter executes the 
 SET QUOTED_IDENTIFIERS ON
  statement in the connection between the 
 bcp
  utility and your 
 SQL Server instance.
 After you type in the command, press Enter to execute the 
 bcp
  utility. In mere milliseconds, over 71,000 rows are 
 exported and copied to the 
 user.dat
  file (see Figure 
 4-20
 ).
 Figure 4-20.
  Output from the bcp export command",NA
Importing the Data,"The next step is to copy the data into the cloud—specifically, to your SQL Azure AWMini database. The syntax for 
 copying 
 into
  a database is very similar to the syntax for copying data 
 out
 . You use the 
 in
  keyword and specify the 
 server name and credentials for your SQL Azure database, as shown in Figure 
 4-21
 .
 Figure 4-21.
  
 Uniqueidentifier
  data type error during 
 bcp
  import
 After you type in the command, press Enter to execute the 
 bcp
  utility. Notice in Figure 
 4-22
  that importing all 
 71,320 rows took no time at all.
 Figure 4-22.
  Successful 
 bcp
  import
 If you ever receive an error stating that an unexpected end-of-file (EOF) was encountered, this error isn’t specific 
 to SQL Azure; the 
 bcp
  utility has issues with columns of the 
 uniqueidentifier
  data type. You can find posts and blogs 
 all over the Internet about this problem.
 The solution is to drop the rowguid column from the table in the SQL Azure table. The cool thing is that you don’t 
 need to re-export the data. You can simply re-execute the 
 bcp import
  command and then put the rowguid column 
 back on the table.
 As stated earlier, SQL Server BOL is full of information about how to use the 
 bcp
  utility. This section is a brief 
 introductory look at how to use this utility to move data from your local SQL Server instance to SQL Azure. The 
 bcp
  
 utility is a bulk-copy method of moving data. It lacks SSIS’s ability to convert data from one data type to another, and 
 SSIS’s workflow components. But if all you’re interested in is moving data from one table to a similar destination table, 
 bcp
  is your best friend.",NA
SQL Azure Backup Strategies,"Your data is in the cloud, but it really doesn’t stop there. Much of the functionality that DBAs have at their fingertips 
 when dealing with local data stores doesn’t exist in the cloud yet. The operative word in that sentence is 
 yet
 .
 Database Copy enables you to copy your database to make a new database on the same SQL Azure server. 
 Alternatively, you can copy to a different SQL Azure server in the same subregion or data center. This functionality is 
 much needed, but at the same time it has some shortcomings; some can be worked around, but others will require 
 future service updates.",NA
Copying a Database,"The Database Copy feature allows you to create a single copy of a source database. You do so by adding a new 
 argument to the 
 CREATE DATABASE
  statement: 
 AS COPY OF
 . As a refresher, the syntax for 
 CREATE DATABASE
  is as 
 follows:
 CREATE DATABASE MyDatabase (MAXSIZE= 10 GB, EDITION= 'Business')
 To create a copy of a source database, the syntax now becomes
 CREATE DATABASE MyDatabase AS COPY OF [
 source_server_name
 ].
 source_database_name
 ]
 Thus, if you want to create a copy of your AWMini database, the syntax is
 CREATE DATABASE AWMini2 AS COPY OF 
 servername
 .AWMini
 Figure 
 4-23
  shows the execution of the previous statement. The interesting thing to note is the message in 
 the Messages window. When you execute the 
 CREATE DATABASE
  statement with the 
 AS COPY OF
  argument, you 
 immediately get the “Command(s) completed successfully” message. Does this mean the copy finished that quickly? 
 No. This message means the copy has 
 started
 . You can also see in Figure 
 4-23
  that the AWMini2 database is already 
 listed in the list of databases; however, that doesn’t mean the database copy has completed.",NA
Knowing When a Copy Is Complete,"The question then becomes, how do you know when the copy is finished? The answer is that Microsoft created a  
 new data management view (DMV) to return the details of the database copy operation. This DMV is called  
 sys.dm_database_copies, and it returns a great deal of information about the status of the database copy, such as 
 when the database copy process started and completed, the percentage of bytes that have been copied, error codes, 
 and more. In addition, Microsoft modified the state and state_desc columns in the sys.databases table to provide 
 detailed information about the status of the new database.
 Figure 
 4-24
  shows the progress of the database copy. The statement looks at the sys.dm_database_copies DMV 
 and checks the status of the copy, reporting the start date and time, and percent complete of the copy process, and 
 the date and time the status was updated (in the modify_date column). Any copy errors are shown in the error_code 
 and error_description columns. Once the copy is complete, the specific row in the table for the copy is removed from 
 the table.
 Figure 4-23.
  Copying a database
 Figure 4-24.
  Checking the database copy status",NA
Automating a Database Copy,"You can schedule a database copy via an on-premises SQL Agent job and an SSIS package (as discussed earlier in this 
 chapter). The job can be scheduled like a normal on-premises SQL job, as long as the connection information for the 
 Execute SQL task points to the SQL Azure database.
 Although this may not be the most favorable solution, it’s certainly an option, and it does provide the scheduling 
 capabilities you’re looking for. The key for this solution is to first delete the copy database before you re-create it.",NA
Maintaining a Backup History,"The Database Copy functionality lets you create an instant backup of your database, but it doesn’t provide a way to 
 create a backup history. In other words, you can’t append to the backup and create multiple days’ worth of backups. 
 You do have several options, however.
 If all you care about is backing up the current day’s data, you can delete the current backup copy and recopy the 
 database. This is a viable option and doesn’t require a lot of maintenance.
 If, on the other hand, you want a backup history, doing so is a bit trickier. Many, if not most, companies like to 
 keep a week’s worth of backups. These companies back up their databases each night and keep seven days’ worth 
 of backups so they have the option to restore past the previous night’s backup. To do this with the Database Copy 
 functionality, you must create seven copies of the source database—you have seven backup copy databases.
 This strategy works, but keep in mind that you’re billed for those additional seven databases. The key here is that 
 if you’re using SQL Azure, a look at your backup plan is critical.",NA
Backing Up Using the Import/Export Features,"While the 
 AS COPY OF
  feature is nice, you are now paying the cost for an additional database. What many companies 
 are now looking at for a backup and restore solution is the Import/Export Services discussed at the beginning of 
 this chapter. Storing database backups in Windows Azure BLOB storage is certainly much cheaper than the cost of 
 a second database, and in addition you can have multiple backups of your database. Now, granted, neither of these 
 give you the Transactional and Differential backup solutions that you are used to on-premises, but the Import/Export 
 service and DAC framework give you a lot more flexibility.",NA
Third-Party Backup Products,"Currently on the market there are two very good products that provide excellent backup and restore capabilities for 
 SQL Azure:
 Enzo Backup for SQL Azure
 •�
 Cloud Services
 •�",NA
Enzo Backup,"Enzo Backup for SQL Azure, by a company called Blue Syntax, is a local Windows application that lets you execute 
 backup and restore requests for your SQL Azure database. The flexibility for this tool comes in several ways. First, you 
 can back up and restore to and from BLOB storage or your local file system. Backups are automatically compressed 
 and include both schema and data. Second, you can back up an entire database or a specific schema. You can also 
 restore a specific schema or a specific table within that schema or database.",NA
Cloud Services,"Red Gate is known for their SQL Server tools. They continue to create great products, and Cloud Services doesn’t 
 disappoint. Cloud Services is a suite of tools that help manage your cloud applications. As part of this suite of tools 
 is the ability to back up your SQL Database. You can schedule backups to Windows Azure BLOB storage as well as 
 restore the backup to a new database or an existing SQL Database. More information on Red Gate’s Cloud Services 
 can be found here:
 http://cloudservices.red-gate.com/",NA
Summary,"In this chapter we discussed the various options for migrating your database schema and associated data to SQL 
 Azure. You learned that there are pros and cons to each method; for example, the SQL Server script-generation wizard 
 will script the schema as well as the data, but SSIS and the 
 bcp
  utility do not. You also learned that if you use SQL 
 Server 2008 R2 or SQL Server 2012, you have the option to script for SQL Azure, which scripts the objects ready for 
 execution in the SQL Azure environment.
 We also discussed the SQL Azure Copy feature, which allows you to make a copy of your SQL Azure database for 
 backup purposes.
 With your database in SQL Azure, we can now focus on how to program applications for SQL Azure, the topic  
 of Chapter 5.",NA
Chapter 5,NA,NA
Programming with SQL Database,"The chapters previous to this one have laid the foundation for the rest of the book. You’ve seen an overview of 
 Windows Azure SQL Database, learned about cloud computing design options, and walked through setting up your 
 Azure account. You’ve read about SQL Database security, including security compliance and encryption topics; 
 and you spent the last chapter learning about data migration (how to get your data into the cloud) and backup 
 strategies—the types of things a DBA likes to hear.
 Starting with this chapter, the rest of the book focuses on developing with SQL Database. This chapter looks at 
 using various Microsoft technologies to program for SQL Database, including ODBC, ADO.NET, LINQ, and others. 
 This chapter also discusses development considerations as you start to design and build new applications to work 
 with SQL Database or consider moving existing applications to SQL Database.
 You should begin getting the picture that SQL Database really isn’t 
 that
  different from your local SQL Server 
 instances. The last chapter talked at length about some of the T-SQL differences in SQL Database but also said that 
 Microsoft is continually adding features and functionality; so, the differences gap is closing at a very rapid pace 
 (which has made writing this book a fun challenge, but that’s another story). The key to developing applications 
 isn’t pointing your application to your cloud version of the database, but rather your approach to developing the 
 application and design considerations for accessing your cloud database. Simply moving a database to the cloud and 
 pointing your application to that instance can have disastrous results, such as degraded application performance 
 and unwanted monetary costs.
 This chapter first focuses on application design approaches to get the most from SQL Database application 
 development. The rest of the chapter discusses various technologies for accessing your SQL Database instance, 
 showing you how to use those technologies to connect to and retrieve data. You learn the right place to use each 
 of the technologies, because there are many technologies to choose from. The end of this chapter provides a 
 simple, best-practice discussion of testing locally before deploying remotely to ensure a successful SQL Database 
 application deployment.
 This chapter will also spend a few pages on application development best practices to help solve many of the 
 issues noted in earlier chapters, such as throttling and latency, all of which affect any type of application whether 
 hosted on-premises or in the cloud.",NA
Application Deployment Factors,"As stated earlier, you could use the information from Chapter 4 to push your 
 entire
  database to SQL Database. But is 
 that the right thing to do?
 Chapter 2 discussed at length the design aspects of architecting and publishing applications for the cloud, 
 including topics such as storage, high availability, security, and performance. Each of those aspects is important 
 (especially performance) and should be discussed when you’re considering moving applications to the cloud. In 
 addition to security, performance is one of the primary items of concern that companies have about cloud computing. 
 One of the last things a company wants to do is decide to take a critical application and move it to the cloud, only to 
 find that it doesn’t perform as well as the on-premises version of the app.",NA
On-Premises Application,"On-premises
  means your application is hosted locally and not in Windows Azure, but your database is in a SQL 
 Database instance. Your application code uses client libraries to access one or more SQL Database instances. Some 
 companies are reluctant to put business logic or application-specific logic outside of their corporate data center, and 
 the on-premises option provides the ability to house the data in the cloud while keeping the application logic local.
 Although this is a viable option, limitations are associated with it. For example, only the following client libraries 
 are supported:
 .NET Framework 3.5 SP1 Data Provider for SQL Server (System.Data.SqlClient) or later
 •�
 Entity Framework 3.5 SP1 or later
 •�
 SQL Server 2008 R2 Native Client ODBC driver
 •�
 SQL Server 2008 Native Client Driver (supported, but with less functionality)
 •�
 SQL Server 2008 Driver for PHP version 1.1 or later
 •�
 If your application uses OLE DB, you have to change it to use one of the client libraries listed here instead.
 The biggest consideration related to keeping your application on-premises is the cost. Any time you move data 
 between SQL Database and your on-premises application, there is an associated cost for inbound data transfers (at 
 the time this book went to press, $0.05 - $0.12 per GB for US and Europe, $0.12 - $0.19 for all other areas). If you’re 
 using Azure Storage, there is also the cost of using that storage (currently, $0.037 – $0.093 per GB). Again, this is per 
 GB, so the cost is low. An example of an expensive pattern is synchronizing large amounts of data multiple times per 
 day. But keep in mind that synching even a 50GB database costs less than a few dollars.
 These costs and limitations shouldn’t deter you from using an on-premises solution for your application. 
 However, let’s look at what an Azure-hosted solution provides.",NA
Azure-Hosted Application,"Azure-hosted
  means that your application code is hosted in Windows Azure and your database is in SQL Database. 
 Your application can still use the same client libraries to access the database or databases in SQL Database. Most 
 companies right now are taking existing ASP.NET applications and publishing them to Windows Azure and accessing 
 SQL Database. However, you aren’t limited to just web apps: you can use a Windows desktop app or Silverlight app 
 that uses the Entity Framework and the WCF (Windows Communication Foundation) Data Services client to access 
 SQL Database as well. Again, you have plenty of options.
 The benefit of using an Azure-hosted solution is the ability to minimize network latency of requests to the SQL 
 Database instance. Just as important is the fact that you’re cutting the costs of data movement between SQL Database 
 and the application. As long as your Windows Azure and SQL Database are in the same subregion, bandwidth usage 
 between SQL Database and Windows Azure is free.",NA
Which to Choose?,"The decision whether to move your application to the cloud or keep it local is entirely up to you, and it shouldn’t be 
 determined solely by what you’ve just read in the last two sections. The decision isn’t that cut-and-dried. You need to 
 look at several other factors, such as costs, data traffic, and bandwidth, and then base your decision on the analysis 
 of this information. For example, it may not be a sound decision for a small company with little web traffic to host 
 an application in Azure, because of the compute costs. However, that same company can keep its database in Azure 
 while keeping the application on-premises because the data-transfer costs are minimal, and still gain the benefits of 
 SQL Database (failover, high availability, and so on).
 In many companies, the initial goal isn’t an all-or-nothing approach. The companies spend some time looking 
 at their databases and applications, decide what functionality makes sense to put in the cloud, and test functionality 
 on that. They test for performance foremost, to ensure that when the app is deployed to Winodws Azure in 
 production, performance is in the same ballpark as their on-premises solution. The thought is to keep the important 
 things up front to ensure a successful Azure deployment. Roll your application out in pieces, if necessary, and test 
 locally prior to deployment.
 Whether you deploy all or part of your database and application is for you to decide. Chapter 2 discussed the 
 issue at length, and this chapter doesn’t rehash it except to say that before you make a decision, you should look at all 
 the facts.",NA
Connecting to SQL Database,"Developing applications that work with SQL Database isn’t rocket science, but it requires knowing what to expect and 
 what functionality you have to work with. You read earlier that not all client libraries work with SQL Database and 
 saw the libraries that are supported. Appendix B will spend some time discussing what T-SQL functionality exists in 
 SQL Database. Even as this book is being written, the list of supported features changes as Microsoft continues to add 
 functionality. This chapter focuses on a client application’s perspective.
 This section looks at using several technologies to connect to and query a SQL Database instance, including 
 ADO.NET, ODBC, and WCF Data Services. You read at length earlier about taking the right approach to move to the 
 Azure platform. You must consider many things, including the following:
 SQL Database is only available via TCP port 1433.
 •�
 SQL Database doesn’t currently support OLE DB.
 •�
 SQL Database only supports SQL Server authentication. Windows Authentication isn’t 
 •�
 supported.
 When connecting to SQL Database, you must specify the target database in the connection 
 •�
 string. Otherwise, you’re connecting to the 
 master
  database.
 Distributed transactions (transactions that affect multiple resources, such as tables, or 
 •�
 different databases via sharding) aren’t supported in SQL Database.
 You must ensure that your SQL Database firewall is configured to accept connections.
 •�",NA
ADO.NET,"Microsoft makes it very easy to connect an application to a SQL Database instance, by providing the necessary 
 connection strings for a number of client libraries such as ADO.NET, Java and PHP, as shown in Figure 
 5-1
 . You can 
 find the connection information in the Windows Azure Management Portal by selecting the SQL Databases option in 
 the Navigation pane, selecting the appropriate database, and then clicking the Show Connection Strings button on the 
 Dashboard page.
 Copy the ADO.NET connection to the clipboard. You’ll be using this shortly.",NA
Making the Connection,"Let’s first look at how to connect to a SQL Database instance using ADO.NET. Fire up an instance of Visual Studio 
 2010, and create a new C# Windows Forms application. Then, follow these steps:
 1. 
 Place a button on Form1, and double-click the new button to view its Click event.
 2. 
 Before you place any code in the click event you’ll need to declaration to use the 
 appropriate SQLConnection class:
 using System.Data.SqlClient;
 Figure 5-1.
  Connection strings",NA
Using a Data Reader,"As you become more and more familiar with SQL Database, you’ll find that you don’t need to make a lot of changes to 
 your application code except possibly any inline T-SQL. The beauty of all this is that you’re using a proven and trusted 
 data-access technology, ADO.NET. Thus, nothing really changes. Let’s modify the application and click-event code to 
 illustrate this. Follow these steps:
 1. 
 Add a new list box to the form.
 2. 
 In the click event, add the code in bold in the following snippet. This new code uses the 
 SqlDataReader
  class to execute a simple 
 SELECT
  command against the SQL Database 
 instance and then iterate over the 
 SqlDataReader
  to populate the list box:
 private void button1_Click(object sender, EventArgs e)
 {
     string connStr = GetConString();
     using (SqlConnection conn = new SqlConnection(connStr))
     {
         
 SqlCommand cmd = new SqlCommand(""SELECT FirstName, LastName  FROM Person.Person"", 
 conn);
         conn.Open();
         
 SqlDataReader rdr = cmd.ExecuteReader();
         try
         {
             
 while (rdr.Read())
             
 {
                 
 listBox1.Items.Add(rdr[0].ToString());
             
 }
             
 rdr.Close();
         }
         catch (SqlException ex)
         {
             MessageBox.Show(ex.Message.ToString());
         }
     }
 }
 3. 
 Run the application, and click the button on the form. Within a few seconds, the list box 
 populates with names from the Users table.",NA
Using a Dataset,"In the last example, you found that there is no difference in syntax when using a 
 SqlDataReader
  to query a SQL 
 Database instance. This example uses the 
 SqlCommand
  class and the 
 SqlDataAdapter
  to query SQL Database and 
 populate a dataset. Here are the steps:
 1. 
 In the button’s click event, replace the existing code with the following:
 using (SqlConnection conn = new SqlConnection(connStr))
 {
     try
     {
         using (SqlCommand cmd = new SqlCommand())
         {
             conn.Open();
             SqlDataAdapter da = new SqlDataAdapter();
             cmd.CommandText = ""SELECT FirstName, LastName  FROM Person.Person"";
             cmd.Connection = conn;
             cmd.CommandType = CommandType.Text;
             da.SelectCommand = cmd;
             DataSet ds = new DataSet(""Person"");
             da.Fill(ds);
             listBox1.DataSource = ds.Tables[0];
             listBox1.DisplayMember = ""FirstName"";
         }
     }
     catch (SqlException ex)
     {
         MessageBox.Show(ex.Message.ToString());
     }
 }
 This code creates a new connection, using the same connection information as the previous example, and then 
 creates a new 
 SqlCommand
   instance. The connection, text, and type of the 
 SqlCommand
  are set and then executed using 
 the instantiated 
 SqlDataAdapter
 . A new dataset is created and filled from the 
 SqlDataAdapter
 , which is then applied 
 to the datasource property of the list box.
 2. 
 Run the application, and click the button on the form. Again, the list box is populated with 
 the names from the Users table in the SQL Database instance. Again, you could change the 
 connection string to point to your local database and the code would work fine.
 So, when would code like this 
 not
  work? Suppose your application had code such as the following, which creates a 
 table without a clustered index:
 using (SqlConnection conn = new SqlConnection(connStr))
 {
     try
     {
         using (SqlCommand cmd = new SqlCommand())",NA
ODBC,"There is nothing earth-shattering or truly groundbreaking here, but let’s walk though an example to see how ODBC 
 connections work and illustrate that your ODBC classes still work as you’re used to. Follow these steps:
 1.
 Do this the proper way and create an enumeration to handle the type of connection you’re 
 using.
 2.
 Modify the 
 GetConString
  method as shown in the following snippet to take a parameter. 
 The parameter lets you specify the connection type so you can return the correct type 
 of connection string (either ADO.NET or ODBC). Be sure to use your correct password, 
 username, and server name with the correct server. If the value of 
 ADO_NET
  is passed into 
 this method, the ADO.NET connection string is returned; otherwise the ODBC connection 
 string is returned:
 enum ConnType
 {
     ADO_NET = 1,
     ODBC = 2
 }
 string GetConString(ConnType connType)
 {
     if (connType == ConnType.ADO_NET)
         return ""Server=tcp:
 servername
 .database.windows.net;Database=AdventureWorks2012;
             User ID=
 username
 @
 servername
 ;Password=
 password
 ;",NA
sqlcmd,"If you’ve worked with SQL Server for any length of time, chances are you’ve worked with the sqlcmd utility. This utility 
 lets you enter and execute T-SQL statements and other objects via a command prompt. You can also use the sqlcmd 
 utility via the Query Editor in sqlcmd mode, in a Windows script file, or via a SQL Server Agent job. 
 This section discusses how to use the sqlcmd utility to connect to a SQL Database instance and execute queries 
 against that database. This section assumes that you have some familiarity with sqlcmd. This utility has many options, 
 or parameters, but this section only discusses those necessary to connect to SQL Database.",NA
WCF Data Services,"WCF Data Services, formerly known as ADO.NET Data Services, is the tool that enables the creation and consumption 
 of OData services. OData, the Open Data Protocol, is a new data-sharing standard that allows for greater sharing of 
 data between different systems. Before it was called WCF Data Services, ADO.NET Data Services was one of the very 
 first Microsoft technologies to support OData with Visual Studio 2008 SP1. Microsoft has broadened its support of 
 OData in products such as SQL Server 2008 R2, Windows Azure Storage, and others. This section discusses how to use 
 WCF Data Services to connect to and query your SQL Database instance.",NA
Creating a Data Service,"First you need to create a data service. Follow these steps:
 1. 
 Fire up Visual Studio (to be safe, run Visual Studio as Administrator) and create a new C# 
 ASP.NET web application (in the New Project dialog, select the Web option from the list of 
 installed templates), and call it WCFDataServiceWebApp. (You can host data services in a 
 number of different environments, but this example uses a web app.)
 2. 
 The next step in creating a data service on top of a relational database is to define a model 
 used to drive your data service tier. The best way to do that is to use the ADO.NET Entity 
 Framework, which allows you to expose your entity model as a data service. And to do 
 that, you need to add a new item to your web project. Right-click the web project and 
 select New Item. In the Add New Item dialog, select Data from the Categories list, and then 
 select ADO.NET Entity Data Model from the Templates list. Give the model the name 
 AW.
 edmx
 , and click OK.
 3. 
 In the first step of the Data Model Wizard, select the Generate from Database option, and 
 click Next. 
 4. 
 The next step is Choose Your Data Connection. Click the New Connection button, and 
 create a connection to the AdventureWorks2012 SQL Database instance. Click OK in the 
 connection dialog.
 5. 
 Back in the Entity Data Model Wizard, select the “Yes, include the sensitive 
 data in the connection string” option. Save the entity connection settings as 
 AdventureWorks2012Entities
 , and then click Next.
 6. 
 The next step of the wizard is the Choose Your Database Objects page. Select the Person.
 Person table. Note the option to pluralize or singularize generated objects names. If you 
 leave this option checked, it comes into play later. Leave it checked, and click Finish.
 7. 
 The Entity Framework looks at all the tables you selected and creates a conceptual model on 
 top of the storage schema that you can soon expose as a data service. In Visual Studio, you 
 should see the Entity Framework Model Designer with a graphical representation of the table 
 you selected, called 
 entities
 . Close the Model Designer—you don’t need it for this example.",NA
Connecting the Service to the Model,"Now you need to wire up your data service to your data model so that the service knows where to get its data. You 
 know where to do this, because as you can see in the code it tells you where to enter that information. Thus, change 
 the line:
 public class AWDataService : DataService< /* TODO: put your data source class name here */ >
 To:
 public class AWDataService : DataService< AdventureWorks2012Entities >
 Wiring up your data service to the model is as simple as that. Believe it or not, you’re ready to test your service. 
 However, let’s finish what you need to do on this page. By default, the WCF Data Service is secured. The WCF Data 
 Service needs to be told explicitly which data you want to see. The instructions in the code tell you this, as you can see 
 in code in the 
 InitializeService
  method. Some examples are even provided in the comments to help you out.",NA
Creating the Client Application,"The next step is to add the client application. In Solution Explorer right click the solution and select Add New Project. 
 In the Add New Project dialog, select the Cloud project type and then select Windows Azure Project, providing a name 
 of AWSite, as shown in Figure 
 5-12
 .
 Figure 5-12.
  Adding an Azure Cloud Service
 Click OK in the Add New Project dialog.
 Next, in the New Cloud Service Project dialog, select ASP.NET Web Role to add it to the Cloud Service Solution 
 pane, leaving the default name of WebRole1, and click OK.
 Next, right-click the Web Role project in Solution Explorer and select Add Service Reference from the context 
 menu. This will bring up the Add Service Reference dialog shown in Figure 
 5-13
 .",NA
Best Practices,"Let’s spend a few minutes talking about some things you should consider when developing applications for the 
 cloud. We’ve spent a large portion of this chapter discussing how to connect to SQL Database, but even before 
 you start coding, the very first thing you should consider is your connection. First and foremost, secure your 
 connection string from injection attacks and man-in-the-middle attacks. The .NET Framework provides a simple 
 class in which to create and manage the contents of connection strings used by the 
 SqlConnection
  class. This is the 
 SqlConnectionStringBuilder
  class.
 The following example illustrates how to use this class. I first define four static variables to hold the username, 
 password, database name and server:
 private static string userName = ""SQLScott@
 server
 "";
 private static string userPassword = 
 password
 ;
 private static string dataSource = ""tcp:
 server
 .database.windows.net"";
 private static string dbName = ""AdventureWorks2012"";
 Figure 5-14.
  Projects in Solution Explorer",NA
Transient Fault Handling Application Block,"The Transient Fault Handling Application Block provides a set of reusable components for adding retry logic into your 
 applications. These components can be used with the many of the Windows Azure services such as SQL Database, 
 Storage, Service Bus, and Caching. These components provide the resiliency applications needed in applications by 
 adding robust transient fault handling logic into applications.
 Transient faults are errors that occur due to some type of temporary condition such as network connectivity 
 issues or unreliable service. In most cases, simply waiting for a brief period of time and then retrying the operation 
 results in a successful operation.
 This is where the Transient Fault Handling Application Block comes in. The Transient Fault Handling Application 
 Block (we’ll call it TFHAB from here to save space. . . and typing) is part of Microsoft’s Enterprise Library. It is 
 the result of some great collaboration between the Microsoft Patterns & Practices group and the Windows Azure 
 Customer Advisory team.
 The TFHAB works by using detection strategies to identify all known transient error conditions, making it easy for 
 developers to define retry policies based on the built-in retry strategies.",NA
Adding the Transient Fault Handling Application Block to Your Project,"The TFHAB is a NuGet package that is added through the Package Manager Console in Visual Studio. However, before you 
 can use the Package Manager Console, you need to install NuGet. Open a browser and navigate to the NuGet web site at 
 http://nuget.org/
 . On the main home page, click the Install NuGet button. NuGet supports Visual Studio 2010 and 2012.
 Once NuGet is installed, open Visual Studio and create a new WinForms application. Once the project is created, 
 from the Tools menu in Visual Studio, select Library Package Manager 
 ➤
  Package Manager Console.
 In the console window type the following command, as shown in Figure 
 5-15
 , to install the Transient Fault 
 Handling Application Block:
 Install-Package EnterpriseLibrary.WindowsAzure.TransientFaultHandling
 Figure 5-15.
  Adding the TFHAB in Package Manager Console
 Press Enter once you have typed in in the 
 Install
  command. The appropriate assemblies and references will be 
 added to your project. Once the install is done, expand the References node in Solution Explorer and you will see over 
 six new Microsft.Practices references added to your project.",NA
Using the Transient Fault Handling Application Block,"With the TFHAB installed, the first thing that needs to be done is to add a couple of directives to the code:
 using Microsoft.Practices.EnterpriseLibrary.WindowsAzure.TransientFaultHandling.SqlAzure
 using Microsoft.TransientFaultHandling",NA
Summary,"We began the chapter with a discussion of the different factors for deploying your application, such as keeping your 
 application on-premises or hosting your application in Azure. We also covered application deployment from the 
 database side, providing some ideas and concepts to consider when moving your database to the cloud, such as how 
 much of your data to move.
 We then discussed the different programming approaches for connecting to and querying a SQL Database 
 instance, providing examples for each method including ADO.NET and ODBC.
 Next, we discussed accessing your SQL Database instance through WCF Data Services. With today’s strong 
 emphasis on SOA architecture coming not only from Microsoft, the discussion of WCF Data Services offered a solid 
 foundation for providing a services layer for your SQL Database instance.
 Lastly, we discussed some crucial best practices, such as implementing Reply Policies in your application 
 that can drastically improve the end-user experience and add the resiliency needed in cloud-based applications. 
 Along with retry, we also discussed other best practices including connection pooling and using the 
 SqlConnectionStringBuilder
  class. An important component to all application development, regardless of whether 
 the application is cloud-based or not, is the implementation of fault handling and retry logic. This chapter spent 
 a few pages on retry login because of how critical it is in cloud-based applications. All of these best practices, if 
 implemented, will help ensure a well-operating application.",NA
Chapter 6,NA,NA
SQL Reporting,"A lot has changed in the two years since the first release of this book. What at one time was in CTP (Community 
 Technology Preview) and still being fleshed out has seen the light of day. Not long after we released the first version of 
 this book, SQL Reporting, formally known as SQL Azure Reporting Services, hit the market in Beta form.
 By the time this book hits the shelves, Windows Azure SQL Reporting will have been officially released. This 
 chapter will first provide an overview of SQL Reporting by looking at its architecture and how fits into the Windows 
 Azure SQL Database picture to support a wide variety of reporting needs. We will then spend the rest of the chapter 
 looking at how to work with SQL Reporting, including how to deploy reports and access those reports from your 
 application. This chapter will also look at managing SQL Reporting security via different roles, as well as discuss some 
 best practices for securing SQL Reporting.
 We’ll also spend some time in this chapter looking at the differences between on-premises SQL Server Reporting 
 Services and SQL Reporting, including the security model as well as the different data sources for reports. This 
 chapter assumes that you’re familiar with SQL Server Reporting Services and how to create and work with reports and 
 subreports. Plenty of great books on SSRS are available if you need an introduction.
 Let’s get started.",NA
SQL Reporting Overview,"When Microsoft set out to create a reporting service for the SQL Database, their goal was to provide many of the great 
 reporting capabilities that currently exist in SQL Server Reporting Services and to ensure that the same cloud benefits 
 that all of the Azure services benefit from (such as elasticity and high availability) were extended to SQL Reporting, 
 without making developers learn a whole reporting structure. With SQL Reporting, they succeeded. SQL Reporting 
 is a cloud-based reporting platform based on the tried-and-true SQL Server Reporting Services. Simply stated, SQL 
 Reporting is SQL Server Reporting Server running as a highly available cloud service.
 This is good news because it means several things. First, you don’t need to install your own Reporting Services 
 instance, and you don’t need to maintain those instances (such as applying updates). Second, as part of the SQL 
 Database platform, SQL Reporting can take advantage of the benefits of the Azure platform, providing high availability 
 and scalability of your reporting services. Third, you can build reports using the same familiar tools that you currently 
 use for building on-premises reports. Lastly, developers can deliver reports as an integrated part of a Windows Azure-
 based solution.
 Think about this for a minute. In an on-premises environment, how long does it typically take to provision a 
 new SQL Server Reporting Services server? Even if you have an extra server lying around, to get that server added to 
 your SQL Server environment would typically take several hours. What if you could provision a new report server in a 
 matter of minutes? Now you can do that with SQL Reporting, while enjoying the benefits of the Azure platform (letting 
 Microsoft manage the physical administration).
 With SQL Reporting you can quickly and easily provision and deploy reporting solutions while taking advantage 
 of the enterprise-class availability, scalability, and security of the Windows Azure platform.
 So, with that introduction it will be helpful to look at the architecture of SQL Reporting to help you get a feel for 
 how reporting works in SQL Database.",NA
Architecture,"The architecture of SQL Reporting Services has load balancing and high availability automatically built in. Figure 
 6-1
  
 illustrates the different components of the SQL reporting architecture which we’ll discuss.
 Figure 6-1.
  SQL Reporting Services Architecture
 Each SQL Reporting report is accessed via the service endpoint and referencing a specific report. Notice that this 
 isn’t any different than accessing your on-premises report server, except that in this case you are accessing a service 
 endpoint instead of a physical machine.
 The request is routed through the load balancer, which routes the request through two more application tiers in 
 the architecture:
 Reporting Service Gateway
 •�
 Reporting Service Nodes
 •�",NA
Gateway,"The Reporting Services Gateway handles all of the intelligent metadata routing. For example, the Gateway implements 
 Smart Routing, which means that as requests for a report come in, each request is processed and sent to the best 
 available report server to handle that request. This allows for increased security and control over the availability of 
 the system. For example, if a report (request) takes down a specific node, the Gateway can intelligently stop routing 
 requests to that specific Reporting Services node.",NA
Nodes,"The nodes are the actual reporting servers. They are described as multi-tenant, meaning that each node has its own 
 catalog and tempDBs (from SQL Database). The nodes are built on top of SQL Database; thus the data tier that is used 
 for SQL Reporting is SQL Database.",NA
Feature Comparison,"Before we get to the “hands-on” portion of the chapter it will be very beneficial to highlight some of the similarities 
 and differences between SQL Server Reporting Services and SQL Reporting (see Table 
 6-1
 ).
 Table 6-1.
  SQL Server Reporting Services and SQL Reporting Feature Comparison
 Category
 SQL Server Reporting Services
 SQL Reporting
 Tooling
 Business Intelligence  
 Development Studio (BIDS)
 Business Intelligence Development Studio (BIDS)
 Data Sources
 Built-in or customizable data sources
 SQL Database Instance
 Report Management
 Report Manager for native mode,  
 SharePoint application pages.
 Can display and render reports  
 to different formats.
 Can create subscriptions and schedule 
 deliveries.
 Reports can be viewed in browsers,  
 Report Viewer via Windows Forms,  
 and ASP.NET and SharePoint.
 Windows Azure Management Portal
 Reports can be viewed in browsers, Report Viewer 
 via Windows Forms and ASP.NET.
 Extensibility
 Custom extensions for data,  
 processing, rendering, delivery,  
 and security
 No extensions are supported in this release
 Security Model
 Windows authentication and other 
 supported authentications
 SQL Database username and password 
 authentication
 Both SQL Server Reporting Services and SQL Reporting share the ability to apply permissions to reports and 
 report-related items via role assignments.
 The list in Table 
 6-1
  is a brief feature comparison; in addition, the following SQL Server Reporting Services 
 features are not currently supported in SQL Reporting:
 Creating subscriptions or schedule report snapshots.
 •�
 Creating SMDL report models.
 •�
 Creating reports from Report Builder version 1.0, 2.0, or 3.0. However, you can create reports 
 •�
 using version 3.0 and then deploy the reports by addingz them to BIDS.
 SharePoint integrated mode is not supported. Native mode is supported.
 •�
 It should be noted that Report Manager is not available, but the SQL Database Reporting portal provides similar 
 features.
 It is time to get “hands-on,” so the rest of the chapter discusses how to create a SQL Reporting Server and deploy 
 a report to that server. This chapter uses the full AdventureWorks database for Windows Azure SQL Database. This 
 database, and instructions for installing it, can be found here:
 http://msftdbprodsamples.codeplex.com/releases/view/3704",NA
Provisioning Your SQL Reporting Server,"Creating your SQL Reporting Server is a simple and quick process. It should be noted that up until now you have spent 
 the majority of the time in the new Windows Azure Management Portal. As of this writing, both SQL Reporting and SQL 
 Data Sync have not been ported over to use the new portal. In the short term, the current Silverlight-based portal will 
 continue to function as it always has so that SQL Reporting and SQL Data Sync remain accessible until they are ported to 
 the new portal.
 1.
 Open your web browser, navigate to 
 https://windows.azure.com/
 , and log in with your 
 LiveID.
 2.
 Once you are in the portal, click the Reporting option in the Navigation pane. If you have 
 previously created any SQL Reporting servers, they will be listed per subscription at the 
 top of the Navigation pane. Figure 
 6-2 
 shows the Navigation pane with the Reporting 
 option selected but no servers yet created.
 Figure 6-2.
  Windows Azure portal navigation pane",NA
Creating a Report c,"The process of creating reports that access data from SQL Database is the same as for reports that access local data, 
 with one slight difference: the connection to the data. The example in this chapter for creating a report will use SQL 
 Server Data Tools, which is installed with SQL Server 2012. SQL Server Data Tools (SSDT) is the new BIDS (Business 
 Intelligence Development Studio) for SQL Server 2012. To create a report, follow these steps:
 1. 
 From the Start menu, select All Programs 
 ➤
  Microsoft SQL Server 2012 
 ➤
  SQL Server 
 Data Tools. When SSDT opens, create a new Report Server Project. From the New Project 
 dialog select the Reporting Services template and then select the Report Server Project. In 
 Figure 
 6-5
  the project is called AzureReport, but feel free to change the name. Click OK.
 Figure 6-5.
  New Report Server Project
 2. 
 In Solution Explorer, right-click the solution and select Add 
 ➤
  New Item from the  
 context menu.
 3. 
 In the Add New Item dialog, select the Report template, and click Add,  
 as shown in Figure 
 6-6
 .",NA
Creating the SQL Database Data Source,"Continue with these steps:
 1. 
 In the Report Data window, right-click the Data Sources node, and select Add Data Source 
 from the context menu, as shown in Figure 
 6-7
 .
 Figure 6-7.
  Adding a data source
 Figure 6-6.
  Adding a report",NA
Creating the Report Design,"With your report in Design view, you can now start laying it out. In this example you don’t do anything flashy or 
 extensive, just something simple to demonstrate your connectivity to SQL Database. Follow these steps:
 1. 
 From the Toolbox, drag a text box and table onto the Report Designer window. Move the 
 text box to the top of the report: it’s the report title. Change the text in the text box to  
 My First SQL Azure Report
 .
 2. 
 The table you placed on the report has three columns, but you need five. Right-click any of 
 the existing columns, and select Insert Column 
 ➤
  Right from the context menu to add an 
 additional column. Add one more column for a total of five.
 3. 
 From the Report Data window, drag the First Name, Last Name, Address Line 1, City, and 
 State columns from the dataset to the columns in the table, as shown in Figure 
 6-12
 .
 Figure 6-12.
  Report Design view
 4. 
 Your simple report is finished—it isn’t complex or pretty, but it’s functional. You’re now 
 ready to test the report: to do that, select the Preview tab. You’ll see the result shown in 
 Figure 
 6-13
 .",NA
Deploying the Report z,"To deploy a report, follow these steps:
 1. 
 Back in the Windows Azure Management Portal, select the Reporting option in the 
 Navigation pane, and then select the report server created earlier.
 2. 
 Highlight and copy the Web service URL from the Server Information section of the portal.
 3. 
 Back in SSDT, right-click the report solution, and select Properties from the context menu.
 4. 
 In the Property Pages dialog, the only thing you need to enter is the TargetServerURL, 
 shown in Figure 
 6-14
 . Notice also the name of the TargetReportFolder, which in this case is 
 AzureReport
 —the name of your Visual Studio solution.",NA
Security,"This section contains some guidelines and best practices for managing the security of your SQL Reporting server  
 and reports.
 When embedding reports into your application, secure your connection string to the  
 •�
 report server.
 Because reports can only use SQL Database instance as a data source, the recommended way 
 •�
 to provide access to shared data sources is to create and deploy shared data sources for reports 
 to a folder on the report server. Store the credentials for each shared data source on the  
 report server.
 By default, report items inherit security based on report folder permissions. Thus, the folder 
 •�
 to which you deploy your report items matters greatly. By assigning specific permissions to a 
 folder, you break the inheritance for permissions in the folder hierarchy.
 SQL Reporting requires not only a username and password to access reports, but a URL as well. These three 
 pieces of information are the URL, username, and password to the report server account. However, you can create 
 users in the Management Portal and assign them to roles that grant permissions. You can then provide the report 
 readers with those usernames and passwords.
 When creating new users, you assign them to Item roles, and optionally, System roles.",NA
Roles,"As part of creating new users, you also assign them to an Item role and, optionally, a System role. The Item roles and 
 System roles are similar to those available on a native mode report server:
 Item roles: Browser, Content Manager, My Reports, Publisher, and Report Builder.
 •�
 System roles: System Admin and System User.
 •�
 Roles are a concept that helps organize  in terms of functions or goals. Roles represent one or more people who 
 perform one or more functions or tasks that require specific permissions. For SQL Reporting, the preceding roles help 
 determine which users can perform specific functions such as viewing or editing reports, publishing reports, creating 
 and managing additional users, and managing database objects.
 For example, the Browser role can run reports, subscribe to reports, and navigate through the folder structure. 
 The My Reports role can manage a personal workspace for storing and using reports and other items.
 It should be noted that these roles are not specific to Windows Azure SQL Database, but to the underlying SQL 
 Server Reporting service. More information about these roles can be found here:
 http://msdn.microsoft.com/library/ms157363.aspx(v=SQL.105
 )",NA
Using the Management Portal,"Except for provisioning the Report Server and managing users, the majority of this chapter has used tools such as SQL 
 Server Data Tools to create and deploy reports. However, a lot of what you did in this chapter can also be done through 
 the Management Portal, which provides additional functionality. Figure 
 6-18
  shows the ribbon in the Management 
 Portal when a SQL Reporting server is selected. Notice that you can create a shared data source, create and set 
 permissions on a folder, upload a report, and look at server usage statistics. Server statistics provides the ability to look 
 at the overall usage of SQL Reporting over a given time span. By default it will show you the usage over a single day, 
 but you can also view usage data over a 2-day period, 1-week period, 1-month period, and 3-month period.
 Figure 6-17.
  The Create User dialog
 Figure 6-18.
  The Management Portal ribbon",NA
Pricing,"Starting August 1, 2012, users of SQL Reporting are billed for using it. SQL Reporting use is charged for each clock 
 hour of a reporting instance (reporting server), a minimum of one reporting instance for each clock hour that a SQL 
 Reporting server is provisioned, even if no reports are deployed.
 The price per reporting instance is $0.88 per hour per reporting instance. This charge covers up to 200 reports 
 each clock hour. For each clock hour during which more than 200 reports are generated, another Reporting Instance 
 Hour at $0.88 will be billed (for each additional block of 200 reports).",NA
Summary,"In this chapter you first learned what Windows Azure SQL Reporting is and the many benefits this cloud-based 
 reporting solution provides. This chapter then discussed the architecture of SQL Reporting and how this architecture 
 provides the high availability and scalability needed in a cloud-based reporting solution.
 It’s also important to understand the functional differences between on-premises SQL Server Reporting 
 Services and SQL Reporting, so we spent some time discussing these differences and how they might impact report 
 development.
 Lastly, this chapter discussed provisioning a new SQL Reporting server, creating and deploying a report, and 
 the security around hosting reports in SQL Reporting. Much of the functionality is shared between on-premises 
 development tools and the Management Portal when developing reports, such as defining folders and data sources 
 and applying permissions. This provides great flexibility in report development.",NA
Chapter 7,NA,NA
SQL Data Sync,"When I first wrote this chapter for the first edition of this book, SQL Data Sync was part of SQL Azure Labs and called 
 SQL Azure Data Sync Services. Not too much later it was moved out of the SQL Azure Labs and has been in CTP 
 (Community Technology Preview) and available in the Windows Azure Management Portal. Since that time, SQL Data 
 Sync Services, or DSS, has been gaining tremendous momentum and acceptance and by the time you read this, will 
 be out of CTP and in full general availability.
 A bit of background is due. In November 2009, at the Microsoft Professional Developers Conference (PDC) in 
 Los Angeles, Microsoft announced Project Huron, which allows database synchronization capabilities in the cloud. 
 If you’ve been following the hype and keeping up with the blog posts regarding Huron, you know that Microsoft has 
 been billing it and associated database sync functionality as “friction free,” meaning easy to set up and maintain. 
 Microsoft’s goal with Huron was to eliminate many of the emblematic complexities and idiosyncrasies that are 
 associated with data sharing between databases, such as scalability and configuration. Along with these goals, 
 Microsoft also wanted to include user-friendly tools that let administrators easily configure and synchronize  
 their data.
 In June 2010, at the start of Tech-Ed in New Orleans, Microsoft announced the public preview availability of the 
 Data Sync Service for SQL Azure, part of the Huron project. This is Microsoft’s solution to allow users to easily and 
 efficiently share data between databases without regard to database locations or connectivity. Sharing data is only the 
 beginning: Microsoft also has visions of including data collaboration, providing users and developers the ability to use 
 and work on data regardless of the data’s location.
 In May 2012, Microsoft formally implemented a new approach to the naming and branding of the services in 
 Windows Azure, including SQL Azure Data Sync Services. As such, SQL Azure Data Sync Services is now called SQL 
 Data Sync.
 This chapter focuses entirely on the capabilities and features of SQL Data Sync. It begins with a brief overview 
 and then shows you how to get started by setting up and configuring the Data Sync service. You will then work 
 through several examples of using SQL Data Sync in different scenarios and situations. You also see some patterns 
 and best practices along the way to help ensure a solid understanding of the Data Sync Service.",NA
Understanding SQL Data Sync,"SQL Data Sync provides bidirectional synchronization between two or more databases. On the surface, it’s as simple 
 as that; but even behind the scenes, it doesn’t get much more complicated. With zero lines of code, you can quickly 
 and easily configure your SQL Database to be synchronized with other SQL Database instances in any of the Microsoft 
 Azure data centers or even with on-premises SQL Server.",NA
Why the Need?,"Why is the ability to sync data between SQL Database instances important? That’s a fair question to ask. Let’s explore 
 a couple of answers.",NA
The Basic Scenario,"Every SQL Database synchronization environment includes a single database 
 hub
  and has one or more database 
 members
 , or 
 spokes
 , as shown in Figure 
 7-1
 . The hub is the central database in a synchronization group. A spoke is 
 a member database which synchronizes with the hub. Setting up synchronization includes creating and defining a 
 synchronization group in which you specify the hub database and then assigning the database members to that hub.
 Figure 7-1.
  Data Sync scenarios
 Let’s talk about the initial synchronization for a minute, because it’s helpful to understand the process and 
 changes that take place. When the initial sync takes place, it’s a two-step process:
 1. 
 The hub database schema is copied to the member database(s).
 2. 
 The data is copied from the hub database to the member database(s).
 This may seem simple, but let’s discuss what happens during these two steps. First, you don’t have to generate 
 the target (member) database schemas yourself. During the first step, the Data Sync service does that for you (for the 
 tables you specify to sync). As part of this process, foreign key constraints are 
 not
  copied. This is because a full schema 
 synchronization capability hasn’t been built into the Data Sync service as of yet.",NA
Common Data Sync Scenarios,"A common requirement in many businesses is the need to distribute data among different locations. These locations 
 could be located geographically close or spread out globally in different parts of the world. Regardless of where offices 
 are located, the need to share, consolidate, or migrate data between locations is a necessary commonplace.
 As the move toward cloud computing becomes more mainstream, the ability to move, distribute, and synchronize 
 that data becomes more critical. In cloud computing, data synchronization typically falls into one of four categories:
 Bursting data from on-premises data into the cloud
 •�
 Aggregating data from multiple locations into a single, central location
 •�
 Backing data up from the cloud to on-premises
 •�
 Data geo-distribution
 •�
 The first two scenarios focus on situations where on-premises applications can leverage and utilize the cloud to 
 easily expand their on-premises infrastructure and take advantage of the cloud environment. For example, a company 
 may have the need of additional computational power but doesn’t have the on-premises resources. By bursting the 
 data into the cloud it can easily take advantage of the readily available cloud resources.
 Data aggregation is a common scenario among many businesses, in which data from two or more satellite 
 locations or partner businesses need to be pulled into a single, central location. In scenarios like this today you will 
 typically see solutions involving BizTalk or SQL Server Integration Services.
 The last two scenarios focus on cloud-based application requirements. Common situations include the need to 
 provide a “backup” of cloud data to an on-premises location. Yet more commonly seen is the need to distribute data 
 geographically, moving data closer to the users who need access to it.
 Regardless of the scenario, however, Windows Azure SQL Data Sync provides the data synchronization and data 
 migration solutions needed by businesses. The ease and speed of configuration, scalability and availability of the 
 service, and powerful features make SQL Data Sync a viable solution for all four of these common scenarios.",NA
Architecture,"It will be helpful to look at the architecture of SQL Data Sync to get a feel for how reporting works in SQL Database. 
 Figure 
 7-2
  shows the basic architecture of SQL Data Sync.",NA
Configuring Synchronization,"Now that you’re familiar with the foundation of how the Data Sync service works, let’s dive in and configure a new 
 sync for SQL Data Sync, in order to sync two databases. This example synchronizes the same AdventureWorks 
 database for Windows Azure SQL Database used in Chapter 6. You can find this database and instructions for 
 installing it here:
 http://msftdbprodsamples.codeplex.com/releases/view/3704
 This database will be the hub database. Additionally, this example will utilize two member databases; one 
 located on-premises and another SQL Database. My second SQL Database will be located in a second region to 
 illustrate regional synchronization, but if you are walking through this example feel free to put the second SQL 
 Database in the same region.",NA
Provision a SQL Data Sync Server,"Log in to the portal with your LiveID at 
 https://windows.azure.com
 . The first task is to provision a Data Sync server. 
 The Navigation pane is broken into two sections. The lower section contains the components and services available in 
 the Management Portal. The top section shows either a list of available subscriptions linked to your LiveID, or helpful 
 links to get started as shown in Figure 
 7-3
 .",NA
Creating a Sync Group,"The next step is to create and configure a 
 sync group
 . A sync group is a collection of SQL Database instances and SQL 
 Server databases that are configured for mutual synchronization. A sync group contains a single 
 hub
  database and 
 one or more 
 member
  databases. The hub database must be a SQL Database instance.
 In the top section of the navigation pane, expand the node that lists your new Data Sync server. Underneath that 
 node you will see two subnodes, labeled Sync Groups and Member Databases. Select the Sync Groups node, and as 
 shown in Figure 
 7-4
  there are several options for creating a sync group.
 Figure 7-4.
  Creating a sync group
 If you are new to SQL Data Sync, the two options under Getting Started with SQL Data Sync provide a step-by-step 
 walkthrough of creating different sync groups types. The first option creates an on-premises–to–SQL Database sync 
 group. The second option creates a sync group between multiple SQL Database instances.
 Notice, however, that there is a third option. In the Sync Group section of the ribbon is an enabled button called 
 Create. The example in this chapter will use that button. Go ahead and click that button. Instead of providing a 
 step-by-step walkthrough, you are presented with the entire template to configure, as shown in Figure 
 7-5
 .",NA
Defining the Hub and Member Databases,"The first step is to define the hub database. To do so, simply click the center icon labeled “Click to add a Windows 
 Azure SQL Database as the Sync Hub.” Yeah, pretty obvious. Clicking that icon brings up the Add Database to Sync 
 Group dialog shown in Figure 
 7-6
 .
 Figure 7-5.
  Creating a Sync Group",NA
Selecting Tables to be Synchronized,"Significant enhancements have been made to SQL Data Sync since the last edition of this book, including how 
 the sync dataset is defined. In the Configuration section of the Sync Group window, click the Edit Dataset button, 
 which will display the Define Dataset dialog. This dialog is quite powerful, in that you can select not only the tables 
 to be synchronized, but also the columns of each table. Additionally, row filtering can be applied to the dataset to 
 synchronize specific data. In this example each of these features will be illustrated.
 As you scroll through the list of tables you will notice that some are red. You should also notice a nice red warning 
 at the top of the dialog stating the some of the tables do not meet schema requirements.
 Tables that appear in red cannot be selected for synchronization. This example will select two tables that are OK for 
 synchronization; HumanResources.Employee and Person.BusinessEntity. As you select the HumanResources.Employee 
 table, you’ll notice that even though the table itself is not red (OK for synchronization), some of the columns are red. That 
 means that those columns cannot be selected and are not available for synchronization. You can see this in Figure 
 7-12
 .
 Figure 7-12.
  Selected tables and columns to be synchronized, and two columns that cannot be selected",NA
Applying a Filter,"The table and columns have been selected. At this point the dataset could be complete, but for this example a 
 filter will be applied. Applying a filter is quite easy. As shown in Figure 
 7-13
 , click the Filter checkbox next to the 
 column you want to filter on and apply the row filtering criteria in the Row Filtering section. In this example the 
 BusinessEntityID column of the Person.BusinessEntity table has been selected. Enter a value of 10000 in the filter 
 Value field to filter all Persons with an 
 BusinessentityID
  greater than 10,000.
 Figure 7-13.
  Row filtering
 Querying the Person.BusinessEntity table in the hub database will reveal IDs 1 to 20777, which were added 
 during the creation of the database. Because of the filter applied to this sync group, only rows with BusinessEntityID 
 greater than 10,000 will be synchronized to the member databases.",NA
Applying Conflict Resolution,"Configuring the sync group is almost complete. The next step is to set the Conflict Resolution. A data conflict occurs 
 whenever the same data in two or more databases within a sync group is changed between synchronizations. In the 
 current release of SQL Data Sync you can select between two resolution policies: Hub Wins and Client Wins.
 •�
 Hub Wins:
  The first row change written to the hub is kept. Subsequent attempts to write to the 
 same row in the hub are ignored. The first write to the hub is propagated out to all member 
 databases.
 •�
 Client Wins:
  Each row change in a member database is written to the hub, overwriting prior 
 changes to the same row. The last write to the hub is then propagated out to all member 
 databases.
 No matter which policy you adopt, one of the changed rows is kept and the others are lost whenever a  
 conflict arises.",NA
Setting the Sync Schedule,"Data synchronization can be scheduled from a minimum of 5 minutes to a maximum of 1 month. The default is 30 
 minutes, but this example won’t wait for the scheduler to pick it up. For the purposes of this example, we’ll do the 
 synchronization manually. Thus, disable the automatic synchronization by unchecking the Enabled check box.
 The complete configuration of the sync group should look like Figure 
 7-14
 .
 Figure 7-14.
  The complete sync group configuration
 The configuration is done; it is now time to deploy and provision.",NA
Deploying the Sync Group,"Provisioning the databases and deploying the sync group is as easy as clicking the Deploy button on the ribbon. 
 Figure 
 7-15
  shows that each database will be set to a status of Provisioning during the deployment process. Once each 
 database is ready to go, its status will be set to Ready.
 Figure 7-15.
  Database provisioning
 The on-premises database will more than likely finish provisioning before the other two, and then would be a 
 good time to look at what changes were applied during the provisioning process. Figure 
 7-16
  shows the changes made 
 to the on-premises database.
 Figure 7-16.
  Selected tables in the on-premises database",NA
Debugging and the Log Viewer,"By now you will have realized that both members of the sync group provisioned correctly, but the hub did not. The 
 icon for the hub database turned red, with an error message stating that the provision failed. Why?
 There is a wonderful way to find out what happened, and that is through the Log Viewer. On the toolbar, click 
 the Log Viewer button. Figure 
 7-17
  shows a list of different types of information display, including Errors. To view the 
 detail of the error, click the [copy] link at the end of the Message column, which will copy the contents of the error 
 message to the Clipboard.",NA
Looking at the Synchronized Data,"To validate that the synchronization was successful, jump over to SQL Server Management Studio and open a query 
 window to the on-premises member database AW2012. Type in the following query:
 SELECT * FROM Person.BusinessEntity
 Figure 
 7-18
  shows that indeed all rows greater than 10,000 were synchronized to the member databases.
 Figure 7-17.
  Selected tables",NA
Editing Data and Resynchronizing,"Now let’s edit some data and resynchronize, and then validate that the changes were made by requerying the data. 
 Go back to SQL Server Management Studio and open a query window to the on-premises member database AW2012. 
 Type in the following query:
 UPDATE HumanResources.Employee SET Job Title = 'Head Geek' WHERE BusinessEntityID = 1
 Open up another query window connected to the SQL Database member instance and execute the  
 following query:
 UPDATE HumanResources.Employee SET Job Title = 'VP of Techy Stuff' WHERE BusinessEntityID = 2
 Instead of waiting for the scheduled sync to pick up, click the Sync Now button on the toolbar. The 
 synchronization should only take a few seconds, at which point you should be able to query the HumanResources.
 Employee table in the hub database and validate that the changes made to the member databases where indeed 
 synchronized to the hub database.",NA
Testing Conflict Resolution,"In this example the Conflict Resolution was set to Hub Wins. For a quick test, change the Conflict resolution to Client 
 Wins and deploy the changes. Once the deployment is done, go back to SQL Server Management Studio and execute 
 the following query against the on-premises database:
 UPDATE HumanResources.Employee SET Job Title = 'Chief Executive Officer' WHERE BusinessEntityID = 1
 Wait 1 minute and then execute the following query against the SQL Database member instance:
 UPDATE HumanResources.Employee SET Job Title = 'Chief Head Officer' WHERE BusinessEntityID = 1
 Manually resynchronize; this should only take several seconds. Once the sync is done, query the 
 HumanResources.Employee table in both the hub database and the on-premises member database. The value of the 
 Title column for BusinessEntityID 1 should now be “Chief Head Officer” in all three locations.
 Figure 7-18.
  Data validation",NA
Data Sync Limitations,"While SQL Data Sync is quite functional and very flexible, there are some limitations in the pre-release version. The 
 following is a list of the current limitations for SQL Data Sync.
 Maximum number of SQL Data Sync Servers per subscription: 1
 •�
 Maximum number of sync groups any database can belong to: 5
 •�
 Filters per table: Up to 12 (optionally 13 if one is on the primary key column)
 •�
 Database, table, schema, and column names: 50 characters per name
 •�
 Tables in a sync group: 100
 •�
 Columns in a table in a sync group: 1000
 •�
 If you query the HumanResources.Employee table, you’ll notice that it contains BusinessEntityID 1 to 290, well 
 below the filter of 10,000. Even though there is a PK/FK constraint on the hub database, why were records less than 
 10,000 synchronized in the HumanResources.Employee table?
 The answer is that currently filtering is on individual tables. In this example the filter would need to be applied to 
 both tables. In all reality, the filter would be for records less than 10,000 and applied to both tables.
 Additionally, in some cases this 
 might
  mean denormalizing the schema of your database so that the value on 
 which you want to filter is available in each table.
 Existing triggers on source tables, CHECK constraints, and indexes on XML-type columns are not provisioned. 
 Indexes are created only for the columns selected to be synchronized.",NA
Data Sync Best Practices,"A lot has been learned since the last release of this book, so this section will focus on best practices learned over the 
 last couple of years. We will discuss topics that will help in the design and planning phase for SQL Data Sync. The key 
 is to keep in mind that SQL Data Sync is a global solution, allowing you to move data around the world.",NA
Design Considerations,"•�
 Avoid synchronization loops:
  A synchronization loop results when there are circular 
 references within a sync group so that each change in one database is replicated through the 
 databases in the sync group circularly and endlessly. Sync loops degrade performance and 
 significantly increase costs. Synchronization loops can be avoided by not including circular 
 references within a database or table, or between two sync groups.
 •�
 On-premises-to-cloud scenario:
  Keep your hub database close to the greatest concentration 
 of the sync group’s database traffic to minimize latency.
 •�
 Cloud-to-cloud scenario:
  When all the databases in a sync group are in the same data center, 
 the hub database should also be located in the same data center. When databases in a sync 
 group are in different data centers, the hub database should be located in the data center 
 where the majority of the traffic takes place.",NA
Initial Synchronization,"SQL Data Sync treats prepopulated rows as data conflicts. Whenever possible, start with an 
 •�
 empty destination database.
 To increase performance, use the auto-provisioning capability only for testing. For production, 
 •�
 pre-provision the destination databases with the schema.
 Views and Stored Procedures are not created on the destination database. You can use the 
 •�
 Generate Scripts wizard in SQL Server Management Studio or SQL Server Data Tools to deploy 
 these objects.",NA
Security,"Install the client agent using the least privileged account with network service access.
 •�
 Install the client agent on a computer separate from the on-premises SQL Server.
 •�
 Databases should not be registered with more than one client agent to avoid any challenges 
 •�
 with deleting sync groups.",NA
Sync Schedule,"No bullet points here but a few words of caution. It is good practice to schedule synchronizations such that each 
 synchronization has time to complete. If one synchronization tries to execute before the prior one completes, the 
 second sync attempt won’t even start. There is no indication via the logs or management portal that the sync did not 
 take place.
 For example, if the sync schedule is set for every five minutes but it takes the group six minutes to complete then 
 your synchronizations will only happen every 10 minutes, not every 5.
 Currently, SQL Data Sync is offered without a charge. Yet, there are data transfer charges for data that leaves 
 any data center. Inbound data transfer (ingress) is free, but there are charges for outbound data (egress). Thus, be 
 cognizant of how frequently data needs to be refreshed per table.",NA
Summary,"Although SQL Data Sync is yet to be officially released, this chapter has provided a detailed walkthrough of the 
 functionality you can expect to see when it’s released.
 The chapter began with an overview of SQL Data Sync, including why it is needed and a basic scenario of SQL 
 Data Sync. From there, the majority of the chapter focused on building a SQL Data Sync example, illustrating how to 
 create, configure, and manage a sync group.
 Lastly, a few pages were spent on SQL Data Sync security and design best practices, considerations, and 
 limitations.",NA
Chapter 8,NA,NA
Windows Azure and ASP.NET,"This chapter walks you through the steps of creating a Windows Azure application and deploying it in the cloud. By now, 
 you know you can’t create WinForms applications in Windows Azure. However, you can create an ASP.NET application 
 that runs locally in IIS and connects to SQL Database, or build an ASP.NET application that runs as a cloud project which 
 can then run entirely in the cloud. This chapter will show you how to build a small web application in ASP.NET that can be 
 deployed in the cloud, and implements the Direct Connection pattern discussed in Chapter 2.
 An application published in the Microsoft cloud is referred to as a 
 cloud service
 , even if it’s a web site. So, you first 
 need to create a new Windows Azure cloud service to host the web site.",NA
Creating a Cloud Service 2,"First, you need to set up a Windows Azure cloud service so you can deploy a Windows Azure application later. Each 
 Windows Azure service created in the cloud is mapped to a virtual machine. In most cases you do not need to have control 
 over the Virtual Machine (VM) itself; you can simply deploy your applications and configure certain parameters. However, 
 you can also perform advanced tasks such as running startup scripts and establishing a remote connection to the VM. 
 To create your Windows Azure cloud service, follow these steps:
 1. 
 Open Internet Explorer, and go to 
 http://manage.windowsazure.com
 . You’re prompted to 
 sign in with your Windows Live account.
 2. 
 When you’ve logged in, select Cloud Services. You will see the list of cloud services you’ve 
 created so far (if you have been granted administrative access to someone else’s Azure 
 account, you will see their cloud services, too), as shown in Figure 
 8-1
 .",NA
Creating a Windows Azure Project,"Let’s create a simple Windows Azure application in Visual Studio that displays a list of database users. The Windows Azure 
 project is an ASP.NET application created with a special project template: Cloud. Other project types are available when 
 creating cloud projects and are discussed later in this section. Although you can create the same project as a regular ASP.
 NET application first and convert it to a cloud project later, it is easier to start with a cloud project right away to avoid 
 additional conversion steps.",NA
Configuring Your Development Environment,"You must first install the Windows Azure Tools on your development environment to be able to develop a Windows Azure 
 ASP.NET application. You must be running Windows Server 2008 or later, or Windows Vista or later. The Windows Azure 
 Tools provide a runtime environment on your machine that allows you to develop and test a Windows Azure project. 
 It basically runs a local cloud for development purposes. After it’s installed, you see a new project type: Cloud. When 
 creating your project in Visual Studio, you can select the Cloud project type; an option to install the Windows Azure Tools 
 is available the first time you do so.
 Figure 8-4.
  A new Windows Azure service created",NA
Creating Your First Visual Studio Cloud Project,"To create a Visual Studio cloud project, follow these steps:
 1. 
 Start Visual Studio in elevated mode (as an Administrator). To do so, right-click Microsoft 
 Visual Studio (2008 or higher), and select “Run as administrator,” as shown in Figure 
 8-5
 . 
 Running as Administrator is required by the Windows Azure simulation tools that give you the 
 ability to test your Azure solution locally.
 Figure 8-5.
  Start Visual Studio in elevated mode",NA
Connecting a GridView to SQL Database,"Now that you have created a sample ASP.NET project, let’s connect to a database in the cloud that will return data that we 
 will display on the web page. Continue the example by following these steps:
 1. 
 Add a SQL Data Source and a 
 GridView
  control on the 
 Default.aspx
  page, and connect it to 
 the data source. These steps next show you how to configure the SqlDataSource manually.
 2. 
 Open the 
 Default.aspx
  page, and select Design view.
 3. 
 Drag a 
 SqlDataSource
  from the Toolbox. Drag a 
 GridView
  control on the page as well, and set 
 its Data Source property to 
 SqlDataSource1
 , as shown in Figure 
 8-13
 .",NA
Deployment and Configuration Files,"You probably noticed that there are multiple configuration files in your solution. You can find a 
 Web.Config
  file in 
 your project, along with two additional files underneath: 
 Web.Debug.Config
  and a 
 Web.Release.Config
 . These two 
 additional files allow you to perform transformations to your Web.Conf file depending on the build configuration. You 
 can use this feature when you want to modify the connection string to point to a production database when you target 
 a Release build. For more information about Web.Config transformations visit this link: 
 http://go.microsoft.com/
 fwlink/?LinkId=125889
 .
 You also probably noticed that the web role itself has configuration files. The project created previously has three 
 configuration files:
 •�
 ServiceDefinition
 .
 csdef
 . Lets you control the definition of the service itself, such as the HTTP 
 endpoint the application will be listening on.
 •�
 ServiceConfiguration
 .
 Local
 .
 cscfg
 . The configuration file being packaged when choosing the 
 Local deployment mode.
 •�
 ServiceConfiguration
 .
 Cloud
 .
 cscfg
 . The configuration file being packaged when choosing the 
 Cloud deployment mode.
 Although the web.config build mode changes the 
 web.config
  file using transformations, the 
 ServiceConfiguration.
 cscfg
  does not; it uses the Local or the Cloud version depending on the deployment mode selected. Figure 
 8-15
  shows 
 how to apply the Release build configuration transformation and the Cloud service configuration file when packaging the 
 project.
 Figure 8-15.
  Selecting the deployment and build configuration settings",NA
Deploying an ASP.NET Application in Windows Azure,"You’re almost there. In this section, you walk through the steps of deploying the ASP.NET application in the cloud. You 
 can publish an application directly from Visual Studio or package deployment files and deploy them manually. Because 
 publishing directly from Visual Studio requires the creation of certificates and may require changing firewall settings, I’ll 
 show you the manual deployment steps to make things simpler to follow.
 1. 
 You need to package your project. Right-click AzureExample (the cloud project), and click 
 Package. This action opens the dialog box seen previously in Figure 
 8-15
 . Select your Build 
 and Service configuration and click Package. Once the package is created successfully, 
 Windows Explorer opens where the deployment files are located. You will need these files 
 shortly, so do not close that window.
 2. 
 Open Internet Explorer, navigate to the Azure Management Portal  
 (
 http://manage.windowsazure.com
 ), and log in using your Live ID.
 3. 
 When you’ve logged in, the Internet Explorer window shows you the home page for your 
 Azure account. Click on Cloud Services.
 4. 
 You should see the page shown in Figure 
 8-1
  at the beginning of this chapter. Click the 
 AzureExample service and then click the Staging link. You now see the staging dashboard 
 of the AzureExample service.
 5. 
 Click Upload. Deploying in staging creates a temporary service that you can use to test 
 before promoting to the final public URL. This way, you can test that your application is 
 working as desired before deploying it in production.
 6. 
 You now see the Upload a package dialog box (see Figure 
 8-19
 ) with the following fields:
 •�
 Deployment Name
 . Under Service Deployment Name, you need to enter a label for your 
 service. Type 
 TEST001
 , for example. To avoid a warning about our test deployment, check 
 the option at the bottom of the screen (Deploy even if one or more roles contain a single 
 instance). You can click the question mark to learn more about this warning.
 •�
 Package
 . This is the package that Visual Studio built for you. It’s the file with extension 
 .cspkg
  in the Windows Explorer window. Clicking Browse opens an Open form. You may 
 find it easier to copy the entire path from the Windows Explorer window and paste it in the 
 Open form. Select the package file, and click Open.
 •�
 Configuration
  . This is the configuration file of your cloud service project with a 
 .cscfg
  
 extension. Again, click Browse and select the configuration file from the Open form.",NA
Summary,"This chapter showed you how to create a simple ASP.NET application that connects to a SQL Database instance. This 
 requires a few configuration steps, including the creation of a Windows Azure service in the cloud and the installation of a 
 Visual Studio extension called the Windows Azure Tools.
 While this chapter focused primarily on deploying an ASP.NET application in the cloud, you can very easily deploy 
 ASP.NET projects on your enterprise IIS servers and configure the connection string to connect to a SQL Database 
 instance. This gives you multiple deployment options to consider, although you must keep in mind that each deployment 
 option has a different cost structure. Data transfer between Windows Azure and SQL Database is free within the same 
 region, but you pay for compute time in Windows Azure.",NA
Chapter 9,NA,NA
Designing for High Performance,"This chapter focuses on a few key topics that can help you design high-performance applications that consume data 
 in SQL Database and SQL Server databases. The approach used in this chapter builds a simple but effective WinForms 
 application that consumes data stored both on premises and in the cloud. You’ll first explore a few general concepts 
 and then quickly go into the design and development of a shard library that reads data from multiple databases. 
 Finally, you’ll see how to add multithreading to the shard library using the Task Parallel Library (TPL) and caching 
 using the Enterprise Library (formally known as the caching application block).",NA
General Performance Concepts,"Before diving into the details, let’s discuss a few concepts related to performance. The first thing you should know 
 is that achieving high performance is difficult. Although making sure applications perform to acceptable levels is 
 important, advanced performance tuning requires careful planning and should be included as a design goal only if 
 requirements drive you to believe that high performance is necessary. For example, if you expect your application 
 to be used by thousands of concurrent users, then you may need to use caching and even multithreading. On the 
 other hand, certain high-performance techniques can make code difficult to read and maintain, and in such cases 
 knowledge transfer may be difficult.",NA
Chatty vs. Chunky,"The encrypted network connection to SQL Database yields slower applications and may impact your application 
 design significantly. An application that opens a database connection for every database call and performs a 
 roundtrip for every update (that is, a 
 chatty
  application) performs slower than an application that loads data for 
 multiple objects in a single call and sends changes in bulk (a 
 chunky
  application). LINQ to SQL and the Entity 
 Framework are data access layers that provide good control over the use of bulk operations (the 
 SaveChanges
  method 
 on the object context).
 For example, if you design a data access layer that contains a lot of business rules, your code may perform many 
 roundtrips to the database to load the data needed to execute the business rules. If this is the case, you can implement 
 certain data-intensive business rules in stored procedures (close to the data) and/or use caching to avoid unnecessary    
 roundtrips.",NA
Lazy Loading,"On the other hand, although it’s good to have fewer roundtrips from a performance standpoint, you should load only 
 the data you need, for two reasons: the more data you load, the more you pay for the SQL Database service if the 
 data is consumed outside of the data center where it resides; and loading more data than necessary can slow down",NA
Caching,"Another important technique used to minimize roundtrips is 
 caching
 . Your application (or service) may use caching 
 to avoid unnecessary roundtrips if some of your data doesn’t change often. This may also impact your database design 
 choices. For example, if you have a table that stores a list of states, the table will probably remain unchanged for a long 
 time, which makes it a great candidate for caching.
 Caching can be performed in memory or on disk (in a local database, for example). You have a few options:
 •�
 ASP.NET caching.
  ASP.NET offers a cache object that provides good caching capabilities. 
 However, ASP.NET caching is tied to IIS. Restarting IIS clears the ASP.NET cache unless you’ve 
 taken the necessary steps to persist the cache.
 •�
 Windows Server AppFabric.
  The AppFabric offers a next-generation distributed cache 
 (previously known as Velocity ). This cache can run on multiple computers and is made 
 available through a .NET API. This caching library is only available on premise and cannot be 
 used in Windows Azure.
 •�
 Windows Azure Caching Service.
  This caching service is a subset of Windows Server 
 AppFabric caching. Some of the methods are not supported in Azure. The Azure Caching 
 Service is only practical for storing IIS session state, or for other key-value pair storage needs 
 in which the key is always known in advance. However the Azure cache can run on local 
 instances, a method that leverages the memcache protocol.
 •�
 Enterprise Library.
  The Enterprise Library offers a collection of application blocks that 
 Microsoft makes available under public license. The Enterprise Library contains a cache 
 mechanism that doesn’t depend on ASP.NET. This caching mechanism is provided natively in 
 .NET 4.0 and can be found under the 
 System.Runtime.Caching
  namespace.",NA
Asynchronous User Interface,"Ultimately, performance is a measure that impacts the user experience and can be controlled to a certain degree by 
 offering highly responsive user interfaces. A Windows application that becomes unresponsive while loading data, or 
 a web page that doesn’t load until all the data has been retrieved, is perceived as slow. As a result, developing with 
 multithreading techniques may become more important to provide a better experience to your users.
 For web development, you should consider using asynchronous controls (such as AJAX) that give you more 
 control over partial page loading. For Windows development, you may need to use a multithreaded user interface 
 development approach.
 To implement a highly responsive application in WinForms, use the 
 Invoke
  method, shown on line 3 of the 
 following example, to refresh your user interface on the UI thread:
 1. void OnPassCompleted()
 2. {
 3.    this.Invoke(new EventHandler(UpdateProgressBar), null);
 4. }
 5.",NA
Parallel Processing,"In addition to asynchronous user interfaces, your code may need to execute on multiple processors. Two primary 
 scenarios can lead you to choose parallel processing for your application:
 •�
 Many calculations.
  Your application is CPU-intensive, especially if computations can be 
 independent from each other. Advanced graphics or complex mathematical computations are 
 examples of CPU-intensive operations.
 •�
 Many waits.
  Your application needs to wait between each call, and the cost of creating parallel 
 threads and aggregating results is insignificant. Database shards are an example: calling five 
 databases in parallel is roughly five times faster than calling five databases serially.
 Two choices are available to write parallel processes. If you can, you should use the Task Parallel Library (TPL), 
 because it’s easier:
 •�
 Task Parallel Library.
  The TPL is a newer library that Microsoft is providing as part of .NET 
 4.0. It allows you to take advantage of multiple CPUs quickly and easily. You can find the TPL 
 under 
 System.Threading.Tasks
 .
 •�
 Threads.
  Managing threads the old-fashioned way using the 
 System.Threading
  namespace 
 gives you the most flexibility.",NA
Shards,"Shards offer another mechanism by which your code can read and write data against any number of databases 
 almost transparently. Later in this chapter, you’ll create a horizontal partition shard (HPS) using the read-write shard 
 (RWS) design pattern as described in Chapter 2, with the round-robin access method. A horizontal partition implies 
 that all the databases have identical schema and that a given record can be written in any database that belongs to 
 the shard. From a performance standpoint, reading from multiple databases in parallel to search for records yields 
 greater performance; however, your code must keep breadcrumbs if you need to perform updates back to the correct 
 database. Finally, using a shard requires parallel processing for optimum performance. In Chapter 10 you will explore 
 federations
 , a SQL Database feature providing built-in capabilities to build database shards.",NA
Coding Strategies Summary,"Table 
 9-1
  provides a summary of the concepts discussed so far regarding some of the coding strategies available to you 
 when you develop against a SQL Database instance with performance in mind.",NA
Building a Shard,"Let’s build a shard library that can be used by applications that need to load and update data against multiple SQL 
 Database instances as quickly and transparently as possible. The shard we are building uses a collection of databases 
 as its underlying storage; however, there is no data affinity. A record could be located in any database; the approach 
 uses a round-robin insert mechanism when adding data. For the purpose of building an efficient shard library, you 
 stipulate the following requirements for the shard:
 1. 
 Adding new databases should be simple and transparent to the client code.
 2. 
 Adding new databases shouldn’t affect performance negatively.
 3. 
 The library should function with SQL Server, SQL Database, or both.
 4. 
 The library should optionally cache results for fast retrieval.
 5. 
 The library should support mass or selective reads and writes.
 6. 
 Data returned by the library should be accepted as a data source for controls.
 These requirements have very specific implications from a technology standpoint. Table 
 9-2
  outlines which 
 requirements are met by which technology.
 Table 9-1.
  Coding Strategies to Design for Performance
 Technique
 Comments
 Bulk data loading/changing
 Minimizes roundtrips by using a data access library that supports loading data in 
 bulk, such as the Entity Framework.
 Lazy loading
 Allows you to create objects for which certain properties are loaded only when 
 first called, to minimize loading unnecessary data (and improve performance). 
 The Entity Framework 4.0 supports this.
 Caching
 Lets you to keep in memory certain objects that don’t change frequently.  
 The caching application blocks provided by Microsoft offer this capability as well 
 as expiration and scavenging configuration settings.
 Asynchronous user interface
 Not technically a performance-improvement technique, but allows users to use 
 the application while your code is performing a long-running transaction and 
 thus provides a better user experience.
 Parallel processing
 Allows you to run code on multiple processors for optimum performance. 
 Although complex, this technique can provide significant performance 
 advantages.
 Shards
 Lets you to store data in multiple databases to optimize reads and spread the load 
 of queries over multiple database servers.",NA
Designing the Shard Library Object,"The library accepts requests directly from client applications and can be viewed as an API. Note that you’re using 
 extension methods to make this API blend in with the existing 
 SqlCommand
  class; this in turn minimizes the amount of 
 code on the client and makes the application easier to read.
 Figure 
 9-1
  shows where the library fits in a typical application design. It also shows how the library hides the 
 complexity of parallel processing and caching from the client application. Finally, the shard library abstracts the client 
 code from dealing directly with multiple databases.
 Figure 9-1.
  Shard library object diagram
 Table 9-2.
  Technologies Used to Build the Shard
 Technology
 Requirement
 Comment
 Configuration file
 1
 The configuration file stores the list of databases that make up the shard, 
 keeping the shard definition transparent to the code.
 Multithreading
 2
 Using the TPL lets the library spawn multiple threads to use computers 
 with multiple CPUs, allowing parallel execution of SQL statements and 
 thus improving performance.
 SqlClient
 3
 Using 
 SqlCommand
  objects allows the shard to connect to both SQL 
 Database and SQL Server databases.
 Caching
 4
 Caching lets the library store results temporarily to avoid unnecessary 
 roundtrips.
 Breadcrumbs
 5
 The library creates a virtual column for each record returned that stores 
 a breadcrumb identifying the database a record it came from, which 
 supports selective writes.
 DataTable
 6
 The library returns a 
 DataTable
  object that can be bound to objects easily 
 and serve as a data source.",NA
Managing Database Connections,"This section walks through a few coding decisions that are necessary when creating this shard. Because the shard 
 library needs to be able to connect to multiple databases, the client has two options to provide this list: it can 
 provide the list of connections to use whenever it makes a call to the library, or it can preload the library with a list of 
 connection objects that are then kept in memory for all future calls.
 The shard library declares the following property to hold the list of preloaded connection objects. The 
 ShardConnections
  property is declared as static so it can be used across multiple calls easily; the client application 
 only needs to set this property once:
 public static List<SqlConnection> ShardConnections {get;set;}
 In addition, an extension method is added to 
 SqlConnection
  to provide a GUID value that uniquely identifies a 
 connection string. The connection GUID is critical for the shard; it provides a breadcrumb for every record returned 
 by the shard. This breadcrumb is later used by the shard to determine, for example, which database to use when 
 performing an update statement.
 The following code shows how a connection GUID is calculated. It uses the 
 SqlConnectionStringBuilder
  helper 
 class and another extension method on strings called 
 GetHash()
  (on line 8). This extension method returns an   
 SHA-256 hash value. Note that if the connection string doesn’t specify a default database (Initial Catalog), you assume 
 the user is connected to the master database. This assumption is correct for SQL Database, but it may not hold true for 
 SQL Server:
 1. public static string ConnectionGuid(this SqlConnection connection)
 2. {
 3.    SqlConnectionStringBuilder cb = new SqlConnectionStringBuilder(connection.ConnectionString);
 4.    string connUID =
 5.    ((cb.UserID != null. ? cb.UserID : ""SSPI"". + ""#"" +
 6.    cb.DataSource + ""#"" +
 7.    ((cb.InitialCatalog != null. ? cb.InitialCatalog : ""master"");
 8.    string connHash = connUID.GetHash().ToString();
 9.    return connHash;
 10. }
 For reference, here is the extension method that returns a hash value for a string. Technically, you could use the 
 string’s native 
 GetHashCode()
  method. However, the built-in 
 GetHashCode
  method varies based on the operating 
 system used (32-bit or 64-bit) and the version of .NET. In this case, you create a simple 
 GetHash()
  method that 
 consistently returns the same value for a given input. The string value is first turned into an array of bytes using UTF-8 
 (on line 3). The hash value is then computed on line 4. Line 5 returns the hash as a string value:
 1. public static string GetHash(this string val)
 2. {
 3.    byte[] buf = System.Text.UTF8Encoding.UTF8.GetBytes(val);",NA
Reading Using the Shard,"Now that you’ve reviewed how connection strings are handled in the application and the library, you need to know 
 how the shard handles a 
 SELECT
  operation against multiple databases. In its simplest form, the library executes the 
 SELECT
  operation against the list of connections defined previously.
 The client application calls 
 ExecuteShardQuery
 , which in turn loops over the list of 
 SqlConnection
  objects  
 (see Figure 
 9-4
 ). If you look at the code, you can see that a copy of each connection object is made first; this is to avoid 
 any potential collisions if the client code makes a call to this method multiple times (a connection can only make 
 one call at a time). Then, for each connection, the code calls 
 ExecuteSingleQuery
 , which is the method in the shard 
 library that makes the call to the database.
 Figure 9-3.
  Viewing shard connections",NA
Caching,"To minimize roundtrips to the source databases, the shard library provides an optional caching mechanism. The 
 caching technique used in this library offers basic capabilities and can be extended to address more complex 
 scenarios. The objective of this library is to cache the entire 
 DataTable
  of each database backend whenever requested. 
 Figure 
 9-6
  shows the logical decision tree of the caching approach. It’s important to note that this library calculates a 
 cache key based on each parameter, the parameter value, each SQL statement, and the database’s GUID.
 Figure 9-6.
  Caching logic
 The effect of the cache is visible when you connect to SQL Database. Considering that connecting to a SQL 
 Database instance takes up to 250 milliseconds the first time, memory access is significantly faster. The importance of 
 the cache increases as the number of records increases and the number of databases increases in the shard.",NA
Updating and Deleting Records in the Shard,"At this point, you’re ready to see how updates and deletes take place through the shard. Updates and deletes against 
 the databases in the shard can be performed either for records in a given database or against all databases. At a high 
 level, here are some guidelines you can use to decide on an approach:
 •�
 Update or delete records in a single database.
  You update or delete one or more records in a 
 database when you already know the database GUID to use. This is the case when you use the 
 shard to retrieve records, because the database GUID is provided for all records returned.
 •�
 Update or delete records across databases.
  Generally speaking, you update or delete records 
 across databases in the shard whenever you don’t know which database a record is in, or when 
 all records need to be evaluated.
 To update or delete records in a single database, you must provide a command parameter that contains the 
 database GUID to use. Here’s the code that updates a single record in the shard. On lines 1 through 7, the code creates 
 a command object that calls a stored procedure that requires two parameters. On line 9, the code adds the database 
 GUID to use. This extra parameter is removed by the shard library before making the call to the requested database:
 1. cmd.CommandText = ""sproc_update_user"";
 2. cmd.CommandType = CommandType.StoredProcedure;
 3. 
 4. cmd.Parameters.Add(new SqlParameter(""@id"", SqlDbType.Int));
 5. cmd.Parameters[""@id""].Value = int.Parse(labelIDVal.Text);
 6. cmd.Parameters.Add(new SqlParameter(""@name"", SqlDbType.NVarChar, 20));
 7. cmd.Parameters[""@name""].Value = textBoxUser.Text;",NA
Adding Records to the Shard,"You see how easy it is to add records to the shard databases. This shard works best from a performance standpoint 
 when all databases in the shard have a roughly equal number of records; this is because parallel processing is 
 performed without any deterministic logic. As a result, the more spread out your records are in the shard, the faster  
 it is. You can add records in the shard in two ways:
 •�
 In a single database.
  If you’re loading the shard for the first time, you may decide to load 
 certain records in specific databases. Or you may decide to load one database with more 
 records than others, if the hardware is superior.
 •�
 Across databases.
  Usually, you load records in the shard without specifying a database. The 
 shard library uses a round-robin mechanism to load records.
 Adding a record in a specific database is no different than updating or deleting a record in a database; all you 
 need to do is create a 
 SqlCommand
  object, set the 
 INSERT
  statement, and add a 
 SqlParameter
  indicating the database 
 GUID to use.
 Figure 9-7.
  Sample application updating a record in the shard",NA
Managing a Shard,"Having created a shard and reached the point of being able to run queries and add data, you can begin to think about 
 higher-level issues: how to handle exceptions, manage performance, control transactions, and more.",NA
Managing Exceptions,"So far, you’ve learned the basic principles of the sample shard library. You saw how to select, insert, update, and delete 
 records in various ways through the methods provided by the library. Let’s discuss how you can manage exceptions 
 that the shard may throw at you.
 The current library doesn’t handle rollbacks, but it may throw exceptions that your code needs to capture. In 
 the previous example (Figure 
 9-8
 ), all the records were inserted except Jim Nastic: that name was too long for the 
 SqlParameter
  object (hence it threw a “Value Would Be Truncated” exception).
 The library handles exceptions through the 
 AggregateException
  class provided by the TPL; this class holds a 
 collection of exceptions. This is necessary because the library executes database calls in parallel. As a result, more 
 than one exception may be taking place at the same time. You need to aggregate these exceptions and return them to 
 the client for further processing.
 For example, the shard library’s 
 ExecuteSingleNonQuery
  method takes a 
 ConcurrentQueue<Exception>
  
 parameter, which represents an object that stores exceptions. This object is thread-safe, meaning that all running 
 threads can add new exceptions to it safely without running into concurrency issues. The following code shows that if 
 an exception is detected in the 
 ExecuteSingleNonQuery
  method, the code adds the exception to the queue on line 14. 
 Also, as a convention, the exception is rethrown if the queue isn’t provided (line 16):
 1. private static long ExecuteSingleNonQuery(
 2.  SqlCommand command,
 3.  SqlConnection connectionToUse,
 4.  System.Collections.Concurrent.ConcurrentQueue<Exception> exceptions",NA
Managing Performance,"So far, you’ve seen how the shard library works and how you can use it in your code. But it’s important to keep in mind 
 why you go through all this trouble—after all, there is nothing trivial in creating a shard library. This shard library does 
 something important: it allows a client application to grow parts (or all) of a database horizontally, with the intention 
 of improving performance, scalability, or both.
 What does this mean? It means the shard library can help an application keep its performance characteristics in 
 a somewhat consistent manner as more users use the application (scalability), or it can help an application perform 
 faster under a given load (performance). If you’re lucky, the shard library may be able to achieve both. However, this 
 won’t happen without proper planning. The shard library by itself is only a splitter, in the sense that it spreads calls  
 to multiple databases.",NA
Working with Partial Shards,"Note that building a shard isn’t an all-or-nothing approach. You can easily create a partial shard for a set of tables. 
 Depending on how your code is structured, you may or may not need to build logic that understands sharding, 
 depending on which tables you need to access. Sharding logic is best built in a data access layer (DAL), where the 
 physical organization of tables is separated from the organization of business objects.
 For example, you can design an application that consumes business objects directly. These business objects in 
 turn consume command objects, which are specialized routines smart enough to load data in memory structures 
 by calling execution objects. Figure 
 9-13
  shows the Authors object calling two command objects that load data from 
 two separate libraries: the standard ADO.NET library and the shard library built in this chapter. The complexity of 
 determining which library to call is deferred to the lowest level possible, protecting the application and business 
 objects from database structural changes.",NA
Managing Transaction Consistency,"Because distributed transactions aren’t supported in SQL Database, shard libraries for SQL Database don’t offer 
 transactional consistency. But you should look carefully at your transactional needs and what this means to your 
 application design.
 You can add transactional capabilities in the shard library described in this chapter by changing the 
 ExecuteShardNonQuery
   and 
 ExecuteParallelRoundRobinLoad
  methods. To do so, you need to add a separate 
 transaction context to all connection objects and commit them in a loop at the end of the last execution. If any 
 exception occurs, you must roll back all the changes.
 ■
  
 Note
  as mentioned earlier, the shard library is an open-source project and is likely to evolve over time. Check for  
 the latest release to see which features are supported.",NA
Managing Foreign Key Constraints,"Another interesting issue to consider in shard databases is related to foreign key constraints. You may quickly realize 
 that maintaining referential integrity can be challenging regardless of the sharding approach you use.
 To maintain relational integrity, the following concerns apply:
 •�
 Data duplication.
  Because you don’t know which records are where in the shard, the parent 
 tables need to be duplicated in every database. For example, a table that contains the list of 
 states (Florida, Illinois, and so on) may need to be replicated across all databases.
 •�
 Identity values.
  Adding records in one database can’t be easily replicated across to other 
 databases. Thus, using an identity value as the primary key may be difficult because you aren’t 
 guaranteed to have the same value in all databases in the shard. For example, the StateID 
 value for Florida may be 10 in one database and 11 in another.
 Figure 9-13.
  Example application design implementing a partial shard",NA
Designing a Multitenant System,"The shard library built in this chapter is designed to access data using a round-robin mechanism. New data could 
 be stored in any underlying database and a breadcrumb is used to temporarily remember which database contains 
 which data. This works well for certain applications but may prove difficult to use for business applications where 
 customer records need to be isolated. Let’s review another HPS: a schema-separation architecture for customer data 
 in a multitenant system.
 Schema-based security and data isolation were described in Chapter 3. You learned that schema-based isolation 
 provides a good security model to protect customer data from other tenants in the same database and how to properly 
 configure access control for this model.
 To design this system we must consider the following points:
 •�
 Service Account.
  In order to leverage connection pooling for each customer you will need  
 to make sure your connection strings use service accounts. A service account is a login that 
 users do not know about and is used only by the system. Each customer will have its own 
 service account.
 •�
 Login Database.
  You will also need to create a central database containing the connection 
 string each customer should use. Alternatively, you could store customer connection strings in 
 a configuration file; however, keeping this setting in a database makes it easier  
 for maintenance.
 •�
 Customer ID.
  A customer ID needs to be created for each customer; the customer ID then 
 needs to be stored in memory (usually in the Session state) so that the system can retrieve the 
 connection string given the Customer ID.
 Figure 
 9-14
  shows how the system can process a request for a specific customer when reading or changing 
 data. This workflow assumes that the customer has already logged in and is authenticated with the application; the 
 authentication process is responsible for storing the Customer ID in memory (the Customer ID could be stored in the 
 application cache or the session state for an ASP.NET application).",NA
Creating Vertical Partition Shards,"The majority of this chapter discussed the HPS, but it’s important to also be familiar with vertical partition shards 
 (VPSs). A VPS stores an application database schema across multiple databases; in essence, the application database 
 is split in such a way that no database in the shard holds the complete schema.
 Let’s consider a simple application database that is made of two tables: Users and Sales. The Users table  contains 
 a few thousand records, and the Sales table contains a few million records. You can design a VPS with two databases: 
 the first contains the Users table, and the second contains the Sales table.
 This type of shard makes it easy to isolate the processing power needs of certain tasks without affecting the other 
 database. For example, you can use the Users table to process login requests from users while at the same time using 
 the Sales table for a CPU-intensive calculation process or to run long reports.
 You can also build a VPS by splitting a table across multiple databases. For example, the Sales table  may store 
 large binary fields (such as documents). In this case, you can split the Sales table such that common fields are stored 
 in one database, and the field containing documents is stored in another database.
 A VPS does bring it own set of challenges, such as the difficulty of keeping strong RI between databases and, in 
 the case of SQL Database, ensuring transaction consistency. These considerations are only meant to bring certain 
 design issues to the surface when you’re thinking about a VPS. Depending on your database design, a VPS may work 
 very well and may be simpler to implement than an HPS.",NA
Big Data with Hadoop for Windows Azure,"Although this book is about SQL Database, you should know that Microsoft is introducing Hadoop for Windows 
 Azure, a big data initiative that offers the Apache Hadoop framework on the Azure cloud.
 Hadoop is a distributed computing model allowing you to scale your processing needs to a very large number of 
 machines reliably for both structured and unstructured data. When working with Hadoop, you implement software 
 logic to distribute your computing needs using open source subprojects, such as the Hadoop Distributed File System 
 and the Hadoop MapReduce. Each machine in the Hadoop network offers local compute and storage for massively 
 distributed computational needs.
 At the time of this writing, Hadoop for Windows Azure is only available by invitation, and limited information is 
 available. If you have large amounts of data to analyze and need a massively scaled out environment, take a look at 
 Microsoft’s offering for Hadoop.",NA
Summary,"As you’ve seen, building databases with SQL Database can be complex if you need to scale your database design or if 
 you’re looking to develop a high-performance system. This chapter introduced you to the design and implementation 
 of a shard library that gives you the necessary building blocks to experiment with a flexible and scalable architecture.
 You saw how to use caching in a data access layer and how parallel processing of SQL statements can increase 
 performance of your applications. You also explored more advanced topics, such as referential integrity and exception 
 management. This chapter provides you with the necessary background to create your own shard and create high-
 performing applications against SQL Database.",NA
Chapter 10,NA,NA
Federations,"One of the major tenets of any cloud computing platform is elasticity, allowing systems to scale beyond certain 
 limits of their current designs. With Federations, SQL Database delivers on that promise; it is a technology your SQL 
 Database instances can use to scale over time based on size or performance bottlenecks.
 You may have certain expectations about how this technology should work, one of which is likely to be 
 transparency to your applications. You may expect that no code changes should be necessary to your existing 
 applications if you were to use Federations. Like anything else, radical features like Federations come with certain 
 compromises. Considering that this is the first incarnation of Microsoft’s answer to scaling out the database tier, you 
 will need to account for certain limitations before you can reap the benefits. This chapter will help you understand 
 what problems Federations answer and how to best leverage this feature.",NA
Introducing Federations,"Until now, the only practical way developers and administrators could improve the performance and scalability of 
 a database that needs both read and write access, without introducing significant code changes, was to add more 
 memory, more CPU, or more disks on the same server. This is referred to as “scaling up.” The Federation feature was 
 introduced on the SQL Database platform with the intent of solving the scalability issues of traditional database 
 servers; it provides a way to “scale out” a database by adding more servers to your database.",NA
Federations versus Sharding,"In the database world, sharding is a general term describing the distribution of highly related data across multiple 
 physical machines. As a result, sharding is the implementation of a scale-out architecture used by large databases in 
 order to harness the computing power of multiple machines. Federations are SQL Database’s implementation of the 
 sharding concept. To explain where Federation fits in the big picture, I’ll outline the various scaling models available. 
 The scaling models described below are architected around the concept of a 
 data domain
 , which is the primary unit 
 of data segregation used in a shard. For example, in most commercial applications the data domain is the customer. In 
 astronomy the data domain could be a galaxy, or a planet. In finance, it could be the fiscal year.
 •�
 Scale Up
 . As described previously, scale-up architecture involves adding more resources to a 
 machine holding data. The fundamental issue with scale-up architecture is that at some point 
 adding more resources on a server doesn’t yield better performance, because the application 
 is not designed to account for all the available resources, and the law of diminishing returns 
 applies. All the data domain instances are stored in the same database, and every query needs 
 to include the data domain it is using (such as a 
 WHERE
  clause when searching for a specific 
 customer ID). This is the architecture used by most systems.",NA
Why Use Federations?,"Let’s start by understanding why some applications are designed (or redesigned) to use Federations. As hinted 
 previously, Federations solve two common problems: storage space in SQL Database and performance over time.
 As you know, SQL Database is limited in size to 150GB of data. What do you do if your database grows beyond 
 this space limitation? Your choices are simple: truncate some of the data (or archive old records) or use Federations. 
 Because Federations allows you to split tables across multiple databases, you could spread your data across multiple 
 150GB databases, giving you terabytes of storage capacity in SQL Database. For example, your database may have a 
 table called PurchaseHistory with hundreds of millions of records in it and representing 90% of all storage needs. You 
 could federate this table across multiple servers, and leave all the other tables unchanged. If your PurchaseHistory 
 table were to grow to 1TB in size, you would need seven SQL Database instances of 150GB each to store that table 
 (called federation members), plus the original database containing all the other tables (called the root database).  
 A great feature of Federations is that you can add more databases over time as your storage needs increase. So if  
 your table grows to 1.1TB, just add another 150GB database.
 Another important driver for adoption of Federations is database performance. When the number of users 
 grows in your application, the database workload typically increases. There are more records to process over time, 
 and because you have more users, the database will typically experience an increase in requests. At some point, 
 the workload increases so much that your application may experience slowdowns, and even throttling from SQL 
 Database. Federations can help you achieve higher performance, in certain cases, because instead of processing all 
 the requests in a single database, you are essentially distributing certain requests to multiple databases, each with 
 its own allocation of CPU, memory, I/O, and TempDB. In other words, Federations increase your computational 
 resources naturally because your data is stored on servers that share virtually nothing in common.",NA
Federations Overview,"Let’s dive a little deeper into Federations and explore how this technology works. The first thing you need to do is 
 learn the terminology that describes the components of Federations.",NA
Creating More Federations,"So far we have created a single federation and split the records in two in order to create two federation members. 
 However, the federation we created only makes sense for historical purchases because its partition key is on 
 purchaseId. As discussed in the introduction to this chapter, one of the major benefits of Federations is the ability to 
 define multiple data domains and manage the compression of each domain separately. What if we also have a large 
 table that tracks customer visits?
 No problem! We can create as many federations as we need and place a collection of tables inside them. Let’s 
 create another federation called CustomerVisits:
 // Connect back to the root database
 USE FEDERATION ROOT WITH RESET
  
 // Create a new federation
 CREATE FEDERATION CustomerVisits (visitId BigInt RANGE)
  
 // Connect to our first federation member
 USE FEDERATION CustomerVisits (visitId=0) WITH FILTERING=OFF, RESET
  
 // And let's create a table with a few records
 CREATE TABLE CustomerVisits (
   vId bigint NOT NULL PRIMARY KEY,
   dateOfVisit dateTime NOT NULL,
   storeId int NOT NULL)
 FEDERATED ON (visitId = vid)
  
 INSERT INTO CustomerVisits VALUES (1, getdate(), 1)
 INSERT INTO CustomerVisits VALUES (2, getdate(), 2)
 INSERT INTO CustomerVisits VALUES (3, getdate(), 2)
 INSERT INTO CustomerVisits VALUES (4, getdate(), 1)
 INSERT INTO CustomerVisits VALUES (5, getdate(), 1)
 We now have two federations and three federation members in total, as shown in Figure 
 10-2
 .",NA
Managing Federations,"So far you have seen how to create federations, split federation members, and add records to federated tables. You 
 may wonder what tools exist to view your federations and their members, and how to tell whether your federations are 
 ready for a split.
 To manage your federations, turn to the SQL Database Management Portal. You access this portal by first logging 
 into your usual Windows Azure Management Portal (
 http://manage.windowsazure.com
 ). Find your newly created 
 database under SQL Database and click the Manage button at the bottom of the screen as seen in Figure 
 10-3
 .
 Figure 10-2.
  Creating multiple federations",NA
Advanced Considerations,"As you can see from the previous examples, Federations offer an important scalability model for cloud applications 
 that need to scale beyond the capabilities of a single SQL Database instance. There are, however, certain challenges 
 and considerations that are worth discussing before you decide to adopt this feature.",NA
Limitations,"Let’s review some of the most important limitations you should be aware of before adopting Federations.
 •�
 No Merging.
  You may have noticed the lack of merging capability. Once you split a federation 
 member, you cannot merge it back in the current version of SQL Database. Although this is a 
 shortcoming, future versions of Federations will likely support this capability.
 •�
 Independent Schemas.
  As hinted previously, once a federation member has been created, its 
 schema definition is independent from all the others in the same federation. In our previous 
 example, you could connect to one of the federation members and drop a table, or add a 
 column, or even change a stored procedure. Because there are no safeguards at this time, it 
 is relatively easy to have federation members that have slightly different schemas. This could 
 have some implications in your code, for example. So if you make an update to the schema of a 
 federation member, remember to make the same changes to all the other federation members.
 •�
 Limited Tool Support.
  For the time being, most tools available on the market do not support 
 federated databases, including some of the tools provided by Microsoft. Although this is very 
 likely to change over time, you cannot use Data Sync services, Azure Reporting Services, or the 
 database 
 COPY
  operation with a federated member.",NA
Sharding Library,"Some of the limitations discussed previously related to fan-out queries can be resolved by coding the logic yourself, or 
 by using open source libraries that have solved this problem for you.
 Of importance is that you can use the Enzo Sharding library discussed in previous chapters. The latest release 
 supports Federations and allows you to execute statements across multiple federation members easily, using 
 multithreaded operations for faster results. You can use the built-in fan-out operation of the library to perform 
 schema changes across your databases, return a single result set, or execute simple aggregations across federation 
 members.
 With the latest library you can execute a distributed query, called D-SQL, using a syntax that extends T-SQL. 
 The library detects the syntax automatically and executes the inner query across the federation members specified.  
 Because this is a .NET library, you cannot use this command structure outside of your code. Here is an example of a 
 D-SQL query that fetches all the PurchaseHistory records, regardless of the number of federation members:
 SELECT * USING (SELECT * FROM Purchases) FEDERATED ON (PurchaseHistory)
 The inner statement is the query being executed across the PurchaseHistory federation; a single data set is 
 returned to the calling code. Here are a few other variations of D-SQL that perform other requests:
 // finds the maximum amount across all federation members
 SELECT MAX(amount) USING (SELECT MAX(amount) FROM Purchases)
   FEDERATED ON (PurchaseHistory)
  
 // finds the sum of all sales for zipcode 33498
 SELECT SUM(amount) USING (SELECT SUM(amount) FROM Purchases P",NA
Summary,"SQL Database offers a unique feature, called Federations, that promises elasticity at the database layer, giving 
 you the option to scale your database tier quickly and efficiently. While the first release of this feature has a few 
 limitations it is clear that many developers will be compelled to evaluate this feature and possibly adopt it to address 
 their scalability needs.",NA
Chapter 11,NA,NA
Performance Tuning,"Designing for high performance becomes more important with a database that runs in the cloud because 
 development objectives tend to minimize roundtrips and the amount of data being returned. This chapter provides 
 an overview of some of the most common tuning and troubleshooting techniques available in SQL Database. Keep in 
 mind that performance tuning is a very complex topic; as a result, this chapter can introduce only selected techniques.
 The techniques presented are similar to the ones available in SQL Server, although some of the tools aren’t 
 supported in SQL Database. Along the way, you will walk through a few examples of how to improve the performance 
 of a SQL statement and the steps required to tune it.",NA
What’s Different with SQL Database,"Before diving into the specifics, let’s review some of the things you need to remember when tuning your Azure 
 databases. Some of the techniques you may be using today are available, but others may not be.",NA
Methods and Tools,"Because a SQL Database runs on a hosted and shared infrastructure, it’s important to understand which tuning methods 
 are available and which aren’t. Table 
 11-1 
 outlines some of the key methods traditionally used by developers and DBAs in 
 tuning a database system. The term 
 system
  is appropriate here because at times you need to tune the database server, and 
 in other instances you need to tune the actual database code or even address database design issues.
 Table 11-1.
  Typical tuning methods and tools
 Method or Tool
 Available?
 Comments
 SQL Profiler
 No
 Tools using server-side traces, such as most auditing tools, SQL Profiler, and 
 the Database Engine Tuning Advisor, aren’t supported.
 Execution plan
 Yes
 SQL Server Management Studio (SSMS) can display actual execution plans 
 against a SQL Database. You’ll review this later in the chapter.
 Perfmon
 No
 Any Windows monitoring tool that is typically used for performance tuning 
 is unavailable.
 DMVs
 Limited
 A few dynamic management views (DMVs) are available and provide insightful 
 information about running sessions and statements previously executed.
 Library metrics
 Yes
 ADO.NET provides library-level statistics that offer additional insight to 
 developers, such as the total processing time from the consumer standpoint 
 and bytes transferred.",NA
Coding Implications,"Because you have no access to the server-side configuration settings of SQL Database, such as disk configuration, 
 memory allocation, CPU affinity, and so forth, you need to place more emphasis on the quality of your SQL  
 statements—and, now more than ever, your network traffic. Indeed, the number of network roundtrips your code 
 generates and the number of packets returned have an impact on performance because the connection to SQL Database 
 is a far link (meaning that it’s far away) when connecting directly with client applications, and the communication is 
 encrypted.
 Your performance-tuning exercise should include the following areas:
 •�
 Connection pooling.
  Because establishing a new connection requires multiple network 
 roundtrips by itself and can affect your application’s performance, you should ensure that 
 your connections are pooled properly by using a service layer when possible and a service 
 account so that your connection string doesn’t change. In addition, SQL Database will throttle 
 you if you establish too many connections. This behavior is controlled by the denial of service 
 (DoS) feature briefly discussed in Chapter 3.
 •�
 Packet count.
  Because the time spent to return data is greater than you may be used to, 
 you need to pay attention to SQL code that generates too many packets. For example, 
 Print
  
 statements generate more network traffic than necessary and should be removed from your 
 T-SQL if at all possible.
 •�
 Indexing.
  You may remember from Chapter 2 that SQL Database may throttle your 
 connection if it detects that your statement is consuming too many resources. As a result, 
 proper indexing becomes critical when tuning for performance.
 •�
 Database design.
  Of course, certain database designs are better than others for performance. 
 A heavily normalized design improves data quality, but a loosely normalized database 
 typically improves performance. Understanding this tradeoff is also important when you’re 
 developing against SQL Database.
 •�
 Parallel Data Access.
  If your code accesses multiple databases, in a shard environment, 
 for example, you may need to introduce multithreading to your code or tune your current 
 parallelism approach to avoid unnecessary locks.",NA
Tuning Techniques,"Let’s dive into the specifics of performance tuning, keeping in mind what you’ve learned so far. This section starts by 
 looking at database tuning capabilities and then moves up the stack, all the way to the client library making the actual 
 SQL call.",NA
Dynamic Management Views,"SQL Database provides a few handy system views called dynamic management views (DMVs) that are also available 
 in SQL Server. SQL Database exposes a subset of the DMVs, but all those related to query execution are available.  
 SQL Database supports the DMVs listed in Table 
 11-2
 .",NA
Connection Pooling,"Earlier, this chapter mentioned that connection pooling is an important consideration for performance. Although this 
 statement is generally accurate in the world of database programming, it becomes critical for SQL Database.  
 A poorly designed application may create too many connection requests, which can end up flooding SQL Database. 
 If too many connection requests are established, your connections may be 
 throttled
 , meaning that you can no longer 
 connect for a period of time.
 You should know that connection pooling is affected if any part of the connection string is changed (even 
 slightly), such as the application name or the login ID (UID). A new connection pool is created even if you change the 
 order of the parameters of a connection string. For example, if you have an application that performs three database 
 Table 11-3.
  Selected columns from 
 sys.dm_exec_sessions
  
 Metric
 Value
 Comment
 login_time
 2010-04-22 16:43:30.557
 The login time of the session. Note that sessions can be reused 
 over time, through connection pooling. This time represents 
 the last successful login.
 host_name
 DEVDSK01
 The machine name that made the connection to SQL Database.
 program_name
 SSMS
 The application name that is executing the statement on the 
 client workstation.
 host_process_id
 7720
 The Windows Process ID (PID) that is executing the statement 
 on the client workstation. You can view the PID of your 
 applications in Task Manager in Windows.
 cpu_time
 15
 The CPU time, in milliseconds, consumed by the SQL 
 statements since the connection was established.
 memory_usage
 2
 Number of 8KB bytes consumed by the connection so far.
 total_elapsed_time
 32
 The duration of the statement in milliseconds. This includes 
 the time to execute the statement and the time it takes to 
 return the data to the client machine.
 reads
 1
 Number of physical reads.
 writes
 1
 Number of physical writes.
 logical_reads
 322
 Number of logical reads.
 row_count
 50
 Number of rows returned.
 original_login_name
 MyTestLogin
 The login name of the user who successfully connected.",NA
Execution Plans with SSMS,"Sometimes you need to dig deeper and understand how SQL Database fetches data, and then use that information 
 to improve performance. In SQL Server, you can also use execution plans to observe the impact of changes to the 
 underlying hardware, such as changing memory configuration. Although you have no control over configuration 
 settings with SQL Database, execution plans can still be very useful to see the impact of your indexes and to view 
 which physical operators are being used.
 Whereas logical operators are used in a SQL statement, such as 
 LEFT JOIN
 , 
 physical operators
  tell you which 
 technique SQL Database is using to solve a given logical operation or to fetch additional data. The most common 
 physical operators SQL Database uses to represent 
 JOIN
  operations are listed in Table 
 11-4
 .
 Table 11-4.
  Physical 
 JOIN
  operators 
 Operator
 SSMS Symbol
 Comment
 Nested loop
 A loop is performed in SQL Database to retrieve data. For each 
 record in Table 1 matching the 
 WHERE
  clause, find the matching 
 records in Table 2. On large recordsets, loops can be costly.
 Hash match
 A hash is calculated for each record in each table participating 
 in a 
 JOIN
 , and the hashes are compared for equality.
 Merge
 Merge operators are usually the fastest operators because 
 they perform a single pass of the tables involved by taking 
 advantage of the order in which the data is stored or retrieved.
 You can give SQL Database certain hints to use a specific physical operator, but using them isn’t generally 
 recommended. You have three proper ways to influence SQL Database to select an effective physical operator:
 •�
 Review your
  
 WHERE
  
 clause.
  This is perhaps the most overlooked aspect of performance tuning. 
 When you have the choice, applying the 
 WHERE
  clause on the tables that have the most rows 
 gives you new opportunities for indexing.
 •�
 Optimize your database design.
  Highly normalized databases force you to create more 
 JOIN
  
 statements. And of course, the more 
 JOIN
  statements, the more tuning you need to do. While 
 you shouldn’t necessarily plan to have tables at first normal form, denormalizing certain 
 tables can offer performance benefits.",NA
Execution Plans with the Management Portal,"You should also note that the SQL Database Management Portal allows you to view execution plans; however, the 
 symbols representing the logical operations and the functionality provided are different.
 For example, the display area changes depending on the zoom level, showing a simplified view when zooming 
 out and allowing you to see problem areas very quickly. Figure 
 11-5
  shows that the T-SQL statement consumes the 
 most resources (total cost) on its Index Seek operation.
 Figure 11-5.
  Viewing an execution plan with the SQL Database management portal
 You probably realized that the Index Seek symbol is different from the one shown by SSMS. Although the symbols 
 are very sleek and largely self-describing, you may find it a bit more difficult to tune certain T-SQL operations, 
 because some of the symbols are simplified. For example the Non-Clustered Index Seek and the Clustered Index Seek 
 operations are combined into a single symbol in the management portal. Nevertheless, if you zoom enough you will 
 see a description of the operation under the symbol, clearly identifying which operation is being represented as will 
 be seen in Figure 
 11-7
 .
 Clicking on one of the symbols will give you additional details, as shown in Figure 
 11-6
 , similar to what you can 
 see in SSMS. Clicking on the View More button will display extensive information about the operation. To close the 
 detailed window, simply click in an empty area in the execution plan area.",NA
Query Performance with the Management Portal,"Another handy screen available on the management portal is the Query Performance summary page. To access this 
 page, simply click on the database and choose Query Performance. You will see a list of queries with a few important 
 metrics such as CPU ms/sec, Duration ms/sec and more, as shown in Figure 
 11-8
 . You can sort the list by clicking on a 
 column title.
 Figure 11-8.
  Query Performance summary screen
 Figure 11-7.
  Sorting the execution plan by CPU",NA
Indexing,"Creating the right indexes can be complex; it can take a long time to fully understand indexing and fine-tune database 
 queries. One of the most important things to remember with indexing is that its primary purpose is to help SQL 
 Database find the data it needs quickly. Like SSMS, the SQL Database Management Portal offers hints when a missing 
 index is detected. While these hints can be useful, it is still important to understand the logic behind indexing so that 
 you can optimize the performance of your database.
 Indexes are like smaller tables that contain a subset of the primary table. The tradeoff is that indexes consume 
 space and must be maintained by the SQL Database engine as the primary data changes, which can impact 
 performance under certain scenarios.",NA
Indexed Views,"Indexed views are an excellent alternative when you absolutely need to 
 JOIN
  data, and traditional indexing doesn’t 
 yield the performance you’re looking for. Indexed views behave like tables; the data covered is materialized to disk 
 so it can be retrieved quickly. Before jumping on indexed views, understand that they have certain limitations and 
 that due to their nature, you may incur a performance hit through the usual 
 Insert
 , 
 Delete
 , and 
 Update
  statements. 
 Taking the previous statement as an example, let’s see how to create an indexed view to support the 
 JOIN
  operation.
 First, create a view that contains the statement you want to tune. Make sure you include all the columns you need 
 in the 
 SELECT
  clause:
 CREATE VIEW vTestUsersType WITH SCHEMABINDING AS
    SELECT T.Name, T.UserType, T.AgeGroup, UT.UserTypeKey
    FROM dbo.TestUsers T INNER JOIN dbo.TestUserType UT ON
    T.UserType = UT.UserType
 Figure 11-13.
  Well-balanced execution plan",NA
Stored Procedures,"You’ve seen various ways to tune your statements and improve execution plans. However, keep in mind that you also 
 have stored procedures at your disposal.
 Stored procedures can give you an edge if you need to execute logic that requires a large volume of data. Because 
 you know that returning lots of data turns into a performance problem in SQL Database, you can place the business 
 logic that needs the data in a stored procedure, and have the procedure return a status code. Because you aren’t 
 charged for CPU time, this becomes an affordable option.
 Stored procedures can also be a security tool, allowing you to proxy the calls to underlying tables through a 
 procedure and never allowing direct access to the tables.
 Imagine that you need to calculate the cost of an item; however, in order to calculate the cost, you must loop to 
 obtain certain values and perform advanced operations. You can make a call from your application and calculate the 
 cost in the application code as follows:
 float cost = 0.0;  // the total cost
 int id = 15;       // the product category
  
 string sql = ""SELECT * FROM category WHERE catergoryId = "" + id.ToString();
 SqlConnection conn = new SqlConnection(connString);
 SqlCommand cmd = new SqlCommand(sql, conn);
 cmd.CommandType = CommandType.Text;
 conn.Open();
 SqlDataReader dr = cmd.ExecuteReader();
  
 try
 {
    while (dr.Read())",NA
Provider Statistics,"Let’s look at the ADO.NET library’s performance metrics to obtain the library’s point of view from a performance 
 standpoint. The library doesn’t return CPU metrics or any other SQL Database metric; however, it can provide 
 additional insights when you’re tuning applications, such as giving you the number of roundtrips performed to the 
 database and the number of packets transferred.
 As previously mentioned, the number of packets returned by a database call is becoming more important 
 because it can affect the overall response time of your application. If you compare the number of packets returned by 
 a SQL statement against a regular SQL Server installation to the number of packets returned when running the same 
 statement against SQL Database, chances are that you see more packets returned against SQL Database because the 
 data is encrypted using SSL. This may not be a big deal most of the time, but it can seriously affect your application’s 
 performance if you’re returning large recordsets, or if each record contains large columns (such as a varbinary column 
 storing a PDF document).
 Taking the performance metrics of an ADO.NET library is fairly simple, but it requires coding. The methods 
 to use on the 
 SqlConnection
  object are 
 ResetStatistics()
  and 
 RetrieveStatistics()
 . Also, keep in mind 
 that the 
 EnableStatistics
  property needs to be set to true. Some of the most interesting metrics to look for are 
 BuffersReceived
  and 
 BytesReceived
 ; they indicate how much network traffic has been generated.
 You can also download from CodePlex an open source project called Enzo SQL Baseline that provides both SQL 
 Database and provider statistics metrics (
 http://EnzoSQLBaseline.CodePlex.Com
 ). This tool allows you to compare 
 multiple executions side by side and review which run was the most efficient. Figure 
 11-15
  shows that the latest 
 execution returned 624 bytes over the network.",NA
Application Design,"Last, but certainly not least, design choices can have a significant impact on application response time. Certain coding 
 techniques can negatively affect performance, such as excessive roundtrips. Although this may not be noticeable 
 when you’re running the application against a local database, it may turn out to be unacceptable when you’re running 
 against SQL Database.
 The following coding choices may impact your application’s performance:
 •�
 Chatty design.
  As previously mentioned, a chatty application uses excessive roundtrips to the 
 database and creates a significant slowdown. An example of a chatty design includes creating a 
 programmatic loop that makes a call to a database to execute a SQL statement over and over again.
 •�
 Normalization.
  It’s widely accepted that although a highly normalized database reduces data 
 duplication, it also generally decreases performance due to the number of 
 JOIN
 s that must be 
 included. As a result, excessive normalization can become a performance issue.",NA
Summary,"This chapter provided an overview of some of the most important tuning techniques that are available to help you 
 address SQL Database performance issues. As you’ve seen, troubleshooting and tuning statements can be complex 
 and require various tools and methods to obtain the desired outcome. You saw a few dynamic management views and 
 execution plans, took a quick tour of indexing, and briefly touched on the statistics provided by the ADO.NET library. 
 You also learned about some design considerations that can affect application performance.
 You can discover additional techniques and concepts on Microsoft’s web site at  
 http://msdn.microsoft.com/en-us/library/ms190610(v=sql.105).aspx
 .",NA
Chapter 12,NA,NA
Windows Azure Mobile Services,"Since the release of the first edition of this book 2010 the SQL Azure OData service has been removed. That’s not a bad 
 thing. OData is still around, and you can still prop up a WCF data service and communicate via the OData protocol. 
 In fact, if you use WCF Data Services, you might very well be using OData. WCF Data Services supports the OData 
 protocol, which allows you to expose your data as a feed with resources that are addressable by URIs. OData allows 
 you to expose data from a variety of sources, ranging from Microsoft Excel to websites, not just relational databases. 
 Creating a data service that exposes an OData feed really boils down to three simple and basic steps:
 Define the model:
  WCF Data Services natively supports models that are based on the ADO.
 NET Entity Framework.
 Create the data service:
  Define a data service which exposes a class that inherits from the 
 DataService class.
 Configure the data service:
  Configure access to resources (such as tables) and service 
 operations, and define service-wide behavior. WCF Data Services disables access to 
 resources that are exposed by the entity container by default.
 However, this chapter is not about WCF Data Services or OData. Windows Azure SQL Database (AKA SQL Azure) 
 doesn’t natively support OData, as it did during the SQL Azure lab days of the OData service. Instead, in late August 
 2012, Microsoft released (in Preview) the Windows Azure Mobile Services.
 Windows Azure Mobile Services is designed specifically to allow developers to quickly and easily connect their 
 client applications to a cloud-based backend running on Windows Azure, regardless of the platform the client is 
 running on (such as Windows 8 device, Android table, or iOS).
 The beauty of Windows Azure Mobile Services is that it enables you to get up and running extremely quickly by 
 providing a foundation for building a rich client experience. As you will see in this chapter, Windows Azure Mobile 
 Services allows you to connect your application very quickly.
 ■
  
 Note
  Keep in mind that the description here is based on Windows Azure Mobile Services in Preview, meaning that it 
 is not in production at the time of this writing, and some details may change when it is actually released.",NA
Getting Started,"To use Windows Azure Mobile Services, you will need a Windows Azure account. Chapter 1 explains how to sign up 
 for an account, so that process won’t be explained here. Working with Windows Azure Mobile Services is as simple 
 as navigating to the Windows Azure Portal at 
 https://manage.WindowsAzure.com
 . Once you log in with your Live ID, 
 you’ll notice a new tab on the Navigation pane called Mobile Services, as shown in Figure 
 12-1
 .",NA
Data,"The Data tab in the portal simply lists the tables that the available Windows Azure Mobile Services are using. 
 Figure 
 12-11
  shows the table used in the example earlier in the chapter. The three columns show the table name, the 
 number of indexes on the table, and the number of records in each table.
 Figure 12-11.
  Running the project
 Figure 12-12.
  The Browse tab
 Mobile Services tables are quite flexible, so much so that you can simply define a table without any columns 
 (although when the table is created, an 
 id
  column is automatically created for you). When data is initially inserted 
 into the table the service creates the appropriate columns automatically. However, for this functionality to work, the 
 Enable Dynamic Schema option must be set to On. This option can be found on the Configure tab (see Figure 
 12-11
 ), 
 three tabs to the right of the Data tab.
 While on the surface the Data page is simple, there is much, much more that is available when working with 
 tables. Clicking the table name in the portal will take you to a series of pages that allow you to browse the data, work 
 with the columns, define security permissions on the table, and perform additional operations during any CRUD 
 (Create, Read, Update, and Delete) operations. The following sections detail each of the four tabs.",NA
Browse,"The Browse tab (Figure 
 12-12
 ) simply lets you browse data in the selected tables. You can switch between tables by 
 selecting the table in the left navigation bar. If the amount of data exceeds the size of the page, you can use the forward 
 and back arrows to navigate between multiple data pages.",NA
Columns,"The Columns tab (Figure 
 12-13
 ) allows you to manage columns in the table. Columns can be added or deleted. Once 
 a column is created, its data type cannot be changed. To delete a column, select any column other than the indexed 
 column and click the Delete button at the bottom of the portal. Columns can be added in either of two ways—first, by 
 dropping and re-creating the table (not recommended, because of the loss of data), or second, by sending from the 
 application an insert request that includes the new property. To do this, Dynamic Schema must be enabled.
 Figure 12-13.
  The Columns tab
 This tab also allows you to define indexes on the table. By default, an index is placed on the 
 id
  column, but 
 columns can be added to the index to improve query performance. To add a column to the index, select the column 
 and click Set as Index. Columns used regularly in queries for sorting or filtering should be included in the index. Be 
 careful not to add too many columns to the index; this can be detrimental to performance. There are many tools 
 available, including SQL Server Management Studio, that allow you to examine queries that are hitting the database to 
 help determine which columns in the table would be beneficial to add to the index.",NA
Permissions,"The Permissions tab provides the ability to define who has Insert, Update, and Delete permissions. As shown in 
 Figure 
 12-14
 , there are four options:",NA
Script,"The Script tab (Figure 
 12-15
 ) provides the ability to create 
 scriptlets
 , server-side custom logic that will run on each 
 operation (Insert, Update, Delete, and Read). Scriptlets are pieces of JavaScript code you can register to be executed. 
 The name of the function must match the operation selected, except for Delete operations, which must be named 
 del
 .
 Figure 12-16.
  Mobile Services tabs
 These scripts allow the injection of business logic into table operations. For example, client authentication can 
 take place here based on the 
 userIdvalue
  of the supplied 
 userobject
 .",NA
Advanced Settings,"We have talked about Mobile Services from the Data point of view, but there are a few other items that need to be 
 highlighted to give Windows Azure Mobile Services a complete preview. The three settings tabs that we will highlight 
 in this section come from the main page of the Mobile Service and can be seen in Figure 
 12-16
 : Push, Identity, and 
 Scale.",NA
Push,"With Windows Azure Mobile Services you can send push notifications to your Windows Store application by using 
 the Windows Push Notification Service (WNS). Sending push notifications to your application requires that you 
 first configure your mobile service to work with WNS. This is accomplished by simply entering the Client Secret and 
 Package SID values in the Push tab, as shown in Figure 
 12-17
 .
 Figure 12-15.
  The Script tab",NA
Identity,"Windows Azure Mobile Services integrates with a number of different social media, third-party, and Microsoft 
 applications to make it easy to authenticate users from within your application. The integration is supported through 
 a number of identity providers. Currently, Mobile Services supports the following identity providers:
 Microsoft
 •�
 Facebook
 •�
 Twitter
 •�
 Google
 •�
 Figure 
 12-18
  shows the information necessary to configure a Microsoft account, Facebook, and Twitter for 
 Windows Azure Mobile Services. To enable authentication, you will first need to register your application with one of 
 the above-mentioned identity providers to obtain the necessary ID or key and secret and then use that information in 
 this settings window to configure the application to work with that provider.",NA
Scale,"The Scale tab is where you define the resources available to your application. Scaling your mobile service is as simple 
 as switching between Shared and Reserved resource mode and setting the number of instances of the mobile service 
 and the size of those instances, as shown in Figure 
 12-19
 .
 Figure 12-18.
  Identity Provider settings",NA
Summary,"While Windows Azure Mobile Services is still in Preview as this book goes to press, it should be obvious that this 
 feature is quite exciting and really puts the “cherry on top” for Windows Azure. This chapter illustrated how easy it is 
 to store and retrieve data in a Windows Azure SQL Database from a Windows Store application.
 We began this chapter by creating a Windows Azure mobile service and then configured that service to use a 
 Windows Azure SQL Database as the data store. We then downloaded the sample application, and you saw how easy 
 it is to read and write data using Windows Azure Mobile Services, as well as to create an application for the Windows 
 Store.
 Lastly, we took a brief look at some of the configuration settings that can be used within your application, such 
 as social media integration with Facebook and Twitter, and with applications from other providers, such as Microsoft 
 and Google. We also looked at the all-important aspect of scaling your mobile service and how easy it is to do that.",NA
Appendix A,NA,NA
SQL Database Management Portal,"This appendix introduces you to the online SQL Database Management Portal built by Microsoft. It is a web-based 
 application that allows you to design, manage, and monitor your SQL Database instances. Although the current 
 version of the portal does not support all the functions of SQL Server Management Studio, the portal offers some 
 interesting capabilities unique to SQL Database.",NA
Launching the Management Portal,"You can launch the SQL Database Management Portal (SDMP) from the Windows Azure Management Portal. Open a 
 browser and navigate to 
 https://manage.windowsazure.com
 , and then login with your Live ID. Once logged in, click 
 SQL Databases on the left and click on your database from the list of available databases. This brings up the database 
 dashboard, as seen in Figure 
 A-1
 . To launch the management portal, click the Manage icon at the bottom.
 Figure A-1.
  SQL Database dashboard in Windows Azure Management Portal",NA
Administration Features,"The administration section of the portal allows you to manage your federations, monitor current query performance, 
 and run T-SQL queries and optionally view their execution plans. Chapter 10 covered federations at length, so this 
 appendix will focus on the other administration features.",NA
Run T-SQL Statements,"Let’s first run a T-SQL statement and look at the options available to us. From the administration Summary page, 
 click the New Query icon on top of the page. This opens up a new query window. You can open as many windows 
 as needed to run multiple queries as you would in SQL Server Management Studio. Type and execute the following 
 statement (click the Run icon to execute it):
 SELECT * FROM sys.tables
 Running this query returns the list of tables currently found in your database. The query window and its result are 
 shown in Figure 
 A-4
 . The output shows you a grid containing the fields as columns and a row for each table. A scrollbar 
 shows up on the right if needed.  Between the query window and results area, two links are provided: Messages and 
 Results. The Results link is selected by default (it has a little downward arrow on top of it). Clicking on Messages will 
 shows you any associated message for the query, including any error messages. As in the SQL Server Management 
 Studio, if an error is found, the Messages window would be shown by default instead of the Results window.
 Figure A-3.
  Administration Summary screen",NA
View Execution Plans,"You can also view either an estimated execution plan or the actual plan from the same query window. The Actual 
 Plan and Estimated Plan icons are next to the Run icon you clicked previously. With the same two T-SQL statements 
 entered previously, click the Actual Plan icon. When the execution is finished, you will notice a Query Plan link next to 
 Results. Click on this link; you will see the execution plan shown in Figure 
 A-6
 .
 The Sort By icons on the left allow you to pinpoint which operators consume the most resources. By default, the 
 execution plan shows a graphical display highlighting the operators that consume the most CPU and I/O resources. 
 You can click on the CPU or IO icon on the left to show the operators that consume the most CPU or I/O resources, 
 respectively.
 The Find By icons on the left allow you to highlight execution warnings, index scans, or index seeks. Note that 
 table scans cannot exist, because SQL Database does not support heap tables. You can also zoom in or out easily using 
 the vertical bar on the bottom left. Doing so changes the display of the icons when you zoom in enough, as shown  
 in Figure 
 A-7
 .
 Figure A-6.
  Viewing the actual execution plan",NA
Monitor Current Query Performance,"To view the statements recently executed on your database, select the database name on the left panel (EnzoLog2 
 in my example). This will bring you back to a page similar to Figure 
 A-3
 ., where you choose Query Performance on 
 the top. A page similar to Figure 
 A-8
  will appear, showing the list of recent statements that were executed against the 
 database with summary performance metrics. Note that these metrics show averages, not the current values. For 
 example, the second column displays CPU ms/sec, which is an average of CPU consumption in millisecond over the 
 duration of the session, expressed in seconds.
 Figure A-7.
  Zooming in on the Actual execution plan
 Figure A-8.
  Recent T-SQL statements executed against your database",NA
Design Features,"The design section of the portal allows you to manage database objects, including tables, views, and stored 
 procedures. As you will see, the design section offers many advanced graphical options, including the ability to create 
 indexes and foreign keys.",NA
Designing Tables,"To access the design section, click Design at the bottom left of the portal. Notice that the top section of the resulting 
 page shows you three headers: Tables, Views, and Stored Procedures. By default the Tables header is selected, and 
 the page shows you the list of existing tables, as seen in Figure 
 A-10
 . Using the icons on the bottom of this page you 
 can create a new table or drop a selected existing table. You can also filter the list of tables by typing characters in the 
 search box.
 Figure A-9.
  Performance details for a specific T-SQL statement",NA
Designing Views,"To manage your views, click the Views header from the default design page (shown earlier in Figure 
 A-10
 ). You will 
 see a list of existing views. You can delete a view by selecting the view and then clicking Drop View at the bottom. 
 Browsing the list with the mouse will also display the Edit and Dependencies icons discussed previously. You can also 
 filter the list of views by typing characters in the Search box on top.
 Let’s create a view. Click the New view icon at the bottom of the list. A new page allows you to specify the name 
 of the view, its schema (you can change the default schema by clicking on it and selecting from a list of available 
 schemas, if any are available in the database), and the T-SQL of the view in the middle of the page. Figure 
 A-19 
 shows 
 you that I am creating a view called v_DBO_Tables in the dbo schema. Click the Save icon on top to create the view.
 Once the view has been created, the page will display two new headers: Design and Data. The Design header 
 allows you to edit the current view, and the Data header displays the list of records returned by the view, similar to the 
 Data page for tables shown in Figure 
 A-17
 . The difference, however, is that the output of the view is read-only, so you 
 cannot change the data returned by the view.
 Figure A-19.
  Creating a view",NA
Designing Stored Procedures,"To manage your stored procedures, choose Stored Procedures from the design page shown in Figure 
 A-10
 . The list of 
 stored procedures is displayed, and the features available on this page are identical to those explained for the views.
 To create a new stored procedure, choose New Stored Procedure at the bottom of the list. A new page allows you 
 to select the schema of the stored procedure, specify its name, add parameters, and enter the T-SQL of the stored 
 procedure. To add a parameter, click the Add Parameter icon and specify the parameter name, its type (and size), 
 its default value, and whether the parameter is an output (see Figure 
 A-20
 ). Click the Save icon to create the stored 
 procedure.
 If an error occurs, a panel will appear on top of the area you are currently working on; you can expand the 
 message to view the details of the error, as shown in Figure 
 A-21
 . In this example the field 
 author_id
  referenced in the 
 T-SQL code does not exist, preventing the T-SQL from compiling.
 Figure A-20.
  Creating a stored procedure",NA
Summary,"As you surely noticed while reading this appendix, the management portal is designed with simplicity and expansion 
 in mind. You saw how easy it is to manage tables, views, and stored procedures, and how to execute statements 
 while monitoring performance metrics from recently executed statements in your database instance. The SMDP will 
 continue to evolve rapidly; as a result, you can expect differences in behavior and additional features over time.",NA
Appendix B,NA,NA
Windows Azure SQL Database  ,NA,NA
Quick Reference,"SQL Azure supports T-SQL. Chances are that you’re already familiar with SQL Server T-SQL syntax if you’re reading 
 this book. However, not everything you know and love about SQL Server is supported yet in Windows Azure SQL 
 Database. For example, many of the T-SQL statements are hardware-related or OS/server-related, such as creating 
 certificates or creating backup devices. This appendix provides a quick reference to the syntax that is currently 
 supported in Windows Azure SQL Database as of Service Update 4.
 ■
  
 Note
  You can find a complete list and reference that describes T-SQL features supported in Windows Azure SQL 
 Database at 
 http://msdn.microsoft.com/en-us/library/ee336281.aspx
 .",NA
Supported T-SQL Statements,"Table 
 B-1
  lists the supported T-SQL statements that you can use in Windows Azure SQL Database. These statements 
 can be used as exactly as you currently know them without any limitations.
 Table B-1.
  Fully Supported T-SQL Statements
 ALTER ROLE
 DENY
  
 Object Permissions
 ORDER BY
  
 Clause
 ALTER SCHEMA
 DENY
  
 Schema Permissions
 OUTPUT
  
 Clause
 ALTER VIEW
 DROP LOGIN
 OVER
  
 Clause
 APPLOCK_MODE
 DROP PROCEDURE
 PRINT
 APPLOCK_TEST
 DROP ROLE
 RAISERROR
 BEGIN_TRANSACTION
 DROP SCHEMA
 RETURN
 BEGIN...END
 DROP STATISTICS
 REVERT
 BINARY_CHECKSUM
 DROP SYNONYM
 REVOKE
  
 Object Permissions
 BREAK
 DROP TYPE
 REVOKE
  
 Schema Permissions
 CAST and CONVERT
 DROP USER
 ROLLBACK TRANSACTION
 (
 continued
 )",NA
Partially Supported T-SQL Statements,"Table 
 B-2
  lists the partially supported T-SQL statements that you can use in Windows Azure SQL Database. “Partially 
 supported” means you can use these statements, but with some variations (or limitations) to the syntax. Examples are 
 provided following the table.
 CATCH (TRY...CATCH)
 DROP VIEW
 ROLLBACK WORK
 CEILING
 END (BEGIN...END)
 SAVE TRANSACTION
 CHECKSUM
 EXCEPT and INTERSECT
 SELECT @
 local_variable
 CLOSE
 FETCH
 SELECT
  
 Clause
 COALESCE
 FOR
  
 Clause
  
 (XML and BROWSE)
 SET @
 local_variable
 COLUMNPROPERTY
 FROM
 SWITCHOFFSET
 COMMIT TRANSACTION
 GO
 TERTIARY_WEIGHTS
 COMMIT WORK
 GOTO
 THROW
 CONTEXT_INFO
 GRANT
  
 Object Permissions
 TODATETIMEOFFSET
 CONTINUE
 GRANT
  
 Schema Permissions
 TOP
 CONVERT
 GROUP BY
 TRIGGER_NESTLEVEL
 CREATE ROLE
 GROUPING_ID
 TRUNCATE TABLE
 CREATE SCHEMA
 HashBytes
 TRY...CATCH
 CREATE STATISTICS
 HAVING
 UNION
 CREATE VIEW
 Hints (Query, Table, Join, 
 etc.)
 UPDATE
 CRYPT_GEN_RANDOM
 IDENTITY (
 Property
 )
 UPDATE STATISTICS
 CURRENT_REQUEST_ID
 IF...ELSE
 USER
 CURSOR_STATUS
 INSERT BULK
 SWITCHOFFSET
 DBCC SHOW_STATISTICS
 IS [NOT] NULL
 WAITFOR
 DEALLOCATE
 MERGE
 WHERE
 DECLARE @local_variable
 MIN_ACTIVE_ROWVERSION
 WHILE
 DECLARE CURSOR
 OPEN
 WITH (
 Common Table Expression
 )
 DELETE
 OPTION Clause
 Table B-1.
  
 (
 continued
 )",NA
Unsupported T-SQL Statements,"The list of unsupported T-SQL statements is long, but that isn’t as negative a thing as it may appear. In most cases, 
 unsupported statements are operating-system or hardware-related, and they don’t apply in the Windows Azure SQL 
 Database environment.
 Because there are so many unsupported statements, this appendix doesn’t list them all. You can find a complete 
 list at 
 http://msdn.microsoft.com/en-us/library/ee336253.aspx
 . Table 
 B-3
  provides a shorter list, highlighting 
 some unsupported statements that you should particularly be aware of.",NA
Supported Data Types,"If you’ve been following Windows Azure SQL Database since its initial release to the public, you realize that  
 Microsoft has come a long way in supporting much of the functionality and many of the data types found in your  
 local, on-premises instance of SQL Server. Table 
 B-4
  lists those data types currently supported in Windows Azure  
 SQL Database as of Service Update 4.
 Table B-3.
  Unsupported T-SQL Statements
 BACKUP CERTIFICATE
 DBCC CHECKTABLE
 BACKUP MASTER KEY
 DBCC DBREINDEX
 BACKUP SERVICE MASTER KEY
 DBCC DROPCLEANBUFFERS
 CHECKPOINT
 DBCC FREEPROCCACHE
 CONTAINS
 DBCC HELP
 CREATE/DROP AGGREGATE
 DBCC PROCCACHE
 CREATE/DROP RULE
 DBCC SHOWCONTIG
 CREATE/DROP XML INDEX
 DBCC SQLPERF
 CREATE/DROP/ALTER APPLICATION ROLE
 DBCC USEROPTIONS
 CREATE/DROP/ALTER ASSEMBLY
 KILL
 CREATE/DROP/ALTER CERTIFICATE
 NEWSEQUENTIALID
 CREATE/DROP/ALTER DEFAULT
 OPENQUERY
 CREATE/DROP/ALTER FULLTEXT
  (
 CATALOG
 , 
 INDEX
 , 
 STOPLIST)
 OPENXML
 CREATE/DROP/ALTER PARTITION FUNCTION
 RECONFIGURE
 CREATE/DROP/ALTER QUEUE
 RESTORE
 CREATE/DROP/ALTER RESOURCE POOL
 SELECT INTO
  
 Clause
 CREATE/DROP/ALTER SERVICE
 SET ANSI_DEFAULTS
 CREATE/DROP/ALTER XML SCHEMA COLLECTION
 SET ANSI_NULLS
 DBCC CHECKALLOC
 SET ANSI PADDING_OFF
 DBCC CHECKDB
 SET OFFSETS
 DBCC CHECKIDENT
 WITH XML NAMESPACES",NA
Index,NA,NA
n,NA,NA
 A,"Advanced encryption standard (AES), 
 49
 ASP.NET
 deployment in Windows Azure
 dialog box, 
 180
 management interface, 
 180
 package fields, 
 179
 steps, 
 179
 web application, 
 181
 roles, 
 171
 AUTHN process, 
 59",NA
n,NA,NA
 B,"Backup strategies, SQL Azure, 
 67,
  
 95
 automation, copy, 
 97
 blue syntax, 
 97
 cloud services, 
 98
 copy, complete, 
 96
 database copy, 
 95
 DMV, 
 96
 Enzo backup, 
 97
 maintain history, 
 97
 status, copy, 
 96
 third party products, 
 97
 using export import, 
 97
 Business Intelligence Development  
 Studio (BIDS), 
 130",NA
n,NA,NA
 C,"Community Technology Preview, 
 143",NA
n,NA,NA
 D,"Data access layer (DAL), 
 30
 Data management view (DMV), 
 96
 Data migration, 
 67
 bcp utility, 
 92
 bcp import, 
 94
 data export, 
 93
 import data, 
 94
 invoking, 
 92
 output, bcp export, 
 93
 uniqueidentifier, 
 94
 database and data to SQL Azure, 
 67
 bacpac, 
 68
 DAC deployment, 
 70
 DAC Fx, 
 68
 data-tier application framework, 
 68
 deploy to SQL Azure, 
 69
 BACPAC import operation, 
 78
 DAC execution results, 
 71
 DAC export, 
 72
 DAC import, 
 75
 export and import, 
 72
 menu, DAC import, 
 75
 results, DAC export, 
 74
 settings, DAC export, 
 73
 settings, DAC import, 
 76
 specifying database instance, 
 77
 SQL Azure via BACPAC, 
 68
 import/export service, 
 68,
  
 79
 AWMini BACPAC, 
 79
 BLOB storage, 
 79
 buttons in WAMP, 
 80
 cloud storage, 
 81
 completed database dialog, 
 82
 dialog, export, 
 83
 export, 
 82
 import, 
 79
 script wizards, 
 83
 additional errors, 
 89
 advanced options, 
 87
 against Azure database, 
 91",NA
n,NA,NA
 E,"Elastic Compute Cloud (EC2) services, 
 2",NA
n,NA,NA
" F, G","Federation
 components, 
 208
 atomic unit, 
 209
 define, 
 209
 key, 
 209
 member, 
 209
 reference table, 
 209
 root, 
 209
 considerations, 
 216
 limitations, 
 216–217
 shard library, 
 217–218
 creation, split operation, 
 211
 introduction, 
 207
 management, 
 214
 database instance, 
 215
 details, 
 215
 drop, 
 215
 properties, 
 215
 SQL DB portal, 
 214
 view, members, 
 216
 problems, 
 208
 vs
 . sharding
 compressed, 
 208
 linear, 
 208
 scale up, 
 207
 scaling models, 
 207
 usage
 database instance, 
 208
 limitation, 
 208
 members, 
 208
 root database, 
 208
 First federation, 
 210
 ALTER FEDERATION, 
 212
 CREATE FEDERATION, 
 210
 database, 
 210
 FILTERING, 
 213
 foreign key, 
 211
 increasing, 
 213
 members, 
 211–212
 and members, 
 213
 multiples, 
 214
 stored procedure, 
 211
 update record, 
 212
 USE FEDERATION, 
 210",NA
n,NA,NA
 H,"Hardware as a service (HaaS), 
 2
 Health Insurance Portability and Accountability  
 Act (HIPAA), 
 66",NA
n,NA,NA
" I, J, K, L","Infrastructure as a service (IaaS), 
 2",NA
n,NA,NA
" M, N, O","Microsoft Systems Architecture (MSA), 
 2
 Migrating databases, 
 67
 data-tier application framework, 
 68
 deploy to SQL Azure via  
 a BACPAC, 
 68
 BACPAC import operation, 
 78
 DAC deployment, 
 70
 DAC execution, 
 71
 DAC export, 
 72
 DAC import, 
 75
 database instance, 
 77
 database to Azure menu options, 
 69
 directly to SQL Azure, 
 69
 export and import, 
 72
 menu, DAC import, 
 75
 results, DAC export, 
 74
 settings, DAC export, 
 73
 settings, DAC import, 
 76
 import/export service, 
 68",NA
n,NA,NA
" P, Q","Performance designing
 general concept
 asynchronous UI, 
 184
 caching, 
 184
 chatty 
 vs
 . chunky, 
 183
 coding strategies, 
 185–186
 data access layer, 
 183
 in disk, caching, 
 184
 horizontal partition shard, 
 185
 lazy loading, 
 183–184
 OnPassCompleted, 
 185
 parallel processing, 
 185
 read-write shard, 
 185
 shards, 
 185
 task parallel library, 
 185
 UI thread, 
 184
 hadoop for Windows Azure, 
 206
 managing shard, 
 198
 data access, 
 200
 database, table, 
 201
 data duplication, 
 203
 document field, 
 202
 exceptions, 
 198–199
 foreign key constraints, 
 203
 identity values, 
 203
 performance, 
 199
 processing time, 
 200
 referential integrity, 
 204
 table, database, 
 201
 transaction consistency, 
 203
 varbinary column, 
 200
 working with partial, 
 202
 multitenant system
 access control, 
 204
 database transfer, 
 205
 data isolation, 
 204
 schema-based architecture, 
 205
 schema security, 
 204
 shard, 
 186
 adding records, 
 196
 application design, 
 187
 caching, 
 193
 ConfigurationStrings, 
 189
 custom connection, 
 189
 DB connection management, 
 188
 deleting, virtually, 
 195
 ExecuteParallelRounRobinLoad, 
 197
 ExecuteShardQuery, 
 190
 ExecuteSingleQuery, 
 190
 extra parameter, 
 194
 GetHashCode method, 
 188
 GUID, 
 192
 library object, 
 187
 logic, caching, 
 193
 new connection to DB, 
 189
 reading applications, 
 190
 reading from, 
 191
 retrieved objects, 
 191
 returned records, 
 192
 round robin logic, 
 197–198
 sample application, 
 192,
  
 196
 SQLCommand class, 
 187
 SQL server, 
 188
 technology to build, 
 187
 time-to-live, 
 194
 updata and delete records, 
 194
 view connections, 
 190
 Vertical Partition Shards, 
 206
 Performance tuning
 with SQL database, 
 219
 implications, code, 
 220
 methods, 
 219
 tools, 
 219
 typical tuning, 
 219
 techniques, 
 220,
  
 226
 adding index, 
 233
 application code, 
 236
 application performance, 
 239
 balanced plan, 
 235
 chatty design, 
 239
 clustered views, 
 236
 connection pooling, 
 223
 connection release, 
 240
 cost calculation, 
 237
 covering index, 
 232
 design, application, 
 239
 disable, SSMS, 
 227
 dynamic management views, 
 220
 Enzo SQL baseline, 
 239
 execution counts, 
 234
 execution, SSMS, 
 224
 indexed views, 
 235
 indexing, 
 227,
  
 231
 index operators, 
 226
 INNER JOIN operator, 
 226
 JOIN operators, 
 224
 loop operation, 
 234
 management portal, execution, 
 228
 metrics for SQL statement, 
 221
 normalization, 
 239
 operation details, 
 229
 performance metrics, 
 222,
  
 238
 physical operator, index, 
 233
 physical operators, 
 224
 potentials, 
 226
 query performance, 
 230
 record counts, 
 226
 session ID, 
 222",NA
n,NA,NA
 R,"Representational state transfer (REST) call, 
 6
 Research as a service (Raas), 
 2",NA
n,NA,NA
 S,"Security, 
 45
 access control, 
 59
 authentication, 
 59
 AUTHN, SQL, 
 59
 authorization, 
 60
 AUTHZ model, 
 60
 compliance, 
 65
 container, schema, 
 64
 database connection, 
 61
 database roles, 
 60
 error object, 
 62
 HIPAA, 
 66
 internal firewall, 
 65
 Kerberos authentication, 
 59
 login and user creation, 
 60
 login, schema, 
 63
 MyTestLogin account, 
 60
 new schema owner, 
 62
 object’s schema, 
 61
 schemas, 
 61–64
 security model, 
 64
 security, schema model, 
 64
 separation, schema, 
 64
 SQL database firewall, 
 65
 SSPI, 
 59
 TCP 1433 out, 
 65
 certificates, 
 55
 BlueSyntaxTest, 
 56
 CipherText, 
 57
 common name, 
 56
 creation commands, 
 55
 line creation, 
 55
 private key, 
 55
 public key, 
 55
 RSA algorithm, 
 57
 thumbprint property, 
 57
 unique identifier, 
 56
 viewing, 
 56
 X 509, 
 55
 CIA triad, 
 45
 data, 
 48
 AES algorithm, 
 49
 byte array, 
 49
 CipherText, 
 50
 connection string, 
 48
 cryptographic methods, 
 50
 3DES, 
 49
 encryption, 
 48–50
 object model, 
 48
 secret key, 
 50
 SSL encryption, 
 48
 T-SQL statement, 
 49
 framework, 
 45
 availability, 
 46
 confidentiality, 
 45
 database architecture, 
 47
 data integrity, 
 46
 requirements, availability, 
 47
 TDE, 
 45
 hashing, 
 50
 access-control, 
 53
 algorithms, 
 54
 byte array, 
 51
 database, 
 51,
  
 53
 data types, 
 51
 extension methods, 
 51
 HASHBYTES, 
 54
 MD5, 
 54
 N converter, 
 55
 parameter, 
 52
 records, 
 54
 Save() method, 
 54
 SHA-1, 
 54
 UserProperties, 
 51
 utility class, 
 51
 variable, 
 54
 Security.sql, 
 48
 Security support provider interface (SSPI), 
 59",NA
n,NA,NA
" T, U","Tabular Data Stream (TDS) protocol, 
 5
 Transparent data encryption [TDE], 
 45
 Triple data encryption standard (3DES), 
 49",NA
n,NA,NA
 V,"Virtual machine (VM), 
 165",NA
n,NA,NA
" W, X, Y, Z","Windows Azure
 cloud services, 
 165
 affinity group, 
 167
 creation steps, 
 165
 final file, 
 168
 new service creation, 
 167
 system view, 
 166
 unique name, 
 166
 URL, 
 167
 project creation
 configuration, 
 176
 configure and deploy, 
 176–179
 connection string nodes, 
 174
 data source property, 
 174
 deployment model, 
 177
 environment configuring, 
 168
 GridView to SQL DB, 
 173
 local database, 
 178
 run and fetch data, 
 175
 scenario deployment, database, 
 178
 ServiceConfiguration.cscfg, 
 177
 settings, deploy and configure, 
 176
 Visual Studio, 
 169
 SQL database, 
 275
 data types, 
 280–281
 invalid statement, 
 279
 NOT FOR REPLICATION, 
 279
 partial T-SQL support, 
 276
 statements, T-SQL, 
 275
 table syntax, 
 278
 T-SQL support, full, 
 275–279
 unsupported T-SQL statements, 
 280
 validity of statement, 
 277
 Windows Azure Management Portal (WAMP), 
 8
 Windows Azure Mobile Services (WAMS), 
 241
 authentication, 
 253
 data, 
 248
 browse, 
 248
 columns, 
 249
 CRUD (create, read, update and delete), 
 248
 execute, 
 248
 permissions, 
 249
 script, 
 251
 identity provider, 
 252
 initialization, 
 241,
  
 243
 C# project directory, 
 246
 database instance, 
 244
 developing application, 
 245
 on navigation pane, 
 242
 REST services, 
 244
 services in portal, 
 244
 solution, 
 247
 support table, 
 246
 .xaml file, 
 247
 OData services, 
 241
 push notification, 
 251
 reserved mode, 
 254
 scale, 
 253
 services tabs, 
 251
 settings, 
 251
 SQL reports (
 cont.
 )",NA
Pro SQL Database for ,NA,NA
Windows Azure,NA,NA
SQL Server in the Cloud  ,NA,NA
Second Edition,NA,NA
Scott Klein,NA,NA
Herve Roggero,NA,NA
Contents,"Foreword ��������������������������������������������������������������������������������������������������������������������������
 xvii
 About the Authors ��������������������������������������������������������������������������������������������������������������
 xix
 About the Technical Reviewer ������������������������������������������������������������������������������������������
 �xxi
 Acknowledgments �����������������������������������������������������������������������������������������������������������
 xxiii
 Introduction ����������������������������������������������������������������������������������������������������������������������
 xxv
 Chapter 1: Getting Started with SQL Database
 ■
  
  ������������������������������������������������������������������
 1
 Introduction to Cloud Computing ���������������������������������������������������������������������������������������������������
 1
 Who Is Doing What in the Cloud? ���������������������������������������������������������������������������������������������������������������������������
 2
 Typical Cloud Services �������������������������������������������������������������������������������������������������������������������������������������������
 2
 Discovering the Microsoft Azure Platform �������������������������������������������������������������������������������������
 3
 Why Microsoft Azure? ��������������������������������������������������������������������������������������������������������������������������������������������
 3
 About Geographic Locations ����������������������������������������������������������������������������������������������������������������������������������
 4
 Storing Data in Azure ���������������������������������������������������������������������������������������������������������������������������������������������
 5
 SQL Database Primer ��������������������������������������������������������������������������������������������������������������������
 6
 Registering for Azure ���������������������������������������������������������������������������������������������������������������������������������������������
 6
 Creating a SQL Database Instance ������������������������������������������������������������������������������������������������������������������������
 7
 Configuring the Firewall ��������������������������������������������������������������������������������������������������������������������������������������
 10
 Connecting with SQL Server Management Studio �����������������������������������������������������������������������������������������������
 11
 Creating Logins and Users �����������������������������������������������������������������������������������������������������������������������������������
 15
 Assigning Access Rights ��������������������������������������������������������������������������������������������������������������������������������������
 18
 Understanding Billing for SQL Database ��������������������������������������������������������������������������������������������������������������
 18",NA
Foreword,"My journey from box software engineer to cloud service engineer began over four years ago when I moved into 
 the SQL cloud team at Microsoft. I would like to say it was my brilliant foresight that led me to make the jump, but 
 the truth is it was a fortuitous accident resulting from a group reorganization. I couldn’t be more thankful for the 
 opportunity, as working on Windows SQL Azure has been the most interesting and exciting project of my career. 
 The journey to build a relational database as a service was not a straightforward one. We tried several different 
 incarnations and had to back out from a couple of dead ends before we landed on the service you see today.
 We also went through several iterations on the value proposition. What are we building? What is it good for? How 
 does it help customers? There were a lot of naysayers who said a SQL relational database had no place in the cloud. 
 Some thought cloud was just a fad and not really any different from what people already did. We believed differently. 
 We knew you could have the benefits of a relational database system (a well understood model and programming 
 APIs, and an existing rich tooling ecosystem) and combine them with the best of a cloud service (high availability, 
 easy and fast provisioning, drastically reduced management, and a pay as you go model).
 Clearly it is resonating with customers. We are seeing adoption from all sorts of usage scenarios. Some are well known 
 scenarios of simple web sites, departmental enterprise applications. Others are net new scenarios like the one we call 
 data 
 hub
 , which is about sharing between islands of data, such as a company and its outlet sites, a company and its vendors/
 suppliers or between multiple companies that are collaborating. Our most common scenario is a mixture of old and new, 
 the SaaS Line of Business application. This includes cloud-only companies as well as existing box software vendors who 
 are moving their software to the cloud. Moving to an SaaS is a huge advantage for these companies. It allows them to 
 streamline the sales cycle, focus their resources on their core competency, and easily extend their global market reach.
 However, running a cloud service is not the same as building box software. While the business logic of the 
 applications is pretty straightforward to get up and running, especially if you have existing code from an on-prem 
 application; keeping it running well in the cloud is not easy. Security is something that goes beyond compliant algorithms 
 and best practices for your customer. You are on the hook to prevent, or least identify and stop, malicious attacks on your 
 system. You have to make sure you isolate one tenant from another. Availability and performance are things customers 
 take for granted and they are your responsibility. Where there is a problem, you get the call at that wakes you out of bed. 
 Troubleshooting is much harder. Root-causing an issue on a single SQL database can be difficult. How do you do it when 
 you have 1000 SQL databases? There are also model shifts to absorb, like embracing failure and building compensation 
 into your system. One of the hardest ones SQL Database customers struggle with is the notion that they have to scale 
 out instead of scaling up. This is hard because scale up was the preferred pattern in the box world and scaling out is a lot 
 harder. There is also a lot of new stuff to deal with, like multi-tenancy. How do I host 1000 customers? Which parts say 
 isolated and which parts are shared? How do I do billing? How do I measure COGS (and make sure they are low enough!).
 These are not trivial problems. In order to tackle them efficiently you need to understand the capabilities of the 
 platform you are running on. Which of the problems can it can help you to solve and for which ones are you on your 
 own? Authors Scott Klein and Herve Roggero have done a great job walking you through the ins and outs of Windows 
 Azure SQL Database. From Programming Model to Migration Strategies, Security to Scale out. Whether you are 
 focused on using the latest high-level interfaces like WCF and OData or diving into the details of performance, tuning 
 the book covers everything you need to know about SQL Database.
 Rick Negrin
 Lead Program Manager
 Windows Azure SQL Database",NA
About the Authors,"Scott Klein
  is a Windows Azure Technical Evangelist for Microsoft focusing on 
 Windows Azure SQL Database (formally known as SQL Azure) and related cloud-ready 
 data services. He started his career in SQL Server with version 4.2 and has worked 
 with every version since then during his 20+ year career. Prior to joining Microsoft 
 he was a SQL Server MVP and then became one of the first four SQL Azure MVPs. 
 Scott is the author or coauthor of several books for both Wrox and Apress, including 
 this one. Scott can be found talking about Windows Azure SQL Database wherever 
 he can get people to listen to him. You can reach Scott at 
 SQLScott@live.com
  
 and 
 read some of his musings at 
 http://www.scottlklein.com
 . 
 Herve Roggero
 , Windows Azure MVP, is the founder of Blue Syntax Consulting, 
 a company specializing in cloud computing products and services. Herve’s 
 experience includes software development, architecture, database administration 
 and senior management with both global corporations and startup companies. 
 Herve holds multiple certifications, including MCDBA, MCSE, and MCSD. He also 
 holds a Master’s degree in Business Administration from Indiana University. Herve 
 is the co-author of 
 Pro SQL Database for Windows Azure
  from Apress. For more 
 information, 
 visit 
 www.bluesyntax.net
 .",NA
About the Technical Reviewer,"Thomas LaRock
  is a seasoned IT professional with over a decade of technical and 
 management experience. Currently serving as a Technical Evangelist for Confio 
 Software, Thomas has progressed through several roles in his career, including 
 programmer, analyst, and DBA. Thomas holds an MS degree in Mathematics 
 from Washington State University and is a member of the Usability Professionals’ 
 Association. Thomas currently serves on the Board of Directors for the Professional 
 Association for SQL Server (PASS), and is also a SQL Server MVP. Thomas can also 
 be found blogging at 
 http://thomaslarock.com
  and is the author of 
 DBA Survivor: 
 Become a Rock Star DBA
  (
 http://dbasurvivor.com
 ).",NA
Acknowledgments,"This book exists because of the diligence and patience of a handful of individuals to whom we are extremely grateful. 
 First, to Jonathan Gennick at Apress for letting us do a second edition and being a wonderful sounding board for 
 ideas and thoughts regarding this book. Second, to Kevin Shea of Apress and Chris Nelson for keeping us on track, 
 reviewing our work, correcting our grammar, and making the writing of this book quite a delightful process. 
 A very special thanks to Tom LaRock, the technical editor, for his meticulous and detailed work ensuring that the 
 our examples actually work and that the message we are sharing is clear and thorough for the readers. If you ever have 
 an opportunity to meet Tom LaRock, be sure to thank him, and take the opportunity to talk SQL Server with him. You 
 will be glad you did.
 We can’t thank the fine individuals at Microsoft enough for their insight and feedback on many of these chapters. So, 
 a huge thank you to Rick Negin, Mike Morrison, Ariel Netz, and Barclay Hill. It is a pleasure to associate with individuals 
 who have a passion in what they do. We also thank Rick Negrin again for contributing the Foreword for this book. 
 Nothing in this life is worth doing without the love and support of family. Thus, Scott would like to profoundly 
 express thanks to his wife, Lynelle, and his children for their endless love, support, understanding, and LOADS of 
 patience. Professionally, Scott would also like to thank a few of his co-workers for their thoughts, insight, and advice. 
 Cory Fowler, Brady Gaster, and Wenming Yi, thank you!",NA
