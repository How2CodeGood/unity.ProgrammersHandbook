Larger Text,Smaller Text,Symbol
Learning Python Network ,NA,NA
Programming,NA,NA
Utilize Python 3 to get network applications up and ,NA,NA
running quickly and easily,NA,NA
Dr. M. O. Faruque Sarker,NA,NA
Sam Washington,"BIRMINGHAM - MUMBAI
 http://freepdf-books.com",NA
Learning Python Network Programming,"Copyright © 2015 Packt Publishing
 All rights reserved. No part of this book may be reproduced, stored in a retrieval 
 system, or transmitted in any form or by any means, without the prior written 
 permission of the publisher, except in the case of brief quotations embedded in 
 critical articles or reviews.
 Every effort has been made in the preparation of this book to ensure the accuracy 
 of the information presented. However, the information contained in this book is 
 sold without warranty, either express or implied. Neither the authors, nor Packt 
 Publishing, and its dealers and distributors will be held liable for any damages 
 caused or alleged to be caused directly or indirectly by this book.
 Packt Publishing has endeavored to provide trademark information about all of the 
 companies and products mentioned in this book by the appropriate use of capitals. 
 However, Packt Publishing cannot guarantee the accuracy of this information.
 First published: June 2015
 Production reference: 1100615
 Published by Packt Publishing Ltd.
 Livery Place
 35 Livery Street
 Birmingham B3 2PB, UK.
 ISBN 978-1-78439-600-8
 www.packtpub.com
 http://freepdf-books.com",NA
Credits,"Authors
 Dr. M. O. Faruque Sarker
 Sam Washington
 Reviewers
 Konstantin Manchev Manchev
 Vishrut Mehta
 Anhad Jai Singh
 Ben Tasker
 Ilja Zegars
 Commissioning Editor
 Kunal Parikh
 Acquisition Editor
 Kevin Colaco
 Content Development Editor
 Rohit Singh
 Technical Editor
 Saurabh Malhotra
 Copy Editors
 Ameesha Green
 Rashmi Sawant
 Trishla Singh
 Project Coordinator
 Izzat Contractor
 Proofreaders
 Stephen Copestake
 Safis Editing
 Indexer
 Hemangini Bari
 Graphics
 Abhinash Sahu
 Production Coordinator
 Shantanu Zagade
 Cover Work
 Shantanu Zagade
 http://freepdf-books.com",NA
About the Authors,"Dr. M. O. Faruque Sarker
  is a software architect based in London, UK, where 
 he has been shaping various Linux and open source software solutions, mainly on 
 cloud computing platforms, for commercial companies, educational institutions, and 
 multinational consultancies. Over the past 10 years, he has been leading a number 
 of Python software development and cloud infrastructure automation projects. In 
 2009, he started using Python, where he was responsible for shepherding a fleet of 
 miniature E-puck robots at the University of South Wales, Newport, UK. Later, he 
 honed his Python skills, and he was invited to work on the Google Summer of Code 
 (2009/2010) programs for contributing to the BlueZ and Tahoe-LAFS open source 
 projects. He is the author of 
 Python Network Programming Cookbook
 , 
 Packt Publishing
 .
 He received his PhD in multirobot systems from the University of South Wales. He is 
 currently working at University College London. He takes an active interest in cloud 
 computing, software security, intelligent systems, and child-centric education. He 
 lives in East London with his wife, Shahinur, and daughter, Ayesha.
 All praises and thanks to Allah, the God who is the Merciful and the 
 Beneficent. I would not be able to finish this book without the help 
 of God. I would like to thank Packt Publishing's entire team and my 
 coauthor, Sam, who were very friendly and cooperative in this long 
 journey. I would also like to thank my family and friends for their 
 sacrifice of time, encouraging words, and smiles.
 http://freepdf-books.com",NA
About the Reviewers,"Konstantin Manchev Manchev
  is a technical support professional, who has 
 more than 15 years of experience in a wide range of operating systems, database 
 services, scripting, networking, and security in the mobile telecommunication 
 systems. He actively participates in the adaption of various vendor equipment 
 projects to live mobile operator networks.
 He has worked on the following technologies:
 • 
 Mobile systems such as GSM, UMTS, 3G, and WiFi
 • 
 Vendors such as Cisco, ALU, NSN, RedHat, and Canonical
 • 
 Network elements such as MSC, VLR, HLR, MSCS, OCS, NGIN, and PCRF
 • 
 Network protocol suites such as SS#7 and TCP/IP
 • 
 Webpage technologies  such as HTTP, XML, HTML, SOAP, and REST
 • 
 Operating systems such as Linux (Debian, Ubuntu, RHEL, and CentOS), 
 Windows, and Unix
 • 
 Virtualisation and Cloud technologies such as EC2, OpenStack, VMware, 
 VirtualBox, and so on
 • 
 Programming languages such as Perl, Python, awk, bash, C, Delphi, Java, 
 and so on
 • 
 Databases such as MongoDB, InfluxDB, MySQL, MS SQL, Oracle, and so on
 • 
 Monitoring systems such as Nagios, Grafana, Zabbix, and so on
 http://freepdf-books.com",NA
www.PacktPub.com,NA,NA
"Support files, eBooks, discount offers, and more TM","For support files and downloads related to your book, please visit 
 www.PacktPub.com
 .
 Did you know that Packt offers eBook versions of every book published, with PDF 
 and ePub files available? You can upgrade to the eBook version at 
 www.PacktPub.com
  
 and as a print book customer, you are entitled to a discount on the eBook copy. Get in 
 touch with us at 
 service@packtpub.com
  for more details.
 At 
 www.PacktPub.com
 , you can also read a collection of free technical articles, sign 
 up for a range of free newsletters and receive exclusive discounts and offers on Packt 
 books and eBooks.
 https://www2.packtpub.com/books/subscription/packtlib
 Do you need instant solutions to your IT questions? PacktLib is Packt's online digital 
 book library. Here, you can search, access, and read Packt's entire library of books.",NA
Why subscribe?,"• 
 Fully searchable across every book published by Packt
 • 
 Copy and paste, print, and bookmark content
 • 
 On demand and accessible via a web browser",NA
Free access for Packt account holders,"If you have an account with Packt at 
 www.PacktPub.com
 , you can use this to access 
 PacktLib today and view 9 entirely free books. Simply use your login credentials for 
 immediate access.
 http://freepdf-books.com",NA
Table of Contents,"Preface 
 vii
 Chapter 1: Network Programming and Python 
 1
 An introduction to TCP/IP networks 
 2
 IP addresses 
 2
 Network interfaces 
 3
 Assigning IP addresses 
 4
 IP addresses on the Internet 
 4
 Packets 
 5
 Networks 
 7
 Routing with IP 
 9
 DNS 
 10
 The protocol stack or why the Internet is like a cake 
 11
 Layer 4 – TCP and UDP 
 13
 Network ports 
 13
 UDP 
 14
 TCP 
 15
 UDP versus TCP 
 16
 Layer 5 – The application layer 
 16
 On to Python! 
 17
 Network programming with Python 
 17
 Breaking a few eggs 
 17
 Taking it from the top 
 19
 Downloading an RFC 
 20
 Looking deeper 
 21
 Programming for TCP/IP networks 
 24
 Firewalls 
 24
 Network Address Translation 
 25
 IPv6 
 26
 Summary 
 28
 http://freepdf-books.com",NA
Preface,"Welcome to the world of network programming with Python. Python is a  
 full-featured object-oriented programming language with a standard library that 
 includes everything needed to rapidly build powerful network applications. In 
 addition, it has a multitude of third-party libraries and packages that extend Python 
 to every sphere of network programming. Combined with the fun of using Python, 
 with this book, we hope to get you started on your journey so that you master these 
 tools and produce some great networking code.
 In this book, we are squarely targeting Python 3. Although Python 3 is still 
 establishing itself as the successor to Python 2, version 3 is the future of the language, 
 and we want to demonstrate that it is ready for network programming prime time. 
 It offers many improvements over the previous version, many of which improve the 
 network programming experience, with enhanced standard library modules and 
 new additions.
 We hope you enjoy this introduction to network programming with Python.",NA
What this book covers,"Chapter 1
 , 
 Network Programming and Python
 , introduces core networking concepts for 
 readers that are new to networking, and also covers how network programming is 
 approached in Python.
 Chapter 2
 , 
 HTTP and Working with the Web
 , introduces you to the HTTP protocol  
 and covers how we can retrieve and manipulate web content using Python as an 
 HTTP client. We also take a look at the standard library 
 urllib
  and third-party 
 Requests
  modules.
 http://freepdf-books.com",NA
What you need for this book,"This book is aimed at Python 3. While many of the examples will work in Python 
 2, you'll get the best experience working through this book with a recent version of 
 Python 3. At the time of writing, the latest version is 3.4.3, and the examples were 
 tested against this.
 http://freepdf-books.com",NA
Virtual environments,"It is highly recommended that you use Python virtual environments, or ""
 venvs
 "", 
 when you work with this book, and in fact, when doing any work with Python. 
 A venv is an isolated copy of the Python executable and associated files, which 
 provides a separate environment for installing Python modules, independent from 
 the system Python installation. You can have as many venvs as you need, which 
 means that you can have multiple module configurations set up, and you can switch 
 between them easily.
 From version 3.3, Python includes a 
 venv
  module, which provides this functionality. 
 The documentation and examples are available at 
 https://docs.python.org/3/
 using/scripts.html
 . There is also a standalone tool available for earlier versions, 
 which can be found at 
 https://virtualenv.pypa.io/en/latest/
 .",NA
Installing Python 3,"Most major Linux distributions come preinstalled with Python 2. When installing 
 Python 3 on such a system, it is important to note that we're not replacing the 
 installation of Python 2. Many distributions use Python 2 for core system operations, 
 and these will be tuned for the major version of the system Python. Replacing the 
 system Python can have severe consequences for the running of the OS. Instead, 
 when we install Python 3, it is installed side by side with Python 2. After installing 
 Python 3, it is invoked using the 
 python3.x
  executable, where 
 x
  is replaced with the 
 corresponding installed minor version. Most packages also provide a 
 symlink
  to this 
 executable called 
 python3
 , which can be run instead.
 http://freepdf-books.com",NA
Ubuntu and Debian,"Ubuntu 15.04 and 14.04 come with Python 3.4 already installed; so if you're running 
 these versions, you're already good to go. Note that there is a bug in 14.04, which 
 means pip must be installed manually in any venvs created using the bundled 
 venv
  
 module. You can find information on working around this at 
 http://askubuntu.
 com/questions/488529/pyvenv-3-4-error-returned-non-zero-exit-status-1
 .
 For earlier versions of Ubuntu, Felix Krull maintains a repository of up-to-date 
 Python installations for Ubuntu. The complete details can be found at 
 https://
 launchpad.net/~fkrull/+archive/ubuntu/deadsnakes
 .
 On Debian, Jessie has a Python 3.4 package (
 python3.4
 ), which can be installed 
 directly with 
 apt-get
 . Wheezy has a package for 3.2 (
 python3.2
 ), and Squeeze 
 has 
 python3.1
 , which can be installed similarly. In order to get working Python 
 3.4 installations on these latter two, it's easiest to use Felix Krull's repositories for 
 Ubuntu.",NA
"RHEL, CentOS, Scientific Linux","These distributions don't provide up-to-date Python 3 packages, so we need to use a 
 third-party repository. For Red Hat Enterprise Linux, CentOS, and Scientific Linux, 
 Python 3 can be obtained from the community supported Software Collections (SCL) 
 repository. Instructions on using this repository can be found at 
 https://www.
 softwarecollections.org/en/scls/rhscl/python33/
 . At the time of writing, 
 Python 3.3 is the latest available version.
 Python 3.4 is available from another repository, the IUS Community repository, 
 sponsored by Rackspace. Instructions on the installation can be found at  
 https://iuscommunity.org/pages/IUSClientUsageGuide.html
 .",NA
Fedora,"Fedora 21 and 22 provide Python 3.4 with the 
 python3
  package:
 $ yum install python3
 For earlier versions of Fedora, use the repositories listed in the preceding  
 Red Hat section.
 http://freepdf-books.com",NA
Alternative installation methods,"If you're working on a system, which isn't one of the systems mentioned earlier,  
 and you can't find packages for your system to install an up-to-date Python 3, there 
 are still other ways of getting it installed. We'll discuss two methods, 
 Pythonz
   
 and 
 JuJu
 .",NA
Pythonz,"Pythonz is a program that manages the compilation of Python interpreters from 
 source code. It downloads and compiles Python from source and installs the 
 compiled Python interpreters in your home directory. These binaries can then be 
 used to create venvs. The only limitation with this installation method is that you 
 need a build environment (that is, a C compiler and supporting packages) installed 
 on your system, and dependencies to compile Python. If this doesn't come with 
 your distribution, you will need root access to install this initially. The complete 
 instructions can be found at 
 https://github.com/saghul/pythonz
 .",NA
JuJu,"JuJu can be used as a last resort, it allows a working Python 3.4 installation on 
 any system without needing root access. It works by creating a tiny Arch Linux 
 installation in a folder, in your home folder and provides tools that allow us to 
 switch to this installation and run commands in it. Using this, we can install  
 Arch's Python 3.4 package, and you can run Python programs using this.  
 The Arch environment even shares your home folder with your system,  
 so sharing files between environments is easy. The JuJu home page is  
 available at 
 https://github.com/fsquillace/juju
 .
 JuJu should work on any distribution. To install it we need to do this:
 $ mkdir ~/.juju
 $ curl https:// bitbucket.org/fsquillace/juju-repo/raw/master/juju-  
   x86_64.tar.gz | tar -xz -C ~/.juju
 This downloads and extracts the JuJu image to 
 ~/.juju
 . You'll need to replace the 
 x86_64
  with 
 x86
  if you're running on a 32-bit system. Next, set up 
 PATH
  to pick up 
 the JuJu commands:
 $ export PATH=~/.juju/opt/juju/bin:$PATH
 http://freepdf-books.com",NA
Windows,"Compared to some of the older Linux distributions, installing Python 3.4 on 
 Windows is relatively easy; just download the Python 3.4 installer from 
 http://
 www.python.org
  and run it. The only hitch is that it requires administrator privileges 
 to do so, so if you're on a locked down machine, things are trickier. The best solution 
 at the moment is WinPython, which is available at 
 http://winpython.github.io
 .",NA
Other requirements,"We assume that you have a working Internet connection. Several chapters use 
 Internet resources extensively, and there is no real way to emulate these offline. 
 Having a second computer is also useful to explore some networking concepts,  
 and for trying out network applications across a real network.
 http://freepdf-books.com",NA
Who this book is for,"If you're a Python developer, or system administrator with Python experience,  
 and you're looking forward to take your first step in network programming, then 
 this book is for you. Whether you're working with networks for the first time or 
 looking to enhance your existing networking and Python skills, you will find this  
 book very useful.",NA
Conventions,"In this book, you will find a number of text styles that distinguish between different 
 kinds of information. Here are some examples of these styles and an explanation of 
 their meaning.
 Code words in text, database table names, folder names, filenames, file extensions, 
 pathnames, dummy URLs, user input, and Twitter handles are shown as follows: 
 ""IP addresses have been assigned to your computer by running the 
 ip addr
  or 
 ipconfig /all
  command on Windows.""
 A block of code is set as follows:
 import sys, urllib.request
 try:
     rfc_number = int(sys.argv[1])
 except (IndexError, ValueError):
     print('Must supply an RFC number as first argument')
     sys.exit(2)
 template = 'http://www.ietf.org/rfc/rfc{}.txt'
 url = template.format(rfc_number)
 rfc_raw = urllib.request.urlopen(url).read()
 rfc = rfc_raw.decode()
 print(rfc)
 http://freepdf-books.com",NA
Reader feedback,"Feedback from our readers is always welcome. Let us know what you think about 
 this book—what you liked or disliked. Reader feedback is important for us as it helps 
 us develop titles that you will really get the most out of.
 http://freepdf-books.com",NA
Customer support,"Now that you are the proud owner of a Packt book, we have a number of things to 
 help you to get the most from your purchase.",NA
Downloading the example code,"You can download the example code files from your account at 
 http://www.
 packtpub.com
  for all the Packt Publishing books you have purchased. If you 
 purchased this book elsewhere, you can visit 
 http://www.packtpub.com/support
  
 and register to have the files e-mailed directly to you.",NA
Errata,"Although we have taken every care to ensure the accuracy of our content, mistakes 
 do happen. If you find a mistake in one of our books—maybe a mistake in the text or 
 the code—we would be grateful if you could report this to us. By doing so, you can 
 save other readers from frustration and help us improve subsequent versions of this 
 book. If you find any errata, please report them by visiting 
 http://www.packtpub.
 com/submit-errata
 , selecting your book, clicking on the 
 Errata Submission Form
  
 link, and entering the details of your errata. Once your errata are verified, your 
 submission will be accepted and the errata will be uploaded to our website or  
 added to any list of existing errata under the Errata section of that title.
 To view the previously submitted errata, go to 
 https://www.packtpub.com/books/
 content/support
  and enter the name of the book in the search field. The required 
 information will appear under the 
 Errata
  section.",NA
Piracy,"Piracy of copyrighted material on the Internet is an ongoing problem across all 
 media. At Packt, we take the protection of our copyright and licenses very seriously. 
 If you come across any illegal copies of our works in any form on the Internet, please 
 provide us with the location address or website name immediately so that we can 
 pursue a remedy.
 http://freepdf-books.com",NA
Questions,"If you have a problem with any aspect of this book, you can contact us at 
 questions@packtpub.com
 , and we will do our best to address the problem.
 http://freepdf-books.com",NA
Network Programming  ,NA,NA
and Python,"This book will focus on writing programs for networks that use the Internet  
 protocol suite. Why have we chosen to do this? Well, of the sets of protocols 
 supported by the Python standard library, the TCP/IP protocol is by far the most 
 widely employable. It contains the principle protocols used by the Internet. By 
 learning to program for TCP/IP, you'll be learning how to potentially communicate 
 with just about every device that is connected to this great tangle of network cables 
 and electromagnetic waves.
 In this chapter, we will be looking at some concepts and methods around networks 
 and network programming in Python, which we'll be using throughout this book.
 This chapter has two sections. The first section, 
 An introduction to TCP/IP networks
 , 
 offers an introduction to essential networking concepts, with a strong focus on the 
 TCP/IP stack. We'll be looking at what comprises a network, how the 
 Internet 
 Protocol
  (
 IP
 ) allows data transfer across and between networks, and how TCP/IP 
 provides us with services that help us to develop network applications. This section 
 is intended to provide a grounding in these essential areas and to act as a point of 
 reference for them. If you're already comfortable with concepts such as IP addresses, 
 routing, TCP and UDP, and protocol stack layers, then you may wish to skip to 
 second part, 
 Network programming with Python
 .
 In the second part, we'll look at the way in which network programming is 
 approached with Python. We'll be introducing the main standard library modules, 
 looking at some examples to see how they relate to the TCP/IP stack, and then  
 we will be discussing a general approach for finding and employing modules  
 that meet our networking needs. We'll also be taking a look at a couple of general 
 issues that we may encounter, when writing applications that communicate over 
 TCP/IP networks.
 http://freepdf-books.com",NA
An introduction to TCP/IP networks,"The Internet protocol suite, often referred to as TCP/IP, is a set of protocols  
 designed to work together to provide end-to-end transmission of messages  
 across interconnected networks.
 The following discussion is based on 
 Internet Protocol version 4
  (
 IPv4
 ). Since the 
 Internet has run out of IPv4 addresses, a new version, IPv6, has been developed, 
 which is intended to resolve this situation. However, although IPv6 is being used in 
 a few areas, its deployment is progressing slowly and a majority of the Internet will 
 likely be using IPv4 for a while longer. We'll focus on IPv4 in this section, and then 
 we will discuss the relevant changes in IPv6 in second part of this chapter.
 TCP/IP is specified in documents called 
 Requests for Comment
  (
 RFCs
 ) which are  
 published by the 
 Internet Engineering Task Force
  (
 IETF
 ). RFCs cover a wide range 
 of standards and TCP/IP is just one of these. They are freely available on the IETF's 
 website, which can be found at 
 www.ietf.org/rfc.html
 . Each RFC has a number, 
 IPv4 is documented by RFC 791, and other relevant RFCs will be mentioned as  
 we progress.
 Note that you won't learn how to set up your own network in this chapter because 
 that's a big topic and unfortunately, somewhat beyond the scope of this book. But, 
 it should enable you at least to have a meaningful conversation with your network 
 support people!",NA
IP addresses,"So, let's get started with something you're likely to be familiar with, that is,  
 IP addresses. They typically look something like this:
 203.0.113.12
 They are actually a single 32-bit number, though they are usually written just  
 like the number shown in the preceding example; they are written in the form of 
 four decimal numbers that are separated by dots. The numbers are sometimes called 
 octets
  or bytes because each one represents 8-bits of the 32-bit number. As such, each 
 octet can only take values from 0 to 255, so valid IP addresses range from 0.0.0.0 to 
 255.255.255.255. This way of writing IP addresses is called 
 dot-decimal notation.
 http://freepdf-books.com",NA
Network interfaces,"You can find out what IP addresses have been assigned to your computer by running 
 ip addr
  (or 
 ipconfig /all
  on Windows) on a terminal. In 
 Chapter 6
 , 
 IP and DNS
 , 
 we'll see how to do this when using Python.
 If we run one of these commands, then we can see that the IP addresses are assigned 
 to our device's network interfaces. On Linux, these will have names, such as 
 eth0
 ; 
 on Windows these will have phrases, such as 
 Ethernet adapter Local Area 
 Connection
 .
 You will get the following output when you run the 
 ip addr
  command on Linux:
 $ ip addr
 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
     inet 127.0.0.1/8 scope host lo
        valid_lft forever preferred_lft forever
 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast  
     state UP qlen 1000
     link/ether b8:27:eb:5d:7f:ae brd ff:ff:ff:ff:ff:ff
     inet 192.168.0.4/24 brd 192.168.0.255 scope global eth0
        valid_lft forever preferred_lft forever
 In the preceding example, the IP addresses for the interfaces appear after the  
 word 
 inet
 .
 An interface is a device's physical connection to its network media. It could be 
 a network card that connects to a network cable, or a radio that uses a specific 
 wireless technology. A desktop computer may only have a single interface for a 
 network cable, whereas a Smartphone is likely to have at least two interfaces, one for 
 connecting to Wi-Fi networks and one for connecting to mobile networks that use 4G 
 or other technologies.
 http://freepdf-books.com",NA
Assigning IP addresses,"IP addresses can be assigned to a device by a network administrator in one of two 
 ways: statically, where the device's operating system is manually configured with 
 the IP address, or dynamically, where the device's operating system is configured by 
 using the 
 Dynamic Host Configuration Protocol
  (
 DHCP
 ).
 When using DHCP, as soon as the device first connects to a network, it is 
 automatically allocated an address by a DHCP server from a predefined pool. Some 
 network devices, such as home broadband routers provide a DHCP server service 
 out-of-the-box, otherwise a DHCP server must be set up by a network administrator. 
 DHCP is widely deployed, and it is particularly useful for networks where different 
 devices may frequently connect and disconnect, such as public Wi-Fi hotspots or 
 mobile networks.",NA
IP addresses on the Internet,"The Internet is a huge IP network, and every device that sends data over it is 
 assigned an IP address.
 The IP address space is managed by an organization called the 
 Internet Assigned 
 Numbers Authority
  (
 IANA
 ). IANA decides the global allocation of the IP address 
 ranges and assigns blocks of addresses to 
 Regional Internet Registries
  (
 RIRs
 ) 
 worldwide, who then allocate address blocks to countries and organizations. The 
 receiving organizations have the freedom to allocate the addresses from their 
 assigned blocks as they like within their own networks.
 http://freepdf-books.com",NA
Packets,"We'll be talking about network traffic in the following sections, so let's get an idea of 
 what it is.
 Many protocols, including the principle protocols in the Internet protocol suite, 
 employ a technique called 
 packetization
  to help manage data while it's being 
 transmitted across a network.
 When a packetizing protocol is given some data to transmit, it breaks it up into small 
 units — sequences of bytes, typically a few thousand bytes long and then it prefixes 
 each unit with some protocol-specific information. The prefix is called a 
 header,
  and 
 the prefix and data together form a 
 packet
 . The data within a packet is often called 
 its 
 payload
 . 
 http://freepdf-books.com",NA
Networks,"A network is a discrete collection of connected network devices. Networks  
 can vary greatly in scale, and they can be made up of smaller networks. Your 
 network-connected devices at home or the network-connected computers in a  
 large office building are examples of networks.
 There are quite a few ways of defining a network, some loose, some very  
 specific. Depending on the context, networks can be defined by physical  
 boundaries, administrative boundaries, institutional boundaries, or network 
 technology boundaries.
 For this section, we're going to start with a simplified definition of a network,  
 and then work toward a more specific definition, in the form of IP subnets.
 So for our simplified definition, our common defining feature of a network will be 
 that all devices on the network share a single point of connection to the rest of the 
 Internet. In some large or specialized networks, you will find that there is more 
 than one point of connection, but for the sake of simplicity we'll stick to a single 
 connection here.
 This connection point is called a
  
 gateway,
  and usually it takes the form of a special 
 network device called a 
 router
 . The job of a router is to direct traffic between 
 networks. It sits between two or more networks and is said to sit at the boundary of 
 these networks. It always has two or more network interfaces; one for each network 
 it is attached to. A router contains a set of rules called a 
 routing table,
  which tells it 
 how to direct the packets that are passing through it onwards, based on the packets' 
 destination IP addresses.
 The gateway forwards the packets to another router, which is said to be 
 upstream
 , 
 and is usually located at the network's 
 Internet Service Provider
  (
 ISP
 ). The ISP's 
 router falls into a second category of routers, that is, it sits outside the networks 
 described earlier, and routes traffic between network gateways. These routers are 
 run by ISPs and other communications entities. They are generally arranged in tiers, 
 and the upper regional tiers route the traffic for some large sections of countries or 
 continents and form the Internet's backbone. 
 http://freepdf-books.com",NA
Routing with IP,"We mentioned that routers are able to route traffic toward a destination network, 
 and implied that this is somehow done by using IP addresses and routing tables.  
 But what's really going on here?
 One perhaps obvious method for routers to determine the correct router to forward 
 traffic to would be to program every router's routing table with a route for every 
 IP address. However, in practice, with 4 billion plus IP addresses and constantly 
 changing network routes, this turns out to be a completely infeasible method.
 So, how is routing done? The answer lies in another property of IP addresses.  
 An IP address can be interpreted as being made up of two logical parts: a 
 network 
 prefix
  and a 
 host identifier
 . The network prefix uniquely identifies the network a 
 device is on, and the device can use this to determine how to handle traffic that it 
 generates, or receives for forwarding. The network prefix is the first 
 n
  bits of the IP 
 address when it's written out in binary (remember an IP address is really just a 32-bit 
 number). The 
 n
  bits are supplied by the network administrator as a part of a device's 
 network configuration at the same time that it is given its IP address.
 You'll see that 
 n
  is written in one of two ways. It can simply be appended to the IP 
 address, separated by a slash, as follows:
 192.168.0.186/24
 This is called 
 CIDR notation
 . Alternatively, it can be written as a 
 subnet mask
 , 
 which is sometimes just called a 
 netmask
 . This is the way in which you will usually 
 see 
 n
  being specified in a device's network configuration. A subnet mask is a 32-bit 
 number written in dot-decimal notation, just like an IP address.
 255.255.255.0
 This subnet mask is equivalent to 
 /24
 . We get 
 n
  from it by looking at it in binary.  
 A few examples are as follows:
 255.0.0.0       = 11111111 00000000 00000000 00000000 = /8
 255.192.0.0     = 11111111 11000000 00000000 00000000 = /10
 255.255.255.0   = 11111111 11111111 11111111 00000000 = /24
 255.255.255.240 = 11111111 11111111 11111111 11110000 = /28
 n
  is simply the number of 1 bits in the subnet mask. (It's always the leftmost bits that 
 are set to 1 because this allows us to quickly get the Network prefix in binary by 
 doing a bitwise 
 AND
  operation on the IP address and the subnet mask).
 http://freepdf-books.com",NA
DNS,"We've discussed connecting to network devices by using IP addresses. However, 
 unless you work with networks or in systems administration, it is unlikely that you 
 will get to see an IP address very often, even though many of us use the Internet 
 every day. When we browse the web or send an e-mail, we usually connect to servers 
 using host names or domain names. These must somehow map to the servers' IP 
 addresses. But how is this done?
 http://freepdf-books.com",NA
The protocol stack or why the Internet is  ,NA,NA
like a cake,"The Internet Protocol is a member of the set of protocols that make up the Internet 
 protocol suite. Each protocol in the suite has been designed to solve specific 
 problems in networking. We just saw how IP solves the problems of addressing  
 and routing.
 http://freepdf-books.com",NA
Layer 4 – TCP and UDP,"Layer 4 is the first layer that we may want to work with in Python. This layer  
 can employ one of two protocols: the 
 Transmission Control Protocol
  (
 TCP
 ) and  
 the 
 User Datagram Protocol
  (
 UDP
 ). Both of these provide the common service of 
 end-to-end transportation of data between applications on different network devices.",NA
Network ports,"Although IP facilitates the transport of data from one network device to another,  
 it doesn't provide us with a way of letting the destination device know what it 
 should do with the data once it receives it. One possible solution to this would  
 be to program every process running on the destination device to check all of the 
 incoming data to see if they are interested in it, but this would quickly lead to  
 obvious performance and security problems.
 TCP and UDP provide the answer by introducing the concept of 
 ports
 .  
 A port is an endpoint, which is attached to one of the IP addresses assigned to  
 the network device. Ports are claimed by a process running on the device, and  
 the process is then said to be 
 listening
  on that port. Ports are represented by a  
 16-bit number, so that each IP address on a device has 65,535 possible ports that the 
 processes can claim (port number 0 is reserved). Ports can only be claimed by one 
 process at a time, even though a process can claim more than one port at a time.
 When a message is sent over the network through TCP or UDP, the sending 
 application sets the destination port number in the header of the TCP or UDP 
 packet. When the message arrives at the destination, the TCP or UDP protocol 
 implementation running on the receiving device reads the port number and then 
 delivers the message payload to the process that is listening on that port.
 Port numbers need to be known before the messages are sent. The main  
 mechanism for this is convention. In addition to managing the IP address space, 
 it is also the responsibility of IANA to manage the assignment of port numbers to 
 network services.
 A service is a class of application, for example a web server, or a DNS server, which 
 is usually tied to an application protocol. Ports are assigned to services rather than 
 specific applications, because it gives service providers the flexibility to choose what 
 kind of software they want to use to provide a service, without having to worry 
 about the users who would need to look up and connect to a new port number 
 simply because the server has started using Apache instead of IIS, for example.
 http://freepdf-books.com",NA
UDP,"UDP is documented as RFC 768. It is deliberately uncomplicated: it provides no 
 services other than those that we described in the previous section. It just takes 
 the data that we want to send, packetizes it with the destination port number 
 (and optional source port number), and hands it off to the local Internet Protocol 
 implementation for delivery. Applications on the receiving end see the data in the 
 same discrete chunks in which it was packetized.
 Both IP and UDP are what are called connectionless protocols. This means that they 
 attempt to deliver their packets on a best effort basis, but if something goes wrong, 
 then they will just shrug their metaphorical shoulders and move on to delivering 
 the next packet. There is no guarantee that our packets will reach their destinations, 
 and no error notification if a delivery fails. If the packets do make it, then there is no 
 guarantee that they will do so in the same order as they were sent. It's up to a higher 
 layer protocol or the sending application to determine if the packets have arrived 
 and whether to handle any problems. These are protocols in the fire-and-forget style.
 The typical applications of UDP are internet telephony and video streaming.  
 DNS queries are also transported using UDP.
 We'll now look at UDP's more dependable sibling, TCP, and then discuss the 
 differences, and why applications may choose to use one or the other.
 http://freepdf-books.com",NA
TCP,"The Transmission Control Protocol is documented as RFC 761. As opposed to UDP, 
 TCP is a connection based protocol. In such a protocol, no data is sent until the server 
 and the client have performed an initial exchange of control packets. This exchange 
 is called a 
 handshake
 . This establishes a connection, and from then on data can be 
 sent. Each data packet that is received is acknowledged by the receiving party, and 
 it does so by sending a packet called an 
 ACK
 . As such, TCP always requires that the 
 packets include a source port number, because it depends on the continual two-way 
 exchange of messages.
 From an application's point of view, the key difference between UDP and TCP is that 
 the application no longer sees the data in discrete chunks; the TCP connection presents 
 the data to the application as a continuous, seamless stream of bytes. This makes 
 things much simpler if we are sending messages that are larger than a typical packet, 
 however it means that we need to start thinking about 
 framing
  our messages. While 
 with UDP, we can rely on its packetization to provide a means of doing this, with TCP 
 we must decide a mechanism for unambiguously determining where our messages 
 start and end. We'll see more about this in 
 Chapter 8
 , 
 Client and Server Applications
 .
 TCP provides the following services:
 • 
 In-order delivery
 • 
 Receipt acknowledgment
 • 
 Error detection
 • 
 Flow and congestion control
 Data sent through TCP is guaranteed to get delivered to the receiving application in 
 the order that it was sent in. The receiving TCP implementation buffers the received 
 packets on the receiving device and then waits until it can deliver them in the correct 
 order before passing them to the application.
 Because the data packets are acknowledged, sending applications can be sure that 
 the data is arriving and that it is okay to continue sending the data. If an ACK is not 
 received for a sent packet, then within a set time period the packet will be resent. 
 If there's still no response, then TCP will keep resending the packet at increasing 
 intervals, until a second, longer timeout period expires. At this point, it will give up 
 and notify the calling application that it has encountered a problem.
 The TCP header includes a checksum of the header data and the payload.  
 This allows the receiver to verify whether a packet's contents have been modified 
 during the transmission.
 http://freepdf-books.com",NA
UDP versus TCP,"Given the features of TCP, you may be wondering what the use of a connectionless 
 protocol like UDP is. Well, the Internet is still a pretty reliable network, and most 
 of the packets do get delivered. The connectionless protocols are useful where the 
 minimum transfer overhead is required, and where the occasional dropped packet is 
 not a big deal. TCP's reliability and congestion control comes at the cost of needing 
 additional packets and round-trips, and the introduction of deliberate delays when 
 packets are lost in order to prevent congestion. These can drastically increase latency, 
 which is the arch-nemesis of real-time services, while not providing any real benefit 
 for them. A few dropped packets might result in a transient glitch or a drop in signal 
 quality in a media stream, but as long as the packets keep coming, the stream can 
 usually recover.
 UDP is also the main protocol that is used for DNS, which is interesting because 
 most DNS queries fit inside a single packet, so TCP's streaming abilities aren't 
 generally needed. DNS is also usually configured such that it does not depend upon 
 a reliable connection. Most devices are configured with multiple DNS servers, and 
 it's usually quicker to resend a query to a second server after a short timeout rather 
 than wait for a TCP back-off period to expire.
 The choice between UDP and TCP comes down to the message size, whether  
 latency is an issue, and how much of TCP's functionality the application wants  
 to perform itself.",NA
Layer 5 – The application layer,"Finally we come to the top of the stack. The application layer is deliberately left open 
 in the IP protocol suite, and it's really a catch-all for any protocol that is developed 
 by application developers on top of TCP or UDP (or even IP, though these are rarer). 
 Application layer protocols include HTTP, SMTP, IMAP, DNS, and FTP.
 http://freepdf-books.com",NA
On to Python!,"Well, that's it for our rundown of the TCP/IP stack. We'll move on to the next section 
 of this chapter, where we'll look at how to start using Python and how to work with 
 some of the topics we've just covered.",NA
Network programming with Python,"In this section, we're going to look at the general approach to network programming 
 in Python. We'll look at how Python lets us interface with the network stack, how to 
 track down useful modules, and cover some general network programming tips.",NA
Breaking a few eggs,"The power of the layer model of network protocols is that a higher layer can easily 
 build on the services provided by the lower layers and this enables them to add new 
 services to the network. Python provides modules for interfacing with protocols 
 at different levels in the network stack, and modules that support higher-layer 
 protocols follow the aforementioned principle by using the interfaces supplied  
 by the lower level protocols. How can we visualize this?
 Well, sometimes a good way to see inside something like this is by breaking it. So, 
 let's break Python's network stack. Or, more specifically, let's generate a traceback.
 http://freepdf-books.com",NA
Taking it from the top,"Before we start writing code for a new network application, we want to make sure 
 that we're taking as much advantage of the existing stack as possible. This means 
 finding a module that provides an interface to the services that we want to use, 
 and that is as high up the stack as we can find. If we're lucky, someone has already 
 written a module that provides an interface that provides the exact service we need.
 Let's use an example to illustrate this process. Let's write a tool for downloading 
 Request for Comments
  (
 RFC
 ) documents from IETF, and then display them on screen.
 Let's keep the RFC downloader simple. We'll make it a command-line program  
 that just accepts an RFC number, downloads the RFC in text format, and then  
 prints it to 
 stdout
 .
 Now, it's possible that somebody has already written a module for doing this,  
 so let's see if we can find anything.
 The first place we look should always be the Python standard library. The modules 
 in the library are well maintained, and well documented. When we use a standard 
 library module, the users of your application won't need to install any additional 
 dependencies for running it.
 http://freepdf-books.com",NA
Downloading an RFC,"Now we can write our program. For this, create a text file called 
 RFC_downloader.py
  
 and save the following code to it:
 import sys, urllib.request
 try:
     rfc_number = int(sys.argv[1])
 http://freepdf-books.com",NA
Looking deeper,"But, what if HTTP was brand new and there were no modules, such as 
 urllib
 , 
 which we could use to speak HTTP for us? Well, then we would have to step down 
 the stack again and use TCP for our purposes. Let's modify our program according 
 to this scenario, as follows:
 import sys, socket
 try:
     rfc_number = int(sys.argv[1])
 except (IndexError, ValueError):
 http://freepdf-books.com",NA
Programming for TCP/IP networks,"To round up, we're going to look at a few frequently encountered aspects of TCP/
 IP networks that can cause a lot of head-scratching for application developers who 
 haven't encountered them before. These are: firewalls, Network Address Translation, 
 and some of the differences between IPv4 and IPv6.",NA
Firewalls,"A firewall is a piece of hardware or software that inspects the network packets  
 that flow through it and, based on the packet's properties, it filters what it lets 
 through. It is a security mechanism for preventing unwanted traffic from moving 
 from one part of a network to another. Firewalls can sit at network boundaries or  
 can be run as applications on network clients and servers. For example, iptables
   
 is the de facto firewall software for Linux. You'll often find a firewall built into 
 desktop anti-virus programs.
 The filtering rules can be based on any property of the network traffic. The 
 commonly used properties are: the transport layer protocol (that is, whether traffic 
 uses TCP or UDP), the source and destination IP addresses, and the source and 
 destination port numbers.
 A common filtering strategy is to deny all inbound traffic and only allow traffic that 
 matches very specific parameters. For example, a company might have a web server 
 it wants to allow access to from the Internet, but it wants to block all traffic from the 
 Internet that is directed towards any of the other devices on its network. To do so, it 
 would put a firewall directly in front of or behind its gateway, and then configure it 
 to block all incoming traffic, except TCP traffic with the destination IP address of the 
 web server, and the destination port number 80 (since port 80 is the standard port 
 number for the HTTP service).
 http://freepdf-books.com",NA
Network Address Translation,"Earlier, we discussed private IP address ranges. While they are potentially very 
 useful, they come with a small catch. Packets with source or destination addresses 
 in the private ranges are forbidden from being routed over the public Internet! So, 
 without some help, devices using private range addresses can't talk to devices using 
 addresses on the public Internet. However, with 
 Network Address Translation
  
 (
 NAT
 ), we can solve this. Since most home networks use private range addresses, 
 NAT is likely to be something that you'll encounter.
 Although NAT can be used in other circumstances, it is most commonly performed 
 by a gateway at the boundary of the public Internet and a network that is using 
 private range IP addresses. To enable the packets from the gateway's network to 
 be routed on the public Internet as the gateway receives packets from the network 
 that are destined for the Internet, it rewrites the packets' headers and replaces the 
 private range source IP addresses with its own public range IP address. If the packets 
 contain TCP or UDP packets, and these contain a source port, then it may also open 
 up a new source port for listening on its external interface and rewrite the source 
 port number in the packets to match this new number.
 As it does these rewrites, it records the mapping between the newly opened source 
 port and the source device on the internal network. If it receives a reply to the new 
 source port, then it reverses the translation process and sends the received packets to 
 the original device on the internal network. The originating network device shouldn't 
 be made aware of the fact that its traffic is undergoing NAT.
 http://freepdf-books.com",NA
IPv6,"We mentioned that the earlier discussion is based on IPv4, but that there is a new 
 version called IPv6. IPv6 is ultimately designed to replace IPv4, but this process is 
 unlikely to be completed for a while yet.
 Since most Python standard library modules have now been updated to support 
 IPv6 and to accept IPv6 addresses, moving to IPv6 in Python shouldn't have much 
 impact on our applications. However, there are a few small glitches to watch out for.
 The main difference that you'll notice in IPv6 is that the address format has been 
 changed. One of the main design goals of the new protocol was to alleviate the 
 global shortage of IPv4 addresses and to prevent it from happening again the IETF 
 quadrupled the length of an address, to 128 bits, creating a large enough address 
 space to give each human on the planet a billion times as many addresses as there 
 are in the entire IPv4 address space. 
 http://freepdf-books.com",NA
Summary,"In the first part of this chapter, we looked at the essentials of networking with  
 TCP/IP. We discussed the concept of network stacks, and looked at the  
 principle protocols of the Internet protocol suite. We saw how IP solves the  
 problem of sending messages between devices on different networks,  
 and how TCP and UDP provide end-to-end transport between applications.
 In the second section, we looked at how network programming is generally 
 approached when using Python. We discussed the general principle of using 
 modules that interface with services as far up the network stack as we can manage. 
 We also discussed where we might find those modules. We looked at examples 
 of employing modules that interface with the network stack at different layers to 
 accomplish a simple network task.
 Finally, we discussed some common pitfalls of programming for TCP/IP networks 
 and some steps that may be taken to avoid them.
 This chapter has been heavy on the networking theory side of things. But, now  
 it's time to get stuck into Python and put some application layer protocols to  
 work for us.
 http://freepdf-books.com",NA
HTTP and Working  ,NA,NA
with the Web,"The 
 Hypertext Transfer Protocol
  (
 HTTP
 ) is probably the most widely-used 
 application layer protocol. It was originally developed to allow academics to 
 share HTML documents. Nowadays, it is used as the core protocol of innumerable 
 applications across the Internet, and it is the principle protocol of the World  
 Wide Web.
 In this chapter, we will cover the following topics:
 • 
 The HTTP protocol structure
 • 
 Using Python for talking to services through HTTP
 • 
 Downloading files
 • 
 HTTP capabilities, such as compression and cookies
 • 
 Handling errors
 • 
 URLs
 • 
 The Python standard library 
 urllib
  package
 • 
 Kenneth Reitz's third-party 
 Requests
  package
 The 
 urllib
  package is the recommended Python standard library package  
 for HTTP tasks. The standard library also has a low-level module called 
 http
 . 
 Although this offers access to almost all aspects of the protocol, it has not been 
 designed for everyday use. The 
 urllib
  package has a simpler interface, and it  
 deals with everything that we are going to cover in this chapter.
 http://freepdf-books.com",NA
Request and response,"HTTP is an application layer protocol, and it is almost always used on top of TCP. 
 The HTTP protocol has been deliberately defined to use a human-readable message 
 format, but it can still be used for transporting arbitrary bytes data.
 An HTTP exchange consists of two elements. A 
 request
  made by the client, which 
 asks the server for a particular resource specified by a URL, and a 
 response
 , sent by 
 the server, which supplies the resource that the client has asked for. If the server can't 
 provide the resource that the client has requested, then the response will contain 
 information about the failure.
 This order of events is fixed in HTTP. All interactions are initiated by the client. The 
 server never sends anything to the client without the client explicitly asking for it.
 This chapter will teach you how to use Python as an HTTP client. We will learn 
 how to make requests to servers and then interpret their responses. We will look at 
 writing server-side applications in 
 Chapter 9
 , 
 Applications for the Web
 .
 By far, the most widely used version of HTTP is 1.1, defined in RFCs 7230 to 7235. 
 HTTP 2 is the latest version, which was officially ratified just as this book was going 
 to press. Most of the semantics and syntax remain the same between versions 1.1 and 
 2, the main changes are in how the TCP connections are utilised. As of now, HTTP 2 
 isn't widely supported, so we will focus on version 1.1 in this book. If you do want to 
 know more, HTTP 2 is documented in RFCs 7540 and 7541.
 HTTP version 1.0, documented in RFC 1945, is still used by some older softwares. 
 Version 1.1 is backwards-compatible with 1.0 though, and the 
 urllib
  package and 
 Requests
  both support HTTP 1.1, so when we're writing a client with Python we 
 don't need to worry about whether we're connecting to an HTTP 1.0 server. It's just 
 that some more advanced features are not available. Almost all services nowadays 
 use version 1.1, so we won't go into the differences here. The stack overflow question 
 is, a good starting point, if you need further information: 
 http://stackoverflow.
 com/questions/246859/http-1-0-vs-1-1
 .
 http://freepdf-books.com",NA
Requests with urllib,"We have already seen some examples of HTTP exchanges while discussing the RFC 
 downloaders in 
 Chapter 1
 , 
 Network Programming and Python
 . The 
 urllib
  package 
 is broken into several submodules for dealing with the different tasks that we may 
 need to perform when working with HTTP. For making requests and receiving 
 responses, we employ the 
 urllib.request
  module.
 Retrieving the contents of a URL is a straightforward process when done using 
 urllib
 . Load your Python interpreter and do the following:
 >>> from urllib.request import urlopen
 >>> response = urlopen('http://www.debian.org')
 >>> response
 <http.client.HTTPResponse object at 0x7fa3c53059b0>
 >>> response.readline()
 b'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""  
   ""http://www.w3.org/TR/html4/strict.dtd"">\n'
 We use the 
 urllib.request.urlopen()
  function for sending a request and 
 receiving a response for the resource at 
 http://www.debian.org
 , in this case an 
 HTML page. We will then print out the first line of the HTML we receive.",NA
Response objects,"Let's take a closer look at our response object. We can see from the preceding 
 example that 
 urlopen()
  returns an 
 http.client.HTTPResponse
  instance. The 
 response object gives us access to the data of the requested resource, and the 
 properties and the metadata of the response. To view the URL for the response  
 that we received in the previous section, do this:
 >>> response.url
 'http://www.debian.org'
 We get the data of the requested resource through a file-like interface using the 
 readline()
  and 
 read()
  methods. We saw the 
 readline()
  method in the previous 
 section. This is how we use the 
 read()
  method:
 >>> response = urlopen('http://www.debian.org')
 >>> response.read(50)
 b'g=""en"">\n<head>\n  <meta http-equiv=""Content-Type"" c'
 http://freepdf-books.com",NA
Status codes,"What if we wanted to know whether anything unexpected had happened to our 
 request? Or what if we wanted to know whether our response contained any data 
 before we read the data out? Maybe we're expecting a large response, and we want 
 to quickly see if our request has been successful without reading the whole response.
 HTTP responses provide a means for us to do this through 
 status codes
 . We can read 
 the status code of a response by using its 
 status
  attribute.
 >>> response.status
 200
 Status codes are integers that tell us how the request went. The 
 200
  code informs us 
 that everything went fine.
 http://freepdf-books.com",NA
Handling problems,"Status codes help us to see whether our response was successful or not. Any code in 
 the 200 range indicates a success, whereas any code in either the 400 range or the 500 
 range indicates failure.
 Status codes should always be checked so that our program can respond 
 appropriately if something goes wrong. The 
 urllib
  package helps us in  
 checking the status codes by raising an exception if it encounters a problem.
 Let's go through how to catch these and handle them usefully. For this try the 
 following command block:
 >>> import urllib.error
 >>> from urllib.request import urlopen
 >>> try:
 ...   urlopen('http://www.ietf.org/rfc/rfc0.txt')
 ... except urllib.error.HTTPError as e:
 ...   print('status', e.code)
 http://freepdf-books.com",NA
HTTP headers,"Requests, and responses are made up of two main parts, 
 headers
  and a 
 body
 . 
 We briefly saw some HTTP headers when we used our TCP RFC downloader in 
 Chapter 1
 , 
 Network Programming and Python
 . Headers are the lines of protocol-specific 
 information that appear at the beginning of the raw message that is sent over the 
 TCP connection. The body is the rest of the message. It is separated from the headers 
 by a blank line. The body is optional, its presence depends on the type of request or 
 response. Here's an example of an HTTP request:
 GET / HTTP/1.1
 Accept-Encoding: identity
 http://freepdf-books.com",NA
Customizing requests,"To make use of the functionality that headers provide, we add headers to a request 
 before sending it. To do this, we can't just use 
 urlopen()
 . We need to follow  
 these steps:
 • 
 Create a 
 Request
  object
 • 
 Add headers to the request object
 • 
 Use 
 urlopen()
  to send the request object
 We're going to learn how to customize a request for retrieving a Swedish version of 
 the Debian home page. We will use the 
 Accept-Language
  header, which tells the 
 server our preferred language for the resource it returns. Note that not all servers 
 hold versions of resources in multiple languages, so not all servers will respond to 
 Accept-Language
 Linux home page.
 First, we create a 
 Request
  object:
 >>> from urllib.request import Request
 >>> req = Request('http://www.debian.org')
 Next we add the header:
 >>> req.add_header('Accept-Language', 'sv')
 The 
 add_header()
  method takes the name of the header and the contents of the 
 header as arguments. The 
 Accept-Language
  header takes two-letter ISO 639-1 
 language codes. The code for Swedish is 
 sv
 .
 Lastly, we submit the customized request with 
 urlopen()
 :
 >>> response = urlopen(req)
 http://freepdf-books.com",NA
Content compression,"The 
 Accept-Encoding
  request header and the 
 Content-Encoding
  response header 
 can work together to allow us to temporarily encode the body of a response for 
 transmission over the network. This is typically used for compressing the response 
 and reducing the amount of data that needs to be transferred.
 This process follows these steps:
 • 
 The client sends a request with acceptable encodings listed in an 
 Accept-
 Encoding
  header
 • 
 The server picks an encoding method that it supports
 • 
 The server encodes the body using this encoding method
 • 
 The server sends the response, specifying the encoding it has used in a 
 Content-Encoding
  header
 • 
 The client decodes the response body using the specified encoding method
 Let's discuss how to request a document and get the server to use 
 gzip
  compression 
 for the response body. First, let's construct the request:
 >>> req = Request('http://www.debian.org')
 Next, add the 
 Accept-Encoding
  header:
 >>> req.add_header('Accept-Encoding', 'gzip')
 And then, submit it with the help of 
 urlopen()
 :
 >>> response = urlopen(req)
 We can check if the server is using 
 gzip
  compression by looking at the response's 
 Content-Encoding
  header:
 >>> response.getheader('Content-Encoding')
 'gzip'
 We can then decompress the body data by using the 
 gzip
  module:
 >>> import gzip
 >>> content = gzip.decompress(response.read())
 >>> content.splitlines()[:5]
 [b'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""  
   ""http://www.w3.org/TR/html4/strict.dtd"">',
   b'<html lang=""en"">',
 http://freepdf-books.com",NA
Multiple values,"To tell the server that we can accept more than one encoding, add more values to the 
 Accept-Encoding
  header and separate them by commas. Let's try it. We create our 
 Request
  object:
 >>> req = Request('http://www.debian.org')
 Then, we add our header, and this time we include more encodings:
 >>> encodings = 'deflate, gzip, identity'
 >>> req.add_header('Accept-Encoding', encodings)
 Now, we submit the request and then check the response encoding:
 >>> response = urlopen(req)
 >>> response.getheader('Content-Encoding')
 'gzip'
 If needed, relative weightings can be given to specific encodings by adding a 
 q
  value:
 >>> encodings = 'gzip, deflate;q=0.8, identity;q=0.0'
 http://freepdf-books.com",NA
Content negotiation,"Content compression with the 
 Accept-Encoding
  header and language selection 
 with the 
 Accept-Language
  header are examples of 
 content negotiation,
  where the 
 client specifies its preferences regarding the format and the content of the requested 
 resource. The following headers can also be used for this:
 • 
 Accept
 : For requesting a preferred file format
 • 
 Accept-Charset
 : For requesting the resource in a preferred character set
 There are additional aspects to the content negotiation mechanism, but because it's 
 inconsistently supported and it can become quite involved, we won't be covering it 
 in this chapter. RFC 7231 contain all the details that you need. Take a look at sections 
 such as 3.4, 5.3, 6.4.1, and 6.5.6, if you find that your application requires this.",NA
Content types,"HTTP can be used as a transport for any type of file or data. The server can use the 
 Content-Type
  header in a response to inform the client about the type of data that 
 it has sent in the body. This is the primary means an HTTP client determines how it 
 should handle the body data that the server returns to it.
 To view the content type, we inspect the value of the response header,  
 as shown here:
 >>> response = urlopen('http://www.debian.org')
 >>> response.getheader('Content-Type')
 'text/html'
 The values in this header are taken from a list which is maintained by IANA. These 
 values are variously called 
 content types
 , 
 Internet media types
 , or 
 MIME types
  
 (
 MIME
  stands for 
 Multipurpose Internet Mail Extensions
 , the specification  
 in which the convention was first established). The full list can be found at  
 http://www.iana.org/assignments/media-types
 .
 http://freepdf-books.com",NA
User agents,"Another request header worth knowing about is the 
 User-Agent
  header. Any client 
 that communicates using HTTP can be referred to as a 
 user agent
 . RFC 7231 suggests 
 that user agents should use the 
 User-Agent
  header to identify themselves in every 
 request. What goes in there is up to the software that makes the request, though it 
 usually comprises a string that identifies the program and version, and possibly the 
 operating system and the hardware that it's running on. For example, the user agent 
 for my current version of Firefox is shown here:
 Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140722  
   Firefox/24.0 Iceweasel/24.7.0
 Although it has been broken over two lines here, it is a single long string. As you can 
 probably decipher, I'm running Iceweasel (Debian's version of Firefox) version 24 on 
 a 64-bit Linux system. User agent strings aren't intended for identifying individual 
 users. They only identify the product that was used for making the request.
 We can view the user agent that 
 urllib
  uses. Perform the following steps:
 >>> req = Request('http://www.python.org')
 >>> urlopen(req)
 >>> req.get_header('User-agent')
 'Python-urllib/3.4'
 http://freepdf-books.com",NA
Cookies,"A cookie is a small piece of data that the server sends in a 
 Set-Cookie
  header as a 
 part of the response. The client stores cookies locally and includes them in any future 
 requests that are sent to the server.
 Servers use cookies in various ways. They can add a unique ID to them, which 
 enables them to track a client as it accesses different areas of a site. They can store 
 a login token, which will automatically log the client in, even if the client leaves 
 the site and then accesses it later. They can also be used for storing the client's user 
 preferences or snippets of personalizing information, and so on.
 http://freepdf-books.com",NA
Cookie handling,"We're going to discuss how to handle cookies with 
 urllib
 . First, we need to create a 
 place for storing the cookies that the server will send us:
 >>> from http.cookiejar import CookieJar
 >>> cookie_jar = CookieJar()
 Next, we build something called an 
 urllib
  
 opener
 .
  This will automatically extract 
 the cookies from the responses that we receive and then store them in our cookie jar:
 >>> from urllib.request import build_opener, HTTPCookieProcessor
 >>> opener = build_opener(HTTPCookieProcessor(cookie_jar))
 Then, we can use our opener to make an HTTP request:
 >>> opener.open('http://www.github.com')
 Lastly, we can check that the server has sent us some cookies:
 >>> len(cookie_jar)
 2
 Whenever we use 
 opener
  to make further requests, the 
 HTTPCookieProcessor
  
 functionality will check our 
 cookie_jar
  to see if it contains any cookies for that site 
 and then it will automatically add them to our requests. It will also add any further 
 cookies that are received to the cookie jar.
 The 
 http.cookiejar
  module also contains a 
 FileCookieJar
  class, that works in the 
 same way as 
 CookieJar
 , but it provides an additional function for easily saving the 
 cookies to a file. This allows persistence of cookies across Python sessions.
 http://freepdf-books.com",NA
Know your cookies,"It's worth looking at the properties of cookies in more detail. Let's examine the 
 cookies that GitHub sent us in the preceding section.
 To do this, we need to pull the cookies out of the cookie jar. The 
 CookieJar
  module 
 doesn't let us access them directly, but it supports the iterator protocol. So, a quick 
 way of getting them is to create a 
 list
  from it:
 >>> cookies = list(cookie_jar)
 >>> cookies
 [Cookie(version=0, name='logged_in', value='no', ...),
  Cookie(version=0, name='_gh_sess', value='eyJzZxNzaW9uX...', ...)
 ]
 You can see that we have two 
 Cookie
  objects. Now, let's pull out some information 
 from the first one:
 >>> cookies[0].name
 'logged_in'
 >>> cookies[0].value
 'no'
 The cookie's name allows the server to quickly reference it. This cookie is clearly a 
 part of the mechanism that GitHub uses for finding out whether we've logged in yet. 
 Next, let's do the following:
 >>> cookies[0].domain
 '.github.com'
 >>> cookies[0].path
 '/'
 The domain and the path are the areas for which this cookie is valid, so our 
 urllib
  
 opener will include this cookie in any request that it sends to 
 www.github.com
  and 
 its sub-domains, where the path is anywhere below the root.
 Now, let's look at the cookie's lifetime:
 >>> cookies[0].expires
 2060882017
 This is a Unix timestamp; we can convert it to 
 datetime
 :
 >>> import datetime
 >>> datetime.datetime.fromtimestamp(cookies[0].expires)
 datetime.datetime(2035, 4, 22, 20, 13, 37)
 http://freepdf-books.com",NA
Redirects,"Sometimes servers move their content around. They also make some content obsolete 
 and put up new stuff in a different location. Sometimes they'd like us to use the 
 more secure HTTPS protocol instead of HTTP. In all these cases, they may get traffic 
 that asks for the old URLs, and in all these cases they'd probably prefer to be able to 
 automatically send visitors to the new ones.
 The 300 range of HTTP status codes is designed for this purpose. These codes 
 indicate to the client that further action is required on their part to complete the 
 request. The most commonly encountered action is to retry the request at a different 
 URL. This is called a 
 redirect
 .
 We'll learn how this works when using 
 urllib
 . Let's make a request:
 >>> req = Request('http://www.gmail.com')
 >>> response = urlopen(req)
 Simple enough, but now, look at the URL of the response:
 >>> response.url
 'https://accounts.google.com/ServiceLogin?service=mail&passive=true&r  
   m=false...'
 This is not the URL that we requested! If we open this new URL in a browser, 
 then we'll see that it's actually the Google login page (you may need to clear your 
 browser cookies to see this if you already have a cached Google login session). 
 Google redirected us from 
 http://www.gmail.com
  to its login page, and 
 urllib
  
 automatically followed the redirect. Moreover, we may have been redirected more 
 than once. Look at the 
 redirect_dict
  attribute of our request object:
 >>> req.redirect_dict
 {'https://accounts.google.com/ServiceLogin?service=...': 1,  
   'https://mail.google.com/mail/': 1}
 The 
 urllib
  package adds every URL that we were redirected through to this 
 dict
 . 
 We can see that we have actually been redirected twice, first to 
 https://mail.
 google.com
 , and second to the login page.
 When we send our first request, the server sends a response with a redirect status 
 code, one of 301, 302, 303, or 307. All of these indicate a redirect. This response 
 includes a 
 Location
  header, which contains the new URL. The 
 urllib
  package will 
 submit a new request to that URL, and in the aforementioned case, it will receive yet 
 another redirect, which will lead it to the Google login page.
 http://freepdf-books.com",NA
URLs,"Uniform Resource Locators, or 
 URL
 s are fundamental to the way in which the web 
 operates, and they have been formally described in RFC 3986. A URL represents a 
 resource on a given host. How URLs map to the resources on the remote system is 
 entirely at the discretion of the system admin. URLs can point to files on the server, 
 or the resources may be dynamically generated when a request is received.  
 What the URL maps to though doesn't matter as long as the URLs work when  
 we request them.
 URLs are comprised of several sections. Python uses the 
 urllib.parse
  module for 
 working with URLs. Let's use Python to break a URL into its component parts:
 >>> from urllib.parse import urlparse
 >>> result = urlparse('http://www.python.org/dev/peps')
 >>> result
 ParseResult(scheme='http', netloc='www.python.org', path='/dev/peps',  
   params='', query='', fragment='')
 The 
 urllib.parse.urlparse()
  function interprets our URL and recognizes 
 http
  as 
 the 
 scheme
 , 
 www.python.org
  as the 
 network location
 , and 
 /dev/peps
  as the 
 path
 . 
 We can access these components as attributes of the 
 ParseResult
 :
 >>> result.netloc
 'www.python.org'
 >>> result.path
 '/dev/peps'
 For almost all resources on the web, we'll be using the 
 http
  or 
 https
  schemes. In 
 these schemes, to locate a specific resource, we need to know the host that it resides 
 on and the TCP port that we should connect to (together these are the 
 netloc
  
 component), and we also need to know the path to the resource on the host  
 (the 
 path
  component).
 http://freepdf-books.com",NA
Paths and relative URLs,"The path in a URL is anything that comes after the host and the port. Paths always 
 start with a forward-slash (
 /
 ), and when just a slash appears on its own, it's called 
 the 
 root
 . We can see this by performing the following:
 >>> urlparse('http://www.python.org/')
 ParseResult(scheme='http', netloc='www.python.org', path='/',  
   params='', query='', fragment='')
 If no path is supplied in a request, then by default
  urllib
  will send a request for  
 the root.
 When a scheme and a host are included in a URL (as in the previous example), the 
 URL is called an 
 absolute URL
 . Conversely, it's possible to have 
 relative URL
 s, 
 which contain just a path component, as shown here:
 >>> urlparse('../images/tux.png')
 ParseResult(scheme='', netloc='', path='../images/tux.png',  
   params='', query='', fragment='')
 We can see that 
 ParseResult
  only contains a 
 path.
  If we want to use a relative URL 
 to request a resource, then we need to supply the missing scheme, the host, and the 
 base path.
 Usually, we encounter relative URLs in a resource that we've already retrieved from 
 a URL. So, we can just use this resource's URL to fill in the missing components. Let's 
 look at an example.
 http://freepdf-books.com",NA
Query strings,"RFC 3986 defines another property of URLs. They can contain additional parameters 
 in the form of key/value pairs that appear after the path. They are separated from 
 the path by a question mark, as shown here:
 http://docs.python.org/3/search.html?q=urlparse&area=default
 This string of parameters is called a query string. Multiple parameters are separated 
 by ampersands (
 &
 ). Let's see how 
 urlparse
  handles it:
 >>> urlparse('http://docs.python.org/3/search.html?  
   q=urlparse&area=default')
 ParseResult(scheme='http', netloc='docs.python.org',  
   path='/3/search.html', params='', query='q=urlparse&area=default',  
   fragment='')
 So, 
 urlparse
  recognizes the query string as the 
 query
  component.
 Query strings are used for supplying parameters to the resource that we 
 wish to retrieve, and this usually customizes the resource in some way. In the 
 aforementioned example, our query string tells the Python docs search page that  
 we want to run a search for the term 
 urlparse
 .
 The 
 urllib.parse
  module has a function that helps us turn the 
 query
  component 
 returned by 
 urlparse
  into something more useful:
 >>> from urllib.parse import parse_qs
 >>> result = urlparse  
   ('http://docs.python.org/3/search.html?q=urlparse&area=default')
 >>> parse_qs(result.query)
 {'area': ['default'], 'q': ['urlparse']}
 http://freepdf-books.com",NA
URL encoding,"URLs are restricted to the ASCII characters and within this set, a number of 
 characters are reserved and need to be escaped in different components of a URL. 
 We escape them by using something called URL encoding. It is often called 
 percent 
 encoding
 , because it uses the percent sign as an escape character. Let's URL-encode  
 a string:
 >>> from urllib.parse import quote
 >>> quote('A duck?')
 'A%20duck%3F'
 The special characters 
 ' '
  and 
 ?
  have been replaced by escape sequences. The 
 numbers in the escape sequences are the characters' ASCII codes in hexadecimal.
 http://freepdf-books.com",NA
URLs in summary,"There are quite a few functions that we've used in the preceding sections. Let's just 
 review what we have used each function for. All of these functions can be found in 
 the 
 urllib.parse
  module. They are as follows:
 • 
 Splitting a URL into its components: 
 urlparse
 • 
 Combining an absolute URL with a relative URL: 
 urljoin
 • 
 Parsing a query string into a 
 dict
 : 
 parse_qs
 • 
 URL-encoding a path: 
 quote
 • 
 Creating a URL-encoded query string from a 
 dict
 : 
 urlencode
 • 
 Creating a URL from components (reverse of 
 urlparse
 ): 
 urlunparse
 http://freepdf-books.com",NA
HTTP methods,"So far, we've been using requests for asking servers to send web resources to us, but 
 HTTP provides more actions that we can perform. The 
 GET
  in our request lines is 
 an HTTP 
 method
 , and there are several methods, such as 
 HEAD
 , 
 POST
 , 
 OPTION
 , 
 PUT
 , 
 DELETE
 , 
 TRACE
 , 
 CONNECT
 , and 
 PATCH
 .
 We'll be looking at several of these in some detail in the next chapter, but there are 
 two methods, we're going to take a quick look at now.",NA
The HEAD method,"The 
 HEAD
  method is the same as the 
 GET
  method. The only difference is that the 
 server will never include a body in the response, even if there is a valid resource at 
 the requested URL. The 
 HEAD
  method is used for checking if a resource exists or if it 
 has changed. Note that some servers don't implement this method, but when they 
 do, it can prove to be a huge bandwidth saver.
 We use alternative methods with 
 urllib
  by supplying the method name to a 
 Request
  object when we create it:
 >>> req = Request('http://www.google.com', method='HEAD')
 >>> response = urlopen(req)
 >>> response.status
 200
 >>> response.read()
 b''
 Here the server has returned a 
 200 OK
  response, yet the body is empty, as expected.",NA
The POST method,"The 
 POST
  method is in some senses the opposite of the 
 GET
  method. We use the 
 POST
  
 method for sending data to the server. However, in return the server can still send 
 us a full response. The 
 POST
  method is used for submitting user input from HTML 
 forms and for uploading files to a server.
 When using 
 POST
 , the data that we wish to send will go in the body of the request. 
 We can put any bytes data in there and declare its type by adding a 
 Content-Type
  
 header to our request with an appropriate MIME type.
 http://freepdf-books.com",NA
Formal inspection,"In the previous section we used the URL 
 http://search.debian.org/cgibin/
 omega
 , and the dictionary 
 data_dict = {'P': 'Python'}
 . But where did these 
 come from?
 http://freepdf-books.com",NA
HTTPS,"Unless otherwise protected, all HTTP requests and responses are sent in clear text. 
 Anyone with access to the network that the messages travel over can potentially 
 intercept our traffic and read it without hindrance.
 Since the web is used for transferring quite a lot of sensitive data, solutions have 
 been created for preventing eavesdroppers from reading the traffic, even if they  
 are able to intercept it. These solutions, for the most part, employ some form  
 of encryption.
 http://freepdf-books.com",NA
The Requests library,"So that's it for the 
 urllib
  package. As you can see, access to the standard library 
 is more than adequate for most HTTP tasks. We haven't touched upon all of its 
 capabilities. There are numerous handler classes which we haven't discussed, plus 
 the opener interface is extensible.
 However, the API isn't the most elegant, and there have been several attempts made 
 to improve it. One of these is the very popular third-party library called 
 Requests
 . 
 It's available as the 
 requests
  package on PyPi. It can either be installed through  
 Pip or be downloaded from 
 http://docs.python-requests.org
 , which hosts  
 the documentation.
 The 
 Requests
  library automates and simplifies many of the tasks that we've been 
 looking at. The quickest way of illustrating this is by trying some examples.
 The commands for retrieving a URL with 
 Requests
  are similar to retrieving a URL 
 with the 
 urllib
  package, as shown here:
 >>> import requests
 >>> response = requests.get('http://www.debian.org')
 http://freepdf-books.com",NA
Handling errors with Requests,"Errors in 
 Requests
  are handled slightly differently from how they are handled with 
 urllib
 . Let's work through some error conditions and see how it works. Generate a 
 404 error by doing the following:
 >>> response = requests.get('http://www.google.com/notawebpage')
 >>> response.status_code
 404
 In this situation, 
 urllib
  would have raised an exception, but notice that 
 Requests
  
 doesn't. The 
 Requests
  library can check the status code and raise a corresponding 
 exception, but we have to ask it to do so:
 >>> response.raise_for_status()
 ...
 requests.exceptions.HTTPError: 404 Client Error
 Now, try it on a successful request:
 >>> r = requests.get('http://www.google.com')
 >>> r.status_code
 200
 >>> r.raise_for_status()
 None
 It doesn't do anything, which in most situations would let our program exit a 
 try/
 except
  block and then continue as we would want it to.
 What happens if we get an error that is lower in the protocol stack? Try the following:
 >>> r = requests.get('http://192.0.2.1')
 ...
 requests.exceptions.ConnectionError: HTTPConnectionPool(...
 We have made a request for a host that doesn't exist and once it has timed out,  
 we get a 
 ConnectionError
  exception.
 The 
 Requests
  library simply reduces the workload that is involved in using HTTP  
 in Python as compared to 
 urllib
 . Unless you have a requirement for using 
 urllib
 ,  
 I would always recommend using 
 Requests
  for your projects.
 http://freepdf-books.com",NA
Summary,"We looked at the principles of the HTTP protocol. We saw how to perform  
 numerous fundamental tasks with the standard library 
 urllib
  and the  
 third-party 
 Requests
  packages.
 We looked at the structure of HTTP messages, HTTP status codes, the different 
 headers that we may encounter in requests and responses, and how to interpret them 
 and use them for customizing our requests. We looked at how URLs are formed, and 
 how to manipulate and construct them.
 We saw how to handle cookies and redirects, how to handle errors that might occur, 
 and how to use secure HTTP connections.
 We also covered how to submit data to websites in the manner of submitting a  
 form on a web page, and how to extract the parameters that we need from a page's 
 source code.
 Finally, we looked at the third-party 
 Requests
  package. We saw that as compared  
 to the 
 urllib
  package, 
 Requests
 , automates and simplifies many of the tasks that 
 we may routinely need to carry out with HTTP. This makes it a great choice for  
 day-to-day HTTP work.
 In the next chapter, we'll be employing what we've learned here to carry out detailed 
 interactions with different web services, querying APIs for data, and uploading our 
 own objects to the web.
 http://freepdf-books.com",NA
APIs in Action,"When we talk about APIs in relation to Python, we usually refer to the classes and 
 the functions that a module presents to us to interact with. In this chapter, we'll be 
 talking about something different, that is, web APIs.
 A web API is a type of API that you interact with through the HTTP protocol. 
 Nowadays, many web services provide a set of HTTP calls, which are designed to 
 be used programmatically by clients, that is, they are meant to be used by machines 
 rather than by humans. Through these interfaces it's possible to automate interaction 
 with the services and to perform tasks such as extracting data, configuring the 
 service in some way, and uploading your own content into the service.
 In this chapter, we'll look at:
 • 
 Two popular data exchange formats used by web APIs: XML and JSON
 • 
 How to interact with two major web APIs: Amazon S3 and Twitter
 • 
 How to pull data from HTML pages when an API is not available
 • 
 How to make life easier for the webmasters that provide these APIs  
 and websites
 There are hundreds of services that offer web APIs. A quite comprehensive and ever-
 growing list of these services can be found at 
 http://www.programmableweb.com
 .
 We're going to start by introducing how XML is used in Python, and then we will 
 explain an XML-based API called the Amazon S3 API.
 http://freepdf-books.com",NA
Getting started with XML,"The 
 Extensible Markup Language
  (
 XML
 ) is a way of representing hierarchical 
 data in a standard text format. When working with XML-based web APIs, we'll be 
 creating XML documents and sending them as the bodies of HTTP requests and 
 receiving XML documents as the bodies of responses.
 Here's the text representation of an XML document, perhaps this represents the stock 
 at a cheese shop:
 <?xml version='1.0'?>
 <inventory>
     <cheese id=""c01"">
         <name>Caerphilly</name>
         <stock>0</stock>
     </cheese>
     <cheese id=""c02"">
         <name>Illchester</name>
         <stock>0</stock>
     </cheese>
 </inventory>
 If you've coded with HTML before, then this may look familiar. XML is a markup 
 based format. It is from the same family of languages as HTML. The data is 
 structured in an hierarchy formed by elements. Each element is represented by two 
 tags, a start tag, for example, 
 <name>
 , and a matching end tag, for example, 
 </name>
 . 
 Between these two tags, we can either put data, such as 
 Caerphilly
 , or add more 
 tags, which represent child elements.
 Unlike HTML, XML is designed such that we can define our own tags and create our 
 own data formats. Also, unlike HTML, the XML syntax is always strictly enforced. 
 Whereas in HTML small mistakes, such as tags being closed in the wrong order, 
 closing tags missing altogether, or attribute values missing quotes are tolerated,  
 in XML, these mistakes will result in completely unreadable XML documents.  
 A correctly formatted XML document is called well formed.",NA
The XML APIs,"There are two main approaches to working with XML data:
 • 
 Reading in a whole document and creating an object-based representation of 
 it, then manipulating it by using an object-oriented API
 • 
 Processing the document from start to end, and performing actions as 
 specific tags are encountered
 http://freepdf-books.com",NA
The basics of ElementTree,"We'll be using the Python standard library implementation of the 
 ElementTree
  API, 
 which is in the 
 xml.etree.ElementTree
  module.
 Let's see how we may create the aforementioned example XML document by using 
 ElementTree
 . Open a Python interpreter and run the following commands:
 >>> import xml.etree.ElementTree as ET
 >>> root = ET.Element('inventory')
 >>> ET.dump(root)
 <inventory />
 We start by creating the root element, that is, the outermost element of the document. 
 We create a root element 
 <inventory>
  here, and then print its string representation 
 to screen. The 
 <inventory />
  representation is an XML shortcut for 
 <inventory></
 inventory>
 . It's used to show an empty element, that is, an element with no data 
 and no child tags.
 We create the 
 <inventory>
  element by creating a new 
 ElementTree.Element
   
 object. You'll notice that the argument we give to 
 Element()
  is the name of the  
 tag that is created.
 Our 
 <inventory>
  element is empty at the moment, so let's put something in it.  
 Do this:
 >>> cheese = ET.Element('cheese')
 >>> root.append(cheese)
 >>> ET.dump(root)
 <inventory><cheese /></inventory>
 Now, we have an element called 
 <cheese>
  in our 
 <inventory>
  element. When an 
 element is directly nested inside another, then the nested element is called a 
 child
  of 
 the outer element, and the outer element is called the 
 parent
 . Similarly, elements that 
 are at the same level are called 
 siblings
 .
 http://freepdf-books.com",NA
Pretty printing,"It would be useful for us to be able to produce output in a more legible format, such 
 as the example shown at the beginning of this section. The ElementTree API doesn't 
 have a function for doing this, but another XML API, 
 minidom
 , provided by the 
 standard library, does, and it's simple to use. First, import 
 minidom
 :
 >>> import xml.dom.minidom as minidom
 Second, use the following command to print some nicely formatted XML:
 >>> print(minidom.parseString(ET.tostring(root)).toprettyxml())
 <?xml version=""1.0"" ?>
 <inventory>
     <cheese>
       <name>Caerphilly</name>
     </cheese>
 </inventory>
 http://freepdf-books.com",NA
Element attributes,"In the example shown at the beginning of this section, you may have spotted 
 something in the opening tag of the 
 <cheese>
  element, that is, the 
 id=""c01""
  text. 
 This is called an 
 attribute
 . We can use attributes to attach extra information to 
 elements, and there's no limit to the number of attributes an element can have. 
 Attributes are always comprised of an attribute name, which in this case is 
 id
 ,  
 and a value, which in this case is 
 c01
 . The values can be any text, but they must  
 be enclosed in quotes.
 Now, add the 
 id
  attribute to the 
 <cheese>
  element, as shown here:
 >>> cheese.attrib['id'] = 'c01'
 >>> xml_pprint(cheese)
 <?xml version=""1.0"" ?>
 <cheese id=""c01"">
     <name>Caerphilly</name>
 </cheese>
 The 
 attrib
  attribute of an element is a dict-like object which holds an element's 
 attribute names and values. We can manipulate the XML attributes as we would a 
 regular 
 dict
 .
 http://freepdf-books.com",NA
Converting to text,"Once we have an XML tree that we're happy with, usually we would want to 
 convert it into a string to send it over the network. The 
 ET.dump()
  function that 
 we've been using isn't appropriate for this. All the 
 dump()
  function does is print the 
 tag to the screen. It doesn't return a string which we can use. We need to use the 
 ET.tostring()
  function for this, as shown in the following commands:
 >>> text = ET.tostring(name)
 >>> print(text)
 b'<name>Caerphilly</name>'
 Notice that it returns a bytes object. It encods our string for us. The default character 
 set is 
 us-ascii
  but it's better to use UTF-8 for transmitting over HTTP, since it  
 can encode the full range of Unicode characters, and it is widely supported by  
 web applications.
 >>> text = ET.tostring(name, encoding='utf-8')
 For now, this is all that we need to know about creating XML documents, so let's see 
 how we can apply it to a web API.",NA
The Amazon S3 API,"Amazon S3 is a data storage service. It underpins many of today's high-profile  
 web services. Despite offering enterprise-grade resilience, performance and  
 features, it's pretty easy to start with. It is affordable, and it provides a simple  
 API for automated access. It's one of many cloud services in the growing  
 Amazon Web Services
  (
 AWS
 ) portfolio.
 APIs change every now and then, and they are usually given a version number so 
 that we can track them. We'll be working with the current version of the S3 REST 
 API, ""2006-03-01"".
 You'll notice that in the S3 documentation and elsewhere, the S3 web API is referred 
 to as a 
 REST API
 . 
 REST
  stands for 
 Representational State Transfer
 , and it is a fairly 
 academic conception of how HTTP should be used for APIs, originally presented 
 by Roy Fielding in his PhD dissertation. Although the properties that an API should 
 possess so as to be considered RESTful are quite specific, in practice pretty much 
 any API that is based on HTTP is now slapped with the RESTful label. The S3 API is 
 actually among the most RESTful high-profile APIs, because it appropriately uses a 
 good range of the HTTP methods.
 http://freepdf-books.com",NA
Registering with AWS,"Before we can access S3, we need to register with AWS. It is the norm for APIs to 
 require registration before allowing access to their features. You can use either an 
 existing Amazon account or create a new one at 
 http://www.amazonaws.com
 . 
 Although S3 is ultimately a paid-for service, if you are using AWS for the first time, 
 then you will get a year's free trial for low-volume use. A year is plenty of time for 
 finishing this chapter! The trial provides 5GB of free S3 storage.",NA
Authentication,"Next, we need to discuss authentication, which is an important topic of discussion 
 when using many web APIs. Most web APIs we use will specify a way for supplying 
 authentication credentials that allow requests to be made to them, and typically 
 every HTTP request we make must include authentication information.
 APIs require this information for the following reasons:
 • 
 To ensure that others can't abuse your application's access permissions
 • 
 To apply per-application rate limiting
 • 
 To manage delegation of access rights, so that an application can act on the 
 behalf of other users of a service or other services
 • 
 Collection of usage statistics
 All of the AWS services use an HTTP request signing mechanism for authentication. 
 To sign a request, we hash and sign unique data in an HTTP request using a 
 cryptographic key, then add the signature to the request as a header. By recreating 
 the signature on the server, AWS can ensure that the request has been sent by us,  
 and that it doesn't get altered in transit.
 http://freepdf-books.com",NA
Setting up an AWS user,"To use authentication, we need to acquire some credentials.
 We will set this up through the AWS Console. Once you've registered with AWS, log 
 into the Console at 
 https://console.aws.amazon.com
 .
 Once you are logged in, you need to perform the steps shown here:
 1. Click on your name at the top-right, and then choose 
 Security Credentials
 .
 2. Click on 
 Users
 , which is on the list in the left-hand side of the screen, and 
 then click on the 
 Create New Users
  button at the top.
 3. Type in the 
 username
 , and make sure that 
 Generate an access key for each 
 user
  has been checked, and then click on the 
 Create
  button in the bottom 
 right-hand corner.
 You'll see a new page saying that the user has been created successfully. Click on 
 the 
 Download credentials
  button at the bottom right corner to download a CSV file, 
 which contains the 
 Access ID
  and 
 Access Secret
  for this user. These are important 
 because they will help in authenticating ourselves to the S3 API. Make sure that you 
 store them securely, as they will allow full access to your S3 files.
 Then, click on 
 Close
  at the bottom of the screen, and click on the new user in the 
 list that will appear, and then click on the 
 Attach Policy
  button. A list of policy 
 templates will appear. Scroll down this list and select the 
 AmazonS3FullAccess
  
 policy, as shown in the following screenshot:
 http://freepdf-books.com",NA
Regions,"AWS has datacenters around the world, so when we activate a service in AWS we 
 pick the region we want it to live in. There is a list of regions for S3 at 
 http://docs.
 aws.amazon.com/general/latest/gr/rande.html#s3_region
 .
 It's best to choose a region that is closest to the users who will be using the service. 
 For now, you'll be the only user, so just decide on the region that is closest to you for 
 our first S3 tests.
 http://freepdf-books.com",NA
S3 buckets and objects,"S3 organizes the data that we store in it using two concepts: buckets and objects. An 
 object is the equivalent of a file, that is, a blob of data with a name, and a bucket is 
 equivalent to a directory. The only difference between buckets and directories is that 
 buckets cannot contain other buckets.
 Every bucket has its own URL of the form:
 http://<bucketname>.s3-<region>.amazonaws.com
 .
 In the URL, 
 <bucketname>
  is the name of the bucket and 
 <region>
  is the AWS 
 region where the bucket is present, for example 
 eu-west-1
 . The bucket name and 
 region are set when we create the bucket.
 Bucket names are shared globally among all S3 users, and so they must be unique.  
 If you own a domain, then a subdomain of that will make an appropriate bucket 
 name. You could also use your email address by replacing the 
 @
  symbol with a 
 hyphen or underscore.
 Objects are named when we first upload them. We access objects by adding the 
 object name to the end of the bucket's URL as a path. For example, if we have a 
 bucket called 
 mybucket.example.com
  in the 
 eu-west-1
  region containing the 
 object 
 cheeseshop.txt
 , then we can access it by using the URL  
 http://mybucket.
 example.com.s3-eu-west-1.amazonaws.com/cheeseshop.txt
 .
 Let's create our first bucket through the AWS Console. We can perform most of the 
 operations that the API exposes manually through this web interface, and it's a good 
 way of checking that our API client is performing the desired tasks:
 1. Log into the Console at 
 https://console.aws.amazon.com
 .
 2. Go to the S3 service. You will see a page, which will prompt you to  
 create a bucket.
 3. Click on the 
 Create Bucket
  button.
 4. Enter a bucket name, pick a region, and then click on 
 Create
 .
 5. You will be taken to the bucket list, and you will be able to see your bucket.",NA
An S3 command-line client,"Okay, enough preparation, let's get to coding. For the rest of this section on S3, we 
 will be writing a small command line client that will enable us to interact with the 
 service. We will create buckets, and then upload and download files.
 http://freepdf-books.com",NA
Creating a bucket with the API,"Whenever we write a client for an API, our main point of reference is the API 
 documentation. The documentation tells us how to construct the HTTP requests 
 for performing operations. The S3 documentation can be found at 
 http://docs.
 aws.amazon.com/AmazonS3/latest/API/APIRest.html
 . The 
 http://docs.aws.
 amazon.com/AmazonS3/latest/API/RESTBucketPUT.html
  URL will provide the 
 details of bucket creation.
 This documentation tells us that to create a bucket we need to make an HTTP request 
 to our new bucket's endpoint by using the HTTP 
 PUT
  method. It also tells us that the 
 request body must  contain some XML, which specifies the AWS region that we want 
 the bucket to be created in.
 So, now we know what we're aiming for, let's discuss our function. First, let's create 
 the XML. Replace the content of 
 create_bucket()
  with the following code block:
 def create_bucket(bucket):
     XML = ET.Element('CreateBucketConfiguration')
     XML.attrib['xmlns'] = ns
     location = ET.SubElement(XML, 'LocationConstraint')
 http://freepdf-books.com",NA
Uploading a file,"Now that we've created a bucket, we can upload some files. Writing a function for 
 uploading a file is similar to creating a bucket. We check the documentation to see 
 how to construct our HTTP request, figure out what information should be collected 
 at the command line, and then write the function.
 We need to use an HTTP 
 PUT
  again. We need the name of the bucket that we want to 
 store the file in and the name that we want the file to be stored under in S3. The body 
 of the request will contain the file data. At the command line, we'll collect the bucket 
 name, the name we want the file to have in the S3 service and the name of the local 
 file to upload.
 Add the following function to your 
 s3_client.py
  file after the 
 create_bucket()
  
 function:
 def upload_file(bucket, s3_name, local_path):
     data = open(local_path, 'rb').read()
     url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
     r = requests.put(url, data=data, auth=auth)
     
 if r.ok:
         print('Uploaded {} OK'.format(local_path))
     else:
         xml_pprint(r.text)
 In creating this function, we follow a pattern similar to that for creating a bucket:
 1. Prepare the data that will go in the request body.
 2. Construct our URL.
 3. Make the request.
 4. Check the outcome.
 http://freepdf-books.com",NA
Retrieving an uploaded file through a web browser,"By default, S3 applies restrictive permissions for buckets and objects. The account 
 that creates them has full read-write permissions, but access is completely denied 
 for anyone else. This means that the file that we've just uploaded can only be 
 downloaded if the download request includes authentication for our account. If we 
 try the resulting URL in a browser, then we'll get an access denied error. This isn't 
 very useful if we're trying to use S3 for sharing files with other people.
 The solution for this is to use one of S3's mechanisms for changing the permissions. 
 Let's look at the simple task of making our uploaded file public. Change  
 upload_file()
  to the following:
 def upload_file(bucket, s3_name, local_path, acl='private'):
     data = open(local_path, 'rb').read()
     url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
     headers = {'x-amz-acl': acl}
     r = requests.put(url, data=data, headers=headers, auth=auth)
     
 if r.ok:
         print('Uploaded {} OK'.format(local_path))
     else:
         xml_pprint(r.text)
 http://freepdf-books.com",NA
Displaying an uploaded file in a web browser,"If you have uploaded an image, then you may be wondering why the browser had 
 asked us to save it instead of just displaying it. The reason is that we haven't set the 
 file's 
 Content-Type
 .
 If you remember from the last chapter, the 
 Content-Type
  header in an HTTP 
 response tells the client, which in this case is our browser, the type of file that is in 
 the body. By default, S3 applies the content type of 
 binary/octet-stream
 . Because 
 of this 
 Content-Type
 , the browser can't tell that it's downloading an image, so it just 
 presents it as a file that can be saved. We can fix this by supplying a 
 Content-Type
  
 header in the upload request. S3 will store the type that we specify, and it will use it 
 as the 
 Content-Type
  in the subsequent download responses.
 Add the code block shown here to the import at the beginning of 
 s3_client.py
 :
 import mimetypes
 Then change 
 upload_file()
  to this:
 def upload_file(bucket, s3_name, local_path, acl='private'):
     data = open(local_path, 'rb').read()
     url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
     headers = {'x-amz-acl': acl}
     mimetype = mimetypes.guess_type(local_path)[0]
     if mimetype:
         headers['Content-Type'] = mimetype
 http://freepdf-books.com",NA
Downloading a file with the API,"Downloading a file through the S3 API is similar to uploading it. We simply take the 
 bucket name, the S3 object name and the local filename again but issue a 
 GET
  request 
 instead of a 
 PUT request
 , and then write the data received to disk.
 Add the following function to your program, underneath the 
 upload_file()
  function:
 def download_file(bucket, s3_name, local_path):
     url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
     r = requests.get(url, auth=auth)
     if r.ok:
         open(local_path, 'wb').write(r.content)
         print('Downloaded {} OK'.format(s3_name))
     else:
         xml_pprint(r.text)
 Now, run the client and download a file, which you have uploaded previously,  
 by using the following command:
 $ python3.4 s3_client.py download_file mybucket.example.com test.jpg  
   ~/test_downloaded.jpg
 Downloaded test.jpg OK
 http://freepdf-books.com",NA
Parsing XML and handling errors,"If you ran into any errors while running the aforementioned code, then you'll notice 
 that a clear error message will not get displayed. S3 embeds error messages in the 
 XML returned in the response body, and until now we've just been dumping the raw 
 XML to the screen. We can improve on this and pull the text out of the XML. First, 
 let's generate an error message so that we can see what the XML looks like. In  
 s3_client.py
 , replace your access secret with an empty string, as shown here:
 access_secret = ''
 Now, try and perform the following operation on the service:
 $ python3.4 s3_client.py create_bucket failbucket.example.com
 <?xml version=""1.0"" ?>
 <Error>
     <Code>SignatureDoesNotMatch</Code>
     <Message>The request signature we calculated does not match the  
     signature you provided. Check your key and signing  
     method.</Message>
     <AWSAccessKeyId>AKIAJY5II3SZNHZ25SUA</AWSAccessKeyId>
     <StringToSign>AWS4-HMAC-SHA256...</StringToSign>
     <SignatureProvided>e43e2130...</SignatureProvided>
     <StringToSignBytes>41 57 53 34...</StringToSignBytes>
     <CanonicalRequest>PUT...</CanonicalRequest>
     <CanonicalRequestBytes>50 55 54...</CanonicalRequestBytes>
     <RequestId>86F25A39912FC628</RequestId>
     <HostId>kYIZnLclzIW6CmsGA....</HostId>
 </Error>
 The preceding XML is the S3 error information. I've truncated several of the fields 
 so as to show it here. Your code block will be slightly longer than this. In this case, 
 it's telling us that it can't authenticate our request, and this is because we have set a 
 blank access secret.",NA
Parsing XML,"Printing all of the XML is too much for an error message. There's a lot of extraneous 
 information which isn't useful to us. It would be better if we could just pull out the 
 useful parts of the error message and display them.
 Well, 
 ElementTree
  gives us some powerful tools for extracting such information 
 from XML. We're going back to XML for a while to explore these tools a little.
 http://freepdf-books.com",NA
Finding elements,"The simplest way of navigating the tree is by using the elements as iterators.  
 Try doing the following:
 >>> for element in root:
 ...     print('Tag: ' + element.tag)
 Tag: Code
 Tag: Message
 Tag: AWSAccessKeyId
 Tag: StringToSign
 Tag: SignatureProvided
 ...
 Iterating over 
 root
  returns each of its child elements, and then we print out the tag 
 of an element by using the 
 tag
  attribute.
 We can apply a filter to the tags that we iterate over by using the following command:
 >>> for element in root.findall('Message'):
 ...     print(element.tag + ': ' + element.text)
 Message: The request signature we calculated does not match the  
   signature you provided. Check your key and signing method.
 http://freepdf-books.com",NA
Handling errors,"We can go back and add this to our 
 s3_client.py
  file, but let's include a little more 
 information in the output, and structure the code to allow re-use. Add the following 
 function to the file underneath the
  download_file()
  function:
 def handle_error(response):
     output = 'Status code: {}\n'.format(response.status_code)
     root = ET.fromstring(response.text)
     code =  root.find('Code').text
     output += 'Error code: {}\n'.format(code)
     message = root.find('Message').text
     output += 'Message: {}\n'.format(message)
     print(output)
 You'll notice that we have used a new function here, namely, 
 root.find()
 .  
 This works in the same way as 
 findall()
  except that it only returns the first 
 matching element, as opposed to a list of all matching elements.
 Then, replace each instance of 
 xml_pprint(r.text)
  in your file with  
 handle_error(r)
  and then run the client again with the incorrect access  
 secret. Now, you will see a more informative error message:
 $ python3.4 s3_client.py create_bucket failbucket.example.com
 Status code: 403
 Error code: SignatureDoesNotMatch
 Message: The request signature we calculated does not match the  
   signature you provided. Check your key and signing method.",NA
Further enhancements,"That's as far as we're going to take our client. We've written a command line 
 program that can perform essential operations, such as creating buckets and 
 uploading and downloading objects on the Amazon S3 service. There are still 
 plenty of operations that can be implemented, and these can be found in the S3 
 documentation; operations such as listing buckets' contents, deleting objects, and 
 copying objects.
 http://freepdf-books.com",NA
The Boto package,"We've discussed working directly with the S3 REST API, and this has given us some 
 useful techniques that will allow us to program against similar APIs in the future.  
 In many cases, this will be the only way in which we can interact with a web API.
 However, some APIs, including AWS, have ready-to-use packages which expose the 
 functionality of the service without having to deal with the complexities of the HTTP 
 API. These packages generally make the code cleaner and simpler, and they should 
 be preferred for doing production work if they're available.
 The AWS package is called 
 Boto
 . We will take a very quick look at the 
 Boto
  package 
 to see how it can provide some of the functionalities that we wrote earlier.
 The 
 boto
  package is available in PyPi, so we can install it with 
 pip
 :
 $ pip install boto
 Downloading/unpacking boto
 ...
 Now, fire up a Python shell and let's try it out. We need to connect to the service first:
 >>> import boto
 >>> conn = boto.connect_s3('<ACCESS ID>', '<ACCESS SECRET>')
 You'll need to replace 
 <ACCESS ID>
  and 
 <ACCESS SECRET>
  with your access ID and 
 access secret. Now, let's create a bucket:
 >>> conn.create_bucket('mybucket.example.com')
 http://freepdf-books.com",NA
Wrapping up with S3,"So, we've discussed some of the uses of the Amazon S3 API, and learned some things 
 about working with XML in Python. These skills should give you a good start in 
 working with any XML based REST API, whether or not it has a pre-built library  
 like 
 boto
 .
 However, XML isn't the only data format that is used by web APIs, and the S3 way 
 of working with HTTP isn't the only model used by web APIs. So, we're going to 
 move on and take a look at the other major data format in use today, JSON and 
 another API: Twitter.",NA
JSON,"JavaScript Object Notation (JSON)
  is a standard way of representing simple objects, 
 such as 
 lists
  and 
 dicts
 , in the form of text strings. Although, it was originally 
 developed for JavaScript, JSON is language independent and most languages can 
 work with it. It's lightweight, yet flexible enough to handle a broad range of data. 
 This makes it ideal for exchanging data over HTTP, and a large number of web APIs 
 use this as their primary data format.
 http://freepdf-books.com",NA
Encoding and decoding,"We use the 
 json
  module for working with JSON in Python. Let's create a JSON 
 representation of a Python list by using the following commands:
 >>> import json
 >>> l = ['a', 'b', 'c']
 >>> json.dumps(l)
 '[""a"", ""b"", ""c""]'
 We use the 
 json.dumps()
  function for converting an object to a JSON string.  
 In this case, we can see that the JSON string appears to be identical to Python's  
 own representation of a list, but note that this is a string. Confirm this by doing  
 the following:
 >>> s = json.dumps(['a', 'b', 'c'])
 >>> type(s)
 <class 'str'>
 >>> s[0]
 '['
 Converting JSON to a Python object is also straightforward, as shown here:
 >>> s = '[""a"", ""b"", ""c""]'
 >>> l = json.loads(s)
 >>> l
 ['a', 'b', 'c']
 >>> l[0]
 'a'
 We use the 
 json.loads()
  function, and just pass it a JSON string. As we'll see, this 
 is very powerful when interacting with web APIs. Typically, we will receive a JSON 
 string as the body of an HTTP response, which can simply be decoded using  
 json.loads()
  to provide immediately usable Python objects.",NA
Using dicts with JSON,"JSON natively supports a mapping-type object, which is equivalent to a Python 
 dict
 . 
 This means that we can work directly with 
 dicts
  through JSON.
 >>> json.dumps({'A':'Arthur', 'B':'Brian', 'C':'Colonel'})
 '{""A"": ""Arthur"", ""C"": ""Colonel"", ""B"": ""Brian""}'
 http://freepdf-books.com",NA
Other object types,"JSON cleanly handles only Python 
 lists
  and 
 dicts
 , for other object types 
 json
  may 
 attempt to cast the object type as one or the other, or fail completely. Try a tuple,  
 as shown here:
 >>> json.dumps(('a', 'b', 'c'))
 '[""a"", ""b"", ""c""]'
 http://freepdf-books.com",NA
The Twitter API,"The Twitter API provides access to all the functions that we may want a Twitter 
 client to perform. With the Twitter API, we can create clients that search for recent 
 tweets, find out what's trending, look up user details, follow users' timelines, and 
 even act on the behalf of users by posting tweets and direct messages for them.
 We'll be looking at Twitter API version 1.1, the version current at time of writing  
 this chapter.
 Twitter maintains comprehensive documentation for its API, 
 which can be found at 
 https://dev.twitter.com/overview/
 documentation
 .
 http://freepdf-books.com",NA
A Twitter world clock,"To illustrate some of the functionalities of the Twitter API, we're going to write 
 the code for a simple Twitter world clock. Our application will periodically poll its 
 Twitter account for mentions which contain a recognizable city name, and if it finds 
 one, then it will reply to the Tweet with the current local time of that city. In Twitter 
 speak, a mention is any Tweet which includes our account name prefixed by an 
 @
 , for 
 example, 
 @myaccount
 .",NA
Authentication for Twitter,"Similar to S3, we need to determine how authentication will be managed before we 
 get started. We need to register, and then we need to find out how Twitter expects us 
 to authenticate our requests.",NA
Registering your application for the Twitter API,"We need to create a Twitter account, register our application against the account, 
 and then we will receive the authentication credentials for our app. It's also a good 
 idea to set up a second account, which we can use for sending test tweets to the 
 application account. This provides for a cleaner way of checking whether the app is 
 working properly, rather than having the app account send tweets to itself. There's 
 no limit on the number of Twitter accounts that you can create.
 To create an account, go to 
 http://www.twitter.com
  and complete the signup 
 process. Do the following for registering your application once you have a  
 Twitter account:
 1. Log into 
 http://apps.twitter.com
  with your main Twitter account, and 
 then create a new app.
 2. Fill out the new app form, note that Twitter application names need to be 
 unique globally.
 3. Go to the app's settings and then change the app permissions to have read 
 and write access. You may need to register your mobile number for enabling 
 this. Even if you're unhappy about supplying this, we can create the full app; 
 however the final function that sends a tweet in reply won't be active.
 Now we need to get our access credentials, as shown here:
 1. Go to the 
 Keys and Access Tokens
  section and then note the 
 Consumer Key
  
 and the 
 Access Secret
 .
 2. Generate an 
 Access Token
 .
 3. Note down the 
 Access Token
  and the 
 Access Secret
 .
 http://freepdf-books.com",NA
Authenticating requests,"We now have enough information for authenticating requests. Twitter uses an 
 authentication standard called 
 oAuth, 
 version 1.0a. It's described in detail at  
 http://oauth.net/core/1.0a/
 .
 The oAuth authentication standard is a little tricky, but fortunately the 
 Requests
  
 module has a companion library called 
 requests-oauthlib
 , which can handle most 
 of the complexity for us. This is available on PyPi, so we can download and install it 
 with 
 pip
 .
 $ pip install requests-oauthlib
 Downloading/unpacking requests-oauthlib
 ...
 Now, we can add authentication to our requests, and then write our application.",NA
A Twitter client,"Save the code mentioned here to a file, and save it as 
 twitter_worldclock.py
 . 
 You'll need to replace 
 <CONSUMER_KEY>
 , 
 <CONSUMER_SECRET>
 , 
 <ACCESS_TOKEN>
 , 
 and 
 <ACCESS_SECRET>
  with the values that you have taken down from the 
 aforementioned Twitter app configuration:
 import requests, requests_oauthlib, sys
 consumer_key = '<CONSUMER_KEY>'
 consumer_secret = '<CONSUMER_SECRET>'
 access_token = '<ACCESS_TOKEN>'
 access_secret = '<ACCESS_KEY>'
 def init_auth():
     auth_obj = requests_oauthlib.OAuth1(
                     consumer_key, consumer_secret,
                     access_token, access_secret)
     if verify_credentials(auth_obj):
         print('Validated credentials OK')
         return auth_obj
     else:
         print('Credentials validation failed')
         sys.exit(1) 
 def verify_credentials(auth_obj):
     url = 'https://api.twitter.com/1.1/' \
           'account/verify_credentials.json'
     response = requests.get(url, auth=auth_obj)
     return response.status_code == 200
 http://freepdf-books.com",NA
Polling for Tweets,"Let's add a function for checking and retrieving new tweets from our mentions 
 timeline. We'll get this to work before we add the loop. Add the new function 
 underneath 
 verify_credentials()
 , and then add a call this function to the main 
 section, as shown here; also, add 
 json
  to the list of the imports at the beginning of 
 the file:
 def get_mentions(since_id, auth_obj):
     params = {'count': 200, 'since_id': since_id,
               'include_rts':  0, 'include_entities': 'false'}
     url = 'https://api.twitter.com/1.1/' \
           'statuses/mentions_timeline.json'
     response = requests.get(url, params=params, auth=auth_obj)
     response.raise_for_status()
     return json.loads(response.text)
 if __name__ == '__main__':
     auth_obj = init_auth()
     since_id = 1
     for tweet in get_mentions(since_id, auth_obj):
         print(tweet['text'])
 Using 
 get_mentions()
 , we check for and download any tweets that mention our 
 app account by connecting to the 
 statuses/mentions_timeline.json
  endpoint. 
 We supply a number of parameters, which 
 Requests
  passes on as a query string. 
 These parameters are specified by Twitter and they control how the tweets will be 
 returned to us. They are as follows:
 • 
 'count'
 : This specifies the maximum number of tweets that will be returned. 
 Twitter will allow 200 tweets to be received by a single request made to this 
 endpoint.
 • 
 'include_entities'
 : This is used for trimming down some extraneous 
 information from the tweets retrieved.
 • 
 'include_rts'
 : This tells Twitter not to include any retweets. We don't want 
 the user to receive another time update if someone retweets our reply.
 http://freepdf-books.com",NA
Processing the Tweets,"The next step is to parse our mentions and then generate the times that we want  
 to include in our replies. Parsing is a straightforward process. In this, we just  
 check the 'text' value of the tweets, but it takes a little more work to generate the 
 times. In fact, for this, we'll need a database of cities and their time zones. This is 
 available in the 
 pytz
  package, which can be found at PyPi. For doing this, install  
 the following package:
 $ pip install pytz
 Downloading/unpacking pytz
 ...
 And then, we can write our tweet processing function. Add this function underneath 
 get_mentions()
 , and then add 
 datetime
  and 
 pytz
  to the list of the imports at the 
 beginning of the file:
 def process_tweet(tweet):
     username = tweet['user']['screen_name']
     text = tweet['text']
     words = [x for x in text.split() if
                         x[0] not in ['@', '#']]
     place = ' '.join(words)
     check = place.replace(' ', '_').lower()
     found = False
     for tz in pytz.common_timezones:
         tz_low = tz.lower()
         if check in tz_low.split('/'):
 http://freepdf-books.com",NA
Rate limits,"If we run the aforementioned several times, then we'll find that it will stop working 
 after a while. Either the credentials will temporarily fail to validate, or the HTTP 
 request in 
 get_mentions()
  will fail.
 http://freepdf-books.com",NA
Sending a reply,"The final function that we need to perform is sending a tweet in response to a 
 mention. For this, we use the 
 statuses/update.json
  endpoint. If you've not 
 registered your mobile number with your app account, then this won't work. So, just 
 leave your program as it is. If you have registered your mobile number, then add this 
 function under 
 process_tweets()
 :
 def post_reply(reply_to_id, text, auth_obj):
     params = {
 http://freepdf-books.com",NA
Final touches,"The building blocks are in place, and we can add our main loop to make the program 
 a daemon. Add the 
 time
  module to the imports at the top, and then change the main 
 section to what is shown here:
 if __name__ == '__main__':
     auth_obj = init_auth()
     since_id = 1
     error_count = 0
     while error_count < 15:
         try:
             for tweet in get_mentions(since_id, auth_obj):
                 process_tweet(tweet)
                 since_id = max(since_id, tweet['id'])
             error_count =  0
         except requests.exceptions.HTTPError as e:
             print('Error: {}'.format(str(e)))
             error_count += 1
         time.sleep(60)
 http://freepdf-books.com",NA
Taking it further,"Now that we've written a basic functional Twitter API client, there are certainly some 
 things that we could improve upon. Although we don't have space in this chapter to 
 explore enhancements in detail, it's worth mentioning a few to inform future projects 
 you may want to undertake.",NA
Polling and the Twitter streaming APIs,"You may have already spotted a problem that our client will only pull a maximum 
 of 200 tweets per poll. In each poll, Twitter provides the most recent tweets first. This 
 means that if we get more than 200 tweets in 60 seconds, then we will permanently 
 lose the tweets that come in first. In fact, there is no complete solution for this using 
 the 
 statuses/mentions_timeline.json
  endpoint.
 Twitter's solution for this problem is to provide an alternative type of API, which 
 is called a 
 streaming API
 . When connecting to these APIs, the HTTP response 
 connection is actually left open and the incoming tweets are continuously streamed 
 through it. The 
 Requests
  package provides neat functionality for handling this. 
 The 
 Requests
  response objects have an 
 iter_lines()
  method, which runs 
 indefinitely. It is capable of outputting a line of data whenever the server sends one, 
 which can then be processed by us. If you do find that you need this, then there's 
 an example that will help you in getting started in the Requests documentation, 
 and it can be found at 
 http://docs.python-requests.org/en/latest/user/
 advanced/#streaming-requests
 .",NA
Alternative oAuth flows,"Our setup for having our app operate against our main account and having a second 
 account for sending the test tweets is a little clunky, especially so if you use your 
 app account for regular tweeting. Wouldn't it be better to have a separate account 
 dedicated to handling the world clock tweets?
 http://freepdf-books.com",NA
HTML and screen scraping,"Although more and more services are offering their data through APIs, when a 
 service doesn't do this then the only way of getting the data programmatically is to 
 download its web pages and then parse the HTML source code. This technique is 
 called 
 screen scraping
 .
 Though it sounds simple enough in principle, screen scraping should be approached 
 as a last resort. Unlike XML, where the syntax is strictly enforced and data structures 
 are usually reasonably stable and sometimes even documented, the world of web 
 page source code is a messy one. It is a fluid place, where the code can change 
 unexpectedly and in a way that can completely break your script and force you to 
 rework the parsing logic from scratch. 
 Still, it is sometimes the only way to get essential data, so we're going to take a brief 
 look at developing an approach toward scraping. We will discuss ways to reduce the 
 impact when the HTML code does change.
 http://freepdf-books.com",NA
HTML parsers,"We'll be parsing HTML just as we parsed XML. We again have a choice between 
 pull-style APIs and object-oriented APIs. We are going to use 
 ElementTree
  for  
 the same reasons as mentioned before.
 There are several HTML parsing libraries that are available. They're differentiated 
 by their speed, the interfaces that they offer for navigating within HTML documents, 
 and their ability at handling badly constructed HTML. The Python standard library 
 doesn't include an object-oriented HTML parser. The universally recommended 
 third-party package for this is 
 lxml
 , which is primarily an XML parser. However, 
 it does include a very good HTML parser. It's quick, it offers several ways of 
 navigating documents, and it is tolerant of broken HTML.
 The 
 lxml
  library can be installed on Debian and Ubuntu through the 
 python-lxml
  
 package. If you need an up-to-date version or if you're not able to install the system 
 packages, then 
 lxml
  can be installed through 
 pip
 . Note that you'll need a build 
 environment for this. Debian usually comes with an environment that has already 
 been set up but if it's missing, then the following will install one for both Debian  
 and Ubuntu:
 $ sudo apt-get install build-essential
 Then you should be able to install 
 lxml
 , like this:
 $ sudo STATIC_DEPS=true pip install lxml
 If you hit compilation problems on a 64-bit system, then you can also try:
 $ CFLAGS=""$CFLAGS -fPIC"" STATIC_DEPS=true pip install lxml
 On Windows, installer packages are available from the 
 lxml
  website at  
 http://lxml.de/installation.html
 . Check the page for links to third-party 
 installers in case an installer for your version of Python isn't available.
 The next best library, in case 
 lxml
  doesn't work for you, is BeautifulSoup. 
 BeautifulSoup is pure Python, so it can be installed with 
 pip
 , and it should run 
 anywhere. Although it has its own API, it's a well-respected and capable library,  
 and it can, in fact, use 
 lxml
  as a backend library.
 http://freepdf-books.com",NA
Show me the data,"Before we start parsing HTML, we need something to parse! Let's grab the 
 version and codename of the latest stable Debian release from the Debian website. 
 Information about the current stable release can be found at 
 https://www.debian.
 org/releases/stable/
 .
 The information that we want is displayed in the page title and in the first sentence:
 So, we should extract the 
 ""jessie""
  codename and the 8.0 version number.",NA
Parsing HTML with lxml,"Let's open a Python shell and get to parsing. First, we'll download the page with  
 Requests
 .
 >>> import requests
 >>> response = requests.get('https://www.debian.org/releases/stable')
 Next, we parse the source into an 
 ElementTree
  tree. This is the same as it is for 
 parsing XML with the standard library's 
 ElementTree
 , except here we will use the 
 lxml
  specialist 
 HTMLParser
 .
 >>> from lxml.etree import HTML
 >>> root = HTML(response.content)
 The 
 HTML()
  function is a shortcut that reads the HTML that is passed to it, and 
 then it produces an XML tree. Notice that we're passing 
 response.content
  and 
 not 
 response.text
 . The 
 lxml
  library produces better results when it uses the raw 
 response rather than the decoded Unicode text.
 http://freepdf-books.com",NA
Zeroing in,"Screen scraping is the art of finding a way to unambiguously address the elements 
 in the HTML that contain the information that we want, and extract the information 
 from only those elements.
 However, we also want the selection criteria to be as simple as possible. The less we 
 rely on the contents of the document, the lesser the chance of it being broken if the 
 page's HTML changes.
 Let's inspect the HTML source of the page, and see what we're dealing with. For this, 
 either use 
 View Source
  in a web browser, or save the HTML to a file and open it in 
 a text editor. The page's source code is also included in the source code download 
 for this book. Search for the text 
 Debian 8.0
 , so that we are taken straight to the 
 information we want. For me, it looks like the following block of code:
 <body>
 ...
 <div id=""content"">
 <h1>Debian &ldquo;jessie&rdquo; Release Information</h1>
 <p>
 Debian 8.0
  was
 released October 18th, 2014.
 The release included many major
 changes, described in
 ...
 I've skipped the HTML between the 
 <body>
  and the 
 <div>
  to show that the 
 <div> 
 is a direct child of the 
 <body>
  element. From the above, we can see that we want the 
 contents of the 
 <p>
  tag child of the 
 <div>
  element.
 http://freepdf-books.com",NA
Searching with XPath,"In order to avoid exhaustive iteration and the checking of every element, we need 
 to use 
 XPath
 , which is more powerful than what we've used so far. It is a query 
 language that was developed specifically for XML, and it's supported by 
 lxml
 . Plus, 
 the standard library implementation provides limited support for it.
 We're going to take a quick look at XPath, and in the process we will find the answer 
 to the question posed earlier.
 To get started, use the Python shell from the last section, and do the following:
 >>> root.xpath('body')
 [<Element body at 0x39e0908>]
 http://freepdf-books.com",NA
XPath conditions,"So, we can be quite specific by supplying paths, but the real power of XPath lies 
 in applying additional conditions to the elements in the path. In particular, our 
 aforementioned problem, which is, testing element attributes.
 >>> root.xpath('//div[@id=""content""]')
 [<Element div at 0x39e05c8>]
 http://freepdf-books.com",NA
Pulling it together,"Now that we've added XPath to our superpowers, let's finish up by writing a script 
 to get our Debian version information. Create a new file, 
 get_debian_version.py
 , 
 and save the following to it:
 import re
 import requests
 from lxml.etree import HTML
 response = requests.get('http://www.debian.org/releases/stable/')
 root = HTML(response.content)
 title_text = root.find('head').find('title').text
 release = re.search('“(.*)”', title_text).group(1)
 p_text = root.xpath('//div[@id=""content""]/p[1]')[0].text
 version = p_text.split()[1]
 print('Codename: {}\nVersion: {}'.format(release, version))
 http://freepdf-books.com",NA
With great power...,"As an HTTP client developer, you may have different priorities to the webmasters 
 that run websites. A webmaster will typically provide a site for human users; 
 possibly offering a service designed for generating revenue, and it is most likely 
 that all this will need to be done with the help of very limited resources. They will 
 be interested in analyzing how humans use their site, and may have areas of the site 
 they would prefer that automated clients didn't explore.
 HTTP clients that automatically parse and download pages on websites are called 
 various things, such as 
 bots
 , 
 web crawlers
 , and 
 spiders
 . Bots have many legitimate 
 uses. All the search engine providers make extensive use of bots for crawling the 
 web and building their huge page indexes. Bots can be used to check for dead links, 
 and to archive sites for repositories, such as the Wayback Machine. But, there are 
 also many uses that might be considered as illegitimate. Automatically traversing 
 an information service to extract the data on its pages and then repackaging that 
 data for presentation elsewhere without permission of the site owners, downloading 
 large batches of media files in one go when the spirit of the service is online viewing 
 and so on could be considered as illegitimate. Some sites have terms of service 
 which explicitly bar automated downloads. Although some actions such as copying 
 and republishing copyrighted material are clearly illegitimate, some other actions 
 are subject to interpretation. This gray area is a subject of ongoing debate, and it is 
 unlikely that it will ever be resolved to everyone's satisfaction.
 However, even when they do serve a legitimate purpose, in general, bots do make 
 webmasters lives somewhat more difficult. They pollute the webserver logs, which  
 webmasters use for calculating statistics on how their site is being used by their 
 human audience. Bots also consume bandwidth and other server resources.
 http://freepdf-books.com",NA
Choosing a User Agent,"There are a few things that we can do to help our webmasters out. We should always 
 pick an appropriate user agent for our client. The principle way in which webmasters 
 filter out bot traffic from their logfiles is by performing user agent analysis.
 There are lists of the user agents of known bots, for example, one such list can be 
 found at 
 http://www.useragentstring.com/pages/Crawlerlist/
 .
 Webmasters can use these in their filters. Many webmasters also simply filter out 
 any user agents that contain the words 
 bot
 , 
 spider
 , or 
 crawler
 . So, if we are writing an 
 automated bot rather than a browser, then it will make the webmasters' lives a little 
 easier if we use a user agent that contains one of these words. Many bots used by the 
 search engine providers follow this convention, some examples are listed here:
 • 
 Mozilla/5.0 compatible; bingbot/2.0; http://www.bing.com/
 bingbot.htm
 • 
 Baiduspider: http://www.baidu.com/search/spider.htm
 • 
 Mozilla/5.0 compatible; Googlebot/2.1; http://www.google.com/
 bot.html
 There are also some guidelines in section 5.5.3 of the HTTP RFC 7231.",NA
The Robots.txt file,"There is an unofficial but standard mechanism to tell bots if there are any parts of 
 a website that they should not crawl. This mechanism is called 
 robots.txt
 , and 
 it takes the form of a text file called, unsurprisingly, 
 robots.txt
 . This file always 
 lives in the root of a website so that bots can always find it. It has rules that describe 
 the accessible parts of the website. The file format is described at 
 http://www.
 robotstxt.org
 .
 The Python standard library provides the 
 urllib.robotparser
  module for parsing 
 and working with 
 robots.txt
  files. You can create a parser object, feed it a 
 robots.
 txt
  file and then you can simply query it to see whether a given URL is permitted 
 for a given user agent. A good example can be found in the documentation in the 
 standard library. If you check every URL that your client might want to access before 
 you access it, and honor the webmasters wishes, then you'll be helping them out.
 http://freepdf-books.com",NA
Summary,"We've covered a lot of ground in this chapter, but you should now be able to start 
 making real use of the web APIs that you encounter.
 We looked at XML, how to construct documents, parse them and extract data from 
 them by using the 
 ElementTree
  API. We looked at both the Python 
 ElementTree
  
 implementation and 
 lxml
 . We also looked at how the XPath query language can be 
 used efficiently for extracting information from documents.
 We looked at the Amazon S3 service and wrote a client that lets us perform basic 
 operations, such as creating buckets, and uploading and downloading files through 
 the S3 REST API. We learned about setting access permissions and setting content 
 types, such that the files work properly in web browsers.
 We looked at the JSON data format, how to convert Python objects into the JSON 
 data format and how to convert them back to Python objects.
 We then explored the Twitter API and wrote an on-demand world clock service, 
 through which we learned how to read and process tweets for an account, and how 
 to send a tweet as a reply.
 We saw how to extract or scrape information from the HTML source of web pages. 
 We saw how to work with HTML when using 
 ElementTree
  and the 
 lxml
  HTML 
 parser. We also learned how to use XPath to help make this process more efficient.
 And finally, we looked at how we can give back to the webmasters that provide  
 us with all the data. We discussed a few ways in which we can code our clients to 
 make the webmasters lives a little easier and respect how they would like us to use 
 their sites.
 So, that's it for HTTP for now. We'll re-visit HTTP in 
 Chapter 9
 , 
 Applications for the 
 Web
 , where we'll be looking at using Python for constructing the server-side of web 
 applications. In the next chapter, we'll discuss the other great workhorse of the 
 Internet: e-mail.
 http://freepdf-books.com",NA
Engaging with E-mails,"E-mail is one of the most popular ways of digital communication. Python has a  
 rich number of built-in libraries for dealing with e-mails. In this chapter, we will 
 learn how to use Python to compose, send, and retrieve e-mails. The following  
 topics will be covered in this chapter:
 • 
 Sending e-mails with SMTP through the 
 smtplib
  library
 • 
 Securing e-mails transport with TLS
 • 
 Retrieving e-mails by using POP3 with 
 poplib
 • 
 Retrieving e-mails by using IMAP with 
 imapclient
 • 
 Manipulating e-mails on the server with IMAP
 • 
 Sending e-mails with the help of the 
 logging
  module",NA
E-mail terminologies,"Before we start composing our first e-mail with the help of Python, let us revisit  
 some of the elementary concepts of e-mail. Often, an end-user uses a piece of 
 software or a graphical user interface (GUI) for composing, sending, and receiving 
 e-mails. This piece of software is known as an e-mail client, for example, Mozilla 
 Thunderbird, Microsoft Outlook, and so on are e-mail clients. The same tasks can be 
 done by a web interface, that is, a webmail client interface. Some common examples 
 of these are: Gmail, Yahoo mail, Hotmail and so on.
 The mail that you send from your client interface does not reach the receiver's 
 computer directly. Your mail travels through a number of specialized e-mail servers. 
 These servers run a piece of software called the 
 Mail Transfer Agent
  (
 MTA
 ), and its 
 primary job is to route the e-mail to the appropriate destinations by analyzing the 
 mail header, among other things. 
 http://freepdf-books.com",NA
Sending e-mails with SMTP,"We can send an e-mail from a Python script by using 
 smtplib
  and 
 e-mail
  packages. 
 The 
 smtplib
  module provides an SMTP objects which is used for sending mail by 
 using either an SMTP or an 
 Extended SMTP
  (
 ESMTP
 ) protocol. The 
 e-mail
  module 
 helps us in constructing the e-mail messages with the help of the various header 
 information and attachments. This module conforms to the 
 Internet Message Format
  
 (
 IMF
 ) described at 
 http://tools.ietf.org/html/rfc2822.html
 .
 http://freepdf-books.com",NA
Composing an e-mail message,"Let us construct the e-mail message by using classes from the 
 email
  module. The 
 email.mime
  module provides classes for creating the e-mail and MIME objects from 
 scratch. 
 MIME
  is an acronym for 
 Multi-purpose Internet Mail Extensions
 . This is an 
 extension of the original Internet e-mail protocol. This is widely used for exchanging 
 different kinds of data files, such as audio, video, images, applications, and so on.
 Many classes have been derived from the MIME base class. We will use an SMTP 
 client script using 
 email.mime.multipart.MIMEMultipart()
  class as an example. 
 It accepts passing the e-mail header information through a keyword dictionary. Let's 
 have a look at how we can specify an e-mail header by using the 
 MIMEMultipart()
  
 object. Multi-part mime refers to sending both the HTML and the TEXT part of 
 an e-mail message in a single e-mail. When an e-mail client receives a multipart 
 message, it accepts the HTML version if it can render HTML, otherwise it presents 
 the plain text version, as shown in the following code block:
     from email.mime.multipart import MIMEMultipart()
     msg = MIMEMultipart()
     msg['To'] = recipient
     msg['From'] = sender
     msg['Subject'] = 'Email subject..'
 Now, attach a plain text message to this multi-part message object. We can wrap 
 a plain-text message by using the 
 MIMEText()
  object. The constructor of this class 
 takes the additional arguments. For example, we can pass 
 text
  and 
 plain
  as its 
 arguments. The data of this message can be set by using the 
 set_payload()
  method, 
 as shown here:
     part = MIMEText('text', 'plain')
     message = 'Email message ….'
     part.set_payload(message)
 Now, we will attach the plain text message to the Multi-part message, as shown here:
     msg.attach(part)
 The message is ready to be routed to the destination mail server by using one or 
 more SMTP MTA servers. But, obviously, the script only talks to a specific MTA  
 and that MTA handles the routing of the message.
 http://freepdf-books.com",NA
Sending an e-mail message,"The 
 smtplib
  module supplies us with an SMTP class, which can be initialized by an 
 SMTP server socket. Upon successful initialization, this will give us an SMTP session 
 object. The SMTP client will establish a proper SMTP session with the server. This 
 can be done by using the 
 ehlo()
  method for an SMTP 
 session
  object. The actual 
 message sending will be done by applying the 
 sendmail()
  method to the SMTP 
 session. So, a typical SMTP session will look like the following:
     session = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
     session.ehlo()
     session.sendmail(sender, recipient, msg.as_string())
     session.quit()
 In our example SMTP client script, we have made use of Google's free Gmail service. 
 If you have a Gmail account, then you can send an e-mail from a Python script to 
 that account by using SMTP. Your e-mail may get blocked initially, as Gmail may 
 detect that it had been sent from a less secure e-mail client. You can change your 
 Gmail account settings and enable your account to send/receive e-mails from less 
 secure e-mail clients. You can learn more about sending e-mail from an app on 
 the Google website, which can be found at 
 https://support.google.com/a/
 answer/176600?hl=en
 .
 If you don't have a Gmail account, then you can use a local SMTP server setup in 
 a typical Linux box and run this script. The following code shows how to send an 
 e-mail through a public SMTP server:
 #!/usr/bin/env python3
 # Listing 1 – First email client
 import smtplib
 from email.mime.image import MIMEImage
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
 SMTP_SERVER = 'aspmx.l.google.com'
 SMTP_PORT = 25
 def send_email(sender, recipient):
     """""" Send email message """"""
     msg = MIMEMultipart()
 http://freepdf-books.com",NA
Sending e-mails securely with TLS,"TLS
  protocol is a successor of 
 SSL
  or 
 Secure Socket Layer
 . This ensures that the 
 communication between the client and the server is secure. This is done by sending 
 the message in an encrypted format so that unauthorized people cannot see the 
 message. It is not difficult to use TLS with 
 smtplib
 . After you create an SMTP 
 session object, you need to call the 
 starttls()
  method. Before sending an e-mail, 
 you need to login to the server by using the SMTP server credentials.
 Here is an example for the second e-mail client:
 #!/usr/bin/env python3
 # Listing 2
 import getpass
 import smtplib
 from email.mime.image import MIMEImage
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
 SMTP_SERVER = 'smtp.gmail.com'
 SMTP_PORT = 587 # ssl port 465, tls port 587
 def send_email(sender, recipient):
     """""" Send email message """"""
     msg = MIMEMultipart()
     msg['To'] = recipient
     msg['From'] = sender
 http://freepdf-books.com",NA
Retrieving e-mails by using POP3 with ,NA,NA
poplib,"The stored e-mail messages can be downloaded and read by the local computer. 
 The POP3 protocol can be used to download the messages from the e-mail server. 
 Python has a module called 
 poplib
 , and it can be used for this purpose. This module 
 provides two high-level classes, 
 POP()
  and 
 POP3_SSL()
 ,which implement the POP3 
 and POP3S protocols respectively for communicating with a POP3/POP3S server.  
 It accepts three arguments, host, port, and timeout. If port is omitted, then the  
 default port (110) can be used. The optional timeout parameter determines the  
 length (in seconds) of the connection timeout at the server.
 The secure version of 
 POP3()
  is its subclass 
 POP3_SSL()
 . It takes additional 
 parameters, such as keyfile and certfile, which are used for supplying the SSL 
 certificate files, namely the private key and certificate chain file.
 Writing for a POP3 client is also very straightforward. To do this, instantiate a mailbox 
 object by initializing the 
 POP3()
  or 
 POP3_SSL()
  class. Then, invoke the 
 user()
  and 
 pass_()
  methods to login to the server by using the following command:
   mailbox = poplib.POP3_SSL(<POP3_SERVER>, <SERVER_PORT>) 
   mailbox.user('username')
        mailbox.pass_('password')
 Now, you can call the various methods for manipulating your accounts and 
 messages. A few interesting methods have been listed here:
 • 
 stat()
 : This method returns the mailbox status according to tuples  
 of two integers, that is, the message count and the size of the mailbox.
 • 
 list
 (): This method sends a request for getting a message list,  
 which has been demonstrated in the example shown later in this section.
 • 
 retr()
 : This method gives an argument message a number  
 that indicates the message that has to be retrieved. It also marks the  
 message as read.
 • 
 dele()
 : This method provides an argument for the message that has to be 
 deleted. On many POP3 servers, the deletion is not performed until QUIT. 
 You can reset the delete flag by using the 
 rset()
  method.
 • 
 quit()
 : This method takes you off the connection by committing a few 
 changes and disconnecting you from the server.
 http://freepdf-books.com",NA
Retrieving e-mails by using IMAP with ,NA,NA
imaplib,"As we mentioned previously, accessing e-mail over the IMAP protocol doesn't 
 necessarily download the message to the local computer or mobile phone. So, this 
 can be very efficient, even when used over any low bandwidth Internet connection.
 Python provides a client-side library called 
 imaplib
 , which can be used for accessing 
 e-mails over the IMAP protocol. This provides the 
 IMAP4()
  class, which implements 
 the IMAP protocol. It takes two arguments, that is, host and port for implementing 
 this protocol. By default, 
 143
  has been used as the port number.
 The derived class, that is, 
 IMAP4_SSL(),
  provides a secure version of the IMAP4 
 protocol. It connects over an SSL encrypted socket. So, you will need an SSL friendly 
 socket module. The default port is 
 993
 . Similar to 
 POP3_SSL()
 , you can supply the 
 path to a private key and a certificate file path.
 http://freepdf-books.com",NA
Sending e-mail attachments,"In the previous section, we have seen how plain text messages can be sent by using 
 the SMTP protocol. In this section, let us explore how to send attachments through 
 e-mail messages. We can use our second example, in which we have sent an e-mail 
 by using TLS, for this. While composing the e-mail message, in addition to adding a 
 plain text message, include the additional attachment field.
 In this example, we can use the 
 MIMEImage
  type for the 
 email.mime.image
  sub-
 module. A GIF type of image will be attached to the e-mail message. It is assumed 
 that a GIF image can be found anywhere in the file system path. This file path is 
 generally taken on the basis of the user input. 
 The following example shows how to send an attachment along with your  
 e-mail message:
 #!/usr/bin/env python3
 import os
 import getpass
 import re
 import sys
 import smtplib
 from email.mime.image import MIMEImage
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
 SMTP_SERVER = 'aspmx.l.google.com'
 SMTP_PORT = 25
 def send_email(sender, recipient):
     """""" Sends email message """"""
     msg = MIMEMultipart()
     msg['To'] = recipient
     msg['From'] = sender
     subject = input('Enter your email subject: ')
     msg['Subject'] = subject
     message = input('Enter your email message. Press Enter when  
     finished. ')
 http://freepdf-books.com",NA
Sending e-mails via the logging module,"In any modern programming language, the logging facilities are provided with 
 common features. Similarly, Python's logging module is very rich in features and 
 flexibilities. We can use the different types of log handlers with the logging module, 
 such as the console or the file logging handler. One way in which you can maximize 
 your logging benefits is by e-mailing the log messages to the user just as the log 
 is being produced. Python's logging module provides a type of handler called 
 BufferingHandler,
  which is capable of buffering the log data.
 An example of extending 
 BufferingHandler
  has been displayed later. A child class 
 called 
 BufferingSMTPHandler
  is defined by 
 BufferingHandler
 . In this example, 
 an instance of the logger object is created by using the logging module. Then, an 
 instance of 
 BufferingSMTPHandler
  is tied to this logger object. The logging level is 
 set to DEBUG so that it can log any message. A sample list of four words has been 
 used for creating the four log entries. Each log entry should resemble the following:
 <Timestamp> INFO  First line of log
 This accumulated log message will be emailed to a local user as set  
   on top of the script.
 Now, let us take a look at the full code. The following is an example of sending an 
 e-mail with the help of the logging module:
 import logging.handlers
 import getpass
 MAILHOST = 'localhost'
 FROM = 'you@yourdomain'
 TO = ['%s@localhost' %getpass.getuser()] 
 SUBJECT = 'Test Logging email from Python logging module  
   (buffering)'
 class BufferingSMTPHandler(logging.handlers.BufferingHandler):
     def __init__(self, mailhost, fromaddr, toaddrs, subject,  
     capacity):
         logging.handlers.BufferingHandler.__init__(self, capacity)
         self.mailhost = mailhost
         self.mailport = None
         self.fromaddr = fromaddr
         self.toaddrs = toaddrs
         self.subject = subject
 http://freepdf-books.com",NA
Summary,"This chapter demonstrates how Python can interact with the three major e-mail 
 handling protocols: SMTP, POP3, and IMAP. In each of these cases, how to work  
 the client code has been explained. Finally, an example for using SMTP in the 
 Python's logging module has been shown.
 In the next chapter, you will learn how to use Python to work with remote systems 
 to perform various tasks, such as administrative tasks by using SSH, file transfer 
 through FTP, Samba, and so forth. Some remote monitoring protocols, such as  
 SNMP, and the authentication protocols, such as LDAP, will also be discussed 
 briefly. So, enjoy writing more Python codes in the next chapter.
 http://freepdf-books.com",NA
Interacting with  ,NA,NA
Remote Systems,"If your computer is connected to the Internet or a 
 local area network
  (
 LAN
 ), then 
 it's time to talk to the other computers on the network. In a typical home, office, or 
 campus LAN, you will find that many different types of computers are connected  
 to the network. Some computers act as the servers for specific services, such as a  
 file server, a print server, a user authentication management server, and so on.  
 In this chapter, we will explore how the computers in a network can interact with  
 each other and how they can access a few services through the Python scripts.  
 The following task list will give you an overview of the topics that will be  
 covered in this chapter:
 • 
 Accessing SSH terminals with 
 paramiko
 • 
 Transferring files through SFTP
 • 
 Transferring files with the help of FTP
 • 
 Reading the SNMP packets
 • 
 Reading the LDAP packets
 • 
 Sharing the files with the help of SAMBA
 This chapter requires quite a few third-party packages, such as 
 paramiko
 , 
 pysnmp
 , 
 and so on. You can use your operating system's package management tool for 
 installing them. Here's a quick how-to on installing the 
 paramiko
  module in Ubuntu 
 14, python3, and the other modules that are required for understanding the topics 
 covered in this chapter:
 sudo apt-get install python3
 sudo apt-get install python3-setuptools
 sudo easy_install3 paramiko
 sudo easy_install3 python3-ldap
 sudo easy_install3 pysnmp
 sudo easy_install3 pysmb
 http://freepdf-books.com",NA
Secure shell – access using Python,"SSH has become a very popular network protocol for performing secure data 
 communication between two computers. It provides an excellent cryptographic 
 support, so that unrelated third-parties cannot see the content of the data during 
 the transmission process. Details of the SSH protocol can be found in these RFC 
 documents: RFC4251-RFC4254, available at 
 http://www.rfc-editor.org/rfc/
 rfc4251.txt
 .
 Python's 
 paramiko
  library provides a very good support for the SSH-based network 
 communication. You can use Python scripts to benefit from the advantages of  
 SSH-based remote administration, such as the remote command-line login, 
 command execution, and the other secure network services between two networked 
 computers. You may also be interested in using the 
 pysftp
  module, which is  
 based on 
 paramiko
 . More details regarding this package can be found at PyPI: 
 https://pypi.python.org/pypi/pysftp/
 .
 The SSH is a client/server protocol. Both of the parties use the SSH key pairs to 
 encrypt the communication. Each key pair has one private and one public key. The 
 public key can be published to anyone who may be interested in that. The private 
 key is always kept private and secure from everyone except the owner of the key.
 The SSH public and private keys can be generated and digitally signed by an external 
 or an internal certificate authority. But that brings a lot of overhead to a small 
 organization. So, alternatively, the keys can be generated randomly by utility tools, 
 such as 
 ssh-keygen
 . The public key needs to be available to all participating parties. 
 When the SSH client connects to the server for the first time, it registers the public 
 key of the server on a special file called 
 ~/.ssh/known_hosts
  file. So, the subsequent 
 connection to the server ensures that the client is talking to the same server as it spoke 
 to before. On the server side, if you would like to restrict access to certain clients who 
 have certain IP addresses, then the public keys of the permitted hosts can be stored 
 to another special file called 
 ssh_known_hosts
  file. Of course, if you re-build the 
 machines, such as the server machine, then the old public key of the server won't 
 match with that of the one stored in the 
 ~/.ssh/known_hosts
  file. So, the SSH client 
 will raise an exception and prevent you from connecting to it. You can delete the old 
 key from that file and then try to re-connect, as if for the first time.
 We can use the 
 paramiko
  module to create an SSH client and then connect it to the 
 SSH server. This module will supply the 
 SSHClient()
  class.
 ssh_client = paramiko.SSHClient()
 http://freepdf-books.com",NA
Inspecting the SSH packets,"It would be very interesting to see the network packet exchange between the client 
 and the server. We can use either the native 
 tcpdump
  command or the third-party 
 Wireshark tool to capture network packets. With 
 tcpdump
 , you can specify the target 
 network interface ( 
 -i lo
 ) and the port number (port 
 22
 ) options. In the following 
 packet capture session, five packet exchanges have been shown during an SSH 
 client/server communication session:
 root@debian6box:~# tcpdump -i lo port 22
 tcpdump: verbose output suppressed, use -v or -vv for full protocol  
   decode
 listening on lo, link-type EN10MB (Ethernet), capture size 65535  
   bytes
 12:18:19.761292 IP localhost.50768 > localhost.ssh: Flags [S], seq  
   3958510356, win 32792, options [mss 16396,sackOK,TS val 57162360  
   ecr 0,nop,wscale 6], length 0
 12:18:19.761335 IP localhost.ssh > localhost.50768: Flags [S.], seq  
   1834733028, ack 3958510357, win 32768, options [mss 16396,sackOK,TS  
   val 57162360 ecr 57162360,nop,wscale 6], length 0
 12:18:19.761376 IP localhost.50768 > localhost.ssh: Flags [.], ack 1,  
   win 513, options [nop,nop,TS val 57162360 ecr 57162360], length 0
 http://freepdf-books.com",NA
Transferring files through SFTP,"SSH can be used effectively for securely transferring files between two computer 
 nodes. The protocol used in this case is the 
 secure file transfer protocol
  (
 SFTP
 ). The 
 Python 
 paramiko
  module will supply the classes required for creating the SFTP 
 session. This session can then perform a regular SSH login.
 ssh_transport = paramiko.Transport(hostname, port)
 ssh_transport.connect(username='username', password='password')
 The SFTP session can be created from the SSH transport. The paramiko's working in 
 the SFTP session will support the normal FTP commands such as 
 get()
 .
  sftp_session = paramiko.SFTPClient.from_transport(ssh_transport)
  sftp_session.get(source_file, target_file)
 As you can see, the SFTP 
 get
  command requires the source file's path and the target 
 file's path. In the following example, the script will download a 
 test.txt
  file, which 
 is located on the user's home directory, through SFTP:
 #!/usr/bin/env python3
 import getpass
 import paramiko
 HOSTNAME = 'localhost'
 PORT = 22
 FILE_PATH = '/tmp/test.txt'
 def sftp_download(username, password, hostname=HOSTNAME,  
     port=PORT):
     ssh_transport = paramiko.Transport(hostname, port)
     ssh_transport.connect(username=username, password=password)
     sftp_session =  
     paramiko.SFTPClient.from_transport(ssh_transport)
     file_path = input(""Enter filepath: "") or FILE_PATH
     target_file = file_path.split('/')[-1]
     sftp_session.get(file_path, target_file)
     print(""Downloaded file from: %s"" %file_path)
     sftp_session.close()
     
 if __name__ == '__main__':
     hostname = input(""Enter the target hostname: "")
     port = input(""Enter the target port: "")
     username = input(""Enter yur username: "")
     password = getpass.getpass(prompt=""Enter your password: "")
     sftp_download(username, password, hostname, int(port))
 http://freepdf-books.com",NA
Transferring files with FTP,"Unlike SFTP, FTP uses the plain-text file transfer method. This means any  
 username or password transferred through the wire can be detected by an  
 unrelated third-party. Even though FTP is a very popular file transfer protocol, 
 people frequently use this for transferring a file from their PCs to the remote servers.
 In Python, 
 ftplib
  is a built-in module used for transferring the files to and from 
 the remote machines. You can create an anonymous FTP client connection with the 
 FTP()
  class.
 ftp_client = ftplib.FTP(path, username, email)   
 Then you can invoke the normal FTP commands, such as 
 CWD
 . In order to download 
 a binary file, you need to create a file-handler such as the following:
 file_handler = open(DOWNLOAD_FILE_NAME, 'wb')
 In order to retrieve the binary file from the remote host, the syntax shown here can 
 be used along with the 
 RETR
  command:
 ftp_client.retrbinary('RETR remote_file_name', file_handler.write)
 http://freepdf-books.com",NA
Inspecting FTP packets,"If we capture the FTP session in Wireshark on port 
 21
  of the public network 
 interface, then we can see how the communication happens in plain-text. This will 
 show you why SFTP should be preferred. In the following figure, we can see that, 
 after successfully establishing connection with a client the server sends the banner 
 message: 
 220
  Welcome to kernel.org. Following this, the client will anonymously 
 send a request for login. In response, the server will ask for a password. The client 
 can send the user's e-mail address for authentication.
 To your surprise, you can see that the password has been sent in clear-text. In the 
 following screenshot, the contents of the password packet have been displayed. It 
 shows the supplied fake e-mail address, 
 nobody@nourl.com
 .
 http://freepdf-books.com",NA
Fetching Simple Network Management ,NA,NA
Protocol data,"SNMP is a ubiquitous network protocol that is used by the network routers, such 
 as switches, servers, and so on, for communicating the device's configuration, 
 performance data, and the commands that are meant for the control devices. 
 Although SNMP starts with the word 
 simple
 , it's not a simple protocol. Internally, 
 each device's information is stored in a sort of a database of information called the 
 management information base
  (
 MIB
 ). The SNMP protocol offers varying levels of 
 security depending on the protocol version number. In SNMP 
 v1
  and 
 v2c
 , the data is 
 protected by a pass phrase known as the community string. In SNMP 
 v3
 , a username 
 and a password are required for storing the data. And, the data can be encrypted 
 with the help of SSL. In our example, we will use the 
 v1
  and 
 v2c
  versions of the 
 SNMP protocol.
 http://freepdf-books.com",NA
Inspecting SNMP packets,"We can inspect the SNMP packet by capturing the packets on port 161 of  
 your network interface. If the server is running locally, then listening on the 
 loopbook
  interface is sufficient. The 
 snmp-get
  request format and the 
 snmp-get
  
 response packet formats, which are produced by Wireshak, is shown in the  
 following screenshot:
 http://freepdf-books.com",NA
Reading Light-weight Directory Access ,NA,NA
Protocol data,"LDAP has been used for a long time for accessing and managing distributed 
 directory information. This is an application level protocol that works over the 
 IP network. Directory service is heavily used in organizations for managing the 
 information about the users, the computer systems, the networks, the applications, 
 and so on. The LDAP protocol contains plenty of technical jargon. It is a client/
 server-based protocol. So, the LDAP client will make a request to a properly 
 configured LDAP server. After initializing the LDAP connection, the connection will 
 need to be authenticated by using a few parameters. A simple BIND operation will 
 establish an LDAP session. In a simple case, you can set up a simple anonymous 
 BIND that would not need no password or any other credentials.
 http://freepdf-books.com",NA
Inspecting LDAP packets,"If we analyze the communication between the LDAP client and the server, then we 
 can see the format of the LDAP search request and response. The parameters that we 
 have used in our code have a direct relationship with the 
 searchRequest
  section of 
 an LDAP packet. As shown in the following screenshot produced by Wireshark, it 
 contains data, such as 
 baseObject
 , 
 scope
  and 
 Filter
 .
 http://freepdf-books.com",NA
Sharing files with SAMBA,"In a LAN environment, you will often need to share the files between different types 
 of machines, such as Windows and Linux machines. The protocol used for sharing 
 the files and the printers among these machines is either the 
 Server Message Block
  
 (
 SMB
 ) protocol or its enhanced version called the 
 Common Internet File System
  
 (
 CIFS
 ) protocol. CIFS runs over TCP/IP and it is used by the SMB clients and 
 servers. In Linux, you will find a package called Samba, which implements the  
 SMB
  protocol.
 If you are running a Linux virtual machine within a Windows box with the help of 
 software, such as VirtualBox, then we can test file sharing among the Windows and 
 the Linux machines. Let us create a folder at 
 C:\share
  on the Windows machine as 
 you can see in the following screenshot:
 http://freepdf-books.com",NA
Inspecting SAMBA packets,"If we capture the SMABA packets on port 
 445
 , then we can see how the Windows 
 Server communicates with the Linux SAMBA client over the CIFS protocol.  
 In the following two screenshots, a detailed communication between the client  
 and the server, has been presented. The connection setup has been shown in the  
 following screenshot:
 http://freepdf-books.com",NA
Summary,"In this chapter, we have come across several network protocols and Python libraries, 
 which are used for interacting with remote systems. SSH and SFTP are used for 
 securely connecting and transferring files to the remote hosts. FTP is still used as 
 a simple file transfer mechanism. However, it's not secure due to user credentials 
 being transferred over the wire as plain-text. We also examined Python libraries for 
 dealing with SNMP, LDAP, and SAMBA packets.
 In the next chapter, one of the most common networking protocols—that is, DNS 
 and IP—will be discussed. We will explore TCP/IP networking using Python scripts.
 http://freepdf-books.com",NA
IP and DNS,"Every computer that is connected to a network needs an IP address. In 
 Chapter 1
 ,  
 Network Programming and Python
 , an introduction to TCP/IP networking was 
 presented. The IP address labels a machine's network interface with a numeric 
 identifier, which also identifies the location of the machine, albeit with limited 
 reliability. 
 Domain Name System
  (
 DNS
 ) is a core network service that maps the 
 names to the IP addresses and vice-verse. In this chapter, we will mainly focus 
 on manipulating the IP and DNS protocols with the help of Python. In addition 
 to this, we will briefly discuss the 
 Network Time Protocol
  (
 NTP
 ), which helps in 
 synchronizing the time with a centralized time server. The following topics will be 
 discussed here:
 • 
 Retrieving the network configuration of a local machine
 • 
 Manipulating the IP addresses
 • 
 The GeoIP look-ups
 • 
 Working with DNS
 • 
 Working with NTP",NA
Retrieving the network configuration of a ,NA,NA
local machine,"Before doing anything else, let's ask in the Python language, 
 What's my name?
 . In 
 networking terms, this is equivalent to finding out the machine's name or the host's 
 name. On the shell command-line, this can be discovered by using the 
 hostname
  
 command. In Python, you can do this by using the socket module.
 >>> import socket
 >>> socket.gethostname()
 'debian6box.localdomain.loc'
 http://freepdf-books.com",NA
Manipulating IP addresses,"Often you will need to manipulate IP addresses and perform some sort of 
 operations on them. Python3 has a built-in 
 ipaddress
  module  to help you in 
 carrying out this task. It has convenient functions for defining the IP addresses 
 and the IP networks and for finding lots of useful information. For example, if you 
 would like to know how many IP addresses exist in a given subnet, for instance, 
 10.0.1.0/255.255.255.0
  or 
 10.0.2.0/24
 , then you can find them with the help of 
 the code snippet shown here. This module will provide several classes and factory 
 functions; for example, the IP address and the IP network has separate classes. Each 
 class has a variant for both IP version 4 (IPv4) and IP version 6 (IPv6). Some of the 
 features have been demonstrated in the following section:
 http://freepdf-books.com",NA
IP network objects,"Let us import the 
 ipaddress
  module and define a 
 net4
  network.
 >>> import ipaddress as ip
 >>> net4 = ip.ip_network('10.0.1.0/24')
 Now, we can find some useful information, such as 
 netmask
 , the network/broadcast 
 address, and so on, of 
 net4
 :
 >>> net4.netmask
 IP4Address(255.255.255.0)
 The 
 netmask
  properties of 
 net4
  will be displayed as an 
 IP4Address
  object. If you  
 are looking for its string representation, then you can call the 
 str()
  method, as 
 shown here:
 >>> str(net4.netmask)
 '255.255.255.0'
 Similarly, you can find the network and the  broadcast addresses of 
 net4, 
 by doing 
 the following:
 >>> str(net4.network_address)
 10.0.1.0
 >>> str(net4.broadcast_address)
 10.0.1.255
 How many addresses does 
 net4
  hold in total? This can be found by using the 
 command shown here:
 >>> net4.num_addresses
 256
 So, if we subtract the network and the broadcast addresses, then the total  
 available IP addresses will be 254. We can call the 
 hosts()
  method on the 
 net4
  
 object. It will produce a Python generator, which will supply all the hosts as 
 IPv4Adress
  objects.
 >>> all_hosts = list(net4.hosts())
 >>> len(all_hosts)
 254
 You can access the individual IP addresses by following the standard Python list 
 access notation. For example, the first IP address would be the following:
 >>> all_hosts[0]
 IPv4Address('10.0.1.1')
 http://freepdf-books.com",NA
Network interface objects,"In the 
 ipaddress
  module, a convenient class is used for representing an interface's 
 IP configuration in detail. The IPv4 Interface class takes an arbitrary address and 
 behaves like a network address object. Let us define and discuss our network 
 interface 
 eth0,
  as shown in following screenshot:
 http://freepdf-books.com",NA
The IP address objects,"The IP address classes have many more interesting properties. You can perform 
 some arithmetic and logical operations on those objects. For example, if an IP address 
 is greater than another IP address, then you can add numbers to the IP address 
 objects, and this will give you a corresponding IP address. Let's see a demonstration 
 of this in the following screenshot:
 Demonstration of the 
 ipaddress
  module
 http://freepdf-books.com",NA
Planning IP addresses for your local area ,NA,NA
network,"If you are wondering how to pick-up a suitable IP subnet, then you can experiment 
 with the 
 ipaddress
  module. The following code snippet will show an example of 
 how to choose a specific subnet, based on the number of necessary host IP addresses 
 for a small private network:
 #!/usr/bin/env python
 import ipaddress as ip
 CLASS_C_ADDR = '192.168.0.0'
 if __name__ == '__main__':
     not_configed = True
     while not_configed:
         prefix = input(""Enter the prefixlen (24-30): "")
         prefix = int(prefix)
         if prefix not in range(23, 31):
             raise Exception(""Prefixlen must be between 24 and 30"")
         net_addr = CLASS_C_ADDR + '/' + str(prefix)
         print(""Using network address:%s "" %net_addr)
         try:
             network = ip.ip_network(net_addr)
         except:
             raise Exception(""Failed to create network object"")
         print(""This prefix will give %s IP addresses""  
         %(network.num_addresses))
         print(""The network configuration will be"")
         print(""\t network address: %s""  
         %str(network.network_address))
         print(""\t netmask: %s"" %str(network.netmask))
 http://freepdf-books.com",NA
GeoIP look-ups,"At times, it will be necessary for many applications to look-up the location of the 
 IP addresses. For example, many website owners can be interested in tracking the 
 location of their visitors and in classifying their IPs according to criteria, such as 
 country, city, and so on. There is a third-party library called 
 python-geoip,
  which 
 has a robust interface for giving you the answer to your IP location query. This 
 library is provided by MaxMind, which also provides the option for shipping a 
 recent version of the Geolite2 database as the 
 python-geoip-geolite2
  package. 
 This includes the GeoLite2 data created by MaxMind, which is available at 
 www.
 maxmind.com
  under the creative commons Attribution-ShareAlike 3.0 Unported 
 License. You can also buy a commercial license from their website.
 Let's see an example of how to use this Geo-lookup library.:
 import socket
 from geoip import geolite2
 import argparse
 if __name__ == '__main__':
     # Setup commandline arguments
     parser = argparse.ArgumentParser(description='Get IP Geolocation 
 info')
     parser.add_argument('--hostname', action=""store"", dest=""hostname"", 
 required=True)
     
     # Parse arguments
     given_args = parser.parse_args()
     hostname =  given_args.hostname
     ip_address = socket.gethostbyname(hostname)
     print(""IP address: {0}"".format(ip_address))
     
     match = geolite2.lookup(ip_address)
     if match is not None:
         print('Country: ',match.country)
         print('Continent: ',match.continent) 
         print('Time zone: ', match.timezone) 
 http://freepdf-books.com",NA
DNS look-ups,"The IP address can be translated into human readable strings called domain  
 names. DNS is a big topic in the world of networking. In this section, we will  
 create a DNS client in Python, and see how this client will talk to the server  
 by using Wirshark.
 A few DNS cleint libraries are available from PyPI. We will focus on the 
 dnspython
  
 library, which is available at 
 http://www.dnspython.org/
 . You can install this 
 library by using either the 
 easy_install
  command or the 
 pip
  command:
 $ pip install dnspython
 Making a simple query regarding the IP address of a host is very simple. You can use 
 the 
 dns.resolver
  submodule, as follows:
 import dns.resolver
 answers = dns.resolver.query('python.org', 'A')
 for rdata in answers:
     print('IP', rdata.to_text())
 If you want to make a reverse look-up, then you need to use the 
 dns.reversename
  
 submodule, as shown here:
 import dns.reversename
 name = dns.reversename.from_address(""127.0.0.1"")
 print name
 print dns.reversename.to_address(name)
 http://freepdf-books.com",NA
Inspecting DNS client/server communication,"In previous chapters, perhaps you noticed how we captured network packets 
 between the client and the server by using Wireshark. Here is an example of the 
 session capturing, while a Python package was being installed from PyPI:
 FDNS client/server communication
 http://freepdf-books.com",NA
NTP clients,"The final topic that will be covered  in this chapter is NTP. Synchronizing time 
 with a centralized time server is a key step in any corporate network. We would 
 like to compare the log files between various servers and see if the timestamp on 
 each server is accurate; the log events may not then co-relate. Many authentication 
 protocols, such as Kerberos, strictly rely on the accuracy of the time stamp reported 
 by the client to the servers. Here, a third-party Python 
 ntplib
  library will be 
 introduced, and then the communication between the NTP client and the server  
 will be investigated.
 http://freepdf-books.com",NA
Inspecting the NTP client/server ,NA,NA
communication,"You may be able to learn more about NTP by looking at captured packets. For this 
 purpose, the preceding NTP client/server communication has been captured as 
 shown in the following two screenshots:
 The first screenshot shows the NTP client request. If you look inside the flag fields, 
 then you will see the client's version number.
 http://freepdf-books.com",NA
Summary,"In this chapter, the standard Python libraries for IP address manipulation were 
 discussed. Two third-party libraries 
 dnspython
  and 
 ntplib
  have been presented to 
 interact with the DNS and the NTP servers respectively. As you have seen through 
 the aforementioned examples, these libraries provide you with the necessary 
 interface for talking to those services.
 In the following chapter, we will introduce socket programming in Python.  
 This is another interesting and popular topic for networking programmers.  
 There, you will find both low and high-level Python libraries for programming  
 with BSD sockets.
 http://freepdf-books.com",NA
Programming with Sockets,"After you have interacted with various clients/servers in Python, you will be keen to 
 create your own custom clients and servers for any protocol of your choice. Python 
 provides a good coverage on the low-level networking interface. It all starts with 
 BSD socket interface. As you can assume, Python has a 
 socket
  module that gives 
 you the necessary functionality to work with the socket Interface. If you have ever  
 done socket programming in any other language like C/C++, you will love the 
 Python 
 socket
  module.
 In this chapter, we will explore the socket module by creating a diverse range of 
 Python scripts.
 The following are the highlights of this chapter:
 • 
 Basics of sockets
 • 
 Working with TCP sockets
 • 
 Working with UDP sockets
 • 
 TCP port forwarding
 • 
 Non-blocking  socket I/O
 • 
 Securing sockets with SSL/TLS
 • 
 Creating custom SSL client/server",NA
Basics of sockets,"Network programming in any programming language can begin with sockets. But 
 what is a socket? Simply put, a network socket is a virtual end point where entities 
 can perform inter-process communication. For example, one process sitting in a 
 computer, exchanges data with another process sitting on the same or another 
 computer. We typically label the first process which initiates the communication  
 as the client and the latter one as the server.
 http://freepdf-books.com",NA
Working with TCP sockets,"Creating a socket object in Python is very straightforward. You just need to import 
 the 
 socket
  module and call the 
 socket()
  class:
 from socket import*
 import socket
 #create a TCP socket (SOCK_STREAM)
 s = socket.socket(family=AF_INET, type=SOCK_STREAM, proto=0)
 print('Socket created')
 Traditionally, the class takes plenty of parameters. Some of them are listed in  
 the following:
 • 
 Socket family
 : This is the domain of socket, such as 
 AF_INET
  (about  
 90 percent of the sockets of the Internet fall under this category) or 
 AF_UNIX,
   
 which is sometimes used as well. In Python 3, you can create a Bluetooth 
 socket using 
 AF_BLUETOOTH
 .
 • 
 Socket type
 : Depending on your need, you need to specify the type of  
 socket. For example, TCP and UDP-based sockets are created by specifying 
 SOCK_STREAM
  and 
 SOCK_DGRAM,
  respectively.
 • 
 Protocol
 : This specifies the variation of protocol within a socket family  
 and type. Usually, it is left as zero.
 For many reasons, socket operations may not be successful. For example, if you  
 don't have permission to access a particular port as a normal user, you may not be 
 able to bind to a socket. This is why it is a good idea to do proper error handling 
 when creating a socket or doing some network-bound communication.
 http://freepdf-books.com",NA
Inspecting the client/server communication,"The interaction between the client and server through the exchange of network 
 packets can be analyzed using any network packet capturing tool, such as Wireshark. 
 You can configure Wireshark to filter packets by port or host. In this case, we can 
 filter by port 80. You can get the options under the 
 Capture
  | 
 Options
  menu and 
 type 
 port 80
  in the input box next to the 
 Capture Filter
  option, as shown in the 
 following screenshot:
 http://freepdf-books.com",NA
TCP servers,"As you understood from the very first client/server interaction diagram, the server 
 process needs to carry out a bit of extra work. It needs to bind to a socket address 
 and listen for incoming connections. The following code snippet shows how to  
 create a TCP server:
 import socket
 from time import ctime
 HOST = 'localhost'
 PORT = 12345
 BUFSIZ = 1024
 ADDR = (HOST, PORT)
 if __name__ == '__main__':
     server_socket = socket.socket(socket.AF_INET,  
     socket.SOCK_STREAM)
     server_socket.bind(ADDR)
     server_socket.listen(5)
     server_socket.setsockopt( socket.SOL_SOCKET,  
     socket.SO_REUSEADDR, 1 )
     while True:
         print('Server waiting for connection...')
         client_sock, addr = server_socket.accept()
         print('Client connected from: ', addr)
         while True:
             data = client_sock.recv(BUFSIZ)
             if not data or data.decode('utf-8') == 'END':
                 break
             print(""Received from client: %s"" % data.decode('utf-  
             8'))
             print(""Sending the server time to client: %s""   
             %ctime())
             try:
                 client_sock.send(bytes(ctime(), 'utf-8'))
             except KeyboardInterrupt:
                 print(""Exited by user"")
         client_sock.close()
     server_socket.close()
 http://freepdf-books.com",NA
Inspecting client/server interaction,"Now, once again, you can configure Wireshark to capture packets, as discussed  
 in the last section. But, in this case, you need to specify the port that your  
 server is listening on (in the preceding example it's 
 12345
 ), as shown in the  
 following screenshot:
 http://freepdf-books.com",NA
Working with UDP sockets,"Unlike TCP, UDP doesn't check for errors in the exchanged datagram. We can create 
 UDP client/servers similar to the TCP client/servers. The only difference is you have 
 to specify 
 SOCK_DGRAM
  instead of 
 SOCK_STREAM
  when you create the socket object.
 Let us create a UDP server. Use the following code to create the UDP server:
 from socket import socket, AF_INET, SOCK_DGRAM
 maxsize = 4096
 sock = socket(AF_INET,SOCK_DGRAM)
 sock.bind(('',12345))
 while True:    
   data, addr = sock.recvfrom(maxsize)
     resp = ""UDP server sending data""    
   sock.sendto(resp,addr)
 Now, you can create a UDP client to send some data to the UDP server, as shown in 
 the following code:
 from socket import socket, AF_INET, SOCK_DGRAM
 MAX_SIZE = 4096
 PORT = 12345
 if __name__ == '__main__':
     sock = socket(AF_INET,SOCK_DGRAM)
     msg = ""Hello UDP server""
     sock.sendto(msg.encode(),('', PORT))
     data, addr = sock.recvfrom(MAX_SIZE)
     print(""Server says:"")
     print(repr(data))
 In the preceding code snippet, the UDP client sends a single line of text 
 Hello UDP 
 server
  and receives the response from the server. The following screenshot shows 
 the request sent from the client to the server:
 http://freepdf-books.com",NA
TCP port forwarding,"One of the interesting experiments we can do with TCP socket programming is to set 
 up a TCP port forwarding. This has very good use cases. Say, for example, if you are 
 running an insecure program like FTP in a public server that doesn't have any SSL 
 capability to do secure communication (FTP passwords can be seen clear-text over 
 the wires). Since this server is accessible from Internet, you must not login with your 
 password to the server without ensuring that the passwords are encrypted. One way 
 of doing this is to use Secure FTP or SFTP. We can use a simple SSH tunnel in order 
 to show how this approach works. So, any communication between your local FTP 
 client and remote FTP server will happen via this encrypted channel.
 Let us run the FTP program to the same SSH server host. But create an SSH tunnel 
 from your local machine that will give you a local port number and will directly 
 connect you to the remote FTP server daemon.
 Python has a third party 
 sshtunnel
  module that is a wrapper around the Paramiko's 
 SSH
  library. The following is a code snippet of TCP port forwarding that shows how 
 the concept can be realized:
 import sshtunnel
 from getpass import getpass
 ssh_host = '192.168.56.101'
 ssh_port = 22
 ssh_user = 'YOUR_SSH_USERNAME'
 REMOTE_HOST = '192.168.56.101'
 REMOTE_PORT = 21
 from sshtunnel import SSHTunnelForwarder
 ssh_password = getpass('Enter YOUR_SSH_PASSWORD: ')
 server = SSHTunnelForwarder(
     ssh_address=(ssh_host, ssh_port),
     ssh_username=ssh_user,
     ssh_password=ssh_password,
 http://freepdf-books.com",NA
A non-blocking socket I/O,"In this section, we will see a small example code snippet to test a non-blocking socket 
 I/O. This is useful if you know that the synchronous blocking connection is not 
 necessary for your program. The following is an example of non-blocking I/O:
 import socket
 if __name__ == '__main__':
     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     sock.setblocking(0)
     sock.settimeout(0.5)
     sock.bind((""127.0.0.1"", 0))
     socket_address =sock.getsockname()
     print(""Asynchronous socket server launched on socket: %s""  
     %str(socket_address))
     while(1):
         sock.listen(1)
 This script will run a socket server and listen in a non-blocking style. This means you 
 can connect more clients who won't be necessarily blocked for I/O.",NA
Securing sockets with TLS/SSL,"You have probably come across the discussion around secure web communication 
 using 
 Secure Socket Layer
  (
 SSL
 ), or more precisely 
 Transport Layer Security
  (
 TLS
 ), 
 which is adopted by many other high-level protocols. Let us see how we can wrap a 
 plain sockets connection with SSL. Python has the built-in 
 ssl
  module, which serves 
 this purpose.
 In this example, we would like to create a plain TCP socket and connect to an HTTPS 
 enabled web server. Then, we can wrap that connection using SSL and check the 
 various properties of the connection. For example, to check the identity of the remote 
 web server, we can see if the hostname is same in the SSL certificate as we expect it to 
 be. The following is an example of a secure socket-based client:
 import socket
 import ssl
 from ssl import wrap_socket, CERT_NONE, PROTOCOL_TLSv1, SSLError
 from ssl import SSLContext
 http://freepdf-books.com",NA
Inspecting standard SSL client/server ,NA,NA
communication,"The following screenshot shows the interaction between the SSL client and the 
 remote server:
 http://freepdf-books.com",NA
Creating a custom SSL client/server,"So far, we have been dealing more with the SSL or TLS client. Now, let us have a 
 look at the server side briefly. As you are already familiar with the TCP/UDP socket 
 server creation process, let's skip that part and just concentrate on the SSL wrapping 
 part. The following code snippet shows an example of a simple SSL server:
 import socket
 import ssl
 SSL_SERVER_PORT = 8000
 if __name__ == '__main__':
     server_socket = socket.socket()
 http://freepdf-books.com",NA
Inspecting interaction between a custom SSL ,NA,NA
client/server,"Let us inspect the SSL client/server interaction once again in order to observe the 
 differences. The first screenshot shows the entire communication sequence. In the 
 following screenshot we can see that the server's 
 Hello
  and certificate are combined 
 in the same message.
 http://freepdf-books.com",NA
Summary,"In this chapter, we discussed basic TCP/IP socket programming using Python's 
 socket
  and 
 ssl
  module. We demonstrated how simple TCP sockets can be wrapped 
 with TLS and used to carry encrypted data. We also found the ways to validate 
 the authenticity of a remote server using SSL certificates. Some other minor issues 
 around socket programming, such as non-blocking socket I/O were also presented. 
 The detailed packet analysis in each section helps us to understand what happens 
 under the hood in our socket programming exercises.
 In the next chapter, we will learn about the socket server design, particularly the 
 popular multithreaded and event-driven approaches will be touched upon.
 http://freepdf-books.com",NA
Client and Server ,NA,NA
Applications,"In the previous chapter, we looked at exchanging data between devices by using 
 the sockets interface. In this chapter, we're going to use sockets to build network 
 applications. Sockets follow one of the main models of computer networking, that 
 is, the 
 client/server
  model. We'll look at this with a focus on structuring server 
 applications. We'll cover the following topics:
 • 
 Designing a simple protocol
 • 
 Building an echo server and client
 • 
 Building a chat server and client
 • 
 Multithreaded and event-driven server architectures
 • 
 The 
 eventlet
  and 
 asyncio
  libraries
 The examples in this chapter are best run on Linux or a Unix operating system.  
 The Windows sockets implementation has some idiosyncrasies, and these can create 
 some error conditions, which we will not be covering here. Note that Windows does 
 not support the 
 poll
  interface that we'll use in one example. If you do use Windows, 
 then you'll probably need to use 
 ctrl
  + 
 break
  to kill these processes in the console, 
 rather than using 
 ctrl
  - 
 c
  because Python in a Windows command prompt doesn't 
 respond to 
 ctrl
  – 
 c
  when it's blocking on a socket send or receive, which will be quite 
 often in this chapter! (and if, like me, you're unfortunate enough to try testing these 
 on a Windows laptop without a 
 break
  key, then be prepared to get very familiar with 
 the Windows Task Manager's 
 End task
  button).
 http://freepdf-books.com",NA
Client and server,"The basic setup in the client/server model is one device, the server that runs a 
 service and patiently waits for clients to connect and make requests to the service. 
 A 24-hour grocery shop may be a real world analogy. The shop waits for customers 
 to come in and when they do, they request certain products, purchase them and 
 leave. The shop might advertise itself so people know where to find it, but the actual 
 transactions happen while the customers are visiting the shop.
 A typical computing example is a web server. The server listens on a TCP port for 
 clients that need its web pages. When a client, for example a web browser, requires 
 a web page that the server hosts, it connects to the server and then makes a request 
 for that page. The server replies with the content of the page and then the client 
 disconnects. The server advertises itself by having a hostname, which the clients  
 can use to discover the IP address so that they can connect to it.
 In both of these situations, it is the client that initiates any interaction – the server is 
 purely responsive to that interaction. So, the needs of the programs that run on the 
 client and server are quite different.
 Client programs are typically oriented towards the interface between the user and 
 the service. They retrieve and display the service, and allow the user to interact 
 with it. Server programs are written to stay running for indefinite periods of time, 
 to be stable, to efficiently deliver the service to the clients that are requesting it, and 
 to potentially handle a large number of simultaneous connections with a minimal 
 impact on the experience of any one client.
 In this chapter, we will look at this model by writing a simple echo server and client, 
 and then upgrading it to a chat server, which can handle a session with multiple 
 clients. The 
 socket
  module in Python perfectly suits this task.",NA
An echo protocol,"Before we write our first client and server programs, we need to decide how  
 they are going to interact with each other, that is we need to design a protocol  
 for their communication.
 http://freepdf-books.com",NA
Framing,"This problem is called 
 framing
 , and there are several approaches that we can take to 
 handle it. The main ones are described here:
 1. Make it a protocol rule that only one message will be sent per connection, 
 and once a message has been sent, the sender will immediately close  
 the socket.
 2. Use fixed length messages. The receiver will read the number of bytes and 
 know that they have the whole message.
 3. Prefix the message with the length of the message. The receiver will read the 
 length of the message from the stream first, then it will read the indicated 
 number of bytes to get the rest of the message.
 4. Use special character delimiters for indicating the end of a message. The 
 receiver will scan the incoming stream for a delimiter, and the message 
 comprises everything up to the delimiter.
 http://freepdf-books.com",NA
A simple echo server,"As we work through this chapter, we'll find ourselves reusing several pieces of code, 
 so to save ourselves from repetition, we'll set up a module with useful functions 
 that we can reuse as we go along. Create a file called 
 tincanchat.py
  and save the 
 following code in it:
 import socket
 HOST = ''
 PORT = 4040
 def create_listen_socket(host, port):
     """""" Setup the sockets our server will receive connection  
     requests on """"""
     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
     sock.bind((host, port))
     sock.listen(100)
     return sock
 def recv_msg(sock):
     """""" Wait for data to arrive on the socket, then parse into  
     messages using b'\0' as message delimiter """"""
     data = bytearray()
     msg = ''
     # Repeatedly read 4096 bytes off the socket, storing the bytes
     # in data until we see a delimiter
     while not msg:
         recvd = sock.recv(4096)
         if not recvd:
             # Socket has been closed prematurely
             raise ConnectionError()
         data = data + recvd
         if b'\0' in recvd:
             # we know from our protocol rules that we only send
             # one message per connection, so b'\0' will always be
             # the last character
             msg = data.rstrip(b'\0')
     msg = msg.decode('utf-8')
     return msg
 http://freepdf-books.com",NA
Handling the received data,"Note that we're drawing ourselves a careful line with these send and receive 
 functions as regards string encoding. Python 3 strings are Unicode, while the data 
 that we receive over the network is bytes. The last thing that we want to be doing 
 is handling a mixture of these in the rest of our program code, so we're going to 
 carefully encode and decode the data at the boundary of our program, where the 
 data enters and leaves the network. This will ensure that any functions in the rest of 
 our code can assume that they'll be working with Python strings, which will later on 
 make things much easier for us.
 http://freepdf-books.com",NA
The server itself,"Now, let's write our echo server. Open a new file called 
 1.1-echo-server-uni.py
  
 and save the following code in it:
 import tincanchat
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 def handle_client(sock, addr):
     """""" Receive data from the client via sock and echo it back """"""
     try:
         msg = tincanchat.recv_msg(sock)  # Blocks until received
                                          # complete message
         print('{}: {}'.format(addr, msg))
         tincanchat.send_msg(sock, msg)  # Blocks until sent
     except (ConnectionError, BrokenPipeError):
         print('Socket error')
     finally:
         print('Closed connection to {}'.format(addr))
         sock.close()
 http://freepdf-books.com",NA
A simple echo client,"Create a file called 
 1.2-echo_client-uni.py
  and save the following code in it:
 import sys, socket
 import tincanchat
 HOST = sys.argv[-1] if len(sys.argv) > 1 else '127.0.0.1'
 PORT = tincanchat.PORT
 if __name__ == '__main__':
     while True:
         try:
             sock = socket.socket(socket.AF_INET,
                                  socket.SOCK_STREAM)
             sock.connect((HOST, PORT))
             print('\nConnected to {}:{}'.format(HOST, PORT))
             print(""Type message, enter to send, 'q' to quit"")
             msg = input()
             if msg == 'q': break
             tincanchat.send_msg(sock, msg)  # Blocks until sent
             print('Sent message: {}'.format(msg))
 http://freepdf-books.com",NA
Concurrent I/O,"If you're adventurous, then you may have tried connecting to our server using more 
 than one client at once. If you tried sending messages from both of them, then you'd 
 have seen that it does not work as we might have hoped. If you haven't tried this, 
 then give it a go.
 A working echo session on the client should look like this:
 Type message, enter to send. 'q' to quit
 hello world
 Sent message: hello world
 Received echo: hello world
 Closed connection to server
 However, when trying to send a message by using a second connected client, we'll 
 see something like this:
 Type message, enter to send. 'q' to quit
 hello world
 Sent message: hello world
 The client will hang when the message is sent, and it won't get an echo reply.  
 You may also notice that if we send a message by using the first connected client, 
 then the second client will get its response. So, what's going on here?
 The problem is that the server can only listen for the messages from one client at a 
 time. As soon as the first client connects, the server blocks at the 
 socket.recv()
  call 
 in 
 tincanchat.recv_msg()
 , waiting for the first client to send a message. The server 
 isn't able to receive messages from other clients while this is happening and so, when 
 another client sends a message, that client blocks too, waiting for the server to send  
 a reply.
 This is a slightly contrived example. The problem in this case could easily be fixed 
 in the client end by asking the user for an input before establishing a connection to 
 the server. However in our full chat service, the client will need to be able to listen 
 for messages from the server while simultaneously waiting for user input. This is not 
 possible in our present procedural setup.
 There are two solutions to this problem. We can either use more than one thread  
 or process, or use 
 non-blocking
  sockets along with an 
 event-driven
  architecture. 
 We're going to look at both of these approaches, starting with 
 multithreading
 .
 http://freepdf-books.com",NA
Multithreading and multiprocessing,"Python has APIs that allow us to write both multithreading and multiprocessing 
 applications. The principle behind multithreading and multiprocessing is simply 
 to take copies of our code and run them in additional threads or processes. The 
 operating system automatically schedules the threads and processes across available 
 CPU cores to provide fair processing time allocation to all the threads and processes. 
 This effectively allows a program to simultaneously run multiple operations. In 
 addition, when a thread or process blocks, for example, when waiting for IO, the 
 thread or process can be de-prioritized by the OS, and the CPU cores can be allocated 
 to other threads or processes that have actual computation to do.
 Here is an overview of how threads and processes relate to each other:
 Threads exist within processes. A process can contain multiple threads but it always 
 contains at least one thread, sometimes called the 
 main thread
 . Threads within 
 the same process share memory, so data transfer between threads is just a case of 
 referencing the shared objects. Processes do not share memory, so other interfaces, 
 such as files, sockets, or specially allocated areas of shared memory, must be used for 
 transferring data between processes.
 http://freepdf-books.com",NA
Threading and the GIL,"The CPython interpreter (the standard version of Python available for download 
 from 
 www.python.org
 ) contains something called the 
 Global Interpreter Lock
  
 (
 GIL
 ). The GIL exists to ensure that only a single thread in a Python process can run 
 at a time, even if multiple CPU cores are present. The reason for having the GIL is 
 that it makes the underlying C code of the Python interpreter much easier to write 
 and maintain. The drawback of this is that Python programs using multithreading 
 cannot take advantage of multiple cores for parallel computation.
 This is a cause of much contention; however, for us this is not so much of a problem. 
 Even with the GIL present, threads that are blocking on I/O are still de-prioritized 
 by the OS and put into the background, so threads that do have computational work 
 to do can run instead. The following figure is a simplified illustration of this:
 The 
 Waiting for GIL
  state is where a thread has sent or received some data and so is 
 ready to come out of the blocking state, but another thread has the GIL, so the ready 
 thread is forced to wait. In many network applications, including our echo and chat 
 servers, the time spent waiting on I/O is much higher than the time spent processing 
 data. As long as we don't have a very large number of connections (a situation we'll 
 discuss later on when we come to event driven architectures), thread contention 
 caused by the GIL is relatively low, and hence threading is still a suitable architecture 
 for these network server applications.
 http://freepdf-books.com",NA
A multithreaded echo server,"A benefit of the multithreading approach is that the OS handles the thread switches 
 for us, which means we can continue to write our program in a procedural style. 
 Hence we only need to make small adjustments to our server program to make it 
 multithreaded, and thus, capable of handling multiple clients simultaneously.
 Create a new file called 
 1.3-echo_server-multi.py
  and add the following code  
 to it:
 import threading
 import tincanchat
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 def handle_client(sock, addr):
     """""" Receive one message and echo it back to client, then close
         socket """"""
     try:
         msg = tincanchat.recv_msg(sock)  # blocks until received
                                          # complete message
         msg = '{}: {}'.format(addr, msg)
         print(msg)
         tincanchat.send_msg(sock, msg)  # blocks until sent
     except (ConnectionError, BrokenPipeError):
         print('Socket error')
     finally:
         print('Closed connection to {}'.format(addr))
         sock.close()
 if __name__ == '__main__':
     listen_sock = tincanchat.create_listen_socket(HOST, PORT)
     addr = listen_sock.getsockname()
     print('Listening on {}'.format(addr))
     while True:
         client_sock,addr = listen_sock.accept()
         # Thread will run function handle_client() autonomously
         # and concurrently to this while loop
         thread = threading.Thread(target=handle_client,
                                   args=[client_sock, addr],
                                   daemon=True)
         thread.start()
         print('Connection from {}'.format(addr))
 http://freepdf-books.com",NA
Designing a chat server,"We've got a working echo server and it can handle multiple clients simultaneously, 
 so we're pretty close to having a functional chat client. However, our server needs 
 to broadcast the messages it receives to all the connected clients. Sounds simple, but 
 there are two problems that we need to overcome to make this happen.
 First, our protocol needs an overhaul. If we think about what needs to happen from a 
 client's point of view, then we can no longer rely on the simple work flow:
 client connect > client send > server send > client disconnect.
 Clients can now potentially receive messages at any time, and not just when they 
 send a message to the server themselves.
 Second, we need to modify our server to send messages to all of the connected 
 clients. As we are using multiple threads to handle our clients, this means that we 
 need to set up communication between the threads. With this, we're dipping our 
 toe into the world of concurrent programming, and it should be approached with 
 care and forethought. While the shared state of threads is useful, it is also deceptive 
 in its simplicity. Having multiple threads of control asynchronously accessing and 
 changing the same resources is a perfect breeding ground for race conditions and 
 subtle deadlock bugs. While a full discussion on concurrent programming is well 
 beyond the scope of this text, we'll cover some simple principles, which can help 
 preserve your sanity.
 http://freepdf-books.com",NA
A chat protocol,"The main purpose of our protocol update will be to specify that clients must be able 
 to accept all messages that are sent to them, whenever they are sent.
 In theory, one solution for this would be for our client itself to set up a listening 
 socket, so that the server can connect to it whenever it has a new message to deliver. 
 In the real world, this solution will rarely be applicable. Clients are almost always 
 protected by some kind of firewall, which prevents any new inbound connections 
 from connecting to the client. In order for our server to make a connection to a port 
 on our client, we would need to ensure that any intervening firewalls are configured 
 to allow our server to connect. This requirement would make our software much  
 less appealing to most users since there are already chat solutions which don't 
 require this.
 If we can't assume that the server can connect to the client, then we need to meet  
 our requirement by only using the client-initiated connection to the server. There 
 are two ways in which we can do this. First, we can have our clients run in a 
 disconnected state by default, then have them periodically connect to the server, 
 download any waiting messages, and then disconnect again. Alternatively, we can 
 have our clients connect to the server and then leave the connection open. They can 
 then continuously listen on the connection and handle new messages sent by the 
 server in one thread, while accepting user input and sending messages over the  
 same connection in another thread.
 You may recognize these scenarios as the 
 pull
  and 
 push
  options that are available 
 in some e-mail clients. They are called pull and push because of how the operations 
 appear to the client. The client either pulls data from the server, or the server pushes 
 data to the client.
 There are pros and cons to using either of the two approaches, and the decision 
 depends on an application's needs. Pull results in a lower load on the server, but 
 higher latency for the client in receiving messages. While this is fine for many 
 applications, such as e-mail, in a chat server we usually expect immediate updates. 
 While we could poll very frequently, this imposes unneeded load on the client, 
 server, and network as the connections are repeatedly set up and torn down.
 Push is better suited for a chat server. As the connection remains open continuously 
 the amount of network traffic is limited to the initial connection setup, and  
 the messages themselves. Also, the client gets new messages from the server  
 almost immediately.
 http://freepdf-books.com",NA
Handling data on persistent connections,"A new problem which our persistent connection approach raises is that we can no 
 longer assume that our 
 socket.recv()
  call will contain data from only one message. 
 In our echo server, because of how we have defined the protocol, we know that as 
 soon as we see a null byte, the message that we have received is complete, and that 
 the sender won't be sending anything further. That is, everything we read in the last 
 socket.recv()
  call is a part of that message.
 In our new setup, we'll be reusing the same connection to send an indefinite number 
 of messages, and these won't be synchronized with the chunks of data that we will 
 pull from each 
 socket.recv()
 . Hence, it's quite possible that the data from one 
 recv()
  call will contain data from multiple messages. For example, if we send  
 the following:
 caerphilly,
 illchester,
 brie
 Then on the wire they will look like this:
 caerphilly
 \0
 illchester
 \0
 brie
 \0
 Due to the vagaries of network transmission though, a set of successive 
 recv()
  calls 
 may receive them as:
 recv 1: caerphil
 recv 2: ly
 \0
 illches
 recv 3: ter
 \0
 brie
 \0
 http://freepdf-books.com",NA
A multithreaded chat server,"So let's put this to use and write our chat server. Make a new file called 
 2.1-chat_
 server-multithread.py
  and put the following code in it:
 import threading, queue
 import tincanchat
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 send_queues = {}
 lock = threading.Lock()
 def handle_client_recv(sock, addr):
     """""" Receive messages from client and broadcast them to
         other clients until client disconnects """"""
     rest = bytes()
     while True:
         try:
             (msgs, rest) = tincanchat.recv_msgs(sock, rest)
         except (EOFError, ConnectionError):
             handle_disconnect(sock, addr)
             break
 http://freepdf-books.com",NA
Queues,"A 
 Queue
  is a 
 first-in first-out
  (
 FIFO
 ) pipe. You add items to it by using the 
 put()
  
 method, and pull them out by using the 
 get()
  method. The important thing about 
 Queue
  objects is that they are completely 
 thread safe
 . Objects in Python are generally 
 not thread safe unless it is explicitly specified in their documentation. Being thread 
 safe means that operations on the object are guaranteed to be 
 atomic
 , that is, they 
 will always complete without any chance of another thread getting to that object and 
 doing something unexpected to it.
 http://freepdf-books.com",NA
Locks,"Contrast our use of the 
 Queues
  object with our use of 
 send_queues
 . 
 Dict
  objects 
 are not thread safe, and unfortunately there isn't a thread safe associative array 
 type in Python. Since we need to share this 
 dict
 , we need to take extra precautions 
 whenever we access it, and this is where the 
 Lock
  comes in. 
 Lock
  objects are a type of 
 synchronization primitive
 . These are special objects built with functionality to help 
 manage our threads and ensure that they don't trip over each others' accesses.
 http://freepdf-books.com",NA
A multithreaded chat client,"Now that we have a new, all receiving and broadcasting chat server, we just need 
 a client to go with it. We have mentioned before that we will hit a problem with 
 our procedural client when trying to listen for both network data and user input 
 at the same time. Well, now that we have some idea of how to employ threads, we 
 can have a go at addressing this. Create a new text file called 
 2.2-chat_client-
 multithread.py
  and save the following code in it:
 import sys, socket, threading
 import tincanchat
 HOST = sys.argv[-1] if len(sys.argv) > 1 else '127.0.0.1'
 PORT = tincanchat.PORT
 def handle_input(sock):
     """""" Prompt user for message and send it to server """"""    
     print(""Type messages, enter to send. 'q' to quit"")
 http://freepdf-books.com",NA
Event-driven servers,"For many purposes threads are great, especially because we can still program 
 in the familiar procedural, blocking-IO style. But they suffer from the drawback 
 that they struggle when managing large numbers of connections simultaneously, 
 because they are required to maintain a thread for each connection. Each thread 
 consumes memory, and switching between threads incurs a type of CPU overhead 
 called 
 context switching
 . Although these aren't a problem for small numbers of 
 threads, they can impact performance when there are many threads to manage. 
 Multiprocessing suffers from similar problems.
 An alternative to threading and multiprocessing is using the 
 event-driven
  model. 
 In this model, instead of having the OS automatically switch between active threads 
 or processes for us, we use a single thread which registers blocking objects, such as 
 sockets, with the OS. When these objects become ready to leave the blocking state, 
 for example a socket receives some data, the OS notifies our program; our program 
 can then access these objects in non-blocking mode, since it knows that they are in 
 a state that is ready for immediate use. Calls made to objects in non-blocking mode 
 always return immediately. We structure our application around a loop, where we 
 wait for the OS to notify us of activity on our blocking objects, then we handle that 
 activity, and then we go back to waiting. This loop is called the 
 event loop
 .
 This approach provides comparable performance to threading and multiprocessing, 
 but without the memory or context switching overheads, and hence allows for 
 greater scaling on the same hardware. The challenge of engineering applications 
 that can efficiently handle very large numbers of simultaneous connections 
 has historically been called the 
 c10k problem
 , referring to the handling of ten-
 thousand concurrent connections in a single thread. With the help of event-driven 
 architectures, this problem was solved, though the term is still often used to refer to 
 the challenges of scaling when it comes to handling many concurrent connections.
 http://freepdf-books.com",NA
A low-level event-driven chat server,"So the event-driven architecture has a few great benefits, the catch is that for a  
 low-level implementation, we need to write our code in a completely different  
 style. Let's write an event-driven chat server to illustrate this.
 Note that this example will not at all work on Windows as Windows lacks the 
 poll
  
 interface which we will be employing here. There is an older interface, called 
 select
 , 
 which Windows does support, however it is slower and more complicated to work 
 with. The event-driven frameworks that we look at later do automatically switch to 
 select
  for us though, if we're running on Windows.
 There is a higher performance alternative to 
 poll
  called 
 epoll
 , available on Linux 
 operating systems, however it also more complicated to use, so for simplicity we'll 
 stick with 
 poll
  here. Again, the frameworks we discuss later automatically take 
 advantage of 
 epoll
  if it is available.
 Finally, counter-intuitively, Python's 
 poll
  interface lives in a module called 
 select
 , 
 hence we will import 
 select
  in our program.
 Create a file called 
 3.1-chat_server-poll.py
  and save the following code in it:
 import select
 import tincanchat
 from types import SimpleNamespace
 from collections import deque
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 clients = {}
 def create_client(sock):
     """""" Return an object representing a client """"""
     return SimpleNamespace(
                     sock=sock,
                     rest=bytes(),
                     send_queue=deque())
 def broadcast_msg(msg):
     """""" Add message to all connected clients' queues """"""
     data = tincanchat.prep_msg(msg)
     for client in clients.values():
         client.send_queue.append(data)
         poll.register(client.sock, select.POLLOUT)
 http://freepdf-books.com",NA
Frameworks,"As you can see, writing servers using these lower level threading and 
 poll
  APIs 
 can be quite involved, especially considering that various things which would be 
 expected in a production system, such as logging and comprehensive error handling, 
 haven't been included in our examples due to brevity.
 Many people have hit these problems before us, and several libraries and 
 frameworks are available for taking some of the leg work out of writing the  
 network servers.
 http://freepdf-books.com",NA
An eventlet-based chat server,"The 
 eventlet
  library provides a high-level API for event-driven programming, but 
 it does so in a style that mimics the procedural, blocking-IO style that we used in our 
 multithreaded servers. The upshot is that we can effectively take our multithreaded 
 chat server code, make a few minor modifications to it to use 
 eventlet
  instead,  
 and immediately gain the benefits of the event-driven model!
 The 
 eventlet
  library is available in PyPi, and it can be installed with 
 pip
 ,  
 as shown here:
 $ pip install eventlet
 Downloading/unpacking eventlet
 The 
 eventlet
  library automatically falls back to 
 select
  if 
 poll
  is not 
 available, so it will run properly on Windows.
 Once it's installed, create a new file called 
 4.1-chat_server-eventlet.py
  and save 
 the following code in it:
 import eventlet
 import eventlet.queue as queue
 import tincanchat
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 send_queues = {}
 def handle_client_recv(sock, addr):
     """""" Receive messages from client and broadcast them to
         other clients until client disconnects """"""
     rest = bytes()
     while True:
         try:
             (msgs, rest) = tincanchat.recv_msgs(sock)
         except (EOFError, ConnectionError):
             handle_disconnect(sock, addr)
             break
         for msg in msgs:
             msg = '{}: {}'.format(addr, msg)
             print(msg)
             broadcast_msg(msg)
 http://freepdf-books.com",NA
An asyncio-based chat server,"The 
 asyncio
  Standard Library module is new in Python 3.4 and it is an effort at 
 bringing some standardization around asynchronous I/O into the Standard Library. 
 The 
 asyncio
  library uses a co-routine based style of programming. It provides a 
 powerful loop class, which our programs can submit prepared tasks, called  
 co-routines, to, for asynchronous execution. The event loop handles the scheduling 
 of the tasks and optimization of performance around blocking I/O calls.
 It has built-in support for socket-based networking, which makes building a basic 
 server a straightforward task. Let's see how this can be done. Create a new file called 
 5.1-chat_server-asyncio.py
  and save the following code in it:
 import asyncio
 import tincanchat
 HOST = tincanchat.HOST
 PORT = tincanchat.PORT
 clients = []
 class ChatServerProtocol(asyncio.Protocol):
   """""" Each instance of class represents a client and the socket 
        connection to it. """"""
 http://freepdf-books.com",NA
More on frameworks,"I've broken from our usual procedural form and used an object-oriented approach 
 in the last example for two reasons. First, although it is possible to write a purely 
 procedural style server with 
 asyncio
 , it requires a deeper understanding of co-
 routines than what we were able to provide here. If you're curious, then you can 
 go through an example co-routine style echo server, which is in the 
 asyncio 
 documentation at 
 https://docs.python.org/3/library/asyncio-stream.
 html#asyncio-tcp-echo-server-streams
 .
 The second reason is that this kind of class-based approach is generally a more 
 manageable model to follow in a full system.
 There is in fact a new module called 
 selectors
  in Python 3.4, which provides an 
 API for quickly building an object-oriented server based on the IO primitives in the 
 select
  module (including 
 poll
 ). The documentation and an example can be seen at  
 https://docs.python.org/3.4/library/selectors.html
 .
 There are other third-party event-driven frameworks available, popular ones are 
 Tornado (
 www.tornadoweb.org
 ) and circuits (
 https://github.com/circuits/
 circuits
 ). Both are worth investigating for comparison, if you intend to choose a 
 framework for a project.
 Moreover, no discussion of Python asynchronous I/O would be complete without a 
 mention of the Twisted framework. Until Python 3, this has been the go to solution 
 for any serious asynchronous I/O work. It is an event-driven engine, with support 
 for a large number of network protocols, good performance, and a large and active 
 community. Unfortunately, it hasn't finished the jump to Python 3 yet (a view of the 
 migration progress can be seen at 
 https://rawgit.com/mythmon/twisted-py3-
 graph/master/index.html
 ). Since we're focused squarely on Python 3 in this book, 
 we decided to not include a detailed treatment of it. However, once it does get there, 
 Python 3 will have another very powerful asynchronous framework, which will be 
 well worth investigating for your projects.",NA
Taking our servers forward,"There are a number of things that we can do to improve our servers. For 
 multithreaded systems, it's common to have a mechanism for capping the number 
 of threads in use at any one time. This can be done by keeping a count of the active 
 threads and immediately closing any new incoming connections from clients while 
 it's above a threshold.
 http://freepdf-books.com",NA
Summary,"We looked at how to develop network protocols while considering aspects such 
 as the connection sequence, framing of the data on the wire, and the impact these 
 choices will have on the architecture of the client and server programs.
 We worked through different architectures for network servers and clients, 
 demonstrating the differences between the multithreaded and event-driven models 
 by writing a simple echo server and upgrading it to a multi-client chat server. We 
 discussed performance issues around threaded and event-driven architectures. 
 Finally, we looked at the 
 eventlet
  and 
 asyncio
  frameworks, which can greatly 
 simplify the process of writing servers when using an event-driven approach.
 In the next and final chapter of this book, we will look at bringing several threads of 
 this book together for writing server-side web applications.
 http://freepdf-books.com",NA
Applications for the Web,"In 
 Chapter 2
 , 
 HTTP and Working with the Web
 , we explored the HTTP protocol—the 
 primary protocol used by the World Wide Web—and we learned how to use Python 
 as an HTTP client. In 
 Chapter 3
 , 
 APIs in Action
 , we expanded on this and looked at 
 ways to consume web APIs. In this chapter, we'll be turning our focus around and 
 looking at how we can use Python to build applications that serve responses to 
 HTTP requests.
 In this chapter, we'll cover the following:
 • 
 Python web frameworks
 • 
 A Python web application
 • 
 Hosting Python and WSGI
 I should note up front that hosting modern web applications is a very large topic, 
 and a complete treatment is well beyond the scope of this book, where we're focusing 
 on applying Python code to network problems. Topics such as database access, 
 selecting and configuring load balancers and reverse-proxies, containerization, 
 and the system administration techniques needed to keep the whole show up and 
 running won't be covered here. There are many great resources online though that 
 can give you a start, and we'll try to mention as many as we can where relevant,  
 as we go along.
 Having said that, the technologies listed above aren't a requirement for creating  
 and serving Python-based web applications, they're simply what a service comes  
 to require as it reaches scale. As we'll see, there are options for easily manageable 
 small-scale application hosting too.
 http://freepdf-books.com",NA
What's in a web server?,"To understand how we can employ Python in responding to HTTP requests, we 
 need to know a bit about what typically needs to occur in order to respond to a 
 request, and what tools and patterns already exist to do this.
 A basic HTTP request and response might look like this:
 Here our web client sends an HTTP request to a server, where a web server program 
 interprets the request, creates a suitable HTTP response, and sends it back. In this 
 case, the response body is simply the contents of an HTML file read from, with the 
 response headers added by the web server program.
 The web server is responsible for the entire process of responding to the client's 
 request. The basic steps it needs to perform are:
 http://freepdf-books.com",NA
Python and the Web,"Using some of the techniques discussed in this book, in particular 
 Chapter 8
 , 
 Client 
 and Server Applications
 , it is possible to use Python to write a full web server that 
 handles all four of the steps of handling an HTTP request that we listed in the 
 previous section. There are several actively developed web servers already in 
 existence written in pure Python, including Gunicorn (
 http://gunicorn.org
 ),  
 and CherryPy (
 http://www.cherrypy.org
 ). There is even a very basic HTTP  
 server in the standard library http.server module.
 Writing a full HTTP server is not a trivial task and a detailed treatment is well 
 beyond the scope of this book. It is also not a very common requirement nowadays, 
 primarily due to the prevalence of excellent web servers that are already ready to 
 deploy. If you do feel the need to have a crack at this challenge though, I would start 
 with looking through the source code of the web servers mentioned earlier, looking 
 in more detail at the frameworks listed in 
 Chapter 8
 , 
 Client and Server Applications
 , and 
 reading the full HTTP specifications in the relevant RFCs. You may also want to read 
 the WSGI specifications, discussed in the WSGI section later on, so as to allow the 
 server to act as a host for other Python web applications.
 http://freepdf-books.com",NA
Web frameworks,"A web framework is a layer that sits between the web server and our Python code, 
 which provides abstractions and streamlined APIs to perform many of the common 
 operations of interpreting HTTP requests and generating responses. Ideally, it is 
 also structured so that it guides us into employing well-tested patterns for good 
 web development. Frameworks for Python web applications are usually written in 
 Python, and can be considered part of the web application.
 The basic services a framework provides are:
 • 
 Abstraction of HTTP requests and responses
 • 
 Management of the URL space (routing)
 • 
 Separation of Python code and markup (templating)
 There are many Python web frameworks in use today, and here's a non-exhaustive 
 list of some popular ones, in no particular order:
 • 
 Django (
 www.djangoproject.com
 )
 • 
 CherryPy (
 www.cherrypy.org
 )
 • 
 Flask (
 flask.pocoo.org
 )
 • 
 Tornado (
 www.tornadoweb.org
 )
 • 
 TurboGears (
 www.turbogears.org
 )
 • 
 Pyramid (
 www.pylonsproject.org
 )
 An up-to-date list of frameworks is maintained at 
 http://wiki.
 python.org/moin/WebFrameworks
  and 
 http://docs.python-
 guide.org/en/latest/scenarios/web/#frameworks
 .
 There are so many frameworks because there are many approaches that can be taken 
 to the tasks they perform, and many different opinions about what tasks they should 
 even perform.
 http://freepdf-books.com",NA
Flask – a microframework,"To get a taste of working with a Python web framework, we're going to write a small 
 app with Flask. We've chosen Flask because it provides a lean interface, giving us 
 the features we need while getting out of the way and letting us code. Also, it doesn't 
 require any significant preconfiguration, all we need to do is install it, like this:
 >>> pip install flask
 Downloading/unpacking flask
 Flask can also be downloaded from the project's homepage at 
 http://flask.pocoo.
 org
 . Note that to run Flask under Python 3, you will need Python 3.3 or higher.
 Now create a project directory, and within the directory create a text file called 
 tinyflaskapp.py
 . Our app is going to allow us to browse the docstrings for the 
 Python built-in functions. Enter this into 
 tinyflaskapp.py
 :
 from flask import Flask, abort
 app = Flask(__name__)
 app.debug = True
 objs = __builtins__.__dict__.items()
 http://freepdf-books.com",NA
Templating,"You can see from our preceding views that even when cheekily omitting the usual 
 HTML formalities such as 
 <DOCTYPE> 
 and the 
 <html>
  tag to save complexity, 
 constructing HTML in Python code is clunky. It's difficult to get a feel for the 
 overall page, and it's impossible for designers with no Python knowledge to work 
 on the page design. Also, mixing the generation of the presentation code with the 
 application logic makes both harder to test.
 Pretty much all web frameworks solve this problem by employing the template 
 idiom. Since the bulk of the HTML is static, the question arises: Why keep it in 
 the application code at all? With templates, we extract the HTML entirely into 
 separate files. These then comprise HTML code, with the inclusion of some special 
 placeholder and logic markup to allow dynamic elements to be inserted.
 Flask uses another Armin Ronacher creation, the 
 Jinja2
  templating engine, for this 
 task. Let's adapt our application to use templates. In your project folder, create a 
 folder called 
 templates
 . In there, create three new text files, 
 base.html
 , 
 index.
 html
 , and 
 docstring.html
 . Fill them out as follows:
 The 
 base.html
  file will be like this:
 <!DOCTYPE html>
 <html>
 <head>
     <title>Python Builtins Docstrings</title>
 </head>
 <body>
 {% block body %}{% endblock %}
 </body>
 </html>
 http://freepdf-books.com",NA
Other templating engines,"Jinja2 is certainly not the only templating package in existence; you can find a 
 maintained list of Python templating engines at 
 https://wiki.python.org/moin/
 Templating
 .
 Like frameworks, different engines exist because of differing philosophies on what 
 makes a good engine. Some feel that logic and presentation should be absolutely 
 separate and that flow control and expressions should never be available in 
 templates, providing only value substitution mechanisms. Others take the opposite 
 tack and allow full Python expressions within template markup. Others, such as 
 Jinja2, take a middleground approach. And some engines use different schemes 
 altogether, such as XML-based templates or declaring logic via special HTML  
 tag attributes.
 http://freepdf-books.com",NA
Adding some style,"At the moment, our pages look a little plain. Let's add some style. We'll do this by 
 including a static CSS document, but the same approach can be used to include 
 images and other static content. The code for this section can be found in the 
 3-style
  folder in this chapter's source code.
 First create a new 
 static
  folder in your project folder, and in there create a new text 
 file called 
 style.css.
  Save the following to it:
 body        { font-family: Sans-Serif; background: white; }
 h1          { color: #38b; }
 pre         { margin: 0px; font-size: 1.2em; }
 .menuitem   { float: left; margin: 1px 1px 0px 0px; }
 .link       { width: 100px; padding: 5px 25px; background: #eee; }
 .link a      { text-decoration: none; color: #555; }
 .link a:hover { font-weight: bold; color: #38b; }
 Next update the 
 <head>
  section of your 
 base.html
  file to look like this:
 <head>
     <title>Python Builtins Docstrings</title>
     <link rel=""stylesheet"" href=""{{ url_for('static', filename='style.
 css') }}""/>
 </head>
 Note the third and forth lines in the preceding code—that is the 
 <link>
  tag—should 
 be a single line in your code. Try your web application in the browser again and 
 notice that it looks (hopefully) a little more up to date.
 Here we've just added a stylesheet to our boilerplate HTML in 
 base.html
 , adding 
 a 
 <link>
  tag pointing to our 
 static/style.css
  file. We use Flask's 
 url_for()
  
 function for this. The 
 url_for()
  function returns paths to named parts of our URL 
 space. In this case, it's the special 
 static
  folder, which by default Flask looks for in 
 the root of our web application. Another thing we can use 
 url_for()
  for is to get the 
 paths of our view functions, for example, 
 url_for('index')
  would return 
 /
 .
 http://freepdf-books.com",NA
A note on security,"If you're new to web programming, then I strongly recommend you read up on two 
 common types of security flaw in web applications. Both are fairly easily avoided but 
 can have serious consequences if not addressed.",NA
XSS,"The first is 
 Cross-Site Scripting
  (
 XSS
 ). This is where an attacker injects malicious 
 script code into a site's HTML, causing a user's browser to carry out operations in 
 the security context of that site without the user's knowledge. A typical vector is user 
 submitted info being redisplayed to users without proper sanitization or escaping.
 For example, one method is to trick users into visiting URLs containing carefully 
 crafted 
 GET
  parameters. As we saw in 
 Chapter 2
 , 
 HTTP and Working with the Web
 , 
 these parameters can be used by web servers to generate pages, and sometimes 
 their content is included in the HTML of the response page itself. If the server is not 
 careful to replace special characters in the URL parameters with their HTML escape 
 codes when displayed, an attacker can put executable code, for example Javascript, 
 into URL parameters and actually have it executed when that URL is visited. If they 
 can trick a victim into visiting that URL, that code will be executed in the user's 
 browser, enabling the attacker to potentially perform any action the user could.
 The basic XSS prevention is to ensure that any input received from outside the web 
 application is escaped properly when returned to the client. Flask is very helpful in 
 this regard since it activates Jinja2's auto-escaping feature by default, meaning that 
 anything we render via template is automatically protected. Not all frameworks 
 have this feature though, and some that do need it to be manually set. Also, this only 
 applies in situations where your user-generated content can't include markup. In 
 situations like a wiki that allows some markup in user-generated content, you need 
 to take much greater care—see the source code download for this chapter in the 
 5-search
  folder for an example of this. You should always make sure you check out 
 your framework's documentation.
 http://freepdf-books.com",NA
CSRF,"The second form of attack is the 
 Cross-Site Request Forgery
  (
 CSRF
 ). In this attack, 
 a site is tricked into carrying out actions in the security context of a user, without 
 the user's knowledge or consent. Frequently this is initiated by an XSS attack that 
 causes a user's browser to perform an operation on the target site while the user is 
 logged in. It should be noted that this can affect sites even when a user isn't actively 
 browsing them; sites often clear cookie authentication tokens only when a user 
 explicitly logs out, and hence from the site and browser's point of view, any request 
 coming from the browser even after the user has stopped browsing a site—if they 
 haven't logged out—will be as if the user is still logged in.
 One technique to help prevent CSRF attacks is to make potentially abusable 
 operations, such as submitting forms, require a one-time nonce value that is  
 only known to the server and the client. CRSF attacks often take the form of a  
 pre-composed HTTP request, mimicking a user submitting a form or similar. 
 However, if every time a server sends a form to a client it includes a different 
 nonce value, then the attacker has no way of including this in the pre-composed 
 request, and hence the attack attempt can be detected and rejected. This technique 
 is less effective against XSS initiated attacks, and attacks where an attacker is 
 eavesdropping the HTTP traffic of a browsing session. The former is difficult to 
 completely protect against, and  the best solution is to ensure XSS vulnerabilities are 
 not present in the first place. The latter can be mitigated using HTTPS rather than 
 HTTP. See the OWASP pages linked to below for further information.
 Different frameworks have different approaches to providing nonce-based CSRF 
 protection. Flask doesn't have this functionality built in, but it is very easy to add 
 something, for example:
 @app.before_request
 def csrf_protect():
     if request.method == ""POST"":
         token = session.pop('_csrf_token', None)
         if not token or token != request.form.get('_csrf_token'):
             abort(403)
 def generate_csrf_token():
     if '_csrf_token' not in session:
         session['_csrf_token'] = some_random_string()
     return session['_csrf_token']
 app.jinja_env.globals['csrf_token'] = generate_csrf_token
 http://freepdf-books.com",NA
Finishing up with frameworks,"That's as far as we're going to take our dip into Flask, here. There are some examples 
 of further adaptations to our application in the downloadable source code of this 
 chapter, notably form submission, accessing form values in the request, and sessions. 
 The Flask tutorial covers many of these elements in some detail, and is well worth 
 checking out 
 http://flask.pocoo.org/docs/0.10/tutorial/
 .
 So that's a taste of what a very basic Python web application can look like. There are 
 obviously as many ways to write the same app as there are frameworks though, so 
 how do you choose a framework?
 Firstly, it helps to have a clear idea of what you're looking to achieve with your 
 application. Do you require database interaction? If so, a more integrated solution 
 like Django may be quicker to get started with. Will you need a web-based data entry 
 or administration interface? Again if so, Django has this out of the box.
 http://freepdf-books.com",NA
Hosting Python web applications,"As we discussed at the beginning of this chapter, in order to run a Python web 
 application, we need a web server to host it. There are many web servers in existence 
 today, and you will very likely have heard of several. Popular examples are Apache, 
 nginx (pronounced 
 engine-x
 ), lhttpd (pronounced 
 lighty
 ), and Microsoft's 
 Internet 
 Information Services
  (
 IIS
 ).
 There is a lot of terminology around web servers and various mechanisms they can 
 use to invoke Python web applications. We're going to take a very brief tour of the 
 history of web applications to help explain some of these concepts.
 http://freepdf-books.com",NA
CGI,"In the early days of the Web, web servers would mostly only be required to send 
 clients HTML pages, or the occasional image file. As in the earlier figure of a HTTP 
 request journey, these static resources would live on the hard disk of the server, and 
 the web server's main task would be to accept socket connections from clients, map 
 the URL of a request to a local file, and send the file back over the socket as an  
 HTTP response.
 However, with the rise of the need for dynamic content, web servers were given 
 the ability to generate pages by invoking external programs and scripts, which we 
 today call web applications. Web applications originally took the form of scripts or 
 compiled executables that lived on disk next to the regular static content as part of 
 the published web tree. The web server would be configured so that when a client 
 requested these web application files, instead of just reading the file and returning 
 it, the web server would launch a new operating system process and execute the file, 
 returning the result as the requested HTML web page.
 If we update our HTTP request's journey from our earlier image, our request's 
 journey would now look something like this:
 http://freepdf-books.com",NA
Recycling for a better world,"CGI works, but the major drawback is that a new process has to be launched for each 
 request. Launching processes is expensive in terms of operating system resources, 
 and so this approach is very inefficient. Alternatives have been developed.
 Two approaches became common. The first was to make web servers launch and 
 maintain multiple processes at startup, ready to accept new connections— a technique 
 known as 
 pre-forking
 . With this technique, there is still a one-process-per- client 
 relationship, but the processes are already created when a new client connects, 
 improving response time. Also the processes can be reused instead of being  
 re-created anew with each connection.
 Alongside this, web servers were made extensible and bindings were created to 
 different languages so that the web application could be embedded within the web 
 server processes themselves. The most commonly seen examples of these are the 
 various language modules for the Apache web server for languages such as PHP  
 and Perl.
 http://freepdf-books.com",NA
Event-driven servers,"Web client numbers continued to grow though, and the need arose for servers to 
 be able to handle very large numbers of simultaneous client connections, numbers 
 that proved problematic using the multiprocessing approaches. This spurred the 
 development of event-driven web servers, such as 
 nginx
  and 
 lighttpd
 , which can 
 handle many thousands of simultaneous connections in a single process. These 
 servers also leverage preforking, maintaining a number of event-driven processes  
 in line with the number of CPU cores in a machine, and hence making sure the 
 server's resources are fully utilized while also receiving the benefits of the  
 event-driven architecture.",NA
WSGI,"Python web applications were originally written against these early integration 
 protocols: CGI, FastCGI, and a now mostly defunct 
 mod_python
  Apache module. 
 This proved troublesome though since Python web applications were tied to the 
 protocol or server they had been written for. Moving them to a different server or 
 protocol required some reworking of the application code.
 This problem was solved with PEP 333, which defined the 
 Web Services Gateway 
 Interface
  (
 WSGI
 ) protocol. This established a common calling convention for web 
 servers to invoke web application code, similar to CGI. When web servers and web 
 applications both support WSGI, servers and applications can be exchanged with 
 ease. WSGI support has been added to many modern web servers and is nowadays 
 the main method of hosting Python applications on the Web. It was updated for 
 Python 3 in PEP 3333.
 Many of the web frameworks we discussed earlier support WSGI behind the scenes 
 to communicate with their hosting web servers, Flask and Django included. This is 
 another big benefit to using such a framework— you get full WSGI compatibility  
 for free.
 There are two ways a web server can use WSGI to host a web application. Firstly 
 it can directly support hosting WSGI applications. Pure Python servers such as 
 Gunicorn follow this approach, and they make serving Python web applications  
 very easy. This is becoming a very popular way to host Python web applications.
 http://freepdf-books.com",NA
Hosting in practice,"So how does this all work in practice? Well as we saw with Flask, many frameworks 
 come with their own built-in development web servers. However, these are not 
 recommended for use in a production environment as they're generally not designed 
 to be used where security and scalability are important.
 Currently, probably the quickest way to host a Python web application with a 
 production quality server is with the Gunicorn server. Using our Flask application from 
 earlier, we can get it up and running using just a few steps. First we install Gunicorn:
 $ pip install gunicorn
 Next we need to slightly modify our Flask app so that it's use of 
 __builtins__
  
 works correctly under Gunicorn. In your 
 tinyflaskapp.py
  file, find the line:
 objs = __builtins__.__dict__.items()
 Change it to:
 objs = __builtins__.items()
 Now we can run Gunicorn. From within your Flask application project folder, run 
 the following command:
 $ gunicorn --bind 0.0.0.0:5000 tinyflaskapp:app
 This will launch the Gunicorn web server, listening on port 5000 on all available 
 interfaces and serving our Flask application. If we now visit it in a web browser via 
 http://127.0.0.1:5000
 , we should see our documentation index page. There are 
 instructions to daemonize Gunicorn, so that it runs in the background and starts 
 and stops automatically with the system, available in the documentation pages at 
 http://gunicorn-docs.readthedocs.org/en/latest/deploy.html#monitoring
 .
 http://freepdf-books.com",NA
Summary,"We've taken a whistle-stop tour of putting Python applications on the Web. We got 
 an overview of web application architectures and their relationship to web servers. 
 We looked at the utility of Python web frameworks, noting how they give us tools 
 and structure to write better web applications more quickly, and help us integrate 
 our applications with web servers.
 We wrote a tiny application in the Flask web framework, we saw how it can help us 
 elegantly manage our URL space, and how templating engines can help us cleanly 
 manage the seperation of application logic and HTML. We also highlighted a couple 
 of common potential security vulnerabilities— XSS and CSRF— and looked at some 
 basic mitigation techniques.
 Finally, we discussed web hosting architectures and the various methods that can 
 be used to deploy Python web applications to the Web. In particular, WSGI is the 
 standard protocol of web server/web application interaction, and Gunicorn can be 
 used for rapid deployment and scaled with an nginx reverse proxy. Apache with 
 mod_wsgi is also an effective hosting approach.
 We've covered a lot of ground in this book, and there's still plenty more exploring to 
 be done. We hope this book has given you a taste of what's possible and an appetite 
 for discovering more, and that this is just the start of your adventures in network 
 programming with Python.
 http://freepdf-books.com",NA
Working with Wireshark,"When developing network applications, it's often useful to be able to see exactly 
 what's being transmitted over the network. Maybe something weird is going on with 
 your framing, you're trying to discover the user agent for your browser, or you want 
 to see what's happening in the IP protocol or lower layers. We can employ a class of 
 tools called 
 packet sniffers
  to do this.",NA
Packet sniffers,"Packet sniffers are designed to capture all the network traffic that enters and leaves 
 a computer, allowing us to see the full, raw contents of all packets that our programs 
 send and receive, and all the headers and payloads of all the protocols on the stack.
 We're going to take a quick look at one of these applications. It not only provides 
 us with a very useful debugging tool for network programming, it also gives you 
 a direct view of the structure of network traffic and gives you a better feel for the 
 concepts of layering and encapsulation.
 A small word of caution before we begin though; if you're using a computer on a 
 network you do not own, such as at your place of work or study, you should get 
 permission from your network administrator before running a packet sniffer. On 
 networks that use network hubs rather than switches, sniffers may capture data 
 destined for computers other than your own. Also, running a packet sniffer may be 
 against your network's usage policy. Even if it's not, packet sniffers are powerful 
 network monitoring tools and administrators generally like to be aware of when 
 they're being used.
 If this turns out to be difficult, don't panic! This book doesn't rely on having access 
 to a packet sniffer at any point; we just think that you'll find them handy while 
 programming for networks.
 http://freepdf-books.com",NA
Wireshark,"The program that we're going to take a look at is called 
 Wireshark
 . It's an open 
 source packet sniffer with support for interpreting a vast range of network protocols.",NA
Installation,"For Windows and Linux, Wireshark can be downloaded from 
 http://www.
 wireshark.org
 . On Debian, Ubuntu, RHEL, CentOS, and Fedora it's available  
 as the 
 wireshark
  package.
 You'll need to have root or administrator access in order to install this. On Windows, 
 make sure that you install or update the 
 WinPcap
  library if it asks you to do so, and 
 also allow it to start the 
 WinPcap
  driver at boot time when prompted.
 On Debian and Ubuntu, you will need to configure Wireshark to allow regular users 
 to run captures. Run the following command:
 $ sudo dpkg-reconfigure wireshark-common
 Say 
 Yes
  to 
 Should non-superusers be able to capture packets?
  Note that this 
 doesn't automatically allow all non-super users to use Wireshark, they still need to 
 be added to the 
 wireshark
  group. Do this now for your own user, for example:
 $ sudo usermod -aG wireshark myuser
 You may need to log out and log in again for this to take effect, or possibly even 
 reboot. For other Linux distributions, check their documentation, or there are 
 instructions on the Wireshark wiki for assigning these rights at 
 http://wiki.
 wireshark.org/CaptureSetup/CapturePrivileges
 .
 If you run into trouble at any point, you can get further help regarding the 
 installation on the wiki at 
 http://wiki.wireshark.org/CaptureSetup
 .
 Once configured, on Linux, just run 
 wireshark
  in an 
 X
  session to start the  
 graphical interface.",NA
Capturing some packets,"Once you have Wireshark installed and running, you'll see a window that looks  
 like this:
 http://freepdf-books.com",NA
Filtering,"Let's see if we can find the packets that our downloader program has generated. 
 There's probably a fair amount of extra network data in the capture, so first,  
 we need to filter this out.
 Wireshark lets us filter using any property of any of the protocols it supports. To 
 filter, we use the filter box that is under the toolbar. Wireshark has a complete filter 
 language, which you can investigate with the help system. For now, we're just going 
 to do a few basic queries to find our packets. Type 
 http
  in the filter box, and click on 
 the 
 Apply
  button. This restricts the displayed packets to just those that involve the 
 HTTP protocol, as shown in the following screenshot:
 http://freepdf-books.com",NA
Inspecting packets,"Getting back to our RFC downloader packets, let's close the expression window if 
 it's open, and turn our attention to the main window. After applying the 
 http and 
 (ip.addr == 104.20.1.85 or ip.addr == 104.20.0.85)
  filter, we should see 
 two packets listed in the top section of the screen:
 The first is the HTTP request that 
 urlopen()
  sent to the server, and the second is the 
 server's HTTP response.
 http://freepdf-books.com",NA
A versatile tool,"As you'll probably notice from browsing the menus, Wireshark is a very feature-rich 
 network analyzer, and we've barely even scratched the surface of its full capabilities. 
 I encourage you to keep it handy as you work with this book, and do use it wherever 
 you'd like to take a closer look at the data being sent or received over the network.
 http://freepdf-books.com",NA
Index,NA,NA
A,"absolute URL  49
 access  100
 Access ID  72
 Access Secret  72
 ACK  15
 Amazon S3 API
 about  70
 AWS, registering with  71
 boto package  85, 86
 elements, searching  83
 enhancements  84
 error handling  82, 84
 file, downloading with  81
 objects  74
 regions  73
 S3 buckets  74
 S3 command-line client  74-76
 wrapping up  87
 XML, parsing  82
 Amazon Web Services (AWS)
 about  70
 Amazon S3 API, registering with  71
 URL  71
 application layer, TCP/IP networks  16
 asyncio-based chat server  241-243
 asyncio documentation
 URL  244
 atomic  229
 attribute  69
 authentication, Amazon S3 API
 about  71
 AWS user, setting up  72, 73
 authentication, for Twitter
 about  91
 application, registering for Twitter API  91
 requests, authenticating  92
 Twitter client  92, 93
 AWS console
 URL  72",NA
B,"Baiduspider
 URL  108
 base.html file  255
 boto package  85, 86
 broadcast address  10",NA
C,"canned ACLs  
 (canned Access Control Lists)  80
 chat protocol  224
 chat server
 designing  223
 CherryPy
 URL  250
 CIDR notation  9
 client/server model  209, 210
 Common Gateway Interface (CGI)  263
 CommunityData() parameter  145
 concurrent I/O  218
 consumer  100
 content negotiation
 about  40
 content types  40
 http://freepdf-books.com",NA
D,"data handling, on persistent  
 connections  225-227
 dele() method  121
 Django
 URL  251
 DNS look-ups
 about  168, 169
 DNS client/server communications,  
 inspecting  170-172
 dnspython library
 URL  168
 docstring.html file  256
 Domain Name System (DNS)  10, 11, 159
 dot-decimal notation  2
 Dynamic Host Configuration  
 Protocol (DHCP)  4",NA
E,"echo protocol
 about  210, 211
 framing  211, 212
 ElementTree
 about  67
 basics  67
 child  67
 converting, to text  70
 element attributes  69
 output format  68, 69
 parent  67
 siblings  67
 e-mail
 about  111
 retrieving, IMAP used with  
 imaplib  123-125
 retrieving, POP3 used with poplib  121, 122
 sending, via logging module  128-131
 sending, with SMTP  112
 sending, with TLS  117-120
 e-mail attachments
 sending  126, 127
 e-mail message
 composing  113
 sending  114-116
 event-driven servers  233, 267
 eventlet-based chat server  239-241
 event loop  233
 Extended SMTP (ESMTP) protocol  112
 Extensible Markup Language  
 API. 
 See
   XML API",NA
F,"files
 sharing, with SAMBA  153-155
 transferring, through FTP  140, 141
 transferring, through SFTP  139, 140
 first-in first-out (FIFO)  229
 Flask
 about  252-255
 finishing up, with frameworks  261, 262
 security  259
 style, adding  258
 templating  255-257
 templating engines  257
 URL  251
 URL, for tutorial  261
 frameworks  238
 framing  211, 212
 FTP
 files, transferring through  140, 141
 FTP packets, inspecting  142
 URL  141",NA
G,"gateway  7
 GeoIP look-ups  167, 168
 Global Interpreter Lock (GIL)
 about  220, 221
 URL  221
 graphical user interface (GUI)  111
 Gunicorn
 URL  250
 http://freepdf-books.com",NA
H,"handshake  15
 header  5
 host identifier  9
 HTTP (Hypertext Transfer Protocol)
 about  29
 content negotiation  40
 cookies  43
 formal inspection  56-58
 problems, handling  33
 redirects  47
 request  30
 requests, customizing  36
 requests, with urllib  31
 response  30
 response objects  31
 status codes  32
 URLs  48
 user agents  42
 HTTP headers
 about  34-36
 body  34
 headers  34
 HTTP methods
 about  55
 HEAD method  55
 POST method  55, 56
 HTTPS  58
 HTTP status code  24",NA
I,"IETF landing page, for RFCs
 URL  20
 imaplib  123
 IMAP, with imaplib
 used, for retrieving e-mail  123-125
 index.html file  256
 inheritance  257
 installation
 Wireshark  272
 Internet Assigned Numbers Authority 
 (IANA)  4
 Internet Engineering Task Force (IETF)
 about  2
 URL  2
 Internet media types  40
 Internet Message Access  
 Protocol (IMAP)  112
 Internet Message Format (IMF)  112
 Internet Protocol (IP)  1
 Internet Protocol version 4 (IPv4)  2
 Internet Service Provider (ISP)  7
 IP addresses, manipulating
 about  161
 IP address objects  164, 165
 IP address, planning for local area  
 network  165, 166
 IP network objects  162
 network interface objects  163",NA
J,"JavaScript Object Notation (JSON)
 about  87
 decoding  88
 dicts, using with  88
 encoding  88
 object types  89, 90",NA
L,"layer 4, TCP/IP networks
 network ports  13
 TCP  15
 UDP  14
 Light-weight Directory Access  
 Protocol (LDAP)
 about  147
 data, reading  147-150
 LDAP packets, inspecting  151-153
 list() method  121
 local area network (LAN)  133
 localhost  4
 locks  230, 231
 logging module
 e-mail, sending via  128-131
 loopback interface  4
 low-level event-driven chat server  235-238
 http://freepdf-books.com",NA
M,"Mail Transfer Agent (MTA)  111
 main thread  219
 management information base (MIB)  143
 man in the middle (MITM) attack  197
 MibVariable parameter  145
 microframeworks  252
 Microsoft's Internet Information  
 Services (IIS)  262
 MIME types  40
 multiprocessing  219
 Multi-purpose Internet Mail  
 Extensions (MIME)  113
 multithreaded chat client  231, 232
 multithreaded chat server
 about  227-229
 locks  230, 231
 queues  229, 230
 multithreaded echo server  222, 223
 multithreading  218, 219",NA
N,"name resolution  278
 netmask  9
 network address  10
 Network Address Translation (NAT)  5
 network configuration, of local machine
 retrieving  159-161
 network location  48
 network ports  13
 network prefix  9
 network programming, with Python  17
 network stack
 about  17
 program, modifying  21-24
 RFC, downloading  20
 TCP/IP networks, programming for  24
 traceback, generating  17, 18
 using  19, 20
 Network Time Protocol (NTP)  159
 non-blocking socket I/O  193
 nslookup tool  11
 NTP client
 about  172, 173
 NTP client/server communication,  
 inspecting  174
 NTP protocol
 URL  174",NA
O,"oAuth
 URL  92
 octets  2
 OWASP site
 URL  261",NA
P,"packet  5
 packetization  5
 packet sniffers  271
 parameters, Twitter
 'count'  94
 'include_entities'  94
 'include_rts'  94
 'since_id'  95
 path  48
 payload  5
 percent encoding  52
 persistent connections
 data, handling on  225-227
 POP3, with poplib
 used, for retrieving e-mail  121, 122
 poplib  121
 ports  13
 POST method  55, 56
 Post Office Protocol 3 (POP3)  112
 pre-forking technique  264
 private addresses  5
 programming Q&A page
 URL  20
 protocol  179
 protocols, e-mail
 Internet Message Access  
 Protocol (IMAP)  112
 Post Office Protocol 3 (POP3)  112
 Simple Mail Transfer Protocol (SMTP)  112
 pull option  224
 push option  224
 http://freepdf-books.com",NA
Q,"queues  229, 230
 quit() method  121",NA
R,"race condition  230
 redirects  47, 48
 Regional Internet Registries (RIRs)  4
 regional routers  8
 relative URLs  49
 request  30
 Request for Comments (RFC)  19
 request line  35
 request method  35
 requests, customizing
 about  36, 37
 content compression  38, 39
 multiple values  39
 Requests library
 about  59-62
 errors handling with  63
 URL  59
 requests-oauthlib documentation
 URL  100
 requests, with urllib  31
 response  30
 response objects  31, 32
 REST API  70
 REST (Representational State Transfer)  70
 retr() method  121
 reverse proxy  269
 Robots.txt file
 about  108
 URL  108
 root  49
 router  7
 routing table  7",NA
S,"S3 buckets  74
 S3 command-line client
 about  74-76
 bucket, creating with API  76-78
 file, uploading  78, 79
 uploaded file, displaying  
 in web browser  80, 81
 uploaded file, retrieving through web 
 browser  79, 80
 SAMBA
 files, sharing with  153-155
 SAMBA packets, inspecting  156-158
 SAX  67
 scheme  48
 http://freepdf-books.com",NA
T,"TCP/IP networks
 about  2
 DNS  10, 11
 IP addresses  2
 IP addresses, assigning  4
 IP addresses, on Internet  4
 layer 4  13
 layer 5  16
 network interfaces  3
 networks  7, 8
 packets  5, 6
 protocol stack  11, 12
 routing, with IP  9, 10
 TCP/IP networks, programming for
 firewall  24, 25
 IPv6  26, 27
 Network Address Translation (NAT)  25, 26
 TCP port forwarding  190-192
 TCP sockets
 client/server communications,  
 inspecting  182-184
 client/server interaction,  
 inspecting  187, 188
 TCP servers  185, 186
 working with  179-181
 http://freepdf-books.com",NA
U,"UDP sockets
 working with  188, 189
 UdpTransportTarget() parameter  145
 upstream  7
 urllib package  29
 URLs
 about  48, 49
 absolute URL  49
 in summary  54
 path URLs  49, 50
 query string  51, 52
 relative URLs  49, 50
 URL encoding  52-54
 user agents
 about  42, 43
 URL  108
 User Datagram Protocol (UDP)
 about  13, 14
 versus TCP  16",NA
V,views  254,NA
W,"web API  65
 web framework
 about  251
 basic services  251
 webmaster
 about  107
 Robots.txt file  108
 user agent, selecting  108
 web server  248-250
 Web Services Gateway Interface (WSGI) 
 protocol  267, 268
 Wireshark
 about  272
 filtering  276-278
 installing  272
 packets, capturing  272-276
 packets, inspecting  279-281
 URL  272
 URL, for wiki  272
 versatile tool  281",NA
X,"XML API
 about  66
 approaches  66
 ElementTree  67, 68
 XML parsing, Amazon S3 API  82
 XSS (Cross-Site Scripting)
 about  259
 URL  261
 http://freepdf-books.com",NA
Thank you for buying ,NA,NA
 ,NA,NA
Learning Python Network  ,NA,NA
Programming,NA,NA
About Packt Publishing,"Packt, pronounced 'packed', published its first book, 
 Mastering phpMyAdmin for Effective 
 MySQL Management
 , in April 2004, and subsequently continued to specialize in publishing 
 highly focused books on specific technologies and solutions.
 Our books and publications share the experiences of your fellow IT professionals in adapting 
 and customizing today's systems, applications, and frameworks. Our solution-based books 
 give you the knowledge and power to customize the software and technologies you're using 
 to get the job done. Packt books are more specific and less general than the IT books you have 
 seen in the past. Our unique business model allows us to bring you more focused information, 
 giving you more of what you need to know, and less of what you don't.
 Packt is a modern yet unique publishing company that focuses on producing quality,  
 cutting-edge books for communities of developers, administrators, and newbies alike.  
 For more information, please visit our website at 
 www.packtpub.com
 .",NA
About Packt Open Source,"In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order  
 to continue its focus on specialization. This book is part of the Packt Open Source brand,  
 home to books published on software built around open source licenses, and offering 
 information to anybody from advanced developers to budding web designers. The Open 
 Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty 
 to each open source project about whose software a book is sold.",NA
Writing for Packt,"We welcome all inquiries from people who are interested in authoring. Book proposals should 
 be sent to 
 author@packtpub.com
 . If your book idea is still at an early stage and you would 
 like to discuss it first before writing a formal book proposal, then please contact us; one of our 
 commissioning editors will get in touch with you. 
 We're not just looking for published authors; if you have strong technical skills but no writing 
 experience, our experienced editors can help you develop a writing career, or simply get some 
 additional reward for your expertise.
 http://freepdf-books.com",NA
Python Network  ,NA,NA
Programming Cookbook,"ISBN: 978-1-84951-346-3              Paperback: 234 pages
 Over 70 detailed recipes to develop practical 
 solutions for a wide range of real-world network 
 programming tasks
 1. 
 Demonstrates how to write various besopke 
 client/server networking applications  
 using standard and popular third-party  
 Python libraries.
 2. 
 Learn how to develop client programs for 
 networking protocols such as HTTP/HTTPS, 
 SMTP, POP3, FTP, CGI, XML-RPC, SOAP  
 and REST.",NA
Functional Python Programming,"ISBN: 978-1-78439-699-2             Paperback: 360 pages
 Create succinct and expressive implementations  
 with functional programming in Python
 1. 
 Implement common functional  
 programming design patterns  
 and techniques in Python.
 2. 
 Learn how to choose between imperative  
 and functional approaches based on 
 expressiveness, clarity, and performance.
  
 Please check 
 www.PacktPub.com
  for information on our titles
 http://freepdf-books.com",NA
Mastering Object-oriented Python,"ISBN: 978-1-78328-097-1            Paperback: 634 pages
 Grasp the intricacies of object-oriented programming 
 in Python in order to efficiently build powerful  
 real-world applications
 1. 
 An object-oriented approach to Python  
 web development gives you a much more  
 fully-realised experience of the language.  
 The flexibility and power of Python, combined 
 with the improvements in design, coding  
 and software maintenance that object-oriented 
 programming allows, is built to respond to 
 the challenges of increasingly more complex 
 and data-intensive application development, 
 making difficult tasks much more manageable.",NA
Parallel Programming  ,NA,NA
with Python,"ISBN: 978-1-78328-839-7             Paperback: 128 pages
 Develop efficient parallel systems using  
 the robust Python environment
 1. 
 Demonstrates the concepts of Python  
 parallel programming.
 2. 
 Boosts your Python computing capabilities.
 3. 
 Contains easy-to-understand explanations  
 and plenty of examples.
  
 Please check 
 www.PacktPub.com
  for information on our titles
 http://freepdf-books.com",NA
