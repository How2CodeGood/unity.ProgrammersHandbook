Larger Text,Smaller Text,Symbol
"Advanced programming language design.  Source: DBLP CITATIONS  READS  132 PUBLICATIONS 8,823 CITATIONS SEE PROFILE The user has requested enhancement of the downloaded file.","Book
  · 
 January 1996
  
  
  
 47
  
  
 21,620
  
 1 author:
  
  
 Raphael Finkel 
  
 University of Kentucky
  
  
  
 All content following this page was uploaded by 
 Raphael Finkel
  on 16 December 2013.",NA
ADVANCED PROGRAMMING,NA,NA
LANGUAGE DESIGN,NA,NA
Raphael A. Finkel,NA,NA
O,UNIVERSITY OF KENTUCKY,NA
L ,NA,NA
MM,"Addison-Wesley Publishing Company
  
 Menlo Park, California • Reading, Massachusetts 
  
 New York • Don Mills, Ontario • Harlow, U.K. • Amsterdam Bonn • 
 Paris • Milan • Madrid • Sydney • Singapore • Tokyo Seoul • Taipei • 
 Mexico City • San Juan, Puerto Rico
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to print or photo-copy this 
 document for a fee of $0.02 per page, per copy, payable to Addison-Wesley Publishing Company. All other rights reserved.",NA
L ,NA,NA
MM,"Addison-Wesley Publishing Company, Inc. 
 2725 Sand Hill Road 
  
 Menlo Park, CA 94025",NA
Preface,"This book stems in part from courses taught at the University of Kentucky and at the 
 University of Wisconsin–Madison on programming language de-sign. There are 
 many good books that deal with the subject at an undergrad-uate level, but there are 
 few that are suitable for a one-semester graduate-level course. This book is my 
 attempt to fill that gap.
  
 The goal of this course, and hence of this book, is to expose first-year grad-uate 
 students to a wide range of programming language paradigms and is-sues, so that 
 they can understand the literature on programming languages and even conduct 
 research in this field. It should improve the students’ ap-preciation of the art of 
 designing programming languages and, to a limited degree, their skill in 
 programming.
  
  
 This book does not focus on any one language, or even on a few languages; it 
 mentions, at least in passing, over seventy languages, including well-known ones 
 (Algol, Pascal, C, C++, LISP, Ada, FORTRAN), important but less known ones (ML, SR, 
 Modula-3, SNOBOL), significant research languages (CLU, Alphard, Linda), and little-
 known languages with important concepts (Io, Go..del). Several languages are 
 discussed in some depth, primarily to rein-force particular programming paradigms. 
 ML and LISP demonstrate func-tional 
  
 programming, 
  
 Smalltalk 
  
 and 
  
 C++ demonstrate 
  
 object-oriented programming, and Prolog 
 demonstrates logic programming.
  
 Students are expected to have taken an undergraduate course in program-ming 
 languages before using this book. The first chapter includes a review of much of the 
 material on imperative programming languages that would be covered in such a 
 course. This review makes the book self-contained, and also makes it accessible to 
 advanced undergraduate students.
  
 Most textbooks on programming languages cover the well-trodden areas of the 
 field. In contrast, this book tries to go beyond the standard territory, making brief 
 forays into regions that are under current research or that have been proposed and 
 even rejected in the past. There are many fascinating con-structs that appear in very 
 few, if any, production programming languages. Some (like power loops) should 
 most likely not be included in a programming language. Others (like Io 
 continuations) are so strange that it is not clear how to program with them. Some 
 (APL arrays) show alternative ways to structure languages. These unusual ideas are 
 important even though they do not pass the test of current usage, because they 
 elucidate important aspects of programming language design, and they allow 
 students to evaluate novel concepts.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 xi",NA
Chapter 1 ,NA,NA
O,NA,NA
Introduction,"The purpose of this book is to study the principles and innovations found in modern 
 programming languages. We will consider a wide variety of lan-guages. The goal is 
 not to become proficient in any of these languages, but to learn what contributions 
 each has made to the “state of the art” in language design.
  
  
 I will discuss various programming paradigms in this book. Some lan-guages 
 (such as Ada, Pascal, Modula-2) are
  imperative
 ; they use variables, assignments, 
 and iteration. For imperative languages, I will dwell on such is-sues as flow of 
 control (Chapter 2) and data types (Chapter 3). Other lan-guages (for example, LISP 
 and FP) are
  functional
 ; they have no variables, assignments, or iteration, but model 
 program execution as expression evalua-tion. I discuss functional languages in 
 Chapter 4. Other languages (for ex-ample, Smalltalk and C++), represent the
  object-
 oriented
  paradigm, in which data types are generalized to collections of data and 
 associated rou-tines (Chapter 5).
  Dataflow languages
  (Val, Sisal, and Post, Chapter 
 6) at-tempt to gain speed by simultaneous execution of independent computations; 
 they require special computer architectures. A more common way to gain speed is 
 by
  concurrent
  programming (typified by languages such as SR and Lynx, discussed 
 in Chapter 7). 
  
 Another major paradigm constitutes the 
 declarative
  languages such as Prolog and Go..del (Chapter 8); they view pro-
 gramming as stating what is wanted and not necessarily how to compute it. 
 Aggregate languages
  (Chapter 9) form a a final loosely knit paradigm that includes 
 languages with special-purpose data formats, such as strings (SNOBOL and Icon), 
 arrays (APL), databases (dBASE and SQL), and mathe-matical formulas 
 (Mathematica and Maple).
  
 In addition to studying actual programming language constructs, I will present 
 formal semantic models in Chapter 10. These models allow a precise specification of 
 what a program means, and provide the basis for reasoning about the correctness of 
 a program.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 1",NA
1,N,NA
 PROGRAMMING LANGUAGES AS ,NA,NA
SOFTWARE TOOLS,"Programming languages fit into a larger subject that might be termed
  soft-ware 
 tools
 . This subject includes such fields as interactive editors (text, pic-ture, 
 spreadsheet, bitmap, and so forth), data transformers (compilers, assemblers, 
 stream editors, macro processors, text formatters), operating sys-tems, database 
 management systems, and tools for program creation, testing, and maintenance 
 (script files, source-code management tools, debuggers).
  
 In general, software tools can be studied as interfaces between clients, which are 
 usually humans or their programs, and lower-level facilities, such as files or 
 operating systems.
  
 Figure 1.1 
 tools
  
 Software
  
 Client
  
 Interface
  
 Implementation
  
 Three questions arising from Figure 1.1 are worth discussing for any software tool:
  
 1. 
  
 2.
  
 3.
  
 What is the nature of the interface?
  
 How can the interface be implemented by using the lower-level facili-ties?
  
 How useful is the interface for humans or their agents?
  
 When we deal with programming languages as software tools, these questions are 
 transformed:
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,N,NA
 EVALUATING PROGRAMMING LANGUAGES,"This book introduces you to some unusual languages and some unusual lan-guage 
 features. As you read about them, you might wonder how to evaluate the quality of a 
 feature or an entire language. Reasonable people disagree on what makes for a great 
 language, which is why so many novel ideas abound in the arena of programming 
 language design. At the risk of oversimplifica-tion, I would like to present a short list 
 of desiderata for programming lan-guages [Butcher 91]. Feel free to disagree with 
 them. Another excellent discussion of this topic is found in Louden [Louden 93].
  
 •
  Simplicity.
  There should be as few basic concepts as possible. Often the job of the 
 language designer is to discard elements that are superfluous, error-prone, hard 
 to read, or hard to compile. Many people consider PL/I, for example, to be much 
 too large a language. Some criticize Ada for the same reason.
  
 •
  Uniformity.
  The basic concepts should be applied consistently and uni-
  
 versally. We should be able to use language features in different contexts 
  
 without changing their form. Non-uniformity can be annoying. In Pascal, 
  
 constants cannot be declared with values given by expressions, even 
  
 though expressions are accepted in all other contexts when a value is 
  
 needed. 
  
 Non-uniformity can also be error-prone. 
  
 In Pascal, some
  for 
  
 loops take a single statement as a body, but
  repeat
  loops can take any 
  
 number of statements. It is easy to forget to bracket multiple statements 
  
 in the 
 body of a
  for
  loop.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,N,NA
 BACKGROUND MATERIAL ON ,NA,NA
PROGRAMMING LANGUAGES 1,"Before showing you anything out of the ordinary, I want to make sure that you are 
 acquainted with the fundamental concepts that are covered in an un-dergraduate 
 course in programming languages. This section is intentionally concise. If you need 
 more details, you might profitably refer to the fine books by Pratt [Pratt 96] and 
 Louden [Louden 93].
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  In a formal sense, all practical languages are Turing-complete; that is, they can express exactly the 
 same algorithms. However, the ease with which a programmer can come up with an appropriate program 
 is part of what I mean by expressiveness. Enumerating binary trees (see Chapter 2) is quite difficult in 
 most languages, but quite easy in CLU.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
"3.1 Variables, Data Types, Literals, and ",NA,NA
Expressions ,"I will repeatedly refer to the following example, which is designed to have a little bit 
 of everything in the way of types. A
  type
  is a set of values on which the same 
 operations are defined.
  
 Figure 1.2
  
 variable
  
 1
  
 First :
  pointer to
  integer;
  
 2
  
 Second :
  array
  0..9
  of
  
 3
  
 record
  
 4
  
 Third: character;
  
 5
  
 Fourth: integer;
  
 6
  
 Fifth : (Apple, Durian, Coconut, Sapodilla,
  
 7
  
 Mangosteen)
  
 8
  
 end
 ;
  
 9
  
 begin
  
 10
  
 First := nil;
  
 11
  
 First := &Second[1].Fourth;
  
 12
  
 Firstˆ := 4;
  
 13
  
 Second[3].Fourth := (Firstˆ + Second[1].Fourth) *
  
 14
  
 Second[Firstˆ].Fourth;
  
 15
  
 Second[0] := [Third : ’x’; Fourth : 0;
  
 16
  
 Fifth : Sapodilla];
  
 17
  
 end
 ;
  
 18
  
 Imperative languages (such as Pascal and Ada) have
  variables
 , which are named 
 memory locations. Figure 1.2 introduces two variables,
  First
  (line 2) and
  Second
  
 (lines 3–9). Programming languages often restrict the values that may be placed in 
 variables, both to ensure that compilers can generate accu-rate code for 
 manipulating those values and to prevent common programming errors. The 
 restrictions are generally in the form of type information. The 
 type
  of a variable is a 
 restriction on the values it can hold and what opera-tions may be applied to those 
 values. For example, the type
  integer
  encom-passes numeric whole-number values 
 between some language-dependent (or implementation-dependent) minimum and 
 maximum value; values of this type may act as operands in arithmetic operations 
 such as addition. The term
  integer
  is not set in bold monospace type, because in most 
 languages, predefined types are not reserved words, but ordinary identifiers that 
 can be given new meanings (although that is bad practice).
  
 Researchers have developed various taxonomies to categorize types
  
 [ISO/IEC 94; Meek 94].
  
 I will present here a fairly simple taxonomy. A
  
 primitive type
  is one that is not built out of other types. Standard primitive types 
 provided by most languages include
  integer
 ,
  Boolean
 ,
  character
 ,
  real
 , and sometimes
  
 string
 . Figure 1.2 uses both
  integer
  and
  character
 . Enu-meration types are also 
 primitive. The example uses an enumeration type in lines 7–8; its values are 
 restricted to the values specified. Enumeration types often define the order of their 
 enumeration constants. In Figure 1.2, however,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.2 Control Constructs ,"Execution of imperative programming languages proceeds one
  statement
  at a time. 
 Statements can be
  simple
  or
  compound
 . Simple statements include the assignment 
 statement, procedure invocation, and
  goto
 . Compound state-ments enclose other 
 statements; they include conditional and iterative state-ments, such as
  if
 ,
  case
 ,
  while
 , 
 and
  for
 . Programming languages need some syntax for delimiting enclosed 
 statements in a compound statement. Some languages, like Modula, provide closing 
 syntax for each compound statement:
  
 Figure 1.5
  
 while
  Firstˆ < 10
  do
  
 1
  
 Firstˆ := 2 * Firstˆ;
  
 2
  
 Second[0].Fourth := 1 + Second[0].Fourth;
  
 3
  
 end
 ;
  
 4
  
 The
  end
  on line 4 closes the
  while
  on line 1. Other languages, like Pascal, only allow a 
 single statement to be included, but it may be a
  block state-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.3 Procedures and Parameter Passing ,"Figure 1.14 will be discussed in detail in this section. For clarity, I have cho-sen a 
 syntax that names each formal parameter at the point of invocation; Ada and 
 Modula-3 have a similar syntax.
  
 Figure 1.14
  
 procedure
  TryAll(
  
 1
  
 ValueInt :
  value
  integer;
  
 2
  
 ReferenceInt :
  reference
  integer;
  
 3
  
 ResultInt :
  result
  integer;
  
 4
  
 ReadOnlyInt :
  readonly
  integer := 10;
  
 5
  
 NameInt :
  name
  integer;
  
 6
  
 MacroInt :
  macro
  integer) : integer;
  
 7
  
 variable
  
 8
  
 LocalInt : integer;
  
 9
  
 begin
  
 10
  
 LocalInt := 10; -- affects only TryAll’s LocalInt
  
 11
  
 ValueInt := 1 + ValueInt; -- formal becomes 16
  
 12
  
 ReferenceInt := 1 + ValueInt;
  
 13
  
 -- actual and formal become 17
  
 14
  
 ResultInt := 1 + ReferenceInt + ReadOnlyInt + NameInt;
  
 15
  
 -- 47
  
 16
  
 return
  2*MacroInt; -- 40
  
 17
  
 end
 ; -- TryAll
  
 18
  
 variable
  
 19
  
 LocalInt : integer;
  
 20
  
 A, B : integer;
  
 21
  
 begin
  -- main program
  
 22
  
 LocalInt := 3;
  
 23
  
 B := TryAll(
  
 24
  
 ValueInt : 15,
  
 25
  
 ReferenceInt : LocalInt,
  
 26
  
 ResultInt : A, -- becomes 47
  
 27
  
 ReadOnlyInt
  
 : 12,
  
 28
  
 NameInt : LocalInt,
  
 29
  
 MacroInt : 2*LocalInt)
  
 30
  
 );
  
 31
  
 -- Final values: LocalInt = 17, A = 47, B = 40
  
 32
  
 end
 ; -- main program
  
 33
  
 Procedures
  (often called
  functions
  if they return values) are usually de-clared 
 with a header, local declarations, and a body. The
  header
  (lines 1–7) indicates the 
 procedure name and the parameters, if any, along with their types and modes. If the 
 procedure is to return a value, the type of the value is also declared. If not, the 
 predeclared type
  void
  is used in some languages to indicate that no value at all is 
 returned. The declarations (lines 8 and 9) introduce local meanings for identifiers. 
 Together, the parameters and the lo-cal identifiers constitute the
  local referencing 
 environment
  of the proce-dure. Identifiers appearing within the procedure are 
 interpreted, if possible, with respect to the local referencing environment. 
 Otherwise, they are inter-preted with respect to parts of the program outside the 
 procedure. The nonlo-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.4 Block Structure ,"I will describe classic Algol block structure here; it has been adopted, with 
 modification, in many programming languages. A program is divided into nested
  
 blocks
 , each of which introduces a new name scope. A
  name scope 
 is a region of 
 program in which particular declarations of identifiers are in ef-fect. A declaration
  
 maps
  an identifier to a meaning. We also say that it 
 binds
  the meaning to the 
 identifier. The meanings can be variables, types, constants, labels, procedures, or 
 other concepts discussed elsewhere in the book, such as modules (Chapter 3), 
 classes (Chapter 5), and monitors (Chap-ter 7). Traditionally, each nested name 
 scope inherits all bindings from the surrounding scope, except that if the same 
 identifier is redefined in the nested scope, the new declaration
  overrides
  the old 
 declaration for the dura-tion of the nested scope. Some languages, such as Ada and 
 C++, allow de-clared procedures to be overloaded; that is, the same name is bound 
 to multiple declarations at the same time, and the compiler chooses which is meant 
 by the number and types of the parameters.
  
 The new declarations can be defined to take effect from the beginning of the 
 block (so that an earlier declaration, say of a variable, can refer to a later 
 declaration, perhaps of a type). More commonly, they take effect (are
  elabo-rated
 ) 
 from the point in the block where the declaration appears. In the fol-lowing 
 example, I could define
  B
  in line 8 to be either
  real
  or
  integer
 , depending on whether 
 the outer declaration of
  T
  is hidden yet by the declara-tion in line 10. Usually, 
 languages either disallow such references or let the new declaration take effect only 
 after the point at which it appears. This de-cision makes one-pass compilers easier 
 to write.
  
 Figure 1.17
  
 type
  -- introduces outer block
  
 1
  
 T : real;
  
 2
  
 variable
  -- continues outer block
  
 3
  
 A : integer;
  
 4
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.5 Runtime Store Organization  9,"Programmers usually don’t care how runtime store is organized. They expect the 
 compiler or interpreter to arrange the program and data for efficient exe-cution. 
 They are only interested if some language constructs are likely to use large amounts 
 of space or time. However, language designers are definitely interested in runtime 
 store organization because it affects the efficient imple-mentation of the language.
  
 Runtime store is typically divided into several regions. The first region holds the 
 compiled program instructions, which I will just call
  code
 . This re-gion contains 
 each procedure in the program as well as runtime libraries. Under some operating 
 systems, the libraries may be shared among processes and may be brought into 
 store dynamically when they are first referenced.
  
 A second region holds global variables. Because the compiler knows the identity, 
 type, and size of these variables, it can allocate precise amounts of store and can 
 generate code that accesses global variables very efficiently.
  
  
 A third region is the
  central stack
 . It holds an activation record for each active 
 procedure instance. Because procedures are invoked and return in 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  
  More recent LISP languages, such as Common LISP and Scheme, are statically scoped.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 FINAL COMMENTS,"This chapter has attempted to introduce the study of programming languages by 
 placing it in the context of software tools in general. The background ma-terial on 
 programming languages is, of necessity, very concise. Its aim is to lay the foundation 
 for the concepts developed in the rest of this book.
  
 The language concepts introduced here are in some sense the classical Al-gol-
 like structures. They are developed in various directions in the following chapters, 
 each of which concentrates first on one programming language and then shows 
 ideas from a few others to flesh out the breadth of the topic. Where appropriate, 
 they end with a more mathematical treatment of the sub-ject. Chapter 2 shows 
 nonclassical control structures. Chapter 3 investigates the concept of data type. It 
 presents a detailed discussion of ML, which shows how polymorphism can be 
 incorporated in a statically typed language. Because ML is mostly a functional 
 language, you may want to read Chapter 4 before the section on ML in Chapter 3.
  
 The next chapters are devoted to nonclassical paradigms, that is, lan-guages not 
 descended from Algol. Chapter 4 discusses functional program-ming, concentrating 
 on LISP. The concept of abstract data type is generalized
  
 in several ways in the next three chapters.
  
 Chapter 5 introduces object-
  
 oriented programming, concentrating on Smalltalk and C++. Chapter 6 dis-cusses 
 dataflow languages, concentrating on Val. Chapter 7 shows some of the wide range 
 of development of languages for concurrent programming. A very different view of 
 programming is presented in Chapter 8, which is is de-
  
 voted to logic programming, concentrating on Prolog.
  
 Languages dealing
  
 with special-purpose data aggregates, such as strings, arrays, databases, and
  
 mathematical formulas, are discussed in Chapter 9.
  
 Finally, Chapter 10
  
 shows several mathematical approaches to formalizing the syntax and se-mantics of 
 programming languages; although it uses imperative languages as its model, such 
 approaches have been used for the other language paradigms as well.",NA
EXERCISES ,NA,NA
Review ,NA,NA
Exercises,"1.1 
  
 1.2 
  
 1.3 
  
 1.4
  
 In what ways does C (or pick another language) fall short of the criteria in 
 Section 2 for excellence?
  
 How would you define the
  mod
  operator?
  
 Show a code fragment in which short-circuit semantics for
  or
  yield a dif-ferent 
 result than complete-evaluation semantics.
  
 Why do most languages with
  case
  statements prefer that the conditions have 
 compile-time values?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Challenge Exercises,"1.9 
  
 Why are array slices usually allowed only in the last dimension?
  
 1.10
  Write a program that prints the index of the first all-zero row of an
  n
  
  n 
 integer 
 matrix
  M
  [Rubin 88]. The program should access each element of the matrix at 
 most once and should not access rows beyond the first all-zero row and 
 columns within a row beyond the first non-zero element. It should have no 
 variables except the matrix
  M
  and two loop indices
  Row 
 and
  Column
 . The 
 program may not use
  goto
 , but it may use multilevel 
 break
  and
  next
 .
  
 1.11
  What is the meaning of a
  goto
  from a procedure when the target is out-
  
 side 
 the procedure?
  
 1.12
  Why do
  goto
  labels passed as parameters require closures?
  
 1.13
  Rewrite Figure 1.21 (page 21) so that procedure
  A
  takes a label instead of a 
 procedure. The rewritten example should behave the same as Fig-ure 1.21.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 2 ,NA,NA
O,NA,NA
Control Structures,"Assembler language only provides
  goto
  and its conditional variants. Early high-level 
 languages such as FORTRAN relied heavily on
  goto
 , three-way arithmetic branches, 
 and many-way indexed branches. Algol introduced con-trol structures that began to 
 make
  goto
  obsolete. Under the banner of “struc-tured programming,” computer 
 scientists such as C. A. R. Hoare, Edsger W. Dijkstra, Donald E. Knuth, and Ole-Johan 
 Dahl showed how programs could be written more clearly and elegantly with
  while
  
 and
  for
  loops,
  case
  state-ments, and loops with internal exits [Knuth 71; Dahl 72]. 
 One of the tenets of structured programming is that procedures should be used 
 heavily to modu-larize effort. In this chapter we will explore control structures that 
 are a lit-tle out of the ordinary.",NA
1,N,NA
 EXCEPTION HANDLING,"If a procedure discovers that an erroneous situation (such as bad input) has arisen, 
 it needs to report that fact to its caller. One way to program this be-havior is to have 
 each procedure provide an error return and to check for that return on each 
 invocation. SNOBOL allows an explicit failure
  goto
  and suc-cess
  goto
  on each 
 statement, which makes this sort of programming conve-
  
 nient.
  
 However, using a
  goto
  to deal with errors does not lead to clear
  
 programs, and checking each procedure invocation for error returns makes for 
 verbose programs.
  
 A control construct for dealing with error conditions was first proposed by 
 Goodenough [Goodenough 75] and has found its way into languages like Ada, Mesa, 
 CLU, ML, Eiffel, and Modula-3. I will use a syntax like Ada’s for de-scribing this 
 control structure.
  
 When a procedure needs to indicate failure, it
  raises
  an
  exception
 . This action 
 causes control to transfer along a well-defined path in the program to where the 
 exception 
 is
  
 handled
 . 
 To 
 embed 
 this 
 concept 
 in 
 programming 
 lan-
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 27",NA
2,N,NA
 COROUTINES 1,"Consider the problem of comparing two binary trees to see if their nodes have the 
 same values in symmetric (also called in-order) traversal. For example, the trees in 
 Figure 2.7 compare as equal.
  
 Figure 2.7 
  
 binary trees
  
 Equivalent
  
 A
  
 B
  
 D
  
 E
  
 A
  
 B
  
 D
  
 E
  
 C
  
 C
  
 We could use a recursive procedure to store the symmetric-order traversal in an 
 array, call the procedure for each tree, and then compare the arrays, but it is more 
 elegant to advance independently in each tree, comparing as we go. Such an 
 algorithm is also far more efficient if the trees are unequal near the 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  An error algebra with good numeric properties is discussed in [Wetherell 83].
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.1 Coroutines in Simula ,"Simula provides explicit coroutines that have just the effect we need. Simula classes 
 are introduced in Chapter 3 as a way to implement abstract data types. Here I will 
 show you another use.
  
 A class is a data type much like a record structure, but it may also contain 
 procedures and initialization code. When a variable is declared of that class type, or 
 when a value is created at runtime from the heap using a
  new
  call, an instance of the 
 class is created. This instance is often called an
  object
 ; in fact, the concept of object-
 oriented programming, discussed in Chapter 5, is derived largely from Simula 
 classes. After space is allocated (either on the stack or the heap), the initialization 
 code is run for this object. Programmers usually use this facility to set up the object’s 
 data fields. However, the initial-ization code may suspend itself before it completes. 
 I will call an object that has not completed its initialization code an
  active object
 . An 
 active object’s fields may be inspected and modified, and its procedures may be 
 called. In addition, its initialization code can be resumed from the point it 
 suspended.
  
 Because the initialization can invoke arbitrary procedures that may sus-pend at 
 any point during their execution, each object needs its own stack un-til its 
 initialization has completed. An active object is therefore a
  coroutine
 , that is, an 
 execution thread that can pass control to other coroutines without losing its current 
 execution environment, such as its location within nested name scopes and nested 
 control structures.
  
 Simula achieves this structure by introducing two new statements. The 
 call
  
 statement specifies a suspended active object, which is thereby allowed to continue 
 execution in its saved execution environment. The callers are saved on a runtime 
 stack. The
  detach
  statement suspends the current object and returns control to the 
 most recent object that invoked
  call
 . (This object is found on the stack just 
 mentioned.) The main program is treated as an ob-ject for this purpose, but it must 
 not invoke
  detach
 .
  
 The program of Figure 2.8 solves the binary-tree equality puzzle. Simula syntax 
 is fairly similar to Ada syntax; the following is close to correct Simula, although I 
 have modified it somewhat so I don’t confuse syntactic with se-mantic issues.
  
 Figure 2.8
  
 class
  Tree; -- used as a Pascal record
  
 1
  
 Value : char;
  
 2
  
 LeftChild, RightChild :
  pointer to
  Tree;
  
 3
  
 end
 ; -- Tree
  
 4
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 Coroutines in CLU ,"The CLU language, designed by Barbara Liskov at MIT, provides a general-ized
  for
  
 loop [Liskov 81]. The control variable takes on successive values provided by a 
 coroutine called an
  iterator
 . This iterator is similar in most ways to an ordinary 
 procedure, but it returns values via a
  yield
  statement. When the
  for
  loop requires 
 another value for the control variable, the itera-tor is resumed from where it left off 
 and is allowed to execute until it encoun-ters another
  yield
 . If the iterator reaches 
 the end of its code instead, the
  for 
 loop that relies on the iterator terminates. CLU’s
  
 yield
  is like Simula’s
  de-tach
 , except that it also passes back a value. CLU’s
  for
  
 implicitly contains the effect of Simula’s
  call
 .
  
 A naive implementation of CLU would create a separate stack for each ac-tive 
 iterator instance. (The same iterator may have several active instances; it does, for 
 example, if there is a
  for
  nested within another
  for
 .) A coroutine linkage, much like 
 Simula’s
  call
  and
  detach
 , would ensure that each iterator instance maintains its own 
 context, so that it may be resumed properly.
  
  
 The following program provides a simple example. CLU syntax is also fairly close 
 to Ada syntax; the following is almost valid CLU.
  
 Figure 2.9
  
 iterator
  B() : integer; -- yields 3, 4
  
 1
  
 begin
  
 2
  
 yield
  3;
  
 3
  
 yield
  4;
  
 4
  
 end
 ; -- B
  
 5
  
 iterator
  C() : integer; -- yields 1, 2, 3
  
 6
  
 begin
  
 7
  
 yield
  1;
  
 8
  
 yield
  2;
  
 9
  
 yield
  3;
  
 10
  
 end
 ; -- C
  
 11
  
 iterator
  A() : integer; -- yields 10, 20, 30
  
 12
  
 variable
  
 13
  
 Answer : integer;
  
 14
  
 begin
  
 15
  
 for
  Answer := C()
  do
  -- ranges over 1, 2, 3
  
 16
  
 yield
  10*Answer;
  
 17
  
 end
 ;
  
 18
  
 end
 ; -- A
  
 19
  
 variable
  
 20
  
 x, y : integer;
  
 21
  
 begin
  
 22
  
 for
  x := A()
  do
  -- ranges over 10, 20, 30
  
 23
  
 for
  y := B()
  do
  -- ranges over 3, 4
  
 24
  
 P(x, y); -- called 6 times
  
 25
  
 end
 ;
  
 26
  
 end
 ;
  
 27
  
 end
 ;
  
 28
  
 The loop in line 23 iterates over the three values yielded by iterator
  A
  (lines
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.3 Embedding CLU Iterators in C,"Surprisingly, it is possible to implement CLU iterators using only the con-
  
 structs available to a C programmer. This implementation clarifies CLU and
  
 shows some interesting aspects of C.
  
 The only machine-independent way to manipulate activation records in C
  
 is to use the library routines
  setjmp
  and
  longjmp
 . They are intended to pro-
  
 vide the equivalent of exception handling; they allow many levels of activa-
  
 tion records to be terminated at once, jumping from an activation record at
  
 the top of the stack directly back to one deep within the stack. I apply these
  
 routines in a way probably unintended by their inventors: to resume an acti-
  
 vation record higher on the stack than the invoker.
  
 Setjmp(Buf)
  takes a snapshot of the current environment — registers,
  
 stack pointers, program counter, and such — and places it in the
  Buf
  data
  
 structure.
  
 Longjmp(Buf, ReturnValue)
  restores the registers from
  Buf
 , ef-
  
 fectively restoring the exact context in which the
  setjmp
  was called. In fact,
  
 it creates another return from the original
  setjmp
  call. In order to let the
  
 program distinguish whether
  setjmp
  is returning the ordinary way or be-
  
 cause of a
  longjmp
 ,
  setjmp
  returns a 0 in the former case and
  ReturnValue
  in
  
 the latter case. For this reason,
  setjmp
  is usually embedded in a conditional
  
 or case statement to identify these cases and take appropriate action.
  
 This facility is very like jumping to a label passed as a parameter, which
  
 has the effect of unwinding the stack to the right activation record for the tar-
  
 get of the
  goto
 .
  Setjmp
  can capture the situation before a procedure call, and
  
 longjmp
  can be invoked from within a procedure; the call unwinds the stack
  
 to its position when
  setjmp
  recorded the situation. Unbridled use of
  setjmp
  
 and
  longjmp
  can be worse than an unconstrained
  goto
 . It allows such activi-
  
 ties as jumping into a control structure (after all, the
  setjmp
  can be in the
  
 middle of a loop or a branch of a conditional) or even jumping back to a proce-
  
 dure that has exited.
  
 This ability to break the rules makes it possible to implement CLU itera-
  
 tors within the C language. My implementation is packaged as a set of C
  
 macros, primarily
  iterFOR
  and
  iterYIELD
 . Whenever
  iterFOR
  is about to in-
  
 voke an iterator, it performs
  setjmp
  to allow the iterator to come back to the
  
 iterFOR
  via
  longjmp
 . Likewise, each
  iterYIELD
  performs
  setjmp
  to allow its
  
 parent
  iterFOR
  to resume it via
  longjmp
 . The macros use a single global
  
 variable (not visible to the programmer) to store a pointer to the associated
  
 Buf
  structures in both these cases.
  
 Now that the linkage between
  iterFOR
  and its iterator can be established,
  
 two problems remain. They both concern managing space on the stack. Un-
  
 fortunately, new activation records are placed on the stack immediately above
  
 the invoker’s activation record, even if other activation records have been
  
 placed there.
  
 The first problem is that even in the simplest situation, with a single
  
 iterFOR
  invoking a single iterator, we need padding on the stack between
  
 their respective activation records. If there is no padding, then attempts by
  
 iterFOR
  to resume the iterator fail. After all,
  iterFOR
  calls
  longjmp
 , and this
  
 invocation places an activation record on the stack (since
  longjmp
  is also a
  
 procedure).
  
 This activation record coincides with the iterator’s activation
  
 record, destroying at least the arguments and quite likely other information
  
 as well. Furthermore, any ordinary procedure calls invoked by the body of
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.4 Coroutines in Icon ,"Icon is discussed in some detail in Chapter 9. It generalizes CLU iterators by 
 providing expressions that can be reevaluated to give different results.
  
 Figure 2.15",NA
3,N,NA
 CONTINUATIONS: IO,"FORTRAN demonstrates that is possible to build a perfectly usable program-
  
 ming language with only procedure calls and conditional
  goto
  as control
  
 structures. The Io language reflects the hope that a usable programming lan-
  
 guage can result from only a single control structure: a
  goto
  with parameters.
  
 I will call the targets of these jumps procedures even though they do not re-
  
 turn to the calling point. The parameters passed to procedures are not re-
  
 stricted to simple values. They may also be
  continuations
 , which represent the 
 remainder of the computation to be performed after the called procedure
  
 is finished with its other work. Instead of returning, procedures just invoke
  
 their continuation. Continuations are explored formally in Chapter 10; here I
  
 will show you a practical use.
  
 Io manages to build remarkably sophisticated facilities on such a simple
  
 foundation. It can form data structures by embedding them in procedures,
  
 and it can represent coroutines.
  
 Io programs do not contain a sequence of statements. A program is a pro-
  
 cedure call that is given the rest of the program as a continuation parameter.
  
 A statement continuation is a closure; it includes a procedure, its environ-
  
 ment, and even its parameters.
  
 Io’s syntax is designed to make statement continuations easy to write. If a
  
 statement continuation is the last parameter, which is the usual case, it is
  
 separated from the other parameters by a semicolon, to remind the program-
  
 mer of sequencing. Continuations and procedures in other parameter posi-
  
 tions must be surrounded by parentheses.
  
 I will present Io by showing
  
 examples from [Levien 89].
  
 write 5;
  
 1
  
 write 6;
  
 2
  
 terminate
  
 3
  
 As you expect, this program prints
  5 6
 . But I need to explain how it works.
  
 The predeclared
  write
  procedure takes two parameters: a number and a con-
  
 tinuation. The call in line 1 has
  5
  as its first parameter and
  write 6; termi-
  
 nate
  as its second.
  
 The
  write
  procedure prints
  5
  and then invokes the
  
 Figure 2.16
  
 continuation. It is a call to another instance of
  write
  (line 2), with parame-
  
 ters
  6
  and
  terminate
 . This instance prints
  6
  and then invokes the parame-
  
 terless predeclared procedure
  terminate
 . This procedure does nothing. It
  
 certainly doesn’t return, and it has no continuation to invoke.
  
 Procedures can be declared as follows:
  
 declare
  writeTwice:
  
  Number;
  
 1
  
 write Number; write Number; terminate.
  
 2
  
 That is, the identifier
  writeTwice
  is associated with an anonymous procedure
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 POWER LOOPS 2,"Although the programmer usually knows exactly how deeply loops must nest, there 
 are some problems for which the depth of nesting depends on the data. 
 Programmers usually turn to recursion to handle these cases; each level of nesting is 
 a new level of recursion. However, there is a clearer alternative that can generate 
 faster code. The alternative has recently
 2
 been called 
 power loops
  [Mandl 90]. The 
 idea is to have an array of control variables and to build a loop that iterates over all 
 control variables.
  
 For example, the
  n
 -queens problem is to find all solutions to the puzzle of placing
  
 n
  queens on an
  n
  
  n
  chessboard so that no queen attacks any other.
  
 Here is a straightforward solution:
  
 Figure 2.28 
  
 variable 
 1
  
 Queen :
  array
  1 .. n
  of
  integer; 2
  
 nest
  Column := 1
  to
  n 3
  
 for
  Queen[Column] := 1
  to
  n
  do 
 4
  
 if
  OkSoFar(Column)
  then 
 5
  
 deeper
 ; 
  
 6
  
 end
 ; -- if OkSoFar(Column) 7
  
 end
 ; -- for Queen[Column] 8
  
 do 
 9
  
 write(Queen[1..n]); 10
  
 end
 ; 11
  
 Any solution will have exactly one queen in each column of the chessboard. Line 2 
 establishes an array that will describe which row is occupied by the queen in each 
 column. The
  OkSoFar
  routine (line 5) checks to make sure that the most recent queen 
 does not attack (and therefore is not attacked by) any of the previously placed 
 queens. Line 3 introduces a set of nested loops. It ef-fectively replicates lines 4–8 for 
 each value of
  Column
 , placing the next replica at the point marked by the
  deeper
  
 pseudostatement (line 6). There must be exactly one
  deeper
  in a
  nest
 . Nested inside 
 the innermost instance is the body shown in line 10. If
  n
  =
  3
 , for example, this 
 program is equivalent to the code of Figure 2.29.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  The Madcap language had power loops in the early 1960s [Wells 63].
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,"i := init
  
 i > final
  
 f
  
 i
  
 d
  
 r
  
 f
  
 i < init
  
 t
  
 replicated part
  
 t
  
 e
  
 body
  
 i := i - 1
  
 N",NA
 FINAL COMMENTS,"This chapter has introduced a variety of control constructs that have a mixed history 
 of success. Exception-handling mechanisms are enjoying increasing popularity. 
 General coroutines are found to some extent in concurrent pro-gramming languages 
 (discussed in Chapter 7). 
  
 CLU iterators and power 
 loops never caught on in mainstream languages. Io continuations have no track 
 record, but appear unlikely to catch on.
  
  
 We can often see good reason for these results. Chapter 1 presented a list of 
 characteristics of good programming languages. Among them were sim-plicity 
 (using as few concepts as possible), clarity (easily understood code se-mantics), and 
 expressiveness (ability to describe algorithms). 
  
 Exception 
 handling scores well on all these fronts. The mechanism introduces only one 
 additional concept (the exception, with the
  raise
  statement and the
  handle 
 syntax). 
 The semantics are clear when an exception is raised, especially if no resumption is 
 possible and if all exceptions must be declared at the global level. The only confusion 
 might come from the fact that the handler is deter-mined dynamically, not statically; 
 dynamic binding tends to be more confus-ing to the programmer, because it cannot 
 easily be localized to any region of the program. The mechanism serves a real need 
 in expressing multilevel pre-mature procedure return.
  
 Coroutines are less successful by my measures. The set of concepts is not too 
 large; Simula manages with per-object initialization code and two new statements:
  
 detach
  and
  call
 . However, the dynamic nature of the
  call
  stack and the fact that each 
 object needs its own private stack make coroutines harder to understand and less 
 efficient to implement. The additional expres-siveness they provide is not generally 
 useful; programmers are not often faced with testing traversals of trees for equality.
  
 CLU iterators are truly elegant. They are clear and expressive. They pro-vide a 
 single, uniform way to program all loops. They can be implemented ef-ficiently on a 
 single stack. Perhaps they have not caught on because, like general coroutines, they 
 provide expressiveness in an arena where most pro-grams do not need it. The only 
 application I have ever found for which CLU iterators give me just what I need has 
 been solving combinatorial puzzles,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"Figure 2.32
  
 2.1
  
 In what way is raising an exception like a
  goto
 ? It what way is it differ-
  
 ent?
  
 2.2
  
 Write a CLU iterator
  upto(a,b)
  that yields all the integer values be-
  
 tween
  a
  and
  b
 . You may use a
  while
  loop, but not a
  for
  loop, in your im-
  
 2.3
  
 plementation.
  
 Write a CLU iterator that generates all Fibonacci numbers, that is, the 
 sequence
  1
 ,
  1
 ,
  2
 ,
  3
 ,
  5
 ,
  8
 , . . . , where each number is the sum of the previ-ous two 
 numbers.
  
 2.4
  
 Write a Simula class
  Fibonacci
  with a field
  Value
  that the initialization code sets 
 to
  1
  and then suspends. Every time the object is resumed, 
 Value
  should be set 
 to the next value in the Fibonacci sequence.
  
 2.5
  
 What does the following Io program do?
  
 2.6
  
 declare
  foo:
  
  Number Continuation;
  
 1
  
 + Number 1
  
  More;
  
 2
  
 write More;
  
 3
  
 Continuation .
  
 4
  
 foo 7;
  
 5
  
 foo 9;
  
 6
  
 terminate
  
 7
  
 Use power loops to initialize a 10
  
  10
  
  10 integer array
  A
  to zeroes.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Challenge Exercises,"2.8
  
 2.9
  
 In Figure 2.3 (page 29), I show how a handler can reraise the same ex-ception 
 (or raise a different one) in order to propagate the raised excep-tion further. 
 Would it make sense to define a language in which 
  
 exceptions were handled by the handler that raised them, not propa-gated 
 further?
  
 What are the ramifications of letting exceptions be first-class values? 
 (First-class values are discussed in Chapter 3.)
  
 2.10
  Prove the contention on page 35 that when a CLU iterator terminates, 
 indicating to its parent
  for
  loop that there are no more values, the itera-tor’s 
 activation record is actually at the top of the stack.
  
 2.11
  Use CLU iterators to write a program that takes a binary tree and 
  
 prints all distinct combinations of four leaves.
  
 2.12
  Prove that in Figure 2.11 (page 36) the references to
  Answer
  generated in line 
 18 are always valid. In particular, prove that by the time an in-stance of
  
 Answer
  is deallocated, there are no remaining pointers to that instance. 
 Actually, CLU requires that control variables, such as
  T
  in line 25, have a 
 scope that only includes the loop they control. You may make use of this 
 restriction in your proof.
  
 2.13
  Show how to use the C iterator macros to write a program that enumer-
  
 ates 
 binary trees.
  
 2.14
  Are CLU iterators as powerful as Simula coroutines? In particular, can 
  
 the 
 binary-tree equality puzzle be solved in CLU?
  
 2.15
  In Figure 2.17 (page 44), could I replace the
  9
  in line 4 with the identi-
  
 fier
  
 Number
 ?
  
 2.16
  What sort of runtime storage organization is appropriate for Io?
  
 2.17
  Does Io support recursion?
  
 2.18
  Show the execution trace of Figure 2.27 (page 48).
  
 2.19
  What is the meaning of a power loop for which the range is empty?
  
 2.20
  Figure 2.31 (page 51) has one potential inefficiency. What is it?
  
 2.21
  Power loops are modeled on
  for
  loops. Can I model them on
  while
  loops 
  
 instead? That is, can they look like the following?
  
 Figure 2.33
  
 nest
  Boolean expression
  
 1
  
 replicated part
  
 2
  
 do
  
 3
  
 body
  
 4
  
 end
 ;
  
 5
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 3 ,NA,NA
O,NA,NA
Types,"The evolution of modern programming languages is closely coupled with the 
 development (and formalization) of the concept of data type. At the machine-
 language level, all values are untyped (that is, simply bit patterns). Assem-bler-
 language programmers, however, usually recognize the fundamental dif-ferences 
 between addresses (considered relocatable) and data (considered absolute). Hence 
 they recognize that certain combinations of addresses and data (for example, the 
 sum of two addresses) are ill defined.
  
  
 This assembler-language view of typing is flawed, however, because it views 
 type as a property of a datum rather than a property of the cell contain-ing the 
 datum. That is, whether or not an operation is meaningful can usu-ally be 
 determined only at runtime when the actual operand values are available. An 
 assembler will probably recognize the invalidity of an expres-sion that adds two 
 labels, while it will accept a code sequence that computes exactly the same thing! 
 This weakness has led to the introduction of tagged architectures that include (at 
 runtime) type information with a datum. Such architectures can detect the label-
 addition error, because the
  add
  instruction can detect that its operands are two 
 addresses. Unfortunately, the type infor-mation included with data is usually limited 
 to the primitive types provided by the architecture. 
  
 Programmer-declared data 
 types cannot receive the same sort of automatic correctness checking.
  
 FORTRAN and later high-level languages improved upon assembler lan-guages 
 by associating type information with the locations holding data rather than the data 
 itself. More generally, languages associate type information with identifiers, which 
 may be variables or formal parameters. When an at-tribute such as a type is 
 associated with an identifier, we say the the identi-fier is
  bound
  to the attribute. 
 Binding that takes place at compile time is usually called
  static
 , and binding that 
 takes place at runtime is called
  dy-namic
 .
  Static-typed
  languages are those that 
 bind types to identifiers at compile time. Since types are known at compile time, the 
 compiler can detect a wide range of type errors (for example, an attempt to multiply 
 two Boolean 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 55",NA
1,N,NA
 DYNAMIC-TYPED LANGUAGES,"It is possible to delay the binding of types to identifiers until runtime, leading to 
 dynamic-typed languages. Interpreted languages (like SNOBOL, APL, and Awk) often 
 bind types only at runtime. These languages have no type decla-rations; the type of 
 an identifier may change dynamically. These are different from
  typeless
  languages, 
 such as Bliss or BCPL, which have only one type of datum, the cell or word.
  
 Delaying the binding of a type to an identifier gains expressiveness at the cost of 
 efficiency, since runtime code must determine its type in order to ma-nipulate its 
 value appropriately. As an example of expressiveness, in dy-namic-typed languages, 
 arrays need not be homogeneous. As an example of loss of efficiency, even in static-
 typed languages, the values of choice types re-quire some runtime checking to 
 ensure that the expected variant is present.",NA
2,N,NA
 STRONG TYPING 1,"One of the major achievements of Pascal was the emphasis it placed on the definition 
 of data types. It viewed the creation of programmer-declared data types as an 
 integral part of program development. Pascal introduced the con-cept of strong 
 typing to protect programmers from errors involving type mis-matches. 
  
 A
  
 strongly typed language
  provides rules that allow the compiler to determine the 
 type of every value (that is, every variable and ev-ery expression).
 1
 Assignments and 
 actual-formal parameter binding involv-ing inequivalent types are invalid, except for 
 a limited number of automatic conversions. The underlying philosophy is that 
 different types represent dif-ferent abstractions, so they ought to interact only in 
 carefully controlled and clearly correct ways.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  Actually, Pascal is not completely strongly typed. Procedure-valued parameters do not specify the 
 full procedure header, so it is possible to provide an actual parameter that does not match the formal in 
 number or type of parameters. Untagged record variants are another loop-hole.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,N,NA
 TYPE EQUIVALENCE,"The concept of strong typing relies on a definition of exactly when types are 
 equivalent. Surprisingly, the original definition of Pascal did not present a definition 
 of type equivalence. The issue can be framed by asking whether the types
  T1
  and
  T2
  
 are equivalent in Figure 3.1:
  
 Figure 3.1
  
 type
  
 T1, T2 =
  array
 [1..10]
  of
  real;
  
 1
  
 2
  
 T3 =
  array
 [1..10]
  of
  real;
  
 3
  
 Structural equivalence
  states that two types are equivalent if, after all type 
 identifiers are replaced by their definitions, the same structure is ob-tained. This 
 definition is recursive, because the definitions of the type identi-fiers may 
 themselves contain type identifiers. It is also vague, because it leaves open what 
 “same structure” means. Everyone agrees that
  T1
 ,
  T2
 , and 
 T3
  are structurally 
 equivalent. However, not everyone agrees that records re-quire identical field 
 names in order to have the same structure, or that arrays require identical index 
 ranges. In Figure 3.2,
  T4
 ,
  T5
 , and
  T6
  would be consid-ered equivalent to
  T1
  in some 
 languages but not others:
  
 Figure 3.2
  
 type
  
 T4 =
  array
 [2..11]
  of
  real; -- same length
  
 1
  
 2
  
 T5
  
 =
  array
 [2..10]
  of
  real; -- compatible index type
  
 3
  
 T6 =
  array
 [blue .. red]
  of
  real; -- incompatible
  
 4
  
 -- index type
  
 5
  
 Testing for structural equivalence is not always trivial, because recursive types 
 are possible. In Figure 3.3, types
  TA
  and
  TB
  are structurally equivalent, as are
  TC
  and
  
 TD
 , although their expansions are infinite.
  
 Figure 3.3
  
 type
  
 TA =
  pointer to
  TA;
  
 1
  
 2
  
 TB =
  pointer to
  TB;
  
 3
  
 TC =
  
 4
  
 record
  
 5
  
 Data : integer;
  
 6
  
 Next :
  pointer to
  TC;
  
 7
  
 end
 ;
  
 8
  
 TD =
  
 9
  
 record
  
 10
  
 Data : integer;
  
 11
  
 Next :
  pointer to
  TD;
  
 12
  
 end
 ;
  
 13
  
 In contrast to structural equivalence,
  name equivalence
  states that two 
 variables are of the same type if they are declared with the same type name, such as
  
 integer
  or some declared type. When a variable is declared using a 
 type constructor
  
 (that is, an expression that yields a type), its type is given a new internal name for 
 the sake of name equivalence. Type constructors in-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 DIMENSIONS,"The example involving
  meters
  and
  feet
  shows that types alone do not pre-vent 
 programming errors. I want to prohibit multiplying two
  feet
  values and assigning 
 the result back into a
  feet
  variable, because the type of the result is square feet, not 
 feet.
  
  
 The AL language, intended for programming mechanical manipulators, in-
 troduced a typelike attribute of expressions called
  dimension
  to prevent such 
 errors [Finkel 76]. This concept was first suggested by C. A. R. Hoare [Hoare 73], and 
 it has been extended in various ways since then. Recent re-search has shown how to 
 include dimensions in a polymorphic setting like ML [Kennedy 94]. (Polymorphism 
 in ML is discussed extensively later in this chapter.) AL has four predeclared base 
 dimensions:
  time
 ,
  distance
 ,
  angle
 , and
  mass
 . Each base dimension has predeclared 
 constants, such as
  second
 , 
 centimeter
 , and
  gram
 . The values of these constants are 
 with respect to an arbitrary set of units; the programmer only needs to know that 
 the constants are mutually consistent. For example,
  60*second = 
  
 minute
 . 
 New dimen-sions can be declared and built from the old ones. AL does not support 
 pro-grammer-declared 
  
 base 
  
 dimensions, 
  
 but 
  
 such an 
  
  
 extension 
  
 would 
  
 be reasonable. Other useful base dimensions would be 
 electrical current (mea-sured, for instance, in amps), temperature (degrees Kelvin), 
 luminous inten-sity (lumens), and currency (florin). In retrospect, angle may be a 
 poor choice for a base dimension; it is equivalent to the ratio of two distances: 
 distance along an arc and the radius of a circle. Figure 3.7 shows how dimensions 
 are used.
  
 Figure 3.7 
  
 dimension 
 1
  
 area = distance * distance; 
  
 2
  
 velocity = distance / time; 
  
 3
  
 constant 
 4
  
 mile = 5280 * foot; --
  
 foot is predeclared 5
  
 acre = mile * mile / 640; 
  
 6
  
 variable 
 7
  
 d1, d2 : distance real; 8
  
 a1 : area real; 9
  
 v1 : velocity real; 10
  
 begin 
 11
  
 d1 := 30 * foot; 12
  
 a1 := d1 * (2 * mile) + (4 * acre); 13
  
 v1 := a1 / (5 * foot * 4 * minute); 14
  
 d2 := 40; -- invalid: dimension error 15
  
 d2 := d1 + v1; -- invalid: dimension error 16
  
 write(d1/foot, ""d1 in feet"", 17
  
 v1*hour/mile, ""v1 in miles per hour""); 18
  
 end
 ; 19
  
 In line 13,
  a1
  is the area comprising 4 acres plus a region 30 feet by 2 miles. In line 
 14, the compiler can check that the expression on the right-hand side has the 
 dimension of
  velocity
 , that is,
  distance/time
 , even though it is hard for a human to 
 come up with a simple interpretation of the expression.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,N,NA
 ABSTRACT DATA TYPES,"An
  abstract data type
  is a set of values and a set of procedures that manip-ulate 
 those values. An abstract data type is analogous to a built-in type, which is also a set 
 of values (such as integers) and operations (such as addi-tion) on those values. Once 
 a program has introduced an abstract data type, variables can be declared of that 
 type and values of the type can be passed to the procedures that make up the type. 
 The
  client
  of an abstract data type (that is, a part of a program that uses that type, as 
 opposed to the part of the program that defines it) can create and manipulate values 
 only by using pro-cedures that the abstract data type allows. The structure of an 
 abstract data type (usually a
  record
  type) is hidden from the clients. Within the 
 definition of the abstract data type, however, procedures may make full use of that 
 structure.
  
 An abstract data type can be seen as having two parts: the specification and the 
 implementation. The specification is needed by clients; it indicates the name of the 
 type and the headers of the associated procedures. It is not necessary for the client 
 to know the structure of the type or the body of the procedures. The implementation 
 includes the full description of the type and the bodies of the procedures; it may 
 include other procedures that are used as subroutines but are not needed directly 
 by clients.
  
 This logical separation allows a programmer to concentrate on the issues at 
 hand. If the programmer is coding a client, there is no need to worry about how the 
 abstract data type is implemented. The implementer may upgrade or even 
 completely redesign the implementation, and the client should still function 
 correctly, so long as the specification still holds.
  
 A popular example of an abstract data type is the stack. The procedures that 
 manipulate stacks are
  push
 ,
  pop
 , and
  empty
 . Whether the implementa-tion uses an 
 array, a linked list, or a data file is irrelevant to the client and may be hidden.
  
 Abstract data types are used extensively in large programs for modularity and 
 abstraction. They put a barrier between the implementor of a set of rou-tines and its 
 clients. Changes in the implementation of an abstract data type will not influence the 
 clients so long as the specification is preserved. Ab-stract data types also provide a 
 clean extension mechanism for languages. If a new data type is needed that cannot 
 be effectively implemented with the ex-isting primitive types and operations (for 
 example, bitmaps for graphics), it can be still specified and prototyped as a new 
 abstract data type and then ef-ficiently implemented and added to the environment.
  
 In order to separate the specification from the implementation, program-ming 
 languages should provide a way to hide the implementation details from client code. 
 Languages like C and Pascal that have no hiding mechanism do not cater to abstract 
 data types, even though they permit the programmer to declare new types. CLU, 
 Ada, C++, and Modula-2 (as well as numerous other languages) provide a name-
 scope technique that allows the programmer to group the procedures and type 
 declarations that make up an abstract data
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
6,N,NA
" LABELS, PROCEDURES, AND TYPES AS ",NA,NA
FIRST-CLASS VALUES     5,"You are used to thinking of integers as values. But to what extent is a label or a 
 procedure a value? Can a type itself be a value? One way to address these questions 
 is to categorize values by what sort of manipulation they al-low. The following chart 
 distinguishes
  first
 ,
  second
 , and
  third-class val-ues
 .
  
 Manipulation
  
 Class of value
  
 First
  
 Second
  
 Third
  
 Pass value as a parameter 
  
 Return value from a procedure 
 Assign value into a variable
  
  
 yes 
  
 yes 
  
 yes
  
 yes 
  
 no 
  
 no
  
 no 
  
 no 
  
 no
  
  
 Languages differ in how they treat labels, procedures, and types. For ex-ample, 
 procedures are third-class values in Ada, second-class values in Pas-
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  
  The difference between restricting exports and restricting imports is identical to the dif-ference 
 between access lists and capability lists in operating systems.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7,N,NA
 ML,"Now that I have covered some issues surrounding types, I will present a de-tailed 
 look at one of the most interesting strongly typed languages, ML [Harper 89; 
 Paulson 92]. ML, designed by Robin Milner, is a functional pro-gramming language 
 (the subject of Chapter 4), which means that procedure calls do not have any
  side 
 effects
  (changing values of variables) and that there are no variables as such. Since 
 the only reason to call a procedure is to get its return value, all procedures are 
 actually functions, and I will call them that. Functions are first-class values: They can 
 be passed as parameters, re-turned as values from procedures, and embedded in 
 data structures. 
 Higher-order functions
  (that is, functions returning other 
 functions) are used extensively. Function application is the most important control 
 con-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.1 Expressions  7,"ML is an expression-based language; all the standard programming con-structs 
 (conditionals, declarations, procedures, and so forth) are packaged as expressions 
 yielding values. Strictly speaking, there are no statements: even operations that 
 have side effects return values.
  
 It is always meaningful to supply an arbitrary expression as the parame-ter to a 
 function (when the type constraints are satisfied) or to combine ex-pressions to 
 form larger expressions in the same way that simple constants can be combined.
  
 Arithmetic expressions have a fairly conventional appearance; the result of 
 evaluating an expression is presented by ML as a value and its type, sepa-rated by a 
 colon, as in Figure 3.13.
  
 Figure 3.13
  
 in:
  
 (3 + 5) * 2;
  
 1
  
 out: 16 : int
  
 2
  
 String expressions are straightforward (Figure 3.14).
  
 Figure 3.14
  
 in:
  
 ""this is it"";
  
 1
  
 out: ""this is it"" : string
  
 2
  
 Tuples of values are enclosed in parentheses, and their elements are sepa-rated by 
 commas. The type of a tuple is described by the type constructor 
 *
  .
 7 
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  The
  *
  constructor, which usually denotes product, denotes here the set-theoretic Carte-sian 
 product of the values of the component types. A value in a Cartesian product is a compound formed by 
 selecting one value from each of its underlying component types. The number of val-ues is the product of 
 the number of values of the component types, which is one reason this set-theoretic operation is called a 
 product.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.2 Global Declarations ,"Values are bound to identifiers by
  declarations
 . Declarations can appear at the top 
 level, in which case their scope is global, or in blocks, in which case they have a 
 limited local scope spanning a single expression. I will first deal with global 
 declarations.
  
 Declarations are not expressions. They establish bindings instead of re-turning 
 values. Value bindings are introduced by the keyword
  val
 ; additional value bindings 
 are prefixed by
  and
  (Figure 3.18).
  
 Figure 3.18
  
 in:
  
 val
  
 a = 3
  and
  
 1
  
 b = 5
  and
  
 2
  
 c = 2;
  
 3
  
 out:
  val
  c = 2 : int
  
 4
  
 val
  b = 5 : int
  
 5
  
 val
  a = 3 : int
  
 6
  
 in:
  
 (a + b)
  div
  c;
  
 7
  
 out: 4 : int
  
 8
  
 In this case, I have declared the identifiers
  a
 ,
  b
 , and
  c
  at the top level; they
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.3 Local Declarations ,"Declarations can be made local by embedding them in a block (see Figure 3.24), 
 which is formed by the keywords
  let
  (followed by the declarations),
  in 
 (followed by 
 a single expression, the body), and
  end
 . The scope of the declara-tion is limited to 
 this body.
  
 Figure 3.24
  
 in:
  
 let
  
 1
  
 val
  a = 3
  and
  b = 5
  
 2
  
 in
  
 3
  
 (a + b)
  div
  2
  
 4
  
 end
 ;
  
 5
  
 out: 4 : int
  
 6
  
 Here the identifiers
  a
  and
  b
  are mapped to the values
  3
  and
  5
  respectively for the 
 extent of the expression
  (a + b)
  div
  2
 . No top-level binding is introduced; the whole
  
 let
  construct is an expression whose value is the value of its body.
  
 Just as in the global scope, identifiers can be locally redeclared, hiding the 
 previous declarations (whether local or not). It is convenient to think of each 
 redeclaration as introducing a new scope. Previous declarations are not af-fected, as 
 demonstrated in Figure 3.25.
  
 Figure 3.25
  
 in:
  
 val
  a = 3
  and
  b = 5;
  
 1
  
 out:
  val
  b = 5 : int;
  
 2
  
 val
  a = 3 : int;
  
 3
  
 in:
  
 (
 let val
  a = 8
  in
  a + b
  end
 , a);
  
 4
  
 out: (13,3) : int * int
  
 5
  
 The body of a block can access all the identifiers declared in the surrounding 
 environment (like
  b
 ), unless they are redeclared (like
  a
 ).
  
  
 Declarations can be composed sequentially in local scopes just as in the global 
 scope, as shown in Figure 3.26.
  
 Figure 3.26
  
 in:
  
 let
  
 1
  
 val
  a = 3;
  
 2
  
 val
  b = 2 * a
  
 3
  
 in
  
 4
  
 (a,b)
  
 5
  
 end
 ;
  
 6
  
 out: (3,6) : int * int
  
 7
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.4 Lists ,"Lists are homogeneous; that is, all their components must have the same type. The 
 component type may be anything, such as strings, lists of integers, and functions 
 from integers to Booleans.
  
 Many functions dealing with lists can work on lists of any kind (for exam-ple to 
 compute the length); they do not have to be rewritten every time a new kind of list 
 is introduced. In other words, these functions are naturally poly-morphic; they 
 accept a parameter with a range of acceptable types and return a result whose type 
 depends on the type of the parameter. Other functions are more restricted in what 
 type of lists they accept; summing a list makes sense for integer lists, but not for 
 Boolean lists. However, because ML allows functions to be passed as parameters, 
 programmers can generalize such re-stricted functions. For example, summing an 
 integer list is a special case of a more general function that accumulates a single 
 result by scanning a list and applying a commutative, associative operation 
 repeatedly to its elements. In particular, it is not hard to code a polymorphic
  
 accumulate
  function that can be used to sum the elements of a list this way, as in 
 Figure 3.27.
  
 Figure 3.27
  
 in:
  
 accumulate([3,4,5],
  fn
  (x,y) => x+y, 0);
  
 1
  
 out:
  
 12 : int
  
 2
  
 Line 1 asks for the list
  [3,4,5]
  to be accumulated under integer summation, whose 
 identity value is
  0
 . Implementing the
  accumulate
  function is left as an exercise.
  
 The fundamental list constructors are
  nil
 , the empty list, and the right-associative 
 binary operator
  ::
  (pronounced “cons,” based on LISP, discussed in Chapter 4), which 
 places an element (its left operand) at the head of a list (its right operand). The 
 square-brackets constructor for lists (for example, 
 [1,2,3]
 ) is an abbreviation for a 
 sequence of
  cons
  operations terminated by 
 nil
 :
  1 :: (2 :: (3 :: nil))
 .
  Nil
  itself may be 
 written
  []
 . ML always uses the square-brackets notation when printing lists.
  
 Expression
  
 Evaluates to
  
 nil 
  
 1 :: [2,3] 
  
 1 :: 2 :: 3 :: nil
  
 [] 
  
 [1,2,3] 
 [1,2,3]
  
 Other predeclared operators on lists include
  
  
 •
  null
 , which returns
  true
  if its parameter is
  nil
 , and
  false
  on any other 
  
  
 list.
  
 •
  hd
 , which returns the first element of a nonempty list.
  
 •
  tl
 , which strips the first element from the head of a nonempty list.
  
 • @ (append), which concatenates lists.
  
 Hd
  and
  tl
  are called
  selectors
 , because they allow the programmer to select a 
 component of a structure. Here are some examples that use the predeclared 
 operators.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.5 Functions and Patterns  8 9,"Because all functions take exactly one parameter, it is often necessary to pass 
 complicated structures in that parameter. The programmer may want the formal 
 parameter to show the structure and to name its components. ML patterns provide 
 this ability, as shown in Figure 3.28.
  
 Figure 3.28
  
 in:
  
 val
  plus =
  fn
  (a,b) => a + b;
  
 I need to say
  a + b : int
 , as I will show later. Here, the function
  plus
  takes a single 
 parameter, which is expressed as a pattern showing that the parame-ter must be a 
 tuple with two elements, which are called formally
  a
  and
  b
 .
 8 
 This pattern does not 
 force the actual parameter to be presented as an ex-plicit tuple, as Figure 3.29 
 shows.
  
 Figure 3.29
  
 in:
  
 plus(3,4)
  
 1
  
 out: 7 : int
  
 2
  
 in:
  
 let
  
 3
  
 val
  x = (3,4)
  
 4
  
 in
  
 5
  
 plus x
  
 6
  
 end
 ;
  
 7
  
 out: 7 : int
  
 8
  
 The first example (line 1) builds the actual parameter to
  plus
  explicitly from two 
 components,
  3
  and
  4
 . The comma between them is the tuple constructor. The syntax 
 is contrived to remind the programmer that the intent is to pro-vide two 
 parameters.
 9
 The second example presents a single variable
  x
  as the actual 
 parameter (line 6); the compiler can tell that it has the right type,
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  The declaration is actually ambiguous; ML cannot determine which meaning of meant.
  
  In practice, a compiler can usually optimize away the extra pair constructions.
  
 +
  
 is
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.6 Polymorphic Types  10,"A function is
  polymorphic
  when it can work uniformly over parameters of different 
 data types. For example, the function in Figure 3.33 computes the length of a list.
  
 Figure 3.33
  
 in:
  
 val rec
  length =
  
 1
  
 fn
  nil => 0
  
 2
  
 | (h :: tail) => 1 + length tail;
  
 3
  
 out:
  val
  length =
  fn
  : ’a
  list
  -> int
  
 4
  
 in:
  
 (length [1,2,3], length [""a"",""b"",""c"",""d""]);
  
 5
  
 out: (3,4) : int * int
  
 6
  
 The type of
  length
  inferred by the compiler (line 4) contains a type identifier (
 ’a
 ), 
 indicating that any kind of list can be used, such as an integer list or a 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  The parentheses in the second pattern are not needed; I put them in for the sake of clari-ty. 
 Parentheses are required in tuples, however.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.7 Type Inference ,"A type can be a type identifier (
 ’a
 ,
  ’b
 , ...), or it can be constructed with type
  
 constructors.
  
 Predeclared type constants, like
  int
  and
  bool
 , are actually
  
 nullary type constructors. Polymorphic type constructors include
  
 ->
  ,
  
 *
  ,
  
 and
  list
 .
  
 As a simple example of type inference, if I declare
  Identity =
  fn
  x => x, then
  Identity
  
 has type
  ’a -> ’a
 , because it returns unchanged expressions of any type. If I have the 
 application
  Identity 0
 , then since
  0
  is of type
  int
 , this application of
  Identity
  is 
 specialized to
  int -> int
 , and hence the value of the application is of type
  int
 .
  
  
 The following table summarizes the types assumed for a variety of literals and 
 operators, some of which are naturally polymorphic.
  
 Expression
  
 Type
  
  
  
 true
  
 bool
  
 false
  
 bool
  
 1
  
 int
  
 +
  
 (int * int) -> int
  
 =
  
 (’a * ’a) -> bool
  
 nil
  
 ’a
  list
  
 ::
  
 (’a * ’a
  list
 ) -> ’a
  list
  
 hd
  
 ’a
  list
  -> ’a
  
 tl
  
 ’a
  list
  -> ’a
  list
  
 null
  
 ’a
  list
  -> bool
  
  
 A type expression may contain several occurrences of the same type iden-tifier, 
 allowing the programmer to specify type dependencies. Thus
  ’a -> ’a 
 represents a 
 function whose parameter and result type are the same, al-though it does not 
 specify what that type is. In a type expression, all occur-rences of a type identifier 
 must represent the same type. Discovering that type is done by an algorithm called
  
 unification
 ; it finds the strongest com-mon type constraint for (possibly 
 polymorphic) types. For example,
  int -> int
  and
  (int -> bool) -> (int -> bool)
  can be 
 unified to
  ’a -> ’a
 . They can also be unified to
  ’a -> ’b
 , but that is a weaker constraint. 
 In fact, they can be unified to the weakest possible type,
  ’a
 .
  
 To perform polymorphic type inference, ML assigns a type identifier to each 
 expression whose type is unknown and then solves for the type identi-fiers. The 
 algorithm to solve for the type identifiers is based on repeatedly applying 
 constraints:
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.8 Higher-Order Functions ,"ML supports higher-order functions, that is, functions that take other func-tions as 
 parameters or deliver functions as results. Higher-order functions are particularly 
 useful to implement
  partial application
 , in which an invo-cation provides only 
 some of the expected parameters of a function, as in Fig-ure 3.40.
  
 Figure 3.40
  
 Figure 3.41
  
 in:
  
 val
  times =
  fn
  a => (
 fn
  b : int => a * b);
  
 1
  
 out:
  val
  times =
  fn
  : int -> (int -> int)
  
 2
  
 in:
  
 times 3 4;
  
 3
  
 out: 12 : int
  
 4
  
 in:
  
 val
  twice = times 2;
  
 5
  
 out:
  val
  twice =
  fn
  : int -> int
  
 6
  
 in:
  
 twice 4;
  
 7
  
 out: 8 : int
  
 8
  
 The type of
  times
  (lines 1–2) is unexpectedly complex, because I have chosen to split 
 the two parameters. (I explicitly indicate that
  b
  is of type
  int
  to re-
  
 solve the
  
 *
  
 operator.) In line 3,
  times 3 4
  is understood as
  (times 3) 4
 .
  
 Times
  first takes the actual parameter
  3
  and returns a function from integers to 
 integers; this anonymous function is then applied to
  4
  to give the result
  12
 . This 
 unusual definition allows me to provide only the first parameter to 
 times
  if I wish, 
 leading to partial application. For example, I declare
  twice
  in line 5 by calling
  times
  
 with only one parameter. When I wish to supply the second parameter, I can do so, 
 as in line 7.
  
  
 The function-composition function, declared in Figure 3.41, is a good ex-ample of 
 partial application. It also has an interesting polymorphic type.
  
 in:
  
 val
  compose =
  fn
  (f,g) => (
 fn
  x => f (g x));
  
 1
  
 out:
  val
  compose =
  fn
  :
  
 2
  
 ((’a -> ’b) * (’c -> ’a)) -> (’c -> ’b)
  
 3
  
 in:
  
 val
  fourTimes = compose(twice,twice);
  
 4
  
 out:
  val
  fourTimes =
  fn
  : int -> int
  
 5
  
 in:
  
 fourTimes 5;
  
 6
  
 out: 20 : int
  
 7
  
 Compose
  takes two functions
  f
  and
  g
  as parameters and returns a function that when 
 applied to a parameter
  x
  returns
  f (g x)
 . Composing
  twice
  with itself, by partially 
 applying
  compose
  to the pair
  (twice,twice)
 , produces a function that multiplies 
 numbers by four. Function composition is actually a predeclared binary operator in 
 ML written as
  o
 . The composition of
  f
  and
  g 
 can be written
  f o g
 .
  
  
 Suppose now that I need to partially apply a function
  f
  that, like
  plus
 , takes a pair 
 of parameters. I could redeclare
  f
  as in Figure 3.42.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.9 ML Types  11,"The type of an expression indicates the set of values it may produce. Types include 
 primitive types (integer, real, Boolean, string) and structured types (tuples, lists, 
 functions, and pointers). An ML type only gives information about attributes that can 
 be computed at compile time and does not distin-guish among different sets of values 
 having the same structure. Hence the set of positive integers is not a type, nor is the 
 set of lists of length 3. In con-trast, Pascal and Ada provide subtypes that restrict the 
 range of allowable values.
  
 On the other hand, ML types can express structural relations within val-ues, for 
 example, that the right part of a pair must have the same type as the left part of the 
 pair, or that a function must return a value of the same type as its parameter 
 (whatever that type may be).
  
  
 Types are described by recursively applied type constructors. Primitive types 
 like
  int
  are type constructors that take no parameters. Structured 
  
  
  
 * 
  
 (Cartesian product, for tuples), types are built by type constructors like 
  
 list
 ,
 ->
  (for functions), and
  ref
  (for pointers). Type constructors are usu-ally infix or 
 suffix:
  int * int
 ,
  int
  list
 ,
  int -> int
 , and
  int
  ref
  are the types 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  
  Haskell B. Curry was a logician who popularized this idea.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
7.10 Constructed Types ,"A
  constructed type
  is a type for which constructors are available. Con-structors can 
 be used in patterns later to decompose data. You have already seen examples of this 
 dual usage with the tuple constructor and the list con-structors
  nil
  and
  ::
  (
 cons
 ).
  
 A constructed type and its constructors should be considered as a single 
 conceptual unit. Whenever a new constructed type is declared, its construc-tors are 
 declared at the same time. Wherever a constructed type is known, its constructors 
 are also known.
  
 The programmer can introduce new constructed types in a type declara-tion. A 
 type declaration introduces a new type name and the names of the constructors for 
 that type. Each of those constructors leads to a component, whose type is also 
 presented. The components together make up a choice type, that is, a type whose 
 values cover all the components. Syntactically, components are separated by
  |
  . Each 
 component starts with its constructor name, followed by the keyword
  of
  and then 
 the type of the component. The keyword
  of
  and the component type can be omitted; 
 in this case the construc-tor is a constant of the new type.
  
 For example, money can be a coin of some value (in cents), a bill of some value 
 (in dollars), a check drawn on some bank for some amount (in cents), or the absence 
 of money (see Figure 3.48).
  
 Figure 3.48
  
 in:
  
 datatype
  money =
  
 1
  
 nomoney |
  
 2
  
 coin
  of
  int |
  
 3
  
 bill
  of
  int |
  
 4
  
 check
  of
  string * int; -- (bank, cents)
  
 5
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
8,N,NA
 MIRANDA,"The Miranda language, designed by David Turner of the University of Kent,
  
 shares many features with ML [Turner 85a, 86; Thompson 86]. It is strongly
  
 typed, infers types from context, provides for abstract data types, and has
  
 higher-order functions. It provides tuples and homogeneous lists, and compo-
  
 nents of tuples are extracted by patterns. Operators are provided for
  cons
  
 and
  append
 , as well as for list length, selection from a list by position, and set
  
 difference.
  
 Miranda differs from ML in some minor ways. It is purely functional;
  
 there are no pointer types. Functions of more than one parameter are auto-
  
 matically curried unless parentheses explicitly indicate a tuple. (ML also has
  
 a declaration form that automatically curries, but I have not shown it.) The
  
 scope rules in Miranda are dynamic, which means that functions may be ref-
  
 erenced textually before they are declared. All declarations implicitly allow
  
 recursion; there is no need for a
  rec
  keyword.
  
 Binary operators may be
  
 passed as actual parameters in Miranda; they are equivalent to curried func-
  
 tions that take two parameters.
  
 Miranda has a nontraditional syntax in which indentation indicates
  
 grouping and conditionals look like the one in Figure 3.54.
  
 max
  
 = a, a>=b
  
 1
  
 = b,
  otherwise
  
 2
  
 However, my examples will follow ML syntax (modified as necessary) for con-
 sistency.
  
 Miranda provides some novel extensions to ML. First, evaluation is nor-mally 
 lazy. I discuss
  lazy evaluation
  in detail in Chapter 4; for now, let me
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
9,N,NA
 RUSSELL ,"The Russell language predates ML but is quite similar in general flavor [Demers 79; 
 Boehm 86]. It was developed to explore the semantics of types,
  
 in particular, to try to make types first-class values.
  
 Russell is strongly
  
 typed, infers types from context, provides for abstract data types, and has higher-
 order functions.
  
 Russell differs from ML in some minor ways. Although it is statically scoped, new 
 function declarations do not override old ones of the same name if the types differ; 
 instead, the name becomes overloaded, and the number and type of the actual 
 parameters are used to distinguish which function is
  
 meant in any particular context.
  
 (Redeclaration of identifiers other than
  
 functions is not allowed at all.) Functions may be declared to be invoked as prefix, 
 suffix, or infix operators. Functions that take more than two parame-ters may still be 
 declared to be invoked with an infix operator; a given num-ber of the parameters 
 are placed before the operator, and the rest after. ML only allows infix notation for 
 binary functions. To prevent side effects in the presence of variables (
 ref
  types), 
 functions do not import identifiers mapped to variables.
  
 Russell’s nomenclature is nonstandard; what ML calls a type is a signa-ture in 
 Russell; an abstract data type (a collection of functions) is a type in
  
 Russell.
  
 So when Russell succeeds in making types first-class values, it
  
 doesn’t accomplish quite as much as we would expect. Russell’s syntax is quite 
 different from ML. For consistency, I will continue to use ML terminol-ogy and 
 syntax as I discuss Russell.
  
 The principal difference between Russell and ML is that in Russell ab-stract data 
 types are first-class values, just like values, pointers, and func-tions. That is, abstract 
 data types may be passed as parameters, returned from functions, and stored in 
 identifiers. Abstract data type values can also be manipulated after they have been 
 constructed.
  
 More specifically, Russell considers an abstract data type to be a collection
  
 of functions that may be applied to objects of a particular domain.
  
 The
  
 Boolean
  abstract data type includes the nullary functions
  true
  and
  false
 , bi-nary 
 operators such as
  and
  and
  or
 , and even statements such as
  if
  and 
 while
 , which have 
 Boolean components. Manipulation of an abstract data type means deleting or 
 inserting functions in its definition.
  
 The border between data and program becomes quite blurred if we look at the 
 world this way. After all, we are not used to treating control constructs like
  while
  as 
 functions that take two parameters, a Boolean and a statement,",NA
10,N,NA
 DYNAMIC TYPING IN STATICALLY TYPED ,NA,NA
LANGUAGES,"It seems strange to include dynamic typing in otherwise statically typed lan-guages, 
 but there are situations in which the types of objects cannot be pre-dicted at 
 compile time. In fact, there are situations in which a program may wish to create a 
 new type during its computation.
  
 An elegant proposal for escaping from static types is to introduce a prede-clared 
 type named
  dynamic
  [Abadi 91]. This method is used extensively in Amber [Cardelli 
 86]. Values of this type are constructed by the polymorphic predeclared function
  
 makeDynamic
 . They are implemented as a pair contain-ing a value and a type 
 description, as shown in Figure 3.68 (in an ML-like syntax).
  
 Figure 3.68
  
 val
  A = makeDynamic 3;
  
 1
  
 val
  B = makeDynamic ""a string"";
  
 2
  
 val
  C = makeDynamic A;
  
 3
  
 The value placed in
  A
  is 3, and its type description is
  int
 . The value placed in 
 B
  is
  ""a 
 string""
 , and its type description is
  string
 . The value placed in
  C
  is the pair 
 representing
  A
 , and its type description is
  dynamic
 .
  
 Values of dynamic type can be manipulated inside a
  typecase
  expression that 
 distinguishes the underlying types and assigns local names to the com-ponent 
 values, as in Figure 3.69.
  
 Figure 3.69
  
 val rec
  Stringify =
  fn
  Arg : dynamic =>
  
 1
  
 typecase
  Arg
  
 2
  
 of
  
 s : string => ’""’ + s + ’""’
  
 3
  
 |
  
 i : int => integerToString(i)
  
 4
  
 |
  
 f : ’a -> ’b => ""function""
  
 5
  
 |
  
 (x, y) => ""("" + (Stringify makeDynamic x) +
  
 6
  
 "", "" + (Stringify makeDynamic y) + "")""
  
 7
  
 |
  
 d : dynamic => Stringify d
  
 8
  
 |
  
 _ => ""unknown"";
  
 9
  
 Stringify
  is a function that takes a dynamic-typed parameter
  Arg
  and re-turns a string 
 version of that parameter. It distinguishes the possible types of
  Arg
  in a
  typecase
  
 expression with patterns both to capture the type and to assign local identifiers to 
 the components of the type. If the underlying type is itself
  dynamic
 ,
  Stringify
  recurses 
 down to the underlying type (line 8). In lines 6–7,
  makeDynamic
  is invoked to ensure 
 that the parameters to
  Stringify 
 are of the right type, that is,
  dynamic
 .
  
 Figure 3.70 shows a more complicated example that nests
  typecase
  ex-pressions. 
 The function
  Apply
  takes two curried dynamic parameters and in-vokes the first one 
 with the second one as a parameter, checking that such an application is valid.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
11,N,NA
 FINAL COMMENTS,"The discussion of derived types and dimensions is part of a larger issue about how 
 restrictive a programming language needs to be in order to permit the art of 
 programming. One way to look at this question [Gauthier 92] is to no-tice that on the 
 one hand the real world is very restrictively typed, as stu-dents of physics realize. 
 One should not add apples and oranges, much less volts and calories. On the other 
 hand, the memory of most computers is com-pletely untyped; everything is 
 represented by bits (organized into equally un-typed bytes or words). The 
 programming language represents a platform for describing the real world via the 
 computer, so it properly lies somewhere be-tween these extremes. It needs to 
 balance type security with simplicity. Type security demands that each different 
 kind of value have its own type in order to match the real world. For example, lists of 
 exactly three elements are dif-ferent from lists of four elements. Integers 
 constrained to even numbers are different from unconstrained integers. Simplicity 
 demands that types be easy to specify and that types be efficiently checked, 
 preferably at compile time. It is not so easy to include lengths or number-theoretic 
 considerations in the type description of lists and integers, respectively.
  
 It is largely a matter of personal taste where this platform should be on the 
 spectrum ranging from restrictively typed, using strong typing and per-haps 
 providing derived types with dimensions, to lax, with dynamic typing and easy 
 coercion. Proponents of the restrictive style point with pride to the clarity of their 
 programs and the fact that sometimes they run correctly the first time. Proponents 
 of the lax style speak disparagingly of “bondage-and-discipline” languages like Ada, 
 and prefer the relative freedom of C.
  
 Such taste is likely to change as a programmer changes. My first experi-ence of 
 programming (after plug-board computers) was in machine language, not even 
 assembler language. Later, I relished the intricacies of SNOBOL, which is quite lax 
 about typing. Algol was a real eye-opener, with its declared types and its control 
 structures. I now prefer strong typing; to me, an elegant program is one that is 
 readable the first time by a novice, not one that plays unexpected tricks. Strong 
 typing helps me to build such programs. Still, I use C heavily because it is 
 implemented so widely, and I often need to port my programs across machines.
  
 ML is an elegant language that shows how to make functions first-class values 
 and how to deal with type polymorphism and still be strongly typed. Type inference 
 relieves the programmer of careful type declarations. Mi-randa extends these ideas 
 with infinite lists and lazy evaluation. (There is also a lazy variant of ML with similar 
 extensions.) Russell even allows some types to be manipulated in fairly simple ways. 
 None of these languages truly allows types themselves to be first-class values. Such 
 an extension would probably require runtime type checking or lose strong typing. 
 (The exercises explore this concept.)
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"Figure 3.73
  
 3.1
  
 What is the difference between the way Modula-2+ and Modula-3 han-dle type 
 equivalence for derived types?
  
 3.2
  
 If two types are name-equivalent, are they necessarily structurally
  
 equivalent?
  
 3.3
  
 Would you consider
  First
  and
  Second
  in Figure 3.73 structurally equiv-
  
 alent? Why or why not?
  
 type
  
 1
  
 First =
  
 2
  
 record
  
 3
  
 A : integer;
  
 4
  
 B :
  record
  
 5
  
 B1, B2 : integer;
  
 6
  
 end
 ;
  
 7
  
 end
 ;
  
 8
  
 Second =
  
 9
  
 record
  
 10
  
 A:
  record
  
 11
  
 A1, A2 : integer;
  
 12
  
 end
 ;
  
 13
  
 B : integer;
  
 14
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Challenge Exercises,"3.7 
  
 3.8
  
 3.9
  
 Enumerate the possible values of type
  TA
  in Figure 3.3 (page 57). Given that
  
 First
  and
  Second
  are not structurally equivalent in exercise 3.3, suggest an 
 algorithm for testing structural equivalence.
  
 Suggest an algorithm for compile-time dimension checking. Is runtime 
 dimension checking needed?
  
 3.10
  Explore adding dimensions to ML.
  
 3.11
  Write an
  accumulate
  procedure in ML that can be used to sum a list, as 
  
 suggested on page 75.
  
 3.12
  Show a valid use of
  leftProjection
 , introduced in Figure 3.52 (page 
  
 88).
  
 3.13
  Program QuickSort in ML.
  
 3.14
  Show how
  datatype
  in ML could give me the effect of
  f1 : ((int
  alt 
  
 bool) -> ’a) -
 > (’a * ’a)
 , as suggested on page 82.
  
 3.15
  Use the
  dynamic
  type in an ML framework to declare a function
  Build-Deep
  such 
 that
  BuildDeep 2
  produces a function of dynamic type
  int -> (int -> int)
 ,
  BuildDeep 
 3
  produces a function of dynamic type
  int -> (int -> (int -> int))
 , and so forth. The 
 produced functions should re-turn the sum of all their parameters.
  
 3.16
  Generalize types in ML so that types are true first-class values. That is, I should 
 be able to build things of type
  type
 , or of type
  type -> int
 . De-cide what the built-
 in functions on type
  type
  should be. Try to keep the language strongly typed.
  
 3.17
  Extend ML so that there is a type
  expression
 . Devise reasonable func-tions 
 that use that type. These functions should have runtime (not just compile-
 time) significance.
  
 3.18
  What is the type (in the ML sense) of
  least
  in Figure 3.66 (page 96)? 
 3.19
  
 Can Io constructs (see Chapter 2) be represented in ML?
  
 3.20
  Show two types in Russell that are type-equivalent, but are neither 
  
 name-equivalent nor structurally equivalent.
  
 3.21
  Are all structurally equivalent types type-equivalent in Russell?
  
 3.22
  Russell prevents side effects in the presence of variables (
 ref
  types) by 
  
 prohibiting functions from importing identifiers mapped to variables.
  
 Why is this rule important?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 4 ,NA,NA
O,NA,NA
Functional Programming,"Most of the programming languages you are familiar with (Pascal, Ada, C) are
  
 imperative
  languages. They emphasize a programming style in which programs 
 execute commands sequentially, use variables to organize memory, and update 
 variables with assignment statements. The result of a program thus comprises the 
 contents of all permanent variables (such as files) at the end of execution.
  
  
 Although imperative programming seems quite natural and matches the 
 execution process of most computer hardware, it has been criticized as funda-
 mentally flawed. For example, John Backus (the designer of FORTRAN) holds that 
 almost all programming languages (from FORTRAN to Ada) ex-hibit a “von 
 Neumann bottleneck” in which programs follow too closely the“fetch 
 instruction/update memory” cycle of typical CPUs. These languages do not lend 
 themselves to simultaneous execution of different parts of the pro-gram, because 
 any command may depend on the changes to variables caused by previous 
 commands. (An enormous amount of effort has gone into creat-ing algorithms that 
 allow compilers to discover automatically to what extent commands may be 
 executed simultaneously.) Execution speed is therefore ul-timately limited by the 
 speed with which individual instructions can be exe-cuted. Another effect of 
 imperative programming is that to know the state of a computation, one must know 
 the values of all the variables. This is why compilers that provide a postexecution 
 dump of the values of all variables (or, better yet, compilers that allow variables to 
 be examined and changed during debugging) are so handy.
  
 In contrast,
  functional
  programming languages have no variables, no as-
 signment statements, and no iterative constructs. This design is based on the 
 concept of mathematical functions, which are often defined by separation into 
 various cases, each of which is separately defined by appealing (possibly re-
 cursively) to function applications. Figure 4.1 presents such a mathematical 
 definition.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 103",NA
1,N,NA
 LISP,"LISP (List Processing language) was designed by John McCarthy at MIT in 1959. LISP 
 actually represents a family of related languages, all sharing the common core of 
 ideas first espoused in LISP 1.5. The most popular versions of LISP today are 
 Scheme and Common LISP. Most dialects of LISP are not purely functional (variables 
 are used sometimes, and certain functions do have side effects). I shall concentrate 
 however on the functional flavor of pro-gramming in LISP.
  
 The fundamental values manipulated by LISP are called
  atoms
 . An atom is either 
 a number (integer or real) or a symbol that looks like a typical iden-tifier (such as
  
 ABC
  or
  L10
 ). Atoms can be structured into
  S-expressions
 , which are recursively 
 defined as either
  
 1. 
  
 2.
  
 An atom, or 
  
 (S1.S2), where S1 and S2 are S-expressions.
  
 Figure 4.2 shows some S-expressions.
  
 Figure 4.2
  
 100
  
 1
  
 (A.B)
  
 2
  
 ((10.AB).(XYZ.SSS))
  
 3
  
 All S-expressions that are not atoms have two components: the head (called,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.1 Function Syntax ,"Programs as well as data are represented as lists. 
  
 That is, LISP is
  ho-moiconic:
  
 Programs and data have the same representation. This property, rarely found in 
 programming languages, allows a LISP program to create or modify other LISP 
 functions. As you will see, it also allows the semantics of LISP to be defined in a 
 particularly simple and concise manner. (Tcl, dis-cussed in Chapter 9, is also 
 homoiconic and enjoys the same benefits.) 
  
  
 To allow programs to be represented as lists, LISP function invocations aren’t 
 represented in the usual form of
  FunctionName(arg1, arg2, ...)
 , but rather as
  
 (FunctionName arg1 arg2 ...)
 . 
  
 For example, the S-expression 
 (10.20)
  can be built by evaluating
  (cons 10 20)
 .
  
 When a list is evaluated, the first element of the list is looked up (in the runtime 
 symbol table) to find what function is to be executed. Except in spe-cial cases (forms 
 such as
  cond
 ), the remaining list elements are evaluated and passed to the function 
 as actual parameters. The value computed by the body of the function is then 
 returned as the value of the list.",NA
1.2 Forms ,"Should the call
  (cons A B)
  mean to join together the atoms
  A
  and
  B
 , or should 
 A
  and
  B
  
 be looked up in the symbol table in case they are formal parameters in the current 
 context? LISP evaluates all actual parameters, so
  A
  and
  B
  are evaluated by looking 
 them up in the symbol table. If I want
  A
  and
  B
  to be treated as atoms rather than 
 identifiers, I need to
  quote
  them, that is, pre-vent their evaluation. The programmer 
 can use
  quote
 , called as
  (
 quote
  arg)
 ,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.3 Programmer-Defined Functions  2,"Functions are first-class values in LISP (as in most functional programming 
 languages). In particular, they can be returned as the result of functions. Therefore, 
 LISP must allow the programmer to construct a function directly without 
 necessarily giving it a name. The function constructor in LISP there-fore builds
  
 anonymous functions
 , that is, functions that are not yet bound to names. To define 
 a function, the programmer must provide a list contain-ing three things: the form
  
 lambda
 , a list of the formal parameters, and the 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  So you see that sometimes form is more important than function.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.4 Scope Rules ,"The same identifier can be used as a function name or as a formal parameter in one 
 or more functions. LISP therefore needs a scope rule to say which dec-laration is to 
 be associated with each use of a symbol. Early dialects of LISP (in particular, LISP 
 1.5) used dynamic scoping: As actual parameters are bound to formal parameters, 
 they are placed at the front of an association list that acts as the runtime symbol 
 table for formal parameters. The association list is searched from front to back, so 
 the most recent association of a value to a formal parameter is always found. If a 
 formal parameter identifier appears more than once, the nearest (that is, most 
 recent) binding of it is used. The order of call, and not static nesting, determines 
 which declaration of a symbol is used. Consider Figure 4.10.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.5 Programming ,"Programming in LISP has a different flavor from programming in imperative 
 languages. Recursion, rather than iteration, is emphasized. To perform a 
 computation on a list, it is convenient to extract the first element of the list (using
  
 car
 ), and then to recursively perform the computation on the remain-der of the list.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.6 Closures and Deep Binding ,"Because LISP functions are represented as lists, functions can be passed as 
 parameters to other functions and returned as the result of functions. In Fig-ure 
 4.12 (page 109), it is important that
  cons
  be quoted in the call to
  f4
 , since I don’t 
 want it evaluated until its parameters are available.
  
 Now consider a more interesting function,
  sc
  (self-compose), that takes a 
 function and returns a new function representing the given function com-posed 
 with itself. (That is, the new function has the effect of the old function applied 
 twice.) I could write
  sc
  as shown in Figure 4.26.
  
 Figure 4.26 
  
 (
 def
  sc (
 lambda
  (F) (
 lambda
  (x) (F (F x)))))
  
 This code isn’t quite right, because a call such as
  (sc car)
  will try to evaluate the 
 resulting
  lambda
  form prematurely. If I quote the
  lambda
  form to obtain the code of 
 Figure 4.27,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.7 Identifier Lookup ,"Shallow and deep binding are also used (unfortunately, ambiguously) to de-note 
 two ways of implementing (as opposed to defining) identifier lookup in a 
 dynamically scoped language such as early versions of LISP. I will call them shallow 
 and deep search to avoid any confusion.
  
 In block-structured languages with static scope rules, identifiers are translated 
 to addresses (or offsets within an activation record) at compile time. In dynamically 
 scoped languages like LISP, some runtime overhead to fetch the current binding 
 (that is, value) of a symbol is to be expected, but this cost must be minimized to 
 obtain reasonable performance. As you might expect, linear search through an 
 association list every time an identifier is referenced is too inefficient to be practical.
  
 A key insight is that an atom is actually represented as a pointer to its property 
 list. It is possible to store the value associated with an atom in its property list, 
 allowing fast access to the atom’s value.
  
 The question is, what happens when a given atom is re-bound; that is, the same 
 identifier is re-bound as a formal parameter during application of a 
 lambda
  form? A 
 deep-search implementation places the original, or top-level, value of an atom in its 
 property list. Re-bindings are pushed onto a runtime stack when an atom is re-
 bound. This stack must be searched when the cur-rent value of an atom is needed. 
 (The first value found for that atom is the right one.) The name
  deep search
  is 
 appropriate, since LISP must usually go deep into the stack to find out if an atom has 
 been re-bound. The advan-tage of deep search is that creating and freeing new 
 bindings is fairly efficient (and somewhat similar to pushing and popping an 
 activation record in a con-ventional block-structured language).
  
 Shallow search
  makes lookup faster by storing the most recent binding of an 
 atom in its property list. Lookup is shallow indeed, but there is in-creased overhead 
 in invoking and returning from functions. In particular, for each local identifier, the 
 current value of that identifier (if there is one) must be saved on the runtime stack 
 before the new binding is stored in the atom’s property list. When a function 
 returns, the last bindings pushed on the stack (if any) must be restored.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.8 The Kernel of a LISP Interpreter ,"It is possible to define a LISP interpreter in terms of a few primitive func-tions (
 car
 ,
  
 cdr
 ,
  cons
 ,
  eq
 ,
  atom
 ,
  get
 ,
  error
 ,
  null
 ), predefined identifiers (
 t
 , 
 nil
 ), forms (
 cond
 ,
  def
 ,
  
 quote
 ), and metanotions of lambda binding and func-tion application. An interpreter 
 is a compact and exact specification of what any LISP program will compute. Few 
 other languages can boast such a sim-ple and elegant definition.
  
 To simplify things, I will ignore fine points like deep binding, although deep 
 binding can be handled without undue complexity. Whenever I invoke one of the 
 primitive functions in the following functions, I assume that the re-sult defined for 
 that function is immediately computed, perhaps by a call to a library routine. 
 Otherwise, the interpreter would encounter infinite recur-sion.
  
 The interpreter is a function called
  Eval
 , shown in Figure 4.31.
  
 Figure 4.31
  
 (
 def
  Eval (
 lambda
  (List Env) -- evaluate List in Env
  
 1
  
 (
 cond
  
 2
  
 ((null List) nil)
  
 3
  
 ((atom List)
  
 4
  
 (
 cond
  
 5
  
 ((get List (
 quote
  APVAL))
  
 6
  
 (get List (
 quote
  APVAL)))
  
 7
  
 (t (Lookup List Env))))
  
 8
  
 ((eq (car List) (
 quote
  quote)) (car (cdr List)))
  
 9
  
 ((eq (car List) (
 quote
  cond))
  
 10
  
 (EvalCond (cdr List) Env))
  
 11
  
 (t (Apply (car List)
  
 12
  
 (EvalList (cdr List) Env) Env)))
  
 13
  
 ))
  
 14
  
 Eval
  evaluates
  List
  in a given environment
  Env
  of identifier-value pairs. Val-ues of 
 atoms are looked up in their property lists (lines 6 and 7) or the envi-ronment
  Env
  
 (line 8). The forms
  quote
  (line 9) and
  cond
  (lines 10–11) are given special treatment. 
 The
  eq
  function tests atoms for equality. (We don’t
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.9 Run-time List Evaluation ,"Not only is
  Eval
  expressible in LISP; it is also provided as a predeclared func-tion in 
 every LISP implementation. Programmers can take advantage of the homoiconic 
 nature of LISP to construct programs at runtime and then pass them as parameters 
 to
  Eval
 .
  
  
 For example, say I would like to write a function
  Interpret
  that accepts lists in the 
 format of Figure 4.34.
  
 Figure 4.34
  
 ’(MyAdd (MyAdd 1 5) (MyMult 2 3))
  
 Here,
  MyAdd
  means “add the two parameters and then double the result,” and 
 MyMult
  means “multiply the two parameters and then subtract one from the result.” 
 The input to
  Interpret
  may be an arbitrarily nested list. One way to solve this puzzle 
 is to program
  Interpret
  recursively. It would check to see if the
  car
  of its parameter 
 was an atom,
  MyAdd
 , or
  MyMult
 , and then apply the appropriate arithmetic rule to 
 the result of recursively interpreting the other parameters. But Figure 4.35 shows a 
 much more straightforward, nonrecur-sive solution that takes advantage of
  Eval
 .
  
 Figure 4.35
  
 (
 def
  MyAdd (
 lambda
  (A B) (* (+ A B) 2))) (
 def
  MyMult 
 (
 lambda
  (A B) (- (* A B) 1)))
  
 (
 def
  Interpret (
 lambda
  (L) (Eval L)))
  
 (Interpret ’(MyAdd (MyAdd 1 5) (MyMult 2 3))) -- result is 34
  
 The list to be interpreted is treated not as data, but as program, and
  Eval
  is capable 
 of executing programs.",NA
1.10 Lazy Evaluation ,"Normally, a LISP evaluator operates by evaluating and binding actual param-eters to 
 formal parameters (first to last) and then evaluating function bodies. If an actual 
 parameter involves a function call, that function is invoked as the parameter is 
 evaluated. This strategy is known as
  strict evaluation
 . Given
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.11 Speculative Evaluation ,"Another interesting evaluation strategy is
  speculative evaluation
 . As the name 
 suggests, a speculative evaluator wants to evaluate as much as possi-ble, as soon as 
 possible. This evaluation strategy is best suited for multipro-cessors or 
 multicomputers that are able to perform many calculations concurrently. Present 
 multicomputers have hundreds of processors; future machines may have hundreds 
 of thousands or even millions.
  
 A crucial problem in a multicomputer is finding a way to keep a reason-able 
 fraction of the processors busy. Speculative evaluation seeks to evaluate 
 independent subexpressions concurrently. For example, in an invocation of 
 SameFrontier
 , a speculative evaluator could flatten both lists concurrently. Within a 
 function, another source of potential concurrency lies in the evalua-tion of a
  cond
  
 form. Individual guards of a
  cond
  can be evaluated concur-rently, as well as their 
 associated bodies.
  
 Care is required because of the evaluation ordering that is assumed in 
 cond
 ’s 
 definition. Evaluation of a subexpression may lead to a runtime error (for example, 
 taking the
  car
  of an atom), because a speculative evaluator will evaluate an 
 expression that a strict evaluator would never examine. With care, faults can be 
 suppressed until their effect on the overall result is known. Given this caveat, a
  cond
  
 form can be a rich source of concurrent evaluations.
  
 Nonetheless, the cost of starting a processor and later receiving its result is often 
 high. If the calculation started speculatively is too small, the over-head will 
 overshadow any advantage provided by the concurrent evaluation. A speculative 
 evaluator for LISP would probably evaluate primitive functions directly and reserve 
 concurrent speculative evaluation for
  lambda
  forms. Such coarse-grain parallelism is 
 discussed further in Chapter 7.
  
 The ability to evaluate expressions in virtually any order makes specula-tive 
 evaluation plausible for functional programming languages. In impera-tive 
 languages, an elaborate analysis of what variables depend on what other variables is 
 required even to consider any form of concurrent evaluation. Once again the von 
 Neumann bottleneck rears its ugly head.",NA
1.12 Strengths and Weaknesses of LISP ,"Functional programming is in many ways simpler and more elegant than con-
 ventional programming styles. Programmers do not need to keep track of po-tential 
 side effects when a procedure is invoked, so programming is less error-prone. The 
 lack of side effects allows implementations a rich variety of evalu-ation strategies.
  
 LISP and its descendants have long been the dominant programming lan-guages 
 in artificial intelligence research. It has been widely used for expert systems, 
 natural-language processing, knowledge representation, and vision
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,"125
  
 modeling. Only recently has Prolog, discussed in Chapter 8, attracted a sig-
  
 nificant following in these areas. LISP is also the foundation of the widely
  
 used Emacs text editor. Much of LISP’s success is due to its homoiconic na-
  
 ture: A program can construct a data structure that it then executes. The se-
  
 mantics of the core of LISP can be described in just a few pages of a
  
 metacircular interpreter.
  
 LISP was the first language to have an extensive program development
  
 environment [Teitelman 81]. (Smalltalk, described in Chapter 5, was the sec-
  
 ond. Such environments are widely available now for Ada, Pascal, and C++.)
  
 Programs can be modified and extended by changing one function at a time
  
 and then seeing what happens. This facility allows elaborate programs to
  
 evolve and supports rapid prototyping, in which a working prototype is used
  
 to evaluate the capabilities of a program. Later, the program is fleshed out by
  
 completing its implementation and refining critical routines.
  
 The most apparent weakness of the early dialects of LISP is their lack of
  
 program and data structures. In LISP 1.5, there are no type-declaration fa-
  
 cilities (although some LISP dialects have adopted facilities for data typing).
  
 Certainly not everything fits LISP’s recursive, list-oriented view of the world.
  
 For example, symbol tables are rarely implemented as lists.
  
 Many LISP programmers view type checking as something that ought to
  
 be done after a program is developed. In effect, type checking screens a pro-
  
 gram for inconsistencies that may lead to runtime errors.
  
 In LISP, type
  
 checking amounts generally to checking for appropriate structures in S-
  
 expressions.
  
 Most production LISP dialects (such as Interlisp, Franz LISP, Common
  
 LISP, and Scheme) have greatly extended the spartan facilities provided in
  
 LISP 1.5, leading to incompatibilities among LISP implementations. Indeed,
  
 it is rare to transport large LISP programs between different implementa-
  
 tions. This failure inhibits the interchange of software tools and research de-
  
 velopments.
  
 It would appear that the corrupting influences of von Neumann program-
  
 ming are so pervasive that even functional languages like LISP can succumb.
  
 Most LISP implementations even have a
  prog
  feature that allows an impera-
  
 tive programming style! In addition, LISP has some decidedly nonfunctional
  
 features, such as the
  set
  function and property lists. In fact, it has been said
  
 that ‘‘LISP . . . is not a functional language at all. [The] success of LISP set
  
 back the development of a properly functional style of programming by at
  
 least ten years.’’ [Turner 85b]
  
 N",NA
 FP,"In comparison to typical block-structured languages, LISP 1.5 stands as a paragon of 
 simplicity. (On the other hand, Common LISP is as big as Ada.) Nonetheless, Backus 
 suggests that even simpler functional programming ap-proaches may be desirable 
 [Backus 78]. He thinks that LISP’s parameter-binding and substitution rules are 
 unnecessary and instead proposes a vari-able-free programming style limited to 
 single-parameter functions. (A pa-rameter may, however, be a sequence, and 
 functions may be curried.) Further, LISP’s ability to combine functions in any form 
 (since functions are just S-expressions) is unnecessarily general. He compares this 
 freedom to the
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.1 Definition of an FP Environment  1 n 1 n,"An FP environment comprises the following:
  
 Figure 4.39
  
 1.
  
 A set of objects. An object is either an atom or a sequence, <x
 1
  , . . . , x
 n 
 >, whose 
 elements are objects, or
  
  (“bottom”) representing “error,” or“undefined.”
  
 Included as atoms are
  
 , the empty sequence (roughly 
 equivalent to
  nil
  in LISP), and
  T
  and
  F
 , representing
  true
  and
  false
 .
  
 2.
  
 Any sequence containing
  
  is equivalent to
  
 . That is, the sequence 
 constructor is bottom-preserving.
  
 A set of functions (which are not objects) mapping objects into objects.
  
 Functions may be primitive (predefined), defined (represented by a
  
 name), or higher-order (a combination of functions and objects using a
  
 3.
  
 predefined higher-order function). All functions are bottom-preserving;
  
 f
  applied to
  
  always yields
  
 .
  
 An application operation that applies a function to an object, yielding an
  
 object. Function
  f
  applied to object
  x
  is denoted as
  f:x
 . Here,
  x
  isn’t a
  
 variable name (there are no variables!), but rather a placeholder for an
  
 4.
  
 expression that will yield an object.
  
 A set of higher-order functions used to combine existing functions and
  
 objects into new functions. Typical higher-order functions include those
  
 shown in Figure 4.39.
  
 Composition
  
 1
  
 (f
  
  g):x
  
  f:(g:x)
  
 2
  
 Construction
  
 3
  
 [f
  , ... , f
  ]:x
  
  <f
 :x, ... ,f
 :x>
  
 4
  
 Condition
  
 5
  
 (p
  
  f;g):x
  
  
 6
  
 if
  p:x = T
  then
  
 7
  
 f:x
  
 8
  
 elsif
  p:x = F
  then
  
 9
  
 g:x
  
 10
  
 else
  
 11
  
 
  
 12
  
 The conditional form handles nicely a problem that arises with bottom-
  
 preserving functions: One or the other branch of a conditional may be
  
 undefined (bottom) while the value of the conditional is itself well de-
  
 fined. If one tries to create a conditional function that takes a triple
  
 representing the Boolean value, the true-part value and the false-part
  
 value, then if any component is
  
 , so is the entire triple, forcing
  
  as the result. 
 Since conditional is a higher-order function, the evaluator
  
 doesn’t apply the “
 then
  function” or “
 else
  function” until the conditional
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 Reduction Semantics ,"FP environments have a particularly simple semantics called
  reduction se-mantics
 . 
 An FP program is composed of a number of functions applied to ob-jects. The 
 meaning of such a program is defined by repeatedly reducing the program by 
 finding a function application and evaluating it. In some cases, function evaluation 
 may be nonterminating. Such functions diverge and are considered undefined (that 
 is,
  
 ). There are only three kinds of valid func-tions: primitive functions, defined 
 functions, and higher-order functions. Primitive functions are automatically 
 evaluable. Defined functions are re-duced by replacing their identifiers with their 
 definitions. Higher-order func-tions are reduced by substituting their definitions. If a 
 function does not belong to one of these three categories, it is invalid.
  
 Reduction semantics have only a very weak form of identifier binding (de-fined 
 names map to functions) and employ no changes to hidden states. There is clearly 
 no way to cause side effects, so an evaluator can reduce a function in any order. In 
 fact, early FP languages were called “Red” (reduc-tion) languages.",NA
3,N,NA
 PERSISTENCE IN FUNCTIONAL LANGUAGES,"A value is
  persistent
  if it is retained after the program that created it has 
 terminated. A database is a good example of persistent values. The conven-tional 
 way to make values persistent is to write them out to a file. Chapter 3 discusses the 
 type-safety considerations of such values.
  
 If persistent values are to be incorporated into a programming language, we 
 must be able to name such values and to be assured that once created, they do not 
 change. Functional languages can incorporate persistent values in a natural way that 
 avoids explicit input and output [Morrison 90].
  
 Persistent values can be named by reference to a
  persistence root
 , which is 
 something like the root of a file-system hierarchy. All such values are automatically 
 saved after execution. If a value is structured, its compo-nents are also preserved; in 
 particular, other values pointed to by a persistent value are also persistent. Using 
 ML as an example, we might have the code
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 LIMITATIONS OF FUNCTIONAL ,NA,NA
LANGUAGES,"The idea that variables are unnecessary is quite attractive. It is often suffi-cient 
 either to bind values through parameter binding or as constants for the duration of 
 a block. For example, in ML, the
  let
  construct allows an identi-fier to be bound to a 
 meaning, but there is no assignment as such. There are situations, however, in 
 which the inability to modify an existing value leads to awkward or inefficient 
 programs [Arvind 89; Yuen 91].
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,N,NA
 LAMBDA CALCULUS,"The mathematician Alonzo Church designed the lambda calculus in the 1930s as a 
 way to express computation [Church 41]. LISP is a direct descendent of this 
 formalism, and ML owes much of its nature to a restricted version called“typed 
 lambda calculus.” In one sense, lambda calculus is a set of rules for manipulating 
 symbols; the symbols represent functions, parameters, and in-vocations. In another 
 sense, lambda calculus is a programming language; it has given rise more or less 
 directly to both LISP and ML.
  
 The underlying ideas of lambda calculus are straightforward. Lambda calculus 
 has only three kinds of terms: identifiers (such as
  x
 ), abstractions, and applications.
  
 Abstractions
  represent functions of a single parameter. They follow the notation 
 shown in Figure 4.46.
  
 Figure 4.46
  
 (
 
  x . (* x 2)) -- Lambda calculus
  
 1
  
 (
 lambda
  (x) (* x 2)) -- LISP
  
 2
  
 fn
  x => x * 2 -- ML
  
 3
  
 In general, an abstraction has the form
  (
 
  x . T)
 , where
  T
  is any term.
  Ap-plications
  
 represent invoking a function with an actual parameter. A func-tion
  F
  is invoked 
 with actual parameter
  P
  by the notation
  (F P)
 ; both
  F
  and
  P 
 are any terms. 
 Parentheses may be dropped; the precedence rules stipulate that application and 
 abstraction are grouped left to right and that application has a higher precedence 
 than abstraction. Therefore, the terms in Figure 4.47 are equivalent.
  
 Figure 4.47
  
 (
 
  x . ((
 
  y . q) x) z) -- fully parenthesized
  
 1
  
 
  x . (
 
  y . q) x z -- minimally parenthesized
  
 2
  
 Another notational convenience is that curried functions may be rewritten without 
 currying, as in Figure 4.48.
  
 Figure 4.48 
  
 (
 
  x . (
 
  y . (
 
  z . T))) = (
 
  x y z . T)
  
 Lambda calculus has a static scope rule. The abstraction
  (
 
  x . T)
  intro-duces a 
 new binding for the identifier
  x
 ; the scope of this binding is the term 
 T
 . In the 
 language of lambda calculus,
  x
  is
  bound
  in
  (
 
  x . T)
 . An unbound identifier in a term is 
 called
  free
  in that term. It is possible to define the con-cept of free identifiers in a 
 recursive way: An identifier
  x
  is free in term
  T
  if (1) the term is just the identifier
  x
 ; 
 (2) the term is an application
  (F P)
 , and
  x 
 is free in
  F
  or in
  P
 ; or (3) the term is an 
 abstraction
  (
 
  y . T)
 , and
  x
  is free in 
 T
 , and
  x
  is not
  y
 . Figure 4.49 presents some 
 examples.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES ,NA,NA
Review Exercises,"4.1 
  
 4.2 
  
 4.3
  
 Why is it natural for a language that has no variables to provide no iter-ative 
 control constructs?
  
 If a language treats functions as first-class values, does the language support 
 higher-order functions?
  
 In Figure 4.3 (page 106), I show that
  (cons (cons ’A (cons ’B nil)) (cons nil (cons 11 
 nil)))
  is the same as
  ((’A ’B) () 11)
 . What is the value of the following expression?
  
 (cons (cons (cons ’A (cons ’B nil)) nil) (cons 11 nil))
  
 4.4 
  
 4.5 
  
 4.6 
  
 4.7
  
 4.8
  
 4.9
  
 In LISP, is parameter passing by value mode? If not, by what mode? In 
 Figure 4.25 (page 113), why introduce the function
  Extend
 ?
  
 Convert the
  Double
  function of Figure 4.20 (page 112) into ML.
  
 Generalize the
  Double
  function of Figure 4.20 (page 112) so that it dou-bles 
 recursively within sublists as well as at the top level.
  
 Generalize the answer to problem 4.7 to make a
  Multiple
  function that accepts 
 two parameters: a list
  L
  and a multiplier
  M
 , so that if
  M
  is 2, the effect is like
  
 Double
 , but higher and lower integer multipliers also work. Under what 
 circumstances does it make a difference in what order the parameters to a 
 function are evaluated?
  
 4.10
  Reduce the following lambda expression to normal form.
  
 (
 
  y . (
 
  z . x z (y z))) (
 
  a . (a b))
  
 4.11
  Reduce the lambda expressions given in Figure 4.67.
  
 Figure 4.67
  
 {a / b}(
 
  a . b) 
  
 {a / b}(
 
  b . a) 
  
 {a / b}(
 
  c . b) 
  
 {a / b}(
 
  b . c) 
  
 {a / b}(
 
  a . a) 
  
 {a / b}(
 
  b . b) 
  
 {a / b}(
 
  c . c)",NA
Challenge Exercises ,"4.12
  What does it mean for something to be a first-class value in a purely 
  
 functional language?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 5 ,NA,NA
O,NA,NA
Object-Oriented Programming,"In the imperative programming paradigm that has dominated the way pro-
 grammers think about solutions to problems for the past twenty years or so, a 
 program consists of one or more procedures that transfer control among themselves 
 and manipulate one or more data items to solve a problem. Ob-ject-oriented 
 programming (OOP) is a different paradigm based on Simula’s classes. Many people 
 like it because it allows code to be reused in an orga-nized fashion.
  
 Object-oriented programming is an area of current research. There is an annual 
 ACM Conference on Object-Oriented Programming Systems, Lan-guages, and 
 Applications (OOPSLA).",NA
1,N,NA
 DEFINITIONS,"An
  object-oriented
  program consists of one or more
  objects
  that interact with one 
 another to solve a problem. An object contains state information (data, represented 
 by other objects) and operations (code). Objects interact by sending
  messages
  to 
 each other. These messages are like procedure calls; the procedures are called
  
 methods
 . Every object is an instance of a
  class
 , which determines what data the 
 object keeps as state information and what messages the object understands. The
  
 protocol
  of the class is the set of mes-sages that its instances understand.
  
 Objects in object-oriented programming correspond to variables and con-stants 
 in structured programming. Classes in object-oriented programming correspond to 
 types: Every object of a particular class has the same structure as every other object 
 of that class.
  
 Objects are a form of abstract data type, in that if two objects respond to the 
 same messages in the same way, there is no way to distinguish them. Such objects 
 may be freely interchanged. For example, I might have two 
 Stack
  objects that 
 respond 
 to
  
 push
  
 and
  
 pop
  
 messages. 
 One 
 object 
 might 
 inter-
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 139",NA
2,"CHAPTER 5
  
 OBJECT-ORIENTED PROGRAMMING
  
 nally use an array, the other a linked list. The two
  stack
  objects are indistin-
  
 guishable to their clients. I might even have an array of stacks, some of
  
 whose components are implemented one way, while others are implemented
  
 the other way.
  
 The term object-oriented has started to appear prominently in many ad-
  
 vertisements, but people disagree about what object-oriented programming is
  
 and is not. The consensus seems to be that a programming language must
  
 support data encapsulation, inheritance, and overloading to be called an ob-
  
 ject-oriented programming language.
  
 Data encapsulation
  dictates that an object
  A
  that wishes to examine or modify 
 another object
  B
  may do so only in ways defined by
  B
 ’s protocol. In
  
 other words, the data associated with an object is hidden from public view.
  
 Only the operations an object supports are known to its clients. Data encap-
  
 sulation makes it unlikely that changes in the implementation of an object or
  
 extensions to its protocol will cause failures in the code for unrelated objects.
  
 As long as the object’s new protocol is a superset of its old one, code that relies
  
 on the old protocol will continue to work correctly.
  
 Inheritance
  allows one class to share the properties of another. For ex-ample, 
 Smalltalk includes the predefined class
  Magnitude
 , which defines sev-
  
 eral operations, including
  max
  (maximum).
  
 Any class that inherits from
  
 Magnitude
 , such as
  Integer
 , inherits this operation. The
  max
  operation for all
  
 subclasses of
  Magnitude
  is thus defined in one place, so any enhancements or
  
 corrections to the
  max
  operation become available automatically to all such
  
 classes. Inheritance is used in practice for two purposes: (1) to indicate that
  
 the new class specializes the old class, and (2) to allow the new class to use
  
 code from the old class. Inheritance makes the job of enhancement and main-
  
 tenance much easier.
  
 Overloading
  dictates that the code invoked to perform an operation must 
 depend not only on the operation but on what sort of objects the operation is
  
 to manipulate. For example, the
  max
  operation provided by the
  Magnitude
  
 class is defined in terms of the
  >
  (greater than) operation. The
  >
  operation
  
 performed to obtain the larger of two integers and the
  >
  operation performed
  
 to obtain the larger of two real numbers are two different operations. Over-
  
 loading ensures that the appropriate
  >
  operation is performed in each case.
  
 Overloading makes it possible to define an operation such as
  max
  in an ab-
  
 stract sense. So long as the parameters to the operation exhibit the appropri-
  
 ate behavior (in this case, they define
  >
  ), the operation will succeed.
  
 N",NA
 A SHORT EXAMPLE,"The principal advantage claimed for object-oriented programming is that it 
 promotes reuse of valuable code. If an abstract data type has been imple-mented as 
 a class, then a related data type can be implemented as a subclass, automatically 
 reusing the code that still applies (by inheriting it) and redefin-ing those operations 
 that differ (by overloading the old names with new im-plementations).
  
 For example, consider the abstract type
  Collection
 , values of which are unordered 
 groups of integers, where individual integers may appear more than once in a 
 collection. Such an abstract data type would have several op-erations, such as the 
 following:
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
 SIMULA,143,NA
3 1,"Object-oriented programming began when Simula introduced a novel concept: A 
 record may contain a procedure field. Such records are called
  classes
 .
 1
 As an 
 example, consider Figure 5.2.
  
 Figure 5.2
  
 class
  Stack;
  
 1
  
 Size : 0..MaxStackSize := 0; -- initialized
  
 2
  
 Data :
  array
  0..MaxStackSize-1
  of
  integer;
  
 3
  
 procedure
  Push(
 readonly
  What : integer);
  
 4
  
 begin
  
 5
  
 Data[Size] := What;
  
 6
  
 Size := Size+1;
  
 7
  
 end
 ; -- Push;
  
 8
  
 procedure
  Pop() : integer;
  
 9
  
 begin
  
 10
  
 Size := Size-1;
  
 11
  
 return
  Data[Size];
  
 12
  
 end
  -- Pop;
  
 13
  
 procedure
  Empty() : Boolean;
  
 14
  
 begin
  
 15
  
 return
  Size = 0;
  
 16
  
 end
 ; -- Empty
  
 17
  
 end
 ; -- Stack
  
 18
  
 variable
  
 19
  
 S1, S2 : Stack;
  
 20
  
 begin
  
 21
  
 S2 := S1;
  
 22
  
 S1.Push(34);
  
 23
  
 if not
  S2.Empty()
  then
  S2.Pop()
  end
 ;
  
 24
  
 end
 ;
  
 25
  
 Classes are like types; variables may be declared of a class type, as in line 20. Each 
 such declaration introduces a new instance of the class, that is, a new object. The 
 object contains fields that are variables (that is, instance vari-ables) and fields that 
 are procedures (that is, methods). Object variables can be assigned (line 22); objects 
 can be manipulated by their methods, which are named just like fields (lines 23 and 
 24). The three methods of
  Stack
  all have an implicit parameter: the stack object itself. 
 Therefore, the call in line 23 implicitly acts on stack
  S1
 .
  
 A problem with classes is that a binary operation, such as testing two stacks for 
 equality, must be performed by either the first or the second object, taking the other 
 object as a parameter, as shown in Figure 5.3.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  Simula’s classes are the ancestors of Pascal’s records and coroutines (Chapter 2), in addi-tion to 
 object-oriented programming.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 SMALLTALK,"Smalltalk is the name of a family of programming languages developed at Xe-rox 
 PARC (Palo Alto Research Center) as part of the Dynabook project. Dyn-abook was 
 envisioned as the ultimate personal computer — small, portable, with excellent 
 graphics and virtually unlimited memory and computing power. Smalltalk was 
 designed as Dynabook’s programming language.
  
 Smalltalk has gone through a long evolution, including Smalltalk-72, Smalltalk-
 76, Smalltalk-78, and Smalltalk-80. Many individuals have con-tributed to the 
 development of its variants, most notably Alan Kay, Daniel In-galls, and Peter 
 Deutsch. I will consider only Smalltalk-80, and whenever I say “Smalltalk,” I mean 
 Smalltalk-80. A standard Smalltalk reference is known as the Blue Book [Goldberg 
 83].
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.1 Assignment and Messages ,"Assignment binds an object to an identifier, as in Figure 5.4,
  
 Figure 5.4 
  
 count := 10
  
 which binds the integer object 10 (that is, the particular instance of the class 
 Integer
  
 that represents the number 10) to the identifier
  count
 .
  Count
  tem-porarily acquires 
 type integer. Smalltalk has no type declarations for vari-ables, but objects are typed 
 by their class. Assignment statements are also expressions; they return the value of 
 the right-hand side.
  
 Literals are provided for some objects. These include numbers (integer or real), 
 single characters (for example,
  $M
  or
  $a
 , where
  $
  quotes a single charac-ter), strings 
 (for example,
  ’hi there’
 ), symbols (for example,
  #red
 ,
  #left
 , where
  #
  quotes symbols), 
 and heterogeneous arrays (for example,
  #(1 $a’xyz’)
 ). Literals actually refer to 
 unnamed objects of an appropriate class that are initialized to the appropriate 
 values. Literals are no different from other objects — the protocol of their class 
 defines the messages they will re-spond to.
  
 Smalltalk predefines several objects, including
  nil
  (the only instance of class
  
 UndefinedObject
 ),
  true
  (the only instance of class
  True
 ), and
  false
  (the only instance of 
 class
  False
 ).
  
 Expressions illustrate the way Smalltalk uses messages to define interob-ject 
 communication. The expression
  1+2
  does not pass the values 1 and 2 to a
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.2 Blocks ,"Smalltalk blocks represent a sequence of actions that are encapsulated into a single 
 object. Blocks are used to implement control structures as well as functions. A block 
 is a sequence of expressions, separated by periods and de-limited by square 
 brackets, as shown in Figure 5.8.
  
 Figure 5.8
  
 [index := index + 1.
  
 anArray
  at:
  index
  put:
  0]
  
 When a block expression is encountered, the statements in the block aren’t 
 immediately executed. For example, the code in Figure 5.9
  
 Figure 5.9
  
 incr := [index := index + 1.
  
 anArray
  at:
  index
  put:
  0]
  
 assigns the block to variable
  incr
 , but doesn’t perform the addition or array update. 
 The unary message selector
  value
  causes a block to be executed. Thus
  incr
  value
  will 
 increment
  index
  and zero an element of
  anArray
 . In particular,
  [statement list]
  value
  
 directly executes the anonymous block. The value returned by a block when it is 
 evaluated is the value returned by the last statement in the block.
  
 Blocks are used in conditional and iterative constructs in an interesting manner. 
 Consider an
  if
  statement, which is coded in Smalltalk, by sending two blocks (one for 
 each branch) to a Boolean value, which selects the appro-priate block and then 
 executes it, as in Figure 5.10.
  
 Figure 5.10
  
 a < 0
  
 ifTrue:
  [b := 0]
  
 1
  
 2
  
 ifFalse:
  [b := a
  sqrt
 ]
  
 3
  
 Our usual model of conditional statements has been inverted. A Boolean value isn’t 
 passed to an
  if
  statement; rather the
  if
  statement is passed to the Boolean!
  
  
 Repetitive execution is obtained by passing a loop body to an integer or to a 
 Boolean block, as in Figure 5.11.
  
 Figure 5.11
  
 4
  timesRepeat:
  [x := x
  sin
 ]
  
 1
  
 [a < b]
  whileTrue:
  [b := b
  sqrt
 ]
  
 2
  
 The Boolean value
  a < b
  is enclosed in a block in line 2. The
  whileTrue:
  mes-sage is 
 only understood by blocks, not by Booleans. The reason for this design is that the 
 block can be reevaluated after each iteration, eventually resulting in
  False
  and 
 terminating the loop. A Boolean value is immutable, so it is worthless for loop 
 control.
  
 Blocks can also take parameters and be used as functions. Block parame-ters are 
 prefixed with
  :
  and are separated from the block body by
  |
  , as in Figure 5.12.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.3 Classes and Methods ,"Since all objects are instances of classes, the properties of an object are de-fined in its 
 class definition. A class contains instance variables (each instance of the class 
 contains an instance of each of these variables) and instance methods (each instance 
 of the class responds to messages that invoke these methods). Instance variables 
 have values that are private to a single object. Syntax rules require that instance 
 variables begin with a lowercase letter. (Uppercase letters are only used for
  shared 
 variables
 , visible globally, such as
  Object
 .) 
  
  
 Classes are themselves objects, and therefore are members (instances) of some 
 other class. For example,
  Integer
  belongs to
  Integer class
 .
  Integer class
  is called a
  
 metaclass
 . 
  
 All metaclasses belong to class
  Metaclass
 , which is 
 the only instance of
  Metaclass class
 , which is itself an instance of 
 Metaclass
 .
  
  
 A new instance of a class is created by sending the message
  new
  to the cor-
 responding class. Thus the code in Figure 5.15
  
 anArray := Array
  new:
  4
  
 would create a new array object with 4 cells (each initialized to
  nil
 ) and as-sign it to
  
 anArray
 .
  
 Programming languages that support abstract data types (and a class is a 
 glorified abstract data type) often allow the programmer to separate the de-
 scription of a type (here, class) into a specification and implementation part. 
 Smalltalk does not let the programmer separate specification from implemen-tation 
 cleanly, but it does provide much of what you would expect from ab-stract data 
 types, particularly information hiding.
  
 Figure 5.16 shows how to declare the abstract data type
  Stack
 .
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.4 Superclasses and Subclasses ,"There are two hierarchies of objects in Smalltalk. You have already seen one: the 
 hierarchy of instances and classes. Every object is an instance of some class. 
 Climbing up the hierarchy quickly leads to a cycle of
  Metaclass
  and 
 Metaclass class
 . The 
 other, richer, hierarchy is built from the subclass and superclass relations. Climbing 
 up this hierarchy leads to
  Object
 , which has no superclass. The
  Stack
  class of the 
 previous example is a direct subclass of 
 Object
 .
  
 Each new class is defined as a subclass of some existing class. A
  subclass 
 inherits 
 all the members of its immediate
  superclass
  as well as those of its indirect 
 superclasses. You have already seen that instances of class
  Stack
  in-herit the 
 methods
  error:
  and
  new
  from
  Stack
 ’s superclass
  Object
 . A subclass may declare its 
 own members and may introduce methods that override those inherited from its 
 superclass. When a reference to a message or a variable appears in an object, it is 
 resolved (if possible) in that object. Failing this, the object’s superclass is 
 considered, then the superclass of the superclass, up to class
  Object
 . If no definition 
 is found, a runtime error occurs.
  
  
 Subclasses are used to extend or refine the protocol of an existing class. In Figure 
 5.17, I define a subclass of
  Stack
  called
  IntegerStack
 . 
  
 Inte-
  
 gerStack
 s will limit stack elements to integers and will provide a new opera-tion 
  
 +
  , which adds corresponding elements of two stacks, yielding a new stack, 
 similar to vector addition.
  
  
 I will add two additional methods,
  pos:
 , which returns the stack element at a 
 particular position, and
  currentDepth
 , which returns the current depth of the stack. I 
 need
  pos:
  and
  currentDepth
  because 
  
 +
  can directly access 
 only its own stack, not the stack passed to it as a parameter. (The same asymmetry 
 of access plagues Simula, as discussed earlier.) I want these new methods to be 
 private to the class; they are to be used only by
  +
  , not by the clients of the class. 
 Unfortunately, Smalltalk does not provide a way to pre-vent such misuse. Still, I have 
 placed comments on lines 35 and 38 to indi-cate my intent.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.5 Implementation of Smalltalk ,"Smalltalk is designed to be portable. Ironically, Smalltalk has only recently become 
 widely available because of proprietary restrictions. Over 97 percent of the 
 Smalltalk package, including editors, compilers, and debuggers, is writ-ten in 
 Smalltalk. Smalltalk executes under a virtual machine that requires 6–12 KB of code. 
 Creating a Smalltalk virtual machine for a new target ma-chine takes about one staff-
 year.
  
 The Smalltalk virtual machine consists of a storage manager, an inter-preter, and 
 a collection of primitive methods. The storage manager creates
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.6 Subtle Features ,"Blocks are implemented by closures. Even though a block may be executed in an 
 environment quite different from its defining environment, it can still ac-
  
 cess its nonlocal variables correctly.
  
 Thus Smalltalk uses deep binding.
  
 Smalltalk manages to avoid the pitfall of accessing deallocated regions of a stack by 
 using a garbage collector instead of a stack (with its explicit release) to manage 
 object store. The anonymous method in Figure 5.20 prints
  4
 .
  
 |innerBlock outerBlock aVariable|
  
 1
  
 outerBlock := [ :aVariable |
  
 2
  
 innerBlock := [
  
 3
  
 aVariable
  write
  -- write aVariable
  
 4
  
 ]
  
 5
  
 ] .
  
 6
  
 outerBlock
  value:
  4 .
  
 7
  
 aVariable := 6 . -- Try to confuse the issue
  
 8
  
 innerBlock
  value
  -- writes 4
  
 9
  
 !
  
 10
  
 Smalltalk methods are perfectly capable of coercing their parameters; only by 
 looking at the documentation (or the implementation) can a programmer be sure 
 what types are accepted by a method and whether the method will co-erce types.
  
 Even more surprising is the
  become:
  method provided by
  Object
  (and therefore 
 available in all classes unless overridden). It is used as in Figure 5.21.
  
 Figure 5.21 
  
 anObject
  become:
  anotherObject
  
 After this message, all references to
  anObject
  and
  anotherObject
  are inter-changed. 
 Conventionally,
  anotherObject
  had no references before, so it now acquires 
 references. This facility can be used to build abstract data types that change their 
 implementation at some point. That is, in response to some message, an object may 
 execute the code in Figure 5.22.
  
 Figure 5.22 
  
 self
  become:
  newObject
  
 From that point on, all references to the old object are rerouted to the new ob-ject, 
 which could be of a different class entirely.
  
 In many ways tree-structured inheritance rules are too restrictive. For example, I 
 might have a class
  DisplayItem
  that represents items that can be graphically 
 displayed on a screen. Objects of this class would respond to mes-sages like
  rotate:
  
 or
  highlight
 . Another useful class might be
  Invento-ryItem
 , which represents items 
 that I might inventory. Objects of this class would respond to messages like
  
 reportBacklog
  or
  nameSupplier
 . It would be nice to allow some objects to be both
  
 DisplayItem
 s and
  InventoryItem
 s (for example, a bumper or aircraft wing). This can 
 only be done in Smalltalk 1.0 by making
  DisplayItem
  a subclass of
  InventoryItem
  or 
 vice versa. Neither alternative is attractive, because not all objects of one class 
 necessarily belong
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,"157
  
 to the other. (For example, I might be able to display a Saturn V rocket, but I
  
 probably won’t have it in my inventory.)
  
 A means of achieving
  multiple inheritance
 , in which a class is a direct subclass 
 of more than one superclass, was introduced in Smalltalk 2.0. It is
  
 complicated to use (none of the built-in Smalltalk classes uses it), because
  
 there can be name conflicts among the multiple ancestral lines. Smalltalk 2.0
  
 notices such conflicts at runtime and declares a conflict error.
  
 Since all
  
 classes are subclasses of
  Object
 , a class that inherits multiply sees
  Object
  
 along two different ancestry lines.
  
 The programmer needs to indicate
  
 whether such multiply defined ancestors are to be treated as a single ancestor
  
 or whether both are wanted. In the latter case, every invocation of a multiply
  
 defined method or access to a multiply defined instance variable must be
  
 qualified to indicate which ancestor is meant. In Eiffel, the programmer may
  
 rename inherited identifiers to avoid such name conflicts. Circular inheri-
  
 tance is always disallowed.
  
 N",NA
 C++,"C++ was developed at AT&T by Bjarne Stroustrup, who wanted to write
  
 event-driven simulations for which Simula would have been ideal but would
  
 also have been too inefficient. The original version of the language was devel-
  
 oped in 1980; at that time it was known as “C with Classes” and lacked a
  
 number of its present features. The name C++ was coined by Rick Mascitti in
  
 1983 as a pun on the C operator
  
 ++
  , which increments its operand. C++ is
  
 explicitly intended as the successor to C. (The same pun has been used to
  
 name [incr Tcl], an object-oriented enhancement to Tcl.)
  
 C++ was imple-
  
 mented for some time as a preprocessor that generated C. Full compilers are
  
 now available. C++ has an ANSI standard and a standard reference, the
  
 ARM [Ellis 90], which also includes some of the design rationale. Of particu-
  
 lar interest is Meyers’ book [Meyers 92], which explains how to use some of
  
 the language features and also why C++ does things the way it does.",NA
5.1 The Consequences of Static Binding ,"Most of the differences between C++ and Smalltalk can be explained by the fact that 
 C++ is designed to be an efficient, compiled language. It performs as much binding 
 as possible statically, not dynamically. Unlike Smalltalk, C++ is a statically typed 
 programming language. Every identifier in a C++ pro-gram has a type associated 
 with it by a declaration. That type can be either an ordinary C type or a class.
  
 One consequence of static typing is that C++ does not allow classes to be 
 introduced at runtime, unlike Smalltalk, in which introducing a class is a runtime 
 operation accomplished by an appropriate invocation to the super-class. For this 
 reason, the class hierarchy based on the subclass relation is less extensive than in 
 Smalltalk. It is common for C++ programs to build many top-level classes, whereas 
 in Smalltalk, all classes are subclasses, di-rectly or indirectly, of the class
  Object
 .
  
 Another consequence of static typing is that classes are not themselves ob-jects. 
 C++ programs have no hierarchy of instances and classes. In this re-gard, C++ 
 displays less uniformity (in the sense introduced in Chapter 1)
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.2 Sample Classes ,"I will rely on examples to describe C++. The syntax of C++ is compatible with the 
 syntax of C; the examples use correct C++ syntax. The first example shows how to 
 introduce complex numbers in C++, even though the standard C++ library already 
 includes an implementation of complex numbers.
  
 I first declare a new class
  Complex
  and make it a subclass of
  Magnitude
 , which I 
 will not show here. A
  Complex
  object contains two floating-point numbers, one to 
 hold the real part of the number and one to hold the imagi-nary part. In Smalltalk, a 
 program creates a new class dynamically by send-ing a message to its intended 
 superclass, in this case
  Magnitude
 . In C++, the programmer creates a new class 
 statically by declaring it, as in Figure 5.23.
  
 Figure 5.23
  
 class
  Complex : Magnitude {
  
 1
  
 double realPart;
  
 2
  
 double imaginaryPart;
  
 3
  
 };
  
 4
  
 The braces
  {
  and
  }
  take the role of
  begin
  and
  end
 . The class
  Complex
  is de-clared in 
 line 1 to be a subclass of
  Magnitude
 . Top-level classes omit the colon and the name of 
 the superclass.
  Complex
  contains two instance variables,
  re-alPart
  and
  imaginaryPart
 , 
 both declared to be of type
  double
 . Instance vari-ables are called “data members” in 
 C++, and the methods are called “member functions.” I will continue to follow 
 Smalltalk nomenclature for consistency.
  
 The first operation I will declare is to create and initialize a complex num-ber. 
 The Smalltalk class inherits a method called
  new
  from its superclass for this 
 purpose. The C++ compiler provides a default
  new
  function that is passed a hidden 
 parameter that specifies the amount of space to be allocated from the heap; an 
 explicit allocator function can be provided if the programmer de-sires. Complex 
 variables can also be allocated from the central stack in the normal manner without 
 recourse to the
  new
  function, as in Figure 5.24.
  
 Figure 5.24
  
 Complex *pz =
  new
  Complex; // allocated from the heap
  
 1
  
 Complex z; // allocated from the central stack
  
 2
  
 The comment delimiter in C++ is
  
 //
  . The
  
 *
  
 in line 1 declares
  pz
  as a
  
 pointer type, pointing to objects of type
  Complex
 . The proper version of
  new
  is 
 specified by adding the class name.
  
 I will rely on the defaults provided to allocate
  Complex
  objects; however, I must 
 provide a way to initialize such objects. In Smalltalk, I would establish a
  
 real:imaginary:
  method to set the values of a
  Complex
  object. C++ allows the program 
 to provide an initializer (also called a ‘‘constructor’’) and a final-izer (also called a 
 ‘‘destructor’’) for each class. The
  initializer
  is called each time an object of the class 
 is created (either explicitly or implicitly, as when a parameter is passed by value), 
 and the
  finalizer
  is called whenever an in-stance of the class goes out of scope or is 
 explicitly freed. Initializers can be used to establish values of instance variables; 
 finalizers can be used to free
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
6,"CHAPTER 5
  
 OBJECT-ORIENTED PROGRAMMING
  
 rately for each instantiation of the generic class. In this example, three spe-
  
 cific classes are compiled. Line 37 shows that it is possible to declare stacks
  
 of stacks of integer. (The extra space between the
  >
  characters is needed to
  
 prevent the parser from misinterpreting them as the single
  
 >>
  
 operator.)
  
 The header for the overloaded operator
  <<
  (lines 21–32) is unusual; it is de-
  
 clared to be a friend of
  ostream
 , so that the stream output routines have ac-
  
 cess to the contents of the stack. The stack itself is passed as a parameter, so
  
 that the output statements of lines 40, 42, and 44 can be written with
  cout
  on
  
 the left, as is proper style in C++, not the right, as I have been doing previ-
  
 ously.
  
 N",NA
 FINAL COMMENTS,"At first glance, object-oriented languages are just a fancy way of presenting
  
 abstract data types. You could argue that they don’t present a new paradigm
  
 of programming, but rather a structuring principle that languages of any sort
  
 might employ. However, I would counter that Smalltalk and C++ have devel-
  
 oped the concept of abstract data type into a new form of programming.
  
 First, object-oriented programming provides a new view of types. The
  
 type of an object is the protocol it accepts. Two objects are type-compatible if
  
 they respond to the same set of messages. This view is highly abstract, be-
  
 cause it doesn’t say the objects have the same form, only that they are func-
  
 tionally interchangeable.
  
 There is no straightforward way to check type
  
 compatibility.
  
 Second, the nature of instantiation distinguishes a class from an abstract
  
 data type exported from a module. Each instance is independent, with its
  
 own data and procedures, although all instances may share common class
  
 variables. Data types exported from a module may be instantiated, but the
  
 exporting module itself cannot be. A module is much more of a static, com-
  
 pile-time, passive entity. A class is more dynamic, with more runtime and ac-
  
 tive qualities.
  
 Third, the hierarchy of subclasses leads to a style of programming known
  
 as
  programming by classification
 . The abstract data types are organized into a 
 tree, with the most abstract at the root and the most specified at the
  
 leaves. Incremental modifications to a program are accomplished by intro-
  
 ducing new subclasses of existing classes. Each new class automatically ac-
  
 quires much of what it needs by reference to its superclass. Methods are
  
 automatically overloaded. Programming by classification is an important tool
  
 for achieving reuse of valuable code, and this tool goes well beyond the reuse
  
 that comes from modularization into abstract data types.
  
 Smalltalk is attractive in many ways. It provides a highly interactive and
  
 integrated programming environment that uses the latest in computer tech-
  
 nology (in short, a graphical user interface). Its object-oriented style provides
  
 an interesting inversion of our usual view of programming. Objects are pre-
  
 eminent, uniform, and autonomous. They cooperate by passing messages, but
  
 no object controls any other. Objects are fairly robust, since the worst thing a
  
 program can do is send one a message it can’t handle. In this case, the object
  
 doesn’t fail, but rather ignores the message and issues an error.
  
 Smalltalk is not without its problems, both real and perceived. It has only
  
 recently become generally available for small machines. Smalltalk may have
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES ,NA,NA
Review ,NA,NA
Exercises,"5.1 
  
 5.2 
  
 5.3
  
 What are the consequences in C++ of static typing?
  
 Does an object in Smalltalk require its own private stack? In C++?
  
 Write a class in Smalltalk and/or in C for rational numbers, that is, numbers 
 that can be represented by an integer numerator and denomi-nator. Instance 
 variables should include both the numerator and the denominator. Your 
 implementation should always reduce fractions to their lowest terms. You 
 must overload all arithmetic and conditional op-erators.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Challenge Exercises,"5.6
  
 5.7
  
 5.8
  
 5.9
  
 Simula classes contain procedures. Unlike ordinary procedures, proce-dures 
 inside classes may not declare
  own
  variables, that is, variables whose values 
 are retained from one invocation to the next. If you want to add such a feature, 
 what would you like it to mean?
  
 In Smalltalk, not everything is an object. Name three programming-language 
 entities that are not objects. Could Smalltalk be modified so that they are 
 objects?
  
 Show how the
  timesRepeat:
  method in Figure 5.11 (page 147) could be coded.
  
 In line 12 of Figure 5.17 (page 152), show how an element of a different class 
 could masquerade as an
  Integer
  and bypass the type check.
  
 5.10
  True
  and
  False
  are subclasses of
  Boolean
 . Each has only one instance (
 true
  and
  
 false
 , respectively). First, how can class
  True
  prevent other instances from 
 being created? Second, why not use the simpler organi-zation in which
  Boolean
  
 has two instances? Hint: Consider the code for 
 ifTrue:ifFalse:
 .
  
 5.11
  Build a method for
  Block
  that accepts a
  for:from:to:
  message to im-
  
 plement
  for
  loops. Don’t use
  whileTrue:
 .
  
 5.12
  Build a subclass of
  Block
  that accepts a
  for:in:
  message to implement 
  
 CLU-
 style iterators.
  
 5.13
  Defend Smalltalk’s design decision that error messages are to be gener-ated by 
 objects via messages to themselves, and that the
  error:
  method is to be 
 inherited from
  Object
 .
  
 5.14
  Why should
  Magnitude
  define methods like
  >
  but give them error-generating 
 code? In other words, what is the point of introducing pure virtual methods?
  
 5.15
  In Smalltalk, a new class is constructed at runtime by sending a mes-sage to 
 its superclass. In C++, classes are constructed at compile time by declaration. 
 Show how the Smalltalk method is more powerful.
  
 5.16
  Enumerate what is missing in Smalltalk and in C++ for building ab-
  
 stract data types.
  
 5.17
  What is the effect of a C++ class declaring that it is its own friend?
  
 5.18
  C is not block-structured. In particular, one cannot introduce a type within a 
 name scope. What complexities would be introduced if C++ were based on a 
 block-structured language, and classes could be intro-duced in a name 
 scope?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 6 ,NA,NA
O,NA,NA
Dataflow,"Figure 6.1
  
 There is some evidence that the next big wave of change to wash over pro-
 gramming languages will be concurrency. Both architectures and languages for 
 concurrency have been around for some time. In this chapter, I will dis-cuss the 
 dataflow architectural concept and the languages that have been de-
  
 signed to conform to it.
  
 Dataflow is one way to achieve concurrency,
  
 particularly at the fine-grain level: It finds multiple operations that can be 
 undertaken concurrently within the evaluation of a single expression. Ideas from 
 dataflow have found their way into parallelizing compilers for more con-ventional 
 architectures, as well. The ideas here in some ways prepare for Chapter 7, which 
 deals with concurrent programming languages that work at a coarser level of 
 granularity.
  
 Sequential execution is an essential characteristic of the von Neumann computer 
 architecture, in which programs and data are stored in a central memory. The 
 concepts embodied by classical architecture have not been di-rectly applicable to 
 the domain of parallel computation. Most programming languages have evolved 
 from von Neumann languages, designed specifically for the von Neumann 
 architecture, so programmers have been conditioned to analyze problems and write 
 programs in sequential fashion.
  
 The dataflow approach was first suggested by Karp and Miller [Karp 66] as an 
 alternative to the von Neumann architectural and linguistic concepts. Consider 
 computation of the series of statements in Figure 6.1.
  
 A := B*C + D/F;
  
 1
  
 G := H**2 + A;
  
 2
  
 A data-dependency graph called a
  dataflow graph
  represents the order-ing of 
 evaluation imposed by data dependencies. It encodes the fact that an expression 
 can’t be evaluated before its operands are evaluated. The dataflow graph for Figure 
 6.1 appears in Figure 6.2.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 169",NA
1,"CHAPTER 6
  
 DATAFLOW
  
 N",NA
 DATAFLOW COMPUTERS,"There are two classes of dataflow architectures.
  
 The first class is called
  
 “static,” since such architectures do not support reentrant dataflow code (that
  
 is, code that is used simultaneously in multiple places in the dataflow graph)
  
 and recursion.
  
 The simple dataflow computer model introduced above is
  
 static, as is the machine proposed by Dennis [Dennis 77].
  
 The second class is called “dynamic.” Such machines support simultane-
  
 ous multiple incarnations of an activity, recursion, and loop unfolding. In a
  
 dynamic dataflow architecture, an arc may carry multiple tokens, and care is
  
 taken to ensure that activities fire only upon receipt of matching tokens along
  
 their input arcs. Tokens are labeled to distinguish values arising in different
  
 contexts or from different incarnations of an activity. Two tokens match only
  
 if their activity labels match. For example, I might wish to perform the com-
  
 putation specified by a dataflow graph on each element of a vector of 1000
  
 values. With a dynamic dataflow architecture, I can place 1000 tokens on the
  
 input arc. Tokens are labeled to insure that values produced during the com-
  
 putation can be ascribed to the appropriate input value.
  
 The dynamic architecture outlined below is essentially that of the classical
  
 Manchester Dataflow Computer [Gurd 85]. Modern dynamic dataflow archi-
  
 tectures may look different. Tokens are labeled, but I leave out the specific
  
 details of how the labels are generated or used, except that labels are
  
 matched to identify matching tokens.
  
 The architecture is schematically depicted in Figure 6.5. The machine op-
  
 erates as a circular pipeline divided into four sections. The processing unit
  
 receives packets containing operands and an instruction. The instruction is
  
 executed on the accompanying operands, and the result tokens (after appro-
  
 priate labeling) are placed back on the bus to be sent to the I/O switch. The
  
 I/O Switch is included in the pipeline to serve as an I/O port. The matching
  
 unit consists of associative token storage.
  
 When a token arrives at the
  
 matching unit, the storage is searched for any tokens with the same label and
  
 destination. If matches are discovered, these tokens are read out of the store,
  
 and a packet is formed of all these tokens to be sent on to the program store.
  
 The destination field of a token carries the address in the program store of
  
 the instruction to which the token is directed.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,N,NA
 VAL,"Val is a dataflow language developed at MIT by J. Dennis and others [McGraw 82]. 
 This language was originally designed for the static dataflow architecture of Dennis, 
 so it does not support dynamic dataflow features like
  
 recursion.
  
 Val
  
 has
  
 been
  
 meticulously
  
 defined
  
 both
  
 by
  
 an
  
 axiomatic
  
 [Ackerman 80] and a denotational [Gehani 80] semantics (these concepts are 
 discussed in Chapter 10.) Val is functional in nature, so side effects are ab-sent. 
 However, Val is strongly typed (using structural equivalence) and in-tentionally 
 looks more like conventional languages than LISP or FP does. In fact, it looks much 
 like ML (described in Chapter 3). For example, the Val function in Figure 6.6 
 computes the mean and standard deviation for parame-ters
  X
 ,
  Y
 , and
  Z
 .
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,"CHAPTER 6
  
 DATAFLOW
  
 N",NA
 SISAL,"Val was one of the first serious attempts to produce a production-quality dataflow 
 language. A descendent of Val called Sisal was created to exploit the capabilities of 
 dynamic dataflow computers [McGraw 83]. A Sisal compiler exists, with code 
 generators for Vax, Crays, HEP multiprocessors, and the Manchester dataflow 
 computer.
  
 The most obvious advantage of Sisal over Val is its support of recursion. 
 Recursive functions are useful and natural, especially in functional lan-guages. Val’s 
 rejection of recursion was a reflection of the design of early static dataflow 
 machines. Sisal also supports streams, which are needed for ordinary sequential I/O 
 and as a means of composing functions.
  
 Sisal programs can be decomposed into distinct compilation units that ex-plicitly 
 import and export functions. Sisal also extends Val’s iterative and parallel (
 forall
 ) 
 loop forms. They can return arrays or streams. Parallel loops can also define explicit 
 inner and outer products, making array manipu-lation cleaner and potentially more 
 efficient.",NA
4,N,NA
 POST,"Chinya V. Ravishankar developed Post as a Ph.D. thesis starting in 1981 
 [Ravishankar 89]. Post introduces several novel ideas. First, it lets the pro-grammer 
 determine the level of speculation in evaluation. As I mentioned earlier, speculative 
 evaluation can lead to nontermination under certain cir-cumstances, but strictly lazy 
 evaluation reduces parallelism. A second novel concept is
  polychronous data 
 structures
  that are partly
  synchronous 
 (must be available before used) and partly
  
 asynchronous
  (parts can be used when ready). Third, Post provides 
 communication between computational ac-tivities in order to terminate speculative 
 computation that may turn out to be unnecessary. Communication between 
 computations is not natural in purely functional programming languages. Much of 
 their semantic elegance derives from their lack of side effects, so computations 
 scheduled in parallel must not depend on each other’s results. Further, a purely 
 functional language per-mits only deterministic computations and prohibits history-
 sensitivity.
  
 Post was never fully implemented, but a prototype compiler was built. It first 
 builds a dataflow graph from the program and then converts that graph into 
 instructions for a dynamic dataflow machine. Post needs a dataflow ma-chine (never 
 implemented) that has a few special features, such as a “hold”node for implementing 
 lazy evaluation and a “terminate store” to help find and remove terminated tokens.",NA
4.1 Data Types ,"Values can be either primitive (
 integer
 ,
  real
 ,
  Boolean
 ,
  char
 ) or structured. Structured 
 values are constructed using the abstractions
  stream
  and
  tuple
 . Both are sequences 
 of values, but a tuple may be heterogeneous and is of fixed length, while a stream is 
 homogeneous and of unbounded length. All operators and functions are 
 automatically overloaded to apply to streams; they create streams of results of 
 pointwise application. Nested structures are allowed; an element of a stream may 
 itself be a stream.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.2 Programs ,"A program consists of a name, a parameter list (used as a pattern), and a tar-get 
 expression. The pattern is matched against input data and serves to bind formal 
 parameters to values in the input. The target expression generates a value that is 
 returned by the program. The target expression may introduce a new name scope, 
 with type definitions and identifier declarations. Post is statically scoped. Figure 
 6.13 shows a simple program.
  
 Figure 6.13
  
 function
  AddNumbers{a,b,c};
  
 1
  
 type
  a, b, c : int
  
 2
  
 in
  a+b+c
  
 3
  
 end
 ;
  
 4
  
 The structure is much like a classical procedure declaration, with formal pa-
 rameters (the pattern
  {a,b,c}
  in line 1), type declarations for the formal pa-rameters 
 (line 2), and a body (line 3).
  
 The type declarations are separated from the pattern for clarity, because the 
 pattern can become complex. For example,
  {x,y,z,}*
  is a pattern that repeats the 
 template
  {x,y,z}
  indefinitely, matching three more values of the input stream each 
 time. On the other hand,
  {x,{y,}*,z}
  is a 3-tuple with a stream as the second 
 component. Figure 6.14 uses a pattern that matches a stream.
  
 Figure 6.14
  
 function
  AddPairs{x,y,}*;
  
 1
  
 type
  x, y : int
  
 2
  
 in
  x+y
  
 3
  
 end
 ;
  
 4
  
 This program outputs a stream consisting of the sums of adjacent values in
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.3 Synchrony Control  1 2,"By using connectors other than a comma, the programmer may specify the
  
 degree of synchrony required in pattern matching.
  
 The comma indicates
  
 completely asynchronous matching; the actual parameters may be accessed
  
 independently of each other. If the pattern uses only
  
 ˆ
  , matching is syn-
  
 chronous: all elements must be present before any element is made available to the 
 target expression.
 1
 If the pattern uses only
  ˜
  , matching is sequential: the
  i
 th element 
 of the matched data structure is available only after all previ-ous elements have 
 arrived (even if they have not been accessed). Any or all of these combinators may 
 occur within a pattern. If several of them occur, ac-cess is polychronous; precedence 
 rules indicate how to group subpatterns in the absence of parentheses.
  
 In Figure 6.14, I could change the input pattern to
  {xˆy,}*
 , forcing pair-wise 
 synchronization of input elements. The program would compute the same results, 
 because
  +
  requires both operands before proceeding.
  
 Sequential patterns are used for loops, as shown in the Figure 6.15.
  
 function
  Largest{x˜}*
  init
  0;
  
 1
  
 type
  x : int
  
 2
  
 in if
  Largest > x
  then
  Largest
  else
  x
  end
  
 3
  
 end
 ;
  
 4
  
 This program finds the largest value in a stream of integers. It terminates when the 
 last instance of the target expression terminates; the value gener-ated in this last 
 instance is returned as the value of the program. The pro-gram name is used to 
 name the current value, initialized to
  0
  in line 1 and compared with the next value in 
 line 3. The dataflow graph corresponding to this program has a cycle holding an 
 initial token with value
  0
 . Conditional expressions, like the one in line 3, are 
 evaluated speculatively. The result from the branch that is not needed is discarded 
 after evaluation is complete.
 2 
  
 An alternative syntax for conditionals is shown 
 in Figure 6.16.
  
 Figure 6.16
  
 function
  OddSquares{x,}*;
  
 1
  
 type
  x : int
  
 2
  
 in
  [(x
  mod
  2)
  
  0] x*x
  
 3
  
 end
 ;
  
 Line 3 evaluates to
  x*x
  only if the condition in square brackets is true, that 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  The concept of synchronization in concurrent programming languages is related; it is dis-cussed 
 in Chapter 7.
  
  
  If conditionals were evaluated lazily, the programmer could hoist both branches out of the 
 conditional to force them to be evaluated speculatively.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.4 Guardians ,"Guardians
  implement shared data. They look like procedures and act like variables. 
 In a sense, they are like objects in an object-oriented programming language 
 (discussed in Chapter 5). Assignment to the guardian invokes the procedure with 
 the given value as an actual parameter. The procedure com-putes a stored value, 
 which can differ from the value of the actual parameter. When the program accesses 
 the guardian’s value, it gets a copy of the current stored value. Such access is lazy, in 
 the sense that it occurs only after all nonguardian values used in the expression 
 have arrived. The guardian only has one instance, which prevents simultaneous 
 computations from attempt-ing simultaneous assignment. For example, consider the 
 program of Figure 6.17.
  
 Figure 6.17
  
 function
  LargestFactor{x,}*;
  
 1
  
 type
  x : int
  
 2
  
 guardian
  Largest{v}
  init
  0;
  
 3
  
 type
  v : int
  
 4
  
 in if
  Largest < v
  then
  v
  else
  Largest
  end
  
 5
  
 end
  -- Largest
  
 6
  
 in
  Largest :=
  if
  (N
  mod
  x) = 0
  then
  x
  end
  
 7
  
 end
 ; -- LargestFactor
  
 8
  
 LargestFactor(stream(2,99));
  
 9
  
 This program finds the largest factor of
  N
  (a nonlocal variable) smaller than 100. 
 Even though a new instance of
  LargestFactor
  is created for each ele-ment of the input 
 stream, there is only one instance of its local guardian, 
 Largest
  (lines 3–6). Each 
 element of the input stream is tested by an in-stance of line 7; the result is assigned 
 into the guardian. The conditional ex-pression in line 7 evaluates to
  nil
  if the Boolean 
 is
  false
 ;
  nil
  values are filtered out and are not passed to the guardian. Each 
 assignment invokes 
 Largest
 ’s target expression (line 5). This expression computes a 
 new value for the guardian, namely, the largest value so far assigned to it. The value 
 of a program containing guardians is a tuple of the guardians’ final values; in this 
 case, there is only one guardian, so a single value is returned.
  
 Because guardians may have different values if they are evaluated at dif-ferent 
 times, it is necessary to permit lazy evaluation of actual parameters
  
 that are expressions involving guardians.
  
 Post allows parameters to be
  
 passed in value mode (the default) or lazy value mode.",NA
4.5 Speculative Computation ,"Speculative computation can be terminated by a combination of program-defined 
 labels and an explicit
  terminate
  statement. Any expression may be labeled, and the 
 value resulting from that expression carries the label until it exits the scope in which 
 the label is declared. An individual value may carry many labels, since it may be 
 composed of many components, each of which
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,N,NA
 FINAL COMMENTS,"Val and Sisal look, at first glance, like ordinary imperative languages. What makes 
 them dataflow languages is that they are functional, so that specula-tive evaluation 
 is possible, and they provide for explicitly concurrent loop exe-cutions.
  
 Post was developed in reaction to this imperative appearance. It tries to give the 
 programmer a feeling of labeled tokens being routed on arcs. The 
 terminate
  
 statement only makes sense in such a context, for example. Al-though Post is a 
 worthy attempt to mirror dataflow architectures better than Val or Sisal, the result 
 is not particularly readable. Lack of clarity, in the sense introduced in Chapter 1, is 
 its major weakness.
  
 In a sense, all these languages are failures, because dataflow computing never 
 became popular. Very few dataflow computers were ever built, and in-terest in this 
 field has mostly subsided. Still, it is instructive to see how ar-chitectural design and 
 programming language design influence each other. Not only did dataflow 
 architecture lead to new languages, but those lan-guages dictated enhancements to 
 the architecture (such as multiple-field la-
  
 bel stacks on tokens).
  
 A similar interplay is now taking place between
  
 architecture and languages as massively parallel and distributed computers are 
 becoming available. That is the subject of Chapter 7.
  
 Dataflow has had some successes. Optimizing compilers for vector ma-chines 
 build dataflow graphs in order to schedule computations effectively.
  
 The graphs indicate what dependencies constrain the order of evaluation.
  
 From one point of view, you could say that dataflow has been quite suc-cessful 
 and is widely used. Spreadsheets incorporate a form of data-driven computation to 
 update values that depend on other values that may have changed. The internal 
 representation of a spreadsheet is very like a dataflow graph. Strangely, the 
 languages used in spreadsheet programming are quite different from any of the 
 languages described here. First, they are not linear; that is, they are not organized as 
 a text with a start, an ordered set of com-mands, and an end. Instead, each cell of a 
 spreadsheet (typically, a two-dimensional grid of cells) is separately “programmed.” 
 For a cell acting as a leaf in the dataflow graph, the program indicates the value of 
 that cell. For a cell acting as a computation node, the program indicates how to 
 recompute the value of that cell based on the values of other cells. These other cells 
 can be named explicitly, but accumulation operators such as summation and aver-
 aging can also specify a set of cells (generally, a contiguous one-dimensional subset). 
 This organization is reminiscent of declarative programming (the subject of Chapter 
 8), in which there is no necessary order to the pieces that together make up a 
 program.
  
 A second difference in spreadsheet programming is that the user can often 
 control to what extent computation is speculative. This control is specified as the 
 number of times to reevaluate each computational cell when one of its in-puts 
 changes. Zero means do not update; an infinite value means to reevalu-ate until 
 values do not change. In other words, the binding of evaluation strategies, which is 
 usually done at language-design time, and only occasion-ally at compile time, can be 
 deferred until runtime.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"Figure 6.20
  
 6.1
  
 Draw a dataflow graph for the code of Figure 6.20.
  
 (A + B) * (A + B + C)
  
 6.2
  
 Draw a dataflow graph for the code of Figure 6.21.
  
 Figure 6.21
  
 A := 0;
  
 1
  
 6.3
  
 while
  A < 10
  do
  
 2
  
 A := A + 3;
  
 3
  
 end
 ;
  
 4
  
 How do nondataflow languages allow the programmer to specify evalua-tion 
 strategy?",NA
Challenge Exercises,"Figure 6.22
  
 6.4
  
 Draw a dataflow graph for the code of Figure 6.22.
  
 6.5
  
 procedure
  Orbit(A : integer) : integer;
  
 1
  
 begin
  
 2
  
 if
  A = 1
  then
  
 3
  
 return
  1;
  
 4
  
 elsif
  even(A)
  then
  
 5
  
 return
  Orbit(A/2);
  
 6
  
 else
  
 7
  
 return
  Orbit(3*A+1);
  
 8
  
 end
 ;
  
 9
  
 end
 ;
  
 10
  
 In Figure 6.15 (page 180), which version of
  >
  is meant on line 3? That is, does
  
 eos
  act as the minimal or maximal element?
  
 6.6
  
 In Figure 6.18 (page 182), what would be the effect of changing line 13 as 
 follows?
  
 in
  [ AddLabel(x)
  haslabel
  Even] AddLabel(x)
  
 6.7 
  
 6.8
  
 Modify Figure 6.19 (page 182) so that speculative computation is not 
 terminated, but the first factor found is still returned.
  
 In spreadsheets, how can reevaluating more than once have a different effect 
 from evaluating only once?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 7 ,NA,NA
O,NA,NA
Concurrent Programming,"Architectural advances of recent years, coupled with the growing availability of 
 networked computers, have led to a new style of computing, called
  concur-rent 
 programming
 , that allows multiple computations to occur simultane-ously in 
 cooperation with each other. Many people distinguish two classes of concurrent 
 programming:
  Distributed programming
  refers to computa-tions that do not share 
 a common memory, and
  parallel programming 
 refers to computations that share a 
 common memory. This distinction is not always helpful, since it is possible to 
 implement a distributed computation on a shared-memory computer, and to 
 implement a parallel computation on a distributed-memory computer. It is up to the 
 compiler and operating system to implement on the underlying architecture 
 whatever concurrency style the programming language promotes. Terminology is 
 less standard in the area of concurrent programming than elsewhere, so I will be 
 somewhat arbitrary, but consistent, in my nomenclature.
  
 A
  thread
  is a sequential computation that may interact with other simul-taneous 
 computations. A program that depends on a particular thread reach-ing some point 
 in computation before another thread continues must make that dependency 
 explicit; it is erroneous to assume anything about relative speeds of execution. The 
 reason for this rule is that the language does not usually have much control over 
 execution speeds. Individual threads may be implemented by time-sharing a single 
 CPU, and the scheduler may be outside the control of the language (in the operating 
 system). If threads are on differ-ent CPUs, the CPUs may have different speeds or 
 may have other work that renders them slow in an unpredictable way. The 
 communication expense for cooperation among threads may be unpredictable. 
 Threads might be dynami-cally migrated from one CPU to another to improve 
 performance, but with a temporary delay.
  
 The connection between programming languages and operating systems is 
 especially close in the area of concurrent programming. First, threads are 
 sometimes supported by the underlying operating system, so the language 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 187",NA
1,N,NA
 STARTING MULTIPLE THREADS,"Syntax for starting multiple computations tends to be straightforward. In Modula (I 
 mean original Modula [Wirth 77], not Modula-2), a thread is started by invoking a 
 procedurelike object; when the “procedure” returns, the thread disappears. 
 Meanwhile, the computation that started the thread con-tinues executing. The new 
 thread may itself create other threads by invoking them. Figure 7.1 shows a 
 program that merge-sorts an array by recursively creating threads.
  
 Figure 7.1
  
 type
  
 1
  
 DataArray =
  array
  whatever
  of
  integer;
  
 2
  
 thread
  MergeSort(
  
 3
  
 reference
  Tangled : DataArray;
  
 4
  
 value
  LowIndex, HighIndex : integer);
  
 5
  
 variable
  
 6
  
 MidPoint : integer := (LowIndex + HighIndex)
  div
  2;
  
 7
  
 begin
  
 8
  
 if
  LowIndex + 1 < HighIndex
  then
  -- worth sorting
  
 9
  
 MergeSort(Tangled, LowIndex, MidPoint);
  
 10
  
 MergeSort(Tangled, MidPoint+1, HighIndex);
  
 11
  
 Merge(Tangled, 1, MidPoint, MidPoint+1,
  
 12
  
 HighIndex);
  
 13
  
 end
 ; -- worth sorting
  
 14
  
 end
 ; -- MergeSort
  
 15
  
 MergeSort
  is declared in line 3 as a
  thread
 , not a
  procedure
 . All invocations of
  
 MergeSort
 , including the recursive ones on lines 10 and 11, create new threads 
 running instances of
  MergeSort
  that work independently of the main program. 
 Unfortunately,
  MergeSort
  fails to wait for its children to finish and rushes ahead to 
 line 12, merging the two halves of the array before they are properly sorted. You 
 will soon see mechanisms for synchronization that will let me fix this bug.
  
 Each thread gets its own stack. Variables declared locally to a thread are like 
 local variables in a procedure; each thread gets its own local variables. Likewise, any 
 procedures called from a thread (such as
  Merge
 , called in line 12) get new activation 
 records on the thread’s stack. However, variables that are outside the scope of the 
 thread are shared among all threads in which they are visible by normal scope rules. 
 That is, the static chain in a thread’s stack eventually points outside of the private 
 stack of the thread into shared
  
 stack.
  
 (Sometimes this arrangement is called a
  cactus stack
 , since the
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,N,NA
 COOPERATION BY MEANS OF SHARED ,NA,NA
VARIABLES,"The
  MergeSort
  example shows that threads sometimes need to wait for each other. 
 We say that a waiting thread is
  blocked
 . Generally, there are two reasons why 
 threads need to block. First, they may be using variables that are shared with other 
 threads, and they need to take turns. Taking turns is often called
  mutual exclusion
 , 
 because while one thread is executing in-structions that deal with the shared 
 variables, all other threads must be ex-cluded from such instructions. 
  
 Second, 
 they may need to wait for some operation to complete in some other thread before 
 they may reasonably do their own work. We can explain the
  MergeSort
  example by 
 either reason. First, the variables in the
  Tangled
  array are shared between parents 
 and children. Second, it makes no sense to merge the two halves of
  Tangled
  until they 
 have been sorted.",NA
2.1 Join  1,"The simplest form of synchronization is to block until another thread com-pletes. 
  
 Such blocking is achieved by the
  join
  statement, which specifies which 
 thread is to be awaited. The thread that invokes
  join
  is blocked until that thread has 
 completed. Some languages, such as Modula-3, make
  join 
 an expression that 
 evaluates to the value returned by the thread at the time it terminates.
  Cobegin
  
 implicitly invokes
  join
  at the end of the compound statement for each thread started 
 by that statement.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  In Modula-3, the parameter is an object of a particular class that provides a method called
  apply
 .
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 Semaphores ,"The heart of most synchronization methods is the
  semaphore
 . Its imple-mentation 
 (often hidden from the programmer) is shown in Figure 7.2.
  
 Figure 7.2
  
 type
  
 Semaphore =
  
 1
  
 2
  
 record
  -- fields initialized as shown
  
 3
  
 Value : integer := 1;
  
 4
  
 Waiters :
  queue of
  thread := empty;
  
 5
  
 end
 ;
  
 6
  
 Semaphores have two operations, which are invoked by statements. (The op-
 erations can be presented as procedure calls instead.) I call the first
  down 
 (sometimes people call it
  P
 ,
  wait
 , or
  acquire
 ). The second operation is
  up 
 (also called
  
 V
 ,
  signal
 , or
  release
 ):
  
 •
  
 down
  S
  decrements
  S.Value
  (line 4). It then blocks the caller, saving its 
  
 identity in
  S.Waiters
 , if
  Value
  is now negative.
  
 •
  
 up
  S
  increments
  S.Value
 . 
  
 It unblocks the first waiting thread in 
  
 S.Waiters
  if
  Value
  is now nonpositive.
  
 Both these operations are
  indivisible
 , that is, they complete in a thread in-
 stantaneously so far as other threads are concerned. 
  
 Therefore, only one 
 thread at a time can either
  up
  or
  down
  a particular semaphore at a time.
  
 Semaphores can be used to implement mutual exclusion. All regions that use the 
 same shared variables are associated with a particular semaphore, initialized with
  
 Value = 1
 . A thread that wishes to enter a region
  down
 s the associated semaphore. It 
 has now achieved mutual exclusion by acquiring an exclusive lock. When the thread 
 exits the region, it
  up
 s the same semaphore, releasing the lock. The first thread to try 
 to enter its region succeeds. An-other thread that tries to enter while the first is still 
 in its region will be blocked. When the first thread leaves the region, the second 
 thread is un-blocked.
  Value
  is always either 0 or 1 (if only two threads are 
 competing). For this reason, semaphores used for mutual exclusion are often called
  
 bi-nary semaphores
 .
  
 Besides mutual exclusion, semaphores can also help achieve more complex 
 synchronization. If thread
  T
  needs to wait until thread
  S
  accomplishes some goal, 
 they can share a semaphore initialized with
  Value = 0
 . When
  S
  accom-plishes its goal, 
 it
  up
 s the semaphore. When
  T
  reaches the place where it must wait, it
  down
 s the 
 same semaphore. No matter which one reaches the semaphore call first,
  T
  will not 
 proceed until
  S
  has accomplished its goal.",NA
2.3 Mutexes ,"Some languages, such as Modula-3, predeclare a
  mutex
  type that is imple-mented by 
 binary semaphores. The
  lock
  statement surrounds any state-ments that must 
 exclude other threads, as shown in Figure 7.3.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.4 Conditional Critical Regions ,"The Edison language has a way to program synchronization that is more ex-pressive 
  
 than 
  
 mutexes 
  
 but 
  
 less 
  
 error-prone than 
  
 bare 
  
 semaphores 
 [Brinch Hansen 80]. As I mentioned before, synchronization in general is the desire 
 to block an action until a particular condition becomes true.
  
 A standard example that displays the need for synchronization is the 
 bounded 
 buffer
 , which is an array that is filled by producer threads and emptied by 
 consumer threads. All producers and consumers must mutually exclude each other 
 while they are inspecting and modifying the variables that make up the bounded 
 buffer. In addition, when the buffer is full, producers should block instead of
  busy 
 waiting
 , which is repeatedly testing to see if the buffer has room. Likewise, when 
 the buffer is empty, consumers should block. Figure 7.4 shows how to code this 
 application with conditional critical regions.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.5 Monitors ,"One objection to using conditional critical regions is the cost of checking con-ditions, 
 which must occur whenever a thread leaves a region. A second objec-tion is that code 
 that modifies shared data may be scattered throughout a program. The
  monitor
  
 construct, found in Modula and Mesa, was invented to
  
 address both issues [Hoare 74; Lampson 80].
  
 It acts both as a data-
  
 abstraction device (providing modularity) and a synchronization device.
  
 Monitors introduce a new name scope that contains shared data and the 
 procedures that are allowed to access the data. Procedures exported from the 
 monitor are mutually exclusive; that is, only one thread may execute an ex-ported 
 procedure from a particular monitor at a time.
  
 The most straightforward use of monitors is to package all routines that use a set 
 of shared data (represented by a collection of variables) into a single
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.6 Crowd Monitors ,"Crowd monitors are a nice extension to monitors that address this last prob-lem 
 [Horn 77]. Crowd monitors distinguish exclusive procedures from ordi-nary 
 procedures within the monitor. Only exclusive procedures are mutually exclusive. 
 Ordinary procedures may be invoked only by activities that have permission to do 
 so; this permission is dynamically granted and revoked by exclusive procedures. A 
 skeleton of the crowd-monitor solution to the read-ers-writers problem appears in 
 Figure 7.10.
  
 Figure 7.10
  
 crowd monitor
  ReadWrite;
  
 1
  
 export
  StartRead, EndRead, Read, StartWrite,
  
 2
  
 EndWrite, Write;
  
 3
  
 variable
  
 4
  
 Readers :
  crowd
  Read;
  
 5
  
 Writers :
  crowd
  Read, Write;
  
 6
  
 exclusive procedure
  StartRead();
  
 7
  
 ... -- block the caller until reading is safe
  
 8
  
 enter
  Readers;
  
 9
  
 ...
  
 10
  
 exclusive procedure
  EndRead();
  
 11
  
 ...
  
 12
  
 leave
  Readers;
  
 13
  
 ... -- bookkeeping, maybe signal a waiting writer
  
 14
  
 exclusive procedure
  StartWrite();
  
 15
  
 ... -- block the caller until writing is safe
  
 16
  
 enter
  Writers;
  
 17
  
 ...
  
 18
  
 exclusive procedure
  EndWrite();
  
 19
  
 ...
  
 20
  
 leave
  Writers;
  
 21
  
 ... -- bookkeeping, maybe signal waiter
  
 22
  
 procedure
  Read(...);
  
 23
  
 ... -- actually read from the shared data
  
 24
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.7 Event Counts and Sequencers ,"Mutual exclusion is not always desirable because it limits concurrency. It is also 
 unnecessary in some cases on physically distributed computers. In fact, if one hasn’t 
 yet implemented mutual exclusion, the method discussed here can be used to build 
 semaphores to provide mutual exclusion, too [Reed 79]. Those semaphores will 
 even allow simultaneous
  down
  operations on several semaphores.
  
 The first type needed is the
  event count
 . An event count is implemented as a 
 nondecreasing integer variable. It keeps a count of the number of events of interest 
 to the program, such as the number of times a variable has been modified. Event 
 counts have three operations:
  
 1. 
  
 2.
  
 3.
  
 advance
  E
  is used to signal the occurrence of events associated with event 
 count
  E
 . It has the effect of incrementing
  E
  indivisibly.
  
 read
  E
  is an expression that evaluates to the value of the event count
  E
 . If
  read
  
 returns some number
  n
 , then at least
  n
  advance
  operations must have 
 happened. By the time this number is evaluated, the event count may have 
 been advanced again a number of times.
  
 await
  E
  reaches
  v
  waits for the event count
  E
  to have the value
  v
 . It blocks the 
 calling thread until at least
  v
  advance
  operations have oc-curred. It is 
 acceptable if more than
  v
  advance
  operations have occurred when the thread 
 is finally unblocked. This overshoot could result from very frequent
  advance
  
 operations.
  
 These definitions allow both
  await
  and
  read
  to be concurrent with
  advance
 , since the 
 programmer won’t care if
  read
  gives a somewhat stale value or if 
 await
  waits a trifle 
 too long.
  
 Figure 7.11 shows how to encode the bounded buffer using event counts. For the 
 time being, I assume that there is only one producer and one con-sumer.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.8 Barriers ,"Some computations occur in phases, and threads that finish one phase must wait 
 until all have finished until any may proceed to the next phase. The 
 barrier
  type 
 provides the necessary synchronization. It has one operation:
  
 •
  
 meet
  B
  causes the thread to block on barrier
  B
  until all threads have exe-cuted a
  
 meet
  statement on
  B
 .
  
 An example of barrier synchronization is a bottom-up version of
  MergeSort
 , shown in 
 Figure 7.13.
  
 Figure 7.13
  
 constant
  UpperBound = ... -- size of array
  
 1
  
 type
  DataArray =
  array
  0..UpperBound
  of
  integer;
  
 2
  
 variable
  
 3
  
 Tangled : DataArray;
  
 4
  
 MergeBarrier :
  barrier
  UpperBound
  div
  2;
  
 5
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.9 Performance Issues ,"Concurrent programs may fail not only because they contain programming errors 
 that lead to incorrect results, but also because they make no progress due to 
 blocking. They may also run more slowly than necessary because of poor 
 programming.
  
 I have already mentioned that
  signal
  usually occurs as the last operation in a 
 monitor’s exported procedure. In Modula-3, where the exported proce-dure must 
 explicitly acquire a mutex, it is advisable to release the mutex be-
  
 fore
  signal
 ing.
  
 Otherwise, the awakened thread will try to acquire the
  
 mutex and immediately block again. The same problem occurs with
  broad-cast
 , but 
 now many threads will try to acquire the mutex, and only one will succeed. It may be 
 preferable (although clumsier) to use
  signal
  and to have each awakened thread
  
 signal
  the next one.
  
 Starvation
  is a form of unfairness in which a thread fails to make progress, even 
 though other threads are executing, because of scheduling de-cisions. Although 
 starvation can be the fault of the thread scheduler, it is more often a programming 
 error. For example, a poorly programmed solution to the readers-writers problem 
 will block writers so long as there are any readers. New readers can come and go, 
 but so long as there are any readers, all writers starve. The solution to starvation is 
 to prevent new threads from acquiring mutexes until old threads have completed. In 
 the readers-writers case, new readers can be kept out if any writers are waiting.
  
 Deadlock
  occurs when a group of threads is blocked waiting for resources (such 
 as mutexes) held by other members of the group. For example, the code of Figure 
 7.14 will deadlock.
  
 variable
  
 1
  
 Mutex1, Mutex2 : mutex;
  
 2
  
 BarrierA : barrier;
  
 3
  
 procedure
  ThreadA();
  
 4
  
 begin
  
 5
  
 lock
  Mutex1
  do
  
 6
  
 lock
  Mutex2
  do
  
 7
  
 -- anything
  
 8
  
 end
 ;
  
 9
  
 end
 ;
  
 10
  
 end
 ; -- ThreadA
  
 11
  
 procedure
  ThreadB();
  
 12
  
 begin
  
 13
  
 lock
  Mutex2
  do
  
 14
  
 lock
  Mutex1
  do
  
 15
  
 -- anything
  
 16
  
 end
 ;
  
 17
  
 end
 ;
  
 18
  
 end
 ; -- ThreadB
  
 19
  
 ThreadA
  might reach line 7 just as
  ThreadB
  reaches line 15. Each will then try to lock 
 a mutex held by the other. Neither can make any progress.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,N,NA
 TRANSACTIONS: ARGUS,"The concept of acquiring exclusion over data structures is often extended to deal 
 gracefully with failure. This behavior is especially important for pro-grams that 
 modify large shared databases. A
  transaction
  is a set of opera-tions undertaken by 
 a thread. Transactions have two important properties. First, these operations are 
 indivisible when taken as a whole. From the point of view of other threads, either 
 they have not started or they have all fin-ished. Second, the transaction is
  
 recoverable
 ; that is, it can either
  commit
 , in which case all modifications to shared 
 data take effect, or it can
  abort
 , in which case none of its modifications takes effect. 
 Because transactions are indivisible, threads cannot see modifications performed by 
 other transactions that are still in progress.
  
 For example, in an airline reservation database, a customer may wish to 
 exchange a seat on a given flight for a seat on another flight. The program might give 
 up the first seat and then reserve the second. If the second plane is full, it is 
 necessary to get back the initial seat, which may already have been allocated to 
 another passenger. If both actions (releasing the first seat and reserving the second) 
 are part of a transaction, then the program can just abort when it fails to reserve the 
 second seat. The first seat will still be re-served by the original customer.
  
 Transactions can be nested. In order to increase concurrency, programs might 
 want to start several threads as children of an initial thread. Each can enter its own 
 subtransaction. If any child thread fails, its own data modifica-tions are recovered, 
 but the parent transaction can still proceed. Unrelated transactions do not see any 
 data modifications until and unless the top-level transaction commits.
  
 Argus provides programming-language support for nested transactions [Liskov 
 83a]. The statements comprising a transaction are the body of a 
 transaction
  
 statement. Of course, a procedure may be called from inside a transaction, and the 
 procedure may be recursive, so the lexical nature of transaction entry does not limit 
 the number of transactions. If the transac-tion statements finish execution, the 
 transaction commits. The statement 
 abort
  causes the current transaction to fail.
  
 Data that are shared among threads must be built out of recoverable types. 
 Argus provides recoverable versions of primitive types, such as inte-gers and arrays. 
 Read and write locks are implicitly acquired when recover-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,"CHAPTER 7
  
 CONCURRENT PROGRAMMING
  
 N",NA
 COOPERATION BY PROCEDURE CALL,"So far, I have described ways in which threads that cooperate through shared 
 variables can synchronize access to those variables. A different sort of coop-eration 
 is achieved by procedure calls. When one thread (the
  client
 ) calls an-other (the
  
 server
 ), information can be passed in both directions through parameters. 
 Generally, parameters are restricted to value and result modes. A single thread can 
 act as a client with respect to some calls and a server with respect to others.",NA
4.1 Rendezvous ,"In Ada, SR, and Concurrent C, procedure calls between threads are handled by a 
 mechanism called a
  rendezvous
 , which is an explicit way for the server to accept 
 procedure calls from another thread. A thread executes within a module. This 
 module exports
  entries
 , which are the procedurelike identi-fiers that may be 
 invoked by other threads. The declaration of an entry in-cludes a declaration of its 
 formal parameters.
  
 A server accepts a call from a client by an
  accept
  statement, which names the 
 entry and the formal parameters. The
  accept
  statement blocks until some client 
 invokes this procedure. At that time, the actuals provided by the client are bound to 
 the formals, and the server executes the body of the
  ac-cept
 . The
  accept
  statement 
 may be nested in a
  select
  statement, which may enable several rendezvous, based on 
 values of current variables and even on the values of the actual parameters 
 presented.
  
 A client invokes a rendezvous by a syntax that looks like procedure call. The 
 client blocks until the server executes a matching
  accept
  statement and either 
 completes the body of that
  accept
  or explicitly releases the client. Fig-ure 7.16 shows 
 a bounded buffer (in Ada syntax).
  
 Figure 7.16
  
 task
  BoundedBuffer
  is
  
 1
  
 entry
  GetBuffer(Answer :
  out
  Datum);
  
 2
  
 entry
  PutBuffer(What :
  in
  Datum);
  
 3
  
 end
 ;
  
 4
  
 task body
  BoundedBuffer
  is
  
 5
  
 Size :=
  constant
  10; -- capacity of the buffer
  
 6
  
 type
  Datum
  is
  ... -- contents of the buffer
  
 7
  
 Buffer :
  array
  (0..Size-1)
  of
  Datum;
  
 8
  
 InCount, OutCount : integer := 0;
  
 9
  
 entry
  GetBuffer(Answer :
  out
  Datum);
  
 10
  
 entry
  PutBuffer(What :
  in
  Datum);
  
 11
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.2 Remote Procedure Call (RPC) ,"If threads do not share variables (for example, if they are running on differ-ent 
 machines connected by a network), the only way they can cooperate is by procedure 
 call or messages. Rendezvous is one way of accepting procedure calls. The only calls 
 that are handled are those that match open
  accept 
 statements.
  Remote procedure 
 call
  (RPC) means an invocation that is handled not by
  accept
  statements, but by an 
 ordinary exported procedure. Such calls can cross compilation units, processes, 
 computers, and even pro-grams that are written at different times in different 
 languages.
  
 The model of computation for remote procedure calls is somewhat differ-ent 
 from what I have been discussing so far. Each address space may have multiple 
 threads, which may share variables in that address space, subject to any scope rules 
 the language imposes. Threads in separate address spaces do not share variables.
  
 A thread may invoke an exported procedure inside another address space by a 
 remote procedure call. There are two equivalent ways to picture the ef-fect of such a 
 call. You can imagine the thread migrating temporarily to the address space of the 
 server, performing the call there, and then returning to
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4.3 Remote Evaluation (REV)  2,"Remote procedure call only works if the server exports the procedure that the client 
 needs. But clients are often written long after the server, and they may have needs 
 that were not foreseen in the server. Some clients may need the server to run 
 specialized procedures that most clients would not be interested in but which could 
 run far more efficiently on the server than on a client, be-cause the client would 
 need to repeatedly invoke server routines remotely.
  
  
 Remote evaluation (REV) is a technique that allows clients to send not only 
 parameters, but also procedures, to the server [Stamos 90]. The proce-dures may 
 refer to other procedures exported by the server. For example, a mail-delivery 
 server might export a procedure
  DeliverMail
 . A client that wants to send a hundred 
 identical messages could use RPC, invoking
  Deliv-erMail
  a hundred times, each time 
 passing the message. Alternatively, it 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  
  2550 Garcia Avenue, Mountain View, California, 94043
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,N,NA
 COOPERATION BY MESSAGES,"Although a procedure-call syntax makes concurrent programming look super-
 ficially like sequential programming, not all cooperation is easily shoehorned into 
 the procedure-call model. First, a single query might generate multiple results 
 spread over time. If the query is represented as a procedure call, then the results 
 must either be result parameters, which means the client is blocked until the last 
 result is ready, or the results are independent calls in the other direction, which 
 confuses the issue of which thread is client and which is server. Second, some 
 cooperation is unidirectional; there is no need to block the client until the server 
 receives, acts on, and responds to a call. Third, some computations are best viewed 
 as interactions among peers, where no simple client-server hierarchy applies. 
 Fourth, some computations require multicast of the same data to groups of address 
 spaces. It is wasteful to program multicast as multiple procedure calls. Fifth, it might 
 be necessary to reply to requests in a different order from the order in which they 
 arrive.
  
 For these reasons, some experimental languages provide more primitive 
 message-passing notions instead of or in addition to RPC. Often, message passing is 
 provided as a library package to be used within some other lan-guage such as C. 
 Operating-system support is needed to make the individual operations efficient. The 
 following table indicates some of the facilities that can be provided as simple 
 language extensions or in library packages.
  
 Operation
  
 Parameters
  
 Results
  
 connect
  
 partner 
  
 set of partners 
  
 connection, data 
 connection 
  
 data 
  
 connection
  
 connection 
 connection
  
 group
  
 send
  
 data
  
 receive
  
 reply
  
 forward
  
 This list is neither complete (library packages often provide many more rou-tines) 
 nor required (many library packages have no
  group
  or
  forward
  opera-tions, for 
 example). Still, it provides a reasonable set of functions for message passing.
  
 The
  connect
  operation builds a connection, that is, a channel across which 
 communication takes place; thus, individual
  send
  operations need not specify which 
 process is to receive the message. Such a specification is given only once. It might be 
 as simple as a process identifier or as complex as giving a process name or other 
 characteristics to be looked up in a database. The 
 group
  operation builds a 
 connection that leads to multiple recipients. This fa-cility is helpful for multicast.
  
 The
  send
  operation might be designed to block the sender until the mes-sage can 
 be copied to a safe place, until the message is sent, until the destina-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.1 CSP,"CSP
  
 (Communicating
  
 Sequential
  
 Processes)
  
 is
  
 a
  
 proposal
  
 made
  
 by
  
 C. A. R. Hoare for message passing between threads that do not share vari-
  
 ables [Hoare 78].
  
 It is the framework upon which Occam was developed
  
 [May 83]. Communication is accomplished by
  send
  and
  receive
  statements. Although 
 the
  send
  statement looks like a procedure invocation, in fact it is a pattern 
 specification, much like Prolog (discussed in Chapter 8). The pattern is built out of 
 an identifier and actual parameters. It is matched against a pattern in a
  receive
  
 statement in the destination thread. Variables in the 
 receive
  pattern are like formal 
 parameters; they acquire the values of the ac-tual parameters in the matching
  send
  
 pattern. Patterns match if the pattern name and the number of parameters are the 
 same and all formal parameter patterns match the actual parameter patterns. 
 Matching is even used for the assignment statements, as in the examples shown in 
 Figure 7.20.
  
 left := 3;
  
 1
  
 right := 4;
  
 2
  
 x := cons(left, right); -- assigns pattern ""cons(3,4)""
  
 3
  
 form(right) := form(right+1); -- right := right+1
  
 4
  
 factor(cons(left,right)) := factor(cons(5,6));
  
 5
  
 -- left := 5; right := 6
  
 6
  
 right = imply(); -- pattern with no parameters
  
 7
  
 muckle(left) := mickle(left+1); -- match error
  
 8
  
 Variables can hold pattern values, as in line 3. Here,
  cons
  is not a procedure call, just 
 a pattern constructor. Line 4 shows that matching the actual to the formal is like an 
 ordinary assignment. Line 5 shows that matching works re-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.2 Lynx ,"Lynx is an experimental language implemented at the University of Wiscon-sin and 
 at the University of Rochester [Scott 84, 86]. Address spaces and modules in Lynx 
 reflect the structure of a multicomputer, that is, a dis-tributed-memory machine. 
 Each outermost module represents an address space. As in DP, each address space 
 begins executing a single thread. That thread can create new threads locally and 
 arrange for threads to be created in response to messages from other processes. 
 Threads in the same address space do not execute simultaneously; a thread 
 continues to execute until it blocks, yielding control to some other thread. It is not 
 an error for all threads to be blocked waiting for a message to be sent or received.
  
 Lynx is quite helpful for programming long-running processes (called 
 server 
 processes
 ) that provide assistance to ephemeral processes (called 
 client 
 processes
 ). Typically, server processes are programmed to build a separate thread 
 for each client process to keep track of the ongoing conversa-tion between server 
 and client processes. That thread may subdivide into new threads if appropriate. 
 Lexical scope rules determine what variables are visible to any thread; the runtime 
 organization uses a cactus stack.
  
 Lynx provides two-way communication
  links
  as first-class values. A link 
 represents a two-way channel between address spaces. The program dynami-cally 
 binds links to address spaces and entries. Links can be used for recon-figurable, 
 type-checked connections between very loosely coupled processes that are 
 designed in isolation and compiled and loaded at disparate times.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.3 Linda ,"Like CSP, Linda also uses patterns instead of procedure calls in its messages 
 [Gelernter 85]. Unlike CSP, the
  send
  statement does not indicate the thread to which 
 data are to be sent, nor does
  receive
  indicate from which thread the data are coming. 
 Instead,
  send
  places the data in a global data pool that can be accessed by any 
 thread, and
  receive
  takes data from that pool. It is up to the implementation to 
 organize data so that threads running on multiple ma-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.4 SR  3,"The SR language was developed over a period of ten years by Gregory An-drews at 
 the University of Arizona [Andrews 88]. It contains features for both distributed- and 
 shared-memory concurrency.
  
 SR modules are separately compiled. A module specification and its body may be 
 compiled separately. At runtime, modules are dynamically instanti-ated and given 
 initial actual parameters. By default, a new module shares the address space of its 
 creator, but it can be placed on any machine (physical or virtual) instead.
  
  
 Ordinary modules may import declaration modules. 
  
 Each declaration 
 module may import other declaration modules and introduce constants, 
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  
  What I call the pattern name would actually be the first element of a tuple in Linda, but I find CSP 
 nomenclature a bit easier to understand.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.5 Object-Oriented Programming  4,"The object-oriented paradigm (see Chapter 5) lends itself nicely to dis-tributed-
 memory machines, because each object may reside entirely within a single memory, 
 and interaction between that object and the rest of the compu-tation is entirely 
 mediated by messages. There are several object-oriented languages for concurrent 
 programming. For example, DC++ is a version of C++ with threads [Carr 93], 
 Distributed Eiffel [Gunaseelan 92] and Eiffel Linda [Jellinghaus 90] extend the 
 object-oriented Eiffel language, and CST (Concurrent Smalltalk) extends Smalltalk 
 [Dally 89]. You can read a survey of these languages and others in [M. Nelson 91].
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  I am changing the keywords, as usual, for the sake of consistency.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5.6 Data-Parallel Programming ,"Scientific applications often require similar computations across very large data sets, 
 which may represent a connected physical entity. For example, a weather simulator 
 might advance time in small increments while keeping track of wind patterns, cloud 
 cover, precipitation, sunlight-induced wind cur-rents, and so forth over a large 
 geographical area represented as intercon-nected records, each covering a few 
 square miles. Such applications often strain the computational ability of any single 
 computer, so they are pro-grammed on shared-memory or distributed-memory 
 computers. Each com-puter is given a region of the data and computes as 
 independently as possible of the other computers. When necessary, computers 
 exchange information with others that deal with neighboring data. This style of 
 computing is called 
 data-parallel
  computing with
  coarse-grain parallelism
 . That 
 is, the ma-chines work in parallel on different parts of the data, and they only coordi-
 nate their activities on occasion.
  
 Several languages have been implemented specifically to deal with coarse-grain 
 parallelism. Some, like PVM [Sunderam 89], are implemented as li-brary packages to 
 be invoked from any conventional language for passing messages. Charm is a more 
 complex language that extends C with dynami-cally creatable threads that inhabit 
 modules [Kale´ 90]. Global variables can be accessed only by a runtime procedure 
 because they may be stored any-where. The threads communicate both by messages 
 (much like method invo-cations in object-oriented programming) and through 
 serialized modules that accumulate data, creating such results as sums and 
 averages.
  
 The Canopy language is more complex yet. It is implemented as a library package 
 to be used by ordinary C programs. Unlike PVM and Charm, it im-poses a 
 distinctively data-parallel view on the programmer.
  
 Data in Canopy are represented as records stored on a grid of sites. Grids are 
 dynamically constructed by calls to a runtime routine. Definition rou-tines are 
 provided for many standard topologies, such as three-dimensional meshes, and the 
 programmer may define any desired topology by using a more primitive routine. A 
 computation may use several different grids, al-though using more than one is 
 unusual. Each site in a grid has coordinates and is connected to its neighbors by 
 links. Data records are associated with each site and each link.
  
 The runtime support software arranges for sites to be located on physical 
 machines. Typically, there are far more sites than machines; a typical prob-lem may 
 have a million sites running on a hundred machines. The program-mer has no 
 control over the mapping of sites to machines, and there is no way for a program to 
 discover that mapping. Each machine has a complete copy of all code and global data 
 and has space to allocate site-local data.
  
 Computation proceeds in phases.
  
 Each phase is initiated by a distin-
  
 guished site called the controller, which executes the control program. Before the 
 first phase, the controller establishes the grids, site sets, mappings be-tween grids, 
 and record fields (local variables) that will be used. It then calls 
 CompleteDefinitions
  
 to activate these definitions. For each phase, the con-trol program may initialize 
 global variables by broadcasting a copy to all sites (actually, to all machines). 
 Individual sites should treat such global data as readonly. The controller then 
 invokes a procedure on each site in a grid or subset of a grid by calling
  DoTask
 . Each 
 such site gets its own thread to exe-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
6,N,NA
 FINAL COMMENTS,"Concurrent programming has been studied for at least twenty years, but it has been 
 steadily gaining popularity. One reason is that high-performance computers have 
 turned increasingly to parallelism as a way of achieving a high rate of computation. 
 Another is that workstation clusters are increas-ingly common in research 
 environments. The former trend has led to in-creased interest in threads that 
 cooperate by shared variables; the latter makes message passing attractive. 
 Operating systems are being designed that make shared variables meaningful across 
 memories and that make mes-sage passing fast within a single memory, so the 
 correspondence between physical architecture and programming language 
 approach is not straightfor-ward.
  
 Languages that provide some modest extensions to successful sequential 
 languages, such as ML, C++, or even FORTRAN, might be more successful in the long 
 run than specialty languages, because they already have widespread use and are 
 perhaps easier to learn than completely new languages. Concur-rent C, Concurrent 
 Pascal, and HPF (High Performance FORTRAN) extend standard imperative 
 languages; CST (Concurrent Smalltalk), DC++, and Dis-tributed Eiffel extend object-
 oriented languages.
  
 High-level operations can go a long way toward efficient use of the under-lying 
 architecture without introducing concurrency explicitly into the lan-guage. For 
 example, FORTRAN 90 specifies vector and matrix operations that a subroutine 
 library may implement quite efficiently in a concurrent fashion. As another example, 
 speculative evaluation in functional program-ming languages, as discussed in 
 Chapter 4, can take advantage of implicit concurrency.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"7.1 
  
 7.2
  
 7.3
  
 7.4
  
 7.5
  
 What is the usual initial value for the
  Value
  field in a semaphore? Show a code 
 fragment that, if executed by two threads, can leave the value of
  x
  either 0 or 
 14, depending on the order in which the two threads interleave their execution. 
 Don’t use any synchronization. Show how to use each of the following methods 
 to restrict a code frag-ment
  C
  so that it can only be executed by one thread at a 
 time: semaphores, mutexes, conditional critical regions.
  
 Make a deadlock situation with only one thread, using each of the fol-lowing 
 methods: semaphores, mutexes, conditional critical regions. What will be the 
 effect in a CSP program if I misspell a pattern in an in-put guard?",NA
Challenge Exercises,"7.6
  
 7.7
  
 7.8
  
 7.9
  
 On page 189, I say that arguments to
  fork
  are usually restricted to global 
 procedures so that cactus stacks do not need to be built. What is the 
 connection between using global procedures and cactus stacks?
  
 Does Ada require cactus stacks?
  
 What will be the effect of a semaphore whose
  Value
  field is initialized to 
 2
  if it is 
 used for mutual exclusion?
  
 What would be the use of a semaphore whose
  Value
  field is initialized to
 -2
  with 
 two dummy threads initially enqueued on its
  Waiters
  field?
  
 7.10
  Show how to implement conditional critical regions using semaphores.
  
 You will need an indivisible
  updown
  statement that
  up
 s one semaphore and
  
 down
 s another, and
  upall
  which performs
  up
  until there are no more threads 
 blocked on the semaphore.
  
 7.11
  Show how to implement a capacity-2 barrier using two semaphores. You may 
 use different code for the two threads involved. Implement not only
  meet
 , but 
 also
  arrive
  and
  depart
 .
  
 7.12
  Show how to build a multiple-producer, multiple-consumer bounded 
  
 buffer using event counts and sequencers.
  
 7.13
  Figure 7.9 (page 197) shows a Mesa solution to the bounded buffers 
 problem. It assumes that
  signal
  only awakens one waiter. Actually, Mesa 
 provides only
  broadcast
 , not
  signal
 . Fix the code.
  
 7.14
  Show how to build semaphores with event counts and sequencers. The 
  
 up
  
 and
  down
  operations should not require mutual exclusion.
  
 7.15
  I suggest on page 213 representing the type of a procedure as a string in order 
 to implement type-secure RPC. What sort of type equivalence does this 
 method represent?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 8 ,NA,NA
O,NA,NA
Logic Programming,NA,NA
1,"Although LISP has long been the language of choice for artificial intelligence (AI) 
 research, other languages are increasingly common for some branches of AI. C and 
 C++ have become quite popular. Some AI programs are meant to reason about the 
 world, given some initial knowledge. Knowledge can be rep-resented in property 
 lists of LISP atoms, but it can also be stored as a set of rules and facts. One form of 
 reasoning is to try to derive new facts or to prove or disprove conjectures from the 
 current set of facts. Programs that follow this approach are called “inference 
 engines.”
  
  
 In this chapter, I will present several languages intended for knowledge
  
 representation and inference engines.
  
 These logic languages tend to be
  
 declarative
 . Programs state goals and rules to achieve goals, but do not ex-plicitly 
 invoke those rules in order to achieve the goals. In contrast, both im-perative and 
 functional languages tend to be
  procedural
 ; that is, programs are organized around 
 control structures such as iteration and procedure invo-cation.
  
 N",NA
 PROLOG,"Prolog is a declarative programming language designed in 1972 by Philippe Roussel 
 and Alain Colmerauer of the University of Aix-Marseille and Robert Kowalski at the 
 University of Edinburgh. Prolog programs are related to computations in a formal 
 logic. A programmer first provides a
  database
  of facts and rules of inference. 
 Programs are formulated as assertions involving facts in the database. Programs are 
 executed by proving or disproving a par-ticular assertion.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 231",NA
"1.1 Terms, Predicates, and Queries ","Elementary values in Prolog are called
  terms
 . Terms are either constants (numbers 
 like
  43
  and identifiers like
  parsley
 , starting with a lowercase let-ter), variables 
 (identifiers like
  X
 , starting with an uppercase letter), or struc-tures (identifiers 
 starting with a lowercase letter, followed by parameters that are themselves terms, 
 such as
  tasty(parsley)
 ). The identifier that heads a structure (like
  tasty
 ) is called a
  
 functor
 , based on its similarity in appear-ance to a function name. Figure 8.1 shows 
 a sample term.
  
 Figure 8.1 
  
 near(house, X, 22, distance(Y))
  
 There are two constants (
 22
  and
  house
 ), two variables (
 X
  and
  Y
 ), one unary functor 
 (
 distance
 ), one 4-ary functor (
 near
 ), and two structures (
 distance(Y) 
 and the whole 
 term). This term has no inherent meaning; a program could use it to mean that
  house
  
 is within 22 miles of some object
  X
 , and that the ac-tual distance is
  Y
  miles.
  
 Programs are built out of facts, rules, and queries, which are all based on 
 predicates. A
  predicate
  has the same form as a structure: a name in lower-case 
 followed by parameters, which must be terms. Predicates represent a fact (actual or 
 to be proven) relating the values of their parameters. I will of-ten call the predicate 
 name itself a predicate when there is no chance for con-fusion.
  
 Although structures and predicates have parameters and otherwise look like 
 function calls, this appearance is deceiving. Structures are used as pat-terns, and 
 predicates are used to define rules and facts and to pose queries. Only in their role 
 as queries are predicates at all like function calls.
  
 A database is constructed out of facts and rules. To build a simple family-relation 
 database, I will start with constants representing people:
  tom
 ,
  dick
 , 
 harry
 ,
  jane
 ,
  judy
 , 
 and
  mary
 . I describe relationships among these people with binary predicates:
  
 fatherOf
 ,
  motherOf
 ,
  parentOf
 ,
  grandparentOf
 , and 
 siblingOf
 .
  
 One of the hardest problems in reading Prolog programs is figuring out what 
 predicates are supposed to mean. The predicate
  motherOf(mary,judy) 
 could be taken 
 to mean that Judy is the mother of Mary or that Mary is the mother of Judy; the 
 proper interpretation is up to the programmer. I follow the convention that the first 
 parameters represent the traditional inputs, and the last parameters represent 
 outputs, although Prolog does not make this distinction. I therefore understand
  
 motherOf(mary,judy)
  to mean that the mother of Mary is Judy. The predicate
  
 motherOf(mary,judy)
  may be true, but
  motherOf(mary,tom)
  is very likely to be false.
  
 My Prolog program begins by stating facts that define fundamental rela-tions 
 among the terms.
  Facts
  are predicates that are assumed true, such as those in 
 Figure 8.2.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.2 Separating Logic and Control ,"A well-known textbook on data structures by Niklaus Wirth is titled 
 Algorithms + 
 Data Structures = Programs
  [Wirth 76]. This equation defines the programming 
 model implicit in modern procedural languages. The pro-gramming task is 
 essentially to design data structures and the algorithms that manipulate them.
  
 An interesting alternative view is presented by Robert Kowalski in ‘‘Algo-rithms 
 = Logic + Control’’ [Kowalski 79]. This article proposes a declarative programming 
 view. The programmer first specifies the logic of an algorithm. This component 
 specifies what the result of the algorithm is to be. Then the control component is 
 defined; it specifies how an evaluator may proceed to ac-tually produce an answer. 
 The logic component is essential because it defines what the programmer wants the 
 program to generate. The control compo-nent may be optional, since it controls how 
 fast an answer may be obtained. This represents a two-tiered approach in which you 
 think first about getting the correct answer, and then about making the computation 
 sufficiently fast.
  
 Prolog supports this view of programming; its facts and rules are essen-tially the 
 logic component of a program. The control component is largely hidden in Prolog’s 
 evaluator, although certain Prolog commands have been devised to aid an evaluator, 
 as you will see shortly.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.3 Axiomatic Data Types ,"One advantage of languages that support abstract data types is that the spec-
 ification of an abstract data type can be separated from its implementation details. 
 Prolog carries this idea further still: you can specify an abstract data type by axioms 
 without any implementation at all. The axioms define the properties of the abstract 
 data type, which is all the programmer really cares about. Of course, the evaluator 
 must find some way to actually realize the op-erations of an abstract data type, but 
 since Prolog is declarative, this isn’t the programmer’s concern!
  
 I will show you how to define lists, a fundamental data structure of LISP, as seen 
 in Chapter 5, based on terms, predicates, facts, and rules. I define the set of valid lists 
 by indicating when
  cons
  generates a list, as in Figure 8.17.
  
 Figure 8.17
  
 Figure 8.18
  
 isList(nil) .
  
 1
  
 isList(cons(_, T)) :- isList(T) .
  
 2
  
 Line 1 indicates that
  nil
  is a valid list, and line 2 shows that
  cons
  is a binary functor 
 that builds new lists if the second parameter is a list. Because the body of this rule 
 does not refer to the first parameter, I use the don’t-care
  
 variable
  
 _
  . If
  
 _
  is used more than once in a rule or fact, each occurrence
  
 represents a different don’t-care variable. If you want the same value to be forced in 
 two different positions, you must use an explicit variable name. Line 2 shows that 
 predicates can take parameters that are arbitrary terms, including structures. 
 Although you may be tempted to treat
  cons(_,T)
  as a procedure call, it is only a 
 structure. When it appears nested in the head of a rule, it is treated as a pattern to 
 be matched to a query.
  
  
 Predicates specifying the
  car
  and
  cdr
  functors are easy, as shown in Fig-ure 8.18.
  
 /* car(X,Y) means the car of X is Y */
  
 1
  
 car(cons(H,T),H) :- isList(T) .
  
 2
  
 /* cdr(X,Y) means the cdr of X is Y */
  
 3
  
 cdr(cons(H,T),T) :- isList(T) .
  
 4
  
 Once again, the functor
  cons
  is used in the head of rules as a pattern. Using these 
 definitions, I can pose queries like those in Figure 8.19.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.4 List Processing ,"Because lists are such a familiar and flexible data structure, Prolog provides
  
 a notation for the structures that represent lists.
  
 Predefining lists also
  
 makes their manipulation more efficient.
  
 Predefined arithmetic operators
  
 are provided for much the same reason. In Prolog, lists are delimited by brackets. 
 The empty list is
  []
 , and
  [[a,b],[c,d,e]]
  is a list containing two sublists (with 2 and 3 
 elements respectively). The notation
  [H | T]
  is used to represent any list with car
  H
  
 and cdr
  T
 , as in Figure 8.22.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.5 Difference Lists ,"List processing can be expensive. The
  append
  operation must step to the end of the 
 first parameter in a recursive fashion before it can begin to construct the result. 
 Prolog programmers have invented a programming trick called a 
 difference list
  to 
 alleviate this problem [Sterling 94]. Each list is repre-sented in two pieces, which I 
 will call
  listextra
  and
  extra
 . The actual list is 
 listextra
  with
  extra
  removed from the end. 
 What extra information to place at the end of a list is arbitrary and is based on 
 convenience. For exam-ple, the list
  [a,b]
  can be represented in many ways, including
  
 [a,b] []
  and 
 [a,b,c,d] [c,d]
 . In general, I can represent the list as
  [a,b
 |
  Extra] [Ex-tra]
  and 
 not specify what
  Extra
  might be. The
  append
  routine can now be written as a single, 
 nonrecursive rule, as in Figure 8.28.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.6 Arithmetic ,"In Prolog, the infix equality predicate
  =
  can be used for two purposes. In
  
  =
 
 , if
  
  
 and
  
  are both constants, literals, structures, or bound variables, then the predicate 
 succeeds or fails depending on whether or not the two operands are identical. Thus
  
 1 = 1
  is true,
  [1,2] = []
  is false, and
  X = 2
  is true if
  X
  has been bound to 2. This is the 
 natural interpretation of the equality operator.
  
 However, if
  
  or
  
  (or both) are unbound variables, then
  
  =
  
  succeeds and 
 binds them together. So the same symbol acts both as an equality operator and as an 
 assignment operator. Actually, the symbol introduces a constraint. Figure 8.30 
 illustrates this point.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.7 Termination Issues ,"If free variables in queries may be bound only to a finite set of values, a Pro-log 
 evaluator should be able to prove or disprove any query. However, Prolog allows 
 recursive definitions (such as the ones I showed earlier for lists and stacks) that 
 imply infinite domains, as well as primitive objects (such as inte-gers) with infinite 
 domains. Not all queries will necessarily terminate. Pro-log specifies that the order 
 in which rules and facts are specified determines the order in which an evaluator 
 attempts to apply them in proofs. Bad orders can sometimes lead to an infinite 
 recursion. Suppose that I define the
  isList 
 predicate as in Figure 8.32.
  
 Figure 8.32
  
 isList(cons(H,T)) :- isList(T) .
  
 1
  
 isList(nil) .
  
 2
  
 A query like
  isList(nil)
  works fine, but
  isList(X)
  runs into a real snag. The top-down 
 evaluator will set
  X = cons(H,T)
  and try to prove
  isList(T)
 . To do this, it will set
  T = 
 cons(H
 
 ,T
 
 )
 1
 and try to prove
  isList(T
 
 )
 , and so forth. Eventually, the evaluator runs 
 out of stack space. Putting the fact
  is-List(nil)
  before the rule solves the problem.
  
 Other problems may arise because of an inadequate set of rules. For ex-ample, I 
 might provide a definition for odd integers and ask if any odd integer is equal to
  2
 , as 
 in Figure 8.33.
  
 Figure 8.33
  
 in:
  
 odd(1) .
  
 1
  
 odd(N) :- odd(M), N
  is
  M + 2 .
  
 2
  
 ?- odd(2) .
  
 3
  
 out: [does not terminate]
  
 4
  
 The evaluator never finishes the query in line 3. It keeps generating odd numbers (
 1
 ,
  
 3
 , ...) to match
  M
  in line 2, but none of them satisfies
  2
  is
  M + 2
 . It doesn’t know that 
 after considering
  1
 , all succeeding odd numbers will be greater than
  2
  and hence not 
 equal to
  2
 . The query is false, but Prolog has no mechanism to prove it!",NA
1.8 Resolution Proof Techniques  1,"Prolog is designed to be amenable to a particular class of automatic proof 
 techniques termed “resolution techniques.”
  Resolution
  is a general infer-ence rule 
 shown in Figure 8.34.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  All variable names in a rule are local to that rule; hence, recursive applications cause no naming 
 conflicts.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.9 Control Aspects ,"So far I have emphasized the logic component of Prolog. The language also contains 
 features that exercise control over the evaluation process. Such fea-tures in general 
 compromise the otherwise declarative nature of Prolog. (Pro-log also contains I/O, 
 testing, and debugging features that are not purely declarative.)
  
 The most frequently used control operator is
  cut
  (represented by
  
 !
  
 in
  
 Prolog syntax).
  Cut
  terminates backtracking within a rule. (
 Cut
  is similar to 
 fence
  in 
 SNOBOL, described in Chapter 9. SNOBOL patterns use a back-track mechanism that 
 is very similar to the one Prolog uses.) In particular, if 
 cut
  is encountered, all 
 alternatives prior to
  cut
  in the rule are frozen, as shown in the box model in Figure 
 8.36.
  
 Cut
  
 Start
  
 Predicate 1
  
 Cut
  
 Predicate 2
  
 Succeed
  
 Fail
  
 Retry
  
 Prolog will not try alternative matches to earlier conjuncts of the rule’s body. In fact, 
 it will not try alternative rules to the rule that contains
  cut
 . Consider Figure 8.37.
  
 Figure 8.37
  
 even(X) :- X=2 , X>0, !, X < 0 .
  
 1
  
 even(X) :- X=10 .
  
 2
  
 in:
  
 even(E) .
  
 3
  
 out: No
  
 4
  
 The query in line 3 matches the rule in line 1. The first conjunct binds
  X
  to
  2
 , the 
 second succeeds, but the last conjunct fails. The
  cut
  prevents not only a reevaluation 
 of the first conjuncts (which wouldn’t find anything new in any case) but also any 
 attempt to use the rule of line 2, which would succeed. Without the
  cut
  operation, 
 Prolog would report that
  X = 10
 .
  
 Cut
  can be useful in cases where once one rule is found to match, it is un-
 necessary to try other rules. For example, recall the list membership predi-cate,
  
 memberOf
 , shown in Figure 8.38.
  
 Figure 8.38
  
 memberOf(X,[X| _]) .
  
 1
  
 memberOf(X,[_| Y]) :- memberOf(X,Y) .
  
 2
  
 Since an element may appear in a list more than once, if a goal containing a 
 successful
  memberOf
  conjunct later fails, Prolog might try to resatisfy it by looking 
 for an alternative match to
  memberOf
 . In general this search will be futile, since once 
 an item is known to be in a list, backtracking can’t establish anything new. Thus, we 
 have Figure 8.39.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.10 An Example of Control Programming ,"To illustrate some of the subtleties of programming in Prolog, I will consider the 
 canonical (mis-)example of recursive programming, factorial. The obvious definition 
 appears in Figure 8.43.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.11 Negation ,"If a query cannot be satisfied by any binding of its free variables, Prolog con-siders it 
 false. Prolog has a built-in higher-order predicate
  not
  that tests for unsatisfiability. 
 (It is higher-order in that it takes a predicate, not a term, as its parameter.) Consider 
 Figure 8.45.
  
 Figure 8.45
  
 motherOf(nora, fatima) .
  
 1
  
 in:
  
 ?- not(motherOf(nora, jaleh)) .
  
 2
  
 out: Yes
  
 3
  
 in:
  
 ?- not(motherOf(nora, fatima)) .
  
 4
  
 out: No
  
 5
  
 Line 1 introduces a new fact. Line 2 tests to see if a particular fact is un-known. 
 Because it is, the response is
  Yes
 . Line 4 tests to see if a known fact is unknown; it 
 elicits
  No
 .
  
 Under the closed-world assumption that facts that cannot be proved are false, 
 the facts and rules known to the program constitute the entire world; no new facts 
 or rules from “outside” will be added that might render a previ-ously unprovable 
 conclusion true. The closed-world assumption is an exam-ple of
  nonmonotonic 
 reasoning
 , which is a property of a logic in which adding information (in the form 
 of facts and rules) can reduce the number of conclusions that can be proved.
  
  
 It is only safe to use
  not
  with all parameters bound. Otherwise, unex-pected 
 results may occur, as in Figure 8.46.
  
 Figure 8.46
  
 motherOf(nora, fatima) .
  
 1
  
 in:
  
 ?- not(motherOf(X,jaleh)) .
  
 2
  
 out: X=_1
  
 3
  
 in:
  
 ?- not(motherOf(_,jaleh)) .
  
 4
  
 out: Yes
  
 5
  
 in:
  
 ?- not(motherOf(X, fatima)) .
  
 6
  
 out: No
  
 7
  
 in:
  
 ?- not(motherOf(nora, Y)) .
  
 8
  
 out: No
  
 9
  
 in:
  
 ?- not(motherOf(X, fatima)), X=jaleh .
  
 10
  
 out: No
  
 11
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.12 Other Evaluation Orders ,"Prolog programmers must sort rules to avoid infinite loops. They must also sort the 
 conjuncts within rules for the sake of efficiency. The
  naiveSort
  ex-ample in Figure 
 8.26 (page 243) would fail to terminate if line 1 were written as in Figure 8.50.
  
 Figure 8.50 
  
 naiveSort(L1,L2) :- inOrder(L2) , permutation(L1,L2) .
  
 The Prolog interpreter builds more and more fanciful values of
  L2
  that have nothing 
 at all to do with
  L1
  and fails on each one. Prolog programmers learn to build rules so 
 that the first conjunct generates potential solutions, and the remaining conjuncts 
 test them for acceptability. If the generator builds too many unacceptable results, 
 the rule will be very inefficient.
  
 The fact that rule and conjunct order is so crucial to efficiency detracts from the 
 declarative nature of Prolog. It would be nice if the rules merely stated the desired 
 result, and if the implementation were able to dynamically sort the rules and 
 conjuncts to generate the result efficiently.
  
 One proposal for a different evaluation strategy is found in Specint [Darlington 
 90]. A static version of the idea, called “sideways information passing,” appears in 
 Datalog. The idea is to reorder the conjuncts as they are satisfied, so that attention is 
 directed to the first conjunct that has not yet been satisfied. As each conjunct is 
 satisfied, it is rotated to the
  end
  of the list of conjuncts; it may be retested (and 
 resatisfied) later if other conjuncts fail in the meantime. The programmer can 
 supply hints for each predicate that sug-gest what parameters will satisfy that 
 predicate. Predefined predicates have their own hints. For example, Figure 8.51 
 gives a slightly different version of 
 naiveSort
 .
  
 Figure 8.51
  
 naiveSort(L1,L2) :- permutation(L1,L2) , inOrder(L2) .
  
 1
  
 permutation(X,Y) :- X = Y
  
 2
  
 permutation(X| Z,Y) :- delete(X,Y,T) , permutation(Z,T)
  
 3
  
 inOrder([]) .
  
 4
  
 inOrder([_]) .
  
 5
  
 inOrder([A,B| T]) :- A =< B , inOrder([B| T]) .
  
 6
  
 To evaluate
  naiveSort([1,3,2],result)
 , the evaluator first tries to satisfy the first 
 conjunct of line 1. This conjunct brings it to line 2 to find an accept-able permutation
  
 Y
  of
  X
  =
  [1,3,2]
 . By default,
  permutation
  will first try the empty list for
  Y
 . It fails, 
 because it satisfies neither line 2 nor line 3. How-ever, the equality test of line 2 has 
 a default hint: set
  Y
  to
  X
 . Now
  permuta-tion(X,Y)
  is satisfied, so the Specint evaluator 
 moves to the
  inOrder 
 conjunct of line 1, bringing it to line 6. In line 6,
  A
  is
  1
 ,
  B
  is 3, and
  
 T
  is
  [2]
 . The first conjunct succeeds, and
  inOrder
  is called recursively on
  [3,2]
 . In the 
 recursive call of line 6,
  A
  is
  3
 ,
  B
  is
  2
 , and
  T
  is
  []
 . The first conjunct fails. The hint for 
 satisfying
  =<
  is to interchange the two values. Now line 6 suc-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.13 Constraint-Logic Programming (CLP) ,"A small extension to Prolog’s evaluation mechanism simplifies programs like 
 factorial in Figure 8.44 (page 250). This extension, called constraint-logic 
 programming, or CLP, lets identifiers have a
  constrained
  status, which lies between 
 bound and free [Fruhwirth 92]. CLP(
 R
 ) is Prolog with constraints expressed with 
 respect to real numbers.
  
 A conjunct in CLP(
 R
 ) such as
  X < 5
  is merely checked if
  X
  is bound, but if
  X 
 is free 
 or constrained, this conjunct introduces a constraint on
  X
 . If the new constraint, in 
 combination with previous constraints, makes a variable unsat-isfiable, the 
 evaluator must backtrack. The power of this idea can be seen in Figure 8.52.
  
 Figure 8.52
  
 in:
  
 Nice(X, Y) :- X = 6 , Y < 5 .
  
 1
  
 ?- Nice(A,B) .
  
 2
  
 out: A = 6, B < 5
  
 3
  
 in:
  
 ?- Nice(A,B) , B > 7 .
  
 4
  
 out: No
  
 5
  
 Line 2 is only satisfied by a restricted set of values; the output shows the ap-plicable 
 constraints. When I add a conflicting constraint in line 4, no results can be found. 
 Figure 8.53 is a factorial predicate.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.14 Metaprogramming  3,"Chapter 4 shows how a LISP interpreter may be written in LISP. Prolog pro-vides a 
 similar ability. As with LISP, the trick is to make the language ho-moiconic, that is, to 
 be able to treat programs as data. Programs are just sets of rules (a fact is a rule with 
 a body of
  true
 ). The body of a rule is a comma-separated list of predicates. In addition 
 to the bracket-delimited lists shown earlier, Prolog also accepts simple comma-
 separated lists.
 3
 The head of a rule
  
 is also a predicate, and the
  
 :-
  separator can be treated as an infix binary
  
 predicate.
  
 So a program is a set of predicates, and Prolog provides a way to inspect, 
 introduce, and delete the predicates that currently make up the program, that is, to 
 treat the program as data. The
  clause
  predicate is used for in-specting rules, as in 
 Figure 8.58.
  
 grandmotherOf(X,GM) :- motherOf(M,GM) , motherOf(X,M) .
  
 1
  
 grandmotherOf(X,GM) :- motherOf(F,GM) , fatherOf(X,F) .
  
 2
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
  
  
  Comma-separated lists are the underlying concept. The list
  a,b,c
  is equivalent to 
 a,(b,c)
 . The 
 bracketed list
  [H
  |
  T]
  is syntactic sugar for the predicate
  .(H,T)
 , where the dot is a binary
  cons
  functor.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,N,NA
 GO..DEL ,"Go..del, developed by P. M. Hill, is intended to be the successor of Prolog [Hill 94]. It 
 borrows much of Prolog’s form, but attempts to address many of the problems and 
 deficiencies of Prolog. It provides modules to make the lan-guage suitable for large 
 projects, has a strong type system, permits enhanced logical forms, and has more 
 consistent search-pruning operators. Go..del also directly supports integers, floats, 
 rational numbers, strings, lists, and sets.",NA
2.1 Program Structure ,"A program in Go..del consists of at least one module. Each module is divided into 
 several parts, most of which are optional.
  
 •
  module
  names the module. Modules can also be declared
  closed
  (which means the 
 implementation is not provided, and may well be in a different language),
  export
  
 (all definitions are exported to other modules), or
  local 
 (all definitions are 
 private).
  
 •
  import
  lists the modules whose declarations are imported.
  
 •
  base
  declares types.
  
 •
  constructor
  declares type constructors.
  
 •
  constant
  declares constants.
  
 •
  function
  lists functions and declares their type. (These are like functors 
  
 in 
 Prolog.)
  
 •
  predicate
  lists predicates and defines their type.
  
 •
  delay
  lists conditions for controlling evaluation of predicates.
  
 •
  proposition
  lists propositions.
  
 • Finally, there is a list of rules.
  
 For example, the module in Figure 8.61 calculates factorials.
  
 Figure 8.61
  
 module
  Factorial.
  
 1
  
 import
  Integers.
  
 2
  
 predicate
  Fact : Integer * Integer.
  
 3
  
 Fact(0,1).
  
 4
  
 Fact(1,1).
  
 5
  
 Fact(n,f) <- n > 1 & Fact(n-1,g) & f = g * n.
  
 6
  
 The module is named
  Factorial
  and imports types, functions, and predicates from the 
 (library) module
  Integers
 . It has one predicate,
  Fact
 , which has two integer 
 parameters. Three rules define
  Fact
  (lines 4–6). The program is executed by 
 supplying a query, as in Figure 8.62.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 Types ,"Figure 8.63 illustrates construction of programmer-defined types.
  
 Figure 8.63
  
 module
  M1.
  
 1
  
 base
  Day, ListOfDay.
  
 2
  
 constant
  
 3
  
 Nil : ListOfDay;
  
 4
  
 Monday, Tuesday, Wednesday, Thursday, Friday,
  
 5
  
 Saturday, Sunday : Day.
  
 6
  
 function
  Cons : Day * ListOfDay -> ListOfDay.
  
 7
  
 predicate
  Append : ListOfDay * ListOfDay * ListOfDay.
  
 8
  
 /* Append(a,b,c) means list a appended to list b;
  
 9
  
 results in list c. */
  
 10
  
 Append(Nil,x,x).
  
 11
  
 Append(Cons(u,x),y,Cons(u,z)) <- Append(x,y,z).
  
 12
  
 Day
  and
  ListOfDay
  (line 2) are the only types of this program.
  Cons
  (line 7) is not a 
 pattern symbol, as it would be in Prolog, but rather a function. Every constant, 
 function, proposition, and predicate of the language must be de-clared, but variable 
 types are inferred, as in ML. Constructors can be used in type declarations. They 
 may be applied to the ground types defined in the 
 base
  clause to create new types. 
 This process can be recursively applied to make an infinite number of types. I can 
 improve this module by making the concept of list polymorphic, as in Figure 8.64.
  
 Figure 8.64
  
 module
  M2.
  
 1
  
 base
  Day, Person.
  
 2
  
 constructor
  List/1.
  
 3
  
 constant
  
 4
  
 Nil : List(’a);
  
 5
  
 Monday, Tuesday, Wednesday, Thursday, Friday,
  
 6
  
 Saturday, Sunday : Day;
  
 7
  
 Fred, Barney, Wilma, Betty : Person.
  
 8
  
 function
  Cons : ’a * List(’a) -> List(’a).
  
 9
  
 predicate
  Append : List(’a) * List(’a) * List(’a).
  
 10
  
 Append(Nil,x,x).
  
 11
  
 Append(Cons(u,x),y,Cons(u,z)) <- Append(x,y,z).
  
 12
  
 The constructor
  List
  (line 3) is followed by an integer indicating its arity. The 
 identifier
  ’a
  in lines 5, 9, and 10 is a type identifier. The types for this program are
  
 Day
 ,
  Person
 ,
  List(Day)
 ,
  List(Person)
 ,
  List(List(Day))
 , and so forth.
  
 LISP-like lists form such a common structure in declarative programming that 
 Go..del, like Prolog, predeclares the
  List
  constructor, the
  Cons
  function,
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.3 Logic Programming ,"Unlike Prolog programs, Go..del programs are not limited to Horn clauses. The 
 following quantifiers and connectives are allowed.
  
 Symbol
  
 Meaning
  
 & 
  
 \/
  
 ˜
  
 <-
  
 -> 
  
 <-> 
  
 all
  
 some
  
 conjunction (
 and
 ) 
  
 disjunction (
 or
 ) 
  
 negation (
 not
 ) 
  
 implication 
  
 right implication 
  
 equivalence 
  
 universal quantifier 
 existential quantifier
  
 The quantifiers have two parameters, a list of variables and the body, as in Figure 
 8.65.
  
 Figure 8.65
  
 module
  Inclusion.
  
 1
  
 import
  Lists.
  
 2
  
 predicate
  IncludedIn : List(a) * List(a)
  
 3
  
 -- IncludedIn(a,b) means list a is included in list b.
  
 4
  
 IncludedIn(x,y) <-
  all
  [z] (MemberOf(z,y) <- MemberOf(z,x)).
  
 5
  
 -- MemberOf(a,b) means element a is a member of list b.
  
 6
  
 The rule in line 5 indicates that list
  x
  is included in list
  y
  if all members of
  x 
 are also 
 members of
  y
 . This example also illustrates some of the use of mod-ules. The 
 predicate
  MemberOf
  (used in line 5) is declared in the imported module
  Lists
 .
  
 Queries are quite simple to write. For example, assume that a module has been 
 declared with the classic family relationship predicates and facts
  Fa-therOf
 ,
  MotherOf
 ,
  
 ParentOf
 ,
  AncestorOf
 , and so forth. Then the query “Does everyone who has a mother 
 also have a father?” can be written, as in Figure 8.66.
  
 Figure 8.66
  
 <-
  all
  [x]
  
 1
  
 (
 some
  [z] FatherOf(x,z) <-
  some
  [y] MotherOf(x,y))).
  
 2
  
 In practice,
  some
  is seldom used, because Go..del provides 
  
 _
  as a don’t-care
  
 pattern. The above query can be written more simply as in Figure 8.67.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.4 Conditionals ,"Go..del allows the use of conditional statements primarily as a concession to 
 computational efficiency. The structure
  if
  condition
  then
  formula
  is logi-cally 
 equivalent to
  condition -> formula
 . The semantics of conditionals dif-
  
 fer
  
 procedurally
  
 from
  
 implications,
  
 however.
  
 Unlike
  
 implications,
  
 the
  
 evaluation of a conditional waits until the condition has no free variables. 
  
 If the 
 condition and the formula share local variables, the form in Figure 8.70 is used.
  
 if some
  [r1, ..., rn] condition
  then
  formula
  
 This form is equivalent to that in Figure 8.71.
  
 Figure 8.71
  
 (
 some
  [r1, ..., rn] (condition & formula)) \/
  
 1
  
 ˜
  some
  [r1, ..., rn] condition.
  
 2
  
 The
  if
  construct is defined similarly, but oddly enough, the rule for resolving the 
 dangling-
 else
  problem is contrary to standard convention.
  
  
 The module for defining LISP-like association lists in Figure 8.72 illus-trates 
 conditionals.
  
 Figure 8.72
  
 module
  AssocList.
  
 1
  
 import
  Strings.
  
 2
  
 base
  PairType.
  
 3
  
 function
  Pair : Integer * String -> PairType.
  
 4
  
 predicate
  Lookup : Integer * String * List(PairType)
  
 5
  
 * List(PairType).
  
 6
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.5 Control ,"Logic programming in its pure form allows the parameters of predicates to be 
 arbitrarily bound or unbound. As you saw in Prolog, it is often difficult (and 
 unnecessary) to write rules that cover all cases. Go..del uses something like Prolog’s
  
 bound
  predicate, but it enhances it with a control structure that de-lays evaluation of 
 predicates until certain conditions are met. Predicates in a conjunction can be 
 processed like coroutines. For example, the definition of 
 Permutation
  in Figure 8.73 
 might be placed in the
  Lists
  module.
  
 Figure 8.73
  
 predicate
  Permutation : List(a) * List(a).
  
 1
  
 -- Permutation(a,b) means list a is
  
 2
  
 -- a permutation of list b
  
 3
  
 delay
  Permutation(x,y)
  until
  bound(x) \/ bound(y).
  
 4
  
 Permutation([],[]).
  
 5
  
 Permutation([x| y],[u| v]) <-
  
 6
  
 Delete(u,[x| y],z) & Permutation(z,v).
  
 7
  
 -- Delete(a,b,c) means deleting element a
  
 8
  
 -- from list b gives list c
  
 9
  
 The
  delay
  construct in line 4 causes
  Permutation
  to pause until one of its pa-rameters 
 is bound. If it is invoked with both parameters unbound and no other predicates can 
 be explored to bind one of the parameters, as in Figure 8.74,
  Permutation
  will fail. 
 (This behavior is nonmonotonic.)
  
 Figure 8.74
  
 in:
  
 <- Permutation(x,y).
  
 1
  
 out: No
  
 2
  
 in:
  
 <- Permutation(x,y) & x = [1,2].
  
 3
  
 out: x = [1,2], y = [1,2];
  
 4
  
 x = [1,2], y = [2,1]
  
 5
  
 In line 1, neither parameter is bound, so the query fails. In line 3, evaluation of
  
 Permutation
  delays until the second conjunct is evaluated. That conjunct binds
  x
  to a 
 value, so now
  Permutation
  may be invoked.
  
 In order to build a sort program similar to
  naiveSort
  in Figure 8.26 (page 243), I 
 will introduce in Figure 8.75 a
  Sorted
  predicate for the
  Lists
  module.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,"CHAPTER 8
  
 LOGIC PROGRAMMING
  
 N",NA
 FINAL COMMENTS,"In some ways, Prolog is a hybrid of three ideas: LISP data structures, recur-
  
 sive pattern matching as in SNOBOL, and resolution theorem-proving. As a
  
 programming language, Prolog lacks mechanisms for structuring programs
  
 and has no type facilities. It is hard to read Prolog programs, because the or-
  
 der of parameters in predicates is not always obvious. This is a problem in
  
 other languages, but it seems especially severe in Prolog. For all its short-
  
 comings, Prolog is widely used, especially in Europe, for artificial intelligence
  
 rule-based programs. It has been successfully used in such applications as
  
 genetic sequence analysis, circuit design, and stock-market analysis.
  
 Enhancements of Prolog to give it an understanding of constraints and to
  
 organize search differently reduce the difficulty of writing clear and efficient
  
 programs. Various constraint-based extensions to Prolog have been devel-
  
 oped, 
  
 including 
  
 CLP(
 ©
 *), 
  
 which 
  
 understands 
  
 regular 
  
 expressions
  
 [Walinsky 89], and versions that deal with strings in general [Rajasekar 94].
  
 Concurrent logic programming is an active research topic. All the con-
  
 juncts in the body of a rule can be evaluated simultaneously, with bindings of
  
 common variables communicated as they arise between otherwise indepen-
  
 dent evaluators. This technique is called
  and
 -parallelism. Similarly, multiple
  
 rules whose heads match a goal can be evaluated simultaneously; this tech-
  
 nique is called
  or
 -parallelism. Research topics include the ramifications of
  
 using shared and distributed memory, how to manage bindings for variables,
  
 how much parallelism can be discovered by the compiler in ordinary Prolog
  
 programs, and how the programmer can assist that task in extensions to Pro-
  
 log. One such extension, called Guarded Horn Clauses, allows guard predi-
  
 cates, much like the guards in Ada’s
  select
  statement (discussed in Chapter
  
 7), to restrict the rules that are to be considered during concurrent evalua-
  
 tion. Much of the literature on concurrent logic programming has been sur-
  
 veyed by Shapiro [Shapiro 89].
  
 Go..del manages to blend Prolog with strong typing, some type polymor-
  
 phism, and modularization, while increasing the range of logical operators. It
  
 also provides lazy evaluation, which makes some naive programs far more ef-
  
 ficient. However, it is a much more complex language; one of Prolog’s advan-
  
 tages is its relative simplicity.
  
 There are other languages specifically intended for knowledge-based rea-soning. 
 In particular, OPS5 shares with Prolog and Go..del the concept of
  
 rules, facts, and queries [Brownston 86]. It is based on an inference engine,
  
 which repeatedly (1) determines which rules match existing facts, (2) selects
  
 one of those rules based on some strategy, and then (3) applies the actions
  
 specified in the selected rule, usually adding to or altering the set of facts.
  
 Step 1 can be extremely costly, but step 3 can propagate changes to a data
  
 structure to make step 1 reasonably efficient.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"Figure 8.80
  
 8.1
  
 Figure 8.10 (page 236) shows the backtrack tree for the query
  grand-
  
 motherOf(tom,X)
 . Show the backtrack tree with the rules for
  grand-
  
 motherOf
  in Figure 8.8 (page 234) reordered as in Figure 8.80.
  
 grandmotherOf(X,GM) :- motherOf(X,M) , motherOf(M,GM) .
  
 1
  
 8.2
  
 grandmotherOf(X,GM) :- fatherOf(X,F) , motherOf(F,GM) .
  
 2
  
 It appears that the rules on lines 3 and 4 of Figure 8.23 (page 242) can
  
 Figure 8.81
  
 be replaced by the single rule
  append([],X,X)
 . Is this so?
  
 8.3
  
 What is the result of the query in Figure 8.81?
  
 in:
  
 ?- append([1,2],X,[1,2,3,4]) .
  
 8.4
  
 Modify the
  eval
  rules in Figure 8.59 (page 257) so that bodies are inter-preted 
 from right to left, that is, with the last conjunct first.
  
 8.5
  
 Design a functor
  fraction
  with two parameters (the numerator and de-
  
 nominator) and predicate
  lessThan
  that takes two fractions and is satis-
  
 fied if the first fraction is less than the second. The
  lessThan
  predicate
  
 does not need to be defined for unbound parameters.",NA
Challenge Exercises,"8.6 
  
 8.7 
  
 8.8 
  
 8.9
  
 Does Prolog have static or dynamic scope rules for formal parameters?
  
 Are predicates in Prolog first-, second-, or third-class values? How about 
 predicate names, functors, and terms?
  
 Show how to build a stack containing {1,2,3} and to verify that it is a stack, 
 using the definitions of Figure 8.20 (page 240).
  
 In Figure 8.20 (page 240), I defined nonhomogeneous stacks. Show how the 
 existence of a built-in
  integer
  predicate allows you to define integer stacks.
  
 8.10
  In Figure 8.20 (page 240),
  pop
  is a predicate name. Rewrite this exam-
  
 ple 
 so that
  pop
  is a functor.
  
 8.11
  In Figure 8.26 (page 243), how many solutions are there to
  naive-
  
 Sort([11,2,11],S)
 ?
  
 8.12
  In Figure 8.26 (page 243), how many solutions are there to 
  
 naiveSort(S,[1,2])
 ?
  
 8.13
  As an alternative to
  naiveSort
  and
  bubbleSort
 , encode
  insertionSort 
 in Prolog. 
 Make sure your program works correctly in all four cases of 
 insertionSort(
 
 ,
 
 )
 , 
 whether
  
  or
  
  is a constant or a variable.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 9 ,NA,NA
O,NA,NA
Aggregates,"This chapter deals with language features for dealing with
  aggregates
 , which are 
 data that are structured according to some commonly useful orga-nization, such as 
 strings, arrays, and databases. Although many program-ming languages provide 
 general-purpose facilities to structure data (such as records) and organize routines 
 that manipulate the data (such as abstract data types), some structures are so 
 important that languages deal with them specifically in order to make it easier to 
 write clear and efficient programs. In this chapter, I concentrate on strings, arrays, 
 databases, and mathematical formulas.",NA
1,N,NA
 STRINGS,"Most languages provide some facility for dealing with strings, that is, con-nected 
 groups of characters. Some languages, however, specialize in string processing. This 
 chapter will look at both elementary string operations and more complex control 
 and data structures introduced in specialized string-processing languages.",NA
1.1 Literals and Simple Operations ,"String literals are usually enclosed in double quotes (
  ""
  ). Some syntax is of-ten 
 provided to include unusual characters in string literals. For example, the C 
 language allows an escape character to precede special forms, such as 
 \r
  for a 
 carriage return,
  \t
  for a tab,
  \""
  for a double quote, and
  \023
  for the character whose 
 internal representation is octal 23. One nice escape se-quence that doesn’t exist in 
 any language I know of skips to the next non-white text without including the white 
 space in the string. I use 
  
  
 \c 
  
 to 
 represent this special form, as in Figure 9.1.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 269",NA
1.2 Representation ,"Usually, programmers don’t need to worry about how a language implementa-
  
 tion represents strings.
  
 However, the representation can affect both the
  
 speed of computation and the way the program must manipulate strings. For 
 example, C defines strings as consecutive characters terminated by a null (bi-nary 
 zero) character. This representation makes it slow to concatenate a string to the end 
 of another (the implementation must find the end of the sec-ond string by a linear 
 method) and does not allow nulls to be contained within strings. It encourages a 
 programming style in which a variable points into the string and advances character 
 by character until the terminating null is seen.
  
 Alternative representations have some advantages. If the length of the string is 
 encoded, perhaps in the first few bytes, then concatenation becomes faster, and 
 strings may contain null characters. If strings are declared with a compile-time 
 length, many operations become faster, and the compiler can keep track of the 
 length of intermediate strings in complex expressions. However, some operations 
 produce results whose length cannot be predicted. For example, a substring 
 operation might take a variable length parameter. Therefore, languages in which 
 strings are explicitly declared usually declare the maximum length that the string 
 value might attain. This information de-termines storage requirements but does not 
 dictate the length of particular values put into storage.
  
 One attractive proposal is to omit a string-length code at the start of the storage 
 area for a string, use a terminating null, but use the last byte of the storage area to 
 indicate the distance back to the terminating null [Bron 89]. If the string just fits in 
 the storage area, so that the terminating null is in the last place, the null looks like 
 the number
  0
 , indicating zero distance to the terminating null. This representation 
 makes it a bit harder for programs to build new strings directly, but a reasonable 
 library of string-building opera-tions can circumvent this problem, and programs 
 may still scan through strings by using explicit pointers.",NA
1.3 Pattern Matching ,"Sal, Awk, and Perl provide a match operator
  ˜
  that compares a target string to a 
 regular expression. The result is Boolean, indicating success. A
  regular expression
  
 is a string, where most characters match themselves, but some characters and 
 character combinations have special meanings. The following table lists some of 
 these special meanings; Perl has an even richer set.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.4 Associative Arrays ,"Languages dealing with strings often provide a data type known as an
  asso-ciative 
 array
 , which is indexed by strings instead of by integers or other scalar types. 
 Associative arrays are usually implemented by hash tables. In some languages, like 
 Sal and SNOBOL, the declaration of such an array indi-cates how large to make the 
 hash table. If more elements are stored than the hash table size, access will become 
 progressively slower but will still work. Other languages, like Perl, use extensible 
 hashing and do not require any size declaration. ABC uses binary trees instead of 
 hashing, so that a program can iterate through the array in key order. Other 
 languages can only iterate in an implementation-dependent order.
  
 Associative arrays are quite helpful in database applications. For exam-ple, to 
 check for duplicates in a database with one field, say,
  StudentName
 , I could use the 
 Boolean associative array
  Present
  of Figure 9.11.
  
 Figure 9.11
  
 variable
  
 1
  
 Present :
  array
  string
  of
  Boolean;
  
 2
  
 ThisEntry : string;
  
 3
  
 loop
  
 4
  
 ThisEntry := GetNextEntryOfDatabase();
  
 5
  
 if
  ThisEntry = """"
  then break end
 ; -- exit loop
  
 6
  
 if defined
  Present[ThisEntry]
  then
  -- found duplicate
  
 7
  
 write(""{ThisEntry} is a duplicate."");
  
 8
  
 end
 ;
  
 9
  
 Present[ThisEntry] := true;
  
 10
  
 end
 ;
  
 11
  
 In line 7, the
  defined
  operator indicates whether a value has been defined for the 
 particular index value given; it returns a Boolean. The assignment in line 10 could 
 just as easily use
  false
 ; what counts is that some value is placed in
  Present[ThisEntry]
 .
  
 Associative arrays often come with a control structure for iterating over all index 
 values that have been defined. Figure 9.12 continues the previous example.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.5 Substrings as First-Class Values ,"Allowing substrings to be first-class values of a predeclared substring type has 
 several advantages [Hansen 92]. Substring values can record not only their contents 
 but also the identity of their base string. Dynamic allocation of space for substrings 
 can be handled by the language at runtime.
  
 Each value of the substring type contains a base string (perhaps imple-mented as 
 a pointer) and the left and right positions in that string that de-
  
 limit the substring.
  
 As with Icon, I understand positions to be between
  
 characters of the string.
  
  
 The primitive operations on substrings can be simple and few. Here is a 
 reasonable set of primitive operations:
  
 •
  start(x)
 . Returns a substring with the same base as
  x
 , with both left 
  
 and right set to 
 left of
  x
 .
  
 •
  base(x)
 . Returns a substring with the same base as
  x
 , left set before the 
  
 first 
 character of
  x
 , and right set to after the last character of
  x
 .
  
 •
  next(x)
 . Returns a substring with the same base as
  x
 , left set to right of 
 x
 , and right 
 set one character after left if possible. Otherwise, right is set to the same 
 position as left.
  
 •
  prev(x)
 . Returns a substring with the same base as
  x
 , right set to left of 
 x
 , and left 
 set one character before right if possible. Otherwise, left is set to the same 
 position as right.
  
 •
  extent(x,y)
 . If
  x
  and
  y
  have different base strings, returns an empty substring of the 
 empty base
  """"
 . Otherwise, returns a substring with right set to the right of
  y
  and 
 left set to either left of
  x
  or right of
  y
 , whichever is earlier in the base.
  
 •
  x = y
 . The base strings of the two substrings are compared character by character 
 between their left and right positions. The result is
  true
  if and only if the lengths are 
 identical and the selected characters match exactly.•
  x + y
 . Returns a substring 
 containing a new base string that is the con-catenation of the substrings
  x
  and
  y
 , and 
 left and right at the beginning and end of that new base string.
  
 •
  x := y
 . The old value of
  x
  is discarded;
  x
  acquires the same value as
  y
 , 
  
 including the base string and the left and right positions.
  
 Given these primitive operations, I can write a function that takes a sub-string 
 representing a word terminated by blanks and returns a substring rep-resenting the 
 next word, as in Figure 9.13 [Hansen 92].
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.6 SNOBOL ,"SNOBOL was developed by Ralph E. Griswold and others at Bell Telephone 
 Laboratories around 1965 [Griswold 71]. It has a strange syntax, partially because it 
 was developed before Algol-like syntax became popular. Spaces act as both the 
 concatenation and the match operators. The only statement form includes pattern 
 match, replacement, and success and failure
  goto
 s. To avoid confusion, I translate all 
 the SNOBOL examples into an Ada-like syntax, us-ing
  match
  and
  replace
  operators. 
 SNOBOL uses dynamic typing and dy-namic scope rules; its primitive data types are 
 strings, integers, and reals. The structured types include patterns (distinct from 
 strings), nonhomoge-neous arrays, and associative arrays.
  
 Variables are not declared; all conceivable strings (even the empty string) name 
 variables. Initially, all variables have the value
  """"
 . In a sense, there-fore, all string 
 values point to other strings, as in Figure 9.14.
  
 Figure 9.14
  
 somewhere := ""over"";
  
 1
  
 over := ""the"";
  
 2
  
 the := ""rainbow"";
  
 3
  
 write(somewhereˆˆ); -- writes ""rainbow""
  
 4
  
 SNOBOL is homoiconic, after a fashion. A program is a string, and it is possible at 
 runtime to compile a string and to branch to a label in it. How-ever, this facility is 
 much less attractive than LISP’s equal treatment of pro-gram and data structure. 
 SNOBOL has not been heavily used for artificial intelligence programming.
  
 SNOBOL patterns are like regular expressions, but more powerful. They are 
 structured values built recursively. The simplest patterns are string liter-als and 
 string-valued expressions, which match themselves. More complex patterns are 
 formed by sequencing (somewhat like
  and
 ), alternation (some-what like
  or
 ), and by 
 invoking pattern-returning predeclared functions. Pat-terns are matched by a 
 backtracking algorithm, trying earlier alternatives first. Backtracking in pattern 
 matching is very similar to backtracking in
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.7 Icon ,"Icon was developed by Ralph E. Griswold, one of the developers of SNOBOL, in the late 
 1970s as a result of his dissatisfaction with how SNOBOL’s pat-terns fit into the 
 language [Griswold 80]. It retains the virtues of SNOBOL’s pattern matching without a 
 pattern data type. It is an expression-oriented language, with each evaluation 
 resulting in either a value (counted as a suc-cess) or failure. Instead of using Boolean 
 values, conditionals base their ac-tions on the success or failure of evaluating their 
 conditions.
  
 The first novel idea in Icon is the
  scan
  statement. (I call it a statement, even 
 though all constructs in Icon are actually expressions, because it is usu-ally not used 
 for its value.) This statement introduces a name scope that cre-ates a new binding 
 for two predeclared variables,
  subject
  and
  pos
 , which specify the current string being 
 matched and the current position within the string. Consider Figure 9.20 (I take 
 liberties with actual Icon syntax to keep my examples consistent).
  
 Figure 9.20
  
 scan
  ""peristalsis""
  using
  
 1
  
 write(""["" + move(4) + ""]"")
  
 2
  
 end
 ;
  
 3
  
 This program prints
  ""[peri]""
 . The
  scan
  in line 1 maps
  subject
  to
  ""peri-stalsis""
  and sets
  
 pos
  initially to 1. The body of
  scan
  is in line 2; it implicitly uses both
  subject
  and
  pos
  
 (modifying the latter). The predeclared procedure 
 move
  causes the position to be 
 incremented, if
  subject
  is long enough, and if it succeeds, it returns the substring of
  
 subject
  over which it has advanced. The
  +
  operator is string concatenation. After the 
 body, both
  subject
  and
  pos 
 revert to whatever values they had before. Figure 9.21 
 shows a more complex nested example.
  
 Figure 9.21
  
 scan
  MyString
  using
  
 1
  
 loop
  -- each iteration deals with one word
  
 2
  
 scan
  tab(upto("" ""))
  using
  
 3
  
 if
  upto(""-"")
  then
  -- word has a hyphen
  
 4
  
 write
 (subject);
  
 5
  
 end
 ;
  
 6
  
 end
 ; -- scan tab(upto("" ""))
  
 7
  
 move(1); -- past "" ""
  
 8
  
 end
 ; -- loop
  
 9
  
 end
 ; -- scan MyString
  
 10
  
 This program prints out all space-delimited words in
  MyString
  that contain a hyphen. 
 The outer
  scan
  (lines 1–10) contains a loop that repeatedly advances 
 pos
  to a space, 
 scans the intervening word (lines 3–7), and then moves past the space (line 8). The 
 predefined function
  upto
  (lines 3 and 4) returns the
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
1.8 Homoiconic Use of Strings: Tcl ,"Several syntax rules in Tcl interact to make it homoiconic. Lists are repre-sented as 
 strings; the individual elements are delimited by white space. Ev-ery string names a 
 variable. The R-value of a variable is denoted by
  $
  before the string that represents 
 the variable. (This rule makes Tcl programs error-prone, because it is so easy to 
 forget the
  $
  .) Strings need not be delimited by quotes unless they have embedded 
 spaces. There are quotes (
  {
  and
  }
  ) that prevent any evaluation within a string, 
 quotes (
  ""
  ) that allow evaluation, and quotes (
  [
  and
  ]
  ) that force the string to be 
 evaluated. Evaluating a string means treating it as a series of commands delimited 
 by end-of-line characters or semicolons. Each command is the name of a procedure 
 (many are prede-clared; I will show them in
  bold monospace
 ) followed by 
 parameters. The whole program is a string to be evaluated. Figure 9.30 shows a 
 simple Tcl ex-ample.
  
 Figure 9.30
  
 set
  a 4 -- a := 4
  
 1
  
 set
  b [
 expr
  $a + 5] -- b := 9
  
 2
  
 while
  {$b > 0} {
  
 3
  
 puts
  ""b is now $b""
  
 4
  
 set
  b [expr $b - 2]
  
 5
  
 }
  
 6
  
 This program prints
  b is now 9
  and then four more similar outputs. Line 1 is the 
 assignment statement. It takes the name, not the R-value, of the vari-able to be 
 assigned. Line 2 shows the quotes that force evaluation:
  [
  and
  ]
  . The
  expr
  command 
 evaluates any number of parameters as an arithmetic ex-
  
 pression.
  
 It returns the value of that expression.
  
 Line 3 introduces the
  
 quotes that prevent evaluation:
  
 {
  and
  
 }
  . The
  while
  command takes two
  
 unevaluated strings, the first representing a conditional and the second rep-
 resenting the body of the loop. It repeatedly invokes
  expr
  on the first param-eter, 
 and if the result is true, it evaluates the second parameter, thereby
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2,N,NA
 ARRAYS: APL,"Arrays are especially important in mathematical computation. One of the principal 
 advances in FORTRAN 90 over earlier versions of FORTRAN is its ability to 
 manipulate arrays without dealing with the individual array ele-ments. However, 
 the best example of an array language is not FORTRAN, but APL. The APL language 
 was invented by Kenneth E. Iverson in the early 1960s and has had a small but 
 devoted following ever since. It could be con-sidered a single-minded language: All 
 computation is cast in the mold of ar-ray manipulation. Its practitioners point with 
 pride at the conciseness of their programs; detractors point with scorn at the 
 unreadability of the same programs. APL has long suffered from the fact that most of 
 its operators are not normal ASCII symbols, so ordinary keyboards are not adequate 
 for repre-senting APL programs. Dialects such as J and APL/11 use several ASCII 
 characters together to represent the unusual symbols. My examples expand unusual 
 symbols into keywords to help you read them.
  
 APL programs must be studied; they cannot simply be read. Not only does APL 
 have an unusual character set, but it lacks control structures such as 
 while
  and 
 conditionals.
  
 APL’s greatest strength is its ability to handle arrays of any dimension with the 
 same operators that apply to scalars (which are zero-dimensional ar-rays). The 
 meaning is to apply the operator pointwise to each member of the array. The 
 resulting uniformity, along with the wealth of arithmetic opera-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.1 Operators and Meta-operators,"APL
  
 is
  
 generally
  
 interpreted,
  
 not
  
 compiled.
  
 All
  
 operators
  
 are
  
 right-
  
 associative and have the same precedence. Most operator symbols can be used 
 either as unary or as binary operators, often with different meanings. To keep things 
 clear, I use different keywords for the two meanings. Besides
  
 ordinary operators such as
  
 +
  , APL has many unusual operators, including
  
 the following:
  
 Operator
  
 Meaning
  
 x
  min
  y 
  
 floor
  x 
  
 ceil
  x 
  
 recip
  x 
  
 sign
  x 
  
 abs
  x 
  
 x
  max
  y 
  
 exp
  x 
  
 x
  power
  y 
  
 x
  log
  y 
  
 ln
  x 
  
 x
  comb
  y 
  
 fact
  x 
  
 x
  deal
  y 
  
 rand
  x 
  
 x
  layout
  y 
  
 fill
  x 
  
 shape
  x 
  
 x
  drop
  y 
  
 x
  take
  y 
  
 transpose
  x 
  
 x
  member
  y 
  
 x
  cat
  y
  
 min(
 x
 ,
 y
 ) -- lesser value 
  
 floor(
 x
 ) -- greatest integer
  
  x 
  
 ceiling(
 x
 ) -- least integer
  
  x 
  
 1/
 x
  -- reciprocal 
  
 abs(
 x
 ) /
  x
  -- sign of
  x 
  
 abs(
 x
 ) -- absolute value 
  
 max(
 x
 ,
 y
 ) -- greater value 
  
 exp(
 x
 ) -- e to power
  x 
  
 x
  ^
 y
  --
  x
  to power
  y 
  
 logarithm (base
  x
 ) of
  y 
  
 logarithm (base e) of
  x 
  
 C(
 y
 ,
 x
 ) -- number of combinations of
  y
  taken
  x
  at a time 
 factorial(
 x
 ) --
  x
  can be fractional 
  
 x
  integers picked randomly (no replacement) from 1. . .
 y 
 random integer from 1..ceiling(
 x
 ) 
  
 array with dimensions
  x
  and initial value
  y 
  
 one-dimensional array with initial values 1. . .
 x 
  
 array of bounds of
  x 
  
 remove first
  x
  elements of
  y 
  
 keep only first
  x
  elements of
  y 
  
 reverse the order of dimensions of
  x 
  
 0 or 1, depending on whether
  x
  is found in
  y 
  
 x
  concatenated with
  y
  (spread if necessary)
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 An APL Evaluator ,"One of the most delightful things about APL is that it lends itself to lazy eval-uation. 
 For example,
  transpose
  need not actually create a new array and fill it with data; it 
 needs only to wait until one of its values is required. It can then convert the indices of 
 the desired access into the nontransposed indices and fetch the value from its 
 operand. Likewise, the
  fill
  operator need not actually build an array; it can easily 
 return values when they are actually re-quired. Although lazy evaluation will 
 generally not be faster than full evalu-ation, it can avoid allocating large amounts of 
 space.
  
 A lazy evaluator can be written for APL in an object-oriented language. In 
 Smalltalk nomenclature, the class
  Expression
  has instance variables
  dimen-sion
 ,
  
 bounds
 , and
  values
 . For example,
  (3 4)
  layout
  4
  can be represented by an object in 
 which
  dimension = 2
  and
  bounds = (3 4)
 . The instance variable 
 values
  caches the values 
 that have already been computed, so they do not need to be computed again. The
  
 Expression
  class has a method
  inRange: 
 that reports whether a given index 
 expression is valid for the dimensions and
  
 Figure 9.36
  
 bounds given.
  
 It also provides methods
  store:at:
  and
  retrieve:at:
  for
  
 caching computed values in
  values
 , a method
  write
  for displaying all values, and 
 methods
  dimension
  and
  bounds
  to report these instance variables.
  
 The
  Expression
  class has subclasses for every operator. Each subclass has 
 methods for initialization (to set the dimension and bounds) and for ac-cess at any 
 index. For example,
  Fill
  sets
  dimension = 1
 . It can compute the value at any valid index 
 without needing to store any array. Subclasses like 
 Matinv
  that wish to cache 
 computed values may do so via
  store:at:
 . The
  Ex-pression
  class has methods for 
 creating and initializing an instance of each subclass. One final subclass of
  Expression
  
 is
  Spread
 , which is used to accom-plish coercion to a higher dimension. It can be 
 called explicitly, but it will also be called implicitly by operators such as
  Plus
  when 
 necessary.
  
  
 Some of the examples above could be cast as shown in Figure 9.36 into Smalltalk.
  
 APL:
  fill
  5
  
 1
  
 OOP: Expression
  fill:
  5
  
 2
  
 APL:
  
  accumulate
  a=a
  
 3
  
 OOP: Expression
  accumulate:
  NotEqual
  of:
  
 4
  
 (Expression
  equal:
  a
  and:
  a)
  
 5
  
 APL: (
 fill
  4) +
  inner
  * (
 fill
  4)
  
 6
  
 OOP: Expression
  inner:
  Plus
  with:
  Times
  of:
  
 7
  
 (Expression
  fill:
  4)
  and:
  (Expression
  fill:
  4)
  
 8
  
 APL: (2 3)
  layout fill
  6
  
 9
  
 OOP: Expression
  layout:
  #(2 3)
  with:
  
 10
  
 (Expression
  fill:
  6)
  
 11
  
 In Line 2, the
  fill:
  method in
  Expression
  returns an instance of the
  Fill 
 subclass, 
 suitably initialized. I have omitted an invocation to
  write
  that would display all the 
 values of this object. In lines 4–5, the
  accumulate:of: 
 method of
  Expression
  creates an 
 instance of the
  Accumulate
  subclass and
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.3 Incremental Evaluation ,"In some applications, the same program is executed repeatedly on slightly dif-ferent 
 inputs. For example, spreadsheet programs are often reevaluated with slightly 
 different data. Functional programming languages have been de-signed that can 
 quickly evaluate expressions given new data expressed as a modification of previous 
 data [Yellin 91].
  
 I want to show you how this idea can be embedded in an APL interpreter. To 
 keep the discussion simple, I do not use a lazy interpreter, and I assume that the 
 program is a single function with no internal variables. Given an old value and a new 
 value, a
  delta
  represents how to change the old value to the new value. Of course, by 
 value I mean an array of some shape. The delta and the old value together are 
 enough to completely specify the new value.
  
 Every operator instance records the most recent value it has produced. It 
 provides that value to its caller as a delta. The ultimate caller is typically the outer-
 level
  write
  routine, which uses the delta it receives to display the value of the 
 program. (It might even display the delta, if the user is interested in that 
 representation instead of the fully expanded result.) The first time the program is 
 run, the deltas show the difference between the void value (not even a zero-
 dimensional array!) and the initial value.
  
 In order for this scheme to be efficient, incremental computation should usually 
 not be more expensive than computing from scratch. If we are lucky, incremental 
 computation is very inexpensive. An occasional inefficient re-computation is 
 perfectly acceptable, though.
  
  
 Achieving efficiency has two parts. First, the format for the deltas should not be 
 longer than the new value. If a value has changed in major ways, it is better just to 
 provide the new value outright. For APL arrays, a delta might indicate dimensions to 
 delete, indices within a dimension to delete, particular values to change, and new 
 indices within a dimension to add (with their val-ues). For example, the delta from
  1 
 3 4 5 7
  to
  1 2 4 5
  might be represented as“change at index
  2
  to value
  2
 , delete index
  5
 .”
  
  
 Second, each operator and meta-operator should be implemented to take 
 advantage of deltas. For example, the
  +
  operator generates an output delta that only 
 includes indices where the input deltas indicate a change. The
  ac-cumulate
  meta-
 operator could make use of an inverse to the operator it is given, if one exists, in 
 order to remove the effects of any deleted array ele-ments before adding the effects 
 of inserted elements.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,N,NA
 DATABASE LANGUAGES,"Databases are much more varied in structure than strings or arrays. The range of 
 languages designed for databases is also quite wide. Database lan-guages tend to 
 look like ordinary algebraic languages and are often Algol-based. They integrate 
 database operations by providing additional data types and control constructs. 
 Typically, programmers need to keep two “current lo-cations” in mind: the current 
 point of execution of the program, and the cur-rent record of a database. In some 
 languages, it is also necessary to keep the current relation in mind.
  
 Figure 9.37",NA
3.1 Data Types ,"There are several ways to represent data in a database, known as hierarchi-cal, 
 network, and relational. I concentrate on relational databases, in which information 
 is stored in
  relations
 , which are persistent homogeneous arrays of records.
  
 My examples are taken from dBASE [Simpson 87], Sal [Sturgill 89], and a
  
 higher-level language, SQL.
  
 My examples will be based on the relations
  
 shown in Figure 9.37.
  
 People :
  relation
  
 1
  
 FirstName, LastName : string;
  
 2
  
 BirthYear : integer;
  
 3
  
 end
 ;
  
 4
  
 Events :
  relation
  
 5
  
 Place, What : string;
  
 6
  
 EventYear : integer;
  
 7
  
 end
 ;
  
 8
  
 That is,
  People
  and
  Events
  are homogeneous persistent arrays of records with the 
 fields as shown. I have not limited the length of the string fields (dBASE requires 
 declaring the exact length; Sal does not, but does allow pat-terns that restrict valid 
 values) nor the range of the integer fields (dBASE re-quires specifying the number of 
 characters in a string version of the field; Sal
  
 allows explicit range specification).
  
 The data specifications, known as
  
 schemata
 , are usually stored in files, as are the relations themselves. Schemata are 
 built either interactively (dBASE) or by a specification file (Sal).
  
 In dBASE, a program that uses a relation opens it for use, at which time the field 
 names become defined. dBASE is dynamic-typed. Runtime func-tions are available 
 to determine the types of fields. In Sal, a program must read the relation into a local 
 relation variable before using it and must specify which fields are to be read. 
 Runtime type checking verifies that the specified fields actually exist and are 
 consistent with the uses to which they are put.
  
 Both dBASE and Sal allow the programmer to restrict attention to those records 
 in a relation for which some Boolean expression holds. In dBASE, there are two 
 techniques for restriction. First, a
  filter
  statement causes records to be invisible, as 
 in Figure 9.38.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.2 Control Structures ,"Control structures are needed for setting the current-record mark and for it-erating 
 through all relevant records in a relation. Sal has no methods for ex-plicitly moving 
 the current-record mark; it only provides for iteration.
  
 In dBASE,
  seek
  uses the current order to search quickly for a record whose order 
 value matches the given expression. In addition, the program-mer can undertake a 
 search within a subset of the records for one whose fields match any Boolean 
 expression. Such a search is slower than
  seek
 , be-cause the order information allows 
 an O(log
  n
 ) binary search, where
  n
  is the number of records. Finally, dBASE provides 
 a
  goto
  statement that sets the current-record mark to any given record by serial 
 number in the natural or-der, and a
  skip
  statement that moves any number of 
 records relative to the current record in the current order. There are predeclared 
 routines that indi-cate the value of the current-record mark and the number of 
 records in the relation.
  
 Iteration is accomplished in Sal by a
  foreach
  statement. In Sal,
  foreach 
 indicates 
 which relation variable to use and names a control variable, as in Figure 9.41.
  
 Figure 9.41
  
 variable
  
 1
  
 People :
  relation
  
 2
  
 FirstName, LastName : string;
  
 3
  
 BirthYear : integer;
  
 4
  
 end
 ;
  
 5
  
 Person :
  tuple
  of People;
  
 6
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.3 Modifying Data ,"Sal is not intended for modifying data (there are related programs for that purpose 
 in the package that contains Sal). dBASE modifies data in the cur-rent record by a
  
 replace
  statement, which indicates new field-value pairs. Fields that are not 
 mentioned are left alone. New records are added to the end of the relation by an
  
 append
  statement, after which it is necessary to
  re-place
  the values of all fields. The 
 current record can be deleted or undeleted; a separate statement is needed to 
 accomplish the fairly expensive operation of physically removing all records that 
 have been deleted and rebuilding or-der information. dBASE is also capable of 
 copying a relation (or a part of it based on Boolean expressions) to a new relation, 
 with an option to sort the new relation in the process.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.4 SQL ,"SQL (Structured Query Language) was developed during the mid-1970s and 
 introduced commercially in 1979. Since then, it has become widely available. SQL is in 
 a sense a single-minded language: All computation is cast in the mold of relation 
 manipulation. This is just the right level of abstraction for many database operations. I 
 concentrate on expressions that access existing relations; there are also commands 
 that update existing relations. First, Fig-ure 9.47 shows how to compute all people 
 born before 1990 with distinct first and last names.
  
 Figure 9.47
  
 select
  *
  
 1
  
 from
  People
  
 2
  
 where
  BirthYear < 1990
  and
  LastName
  
  FirstName;
  
 3
  
 This program fragment is an expression. If it stands by itself, the resulting data are 
 be displayed; it can be placed in an assignment statement or any-where else that a 
 relation is expected. The
  *
  in line 1 indicates that the re-sulting relation is to contain 
 all fields of the underlying relation, which in line 2 is specified to be
  People
 . Line 3 
 restricts which records are to be selected for the result.
  
  
 Figure 9.48 shows how to find the names of people whose last name ap-pears 
 after
  ""Jones""
  and were born before 1990.
  
 Figure 9.48
  
 select
  FirstName, LastName
  
 1
  
 from
  People
  
 2
  
 where
  BirthYear < 1990
  and
  LastName > ""Jones""
  
 3
  
 Figure 9.49 shows how to find all people by age category.
  
 Figure 9.49
  
 select
  FirstName, LastName, BirthYear
  
 1
  
 from
  People
  
 2
  
 orderby
  BirthYear;
  
 3
  
 The
  orderby
  clause in line 3 indicates that the resulting relation is to be sorted by 
 birth year.
  
  
 The code of Figure 9.50 will print the events that occurred in every per-son’s 
 birth year.
  
 Figure 9.50
  
 select
  FirstName, LastName, What, Place
  
 1
  
 from
  People, Events
  
 2
  
 where
  EventYear = BirthYear
  
 3
  
 orderby
  LastName + FirstName;
  
 4
  
 This example builds a single relation from multiple relations. Such a compu-tation is 
 known as a
  join
 . In this case, line 2 specifies that the fields of
  Peo-ple
  and
  Events
  are to 
 be combined. 
  
 Line 3 restricts attention to those 
 records where the
  EventYear
  is the same as the
  BirthYear
 . Such restriction is common, 
 but not required. It is not necessary to build the restriction out of an equality test. 
 Line 1 restricts attention to four of the resulting fields. Line
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 SYMBOLIC MATHEMATICS,"Early languages like FORTRAN were intended primarily for numeric mathe-
  
 matical computation. A completely different class of languages has been de-
  
 veloped for symbolic mathematical computation. The major novelty of these
  
 languages is that unbound identifiers can be treated as algebraic symbols to
  
 be manipulated.
  
 The best-known languages in this family are Macsyma,
  
 Maple, and Mathematica. These languages can simplify algebraic expres-
  
 sions, perform symbolic integration and differentiation, calculate limits, gen-
  
 erate series and sequences, solve systems of equations, and produce graphs.
  
 They are almost always used interactively.
  
 Since there are so many different mathematical manipulations possible,
  
 mathematical programming languages tend to organize their functions into
  
 libraries that are dynamically loaded when they are needed. This organiza-
  
 tion reduces the amount of memory that a typical session will need. For ex-
  
 ample, Maple’s linear algebra library contains routines for solving linear
  
 systems of equations, inverting matrices, and finding eigenvectors and eigen-
  
 values. There are also libraries for combinatorics, for the simplex method, for
  
 trigonometric functions, and many other applications. Arrays can be manipu-
  
 lated much as in APL, including extraction of slices in any dimension, so op-
  
 erations like Gaussian elimination are easy to write. In fact, Mathematica
  
 has APL’s
  inner
  and
  outer
  operators.
  
 Figure 9.59 shows some examples of the manipulations possible in these
  
 languages.
  
 in:
  
 poly := 2*xˆ5 - 3*xˆ4 + 38*xˆ3 - 57*xˆ2 - 300*x+450;
  
 1
  
 solve(poly=0,x); -- solve with respect to x
  
 2
  
 out:
  
 1/2
  
 1/2
  
 3
  
 3/2, 5 I, - 5 I, 6
  
 , - 6
  
 4
  
 in:
  
 e1 := a + b + c + d = 1;
  
 5
  
 e2 := 2*a + 5*b + c + 4*d = 4;
  
 6
  
 e3 := -5*a + 4*b + 5*c - 3*d = -1;
  
 7
  
 e4 := b + 4*c - 5*d = 0;
  
 8
  
 SolutSet := solve({e1,e2,e3,e4},{a,b,c,d});
  
 9
  
 out: SolutSet := {d = 0, c = -2/13, a = 7/13, b = 8/13}
  
 10
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
5,N,NA
 FINAL COMMENTS,"Languages that are aimed at special applications tend to concentrate on par-ticular 
 aggregates in order to help the programmer write clear and efficient code. SNOBOL 
 and Icon are particularly designed for applications that need to read and manipulate 
 textual data. The related
  scripting languages
  are used to scan text files, extract 
 information, print reports, construct input for other programs, and collect output 
 from other programs. Such languages in-clude command interpreters like Csh, 
 stream editors like Awk and Sed, and interpreted languages such as Perl and Tcl. 
 These languages generally have many features for manipulating strings. Extensions 
 to Prolog (see Chapter 8) for dealing with strings are actively being researched, 
 giving rise to lan-guages such as CLP(
 ©
 ). The problem that string Prolog must 
 grapple with is
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"9.1 
  
 9.2 
  
 9.3
  
 9.4
  
 9.5 
  
 9.6
  
 9.7
  
 9.8
  
 In Icon, is the expression
  tab(4
 |
  3)
  equivalent to
  tab(4)
 |
  tab(3)
 ? Write a 
 regular expression that matches either
  ""begin""
  or
  ""end""
 .
  
 Write a regular expression that matches any word starting with
  ""pre"" 
 and 
 ending with
  ""ion""
 .
  
 Modify the Icon program of example 21 on page 279 so that the final word in
  
 MyString
  may continue to the end of
  MyString
  without a final space character.
  
 What is the Icon equivalent of SNOBOL’s
  fence
  pattern?
  
 In dBASE, it is quite awkward to generate a list of all first names com-bined 
 with all last names in the
  People
  relation. Suggest how to man-age such a feat.
  
 Design an SQL expression that builds a relation containing the first name of 
 everyone born before all earthquakes in San Francisco.
  
 Write a SNOBOL pattern that prints all contiguous substrings of the subject and 
 then fails.",NA
Challenge Exercises,"9.9
  
 Referring to Figure 9.5 (page 271), design a variety of
  CharSearch
  that finds 
 the second
  s
  in
  ""sample string""
 .
  
 9.10
  Write a regular expression that matches all words that can be typed by 
  
 alternating hands on a standard qwerty keyboard.
  
 9.11
  Refer to Figure 9.11 (page 274), and suggest a better component type 
  
 than
  Boolean
  for
  Present
 .
  
 9.12
  Write a SNOBOL program that has the same effect as the Icon program 
  
 in 
 Figure 9.20 (page 279).
  
 9.13
  Modify the Icon program of Figure 9.21 (page 279) so that it writes all words 
 that contain telephone numbers, that is, sequences of only digits and an 
 obligatory single hyphen.
  
 9.14
  The simple program for
  MatchDouble
  in Figure 9.23 (page 281) becomes more 
 complex if it doesn’t use concatenation. Show how to code it, using neither 
 concatenation nor explicit reference to
  pos
 .
  
 9.15
  Write an Icon program that generates all binary trees on
  n
  nodes, simi-
  
 lar to 
 the ones written in C and CLU in Chapter 2.
  
 9.16
  Why is it impossible to write an Icon program that solves the binary-
  
 tree equality puzzle of Chapter 2?
  
 9.17
  Can Icon iterator expressions and iterator procedures be implemented 
  
 with 
 a single stack?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Chapter 10 ,NA,NA
O,NA,NA
Formal Syntax and Semantics,Figure 10.1,NA
1,N,NA
 SYNTAX,"A programming language is defined by specifying its
  syntax
  (structure) and its
  
 semantics
  (meaning). Syntax normally means context-free syntax be-cause of the 
 almost universal use of context-free grammars as a syntax-
  
 specification mechanism.
  
 Syntax defines what sequences of symbols are
  
 valid; syntactic validity is independent of any notion of what the symbols
  
 mean. For example, a context-free syntax might say that
  A := B + C
  is syntac-
  
 tically valid, while
  A := B +
 ; is not.
  
 Context-free grammars are described by
  productions
  in
  BNF
  (Backus-Naur 
 Form, or Backus Normal Form, named after John Backus and Peter
  
 Naur, major designers of Algol-60). For example, part of the syntax of Pascal
  
 is shown in Figure 10.1.
  
 Program ::=
  program
  IDENTIFIER ( FileList ) ;
  
 1
  
 Declarations
  begin
  Statements
  end
  .
  
 2
  
 FileList ::= IDENTIFIER | IDENTIFIER , FileList
  
 3
  
 Declarations ::= ConstantDecs TypeDecs VarDecs ProcDecs
  
 4
  
 ConstantDecs ::=
  const
  ConstantDecList |
  
  
 5
  
 ConstantDecList ::= IDENTIFIER = Value; ConstantDecList |
  
  
 6
  
 The identifiers on the left-hand sides of the rules are called
  nonterminals
 . Each rule 
 shows how such a nonterminal can be expanded into a collection of nonterminals 
 (which require further expansion) and
  terminals
 , which are lexical tokens of the 
 programming language. In our example,
  program
  and 
 IDENTIFIER
  are terminals, and
  
 Program
  and
  Statements
  are nonterminals. I use
  |
  to indicate alternatives and
  
  to 
 indicate an empty string.
  
 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
  
 On-line edition copyright
  
  1996 by Addison-Wesley Publishing Company. Permission is granted to 
 print or photocopy this document for a fee of $0.02 per page, per copy, payable to Addi-son-Wesley 
 Publishing Company. All other rights reserved.
  
 307",NA
2,N,NA
 AXIOMATIC SEMANTICS,"Semantics
  are used to specify what a program does (that is, what it com-putes). 
 These semantics are often specified very informally in a language manual or report. 
 Alternatively, a more formal
  operational semantics
  in-terpreter model can be 
 used. In such a model, a program state is defined, and program execution is 
 described in terms of changes to that state. For exam-ple, the semantics of the 
 statement
  A := 1
  is that the state component corre-sponding to
  A
  is changed to
  1
 . The 
 LISP interpreter presented in Chapter 4 is operational in form. It defines the 
 execution of a LISP program in terms of the steps needed to convert it to a final 
 reduced form, which is deemed the re-sult of the computation. The Vienna 
 Definition Language (VDL) embodies an operational model in which abstract trees 
 are traversed and decorated to model program execution [Wegner 72]. VDL has 
 been used to define the se-mantics of PL/I, although the resulting definition is quite 
 large and verbose.
  
 Axiomatic semantics
  model execution at a more abstract level than op-
 erational models [Gries 81]. The definitions are based on formally specified 
 predicates that relate program variables. Statements are defined by how they 
 modify these relations.
  
 As an example of axiomatic definitions, the axiom defining
  var := exp
  usu-ally 
 states that a predicate involving
  var
  is true after statement execution if and only if 
 the predicate obtained by replacing all occurrences of
  var
  by
  exp
  is true beforehand. 
 For example, for
  y > 3
  to be true after execution of the state-ment
  y := x + 1
 , the 
 predicate
  x + 1 > 3
  would have to be true before the state-ment is executed.
  
 Similarly,
  y = 21
  is true after execution of
  x := 1
  if
  y = 21
  is true before its 
 execution, which is a roundabout way of saying that changing
  x
  doesn’t affect 
 y
 . 
 However, if
  x
  is an alias for
  y
  (for example, if
  x
  is a formal reference-mode parameter 
 bound to an actual parameter
  y
 ), the axiom is invalid. In fact, aliasing makes 
 axiomatic definitions much more complex. This is one reason why attempts to limit 
 or ban aliasing are now common in modern language designs (for example, Euclid 
 and Ada).
  
 The axiomatic approach is good for deriving proofs of program correctness, 
 because it avoids implementation details and concentrates on how relations among 
 variables are changed by statement execution. In the assignment ax-iom, there is no 
 concept of a location in memory being updated; rather, rela-tions among variables 
 are transformed by the assignment. Although axioms can formalize important 
 properties of the semantics of a programming lan-guage, it is difficult to use them to 
 define a language completely. For exam-ple, they cannot easily model stack overflow 
 or garbage collection.
  
 Denotational semantics
  is more mathematical in form than operational 
 semantics, yet it still presents the notions of memory access and update that are 
 central to von Neumann languages. Because they rely upon notation and
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.1 Axioms  1 2 1 2,"The programming language designer can specify the meaning of control structures by 
 stating axioms, such as the axiom of assignment in Figure 10.4.
  
 Figure 10.4
  
 Axiom of assignment 
  
 {P[x
  
  y]} x := y {P}
  
 1
  
 2
  
 where
  
 3
  
 x
  
 is an identifier
  
 4
  
 y
  
 is an expression without side effects, possibly containing x
  
 5
  
 This notation says that to prove
  P
  after the assignment, one must first prove a 
 related predicate.
  P[x
  
  y]
  means the predicate
  P
  with all references to
  x 
 replaced by 
 references to
  y
 . For instance,
  
 {y < 3
  
  z < y} x := y {x < 3
  
  z < y}
  
 is a consequence of this axiom.
  
 In addition to axioms, axiomatic semantics contain
  rules of inference
 , which 
 specify how to combine axioms to create provable propositions. They have the form:
  
 if
  X
  and
  Y
  then
  Z
  
 That is, if one already knows
  X
  and
  Y
 , then proposition
  Z
  is proven as well. Figure 
 10.5 shows some obvious rules of inference.
  
 Figure 10.5
  
 Rules of consequence 
  
 if
  {P} S {R}
  and
  R
  
  Q
  then
  {P} S {Q}
  
 1
  
 2
  
 if
  {P} S {R}
  and
  Q
  
  P
  then
  {Q} S {R}
  
 3
  
 Since
  R
  
  S
  means “
 R
  is stronger than
  S
 ,” the rules of consequence say that one may 
 always weaken a postcondition or strengthen a precondition. In other words, one 
 may weaken a proposition that is already proven.
  
  
 The easiest control structure to say anything about is the composition of two 
 statements, as in Figure 10.6.
  
 Figure 10.6
  
 Axiom of composition
  
 1
  
 if
  {P} S
  {Q}
  and
  {Q} S
  {R}
  then
  {P} S
 ; S
  {R}
  
 2
  
  
 Iteration with a
  while
  loop is also easy to describe, as shown in Figure 10.7.
  
 Figure 10.7
  
 Axiom of iteration 
  
 if
  {P
  
  B} S {P}
  then
  
 1
  
 2
  
 {P}
  while
  B
  do
  S
  end
  {
 
  B
  
  P}
  
 3
  
 That is, to prove that after the loop
  B
  will be false and that
  P
  still holds, it suf-fices to 
 show that each iteration through the loop preserves
  P
 , given that
  B
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.2 A Simple Proof ,"I will now use these axioms to prove a simple program correct. The program of 
 Figure 10.9 is intended to find the quotient and remainder obtained by di-viding a 
 dividend by a divisor. It is not very efficient.
  
 Figure 10.9
  
 remainder := dividend;
  
 1
  
 quotient := 0;
  
 2
  
 while
  divisor
  
  remainder
  do
  
 3
  
 remainder := remainder - divisor;
  
 4
  
 quotient := quotient + 1
  
 5
  
 end
 ;
  
 6
  
 I would like the predicate shown in Figure 10.10 to be true at the end of this 
 program.
  
 Figure 10.10
  
 {FINAL: remainder < divisor
  
  
 1
  
 dividend = remainder + (divisor * quotient)}
  
 2
  
 The proposition I must prove is
  {true} Divide {FINAL}
 . Figure 10.11 pre-sents a proof.
  
 Figure 10.11
  
 true
  
  dividend = dividend + divisor * 0 [algebra]
  
 1
  
 {dividend = dividend + divisor*0} remainder := dividend 
  
 {dividend = remainder + divisor*0} [assignment] 
  
 2
  
 {dividend = remainder + divisor*0} quotient := 0 
  
 {dividend = remainder + divisor*quotient} [assignment] 
  
 3
  
 {true} remainder := dividend {dividend = remainder+divisor*0} [consequence, 1, 2] 
  
 4
  
 {true} remainder := dividend; quotient := 0 
  
 {dividend = remainder+divisor*quotient} 
  
 [composition, 3, 4] 
  
 5
  
 dividend = remainder+divisor*quotient
  
  divisor
  
  remainder
  
 dividend=(remainder-
 divisor)+divisor*(1+quotient) 
  
 [algebra] 
  
 6
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
2.3 Weakest Preconditions  1 2 1 2,"The suggestion that a program can itself be developed by attention to the ax-iomatic 
 meaning of language constructs and that programmers should de-velop their 
 programs backward was elucidated by Edsger W. Dijkstra [Dijkstra 75]. Instead of 
 seeing the axioms as static relations between pre-conditions and postconditions, 
 Dijkstra introduced the concept of weakest precondition. I will say that
  P = wp(S, Q)
  if 
 the following statements hold:
  
 •
  {P} S {Q}
 . That is,
  P
  is a precondition to
  S
 .
  
 •
  S
  is guaranteed to terminate, given
  P
 . That is,
  S
  shows
  total correct-
  
 ness
 , 
 not just conditional correctness.
  
 • If
  {R} S {Q}
 , then
  R
  
  P
 . That is,
  P
  is the weakest precondition, so
  {P} S 
  
 {Q}
  is 
 the strongest proposition that can be made given
  S
  and
  Q
 .
  
 Weakest preconditions satisfy several properties:
  
 1. 
  
 2. 
  
 3.
  
 For any statement
  S
 ,
  wp(S, false) = false
  (law of the excluded mira-cle).
  
 If
  P
  
  Q
 , then
  wp(S, P)
  
  wp(S, Q)
  (related to the rules of consequence). 
 wp(S, 
 P)
  
  wp(S, Q) = wp(S, P
  
  Q)
  (again, related to rules of conse-quence).
  
  
 The axioms shown earlier can be restated in terms of
  wp
 , as shown in Fig-ure 
 10.14.
  
 Figure 10.14
  
 Empty statement
  
 1
  
 wp(
 skip
 , R) = R
  
 2
  
 Assignment statement 
  
 wp(x := y, R) = R[x
  
  y]
  
 3
  
 4
  
 5
  
 Composition 
  
 wp(S
 ,
  S
 ) = wp(S
 , wp(S
 ))
  
 6
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3,N,NA
 DENOTATIONAL SEMANTICS ,"The study of denotational semantics was pioneered by Dana Scott and Christopher 
 Strachey of Oxford University, although many individuals have
  
 contributed to its development.
  
 A denotational definition is composed of
  
 three components: a syntactic domain, a semantic domain, and a number of 
 semantic functions.
  Semantic functions
  map elementary syntactic objects (for 
 example, numerals or identifiers) directly to their semantic values (inte-gers, files, 
 memory configurations, and so forth). Syntactic structures are de-fined in terms of 
 the composition of the meanings of their syntactic constituents. This method 
 represents a structured definitional mechanism in which the meaning of a 
 composite structure is a function of the meaning of progressively simpler 
 constituents. As you might guess, unstructured lan-guage features (most notably
  
 goto
 s) are less easily modeled in a denotational framework than structured 
 features.
  
 The
  syntactic domain
  contains the elementary tokens of a language as well as 
 an abstract syntax. The syntax specified by a conventional context-free grammar is 
 termed a
  concrete syntax
  because it specifies the exact syntactic structure of 
 programs as well as their phrase structure. That is, a concrete syntax resolves issues 
 of grouping, operator associativity, and so forth. An
  abstract syntax
  is used to 
 categorize the kinds of syntactic struc-tures that exist. It need not worry about exact 
 details of program representa-tion or how substructures interact; these issues are 
 handled by the concrete syntax. Thus, in an abstract syntax, an
  if
  statement might be 
 represented by
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.1 Domain Definitions ,"Denotational semantics is careful to specify the exact domains on which se-mantic 
 functions are defined. This specification is essential to guarantee that only valid 
 programs are ascribed a meaning. I will use the term
  domain
  to mean a set of values 
 constructed (or defined) in one of the ways discussed be-low. This careful approach 
 allows me to talk about actual sets and functions as the denotations of syntactic 
 objects while avoiding the paradoxes of set theory.
  
 I will always begin with a set of
  basic domains
 . For a simple program-ming 
 language, basic syntactic domains might include:
  Op
 , the finite domain of operators;
  
 Id
 , the identifiers; and
  Numeral
 , the numerals. Basic semantic domains include
  N
 , the 
 natural numbers, and
  Bool
 , the domain of truth val-ues. I can also define finite basic 
 domains by enumeration (that is, by simply listing the elements). For example the 
 finite domain
  {true, false}
  defines the basic semantic domain of Boolean values. I 
 assume the basic domains are familiar objects whose properties are well 
 understood.
  
 New domains can be defined by applying
  domain constructors
  to exist-ing 
 domains. I will show three domain constructors corresponding to Carte-sian 
 product, disjoint union, and functions. For each constructor, I will show an ML 
 equivalent. All the denotational semantic specifications I will show can be coded 
 (and tested) in ML (discussed in Chapter 3).",NA
3.2 Product Domains  1 2 1 2 D 1 2 1  D 1 2 2,"Given domains D
 1
  and D
 2
 , their
  product domain
 , D
  =
  D
 1
  
  D
 2
 , consists of or-dered 
 pairs of elements of the component domains. That is,
  
 x
   
  D
  
  D
  
  x
  =
  < x
 , x
  >
  
 where x
 1
   
  D
 1
  and x
 2
   
  D
 2
 .
  
 Product domain
  D
  provides two selector functions, Hd
 D
  (the head of a tu-ple), 
 and Tl
 D
  (the tail). These behave in a fairly natural way, as shown in Figure 10.24.
  
 Figure 10.24
  
 Hd
 (< x
 , x
  >)
  =
  x
  
 Tl
 (< x
 , x
  >)
  =
  x
  
 1
  
 2
  
 Again, x
 1
  
 D
 1
 , and x
 2
  
 D
 2
 . I will rarely need to mention these functions ex-plicitly.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.3 Disjoint-Union Domains  1 1 1 2 2 2 1 1 1 2 2 2 1 1 2 2 1 1 2 2 1 2;,"Let D
 1
  and D
 2
  be domains. Their
  disjoint union
 , D
 1
  
  D
 2
 , consists of ele-ments of 
 either D
 1
  or D
 2
 , where each value carries with it an indication of which domain it 
 came from. Formally, the elements of D
  =
  D
 1
  
  D
 2
  are
  
 { < 1, x
  > | x
  
 D
 }
  
  { < 2, x
  > | x
  
 D
 } .
  
 Disjoint-union domain
  D
  provides two injection functions, InD
 1
  and InD
 2
 , as in 
 Figure 10.25.
  
 Figure 10.25
  
 InD
 (x
 )
  =
  < 1, x
  > 
  
 InD
 (x
 )
  =
  < 2, x
  >
  
 1
  
 2
  
 As usual, x
 1
   
  D
 1
 , and x
 2
   
  D
 2
 .
  
 This form of disjoint union may seem unnecessarily complicated, but it has the 
 advantage that the meaning of D
 1
  
  D
 2
  is independent of whether D
 1 
 and D
 2
  are 
 disjoint. For example, such obvious properties as
  
  
 x
   
  D
   
 x
   
  D
  . InD
 (x
 )
  
  InD
 (x
 )
  
 remain true even if D
 1
  =
  D
 2
 .
  
  
 The ML equivalent of a disjoint union is a
  datatype
 . That is, if D
 1
  and D
 2 
 are ML 
 types, then Figure 10.26 shows the disjoint-union type D
  =
  D
 1
 
 D
 2
 .
  
 Figure 10.26
  
 datatype
  D
  
 1
  
 = FirstComponent
  of
  D
  
 2
  
 | SecondComponent
  of
  D
  
 3
  
 The ML notation allows me to introduce names for the two components, which will 
 be helpful in testing from which underlying domain a member of the disjoint-union 
 domain comes.",NA
3.4 Function Domains ,"Given domains D
 1
  and D
 2
 , their
  function domain
  D
 1
  
  D
 2
  is a set of func-tions 
 mapping elements of D
 1
  to elements of D
 2
 . 
  
 For technical reasons, 
 D
 1
  
  D
 2
  means not all functions from D
 1
  to D
 2
 , but rather a subset of them, called the 
 continuous ones. Every computable function (hence every function I will need) is 
 continuous. If f
   
  D
 1
  
  D
 2
  and x
 1
  
 D
 1
 , the application of
  f
  to x
 1
 , written f(x
 1
 ) or f x
 1
 , is 
 an element of D
 2
 .
  
 There are several ways to package multiple parameters to a function. Just as in 
 ML, they can be packaged into a single tuple or curried. Function values can be 
 parameters or returned results, just like values of any other
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.5 Domain Equations  1 1  k k,"I will define a collection of domains D
 1
 , . . . , D
 k
  by a system of formal equa-tions, as in 
 Figure 10.27.
  
 Figure 10.27
  
 D
  =
  rhs
  
  
 ...
  
 1
  
 2
  
 D
  =
  rhs
  
 3
  
 Each right-hand side rhs
 i
  is a domain expression, built from basic domains (and 
 possibly from some of the D
 i
  themselves) using the domain constructors given 
 above.
  
 For technical reasons, it is important that I not treat these formal equa-tions as 
 meaning strict equality. Instead, I use a somewhat more liberal in-terpretation. I say 
 that domains D
 1
 , . . . , D
 k
  comprise a solution to the above system of domain 
 equations if, for each i, D
 i
  is isomorphic to the domain de-noted by rhs
 i
 ; that is, there 
 exists a one-to-one, onto function between them.
  
 While I have not shown that this liberal interpretation of domain equa-tions is 
 technically necessary, you can certainly appreciate its convenience.
  
 Consider the single equation:
  
 D = N
  
  Bool .
  
 Intuitively, the set N
 
 Bool has all the properties required of a solution to this 
 equation. The right-hand side of this equation denotes
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.6 Nonrecursive Definitions  17 11 11 12 1 1  k k,"I need to introduce just a bit more terminology. In a system of domain equa-tions, 
 each right-hand side is a domain expression, consisting of applications of domain 
 constructors to basic domains and possibly to some of the domains D
 i
  being defined 
 by the system of equations. A right-hand side that uses no D
 i
 , that is, one that 
 consists entirely of applications of domain constructors to basic domains, is
  closed
 . 
 A right-hand side
  rhs
  that is not closed has at least one use of a D
 i
 ; I will say that D
 i
  
 occurs free
  in
  rhs
 . For example, in
  
 D
  =
  D
  
  (D
  
  N)
  
  (D
  
  N)
  
 rhs
 17
  has two free occurrences of the name D
 11
  and one free occurrence of the name 
 D
 12
 ; no other names occur free in rhs
 17
 .
  
  
 A system
  S
  of domain equations is nonrecursive if it can be ordered as in Figure 
 10.28,
  
 Figure 10.28
  
 D
  =
  rhs
  
  
 ...
  
 1
  
 2
  
 D
  =
  rhs
  
 3
  
 where only the names D
 1
 , . . . , D
 i
  
 1
  are allowed to appear free in rhs
 i
 . In par-ticular, 
 this definition implies that rhs
 1
  is closed.
  
 A solution to a nonrecursive system of domain equations
  S
  can be found easily by 
 a process of repeated back substitution, as follows. Begin with the system
  S
 , in which 
 rhs
 1
  is closed. Build a new system S
 2
 from
  S
  by substitut-ing rhs
 1
  for every 
 occurrence of the name D
 1
  in the right-hand sides of S.
  
 You should convince yourself of the following:
  
 1. 
  
 2.
  
 3.
  
 S
 2
 has no free occurrences of D
 1
 .
  
 S
 2
 is equivalent to S in the sense that every solution to S
 2
 is a solution to S, and 
 conversely.
  
 Both rhs
 2 1
 and rhs
 2 2
 are closed.
  
  
 Now build system S
 3
 from S
 2
 by substituting rhs
 2 2
 for every occurrence of D
 2
  in 
 the right-hand sides of S
 2
 . Just as above, the following hold:
  
 1. 
  
 2. 
  
 3.
  
 S
 3
 has no free occurrences of D
 1
  or D
 2
 . S
 3
 is 
 equivalent to S
 2
 (and hence to S).
  
 All of rhs
 3 1
 , rhs
 3 2
 , and rhs
 3 3
 are closed.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.7 Recursive Definitions ,"A system of domain equations is recursive if no matter how it is ordered there is at 
 least one
  i
  such that rhs
 i
  contains a free occurrence of D
 j
  for some j
  
  i. That is, a 
 system is recursive if it cannot be reordered to eliminate for-ward references. 
 Intuitively, such a system is an inherently circular defini-tion.
  
  
 BNF definitions for syntax can be recursive as well. 
  
 The context-free 
 grammar descriptions of typical programming languages routinely contain re-
 cursive production rules like:
  
 Expr
  
  Expr op Expr
  
 Intuitively, this rule states that an expression can be built by applying an op-erator 
 to two subexpressions. A recursive collection of grammar rules defines the set of all 
 objects that can be constructed by finitely many applications of the rules. Such 
 recursive rules are indispensable; they are the only way a fi-nite set of context-free 
 production rules can describe the infinite set of all valid programs. Similarly, if you 
 try to define semantics with only nonrecur-sive domain equations, you will soon 
 discover they are not powerful enough.
  
 Unfortunately, interpreting a recursive system of domain equations can be 
 subtle. In an ML representation of domain equations, I will just declare the 
 equations with the
  rec
  modifier, so that they can depend on each other. I will ignore 
 any problems that circularity might raise. But consider the innocu-ous-looking 
 equation of Figure 10.32.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.8 Expressions  1 2 1 2 1 2 1 2 1 2 1 2,"Now that I have discussed domains, I can begin to create richer and more re-alistic 
 semantic functions. I first extend my definition of binary literals to in-clude infix 
 operators; see Figure 10.34.
  
 Figure 10.34
  
 Abstract syntax
  
 1
  
 2
  
 T
   
  Exp
  
 3
  
 T
  
  T + T
  
 T
  
  T - T
  
 4
  
 T
  
  T * T
  
 5
  
 T
  
  Seq
  
 6
  
 Seq
  
  0 | 1 | Seq 0 | Seq 1
  
 7
  
 8
  
 Semantic domain
  
 9
  
 N = {0,1,2, ..., -1, -2, ...}
  
 Semantic function
  
 10
  
 11
  
 E: Exp
  
  N
  
 12
  
 E[0] = 0
  
 E[1] = 1
  
 13
  
 E[Seq 0] = 2
  
  E[Seq]
  
 14
  
 E[Seq 1] = 2
  
  E[Seq] + 1
  
 15
  
 16
  
 E[T
  +
  T
 ]
  =
  E[T
 ]
  +
  E[T
 ] 
  
 E[T
   
  T
 ]
  =
  E[T
 ]
   
  E[T
 ] 
  
 E[T
  * T
 ]
  =
  E[T
 ]
  
  E[T
 ]
  
 17
  
 18
  
 This example can be specified in ML as shown in Figure 10.35.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.9 Identifiers ,"I can now introduce predeclared identifiers, including
  true
  and
  false
 ,
  max-int
 ,
  minint
 , 
 and so forth. Let Id be the syntactic domain of identifiers, and let 
  
 L 
  
 be a 
  
 semantic 
  
 lookup function 
  
 such 
  
 that 
  
 L: Id
  
  V, 
  
 where 
 V
  =
  N
  
  Bool
  
  {udef}. That is, L returns an integer or Boolean value, or 
 udef
  if the 
 identifier is undefined. The additions needed for Figure 10.40 are given in Figure 
 10.41.
  
 Figure 10.41 
  
 Abstract syntax 
 1
  
 I
   
  Id 2
  
 T
  
  I 
  
 3
  
 Semantic domains 
  
 4
  
 V
  =
  N
  
  Bool
  
  {udef} 
  
 5
  
 Semantic functions 
 6
  
 L: Id
  
  V 7
  
 E[I]
  =
  L[I]?{udef}
   
 , L[I] 
  
 8
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.10 Environments  0 0,"The next step is to introduce programmer-defined named constants. This step 
 requires the concept of an environment that is updated when declara-tions are 
 made. An
  environment
  is a function that maps identifiers (drawn from the syntactic 
 domain) into results. I will denote the domain of environ-ments as U, where U
  =
  Id
  
  
 V and V
  =
  N
  
  Bool
  
  {udef}
  
  {
 
 }, as in Fig-ure 10.42. If u
   
  U and I
   
  Id, then
  u[I]
  is 
 an integer, Boolean,
  udef
 , or
  
 , depending on how and whether
  I
  has been declared. I 
 can incorporate the definition of predeclared named constants by including them in 
 u
 0
 , a prede-fined environment. I no longer need the lookup function L.
  
 Figure 10.42
  
 Semantic domain
  
 1
  
 2
  
 V
  =
  N
  
  Bool
  
  {udef}
  
  {
 
 }
  
 U
  =
  Id
  
  V
  
 3
  
 4
  
 Semantic functions
  
 5
  
 E[I]
  =
  u
 [I]?{udef}
   
 , u
 [I]
  
 The environment approach is useful because environments can be computed as the 
 results of semantic functions (those that define the meaning of a local constant 
 declaration).
  
 It is time to expand my abstract syntax for a program into a sequence of 
 declarations followed by an expression that yields the result of a program. I can 
 specify whatever I like for the meaning of a redefinition of an identifier. In Figure 
 10.43, redefinitions will have no effect.
  
 I will introduce two new semantic functions: D, which defines the semantic effect 
 of declarations, and M, which defines the meaning of a program. D is curried; it maps 
 a declaration and an old environment into a new environ-ment in two steps. There 
 is a major change to E; it now maps an expression and an environment into a result. 
 Pr is the syntactic domain of all programs; Decls is the syntactic domain of 
 declarations.
  
 Figure 10.43
  
 Abstract syntax
  
 1
  
 2
  
 P
   
  Pr -- a program
  
 T
   
  Exp -- an expression
  
 3
  
 I
   
  Id -- an identifier
  
 4
  
 Def
   
  Decls -- a declaration
  
 5
  
 6
  
 P
  
  Def T
  
 Def
   
  -- empty declaration
  
 7
  
 Def
  
  I = T ; -- constant declaration
  
 8
  
 Def
  
  Def Def -- declaration list
  
 9
  
 T
  
  I -- identifier expression
  
 10
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.11 Variables ,"I can model variables in several ways. The most general model employs an 
 environment that maps identifiers to locations and a store that maps loca-tions to 
 values. This is how most languages are implemented, and it would allow me to 
 model aliasing, reuse of storage, and so forth.
  
 For the present, I’ll use a simpler interpreter model and continue to use the 
 environment function to map identifiers directly to values. I will also store a flag 
 that indicates if a value can be changed (that is, if it’s an L-value, not an R-value). An 
 interpreter does roughly the same thing, maintaining a runtime symbol table for all 
 program variables. From the semantic point of view, the distinction between 
 interpreters and compilers is irrelevant—what is important is what the answer is, 
 not how it’s produced. The interpreter ap-proach will allow interesting variations. 
 For example, an untyped language (like Smalltalk) is just as easy to model as a 
 strongly typed language.
  
  
 I begin by extending the environment domain U as in Figure 10.45 to in-clude an 
 indication of how an identifier can be used:
  
 U
  =
  Id
  
  {var, const, uninit}
  
  V
  
 Uninit
  models the fact that after a variable is declared, it may be assigned to, but not 
 yet used. After a variable is assigned a value, its flag changes from 
 uninit
  to
  var
 . 
  
 It is time to introduce statements. 
  
 (In denotational for-
 malisms, statements are usually called commands.) A statement maps an en-
 vironment into a new environment (or
  
 ). That is,
  
 S: Stm
  
  U
  
  (U
  
  {
 
 })
  
 where S is the semantic function for statements, and Stm is the syntactic do-main of 
 statements.
  
 I will first add only variable declarations and assignment statements to the 
 programming language. Since there is no I/O, I will define the result of the program 
 to be the final value of an identifier that is mentioned in the pro-gram header, as in 
 Figure 10.45, which produces 1 as its result.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.12 Conditional and Iterative Statements ,"Conditional execution and iterative execution for a fixed number of iterations are 
 readily modeled with the additions to the previous definition shown in Figure 10.50.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.13 Procedures ,"I now consider simple procedures of the abstract form shown in Figure 10.55.
  
 Figure 10.55
  
 procedure
  I;
  
 1
  
 St
  
 2
  
 Procedures are invoked by a
  call
  statement (for example,
  call
  I
 ). Since there are no 
 scope rules yet, a procedure invocation is equivalent to macro substitution and 
 immediate execution of the procedure’s body. A procedure can call another 
 procedure, but I will forbid recursion for now. Since a proce-dure name is a 
 synonym for a list of statements, it represents a mapping from an environment to an 
 updated environment or to
  
 . The semantic domain for procedure declarations is 
 given in Figure 10.56.
  
 Figure 10.56 
  
 Proc
  =
  U
  
  (U
  
  {
 
 })
  
 I need to upgrade the environment domain to include procedures, as well as 
 introduce a new flag
  opencall
 . I will set
  opencall
  when a procedure call is in progress, 
 but not yet completed. To prevent recursion, I will disallow in-voking a procedure 
 that has
  opencall
  set. The environment domain U is now as shown in Figure 10.57.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.14 Functions ,"Functions, like procedures, execute a list of statements. They also return a value by 
 evaluating an expression immediately prior to return. For the pre-sent, I will 
 constrain functions to be nonrecursive. The abstract syntax of in-teger functions will 
 be as shown in Figure 10.59.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.15 Recursive Routines ,"The danger in allowing recursive routines is that the definitions may become 
 circular. As it stands, I define the meaning of a call in terms of the meaning of its 
 body. If recursion is allowed, the meaning of a routine’s body may itself be defined 
 in terms of any calls it contains. My current definition breaks this potential 
 circularity by forbidding calls of a routine (directly or indirectly) from its own body.
  
 I will generalize the definition of a subroutine call to allow calls of bounded 
 depth. The meaning of a routine with a maximum call depth of
  n 
 will be defined in 
 terms of the meaning of the subroutine’s body with subse-quent calls limited to a 
 depth of
  n
  
 1. The meaning of a call with a maximum depth of zero is
  
 .
  
  
 If a call to a routine will ever return, then it can be modeled by a call lim-ited to 
 depth
  n
  as long as
  n
  is sufficiently large. As
  n
  approaches
   
 , the bounded-call-depth 
 model converges to the unbounded-call model if the latter ever returns. But if a 
 routine call doesn’t ever return, then the bounded-call-depth model will always 
 produce an error result
  
 , which is a correct defini-tion of an infinite recursion. 
 Thus the limit as
  n
  approaches
   
  of the bounded-call-depth model is
  
 , which I will 
 take as the definition of the meaning of a call of unbounded depth that never 
 returns. This approach par-allels how I handled unbounded iteration, which isn’t 
 surprising, given the similarity of looping and subroutine call.
  
 I will redefine U to replace the
  opencall
  flag with an integer representing the 
 maximum depth to which a given procedure or function can be called. If this value is 
 zero, the call is invalid. What used to be
  opencall
  is now repre-sented by 0; the 
 previous model always had a maximum call depth of 1. Fig-ure 10.61 shows the 
 necessary additions.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.16 Modeling Memory and Files ,"I am now ready to model variables more accurately. I will use a finite seman-tic 
 domain
  Loc
  to name addressable memory locations. A semantic domain 
 Mem
  will 
 model memories as a mapping from
  Loc
  to an integer or Boolean value or to error 
 values
  uninitInt
 ,
  uninitBool
 ,
  unalloc
 :
  
 Mem = Loc
  
  N
  
  Bool
  
  uninitInt
  
  uninitBool
  
  unalloc
  
 The uninitialized flag will now be in the memory mapping, not the environ-ment 
 mapping. Two different
  uninit
  flags are used to remember the type an uninitialized 
 location is expected to hold. If a memory location is marked as 
 unalloc
 , then it can be 
 allocated for use (and possibly deallocated later). If m
   
  Mem, then I define
  alloc
  as 
 follows:
  
 alloc(m)
  =
  any
  l
   
  Loc such that m(
 l
 ) = unalloc 
  
  
 =
  
  if no such
  l
  exists
  
 Alloc
  specifies no particular memory allocation pattern; this definition allows 
 implementations the widest latitude in memory management.
  
  
 I will model files as finite sequences over integers, Booleans, and
  eof
 , the end-of-
 file flag. I define the semantic domain File as:
  
 File
  =
  (N
  
  Bool
  
  eof) *
  
 That is, a file is a potentially infinite string of typed values. My definitions will never 
 consider values in files following the first
  eof
 . Programs will now take an input file 
 and produce an output file (or
  
 ). To model this semantics, I will have a semantic 
 domain State that consists of a memory and a pair of files:
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.17 Blocks and Scoping  Loc,"I will now model block structure and name scoping by adding a
  begin-end 
 block to the 
 syntax, as in Figure 10.63.
  
 Figure 10.63 
  
 St
  
  begin
  Def St
  end
  
 As in most block-structured languages, declarations within a block are lo-cal to 
 it, and local redefinition of a nonlocal identifier is allowed. Rather than a single 
 environment, I will employ a sequence of environments, with the first environment 
 representing local declarations, and the last environment repre-senting the 
 outermost (predeclared) declarations. The new semantic domain UU
  =
  U
 *
  will 
 represent this sequence of environments. All definitions will be made in the head of 
 the environment sequence, while lookup will proceed through the sequence of 
 environments, using the functions
  Top
  and
  Find
 , shown in Figure 10.64.
  
 Figure 10.64
  
 Top: UU
  
  U 
  
 Top(u)
  =
  u?U
  
  u, Hd(u)
  
 1
  
 2
  
 3
  
 Find: UU
  
  Id
  
  V 
  
 Find(u)[I]
  =
  Top(u)[I]?{udef}
  
  
 4
  
 (u?U
   
 , Find(Tl(u))[I]), Top(u)[I]
  
 5
  
 Block structure introduces a memory-management issue. Most languages specify 
 that memory for local variables is created (or allocated) upon block en-try and 
 released upon block exit. To model allocation, I create a function
  Free 
 (Figure 10.65) 
 that records the set of free memory locations.
  
 Figure 10.65
  
 Free: Mem
  
  2
  
 1
  
 Free(m) = {
 l
  | m(
 l
 )=unalloc}
  
 2
  
 I will record free locations at block entry and reset them at block exit. Most 
 implementations do this by pushing and later popping locations from a run-time 
 stack. My definition, of course, does not require any particular imple-mentation.
  
 Figure 10.66 presents the definition of block structure, updating all defini-tions 
 that explicitly use environments so that they now use sequences of envi-ronments. I 
 also modify slightly the definition of the main program to put predeclared 
 identifiers in a scope outside that of the main program.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.18 Parameters ,"Now that I have scoping, I will turn my attention to procedures and func-tions. As 
 defined above, procedures and functions execute in the environ-ment of the call, not 
 the environment of definition. No environment is stored with a procedure or 
 function definition; rather, they use an environment pro-vided at the point of call. In 
 other words, I have provided dynamic scoping and shallow binding, which is 
 common in interpreted, but not in compiled, languages. I will now refine the model 
 to use the more common static model of scoping.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.19 Continuations ,"The denotational approach is very structured; the meaning of a construct is defined 
 in terms of a composition of the meanings of the construct’s con-stituents. The 
 meaning of a program can be viewed as a top-down traversal of an abstract syntax 
 tree from the root (the program nonterminal) to the leaves (identifiers, constants, 
 and so forth). The meanings associated with the leaves are then percolated back up 
 to the root, where the meaning of the whole program is determined.
  
 This structured approach has problems with statements such as
  break
  or 
 goto
  
 that don’t readily fit the composition model. Further, it forces values to percolate 
 throughout the whole tree, even if this action is unnecessary. Con-sider, for example, 
 a
  stop
  statement. When
  stop
  is executed, I would like to discontinue statement 
 evaluation and immediately return to the main pro-gram production, where the 
 final result (the output file) is produced. But I can’t; the meaning of
  stop
  must be 
 composed with that of the remaining statements (even though
  stop
  means one must 
 ignore the remaining state-ments!). As it stands, my definition of the meaning of a 
 statement sequence (see lines 90–91 in Figure 10.62, page 345) checks for error on 
 the first state-ment before evaluating the second. I could add another sort of value, 
 like
  
 , that indicates that execution should stop, even though there is no error. This 
 device would work but would be rather clumsy, as I would model
  stop
  not by 
 stopping but by continuing to traverse program statements while ignoring them.
  
 Continuations were invented to remedy these problems. A
  continuation 
 is a 
 function passed as a parameter to every semantic function. The semantic function 
 determines its value as usual and then calls (directly or indirectly) the continuation 
 with its value as a parameter. This approach is quite clever but is much less intuitive 
 than the structured approach I have presented so far.
  
  
 I will first consider expression continuations, which have a semantic do-main EC, 
 defined as:
  
 EC
  =
  (N
  
  Bool)
  
  State
  
  R
  
 The expression continuation takes a value and a state (since side effects in
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.20 Statement Continuations  1 2 1 2,"I am now ready to consider statement continuations, which are particularly useful 
 because they allow me to handle nonstructured control flows. I will first define
  SC
 , 
 the semantic domain of statement continuations (see Figure 10.74). I will also 
 slightly alter
  EC
 , the semantic domain of expression contin-uations. In both cases, 
 the continuations will return
  Ans
 , the domain of pro-gram answers, reflecting the 
 fact that expressions and statements are not executed in isolation, but rather in 
 contexts in which they contribute to the fi-nal answer to be computed by the whole 
 program.
  
 Figure 10.74
  
 Semantic domains
  
 1
  
 2
  
 Ans
  =
  File
  
  {
 
 }
  
 EC
  =
  (N
  
  Bool)
  
  State
  
  Ans
  
 3
  
 SC
  =
  State
  
  Ans
  
 4
  
 Statement continuations take only one parameter because the only program 
 component updated by a statement is the state. Figure 10.75 extends the
  S 
 semantic 
 function to include a statement continuation parameter. All seman-tic functions now 
 return
  Ans
  because they all execute by evaluating (directly or indirectly) some 
 continuation function. The values that change during the computation of a semantic 
 function (a result, environment, or state) are now parameters to a continuation 
 function.
  
 Figure 10.75
  
 Semantic functions
  
 1
  
 2
  
 E: Exp
  
  UU
  
  State
  
  EC
  
  Ans S: Stm
  
  
 UU
  
  State
  
  SC
  
  Ans
  
 3
  
 To see the utility of statement continuations, consider the definition of state-ment 
 composition in Figure 10.76.
  
 Figure 10.76
  
 S[St
  St
 ] u s c
  =
  S[St
 ] u s c
 
  
 where
  c
 
 (s
 
 )
  =
  S[St
 ] u s
 
 c.
  
 1
  
 2
  
 The statement continuation has a fairly intuitive interpretation: what to exe-cute 
 after the current statement. The advantage of the continuation ap-
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.21 Declaration Continuations ,"A declaration continuation will map an environment and state into an an-swer. The
  
 D
  function will now take a declaration continuation from the do-main
  DC
 , as in 
 Figure 10.78.
  
 Figure 10.78
  
 DC
  =
  UU
  
  State
  
  Ans 
  
 D: Decls
  
  UU
  
  State
  
  DC
  
  Ans
  
 1
  
 2
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
"3.22 Procedures, Functions, and ",NA,NA
Parameters  1 2 1 2,"I now define routines and parameters in the new continuation notation. First, 
 declarations need to be handled, using
  D
 ,
  DC
 ,
  FP
 , and
  FC
  (formal param-eter 
 continuation), as shown in Figure 10.80.
  
 Figure 10.80
  
 FC
  =
  Parms
  
  Ans
  
 1
  
 2
  
 FP: Fparms
  
  Parms
  
  FC
  
  Ans
  
 3
  
 FP[I : integer] p f
  =
  f(append(p, << 0, I > , eol >))
  
 FP[I : Boolean] p f
  =
  f(append(p, << false, I > , eol >))
  
 4
  
 FP[
 
 ] p f
  =
  f(p)
  
 5
  
 FP[Formals
  Formals
 ] p f
  =
  FP[Formals
 ] p f
 
  
 where
  
 f
 
 (p
 
 )
  =
  FP[Formals
 ]p
 
 f.
  
 6
  
 7
  
 Procedures are generalizations of statements, and, like all statements, take a 
 statement continuation as a parameter. This continuation is essentially the return 
 point of the procedure; see Figure 10.81.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.23 Flow of Control  1 2 1 2 m 0 i + 1 i i i     0 i + 1 i + 1 i + 1 i,"Now that I have the machinery of continuations in place, I can illustrate how to 
 implement statements that alter the flow of control. I begin with the
  stop 
 statement, 
 which forces immediate termination of execution. Figure 10.85 shows the semantic 
 function.
  
 Figure 10.85 
  
 S[
 stop
 ] u s c
  =
  Tl(Tl(s))
  
 Stop
  returns the output file component of the current state. It avoids the nor-mal 
 flow of control by ignoring its continuation parameter.
  
 A more interesting illustration is
  break
 , which I will use to exit any of the 
 structured statements in the language (
 if
 ,
  do
 ,
  while
 ,
  begin-end
 ). I will let any of these 
 statements be optionally labeled with an identifier, which will follow normal 
 scoping rules. I define
  break
  I
  to cause execution to immedi-ately break out of the 
 structure labeled with
  I
  and then to continue execution with the normal successor to 
 the labeled statement. If
  I
  isn’t declared as a la-bel in the scope of the break, the 
 statement produces an error value.
  
  
 I extend
  V
 , the domain of environment contents, to include statement con-
 tinuations:
  
 U
  =
  Id
  
  V 
  
 V
  =
  N
  
  Bool
  
  Loc
  
  Proc
  
  Func
  
  SC
  
  {
 
 }
 
 {udef}
 
 {redef}
  
 The meaning of a label on a structured statement will be the continuation as-
 sociated with that statement. Figure 10.86 adds definitions for structured 
 statements with labels (the definitions for unlabeled statements are, of course, 
 retained).
  
 Figure 10.86
  
 S[I:
  if
  T
  then
 St
  else
  St
 ] u s c
  =
  E[T] uu s k 
  
 where
  k(r, t)
  =
  r?Bool
  
  (r
  
  S[St
 ] uu t c, S[St
 ] uu t c),
  
 ; uu
  =
  
 Hd(u)[I]?{udef}
  
  u[Hd[I
  
  c
 
 ]], u[Hd[I
  
  redef]]; c
 
 (t
 
 )
  =
  
 c(t
 
 [Hd[Free(Hd(s))
  
  unalloc]]).
  
 S[I:
  do
  T
  times
  St] u s c
  =
  E[T] uu s k
  
 1
  
 2
  
 3
  
 4
  
 5
  
 where
  k(r, t)
  =
  r?N
  
  v
 (t),
  
 ; 
  
 m
  =
  max(0, t); v
 (s
 
 )
  =
  c(s
 
 ); 
  
 v
 (s
 
 )
  =
  S[St] uu s
 
  v
 ; 
  
 uu
  =
  Hd(u)[I]?{udef}
  
  u[Hd[I
  
  c
 
 ]], u[Hd[I
  
  redef]]; c
 
 (t
 
 )
  =
  
 c(t
 
 [Hd[Free(Hd(s))
  
  unalloc]]).
  
 S[I:
  while
  T
  do
  St] u s c
  = 
 lim p
 (s) 
  
  
  
  
  
  
 where
  p
 (s
 
 )
  = 
 ; p
 (s
 
 )
  =
  E[T] uu s
 
  k
 ; 
  
  
  
 k
 (r, t)
  =
  r?Bool
  
  (r
  
  S[St] uu t p
 , c(t)),
  
 ; 
  
  
  
 uu
  =
  Hd(u)[I]?{udef}
  
  u[Hd[I
  
  c
 
 ]], u[Hd[I
  
  redef]]; 
  
 c
 
 (t
 
 )
  =
  c(t
 
 [Hd[Free(Hd(s))
  
  unalloc]]);
  
 6
  
 7
  
 8
  
 9
  
 10
  
 11
  
 12
  
 13
  
 14
  
 15
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
3.24 Summary of Syntactic and Semantic ,NA,NA
Domains and Semantic Functions ,"The domains used in the denotational definitions in this chapter have been 
 upgraded during the progression of examples. Figure 10.88 lists the most re-cent 
 meanings.
  
 Figure 10.88
  
 Syntactic domains
  
 1
  
 2
  
 BinLit: binary literals; nonterminals BN, Seq
  
 Exp: expressions; nonterminal T
  
 3
  
 Id: identifiers; nonterminal I
  
 4
  
 Pr: programs; nonterminal P
  
 5
  
 Decls: declarations; nonterminal Def
  
 6
  
 Stm: statements; nonterminal St
  
 7
  
 Semantic domains
  
 8
  
 9
  
 Basic:
  
 N = {0, 1, 2, ...} (natural numbers)
  
 10
  
 Bool = {false, true} (Boolean values)
  
 11
  
 Complex: 
  
 Loc
  =
  {0, 1, . . .} -- finite domain of memory locations
  
 12
  
 13
  
 Mem
  =
  Loc
  
  N
  
  Bool
  
  {uninitInt}
  
  {uninitBool}
  
  
 14
  
 {unalloc} -- memory location
  
 15
  
 File
  =
  (N
  
  Bool
  
  {eof}) * -- contents of a file
  
 16
  
 R
  =
  N
  
  Bool
  
  {
 
 } -- value of an expression
  
 17
  
 RR
  =
  State
  
  (N
  
  Bool
  
  {
 
 })
  
 18
  
 -- result of function
  
 19
  
 State
  =
  Mem
  
  File
  
  File -- program state
  
 20
  
 Ans
  =
  File
  
  {
 
 } -- program result
  
 21
  
 V
  =
  N
  
  Bool
  
  Loc
  
  Proc
  
  Func
  
  SC
  
  {
 
 }
  
  {udef}
  
  
 22
  
 {redef} -- value of an identifier
  
 23
  
 U
  =
  Id
  
  V -- environment 
  
 UU
  =
  U * -- sequence of environments
  
 24
  
 25
  
 Proc
  =
  (U
  
  State
  
  SC
  
  Ans)
  
  Loc
  
  Parms
  
 26
  
 -- procedure
  
 27
  
 Func
  =
  (U
  
  State
  
  EC
  
  Ans)
  
  Loc
  
  Parms
  
 28
  
 -- function
  
 29
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
4,N,NA
 FINAL COMMENTS,"This long (and somewhat tedious) exercise shows that it is possible to specify 
 exactly what a programming language designer allows in the syntax and means by 
 the constructs of the language. Such a specification can guide the designer (to make 
 sure that all cases are properly covered), the implementer (to make sure that the 
 compiler and runtime support live up to the specifica-tions), and the programmer 
 (to make sure that language constructs are used as intended).
  
 Formal specification can also be used to evaluate the clarity of a language. If the 
 axiomatic semantics of a construct are hard to build and hard to under-stand, then 
 perhaps the construct itself is hard to understand. For example, a multiple 
 assignment statement has this structure:
  
 x, y, z := 13, 16, x + 3;
  
 Three assignments are made simultaneously. However,
  x + 3
  on the right-hand side 
 depends on
  x
 , which is on the left-hand side. The order of evalua-tion makes a 
 difference. It is not easy in axiomatic semantics to specify the rule for multiple 
 assignment for this reason. Perhaps that complexity is a symptom that multiple 
 assignment is itself an unclear concept.
  
 As my brief forays into ML have shown, the specification can even be writ-ten in 
 a programming language so that it can be checked for syntax and meaning. (Have 
 you really read all the specifications? Did you find any mis-takes?) Such a 
 specification can even be used to interpret programs (written in abstract syntax, of 
 course), more as a way of debugging the specification than understanding the 
 meaning of the programs.
  
 However, the fact that the specification is in a language, albeit a program-ming 
 language, seems to reduce the question of formally specifying one lan-guage (the 
 target) to specifying another (ML, for example). It requires that someone who wants 
 to understand the target language specification needs to learn and understand some 
 fairly complex notions, such as domain equations.
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
EXERCISES,NA,NA
Review Exercises,"10.1
  Describe the language (that is, the set of strings) generated by this BNF 
  
 grammar:
  
 S ::= ( S ) S |
  
  
 10.2
  Show a different BNF grammar that generates exactly the same lan-
  
 guage 
 as the grammar in Exercise 10.1.
  
 10.3
  Write BNF productions for
  if
  statements.
  
 10.4
  An
  ambiguous grammar
  is one that generates strings that have more than 
 one parse tree. Is the grammar of Figure 10.89 ambiguous? Does it have any 
 other problems?
  
 Figure 10.89
  
 Expression ::=
  
 1
  
 Expression + Expression |
  
 2
  
 Expression * Expression |
  
 3
  
 INTEGER
  
 4
  
 10.5
  Prove the program in Figure 10.90 correct.
  
 Figure 10.90
  
 {a < 3}
  
 1
  
 if
  a < 4
  then
  x := 2
  else
  x := 10
  end
 ;
  
 2
  
 {x = 2}
  
 3
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
Challenge Exercises View publication stats,"10.7
  In Figure 10.43 (page 330), I specify that redefinition of an identifier has no 
 effect. Show how to modify the example so that redefinition hides the previous 
 definition.
  
 10.8 
  
 10.9
  
 In line 25 of Figure 10.46 (page 333), why check that
  c
  is a member of 
 U
 ? 
 What else could it be?
  
 On page 331, I introduce
  redef
 . Why not just use
  udef
  for this pur-pose?
  
 10.10
  How would you code the semantics of a
  while
  loop (see Figure 10.52, 
  
 page 337) in ML?
  
 Copyright
  
  Addison-Wesley. Reproduction fee $.02 per page, per copy.",NA
