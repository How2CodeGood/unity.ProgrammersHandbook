Larger Text,Smaller Text,Symbol
Clean Code in Python,NA,NA
Refactor your legacy code base,NA,NA
Mariano Anaya,BIRMINGHAM - MUMBAI,NA
Clean Code in Python,"Copyright 
 Â©
  2018 Packt Publishing
  
 All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form 
 or by any means, without the prior written permission of the publisher, except in the case of brief quotations 
 embedded in critical articles or reviews.
  
 Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
  
 However, the information contained in this book is sold without warranty, either express or implied. Neither the 
 author, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to 
 have been caused directly or indirectly by this book.
  
 Packt Publishing has endeavored to provide trademark information about all of the companies and products 
 mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy 
 of this information.
  
 Commissioning Editor:
  Merint Mathew 
  
 Acquisition Editor:
  Denim Pinto 
  
 Content Development Editor:
  Priyanka Sawant 
  
 Technical Editor:
  Gaurav Gala 
  
 Copy Editor:
  Safis Editing 
  
 Project Coordinator:
  Vaidehi Sawant 
  
 Proofreader:
  Safis Editing 
  
 Indexer:
  Rekha Nair 
  
 Graphics:
  Jason Monteiro 
  
 Production Coordinator:
  Shantanu Zagade
  
 First published: August 2018
  
 Production reference: 1270818
  
 Published by Packt Publishing Ltd.
  
 Livery Place 
  
 35 Livery Street 
  
 Birmingham 
  
 B3 2PB, UK.
  
 ISBN 978-1-78883-583-1
  
 www.packtpub.com",NA
Why subscribe?,"Spend less time learning and more time coding with practical eBooks and Videos 
  
 from over 4,000 industry professionals
  
 Improve your learning with Skill Plans built especially for you
  
 Get a free eBook or video every month
  
 Mapt is fully searchable
  
 Copy and paste, print, and bookmark content",NA
PacktPub.com,"Did you know that Packt offers eBook versions of every book published, with PDF and 
 ePub files available? You can upgrade to the eBook version at 
 www.PacktPub.com
  and as a 
 print book customer, you are entitled to a discount on the eBook copy. Get in touch with us 
 at 
 service@packtpub.com
  for more details.
  
 At 
 www.PacktPub.com
 , you can also read a collection of free technical articles, sign up for a 
 range of free newsletters, and receive exclusive discounts and offers on Packt books and 
 eBooks.",NA
Contributors,NA,NA
About the author,"Mariano Anaya
  is a software engineer who spends most of his time creating software with 
 Python and mentoring fellow programmers. Mariano's main areas of interests besides 
 Python are software architecture, functional programming, distributed systems, and 
 speaking at conferences.
  
 He was a speaker at Euro Python 2016 and 2017. To know more about him, you can refer to 
 his GitHub account with the username rmariano.
  
 His speakerdeck username is rmariano.",NA
About the reviewer,"Nimesh Kiran Verma
  has a dual degree in Maths and Computing from IIT Delhi and has 
 worked with companies such as LinkedIn, Paytm and ICICI for about 5 years in software 
 development and data science.
  
 He co-founded a micro-lending company, Upwards Fintech and presently serves as its 
 CTO. He loves coding and has mastery in Python and its popular frameworks, Django and 
 Flask.
  
 He extensively leverages Amazon Web Services, design patterns, SQL and NoSQL 
 databases, to build reliable, scalable and low latency architectures.
  
 To my mom, Nutan Kiran Verma, who made me what I am today and gave the confidence 
 to pursue all my dreams. Thanks Papa, Naveen, and Prabhat, who motivated me to steal 
 time for this book when in fact I was supposed to be spending it with them. Ulhas and the 
 entire Packt team's support was tremendous. Thanks Varsha Shetty for introducing me to 
 Packt.",NA
Packt is searching for authors like you,"If you're interested in becoming an author for Packt, please visit 
 authors.packtpub.com 
 and 
 apply today. We have worked with thousands of developers and tech professionals, just 
 like you, to help them share their insight with the global tech community. You can make 
 a general application, apply for a specific hot topic that we are recruiting an author for, 
 or submit your own idea.",NA
Table of Contents,"Preface
  
 1
  
 Chapter 1: Introduction, Code Formatting, and Tools 
 The meaning of clean code 
  
 The importance of having clean code 
  
  
 The role of code formatting in clean code 
  
  
 Adhering to a coding style guide on your project 
  
 Docstrings and annotations 
  
  
 Docstrings 
  
  
 Annotations 
  
  
 Do annotations replace docstrings?
  
  
 Configuring the tools for enforcing basic quality gates 
  
  
 Type hinting with Mypy 
  
  
  
 Checking the code with Pylint 
  
  
  
 Setup for automatic checks 
  
 Summary
  
 Chapter 2: Pythonic Code 
  
 Indexes and slices 
  
  
 Creating your own sequences 
  
 Context managers 
  
  
 Implementing context managers 
  
 Properties, attributes, and different types of methods for objects 
  
 Underscores in Python 
  
  
 Properties 
  
 Iterable objects 
  
  
 Creating iterable objects 
  
  
 Creating sequences 
  
 Container objects 
  
 Dynamic attributes for objects 
  
 Callable objects 
  
 Summary of magic methods 
  
 Caveats in Python 
  
  
 Mutable default arguments 
  
  
 Extending built-in types 
  
 Summary 
  
 References
  
 Chapter 3: General Traits of Good Code 
 Design by contract
  
 7 
  
 8 
  
 8 
  
 9 
  
 10 
  
 12 
  
 13 
  
 16 
  
 18 
  
 20 
  
 20 
  
 21 
  
 21 
  
 24
  
 2
 5 
  
 2
 6 
  
 2
 8 
  
 2
 9 
  
 3
 2 
  
 3
 5 
  
 3
 5 
  
 3
 8 
  
 4
 0 
  
 4
 0 
  
 4
 3 
  
 4
 5 
  
 4
 6 
  
 4
 8 
  
 4",NA
Preface,"This is a book about software engineering principles applied to Python.
  
 There are many books about software engineering, and many resources available with 
 information about Python. The intersection of those two sets, though, is something that 
 requires action, and that's the gap this book tries to bridge.
  
 It would not be realistic to cover all possible topics about software engineering in a single 
 book because the field is so wide that there are entire books dedicated to certain topics. This 
 book focuses on the main practices or principles of software engineering that will help us 
 write more maintainable code, and how to write it by taking advantage of the features of 
 Python at the same time.
  
 A word to the wise: there is no single solution to a software problem. It's usually about 
 trade-offs. Each solution will have upsides and downsides, and some criteria must be 
 followed to choose between them, accepting the costs and getting the benefits. There is 
 usually no single best solution, but there are principles to be followed, and as long as we 
 follow them we will be walking a much safer path. And that is what this book is about: 
 inspiring the readers to follow principles and make the best choices, because even when 
 facing difficulties, we will be much better off if we have followed good practices.
  
 And, speaking of good practices, while some of the explanations follow established and 
 proven principles, other parts are opinionated. But that doesn't mean it has to be done in 
 that particular way only. The author does not claim to be any sort of authority on the matter 
 of clean code, because such a title cannot possible exist. The reader is encouraged to engage 
 in critical thinking: take what works the best for your project, and feel free to disagree. 
 Differences of opinions are encouraged as long as they yield an enlightening debate.
  
 My intention behind this book is to share the joys of Python, and idioms I have learned 
 from experience, in the hope that readers will find them useful to elevate their expertise 
 with the language.
  
 The book explains the topics through code examples. These examples assume the latest 
 version of Python at the time of this writing is used, namely Python 3.7, although future 
 versions should be compatible as well. There are no peculiarities in the code that bind it to",NA
Who this book is for,"This book is suitable for all software engineering practitioners who are interested in 
 software design or learning more about Python. It is assumed that the reader is already 
 familiar with the principles of object-oriented software design and has some experience 
 writing code. 
  
 In terms of Python, the book is suitable for all levels. It's good for learning Python because 
 it is organized in such a way that the content is in increasing order of complexity. The first 
 chapters will cover the basics of Python, which is a good way to learn the main idioms, 
 functions, and utilities available in the language. The idea is not just to solve some 
 problems with Python, but to do so in an idiomatic way.
  
 Experienced programmers will also benefit from the topics in this book, as some sections 
 cover advanced topics in Python, such as decorators, descriptors, and an introduction to 
 asynchronous programming. It will help the reader discover more about Python because 
 some of the cases are analyzed from the internals of the language itself.
  
 It is worth emphasizing the word 
 practitioners
  in the first sentence of this section. This is a 
 pragmatic book. Examples are limited to what the case of study requires, but are also 
 intended to resemble the context of a real software project. It is not an academic book, and 
 as such the definitions made, the remarks made, and the recommendations given are to be 
 taken with caution. The reader is expected to examine these recommendations critically and 
 pragmatically rather than dogmatically. After all, practicality beats purity.",NA
What this book covers,"Chapter 1
 , 
 Introduction, Code Formatting, and Tools
 , is an introduction to the main tools you 
 need to set up a development environment in Python. We cover the basics a Python 
 developer is recommended to know to start working with the language, as well as some 
 guidelines for maintaining readable code in the project, such as tools for static analysis, 
 documentation, type checking, and code formatting.
  
 Chapter 2
 , 
 Pythonic Code
 , looks at the first idioms in Python, which we will continue to use in 
 the following chapters. We cover the particular features of Python, how they should be 
 used, and we start building knowledge around the idea that Pythonic code is in general 
 much better quality code.
  
 Chapter 3
 , 
 General Traits of Good Code
 , reviews general principles of software engineering 
 that focus on writing maintainable code. We explore the idea and apply the concepts with 
 the tools in the language.
  
 Chapter 4
 , 
 The SOLID Principles
 , covers a set of design principles for object-oriented 
 software design. This acronym is part of the language or jargon of software engineering, 
 and we see how each one of them can be applied to Python. Arguably not all of them are 
 entirely applicable due to the nature of the language.
  
 Chapter 5
 , 
 Using Decorators to Improve Our Code
 , looks at one of the greatest features of 
 Python. After understanding how to create decorators (for functions and classes), we put 
 them in action for reusing code, separating responsibilities, and creating more granular 
 functions.
  
 Chapter 6
 , 
 Getting More Out of Our Objects with Descriptors
 , explores descriptors in Python, 
 which take object-oriented design to a new level. While this is a feature more related to 
 frameworks and tools, we can see how to improve the readability of our code with 
 descriptors, and also reuse code.
  
 Chapter 7
 , 
 Using Generators
 , shows that generators are probably the best feature of Python. 
 The fact that iteration is a core component of Python could make us think that it leads to a 
 new programming paradigm. By using generators and iterators in general, we can think 
 about the way we write our programs. With the lessons learned from generators, we go 
 further and learn about coroutines in Python, and the basics of asynchronous 
  
 programming.",NA
To get the most out of this book,"The reader should be familiarized with Python's syntax and have a valid Python interpreter 
 installed, which can be downloaded from
  https://www.python.org/downloads/
  
 It is recommended to follow the examples in the book and test the code locally. For this, it is 
 highly recommended to create a virtual environment with Python 3.7 and run the code with 
 this interpreter. Instructions to create a virtual environment can be found at 
 https:// docs.
 python.org/3/tutorial/venv.html
 .",NA
Download the example code files,"You can download the example code files for this book from your account at 
 www.packtpub.com
 . If you purchased this book elsewhere, you can visit 
  
 www.packtpub.com/support
  and register to have the files emailed directly to you.
  
 You can download the code files by following these steps:
  
 1. 
  
 Log in or register at 
 www.packtpub.com
 .
  
 2. 
  
 Select the 
 SUPPORT
  tab.
  
 3. 
  
 Click on 
 Code Downloads & Errata
 .
  
 4. 
  
 Enter the name of the book in the 
 Search
  box and follow the onscreen 
  
 instructions.",NA
Conventions used,"There are a number of text conventions used throughout this book.
  
 CodeInText
 : Indicates code words in text, database table names, folder names, filenames, 
 file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an 
 example: ""Then, just running the 
 pylint
  command is enough to check it in the code.""
  
 A block of code is set as follows:
  
 class Point:
  
  
  def __init__(self, lat, long):
  
  
  
  self.lat = lat
  
  
  
  self.long = long
  
 When we wish to draw your attention to a particular part of a code block, the relevant lines 
 or items are set in bold:
  
 setup( 
  
  
 name=""apptool"",
  
  
  description=""Description of the intention of the package"",
  
  
 long_description=long_description,
  
 Any command-line input or output is written as follows:
  
 >>> locate.__annotations__ 
  
 {'latitude': float, 'longitue': float, 'return': __main__.Point}",NA
Get in touch,"Feedback from our readers is always welcome.
  
 General feedback
 : Email 
 feedback@packtpub.com
  and mention the book title in the subject 
 of your message. If you have questions about any aspect of this book, please email us at 
 questions@packtpub.com
 .
  
 Errata
 : Although we have taken every care to ensure the accuracy of our content, mistakes 
 do happen. If you have found a mistake in this book, we would be grateful if you would 
 report this to us. Please visit 
 www.packtpub.com/submit-errata
 , selecting your book, clicking on 
 the Errata Submission Form link, and entering the details.
  
 Piracy
 : If you come across any illegal copies of our works in any form on the Internet, we 
 would be grateful if you would provide us with the location address or website name. 
 Please contact us at 
 copyright@packtpub.com
  with a link to the material.
  
 If you are interested in becoming an author
 : If there is a topic that you have expertise in 
 and you are interested in either writing or contributing to a book, please visit 
  
 authors.packtpub.com
 .",NA
Reviews,"Please leave a review. Once you have read and used this book, why not leave a review on 
 the site that you purchased it from? Potential readers can then see and use your unbiased 
 opinion to make purchase decisions, we at Packt can understand what you think about our 
 products, and our authors can see your feedback on their book. Thank you!
  
 For more information about Packt, please visit 
 packtpub.com
 .",NA
"Introduction, Code Formatting, ",NA,NA
ï±,NA,NA
and Tools,"In this chapter, we will explore the first concepts related to clean code, starting with what it 
 is and what it means. The main point of the chapter is to understand that clean code is not 
 just a nice thing to have or a luxury in software projects. It's a necessity. Without quality 
 code, the project will face the perils of failing due to an accumulated technical debt.
  
 Along the same lines, but going into a bit more detail, are the concepts of formatting and 
 documenting the code. This also might sound like a superfluous requirement or task, but 
 again, we will discover that it plays a fundamental role in keeping the code base 
 maintainable and workable.
  
 We will analyze the importance of adopting a good coding guideline for this project. 
 Realizing that maintaining the code align to the reference is a continuous task, and we will 
 see how we can get help from automated tools that will ease our work. For this reason, we 
 quickly discuss how to configure the main tools so that they automatically run on the 
 project as part of the build.
  
 After reading this chapter, you will have an idea of what clean code is, why it is important, 
 why formatting and documenting the code are crucial tasks, and how to automate this 
 process. From this, you should acquire the mindset for quickly organizing the structure of a 
 new project, aiming for good code quality.
  
 After reading this chapter, you will have learned the following:
  
 That clean code really means something far more important than formatting in 
  
 software construction",NA
The meaning of clean code,"There is no sole or strict definition of clean code. Moreover, there is probably no way of 
 formally measuring clean code, so you cannot run a tool on a repository that could tell you 
 how good, bad, or maintainable or not that code is. Sure, you can run tools such as checkers, 
 linters, static analyzers, and so on. And those tools are of much help. They are necessary, 
 but not sufficient. Clean code is not something a machine or script could tell (so far), but 
 rather something that us, as professionals, can decide.
  
 For decades of using the terms programming languages, we thought that they were 
 languages to communicate our ideas to the machine, so it can run our programs. We were 
 wrong. That's not the truth, but part of the truth. The real language behind programming 
 languages is to communicate our ideas to other developers.
  
 Here is where the true nature of clean code lies. It depends on other engineers to be able to 
 read and maintain the code. Therefore, we, as professionals, are the only ones who can 
 judge this. Think about it; as developers, we spend much more time reading code than 
 actually writing it. Every time we want to make a change or add a new feature, we first 
 have to read all the surroundings of the code we have to modify or extend. The language 
 (Python), is what we use to communicate among ourselves.
  
 So, instead of giving you a definition (or my definition) of clean code, I invite you to go 
 through the book, read all about idiomatic Python, see the difference between good and bad 
 code, identify traits of good code and good architecture, and then come up with your own 
 definition. After reading this book, you will be able to judge and analyze code for yourself, 
 and you will have a more clear understanding of clean code. You will know what it is and 
 what it means, regardless of any definition given to you.",NA
The importance of having clean code,"There are a huge number of reasons why clean code is important. Most of them revolve 
 around the ideas of maintainability, reducing technical debt, working effectively with agile 
 development, and managing a successful project.",NA
The role of code formatting in clean code,NA,NA
Adhering to a coding style guide on your project,"A coding guideline is a bare minimum a project should have to be considered being 
 developed under quality standards. In this section, we will explore the reasons behind this, 
 so in the following sections, we can start looking at ways to enforce this automatically by 
 the means of tools.
  
 The first thing that comes to my mind when I try to find good traits in a code layout is 
 consistency. I would expect the code to be consistently structured so that it is easier to read 
 and follow. If the code is not correct or consistently structured, and everyone on the team is 
 doing things in their own way, then we will end up with code that will require extra effort 
 and concentration to be followed correctly. It will be error-prone, misleading, and bugs or 
 subtleties might slip through easily.
  
 We want to avoid that. What we want is exactly the opposite of that
 â
 code that we can read 
 and understand as quickly as possible at a single glance.
  
 If all members of the development team agree on a standardized way of structuring the 
 code, the resulting code would look much more familiar. As a result of that, you will 
 quickly identify patterns (more about this in a second), and with these patterns in mind, it 
 will be much easier to understand things and detect errors. For example, when something is 
 amiss, you will notice that somehow, there is something odd in the patterns you are used to 
 seeing, which will catch your attention. You will take a closer look, and you will more than 
 likely spot the mistake!",NA
Docstrings and annotations,"This section is about documenting the code in Python, from within the code. Good code is 
 self-explanatory but is also well-documented. It is a good idea to explain what it is 
 supposed to do (not how).",NA
Docstrings,"In simple terms, we can say that docstrings are basically documentation embedded in the 
 source code. A 
 docstring
  is basically a literal string, placed somewhere in the code, with the 
 intention of documenting that part of the logic.
  
 Notice the emphasis on the word 
 documentation
 . This subtlety is important because it's 
 meant to represent explanation, not justification. Docstrings are not comments; they are 
 documentation.
  
 Having comments in the code is a bad practice for multiple reasons. First, comments 
 represent our failure to express our ideas in the code. If we actually have to explain why or 
 how we are doing something, then that code is probably not good enough. For starters, it 
 fails to be self-explanatory. Second, it can be misleading. Worst than having to spend some 
 time reading a complicated section is to read a comment on how it is supposed to work, and 
 figuring out that the code actually does something different. People tend to forget to update 
 comments when they change the code, so the comment next to the line that was just 
 changed will be outdated, resulting in a dangerous misdirection.
  
 Sometimes, on rare occasions, we cannot avoid having comments. Maybe there is an error 
 on a third-party library that we have to circumvent. In those cases, placing a small but 
 descriptive comment might be acceptable.
  
 With docstrings, however, the story is different. Again, they do not represent comments, 
 but the documentation of a particular component (a module, class, method, or function) in 
 the code. Their use is not only accepted but also encouraged. It is a good practice to add 
 docstrings whenever possible.",NA
Annotations ,"PEP-3107 introduced the concept of annotations. The basic idea of them is to hint to the 
 readers of the code about what to expect as values of arguments in functions. The use of the 
 word 
 hint
  is not casual; annotations enable type hinting, which we will discuss later on in 
 this chapter, after the first introduction to annotations.
  
 Annotations let you specify the expected type of some variables that have been defined. It is 
 actually not only about the types, but any kind of metadata that can help you get a better 
 idea of what that variable actually represents.
  
 Consider the following example:
  
 class Point:
  
  
  def __init__(self, lat, long):
  
  
  
  self.lat = lat
  
  
  
  self.long = long
  
 def locate(latitude: float, longitude: float) -> Point:
  
  """"""Find an object in the 
 map by its coordinates""""""
  
 Here, we use 
 float
  to indicate the expected types of 
 latitude
  and 
 longitude
 . This is merely 
 informative for the reader of the function so that they can get an idea of these expected 
 types. Python will not check these types nor enforce them.
  
 We can also specify the expected type of the returned value of the function. In this case, 
 Point
  is a user-defined class, so it will mean that whatever is returned will be an instance 
 of 
 Point
 .",NA
Do annotations replace docstrings?,"This is a valid question, since on older versions of Python, long before annotations were 
 introduced, the way of documenting the types of the parameters of functions or attributes 
 was done by putting docstrings on them. There are even some conventions on formats on 
 how to structure docstrings to include the basic information for a function, including types 
 and meaning of each parameter, type, and meaning of the result, and possible exceptions 
 that the function might raise.
  
 Most of this has been addressed already in a more compact way by means of annotations, 
 so one might wonder if it is really worth having docstrings as well. The answer is yes, and 
 this is because they complement each other.
  
 It is true that a part of the information previously contained on the docstring can now be 
 moved to the annotations. But this should only leave more room for a better documentation 
 on the docstring. In particular, for dynamic and nested data types, it is always a good idea 
 to provide examples of the expected data so that we can get a better idea of what we are 
 dealing with.
  
 Consider the following example. Let's say we have a function that expects a dictionary to 
 validate some data:
  
 def data_from_response(response: dict) -> dict:
  
  if 
 response[""status""] != 200:
  
  
  
  raise ValueError
  
  
  return {""data"": response[""payload""]}",NA
Configuring the tools for enforcing basic quality ,NA,NA
gates,"In this section, we will explore how to configure some basic tools and automatically run 
 checks on the code, with the goal of leveraging part of the repetitive verification checks.
  
 This is an important point: remember that code is for us, people, to understand, so only we 
 can determine what is good or bad code. We should invest time in code reviews, thinking 
 about what good code is, and how readable and understandable it is. When looking at the 
 code written by a peer, you should ask such questions:
  
 Is this code easy to understand and follow for a fellow programmer?
  
 Does it speak in terms of the domain of the problem?
  
 Would a new person joining the team be able to understand it and work with it 
  
 effectively?
  
 As we saw previously, code formatting, consistent layout, and proper indentation are 
 required but not sufficient traits to have in a code base. Moreover, this is something that 
 we, as engineers with a high sense of quality, would take for granted, so we would read 
 and write code far beyond the basic concepts of its layout. Therefore, we are not willing to 
 waste time reviewing these kinds of items, so we can invest our time more effectively by 
 looking at actual patterns in the code in order to understand its true meaning and provide 
 valuable results.
  
 All of these checks should be automated. They should be part of the tests or checklist, and 
 this, in turn, should be part of the continuous integration build. If these checks do not pass, 
 make the build fail. This is the only way to actually ensure the continuity of the structure of 
 the code at all times. It also serves as an objective parameter for the team to have as a 
 reference. Instead of having some engineers or the leader of the team always having to tell 
 the same comments about PEP-8 on code reviews, the build will automatically fail, making 
 it something objective.",NA
Type hinting with Mypy,"Mypy (
 http://mypy-lang.org/
 ) is the main tool for optional static type checking in Python. The 
 idea is that, once you install it, it will analyze all of the files on your project, checking for 
 inconsistencies on the use of the types. This is useful since, most of the time, it will detect 
 actual bugs early, but sometimes it can give false positives.",NA
Checking the code with Pylint,"There are many tools for checking the structure of the code (basically, this is compliance 
 with PEP-8) in Python, such as pycodestyle (formerly known as PEP-8), Flake8, and many 
 more. They all are configurable and are as easy to use as running the command they 
 provide. Among all of them, I have found Pylint to be the most complete (and strict). It is 
 also configurable.
  
 Again, you just have to install it in the virtual environment with 
 pip
 :
  
 $ pip install pylint
  
 Then, just running the 
 pylint
  command would be enough to check it in the code.
  
 It is possible to configure Pylint via a configuration file named 
 pylintrc
 .
  
 In this file, you can decide the rules you would like to enable or disable, and parametrize 
 others (for example, to change the maximum length of the column).",NA
Setup for automatic checks,"On Unix development environments, the most common way of working is through 
 makefiles. 
 Makefiles
  are powerful tools that let us configure commands to be run in the 
 project, mostly for compiling, running, and so on. Besides this, we can use a makefile in the 
 root of our project, with some commands configured to run checks of the formatting and 
 conventions on the code, automatically.",NA
Summary,"We now have a first idea of what clean code is, and a workable interpretation of it, which 
 will serve us as a reference point for the rest of this book.
  
 More importantly, we understood that clean code is something much more important than 
 the structure and layout of the code. We have to focus on how the ideas are represented on 
 the code to see if they are correct. Clean code is about readability, maintainability of the 
 code, keeping technical debt to the minimum, and effectively communicating our ideas into 
 the code so that others can understand the same thing we intended to write in the first 
 place.
  
 However, we discussed that the adherence to coding styles or guidelines is important for 
 multiple reasons. We have agreed that this is a condition that is necessary, but not 
 sufficient, and since it is a minimal requirement every solid project should comply with, it is 
 clear that is something we better leave to the tools. Therefore, automating all of these checks 
 becomes critical, and in this regard, we have to keep in mind how to configure tools such as 
 Mypy, Pylint, and more.
  
 The next chapter is going to be more focused on the Python-specific code, and how to 
 express our ideas in idiomatic Python. We will explore the idioms in Python that make for 
 more compact and efficient code. In this analysis, we will see that, in general, Python has 
 different ideas or different ways to accomplish things compared to other languages.",NA
Pythonic Code ,NA,NA
ï²,"In this chapter, we will explore the way ideas are expressed in Python, with its own 
 particularities. If you are familiar with the standard ways of accomplishing some tasks in 
 programming (such as getting the last element of a list, iterating, searching, and so on), or if 
 you come from more traditional programming languages (like C, C++, and Java), then you 
 will find that, in general, Python provides its own mechanism for most common tasks.
  
 In programming, an idiom is a particular way of writing code in order to perform a specific 
 task. It is something common that repeats and follows the same structure every time. Some 
 could even argue and call them a pattern, but be careful because they are not designed 
 patterns (which we will explore later on). The main difference is that design patterns are 
 high-level ideas, independent from the language (sort of), but they do not translate into 
 code immediately. On the other hand, idioms are actually coded. It is the way things should 
 be written when we want to perform a particular task.
  
 As idioms are code, they are language dependent. Every language will have its own idioms, 
 which means the way things are done in that particular language (for example, how you 
 would open and write a file in C, C++, and so on). When the code follows these idioms, it is 
 known as being idiomatic, which in Python is often referred to as 
 Pythonic
 .
  
 There are multiple reasons to follow these recommendations and write Pythonic code first 
 (as we will see and analyze), writing code in an idiomatic way usually performs better. It is 
 also more compact and easier to understand. These are traits that we always want in our 
 code so that it works effectively. Secondly, as introduced in the previous chapter, it is 
 important that the entire development team can get used to the same patterns and structure 
 of the code because this will help them focus on the true essence of the problem, and will 
 help them avoid making mistakes.",NA
Indexes and slices ,"In Python, as in other languages, some data structures or types support accessing its 
 elements by index. Another thing it has in common with most programming languages is 
 that the first element is placed in the index number zero. However, unlike those languages, 
 when we want to access the elements in a different order than usual, Python provides extra 
 features.
  
 For example, how would you access the last element of an array in C? This is something I 
 did the first time I tried Python. Thinking the same way as in C, I would get the element in 
 the position of the length of the array minus one. This could work, but we could also use a 
 negative index number, which will start counting from the last, as shown in the following 
 commands:
  
 >>> my_numbers = (4, 5, 3, 9) 
  
 >>> my_numbers[-1] 
  
 9 
  
 >>> my_numbers[-3] 
  
 5
  
 In addition to getting just one element, we can obtain many by using 
 slice
 , as shown in 
 the following commands:
  
 >>> my_numbers = (1, 1, 2, 3, 5, 8, 13, 21) >>> 
 my_numbers[2:5] 
  
 (2, 3, 5)
  
 In this case, the syntax on the square brackets means that we get all of the elements on the 
 tuple, starting from the index of the first number (inclusive), up to the index on the second 
 one (not including it). Slices work this way in Python by excluding the end of the selected 
 interval.",NA
Creating your own sequences,"The functionality we just discussed works thanks to a magic method called 
 __getitem__
 . 
 This is the method that is called, when something like 
 myobject[key]
  is called, passing the 
 key (value inside the square brackets) as a parameter. A sequence, in particular, is an 
 object that implements both 
 __getitem__
  and 
 __len__
 , and for this reason, it can be iterated 
 over. Lists, tuples, and strings are examples of sequence objects in the standard library.
  
 In this section, we care more about getting particular elements from an object by a key than 
 building sequences or iterable objects, which is a topic explored in 
 Chapter 7
 , 
 Using 
 Generators
 .
  
 If you are going to implement 
 __getitem__
  in a custom class in your domain, you will have 
 to take into account some considerations in order to follow a Pythonic approach.
  
 In the case that your class is a wrapper around a standard library object, you might as well 
 delegate the behavior as much as possible to the underlying object. This means that if your 
 class is actually a wrapper on the list, call all of the same methods on that list to make sure 
 that it remains compatible. In the following listing, we can see an example of how an object 
 wraps a list, and for the methods we are interested in, we just delegate to its corresponding 
 version on the 
 list
  object:
  
 class Items:
  
  
  def __init__(self, *values):
  
  
  
  self._values = list(values)
  
  def __len__(self):
  
  return len(self._values)
  
  def __getitem__(self, item):
  
  return self._values.__getitem__(item)
  
 This example uses encapsulation. Another way of doing it is through inheritance, in which 
 case we will have to extend the 
 collections.UserList
  base class, with the 
  
 considerations and caveats mentioned in the last part of this chapter.
  
 If, however, you are implementing your own sequence, that is not a wrapper or does not 
 rely on any built-in object underneath, then keep in mind the following points:
  
 When indexing by a range, the result should be an instance of the same type of 
  
 the class",NA
Context managers,"Context managers are a distinctively useful feature that Python provides. The reason why 
 they are so useful is that they correctly respond to a pattern. The pattern is actually every 
 situation where we want to run some code, and has preconditions and postconditions, 
 meaning that we want to run things before and after a certain main action.
  
 Most of the time, we see context managers around resource management. For example, on 
 situations when we open files, we want to make sure that they are closed after processing 
 (so we do not leak file descriptors), or if we open a connection to a service (or even a 
 socket), we also want to be sure to close it accordingly, or when removing temporary files, 
 and so on.",NA
Implementing context managers,"In general, we can implement context managers like the one in the previous example. All 
 we need is just a class that implements the 
 __enter__
  and 
 __exit__
  magic methods, and then 
 that object will be able to support the context manager protocol. While this is the most 
 common way for context managers to be implemented, it is not the only one.
  
 In this section, we will see not only different (sometimes more compact) ways of 
  
 implementing context managers but also how to take full advantage of them by using the 
 standard library, in particular with the 
 contextlib
  module.
  
 The 
 contextlib
  module contains a lot of helper functions and objects to either implement 
 context managers or use some already provided ones that can help us write more compact 
 code.
  
 Let's start by looking at the 
 contextmanager
  decorator.
  
 When the 
 contextlib.contextmanager
  decorator is applied to a function, it converts the code 
 on that function into a context manager. The function in question has to be a particular kind 
 of function called a 
 generator
  function, which will separate the statements into what is 
 going to be on the 
 __enter__
  and 
 __exit__
  magic methods, respectively.",NA
"Properties, attributes, and different types of ",NA,NA
methods for objects,"All of the properties and functions of an object are public in Python, which is different from 
 other languages where properties can be public, private, or protected. That is, there is no 
 point in preventing caller objects from invoking any attributes an object has. This is another 
 difference with respect to other programming languages in which you can mark some 
 attributes as private or protected.
  
 There is no strict enforcement, but there are some conventions. An attribute that starts with 
 an underscore is meant to be private to that object, and we expect that no external agent 
 calls it (but again, there is nothing preventing this).
  
 Before jumping into the details of properties, it's worth mentioning some traits of 
 underscores in Python, understanding the convention, and the scope of attributes.",NA
Underscores in Python,"There are some conventions and implementation details that make use of underscores in 
 Python, which is an interesting topic that's worthy of analysis.",NA
Properties,"When the object needs to just hold values, we can use regular attributes. Sometimes, we 
 might want to do some computations based on the state of the object and the values of 
 other attributes. Most of the time, properties are a good choice for this.
  
 Properties are to be used when we need to define access control to some attributes in an 
 object, which is another point where Python has its own way of doing things. In other 
 programming languages (like Java), you would create access methods (getters and setters), 
 but idiomatic Python would use properties instead.
  
 Imagine that we have an application where users can register and we want to protect 
 certain information about the user from being incorrect, such as their email, as shown in the 
 following code:
  
 import re
  
 EMAIL_FORMAT = re.compile(r""[^@]+@[^@]+\.[^@]+"")
  
 def is_valid_email(potentially_valid_email: str):
  
  
  return re.match(EMAIL_FORMAT, potentially_valid_email) is not None
  
 class User:
  
  
  def __init__(self, username):
  
  
  
  self.username = username
  
  
  
  self._email = None
  
  @property
  
  def email(self):
  
  return self._email
  
  @email.setter
  
  def email(self, new_email):
  
  if not is_valid_email(new_email):
  
  
  raise ValueError(f""Can't set {new_email} as it's not a
  
  valid email"")
  
  self._email = new_email",NA
Iterable objects,"In Python, we have objects that can be iterated by default. For example, lists, tuples, sets, 
 and dictionaries can not only hold data in the structure we want but also be iterated over 
 a 
 for
  loop to get those values repeatedly.
  
 However, the built-in iterable objects are not the only kind that we can have in a 
 for
  loop. 
 We could also create our own iterable, with the logic we define for iteration.
  
 In order to achieve this, we rely on, once again, magic methods.
  
 Iteration works in Python by its own protocol (namely the iteration protocol). When you try 
 to iterate an object in the form 
 for e in myobject:...
 , what Python checks at a very high level 
 are the following two things, in order:
  
 If the object contains one of the iterator methods
 â
 __next__
  or 
 __iter__
  
 If the object is a sequence and has 
 __len__
  and 
 __getitem__
  
 Therefore, as a fallback mechanism, sequences can be iterated, and so there are two ways of 
 customizing our objects to be able to work on 
 for
  loops.",NA
Creating iterable objects,"When we try to iterate an object, Python will call the 
 iter()
  function over it. One of the first 
 things this function checks for is the presence of the 
 __iter__
  method on that object, which, 
 if present, will be executed.",NA
Creating sequences,"Maybe our object does not define the 
 __iter__()
  method, but we still want to be able to 
 iterate over it. If 
 __iter__
  is not defined on the object, the 
 iter()
  function will look for the 
 presence of 
 __getitem__
 , and if this is not found, it will raise 
 TypeError
 .
  
 A sequence is an object that implements 
 __len__
  and 
 __getitem__
  and expects to be able to get 
 the elements it contains, one at a time, in order, starting at zero as the first index. This 
 means that you should be careful in the logic so that you correctly implement 
  
 __getitem__
  to expect this type of index, or the iteration will not work.
  
 The example from the previous section had the advantage that it uses less memory. This 
 means that is only holding one date at a time, and knows how to produce the days one by 
 one. However, it has the drawback that if we want to get the n 
 th
  element, we have no way 
 to do so but iterate n-times until we reach it. This is a typical trade-off in computer science 
 between memory and CPU usage.",NA
Container objects,"Containers are objects that implement a 
 __contains__
  method (that usually returns a 
 Boolean value). This method is called in the presence of the 
 in
  keyword of Python.
  
 Something like the following:
  
 element in container
  
 When used in Python becomes this:
  
 container.__contains__(element)
  
 You can imagine how much more readable (and Pythonic!) the code can be when this 
 method is properly implemented.
  
 Let's say we have to mark some points on a map of a game that has two-dimensional 
 coordinates. We might expect to find a function like the following:
  
 def mark_coordinate(grid, coord):
  
  
  if 0 <= coord.x < grid.width and 0 <= coord.y < grid.height:
  
  
  grid[coord] 
 = MARKED
  
 Now, the part that checks the condition of the first 
 if
  statement seems convoluted; it 
 doesn't reveal the intention of the code, it's not expressive, and worst of all it calls for code 
 duplication (every part of the code where we need to check the boundaries before 
 proceeding will have to repeat that 
 if
  statement).
  
 What if the map itself (called 
 grid
  on the code) could answer this question? Even better, 
 what if the map could delegate this action to an even smaller (and hence more cohesive) 
 object? Therefore, we can ask the map if it contains a coordinate, and the map itself can 
 have information about its limit, and ask this object the following:
  
 class Boundaries:
  
  
  def __init__(self, width, height):
  
  
  
  self.width = width
  
  
  
  self.height = height
  
  def __contains__(self, coord):",NA
Dynamic attributes for objects,"It is possible to control the way attributes are obtained from objects by means of the 
 __getattr__
  magic method. When we call something like 
 <myobject>.<myattribute>
 , Python 
 will look for 
 <myattribute>
  in the dictionary of the object, calling 
  
 __getattribute__
  on it. If this is not found (namely, the object does not have the attribute we 
 are looking for), then the extra method, 
 __getattr__
 , is called, passing the name of the 
 attribute (
 myattribute
 ) as a parameter. By receiving this value, we can control the way things 
 should be returned to our objects. We can even create new 
  
 attributes, and so on.
  
 In the following listing, the 
 __getattr__
 method is demonstrated:
  
 class DynamicAttributes:
  
  def __init__(self, attribute):
  
  self.attribute = attribute",NA
Callable objects,"It is possible (and often convenient) to define objects that can act as functions. One of the 
 most common applications for this is to create better decorators, but it's not limited to that.
  
 The magic method 
 __call__
  will be called when we try to execute our object as if it were a 
 regular function. Every argument passed to it will be passed along to the 
 __call__ 
 method.
  
 The main advantage of implementing functions this way, through objects, is that objects 
 have states, so we can save and maintain information across calls.
  
 When we have an object, a statement like this 
 object(*args, **kwargs)
  is translated in Python 
 to 
 object.__call__(*args, **kwargs)
 .
  
 This method is useful when we want to create callable objects that will work as 
 parametrized functions, or in some cases functions with memory.
  
 The following listing uses this method to construct an object that when called with a 
 parameter returns the number of times it has been called with the very same value:
  
 from collections import defaultdict
  
 class CallCount:
  
  def __init__(self):
  
  self._counts = defaultdict(int)
  
  def __call__(self, argument):
  
  self._counts[argument] += 1
  
  return self._counts[argument]
  
 Some examples of this class in action are as follows:
  
 >>> cc = CallCount() 
  
 >>> cc(1) 
  
 1 
  
 >>> cc(2) 
  
 1",NA
Summary of magic methods,"We can summarize the concepts we described in the previous sections in the form of a cheat 
 sheet like the one presented as follows. For each action in Python, the magic method 
 involved is presented, along with the concept that it represents:
  
 Statement
  
 Magic method
  
 Python concept
  
 obj[key] 
  
 obj[i:j] 
  
 obj[i:j:k]
  
 __getitem__(key)
  
 Subscriptable object
  
 with obj: ...
  
 __enter__
  / 
 __exit__
  
 Context manager
  
 for i in obj: ...
  
 __iter__
  / 
 __next__ 
  
 __len__
  / 
 __getitem__
  
 Iterable object 
  
 Sequence
  
 obj.<attribute>
  
 __getattr__
  
 Dynamic attribute retrieval
  
 obj(*args, **kwargs) 
  
 __call__(*args, **kwargs)
  
 Callable object",NA
Caveats in Python,"Besides understanding the main features of the language, being able to write idiomatic code 
 is also about being aware of the potential problems of some idioms, and how to avoid them.
  
 In this section, we will explore common issues that might cause you long debugging 
 sessions if they catch you off guard.
  
 Most of the points discussed in this section are things to avoid entirely, and I will dare to 
 say that there is almost no possible scenario that justifies the presence of the anti-pattern (or 
 idiom, in this case). Therefore, if you find this on the code base you are working on, feel free 
 to refactor it in the way that is suggested. If you find these traits while doing a code review, 
 this is a clear indication that something needs to change.",NA
Mutable default arguments,"Simply put, don't use mutable objects as the default arguments of functions. If you use 
 mutable objects as default arguments, you will get results that are not the expected ones.
  
 Consider the following erroneous function definition:
  
 def wrong_user_display(user_metadata: dict = {""name"": ""John"", ""age"": 30}): name = 
 user_metadata.pop(""name"")
  
  age = user_metadata.pop(""age"")
  
  return f""{name} ({age})""
  
 This has two problems, actually. Besides the default mutable argument, the body of the 
 function is mutating a mutable object, hence creating a side effect. But the main problem is 
 the default argument for 
 user_medatada
 .
  
 This will actually only work the first time it is called without arguments. For the second 
 time, we call it without explicitly passing something to 
 user_metadata
 . It will fail with a 
 KeyError
 , like so:
  
 >>> wrong_user_display() 
  
 'John (30)' 
  
 >>> wrong_user_display({""name"": ""Jane"", ""age"": 25}) 'Jane (25)' 
  
 >>> wrong_user_display() 
  
 Traceback (most recent call last):
  
  File ""<stdin>"", line 1, in <module>
  
  File ... in wrong_user_display
  
   
  name = user_metadata.pop(""name"") 
  
 KeyError: 'name'
  
 The explanation is simple
 â
 by assigning the dictionary with the default data to 
  
 user_metadata
  on the definition of the function, this dictionary is actually created once and 
 the variable 
 user_metadata
  points to it. The body of the function modifies this object, which 
 remains alive in memory so long as the program is running. When we pass a value to it, this 
 will take the place of the default argument we just created. When we don't want this object 
 it is called again, and it has been modified since the previous run; the next time we run it, 
 will not contain the keys since they were removed on the previous call.",NA
Extending built-in types,"The correct way of extending built-in types such as lists, strings, and dictionaries is by 
 means of the 
 collections
  module.
  
 If you create a class that directly extends dict, for example, you will obtain results that are 
 probably not what you are expecting. The reason for this is that in CPython the methods of 
 the class don't call each other (as they should), so if you override one of them, this will not 
 be reflected by the rest, resulting in unexpected outcomes. For example, you might want to 
 override 
 __getitem__
 , and then when you iterate the object with a 
 for
  loop, you will notice 
 that the logic you have put on that method is not applied.
  
 This is all solved by using 
 collections.UserDict
 , for example, which provides a 
 transparent interface to actual dictionaries, and is more robust.
  
 Let's say we want a list that was originally created from numbers to convert the values to 
 strings, adding a prefix. The first approach might look like it solves the problem, but it is 
 erroneous:
  
 class BadList(list):
  
  
  def __getitem__(self, index):
  
  
  
  value = super().__getitem__(index)
  
  
  if index % 2 == 0:
  
  
  
  prefix = ""even""
  
  
  
  else:
  
  
  
  prefix = ""odd""
  
  
  
  return f""[{prefix}] {value}""",NA
Summary,"In this chapter, we have explored the main features of Python, with the goal of 
  
 understanding its most distinctive features, those that make Python a peculiar language 
 compared to the rest. On this path, we have explored different methods of Python, 
 protocols, and their internal mechanics.
  
 As opposed to the previous chapter, this one is more Python-focused. A key takeaway of 
 the topics of this book is that clean code goes beyond following the formatting rules (which, 
 of course, are essential to a good code base). They are a necessary condition, but not 
 sufficient. Over the next few chapters, we will see ideas and principles that relate more to 
 the code, with the goal of achieving a better design and implementation of our software 
 solution.
  
 With the concepts and the ideas of this chapter, we explored the core of Python: its 
 protocols and magic methods. It should be clear by now that the best way of having 
 Pythonic, idiomatic code is not only by following the formatting conventions but also by 
 taking full advantage of all the features Python has to offer. This means that you should 
 sometimes use a particular magic method, implement a context manager, and more.
  
 In the next chapter, we will put these concepts into action, relating general concepts of 
 software engineering with the way they can be written in Python.",NA
References,"The reader will find more information about some of the topics that we have covered in this 
 chapter in the following references. The decision of how indices work in Python is based on 
 (EWD831), which analyzes several alternatives for ranges in math and programming 
 languages:
  
 EWD831
 : Why numbering should start at zero (
 https://www.cs.utexas.edu/ 
  
 users/EWD/transcriptions//EWD831.html
 )
  
 PEP-343
 : The ""with"" Statement (
 https://www.python.org/dev/peps/pep-0343/
 )",NA
General Traits of Good Code ,NA,NA
ï³,"This is a book about software construction with Python. Good software is built from a good 
 design. By saying things such as clean code, one might think that we will explore good 
 practices that relate only to the implementation details of the software, instead of its design. 
 However, this assumption would be wrong since the code is not something different from 
 the design
 â
 the code is the design.
  
 The code is probably the most detailed representation of the design. In the first two 
 chapters, we discussed why structuring the code in a consistent way was important, and we 
 have seen idioms for writing more compact and idiomatic code. Now it's time to 
 understand that clean code is that, and much more
 â
 the ultimate goal is to make the code 
 as robust as possible, and to write it in a way that minimizes defects or makes them utterly 
 evident, should they occur.
  
 This chapter and the next one are focused on design principles at a higher level of 
 abstraction. These ideas not only relate to Python in particular but are instead general 
 principles of software engineering.
  
 In particular, for this chapter, we will review different principles that make for good 
 software design. Good-quality software should be built around these ideas, and they will 
 serve as design tools. This does not mean that all of them should always be applied; in fact, 
 some of them represent different points of view (such is the case with the 
 Design by 
 Contract
  (
 DbC
 ) approach, as opposed to defensive programming). Some of them depend 
 on the context and are not always applicable.
  
 A high-quality code is a concept that has multiple dimensions. We can think of this 
 similarly to how we think about the quality attributes of a software architecture. For",NA
Design by contract,"Some parts of the software we are working on are not meant to be called directly by users, 
 but instead by other parts of the code. Such is the case when we divide the responsibilities 
 of the application into different components or layers, and we have to think about the 
 interaction between them.
  
 We will have to encapsulate some functionality behind each component, and expose an 
 interface to clients who are going to use that functionality, namely an 
 Application 
 Programming Interface
  (
 API
 ). The functions, classes, or methods we write for that 
 component have a particular way of working under certain considerations that, if they are 
 not met, will make our code crash. Conversely, clients calling that code expect a particular 
 response, and any failure of our function to provide this would represent a defect.
  
 That is to say that if, for example, we have a function that is expected to work with a series 
 of parameters of type integers, and some other function invokes our passing strings, it is 
 clear that it should not work as expected, but in reality, the function should not run at all 
 because it was called incorrectly (the client made a mistake). This error should not pass 
 silently.
  
 Of course, when designing an API, the expected input, output, and side-effects should be 
 documented. But documentation cannot enforce the behavior of the software at runtime.
  
 These rules, what every part of the code expects in order to work properly and what the 
 caller is expecting from them, should be part of the design, and here is where the concept of 
 a 
 contract
  comes into place.",NA
Preconditions,"Preconditions are all of the guarantees a function or method expects to receive in order to 
 work correctly. In general programming terms, this usually means to provide data that is 
 properly formed, for example, objects that are initialized, non-null values, and many more.
  
 For Python, in particular, being dynamically typed, this also means that sometimes we 
 need to check for the exact type of data that are provided. This is not exactly the same as 
 type checking, the kind 
 mypy
  would do this, but rather verify for exact values that are 
 needed.
  
 Part of these checks can be detected early on by using static analysis tools, such as 
 mypy
 , 
 which we already introduced in 
 Chapter 1
 , 
 Introduction, Code Formatting, and Tools
 , but these 
 checks are not enough. A function should have proper validation for the information that 
 it is going to handle.
  
 Now, this poses the question of where to place the validation logic, depending on whether 
 we let the clients validate all the data before calling the function, or allow this one to 
 validate everything that it received prior running its own logic. The former corresponds to 
 a tolerant approach (because the function itself is still allowing any data, potentially 
 malformed data as well), whereas the latter corresponds to a demanding approach.
  
 For the purposes of this analysis, we prefer a demanding approach when it comes to DbC, 
 because it is usually the safest choice in terms of robustness, and usually the most common 
 practice in the industry.
  
 Regardless of the approach we decide to take, we should always keep in mind the non-
 redundancy principle, which states that the enforcement of each precondition for a function 
 should be done by only one of the two parts of the contract, but not both. This means that 
 we put the validation logic on the client, or we leave it to the function itself, but in no cases 
 should we duplicate it (which also relates to the DRY principle, which we will discuss later 
 on in this chapter).",NA
Postconditions,"Postconditions are the part of the contract that is responsible for enforcing the state after the 
 method or function has returned.
  
 Assuming that the function or method has been called with the correct properties (that is, 
 with its preconditions met), then the postconditions will guarantee that certain 
 properties are preserved.
  
 The idea is to use postconditions to check and validate for everything that a client might 
 need. If the method executed properly, and the postcondition validations pass, then any 
 client calling that code should be able to work with the returned object without problems, 
 as the contract has been fulfilled.",NA
Pythonic contracts,"At the time of writing this book, a PEP-316, named Programming by Contract for Python, is 
 deferred. This doesn't mean that we cannot implement it in Python, because, as introduced 
 at the beginning of the chapter, this is a general design principle.
  
 Probably the best way to enforce this is by adding control mechanisms to our methods, 
 functions, and classes, and if they rail raise a 
 RuntimeError
  exception or 
 ValueError
 . It's hard 
 to devise a general rule for the correct type of exception, as that would pretty much depend 
 on the application in particular. These previously mentioned exceptions are the most 
 common types of exception, but if they don't fit accurately with the problem, creating a 
 custom exception would be the best choice.
  
 We would also like to keep the code as isolated as possible. That is, the code for the 
 preconditions in one part, the one for the postconditions in another, and the core of the 
 function separated. We could achieve this separation by creating smaller functions, but in 
 some cases implementing a decorator would be an interesting alternative.",NA
Design by contract ,NA,NA
â,NA,NA
 conclusions,"The main value of this design principle is to effectively identify where the problem is. By 
 defining a contract, when something fails at runtime it will be clear what part of the code is 
 broken, and what broke the contract.",NA
Defensive programming,"Defensive programming follows a somewhat different approach than DbC; instead of 
 stating all conditions that must be held in a contract, that if unmet will raise an exception 
 and make the program fail, this is more about making all parts of the code (objects, 
 functions, or methods) able to protect themselves against invalid inputs.
  
 Defensive programming is a technique that has several aspects, and it is particularly useful 
 if it is combined with other design principles (this means that the fact that it follows a 
 different philosophy than DbC does not mean that it is a case of either one or the other
 â
 it 
 could mean that they might complement each other).
  
 The main ideas on the subject of defensive programming are how to handle errors for 
 scenarios that we might expect to occur, and how to deal with errors that should never 
 occur (when impossible conditions happen). The former will fall into error handling 
 procedures, while the latter will be the case for assertions, both topics we will be exploring 
 in the following sections.",NA
Error handling,"In our programs, we resort to error handling procedures for situations that we anticipate as 
 prone to cause errors. This is usually the case for data input.
  
 The idea behind error handling is to gracefully respond to these expected errors in an 
 attempt to either continue our program execution or decide to fail if the error turns out to 
 be insurmountable.
  
 There are different approaches by which we can handle errors on our programs, but not all 
 of them are always applicable. Some of these approaches are as follows:
  
 Value substitution
  
 Error logging
  
 Exception handling",NA
Value substitution,"In some scenarios, when there is an error and there is a risk of the software producing an 
 incorrect value or failing entirely, we might be able to replace the result with another, safer 
 value. We call this value substitution, since we are in fact replacing the actual erroneous 
 result for a value that is to be considered non-disruptive (it could be a default, a well-
 known constant, a sentinel value, or simply something that does not affect the result at all, 
 like returning zero in a case where the result is intended to be applied to a sum).
  
 Value substitution is not always possible, however. This strategy has to be carefully chosen 
 for cases where the substituted value is actually a safe option. Making this decision is a 
 trade-off between robustness and correctness. A software program is robust when it does 
 not fail, even in the presence of an erroneous scenario. But this is not correct either.
  
 This might not be acceptable for some kinds of software. If the application is critical, or the 
 data being handled is too sensitive, this is not an option, since we cannot afford to provide 
 users (or other parts of the application) with erroneous results. In these cases, we opt for 
 correctness, rather than let the program explode when yielding the wrong results.",NA
Exception handling,"In the presence of incorrect or missing input data, sometimes it is possible to correct the 
 situation with some examples such as the ones mentioned in the previous section. In other 
 cases, however, it is better to stop the program from continuing to run with the wrong data 
 than to leave it computing under erroneous assumptions. In those cases, failing and 
 notifying the caller that something is wrong is a good approach, and this is the case for a 
 precondition that was violated, as we saw in DbC.",NA
Handle exceptions at the right level of abstraction,"Exceptions are also part of the principal functions that do one thing, and one thing only. 
 The exception the function is handling (or raising) has to be consistent with the logic 
 encapsulated on it.
  
 In this example, we can see what we mean by mixing different levels of abstractions. 
 Imagine an object that acts as a transport for some data in our application. It connects to an 
 external component where the data is going to be sent upon decoding. In the following 
 listing, we will focus on the 
 deliver_event
  method:
  
 class DataTransport:
  
  
  """"""An example of an object handling exceptions of different levels.""""""
  
  retry_threshold: int = 5
  
  retry_n_times: int = 3
  
  def __init__(self, connector):
  
  self._connector = connector
  
  self.connection = None
  
  def deliver_event(self, event):
  
  try:
  
  
  self.connect()
  
  
  data = event.decode()
  
  
  self.send(data)
  
  except ConnectionError as e:
  
  
  logger.info(""connection error detected: %s"", e)
  
  
  raise
  
  except ValueError as e:
  
  
  logger.error(""%r contains incorrect data: %s"", event, e)
  
  raise
  
  def connect(self):
  
  for _ in range(self.retry_n_times):
  
  
  try:
  
  
  
  self.connection = self._connector.connect()
  
  except 
 ConnectionError as e:
  
  
  
  logger.info(
  
  
  
  
  
  ""%s: attempting new connection in %is"",
   
  
  
  e,
  
  
  
  
  
  self.retry_threshold,
  
  
  
  )
  
  
  
  time.sleep(self.retry_threshold)
  
  
  else:",NA
Do not expose tracebacks,"This is a security consideration. When dealing with exceptions, it might be acceptable to let 
 them propagate if the error is too important, and maybe even let the program fail if this is 
 the decision for that particular scenario and correctness was favored over robustness.
  
 [ 66 ]",NA
Avoid empty except blocks,"This was even referred to as the most diabolical Python anti-pattern (REAL 01). While it is 
 good to anticipate and defend our programs against some errors, being too defensive might 
 lead to even worse problems. In particular, the only problem with being too defensive is 
 that there is an empty 
 except
  block that silently passes without doing anything.
  
 Python is so flexible that it allows us to write code that can be faulty and yet, will not raise 
 an error, like this:
  
 try:
  
  
  process_data() 
  
 except:
  
  
  pass
  
 The problem with this is that it will not fail, ever. Even when it should. It is also non-
 Pythonic if you remember from the zen of Python that errors should never pass silently.
  
 If there is a true exception, this block of code will not fail, which might be what we wanted 
 in the first place. But what if there is a defect? We need to know if there is an error in our 
 logic to be able to correct it. Writing blocks such as this one will mask problems, making 
 things harder to maintain.",NA
Include the original exception,"As part of our error handling logic, we might decide to raise a different one, and 
 maybe even change its message. If that is the case, it is recommended to include the 
 original exception that led to that.
  
 In Python 3 (PEP-3134), we can now use the 
 raise <e> from <original_exception> 
 syntax. When 
 using this construction, the original traceback will be embedded into the new exception, 
 and the original exception will be set in the 
 __cause__
  attribute of the resulting one.",NA
Using assertions in Python,"Assertions are to be used for situations that should never happen, so the expression on the 
 assert
  statement has to mean an impossible condition. Should this condition happen, it 
 means there is a defect in the software.
  
 In contrast with the error handling approach, here there is (or should not be) a possibility of 
 continuing the program. If such an error occurs, the program must stop. It makes sense to 
 stop the program because, as commented before, we are in the presence of a defect, so there 
 is no way to move forward by releasing a new version of the software that corrects this 
 defect.
  
 The idea of using assertions is to prevent the program from causing further damage if such 
 an invalid scenario is presented. Sometimes, it is better to stop and let the program crash, 
 rather than let it continue processing under the wrong assumptions.
  
 For this reason, assertions should not be mixed with the business logic, or used as control 
 flow mechanisms for the software. The following example is a bad idea:
  
 try:
  
  
  assert condition.holds(), ""Condition is not satisfied"" except AssertionError:
  
  
  alternative_procedure()",NA
Separation of concerns,"This is a design principle that is applied at multiple levels. It is not just about the low-level 
 design (code), but it is also relevant at a higher level of abstraction, so it will come up later 
 when we talk about architecture.
  
 Different responsibilities should go into different components, layers, or modules of the 
 application. Each part of the program should only be responsible for a part of the 
 functionality (what we call its concerns) and should know nothing about the rest.
  
 The goal of separating concerns in software is to enhance maintainability by minimizing 
 ripple effects. A 
 ripple
  effect means the propagation of a change in the software from a 
 starting point. This could be the case of an error or exception triggering a chain of other 
 exceptions, causing failures that will result in a defect on a remote part of the application. It 
 can also be that we have to change a lot of code scattered through multiple parts of the code 
 base, as a result of a simple change in a function definition.
  
 Clearly, we do not want these scenarios to happen. The software has to be easy to change. If 
 we have to modify or refactor some part of the code that has to have a minimal impact on 
 the rest of the application, the way to achieve this is through proper encapsulation.",NA
Cohesion and coupling,"These are important concepts for good software design.
  
 On the one hand, 
 cohesion
  means that objects should have a small and well-defined 
 purpose, and they should do as little as possible. It follows a similar philosophy as Unix 
 commands that do only one thing and do it well. The more cohesive our objects are, the 
 more useful and reusable they become, making our design better.
  
 On the other hand, 
 coupling
  refers to the idea of how two or more objects depend on each 
 other. This dependency poses a limitation. If two parts of the code (objects or methods) are 
 too dependent on each other, they bring with them some undesired consequences:
  
 No code reuse
 : If one function depends too much on a particular object, or takes 
 too many parameters, it's coupled with this object, which means that it will be 
 really difficult to use that function in a different context (in order to do so, we 
 will have to find a suitable parameter that complies with a very restrictive 
 interface)
  
 Ripple effects
 : Changes in one of the two parts will certainly impact the other, as 
  
 they are too close
  
 Low level of abstraction
 : When two functions are so closely related, it is hard to 
 see them as different concerns resolving problems at different levels of 
  
 abstraction",NA
Acronyms to live by,"In this section, we will review some principles that yield some good design ideas. The point 
 is to quickly relate to good software practices by acronyms that are easy to remember, 
 working as a sort of mnemonic rule. If you keep these words in mind, you will be able to 
 associate them with good practices more easily, and finding the right idea behind a 
 particular line of code that you are looking at will be faster.
  
 These are by no means formal or academic definitions, but more like empirical ideas that 
 emerged from years of working in the software industry. Some of them do appear in books, 
 as they were coined by important authors (see the references to investigate them in more 
 detail), and others have their roots probably in blog posts, papers, or conference talks.",NA
DRY/OAOO,"The ideas of 
 Don't Repeat Yourself
  (
 DRY
 ) and 
 Once and Only Once
  (
 OAOO
 ) are closely 
 related, so they were included together here. They are self-explanatory, you should avoid 
 duplication at all costs.
  
 Things in the code, knowledge, have to be defined only once and in a single place. When 
 you have to make a change in the code, there should be only one rightful location to 
 modify. Failure to do so is a sign of a poorly designed system.
  
 Code duplication is a problem that directly impacts maintainability. It is very undesirable 
 to have code duplication because of its many negative consequences:
  
 It's error prone
 : When some logic is repeated multiple times throughout the 
 code, and this needs to change, it means we depend on efficiently correcting all 
 the instances with this logic, without forgetting of any of them, because in that 
 case there will be a bug.
  
 It's expensive
 : Linked to the previous point, making a change in multiple places 
 takes much more time (development and testing effort) than if it was defined 
 only once. This will slow the team down.
  
 It's unreliable
 : Also linked to the first point, when multiple places need to be 
 changed for a single change in the context, you rely on the person who wrote the 
 code to remember all the instances where the modification has to be made. There 
 is no single source of truth.",NA
YAGNI,"YAGNI
  (short for 
 You Ain't Gonna Need It
 ) is an idea you might want to keep in mind 
 very often when writing a solution if you do not want to over-engineer it.
  
 We want to be able to easily modify our programs, so we want to make them future-proof. 
 In line with that, many developers think that they have to anticipate all future requirements 
 and create solutions that are very complex, and so create abstractions that are hard to read, 
 maintain, and understand. Sometime later, it turns out that those anticipated requirements 
 do not show up, or they do but in a different way (surprise!), and the original code that was 
 supposed to handle precisely that does not work. The problem is that now it is even harder 
 to refactor and extend our programs. What happened was that the original solution did not 
 handle the original requirements correctly, and neither do the current ones, simply because 
 it is the wrong abstraction.
  
 Having maintainable software is not about anticipating future requirements (do not do 
 futurology!). It is about writing software that only addresses current requirements in such a 
 way that it will be possible (and easy) to change later on. In other words, when designing, 
 make sure that your decisions don't tie you down, and that you will be able to keep on 
 building, but do not build more than what's necessary.",NA
KIS,"KIS 
 (stands for 
 Keep It Simple
 ) relates very much to the previous point. When you are 
 designing a software component, avoid over-engineering it; ask yourself if your solution is 
 the minimal one that fits the problem.
  
 Implement minimal functionality that correctly solves the problem and does not complicate 
 your solution more than is necessary. Remember: the simpler the design, the more 
  
 maintainable it will be.
  
 This design principle is an idea we will want to keep in mind at all levels of abstraction, 
 whether we are thinking of a high-level design, or addressing a particular line of code.
  
 At a high-level, think on the components we are creating. Do we really need all of them?
  
 Does this module actually require being utterly extensible right now? Emphasize the last 
 part
 â
 maybe we want to make that component extensible, but now is not the right time, or 
 it is not appropriate to do so because we still do not have enough information to create the 
 proper abstractions, and trying to come up with generic interfaces at this point will only 
 lead to even worse problems.
  
 In terms of code, keeping it simple usually means using the smallest data structure that fits 
 the problem. You will most likely find it in the standard library.
  
 Sometimes, we might over-complicate code, creating more functions or methods than 
 what's necessary. The following class creates a namespace from a set of keyword arguments 
 that have been provided, but it has a rather complicated code interface:
  
 class ComplicatedNamespace:
  
  
  """"""An convoluted example of initializing an object with some properties.""""""
  
  ACCEPTED_VALUES = (""id_"", ""user"", ""location"")
  
  @classmethod
  
  def init_with_data(cls, **data):
  
  instance = cls()
  
  for key, value in data.items():
  
  
  if key in cls.ACCEPTED_VALUES:
  
  
  
 setattr(instance, key, value) return instance",NA
EAFP/LBYL,"EAFP
  (stands for 
 Easier to Ask Forgiveness than Permission
 ), while 
 LBYL
  (stands for 
 Look Before You Leap
 ).
  
 The idea of EAFP is that we write our code so that it performs an action directly, and then 
 we take care of the consequences later in case it doesn't work. Typically, this means try 
 running some code, expecting it to work, but catching an exception if it doesn't, and then 
 handling the corrective code on the except block.",NA
Composition and inheritance,"In object-oriented software design, there are often discussions as to how to address some 
 problems by using the main ideas of the paradigm (polymorphism, inheritance, and 
 encapsulation).
  
 Probably the most commonly used of these ideas is inheritance
 â
 developers often start by 
 creating a class hierarchy with the classes they are going to need and decide the methods 
 each one should implement.
  
 While inheritance is a powerful concept, it does come with its perils. The main one is that 
 every time we extend a base class, we are creating a new one that is tightly coupled with 
 the parent. As we have already discussed, coupling is one of the things we want to reduce 
 to a minimum when designing software.",NA
When inheritance is a good decision,"We have to be careful when creating a derived class, because this is a double-edged 
 sword
 â
 on the one hand, it has the advantage that we get all the code of the methods from 
 the parent class for free, but on the other hand, we are carrying all of them to a new class, 
 meaning that we might be placing too much functionality in a new definition.
  
 When creating a new subclass, we have to think if it is actually going to use all of the 
 methods it has just inherited, as a heuristic to see if the class is correctly defined. If instead, 
 we find out that we do not need most of the methods, and have to override or replace them, 
 this is a design mistake that could be caused by several reasons:
  
 The superclass is vaguely defined and contains too much responsibility, instead 
  
 of a well-defined interface
  
 The subclass is not a proper specialization of the superclass it is trying to extend
  
 A good case for using inheritance is the type of situation when you have a class that defines 
 certain components with its behavior that are defined by the interface of this class (its public 
 methods and attributes), and then you need to specialize this class in order to create objects 
 that do the same but with something else added, or with some particular parts of its 
 behavior changed.
  
 You can find examples of good uses of inheritance in the Python standard library itself. For 
 example, in the 
 http.server
  package (
 https://docs.python.org/3/library/http.
  
 server.html#http.server.BaseHTTPRequestHandler
 ), we can find a base class such as 
 BaseHTTPRequestHandler
 , and subclasses such as 
 SimpleHTTPRequestHandler
  that extend this 
 one by adding or changing part of its base interface.
  
 Speaking of interface definition, this is another good use for inheritance. When we want to 
 enforce the interface of some objects, we can create an abstract base class that does not 
 implement the behavior itself, but instead just defines the interface
 â
 every class that 
 extends this one will have to implement these to be a proper subtype.",NA
Anti-patterns for inheritance,"If the previous section had to be summarized into a single word, it would 
  
 be specialization
 . 
 The correct use for inheritance is to specialize objects and create more 
 detailed abstractions starting from base ones.
  
 The parent (or base) class is part of the public definition of the new derived class. This is 
 because the methods that are inherited will be part of the interface of this new class. For this 
 reason, when we read the public methods of a class, they have to be consistent with what 
 the parent class defines.
  
 For example, if we see that a class derived from 
 BaseHTTPRequestHandler
  implements a 
 method named 
 handle()
 , it would make sense because it is overriding one of the parents.
  
 If it had any other method whose name relates to an action that has to do with an HTTP 
 request, then we could also think that is correctly placed (but we would not think that if we 
 found something called 
 process_purchase()
  on that class).
  
 The previous illustration might seem obvious, but it is something that happens very often, 
 especially when developers try to use inheritance with the sole goal of reusing code. In the 
 next example, we will see a typical situation that represents a common anti-pattern in 
 Python
 â
 there is a domain problem that has to be represented, and a suitable data structure 
 is devised for that problem, but instead of creating an object that uses such a data structure, 
 the object becomes the data structure itself.
  
 Let's see these problems more concretely through an example. Imagine we have a system 
 for managing insurance, with a module in charge of applying policies to different clients. 
 We need to keep in memory a set of customers that are being processed at the time in order 
 to apply those changes before further processing or persistence. The basic operations we 
 need are to store a new customer with its records as satellite data, apply a change on a 
 policy, or edit some of the data, just to name a few. We also need to support a batch 
 operation, that is, when something on the policy itself changes (the one this module is",NA
Multiple inheritance in Python,"Python supports multiple inheritance. As inheritance, when improperly used, leads to 
 design problems, you could also expect that multiple inheritance will also yield even bigger 
 problems when it's not correctly implemented.
  
 Multiple inheritance is, therefore, a double-edged sword. It can also be very beneficial in 
 some cases. Just to be clear, there is nothing wrong with multiple inheritance
 â
 the only 
 problem it has is that when it's not implemented correctly, it will multiply the problems.
  
 Multiple inheritance is a perfectly valid solution when used correctly, and this opens up 
 new patterns (such as the adapter pattern we discussed in 
 Chapter 9
 , Common Design 
 Patterns)
  and mixins.
  
 One of the most powerful applications of multiple inheritance is perhaps that which 
 enables the creation of mixins. Before exploring mixins, we need to understand how 
 multiple inheritance works, and how methods are resolved in a complex hierarchy.",NA
Method Resolution Order (MRO),"Some people don't like multiple inheritance because of the constraints it has in other 
 programming languages, for instance, the so-called diamond problem. When a class 
 extends from two or more, and all of those classes also extend from other base classes, the 
 bottom ones will have multiple ways to resolve the methods coming from the top-level 
 classes. The question is, which of these implementations is used?
  
 Consider the following diagram, which has a structure with multiple inheritance. The top-
 level class has a class attribute and implements the 
 __str__
  method. Think of any of the 
 concrete classes, for example, 
 ConcreteModuleA12
 â
 it extends from 
 BaseModule1
  and 
 BaseModule2
 , and each one of them will take the implementation of 
 __str__
  from 
 BaseModule
 . 
 Which of these two methods is going to be the one for 
 ConcreteModuleA12
 ?",NA
Mixins,"A mixin is a base class that encapsulates some common behavior with the goal of reusing 
 code. Typically, a mixin class is not useful on its own, and extending this class alone will 
 certainly not work, because most of the time it depends on methods and properties that are 
 defined in other classes. The idea is to use mixin classes along with other ones, through 
 multiple inheritance, so that the methods or properties used on the mixin will be available.
  
 Imagine we have a simple parser that takes a string and provides iteration over it by its 
 values separated by hyphens (-):
  
 class BaseTokenizer:
  
  def __init__(self, str_token):
  
  self.str_token = str_token
  
  def __iter__(self):
  
  yield from self.str_token.split(""-"")
  
 This is quite straightforward:
  
 >>> tk = BaseTokenizer(""28a2320b-fd3f-4627-9792-a2b38e3c46b0"") >>> list(tk) 
  
 ['28a2320b', 'fd3f', '4627', '9792', 'a2b38e3c46b0']",NA
Arguments in functions and methods,"In Python, functions can be defined to receive arguments in several different ways, and 
 these arguments can also be provided by callers in multiple ways.
  
 There is also an industry-wide set of practices for defining interfaces in software 
 engineering that closely relates to the definition of arguments in functions.
  
 In this section, we will first explore the mechanics of arguments in Python functions and 
 afterwards review the general principles of software engineering that relate to good 
 practices on this subject to finally relate both concepts.",NA
How function arguments work in Python,"First, we will explore the particularities of how arguments are passed to functions in 
 Python, and then we will review the general theory of good software engineering practices 
 that relate to these concepts.",NA
How arguments are copied to functions,"The first rule in Python is that all arguments are passed by a value. Always. This means 
 that when passing values to functions, they are assigned to the variables on the signature 
 definition of the function to be later used on it. You will notice that a function changing 
 arguments might depend on the type arguments
 â
 if we are passing 
 mutable
  objects, and 
 the body of the function modifies this, then, of course, we have side-effect that they will 
 have been changed by the time the function returns.
  
 In the following we can see the difference:
  
 >>> def function(argument): 
  
 ...     argument += "" in function"" 
  
 ...     print(argument) 
  
 ...
  
 >>> immutable = ""hello"" 
  
 >>> function(immutable) 
  
 hello in function 
  
 >>> mutable = list(""hello"") 
  
 >>> immutable 
  
 'hello' 
  
 >>> function(mutable) 
  
 ['h', 'e', 'l', 'l', 'o', ' ', 'i', 'n', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n'] 
  
 >>> mutable 
  
 ['h', 'e', 'l', 'l', 'o', ' ', 'i', 'n', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n'] 
  
 >>>
  
 This might look like an inconsistency, but it's not. When we pass the first argument, a 
 string, this is assigned to the argument on the function. Since string objects are immutable, a 
 statement like 
 ""argument += <expression>""
  will in fact create the new object, 
 ""argument + 
 <expression>""
 , and assign that back to the argument. At that point, an 
 argument
  is just a local 
 variable inside the scope of the function and has nothing to do with the original one in the 
 caller.",NA
Variable number of arguments,"Python, as well as other languages, has built-in functions and constructions that can take a 
 variable number of arguments. Consider for example string interpolation functions 
 (whether it be by using the 
 %
  operator or the 
 format
  method for strings), which follow a 
 similar structure to the 
 printf
  function in C, a first positional parameter with the string 
 format, followed by any number of arguments that will be placed on the markers of that 
 formatting string.
  
 Besides taking advantage of these functions that are available in Python, we can also create 
 our own, which will work in a similar fashion. In this section, we will cover the basic 
 principles of functions with a variable number of arguments, along with some 
  
 recommendations, so that in the next section, we can explore how to use these features to 
 our advantage when dealing with common problems, issues, and constraints that functions 
 might have if they have too many arguments.
  
 For a variable number of positionalarguments, the star symbol (
 *
 ) is used, preceding the 
 name of the variable that is packing those arguments. This works through the packing 
 mechanism of Python.",NA
The number of arguments in functions,"In this section, we agree on the idea that having functions or methods that take too many 
 arguments is a sign of bad design (a code smell). Then, we propose ways of dealing with 
 this issue.
  
 The first alternative is a more general principle of software design
 â
 reification
  (creating a 
 new object for all of those arguments that we are passing, which is probably the abstraction 
 we are missing). Compacting multiple arguments into a new object is not a solution specific 
 to Python, but rather something that we can apply in any programming language.
  
 Another option would be to use the Python-specific features we saw in the previous section, 
 making use of variable positional and keyword arguments to create functions that have a 
 dynamic signature. While this might be a Pythonic way of proceeding, we have to be 
 careful not to abuse the feature, because we might be creating something that is so dynamic 
 that it is hard to maintain. In this case, we should take a look at the body of the function. 
 Regardless of the signature, and whether the parameters seem to be correct, if the function 
 is doing too many different things responding to the values of the parameters, then it is a 
 sign that it has to be broken down into multiple smaller functions (remember, functions 
 should do one thing, and one thing only!).",NA
Function arguments and coupling,"The more arguments a function signature has, the more likely this one is going to be tightly 
 coupled with the caller function.
  
 Let's say we have two functions, 
 f1
 , and 
 f2
 , and the latter takes five parameters. The more 
 parameters 
 f2
  takes, the more difficult it would be for anyone trying to call that function to 
 gather all that information and pass it along so that it can work properly.
  
 Now, 
 f1
  seems to have all of this information because it can call it correctly. From this, we 
 can derive two conclusions: first, 
 f2
  is probably a leaky abstraction, which means that since 
 f1
  knows everything that 
 f2
  requires, it can pretty much figure out what it is doing 
 internally and will be able to do it by itself. So, all in all, 
 f2
  is not abstracting that much. 
 Second, it looks like 
 f2
  is only useful to 
 f1
 , and it is hard to imagine using this function in a 
 different context, making it harder to reuse.
  
 When functions have a more general interface and are able to work with higher-level 
 abstractions, they become more reusable.",NA
Compact function signatures that take too many ,NA,NA
arguments,"Suppose we find a function that requires too many parameters. We know that we cannot 
 leave the code base like that, and a refactor is imperative. But, what are the options?
  
 Depending on the case, some of the following rules might apply. This is by no means 
 extensive, but it does provide an idea of how to solve some scenarios that occur quite often.
  
 Sometimes, there is an easy way to change parameters if we can see that most of them 
 belong to a common object. For example, consider a function call like this one:
  
 track_request(request.headers, request.ip_addr, request.request_id)
  
 Now, the function might or might not take additional arguments, but something is really 
 obvious here: all of the parameters depend upon 
 request
 , so why not pass the 
  
 request
  object instead? This is a simple change, but it significantly improves the code. The 
 correct function call should be 
 track_request(request)
 â
 not to mention that, 
  
 semantically, it also makes much more sense.
  
 While passing around parameters like this is encouraged, in all cases where we pass 
 mutable objects to functions, we must be really careful about side-effects. The function we 
 are calling should not make any modifications to the object we are passing because that will 
 mutate the object, creating an undesired side-effect. Unless this is actually the desired effect 
 (in which case, it must be made explicit), this kind of behavior is discouraged. Even when 
 we actually want to change something on the object we are dealing with, a better 
  
 alternative would be to copy it and return a (new) modified version of it.",NA
Final remarks on good practices for ,NA,NA
software design,"A good software design involves a combination of following good practices of software 
 engineering and taking advantage of most of the features of the language. There is a great 
 value in using everything that Python has to offer, but there is also a great risk of abusing 
 this and trying to fit complex features into simple designs. 
  
 In addition to this general principle, it would be good to add some final recommendations.",NA
Orthogonality in software,"This word is very general and can have multiple meanings or interpretations. In math, 
 orthogonal means that two elements are independent. If two vectors are orthogonal, their 
 scalar product is zero. It also means they are not related at all: a change in one of them 
 doesn't affect the other one at all. That's the way we should think about our software.
  
 Changing a module, class, or function should have no impact on the outside world to that 
 component that is being modified. This is of course highly desirable, but not always 
 possible. But even for cases where it's not possible, a good design will try to minimize the 
 impact as much as possible. We have seen ideas such as separation of concerns, cohesion, 
 and isolation of components.
  
 In terms of the runtime structure of software, orthogonality can be interpreted as the fact 
 that makes changes (or side-effects) local. This means, for instance, that calling a method on 
 an object should not alter the internal state of other (unrelated) objects. We have already 
 (and will continue to do so) emphasized in this book the importance of minimizing side-
 effects in our code.
  
 In the example with the mixin class, we created a 
 tokenizer
  object that returned an iterable. 
 The fact that the 
 __iter__
  method returned a new generator increases the chances that all 
 three classes (the base, the mixing, and the concrete class) are orthogonal. If this had 
 returned something in concrete (a list, let's say), this would have created a dependency on 
 the rest of the classes, because when we changed the list to something else, we might have 
 needed to update other parts of the code, revealing that the classes were not as independent 
 as they should be.
  
 Let's show you a quick example. Python allows passing functions by parameter because 
 they are just regular objects. We can use this feature to achieve some orthogonality. We 
 have a function that calculates a price, including taxes and discounts, but afterward we 
 want to format the final price that's obtained:
  
 def calculate_price(base_price: float, tax: float, discount: float) ->
  
  return (base_price * (1 + 
 tax)) * (1 - discount)
  
 def show_price(price: float) -> str:
  
  
  return ""$ {0:,.2f}"".format(price)
  
 def str_final_price(
  
  
  base_price: float, tax: float, discount: float, fmt_function=str ) -> str:
  
  
  return fmt_function(calculate_price(base_price, tax, discount))",NA
Structuring the code,"The way code is organized also impacts the performance of the team and its 
 maintainability.
  
 In particular, having large files with lots of definitions (classes, functions, constants, and so 
 on) is a bad practice and should be discouraged. This doesn't mean going to the extreme of",NA
Summary,"In this chapter, we have explored several principles to achieve a clean design.
  
 Understanding that the code is part of the design is key to achieving high-quality software. 
 This and the following chapter are focused precisely on that.
  
 With these ideas, we can now construct more robust code. For example, by applying DbC, 
 we can create components that are guaranteed to work under their constraints. More 
 importantly, if errors occur, this will not happen out of the blue, but instead, we will have a",NA
References,"Here is a list of information you can refer to:
  
 Object-Oriented Software Construction, Second Edition
 , written by Bertrand Meyer
  
 The Pragmatic Programmer: From Journeyman to Master
 , by Andrew Hunt and 
  
 David Thomas published by Addison-Wesley, 2000.
  
 PEP-316
 : Programming by Contract for Python (
 https://www.python.org/dev/ 
  
 peps/
 pep-0316/
 )
  
 REAL 01
 : The Most Diabolical Python Antipattern: 
 https://realpython.com/ 
  
 blog/python/the-most-diabolical-python-antipattern/
  
 PEP-3134:
  Exception Chaining and Embedded Tracebacks (
 https://www.python. 
  
 org/dev/peps/pep-3134/
 )",NA
The SOLID Principles ,NA,NA
ï´,"In this chapter, we will continue to explore concepts of clean design applied to Python. In 
 particular, we will review the so-called 
 SOLID
  principles, and how to implement them in a 
 Pythonic way. These principles entail a series of good practices to achieve better-quality 
 software. In case some of us aren't aware of what SOLID stands for, here it is:
  
 S
 : Single responsibility principle
  
 O
 : Open/closed principle
  
 L
 : Liskov's substitution principle
  
 I
 : Interface segregation principle
  
 D
 : Dependency inversion principle
  
 The goals of this chapter are as follows:
  
 To become acquainted with SOLID principles for software design
  
 To design software components that follow the single responsibility principle
  
 To achieve more maintainable code through the open/closed principle
  
 To implement proper class hierarchies in object-oriented design, by complying 
  
 with Liskov's substitution principle
  
 To design with interface segregation and dependency inversion",NA
Single responsibility principle,"The 
 single responsibility principle
  (
 SRP
 ) states that a software component (in general, a 
 class) must have only one responsibility. The fact that the class has a sole responsibility 
 means that it is in charge of doing just one concrete thing, and as a consequence of that, we 
 can conclude that it must have only one reason to change.",NA
A class with too many responsibilities,"In this example, we are going to create the case for an application that is in charge of 
 reading information about events from a source (this could be log files, a database, or many 
 more sources), and identifying the actions corresponding to each particular log.
  
 A design that fails to conform to the SRP would look like this:",NA
Distributing responsibilities,"To make the solution more maintainable, we separate every method into a different class. 
 This way, each class will have a single responsibility:
  
  
 The same behavior is achieved by using an object that will interact with instances of these 
 new classes, using those objects as collaborators, but the idea remains that each class 
 encapsulates a specific set of methods that are independent of the rest. The idea now is that 
 changes on any of these classes do not impact the rest, and all of them have a clear and 
 specific meaning. If we need to change something on how we load events from the data 
 sources, the alert system is not even aware of these changes, so we do not have to modify 
 anything on the system monitor (as long as the contract is still preserved), and the data 
 target is also unmodified.
  
 Changes are now local, the impact is minimal, and each class is easier to maintain.
  
 The new classes define interfaces that are not only more maintainable but also reusable. 
 Imagine that now, in another part of the application, we also need to read the activity from 
 the logs, but for different purposes. With this design, we can simply use objects of type 
 ActivityReader
  (which would actually be an interface, but for the purposes of this section, 
 that detail is not relevant and will be explained later for the next principles). This would 
 make sense, whereas it would not have made sense in the previous design, because 
 attempts to reuse the only class we had defined would have also carried extra methods 
 (such as 
 identify_events()
 , or 
 stream_events()
 ) that were not needed at all.
  
 One important clarification is that the principle does not mean at all that each class must 
 have a single method. Any of the new classes might have extra methods, as long as they 
 correspond to the same logic that that class is in charge of handling.",NA
The open/closed principle,"The 
 open/closed principle
  (
 OCP
 ) states that a module should be both open and closed (but 
 with respect to different aspects).
  
 When designing a class, for instance, we should carefully encapsulate the logic so that it has 
 good maintenance, meaning that we will want it to be 
 open to extension but closed for 
 modification.
  
 What this means in simple terms is that, of course, we want our code to be extensible, to 
 adapt to new requirements, or changes in the domain problem. This means that, when 
 something new appears on the domain problem, we only want to add new things to our 
 model, not change anything existing that is closed to modification.
  
 If, for some reason, when something new has to be added, we found ourselves modifying 
 the code, then that logic is probably poorly designed. Ideally, when requirements change, 
 we want to just have to extend the module with the new required behavior in order to 
 comply with the new requirements, but without having to modify the code.
  
 This principle applies to several software abstractions. It could be a class or even a module. 
 In the following two subsections, we will see examples of each one, respectively.",NA
Example of maintainability perils for not following ,NA,NA
the open/closed principle,"Let's begin with an example of a system that is designed in such a way that does not follow 
 the open/closed principle, in order to see the maintainability problems this carries, and the 
 inflexibility of such a design.
  
 The idea is that we have a part of the system that is in charge of identifying events as they 
 occur in another system, which is being monitored. At each point, we want this component 
 to identify the type of event, correctly, according to the values of the data that was 
 previously gathered (for simplicity, we will assume it is packaged into a dictionary, and 
 was previously retrieved through another means such as logs, queries, and many more). We 
 have a class that, based on this data, will retrieve the event, which is another type with its 
 own hierarchy.
  
 A first attempt to solve this problem might look like this:",NA
Refactoring the events system for extensibility,"The problem with the previous example was that the 
 SystemMonitor
  class was interacting 
 directly with the concrete classes it was going to retrieve.
  
 In order to achieve a design that honors the open/closed principle, we have to design 
 toward abstractions
 .
  
 A possible alternative would be to think of this class as it collaborates with the events, and 
 then we delegate the logic for each particular type of event to its corresponding class:",NA
Extending the events system,"Now, let's prove that this design is actually as extensible as we wanted it to be. Imagine that 
 a new requirement arises, and we have to also support events that correspond to 
  
 transactions that the user executed on the monitored system.
  
 The class diagram for the design has to include such a new event type, as in the following:
  
  
 Only by adding the code to this new class does the logic keep working as expected:
  
 # openclosed_3.py 
  
 class Event:
  
  
  def __init__(self, raw_data):
  
  
  
  self.raw_data = raw_data
  
  @staticmethod",NA
Final thoughts about the OCP,"As you might have noticed, this principle is closely related to effective use of 
  
 polymorphism. We want to design toward abstractions that respect a polymorphic contract 
 that the client can use, to a structure that is generic enough that extending the model is 
 possible, as long as the polymorphic relationship is preserved.
  
 This principle tackles an important problem in software engineering: maintainability .The 
 perils of not following the OCP are ripple effects and problems in the software where a 
 single change triggers changes all over the code base, or risks breaking other parts of the 
 code.",NA
Liskov's substitution principle,"Liskov's substitution principle
  (
 LSP
 ) states that there is a series of properties that an object 
 type must hold to preserve reliability on its design.
  
 The main idea behind LSP is that, for any class, a client should be able to use any of its 
 subtypes indistinguishably, without even noticing, and therefore without compromising 
 the expected behavior at runtime. This means that clients are completely isolated and 
 unaware of changes in the class hierarchy.
  
 More formally, this is the original definition (LISKOV 01) of Liskov's substitution principle: 
 if 
 S
  is a subtype of 
 T
 , then objects of type 
 T
  may be replaced by objects of type 
 S
 , without 
 breaking the program.
  
 This can be understood with the help of a generic diagram such as the following one.
  
 Imagine that there is some client class that requires (includes) objects of another type. 
 Generally speaking, we will want this client to interact with objects of some type, namely, it 
 will work through an interface.
  
 Now, this type might as well be just a generic interface definition, an abstract class or an 
 interface, not a class with the behavior itself. There may be several subclasses extending this 
 type (described in the diagram with the name 
 Subtype
 , up to 
 N
 ). The idea behind this 
 principle is that, if the hierarchy is correctly implemented, the client class has to be able to 
 work with instances of any of the subclasses without even noticing. These objects should be 
 interchangeable, as shown here:",NA
Detecting LSP issues with tools,"There are some scenarios so notoriously wrong with respect to the LSP that they can be 
 easily identified by the tools we have learned to configure in 
 Chapter 1
 , 
 Introduction, Code 
 Formatting, and Tools
  (mainly Mypy and Pylint).",NA
Detecting incorrect datatypes in method signatures ,NA,NA
with Mypy,"By using type annotations (as recommended previously in 
 Chapter 1
 , 
 Introduction, Code 
 Formatting, and Tools
 ), throughout our code, and configuring Mypy, we can quickly detect 
 some basic errors early, and check basic compliance with LSP for free.",NA
Detecting incompatible signatures with Pylint,"Another strong violation of LSP is when, instead of varying the types of the parameters on 
 the hierarchy, the signatures of the methods differ completely. This might seem like quite a 
 blunder, but detecting it would not always be so easy to remember; Python is interpreted, 
 so there is no compiler to detect these type of error early on, and therefore they will not be 
 caught until runtime. Luckily, we have static code analyzers such as Mypy and Pylint to 
 catch errors such as this one early on.
  
 While Mypy will also catch these type of error, it is not bad to also run Pylint to gain more 
 insight.
  
 In the presence of a class that breaks the compatibility defined by the hierarchy (for 
 example, by changing the signature of the method, adding an extra parameter, and so on) 
 shown as follows:
  
 # lsp_1.py 
  
 class LogoutEvent(Event):
  
  
  def meets_condition(self, event_data: dict, override: bool) -> bool:
   
  if override:
  
  
  
  return True
  
  
  
  ...
  
 Pylint will detect it, printing an informative error:
  
 Parameters differ from overridden 'meets_condition' method (arguments-differ)
  
 Once again, like in the previous case, do not suppress these errors. Pay attention to the 
 warnings and errors the tools give and adapt the code accordingly.",NA
More subtle cases of LSP violations,"In other cases, however, the way LSP is broken is not so clear or obvious that a tool can 
 automatically identify it for us, and we have to rely upon careful code inspection when 
 doing a code review.
  
 Cases where contracts are modified are particularly harder to detect automatically. Given 
 that the entire idea of LSP is that subclasses can be used by clients just like their parent 
 class, it must also be true that contracts are correctly preserved on the hierarchy.",NA
Remarks on the LSP,"The LSP is fundamental to a good object-oriented software design because it emphasizes 
 one of its core traits
 â
 polymorphism. It is about creating correct hierarchies so that classes 
 derived from a base one are polymorphic along the parent one, with respect to the methods 
 on their interface.
  
 It is also interesting to notice how this principle relates to the previous one
 â
 if we attempt 
 to extend a class with a new one that is incompatible, it will fail, the contract with the client 
 will be broken, and as a result such an extension will not be possible (or, to make it possible, 
 we would have to break the other end of the principle and modify code in the client that 
 should be closed for modification, which is completely undesirable and 
  
 unacceptable).",NA
Interface segregation,"The 
 interface segregation principle
  (
 ISP
 ) provides some guidelines over an idea that we 
 have revisited quite repeatedly already: that interfaces should be small.
  
 In object-oriented terms, an 
 interface
  is represented by the set of methods an object exposes. 
 This is to say that all the messages that an object is able to receive or interpret constitute its 
 interface, and this is what other clients can request. The interface separates the definition of 
 the exposed behavior for a class from its implementation.
  
 In Python, interfaces are implicitly defined by a class according to its methods. This is 
 because Python follows the so-called 
 duck typing
  principle.
  
 Traditionally, the idea behind duck typing was that any object is really represented by the 
 methods it has, and by what it is capable of doing. This means that, regardless of the type of 
 the class, its name, its docstring, class attributes, or instance attributes, what ultimately 
 defines the essence of the object are the methods it has. The methods defined on a class 
 (what it knows how to do) are what determines what that object will actually be. It was 
 called duck typing because of the idea that ""If it walks like a duck, and quacks like a duck, it 
 must be a duck.""
  
 For a long time, duck typing was the sole way interfaces were defined in Python. Later on, 
 Python 3 (PEP-3119) introduced the concept of abstract base classes as a way to define 
 interfaces in a different way. The basic idea of abstract base classes is that they define a 
 basic behavior or interface that some derived classes are responsible for implementing. This 
 is useful in situations where we want to make sure that certain critical methods are actually 
 overridden, and it also works as a mechanism for overriding or extending the functionality 
 of methods such as 
 isinstance()
 .
  
 This module also contains a way of registering some types as part of a hierarchy, in what is 
 called a 
 virtual subclass
 . The idea is that this extends the concept of duck typing a little bit 
 further by adding a new criterion
 â
 walks like a duck, quacks like a duck, or... it says it is a 
 duck.
  
 These notions of how Python interprets interfaces are important for understanding this 
 principle and the next one.",NA
An interface that provides too much,"Now, we want to be able to parse an event from several data sources, in different formats 
 (XML and JSON, for instance). Following good practice, we decide to target an interface as 
 our dependency instead of a concrete class, and something like the following is devised:
  
  
 In order to create this as an interface in Python, we would use an abstract base class and 
 define the methods (
 from_xml()
  and 
 from_json()
 ) as abstract, to force derived classes to 
 implement them. Events that derive from this abstract base class and implement these 
 methods would be able to work with their corresponding types.
  
 But what if a particular class does not need the XML method, and can only be constructed 
 from a JSON? It would still carry the 
 from_xml()
  method from the interface, and since it 
 does not need it, it will have to pass. This is not very flexible as it creates coupling and 
 forces clients of the interface to work with methods that they do not need.",NA
"The smaller the interface, the better","It would be better to separate this into two different interfaces, one for each method:",NA
How small should an interface be?,"The point made in the previous section is valid, but it also needs a warning
 â
 avoid a 
 dangerous path if it's misunderstood or taken to the extreme.
  
 A base class (abstract or not) defines an interface for all the other classes to extend it. The 
 fact that this should be as small as possible has to be understood in terms of cohesion
 â
 it 
 should do one thing. That doesn't mean it must necessarily have one method. In the 
 previous example, it was by coincidence that both methods were doing totally disjoint 
 things, hence it made sense to separate them into different classes.
  
 But it could be the case that more than one method rightfully belongs to the same class.
  
 Imagine that you want to provide a mixin class that abstracts certain logic in a context 
 manager so that all classes derived from that mixin gain that context manager logic for free. 
 As we already know, a context manager entails two methods: 
 __enter__
  and 
 __exit__
 . They 
 must go together, or the outcome will not be a valid context manager at all!
  
 Failure to place both methods in the same class will result in a broken component that is 
 not only useless, but also misleadingly dangerous. Hopefully, this exaggerated example 
 works as a counter-balance to the one in the previous section, and together the reader can 
 get a more accurate picture about designing interfaces.",NA
Dependency inversion,"This is a really powerful idea that will come up again later when we explore some design 
 patterns in 
 Chapter 9
 , 
 Common Design Patterns
 ,and 
 Chapter 10
 , 
 Clean Architecture
 .",NA
A case of rigid dependencies,"The last part of our event's monitoring system is to deliver the identified events to a data 
 collector to be further analyzed. A naive implementation of such an idea would consist of 
 having an event streamer class that interacts with a data destination, for example, 
 Syslog
 :",NA
Inverting the dependencies,"The solution to these problems is to make 
 EventStreamer
  work with an interface, rather 
 than a concrete class. This way, implementing this interface is up to the low-level classes 
 that contain the implementation details:
  
  
 Now there is an interface that represents a generic data target where data is going to be sent 
 to. Notice how the dependencies have now been inverted since 
 EventStreamer
  does not 
 depend on a concrete implementation of a particular data target, it does not have to change 
 in line with changes on this one, and it is up to every particular data target; to implement 
 the interface correctly and adapt to changes if necessary.
  
 In other words, the original 
 EventStreamer
  of the first implementation only worked with 
 objects of type 
 Syslog
 , which was not very flexible. Then we realized that it could work with 
 any object that could respond to a 
 .send()
  message, and identified this method as the 
 interface that it needed to comply with. Now, in this version, 
 Syslog
  is actually extending 
 the abstract base class named 
 DataTargetClient
 , which defines the 
 send()
  method.
  
 From now on, it is up to every new type of data target (email, for instance) to extend this 
 abstract base class and implement the 
 send()
  method.",NA
Summary,"The SOLID principles are key guidelines for good object-oriented software design.
  
 Building software is an incredibly hard task
 â
 the logic of the code is complex, its behavior 
 at runtime is hard (if even possible, sometimes) to predict, requirements change constantly 
 as well as the environment, and there are multiple things that can go wrong.
  
 In addition, there are multiple ways of constructing software with different techniques, 
 paradigms, and a lot of different designs, which can work together to solve a particular 
 problem in a specific manner. However, not all of these approaches will prove to be correct 
 as time passes, and requirements change or evolve. However, by this time, it will already be 
 too late to do something about an incorrect design, as it is rigid, inflexible, and therefore 
 hard to change a refactor into the proper solution.",NA
References,"Here is a list of information you may refer to:
  
 SRP 01
 : The Single Responsibility Principle (
 https://8thlight.com/blog/ 
  
 uncle-
 bob/2014/05/08/SingleReponsibilityPrinciple.html
 )
  
 PEP-3119
 : Introducing Abstract Base Classes (
 https://www.python.org/dev/ 
  
 peps/pep-3119/
 )
  
 LISKOV 01
 : A paper written by Barbara Liskov named 
 Data Abstraction and 
  
 Hierarchy",NA
Using Decorators to Improve ,NA,NA
ïµ,NA,NA
Our Code,"In this chapter, we will explore decorators and see how they are useful in many situations 
 where we want to improve our design. We will start by first exploring what decorators are, 
 how they work, and how they are implemented.
  
 With this knowledge, we will then revisit concepts that we learned in previous chapters 
 regarding general good practices for software design, and see how decorators can help us 
 comply with each principle.
  
 The goals of this chapter are as follows:
  
 To understand how decorators work in Python
  
 To learn how to implement decorators that apply to functions and classes
  
 To effectively implement decorators, avoiding common implementation mistakes
  
 To analyze how to avoid code duplication (the DRY principle) with decorators
  
 To study how decorators contribute to separation of concerns
  
 To analyze examples of good decorators
  
 To review common situations, idioms, or patterns for when decorators are the 
  
 right choice",NA
What are decorators in Python?,NA,NA
Decorate functions,"Functions are probably the simplest representation of a Python object that can be decorated.
  
 We can use decorators on functions to apply all sorts of logic to them
 â
 we can validate 
 parameters, check preconditions, change the behavior entirely, modify its signature, cache 
 results (create a memorized version of the original function), and more.
  
 As an example, we will create a basic decorator that implements a 
 retry
  mechanism, 
 controlling a particular domain-level exception and retrying a certain number of times:
  
 # decorator_function_1.py 
  
 class ControlledException(Exception):
  
  
  """"""A generic exception on the program's domain.""""""
  
 def retry(operation):
  
  
  @wraps(operation)
  
  
  def wrapped(*args, **kwargs):
  
  
  
  last_raised = None
  
  
  
  RETRIES_LIMIT = 3
  
  
  
  for _ in range(RETRIES_LIMIT):
  
  
  
  try:
  
  
  
  
  return operation(*args, **kwargs)
  
  
  
  except ControlledException as e:
  
  
  
  
  logger.info(""retrying %s"", operation.__qualname__)
  
  
  
  
 last_raised = e
  
  
  
  raise last_raised
  
  return wrapped
  
 The use of 
 @wraps
  can be ignored for now, as it will be covered in the section 
  
 named 
 Effective decorators - avoiding common mistakes
 . The use of 
 _
  in the for loop, means that 
 the number is assigned to a variable we are not interested in at the moment, because it's not 
 used inside the for loop (it's a common idiom in Python to name 
 _
  values that are ignored).
  
 The 
 retry
  decorator doesn't take any parameters, so it can be easily applied to any 
 function, as follows:
  
 @retry 
  
 def run_operation(task):
  
  
  """"""Run a particular task, simulating some failures on its execution.""""""
  
  return task.run()",NA
Decorate classes,"Classes can also be decorated (PEP-3129) with the same as can be applied to syntax 
 functions. The only difference is that when writing the code for this decorator, we have to 
 take into consideration that we are receiving a class, not a function.
  
 Some practitioners might argue that decorating a class is something rather convoluted and 
 that such a scenario might jeopardize readability because we would be declaring some 
 attributes and methods in the class, but behind the scenes, the decorator might be applying 
 changes that would render a completely different class.
  
 This assessment is true, but only if this technique is heavily abused. Objectively, this is no 
 different from decorating functions; after all, classes are just another type of object in the 
 Python ecosystem, as functions are. We will review the pros and cons of this issue with 
 decorators in the section titled 
 Decorators and separation of concerns
 , but for now, we'll 
 explore the benefits of decorators that apply particularly to classes:
  
 All the benefits of reusing code and the DRY principle. A valid case of a class 
 decorator would be to enforce that multiple classes conform to a certain interface 
 or criteria (by making this checks only once in the decorator that is going to be 
 applied to those many classes).
  
 We could create smaller or simpler classes that will be enhanced later on by 
  
 decorators
  
 The transformation logic we need to apply to a certain class will be much easier 
 to maintain if we use a decorator, as opposed to more complicated (and often 
 rightfully discouraged) approaches such as metaclasses
  
 Among all possible applications of decorators, we will explore a simple example to give an 
 idea of the sorts of things they can be useful for. Keep in mind that this is not the only 
 application type for class decorators, but also that the code we show you could have many",NA
Other types of decorator,"Now that we know what the 
 @
  syntax for decorators actually means, we can conclude that 
 it isn't just functions, methods, or classes that can be decorated; actually, anything that can 
 be defined, such as generators, coroutines, and even objects that have already been 
 decorated, can be decorated, meaning that decorators can be stacked.
  
 The previous example showed how decorators can be chained. We first defined the class, 
 and then applied 
 @dataclass
  to it, which converted it into a data class, acting as a container 
 for those attributes. After that, the 
 @Serialization
  will apply the logic to that class, resulting 
 in a new class with the new 
 serialize()
  method added to it.
  
 Another good use of decorators is for generators that are supposed to be used as 
  
 coroutines. We will explore the details of generators and coroutines in 
 Chapter 7
 , 
 Using 
 Generators,
  but the main idea is that, before sending any data to a newly created generator, 
 the latter has to be advanced up to their next 
 yield
  statement by calling 
 next()
  on it. This is a 
 manual process that every user will have to remember and hence is error-prone. We could 
 easily create a decorator that takes a generator as a parameter, calls 
 next()
  to it, and then 
 returns the generator.",NA
Passing arguments to decorators,"At this point, we already regard decorators as a powerful tool in Python. However, they 
 could be even more powerful if we could just pass parameters to them so that their logic is 
 abstracted even more.
  
 There are several ways of implementing decorators that can take arguments, but we will go 
 over the most common ones. The first one is to create decorators as nested functions with a 
 new level of indirection, making everything in the decorator fall one level deeper. The 
 second approach is to use a class for the decorator.",NA
Decorators with nested functions,"Roughly speaking, the general idea of a decorator is to create a function that returns a 
 function (often called a higher-order function). The internal function defined in the body of 
 the decorator is going to be the one actually being called.
  
 Now, if we wish to pass parameters to it, we then need another level of indirection. The first 
 one will take the parameters, and inside that function, we will define a new function, which 
 will be the decorator, which in turn will define yet another new function, namely the one to 
 be returned as a result of the decoration process. This means that we will have at least three 
 levels of nested functions.
  
 Don't worry if this didn't seem clear so far. After reviewing the examples that are about to 
 come, everything will become clear.
  
 One of the first examples we saw of decorators implemented the 
 retry
  functionality over 
 some functions. This is a good idea, except it has a problem; our implementation did not 
 allow us to specify the numbers of retries, and instead, this was a fixed number inside the 
 decorator.
  
 Now, we want to be able to indicate how many retries each instance is going to have, and 
 perhaps we could even add a default value to this parameter. In order to do this, we need 
 another level of nested functions
 â
 first for the parameters, and then for the decorator 
 itself.
  
 This is because we are now going to have something in the form of the following:
  
  @retry(arg1, arg2,... )
  
 And that has to return a decorator because the 
 @
  syntax will apply the result of that 
 computation to the object to be decorated. Semantically, it would translate to something 
 like the following:
  
  <original_function> = retry(arg1, arg2, ....)(<original_function>)",NA
Decorator objects,"The previous example requires three levels of nested functions. The first it is going to be a 
 function that receives the parameters of the decorator we want to use. Inside this function, 
 the rest of the functions are closures that use these parameters along with the logic of the 
 decorator.
  
 A cleaner implementation of this would be to use a class to define the decorator. In this 
 case, we can pass the parameters in the 
 __init__
  method, and then implement the logic of the 
 decorator on the magic method named 
 __call__
 .
  
 The code for the decorator will look like it does in the following example:
  
 class WithRetry:
  
  
  def __init__(self, retries_limit=RETRIES_LIMIT, 
 allowed_exceptions=None):
  
  
  
  self.retries_limit = retries_limit
  
  
  
  self.allowed_exceptions = allowed_exceptions or 
 (
 ControlledException
 ,)
  
  def __call__(self, operation):
  
  @wraps(operation)
  
  def wrapped(*args, **kwargs):
  
  last_raised = None
  
  for _ in range(self.retries_limit):
  
  try:
  
  
  
  return operation(*args, **kwargs)
  
  except self.allowed_exceptions as e:
  
  
  
  logger.info(""retrying %s due to %s"", operation, e)
   
  
 last_raised = e
  
  raise last_raised
  
  return wrapped
  
 And this decorator can be applied pretty much like the previous one, like so:",NA
Good uses for decorators,"In this section, we will take a look at some common patterns that make good use of 
 decorators. These are common situations for when decorators are a good choice.
  
 From all the countless applications decorators can be used for, we will enumerate a few, the 
 most common or relevant:
  
 Transforming parameters
 : Changing the signature of a function to expose a nicer 
 API, while encapsulating details on how the parameters are treated and 
  
 transformed underneath
  
 Tracing code
 : Logging the execution of a function with its parameters
  
 Validate parameters
  
 Implement retry operations
  
 Simplify classes by moving some (repetitive) logic into decorators
  
 Let's discuss the first two applications in detail in the following section.",NA
Transforming parameters,"We have mentioned before that decorators can be used to validate parameters (and even 
 enforce some preconditions or postconditions under the idea of DbC), so from this you 
 probably have got the idea that it is somehow common to use decorators when dealing 
 with or manipulating parameters.",NA
Tracing code,"When talking about 
 tracing
  in this section, we will refer to something more general that has 
 to do with dealing with the execution of a function that we wish to monitor. This could refer 
 to scenarios in which we want to:
  
 Actually trace the execution of a function (for example, by logging the lines it 
  
 executes)
  
 Monitor some metrics over a function (such as CPU usage or memory footprint)
  
 Measure the running time of a function
  
 Log when a function was called, and the parameters that were passed to it
  
 In the next section, we will explore a simple example of a decorator that logs the execution 
 of a function, including its name and the time it took to run.",NA
Effective decorators ,NA,NA
â,NA,NA
 avoiding common ,NA,NA
mistakes,"While decorators are a great feature of Python, they are not exempt from issues if used 
 incorrectly. In this section, we will see some common issues to avoid in order to create 
 effective decorators.",NA
Preserving data about the original wrapped ,NA,NA
object,"One of the most common problems when applying a decorator to a function is that some of 
 the properties or attributes of the original function are not maintained, leading to 
  
 undesired, and hard-to-track, side-effects.",NA
Dealing with side-effects in decorators,"In this section, we will learn that it is advisable to avoid side-effects in the body of the 
 decorator. There are cases where this might be acceptable, but the bottom line is that, if in 
 case of doubt, decide against it, for the reasons that are explained ahead. Everything that 
 the decorator needs to do aside from the function that it's decorating should be placed in 
 the innermost function definition, or there will be problems when it comes to importing.
  
 Nonetheless, sometimes these side-effects are required (or even desired) to run at import 
 time, and the obverse applies.
  
 We will see examples of both, and where each one applies. If in doubt, err on the side of 
 caution, and delay all side-effects until the very latest, right after the 
 wrapped
  function is 
 going to be called.
  
 Next, we will see when it's not a good idea to place extra logic outside the 
 wrapped 
 function.",NA
Incorrect handling of side-effects in a decorator,"Let's imagine the case of a decorator that was created with the goal of logging when a 
 function started running and then logging its running time:
  
 def traced_function_wrong(function):
  
  logger.info(""started execution of %s"", function) start_time = 
 time.time()
  
  @functools.wraps(function)
  
  def wrapped(*args, **kwargs):
  
  result = function(*args, **kwargs)
  
  logger.info(
  
  
  ""function %s took %.2fs"",
  
  
  function,
  
  
  time.time() - start_time
  
  )
  
  return result
  
  return wrapped",NA
Requiring decorators with side-effects,"Sometimes, side-effects on decorators are necessary, and we should not delay their 
 execution until the very last possible time, because that's part of the mechanism which is 
 required for them to work.
  
 One common scenario for when we don't want to delay the side-effect of decorators is 
 when we need to register objects to a public registry that will be available in the module.
  
 For instance, going back to our previous 
 event
  system example, we now want to only 
 make some events available in the module, but not all of them. In the hierarchy of events, 
 we might want to have some intermediate classes that are not actual events we want to 
 process on the system, but some of their derivative classes instead.
  
 Instead of flagging each class based on whether it's going to be processed or not, we could 
 explicitly register each class through a decorator.",NA
Creating decorators that will always work,"There are several different scenarios to which decorators might apply. It can also be the case 
 that we need to use the same decorator for objects that fall into these different multiple 
 scenarios, for instance, if we want to reuse our decorator and apply it to a function, a class, a 
 method, or a static method.
  
 If we create the decorator, just thinking about supporting only the first type of object we 
 want to decorate, we might notice that the same decorator does not work equally well on a 
 different type of object. The typical example is where we create a decorator to be used on a 
 function, and then we want to apply it to a method of a class, only to realize that it does not 
 work. A similar scenario might occur if we designed our decorator for a method, and then 
 we want it to also apply for static methods or class methods.
  
 When designing decorators, we typically think about reusing code, so we will want to use 
 that decorator for functions and methods as well.
  
 Defining our decorators with the signature 
 *args
 , and 
 **kwargs
 , will make them work in all 
 cases, because it's the most generic kind of signature that we can have. However, sometimes 
 we might want not to use this, and instead define the decorator wrapping function 
 according to the signature of the original function, mainly because of two reasons:
  
 It will be more readable since it resembles the original function.
  
 It actually needs to do something with the arguments, so receiving 
 *args
  and 
  
 **kwargs
  wouldn't be convenient.
  
 Consider the case on which we have many functions in our code base that require a 
 particular object to be created from a parameter. For instance, we pass a string, and",NA
The DRY principle with decorators,"We have seen how decorators allow us to abstract away certain logic into a separate 
 component. The main advantage of this is that we can then apply the decorator multiple 
 times into different objects in order to reuse code. This follows the 
 Don't Repeat Yourself 
 (
 DRY
 ) principle since we define certain knowledge once and only once.
  
 The 
 retry
  mechanism implemented in the previous sections is a good example of a decorator 
 that can be applied multiple times to reuse code. Instead of making each particular function 
 include its 
 retry
  logic, we create a decorator and apply it several times.
  
 This makes sense once we have made sure that the decorator can work with methods and 
 functions equally.
  
 The class decorator that defined how events are to be represented also complies with the 
 DRY principle in the sense that it defines one specific place for the logic for serializing an 
 event, without needing to duplicate code scattered among different classes. Since we expect 
 to reuse this decorator and apply it to many classes, its development (and complexity) pay 
 off.
  
 This last remark is important to bear in mind when trying to use decorators in order to 
 reuse code
 â
 we have to be absolutely sure that we will actually be saving code.",NA
Decorators and separation of concerns,"The last point on the previous list is so important that it deserves a section of its own. We 
 have already explored the idea of reusing code and noticed that a key element of reusing 
 code is having components that are cohesive. This means that they should have the 
 minimum level of responsibility
 â
 do one thing, one thing only, and do it well. The smaller 
 our components, the more reusable, and the more they can be applied in a different context 
 without carrying extra behavior that will cause coupling and dependencies, which will 
 make the software rigid.
  
 To show you what this means, let's reprise one of the decorators that we used in a previous 
 example. We created a decorator that traced the execution of certain functions with code 
 similar to the following:",NA
Analyzing good decorators,"As a closing note for this chapter, let's review some examples of good decorators and how 
 they are used both in Python itself, as well as in popular libraries. The idea is to get 
 guidelines on how good decorators are created.
  
 Before jumping into examples, let's first identify traits that good decorators should have:
  
 Encapsulation, or separation of concerns
 : A good decorator should effectively 
  
 separate different responsibilities between what it does and what it is decorating.
  
 It cannot be a leaky abstraction, meaning that a client of the decorator should 
 only invoke it in black box mode, without knowing how it is actually 
  
 implementing its logic.
  
 Orthogonality
 : What the decorator does should be independent, and as 
  
 decoupled as possible from the object it is decorating.
  
 Reusability
 : It is desirable that the decorator can be applied to multiple types, 
 and not that it just appears on one instance of one function, because that means 
 that it could just have been a function instead. It has to be generic enough.
  
 A nice example of decorators can be found in the Celery project, where a 
 task
  is defined by 
 applying the decorator of the 
 task
  from the application to a function:
  
 @app.task 
  
 def mytask():
  
  ....
  
 One of the reasons why this is a good decorator is because it is very good at 
  
 something
 â
 encapsulation. The user of the library only needs to define the function body 
 and the decorator will convert that into a task automatically. The 
 ""@app.task""
  decorator 
 surely wraps a lot of logic and code, but none of that is relevant to the body of 
  
 ""mytask()""
 . It is complete encapsulation and separation of concerns
 â
 nobody will have to 
 take a look at what that decorator does, so it is a correct abstraction that does not leak any 
 details.",NA
Summary,"Decorators are powerful tools in Python that can be applied to many things such as classes, 
 methods, functions, generators, and many more. We have demonstrated how to create 
 decorators in different ways, and for different purposes, and drew some conclusions along 
 the way.
  
 When creating a decorator for functions, try to make its signature match the original 
 function being decorated. Instead of using the generic 
 *args
 , and 
 **kwargs
 , making the 
 signature match the original one will make it easier to read, and maintain, and it will 
 resemble the original function more closely, so it will be more familiar to readers of that 
 code.",NA
References,"Here is a list of information you can refer to:
  
 PEP-318
 : Decorators for Functions and Methods (
 https://www.python.org/dev/ 
  
 peps/
 pep-0318/
 )
  
 PEP-3129
 :Class Decorators (
 https://www.python.org/dev/peps/pep-3129/
 )
  
 WRAPT 01
 : 
 https://pypi.org/project/wrapt/
  
 WRAPT 02
 : 
 https://wrapt.readthedocs.io/en/latest/decorators. 
  
 html#universal-decorators
  
 The Functools module
 : The 
 wraps
  function in the 
 functools
  module of Python's 
  
 standard library (
 https://docs.python.org/3/library/functools.
  
 html#functools.wrap
 )
  
 ATTRS 01:
  The 
 attrs
  library (
 https://pypi.org/project/attrs/
 )
  
 PEP-557
 : Data Classes (
 https://www.python.org/dev/peps/pep-0557/
 )
  
 GLASS 01
 : The book written by Robert L. Glass named 
 Facts and Fallacies of 
  
 Software Engineering",NA
Getting More Out of Our ,NA,NA
ï¶,NA,NA
Objects with Descriptors,"This chapter introduces a new concept that is more advanced in Python development since 
 it features descriptors. Moreover, descriptors are not something programmers of other 
 languages are familiar with, so there are no easy analogies or parallelisms to make.
  
 Descriptors are another distinctive feature of Python that takes object-oriented 
  
 programming to another level, and their potential allows users to build more powerful and 
 reusable abstractions. Most of the time, the full potential of descriptors is observed in 
 libraries or frameworks.
  
 In this chapter, we will achieve the following goals that relate to descriptors:
  
 Understand what descriptors are, how they work, and how to implement them 
  
 effectively
  
 Analyze the two types of descriptors (data and non-data descriptors), in term of 
  
 their conceptual differences and implementation details
  
 Reuse code effectively through descriptors
  
 Analyze examples of good uses of descriptors, and how to take advantage of 
  
 them for our own libraries of APIs",NA
A first look at descriptors,"First, we will explore the main idea behind descriptors to understand their mechanics and 
 internal workings. Once this is clear, it will be easier to assimilate how the different types of 
 descriptors work, which we will explore in the next section.",NA
The machinery behind descriptors,"The way descriptors work is not all that complicated, but the problem with them is that 
 there are a lot of caveats to take into consideration, so the implementation details are of the 
 utmost importance here.
  
 In order to implement descriptors, we need at least two classes. For the purposes of this 
 generic example, we are going to call the 
 client
  class to the one that is going to take 
 advantage of the functionality we want to implement in the 
 descriptor
  (this class is generally 
 just a domain model one, a regular abstraction we create for our solution), and we are going 
 to call the 
 descriptor
  class to the one that implements the logic of the 
  
 descriptor.
  
 A descriptor is, therefore, just an object that is an instance of a class that implements the 
 descriptor protocol. This means that this class must have its interface containing at least one 
 of the following magic methods (part of the descriptor protocol as of Python 3.6+):
  
 __get__
  
 __set__
  
 __delete__
  
 __set_name__
  
 For the purposes of this initial high-level introduction, the following naming convention 
 will be used:
  
 Name
  
 Meaning
  
 ClientClass
  
 The domain-level abstraction that will take advantage of the 
  
 functionality to be implemented by the descriptor. This class is said to 
 be a client of the descriptor.
  
 This class contains a class attribute (named 
 descriptor
  by this 
 convention), which is an instance of 
 DescriptorClass
 .
  
 DescriptorClass
  
 The class that implements the 
 descriptor
  itself. This class should 
 implement some of the aforementioned magic methods that entail the 
 descriptor protocol.
  
 client
  
 An instance of 
 ClientClass
 . 
  
 client = ClientClass()
  
 descriptor
  
 An instance of 
 DescriptorClass
 .
  
 descriptor = DescriptorClass()
 .
  
 This object is a class attribute that is placed in 
 ClientClass
 .",NA
Exploring each method of the descriptor protocol,"Up until now, we have seen quite a few examples of descriptors in action, and we got the 
 idea of how they work. These examples gave us a first glimpse of the power of descriptors, 
 but you might be wondering about some implementation details and idioms whose 
 explanation we failed to address.
  
 Since descriptors are just objects, these methods take 
 self
  as the first parameter. For all of 
 them, this just means the 
 descriptor
  object itself.
  
 In this section, we will explore each method of the descriptor protocol, in full detail, 
 explaining what each parameter signifies, and how they are intended to be used.",NA
"__get__(self, instance, owner)","The first parameter, 
 instance
 , refers to the object from which the 
 descriptor
  is being called. 
 In our first example, this would mean the 
 client
  object.
  
 The 
 owner
  parameter is a reference to the class of that object, which following our example 
 (from the previous class diagram in 
 The machinery behind descriptors
  section) would be 
 ClientClass
 .
  
 From the previous paragraph we conclude that the parameter named 
 instance
  in the 
 signature of 
 __get__
  is the object over which the descriptor is taking action, and 
 owner
  is the 
 class of 
 instance
 . The avid reader might be wondering why is the signature define like this, 
 after all the class can be taken from 
 instance
  directly (
 owner = 
  
 instance.__class__
 ). There is an edge case
 â
 when the 
 descriptor
  is called from the class 
 (
 ClientClass
 ), not from the instance (
 client
 ), then the value of 
 instance
  is 
 None
 , but we might 
 still want to do some processing in that case.",NA
"__set__(self, instance, value)","This method is called when we try to assign something to a 
 descriptor
 . It is activated with 
 statements such as the following, in which a 
 descriptor
  is an object that implements 
 __set__ ()
 .
  
 The 
 instance
  parameter, in this case, would be 
 client
 , and 
  
 the 
 value 
 would be the 
 ""value""
  string:
  
 client.descriptor = ""value""
  
 If 
 client.descriptor
  doesn't implement 
 __set__()
 , then 
 ""value""
  will override the 
 descriptor
  
 entirely.",NA
"__delete__(self, instance)","This method is called upon with the following statement, in which 
 self
  would be the 
 descriptor
  attribute, and 
 instance
  would be the 
 client
  object in this example:
  
 >>> del client.descriptor
  
 In the following example, we use this method to create a 
 descriptor
  with the goal of 
 preventing you from removing attributes from an object without the required 
  
 administrative privileges. Notice how, in this case, that the 
 descriptor
  has logic that is 
 used to predicate with the values of the object that is using it, instead of different related 
 objects:
  
 # descriptors_methods_3.py
  
 class ProtectedAttribute:
  
  
  def __init__(self, requires_role=None) -> None:
   
  
 self.permission_required = requires_role",NA
"__set_name__(self, owner, name)","When we create the 
 descriptor
  object in the class that is going to use it, we generally 
 need the 
 descriptor
  to know the name of the attribute it is going to be handling.
  
 This attribute name is the one we use to read from and write to 
 __dict__
  in the 
 __get__ 
 and 
 __set__
  methods, respectively.
  
 Before Python 3.6, the descriptor couldn't take this name automatically, so the most general 
 approach was to just pass it explicitly when initializing the object. This works fine, but it has 
 an issue in that it requires that we duplicate the name every time we want to use the 
 descriptor for a new attribute.",NA
Types of descriptors,"Based on the methods we have just explored, we can make an important distinction among 
 descriptors in terms of how they work. Understanding this distinction plays an important 
 role in working effectively with descriptors, and will also help to avoid caveats or common 
 errors at runtime.
  
 If a descriptor implements the 
 __set__
  or 
 __delete__
  methods, it is called a
  data 
 descriptor
 . Otherwise, a descriptor that solely implements 
 __get__
  is a
  non-data 
 descriptor
 .
  Notice that 
 __set_name__
  does not affect this classification at all.
  
 When trying to resolve an attribute of an object, a data descriptor will always take 
 precedence over the dictionary of the object, whereas a non-data descriptor will not
 .
  This 
 means that in a non-data descriptor if the object has a key on its dictionary with the same 
 name as the descriptor, this one will always be called, and the descriptor itself will never 
 run. Conversely, in a data descriptor, even if there is a key in the dictionary with the same 
 name as the descriptor, this one will never be used since the descriptor itself will always 
 end up being called.
  
 The following two sections explain this in more detail, with examples, in order to get a 
 deeper idea of what to expect from each type of descriptor.",NA
Non-data descriptors,"We will start with a 
 descriptor
  that only implements the 
 __get__
  method, and see how it is 
 used:
  
 class NonDataDescriptor:
  
  
  def __get__(self, instance, owner):
  
  
  
  if instance is None:",NA
Data descriptors,"Now, let's look at the difference of using a data descriptor. For this, we are going to create 
 another simple 
 descriptor
  that implements the 
 __set__
  method:
  
 class DataDescriptor:
  
  def __get__(self, instance, owner):
  
  if instance is None:
  
  
  return self
  
  return 42
  
  def __set__(self, instance, value):
  
  logger.debug(""setting %s.descriptor to %s"", instance, value) 
 instance.__dict__[""descriptor""] = value
  
 class ClientClass:
  
  
  descriptor = DataDescriptor()
  
  Let's see what the value of the 
 descriptor
  returns:
  
 >>> client = ClientClass() 
  
 >>> client.descriptor 
  
 42",NA
Descriptors in action,"Now that we have seen what descriptors are, how they work, and what the main ideas 
 behind them are, we can see them in action. In this section, we will be exploring some 
 situations that can be elegantly addressed through descriptors.
  
 Here, we will look at some examples of working with descriptors, and we will also cover 
 implementation considerations for them (different ways of creating them, with their pros 
 and cons), and finally we will discuss what the most suitable scenarios for descriptors are.",NA
An application of descriptors,"We will start with a simple example that works, but that will lead to some code duplication.
  
 It is not very clear how this issue will be addressed. Later on, we will devise a way of 
 abstracting the repeated logic into a descriptor, which will address the duplication 
 problem, and we will notice that the code on our client classes will be reduced drastically.",NA
A first attempt without using descriptors,"The problem we want to solve now is that we have a regular class with some attributes, but 
 we wish to track all of the different values a particular attribute has over time, for example, 
 in a list. The first solution that comes to our mind is to use a property, and every time a 
 value is changed for that attribute in the setter method of the property, we add it to an 
 internal list that will keep this trace as we want it.
  
 Imagine that our class represents a traveler in our application that has a current city, and 
 we want to keep track of all the cities that user has visited throughout the running of the 
 program. The following code is a possible implementation that addresses these 
  
 requirements:
  
 class Traveller:
  
  def __init__(self, name, current_city): self.name = name
  
  self._current_city = current_city self._cities_visited = 
 [current_city]
  
  @property
  
  def current_city(self):
  
  return self._current_city
  
  @current_city.setter",NA
The idiomatic implementation,"We will now look at how to address the questions of the previous section by using a 
 descriptor that is generic enough as to be applied in any class. Again, this example is not 
 really needed because the requirements do not specify such generic behavior (we haven't 
 even followed the rule of three instances of the similar pattern previously creating the 
 abstraction), but it is shown with the goal of portraying descriptors in action.",NA
Different forms of implementing descriptors,"We have to first understand a common issue that's specific to the nature of descriptors 
 before thinking of ways of implementing them. First, we will discuss the problem of a 
 global shared state, and afterward we will move on and look at different ways descriptors 
 can be implemented while taking this into consideration.",NA
The issue of global shared state,"As we have already mentioned, descriptors need to be set as class attributes to work. This 
 should not be a problem most of the time, but it does come with some warnings that 
 need to be taken into consideration.
  
 The problem with class attributes is that they are shared across all instances of that class. 
 Descriptors are not an exception here, so if we try to keep data in a 
 descriptor
  object, keep 
 in mind that all of them will have access to the same value.
  
 Let's see what happens when we incorrectly define a 
 descriptor
  that keeps the data itself, 
 instead of storing it in each object:
  
 class SharedDataDescriptor:
  
  
  def __init__(self, initial_value):
  
  
  
  self.value = initial_value
  
  def __get__(self, instance, owner):
  
  if instance is None:
  
  
  return self
  
  return self.value
  
  def __set__(self, instance, value):
  
  self.value = value
  
 class ClientClass:
  
  
  descriptor = SharedDataDescriptor(""first value"")",NA
Accessing the dictionary of the object,"The way we implement descriptors throughout this book is making the 
 descriptor
  object 
 store the values in the dictionary of the object, 
 __dict__
 , and retrieve the parameters from 
 there as well.",NA
Using weak references,"Another alternative (if we don't want to use 
 __dict__
 ) is to make the 
 descriptor
  object keep 
 track of the values for each instance itself, in an internal mapping, and return values from 
 this mapping as well.
  
 There is a caveat, though. This mapping cannot just be any dictionary. Since the 
 client 
 class 
 has a reference to the descriptor, and now the descriptor will keep references to the objects 
 that use it, this will create circular dependencies, and, as a result, these objects will never 
 be garbage-collected because they are pointing at each other.
  
 In order to address this, the dictionary has to be a weak key one, as defined in the 
 weakref
  (WEAKREF 01) module.
  
 In this case, the code for the 
 descriptor
  might look like the following:
  
 from weakref import WeakKeyDictionary
  
 class DescriptorClass:
  
  
  def __init__(self, initial_value):
  
  
  
 self.value = initial_value
  
  
  
  self.mapping = WeakKeyDictionary()
  
  def __get__(self, instance, owner):
  
  if instance is None:
  
  
  return self
  
  return self.mapping.get(instance, self.value)
  
  def __set__(self, instance, value):
  
  self.mapping[instance] = value
  
 This addresses the issues, but it does come with some considerations:
  
 The objects no longer hold their attributes
 â
 the descriptor does instead. This is 
 somewhat controversial, and it might not be entirely accurate from a conceptual 
 point of view. If we forget this detail, we might be asking the object by inspecting 
 its dictionary, trying to find things that just aren't there (calling 
  
 vars(client)
  will not return the complete data, for example).",NA
More considerations about descriptors,"Here, we will discuss general considerations about descriptors in terms of what we can do 
 with them when it is a good idea to use them, and also how things that we might have 
 initially conceived as having been resolved by means of another approach can be improved 
 through descriptors. We will then analyze the pros and cons of the original implementation 
 versus the one after descriptors have been used.",NA
Reusing code,"Descriptors are a generic tool and a powerful abstraction that we can use to avoid code 
 duplication. The best way to decide when to use descriptors is to identify cases where we 
 would be using a property (whether for its 
 get
  logic, 
 set
  logic, or both), but repeating its 
 structure many times.
  
 Properties are just a particular case of descriptors (the 
 @property
  decorator is a descriptor 
 that implements the full descriptor protocol to define their 
 get
 , 
 set
 , and 
 delete
  actions), 
 which means that we can use descriptors for far more complex tasks.
  
 Another powerful type we have seen for reusing code was decorators, as explained in 
 Chapter 5
 , 
 Using Decorators to Improve Our Code
 . Descriptors can help us create to better 
 decorators by making sure that they will be able to work correctly for class methods as 
 well.
  
 When it comes to decorators, we could say that it is safe to always implement the 
  
 __get__()
  method on them, and also make it a descriptor. When trying to decide whether the 
 decorator is worth creating, consider the three problems rule we stated in 
 Chapter 5
 , 
 Using 
 Decorators to Improve Our Code
 , but note that there are no extra considerations toward 
 descriptors.",NA
Avoiding class decorators,"If we recall the class decorator we used in 
 Chapter 5,
 Using Decorators to Improve Our Code
 , to 
 determine how an event object is going to be serialized, we ended up with an 
  
 implementation that (for Python 3.7+) relied on two class decorators:
  
 @Serialization(
  
  
  username=show_original,
  
  
  password=hide_field,
  
  
  ip=show_original,
  
  
  timestamp=format_time, 
  
 ) 
  
 @dataclass 
  
 class LoginEvent:
  
  
  username: str
  
  
  password: str
  
  
  ip: str
  
  
  timestamp: datetime
  
 The first one takes the attributes from the annotations to declare the variables, whereas the 
 second one defines how to treat each file. Let's see whether we can change these two 
 decorators for descriptors instead.",NA
Analysis of descriptors,"We have seen how descriptors work so far and explored some interesting situations in 
 which they contribute to clean design by simplifying their logic and leveraging more 
 compact classes.
  
 Up to this point, we know that by using descriptors, we can achieve cleaner code, 
  
 abstracting away repeated logic and implementation details. But how do we know our 
 implementation of the descriptors is clean and correct? What makes a good descriptor? Are 
 we using this tool properly or over-engineering with it?
  
 In this section, we will analyze descriptors in order to answer these questions.",NA
How Python uses descriptors internally,"Referring to the question as to what makes a good descriptor?, a simple answer would be 
 that a good descriptor is pretty much like any other good Python object. It is consistent with 
 Python itself. The idea that follows this premise is that analyzing how Python uses 
  
 descriptors will give us a good idea of good implementations so that we know what to 
 expect from the descriptors we write.
  
 We will see the most common scenarios where Python itself uses descriptors to solve parts 
 of its internal logic, and we will also discover elegant descriptors and that they have been 
 there in plain sight all along.",NA
Functions and methods,"The most resonating case of an object that is a descriptor is probably a function. Functions 
 implement the 
 __get__
  method, so they can work as methods when defined inside a class.
  
 Methods are just functions that take an extra argument. By convention, the first argument 
 of a method is named ""self"", and it represents an instance of the class that the method is 
 being defined in. Then, whatever the method does with ""self"", would be the same as any 
 other function receiving the object and applying modifications to it.",NA
Built-in decorators for methods,"As you might have known from looking at the official documentation (PYDESCR-02), all 
 @property
 , 
 @classmethod
 , and 
 @staticmethod
  decorators are descriptors.",NA
Slots,"When a class defines the 
 __slots__
  attribute, it can contain all the attributes that the class 
 expects and no more.
  
 Trying to add extra attributes dynamically to a class that defines 
 __slots __
 will result in an 
 AttributeError
 . By defining this attribute, the class becomes static, so it will not have a 
 __dict__
  
 attribute where you can add more objects dynamically.
  
 How, then, are its attributes retrieved if not from the dictionary of the object? By using 
 descriptors. Each name defined in a slot will have its own descriptor that will store the 
 value for retrieval later:
  
 class Coordinate2D:
  
  
  __slots__ = (""lat"", ""long"")
  
  def __init__(self, lat, long):
  
  self.lat = lat
  
  self.long = long
  
  def __repr__(self):
  
  return f""{self.__class__.__name__}({self.lat}, {self.long})""
  
 While this is an interesting feature, it has to be used with caution because it is taking away 
 the dynamic nature of Python. In general, this ought to be reserved only for objects that we 
 know are static, and if we are absolutely sure we are not adding any attributes to them 
 dynamically in other parts of the code.
  
 As an upside of this, objects defined with slots use less memory, since they only need a 
 fixed set of fields to hold values and not an entire dictionary.",NA
Implementing descriptors in decorators,"We now understand how Python uses descriptors in functions to make them work as 
 methods when they are defined inside a class. We have also seen examples of cases where 
 we can make decorators work by making them comply with the descriptor protocol by 
 using the 
 __get__()
  method of the interface to adapt the decorator to the object it is being 
 called with. This solves the problem for our decorators in the same way that Python solves 
 the issue of functions as methods in objects.",NA
Summary,"Descriptors are a more advanced feature in Python that push the boundaries, closer to 
 metaprogramming. One of their most interesting aspects is how they make crystal-clear that 
 classes in Python are just regular objects, and, as such, they have properties and we can 
 interact with them. Descriptors are, in this sense, the most interesting type of attribute a 
 class can have because its protocol facilitates more advanced, object-oriented possibilities.
  
 We have seen the mechanics of descriptors, their methods, and how all of this fits together, 
 making a more interesting picture of object-oriented software design. By understanding 
 descriptors, we were able to create powerful abstractions that yield clean and compact 
 classes. We have seen how to fix decorators that we want to apply to functions and 
 methods, we have understood a lot more about how Python works internally, and how 
 descriptors play such a core and critical role in the implementation of the language.
  
 This study of how descriptors are used internally in Python should work as a reference to 
 identify good uses of descriptors in our own code, with the goal of achieving idiomatic 
 solutions.
  
 Despite all of the powerful options that descriptors represent to our advantage, we have to 
 keep in mind when to properly make use of them without over-engineering. In this line, we 
 have suggested that we should reserve the functionality of descriptors for truly generic 
 cases, such as the design of internal development APIs, libraries, or frameworks. Another 
 important consideration along these lines is that, in general, we should not place business 
 logic in descriptors, but rather logic that implements technical functionality to be used by 
 other components that do contain business logic.",NA
References,"Here is a list of a few things you can reference for more information:
  
 Python's official documentation on descriptors (
 https://docs.python.org/3/ 
  
 reference/datamodel.html#implementing-descriptors
 )
  
 WEAKREF 01
 : Python 
 weakref
  module (
 https://docs.python.org/3/library/ 
  
 weakref.html
 )
  
 PYDESCR-02
 : Built-in decorators as descriptors (
 https://docs.python.org/3/ 
  
 howto/descriptor.html#static-methods-and-class-methods
 )",NA
Using Generators ,NA,NA
ï·,"Generators are another of those features that makes Python a peculiar language over more 
 traditional ones. In this chapter, we will explore their rationale, why they were introduced 
 in the language, and the problems they solve. We will also cover how to address problems 
 idiomatically by using generators, and how to make our generators (or any iterable, for that 
 matter) Pythonic.
  
 We will understand why iteration (in the form of the iterator pattern) is automatically 
 supported in the language. From there, we will take another journey and explore how 
 generators became such a fundamental feature of Python in order to support other 
 functionality, such as coroutines and asynchronous programming.
  
 The goals for this chapter are as follows:
  
 To create generators that improve the performance of our programs
  
 To study how iterators (and the iterator pattern, in particular) are deeply 
  
 embedded in Python
  
 To solve problems that involve iteration idiomatically
  
 To understand how generators work as the basis for coroutines and 
  
 asynchronous programming
  
 To explore the syntactic support for coroutines
 â
 yield from
 , 
 await
 , and 
 async 
  
 def",NA
Technical requirements,"The examples in this chapter will work with any version of Python 3.6 on any platform.
  
 The code used in this chapter can be found at 
 https://github.com/PacktPublishing/ Clean-
 Code-in-Python.
  
 The instructions are available in the 
 README
  file.",NA
Creating generators,"Generators were introduced in Python a long time ago (PEP-255), with the idea of 
  
 introducing iteration in Python while improving the performance of the program (by using 
 less memory) at the same time.
  
 The idea of a generator is to create an object that is iterable, and, while it's being iterated, 
 will produce the elements it contains, one at a time. The main use of generators is to save 
 memory
 â
 instead of having a very large list of elements in memory, holding everything at 
 once, we have an object that knows how to produce each particular element, one at a time, 
 as they are required.
  
 This feature enables lazy computations or heavyweight objects in memory, in a similar 
 manner to what other functional programming languages (Haskell, for instance) provide. It 
 would even be possible to work with infinite sequences because the lazy nature of 
  
 generators allows for such an option.",NA
A first look at generators,"Let's start with an example. The problem at hand now is that we want to process a large list 
 of records and get some metrics and indicators over them. Given a large data set with 
 information about purchases, we want to process it in order to get the lowest sale, highest 
 sale, and the average price of a sale.
  
 For the simplicity of this example, we will assume a CSV with only two fields, in the 
 following format:
  
 <purchase_date>, <price> 
  
 ...
  
 We are going to create an object that receives all the purchases, and this will give us the 
 necessary metrics. We could get some of these values out of the box by simply using the 
 min()
  and 
 max()
  built-in functions, but that would require iterating all of the purchases more 
 than once, so instead, we are using our custom object, which will get these values in a single 
 iteration.",NA
Generator expressions,"Generators save a lot of memory, and since they are iterators, they are a convenient 
 alternative to other iterables or containers that require more space in memory such as lists, 
 tuples, or sets.
  
 Much like these data structures, they can also be defined by comprehension, only that it is 
 called a generator expression (there is an ongoing argument about whether they should be 
 called generator comprehensions. In this book, we will just refer to them by their canonical 
 name, but feel free to use whichever you prefer).
  
 In the same way, we would define a list comprehension. If we replace the square brackets 
 with parenthesis, we get a generator that results from the expression. Generator 
  
 expressions can also be passed directly to functions that work with iterables, such as 
 sum()
 , 
 and, 
 max()
 :
  
 >>> [x**2 for x in range(10)] 
  
 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
  
 >>> (x**2 for x in range(10)) 
  
 <generator object <genexpr> at 0x...>
  
 >>> sum(x**2 for x in range(10)) 
  
 285",NA
Iterating idiomatically,"In this section, we will first explore some idioms that come in handy when we have to deal 
 with iteration in Python. These code recipes will help us get a better idea of the types of 
 things we can do with generators (especially after we have already seen generator 
 expressions), and how to solve typical problems in relation to them.
  
 Once we have seen some idioms, we will move on to exploring iteration in Python in more 
 depth, analyzing the methods that make iteration possible, and how iterable objects work.",NA
Idioms for iteration,"We are already familiar with the built-in 
 enumerate()
  function that, given an iterable, will 
 return another one on which the element is a tuple, whose first element is the enumeration 
 of the second one (corresponding to the element in the original iterable):
  
 >>> list(enumerate(""abcdef"")) 
  
 [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e'), (5, 'f')]
  
 We wish to create a similar object, but in a more low-level fashion; one that can simply 
 create an infinite sequence. We want an object that can produce a sequence of numbers, 
 from a starting one, without any limits.
  
 An object as simple as the following one can do the trick. Every time we call this object, we 
 get the next number of the sequence 
 ad infinitum
 :
  
 class NumberSequence:
  
  def __init__(self, start=0):
  
  self.current = start
  
  def next(self):
  
  current = self.current
  
  self.current += 1
  
  return current",NA
The next() function,"The 
 next()
  built-in function will advance the iterable to its next element and return it:
  
 >>> word = iter(""hello"") 
  
 >>> next(word) 
  
 'h' 
  
 >>> next(word) 
  
 'e'  # ...
  
 If the iterator does not have more elements to produce, the 
 StopIteration
  exception is 
 raised:
  
 >>> ...
  
 >>> next(word) 
  
 'o' 
  
 >>> next(word) 
  
 Traceback (most recent call last):
  
  File ""<stdin>"", line 1, in <module> 
  
 StopIteration 
  
 >>>
  
 This exception signals that the iteration is over and that there are no more elements to 
 consume.
  
 If we wish to handle this case, besides catching the 
 StopIteration
  exception, we could 
 provide this function with a default value in its second parameter. Should this be provided, 
 it will be the return value in lieu of throwing 
 StopIteration
 :
  
 >>> next(word, ""default value"") 
  
 'default value'",NA
Using a generator,"The previous code can be simplified significantly by simply using a generator. Generator 
 objects are iterators. This way, instead of creating a class, we can define a function that 
 yield
  the values as needed:
  
 def sequence(start=0):
  
  
  while True:
  
  
  
  yield start
  
  
  
  start += 1
  
 Remember that from our first definition, the 
 yield
  keyword in the body of the function 
 makes it a generator. Because it is a generator, it's perfectly fine to create an infinite loop 
 like this, because, when this generator function is called, it will run all the code until the 
 next 
 yield 
 statement is reached. It will produce its value and suspend there:
  
 >>> seq = sequence(10) 
  
 >>> next(seq) 
  
 10 
  
 >>> next(seq) 
  
 11
  
 >>> list(zip(sequence(), ""abcdef"")) 
  
 [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e'), (5, 'f')]",NA
Itertools,"Working with iterables has the advantage that the code blends better with Python itself 
 because iteration is a key component of the language. Besides that, we can take full 
 advantage of the 
 itertools
  module (ITER-01). Actually, the 
 sequence()
  generator we just 
 created is fairly similar to 
 itertools.count()
 . However, there is more we can do.
  
 One of the nicest things about iterators, generators, and itertools, is that they are 
 composable objects that can be chained together.
  
 For instance, getting back to our first example that processed 
 purchases
  in order to get 
 some metrics, what if we want to do the same, but only for those values over a certain 
 threshold? The naive approach of solving this problem would be to place the condition 
 while iterating:
  
 # ...
  
  def process(self):
  
  for purchase in self.purchases:",NA
Simplifying code through iterators,"Now, we will briefly discuss some situations that can be improved with the help of 
 iterators, and occasionally the 
 itertools
  module. After discussing each case, and its 
 proposed optimization, we will close each point with a corollary.",NA
Repeated iterations,"Now that we have seen more about iterators, and introduced the 
 itertools
  module, we can 
 show you how one of the first examples of this chapter (the one for computing statistics 
 about some purchases), can be dramatically simplified:
  
 def process_purchases(purchases):
  
  min_, max_, avg = itertools.tee(purchases, 3) return min(min_), 
 max(max_), median(avg)
  
 In this example, 
 itertools.tee
 will split the original iterable into three new ones. We will use 
 each of these for the different kinds of iterations that we require, without needing to 
 repeat three different loops over 
 purchases
 .
  
 The reader can simply verify that if we pass an iterable object as the 
 purchases
  parameter, 
 this one is traversed only once (thanks to the 
 itertools.tee
  function [see references]), which 
 was our main requirement. It is also possible to verify how this version is equivalent to our 
 original implementation. In this case, there is no need to manually raise 
 ValueError 
 because 
 passing an empty sequence to the 
 min()
  function will do the same.
  
  
 If you are thinking about running a loop over the same object more than 
 one time, stop and think if 
 itertools.tee
  can be of any help.",NA
Nested loops,"In some situations, we need to iterate over more than one dimension, looking for a value, 
 and nested loops come as the first idea. When the value is found, we need to stop iterating, 
 but the 
 break
  keyword doesn't work entirely because we have to escape from two (or more) 
 for
  loops, not just one.
  
 What would be the solution for this? A flag signaling escape? No. Raising an exception? 
 No, this would be the same as the flag, but even worse because we know that exceptions 
 are not to be used for control flow logic. Moving the code to a smaller function and return 
 it? Close, but not quite.
  
 The answer is, whenever possible, flat the iteration to a single 
 for
  loop.
  
 This is the kind of code we would like to avoid:
  
 def search_nested_bad(array, desired_value): coords = None
  
  for i, row in enumerate(array):",NA
The iterator pattern in Python,"Here, we will take a small detour from generators to understand iteration in Python more 
 deeply. Generators are a particular case of iterable objects, but iteration in Python goes 
 beyond generators, and being able to create good iterable objects will give us the chance to 
 create more efficient, compact, and readable code.
  
 In the previous code listings, we have been seeing examples of iterable objects that are 
 also iterators, because they implement both the 
 __iter__()
  and 
 __next__()
 magic methods. 
 While this is fine in general, it's not strictly required that they always have to 
 implement both methods, and here we'll show the subtle differences between 
  
 an iterable object (one that implements 
 __iter__
 ) and an iterator (that 
  
 implements 
 __next__
 ).
  
 We also explore other topics related to iterations, such as sequences and container objects.",NA
The interface for iteration,"An iterable is an object that supports iteration, which, at a very high level, means that we 
 can run a 
 for .. in ...
  loop over it, and it will work without any issues. 
  
 However, iterable does not mean the same as iterator
 .
  
 Generally speaking, an iterable is just something we can iterate, and it uses an iterator to do 
 so. This means that in the 
 __iter__
  magic method, we would like to return an iterator, 
 namely, an object with a 
 __next__()
  method implemented.
  
 An iterator is an object that only knows how to produce a series of values, one at a time, 
 when it's being called by the already explored built-in 
 next()
  function. While the iterator is 
 not called, it's simply frozen, sitting idly by until it's called again for the next value to 
 produce. In this sense, generators are iterators.
  
 Python concept
  
  Magic 
 method
  
 Considerations
  
 Iterable
  
 __iter__
  
 They work with an iterator to construct the iteration logic. 
 These objects can be iterated in a 
 for ... in ...:
  loop
  
 Iterator
  
 __next__
  
 Define the logic for producing values one at the time.
  
 The 
 StopIteration
  exception signals that the iteration is 
 over.
  
 The values can be obtained one by one via the built-in 
 next()
  
 function.",NA
Sequence objects as iterables,"As we have just seen, if an object implements the 
 __iter__()
  magic method, it means it can 
 be used in a 
 for
  loop. While this is a great feature, it's not the only possible form of 
 iteration we can achieve. When we write a 
 for
  loop, Python will try to see if the object",NA
Coroutines,"As we already know, generator objects are iterables. They implement 
 __iter__()
  and 
 __next__()
 . 
 This is provided by Python automatically so that when we create a generator object 
 function, we get an object that can be iterated or advanced through the 
 next() 
 function.
  
 Besides this basic functionality, they have more methods so that they can work as 
 coroutines (PEP-342). Here, we will explore how generators evolved into coroutines to 
 support the basis of asynchronous programming before we go into more detail in the next 
 section, where we explore the new features of Python and the syntax that covers 
  
 programming asynchronously. The basic methods added in (PEP-342) to support 
 coroutines are as follows:
  
 .close()
  
 .throw(ex_type[, ex_value[, ex_traceback]])
  
 .send(value)",NA
The methods of the generator interface,"In this section, we will explore what each of the aforementioned methods does, how it 
 works, and how it is expected to be used. By understanding how to use these methods, we 
 will be able to make use of simple coroutines.
  
 Later on, we will explore more advanced uses of coroutines, and how to delegate to sub-
 generators (coroutines) in order to refactor code, and how to orchestrate different 
 coroutines.",NA
close(),"When calling this method, the generator will receive the 
 GeneratorExit
  exception. If it's not 
 handled, then the generator will finish without producing any more values, and its 
 iteration will stop.
  
 This exception can be used to handle a finishing status. In general, if our coroutine does 
 some sort of resource management, we want to catch this exception and use that control 
 block to release all resources being held by the coroutine. In general, it is similar to using a 
 context manager or placing the code in the 
 finally
  block of an exception control, but 
 handling this exception specifically makes it more explicit.
  
 In the following example, we have a coroutine that makes use of a database handler object 
 that holds a connection to a database, and runs queries over it, streaming data by pages of a 
 fixed length (instead of reading everything that is available at once):
  
 def stream_db_records(db_handler):
  
  
  try:
  
  
  
  while True:
  
  
  
  yield db_handler.read_n_records(10)
  
  except 
 GeneratorExit:
  
  
  
  db_handler.close()
  
 At each call to the generator, it will return 
 10
  rows obtained from the database handler, but 
 when we decide to explicitly finish the iteration and call 
 close()
 , we also want to close the 
 connection to the database:
  
 >>> streamer = stream_db_records(DBHandler(""testdb"")) 
  
 >>> next(streamer) 
  
 [(0, 'row 0'), (1, 'row 1'), (2, 'row 2'), (3, 'row 3'), ...] >>> next(streamer) 
  
 [(0, 'row 0'), (1, 'row 1'), (2, 'row 2'), (3, 'row 3'), ...] >>> streamer.close() 
  
 INFO:...:closing connection to database 'testdb'",NA
"throw(ex_type[, ex_value[, ex_traceback]])","This method will 
 throw
  the exception at the line where the generator is currently 
 suspended. If the generator handles the exception that was sent, the code in that 
 particular 
 except
  clause will be called, otherwise, the exception will propagate to the 
 caller.
  
 Here, we are modifying the previous example slightly to show the difference when we use 
 this method for an exception that is handled by the coroutine, and when it's not:
  
 class CustomException(Exception):
  
  
  pass
  
 def stream_data(db_handler):
  
  
  while True:
  
  
  
  try:
  
  
  
  yield db_handler.read_n_records(10)
  
  
  
  except CustomException as e:
  
  
  
  logger.info(""controlled error %r, continuing"", e)
   
  except 
 Exception as e:
  
  
  
  logger.info(""unhandled error %r, stopping"", e)
  
  
  
 db_handler.close()
  
  
  
  break
  
 Now, it is a part of the control flow to receive a 
 CustomException
 , and, in such a case, the 
 generator will log an informative message (of course, we can adapt this according to our 
 business logic on each case), and move on to the next 
 yield
  statement, which is the line 
 where the coroutine reads from the database and returns that data.
  
 This particular example handles all exceptions, but if the last block (
 except Exception:
 ) wasn't 
 there, the result would be that the generator is raised at the line where the generator is 
 paused (again, the 
 yield
 ),
  and it will propagate from there to the caller:
  
 >>> streamer = stream_data(DBHandler(""testdb"")) 
  
 >>> next(streamer) 
  
 [(0, 'row 0'), (1, 'row 1'), (2, 'row 2'), (3, 'row 3'), (4, 'row 4'), ...] >>> next(streamer) 
  
 [(0, 'row 0'), (1, 'row 1'), (2, 'row 2'), (3, 'row 3'), (4, 'row 4'), ...] >>> 
 streamer.throw(CustomException)",NA
send(value),"In the previous example, we created a simple generator that reads rows from a database, 
 and when we wished to finish its iteration, this generator released the resources linked to 
 the database. This is a good example of using one of the methods that generators provide 
 (close), but there is more we can do.
  
 An obvious of such a generator is that it was reading a fixed number of rows from the 
 database.
  
 We would like to parametrize that number (
 10
 ) so that we can change it throughout 
 different calls. Unfortunately, the 
 next()
  function does not provide us with options for 
 that. But luckily, we have 
 send()
 :
  
 def stream_db_records(db_handler):
  
  
  retrieved_data = None
  
  
  previous_page_size = 10
  
  
  try:
  
  
  
  while True:
  
  
  
  page_size = yield retrieved_data
  
  
  if 
 page_size is None:
  
  
  
  
  page_size = previous_page_size
  
  previous_page_size = page_size
  
  
  retrieved_data = db_handler.read_n_records(page_size) except GeneratorExit:
  
  db_handler.close()",NA
More advanced coroutines,"So far, we have a better understanding of coroutines, and we are able to create simple ones 
 to handle small tasks. We can say that these coroutines are, in fact, just more advanced 
 generators (and that would be right, coroutines are just fancy generators), but, if we 
 actually want to start supporting more complex scenarios, we usually have to go for a 
 design that handles many coroutines concurrently, and that requires more features.
  
 When handling many coroutines, we find new problems. As the control flow of our 
 application becomes more complex, we want to pass values up and down the stack (as well 
 as exceptions), be able to capture values from sub-coroutines we might call at any level, and 
 finally schedule multiple coroutines to run toward a common goal.
  
 To make things simpler, generators had to be extended once again. This is what PEP-380 
 addressed
 â
 by changing the semantic of generators so that they are able to return values, 
 and introducing the new 
 yield from
  construction.",NA
Returning values in coroutines,"As introduced at the beginning of this chapter, the iteration is a mechanism that calls 
 next()
  on an iterable object many times until a 
 StopIteration
  exception is raised.
  
 So far, we have been exploring the iterative nature of generators
 â
 we produce values one at 
 a time, and, in general, we only care about each value as it's being produced at every step of 
 the 
 for
  loop. This is a very logical way of thinking about generators, but coroutines have a 
 different idea; even though they are technically generators, they weren't conceived with the 
 idea of iteration in mind, but with the goal of suspending the execution of a code until it's 
 resumed later on.
  
 This is an interesting challenge; when we design a coroutine, we usually care more about 
 suspending the state rather than iterating (and iterating a coroutine would be an odd case).
  
 The challenge lies in that it is easy to mix them both. This is because of a technical 
 implementation detail; the support for coroutines in Python was built upon generators.",NA
Delegating into smaller coroutines ,NA,NA
â,NA,NA
 the yield from ,NA,NA
syntax,"The previous feature is interesting in the sense that it opens up a lot of new possibilities 
 with coroutines (generators), now that they can return values. But this feature, by itself,",NA
The simplest use of yield from,"In its most basic form, the new 
 yield from
  syntax can be used to chain generators from 
 nested 
 for
  loops into a single one, which will end up with a single string of all the values in 
 a continuous stream.
  
 The canonical example is about creating a function similar to 
 itertools.chain()
  from the 
 standard
  library. This is a very nice function because it allows you to pass any number of 
 iterables
 , and will return them all together in one stream. 
  
 The naive implementation might look like this:
  
 def chain(*iterables):
  
  
  for it in iterables:
  
  
  
  for value in it:
  
  
  
  yield value
  
 It receives a variable number of 
 iterables
 , traverses through all of them, and since each 
 value is 
 iterable
 , it supports a 
 for... in..
  construction, so we have another 
 for
  loop to get 
 every value inside each particular iterable, which is produced by the caller function.
  
 This might be helpful in multiple cases, such as chaining generators together or trying to 
 iterate things that it wouldn't normally be possible to compare in one go (such as lists with 
 tuples, and so on).
  
 However, the 
 yield from
  syntax allows us to go further and avoid the nested loop because 
 it's able to produce the values from a sub-generator directly. In this case, we could 
 simplify the code like this:
  
 def chain(*iterables):
  
  
  for it in iterables:
  
  
  
  yield from it
  
 Notice that for both implementations, the behavior of the generator is exactly the same:
  
 >>> list(chain(""hello"", [""world""], (""tuple"", "" of "", ""values.""))) ['h', 'e', 'l', 'l', 'o', 'world', 
 'tuple', ' of ', 'values.']",NA
Capturing the value returned by a sub-generator,"In the following example, we have a generator that calls another two nested generators, 
 producing values in a sequence. Each one of these nested generators returns a value, and 
 we will see how the top-level generator is able to effectively capture the return value since 
 it's calling the internal generators through 
 yield from
 :
  
 def sequence(name, start, end):
  
  logger.info(""%s started at %i"", name, start) yield from 
 range(start, end)
  
  logger.info(""%s finished at %i"", name, end) return end
  
 def main():
  
  step1 = yield from sequence(""first"", 0, 5)
  
  step2 = yield from sequence(""second"", step1, 10) return step1 + step2
  
 This is a possible execution of the code in main while it's being iterated:
  
 >>> g = main() 
  
 >>> next(g) 
  
 INFO:generators_yieldfrom_2:first started at 0 0 
  
 >>> next(g) 
  
 1 
  
 >>> next(g) 
  
 2 
  
 >>> next(g)",NA
Sending and receiving data to and from a sub-generator,"Now, we will see the other nice feature of the 
 yield from
  syntax, which is probably what 
 gives it its full power. As we have already introduced when we explored generators acting 
 as coroutines, we know that we can send values and throw exceptions at them, and, in such 
 cases, the coroutine will either receive the value for its internal processing, or it will have to 
 handle the exception accordingly.",NA
Asynchronous programming,"With the constructions we have seen so far, we are able to create asynchronous programs in 
 Python. This means that we can create programs that have many coroutines, schedule them 
 to work in a particular order, and switch between them when they're suspended after a 
 yield from
  has been called on each of them.
  
 The main advantage that we can take out of this is the possibility of parallelizing I/O 
 operations in a non-blocking way. What we would need is a low-level generator (usually 
 implemented by a third-party library) that knows how to handle the actual I/O while the 
 coroutine is suspended. The idea is for the coroutine to effect suspension so that our 
 program can handle another task in the meantime. The way the application would retrieve 
 the control back is by means of the 
 yield from
  statement, which will suspend and produce a 
 value to the caller (as in the examples we saw previously when we used this syntax to 
 alter the control flow of the program).
  
 This is roughly the way asynchronous programming had been working in Python for quite 
 a few years, until it was decided that better syntactic support was needed.",NA
Summary,"Generators are everywhere in Python. Since their inception in Python a long time ago, they 
 proved to be a great addition that makes programs more efficient and iteration much 
 simpler.
  
 As time moved on, and more complex tasks needed to be added to Python, generators 
 helped again in supporting coroutines.
  
 And, while in Python, coroutines are generators, we still don't have to forget that they're 
 semantically different. Generators are created with the idea of iteration, while coroutines 
 have the goal of asynchronous programming (suspending and resuming the execution of 
 a part of our program at any given time). This distinction became so important that it 
 made Python's syntax (and type system) evolve.
  
 Iteration and asynchronous programming constitute the last of the main pillars of Python 
 programming. Now, it's time to see how everything fits together and to put all of these 
 concepts we have been exploring over the past few chapters into action.
  
 The following chapters will describe other fundamental aspects of Python projects, such as 
 testing, design patterns, and architecture.",NA
References,"Here is a list of information you can refer to:
  
 PEP-234
 : Iterators (
 https://www.python.org/dev/peps/pep-0234/
 )
  
 PEP-255
 : Simple Generators (
 https://www.python.org/dev/peps/pep-0255/
 )
  
 ITER-01
 : Python's itertools module (
 https://docs.python.org/3/library/ 
  
 itertools.
 html
 )
  
 GoF
 : The book written by Erich Gamma, Richard Helm, Ralph Johnson, John 
 Vlissides named 
 Design Patterns: Elements of Reusable Object-Oriented Software
  
 PEP-342
 : Coroutines via Enhanced Generators(
 https://www.python.org/dev/ 
  
 peps/
 pep-0342/
 )
  
 PYCOOK
 : The book  written by Brian Jones, David Beazley named 
 Python 
  
 Cookbook: Recipes for Mastering Python 3, Third Edition
  
 PY99
 : Fake threads (generators, coroutines, and continuations) (
 https://mail. 
  
 python.org/pipermail/python-dev/1999-July/000467.html
 )
  
 CORO-01
 : Co Routine (
 http://wiki.c2.com/?CoRoutine
 )
  
 CORO-02
 : Generators Are Not Coroutines (
 http://wiki.c2.com/? 
  
 GeneratorsAreNotCoroutines
 )
  
 TEE
 : The 
 itertools.tee
  function (
 https://docs.python.org/3/library/ 
  
 itertools.
 html#itertools.tee
 )",NA
Unit Testing and Refactoring ,NA,NA
ï¸,"The ideas explored in this chapter are fundamental pillars in the global context of the book, 
 because of their importance towards our ultimate goal: to write better and more 
  
 maintainable software.
  
 Unit tests (and any form of automatic tests, for that matter) are critical to software 
 maintainability, and therefore are something that cannot be missing from any quality 
 project. It is for that reason that this chapter is dedicated exclusively to aspects of 
 automated testing as a key strategy, to safely modify the code, and iterate over it, in 
 incrementally better versions.
  
 After this chapter, we will have gained more insight into the following:
  
 Why automated tests are critical for projects that run under an agile software 
  
 development methodology
  
 How unit tests work as a heuristic of the quality of the code
  
 What frameworks and tools are available to develop automated tests and set up 
  
 quality gates
  
 Taking advantage of unit tests to understand the domain problem better and 
  
 document code
  
 Concepts related to unit testing, such as test-driven development",NA
Design principles and unit testing,NA,NA
A note about other forms of automated testing,"Unit tests are intended to verify very small units, for example a function, or a method. We 
 want from our unit tests to reach a very detailed level of granularity, testing as much code 
 as possible. To test a class we would not want to use a unit tests, but rather a test suite, 
 which is a collection of unit tests. Each one of them will be testing something more specific, 
 like a method of that class.
  
 This is not the only form of unit tests, and it cannot catch every possible error. There are 
 also acceptance and integration tests, both out of the scope of this book.
  
 In an integration test, we will want to test multiple components at once. In this case we 
 want to validate if collectively, they work as expected. In this case is acceptable (more than 
 that, desirable) to have side-effects, and to forget about isolation, meaning that we will 
 want to issue HTTP requests, connect to databases, and so on.
  
 An acceptance test is an automated form of testing that tries to validate the system from the 
 perspective of an user, typically executing use cases.
  
 These two last forms of testing lose another nice trait with respect of unit tests: velocity. As 
 you can imagine, they will take more time to run, therefore they will be run less frequently.
  
 In a good development environment, the programmer will have the entire test suite, and 
 will run unit tests all the time, repeatedly, while he or she is making changes to the code, 
 iterating, refactoring, and so on. Once the changes is ready, and the pull request is open, the 
 continuous integration service will run the build for that branch, where the unit tests will 
 run, as long as the integration or acceptance tests that might exist. Needless to say, the 
 status of the build should be successful (green) before merging, but the important part is the 
 difference between the kind of tests: we want to run unit tests all the  time, and less 
 frequently those test that take longer. For this reason, we want to have a lot of small unit 
 tests, and a few automated tests, strategically designed to cover as much as possible of 
 where the unit tests could not reach (the database, for instance).
  
 Finally, a word to the wise. Remember that this book encourages pragmatism. Besides these 
 definitions give, and the points made about unit tests in the beginning of the section, the 
 reader has to keep in mind that the best solution according to your criteria and context, 
 should predominate. Nobody knows your system better than you. Which means, if for 
 some reason you have to write an unit tests that needs to launch a Docker container to test 
 against a database, go for it. As we have repeatedly remembered throughout the book, 
 practicality beats purity
 .",NA
Unit testing and agile software development,"In modern software development, we want to deliver value constantly, and as quickly as 
 possible. The rationale behind these goals is that the earlier we get feedback, the less the 
 impact, and the easier it will be to change. These are no new ideas at all; some of them 
 resemble manufacturing principles from decades ago, and others (such as the idea of 
 getting feedback from stakeholders as soon as possible and iterating upon it) you can find 
 in essays such as 
 The Cathedral and the Bazaar
  (abbreviated as 
 CatB
 ).
  
 Therefore, we want to be able to respond effectively to changes, and for that, the software 
 we write will have to change. Like we mentioned in the previous chapters, we want our 
 software to be adaptable, flexible, and extensible.
  
 The code alone (regardless of how well written and designed it is) cannot guarantee us that 
 it's flexible enough to be changed. Let's say we design a piece of software following the 
 SOLID principles, and in one part we actually have a set of components that comply with 
 the open/closed principle, meaning that we can easily extend them without affecting too 
 much existing code. Assume further that the code is written in a way that favors 
  
 refactoring, so we could change it as required. What's to say that when we make these 
 changes, we aren't introducing any bugs? How do we know that existing functionality is 
 preserved? Would you feel confident enough releasing that to your users? Will they believe 
 that the new version works just as expected?
  
 The answer to all of these questions is that we can't be sure unless we have a formal proof 
 of it. And unit tests are just that, formal proof that the program works according to the 
 specification.
  
 Unit (or automated) tests, therefore, work as a safety net that gives us the confidence to 
 work on our code. Armed with these tools, we can efficiently work on our code, and 
 therefore this is what ultimately determines the velocity (or capacity) of the team working 
 on the software product. The better the tests, the more likely it is we can deliver value 
 quickly without being stopped by bugs every now and then.",NA
Unit testing and software design,"This is the other face of the coin when it comes to the relationship between the main code 
 and unit testing. Besides the pragmatic reasons explored in the previous section, it comes 
 down to the fact that good software is testable software. 
 Testability
  (the quality attribute 
 that determines how easy to test software is) is not just a nice to have, but a driver for clean 
 code.",NA
Defining the boundaries of what to test,"Testing requires effort. And if we are not careful when deciding what to test, we will never 
 end testing, hence wasting a lot of effort without achieving much.
  
 We should scope the testing to the boundaries of our code. If we don't, we would have to 
 also test the dependencies (external/third-party libraries or modules) or our code, and then 
 their respective dependencies, and so on and so forth in a never-ending journey. It's not our 
 responsibility to test dependencies, so we can assume that these projects have tests of their 
 own. It would be enough just to test that the correct calls to external dependencies are done",NA
Frameworks and tools for testing,"There are a lot of tools we can use for writing out unit tests, all of them with pros and cons 
 and serving different purposes. But among all of them, there are two that will most likely 
 cover almost every scenario, and therefore we limit this section to just them.
  
 Along with testing frameworks and test running libraries, it's often common to find projects 
 that configure code coverage, which they use as a quality metric. Since coverage (when 
 used as a metric) is misleading, after seeing how to create unit tests we'll discuss why it's 
 not to be taken lightly.",NA
Frameworks and libraries for unit testing,"In this section, we will discuss two frameworks for writing and running unit tests. The first 
 one, 
 unittest
 , is available in the standard library of Python, while the second 
  
 one, 
 pytest
 , has to be installed externally via 
 pip
 .
  
 unittest
 : 
 https://docs.python.org/3/library/unittest.html
  
 pytest
 : 
 https://docs.pytest.org/en/latest/
  
 When it comes to covering testing scenarios for our code, 
 unittest
  alone will most likely 
 suffice, since it has plenty of helpers. However, for more complex systems on which we 
 have multiple dependencies, connections to external systems, and probably the need to 
 patch objects, and define fixtures parameterize test cases, then 
 pytest
  looks like a more 
 complete option.",NA
unittest,"The 
 unittest
  module is a great option with which to start writing unit tests because it 
 provides a rich API to write all kinds of testing conditions, and since it's available in the 
 standard library, it's quite versatile and convenient.
  
 The 
 unittest
  module is based on the concepts of JUnit (from Java), which in turn is also 
 based on the original ideas of unit testing that come from Smalltalk, so it's object-oriented in 
 nature. For this reason, tests are written through objects, where the checks are verified by 
 methods, and it's common to group tests by scenarios in classes.
  
 To start writing unit tests, we have to create a test class that inherits from 
  
 unittest.TestCase
 , and define the conditions we want to stress on its methods. These 
 methods should start with 
 test_*
 , and can internally use any of the methods inherited 
 from 
 unittest.TestCase
  to check conditions that must hold true.
  
 Some examples of conditions we might want to verify for our case are as follows:
  
 class TestMergeRequestStatus(unittest.TestCase):
  
  def test_simple_rejected(self):
  
  merge_request = MergeRequest()
  
  merge_request.downvote(""maintainer"")
  
  self.assertEqual(merge_request.status, MergeRequestStatus.REJECTED)
  
  def test_just_created_is_pending(self):
  
  self.assertEqual(MergeRequest().status, MergeRequestStatus.PENDING)
  
  def test_pending_awaiting_review(self):
  
  merge_request = MergeRequest()
  
  merge_request.upvote(""core-dev"")
  
  self.assertEqual(merge_request.status, MergeRequestStatus.PENDING)
  
  def test_approved(self):
  
  merge_request = MergeRequest()
  
  merge_request.upvote(""dev1"")
  
  merge_request.upvote(""dev2"")
  
  self.assertEqual(merge_request.status, MergeRequestStatus.APPROVED)
  
 The API for unit testing provides many useful methods for comparison, the most common 
 one being 
 assertEquals(<actual>, <expected>[, message])
 , which can be used to compare the 
 result of the operation against the value we were expecting, optionally using a message that 
 will be shown in the case of an error.",NA
Parametrized tests,"Now, we would like to test how the threshold acceptance for the merge request works, just 
 by providing data samples of what the 
 context
  looks like without needing the entire 
 MergeRequest
  object. We want to test the part of the 
 status
  property that is after the line that 
 checks if it's closed, but independently.
  
 The best way to achieve this is to separate that component into another class, use 
 composition, and then move on to test this new abstraction with its own test suite:
  
 class AcceptanceThreshold:
  
  
  def __init__(self, merge_request_context: dict) -> None:
   
  self._context = 
 merge_request_context",NA
pytest,"Pytest is a great testing framework, and can be installed via 
 pip install pytest
 . A difference 
 with respect to 
 unittest
  is that, while it's still possible to classify test scenarios in classes and 
 create object-oriented models of our tests, this is not actually mandatory, and it's possible to 
 write unit tests with less boilerplate by just checking the conditions we want to verify with 
 the 
 assert
 statement.
  
 By default, making comparisons with an 
 assert
  statement will be enough for 
 pytest
  to 
 identify a unit test and report its result accordingly. More advanced uses such as those seen 
 in the previous section are also possible, but they require using specific functions from the 
 package.",NA
Basic test cases with pytest,"The conditions we tested in the previous section can be rewritten in simple functions with 
 pytest
 .
  
 Some examples with simple assertions are as follows:
  
 def test_simple_rejected():
  
  merge_request = MergeRequest()
  
  merge_request.downvote(""maintainer"")
  
  assert merge_request.status == MergeRequestStatus.REJECTED
  
 def test_just_created_is_pending():
  
  
  assert MergeRequest().status == MergeRequestStatus.PENDING
  
 def test_pending_awaiting_review():
  
  merge_request = MergeRequest()
  
  merge_request.upvote(""core-dev"")
  
  assert merge_request.status == MergeRequestStatus.PENDING
  
 Boolean equality comparisons don't require more than a simple 
 assert
  statement, whereas 
 other kinds of checks like the ones for the exceptions do require that we use some functions:
  
 def test_invalid_types():
  
  merge_request = MergeRequest()
  
  pytest.raises(TypeError, merge_request.upvote, {""invalid-object""})
  
 def test_cannot_vote_on_closed_merge_request():
  
  
  merge_request = MergeRequest()
  
  
  merge_request.close()
  
  
  pytest.raises(MergeRequestException, merge_request.upvote, ""dev1"")
  
  with pytest.raises(
  
  
  
  MergeRequestException,
  
  
  
  match=""can't vote on a closed merge request"",
  
  
  ):
  
  
  
  merge_request.downvote(""dev1"")",NA
Parametrized tests,"Running parametrized tests with 
 pytest
  is better, not only because it provides a cleaner 
 API, but also because each combination of the test with its parameters generates a new test 
 case.
  
 To work with this, we have to use the 
 pytest.mark.parametrize
 decorator on our test. The 
 first parameter of the decorator is a string indicating the names of the parameters to pass 
 to the 
 test
  function, and the second has to be iterable with the respective values for those 
 parameters.
  
 Notice how the body of the testing function is reduced to one line (after removing the 
 internal 
 for
  loop, and its nested context manager), and the data for each test case is 
 correctly isolated from the body of the function, making it easier to extend and maintain:
  
 @pytest.mark.parametrize(""context,expected_status"", (
  
  
  (
  
  
  
  {""downvotes"": set(), ""upvotes"": set()},
  
  
  
  MergeRequestStatus.PENDING
  
  
  ),
  
  
  (
  
  
  
  {""downvotes"": set(), ""upvotes"": {""dev1""}},
  
  
  
  MergeRequestStatus.PENDING,
  
  
  ),
  
  
  (
  
  
  
  {""downvotes"": ""dev1"", ""upvotes"": set()},
  
  
  
  MergeRequestStatus.REJECTED
  
  
  ),
  
  
  (
  
  
  
  {""downvotes"": set(), ""upvotes"": {""dev1"", ""dev2""}},
  
  
  
  MergeRequestStatus.APPROVED
  
  
  ), 
  
 ))",NA
Fixtures,"One of the great things about 
 pytest
  is how it facilitates creating reusable features so that 
 we can feed our tests with data or objects in order to test more effectively and without 
 repetition.
  
 For example, we might want to create a 
 MergeRequest
  object in a particular state, and use 
 that object in multiple tests. We define our object as a fixture by creating a function and 
 applying the 
 @pytest.fixture
  decorator. The tests that want to use that fixture will have to 
 have a parameter with the same name as the function that's defined, and 
 pytest
  will make 
 sure that it's provided:
  
 @pytest.fixture 
  
 def rejected_mr():
  
  
  merge_request = MergeRequest()
  
  merge_request.downvote(""dev1"")
  
  merge_request.upvote(""dev2"")
  
  merge_request.upvote(""dev3"")
  
  merge_request.downvote(""dev4"")
  
  return merge_request
  
 def test_simple_rejected(rejected_mr):
  
  
  assert rejected_mr.status == MergeRequestStatus.REJECTED
  
 def test_rejected_with_approvals(rejected_mr):
  
  rejected_mr.upvote(""dev2"")
  
  rejected_mr.upvote(""dev3"")
  
  assert rejected_mr.status == MergeRequestStatus.REJECTED
  
 def test_rejected_to_pending(rejected_mr):
  
  rejected_mr.upvote(""dev1"")
  
  assert rejected_mr.status == MergeRequestStatus.PENDING
  
 def test_rejected_to_approved(rejected_mr):
  
  rejected_mr.upvote(""dev1"")
  
  rejected_mr.upvote(""dev2"")
  
  assert rejected_mr.status == MergeRequestStatus.APPROVED",NA
Code coverage,"Tests runners support coverage plugins (to be installed via 
 pip
 ) that will provide useful 
 information about what lines in the code have been executed while the tests were running.
  
 This information is of great help so that we know which parts of the code need to be 
 covered by tests, as well identifying improvements to be made (both in the production code 
 and in the tests). One of the most widely used libraries for this is 
 coverage
  (
 https://pypi. org/
 project/coverage/
 ).
  
 While they are of great help (and we highly recommend that you use them and configure 
 your project to run coverage in the CI when tests are run), they can also be misleading; 
 particularly in Python, we can get a false impression if we don't pay close attention to the 
 coverage report.",NA
Setting up rest coverage,"In the case of 
 pytest
 , we have to install the 
 pytest-cov
  package (at the time of this writing, 
 version 
 2.5.1
  is used in this book). Once installed, when the tests are run, we have to tell the 
 pytest
  runner that 
 pytest-cov
  will also run, and which package (or packages) should be 
 covered (among other parameters and configurations).
  
 This package supports multiple configurations, like different sorts of output formats, and 
 it's easy to integrate it with any CI tool, but among all these features a highly recommended 
 option is to set the flag that will tell us which lines haven't been covered by tests yet, 
 because this is what's going to help us diagnose our code and allow us to start writing more 
 tests.",NA
Caveats of test coverage,"Python is interpreted and, at a very high-level, coverage tools take advantage of this to 
 identify the lines that were interpreted (run) while the tests were running. It will then 
 report this at the end. The fact that a line was interpreted does not mean that it was 
 properly tested, and this is why we should be careful about reading the final coverage 
 report and trusting what it says.",NA
Mock objects,"There are cases where our code is not the only thing that will be present in the context of 
 our tests. After all, the systems we design and build have to do something real, and that 
 usually means connecting to external services (databases, storage services, external APIs, 
 cloud services, and so on). Because they need to have those side-effects, they're inevitable.
  
 As much as we abstract our code, program towards interfaces, and isolate code from 
 external factors in order to minimize side-effects, they will be present in our tests, and we 
 need an effective way to handle that.
  
 Mock
  objects are one of the best tactics to defend against undesired side-effects. Our code 
 might need to perform an HTTP request or send a notification email, but we surely don't 
 want that to happen in our unit tests. Besides, unit tests should run quickly, as we want to 
 run them quite often (all the time, actually), and this means we cannot afford latency. 
 Therefore, real unit tests don't use any actual service
 â
 they don't connect to any database, 
 they don't issue HTTP requests, and basically, they do nothing other than exercise the logic 
 of the production code.
  
 We need tests that do such things, but they aren't units. Integration tests are supposed to 
 test functionality with a broader perspective, almost mimicking the behavior of a user. But 
 they aren't fast. Because they connect to external systems and services, they take longer to 
 run and are more expensive. In general, we would like to have lots of unit tests that run 
 really quickly in order to run them all the time, and have integration tests run less often (for 
 instance, on any new merge request).
  
 While mock objects are useful, abusing their use ranges between a code smell or an anti-
 pattern is the first caveat we would like to mention before going into the details of it.",NA
A fair warning about patching and mocks,"We said before that unit tests help us write better code, because the moment we want to 
 start testing parts of the code, we usually have to write them to be testable, which often 
 means they are also cohesive, granular, and small. These are all good traits to have in a 
 software component.
  
 Another interesting gain is that testing will help us notice code smells in parts where we 
 thought our code was correct. One of the main warnings that our code has code smells is 
 whether we find ourselves trying to monkey-patch (or mock) a lot of different things just to 
 cover a simple test case.
  
 The 
 unittest
  module provides a tool for patching our objects at 
 unittest.mock.patch
 .
  
 Patching means that the original code (given by a string denoting its location at import 
 time), will be replaced by something else, other than its original code, being the default a 
 mock object. This replaces the code at run-time, and has the disadvantage that we are losing 
 contact with the original code that was there in the first place, making our tests a little more 
 shallow. It also carries performance considerations, because of the overhead that imposes 
 modifying objects in the interpreter at run-time, and it's something that might end up 
 update if we refactor our code and move things around.
  
 Using monkey-patching or mocks in our tests might be acceptable, and by itself it doesn't 
 represent an issue. On the other hand, abuse in monkey-patching is indeed a flag that 
 something has to be improved in our code.",NA
Using mock objects,"In unit testing terminology, there are several types of object that fall into the category 
 named 
 test doubles
 . A test double is a type of object that will take the place of a real one in 
 our test suite for different kinds of reasons (maybe we don't need the actual production 
 code, but just a dummy object would work, or maybe we can't use it because it requires 
 access to services or it has side-effects that we don't want in our unit tests, and so on).
  
 There are different types of test double, such as dummy objects, stubs, spies, or mocks. 
 Mocks are the most general type of object, and since they're quite flexible and versatile, they 
 are appropriate for all cases without needing to go into much detail about the rest of them.
  
 It is for this reason that the standard library also includes an object of this kind, and it is 
 common in most Python programs. That's the one we are going to be using 
  
 here: 
 unittest.mock.Mock
 .",NA
Types of mocks,"The standard library provides 
 Mock
  and 
 MagicMock
  objects in the 
 unittest.mock 
 module. The 
 former is a test double that can be configured to return any value and will keep track of the 
 calls that were made to it. The latter does the same, but it also supports magic methods. 
 This means that, if we have written idiomatic code that uses magic methods (and parts of 
 the code we are testing will rely on that), it's likely that we will have to use a 
 MagicMock
  
 instance instead of just a 
 Mock
 .
  
 Trying to use 
 Mock
  when our code needs to call magic methods will result in an error. See 
 the following code for an example of this:
  
 class GitBranch:
  
  
  def __init__(self, commits: List[Dict]):
  
  
  
  self._commits = {c[""id""]: c for c in commits}
  
  def __getitem__(self, commit_id):
  
  return self._commits[commit_id]
  
  def __len__(self):
  
  return len(self._commits)
  
 def author_by_id(commit_id, branch):
  
  
  return branch[commit_id][""author""]",NA
A use case for test doubles,"To see a possible use of mocks, we need to add a new component to our application that 
 will be in charge of notifying the merge request of the 
 status
  of the 
 build
 . When a 
 build 
 is 
 finished, this object will be called with the ID of the merge request and the 
 status
  of the 
 build
 , and it will update the 
 status
  of the merge request with this information by sending an 
 HTTP 
 POST
  request to a particular fixed endpoint:
  
 # mock_2.py
  
 from datetime import datetime
  
 import requests 
  
 from constants import STATUS_ENDPOINT
  
 class BuildStatus:
  
  
  """"""The CI status of a pull request.""""""",NA
Refactoring,"Refactoring
  is a critical activity in software maintenance, yet something that can't be done 
 (at least correctly) without having unit tests. Every now and then, we need to support a 
 new feature or use our software in unintended ways. We need to realize that the only way 
 to accommodate such requirements is by first refactoring our code, make it more generic. 
 Only then can we move forward.",NA
Evolving our code,"In the previous example, we were able to separate out the side-effects from our code to 
 make it testable by patching those parts of the code that depended on things we couldn't 
 control on the unit test. This is a good approach since, after all, the 
 mock.patch
  function 
 comes in handy for these sorts of task and replaces the objects we tell it to, giving us back a 
 Mock
  object.
  
 The downside of that is that we have to provide the path of the object we are going to mock, 
 including the module, as a string. This is a bit fragile, because if we refactor our code (let's 
 say we rename the file or move it to some other location), all the places with the patch will 
 have to be updated, or the test will break.
  
 In the example, the fact that the 
 notify()
  method directly depends on an implementation 
 detail (the 
 requests
  module) is a design issue, that is, it is taking its toll on the unit tests as 
 well with the aforementioned fragility that is implied.
  
 We still need to replace those methods with doubles (mocks), but if we refactor the code, we 
 can do it in a better way. Let's separate these methods into smaller ones, and most 
 importantly inject the dependency rather than keep it fixed. The code now applies the 
 dependency inversion principle,and it expects to work with something that supports an 
 interface (in this example, implicit one) such as the one the 
 requests
  module provides:
  
 from datetime import datetime
  
 from constants import STATUS_ENDPOINT
  
 class BuildStatus:",NA
Production code isn't the only thing that evolves,"We keep saying that unit tests are as important as production code. And if we are careful 
 enough with production code as to create the best possible abstraction, why wouldn't we 
 do the same for unit tests?
  
 If the code for unit tests is as important as the main code, then it's definitely wise to design 
 it with extensibility in mind and make it as maintainable as possible. After all, this is the 
 code that will have to be maintained by an engineer other than its original author, so it has 
 to be readable.
  
 The reason why we pay so much attention to make the code's flexibility is that we know 
 requirements change and evolve over time, and eventually as domain business rules 
 change, our code will have to change as well to support these new requirements. Since the 
 production code changed to support new requirements, in turn, the testing code will have 
 to change as well to support the newer version of the production code.
  
 In one of the first examples we used, we created a series of tests for the merge request 
 object, trying different combinations and checking the status at which the merge request 
 was left. This is a good first approach, but we can do better than that.
  
 Once we understand the problem better, we can start creating better abstractions. With this, 
 the first idea that comes to mind is that we can create a higher-level abstraction that checks 
 for particular conditions. For example, if we have an object that is a test suite that 
  
 specifically targets the 
 MergeRequest
  class, we know its functionality will be limited to the 
 behavior of this class (because it should comply to the SRP), and therefore we could create 
 specific testing methods on this testing class. These will only make sense for this class, but 
 that will be helpful in reducing a lot of boilerplate code.",NA
More about unit testing,"With the concepts we have revisited so far, we know how to test our code, think about our 
 design in terms of how it is going to be tested, and configure the tools in our project to run 
 the automated tests that will give us some degree of confidence over the quality of the 
 software we have written.
  
 [ 247 ]",NA
Property-based testing,"Property-based testing consists of generating data for tests cases with the goal of finding 
 scenarios that will make the code fail, which weren't covered by our previous unit tests.
  
 The main library for this is 
 hypothesis
  which, configured along with our unit tests, will 
 help us find problematic data that will make our code fail.
  
 We can imagine that what this library does is find counter examples for our code. We write 
 our production code (and unit tests for it!), and we claim it's correct. Now, with this library, 
 we define some 
 hypothesis
  that must hold for our code, and if there are some cases where 
 our assertions don't hold, the 
 hypothesis
  will provide a set of data that causes the error.
  
 The best thing about unit tests is that they make us think harder about our production code. 
 The best thing about the 
 hypothesis
  is that it makes us think harder about our unit tests.",NA
Mutation testing,"We know that tests are the formal verification method we have to ensure that our code is 
 correct. And what makes sure that the test is correct? The production code, you might 
 think, and yes, in a way this is correct, we can think of the main code as a counter balance 
 for our tests.",NA
A brief introduction to test-driven ,NA,NA
development,"There are entire books dedicated only to TDD, so it would not be realistic to try and cover 
 this topic comprehensively in this book. However, it's such an important topic that it has to 
 be mentioned.
  
 The idea behind TDD is that tests should be written before production code in a way that 
 the production code is only written to respond to tests that are failing due to that missing 
 implementation of the functionality.
  
 There are multiple reasons why we would like to write the tests first and then the code.
  
 From a pragmatic point of view, we would be covering our production code quite 
  
 accurately. Since all of the production code was written to respond to a unit test, it would be 
 highly unlikely that there are tests missing for functionality (that doesn't mean that there is 
 100% of coverage of course, but at least all main functions, methods, or components will 
 have their respective tests, even if they aren't completely covered).
  
 The workflow is simple and at a high-level consist of three steps. First, we write a unit test 
 that describes something we need to be implemented. When we run this test, it will fail, 
 because that functionality has not been implemented yet. Then, we move onto 
  
 implementing the minimal required code that satisfies that condition, and we run the test 
 again. This time, the test should pass. Now, we can improve (refactor) the code.
  
 This cycle has been popularized as the famous 
 red-green-refactor
 , meaning that in the 
 beginning, the tests fail (red), then we make them pass (green), and then we proceed to 
 refactor the code and iterate it.",NA
Summary,"Unit testing is a really interesting and deep topic, but more importantly, it is a critical part 
 of the clean code. Ultimately, unit tests are what determine the quality of the code. Unit 
 tests often act as a mirror for the code
 â
 when the code is easy to test, it's clear and correctly 
 designed, and this will be reflected in the unit tests.
  
 The code for the unit tests is as important as production code. All principles that apply to 
 production code also apply to unit tests. This means that they should be designed and 
 maintained with the same effort and thoughtfulness. If we don't care about our unit tests, 
 they will start to have problems and become defective (or problematic), and as a result of 
 that, useless. If this happens, and they are hard to maintain, they become a liability which 
 makes things even worse, because people will tend to ignore them or disable them entirely.
  
 This is the worst scenario because once this happens, the entire production code is in 
 jeopardy. Moving forward blindly (without unit tests) is a recipe for disaster.
  
 Luckily, Python provides many tools for unit testing, both in the standard library and 
 available through 
 pip
 . They are of great help, and investing a time in configuring them 
 really pays off in the long run.
  
 We have seen how unit tests work as the formal specification of the program, and the proof 
 that a piece of software works according to the specification, and we also learned that when 
 it comes to discovering new testing scenarios, there is always room for improvement, and 
 that we can always create more tests. In this sense, expanding our unit tests with different 
 approaches (like property-based testing or mutation testing) is a good investment.",NA
References,"Here is a list of information you can refer to:
  
 The 
 unittest
  module of the Python standard library contains comprehensive 
 documentation on how to start building a test suite (
 https://docs.python.org/ 3/
 library/unittest.html
 )
  
 Hypothesis official documentation (
 https://hypothesis.readthedocs.io/en/ 
  
 latest/
 )
  
 pytest
  official documentation (
 https://docs.pytest.org/en/latest/
 )
  
 The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental 
  
 Revolutionary
  (
 CatB
 ), written by Eric S. Raymond (publisher O'Reilly Media, 1999)",NA
Common Design Patterns ,NA,NA
ï¹,"Design patterns have been a widespread topic in software engineering since their original 
 inception in the famous 
 Gang of Four
  (
 GoF
 ) book, 
 Design Patterns: Elements of Reusable 
 Object-Oriented Software
 . Design patterns help to solve common problems with abstractions 
 that work for certain scenarios. When they are implemented properly, the general design of 
 the solution can benefit from them.
  
 In this chapter we take a look at some of the most common design patterns, but not from 
 the perspective of tools to apply under certain conditions (once the patterns have been 
 devised), but rather we analyze how design patterns contribute to clean code. After 
 presenting a solution that implements a design pattern, we analyze how the final 
 implementation is comparatively better as if we had chosen a different path.
  
 As part of this analysis, we will see how to concretely implement design patterns in Python. 
 As a result of that, we will see that the dynamic nature of Python implies some differences 
 of implementation, with respect to other static typed languages, for which many of the 
 design patterns were originally thought of. This means that there are some particularities 
 about design patterns that you should bear in mind when it comes to Python, and, in some 
 cases, trying to apply a design pattern where it doesn't really fit is non-Pythonic.
  
 In this chapter, we will cover the following topics:
  
 Common design patterns.
  
 Design patterns that don't apply in Python, and the idiomatic alternative that 
  
 should be followed.
  
 The Pythonic way of implementing the most common design patterns.
  
 Understanding how good abstractions evolve naturally into patterns.",NA
Considerations for design patterns in ,NA,NA
Python,"Object-oriented design patterns are ideas of software construction that appear in different 
 scenarios when we deal with models of the problem we're solving. Because they're high-
 level ideas, it's hard to think of them as being tied to particular programming languages. 
 They are instead more general concepts about how objects will interact in the application. 
 Of course, they will have their implementation details, varying from language to language, 
 but that doesn't form the essence of a design pattern.
  
 That's the theoretical aspect of a design pattern, the fact that it is an abstract idea that 
 expresses concepts about the layout of the objects in the solution. There are plenty of other 
 books and several other resources about object-oriented design, and design patterns in 
 particular, so in this book, we are going to focus on those implementation details for 
 Python.
  
 Given the nature of Python, some of the classical design patterns aren't actually needed.
  
 That means that Python already supports features that render those patterns invisible. 
 Some argue that they don't exist in Python, but keep in mind that invisible doesn't mean 
 non-existing. They are there, just embedded in Python itself, so it's likely that we won't 
 even notice them.
  
 Others have a much simpler implementation, again thanks to the dynamic nature of the 
 language, and the rest of them are practically the same as they are in other platforms, with 
 small differences.
  
 In any case, the important goal for achieving clean code in Python is knowing what patterns 
 to implement and how. That means recognizing some of the patterns that Python already 
 abstracts and how we can leverage them. For instance, it would be completely non-Pythonic 
 to try to implement the standard definition of the iterator pattern (as we would do in 
 different languages), because (as we have already covered) iteration is deeply embedded in 
 Python, and the fact that we can create objects that will directly work in a 
 for
  loop makes 
 this the right way to proceed.",NA
Design patterns in action,"The canonical reference in this subject, as written by the GoF, introduces 23 design patterns, 
 each falling under one of the creational, structural, and behavioral categories. There are 
 even more patterns or variations of existing ones, but rather than learning all of these 
 patterns off by heart, we should focus on keeping two things in mind. Some of the patterns 
 are invisible in Python, and we use them probably without even noticing. Secondly, not all 
 patterns are equally common; some of them are tremendously useful, and so they are found 
 very frequently, while others are for more specific cases.
  
 In this section, we will revisit the most common patterns, those that are most likely to 
 emerge from our design. Note the use of the word 
 emerge
  here. It is important. We should 
 not force the application of a design pattern to the solution we are building, but rather 
 evolve, refactor, and improve our solution until a pattern emerges.
  
 Design patterns are therefore not invented but discovered. When a situation that occurs 
 repeatedly in our code reveals itself, the general and more abstract layout of classes, objects, 
 and related components appears under a name by which we identify a pattern.",NA
Creational patterns,"In software engineering, creational patterns are those that deal with object instantiation, 
 trying to abstract away much of the complexity (like determining the parameters to 
 initialize an object, all the related objects that might be needed, etc.), in order to leave the 
 user with a simpler interface, that should be safer to use. The basic form of object creation 
 could result in design problems or added complexity to the design. Creational design 
 patterns solve this problem by somehow controlling this object creation.
  
 Out of the five patterns for creating objects, we will discuss mainly the variants that are 
 used to avoid the singleton pattern, and replace it with the Borg pattern (most commonly 
 used in Python applications), discussing their differences and advantages.",NA
Factories,"As was mentioned in the introduction, one of the core features of Python is that everything 
 is an object, and as such, they can all be treated equally. This means that there are no special 
 distinctions of things that we can or cannot do with classes, functions, or custom objects. 
 They can all be passed by parameter, assigned, and so on.
  
 It is for this reason that many of the factory patterns are not really needed. We could just 
 simply define a function that will construct a set of objects, and we can even pass the class 
 that we want to create by a parameter.",NA
Singleton and shared state (monostate),"The singleton pattern, on the other hand, is something not entirely abstracted away by 
 Python. The truth is that most of the time, this pattern is either not really needed or is a bad 
 choice. There are a lot of problems with singletons (after all, they are, in fact, a form of 
 global variables for object-oriented software, and as such, are a bad practice). They are hard 
 to unit test, the fact that they might be modified at any time by any object makes them hard 
 to predict, and their side-effects can be really problematic.
  
 As a general principle, we should avoid using singletons as much as possible. If in some 
 extreme case, they are required, the easiest way of achieving this in Python is by using a 
 module. We can create an object in a module, and once it's there, it will be available from 
 every part of the module that is imported. Python itself makes sure that modules are 
 already singletons, in the sense that no matter how many times they're imported, and from 
 how many places, the same module is always the one that is going to be loaded 
  
 into 
 sys.modules
 .",NA
Shared state,"Rather than forcing our design to have a singleton in which only one instance is created, no 
 matter how the object is invoked, constructed, or initialized, it is better to replicate the data 
 across multiple instances.
  
 The idea of the monostate pattern (SNGMONO) is that we can have many instances that are 
 just regular objects, without having to care whether they're singletons or not (seeing as 
 they're just objects). The good thing about this pattern is that these objects will have their 
 information synchronized, in a completely transparent way, without us having to worry 
 about how this works internally.
  
 This makes this pattern a much better choice, not only for its convenience, but also because 
 it is less error-prone, and suffers from fewer of the disadvantages of singletons (regarding 
 their testability, creating derived classes, and so on).
  
 We can use this pattern on many levels, depending on how much information we need to 
 synchronize.
  
 In its simplest form, we can assume that we only need to have one attribute to be reflected 
 across all instances. If that is the case, the implementation is as trivial as using a class 
 variable, and we just need to take care in providing a correct interface to update and 
 retrieve the value of the attribute.",NA
The borg pattern,"The previous solutions should work for most cases, but if we really have to go for a 
 singleton (and this has to be a really good exception), then there is one last better 
 alternative to it, only this is a riskier one.
  
 This is the actual monostate pattern, referred to as the borg pattern in Python. The idea is to 
 create an object that is capable of replicating all of its attributes among all instances of the 
 same class. The fact that absolutely every attribute is being replicated has to be a warning to 
 keep in mind undesired side-effects. Still, this pattern has many advantages over the 
 singleton.
  
 In this case, we are going to split the previous object into two
 â
 one that works over Git 
 tags, and the other over branches. And we are using the code that will make the borg 
 pattern work:
  
 class BaseFetcher:
  
  
  def __init__(self, source):
  
  
  
  self.source = source
  
 class TagFetcher(BaseFetcher):
  
  
  _attributes = {}
  
  def __init__(self, source):
  
  self.__dict__ = self.__class__._attributes 
 super().__init__(source)
  
  def pull(self):
  
  logger.info(""pulling from tag %s"", self.source) return f""Tag = 
 {self.source}""",NA
Builder,"The builder pattern is an interesting pattern that abstracts away all the complex 
  
 initialization of an object. This pattern does not rely on any particularity of the language, so 
 it's as equally applicable in Python as it would be in any other language.
  
 While it solves a valid case, it's usually also a complicated case that is more likely to appear 
 in the design of a framework, library, or an API. Similar to the recommendations given for 
 descriptors, we should reserve this implementation for cases where we expect to expose an 
 API that is going to be consumed by multiple users.
  
 The high level idea of this patter is that we need to create a complex object, that is an object 
 that also requires many others to work with. Rather than letting the user create all those 
 auxiliary objects, and then assign them to the main one, we would like to create an 
  
 abstraction that allows all of that to be done in a single step. In order to achieve this, we will 
 have a builder object that knows how to create all the parts and link them together, giving 
 the user an interface (which could be a class method), to parametrize all the information 
 about what the resulting object should look like.",NA
Structural patterns,"Structural patterns are useful for situations where we need to create simpler interfaces or 
 objects that are more powerful by extending their functionality without adding complexity 
 to their interfaces.",NA
Adapter,"The adapter pattern is probably one of the simplest design patterns there are, and one of the 
 most useful ones at the same time. Also known as a wrapper, this pattern solves the 
 problem of adapting interfaces of two or more objects that are not compatible.
  
 We typically encounter the situation where part of our code works with a model or set of 
 classes that were polymorphic with respect to a method. For example, if there were 
 multiple objects for retrieving data with a 
 fetch()
  method, then we want to maintain this 
 interface so we don't have to make major changes to our code.
  
 But then we come to a point where the need to add a new data source, and alas, this one 
 won't have a 
 fetch()
  method. To make things worse, not only is this type of object not 
 compatible, but it is also not something we control (perhaps a different team decided on the 
 API, and we cannot modify the code).
  
 Instead of using this object directly, we adopt its interface to the one we need. There are 
 two ways of doing this.
  
 The first way would be to create a class that inherits from the one we want to use, and that 
 creates an alias for the method (if required, it will also have to adapt the parameters and the 
 signature).
  
 By means of inheritance, we import the external class and create a new one that will define 
 the new method, calling the one that has a different name. In this example, let's say the 
 external dependency has a method named 
 search()
 , which takes only one parameter for 
 the search because it queries in a different fashion, so our 
 adapter
  method not only calls 
 the external one, but it also translates the parameters accordingly, as shown in the 
 following code:
  
 from _adapter_base import UsernameLookup
  
 class UserSource(UsernameLookup):
  
  
  def fetch(self, user_id, username):
  
  
  
  user_namespace = self._adapt_arguments(user_id, username)
  
  
  
 return self.search(user_namespace)",NA
Composite,"There will be parts of our programs that require us to work with objects that are made out 
 of other objects. We have base objects that have a well-defined logic, and then we will have 
 other container objects that will group a bunch of base objects, and the challenge is that we 
 want to treat both of them (the base and the container objects) without noticing any 
 differences.
  
 The objects are structured in a tree hierarchy, where the basic objects would be the leaves of 
 the tree, and the composed objects intermediate nodes. A client might want to call any of 
 them to get the result of a method that is called. The composite object, however, will act as a",NA
Decorator,"Don't confuse the decorator pattern with the concept of a Python decorator which we have 
 gone through in 
 Chapter 5
 , 
 Using Decorators to Improve Our Code
 . There is some 
  
 resemblance, but the idea of the design pattern is quite different.
  
 This pattern allows us to dynamically extend the functionality of some objects, without 
 needing inheritance. It's a good alternative to multiple inheritance in creating more flexible 
 objects.
  
 We are going to create a structure that let's a user define a set of operations (decorations) to 
 be applied over an object, and we'll see how each step takes place in the specified order.
  
 The following code example is a simplified version of an object that constructs a query in 
 the form of a dictionary from parameters that are passed to it (it might be an object that we 
 would use for running queries to elasticsearch, for instance, but the code leaves out 
 distracting implementation details to focus on the concepts of the pattern).
  
 In its most basic form, the query just returns the dictionary with the data it was provided 
 when it was created. Clients expect to use the 
 render()
  method of this object:
  
 class DictQuery:
  
  
  def __init__(self, **kwargs):
  
  
  
  self._raw_query = kwargs
  
  def render(self) -> dict:
  
  return self._raw_query
  
 Now we want to render the query in different ways by applying transformations to the 
 data (filtering values, normalizing them, and so on). We could create decorators and apply 
 them to the 
 render
  method, but that wouldn't be flexible enough what if we want to 
 change them at runtime? Or if we want to select some of them, but not others?
  
 The design is to create another object, with the same interface and the capability of 
 enhancing (decorating) the original result through many steps, but which can be combined.
  
 These objects are chained, and each one of them does what it was originally supposed to 
 do, plus something else. This something else is the particular decoration step.
  
 Since Python has duck typing, we don't need to create a new base class and make these new 
 objects part of that hierarchy, along with 
 DictQuery
 . Simply creating a new class that has a 
 render()
  method will be enough (again, polymorphism should not require inheritance).
  
 This process is shown in the following code:",NA
Facade,"Facade is an excellent pattern. It's useful in many situations where we want to simplify the 
 interaction between objects. The pattern is applied where there is a relation of many-to-
 many among several objects, and we want them to interact. Instead of creating all of these 
 connections, we place an intermediate object in front of many of them that act as a facade.
  
 The facade works as a hub or a single point of reference in this layout. Every time a new 
 object wants to connect to another one, instead of having to have 
 N
  interfaces for all 
 N 
 possible objects it needs to connect to, it will instead just talk to the facade, and this will 
 redirect the request accordingly. Everything that's behind the facade is completely opaque 
 to the rest of the external objects.
  
 Apart from the main and obvious benefit (the decoupling of objects), this pattern also 
 encourages a simpler design with fewer interfaces and better encapsulation.",NA
Behavioral patterns,"Behavioral patterns aim to solve the problem of how objects should cooperate, how they 
 should communicate, and what their interfaces should be at run-time.
  
 We discuss mainly the following behavioral patterns:
  
 Chain of responsibility
  
 Template method",NA
Chain of responsibility,"Now we are going to take another look at our event systems. We want to parse information 
 about the events that happened on the system from the log lines (text files, dumped from 
 our HTTP application server, for example), and we want to extract this information in a 
 convenient way.
  
 In our previous implementation, we achieved an interesting solution that was compliant 
 with the open/closed principle and relied on the use of the 
 __subclasses__()
  magic method 
 to discover all possible event types and process the data with the right event, resolving 
 the responsibility through a method encapsulated on each class.
  
 This solution worked for our purposes, and it was quite extensible, but as we'll see, this 
 design pattern will bring additional benefits.
  
 The idea here is that we are going to create the events in a slightly different way. Each event 
 still has the logic to determine whether or not it can process a particular log line, but it will 
 also have a successor. This successor is a new event, the next one in the line, that will 
 continue processing the text line in case the first one was not able to do so. The logic is 
 simple
 â
 we chain the events, and each one of them tries to process the data. If it can, then it 
 just returns the result. If it can't, it will pass it to its successor and repeat, as shown in the 
 following code:
  
 import re
  
 class Event:
  
  
  pattern = None
  
  def __init__(self, next_event=None):
  
  self.successor = next_event
  
  def process(self, logline: str):
  
  if self.can_process(logline):
  
  
  return self._process(logline)",NA
The template method,"The 
 template
  method is a pattern that yields important benefits when implemented 
 properly. Mainly, it allows us to reuse code, and it also makes our objects more flexible and 
 easy to change while preserving polymorphism.
  
 The idea is that there is a class hierarchy that defines some behavior, let's say an important 
 method of its public interface. All of the classes of the hierarchy share a common template 
 and might need to change only certain elements of it. The idea, then, is to place this generic 
 logic in the public method of the parent class that will internally call all other (private) 
 methods, and these methods are the ones that the derived classes are going to modify; 
 therefore, all the logic in the template is reused.",NA
Command,"The command pattern provides us with the ability to separate an action that needs to be 
 done from the moment that it is requested to its actual execution. More than that, it can also 
 separate the original request issued by a client from its recipient, which might be a different 
 object. In this section, we are going to focus mainly on the first aspect of the patterns; the 
 fact that we can separate how an order has to be run from when it actually executes.
  
 We know we can create callable objects by implementing the 
 __call__()
  magic method, so 
 we could just initialize the object and then call it later on. In fact, if this is the only 
 requirement, we might even achieve this through a nested function that, by means of a 
 closure, creates another function to achieve the effect of a delayed execution. But this 
 pattern can be extended to ends that aren't so easily achievable.",NA
State,"The state pattern is a clear example of reification in software design, making the concept 
 of our domain problem an explicit object rather than just a side value.
  
 In 
 Chapter 8
 , 
 Unit Testing and Refactoring
 , we had an object that represented a merge 
 request, and it had a state associated with it (open, closed, and so on). We used an enum to 
 represent those states because, at that point, they were just data holding a value the string 
 representation of that particular state. If they had to have some behavior, or the entire 
 merge request had to perform some actions depending on its state and transitions, this 
 would not have been enough.",NA
The null object pattern,"The null object pattern is an idea that relates to the good practices that were mentioned in 
 previous chapters of this book. Here, we are formalizing them, and giving more context 
 and analysis to this idea.
  
 The principle is rather simple
 â
 functions or methods must return objects of a consistent 
 type. If this is guaranteed, then clients of our code can use the objects that are returned with 
 polymorphism, without having to run extra checks on them.
  
 In the previous examples, we explored how the dynamic nature of Python made things 
 easier for most design patterns. In some cases, they disappear entirely, and in others, they 
 are much easier to implement. The main goal of design patterns as they were originally 
 thought of is that methods or functions should not explicitly name the class of the object 
 that they need in order to work. For this reason, they propose the creation of interfaces and 
 a way of rearranging the objects to make them fit these interfaces in order to modify the 
 design. But most of the time, this is not needed in Python, and we can just pass different 
 objects, and as long as they respect the methods they must have, then the solution will 
 work.
  
 On the other hand, the fact that objects don't necessarily have to comply with an interface 
 requires us to be more careful as to the things that are returning from such methods and 
 functions. In the same way that our functions didn't make any assumptions about what 
 they were receiving, it's fair to assume that clients of our code will not make any 
  
 assumptions either (it is our responsibility to provide objects that are compatible). This can 
 be enforced or validated with design by contract. Here, we will explore a simple pattern 
 that will help us avoid these kinds of problems.
  
 Consider the chain or responsibility design pattern explored in the previous section. We 
 saw how flexible it is and its many advantages, such as decoupling responsibilities into 
 smaller objects. One of the problems it has is that we never actually know what object will 
 end up processing the message, if any. In particular, in our example, if there was no 
 suitable object to process the log line, then the method would simply return 
 None
 .
  
 We don't know how users will use the data we passed, but we do know that they are 
 expecting a dictionary. Therefore, the following error might occur:
  
 AttributeError: 'NoneType' object has no attribute 'keys'
  
 In this case, the fix is rather simple
 â
 the default value of the 
 process()
  method should be an 
 empty dictionary rather than 
 None
 .",NA
Final thoughts about design patterns,"We have seen the world of design patterns in Python, and in doing so, we have found 
 solutions to common problems, as well as more techniques that will help us achieve a clean 
 design.
  
 All of this sounds good, but it begs the question, how good are design patterns? Some 
 people argue that they do more harm than good, that they were created for languages 
 whose limited type system (and lack of first-class functions) makes it impossible to 
 accomplish things we would normally do in Python. Others claim that design patterns 
 force a design solution, creating some bias that limits a design that would have otherwise 
 emerged, and which would have been better. Let's look at each of these points in turn.",NA
The influence of patterns over the design,"A design patterns, as with any other topic in software engineering, cannot be good or bad in 
 and of itself, but rather in how it's implemented. In some cases, there is actually no need for 
 a design pattern, and a simpler solution would do. Trying to force a pattern where it doesn't 
 fit is a case of over-engineering, and that's clearly bad, but it doesn't mean that there is a 
 problem with the design patterns, and most likely in these scenarios, the problem is not 
 even related to patterns at all. Some people try to over-engineer everything because they 
 don't understand what flexible and adaptable software really means. As we mentioned 
 before in this book, making good software is not about anticipating future requirements 
 (there is no point in doing futurology), but just solving the problem that we have at hand 
 right now, in a way that doesn't prevent us from making changes to it in the future. It 
 doesn't have to handle those changes now; it just needs to be flexible enough so that it can 
 be modified in the future. And when that future comes, we will still have to remember the 
 rule of three or more instances of the same problem before coming up with a generic 
 solution or a proper abstraction.
  
 This is typically the point where the design patterns should emerge, once we have 
 identified the problem correctly and are able to recognize the pattern and abstract 
 accordingly.",NA
Names in our models,"Should we mention that we are using a design pattern in our code?
  
 If the design is good and the code is clean, it should speak for itself. It is not recommended 
 that you name things after the design patterns you are using for a couple of reasons:
  
 Users of our code and other developers don't need to know the design pattern 
  
 behind the code, as long as it works as intended.
  
 Stating the design pattern ruins the intention revealing principle. Adding the 
 name of the design pattern to a class makes it lose part of its original meaning. If 
 a class represents a query, it should be named 
 Query
  or 
 EnhancedQuery
 , 
 something that reveals the intention of what that object is supposed to 
  
 do. 
 EnhancedQueryDecorator
  doesn't mean anything meaningful, and the 
 Decorator
  suffix creates more confusion than clarity.",NA
Summary,"Design patterns have always been seen as proven solutions to common problems. This is a 
 correct assessment, but in this chapter, we explored them from the point of view of good 
 design techniques, patterns that leverage clean code. In most of the cases, we looked at how 
 they provide a good solution to preserve polymorphism, reduce coupling, and create the 
 right abstractions that encapsulate details as needed. All traits that relate to the concepts 
 explored in 
 Chapter 8
 , 
 Unit Testing and Refactoring
 .
  
 Still, the best thing about design patterns is not the clean design we can obtain from 
 applying them, but the extended vocabulary. Used as a communication tool, we can use 
 their names to express the intention of our design. And sometimes, it's not the entire 
 pattern that we need to apply, but we might need to take a particular idea (a substructure, 
 for example) of a pattern from our solution, and here, too, they prove to be a way of 
 communicating more effectively.
  
 When we create solutions by thinking in terms of patterns, we are solving problems at a 
 more general level. Thinking in terms of design patterns, brings us closer to higher-level 
 design. We can slowly ""zoom-out"" and think more in terms of an architecture. And now 
 that we are solving more general problems, it's time to start thinking about how the system 
 is going to evolve and be maintained in the long run (how it's going to scale, change, adapt, 
 and so on).
  
 For a software project to be successful in these goals, it requires clean code at its core, but 
 the architecture also has to be clean as well, which is what we are going to look at in the 
 next chapter.",NA
References ,"Here is a list of information you can refer to:
  
 GoF
 : The book written by Erich Gamma, Richard Helm, Ralph Johnson, and John 
  
 Vlissides named 
 Design Patterns: Elements of Reusable Object-Oriented Software
  
 SNGMONO
 : An article written by Robert C. Martin, 2002 named 
 SINGLETON 
  
 and MONOSTATE
  
 The Null Object Pattern
 , written by Bobby Woolf
  
 [ 285 ]",NA
Clean Architecture ,NA,NA
ï±ï°,"In this final chapter, we focus on how everything fits together in the design of a whole 
 system. This is a more theoretical chapter. Given the nature of the topic, it would be too 
 complex to delve down into the more low-level details. Besides, the point is precisely to 
 escape from those details, assume that all the principles explored in previous chapters are 
 assimilated, and focus on the design of a system at scale.
  
 The main concerns and goals for this chapter are as follows:
  
 Designing software systems that can be maintained in the long run
  
 Working effectively on a software project by maintaining quality attributes
  
 Studying how all concepts applied to code relate to systems in general",NA
From clean code to clean architecture,"This section is a discussion of how concepts that were emphasized in previous chapters 
 reappear in a slightly different shape when we consider aspects of large systems. There is 
 an interesting resemblance to how concepts that apply to more detailed design, as well as 
 code, also apply to large systems and architectures.
  
 The concepts explored in previous chapters were related to single applications, generally, a 
 project, that might be a single repository (or a few), for a source control version system (git).
  
 This is not to say that those design ideas are only applicable to code, or that they are of no 
 use when thinking of an architecture, for two reasons: the code is the foundation of the 
 architecture, and, if it's not written carefully, the system will fail regardless of how well 
 thought-out the architecture is.
  
 Second, some principles that were revisited in previous chapters do not apply to code but 
 are instead design ideas. The clearest example comes from design patterns. They are high-",NA
Separation of concerns,"Inside an application, there are multiple components. Their code is divided into other 
 subcomponents, such as modules or packages, and the modules into classes or functions, 
 and the classes into methods. Throughout the book, the emphasis has been on keeping 
 these components as small as possible, particularly in the case of functions
 â
 functions 
 should do one thing, and be small.
  
 Several reasons were presented to justify this rationale. Small functions are easier to 
 understand, follow, and debug. They are also easier to test. The smaller the pieces in our 
 code, the easier it will be to write unit tests for it.
  
 For the components of each application, we wanted different traits, mainly high cohesion, 
 and low coupling. By dividing components into smaller units, each one with a single and 
 well-defined responsibility, we achieve a better structure where changes are easier to 
 manage. In the face of new requirements, there will be a single rightful place to make the 
 changes, and the rest of the code should probably be unaffected.
  
 When we talk about code, we say 
 component
  to refer to one of these cohesive units (it might 
 be a class, for example). When speaking in terms of an architecture, a component means 
 anything in the system that can be treated as a working unit. The term component itself is 
 quite vague, so there is no universally accepted definition in software architecture of what 
 this means more concretely. The concept of a working unit is something that can vary from 
 project to project. A component should be able to be released or deployed with its own 
 cycles, independently from the rest of the parts of the system. And it is precisely that, one of 
 the parts of a system, is namely the entire application.
  
 For Python projects, a component could be a package, but a service can also be a 
  
 component. Notice how two different concepts, with different levels of granularity, can be 
 considered under the same category. To give an example, the event systems we used in 
 previous chapters could be considered a component. It's a working unit with a clearly 
 defined purpose (to enrich events identified from logs), it can be deployed independently 
 from the rest (whether as a Python package, or, if we expose its functionality, as a service), 
 and it's a part of the entire system, but not the whole application itself.",NA
Abstractions,"This is where encapsulation appears again. From our systems (as we do in relation to the 
 code), we want to speak in terms of the domain problem, and leave the implementation 
 details as hidden as possible.
  
 In the same way that the code has to be expressive (almost to the point of being self-
 documenting), and have the right abstractions that reveal the solution to the essential 
 problem (minimizing accidental complexity), the architecture should tell us what the 
 system is about. Details such as the solution used to persist data on disk, the web 
 framework of choice, the libraries used to connect to external agents, and interaction 
 between systems, are not relevant. What is relevant is what the system does. A concept 
 such as a scream architecture (SCREAM) reflects this idea.",NA
Software components,"We have a large system now, and we need to scale it. It also has to be maintainable. At this 
 point, the concerns aren't only technical but also organizational. This means it's not just 
 about managing software repositories; each repository will most likely belong to an 
 application, and it will be maintained by a team who owns that part of the system. 
  
 This demands we keep in mind how a large system is divided into different components. 
 This can have many phases, from a very simple approach about, say, creating Python 
 packages, to more complex scenarios in a microservice architecture.
  
 The situation could be even more complex when different languages are involved, but in 
 this chapter, we will assume they are all Python projects.
  
 These components need to interact, as do the teams. The only way this can work at scale is 
 if all the parts agree on an interface, a contract.",NA
Packages,"A Python package is a convenient way to distribute software and reuse code in a more 
 general way. Packages that have been built can be published to an artifact repository (such 
 as an internal PyPi server for the company), from where it will be downloaded by the rest 
 of the applications that require it.
  
 The motivation behind this approach has many elements to it
 â
 it's about reusing code at 
 large, andalso achieving conceptual integrity.
  
 Here, we discuss the basics of packaging a Python project that can be published in a 
 repository. The default repository might be PyPi (
 https://pypi.org/
 ), but also internal; or 
 custom setups will work with the same basics.
  
 We are going to simulate that we have created a small library, and we will use that as an 
 example to review the main points to take into consideration.
  
 Aside from all the open source libraries available, sometimes we might need some extra 
 functionality
 â
 perhaps our application uses a particular idiom repeatedly or relies on a 
 function or mechanism quite heavily and the team has devised a better function for these 
 particular needs. In order to work more effectively, we can place this abstraction into a 
 library, and encourage all team members to use the idioms as provided by it, because doing 
 so will help avoid mistakes and reduce bugs.",NA
Containers,"This chapter is dedicated to architecture, so the term container refers to something 
 completely different from a Python container (an object with a 
 __contains__
  method), 
 explored in 
 Chapter 2
 , 
 Pythonic Code
 . A container is a process that runs in the operating 
 system under a group with certain restrictions and isolation considerations. Concretely we 
 refer to Docker containers, which allow managing applications (services or processes) as 
 independent components.
  
 Containers represent another way of delivering software. Creating Python packages taking 
 into account the considerations in the previous section is more suitable for libraries, or 
 frameworks, where the goal is to reuse code and take advantage of using a single place 
 where specific logic is gathered.",NA
Use case,"As an example of how we might organize the components of our application, and how the 
 previous concepts might work in practice, we present the following simple example.
  
 The use case is that there is an application for delivering food, and this application has a 
 specific service for tracking the status of each delivery at its different stages. We are going 
 to focus only on this particular service, regardless of how the rest of the application might 
 appear. The service has to be really simple
 â
 a REST API that, when asked about the status 
 of a particular order, will return a JSON response with a descriptive message.
  
 We are going to assume that the information about each particular order is stored in a 
 database, but this detail should not matter at all.
  
 Our service has two main concerns for now: getting the information about a particular 
 order (from wherever this might be stored), and presenting this information in a useful way 
 to the clients (in this case, delivering the results in JSON format, exposed as a web service).
  
 As the application has to be maintainable and extensible, we want to keep these two 
 concerns as hidden as possible and focus on the main logic. Therefore, these two details are 
 abstracted and encapsulated into Python packages that the main application with the core 
 logic will use, as shown in the following diagram:
  
  
 In the following sections, we briefly demonstrate how the code might appear, in terms of 
 the packages mainly, and how to create services from these, in order to finally see what 
 conclusions we can infer.",NA
The code,"The idea of creating Python packages in this example is to illustrate how abstracted and 
 isolated components can be made, in order to work effectively. In reality, there is no actual 
 need for these to be Python packages; we could just create the right abstractions as part of 
 the ""delivery service"" project, and, while the correct isolation is preserved, it will work 
 without any issues.
  
 Creating packages makes more sense when there is logic that is going to be repeated and is 
 expected to be used across many other applications (that will import from those packages) 
 because we want to favor code reuse. In this particular case, there are no such 
  
 requirements, so it might be beyond the scope of the design, but such distinction still makes 
 more clear the idea of a ""pluggable architecture"" or component, something that is really a 
 wrapper abstracting technical details we don't really want to deal with, much less depend 
 upon.
  
 The 
 storage
  package is in charge of retrieving the data that is required, and presenting this 
 to the next layer (the delivery service) in a convenient format, something that is suitable for 
 the business rules. The main application should now know where this data came from, 
 what its format is, and so on. This is the entire reason why we have such an abstraction in 
 between so the application doesn't use a row or an ORM entity directly, but rather 
  
 something workable.",NA
Domain models,"The following definitions apply to classes for business rules. Notice that they are meant to 
 be pure business objects, not bound to anything in particular. They aren't models of an 
 ORM, or objects of an external framework, and so on. The application should work with 
 these objects (or objects with the same criteria).
  
 In each case, the dosctring documents the purpose of each class, according to the business 
 rule:
  
 from typing import Union
  
 class DispatchedOrder:
  
  
  """"""An order that was just created and notified to start its delivery.""""""
  
  status = ""dispatched""
  
  def __init__(self, when):
  
  self._when = when",NA
Calling from the application,"Here is how these objects are going to be used in the application. Notice how this depends 
 on the previous packages (
 web
  and 
 storage
 ), but not the other way round:
  
 from storage import DBClient, DeliveryStatusQuery, OrderNotFoundError from web import 
 NotFound, View, app, register_route
  
 class DeliveryView(View):
  
  
  async def _get(self, request, delivery_id: int):
  
  
  
  dsq = DeliveryStatusQuery(int(delivery_id), await DBClient())
  
  
  try
  
  
  
  result = await dsq.get()
  
  
  
  except OrderNotFoundError as e:
  
  
  
  
  
  raise NotFound(str(e)) from e
  
  return result.message()
  
 register_route(DeliveryView, ""/status/<delivery_id:int>"")
  
 In the previous section, the 
 domain
  objects were shown and here the code for the 
 application is displayed. Aren't we missing something? Sure, but is it something we really 
 need to know now? Not necessarily.
  
 The code inside the 
 storage
  and 
 web
  packages was deliberately left out (although the reader 
 is more than encouraged to look at it
 â
 the repository for the book contains the full 
 example). Also, and this was done on purpose, the names of such packages were chosen so 
 as not to reveal any technical detail
 â
 storage
  and 
 web
 .",NA
Adapters,"Still, without looking at the code in the packages, we can conclude that they work as 
 interfaces for the technical details of the application.
  
 In fact, since we are seeing the application from a high-level perspective, without needing 
 to look at the code, we can imagine that inside those packages there must be an 
  
 implementation of the adapter design pattern (introduced in 
 Chapter 9
 , 
 Common Design 
 Patterns
 ). One or more of these objects is adapting an external implementation to the API 
 defined by the application. This way, dependencies that want to work with the application 
 must conform to the API, and an adapter will have to be made.
  
 There is one clue pertaining to this adapter in the code for the application though. Notice 
 how the view is constructed. It inherits from a class named 
 View
  that comes from our 
 web 
 package. We can deduce that this 
 View
  is, in turn, a class derived from one of the web 
 frameworks that might be being used, creating an adapter by inheritance. The important 
 thing to note is that once this is done, the only object that matters is our 
 View
  class, because, 
 in a way, we are creating our own framework, which is based on adapting an existing one 
 (but again changing the framework will mean just changing the adapters, not the entire 
 application).",NA
The services,"To create the service, we are going to launch the Python application inside a Docker 
 container. Starting from a base image, the container will have to install the dependencies 
 for the application to run, which also has dependencies at the operating system level.
  
 This is actually a choice because it depends on how the dependencies are used. If a package 
 we use requires other libraries on the operating system to compile at installation time, we 
 can avoid this simply by building a wheel for our platform of the library and installing this 
 directly. If the libraries are needed at runtime, then there is no choice but to make them part 
 of the image of the container.",NA
Analysis,"There are many conclusions to be drawn from the previous implementation. While it might 
 seem like a good approach, there are cons that come with the benefits; after all, no 
  
 architecture or implementation is perfect. This means that a solution such as this one cannot 
 be good for all cases, so it will pretty much depend on the circumstances of the project, the 
 team, the organization, and more.
  
 While it's true that the main idea of the solution is to abstract details as much as possible, as 
 we shall see some parts cannot be fully abstracted away, and also the contracts between the 
 layers imply an abstraction leak.",NA
The dependency flow,"Notice that dependencies flow in only one direction, as they move closer to the kernel, 
 where the business rules lie. This can be traced by looking at the 
 import
 statements. The 
 application imports everything it needs from storage, for example, and in no part is this 
 inverted. 
  
 Breaking this rule would create coupling. The way the code is arranged now means that 
 there is a weak dependency between the application and storage. The API is such that we 
 need an object with a 
 get()
  method, and any storage that wants to connect to the 
  
 application needs to implement this object according to this specification. The dependencies 
 are therefore inverted
 â
 it's up to every storage to implement this interface, in order to 
 create an object according to what the application is expecting.",NA
Limitations,"Not everything can be abstracted away. In some cases, it's simply not possible, and in 
 others, it might not be convenient. Let's start with the convenience aspect.
  
 In this example, there is an adapter of the web framework of choice to a clean API to be 
 presented to the application. In a more complex scenario, such a change might not be 
 possible. Even with this abstraction, parts of the library were still visible to the application. 
 Adapting an entire framework might not only be hard but also not possible in some cases.
  
 It's not entirely a problem to be completely isolated from the web framework because, 
 sooner or later, we will need some of its features or technical details.
  
 The important takeaway here is not the adapter, but the idea of hiding technical details as 
 much as possible. That means, that the best thing that was displayed on the listing for the 
 code of the application was not the fact that there was an adapter between our version of 
 the web framework and the actual one, but instead the fact that the latter was not 
  
 mentioned by name in any part of the visible code. The service was made clear that 
 web 
 was 
 just a dependency (a detail being imported), and revealed the intention behind what it was 
 supposed to do. The goal is to reveal the intention (as in the code) and to defer details as 
 much as possible.
  
 As to what things cannot be isolated, those are the elements that are closest to the code. In 
 this case, the web application was using the objects operating within them in an 
  
 asynchronous fashion. That is a hard constraint we cannot circumvent. It's true that 
 whatever is inside the 
 storage
  package can be changed, refactored, and modified, but 
 whatever these modifications might be, it still needs to preserve the interface, and that 
 includes the asynchronous interface.",NA
Testability,"Again, much like with the code, the architecture can benefit from separating pieces into 
 smaller components. The fact that dependencies are now isolated and controlled by 
 separate components leaves us with a cleaner design for the main application, and now it's 
 easier to ignore the boundaries to focus on testing the core of the application.
  
 We could create a patch for the dependencies, and write unit tests that are simpler (they 
 won't need a database), or to launch an entire web service, for instance. Working with pure 
 domain
  objects means it will be easier to understand the code and the unit tests. Even the 
 adapters will not need that much testing because their logic should be very simple.",NA
Intention revealing,"These details included keeping functions short, concerns separated, dependencies isolated, 
 and assigning the right meaning to abstractions in every part of the code. Intention 
 revealing was a critical concept for our code
 â
 every name has to be wisely chosen, clearly 
 communicating what it's supposed to do. Every function should tell a story.
  
 A good architecture should reveal the intent of the system it entails. It should not mention 
 the tools it's built with; those are details, and as we discussed at length, details should be 
 hidden, encapsulated.",NA
Summary,"The principles for good software design apply on all levels. In the same way that we want 
 to write readable code, and for that we need to mind the intention revealing degree of the 
 code, the architecture also has to express the intent of the problem it is trying to solve.
  
 All these ideas are interconnected. The same intention revealing that ensures our 
  
 architecture is defined in terms of the domain problem also leads us to abstract details as 
 much as possible, create layers of abstraction, invert dependencies, and separate concerns.
  
 When it comes to reusing code, Python packages are a great and flexible alternative.
  
 Criteria, such as cohesion and the 
 single responsibility principle
  (
 SRP
 ), are the most 
 important considerations when deciding to create a package. In line with having 
  
 components with cohesion and few responsibilities, the concept of microservices comes 
 into play, and for that, we have seen how a service can be deployed in a Docker container 
 starting from a packaged Python application.
  
 As with everything in software engineering, there are limitations, and there are exceptions.
  
 It will not always be possible to abstract things as much as we would like to or to 
  
 completely isolate dependencies. Sometimes, it will just not be possible (or practical) to 
 comply with the principles explained here in the book. But that is probably the best piece of 
 advice the reader should take from the book
 â
 they are just principles, not laws. If it's not 
 possible, or practical, to abstract from a framework, it should not be a problem. Remember 
 what has been quoted from the zen of Python itself, throughout the book
 â
 practicality 
 beats purity
 .",NA
References,"Here is a list of information you can refer to:
  
 SCREAM
 : The Screaming Architecture (
 https://8thlight.com/blog/uncle-
  
 bob/2011/
 09/30/Screaming-Architecture.html
 )
  
 CLEAN-01
 : The Clean Architecture (
 https://8thlight.com/blog/uncle-bob/ 
  
 2012/08/13/the-clean-architecture.html
 )
  
 HEX
 : Hexagonal Architecture (
 https://staging.cockburn.us/hexagonal-
  
 architecture/
 )
  
 PEP-508
 : Dependency specification for Python software packages (
 https://www. 
  
 python.org/dev/peps/pep-0508/
 )
  
 Packaging and distributing projects in Python (
 https://python-packaging-user-
 guide.readthedocs.io/guides/distributing-packages-using-setuptools/#distributing-packages",NA
Summing it all up,"The content of the book is a reference, a possible way of implementing a software solution 
 by following criteria. These criteria are explained through examples, and the rationale for 
 every decision presented. The reader might very well disagree with the approach taken on 
 the examples, and this is actually desirable: the more viewpoints, the richer the debate. But 
 regardless of opinions, it's important to make clear that what is presented here is by no 
 means a strong directive, something that must be followed imperatively. Quite the 
 opposite, it's a way of presenting the reader with a solution and a set of ideas that they 
 might find helpful.
  
 As introduced at the beginning of the book, the goal of the book was not to give you recipes 
 or formulas that you can apply directly, but rather to make you develop critical thinking.
  
 Idioms and syntax features come and go, they change over time. But ideas, and core 
 software concepts, remain. With these tools given, and the examples provided, you should 
 have a better understanding of what clean code means.
  
 I sincerely hope the book has helped you become a better developer than you were before 
 you started it, and I wish you the best of luck in your projects.",NA
Other Books You May Enjoy,"If you enjoyed this book, you may be interested in these other books by Packt:
  
  
 Secret Recipes of the Python Ninja
  
 Cody Jackson
  
 ISBN: 978-1-78829-487-4
  
 Know the differences between .py and .pyc files 
  
 Explore the different ways to install and upgrade Python packages
  
 Understand the working of the PyPI module that enhances built-in decorators
  
 See how coroutines are different from generators and how they can simulate
  
 multithreading
  
 Grasp how the decimal module improves floating point numbers and their
  
 operations
  
 Standardize sub interpreters to improve concurrency
  
 Discover Python
 â
 s built-in docstring analyzer",NA
Leave a review - let other readers know what ,NA,NA
you think,"Please share your thoughts on this book with others by leaving a review on the site that you 
 bought it from. If you purchased the book from Amazon, please leave us an honest review 
 on this book's Amazon page. This is vital so that other potential readers can see and use 
 your unbiased opinion to make purchasing decisions, we can understand what our 
 customers think about our products, and our authors can see your feedback on the title that 
 they have worked with Packt to create. It will only take a few minutes of your time, but is 
 valuable to other potential customers, our authors, and Packt. Thank you!
  
 [ 310 ]",NA
Index,NA,NA
A ,"abstractions  
 288 
  
 acronyms
  
  about  
 72
  
  DRY/OAOO  
 72
  
  EAFP/LBYL  
 76
  
  KISS  
 75
  
  YAGNI  
 74 
  
 analysis
  
  dependency flow  
 304
  
  intention revealing  
 306
  
  limitations  
 305
  
  testability  
 305 
  
 annotations
  
  about  
 13
 , 
 16
  
  using, instead of docstrings  
 18 
  
 Application Programming Interface (API)  
 56 
 arguments
  
  compact function signatures  
 92
  
  copying, to functions  
 86
  
  in functions  
 85
 , 
 91
  
  in methods  
 85
  
  passing, to Python  
 85
  
  variable arguments  
 89
  
  variable number  
 87 
  
 asynchronous programming  
 215
 , 
 217 
  
 automatic checks
  
  setup  
 21",NA
B ,"basic quality gates
  
  enforcing, by tools configuration  
 20 
 behavioral patterns
  
  about  
 270
  
  chain of responsibility  
 270
 , 
 272
  
 command pattern  
 273
  
  state pattern  
 274
 , 
 279
  
  template method  
 272 
  
 best practices, software design 
 about  
 93
  
  code, structuring  
 96
  
  orthogonality in software  
 94 
 Black
  
  reference  
 22 
  
 borg pattern
  
  about  
 260
  
  builder pattern  
 262",NA
C ,"C3 linearization  
 84 
  
 callable objects  
 48 
  
 caveats, Python
  
  about  
 49
  
  built-in types, extending  
 51
  
  mutable default arguments  
 50 
  
 clean architecture
  
  about  
 286
  
  reference  
 307 
  
 clean code
  
  code formatting, role  
 9
  
  coding style guide, adhering to  
 10
  
 defining  
 8
  
  importance  
 8 
  
 code coverage
  
  about  
 236
  
  rest coverage, setting up  
 236
  
  test coverage, caveats  
 237 
  
 code dependency, consequences 
 low level of abstraction  
 71
  
  no code reuse  
 71
  
  ripple effects  
 71 
  
 code duplication, limitations
  
  error prone  
 72",NA
D ,"data descriptors  
 163
 , 
 165 
 decorate classes  
 127
 , 
 130 
 decorate functions  
 126
  
  data descriptors  
 165
 , 
 167
  
  idiomatic implementation  
 169
  
 implementing, forms  
 172
  
  implementing, in decorators  
 185
  
 machinery  
 153
 , 
 156
  
  non-data descriptors  
 163
 , 
 165
  
 [ 312 ]",NA
E ,"Easier to Ask Forgiveness than Permission (EAFP) 
   
 76 
  
 error handling
  
  about  
 61
  
  exception handling  
 62
  
  value substitution  
 61
 , 
 62 
  
 exception handling
  
  about  
 62
  
  at right level of abstraction  
 64
 , 
 65
  
  empty except blocks, avoiding  
 67
  
  original exception, exploring  
 69
  
  tracebacks exposure, avoiding  
 66
  
  working, in Python  
 85",NA
G ,"generators
  
  about  
 189
 , 
 191
  
  creating  
 189
  
  expressions  
 192
  
  references  
 218 
  
 god-objects  
 100",NA
H ,"Hexagonal Architecture
  
  reference  
 307 
  
 hint  
 16",NA
I ,"idioms for iteration
  
  about  
 193
  
  code, simplifying through iterators  
 197
  
 generator, using  
 196
  
  itertools  
 196
  
  next() function  
 195 
  
 indexes  
 26 
  
 inheritance
  
  about  
 77
  
  anti-patterns  
 79
 , 
 81
  
  benefits  
 78 
  
 interface  
 117 
  
 interface segregation principle (ISP)
  
  about  
 117
  
  guidelines  
 118
 , 
 119 
  
 iterable objects
  
  about  
 40
  
  creating  
 40
 , 
 43
  
  sequences, creating  
 43 
  
 iteration
  
  about  
 193
  
  idioms, using  
 193 
  
 iterator pattern
  
  in Python  
 200
  
  interface for iteration  
 200",NA
F,"sequence objects, using as iterables  
 201
  
 function arguments
  
  and coupling  
 91
  
 [ 313 ]",NA
K ,"KIS (Keep It Simple)  
 75",NA
L ,"Liskov's substitution principle (LSP)
  
  about  
 110
  
  issue detection, with tools  
 111
  
  reviews  
 116
  
  violation, cases  
 113 
  
 Look Before You Leap (LBYL)  
 76 
  
 LSP issues
  
  incompatible signatures, detecting with Pylint 
   
 113",NA
N ,"name mangling  
 37 
  
 non-data descriptors  
 163 
  
 null object pattern  
 280",NA
O ,"Object Relational Mapper SQLAlchemy (ORM 
   
 SQLAlchemy)  
 274 
  
 objects
  
  attributes  
 35
  
  dynamic attributes  
 46
  
  methods  
 35
  
  properties  
 35
 , 
 38
 , 
 39
  
  
  incorrect datatypes, detecting in method 
  
 signatures with Mypy  
 112
  
 Once and Only Once (OAOO)  
 72 
 open/closed principle (OCP)",NA
M,"about  
 103
 , 
 109
  
  events system, extending  
 107
  
  
 magic methods  
 49 
 makefiles  
 21
  
  events system, refactoring for extensibility  
 105
  
 maintainability perils example  
 103
 , 
 105
  
 methods, descriptor protocol
  
  __delete__(self, instance)  
 159
  
  __set__(self, instance, value)  
 158
  
  __set_name__(self, owner, name)  
 161
  
  about  
 156 
  
 methods, generator interface
  
  about  
 204
  
  close() method  
 204
  
  send(value)  
 206
  
  throw(ex_type[, ex_value[, ex_traceback]])  
 205 
 mock objects
  
  about  
 238
  
  using  
 239 
  
 mock
  
  about  
 239
 , 
 240
  
  types  
 240",NA
P ,"patching  
 239 
  
 PEP-8 characteristics
  
  code quality  
 12
  
  consistency  
 12
  
  grepability  
 11 
  
 Portable Operating System Interface (POSIX)  
 269 
 production code  
 246 
  
 property-based testing  
 248 
  
 Pylint
  
  used, for checking code  
 21
  
  used, for detecting incompatible signatures  
 113 
 pytest
  
  about  
 232
  
  fixtures  
 235
  
  
 multiple inheritance, Python
  
  Method Resolution Order (MRO)  
 82
  
  parametrized tests  
 234
  
  used, for writing test cases  
 233
  
  mixins  
 84 
  
 Python 
  
 mutation testing  
 248
  
  
  assertions, using  
 69 
 Mypy
  
  
  caveats  
 49
  
  reference  
 20
  
  
  decorators, using  
 125
  
  used, for detecting incorrect datatypes in method 
  
 signatures  
 111
  
  descriptors, using  
 180
  
  design patterns, considerations  
 254
  
  used, for type hinting  
 20
  
  iterator pattern  
 200
  
 [ 314 ]",NA
R ,"red-green-refactor cycle  
 251 
 refactoring  
 244 
  
 ripple effect  
 70",NA
S ,"Screaming Architecture
  
  reference  
 307 
  
 separation of concerns (SoC)
  
  about  
 70
 , 
 287
 , 
 294
  
  cohesion  
 71
  
  coupling  
 71 
  
 sequences
  
  creating  
 28 
  
 single responsibility principle (SRP)
  
  about  
 99
  
  class, with multiple responsibilities  
 100
  
 responsibilities, distributing  
 102 
  
 slices  
 26 
  
 software components
  
  containers  
 294
  
  packages  
 290
 , 
 292 
  
 software design
  
  best practices  
 93 
  
 structural patterns
  
  about  
 263
  
  adapter pattern  
 263
  
  composite pattern  
 264
  
  decorator pattern  
 266
  
  facade pattern  
 268
  
 test-driven development (TDD)  
 251 
 testability  
 222 
  
 testing  
 226 
  
 testing boundaries
  
  defining  
 225 
  
 testing
  
  tools  
 226",NA
U ,"unit testing
  
  about  
 247
  
  and agile software development  
 222
  
 and software design  
 222
  
  frameworks  
 226
  
  libraries  
 226
  
  mutation testing  
 248
  
  property-based testing  
 248
  
  pytest  
 232
  
  unittest module  
 228 
  
 unittest module
  
  about  
 228
  
  parametrized tests  
 230 
  
 use case
  
  about  
 295
  
  analysis  
 304
  
  code reuse  
 296
  
  services  
 300
 , 
 303 
  
 uses, decorators
  
  parameters, transforming  
 135",NA
W ,"wrapped object  
 125",NA
T,NA,NA
Y ,"YAGNI (You Ain't Gonna Need It)  
 74",NA
