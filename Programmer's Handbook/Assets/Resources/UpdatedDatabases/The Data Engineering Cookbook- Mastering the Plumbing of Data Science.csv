Larger Text,Smaller Text,Symbol
The Data Engineering Cookbook ,Mastering The Plumbing Of Data Science,NA
Andreas Kretz,NA,NA
"July 2, 2019 ",NA,NA
v2.1,NA,NA
Contents,"I 
  
 Introduction 
  
 10
  
 1 How To Use This Cookbook 
  
 11
  
 2 Data Engineer vs Data Scientists 
  
 12
  
 2.1 
  
 Data Scientist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 12
  
 2.2 
  
 Data Engineer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 13
  
 2.3 
  
 Who Companies Need 
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 14
  
 II Basic Data Engineering Skills 
  
 16
  
 3 Learn To Code 
  
 17
  
 4 Get Familiar With Git 
  
 18
  
 5 Agile Development – available 
  
 19
  
 5.1 
  
 Why is agile so important? . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 19
  
 5.2 
  
 Agile rules I learned over the years – available . . . . . . . . . . . . . . . 
  
 20
  
 5.2.1 
  
 Is the method making a difference? . . . . . . . . . . . . . . . . . 
  
 20
  
 5.2.2 
  
 The problem with outsourcing . . . . . . . . . . . . . . . . . . . . 
  
 20
  
 5.2.3 
  
 Knowledge is king: A lesson from Elon Musk . . . . . . . . . . . . 
  
 21
  
 5.2.4 
  
 How you really can be agile 
  
 . . . . . . . . . . . . . . . . . . . . . 
  
 21
  
 5.3 
  
 Agile Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 22
  
 5.3.1 
  
 Scrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 22
  
 5.3.2 
  
 OKR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 22
  
 5.4 
  
 Software Engineering Culture 
  
 . . . . . . . . . . . . . . . . . . . . . . . . 
  
 22",NA
Part I ,NA,NA
Introduction,10,NA
1 How To Use This Cookbook,"What do you actually need to learn to become an awesome data engineer? Look no 
 further, you’ll find it here.
  
 If you are looking for AI algorithms and such data scientist things, this book is not for 
 you.
  
 How to use this document: 
  
 First of all, this is not a training! This cookbook is a collection of skills that I value highly 
 in my daily work as a data engineer. It’s intended to be a starting point for you to find 
 the topics to look into and become an awesome data engineer.
  
 You are going to find
  Five Types of Content
  in this book: Articles I wrote, links to my 
 podcast episodes (video & audio), more then 200 links to helpful websites I like, data 
 engineering interview questions and case studies.
  
 This book is a work in progress!
  
 As you can see, this book is not finished. I’m constantly adding new stuff and doing 
 videos for the topics. But obviously, because I do this as a hobby my time is limited. You 
 can help making this book even better.
  
 Help make this book awesome!
  
 If you have some cool links or topics for the cookbook, please become a contributor on 
 GitHub:
  https://github.com/andkret/Cookbook
 . Pull the repo, add them and create a 
 pull request. Or join the discussion by opening Issues. You can also write me an email 
 any time to plumbersofdatascience@gmail.com. Tell me your thoughts, what you value, 
 you think should be included, or correct where I am wrong.
  
 This Cookbook is and will always be free!
  
 I don’t want to sell you this book, but please support what you like and join my Patreon: 
 https://www.patreon.com/plumbersofds
  
 Check out this podcast episode where I talk in detail why I decided to share all this 
 information for free:
  #079 Trying to stay true to myself and making the cookbook public 
 on GitHub
  
 11",NA
2 Data Engineer vs Data Scientists,"Podcast Episode:
  #050 Data Engineer Scientist or Analyst Which One Is For You? In 
 this podcast we talk about the differences between data scientists, analysts and 
 engineers. Which are the three main data science jobs. All three super important.
  
 This makes it easy to decide
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 2.1: Podcast: 050 Data Engineer Scientist or Analyst Which One Is For You?",NA
2.1 Data Scientist,"Data scientists aren’t like every other scientist.
  
 Data scientists do not wear white coats or work in high tech labs full of science fiction 
 movie equipment. They work in offices just like you and me.
  
 What differs them from most of us is that they are the math experts. They use linear 
 algebra and multivariable calculus to create new insight from existing data.
  
 How exactly does this insight look?
  
 Here’s an example: 
  
 An industrial company produces a lot of products that need to be tested before shipping. 
 Usually such tests take a lot of time because there are hundreds of things to be tested. All 
 to make sure that your product is not broken.
  
 Wouldn’t it be great to know early if a test fails ten steps down the line? If you knew that 
 you could skip the other tests and just trash the product or repair it.
  
 That’s exactly where a data scientist can help you, big-time. This field is called predictive 
 analytics and the technique of choice is machine learning.
  
 Machine what? Learning?
  
 12",NA
2.2 Data Engineer,"Data Engineers are the link between the management’s big data strategy and the data 
 scientists that need to work with data.
  
 What they do is building the platforms that enable data scientists to do their magic.
  
 These platforms are usually used in five different ways:
  
 •
  Data ingestion and storage of large amounts of data
  
 13",NA
2.3 Who Companies Need,"For a good company it is absolutely important to get well trained data engineers and 
 data scientists. Think of the data scientist as the professional race car driver. A fit 
 athlete with talent and driving skills like you have never seen.
  
 What he needs to win races is someone who will provide him the perfect race car to 
 drive. That’s what the solution architect is for.
  
 Like the driver and his team the data scientist and the data engineer need to work 
 closely together. They need to know the different big data tools Inside and out.
  
 That’s why companies are looking for people with Spark experience. It is a common 
 ground between both that drives innovation.
  
 Spark gives data scientists the tools to do analytics and helps engineers to bring the data 
 scientist’s algorithms into production. After all, those two decide how good the data
  
 14",NA
Part II ,NA,NA
Basic Data Engineering Skills,16,NA
3 Learn To Code,"Why this is important: Without coding you cannot do much in data engineering. I cannot 
 count the number of times I needed a quick Java hack.
  
 The possibilities are endless:
  
  
 •
  Writing or quickly getting some data out of a SQL DB
  
 •
  Testing to produce messages to a Kafka topic
  
  
 •
  Understanding Source code of a Java Webservice
  
  
 •
  Reading counter statistics out of a HBase key value store 
 So, which language do I recommend then?
  
 I highly recommend Java. It’s everywhere!
  
 When you are getting into data processing with Spark you should use Scala. But, after 
 learning Java this is easy to do.
  
 Also Python is a great choice. It is super versatile.
  
 Personally however, I am not that big into Python. But I am going to look into it
  
 Where to Learn? There’s a Java Course on Udemy you could look at: 
 https://www.udemy.com/java-programming-tutorial-for-beginners
  
  
 •
  OOP Object oriented programming
  
  
 •
  What are Unit tests to make sure what you code is working
  
  
 •
  Functional Programming
  
  
 •
  How to use build management tools like Maven
  
  
 •
  Resilient testing (?) 
  
 I talked about the importance of learning by doing in this podcast:
  https://anchor.fm/ 
 andreaskayy/episodes/Learning-By-Doing-Is-The-Best-Thing-Ever---PoDS-035-e25g44
  
 17",NA
4 Get Familiar With Git,"Why this is important: One of the major problems with coding is to keep track of changes.
  
 It is also almost impossible to maintain a program you have multiple versions of.
  
 Another is the topic of collaboration and documentation. Which is super Important.
  
 Let’s say you work on a Spark application and your colleagues need to make changes 
 while you are on holiday. Without some code management they are in huge trouble: 
 Where is the code? What have you changed last? Where is the documentation? How do 
 we mark what we have changed?
  
 They can But if you put your code on GitHub your colleagues can find your code.
  
 understand it through your documentation (please also have in-line comments) 
  
 Developers can pull your code, make a new branch and do the changes. After your holiday 
  
 you can inspect what they have done and merge it with your original code. and you end 
  
 up having only one application 
  
 Where to learn: Check out the GitHub Guides page where you can learn all the basics: 
  
 https://guides.github.com/introduction/flow/ 
  
 This great GitHub commands cheat sheet saved my butt multiple times: 
 https://www.atlassian.com/git/ git-cheatsheet 
  
 Also look into:
  
  
 •
  Pull
  
  
 •
  Push
  
  
 •
  Branching
  
  
 •
  Forking
  
 18",NA
5 Agile Development – available,"Agility, the ability to adapt quickly to changing circumstances.
  
 These days everyone wants to be agile. Big or small company people are looking for 
 the“startup mentality”.
  
 Many think it’s the corporate culture. Others think it’s the process how we create things 
 that matters.
  
 In this article I am going to talk about agility and self-reliance. About how you can 
 incorporate agility in your professional career.",NA
5.1 Why is agile so important?,"Historically development is practiced as a hard defined process. You think of something, 
 specify it, have it developed and then built in mass production.
  
 It’s a bit of an arrogant process. You assume that you already know exactly what a 
 customer wants. Or how a product has to look and how everything works out.
  
 The problem is that the world does not work this way!
  
 Often times the circumstances change because of internal factors.
  
 Sometimes things just do not work out as planned or stuff is harder than you think. 
 You need to adapt.
  
 Other times you find out that you build something customers do not like and need to be 
 changed.
  
 You need to adapt.
  
 That’s why people jump on the Scrum train. Because Scrum is the definition of agile 
 development, right?
  
 19",NA
5.2 Agile rules I learned over the years – available,"5.2.1 Is the method making a difference?
  
 Yes, Scrum or Google’s OKR can help to be more agile. The secret to being agile however, 
 is not only how you create.
  
 What makes me cringe is people try to tell you that being agile starts in your head. So, 
 the problem is you?
  
 No!
  
 The biggest lesson I have learned over the past years is this: Agility goes down the drain 
 when you outsource work.
  
 5.2.2 The problem with outsourcing
  
 I know on paper outsourcing seems like a no brainer: Development costs against the 
 fixed costs.
  
 It is expensive to bind existing resources on a task. It is even more expensive if you need 
 to hire new employees.
  
 The problem with outsourcing is that you pay someone to build stuff for you.
  
 It does not matter who you pay to do something for you. He needs to make money. His 
 agenda will be to spend as less time as possible on your work. That is why outsourcing 
 requires contracts, detailed specifications, timetables and delivery dates.
  
 He doesn’t want to spend additional time on a project, only because you want changes in 
 the middle. Every unplanned change costs him time and therefore money.
  
 If so, you need to make another detailed specification and a contract change.
  
 He is not going to put his mind into improving the product while developing. Firstly 
 because he does not have the big picture. Secondly because he does not want to. He is 
 doing as he is told.
  
 Who can blame him? If I was the subcontractor I would do exactly the same!
  
 Does this sound agile to you?
  
 20",NA
5.3 Agile Frameworks,"5.3.1 Scrum
  
 There’s a interesting Scrum Medium publication with a lot of details about Scrum:
  https: 
 //medium.com/serious-scrum
  
 Also this scrum guide webpage has good infos about Scrum:
  https://www.scrumguides. 
 org/scrum-guide.html
  
 5.3.2 OKR
  
 I personally love OKR, been doing it for years. Especially for smaller teams OKR is great. 
 You don’t have a lot of overhead and get work done. It helps you stay focused and look 
 at the bigger picture.
  
 I recommend to do a sync meeting every Monday. There you talk about what happened 
 last week and what you are going to work on this week.
  
 I talked about this in this Podcast:
  https://anchor.fm/andreaskayy/embed/episodes/ 
 Agile-Development-Is-Important-But-Please-Dont-Do-Scrum--PoDS-041-e2e2j4
  
 This is also this awesome 1,5 hours Startup guide from Google:
  
 https://youtu.be/mJB83EZtAjc 
 I really love this video, I rewatched it multiple times.",NA
5.4 Software Engineering Culture,"The software engineering and development culture is super important. How does a com-
 pany handle product development with hundreds of developers. Check out this podcast:
  
 Podcast Episode:
  #070 Engineering Culture At Spotify 
  
 In this podcast we look at the engineering culture at Spotify, my favorite music 
 streaming service. The process behind the development of Spotify is really 
 awesome.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 5.1: Podcast: 070 Engineering Culture At Spotify
  
 Some interesting slides:
  
 https://labs.spotify.com/2014/03/27/spotify-engineering-culture-part-1/",NA
6 Learn how a Computer Works,NA,NA
"6.1 CPU,RAM,GPU,HDD",NA,NA
6.2 Differences between PCs and Servers,"I talked about computer hardware and GPU processing in this podcast:
  https://anchor.
  
 fm/andreaskayy/embed/episodes/Why-the-hardware-and-the-GPU-is-super-important--PoDS-030-
 e23",NA
7 Computer Networking - Data,NA,NA
Transmission,NA,NA
7.1 OSI Model,"The OSI Model describes how data is flowing through the network. It consists of layers 
 starting from physical layers, basically how the data is transmitted over the line or optic 
 fiber.
  
 Cisco page that shows the layers of the OSI model and how it works:
  
 https://learningnetwork. cisco.com/docs/DOC-30382
  
 Check out this page:
  https://www.studytonight.com/computer-networks/complete-osi-
 model
  
 The Wikipedia page is also very good:
  https://en.wikipedia.org/wiki/OSI model
  
 Which protocol lives on which layer? 
  
 Check out this network protocol map. Un-
 fortunately it is really hard to find it theses days:
  https://www.blackmagicboxes.com/ 
 wp-content/uploads/2016/12/Network-Protocols-Map-Poster.jpg",NA
7.2 IP Subnetting,"Check out this IP Adress and Subnet guide from Cisco:
  https://www.cisco.com/c/en/ 
 us/support/docs/ip/routing-information-protocol-rip/13788-3.html
  
 A calculator for Subnets:
  https://www.calculator.net/ip-subnet-calculator.html",NA
"7.3 Switch, Level 3 Switch",NA,NA
7.4 Router,NA,NA
7.5 Firewalls,"I talked about Network Infrastructure and Techniques in this podcast:
  https://anchor.
  
 fm/andreaskayy/embed/episodes/IT-Networking-Infrastructure-and-Linux-031-PoDS-
 e242bh",NA
8 Security and Privacy,NA,NA
8.1 SSL Public & Private Key Certificates,NA,NA
8.2 What is a certificate authority,NA,NA
8.3 JSON Web Tokens,"Link to the Wiki page:
  https://en.wikipedia.org/wiki/JSON Web Token",NA
8.4 GDPR regulations,NA,NA
8.5 Privacy by design,27,NA
9 Linux,"Linux is very important to learn, at least the basics. Most Big Data tools or NoSQL 
 databases are running on Linux.
  
 From time to time you need to modify stuff through the operation system. Especially if 
 you run a infrastructure as a service solution like Cloudera CDH, Hortonworks or a 
 MapR Hadoop distribution",NA
9.1 OS Basics,"Show all historic commands
  
 history
  
  
 |
  
 grep docker",NA
9.2 Shell scripting,NA,NA
9.3 Cron jobs,"Cron jobs are super important to automate simple processes or jobs in Linux. You need 
 this here and there I promise. Check out this three guides: 
  
 https://linuxconfig.org/linux-crontab-reference-guide 
  
 https://www.ostechnix.com/a-beginners-guide-to-cron-jobs/ 
  
 And of course Wikipedia, which is surprisingly good:
  
 https://en.wikipedia.org/wiki/Cron 
 Pro tip: Don’t forget to end your cron files with an 
 empty line or a comment, otherwise it will not work.
  
 28",NA
9.4 Packet management ,"Linux Tips are the second part of this podcast:
  https://anchor.fm/andreaskayy/embed/
  
 episodes/IT-Networking-Infrastructure-and-Linux-031-PoDS-e242bh
  
 29",NA
10 The Cloud,NA,NA
10.1 IaaS vs PaaS vs SaaS,"Check out this Podcast it will help you understand where’s the difference and how to
  
 decide on what you are going to use.
  
 Podcast Episode:
  #082 Reading Tweets With Apache Nifi & IaaS vs PaaS vs SaaS In 
 this episode we are talking about the differences between infrastructure as a service, 
 platform as a service and application as a service. Then we install the Nifi docker 
 container and look into how we can extract the twitter data.
  
 Youtube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 10.1: Podcast: 082 Reading Tweets With Apache Nifi & IaaS vs PaaS vs SaaS",NA
"10.2 AWS,Azure, IBM, Google Cloud basics",NA,NA
10.3 Cloud vs On-Premises,"Podcast Episode:
  #076 Cloud vs On-Premise 
  
 How do you choose between Cloud vs On-Premises, pros and cons and what you 
 have to think about. Because there are good reasons to not go cloud. Also thoughts 
 on how to choose between the cloud providers by just comparing instance prices. 
 Otherwise the comparison will drive you insane. My suggestion: Basically use them 
 as IaaS and something like Cloudera as PaaS. Then build your solution on top of 
 that.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 10.2: Podcast: 076 Cloud vs On-Premise
  
 30",NA
10.4 Security ,"Listen to a few thoughts about the cloud in this podcast:
  
 https://anchor.fm/andreaskayy/
  
 embed/episodes/Dont-Be-Arrogant-The-Cloud-is-Safer-Then-Your-On-Premise-e16k9s",NA
10.5 Hybrid Clouds ,"Hybrid clouds are a mixture of on-premises and cloud deployment. A very interesting
  
 example for this is Google Anthos:
  
 https://cloud.google.com/anthos/",NA
11 Security Zone Design,NA,NA
11.1 How to secure a multi layered application ,"(UI in 
 different zone then SQL DB)",NA
11.2 Cluster security with Kerberos ,"I talked about security zone design and lambda architecture in this podcast:
  https://
  
 anchor.fm/andreaskayy/embed/episodes/How-to-Design-Security-Zones-and-Lambda-Architecture-
 -Po",NA
11.3 Kerberos Tickets,NA,NA
12 Big Data,NA,NA
12.1 What is big data and where is the difference to data ,NA,NA
science and data analytics?,"I talked about the difference in this podcast: 
 https://anchor.fm/andreaskayy/embed/episodes/BI-vs-Data-Science-vs-Big-Data-e199hq",NA
12.2 The 4Vs of Big Data — available,"It is a complete misconception. Volume is only one part of the often called four V’s of big 
 data: Volume, velocity, variety and veracity.
  
 Volume
  is about the size. How much data you have.
  
 Velocity
  is about the speed of how fast the data is getting to you.
  
 How much data is in a specific time needs to get processed or is coming into the system. 
 This is where the whole concept of streaming data and real-time processing comes in to 
 play.
  
 Variety
  is the third one. It means, that the data is very different. That you have very 
 different types of data structures.
  
 Like CSV files, PDFs that you have stuff in XML. That you have JSON logfiles, or that you 
 have data in some kind of a key value store.
  
 It’s about the variety of data types from different sources that you basically want to join 
 together. All to make an analysis based on that data.
  
 Veracity
  is fourth and this is a very very difficult one. The issue with big data is, that it is 
 very unreliable.
  
 You cannot really trust the data.
  
 Especially when you’re coming from the IoT, the
  
 33",NA
12.3 Why Big Data? — available,"What I always emphasize is the four V’s are quite nice. They give you a general direction. 
 There is a much more important issue: Catastrophic Success.
  
 What I mean by catastrophic success is, that your project, your startup or your platform 
 has more growth that you anticipated. Exponential growth is what everybody is looking 
 for.
  
 Because with exponential growth there is the money. It starts small and gets very big 
 very fast. The classic hockey stick curve: 
  
 1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384. . . .BOOM!
  
 Think about it. It starts small and quite slow, but gets very big very fast.
  
 You get a lot of users or customers who are paying money to use your service, the 
 platform or whatever. If you have a system that is not equipped to scale and process the 
 data the whole system breaks down.
  
 That’s catastrophic success. You are so successful and grow so fast that you cannot fulfill 
 the demand anymore. And so you fail and it’s all over.
  
 It’s now like you just can make that up while you go. That you can foresee in a few 
 months or weeks the current system doesn’t work anymore.
  
 34",NA
13 My Big Data Platform Blueprint,"Some time ago I have created a simple and modular big data platform blueprint for 
 myself. It is based on what I have seen in the field and read in tech blogs all over the 
 internet.
  
 Today I am going to share it with you.
  
 Why do I believe it will be super useful to you?
  
 Because, unlike other blueprints it is not focused on technology. 
  
 It is based on four 
 common big data platform design patterns.
  
 Following my blueprint will allow you to create the big data platform that fits exactly 
 your needs. Building the perfect platform will allow data scientists to discover new 
 insights. It will enable you to perfectly handle big data and allow you to make data 
 driven decisions.
  
 THE BLUEPRINT 
 The blueprint is focused on the four key areas: Ingest, store, analyse 
 and display.
  
  
 Figure 13.1: Platfrom Blueprint",NA
13.1 Ingest,"Ingestion is all about getting the data in from the source and making it available to later 
 stages. Sources can be everything form tweets, server logs to IoT sensor data like from 
 cars.
  
 Sources send data to your API Services. 
 temporary storage.
  
 The API is going to push the data into a
  
 The temporary storage allows other stages simple and fast access to incoming data. A 
 great solution is to use messaging queue systems like Apache Kafka, RabbitMQ or AWS 
 Kinesis. Sometimes people also use caches for specialised applications like Redis. A good 
 practice is that the temporary storage follows the publish, subscribe pattern. This way 
 APIs can publish messages and Analytics can quickly consume them.",NA
13.2 Analyse / Process,"The analyse stage is where the actual analytics is done. Analytics, in the form of stream 
 and batch processing.
  
 Streaming data is taken from ingest and fed into analytics. Streaming analyses the 
 “live”data thus, so generates fast results.
  
 As the central and most important stage, analytics also has access to the big data 
 storage.
  
 Because of that connection, analytics can take a big chunk of data and analyse it.",NA
13.3 Store,"This is the typical big data storage where you just store everything. It enables you to 
 analyse the big picture.
  
 Most of the data might seem useless for now, but it is of upmost importance to keep it. 
 Throwing data away is a big no no.
  
 Why not throw something away when it is useless?
  
 Although it seems useless for now, data scientists can work with the data. They might 
 find new ways to analyse the data and generate valuable insight from it.
  
 What kind of systems can be used to store big data?
  
 Systems like Hadoop HDFS, Hbase, Amazon S3 or DynamoDB are a perfect fit to store 
 big data.
  
 Check out my podcast how to decide between SQL and NoSQL: 
 https://anchor.fm/andreaskayy/embed/ Vs-SQL-How-To-Choose-e12f1o
  
 41",NA
13.4 Display,"Displaying data is as important as ingesting, storing and analysing it. People need to be 
 able to make data driven decisions.
  
 This is why it is important to have a good visual presentation of the data. Sometimes you 
 have a lot of different use cases or projects using the platform.
  
 It might not be possible for you to build the perfect UI that fits everyone. What you 
 should do in this case is enable others to build the perfect UI themselves.
  
 How to do that? By creating APIs to access the data and making them available to 
 developers.
  
 Either way, UI or API the trick is to give the display stage direct access to the data in the 
 big data cluster. This kind of access will allow the developers to use analytics results as 
 well as raw data to build the the perfect application.
  
 42",NA
14,NA,NA
Lambda Architecture,"Podcast Episode:
  #077 Lambda Architecture and Kappa Architecture 
  
 In this stream we talk about the lambda architecture with stream and batch process-
 ing as well as a alternative the Kappa Architecture that consists only of streaming. 
 Also Data engineer vs data scientist and we discuss Andrew Ng’s AI Transformation 
 Playbook
  
 Audio 
  
 Youtube
  
 Click here to listen 
  
 Click here to watch
  
 Table 14.1: Podcast: 077 Lambda Architecture and Kappa Architecture",NA
14.1 Batch Processing,"Ask the big questions. Remember your last yearly tax statement?
  
 You break out the folders. You run around the house searching for the receipts. 
 All that fun stuff.
  
 When you finally found everything you fill out the form and send it on its way.
  
 Doing the tax statement is a prime example of a batch process.
  
 Data comes in and gets stored, analytics loads the data from storage and creates an 
 output (insight):
  
  
 Figure 14.1: Batch Processing Pipeline
  
 Batch processing is something you do either without a schedule or on a schedule (tax 
 statement). It is used to ask the big questions and gain the insights by looking at the big 
 picture.
  
 43",NA
14.2 Stream Processing,"Gain instant insight into your data.
  
 Streaming allows users to make quick decisions and take actions based on “real-
 time”insight. Contrary to batch processing, streaming processes data on the fly, as it 
 comes in.
  
 With streaming you don’t have to wait minutes or hours to get results. You gain instant 
 insight into your data.
  
 In the batch processing pipeline, the analytics was after the data storage. It had access to 
 all the available data.
  
 Stream processing creates insight before the data storage. It has only access to 
 fragments of data as it comes in.
  
 As a result the scope of the produced insight is also limited. Because the big picture is 
 missing.
  
  
 Figure 14.2: Stream Processing Pipeline
  
 Only with streaming analytics you are able to create advanced services for the 
 customer. Netflix for instance incorporated stream processing into Chuckwa V2.0 and 
 the new Keystone pipeline.
  
 One example of advanced services through stream processing is the Netflix “Trending 
 Now” feature. Check out the Netflix case study.
  
 44",NA
14.3 Should you do stream or batch processing?,"It is a good idea to start with batch processing. Batch processing is the foundation of 
 every good big data platform.
  
 A batch processing architecture is simple, and therefore quick to set up. Platform sim-
 plicity means, it will also be relatively cheap to run.
  
 A batch processing platform will enable you to quickly ask the big questions. They will 
 give you invaluable insight into your data and customers.
  
 When the time comes and you also need to do analytics on the fly, then add a streaming 
 pipeline to your batch processing big data platform.",NA
14.4 Lambda Architecture Alternative,"14.4.1 Kappa Architecture
  
 14.4.2 Kappa Architecture with Kudu",NA
14.5 Why a Good Data Platform Is Important,"Podcast Episode:
  #066 How To Do Data Science From A Data Engineers Perspective A 
 simple introduction how to do data science in the context of the internet of things.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 14.2: Podcast: 066 How To Do Data Science From A Data Engineers..
  
 45",NA
15 Data Warehouse vs Data Lake,"Podcast Episode:
  #055 Data Warehouse vs Data Lake
  
 On this podcast we are going to talk about data warehouses and data lakes? When
  
 do people use which? What are the pros and cons of both? Architecture examples
  
 for both Does it make sense to completely move to a data lake?
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 15.1: Podcast: 055 Data Warehouse vs Data Lake
  
 46",NA
16 Hadoop Platforms — available,"When people talk about big data, one of the first things come to mind is Hadoop. 
 Google’s search for Hadoop returns about 28 million results.
  
 It seems like you need Hadoop to do big data. Today I am going to shed light onto why 
 Hadoop is so trendy.
  
 You will see that Hadoop has evolved from a platform into an ecosystem. It’s design 
 allows a lot of Apache projects and 3rd party tools to benefit from Hadoop.
  
 I will conclude with my opinion on, if you need to learn Hadoop and if Hadoop is the 
 right technology for everybody.",NA
16.1 What is Hadoop,"Hadoop is a platform for distributed storing and analyzing of very large data sets. 
 Hadoop has four main modules: Hadoop common, HDFS, MapReduce and YARN. The 
 way these modules are woven together is what makes Hadoop so successful.
  
 The Hadoop common libraries and functions are working in the background. That’s why 
 I will not go further into them. They are mainly there to support Hadoop’s modules.
  
 Podcast Episode:
  #060 What Is Hadoop And Is Hadoop Still Relevant In 2019?
  
 A Introduction into Hadoop HDFS, YARN and MapReduce. Yes, Hadoop is still 
 relevant in 2019 even if you look into serverless tools.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 16.1: Podcast: 060 What Is Hadoop And Is Hadoop Still Relevant In 2019?
  
 47",NA
16.2 What makes Hadoop so popular? — available,"Storing and analyzing data as large as you want is nice. But what makes Hadoop so 
 popular?
  
 Hadoop’s core functionality is the driver of Hadoop’s adoption. 
  
 Many Apache side 
 projects use it’s core functions.
  
 Because of all those side projects Hadoop has turned more into an ecosystem. An 
 ecosys-tem for storing and processing big data.
  
 To better visualize this eco system I have drawn you the following graphic. It shows 
 some projects of the Hadoop ecosystem who are closely connected with the Hadoop.
  
 It is not a complete list. There are many more tools that even I don’t know. Maybe I am 
 drawing a complete map in the future.
  
  
 Figure 16.1: Hadoop Ecosystem Components
  
 48",NA
16.3 Hadoop Ecosystem Components,"Remember my big data platform blueprint? The blueprint has four stages: Ingest, store, 
 analyse and display.
  
 Because of the Hadoop ecosystem” the different tools in these stages can work together 
 perfectly.
  
 Here’s an example:
  
  
 Figure 16.2: Connections between tools
  
 You use Apache Kafka to ingest data, and store the it in HDFS. You do the analytics with 
 Apache Spark and as a backend for the display you store data in Apache HBase. To have 
 a working system you also need YARN for resource management. You also need 
 Zookeeper, a configuration management service to use Kafka and HBase 
  
 As you can see in the picture below each project is closely connected to the other. Spark 
 for instance, can directly access Kafka to consume messages. It is able to access HDFS for 
 storing or processing stored data.
  
 It also can write into HBase to push analytics results to the front end.
  
 49",NA
16.4 Hadoop Is Everywhere?,"Although Hadoop is so popular it is not the silver bullet. It isn’t the tool that you should 
 use for everything.
  
 Often times it does not make sense to deploy a Hadoop cluster, because it can be 
 overkill. Hadoop does not run on a single server.
  
 You basically need at least five servers, better six to run a small cluster. Because of that. 
 the initial platform costs are quite high.
  
 50",NA
16.5 Should you learn Hadoop?,"Yes, I definitely recommend you to get to now how Hadoop works and how to use it. As I 
 have shown you in this article, the ecosystem is quite large.
  
 Many big data projects use Hadoop or can interface with it. Thats why it is generally a 
 good idea to know as many big data technologies as possible.
  
 Not in depth, but to the point that you know how they work and how you can use them. 
 Your main goal should be to be able to hit the ground running when you join a big data 
 project.
  
 Plus, most of the technologies are open source. You can try them out for free.
  
 How does a Hadoop System architecture look like
  
 What tools are usually in a with Hadoop Cluster
  
 Yarn Zookeeper HDFS Oozie Flume Hive",NA
16.6 How to select Hadoop Cluster Hardware,51,NA
17 Docker,NA,NA
17.1 What is docker and what do you use it for —,NA,NA
available,"Have you played around with Docker yet? If you’re a data science learner or a data 
 scientist you need to check it out!
  
 It’s awesome because it simplifies the way you can set up development environments 
 for data science. If you want to set up a dev environment you usually have to install a lot 
 of packages and tools.
  
 17.1.1 Don’t Mess Up Your System
  
 What this does is you basically mess up your operating system. If you’re a starter you 
 don’t know which packages you need to install. You don’t know which tools you need to 
 install.
  
 If you want to for instance start with Jupyter notebooks you need to install that on your 
 PC somehow. Or you need to start installing tools like PyCharmor Anaconda.
  
 All that gets added to your system and so you mess up your system more and more and 
 more. What Docker brings you, especially if you’re on a Mac or a Linux system is 
 simplicity.
  
 17.1.2 Preconfigured Images
  
 Because it is so easy to install on those systems. Another cool thing about docker images 
 is you can just search them in the Docker store, download them and install them on your 
 system.
  
 Running them in a completely pre-configured environment. 
  
 You don’t need to think 
 about stuff you go to the Docker library you search for deep learning GPU and Python.
  
 52",NA
17.2 Kubernetes Container Deployment,"I am getting into Docker a lot more myself. For a bit different reasons.
  
 What I’m looking for is using Docker with Kubernetes. With Kubernetes you can auto-
 mate the whole container deployment process.
  
 The idea with is that you have a cluster of machines. Lets say you have 10 server cluster 
 and you run Kubernetes on them.
  
 Kubernetes lets you spin up Docker containers on-demand to execute tasks. You can set 
 up how much resources like CPU, RAM, Network, Docker container can use.
  
 You can basically spin up containers, on the cluster on demand. When ever you need to 
 do a analytics task.
  
 Perfect for Data Science.
  
 53",NA
"17.3 How to create, start,stop a Container",NA,NA
17.4 Docker micro services?,NA,NA
17.5 Kubernetes,NA,NA
17.6 Why and how to do Docker container,NA,NA
orchestration,"Podcast about how data science learners use Docker (for data scientists): 
 https://anchor.fm/andreaskayy Data-Science-Go-Docker-e10n7u",NA
17.7 Useful Docker Commands,"Create a container:
  
 docker run CONTAINER
  −−
 network NETWORK
  
 Start a stopped container:
  
 docker
  
 s t a r t CONTAINERNAME
  
 Stop a running container:
  
 docker
  
 stop
  
 List all running containers
  
 docker ps
  
 List all containers including stopped ones
  
 docker ps
  −
 a
  
 Inspect the container configuration. For instance network settings and so on:
  
 docker
  
 inspect CONTAINER
  
 List all available virtual networks:
  
 docker network
  
 l s
  
 54",NA
18 REST APIs,"APIs, Application Programming Interfaces are the cornerstones of any great data plat-
  
 form.
  
 Podcast Episode:
  #033 How APIs Rule The World 
  
 Strong APIs make a good platform. In this episode I talk about why you need APIs 
 and why Twitter is a great example. Especially JSON APIs are my personal favorite. 
 Because JSON is also important in the Big Data world, for instance in log analytics. 
 How? Check out this episode!
  
 Audio
  
 Click here to listen
  
 Table 18.1: Podcast: 033 How APIs Rule The World",NA
18.1 API Design,"In this podcast episode we look into the Twitter API. It’s a great example how to build
  
 an API
  
 Podcast Episode:
  #081 Twitter API Research Data Engineering Course Part 5 In 
 this episode we look into the Twitter API documentation, which I love by the way. 
 How can we get old tweets for a certain hashtags and how to get current live tweets 
 for these hashtags?
  
 Audio 
  
 Youtube
  
 Click here to listen 
  
 Click here to watch
  
 Table 18.2: Podcast: 081 Twitter API Research",NA
18.2 Implementation Frameworks,"Jersey:
  
 https://jersey.github.io/documentation/latest/getting-started.html
  
 Swagger:
  
 56",NA
18.3 OAuth security,NA,NA
19 Databases,NA,NA
19.1 SQL Databases,"19.1.1 PostgreSQL DB
  
 Homepage: 
  
 https://www.postgresql.org/ 
  
 Postgre vs Mongodb: 
  
 https://blog.panoply.io/postgresql-vs-
 mongodb
  
 19.1.2 Database Design
  
 19.1.3 SQL Queries
  
 19.1.4 Stored Procedures
  
 19.1.5 ODBC/JDBC Server Connections",NA
19.2 NoSQL Stores,"19.2.1 KeyValue Stores (HBase)
  
 19.2.2 Document Store HDFS — available
  
 The Hadoop distributed file system, or HDFS, allows you to store files in Hadoop. The 
 difference between HDFS and other file systems like NTFS or EXT is that it is a dis-
 tributed one.",NA
20 Data Processing and Analytics -,NA,NA
Frameworks,NA,NA
20.1 Is ETL still relevant for Analytics?,"Podcast Episode:
  #039 Is ETL Dead For Data Science & Big Data?
  
 Is ETL dead in Data Science and Big Data? In today’s podcast I share with you my 
 views on your questions regarding ETL (extract, transform, load). Is ETL still 
 practiced or did pre processing & cleansing replace it What would replace ETL in 
 Data Engineering
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 20.1: Podcast: 039 Is ETL Dead for Data Science and Big Data?",NA
20.2 Stream Processing,"20.2.1 Three methods of streaming — available
  
 In stream processing sometimes it is ok to drop messages, other times it is not. 
 Sometimes
  
 it is fine to process a message multiple times, other times that needs to be avoided like
  
 hell.
  
 Today’s topic are the different methods of streaming: At most once, at least once and
  
 exactly once.
  
 What this means and why it is so important to keep them in mind when creating a
  
 solution. That is what you will find out in this article.",NA
20.3 MapReduce,"Since the early days of the Hadoop eco system, the MapReduce framework is one of the 
 main components of Hadoop alongside the Hadoop file system HDFS.
  
 Google for instance used MapReduce to analyse stored html content of websites through 
 counting all the html tags and all the words and combinations of them (for instance 
 headlines). The output was then used to create the page ranking for Google Search.
  
 That was when everybody started to optimise his website for the google search. Serious 
 search engine optimisation was borne. That was the year 2004.
  
 How MapReduce is working is, that it processes data in two phases: The map phase and 
 the reduce phase.
  
 In the map phase, the framework is reading data from HDFS. Each dataset is called an 
 input record.
  
 Then there is the reduce phase. In the reduce phase, the actual computation is done and 
 the results are stored. The storage target can either be a database or back HDFS or 
 something else.
  
 67",NA
20.4 Apache Spark,"I talked about the three methods of data streaming in this podcast: 
 https://anchor.fm/andreaskayy/emb Methods-of-Streaming-Data-e15r6o
  
 20.4.1 What is the difference to MapReduce? – available
  
 Spark is a complete in memory framework. Data gets loaded from, for instance hdfs, into 
 the memory of workers.
  
 There is no longer a fixed map and reduce stage. Your code can be as complex as you 
 want.
  
 71",NA
20.5 Apache Nifi,"Great blog about Nifi:
  https://www.datainmotion.dev",NA
20.6 StreamSets,"https://youtu.be/djt8532UWow
  
 76",NA
21 Apache Kafka,NA,NA
21.1 Why a message queue tool?,NA,NA
21.2 Kakfa architecture,NA,NA
21.3 What are topics,NA,NA
21.4 What does Zookeeper have to do with Kafka,NA,NA
21.5 How to produce and consume messages,"My YouTube video how to set up Kafka at home: https://youtu.be/7F9tBwTUSeY
  
 My YouTube video how to write to Kafka: https://youtu.be/RboQBZvZCh0",NA
21.6 KAFKA Commands,"Start Zookeeper container for Kafka:
  
 docker run
  −
 d
  −−
 name zookeeper
 −
 server
  
 −−
 network app
 −
 t i e r 
  
 \
  
  
  
 −
 e ALLOW ANONYMOUS LOGIN=yes 
  
  
  
 bitnami/ zookeeper : l a t e s t
  
 \ 
  
 \
  
 Start Kafka container:
  
 docker run
  −
 d
  −−
 name kafka
 −
 server 
  
 \
  
 −−
 network app
 −
 t i e r 
 \
  
 −
 e KAFKA CFG ZOOKEEPER CONNECT=zookeeper
 −
 server :2181
  
 \
  
 78",NA
22 Machine Learning,"Podcast Episode:
  Machine Learning In Production 
  
 Doing machine learning in production is very different then for proof of concepts or 
 in education. One of the hardest parts is keeping models updated.
  
 Anchor
  
 Click here to listen
  
 Table 22.1: Podcast: Machine Learning In Production",NA
22.1 Training and Applying models,NA,NA
22.2 What is deep learning,NA,NA
22.3 How to do Machine Learning in production —,NA,NA
available,"Machine learning in production is using stream and batch processing. 
  
 In the batch 
 processing layer you are creating the models, because you have all the data available for 
 training.
  
 In the stream in processing layer you are using the created models, you are applying 
 them to new data.
  
 The idea that you need to incorporate is that it is a constant cycle. Training, applying, re-
 training, pushing into production and applying.
  
 What you don’t want to do is, you don’t want to do this manually. You need to figure out 
 a process of automatic retraining and automatic pushing to into production of models.
  
 In the retraining phase the system automatically evaluates the training. If the model no 
 longer fits it works as long as it needs to create a good model.
  
 After the evaluation of the model is complete and it’s good, the model gets pushed into 
 production. Into the stream processing.
  
 80",NA
22.4 Why machine learning in production is harder ,NA,NA
then you think – available,"How to automate machine learning is something that drives me day in and day out. 
 What you do in development or education is, that you create a model and fit it to the 
 data. Then that model is basically done forever.
  
 Where I’m coming from, the IoT world, the problem is that machines are very different. 
 They behave very different and experience wear.",NA
22.5 Models Do Not Work Forever,"Machines have certain processes that decrease the actual health of the machine. 
 Machine wear is a huge issue. Models that that are built on top of a good machine don’t 
 work forever.
  
 When the Machine wears out, the models need to be adjusted. They need to be main-
 tained, retrained.",NA
22.6 Where The Platforms That Support This?,"Automatic re-training and re-deploying is a very big issue, a very big problem for a lot of 
 companies. Because most existing platforms don’t have this capability (I actually haven’t 
 seen one until now).
  
 Look at AWS machine learning for instance. The process is: build, train, tune deploy. 
 Where’s the loop of retraining?
  
 You can create models and then use them in production. But this loop is almost nowhere 
 to be seen.
  
 It is a very big issue that needs to be solved. If you want to do machine learning in 
 production you can start with manual interaction of the training, but at some point you 
 need to automate everything.
  
 81",NA
22.7 Training Parameter Management,"To train a model you are manipulating input parameters of the models.
  
 Take deep learning for instance. To train you are manipulating for instance: 
  
 How many layers do you use. The depth of the layers, which means how many neurons 
 you have in a layer. What activation function you use, how long are you training and so 
 on.
  
 You also need to keep track of what data you used to train which model.
  
 All those parameters need to be manipulated automatically, models trained and tested.
  
 To do all that, you basically need a database that keeps track of those variables. How to 
 automate this, for me, is like the big secret. I am still working on figuring it out.",NA
22.8 What’s Your Solution?,"Did you already have the problem of automatic re-training and deploying of models as 
 well?
  
 Were you able to use a cloud platform like Google, AWS or Azure?
  
 It would be really awesome if you share your experience :)",NA
22.9 How to convince people machine learning works— ,NA,NA
available,"Many people still are not convinced that machine learning works reliably. But they want 
 analytics insight and most of the time machine learning is the way to go.
  
 This means, when you are working with customers you need to do a lot of convincing. 
 Especially if they are not into machine learning themselves.
  
 But it’s actually quite easy.
  
 82",NA
"22.10 No Rules, No Physical Models","Many people are still under the impression that analytics only works when it’s based on 
 physics. When there are strict mathematical rules to a problem.
  
 Especially in engineering heavy countries like Germany this is the norm:
  
 “Sere has to be a Rule for Everysing!” (imagine a German accent) When you’re engi-
 neering you are calculating stuff based on physics and not based on data. If you are 
 constructing an airplane wing, you better make sure to use calculations so it doesn’t fall 
 off.
  
 And that’s totally fine.
  
 Keep doing that!
  
 Machine learning has been around for decades. It didn’t quite work as good as people 
 hoped. We have to admit that. But there is this preconception that it still doesn’t work. 
 Which is not true: Machine learning works.
  
 Somehow you need to convince people that it is a viable approach. That learning from 
 data to make predictions is working perfectly.",NA
22.11 You Have The Data. USE IT!,"As a data scientist you have one ace up your sleeve, it’s the obvious one: 
 It’s the data and it’s statistics.
  
 You can use that data and those statistics to counter peoples preconceptions. It’s very 
 powerful if someone says: “This doesn’t work”
  
 You bring the data. You show the statistics and you show that it works reliably. A lot of 
 discussions end there.
  
 Data doesn’t lie. You can’t fight data. The data is always right.
  
 83",NA
22.12 Data is Stronger Than Opinions,"This is also why I believe that autonomous driving will come quicker than many of us 
 think. Because a lot of people say, they are not safe. That you cannot rely on those cars. 
 The thing is: When you have the data you can do the statistics.
  
 You can show people that autonomous driving really works reliably. You will see, the 
 question of: Is this is this allowed or is this not allowed? Will be gone quicker than you 
 think.
  
 Because government agencies can start testing the algorithms based on predefined sce-
 narios. They can run benchmarks and score the cars performance.
  
 All those opinions, if it works, or if it doesn’t work, they will be gone.
  
 The motor agency has the statistics. The stats show people how good cars work.
  
 Companies like Tesla, they have it very easy. Because the data is already there. 
 They just need to show us that the algorithms work. The end.",NA
22.13 AWS Sagemaker,"Train and apply models online with Sagemaker 
  
 Link to the OLX Slideshare with pros, cons and how to use Sagemaker:
  https://www.
  
 slideshare.net/mobile/AlexeyGrigorev/image-models-infrastructure-at-olx
  
 84",NA
23 Data Visualization,NA,NA
23.1 Android & IOS,NA,NA
23.2 How to design APIs for mobile apps,NA,NA
23.3 How to use Webservers to display content,"This section does not contain any text that’s why the page is messed up
  
 85",NA
23.4 Business Intelligence Tools,"23.4.1 Tableau
  
 23.4.2 PowerBI
  
 23.4.3 Quliksense",NA
23.5 Identity & Device Management,"23.5.1 What is a digital twin?
  
 23.5.2 Active Directory
  
 86",NA
Part III ,NA,NA
Data Engineering Course: Building A ,NA,NA
Data Platform,87,NA
24 What We Want To Do,"•
  Twitter data to predict best time to post using the hashtag datascience or ai
 •
  
 Find top tweets for the day
  
 •
  Top users
  
 •
  Analyze sentiment and keywords
  
 88",NA
25 Thoughts On Choosing A ,NA,NA
Development Environment,"For a local environment you need a good PC. I thought a bit about a budget build around 
 1.000 Dollars or Euros.
  
 Podcast Episode:
  #068 How to Build a Budget Data Science PC 
  
 In this podcast we look into configuring a sub 1000 dollar PC for data engineering 
 and machine learning
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 25.1: Podcast: 068 How to Build a Budget Data Science PC
  
 89",NA
26 A Look Into the Twitter API,"Podcast Episode:
  #081 Twitter API Research
  
 In this podcast we were looking into how the Twitter API works and how you get
  
 access to it
  
 YouTube
  
 Click here to watch
  
 Table 26.1: Podcast: 081 Twitter API Research
  
 90",NA
27 Ingesting Tweets with Apache,NA,NA
Nifi,"Podcast Episode:
  #082 Reading Tweets With Apache Nifi & IaaS vs PaaS vs SaaS In 
 this podcast we are trying to read Twitter Data with Nifi
  
 YouTube
  
 Click here to watch
  
 Table 27.1: Podcast: 082 Reading Tweets With Apache Nifi
  
 Podcast Episode:
  #085 Trying to read Tweets with Nifi Part 2 
  
 We are looking into the Big Data landscape chart and we are trying to read Twitter 
 Data with Nifi again
  
 YouTube
  
 Click here to watch
  
 Table 27.2: Podcast: 085 Trying to read Tweets with Nifi Part 2
  
 91",NA
28 Writing from Nifi to Apache,NA,NA
Kafka,"Podcast Episode:
  #086 How to Write from Nifi to Kafka Part 1 
  
 I’ve been working a lot on the cookbook, because it’s so much fun. I gotta tell you 
 what I added. Then we are trying to write the Tweets from Apache Nifi into Kafka. 
 Also talk about Kafka basics.
  
 YouTube
  
 Click here to watch
  
 Table 28.1: Podcast: 086 How to Write from Nifi to Kafka Part 1
  
 Podcast Episode:
  #088 How to Write from Nifi to Kafka Part 2 
  
 In this podcast we finally figure out how to write to Kafka from Nifi. The problem 
 was the network configuration of the Docker containers
  
 YouTube
  
 Click here to watch
  
 Table 28.2: Podcast: 088 How to Write from Nifi to Kafka Part 2
  
 92",NA
29 Apache Zeppelin,NA,NA
29.1 Install and Ingest Kafka Topic,"Start the container:
  
 docker run
  −
 d
  −
 p 8081:8080
  −−
 rm
  \
  
 −
 v / Users /xxxx/Documents/ DockerFiles / logs :/ logs
  \
  
 −
 v / Users /xxxx/Documents/ DockerFiles /Notebooks :/ notebook
  \\u2212
 e 
 ZEPPELIN LOG DIR=’/ logs ’
  \
  
 −
 e ZEPPELIN NOTEBOOK DIR=’/notebook ’
  \
  
 −−
 network app
 −
 t i e r
  −−
 name zeppelin 
  
 apache/ zeppelin : 0 . 7 . 3",NA
29.2 Processing Messages with Spark & SparkSQL,NA,NA
29.3 Visualizing Data,93,NA
30 Switch Processing from Zeppelin ,NA,NA
to Spark,NA,NA
30.1 Install Spark,NA,NA
30.2 Ingest Messages from Kafka,NA,NA
30.3 Writing from Spark to Kafka,NA,NA
30.4 Move Zeppelin Code to Spark,94,NA
Part IV ,NA,NA
Case Studies,95,NA
31 How I do Case Studies,NA,NA
31.1 Data Science @Airbnb,"Podcast Episode:
  #063 Data Engineering At Airbnb Case Study How 
 Airbnb is doing Data Engineering? Let’s check it out.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.1: Podcast: 063 Data Engineering At Airbnb Case Study
  
 Slides: 
  
 https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home 
  
 Airbnb Engineering Blog:
  https://medium.com/airbnb-engineering 
  
 Data Infrastructure:
  https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-
 8adfb34f1 
 Scaling the serving tier:
  https://medium.com/airbnb-engineering/unlocking-horizontal-
 scalability-in-ou 
 Druid Analytics:
  https://medium.com/airbnb-engineering/druid-airbnb-data-
 platform-601c312f2a4c 
 Spark Streaming for logging events:
  https://medium.com/airbnb-
 engineering/scaling-spark-streaming-f
 -Druid Wiki:
  https://en.wikipedia.org/wiki/Apache Druid
  
 31.1.1 Data Science @Amazon
  
 https://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-
 opera https://aws.amazon.com/solutions/case-studies/amazon-migration-analytics/",NA
31.2 Data Science @Baidu,"https://www.slideshare.net/databricks/spark-sql-adaptive-execution-unleashes-the-power-of-
 cluster-in",NA
31.3 Data Science @Blackrock,"https://www.slideshare.net/DataStax/maintaining-consistency-across-data-centers-randy-fradin-
 black",NA
31.4 Data Science @BMW,"Big Data in der Automobilindustrie – Daten aus dem Fahrzeug nutzen
  https://www. 
 unibw.de/code.../ws3 bigdata vortrag widmann.pdf",NA
31.5 Data Science @Booking.com,"Podcast Episode:
  #064 Data Engineering At Booking.com Case Study 
 How Booking.com is doing Data Engineering? Let’s check it out.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.2: Podcast: 064 Data Engineering At Booking.com Case Study
  
 Slides: 
  
 https://www.slideshare.net/ConfluentInc/data-streaming-ecosystem-management-at-
 bookingcom?
  
 ref=https://www.confluent.io/kafka-summit-sf18/data-streaming-ecosystem-management 
  
 https://www.slideshare.net/SparkSummit/productionizing-behavioural-features-for-machine-
 learning-https://www.slideshare.net/ConfluentInc/data-streaming-ecosystem-management-at-
 bookingcom?
  
 ref=https://www.confluent.io/kafka-summit-sf18/data-streaming-ecosystem-
 management 
 Druid:
  https://towardsdatascience.com/introduction-to-druid-
 4bf285b92b5a 
  
 Kafka Architecture:
  https://data-flair.training/blogs/kafka-architecture/ 
  
 Confluent Platform:
  https://www.confluent.io/product/confluent-platform/",NA
31.6 Data Science @CERN,NA,NA
31.7 Data Science @Disney,"https://medium.com/disney-streaming/delivering-data-in-real-time-via-auto-scaling-kinesis-
 streams-7",NA
31.8 Data Science @Drivetribe,"https://berlin-2017.flink-forward.org/kb sessions/drivetribes-kappa-architecture-with-apache-
 flink/ https://www.slideshare.net/FlinkForward/flink-forward-berlin-2017-aris-kyriakos-
 koliopoulos-drivetrib",NA
31.9 Data Science @Dropbox,NA,NA
31.10 Data Science @Ebay ,"https://www.slideshare.net/databricks/moving-ebays-data-warehouse-over-to-apache-spark-spark-
 as-c
  
 https://www.slideshare.net/databricks/analytical-dbms-to-apache-spark-auto-migration-
 framework-w",NA
31.11 Data Science @Expedia ,"https://www.slideshare.net/BrandonOBrien/spark-streaming-kafka-best-practices-w-brandon-
 obrien
  
 https://www.slideshare.net/Naveen1914/brandon-obrien-streamingdata",NA
31.12 Data Science @Facebook ,"https://code.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-
 case/",NA
31.13 Data Science @Google ,"http://www.unofficialgoogledatascience.com/
  
 https://ai.google/research/teams/ai-fundamentals-applications/
  
 https://cloud.google.com/solutions/big-data/
  
 https://datafloq.com/read/google-applies-big-data-infographic/385",NA
31.14 Data Science @@Grammarly ,"https://www.slideshare.net/databricks/building-a-versatile-analytics-pipeline-on-top-of-apache-
 spark-",NA
31.15 Data Science @ING Fraud ,"https://sf-2017.flink-forward.org/kb sessions/streaming-models-how-ing-adds-models-at-runtime-
 to-ca",NA
31.16 Data Science @Instagram ,"https://www.slideshare.net/SparkSummit/lessons-learned-developing-and-managing-massive-
 300tb-ap
  
 99",NA
31.17 Data Science @LinkedIn,"Podcast Episode:
  #073 Data Engineering At LinkedIn Case Study 
 Let’s check out how LinkedIn is processing data :)
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.4: Podcast: 073 Data Engineering At LinkedIn Case Study
  
 Slides: 
  
 https://engineering.linkedin.com/teams/data#0 
  
 https://www.slideshare.net/yaelgarten/building-a-healthy-data-ecosystem-around-kafka-and-
 hadoop-l https://engineering.linkedin.com/teams/data/projects/pinot 
  
 https://pinot.readthedocs.io/en/latest/intro.html# 
  
 https://towardsdatascience.com/building-machine-learning-at-linkedin-scale-f08bd9a63f0a 
  
 http://samza.apache.org 
  
 https://www.slideshare.net/ConfluentInc/more-data-more-problems-scaling-kafkamirroring-
 pipelines-a ref=https://www.confluent.io/kafka-summit-sf18/more data more problems 
  
 https://www.slideshare.net/KhaiTran17/conquering-the-lambda-architecture-in-linkedin-metrics-
 platf https://www.slideshare.net/Hadoop Summit/unified-batch-stream-processing-with-apache-
 samza http://druid.io/docs/latest/design/index.html",NA
31.18 Data Science @Lyft,https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff,NA
31.19 Data Science @NASA,"Slides: 
  
 http://sites.nationalacademies.org/cs/groups/ssbsite/documents/webpage/ssb 
 182893.pdf https://esip.figshare.com/articles/Apache Science Data Analytics 
 Platform/5786421",NA
31.20 Data Science @Netflix – available,"Podcast Episode:
  #062 Data Engineering At Netflix Case Study How 
 Netflix is doing Data Engineering using their Keystone platform.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.6: Podcast: 062 Data Engineering At Netflix Case Study
  
 Slides: 
  
 Netflix revolutionized how we watch movies and tv. Currently over 75 million users 
 watch 125 million hours of Netflix content every day!
  
 Netflix’s revenue comes from a monthly subscription service. So, the goal for Netflix is to 
 keep you subscribed and to get new subscribers.
  
 To achieve this, Netflix is licensing movies from studios as well as creating its own 
 original movies and tv series.
  
 But offering new content is not everything. What is also very important is, to keep you 
 watching content that already exists.
  
 To be able to recommend you content, Netflix is collecting data from users. And it is 
 collecting a lot.
  
 Currently, Netflix analyses about 500 billion user events per day. 
  
 That results in a 
 stunning 1.3 petabytes every day.",NA
31.21 Data Science @OLX,"Podcast Episode:
  #083 Data Engineering at OLX Case Study 
  
 This podcast is a case study about OLX with Senior Data Scientist Alexey Grigorev as 
 guest. It was super fun.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.7: Podcast: 083 Data Engineering at OLX Case Study
  
 Slides:
  
 https://www.slideshare.net/mobile/AlexeyGrigorev/image-models-infrastructure-at-olx",NA
31.22 Data Science @OTTO,"https://www.slideshare.net/SparkSummit/spark-summit-eu-talk-by-sebastian-schroeder-and-ralf-
 sigm",NA
31.23 Data Science @Paypal,https://www.paypal-engineering.com/tag/data/,NA
31.24 Data Science @Pinterest,"Podcast Episode:
  #069 Engineering Culture At Pinterest 
  
 In this podcast we look into data platform and processing at Pinterest.
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.8: Podcast: 069 Engineering Culture At Pinterest
  
 Slides:",NA
31.25 Data Science @Salesforce,"https://engineering.salesforce.com/building-a-scalable-event-pipeline-with-heroku-and-salesforce-
 2549c",NA
31.26 Data Science @Siemens Mindsphere,"Podcast Episode:
  #059 What Is The Siemens Mindsphere IoT Platform? The 
 Internet of things is a huge deal. There are many platforms available. But, which one 
 is actually good? Join me on a 50 minute dive into the Siemens Mind-sphere online 
 documentation. I have to say I was super unimpressed by what I found. Many 
 limitations, unclear architecture and no pricing available? Not good!
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.9: Podcast: 059 What Is The Siemens Mindsphere IoT Platform?",NA
31.27 Data Science @Slack,https://speakerdeck.com/vananth22/streaming-data-pipelines-at-slack,NA
31.28 Data Science @Spotify,"Podcast Episode:
  #071 Data Engineering At Spotify Case Study 
  
 In this episode we are looking at the data engineering at Spotify, my favorite music 
 streaming service. How do they process all that data?
  
 YouTube 
 Audio
  
 Click here to watch 
  
 Click here to listen
  
 Table 31.10: Podcast: 071 Data Engineering At Spotify Case Study
  
 Slides: 
  
 https://labs.spotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/ 
 https://labs.spotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/ 
 https://labs.spotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/ 
 https://www.slideshare.net/InfoQ/scaling-the-data-infrastructure-spotify 
  
 https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/ 
  
 https://labs.spotify.com/2017/04/26/reliable-export-of-cloud-pubsub-streams-to-cloud-
 storage/ https://labs.spotify.com/2017/11/20/autoscaling-pub-sub-consumers/",NA
31.29 Data Science @Symantec,"https://www.slideshare.net/planetcassandra/symantec-cassandra-data-modelling-techniques-in-
 action",NA
31.30 Data Science @Tinder,"https://www.slideshare.net/databricks/scalable-monitoring-using-apache-spark-and-friends-with-
 utkar",NA
31.31 Data Science @Twitter,"Slides: 
  
 https://www.slideshare.net/sawjd/real-time-processing-using-twitter-heron-by-karthik-ramasamy 
 https://www.slideshare.net/sawjd/big-data-day-la-2016-big-data-track-twitter-heron-scale-karthik-
 ram",NA
31.32 Data Science @Uber,"https://eng.uber.com/uber-big-data-platform/ 
  
 https://eng.uber.com/aresdb/ 
  
 https://www.uber.com/us/en/uberai/",NA
31.33 Data Science @Upwork,"https://www.slideshare.net/databricks/how-to-rebuild-an-endtoend-ml-pipeline-with-databricks-
 and-u",NA
31.34 Data Science @Woot,"https://aws.amazon.com/de/blogs/big-data/our-data-lake-story-how-woot-com-built-a-serverless-
 data",NA
31.35 Data Science @Zalando,"Podcast Episode:
  #087 Data Engineering At Zalando Case Study Talk 
  
 I had a great conversation about data engineering for online retailing with Michal 
 Gancarski and Max Schultze. They showed Zalando’s data platform and how they 
 build data pipelines. Super interesting especially for AWS users.
  
 YouTube
  
 Click here to watch
  
 Table 31.12: Podcast: 087 Data Engineering At Zalando Case Study
  
 Do me a favor and give these guys a follow on LinkedIn: LinkedIn of Michal:
  https: 
 //www.linkedin.com/in/michalgancarski/ 
  
 LinkedIn of Max:
  https://www.linkedin.com/in/max-schultze-b11996110/ 
  
 Zalando has a tech blog with more infos and there is also a meetup in Berlin: Zalando Blog:
  
 https://jobs.zalando.com/tech/blog/ 
  
 Next Zalando Data Engineering Meetup:
  https://www.meetup.com/Zalando-Tech-Events-Berlin/ 
 events/262032282/ 
  
 Interesting tools: AWS CDK:
  https://docs.aws.amazon.com/cdk/latest/guide/what-is.
  
 html 
  
 Delta Lake:
  https://delta.io/ 
  
 AWS Step Functions:
  https://aws.amazon.com/step-functions/AWSStateLanguage:https: 
  
 //states-language.net/spec.html 
  
 Youtube channel of the meetup:
  https://www.youtube.com/channel/UCxwul7aBm2LybbpKGbCOYNA 
 playliststalkatSpark+AI 
  
 Summit about Zalando’s Processing Platform:
  https://databricks.com/session/continuous-
 applications-",NA
Part V ,NA,NA
1001 Interview Questions,111,NA
32 Live Streams,"First live stream where we started to collect these questions.
  
 Podcast Episode:
  #096 1001 Data Engineering Interview Questions 
  
 First live stream where we collect and try to answer as many interview questions as 
 possible. If this helps people and is fun we do this regularly until we reached 1000 
 and one.
  
 YouTube
  
 Click here to watch
  
 Table 32.1: Podcast: 096 1001 Data Engineering Interview Questions
  
 113",NA
33 All Interview Questions,"The interview questions are roughly structured like the sections in the ”Basic data Engi-
 neering Skills” part. This makes it easier to navigate this document. I still need to sort 
 them accordingly.",NA
SQL DBs,"•
  What are windowing functions?
  
 •
  What is a stored procedure
  
 •
  Why would you use them?
  
 •
  What are atomic attributes
  
 •
  Explain ACID props of a database
  
 •
  How to optimize queries
  
 •
  What are the different types of JOIN (CROSS, INNER, OUTER)
  
 •
  What is the difference between Clustered Index and Non-Clustered Index - with 
 examples?",NA
The Cloud,"•
  What is serverless
  
 •
  What’s the difference between IaaS, PaaS and SaaS
  
 •
  How do you move from the ingest layer to the Cosumption layer? (In Serverless)
 •
  
 Whats the difference between cloud and edge and on-premises
  
 •
  What is edge computing
  
 114",NA
Linux,"•
  What is crontab",NA
Big Data,"•
  What are the 4 V’s
  
 •
  Which one is most important?",NA
Kafka,"•
  What is a topic
  
 •
  How to ensure FIFO
  
 •
  How do you know if all messages in a topic have been fully consumed
 •
  
 What are brokers
  
 •
  What are consumergroups
  
 •
  What is a producer",NA
Coding,"•
  What’s the difference between an object and a class
  
 •
  Explain immutability
  
 •
  What are AWS Lambda functions and why would you use them
 •
  
 Difference between library, framework and package
  
 •
  How to reverse a linked list
  
 •
  difference between args and kwargs
  
 •
  Difference between oop and functional programming
  
 115",NA
NoSQL DBs,"•
  What’s a key/value (rowstore) store
  
 •
  What’s a columnstore
  
 •
  Diff between Row an col.store
  
 •
  What’s a document store
  
 •
  Difference between Redshift and Snowflake",NA
Hadoop,"•
  What File Formats can you use in Hadoop
  
 •
  Whats the difference between a name and a datanode
 •
  
 What is HDFS
  
 •
  What is the purpous of YARN",NA
Lambda Architecture,"•
  what is streaming and batching
  
 •
  what is the upside of streamtin vs batching
  
 •
  What’s the difference between lambda and kappa architecture
 •
  
 Can you sync the batch and streaming layer and if yes how",NA
Python,"•
  Difference between list tuples and dictionary",NA
Data Warehouse & Data Lake,"•
  What is a data lake?
  
 116",NA
APIs (REST),"•
  What does REST mean?
  
 •
  What is idempotency
  
 •
  What are common REST API frameworks (Jersey and Spring)",NA
Apache Spark,"•
  What’s an RDD
  
 •
  What is a dataframe
  
 •
  What is a dataset
  
 •
  How is a dataset typesafe
  
 •
  What is Parquet
  
 •
  What’s Avro
  
 •
  Difference between Parquet and Avro
  
 •
  Tumbling Windows Vs. Sliding Windows
  
 •
  Difference between batch ans stream 
 processing
 •
  What are microbatches
  
 117",NA
MapReduce,"•
  What’s a use case of mapreduce
  
 •
  Write a pseudo code for Wordcount
  
 •
  What is a combiner",NA
Docker & Kubernetes,"•
  What is a container
  
 •
  Difference between Docker Container and a Virtual PC
 •
  
 What s the easiest way to learn kubernetes fast",NA
Data Pipelines,"•
  What is an example of a serverless pipeline
  
 •
  What’s difference between at most once vs at least once vs exactly once
 •
  
 What systems provide transactions
  
 •
  What is a ETL pipeline",NA
Airflow,"•
  What is a DAG (in context of airflow/luigi)
 •
  
 What are Hooks/ is a hook
  
 •
  What are Operators
  
 •
  How to branch?",NA
DataViszualization,"•
  What’s a BI tool
  
 118",NA
Security/Privacy,"•
  What is Kerberos
  
 •
  What is a firewall
  
 •
  Whats GDPR?
  
 •
  What’s anonymization",NA
Distrubuted Systems,"•
  how clusters reach consensus (the answer was using consensus protocols like 
 Paxos or Raft). Good I didnt have to explain paxos
  
 •
  What is the cap theorem / explain it (What factors should be considered when 
 choosing a DB?)
  
 •
  How to choose right storage for different data consumers? 
 question
  
 It’s always a tricky",NA
Apache Flink,"•
  what is Flink used for
  
 •
  Flink vs Spark?",NA
GitHub,"•
  What are branches
  
 •
  What are commits
  
 •
  What’s a pull request",NA
Dev/Ops,"•
  What is continuous integration
  
 119",NA
Development / Agile,"•
  What is Scrum
  
 •
  What is OKR
  
 •
  What is Jira and what is it used for
  
 120",NA
Bibliography,"[1] J. Ely and I. Stavrov1,
  Analyzing chalk dust and writing speeds: computational and
  
 geometric approaches
 , BoDine Journal of Mathematics
  3
  (2001), 14-159.
  
 121",NA
List of Figures,"2.1 
  
 The Machine Learning Pipeline . . . . . . . . . . . . . . . . . . . . . . . 
  
 13
  
 12.1 Common SQL Platform Architecture . . . . . . . . . . . . . . . . . . . . 
  
 3
 5
  
 12.2 Scaling up a SQL Database . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 36
  
 12.3 Scaling out a SQL Database . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 3
 7
  
 13.1 Platfrom Blueprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 3
 9
  
 14.1 Batch Processing Pipeline 
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 43
  
 14.2 Stream Processing Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 4
 4
  
 16.1 Hadoop Ecosystem Components . . . . . . . . . . . . . . . . . . . . . . . 
  
 4
 8
  
 16.2 Connections between tools . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 4
 9
  
 16.3 Flume Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
  
 5
 0
  
 19.1 HDFS Master and Data Nodes . . . . . . . . . . . . . . . . . . . . . . . . 
  
 6
 0
  
 19.2 Distribution of Blocks for a 512MB File . . . . . . . . . . . . . . . . . . . 
  
 6
 0
  
 20.1 Mapping of input files and reducing of mapped records . . . . . . . . . . 
  
 6
 8
  
 20.2 MapReduce Example of Time Series Data 
  
 . . . . . . . . . . . . . . . . . 
  
 70",NA
List of Tables,"2.1 
  
 Podcast: 050 Data Engineer Scientist or Analyst Which One Is For You? 
  
 12
  
 2.2 
  
 Podcast: 048 From Wannabe Data Scientist To Engineer My Journey . . 
  
 14
  
 5.1 
  
 Podcast: 070 Engineering Culture At Spotify . . . . . . . . . . . . . . . . 
  
 22
  
 10.1 Podcast: 082 Reading Tweets With Apache Nifi & IaaS vs PaaS vs SaaS 
  
 3
 0
  
 10.2 Podcast: 076 Cloud vs On-Premise . . . . . . . . . . . . . . . . . . . . . 
  
 3
 0
  
 14.1 Podcast: 077 Lambda Architecture and Kappa Architecture 
  
 . . . . . . . 
  
 43
  
 14.2 Podcast: 066 How To Do Data Science From A Data Engineers.. . . . . . 
  
 4
 5
  
 15.1 Podcast: 055 Data Warehouse vs Data Lake . . . . . . . . . . . . . . . . 
  
 4
 6
  
 16.1 Podcast: 060 What Is Hadoop And Is Hadoop Still Relevant In 2019? . . 
  
 4
 7
  
 18.1 Podcast: 033 How APIs Rule The World . . . . . . . . . . . . . . . . . . 
  
 5
 6
  
 18.2 Podcast: 081 Twitter API Research . . . . . . . . . . . . . . . . . . . . . 
  
 5
 6
  
 19.1 Podcast: 056 NoSQL Key Value Stores Explained with HBase . . . . . . 
  
 5
 9
  
 19.2 Podcast: 093 What is MongoDB . . . . . . . . . . . . . . . . . . . . . . . 
  
 6
 1
  
 19.3 Podcast: What is Elasticsearch & Why is It So Popular? . . . . . . . . . 
  
 6
 2",NA
