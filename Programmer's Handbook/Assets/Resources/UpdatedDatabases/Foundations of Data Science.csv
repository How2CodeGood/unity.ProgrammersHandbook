Larger Text,Smaller Text,Symbol
Foundations of Data Science,NA,NA
∗,NA,NA
"Avrim Blum, John Hopcroft, and Ravindran Kannan ",NA,NA
Thursday 4,th,NA
"January, 2018","∗
 Copyright 2015. All rights reserved
  
 1",NA
Contents,"1
  
 Introduction
  
 9
  
 2
  
 High-Dimensional Space
  
 12
  
 3
  
 2.1
  
 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 12
  
 2.2
  
 The Law of Large Numbers
  
 . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 12
  
 2.3
  
 The Geometry of High Dimensions
  
 . . . . . . . . . . . . . . . . . . . . . .
  
 15
  
 2.4
  
 Properties of the Unit Ball . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 17
  
 2.4.1
  
 Volume of the Unit Ball
  
 . . . . . . . . . . . . . . . . . . . . . . . .
  
 17
  
 2.4.2
  
 Volume Near the Equator
  
 . . . . . . . . . . . . . . . . . . . . . . .
  
 19
  
 2.5
  
 Generating Points Uniformly at Random from a Ball
  
 . . . . . . . . . . . .
  
 22
  
 2.6
  
 Gaussians in High Dimension
  
 . . . . . . . . . . . . . . . . . . . . . . . . .
  
 23
  
 2.7
  
 Random Projection and Johnson-Lindenstrauss Lemma . . . . . . . . . . .
  
 25
  
 2.8
  
 Separating Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 27
  
 2.9
  
 Fitting a Spherical Gaussian to Data . . . . . . . . . . . . . . . . . . . . .
  
 29
  
 2.10 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 31
  
 2.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 32
  
 Best-Fit Subspaces and Singular Value Decomposition (SVD)
  
 40
  
 4
  
 3.1
  
 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 40
  
 3.2
  
 Preliminaries
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 41
  
 3.3
  
 Singular Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 42
  
 3.4
  
 Singular Value Decomposition (SVD) . . . . . . . . . . . . . . . . . . . . .
  
 45
  
 3.5
  
 Best Rank-
 k
  Approximations
  
 . . . . . . . . . . . . . . . . . . . . . . . . .
  
 47
  
 3.6
  
 Left Singular Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 48
  
 3.7
  
 Power Method for Singular Value Decomposition . . . . . . . . . . . . . . .
  
 51
  
 3.7.1
  
 A Faster Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 51
  
 3.8
  
 Singular Vectors and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . .
  
 54
  
 3.9
  
 Applications of Singular Value Decomposition . . . . . . . . . . . . . . . .
  
 54
  
 3.9.1
  
 Centering Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 54
  
 3.9.2
  
 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . .
  
 56
  
 3.9.3
  
 Clustering a Mixture of Spherical Gaussians . . . . . . . . . . . . .
  
 56
  
 3.9.4
  
 Ranking Documents and Web Pages
  
 . . . . . . . . . . . . . . . . .
  
 62
  
 3.9.5
  
 An Application of SVD to a Discrete Optimization Problem
  
 . . . .
  
 63
  
 3.10 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 65
  
 3.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 67
  
 Random Walks and Markov Chains
  
 76
  
 4.1
  
 Stationary Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 80
  
 4.2
  
 Markov Chain Monte Carlo
  
 . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 81
  
 4.2.1
  
 Metropolis-Hasting Algorithm . . . . . . . . . . . . . . . . . . . . .
  
 83
  
 4.2.2
  
 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 84
  
 4.3
  
 Areas and Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 86
  
 2",NA
1 Introduction,"Computer science as an academic discipline began in the 1960’s. Emphasis was on 
 programming languages, compilers, operating systems, and the mathematical theory 
 that supported these areas. Courses in theoretical computer science covered finite 
 automata, regular expressions, context-free languages, and computability. In the 1970’s, 
 the study of algorithms was added as an important component of theory. The emphasis 
 was on making computers useful. Today, a fundamental change is taking place and the 
 focus is more on a wealth of applications. There are many reasons for this change. The 
 merging of computing and communications has played an important role. The enhanced 
 ability to observe, collect, and store data in the natural sciences, in commerce, and in 
 other fields calls for a change in our understanding of data and how to handle it in the 
 modern setting. The emergence of the web and social networks as central aspects of 
 daily life presents both opportunities and challenges for theory.
  
 While traditional areas of computer science remain highly important, increasingly 
 re-searchers of the future will be involved with using computers to understand and 
 extract usable information from massive data arising in applications, not just how to 
 make com-puters useful on specific well-defined problems. With this in mind we have 
 written this book to cover the theory we expect to be useful in the next 40 years, just as 
 an under-standing of automata theory, algorithms, and related topics gave students an 
 advantage in the last 40 years. One of the major changes is an increase in emphasis on 
 probability, statistics, and numerical methods.
  
 Early drafts of the book have been used for both undergraduate and graduate 
 courses. Background material needed for an undergraduate course has been put in the 
 appendix. For this reason, the appendix has homework problems.
  
 Modern data in diverse fields such as information processing, search, and machine 
 learning is often advantageously represented as vectors with a large number of compo-
 nents. The vector representation is not just a book-keeping device to store many fields of 
 a record. Indeed, the two salient aspects of vectors: geometric (length, dot products, 
 orthogonality etc.) and linear algebraic (independence, rank, singular values etc.) turn 
 out to be relevant and useful. Chapters 2 and 3 lay the foundations of geometry and 
 linear algebra respectively. More specifically, our intuition from two or three 
 dimensional space can be surprisingly off the mark when it comes to high dimensions. 
 Chapter 2 works out the fundamentals needed to understand the differences. The 
 emphasis of the chapter, as well as the book in general, is to get across the intellectual 
 ideas and the mathematical foundations rather than focus on particular applications, 
 some of which are briefly described. Chapter 3 focuses on singular value decomposition 
 (SVD) a central tool to deal with matrix data. We give a from-first-principles description 
 of the mathematics and algorithms for SVD. Applications of singular value 
 decomposition include principal component analysis, a widely used technique which we 
 touch upon, as well as modern
  
 9",NA
2 High-Dimensional Space,NA,NA
2.1 ,NA,NA
Introduction,"High dimensional data has become very important. However, high dimensional space 
 is very different from the two and three dimensional spaces we are familiar with. 
 Generate 
 n
  points at random in
  d
 -dimensions where each coordinate is a zero mean, unit 
 variance Gaussian. For sufficiently large
  d,
  with high probability the distances between 
 all pairs of points will be essentially the same. Also the volume of the unit ball in
  d
 -
 dimensions, the set of all points
  x
  such that
  |
 x
 | ≤
  1
 ,
  goes to zero as the dimension goes to 
 infinity. The volume of a high dimensional unit ball is concentrated near its surface and 
 is also concentrated at its equator. These properties have important consequences 
 which we will consider.",NA
2.2 ,NA,NA
The Law of Large Numbers,"If one generates random points in
  d
 -dimensional space using a Gaussian to generate 
 coordinates, the distance between all pairs of points will be essentially the same when
  d 
 is large. The reason is that the square of the distance between two points
  y
  and
  z
 ,
  
 d
  
 |
 y
  −
  z
 |
 2
 =
  
 i
 =1
  
 (
 y
 i
  − z
 i
 )
 2
 ,
  
 can be viewed as the sum of
  d
  independent samples of a random variable
  x
  that is dis-
 tributed as the squared difference of two Gaussians. In particular, we are summing inde-
 pendent samples
  x
 i
  = (
 y
 i
  − z
 i
 )
 2
 of a random variable
  x
  of bounded variance. In such a case, 
 a general bound known as the Law of Large Numbers states that with high proba-bility, 
 the average of the samples will be close to the expectation of the random variable. This 
 in turn implies that with high probability, the sum is close to the sum’s expectation.
  
 Specifically, the Law of Large Numbers states that
  
 Prob
  
 x
 1
  +
  x
 2
  +
  · · ·
  +
  x
 n 
 n
  
 − E
 (
 x
 )
  
  ≥ ϵ
  
 ≤V ar
 (
 x
 )
  
 .
  
 (2.1)
  
 The larger the variance of the random variable, the greater the probability that the error 
 will exceed
  ϵ.
  Thus the variance of
  x
  is in the numerator. The number of samples
  n
  is in 
 the denominator since the more values that are averaged, the smaller the probability 
 that the difference will exceed
  ϵ.
  Similarly the larger
  ϵ
  is, the smaller the probability that 
 the difference will exceed
  ϵ
  and hence
  ϵ
  is in the denominator. Notice that squaring
  ϵ
  
 makes the fraction a dimensionless quantity.
  
 We use two inequalities to prove the Law of Large Numbers. The first is Markov’s 
 inequality that states that the probability that a nonnegative random variable exceeds
  a 
 is bounded by the expected value of the variable divided by
  a
 .",NA
2.3 ,NA,NA
The Geometry of High Dimensions,"An important property of high-dimensional objects is that most of their volume is 
 near the surface. Consider any object
  A
  in
  R
 d
 . Now shrink
  A
  by a small amount
  ϵ
  to 
 produce a new object (1
  − ϵ
 )
 A
  =
  {
 (1
  − ϵ
 )
 x|x ∗ A}
 . Then the following equality holds:
  
 volume
  
 (1
  − ϵ
 )
 A
  
 = (1
  − ϵ
 )
 d
 volume(
 A
 )
 .",NA
2.4 ,NA,NA
Properties of the Unit Ball,"We now focus more specifically on properties of the unit ball in
  d
 -dimensional space. 
 We just saw that most of its volume is concentrated in a small annulus of width
  O
 (1
 /d
 ) 
 near the boundary. Next we will show that in the limit as
  d
  goes to infinity, the volume of 
 the ball goes to zero. This result can be proven in several ways. Here we use integration.
  
 2.4.1 Volume of the Unit Ball
  
 To calculate the volume
  V
  (
 d
 ) of the unit ball in
  R
 d
 , one can integrate in either Cartesian 
 or polar coordinates. In Cartesian coordinates the volume is given by
  
 V
  (
 d
 ) =
  
 x
 1
 =1
  
 x
 2
 =
 √
 1
 −x
 2
  
 x
 d
 =
 √
 1
 −x
 2 1
 −···−x
 2 
 d−
 1
  
 dx
 d
  · · · dx
 2
 dx
 1
 .
  
 x
 1
 =
 −
 1
  
 x
 2
 =
 −
 √
 1
 −x
 2
  
 · · ·
  
 x
 d
 =
 −
 √
 1
 −x
 2 1
 −···−x
 2 
 d−
 1
  
 Since the limits of the integrals are complicated, it is easier to integrate using polar 
 coordinates. In polar coordinates,
  V
  (
 d
 ) is given by
  
 1
  
 V
  (
 d
 ) =
  
 S
 d
  
 r
 =0
  
 r
 d−
 1
 drd
 Ω
 .
  
 Since the variables Ω and
  r
  do not interact,
  
 V
  (
 d
 ) =
  
 S
 d
  
 1
  
 r
 d−
 1
 dr
  = 1 
 d
  
 S
 d
  
 d
 Ω =
 A
 (
 d
 ) 
  
 d
  
 d
 Ω
  
 r
 =0
  
 where
  A
 (
 d
 ) is the surface area of the
  d
 -dimensional unit ball. For instance, for
  d
  = 3 the 
 surface area is 4
 π
  and the volume is
 4 3
 π
 . The question remains, how to determine the
  
 17",NA
2.5,NA,NA
Generating Points Uniformly at Random from a Ball,"Consider generating points uniformly at random on the surface of the unit ball. For 
 the 2-dimensional version of generating points on the circumference of a unit-radius cir-
 cle, independently generate each coordinate uniformly at random from the interval [
 −
 1
 ,
  
 1]. This produces points distributed over a square that is large enough to completely 
 contain the unit circle. Project each point onto the unit circle. The distribution is not 
 uniform since more points fall on a line from the origin to a vertex of the square than fall 
 on a line from the origin to the midpoint of an edge of the square due to the difference in 
 length. To solve this problem, discard all points outside the unit circle and project the 
 remaining points onto the circle.
  
 In higher dimensions, this method does not work since the fraction of points that fall 
 inside the ball drops to zero and all of the points would be thrown away. The solution is 
 to generate a point each of whose coordinates is an independent Gaussian variable. 
 Generate
  
 x
 1
 , x
 2
 , . . . , x
 d
 , using a zero mean, unit variance Gaussian, namely,
  
 1
  
 √
 2
 π
 exp(
 −x
 2
 /
 2) on the",NA
2.6 ,NA,NA
Gaussians in High Dimension,"A 1-dimensional Gaussian has its mass close to the origin. However, as the dimension 
 is increased something different happens. The
  d
 -dimensional spherical Gaussian with 
 zero mean and variance
  σ
 2
 in each coordinate has density function
  
 p
 (
 x
 ) =
  
  
 1 
  
 (2
 π
 )
 d/
 2
 σ
 d
  exp
  
 −
 |
 x
 |
 2
  
 .
  
 The value of the density is maximum at the origin, but there is very little volume there. 
 When
  σ
 2
 = 1, integrating the probability density over a unit ball centered at the origin 
 yields almost zero mass since the volume of such a ball is negligible. In fact, one needs
  
 2
 One might naturally ask: “how do you generate a random number from a 1-dimensional Gaussian?”To 
 generate a number from any distribution given its cumulative distribution function
  P,
  first select a
  
 uniform random number
  u ∗
  [0
 ,
  1] and then choose
  x
  =
  P
 −
 1
 (
 u
 )
 .
  For any
  a < b
 , the probability that
  x
  is 
 between
  a
  and
  b
  is equal to the probability that
  u
  is between
  P
 (
 a
 ) and
  P
 (
 b
 ) which equals
  P
 (
 b
 )
  − P
 (
 a
 ) as 
 desired. For the 2-dimensional Gaussian, one can generate a point in polar coordinates by choosing
  
  
 angle
  θ
  uniform in [0
 ,
  2
 π
 ] and radius
  r
  = the 
 Box-Muller transform.
  
 −
 2 ln(
 u
 ) where
  u
  is uniform random in [0
 ,
  1]. This is called
  
 23",NA
2.7 ,NA,NA
Random Projection and Johnson-Lindenstrauss Lemma,"One of the most frequently used subroutines in tasks involving high dimensional data 
 is nearest neighbor search. In nearest neighbor search we are given a database of
  n
  
 points in
  R
 d
 where
  n
  and
  d
  are usually large. The database can be preprocessed and 
 stored in an efficient data structure. Thereafter, we are presented “query” points in
  
 R
 d
 and are asked to find the nearest or approximately nearest database point to the 
 query point. Since the number of queries is often large, the time to answer each query 
 should be very small, ideally a small function of log
  n
  and log
  d
 , whereas preprocessing 
 time could be larger, namely a polynomial function of
  n
  and
  d
 . For this and other 
 problems, dimension reduction, where one projects the database points to a
  k
 -
 dimensional space with
  k ∗ d 
 (usually dependent on log
  d
 ) can be very useful so long as 
 the relative distances between points are approximately preserved. We will see using 
 the Gaussian Annulus Theorem that such a projection indeed exists and is simple.
  
 The projection
  f
  :
  R
 d
 →
  R
 k
 that we will examine (many related projections are known 
 to work as well) is the following. Pick
  k
  Gaussian vectors
  u
 1
 ,
  u
 2
 , . . . ,
  u
 k
  in
  R
 d 
 with unit-
 variance coordinates. For any vector
  v
 , define the projection
  f
 (
 v
 ) by:
  
 f
 (
 v
 ) = (
 u
 1
  ·
  v
 ,
  u
 2
  ·
  v
 , . . . ,
  u
 k
  ·
  v
 )
 .
  
 The projection
  f
 (
 v
 ) is the vector of dot products of
  v
  with the
  u
 i
 .
  We will show that with 
 high probability,
  |f
 (
 v
 )
 | ≈
  
 √
    
  
 k|
 v
 |.
  For any two vectors
  v
 1
  and
  v
 2
 ,
  f
 (
 v
 1
  
 −
  v
 2
 ) = 
 f
 (
 v
 1
 )
  − f
 (
 v
 2
 )
 .
  Thus, to estimate the distance
  |
 v
 1
  −
  v
 2
 |
  between two vectors
  v
 1
  and
  v
 2
  
 in
  
 √k
  is known and one can divide by it. The reason distances increase when 
 R
 d
 , it suffices to compute
  |f
 (
 v
 1
 )
  − f
 (
 v
 2
 )
 |
  =
  |f
 (
 v
 1
  −
  v
 2
 )
 |
  in the
  k
 -dimensional space since the 
 factor of we project to a lower dimensional space is that the vectors
  u
 i
  are not unit length. 
 Also notice that the vectors
  u
 i
  are not orthogonal. If we had required them to be 
 orthogonal, we would have lost statistical independence.
  
 Theorem 2.10 (The Random Projection Theorem)
  Let
  v
  be a fixed vector in
  R
 d
  
 and let f be defined as above. There exists constant c >
  0
  such that for ε ∗
  (0
 ,
  1)
 ,
  
 Prob 
  
 |f
 (
 v
 )
 | −
  
  
 √
  
  
  
 k|
 v
 | ≥ ε
  
  
  
  
  
 √
  
   
  
   
 k|
 v
 |
  
 ≤
  3
 e
 −ckε
 2
 ,
  
 where the probability is taken over the random draws of vectors
  u
 i
  used to construct f.",NA
2.8 ,NA,NA
Separating Gaussians,"Mixtures of Gaussians are often used to model heterogeneous data coming from multiple 
 sources. For example, suppose we are recording the heights of individuals age 20-30 in a 
 city. We know that on average, men tend to be taller than women, so a natural model 
 would be a Gaussian mixture model
  p
 (
 x
 ) =
  w
 1
 p
 1
 (
 x
 ) +
  w
 2
 p
 2
 (
 x
 ), where
  p
 1
 (
 x
 ) is a Gaussian 
 density representing the typical heights of women,
  p
 2
 (
 x
 ) is a Gaussian density represent-
 ing the typical heights of men, and
  w
 1
  and
  w
 2
  are the
  mixture weights
  representing the 
 proportion of women and men in the city. The
  parameter estimation problem
  for a 
 mixture model is the problem: given access to samples from the overall density
  p
  (e.g., 
 heights of people in the city, but without being told whether the person with that height 
 is male or female), reconstruct the parameters for the distribution (e.g., good 
 approximations to the means and variances of
  p
 1
  and
  p
 2
 , as well as the mixture weights).
  
 There are taller women and shorter men, so even if one solved the parameter estima-
 tion problem for heights perfectly, given a data point, one couldn’t necessarily tell which 
 population it came from. That is, given a height, one couldn’t necessarily tell if it came 
 from a man or a woman. In this section, we will look at a problem that is in some ways 
 easier and some ways harder than this problem of heights. It will be harder in that we 
 will be interested in a mixture of two Gaussians in high-dimensions as opposed to the 
 d
  
 = 1 case of heights. But it will be easier in that we will assume the means are quite well-
 separated compared to the variances. Specifically, our focus will be on a mixture of two 
 spherical unit-variance Gaussians whose means are separated by a distance Ω(
 d
 1
 /
 4
 ). We 
 will show that at this level of separation, we can with high probability uniquely de-
 termine which Gaussian each data point came from. The algorithm to do so will actually 
 be quite simple. Calculate the distance between all pairs of points. Points whose distance 
 apart is smaller are from the same Gaussian, whereas points whose distance is larger are 
 from different Gaussians. Later, we will see that with more sophisticated algorithms, 
 even a separation of Ω(1) suffices.
  
 First, consider just one spherical unit-variance Gaussian centered at the origin. From 
 Theorem 2.9, most of its probability mass lies on an annulus of width
  O
 (1) at radius
 √d
 . 
 Also
  e
 −|
 x
 |
 2
 /
 2
 = 
 i
 e
 −x
 2 
 i
 /
 2
 and almost all of the mass is within the slab
  {
  x
  | −c ≤ x
 1
  ≤ c }, 
 for
  c ∗ 
 O
 (1). Pick a point
  x
  from this Gaussian. After picking
  x
 , rotate the coordinate system to 
 make the first axis align with
  x
 . Independently pick a second point
  y
  from
  
 27",NA
2.9 ,NA,NA
Fitting a Spherical Gaussian to Data,"Given a set of sample points,
  x
 1
 ,
  x
 2
 , . . . ,
  x
 n
 , in a
  d
 -dimensional space, we wish to find 
 the spherical Gaussian that best fits the points. Let
  f
  be the unknown Gaussian with 
 mean
  µ
  and variance
  σ
 2
 in each direction. The probability density for picking these points 
 when sampling according to
  f
  is given by
  
 c
  exp
  
 −
 (
 x
 1
  −
  µ
 )
 2
  + (
 x
 2
  −
  µ
 )
 2
  +
  · · ·
  + (
 x
 n
  −
  µ
 )
 2
  
 where the normalizing constant c is the reciprocal of
  
 e
 −
  |
 x
 −
 µ
 |
 2 2
 σ
 2
 dx
  
 n
  
 .
  In integrating from
  
 −∞
  to
  ∞,
  one can shift the origin to
  µ
  and thus
  c
  is
  
 e
 −
  |
 x
 |
 2 
 2
 σ
 2
 dx
  
 −n
  
 =
  
 1
  
 ndi
  
  
 =
  
 (2
 π
 )
  
 n 
 2
 ndi
  
 independent of
  µ
 .
  
 The
  Maximum Likelihood Estimator
  (MLE) of
  f,
  given the samples
  x
 1
 ,
  x
 2
 , . . . ,
  x
 n
 ,
  is the
  f
  
 that maximizes the above probability density.
  
 Lemma 2.12
  Let {
 x
 1
 ,
  x
 2
 , . . . ,
  x
 n
 } be a set of n d-dimensional points. Then
  (
 x
 1
  −
  µ
 )
 2
 + (
 x
 2
  −
  
 µ
 )
 2
 +
 · · ·
 +(
 x
 n
  −
  µ
 )
 2
 is minimized when
  µ
  is the centroid of the points
  x
 1
 ,
  x
 2
 , . . . ,
  x
 n
 , namely
  µ
  
 =
 1 
 n
 (
 x
 1
  +
  x
 2
  +
  · · ·
  +
  x
 n
 )
 .
  
 Proof:
  Setting the gradient of (
 x
 1
  −
  µ
 )
 2
 + (
 x
 2
  −
  µ
 )
 2
 +
  · · ·
  + (
 x
 n
  −
  µ
 )
 2
 with respect to
  µ
 to zero 
 yields
  
 −
 2 (
 x
 1
  −
  µ
 )
  −
  2 (
 x
 2
  −
  µ
 )
  − · · · −
  2 (
 x
 n
  −
  µ
 ) = 0
 .
  
 Solving for
  µ
  gives
  µ
  =
 1 
 n
 (
 x
 1
  +
  x
 2
  +
  · · ·
  +
  x
 n
 ).
  
 3
 poly(n) means bounded by a polynomial in
  n
 .",NA
2.10 Bibliographic Notes,"The word vector model was introduced by Salton [SWY75]. There is vast literature 
 on the Gaussian distribution, its properties, drawing samples according to it, etc. The 
 reader can choose the level and depth according to his/her background. The Master Tail 
 Bounds theorem and the derivation of Chernoff and other inequalities from it are from 
 [Kan09]. The original proof of the Random Projection Theorem by Johnson and 
 Lindenstrauss was complicated. Several authors used Gaussians to simplify the proof. 
 The proof here is due to Dasgupta and Gupta [DG99]. See [Vem04] for details and 
 applications of the theorem. [MU05] and [MR95b] are text books covering much of the 
 material touched upon here.
  
 31",NA
2.11 Exercises,"Exercise 2.1
  
 1. Let x and y be independent random variables with uniform distribution in
  [0
 ,
  1]
 .
  
 What is the expected value E
 (
 x
 )
 , E
 (
 x
 2
 )
 , E
 (
 x − y
 )
 , E
 (
 xy
 )
 , and E
 (
 x − y
 )
 2
 ?
  
 2. Let x and y be independent random variables with uniform distribution in
  [
 −
 1 2
 ,
  1 2
 ]
 .
  
 What is the expected value E
 (
 x
 )
 , E
 (
 x
 2
 )
 , E
 (
 x − y
 )
 , E
 (
 xy
 )
 , and E
 (
 x − y
 )
 2
 ?
  
 3. What is the expected squared distance between two points generated at random inside
  
 a unit d-dimensional cube?
  
 Exercise 2.2
  Randomly generate 30 points inside the cube
  [
 −
 1 
 between points and the angle 
 between the vectors from the origin to the points for all pairs 
 2
 ,
  1 2
 ]
 100
  and plot distance
  
 of points.
  
 Exercise 2.3
  Show that for any a ≥
  1
  there exist distributions for which Markov’s in-equality is 
 tight by showing the following:
  
 1. For each a
  = 2
 ,
  3
 , and 4 give a probability distribution p
 (
 x
 )
  for a nonnegative random
  
 variable x where Prob 
  
 x ≥ a 
  
 =
 E
 (
 x
 ) 
 a
 .
  
 2. For arbitrary a ≥
  1
  give a probability distribution for a nonnegative random variable x 
 where Prob x ≥ a 
 =
 E
 (
 x
 ) 
 a
 .
  
 Exercise 2.4
  Show that for any c ≥
  1
  there exist distributions for which Chebyshev’s 
 inequality is tight, in other words, Prob
 (
 |x − E
 (
 x
 )
 | ≥ c
 ) =
  V ar
 (
 x
 )
 /c
 2
 .
  
 Exercise 2.5
  Let x be a random variable with probability density
 1
  
 zero elsewhere. 
 4
 for
  0
  ≤ x ≤
  4
  and
  
 1. Use Markov’s inequality to bound the probability that x ≥
  3
 .
  
 2. Make use of Prob
 (
 |x| ≥ a
 ) =
  Prob
 (
 x
 2
 ≥ a
 2
 )
  to get a tighter bound.
  
 3. What is the bound using Prob
 (
 |x| ≥ a
 ) =
  Prob
 (
 x
 r
 ≥ a
 r
 )?
  
 Exercise 2.6
  Consider the probability distribution p
 (
 x
  = 0) = 1
  −
 1 
 Plot the probability that x is 
 greater than or equal to a as a function of a for the bound 
 a
 and p
 (
 x
  =
  a
 ) =
  1 
 a
 .
  
 given by Markov’s inequality and by Markov’s inequality applied to x
 2
 and x
 4
 .
  
 Exercise 2.7
  Consider the probability density function p
 (
 x
 ) = 0
  for x <
  1
  and p
 (
 x
 ) =
  c
 1 
 x
 4
  
 for x ≥
  1
 .
  
 1. What should c be to make p a legal probability density function?",NA
3 Best-Fit Subspaces and Singular Value Decompo-,NA,NA
sition (SVD),NA,NA
3.1 ,NA,NA
Introduction,"In this chapter, we examine the
  Singular Value Decomposition
  (SVD) of a matrix. 
 Consider each row of an
  n × d
  matrix
  A
  as a point in
  d
 -dimensional space. The singular 
 value decomposition finds the best-fitting
  k
 -dimensional subspace for
  k
  = 1
 ,
  2
 ,
  3
 , . . . ,
  for 
 the set of
  n
  data points. Here, “best” means minimizing the sum of the squares of the 
 perpendicular distances of the points to the subspace, or equivalently, maximizing the 
 sum of squares of the lengths of the projections of the points onto this subspace.
 4
 We 
 begin with a special case where the subspace is 1-dimensional, namely a line through 
 the origin. We then show that the best-fitting
  k
 -dimensional subspace can be found by
  k 
 applications of the best fitting line algorithm, where on the
  i
 th
 iteration we find the best 
 fit line perpendicular to the previous
  i −
  1 lines. When
  k
  reaches the rank of the matrix, 
 from these operations we get an exact decomposition of the matrix called the
  singular 
 value decomposition
 .
  
 In matrix notation, the singular value decomposition of a matrix
  A
  with real entries 
 (we assume all our matrices have real entries) is the factorization of
  A
  into the product 
 of three matrices,
  A
  =
  UDV
 T
 ,
  where the columns of
  U
  and
  V
  are orthonormal
 5
 and the 
 matrix
  D
  is diagonal with positive real entries. The columns of
  V
  are the unit length vec-
 tors defining the best fitting lines described above (the
  i
 th
 column being the unit-length 
 vector in the direction of the
  i
 th
 line). The coordinates of a row of
  U
  will be the fractions 
 of the corresponding row of
  A
  along the direction of each of the lines.
  
 The SVD is useful in many tasks. Often a data matrix
  A
  is close to a low rank ma-trix 
 and it is useful to find a good low rank approximation to
  A
 . For any
  k,
  the singular value 
 decomposition of
  A
  gives the best rank-
 k
  approximation to
  A
  in a well-defined sense.
  
 If
  u
 i
  and
  v
 i
  are columns of
  U
  and
  V
  respectively, then the matrix equation
  A
  =
  UDV
 T 
 can be 
 rewritten as
  
 A
  =
  
 d
 ii
 u
 i
 v
 i
  
 T
 .
  
 i
  
 Since
  u
 i
  is a
  n ×
  1 matrix and
  v
 i
  is a
  d ×
  1 matrix,
  u
 i
 v
 i
 T
 is an
  n × d
  matrix with the same 
 dimensions as
  A
 . The
  i
 th
 term in the above sum can be viewed as giving the compo-nents 
 of the rows of
  A
  along direction
  v
 i
 .
  When the terms are summed, they reconstruct
  A
 .
  
 4
 This equivalence is due to the Pythagorean Theorem. For each point, its squared length (its distance to 
 the origin squared) is exactly equal to the squared length of its projection onto the subspace plus the 
 squared distance of the point to its projection; therefore, maximizing the sum of the former is equivalent 
 to minimizing the sum of the latter. For further discussion see Section 3.2.",NA
3.2 ,NA,NA
Preliminaries,"Consider projecting a point
  a
 i
  = (
 a
 i
 1
 , a
 i
 2
 , . . . , a
 id
 ) onto a line through the origin. Then
  
 a
 2 
 i
 1
 +
  a
 2 
 i
 2
 +
  · · ·
  +
  a
 2 
 id
 = (length of projection)
 2
  + (distance of point to line)
 2
  .
  
  
 6
 When
  d
  = 1 there are actually two possible singular vectors, one the negative of the other. The 
 subspace spanned is unique.",NA
3.3 ,NA,NA
Singular Vectors,"n
  points in a
  d
 -dimensional space. Consider the best fit line through the origin. Let
  v 
 We now 
 define the
  singular vectors
  of an
  n × d
  matrix
  A
 . Consider the rows of
  A
  as
  
 be a unit vector along this line. The length of the projection of
  a
 i
 ,
  the
  i
 th
 row of
  A
 , onto 
 v
  is
  
 |
 a
 i
  ·
  v
 |.
  From this we see that the sum of the squared lengths of the projections is 
 7
 But 
 there is a difference: here we take the perpendicular distance to the line or subspace, whereas, in the",NA
3.4 ,NA,NA
Singular Value Decomposition (SVD),"Let
  A
  be an
  n × d
  matrix with singular vectors
  v
 1
 ,
  v
 2
 , . . . ,
  v
 r
  and corresponding",NA
3.5 ,NA,NA
Best Rank-,NA,NA
k,NA,NA
 Approximations,"space. Let Let
  A
  be an
  n × d
  matrix and think of the rows of
  A
  as
  n
  points in
  d
 -dimensional
  
 r
  
 A
  =
  
 σ
 i
 u
 i
 v
 T 
 i
  
 i
 =1
  
 be the SVD of
  A
 . For
  k ∗ {
 1
 ,
  2
 , . . . , r}
 , let
  
 k
  
 A
 k
  =
  
 σ
 i
 u
 i
 v
 T 
 i
  
 i
 =1
  
 be the sum truncated after
  k
  terms. It is clear that
  A
 k
  has rank
  k
 . We show that
  A
 k 
 is the 
 best rank
  k
  approximation to
  A
 , where error is measured in the Frobenius norm.
  
 Geometrically, this says that
  v
 1
 , . . . ,
  v
 k
  define the
  k
 -dimensional space minimizing the 
 sum of squared distances of the points to the space. To see why, we need the following
  
 lemma.
  
 Lemma 3.5
  The rows of A
 k
  are the projections of the rows of A onto the subspace V
 k 
 spanned by the first k singular vectors of A.
  
 Proof:
  Let
  a
  be an arbitrary row vector. Since the
  v
 i
  are orthonormal, the projection
  
 of the vector
  a
  onto
  V
 k
  is given by
 k i
 =1
 (
 a
  ·
  v
 i
 )
 v
 i
 T
 .
  Thus, the matrix whose rows are
  
 the projections of the rows of
  A
  onto
  V
 k
  is given by
 k i
 =1
 A
 v
 i
 v
 T 
 i
 .
  This last expression
  
 simplifies to
  
 k
  
 A
 v
 i
 v
 i
  
 T
  =
  
 k
  
 σ
 i
 u
 i
 v
 i
  
 T
  =
  A
 k
 .
  
 i
 =1
  
 i
 =1
  
 47",NA
3.6 ,NA,NA
Left Singular Vectors,NA,NA
3.7 ,NA,NA
Power Method for Singular Value Decomposition,"Computing the singular value decomposition is an important branch of numerical 
 analysis in which there have been many sophisticated developments over a long period 
 of time. The reader is referred to numerical analysis texts for more details. Here we 
 present an “in-principle” method to establish that the approximate SVD of a matrix
  A
  can 
 be computed in polynomial time. The method we present, called the
  power method
 , is 
 simple and is in fact the conceptual starting point for many algorithms. Let
  A
  be a matrix 
 whose SVD is 
  
 i
 σ
 i
 u
 i
 v
 i
 T
 .
  We wish to work with a matrix that is square and symmetric. Let 
 B
  
 =
  A
 T
 A
 . By direct multiplication, using the orthogonality of the
  u
 i
 ’s that was proved in 
 Theorem 3.7,
  
 B
  =
  A
 T
 A
  =
  
 σ
 i
 v
 i
 u
 T 
 i
  
 j
  
 σ
 j
 u
 j
 v
 T 
 j
  
 i
  
 =
  
 i,j
  
 σ
 i
 σ
 j
 v
 i
 (
 u
 T i
 ·
  u
 j
 )
 v
 T 
 j
 =
  
 i
  
 σ
 2 
 i
 v
 i
 v
 T 
 i
 .
  
 The matrix
  B
  is square and symmetric, and has the same left and right-singular vectors. 
 In particular,
  B
 v
 j
  = ( 
  
 i
 σ
 2 
 i
 v
 i
 v
 T 
 i
 )
 v
 j
  =
  σ
 2 
 j
 v
 j
 , so
  v
 j
 is an eigenvector of
  B
  with eigenvalue
 σ
 2 
 j
 . If
  A
  
 is itself square and symmetric, it will have the same right and left-singular vec-tors, 
 namely
  A
  =
  
 σ
 i
 v
 i
 v
 i
 T
 and computing
  B
  is unnecessary.
  
 i
  
 Now consider computing
  B
 2
 .
  
 B
 2
 =
  
 σ
 2 
 i
 v
 i
 v
 T
  
 σ
 2 
 j
 v
 j
 v
 T
  
 =
  
 σ
 2 
 i
 σ
 2 
 j
 v
 i
 (
 v
 i
  
 T
 v
 j
 )
 v
 j
  
 T
  
 r
  
 σ
 4 
 i
 v
 i
 v
 i
 T
 .
  In
  
 i
  
 j
  
 ij
  
 When
  i ̸
 =
  j
 , the dot product
  v
 i
 T
 v
 j
  is zero by orthogonality.
 9
 Thus,
  B
 2
 =
  
 i
 =1
  
 computing the
  k
 th
 power of
  B
 , all the cross product terms are zero and
  
 r
  
 B
 k
 =
  
 σ
 2
 k 
 i
 v
 i
 v
 i
  
 T
 .
  
 i
 =1
  
 If
  σ
 1
  > σ
 2
 , then the first term in the summation dominates, so
  B
 k
 → σ
 2
 k 
 1
 v
 1
 v
 1
 T
 . This means 
 a close estimate to
  v
 1
  can be computed by simply taking the first column of
  B
 k 
 and 
 normalizing it to a unit vector.
  
 3.7.1 A Faster Method
  
 A problem with the above method is that
  A
  may be a very large, sparse matrix, say a 10
 8
 ×
  
 10
 8
 matrix with 10
 9
 nonzero entries. Sparse matrices are often represented by just 
 9
 The 
 “outer product”
  v
 i
 v
 j
 T
  is a matrix and is not zero even for
  i ̸
 =
  j
 .
  
 51",NA
3.8,NA,NA
Singular Vectors and Eigenvectors,"For a square matrix
  B
 , if
  B
 x
  =
  λ
 x
 , then
  x
  is an
  eigenvector
  of
  B
  and
  λ
  is the corre-
 sponding
  eigenvalue
 . We saw in Section 3.7, if
  B
  =
  A
 T
 A
 , then the right singular vectors 
 v
 j
  
 of
  A
  are eigenvectors of
  B
  with eigenvalues
  σ
 2 
 j
 . The same argument shows that the left 
 singular vectors
  u
 j
  of
  A
  are eigenvectors of
  AA
 T
 with eigenvalues
  σ
 2 
 j
 .
  
 because
  B
  = The matrix
  B
  =
  A
 T
 A
  has the property that for any vector
  x
 ,
  x
 T
 B
 x
  ≥
  0. This is 
 i
 σ
 2 
 i
 v
 i
 v
 i
 T
  and for any
  x
 ,
  x
 T
 v
 i
 v
 i
 T
 x
  = (
 x
 T
 v
 i
 )
 2
  ≥
  0. A matrix
  B
  with the property that
  x
 T
 B
 x
  ≥
  0 
 for all
  x
  is called
  positive semi-definite
 . Every matrix of the form
  A
 T
 A
  is positive semi-
 definite. In the other direction, any positive semi-definite matrix
  B
  can be decomposed 
 into a product
  A
 T
 A
 , and so its eigenvalue decomposition can be obtained from the 
 singular value decomposition of
  A
 . The interested reader should consult a linear algebra 
 book.",NA
3.9 ,NA,NA
Applications of Singular Value Decomposition,"3.9.1 Centering Data
  
 Singular value decomposition is used in many applications and for some of these ap-
 plications it is essential to first center the data by subtracting the centroid of the data 
 from each data point.
 11
 If you are interested in the statistics of the data and how it varies 
 in relationship to its mean, then you would center the data. On the other hand, if you are 
 interested in finding the best low rank approximation to a matrix, then you do not center 
 the data. The issue is whether you are finding the best fitting subspace or the best fitting 
 affine space. In the latter case you first center the data and then find the best fitting 
 subspace. See Figure 3.3.
  
 We first show that the line minimizing the sum of squared distances to a set of 
 points, if not restricted to go through the origin, must pass through the centroid of the 
 points. This implies that if the centroid is subtracted from each data point, such a line 
 will pass through the origin. The best fit line can be generalized to
  k
  dimensional 
 “planes”. The operation of subtracting the centroid from all data points is useful in other 
 contexts as well. We give it the name “centering data”.
  
 11
 The centroid of a set of points is the coordinate-wise average of the points.",NA
3.10 Bibliographic Notes,"Singular value decomposition is fundamental to numerical analysis and linear 
 algebra. There are many texts on these subjects and the interested reader may want to 
 study these. A good reference is [GvL96]. The material on clustering a mixture of 
 Gaussians in Section 3.9.3 is from [VW02]. Modeling data with a mixture of Gaussians is 
 a stan-dard tool in statistics. Several well-known heuristics like the expectation-
 minimization",NA
3.11 Exercises,"Exercise 3.1 (Least squares vertical error)
  In many experiments one collects the value 
 of a parameter at various instances of time. Let y
 i
  be the value of the parameter y at time x
 i
 . 
 Suppose we wish to construct the best linear approximation to the data in the sense that 
 we wish to minimize the mean square error. Here error is measured vertically rather than 
 perpendicular to the line. Develop formulas for m and b to minimize the mean square error 
 of the points {
 (
 x
 i
 , y
 i
 )
  |
 1
  ≤ i ≤ n} to the line y
  =
  mx
  +
  b.
  
 Exercise 3.2
  Given five observed variables, height, weight, age, income, and blood pres-sure 
 of n people, how would one find the best least squares fit affine subspace of the form
  
 a
 1
  (
 height
 ) +
  a
 2
  (
 weight
 ) +
  a
 3
  (
 age
 ) +
  a
 4
  (
 income
 ) +
  a
 5
  (
 blood pressure
 ) =
  a
 6
  
 Here a
 1
 , a
 2
 , . . . , a
 6
  are the unknown parameters. If there is a good best fit 4-dimensional 
 affine subspace, then one can think of the points as lying close to a 4-dimensional sheet 
 rather than points lying in 5-dimensions. Why might it be better to use the perpendicular 
 distance to the affine subspace rather than vertical distance where vertical distance is 
 measured along the coordinate axis corresponding to one of the variables?
  
 Exercise 3.3
  Manually find the best fit lines (not subspaces which must contain the ori-
 gin) through the points in the sets below. Subtract the center of gravity of the points in the 
 set from each of the points in the set and find the best fit line for the resulting points. Does 
 the best fit line for the original data go through the origin?
  
 1. (4,4) (6,2)
  
 2. (4,2) (4,4) (6,2) (6,4)
  
 3. (3,2.5) (3,5) (5,1) (5,3.5)
  
 Exercise 3.4
  Manually determine the best fit line through the origin for each of the 
 following sets of points. Is the best fit line unique? Justify your answer for each of the 
 subproblems.
  
 1. {
 (0
 ,
  1)
  ,
  (1
 ,
  0)
 } 
  
 2. {
 (0
 ,
  1)
  ,
  (2
 ,
  0)
 }
  
 Exercise 3.5
  Manually find the left and right-singular vectors, the singular values, and the 
 SVD decomposition of the matrices in Figure 3.6.
  
 Exercise 3.6
  Consider the matrix
  
 A
  =
  
 1
  
 2
  
  
 −
 1 1
  
 2
  
 −
 2
 −
 2
  
 −
 1
  
 67",NA
4 Random Walks and Markov Chains,"A random walk on a directed graph consists of a sequence of vertices generated from 
 a start vertex by probabilistically selecting an incident edge, traversing the edge to a 
 new vertex, and repeating the process.
  
 We generally assume the graph is
  strongly connected
 , meaning that for any pair of 
 vertices
  x
  and
  y
 , the graph contains a path of directed edges starting at
  x
  and ending at
  y
 . 
 If the graph is strongly connected, then, as we will see, no matter where the walk begins 
 the fraction of time the walk spends at the different vertices of the graph converges to a 
 stationary probability distribution.
  
 Start a random walk at a vertex
  x
  and think of the starting probability distribution as 
 putting a mass of one on
  x
  and zero on every other vertex. More generally, one could 
 start with any probability distribution
  p
 , where
  p
  is a row vector with nonnegative 
 components summing to one, with
  p
 x
  being the probability of starting at vertex
  x
 . The 
 probability of being at vertex
  x
  at time
  t
  + 1 is the sum over each adjacent vertex
  y
  of 
 being at
  y
  at time
  t
  and taking the transition from
  y
  to
  x
 . Let
  p
 (
 t
 ) be a row vector with a 
 component for each vertex specifying the probability mass of the vertex at time
  t
  and let
  
 p
 (
 t
  +
  1
 ) be the row vector of probabilities at time
  t
  + 1. In matrix notation
 14
  
 p
 (
 t
 )
 P
  =
  p
 (
 t
  +
  1
 )
  
 where the
  ij
 th
 entry of the matrix
  P
  is the probability of the walk at vertex
  i
  selecting the 
 edge to vertex
  j.
  
 A fundamental property of a random walk is that in the limit, the long-term average 
 probability of being at a particular vertex is independent of the start vertex, or an initial 
 probability distribution over vertices, provided only that the underlying graph is 
 strongly connected. The limiting probabilities are called the
  stationary probabilities
 . This 
 funda-mental theorem is proved in the next section.
  
 A special case of random walks, namely random walks on undirected graphs, has 
 important connections to electrical networks. Here, each edge has a parameter called 
 conductance
 , like electrical conductance. If the walk is at vertex
  x
 , it chooses an edge to 
 traverse next from among all edges incident to
  x
  with probability proportional to its con-
 ductance. Certain basic quantities associated with random walks are hitting time, which 
 is the expected time to reach vertex
  y
  starting at vertex
  x,
  and cover time, which is the 
 expected time to visit every vertex. Qualitatively, for undirected graphs these quantities 
 are all bounded above by polynomials in the number of vertices. The proofs of these 
 facts will rely on the analogy between random walks and electrical networks.
  
 14
 Probability vectors are represented by row vectors to simplify notation in equations like the one here.
  
 76",NA
4.1 ,NA,NA
Stationary Distribution,"Let
  p
 (
 t
 ) be the probability distribution after
  t
  steps of a random walk. Define the 
 long-
 term average probability distribution
  a
 (
 t
 ) by
  
 a
 (
 t
 ) = 1 
  
 t
  
 p
 (
 0
 ) +
  p
 (
 1
 ) +
  · · ·
  +
  p
 (
 t
  −
  1
 )
  
 .
  
 The fundamental theorem of Markov chains asserts that for a connected Markov chain, 
 a
 (
 t
 ) converges to a limit probability vector
  x
  that satisfies the equations
  x
 P
  =
  x
 .
  Before 
 proving the fundamental theorem of Markov chains, we first prove a technical lemma.
  
 Lemma 4.1
  Let P be the transition probability matrix for a connected Markov chain. The n 
 ×
  (
 n
  + 1)
  matrix A
  = [
 P − I ,
  1
 ]
  obtained by augmenting the matrix P − I with an additional 
 column of ones has rank n.
  
 Proof:
  If the rank of
  A
  = [
 P − I,
  1
 ] was less than
  n
  there would be a subspace of solu-tions 
 to
  A
 x
  =
  0
  of at least two-dimensions. Each row in
  P
  sums to one, so each row in 
 P − I
  sums 
 to zero. Thus
  x
  = (
 1
 ,
  0)
 ,
  where all but the last coordinate of
  x
  is 1, is one solution to
  A
 x
  =
  
 0
 .
  Assume there was a second solution (
 x
 , α
 ) perpendicular to (
 1
 ,
  0)
 . 
 Then (
 P −I
 )
 x
 +
 α
 1
  =
  0
  
 and for each
  i
 ,
  x
 i
  = 
  
 j
 p
 ij
 x
 j
  +
 α.
  Each
  x
 i
  is a convex combination of 
 some
  x
 j
  plus
  α
 . Let
  S
  be the set of
  i
  for which
  x
 i
  attains its maximum value. Since 
 x
  is 
 perpendicular to
  1
 , some
  x
 i
  is negative and thus ¯
 S
  is not empty. Connectedness implies 
 that some
  x
 k
  of maximum value is adjacent to some
  x
 l
  of lower value. Thus,
  
 x
 k
  >
  
 j
 p
 kj
 x
 j
 .
  Therefore
  α
  must be greater than 0 in
  x
 k
  =
  
 j
 p
 kj
 x
 j
  +
  α.
 .
  
 On the other hand, the same argument with
  T
  the set of
  i
  with
  x
 i
  taking its minimum 
 value implies
  α <
  0
 .
  This contradiction falsifies the assumption of a second solution, 
 thereby proving the lemma.
  
 Theorem 4.2 (Fundamental Theorem of Markov Chains)
  For a connected Markov 
 chain there is a unique probability vector
  π
  satisfying
  π
 P
  =
  π
 . Moreover, for any starting 
 distribution,
  lim 
 t→∞
 a
 (
 t
 )
  exists and equals
  π
 .
  
 Proof:
  Note that
  a
 (
 t
 ) is itself a probability vector, since its components are nonnegative 
 and sum to 1. Run one step of the Markov chain starting with distribution
  a
 (
 t
 ); the",NA
4.2 ,NA,NA
Markov Chain Monte Carlo,"The Markov Chain Monte Carlo (MCMC) method is a technique for sampling a mul-
 tivariate probability distribution
  p
 (
 x
 ), where
  x
  = (
 x
 1
 , x
 2
 , . . . , x
 d
 )
 .
  The MCMC method is 
 used to estimate the expected value of a function
  f
 (
 x
 )
  
 E
 (
 f
 ) = 
 f
 (
 x
 )
 p
 (
 x
 )
 .
  
 x
  
 If each
  x
 i
  can take on two or more values, then there are at least 2
 d
 values for
  x
 , so an 
 explicit summation requires exponential time. Instead, one could draw a set of samples, 
 where each sample
  x
  is selected with probability
  p
 (
 x
 )
 .
  Averaging
  f
  over these samples 
 provides an estimate of the sum.
  
 To sample according to
  p
 (
 x
 ), design a Markov Chain whose states correspond to the 
 possible values of
  x
  and whose stationary probability distribution is
  p
 (
 x
 )
 .
  There are two 
 general techniques to design such a Markov Chain: the Metropolis-Hastings algorithm
  
 81",NA
4.3 ,NA,NA
Areas and Volumes,"Computing areas and volumes is a classical problem. 
  
 For many regular figures in 
 two and three dimensions there are closed form formulae. In Chapter 2, we saw how to 
 compute volume of a high dimensional sphere by integration. For general convex sets in 
 d
 -space, there are no closed form formulae. Can we estimate volumes of
  d
 -dimensional 
 convex sets in time that grows as a polynomial function of
  d
 ? The MCMC method answes
  
 86",NA
4.4 ,NA,NA
Convergence of Random Walks on Undirected Graphs,"The Metropolis-Hasting algorithm and Gibbs sampling both involve random walks on 
 edge-weighted undirected graphs. Given an edge-weighted undirected graph, let
  w
 xy 
 denote the weight of the edge between nodes
  x
  and
  y
 , with
  w
 xy
  = 0 if no such edge exists. 
 Let
  w
 x
  = 
  
 y
 w
 xy
 . The Markov chain has transition probabilities
  p
 xy
  =
  w
 xy
 /w
 x
 . We assume 
 the chain is connected.
  
 We now claim that the stationary distribution
  π
  of this walk has
  π
 x
  proportional to
  
 w
 x
 , i.e.,
  π
 x
  =
  w
 x
 /w
 total
  for
  w
 total
  =
  
 x
 ′
  w
 x
 ′
 . Specifically, notice that
  
 wp
 =
  w
  
 w
 xy
  
 =
  w
 =
  w
 =
  w
  
 w
 yx
  
 =
  wp.
  
 xxy
  
 x
  
 w
 x
  
 xy
  
 yx
  
 y
  
 w
 y
  
 yyx",NA
g,NA,NA
(,NA,NA
x,NA,NA
),NA,NA
γ,1,NA
f,NA,NA
(,NA,NA
x,NA,NA
),NA,NA
γ,"3
  
 G
 1
  =
  {
 1
 }
 ;
  G
 2
  =
  {
 2
 ,
  3
 ,
  4
 }
 ;
  G
 3
  =
  {
 5
 }.",NA
γ,"2
  
 γ
 4
  
 γ
 5
  
 x
  
 Figure 4.6:
  Bounding
  l
 1
  distance.
  
 and renumber states so that
  v
 1
  ≥ v
 2
  ≥ v
 3
  ≥ · · ·
  . Thus, early indices
  i
  for which
  v
 i
  >
  1 are 
 states that currently have too much probability, and late indices
  i
  for which
  v
 i
  <
  1 are 
 states that currently have too little probability.
  
 relatively flat and do not drop too fast as we increase
  i
 . We begin by reducing our goal 
 Intuitively, to show that
  ||
 a
  −
  π
 ||
 1
  ≤ ε
  it is enough to show that the values
  v
 i
  are
  
 to a formal statement of that form. Then, in the second part of the proof, we prove that
  
 v
 i
  do not fall fast using the concept of “probability flows”.
  
 We call a state
  i
  for which
  v
 i
  >
  1 “heavy” since it has more probability according to 
 a
  
 than its stationary probability. Let
  i
 0
  be the maximum
  i
  such that
  v
 i
  >
  1; it is the last 
 heavy state. By Proposition (4.4):
  
 i
 0
  
 ||
 a
  −
  π
 ||
 1
  = 2
  
 i
 =1
  
 (
 v
 i
  −
  1)
 π
 i
  = 2
   
 (1
  − v
 i
 )
 π
 i
 . 
 i≥i
 0
 +1
  
 (4.3)
  
 Let
  
 γ
 i
  =
  π
 1
  +
  π
 2
  +
  · · ·
  +
  π
 i
 .
  
 Define a function
  f
  : [0
 , γ
 i
 0
 ]
  → ∗
  by
  f
 (
 x
 ) =
  v
 i
  −
  1 for
  x ∗
  [
 γ
 i−
 1
 , γ
 i
 ). See Figure 4.6. Now,
  
 i
 0
  
 (
 v
 i
  −
  1)
 π
 i
  =
  
  γ
 i
 0
  
 f
 (
 x
 )
  dx.
  
 (4.4)
  
 i
 =1
  
 0
  
 We make one more technical modification. We divide
  {
 1
 ,
  2
 , . . . , i
 0
 }
  into groups
  G
 1
 , G
 2
 , G
 3
 , . . . , G
 r
 , 
 of contiguous subsets. We specify the groups later. Let
  u
 t
  = Max
 i∗G
 t
 v
 i
  be the maximum",NA
4.5 ,NA,NA
Electrical Networks and Random Walks,"In the next few sections, we study the relationship between electrical networks and 
 random walks on undirected graphs. The graphs have nonnegative weights on each 
 edge. A step is executed by picking a random edge from the current vertex with 
 probability proportional to the edge’s weight and traversing the edge.
  
 An electrical network is a connected, undirected graph in which each edge (
 x, y
 ) has
  
 a resistance
  r
 xy
  >
  0. In what follows, it is easier to deal with conductance defined as the 
  
  
 1 
  
 reciprocal of resistance,
  c
 xy
  = 
  
 r
 xy
 , rather than resistance. Associated with an electrical 
 network is a random walk on the underlying graph defined by assigning a probability
  
 p
 xy
  =
  c
 xy
 /c
 x
  to the edge (
 x, y
 ) incident to the vertex
  x,
  where the normalizing constant
  c
 x
  
 equals
  
 y
  
 c
 xy
 . Note that although
  c
 xy
  equals
  c
 yx
 , the probabilities
  p
 xy
  and
  p
 yx
  may not be
  
 equal due to the normalization required to make the probabilities at each vertex sum to 
 one. We shall soon see that there is a relationship between current flowing in an 
 electrical",NA
4.6 ,NA,NA
Random Walks on Undirected Graphs with Unit Edge Weights,"We now focus our discussion on random walks on undirected graphs with uniform 
 edge weights. At each vertex, the random walk is equally likely to take any edge. This 
 corresponds to an electrical network in which all edge resistances are one. Assume the 
 graph is connected. We consider questions such as what is the expected time for a 
 random walk starting at a vertex
  x
  to reach a target vertex
  y
 , what is the expected time 
 until the random walk returns to the vertex it started at, and what is the expected time 
 to reach every vertex?
  
 Hitting time
  
 The
  hitting time h
 xy
 , sometimes called
  discovery time
 , is the expected time of a ran-
 dom walk starting at vertex
  x
  to reach vertex
  y
 . Sometimes a more general definition is 
 given where the hitting time is the expected time to reach a vertex
  y
  from a given 
 starting probability distribution.
  
 One interesting fact is that adding edges to a graph may either increase or decrease 
 h
 xy
  depending on the particular situation. Adding an edge can shorten the distance from 
 x
  to
  y
  thereby decreasing
  h
 xy
  or the edge could increase the probability of a random walk 
 going to some far off portion of the graph thereby increasing
  h
 xy
 . Another interesting fact 
 is that hitting time is not symmetric. The expected time to reach a vertex
  y
  from a vertex
  
 x
  in an undirected graph may be radically different from the time to reach
  x
  from
  y
 .
  
 We start with two technical lemmas. The first lemma states that the expected time to 
 traverse a path of
  n
  vertices is Θ (
 n
 2
 ).
  
 Lemma 4.7
  The expected time for a random walk starting at one end of a path of n vertices 
 to reach the other end is
  Θ (
 n
 2
 )
 .
  
 Proof:
  Consider walking from vertex 1 to vertex
  n
  in a graph consisting of a single path of
  
 n
  vertices. Let
  h
 ij
 ,
  i < j
 , be the hitting time of reaching
  j
  starting from
  i
 . Now
  h
 12
  = 1",NA
4.7,NA,NA
Random Walks in Euclidean Space,"Many physical processes such as Brownian motion are modeled by random walks. 
 Random walks in Euclidean
  d
 -space consisting of fixed length steps parallel to the co-
 ordinate axes are really random walks on a
  d
 -dimensional lattice and are a special case of 
 random walks on graphs. In a random walk on a graph, at each time unit an edge from 
 the current vertex is selected at random and the walk proceeds to the adjacent vertex.
  
 Random walks on lattices
  
 We now apply the analogy between random walks and current to lattices. Consider a 
 random walk on a finite segment
  −n, . . . , −
 1
 ,
  0
 ,
  1
 ,
  2
 , . . . , n
  of a one dimensional lattice 
 starting from the origin. Is the walk certain to return to the origin or is there some prob-
 ability that it will escape, i.e., reach the boundary before returning? The probability of 
 reaching the boundary before returning to the origin is called the escape probability. We 
 shall be interested in this quantity as
  n
  goes to infinity.
  
 Convert the lattice to an electrical network by replacing each edge with a one ohm 
 resistor. Then the probability of a walk starting at the origin reaching
  n
  or –
 n
  before 
 returning to the origin is the escape probability given by
  
 p
 escape
  =
 c
 eff 
  
 c
 a
  
 where
  c
 eff
  is the effective conductance between the origin and the boundary points and
  
 c
 a 
 is the sum of the conductances at the origin. In a
  d
 -dimensional lattice,
  c
 a
  = 2
 d
  
 assuming that the resistors have value one. For the
  d
 -dimensional lattice
  
 =
  
 1
  
 escape
  =
  
 2
 d r
 eff
  
 In one dimension, the electrical network is just two series connections of
  n
  one-ohm re-
 sistors connected in parallel. So as
  n
  goes to infinity,
  r
 eff
  goes to infinity and the escape",NA
4.8,"escape
  
  
 2
 d
  
 r
 eff
 ≤
 6
 .",NA
The Web as a Markov Chain,"A modern application of random walks on directed graphs comes from trying to 
 estab-lish the importance of pages on the World Wide Web. Search Engines output an 
 ordered",NA
4.9 ,NA,NA
Bibliographic Notes,"The material on the analogy between random walks on undirected graphs and electrical 
 networks is from [DS84] as is the material on random walks in Euclidean space. Addi-",NA
4.10 Exercises,"Exercise 4.1
  The Fundamental Theorem of Markov chains says that for a connected 
 Markov chain, the long-term average distribution
  a
 (
 t
 )
  converges to a stationary distribu-
 tion. Does the t step distribution
  p
 (
 t
 )
  also converge for every connected Markov Chain? 
 Consider the following examples: (i) A two-state chain with p
 12
  =
  p
 21
  = 1
 . (ii) A three state 
 chain with p
 12
  =
  p
 23
  =
  p
 31
  = 1
  and the other p
 ij
  = 0
 . Generalize these examples to produce 
 Markov Chains with many states.
  
 Exercise 4.2
  Does
  lim
 t→∞
  a
 (
 t
 )
  − a
 (
 t
  + 1) = 0
  imply that a
 (
 t
 )
  converges to some value? Hint: 
 consider the average cumulative sum of the digits in the sequence
  10
 2
 1
 4
 0
 8
 1
 16
 · · ·
  
 Exercise 4.3
  What is the stationary probability for the following networks.
  
 0.4
  
 0.6
  
 0.4
  
 0.6
  
 0.4
  
 0.6
  
 0.4
  
 0.6
  
 0.4
  
 0.6
  
 a
  
 0
 .
 5
  
 0.5 0 0.5 0 0.5 
  
 0 
  
 0.5 
  
 0
  
  
 1
  
 0.5
  
 0.5
  
 0.5
  
 b
  
 Exercise 4.4
  A Markov chain is said to be symmetric if for all i and j, p
 ij
  =
  p
 ji
 . What is the 
 stationary distribution of a connected symmetric chain? Prove your answer.
  
 Exercise 4.5
  Prove |
 p
  −
  q
 |
 1
  = 2 
 (Proposition 4.4).
  
 i
 (
 p
 i
  − q
 i
 )
 +
  for probability distributions
  p
  and
  q
 ,
  
 Exercise 4.6
  Let p
 (
 x
 )
 , where
  x
  = (
 x
 1
 , x
 2
 , . . . , x
 d
 )
  x
 i
  ∗ {
 0
 ,
  1
 }, be a multivariate probabil-ity 
 distribution. For d
  = 100
 , how would you estimate the marginal distribution
  
 p
 (
 x
 1
 ) = 
  
 p
 (
 x
 1
 , x
 2
 , . . . , x
 d
 ) ?
  
 x
 2
 ,...,x
 d
  
 Exercise 4.7
  Using the Metropolis-Hasting Algorithm create a Markov chain whose sta-
  
 tionary probability is that given in the following table. Use the
  3
  ×
  3
  lattice for the under-
 lying graph.
  
  
 x
 1
 x
 2
  
 00
  
 01
  
 02
  
 10
  
 11
  
 12
  
 20
  
 21
  
 22
  
 Prob
  
 1
 /
 16
  
 1
 /
 8
  
 1
 /
 16
  
 1
 /
 8
  
 1
 /
 4
  
 1
 /
 8
  
 1
 /
 16
  
 1
 /
 8
  
 1
 /
 16
  
  
 Exercise 4.8
  Using Gibbs sampling create a
  4
  ×
  4
  lattice where vertices in rows and columns 
 are cliques whose stationary probability is that given in the following table.",NA
5 Machine Learning,NA,NA
5.1 ,NA,NA
Introduction,"Machine learning
  algorithms are general purpose tools for generalizing from data. 
 They have proven to be able to solve problems from many disciplines without detailed 
 domain-specific knowledge. To date they have been highly successful for a wide range of 
 tasks including computer vision, speech recognition, document classification, automated 
 driving, computational science, and decision support.
  
 The core problem.
  A core problem underlying many machine learning applications 
 is learning a good classification rule from labeled data. This problem consists of a do-
 main of interest
  X
 , called the
  instance space
 , such as the set of email messages or patient 
 records, and a classification task, such as classifying email messages into spam versus 
 non-spam or determining which patients will respond well to a given medical treatment. 
 We will typically assume our instance space
  X
  =
  {
 0
 ,
  1
 }
 d
 or
  X
  = R
 d
 , corresponding to data 
 that is described by
  d
  Boolean or real-valued features. Features for email messages could 
 be the presence or absence of various types of words, and features for patient records 
 could be the results of various medical tests. To perform the learning task, our learning 
 algorithm is given a set
  S
  of labeled
  training examples
 , which are points in
  X
  along with 
 their correct classification. This training data could be a collection of email messages, 
 each labeled as spam or not spam, or a collection of patients, each labeled by whether or 
 not they responded well to the given medical treatment. Our algorithm then aims to use 
 the training examples to produce a classification rule that will perform well over new 
 data, i.e., new points in
  X
 . A key feature of machine learning, which distinguishes it from 
 other algorithmic tasks, is that our goal is
  generalization
 : to use one set of data in order 
 to perform well on new data we have not seen yet. We focus on
  binary classification 
 where items in the domain of interest are classified into two categories (called the 
 positive class and the negative class), as in the medical and spam-detection examples 
 above, but nearly all the techniques described here will also apply to multi-way 
 classification.
  
 How to learn.
  A high-level approach that many algorithms we discuss will follow is 
 to try to find a “simple” rule with good performance on the training data. For instance, in 
 the case of classifying email messages, we might find a set of highly indicative words 
 such that every spam email in the training data has at least one of these words and none 
 of the non-spam emails has any of them; in this case, the rule “if the message has any of 
 these words then it is spam, else it is not” would be a simple rule that performs well on 
 the training data. Or, we might find a way of weighting words with positive and negative 
 weights such that the total weighted sum of words in the email message is positive on 
 the spam emails in the training data, and negative on the non-spam emails. We will then 
 argue that so long as the training data is representative of what future data will look like,",NA
5.2 ,NA,NA
The Perceptron algorithm,"To help ground our discussion, we begin by describing a specific interesting learning 
 algorithm, the
  Perceptron algorithm
 , for the problem of assigning positive and negative 
 weights to features (such as words) so that each positive example has a positive sum of 
 feature weights and each negative example has a negative sum of feature weights.
  
 More specifically, the Perceptron algorithm is an efficient algorithm for finding a lin-ear 
 separator in
  d
 -dimensional space, with a running time that depends on the
  margin of 
 separation
  of the data. We are given as input a set
  S
  of training examples (points in 
 d
 -
 dimensional space), each labeled as positive or negative, and our assumption is that there 
 exists a vector
  w
 ∗
 such that for each positive example
  x
  ∗ S
  we have
  x
 T
 w
 ∗
 ≥
  1 and for each 
 negative example
  x
  ∗ S
  we have
  x
 T
 w
 ∗
 ≤ −
 1. 
  
 Note that the quantity 
 x
 T
 w
 ∗
 /|
 w
 ∗
 |
  is the distance of the point
  x
  to the hyperplane
  x
 T
 w
 ∗
 = 0. Thus, we can view our 
 assumption as stating that there exists a linear separator through the origin with all 
 positive examples on one side, all negative examples on the other side, and all examples 
 at distance at least
  γ
  = 1
 /|
 w
 ∗
 |
  from the separator. This quantity
  γ
  is called the
  margin 
 of 
 separation (see Figure 5.1).
  
 The goal of the Perceptron algorithm is to find a vector
  w
  such that
  x
 T
 w
  >
  0 for all 
 positive examples
  x
  ∗ S
 , and
  x
 T
 w
  <
  0 for all negative examples
  x
  ∗ S
 . It does so via",NA
5.3 ,NA,NA
Kernel Functions,"Suppose that instead of a linear separator decision boundary, the boundary between im-
 portant emails and unimportant emails looks more like a circle, for example as in Figure 
 5.2.
  
 A powerful idea for addressing situations like this is to use what are called
  kernel 
 functions
 , or sometimes the “kernel trick”. Here is the idea. Suppose you have a function 
 K
 , called a “kernel”, over pairs of data points such that for some function
  φ
  : R
 d
 →
  R
 N
 , 
 where perhaps
  N ∗ d
 , we have
  K
 (
 x
 ,
  x
 ′
 ) =
  φ
 (
 x
 )
 T
 φ
 (
 x
 ′
 ). In that case, if we can write the 
 Perceptron algorithm so that it only interacts with the data via dot-products, and then 
 replace every dot-product with an invocation of
  K
 , then we can act as if we had 
 performed the function
  φ
  explicitly without having to actually compute
  φ
 .
  
 For example, consider
  K
 (
 x
 ,
  x
 ′
 ) = (1 +
  x
 T
 x
 ′
 )
 k
 for some integer
  k ≥
  1. It turns out this 
 corresponds to a mapping
  φ
  into a space of dimension
  N ≈ d
 k
 . For example, in the case 
 d
  
 = 2
 , k
  = 2 we have (using
  x
 i
  to denote the
  i
 th coordinate of
  x
 ):
  
 K
 (
 x
 ,
  x
 ′
 )
  
 =
  
 (1 +
  x
 1
 x
 ′
 1
 +
  x
 2
 x
 ′
 2
 )
 2
  
 =
  
 1 + 2
 x
 1
 x
 ′
 1
 + 2
 x
 2
 x
 ′
 2
 +
  x
 2 1
 x
 ′
 2 1
 + 2
 x
 1
 x
 2
 x
 ′
 1
 x
 ′
 2
 +
  x
 2 2
 x
 ′
 2
  
 =
  
 φ
 (
 x
 )
 T
 φ
 (
 x
 ′
 )",NA
5.4 ,NA,NA
Generalizing to New Data,"So far, we have focused on the problem of finding a classification rule that performs well 
 on a given set
  S
  of training data. But what we really want our classification rule to do is 
 to perform well on new data we have not seen yet. To make guarantees of this form, we 
 need some assumption that our training data is somehow representative of what new 
 data will look like; formally, we will assume they are drawn from the same probability 
 distribution. Additionally, we will see that we will want our algorithm’s classification 
 rule to be “simple” in some way. Together, these two conditions will allow us to make 
 generalization guarantees
 : guarantees on the ability of our learned classification rule to 
 perform well on new unseen data.
  
 Formalizing the problem. 
  
 To formalize the learning problem, assume there is some 
 probability distribution
  D
  over the instance space
  X
 , such that (a) our training set
  S
  con-
 sists of points drawn independently at random from
  D
 , and (b) our objective is to predict 
 well on new points that are also drawn from
  D
 . This is the sense in which we assume that 
 our training data is representative of future data. Let
  c
 ∗
 , called the
  target concept
 , denote 
 the subset of
  X
  corresponding to the positive class for the binary classification we are 
 aiming to make. For example,
  c
 ∗
 would correspond to the set of all patients who respond 
 well to a given treatment in a medical scenario, or it could correspond to the set of all 
 spam emails in a spam-detection scenario. So, each point in our training set
  S
  is labeled 
 according to whether or not it belongs to
  c
 ∗
 and our goal is to produce a set
  h ∗ X
 , called 
 our
  hypothesis
 , which is close to
  c
 ∗
 with respect to distribution
  D
 . The
  true error
  of
  h
  is 
 err
 D
 (
 h
 ) = Prob(
 h∗c
 ∗
 ) where “
 ∗
 ” denotes symmetric difference, and probability mass is 
 according to
  D
 . In other words, the true error of
  h
  is the probability it incorrectly clas-",NA
5.5 ,NA,NA
Overfitting and Uniform Convergence,NA,NA
5.6 ,NA,NA
Illustrative Examples and Occam’s Razor,"We now present some examples to illustrate the use of Theorem 5.3 and 5.5 and also use 
 these theorems to give a formal connection to the notion of Occam’s razor.
  
 5.6.1 Learning Disjunctions
  
 Consider the instance space
  X
  =
  {
 0
 ,
  1
 }
 d
 and suppose we believe that the target concept 
 can be represented by a
  disjunction
  (an OR) over features, such as
  c
 ∗
 =
  {x|x
 1
  = 1
  ∗ x
 4
  = 1
  ∗ 
 x
 8
  = 1
 }
 , or more succinctly,
  c
 ∗
 =
  x
 1
  ∗ x
 4
  ∗ x
 8
 . For example, if we are trying to predict 
 whether an email message is spam or not, and our features correspond to the presence 
 or absence of different possible indicators of spam-ness, then this would correspond to 
 the belief that there is some subset of these indicators such that every spam email has at 
 least one of them and every non-spam email has none of them. Formally, let
  H
  denote 
 the class of disjunctions, and notice that
  |H|
  = 2
 d
 . So, by Theorem 5.3, it suffices to find a 
 consistent disjunction over a sample
  S
  of size
  
 |S|
  = 1 
  
 d
  ln(2) + ln(1
 /δ
 ) 
  
 .
  
 How can we efficiently find a consistent disjunction when one exists? Here is a simple 
 algorithm.",NA
5.7 ,NA,NA
Regularization: Penalizing Complexity,"Theorems 5.5 and 5.7 suggest the following idea. Suppose that there is no simple rule 
 that is perfectly consistent with the training data, but we notice there are very simple 
 rules with training error 20%, say, and then some more complex rules with training 
 error 10%, and so on. In this case, perhaps we should optimize some combination of 
 training er-ror and simplicity. This is the notion of
  regularization
 , also called
  complexity 
 penalization
 .
  
 Specifically, a
  regularizer
  is a penalty term that penalizes more complex hypotheses. 
 Given our theorems so far, a natural measure of complexity of a hypothesis is the 
 number of bits we need to write it down.
 20
 Consider now fixing some description 
 language, and let 
 H
 i
  denote those hypotheses that can be described in
  i
  bits in this 
 language, so
  |H
 i
 | ≤
  2
 i
 . Let
  δ
 i
  =
  δ/
 2
 i
 . Rearranging the bound of Theorem 5.5, we know that 
 with probability at
  
 least 1
  − δ
 i
 , all
  h ∗ H
 i
  satisfy
  err
 D
 (
 h
 )
  ≤ err
 S
 (
 h
 ) + 
  
 ln(
 |H
 i
 |
 )+ln(2
 /δ
 i
 ) 
   
 2
 |S| 
  
  
  
 . 
 Now, applying the
  
 union bound over all
  i
 , using the fact that
  δ
 1
  +
  δ
 2
  +
  δ
 3
  +
  . . .
  =
  δ
 , and also the fact that 
 ln(
 |H
 i
 |
 ) + ln(2
 /δ
 i
 )
  ≤ i
  ln(4) + ln(2
 /δ
 ), gives the following corollary.
  
 Corollary 5.8
  Fix any description language, and consider a training sample S drawn from 
 distribution D. With probability greater than or equal to
  1
  − δ, all hypotheses h satisfy
  
 ()
  
  
 rh
 ) 
  
 size(
 h
 ) ln(4) + ln(2
 /δ
 )
  
 D
 ()
  
 ≤
  
 r
 S
 h
 ) 
  
 2
 |S|
  
 where
  size(
 h
 )
  denotes the number of bits needed to describe h in the given language.
  
 Corollary 5.8 gives us the tradeoff we were looking for. 
  
 It tells us that rather than 
 searching for a rule of low training error, we instead may want to search for a rule with a 
 low right-hand-side in the displayed formula. If we can find one for which this quantity is 
 small, we can be confident true error will be low as well.",NA
5.8 ,NA,NA
Online Learning,"So far we have been considering what is often called the
  batch learning
  scenario. You are 
 given a “batch” of data—the training sample
  S
 —and your goal is to use it to produce a 
 hypothesis
  h
  that will have low error on new data, under the assumption that both
  S 
 and 
 the new data are sampled from some fixed distribution
  D
 . We now switch to the more 
 challenging
  online learning
  scenario where we remove the assumption that data is 
 sampled from a fixed probability distribution, or from any probabilistic process at all.
  
 Specifically, the online learning scenario proceeds as follows. At each time
  t
  = 1
 ,
  2
 , . . .
 , two 
 events occur:
  
  
 20
 Later we will see support vector machines that use a regularizer for linear separators based on the 
 margin of separation of data.",NA
5.9 ,NA,NA
Online to Batch Conversion,"Suppose we have an online algorithm with a good mistake bound, such as the 
 Perceptron algorithm. Can we use it to get a guarantee in the distributional (batch) 
 learning setting? Intuitively, the answer should be yes since the online setting is only 
 harder. Indeed, this intuition is correct. We present here two natural approaches for 
 such online to batch conversion.
  
 Conversion procedure 1: Random Stopping.
  
 Suppose we have an online algorithm
  
 A
  with mistake-bound
  M
 . Say we run the algorithm in a single pass on a sample
  S
  of size 
 M/ϵ
 . Let
  X
 t
  be the indicator random variable for the event that
  A
  makes a mistake on 
 example
  x
 t
 . Since
 |S| t
 =1
 X
 t
  ≤ M
  for
  any
  set
  S
 , we certainly have that
  E
 [
 |S| t
 =1
 X
 t
 ]
  ≤ M 
 where the 
 expectation is taken over the random draw of
  S
  from
  D
 |S|
 . By linearity of expectation, and 
 dividing both sides by
  |S|
  we therefore have:
  
 1
  
 |S|
  
 E
 X
  
  
 M/|S|
  =
  ϵ.
  
 (5.1)
  
 |S|
  
 t
 =1
  
 E
 X
 t
  
 ≤
  
 Let
  h
 t
  denote the hypothesis used by algorithm
  A
  to predict on the
  t
 th example. Since the
  
 t
 th example was randomly drawn from
  D
 , we have
  E
 [
 err
 D
 (
 h
 t
 )] =
  E
 [
 X
 t
 ]. This means that if 
 we choose
  t
  at random from 1 to
  |S|
 , i.e., stop the algorithm at a random time, the 
 expected error of the resulting prediction rule, taken over the randomness in the draw 
 of
  
 S
  and the choice of
  t
 , is at most
  ϵ
  as given by equation (5.1). Thus we have:
  
 Theorem 5.12 (Online to Batch via Random Stopping)
  If an online algorithm A with 
 mistake-bound M is run on a sample S of size M/ϵ and stopped at a random time between
  1
  
 and |S|, the expected error of the hypothesis h produced satisfies
  E
 [
 err
 D
 (
 h
 )]
  ≤ ϵ.
  
 Conversion procedure 2: Controlled Testing. 
  
 A second natural approach to us-
 ing an online learning algorithm
  A
  in the distributional setting is to just run a series of",NA
5.10 Support-Vector Machines,"In a batch setting, rather than running the Perceptron algorithm and adapting it via 
 one of the methods above, another natural idea would be just to solve for the vector
  w 
 that minimizes the right-hand-side in Theorem 5.11 on the given dataset
  S
 . This turns 
 out to have good guarantees as well, though they are beyond the scope of this book. In 
 fact, this is the Support Vector Machine (SVM) algorithm. Specifically, SVMs solve the 
 following convex optimization problem over a sample
  S
  =
  {
 x
 1
 ,
  x
 2
 , . . .
  x
 n
 }
  where
  c
  is a 
 constant that is determined empirically.
  
 minimize 
  
 c|
 w
 |
 2
 + 
  
 s
 i
  
  
  
 i 
  
 subject to 
  
 w
  ·
  x
 i
  ≥
  1
  − s
 i
  for all positive examples
  x
 i 
  
 w
  ·
  
 x
 i
  ≤ −
 1 +
  s
 i
  for all negative examples
  x
 i 
  
 s
 i
  ≥
  0 for all
  i.
  
 The variables
  s
 i
  are called
  slack variables
 , and notice that the sum of the slack variables is 
 the total hinge loss of
  w
 . So, this convex optimization is minimizing a weighted sum of 
 1
 /γ
 2
 , where
  γ
  is the margin, and the total hinge loss. If we were to add the constraint that 
 all
  s
 i
  = 0 then this would be solving for the maximum margin linear separator for the 
 data. However, in practice, optimizing a weighted combination generally performs",NA
5.11 VC-Dimension,"In Section 5.5 we presented several theorems showing that so long as the training set 
 S
  is large compared to
 1
 ϵ
 log(
 |H|
 ), we can be confident that every
  h ∗ H
  with
  err
 D
 (
 h
 )
  ≥ ϵ
 will 
 have
  err
 S
 (
 h
 )
  >
  0, and if
  S
  is large compared to
 ϵ
 2
  log(
 |H|
 ), then we can be confident that 
 every
  h ∗ H
  will have
  |err
 D
 (
 h
 )
 −err
 S
 (
 h
 )
 | ≤ ϵ
 . In essence, these results used log(
 |H|
 ) as a 
 measure of complexity of class
  H
 . VC-dimension is a different, tighter measure of 
 complexity for a concept class, and as we will see, is also sufficient to yield confidence 
 bounds. For any class
  H
 , VCdim(
 H
 )
  ≤
  log
 2
 (
 |H|
 ) but it can also be quite a bit smaller. Let’s 
 introduce and motivate it through an example.
  
 Consider a database consisting of the salary and age for a random sample of the adult 
 population in the United States. Suppose we are interested in using the database to an-
 swer questions of the form: “what fraction of the adult population in the United States 
 has age between 35 and 45 and salary between $50,000 and $70,000?” That is, we are 
 interested in queries that ask about the fraction of the adult population within some 
 axis-parallel rectangle. What we can do is calculate the fraction of the database satisfying 
 this condition and return this as our answer. This brings up the following question: How 
 large does our database need to be so that with probability greater than or equal to 1
 −δ
 , 
 our answer will be within
  ±ϵ
  of the truth for
  every
  possible rectangle query of this form?
  
 If we assume our values are discretized such as 100 possible ages and 1,000 possible
  
 salaries, then there are at most (100
  ×
  1
 ,
  000)
 2
 = 10
 10
 possible rectangles. This means we 
 can apply Theorem 5.5 with
  |H| ≤
  10
 10
 . Specifically, we can think of the target concept 
 c
 ∗
 as the empty set so that
  err
 S
 (
 h
 ) is exactly the fraction of the sample inside rectangle
  
 h
  and
  err
 D
 (
 h
 ) is exactly the fraction of the whole population inside
  h
 .
 22
 This would tell
  
 us that a sample size of
  
 1 
  
 2
 ϵ
 2
 (10 ln 10 + ln(2
 /δ
 )) would be sufficient.
  
 However, what if we do not wish to discretize our concept class? Another approach 
 would be to say that if there are only
  N
  adults total in the United States, then there are at 
 most
  N
 4
 rectangles that are truly different with respect to
  D
  and so we could use
  
 |H| ≤ N
 4
 . Still, this suggests that
  S
  needs to grow with
  N
 , albeit logarithmically, and one 
 might wonder if that is really necessary. VC-dimension, and the notion of the
  growth
  
 function
  of concept class
  H
 , will give us a way to avoid such discretization and avoid any 
 dependence on the size of the support of the underlying distribution
  D
 .
  
  
 22
 Technically
  D
  is the uniform distribution over the adult population of the United States, and we want to 
 think of
  S
  as an independent identically distributed sample from this
  D
 .",NA
5.12,NA,NA
Strong and Weak Learning - Boosting,"We now describe
  boosting
 , which is important both as a theoretical result and as a 
 practical and easy-to-use learning method.
  
 A
  strong learner
  for a problem is an algorithm that with high probability is able to 
 achieve any desired error rate
  ϵ
  using a number of samples that may depend 
 polynomially on 1
 /ϵ
 . A
  weak learner
  for a problem is an algorithm that does just a little 
 bit better than random guessing. It is only required to get with high probability an error 
 rate less than or equal to
 1 
 that achieves the weak-learning guarantee for any distribution 
 of data can be boosted to a 
 2
 − γ
  for some 0
  < γ ≤
  1 2
 .
  We show here that a weak-learner for 
 a problem
  
 strong learner, using the technique of boosting. At the high level, the idea will be to take 
 our training sample
  S
 , and then to run the weak-learner on different data distributions 
 produced by weighting the points in the training sample in different ways. Running the 
 weak learner on these different weightings of the training sample will produce a series 
 of hypotheses
  h
 1
 , h
 2
 , . . .
 , and the idea of our reweighting procedure will be to focus 
 attention on the parts of the sample that previous hypotheses have performed poorly on. 
 At the end we will combine the hypotheses together by a majority vote.
  
 Assume the weak learning algorithm
  A
  outputs hypotheses from some class
  H
 . Our 
 boosting algorithm will produce hypotheses that will be majority votes over
  t
 0
  
 hypotheses from
  H
 , for
  t
 0
  defined below. This means that we can apply Corollary 5.20 to 
 bound the VC-dimension of the class of hypotheses our boosting algorithm can produce 
 in terms of the VC-dimension of
  H
 . In particular, the class of rules that can be produced 
 by the booster running for
  t
 0
  rounds has VC-dimension
  O
 (
 t
 0
 VCdim(
 H
 ) log(
 t
 0
 VCdim(
 H
 ))). 
 This in turn gives a bound on the number of samples needed, via Corollary 5.17, to 
 ensure that high accuracy on the sample will translate to high accuracy on new data.
  
 To make the discussion simpler, we will assume that the weak learning algorithm
  A
 , 
 when presented with a weighting of the points in our training sample, always (rather",NA
5.13 Stochastic Gradient Descent,"We now describe a widely-used algorithm in machine learning, called
  stochastic 
 gradi-ent descent
  (SGD). The Perceptron algorithm we examined in Section 5.8.3 can be 
 viewed as a special case of this algorithm, as can methods for deep learning.
  
 vector of parameters. For example, we could think of the class of linear functions where Let
  
 F
  be a class of real-valued functions
  f
 w
  : R
 d
 →
  R where
  w
  = (
 w
 1
 , w
 2
 , . . . , w
 n
 ) is a
  
 n
  =
  d
  and
  f
 w
 (
 x
 ) =
  w
 T
 x
 , or we could have more complicated functions where
  n > d
 . For 
 each such function
  f
 w
  we can define an associated set
  h
 w
  =
  {
 x
  :
  f
 w
 (
 x
 )
  ≥
  0
 }
 , and let 
 H
 F
  =
  {h
 w
  
 :
  f
 w
  ∗ F}
 . For example, if
  F
  is the class of linear functions then
  H
 F
  is the class of linear 
 separators.
  
 To apply stochastic gradient descent, we also need a
  loss function L
 (
 f
 w
 (
 x
 )
 , c
 ∗
 (
 x
 )) that 
 describes the real-valued penalty we will associate with function
  f
 w
  for its prediction on 
 an example
  x
  whose true label is
  c
 ∗
 (
 x
 ). The algorithm is then the following:",NA
5.14 Combining (Sleeping) Expert Advice,"Imagine you have access to a large collection of rules-of-thumb that specify what to 
 predict in different situations. For example, in classifying news articles, you might have 
 one that says “if the article has the word ‘football’, then classify it as sports” and another 
 that says “if the article contains a dollar figure, then classify it as business”. In predicting 
 the stock market, these could be different economic indicators. These predictors might 
 at times contradict each other, e.g., a news article that has both the word “football” and a 
 dollar figure, or a day in which two economic indicators are pointing in different direc-
 tions. It also may be that no predictor is perfectly accurate with some much better than 
 others. We present here an algorithm for combining a large number of such predictors 
 with the guarantee that if any of them are good, the algorithm will perform nearly as 
 well as each good predictor on the examples on which that predictor fires.
  
 Formally, define a “sleeping expert” to be a predictor
  h
  that on any given example
  x 
 either makes a prediction on its label or chooses to stay silent (asleep). We will think of 
 them as black boxes. Now, suppose we have access to
  n
  such sleeping experts
  h
 1
 , . . . , h
 n
 , 
 and let
  S
 i
  denote the subset of examples on which
  h
 i
  makes a prediction (e.g., this could 
 be articles with the word “football” in them). We consider the online learning model, and 
 let
  mistakes
 (
 A, S
 ) denote the number of mistakes of an algorithm
  A
  on a sequence of 
 examples
  S
 . Then the guarantee of our algorithm
  A
  will be that for all
  i
  
 E
  
 mistakes
 (
 A, S
 i
 )
  
 ≤
  (1 +
  ϵ
 )
  · mistakes
 (
 h
 i
 , S
 i
 ) +
  O
  
  log
  n
  
 ϵ
  
 where
  ϵ
  is a parameter of the algorithm and the expectation is over internal randomness 
 in the randomized algorithm
  A
 .
  
 make predictions on every example, then
  A
  performs nearly as well as the best concept As a 
 special case, if
  h
 1
 , . . . , h
 n
  are concepts from a concept class
  H
 , and so they all
  
 in
  H
 . This can be viewed as a noise-tolerant version of the Halving Algorithm of Section 5.8.2 
 for the case that no concept in
  H
  is perfect. predictions on every example is called the 
 problem of
  combining expert advice
 , and the The case of predictors that make
  
 more general case of predictors that sometimes fire and sometimes are silent is called the",NA
5.15 Deep Learning,"i
 p
 ix
  = 1 in two places. So
  w
  never
  
 Deep learning, or
  deep neural networks
 , refers to training many-layered networks of 
 nonlinear computational units.
  
 Each computational unit or gate works as follows: there are a set of “wires” bringing 
 inputs to the gate. Each wire has a “weight”; the gate’s output is a real number obtained 
 by applying a non-linear “activation function”
  g
  :
  R
  →
  R
  to the the weighted sum of the 
 input values. The activation function
  g
  is generally the same for all gates in the network, 
 though, the number of inputs to individual gates may differ.
  
 The first layer of the network The input to the network is an example
  x
  ∗ R
 d
 .
  
 transforms the example into a new vector
  f
 1
 (
 x
 ). Then the second layer transforms
  f
 1
 (
 x
 ) 
 into a new vector
  f
 2
 (
 f
 1
 (
 x
 )), and so on. Finally, the
  k
 th
 layer outputs the final prediction 
 f
 (
 x
 ) 
 =
  f
 k
 (
 f
 k−
 1
 (
 . . .
  (
 f
 1
 (
 x
 )))).
  
 In supervised learning, we are given training examples
  x
 1
 ,
  x
 2
 , . . . ,
  and corresponding 
 labels
  c
 ∗
 (
 x
 1
 )
 , c
 ∗
 (
 x
 2
 )
 , . . .
 . The training process finds a set of weights of all wires so as to 
 minimize the error: (
 f
 0
 (
 x
 1
  − c
 ∗
 (
 x
 1
 ))
 2
 +(
 f
 0
 (
 x
 2
 )
  − c
 ∗
 (
 x
 2
 ))
 2
 +
 · · ·
  . (One could alternatively aim 
 to minimize other quantities besides the sum of squared errors of training examples.)",NA
5.16 Further Current Directions,"We now briefly discuss a few additional current directions in machine learning, focusing 
 on
  semi-supervised
  learning,
  active
  learning, and
  multi-task
  learning.
  
 5.16.1 Semi-Supervised Learning
  
 Semi-supervised learning
  refers to the idea of trying to use a large unlabeled data set
  U
  to 
 augment a given labeled data set
  L
  in order to produce more accurate rules than would 
 have been achieved using just
  L
  alone. The motivation is that in many settings (e.g., 
 document classification, image classification, speech recognition), unlabeled data is 
 much more plentiful than labeled data, so one would like to make use of it if possible. Of 
 course, unlabeled data is missing the labels! Nonetheless it often contains information 
 that an
  
 171",NA
5.17 Bibliographic Notes,"The basic theory underlying learning in the distributional setting was developed by Vap-
 nik [Vap82], Vapnik and Chervonenkis [VC71], and Valiant [Val84]. The connection of 
 this to the notion of Occam’s razor is due to [BEHW87]. For more information on 
 uniform convergence, regularization and complexity penalization, see [Vap98]. The 
 Perceptron al-gorithm for online learning of linear separators was first analyzed by 
 Block [Blo62] and Novikoff [Nov62]; the proof given here is from [MP69]. A formal 
 description of the on-line learning model and its connections to learning in the 
 distributional setting is given in [Lit87]. Support Vector Machines and their connections 
 to kernel functions were first introduced by [BGV92], and extended by [CV95], with 
 analysis in terms of margins given by [STBWA98]. For further reading on SVMs, learning 
 with kernel functions, and regu-larization, see [SS01]. VC dimension is due to Vapnik and 
 Chervonenkis [VC71] with the results presented here given in Blumer, Ehrenfeucht, 
 Haussler and Warmuth [BEHW89]. Boosting was first introduced by Schapire [Sch90], 
 and Adaboost and its guarantees are due to Freund and Schapire [FS97] . Analysis of the 
 problem of combining expert advice was given by Littlestone and Warmuth [LW94] and 
 Cesa-Bianchi et al. [CBFH
 +
 97]; the analysis of the sleeping experts problem given here is 
 from [BM07].
  
 175",NA
5.18 Exercises,"Exercise 5.1 (Section 5.5 and 5.6)
  Consider the instance space X
  =
  {
 0
 ,
  1
 }
 d
 and let H be the 
 class of 3-CNF formulas. That is, H is the set of concepts that can be described as a 
 conjunction of clauses where each clause is an OR of up to 3 literals. (These are also
  
 called 3-SAT formulas). For example c
 ∗
 might be
  (
 x
 1
 ∗
 ¯
 x
 2
 ∗x
 3
 )(
 x
 2
 ∗x
 4
 )(¯
 x
 1
 ∗x
 3
 )(
 x
 2
 ∗x
 3
 ∗x
 4
 )
 . 
 Assume we are in the PAC learning setting, so examples are drawn from some underlying
  
 distribution D and labeled by some 3-CNF formula c
 ∗
 .
  
 1. Give a number of samples m that would be sufficient to ensure that with probability
  
 ≥
  1
  − δ, all 3-CNF formulas consistent with the sample have error at most ϵ with respect 
 to D.
  
 2. Give a polynomial-time algorithm for PAC-learning the class of 3-CNF formulas.
  
 Exercise 5.2 (Section 5.5)
  Consider the instance space X
  = R
 , and the class of func-tions H
  
 =
  {f
 a
  :
  f
 a
 (
 x
 ) = 1
  iff x ≥ a} for a ∗
  R
 . That is, H is the set of all threshold functions on the line. 
 Prove that for any distribution D, a sample S of size O
 (
 1
 ϵ
 log(
 1
 δ
 ))
  
 is sufficient to ensure that with probability ≥
  1
  − δ, any f
 a
 ′
  such that err
 S
 (
 f
 a
 ′
 ) = 0
  has err
 D
 (
 f
 a
 ′
 )
  
 ≤ ϵ. Note that you can answer this question from first principles, without using the concept 
 of VC-dimension.
  
 Exercise 5.3 (Perceptron; Section 5.8.3)
  Consider running the Perceptron algorithm
  
 in the online model on some sequence of examples S. Let S
 ′
 be the same set of examples
  
 as S but presented in a different order. Does the Perceptron algorithm necessarily make
  
 the same number of mistakes on S as it does on S
 ′
 ? If so, why? If not, show such an S
  
 and S
 ′
 (consisting of the same set of examples in a different order) where the Perceptron
  
 algorithm makes a different number of mistakes on S
 ′
 than it does on S.
  
 Exercise 5.4 (representation and linear separators)
  Show that any disjunction (see
  
 Section 5.6.1) over {
 0
 ,
  1
 }
 d
 can be represented as a linear separator. Show that moreover the 
 margin of separation is
  Ω(1
 /√d
 )
 .
  
 Exercise 5.5 (Linear separators; easy)
  Show that the parity function on d ≥
  2 
 Boolean 
 variables cannot be represented by a linear threshold function. The parity function
  
 is 1 if and only if an odd number of inputs is 1.
  
 Exercise 5.6 (Perceptron; Section 5.8.3)
  We know the Perceptron algorithm makes
  
 at most
  1
 /γ
 2
 mistakes on any sequence of examples that is separable by margin γ (we
  
 assume all examples are normalized to have length 1). However, it need not find a sep-
  
 arator of large margin. If we also want to find a separator of large margin, a natural",NA
6 Algorithms for Massive Data Problems: Stream-,NA,NA
"ing, Sketching, and Sampling",NA,NA
6.1 ,NA,NA
Introduction,"This chapter deals with massive data problems where the input data is too large to 
 be stored in random access memory. One model for such problems is the streaming 
 model, where
  n
  data items
  a
 1
 , a
 2
 , . . . , a
 n
  arrive one at a time. For example, the
  a
 i
  might be 
 IP addresses being observed by a router on the internet. The goal is for our algorithm to 
 compute some statistics, property, or summary of these data items without using too 
 much memory, much less than
  n
 . More specifically, we assume each
  a
 i
  itself is a
  b
 -bit 
 quantity where
  b
  is not too large. For example, each
  a
 i
  might be an integer in
  {
 1
 , . . . , m} 
 where
  m
  = 2
 b
 . Our goal will be to produce some desired output using space polynomial 
 in
  b
  and log
  n
 ; see Figure 6.1.
  
 For example, a very easy problem to solve in the streaming model is to compute the 
 sum of all the
  a
 i
 . If each
  a
 i
  is an integer between 1 and
  m
  = 2
 b
 , then the sum of all the
  a
 i 
 is 
 an integer between 1 and
  mn
  and so the number of bits of memory needed to maintain 
 the sum is
  O
 (
 b
  + log
  n
 ). A harder problem, which we discuss shortly, is computing the 
 number of distinct numbers in the input sequence.
  
 One natural approach for tackling a range of problems in the streaming model is to 
 perform random sampling of the input “on the fly”. To introduce the basic flavor of 
 sampling on the fly, consider a stream
  a
 1
 , a
 2
 , . . . , a
 n
  from which we are to select an index 
 i
  
 with probability proportional to the value of
  a
 i
 .
  When we see an element, we do not know 
 the probability with which to select it since the normalizing constant depends on all of 
 the elements including those we have not yet seen. However, the following method 
 works. Let
  s
  be the sum of the
  a
 i
 ’s seen so far. Maintain
  s
  and an index
  i
  selected with 
 probability
 a
 i 
  
 s
 .
  Initially
  i
  = 1 and
  s
  =
  a
 1
 .
  Having seen symbols
  a
 1
 , a
 2
 , . . . , a
 j
 , s
  will equal
  a
 1
  +
  a
 2
  +
  · · ·
  +
  a
 j
  
 and for each
  i
  in
  {
 1
 , . . . , j}
 , the selected index will be
  i
  with probability
 a
 i 
 s
 . On seeing
  a
 j
 +1
 , 
 change the selected index to
  j
  + 1 with probability 
 s
 +
 a
 j
 +1 
  
   
  
  
 a
 j
 +1 
  
 and otherwise keep the same index as before with probability 1
  −
 s
 +
 a
 j
 +1
 .
  If we change the 
 index to
  j
  + 1
 ,
  clearly it was selected with the correct probability. If we keep
  i
  as our 
 selection, then it will have been selected with probability
  
 1
  
 a
 j
 +1
  
  a
 i
  
 =
  
  
 s
  
  
 a
 i
  
 =
  
  
 a
 i
  
 1
  −
  
 s
  +
  a
 j
 +1
  
 s
 =
  
 s
  +
  a
 j
 +1
  
 s
 =
  
 s
  +
  a
 j
 +1
  
 which is the correct probability for selecting index
  i.
  Finally
  s
  is updated by adding
  a
 j
 +1 
 to
  
 s.
  This problem comes up in many areas such as sleeping experts where there is a 
 sequence of weights and we want to pick an expert with probability proportional to its 
 weight. The
  a
 i
 ’s are the weights and the subscript
  i
  denotes the expert.",NA
6.2 ,NA,NA
Frequency Moments of Data Streams,"An important class of problems concerns the frequency moments of data streams. As 
 mentioned above, a data stream
  a
 1
 , a
 2
 , . . . , a
 n
  of length
  n
  consists of symbols
  a
 i
  from an 
 alphabet of
  m
  possible symbols which for convenience we denote as
  {
 1
 ,
  2
 , . . . , m}
 . 
 Throughout this section,
  n, m
 , and
  a
 i
  will have these meanings and
  s
  (for symbol) will 
 denote a generic element of
  {
 1
 ,
  2
 , . . . , m}
 . The frequency
  f
 s
  of the symbol
  s
  is the number 
 of occurrences of
  s
  in the stream. For a nonnegative integer
  p
 , the
  p
 th
 frequency moment 
 of the stream is 
  
  
 m 
  
  
  
  
 f
 p s
 .
  
 s
 =1
  
 Note that the
  p
  = 0 frequency moment corresponds to the number of distinct symbols 
 occurring in the stream using the convention 0
 0
 = 0. The first frequency moment is just
  
 n
 , the length of the string. The second frequency moment,
  
 s
 f
  2 
 s
 , is useful in computing
  
 the variance of the stream, i.e., the average squared difference from the average frequency.
  
 1
  
 m
  
 n
  
 2
  
  1
  
 m
  
 f
 2 
 s
 −
  2
  n mf
 s
  +
  
  n
  
 2
  
 =
  
 1
  
 m
  
 2
  
 −n
 2 
 m
 2
  
 m
  
 s
 =1
  
 f
 s
  −m
  
  
 =
  
 m
  
  
 s
 =1
  
 m
   
 m
  
 s
 =1
  
 f 
 s
  
  m
  
 1
 /p
  
 In the limit as
  p
  becomes large,
  
 f
 p s
  
 is the frequency of the most frequent ele-
  
 s
 =1
  
 ment(s).
  
 We will describe sampling based algorithms to compute these quantities for 
 streaming data shortly. First a note on the motivation for these problems. The identity 
 and fre-quency of the the most frequent item, or more generally, items whose frequency 
 exceeds a given fraction of
  n
 , is clearly important in many applications. If the items are 
 packets on a network with source and/or destination addresses, the high frequency 
 items identify the heavy bandwidth users. If the data consists of purchase records in a 
 supermarket, the high frequency items are the best-selling items. Determining the 
 number of distinct symbols is the abstract version of determining such things as the 
 number of accounts, web users, or credit card holders. The second moment and variance 
 are useful in networking as well as in database and other applications. Large amounts of 
 network log data are generated by routers that can record the source address, 
 destination address, and the number of packets for all the messages passing through 
 them. This massive data cannot be easily sorted or aggregated into totals for each 
 source/destination. But it is important to know",NA
6.3 ,NA,NA
Matrix Algorithms using Sampling,"We now move from the streaming model to a model where the input is stored in 
 memory, but because the input is so large, one would like to produce a much smaller 
 approximation to it, or perform an approximate computation on it in low space. For 
 instance, the input might be stored in a large slow memory and we would like a 
 small“sketch” that can be stored in smaller fast memory and yet retains the important 
 prop-erties of the original input. In fact, one can view a number of results from the 
 chapter on machine learning in this way: we have a large population, and we want to 
 take a small sample, perform some optimization on the sample, and then argue that the 
 optimum solution on the sample will be approximately optimal over the whole 
 population. In the chapter on machine learning, our sample consisted of independent 
 random draws from the overall population or data distribution. Here we will be looking 
 at matrix algorithms and to achieve errors that are small compared to the Frobenius 
 norm of the matrix rather than compared to the total number of entries, we will perform 
 non-uniform sampling.
  
 Algorithms for matrix problems like matrix multiplication, low-rank approximations, 
 singular value decomposition, compressed representations of matrices, linear 
 regression etc. are widely used but some require
  O
 (
 n
 3
 ) time for
  n × n
  matrices.
  
 The natural alternative to working on the whole input matrix is to pick a random 
 sub-matrix and compute with that. Here, we will pick a subset of columns or rows of the 
 input matrix. If the sample size
  s
  is the number of columns we are willing to work with, 
 we will do
  s
  independent identical trials. In each trial, we select a column of the matrix. 
 All that we have to decide is what the probability of picking each column is. Sampling 
 uniformly at random is one option, but it is not always good if we want our error to be a 
 small fraction of the Frobenius norm of the matrix. For example, suppose the input 
 matrix has all entries in the range [
 −
 1
 ,
  1] but most columns are close to the zero vector 
 with only a few significant columns. Then, uniformly sampling a small number of 
 columns is unlikely to pick up any of the significant columns and essentially will 
 approximate the original matrix with the all-zeroes matrix.
 30
  
 30
 There are, on the other hand, many positive statements one
  can
  make about uniform sampling. For 
 example, suppose the columns of
  A
  are data points in an
  m
 -dimensional space (one dimension per row). 
 Fix any
  k
 -dimensional subspace, such as the subspace spanned by the
  k
  top singular vectors. If we",NA
6.4 ,NA,NA
Sketches of Documents,"Suppose one wished to store all the web pages from the World Wide Web. Since 
 there are billions of web pages, one might want to store just a sketch of each page where 
 a sketch is some type of compact description that captures sufficient information to do 
 whatever task one has in mind. For the current discussion, we will think of a web page 
 as a string of characters, and the task at hand will be one of estimating similarities 
 between pairs of web pages.
  
 We begin this section by showing how to estimate similarities between sets via sam-
 pling, and then how to convert the problem of estimating similarities between strings 
 into a problem of estimating similarities between sets.
  
 Consider subsets of size 1000 of the integers from 1 to 10
 6
 . Suppose one wished to 
 compute the resemblance of two subsets
  A
  and
  B
  by the formula
  
 resemblance (
 A, B
 ) =
 |A∩B| 
  
 |A∗B|
  
 Suppose that instead of using the sets
  A
  and
  B
 , one sampled the sets and compared 
 random subsets of size ten. How accurate would the estimate be? One way to sample 
 would be to select ten elements uniformly at random from
  A
  and
  B
 . Suppose
  A
  and
  B 
 were each of size 1000, over lapped by 500, and both were represented by six samples. 
 Even though half of the six samples of
  A
  were in
  B
  they would not likely be among the 
 samples representing
  B
 . See Figure 6.6. This method is unlikely to produce overlapping",NA
6.5 ,NA,NA
Bibliographic Notes,"The hashing-based algorithm for counting the number of distrinct elements in a data 
 stream described in Section 6.2.1 is due to Flajolet and Martin [FM85]. Algorithm Fre-
 quent for identifying the most frequent elements is due to Misra and Gries [MG82]. The 
 algorithm for estimating the second moment of a data stream described in Section 6.2.4 
 is due to Alon, Matias and Szegedy [AMS96], who also gave algorithms and lower bounds 
 for other
  k
 th
 moments. These early algorithms for streaming data significantly influenced 
 further research in the area. Improvements and generalizations of Algorithm Frequent 
 were made in [MM02].
  
 Length-squared sampling was introduced by Frieze, Kannan and Vempala [FKV04]; 
 the algorithms of Section 6.3 are from [DKM06a, DKM06b]. The material in Section 6.4 
 on sketches of documents is from Broder et al. [BGMZ97].
  
 203",NA
6.6 ,NA,NA
Exercises,"Exercise 6.1
  Let a
 1
 , a
 2
 , . . . , a
 n
 , be a stream of symbols each an integer in {
 1
 , . . . , m}. 1. Give 
 an algorithm that will select a symbol uniformly at random from the stream. 
  
 How much 
 memory does your algorithm require?
  
 2. Give an algorithm that will select a symbol with probability proportional to a
 2 
 i
 .
  
 Exercise 6.2
  How would one pick a random word from a very large book where the prob-
 ability of picking a word is proportional to the number of occurrences of the word in the 
 book?
  
 Exercise 6.3
  Consider a matrix where each element has a probability of being selected. 
 Can you select a row according to the sum of probabilities of elements in that row by just 
 selecting an element according to its probability and selecting the row that the element is 
 in?
  
 Exercise 6.4
  For the streaming model give an algorithm to draw t independent samples of 
 indices i, each with probability proportional to the value of a
 i
 . Some images may be drawn 
 multiple times. What is its memory usage?
  
 Exercise 6.5
  For some constant c >
  0
 , it is possible to create
  2
 cm
 subsets of {
 1
 , . . . , m}, each 
 with m/
 2
  elements, such that no two of the subsets share more than
  3
 m/
 8
  elements in 
 common.
 32
 Use this fact to argue that any deterministic algorithm that even guarantees to
  
 approximate
  the number of distinct elements in a data stream with error less than
 m 
 16 
 must 
 use
  Ω(
 m
 )
  bits of memory on some input sequence of length n ≤
  2
 m.
  
 Exercise 6.6
  Consider an algorithm that uses a random hash function and gives an 
 estimate
  ˆ
 x of the true value x of some variable. Suppose that
 x 
 at least 0.6. The probability of 
 the estimate is with respect to choice of the hash function. 
 4
 ≤
  ˆ
 x ≤
  4
 x with probability
  
 How would you improve the probability that
 x 
 know the variance taking average may not help 
 and we need to use some other function 
 4
 ≤
  ˆ
 x ≤
  4
 x to 0.8? Hint: Since we do not
  
 of multiple runs.
  
 Exercise 6.7
  Give an example of a set H of hash functions such that h
 (
 x
 )
  is equally likely to 
 be any element of {
 0
 , . . . , M −
  1
 } (H is 1-universal) but H is not 2-universal.
  
 Exercise 6.8
  Let p be a prime. A set of hash functions
  
 H
  =
  {h| {
 0
 ,
  1
 , . . . , p −
  1
 } → {
 0
 ,
  1
 , . . . , p −
  1
 }} 
  
 is 3-universal if for all x,y,z,u,v,w in {
 0
 ,
  1
 , . . . , p −
  1
 } , where x, y, z are distinct we have
  
 Prob
  
 h
  (
 x
 ) =
  u, h
  (
 y
 ) =
  v, h
  (
 z
 ) =
  w
  
 = 1 
 p
 3
 .
  
  
 32
 For example, choosing them randomly will work with high probability. You expect two subsets of size 
 m/
 2 to share
  m/
 4 elements in common, and with high probability they will share no more than 3
 m/
 8.",NA
7 Clustering,NA,NA
7.1 ,NA,NA
Introduction,"Clustering refers to partitioning a set of objects into subsets according to some de-
 sired criterion. Often it is an important step in making sense of large amounts of data. 
 Clustering comes up in many contexts. One might want to partition a set of news articles 
 into clusters based on the topics of the articles. Given a set of pictures of people, one 
 might want to group them into clusters based on who is in the image. Or one might want 
 to cluster a set of protein sequences according to the protein function. A related problem 
 is not finding a full partitioning but rather just identifying natural clusters that exist. For 
 example, given a collection of friendship relations among people, one might want to 
 identify any tight-knit groups that exist. In some cases we have a well-defined correct 
 answer, e.g., in clustering photographs of individuals by who is in them, but in other 
 cases the notion of a good clustering may be more subjective.
  
 Before running a clustering algorithm, one first needs to choose an appropriate 
 repre-sentation for the data. One common representation is as vectors in
  R
 d
 .
  This 
 corresponds to identifying
  d
  real-valued features that are then computed for each data 
 object. For ex-ample, to represent documents one might use a “bag of words” 
 representation, where each feature corresponds to a word in the English language and 
 the value of the feature is how many times that word appears in the document. Another 
 common representation is as vertices in a graph, with edges weighted by some measure 
 of how similar or dissimilar the two endpoints are. For example, given a set of protein 
 sequences, one might weight edges based on an edit-distance measure that essentially 
 computes the cost of transforming one sequence into the other. This measure is typically 
 symmetric and satisfies the triangle inequality, and so can be thought of as a finite 
 metric. A point worth noting up front is that often the “correct” clustering of a given set 
 of data depends on your goals. For instance, given a set of photographs of individuals, we 
 might want to cluster the images by who is in them, or we might want to cluster them by 
 facial expression. When representing the images as points in space or as nodes in a 
 weighted graph, it is important that the features we use be relevant to the criterion we 
 care about. In any event, the issue of how best to represent data to highlight the relevant 
 information for a given task is generally addressed using knowledge of the specific 
 domain. From our perspective, the job of the clustering algorithm begins after the data 
 has been represented in some appropriate way.
  
 In this chapter, our goals are to discuss (a) some commonly used clustering 
 algorithms and what one can prove about them, and (b) models and assumptions on 
 data under which we can find a clustering close to the correct clustering.
  
 7.1.1 Preliminaries
  
 We will follow the standard notation of using
  n
  to denote the number of data points 
 and
  k
  to denote the number of desired clusters. We will primarily focus on the case that
  
 208",NA
7.2 ,NA,NA
k,NA,NA
-Means Clustering,"We assume in this section that data points lie in
  R
 d
 and focus on the
  k
 -means criterion.
  
 7.2.1 A Maximum-Likelihood Motivation
  
 We now consider a maximum-likelihood motivation for using the
  k
 -means criterion. 
 Suppose that the data was generated according to an equal weight mixture of
  k
  spherical 
 well-separated Gaussian densities centered at
  µ
 1
 , µ
 2
 , . . . , µ
 k
 , each with variance one in 
 every direction. Then the density of the mixture is
  
 Pob() 
  
 1
  
 1
  
 k
  
 −|
 x
 −
 µ
 i
 |
 2
 .
  
 Pob() 
  
 (2
 π
 )
 d/
 2
  
 k
  
 i
 =1
  
 .
  
 Denote by
  µ
 (
 x
 ) the center nearest to
  x
 . Since the exponential function falls off fast, 
 assuming x is noticeably closer to its nearest center than to any other center, we can
  
 211",NA
7.3 ,NA,NA
k,NA,NA
-Center Clustering,"In this section, instead of using the
  k
 -means clustering criterion, we use the
  k
 -center 
 criterion. Recall that the
  k
 -center criterion partitions the points into
  k
  clusters so as to 
 minimize the maximum distance of any point to its cluster center. Call the maximum dis-
 tance of any point to its cluster center the
  radius
  of the clustering. There is a
  k
 -clustering 
 of radius
  r
  if and only if there are
  k
  spheres, each of radius
  r,
  which together cover all the 
 points. Below, we give a simple algorithm to find
  k
  spheres covering a set of points. The 
 following lemma shows that this algorithm only needs to use a radius that is at most 
 twice that of the optimal
  k
 -center solution. Note that this algorithm is equivalent to the 
 farthest traversal strategy for initializing Lloyd’s algorithm.
  
 The Farthest Traversal
  k
 -clustering Algorithm",NA
7.4 ,NA,NA
Finding Low-Error Clusterings,"In the previous sections we saw algorithms for finding a local optimum to the
  k
 -means 
 clustering objective, for finding a global optimum to the
  k
 -means objective on the line, 
 and for finding a factor 2 approximation to the
  k
 -center objective. But what about finding 
 a clustering that is close to the correct answer, such as the true clustering of proteins by 
 function or a correct clustering of news articles by topic? 
  
 For this we need some 
 assumption about the data and what the correct answer looks like. The next few sections 
 consider algorithms based on different such assumptions.",NA
7.5 ,NA,NA
Spectral Clustering,"Let
  A
  be a
  n×d
  data matrix with each row a data point and suppose we want to partition 
 the data points into
  k
  clusters.
  Spectral Clustering
  refers to a class of clustering 
 algorithms which share the following outline:
  
 •
  Find the space
  V
  spanned by the top
  k
  (right) singular vectors of
  A
 .
 •
  
 Project data points into
  V
  .
  
 •
  Cluster the projected points.
  
 7.5.1 Why Project?
  
 The reader may want to read Section 3.9.3, which shows the efficacy of spectral 
 clustering for data stochastically generated from a mixture of spherical Gaussians. Here, 
 we look at general data which may not have a stochastic generation model.
  
 We will later describe the last step in more detail. First, lets understand the central 
 advantage of doing the projection to
  V
  . It is simply that for any reasonable (unknown) 
 clustering of data points, the projection brings data points closer to their cluster centers. 
 This statement sounds mysterious and likely false, since the assertion is for ANY rea-
 sonable unknown clustering. We quantify it in Theorem 7.4. First some notation: We 
 represent a
  k
 -clustering by a
  n × d
  matrix
  C
  (same dimensions as
  A
 ), where row
  i
  of
  C
  is",NA
7.6 ,NA,NA
Approximation Stability,"7.6.1 The Conceptual Idea
  
 We now consider another condition that will allow us to produce accurate clusters 
 from data. To think about this condition, imagine that we are given a few thousand news 
 articles that we want to cluster by topic. These articles could be represented as points in 
 a high-dimensional space (e.g., axes could correspond to different meaningful words, 
 with coordinate
  i
  indicating the frequency of that word in a given article). Or, 
 alternatively, it could be that we have developed some text-processing program that 
 given two articles
  x 
 and
  y
  computes some measure of distance
  d
 (
 x, y
 ) between them. We 
 assume there exists some correct clustering
  C
 T
  of our news articles into
  k
  topics; of 
 course, we do not know what
  C
 T
  is, that is what we want our algorithm to find.
  
 If we are clustering with an algorithm that aims to minimize the
  k
 -means score of its 
 solution, then implicitly this means we believe that the clustering
  C
 OPT kmeans
 of minimum
  
 k
 -means score is either equal to, or very similar to, the clustering
  C
 T
 . Unfortunately, 
 finding the clustering of minimum
  k
 -means score is NP-hard. So, let us broaden our 
 belief a bit and assume that any clustering
  C
  whose
  k
 -means score is within 10% of the 
 minimum is also very similar to
  C
 T
 . This should give us a little bit more slack. 
 Unfortunately, finding a clustering of score within 10% of the minimum is also an NP-
 hard problem. Nonetheless,
  we will be able to use this assumption to efficiently find a 
 clustering that is close to C
 T
 . The trick is that NP-hardness is a worst-case notion, 
 whereas in contrast, this assumption implies structure on our data.In particular, it 
 implies that all clusterings that have score within 10% of the minimum have to be 
 similar to each other. We will then be able to utilize this structure in a natural “ball-
 growing” clustering algorithm.
  
 7.6.2 Making this Formal
  
 To make this discussion formal, we first specify what we mean when we say that two
  
 different ways of clustering some data are “similar” to each other. Let
  C
  =
  {C
 1
 , . . . , C
 k
 } 
 and
  
 C
 ′
 =
  {C
 ′
 1
 , . . . , C
 ′k
 }
  be two different
  k
 -clusterings of some dataset
  A.
  For example,
  C 
 could be 
 the clustering that our algorithm produces, and
  C
 ′
 could be the clustering
  C
 T
 . Let us define 
 the distance between these two clusterings to be the fraction of points that
  
 would have to be re-clustered in
  C
  to make
  C
  match
  C
 ′
 , where by “match” we mean that 
 there should be a bijection between the clusters of
  C
  and the clusters of
  C
 ′
 . We can write 
 this distance mathematically as:
  
 is
 (
 ,
 ′
 )=mn
  
 1
  
 k
  
 C
 ′
  
 is
 (
 C, C
 )=mn
  
 n
  
 i
 =1
  
 |
 i
  \C
 σ
 (
 i
 )
 |
  
 where the minimum is over all permutations
  σ
  of
  {
 1
 , . . . , k}
 .",NA
7.7,NA,NA
High-Density Clusters,"We now turn from the assumption that clusters are center-based to the assumption 
 that clusters consist of high-density regions, separated by low-density moats such as in 
 Figure 7.1.
  
 7.7.1 Single Linkage
  
 One natural algorithm for clustering under the high-density assumption is called
  
 single linkage
 . This algorithm begins with each point in its own cluster and then 
 repeatedly merges the two “closest” clusters into one, where the distance between two 
 clusters is defined as the
  minimum
  distance between points in each cluster. That is,
  
 d
 min
 (
 C, C
 ′
 ) = min
 x
 ∗C,
 y
 ∗C
 ′
  d
 (
 x
 ,
  y
 ), and the algorithm merges the two clusters
  C
  and
  C
 ′
 whose
  
 d
 min
  value is smallest over all pairs of clusters breaking ties arbitrarily. It then continues 
 until there are only
  k
  clusters. This is called an
  agglomerative
  clustering algorithm 
 because it begins with many clusters and then starts merging, or agglomerating them 
 together.
 36
 Single-linkage is equivalent to running Kruskal’s minimum-spanning-tree 
 algorithm, but halting when there are
  k
  trees remaining. The following theorem is fairly 
 immediate.
  
 Theorem 7.8
  Suppose the desired clustering C
 ∗
 1
 , . . . , C
 ∗k
 satisfies the property that there 
 exists some distance σ such that
  
 1. any two data points in different clusters have distance at least σ, and
  
 2. for any cluster C
 ∗
 there exist points on each side of the partition of distance less than σ. 
 i
 and 
 any partition of C
 ∗i
 into two non-empty sets A and C
 ∗i
 \ A,
  
 Then, single-linkage will correctly recover the clustering C
 ∗
 1
 , . . . , C
 ∗k
 .
  
 Proof:
  Consider running the algorithm until all pairs of clusters
  C
  and
  C
 ′
 have
  d
 min
 (
 C, C
 ′
 )
  ≥ σ
 . At 
 that point, by (2), each target cluster
  C
 ∗i
 will be fully contained within some cluster 
  
 of the single-linkage algorithm. On the other hand, by (1) and by induction, each cluster
  
 C
  of the single-linkage algorithm will be fully contained within some
  C
 ∗i
 of the target
  
 clustering, since any merger of subsets of distinct target clusters would require
  d
 min
  ≥ σ
 . 
 Therefore, the single-linkage clusters are indeed the target clusters.
  
 36
 Other agglomerative algorithms include
  complete linkage
  which merges the two clusters whose
  max-
 imum
  distance between points is smallest, and Ward’s algorithm described earlier that merges the two 
 clusters that cause the
  k
 -means cost to increase by the least.",NA
7.8 ,NA,NA
Kernel Methods,"Kernel methods combine aspects of both center-based and density-based clustering. 
 In center-based approaches like
  k
 -means or
  k
 -center, once the cluster centers are fixed, 
 the",NA
7.9 ,NA,NA
Recursive Clustering based on Sparse Cuts,"We now consider the case that data are nodes in an undirected connected graph 
 G
 (
 V, 
 E
 ) where an edge indicates that the end point vertices are similar. Recursive clus-tering 
 starts with all vertices in one cluster and recursively splits a cluster into two parts 
 whenever there is a small number of edges from one part to the other part of the cluster.
  
 Formally, for two disjoint sets
  S
  and
  T
  of vertices, define
  
 Φ(
 S, T
 ) =
  
 Number of edges from
  S
  to
  T 
  
 Total number of edges incident to
  S
  in
  G.
  
 Φ(
 S, T
 ) measures the relative strength of similarities between
  S
  and
  T
 . Let
  d
 (
 i
 ) be the 
 degree of vertex
  i
  and for a subset
  S
  of vertices, let
  d
 (
 S
 ) = 
 i∗S
 d
 (
 i
 ). Let
  m
  be the total 
 number of edges in the graph. The following algorithm aims to cut only a small fraction of 
 the edges and to produce clusters that are internally consistent in that no subset of the 
 cluster has low similarity to the rest of the cluster.
  
 Recursive Clustering:
  Select an appropriate value for
  ϵ.
  If a current cluster 
 W
  
 has a subset
  S
  with
  d
 (
 S
 )
  ≤
 1 2
 d
 (
 W
 ) and Φ(
 S, S ∗ W
 )
  ≤ ε
 , then split
  W 
 into two 
 clusters
  S
  and
  W \ S
 . Repeat until no such split is possible.
  
 Theorem 7.9
  At termination of Recursive Clustering, the total number of edges between 
 vertices in different clusters is at most O
 (
 εm
  ln
  n
 )
 .",NA
7.10 Dense Submatrices and Communities,"has all nonnegative entries. Examples to keep in mind for this section are the document-
 Represent
  n
  data points in
  d
 -space by the rows of an
  n × d
  matrix
  A
 . Assume that
  A
  
 term matrix and the customer-product matrix. We address the question of how to define 
 and find efficiently a coherent large subset of rows. To this end, the matrix
  A
  can be 
 represented by a bipartite graph Figure 7.5. One side has a vertex for each row and the 
 other side a vertex for each column. Between the vertex for row
  i
  and the vertex for 
 column
  j
 , there is an edge with weight
  a
 ij
 .
  
 We want a subset
  S
  of row vertices and a subset
  T
  of column vertices so that
  
 A
 (
 S, T
 ) = 
  
 a
 ij
  
 i∗S,j∗T
  
 is high. This simple definition is not good since
  A
 (
 S, T
 ) will be maximized by taking all 
 rows and columns. We need a balancing criterion that ensures that
  A
 (
 S, T
 ) is high
  
 relative to the sizes of
  S
  and
  T
 . One possibility is to maximize
 A
 (
 S,T
 ) 
 measure either, since it is 
 maximized by the single edge of highest weight. The definition 
 |S||T|
 . This is not a good
  
 we use is the following. Let
  A
  be a matrix with nonnegative entries. For a subset
  S
  of 
  
  
 A
 (
 S,T
 ) 
 rows 
 and a subset
  T
  of columns, the
  density d
 (
 S, T
 ) of
  S
  and
  T
  is
  d
 (
 S, T
 ) =
 √
 |S||T|
 .
  The 
 density d
 (
 A
 ) 
 of
  A
  is defined as the maximum value of
  d
 (
 S, T
 ) over all subsets of rows and
  
 columns. This definition applies to bipartite as well as non bipartite graphs.
  
 One important case is when
  A
 ’s rows and columns both represent the same set and 
 a
 ij
  
 is the similarity between object
  i
  and object
  j.
  Here
  d
 (
 S, S
 ) =
 A
 (
 S,S
 ) 
 0-1 matrix, it can be 
 thought of as the adjacency matrix of an undirected graph, and 
 |S|
 .
  If
  A
  is an
  n × n",NA
7.11 Community Finding and Graph Partitioning,"Assume that data are nodes in a possibly weighted graph where edges represent 
 some notion of affinity between their endpoints. In particular, let
  G
  = (
 V, E
 ) be a 
 weighted graph. Given two sets of nodes
  S
  and
  T
 , define
  
 E
 (
 S, T
 ) = 
  
 e
 ij
 .
  
 i∗S 
  
 j∗T
  
 We then define the
  density
  of a set
  S
  to be
  
 d
 (
 S, S
 ) =
 E
 (
 S, S
 ) 
 |S|
  
 .
  
 If
  G
  is an undirected graph, then
  d
 (
 S, S
 ) can be viewed as the average degree in the 
 vertex-induced subgraph over
  S
 . The set
  S
  of maximum density is therefore the subgraph 
 of maximum average degree. Finding such a set can be viewed as finding a tight-knit 
 community inside some network. In the next section, we describe an algorithm for 
 finding such a set using network flow techniques.
  
 Flow Methods 
 Here we consider dense induced subgraphs of a graph. An induced 
 subgraph of a graph consisting of a subset of the vertices of the graph along with all 
 edges of the graph that connect pairs of vertices in the subset of vertices. We show that 
 finding an induced subgraph with maximum average degree can be done by network 
 flow techniques. This is simply maximizing the density
  d
 (
 S, S
 ) over all subsets
  S
  of the 
 graph. First consider the problem of finding a subset of vertices such that the induced 
 subgraph has average degree at least
  λ
  for some parameter
  λ
 . Then do a binary search on 
 the value of
  λ
  until the maximum
  λ
  for which there exists a subgraph with average degree 
 at least
  λ
  is found.
  
 Given a graph
  G
  in which one wants to find a dense subgraph, construct a directed 
 graph
  H
  from the given graph and then carry out a flow computation on
  H
 .
  H
  has a node 
 for each edge of the original graph, a node for each vertex of the original graph, plus two 
 additional nodes
  s
  and
  t
 . There is a directed edge with capacity one from
  s
  to each node 
 corresponding to an edge of the original graph and a directed edge with infinite capacity 
 from each node corresponding to an edge of the original graph to the two nodes 
 corresponding to the vertices the edge connects. Finally, there is a directed edge with 
 capacity
  λ
  from each node corresponding to a vertex of the original graph to
  t
 .
  
 Notice there are three types of cut sets of the directed graph that have finite capacity, 
 Figure 7.6. The first cuts all arcs from the source. It has capacity
  e
 , the number of edges 
 of the original graph. The second cuts all edges into the sink. It has capacity
  λv
 , where
  v 
 is the number of vertices of the original graph. The third cuts some arcs from
  s
  and some 
 arcs into
  t
 . It partitions the set of vertices and the set of edges of the original graph into 
 two blocks. The first block contains the source node
  s
 , a subset of the edges
  e
 s
 , and a
  
 233",NA
7.12 Spectral clustering applied to social networks,"Finding communities in social networks is different from other clustering for several 
 reasons. First we often want to find communities of size say 20 to 50 in networks with 
 100 million vertices. Second a person is in a number of overlapping communities and 
 thus we are not finding disjoint clusters. Third there often are a number of levels of 
 structure and a set of dominant communities may be hiding a set of weaker 
 communities that are of interest. Spectral clustering is one approach to these issues.
  
 In spectral clustering of the vertices of a graph, one creates a matrix
  V
  whose columns 
 correspond to the first
  k
  singular vectors of the adjacency matrix. 
  
 Each row of
  V
  is 
 the projection of a row of the adjacency matrix to the space spanned by the
  k
  singular 
 vectors. In the example below, the graph has five vertices divided into two cliques, one 
 consisting of the first three vertices and the other the last two vertices. The top two right 
 singular vectors of the adjacency matrix, not normalized to length one, are (1
 ,
  1
 ,
  1
 ,
  0
 ,
  0)
 T 
 and (0
 ,
  0
 ,
  0
 ,
  1
 ,
  1)
 T
 .
  The five rows of the adjacency matrix projected to these vectors form 
 the 5
  ×
  2 matrix in Figure 7.8. Here, there are two ideal clusters with all edges inside a 
 cluster being present including self-loops and all edges between clusters being absent. 
 The five rows project to just two points, depending on which cluster the rows are in. If 
 the clusters were not so ideal and instead of the graph consisting of two disconnected 
 cliques, the graph consisted of two dense subsets of vertices where the two sets were 
 connected by only a few edges, then the singular vectors would not be indicator vectors 
 for the clusters but close to indicator vectors. The rows would be mapped to two clusters 
 of points instead of two points. A
  k
 -means clustering algorithm would find the clusters.
  
 If the clusters were overlapping, then instead of two clusters of points, there would 
 be three clusters of points where the third cluster corresponds to the overlapping 
 vertices of the two clusters. Instead of using
  k
 -means clustering, we might instead find 
 the minimum 1-norm vector in the space spanned by the two singular vectors. The 
 minimum 1-norm vector will not be an indicator vector, so we would threshold its 
 values to create an indicator vector for a cluster. Instead of finding the minimum 1-norm 
 vector in the space spanned by the singular vectors in
  V,
  we might look for a small 1-
 norm vector close to the subspace.
  
 min 
 x
 (1
  − |
 x
 |
 1
  +
  α
  cos(
 θ
 ))
  
 236",NA
7.13 Bibliographic Notes,"Clustering has a long history. For a good general survey, see [Jai10]. For a collection of 
 surveys giving an in-depth treatment of many different approaches to, applications of, 
 and perspectives on clustering, see [HMMR15]; e.g., see [AB15] for a good discussion of 
 center-based clustering. Lloyd’s algorithm for
  k
 -means clustering is from [Llo82], and 
 The
  k
 -means++ initialization method is due to Arthur and Vassilvitskii [AV07]. Ward’s 
 algorithm, from 1963, appears in [War63]. Analysis of the farthest-traversal algorithm 
 for the
  k
 -center problem is due to Gonzalez [Gon85].
  
 Theorem 7.4 is from [KV09]. Analysis of spectral clustering in stochastic models is 
 given in [McS01], and the analysis of spectral clustering without a stochastic model and 
 7.5.2 is due to [KK10].
  
 The definition of approximation-stability is from [BBG13] and [BBV08], and the anal-ysis 
 given in Section 7.6 is due to [BBG13].
  
 Single-linkage clustering goes back at least to Florek et al. [FLP
 +
 51], and Wishart’s 
 robust version is from [Wis69]. Extensions of Theorem 7.8 are given in [BBV08], and 
 the-oretical guarantees for different forms of robust linkage are given in [CD10] and 
 [BLG14]. A good survey of kernel methods in clustering appears in [FCMR08].
  
 Section 7.9 is a simplified version of [KVV04]. Section 7.10 is from [RV99].
  
 239",NA
7.14 Exercises,"Exercise 7.1
  Construct examples where using distances instead of distance squared gives 
 bad results for Gaussian densities. For example, pick samples from two 1-dimensional unit 
 variance Gaussians, with their centers 10 units apart. Cluster these samples by trial and 
 error into two clusters, first according to k-means and then according to the k-median 
 criteria. The k-means clustering should essentially yield the centers of the Gaussians as 
 cluster centers. What cluster centers do you get when you use the k-median criterion?
  
 Exercise 7.2
  Let v
  = (1
 ,
  3)
 . What is the L
 1
  norm of v? The L
 2
  norm? The square of the L
 1
  
 norm?
  
 Exercise 7.3
  Show that in 1-dimension, the center of a cluster that minimizes the sum of 
 distances of data points to the center is in general not unique. Suppose we now require the 
 center also to be a data point; then show that it is the median element (not the mean). 
 Further in 1-dimension, show that if the center minimizes the sum of squared distances to 
 the data points, then it is unique.
  
 Exercise 7.4
  Construct a block diagonal matrix A with three blocks of size 50. Each matrix 
 element in a block has value p
  = 0
 .
 7
  and each matrix element not in a block has value q
  = 
 0
 .
 3
 . Generate a
  150
  ×
  150
  matrix B of random numbers in the range [0,1]. If b
 ij
  ≥ a
 ij
  replace 
 a
 ij
  with the value one. Otherwise replace a
 ij
  with value zero. The rows of A have three 
 natural clusters. Generate a random permutation and use it to permute the rows and 
 columns of the matrix A so that the rows and columns of each cluster are randomly 
 distributed.
  
 1. Apply the k-means algorithm to A with k
  = 3
 . Do you find the correct clusters?
  
 2. Apply the k-means algorithm to A for
  1
  ≤ k ≤
  10
 . Plot the value of the sum of squares to 
 the cluster centers versus k. Was three the correct value for k
 ?
  
 Exercise 7.5
  Let M be a k × k matrix whose elements are numbers in the range [0,1]. A 
 matrix entry close to one indicates that the row and column of the entry correspond to 
 closely related items and an entry close to zero indicates unrelated entities. Develop an 
 algorithm to match each row with a closely related column where a column can be 
 matched with only one row.
  
 Exercise 7.6
  The simple greedy algorithm of Section 7.3 assumes that we know the clus-
 tering radius r. Suppose we do not. Describe how we might arrive at the correct r?
  
 Exercise 7.7
  For the k-median problem, show that there is at most a factor of two ratio 
 between the optimal value when we either require all cluster centers to be data points or 
 allow arbitrary points to be centers.",NA
8 Random Graphs,"Large graphs appear in many contexts such as the World Wide Web, the internet, 
 social networks, journal citations, and other places. What is different about the modern 
 study of large graphs from traditional graph theory and graph algorithms is that here 
 one seeks statistical properties of these very large graphs rather than an exact answer to 
 questions on specific graphs. This is akin to the switch physics made in the late 19
 th 
 century in going from mechanics to statistical mechanics. Just as the physicists did, one 
 formulates abstract models of graphs that are not completely realistic in every situation, 
 but admit a nice mathematical development that can guide what happens in practical 
 situations. Perhaps the most basic model is the
  G
  (
 n, p
 ) model of a random graph. In this 
 chapter, we study properties of the
  G
 (
 n, p
 ) model as well as other models.",NA
8.1 ,NA,NA
The,NA,NA
 G,NA,NA
(,NA,NA
"n, p",NA,NA
),NA,NA
 Model,"The
  G
  (
 n, p
 ) model, due to Erd¨os and R´enyi, has two parameters,
  n
  and
  p
 . Here
  n
  is the 
 number of vertices of the graph and
  p
  is the edge probability. For each pair of distinct 
 vertices,
  v
  and
  w
 ,
  p
  is the probability that the edge (
 v
 ,
 w
 ) is present. The presence of each 
 edge is statistically independent of all other edges. The graph-valued random variable 
 with these parameters is denoted by
  G
  (
 n, p
 ). When we refer to “the graph
  G
  (
 n, p
 )”, we 
 mean one realization of the random variable. In many cases,
  p
  will be a function of
  n 
 such 
 as
  p
  =
  d/n
  for some constant
  d
 . For example, if
  p
  =
  d/n
  then the expected degree of a 
 vertex of the graph is (
 n −
  1)
 d 
 we will often use the approximation that
 n−
 1 
 think of
  n
  as 
 both the total number of vertices and as the number of potential neighbors 
  
  
 n
 ≈ d
 . 
 In order to simplify calculations in this chapter, 
   
  
 n
  
 ≈
  1. In fact, 
 conceptually it is helpful to
  
 of any given node, even though the latter is really
  n −
  1; for all our calculations, when
  n 
 is 
 large, the correction is just a low-order term.
  
 The interesting thing about the
  G
 (
 n, p
 ) model is that even though edges are chosen 
 independently with no “collusion”, certain global properties of the graph emerge from 
 the independent choices. For small
  p
 , with
  p
  =
  d/n
 ,
  d <
  1, each connected component in 
 the graph is small. For
  d >
  1, there is a giant component consisting of a constant fraction 
 of the vertices. In addition, there is a rapid transition at the threshold
  d
  = 1. Below the 
 threshold, the probability of a giant component is very small, and above the threshold, 
 the probability is almost one.
  
 The phase transition at the threshold
  d
  = 1 from very small
  o
 (
 n
 ) size components to a 
 giant Ω(
 n
 ) sized component is illustrated by the following example. Suppose the vertices 
 represent people and an edge means the two people it connects know each other. Given 
 a chain of connections, such as A knows B, B knows C, C knows D, ..., and Y knows Z, we 
 say that A indirectly knows Z. Thus, all people belonging to a connected component of",NA
8.2,NA,NA
Phase Transitions,"Many properties of random graphs undergo structural changes as the edge probability
  
 passes some threshold value. This phenomenon is similar to the abrupt phase transitions in
  
 252",NA
8.3 ,NA,NA
Giant Component,"Consider
  G
 (
 n, p
 ) for
  p
  =
 1+
 ϵn 
  
 where
  ϵ
  is a constant greater than zero. We now show that 
 with high probability, such a graph contains a
  giant component
 , namely a component of 
 size Ω(
 n
 ). Moreover, with high probability, the graph contains only one such component, 
 and all other components are much smaller, of size only
  O
 (log
  n
 ). We begin by arguing 
 existence of a giant component.
  
 8.3.1 Existence of a giant component
  
 To see that with high probability the graph has a giant component, do a depth first 
 search (dfs) on
  G
 (
 n, p
 ) where
  p
  = (1 +
  ϵ
 )
 /n
  with 0
  < ϵ <
  1
 /
 8
 .
  Note that it suffices to 
 consider this range of
  ϵ
  since increasing the value of
  p
  only increases the probability that 
 the graph has a giant component.
  
 To perform the dfs, generate
  
 n
  
 Bernoulli(
 p
 ) independent random bits and answer
  
 2
  
 261",NA
8.4 ,NA,NA
Cycles and Full Connectivity,"This section considers when cycles form and when the graph becomes fully 
 connected. For both of these problems, we look at each subset of
  k
  vertices and see when 
 they form either a cycle or when they form a connected component.
  
 8.4.1 Emergence of Cycles
  
 The emergence of cycles in
  G
  (
 n, p
 ) has a threshold when
  p
  equals to 1
 /n
 . However, the 
 threshold is not sharp.
  
 Theorem 8.9
  The threshold for the existence of cycles in G
  (
 n, p
 )
  is p
  = 1
 /n.
  
 Proof:
  Let
  x
  be the number of cycles in
  G
  (
 n, p
 ). To form a cycle of length
  k
 , the vertices 
  
   
  
 n 
  
 can be selected in 
    
 k 
  
 ways. Given the
  k
  vertices of the cycle, they can be ordered by 
 arbitrarily selecting a first vertex, then a second vertex in one of
  k
 -1 ways, a third in one 
 of
  k −
  2 ways, etc. Since a cycle and its reversal are the same cycle, divide by 2. Thus, 
 there are 
 k
   
  (
 k−
 1)! 
   
 2 
 possible cycles of length
  k
  and
  
 E
  (
 x
 ) =
  
 k
 =3 
  
  
 n 
  
   
 n
  
 k 
  
 (
 k−
 1)! 
   
 2 
  
 p
 k
 ≤
 k
 =3 
    
  
  
 n 
  
  
   
  
 2
 k
 p
 k
  ≤
 k
 =3 
  
   
  
  
  
  
 n 
   
  
   
  
   
  
 (
 np
 )
 k
 = (
 np
 )
 3 
 1
 −
 (
 np
 )
 n−
 2 
   
   
  
   
  
  
  
 1
 −np
  
 ≤
  2(
 np
 )
 3
 ,
  
 provided that
  np <
  1
 /
 2. When
  p
  is asymptotically less than 1
 /n
 , then lim
  
   
 n 
  
  
 n→∞
 np
  = 0 and lim 
   
  
 (
 np
 )
 k
 = 0
 .
  So, as
  n
  goes to infinity,
  E
 (
 x
 ) goes to zero. Thus, the graph almost 
 n→∞k
 =3 
  
 surely has no cycles by the first moment method. A second moment argument can be 
 used to show that for
  p
  =
  d/n
 ,
  d >
  1, a graph will have a cycle with probability tending to 
 one.",NA
8.5 ,NA,NA
Phase Transitions for Increasing Properties,"For many graph properties such as connectivity, having no isolated vertices, having a 
 cycle, etc., the probability of a graph having the property increases as edges are added to 
 the graph. Such a property is called an increasing property.
  Q
  is an
  increasing property 
 of 
 graphs if when a graph
  G
  has the property, any graph obtained by adding edges to
  G 
 must 
 also have the property. In this section we show that any increasing property has a 
 threshold, although not necessarily a sharp one.
  
 The notion of increasing property is defined in terms of adding edges. The following 
 intuitive lemma proves that if
  Q
  is an increasing property, then increasing
  p
  in
  G
  (
 n, p
 ) 
 increases the probability of the property
  Q
 .
  
 Lemma 8.14
  If Q is an increasing property of graphs and
  0
  ≤ p ≤ q ≤
  1
 , then the probability 
 that G
  (
 n, q
 )
  has property Q is greater than or equal to the probability that G
  (
 n, p
 )
  has 
 property Q.
  
 Proof:
  This proof uses an interesting relationship between
  G
  (
 n, p
 ) and
  G
  (
 n, q
 ). Generate 
 G
  (
 n, q
 ) as follows. First generate
  G
  (
 n, p
 ). This means generating a graph on
  n
  vertices
  
 with edge probabilities
  p
 . Then, independently generate another graph
  G n,
 q−p 
  
 and 
  
  
  
 1
 −p 
 take the 
 union by including an edge if either of the two graphs has the edge. Call the resulting 
 graph
  H
 . The graph
  H
  has the same distribution as
  G
  (
 n, q
 ). This follows since the 
 probability that an edge is in
  H
  is
  p
  + (1
  − p
 )
 q−p 
 1
 −p
 =
  q,
  and, clearly, the edges of
  H
  are 
 independent. The lemma follows since whenever
  G
  (
 n, p
 ) has the property
  Q
 ,
  H
  also has 
 the property
  Q
 .
  
 We now introduce a notion called
  replication
 . An
  m
 -fold replication of
  G
 (
 n, p
 ) is a random 
 graph obtained as follows. 
  
 Generate
  m
  independent copies of
  G
 (
 n, p
 ) on the 
 same set of vertices. Include an edge in the
  m
 -fold replication if the edge is in any one of 
 the
  m
  copies of
  G
 (
 n, p
 ). The resulting random graph has the same distribution as 
 G
 (
 n, q
 ) 
 where
  q
  = 1
  −
  (1
  − p
 )
 m
 since the probability that a particular edge is not in the 
 m
 -fold 
 replication is the product of probabilities that it is not in any of the
  m
  copies of
  G
 (
 n, p
 ). If 
 the
  m
 -fold replication of
  G
 (
 n, p
 ) does not have an increasing property
  Q
 , then none of the
  
 m
  copies of
  G
 (
 n, p
 ) has the property. The converse is not true. If no copy has the property, 
 their union may have it. Since
  Q
  is an increasing property and 
 q
  = 1
  −
  (1
  − p
 )
 m
 ≤
  1
  −
  (1
  − 
 mp
 ) =
  mp
  
 Prob
  
 G
 (
 n, mp
 ) has
  Q
  
 ≥
  Prob
  
 G
 (
 n, q
 ) has
  Q
  
 (8.3)
  
 We now show that every increasing property
  Q
  has a phase transition. The transition 
 occurs at the point
  p
 (
 n
 ) at which the probability that
  G
 (
 n, p
 (
 n
 )) has property
  Q
  is
 1 2
 . 
 We 
 will prove that for any function asymptotically less then
  p
 (
 n
 ) that the probability of 
 having property
  Q
  goes to zero as
  n
  goes to infinity.",NA
8.6,NA,NA
Branching Processes,"A
  branching process
  is a method for creating a random tree. Starting with the root 
 node, each node has a probability distribution for the number of its children. The root of 
 the tree is a parent and its descendants are the children with their descendants being 
 the grandchildren. The children of the root are the first generation, their children the 
 second generation, and so on. Branching processes have obvious applications in 
 population stud-ies.
  
 We analyze a simple case of a branching process where the distribution of the 
 number of children at each node in the tree is the same. The basic question asked is what 
 is the probability that the tree is finite, i.e., the probability that the branching process 
 dies out? This is called the
  extinction probability
 .
  
 Our analysis of the branching process will give the probability of extinction, as well as the 
 expected size of the components conditioned on extinction.
  
 An important tool in our analysis of branching processes is the generating func-tion. 
  
 The 
 generating function for a nonnegative integer valued random variable
  y
  is
  
  
  
 ∞
  
 f
  (
 x
 ) = 
   
 p
 i
 x
 i
 where
  p
 i
  is the probability that
  y
  equals
  i
 . The reader not familiar with 
  
  
  
 i
 =0 
  
 generating functions should consult Section 12.9 of the appendix.
  
 Let the random variable
  z
 j
  be the number of children in the
  j
 th
 generation and let 
 f
 j
  (
 x
 ) 
 be the generating function for
  z
 j
 . Then
  f
 1
  (
 x
 ) =
  f
  (
 x
 ) is the generating function for the first 
 generation where
  f
 (
 x
 ) is the generating function for the number of children at a node in 
 the tree. The generating function for the 2
 nd
 generation is
  f
 2
 (
 x
 ) =
  f
  (
 f
  (
 x
 )). In general, the 
 generating function for the
  j
  +1
 st
 generation is given by
  f
 j
 +1
  (
 x
 ) =
  f
 j
  (
 f
  (
 x
 )). To see this, 
 observe two things.
  
 First, the generating function for the sum of two identically distributed integer valued 
 random variables
  x
 1
  and
  x
 2
  is the square of their generating function
  
 f
 2
 (
 x
 ) =
  p
 2 0
 + (
 p
 0
 p
 1
 +
  p
 1
 p
 0
 )
  x
  + (
 p
 0
 p
 2
 +
  p
 1
 p
 1
 +
  p
 2
 p
 0
 )
  x
 2
  +
  · · · .
  
 For
  x
 1
  +
  x
 2
  to have value zero, both
  x
 1
  and
  x
 2
  must have value zero, for
  x
 1
  +
  x
 2
  to have 
 value one, exactly one of
  x
 1
  or
  x
 2
  must have value zero and the other have value one, and 
 so on. In general, the generating function for the sum of
  i
  independent random variables, 
 each with generating function
  f
  (
 x
 ), is
  f
 i
 (
 x
 ).
  
 272",NA
8.7 ,NA,NA
CNF-SAT,"Phase transitions occur not only in random graphs, but in other random structures
  
 as well. An important example is that of satisfiability of Boolean formulas in conjunctive
  
 normal form. A conjunctive normal form (CNF) formula over
  n
  variables
  x
 1
 , . . . , x
 n
  is
  
 an AND of ORs of
  literals
 , where a literal is a variable or its negation. For example, the
  
 following is a CNF formula over the variables
  {x
 1
 , x
 2
 , x
 3
 , x
 4
 }
 :
  
 (
 x
 1
  ∗
  ¯
 x
 2
  ∗ x
 3
 )(
 x
 2
  ∗
  ¯
 x
 4
 )(
 x
 1
  ∗ x
 4
 )(
 x
 3
  ∗ x
 4
 )(
 x
 2
  ∗
  ¯
 x
 3
  ∗ x
 4
 )
 .
  
 Each OR of literals is called a
  clause
 ; for example, the above formula has five clauses. A
  
 k-CNF
  formula is a CNF formula in which each clause has size at most
  k
 , so the above
  
 formula is a 3-CNF formula. An assignment of true/false values to variables is said to
  
 277",NA
8.8 ,NA,NA
Nonuniform Models of Random Graphs,"So far we have considered the
  G
 (
 n, p
 ) random graph model in which all vertices have 
 the same expected degree, and moreover degrees are concentrated close to their 
 expecta-tion. However, large graphs occurring in the real world tend to have
  power law
  
 degree distributions. For a power law degree distribution, the number
  f
 (
 d
 ) of vertices of 
 degree 
 d
  scales as 1
 /d
 α
 for some constant
  α >
  0.
  
 One way to generate such graphs is to stipulate that there are
  f
 (
 d
 ) vertices of degree 
 d
  and choose uniformly at random from the set of graphs with this degree distribution. 
 Clearly, in this model the graph edges are not independent and this makes these random 
 graphs harder to analyze. But the question of when phase transitions occur in random 
 graphs with arbitrary degree distributions is still of interest. In this section, we consider 
 when a random graph with a nonuniform degree distribution has a giant component. 
 Our treatment in this section, and subsequent ones, will be more intuitive without 
 providing rigorous proofs.",NA
8.9,NA,NA
Growth Models,"i
 =0
  
 i
 !
  
 =
  
 i
 =0
  
 i
 !
 −
  
  
 i
 =0
  
 i
 !
 =.
  
 Many graphs that arise in the outside world started as small graphs that grew over 
 time. In a model for such graphs, vertices and edges are added to the graph over time. In 
 such a model there are many ways in which to select the vertices for attaching a new 
 edge. One is to select two vertices uniformly at random from the set of existing vertices. 
 Another is to select two vertices with probability proportional to their degree. This 
 latter method is referred to as preferential attachment. A variant of this method would 
 be to add a new vertex at each unit of time and with probability
  δ
  add an edge where one 
 end of the edge is the new vertex and the other end is a vertex selected with probability 
 proportional to its degree. The graph generated by this latter method is a tree with a 
 power law degree distribution.
  
 286",NA
8.10 Small World Graphs,"In the 1960’s, Stanley Milgram carried out an experiment that indicated that most 
 pairs of individuals in the United States were connected by a short sequence of acquain-
 tances. Milgram would ask a source individual, say in Nebraska, to start a letter on its 
 journey to a target individual in Massachusetts. The Nebraska individual would be given 
 basic information about the target including his address and occupation and asked to 
 send the letter to someone he knew on a first name basis, who was closer to the target 
 individual, in order to transmit the letter to the target in as few steps as possible. Each
  
 294",NA
8.11 Bibliographic Notes,"The
  G
 (
 n, p
 ) random graph model is from Erd¨os R´enyi [ER60]. 
  
 Among the books 
 written on properties of random graphs a reader may wish to consult Frieze and 
 Karonski [FK15], Jansen, Luczak and Ruci´nski [JLR00],or Bollob´as [Bol01]. Material on",NA
8.12 Exercises ,"Exercise 8.1
  Search the World Wide Web to find some real world graphs in machine 
 readable form or data bases that could automatically be converted to graphs.
  
 1. Plot the degree distribution of each graph.
  
 2. Count the number of connected components of each size in each graph.
  
 3. Count the number of cycles in each graph.
  
 4. Describe what you find.
  
 5. What is the average vertex degree in each graph? If the graph were a G
 (
 n, p
 )
  graph, 
  
 what would the value of p be?
  
 6. Spot differences between your graphs and G
 (
 n, p
 )
  for p from Item 5. Look at sizes 
  
 of 
 connected components, cycles, size of giant component.
  
 Exercise 8.2
  In G
 (
 n, p
 )
  the probability of a vertex having degree k is
  
 n
  
 p
 k
 (1
  − p
 )
 n−k
 .
  
 k
  
 1. Show by direct calculation that the expected degree is np.
  
 2. Compute directly the variance of the degree distribution.
  
 3. Where is the mode of the binomial distribution for a given value of p? The mode is 
  
 the point at which the probability is maximum.
  
 Exercise 8.3 
  
 1. Plot the degree distribution for G
 (1000
 ,
  0
 .
 003)
 . 
 2. Plot the degree distribution for G
 (1000
 ,
  0
 .
 030)
 .
  
 n
  
 Exercise 8.4
  To better understand the binomial distribution plot function of k for n
  = 50
  
 and k
  = 0
 .
 05
 ,
  0
 .
 5
 ,
  0
 .
 95
 . For each value of p check the sum over 
  
 k 
  
 p
 k
 (1
  − p
 )
 n−k
 as a all 
 k to ensure that the sum is one.
  
 Exercise 8.5
  In G 
  
 n,
 1 
 n 
  
 , argue that with high probability there is no vertex of degree 
 6 
 log
  n
  
 greater than 
  
 log log
  n
 (i.e.,the probability that such a vertex exists goes to zero as n goes to 
 infinity). You may use the Poisson approximation and may wish to use the fact that k
 !
  ≥
  (
 k 
 e
 )
 k
 .
  
 Exercise 8.6
  The example of Section 8.1.1 showed that if the degrees in G
 (
 n,
 1 
 n
 )
  were 
 independent there would almost surely be a vertex of degree
  Ω(log
  n/
  log log
  n
 )
 . However, 
 the degrees are not independent. Show how to overcome this difficulty.",NA
"9 Topic Models, Nonnegative Matrix Factorization,",NA,NA
"Hidden Markov Models, and Graphical Models","In the chapter on machine learning, we saw many algorithms for fitting functions to 
 data. For example, suppose we want to learn a rule to distinguish spam from nonspam 
 email and we were able to represent email messages as points in
  R
 d
 such that the two 
 categories are linearly separable. Then, we could run the Perceptron algorithm to find a 
 linear separator that correctly partitions our training data. Furthermore, we could argue 
 that if our training sample was large enough, then with high probability, this translates 
 to high accuracy on future data coming from the same probability distribution. An inter-
 esting point to note here is that these algorithms did not aim to explicitly learn a model 
 of the distribution
  D
 +
 of spam emails or the distribution
  D
 −
 of nonspam emails. Instead, 
 they aimed to learn a separator to distinguish spam from nonspam. In this chapter, we 
 look at algorithms that, in contrast, aim to explicitly learn a probabilistic model of the 
 process used to generate the observed data. This is a more challenging problem, and 
 typically requires making additional assumptions about the generative process. For ex-
 ample, in the chapter on high-dimensional space, we assumed data came from a 
 Gaussian distribution and we learned the parameters of the distribution. In the chapter 
 on SVD, we considered the more challenging case that data comes from a mixture of
  k
  
 Gaussian distributions. For
  k
  = 2, this is similar to the spam detection problem, but 
 harder in that we are not told which training emails are spam and which are nonspam, 
 but easier in that we assume
  D
 +
 and
  D
 −
 are Gaussian distributions. In this chapter, we 
 examine other important model-fitting problems, where we assume a specific type of 
 process is used to generate data, and then aim to learn the parameters of this process 
 from observations.",NA
9.1 ,NA,NA
Topic Models,"Topic Modeling is the problem of fitting a certain type of stochastic model to a given 
 collection of documents. The model assumes there exist
  r
  “topics”, that each document is 
 a mixture of these topics, and that the topic mixture of a given document determines the 
 probabilities of different words appearing in the document. For a collection of news arti-
 cles, the topics may be politics, sports, science, etc. A topic is a set of word frequencies. 
 For the topic of politics, words like “president” and “election” may have high 
 frequencies, whereas for the topic of sports, words like “pitcher” and “goal” may have 
 high frequencies. A document (news item) may be 60% politics and 40% sports. In that 
 case, the word frequencies in the document are assumed to be convex combinations of 
 word frequencies for each of these topics with weights 0.6 and 0.4 respectively.
  
 Each document is viewed as a “bag of words” or
  terms
 .
 37
 . Namely, we disregard the 
 order and context in which each word occurs in the document and instead only list the 
 frequency of occurrences of each word. Frequency is the number of occurrences of the
  
  
 37
 In practice, terms are typically words or phrases, and not all words are chosen as terms. For example, 
 articles and simple verbs, pronouns etc. may not be considered terms.
  
 310",NA
9.2 ,NA,NA
An Idealized Model,"The Topic Model inference problem is in general computationally hard. But under 
 certain reasonable assumptions, it can be solved in polynomial time as we will see in this 
 chapter. We start here with a highly idealized model that was historically the first for 
 which a polynomial time algorithm was devised. In this model, we make two 
 assumptions:
  
 The Pure Topic Assumption:
  Each document is purely on a single topic. I.e., each 
 column
  j
  of
  C
  has a single entry equal to 1, and the rest of the entries are 0.
  
 Separability Assumption:
  The sets of terms occurring in different topics are disjoint. 
 I.e., for each row
  i
  of
  B
 , there is a unique column
  l
  with
  b
 il
  ̸
 = 0.
  
 Under these assumptions, the data matrix
  A
  has a block structure. Let
  T
 l
  denote the 
 set of documents on topic
  l
  and
  S
 l
  the set of terms occurring in topic
  l
 . After rearranging 
 columns and rows so that the rows in each
  S
 l
  occur consecutively and the columns of 
 each 
 T
 l
  occur consecutively, the matrix
  A
  looks like:
  
 A
  = 
 S
 1
  
 ∗
  
 ∗
  
 ∗
  
 0 
 T
 1",NA
T O,NA,NA
P,NA,NA
I C,"T
 3
  
 0",NA
T,"T
 2
  
 ∗
  
 ∗
  
 ∗
  
 0 
 ∗
  
 ∗
  
 ∗
  
 0
   
 0
   
 0
   
 0 
 0
 . 
  
 0  
 0 
 ∗
  
 ∗
  
 ∗
  
 0 
 0 
 0 
 0 
 0 
 0 
 0 
 0 
 0 
 0 
 S
 2
  
 0 
 0 
 0 
 0 
 0",NA
E,"∗
  
 ∗
  
 ∗
  
 0 
 ∗
  
 ∗
  
 ∗
  
 0 
 0 
 0 
 0
   
 0
   
 0
  
 ∗
  
 ∗
  
 0 
 0 
 0 
 0 
 0 
 0",NA
R,"0 
 0 
 0 
 0 
 S
 3
  
 0 
 0 
 ∗
  
 ∗
  
 ∗
  
 ∗
  
 ∗
  
 ∗",NA
M,"0 
 0 
 0 
 0 
 0 
 0
 .
  
 0 
 0 
 0 
 0 
 313",NA
9.3 ,NA,NA
Nonnegative Matrix Factorization - NMF,"can be high. Write We saw in Section 9.1, while the expected value
  E
 (
 A|B, C
 ) equals
  BC
 , the 
 variance
  
 A
  =
  BC
  +
  N,
  
 where,
  N
  stands for noise. In topic modeling,
  N
  can be high. But it will be useful to first
  
 look at the problem when there is no noise. This can be thought of as the limiting case
  
 315",NA
9.4 ,NA,NA
NMF with Anchor Terms,"An important case of NMF, which can be solved efficiently, is the case where there 
 are 
 anchor terms
 . An anchor term for a topic is a term that occurs in the topic and does 
 not occur in any other topic. For example, the term “batter” may be an anchor term for 
 the topic baseball and “election” for the topic politics. Consider the case that each topic 
 has an anchor term. This assumption is weaker than the separability assumption of 
 Section 9.2, which says that all terms are anchor terms.
  
 In matrix notation, the assumption that each topic has an anchor term implies that 
 for each column of the term-topic matrix
  B
 , there is a row whose sole nonzero entry is in 
 that column.
  
 Definition 9.3 (Anchor Term)
  For each l
  = 1
 ,
  2
 , . . . r, there is an index i
 l
  such that
  
 b
 i
 l
 ,l
  ̸
 = 0
  
 and
  
 ∗l
 ′
 ̸
 =
  l b
 i
 l
 ,l
 ′
  = 0
  .",NA
9.5 ,NA,NA
Hard and Soft Clustering,"In Section 9.2, we saw that under the assumptions that each document is purely on 
 one topic and each term occurs in only one topic, approximately finding
  B
  was reducible
  
 318",NA
9.6 ,NA,NA
The Latent Dirichlet Allocation Model for Topic Modeling,"The most widely used model for topic modeling is the Latent Dirichlet Allocation 
 (LDA) model. In this model, the topic weight vectors of the documents, the columns of 
 C
 , 
 are picked independently from what is known as a Dirichlet distribution. The term-topic 
 matrix
  B
  is fixed. It is not random. The Dirichlet distribution has a parameter
  µ
 called the 
 “concentration parameter”, which is a real number in (0
 ,
  1), typically set to 1
 /r
 . For each 
 vector
  v
  with
  r
  nonnegative components summing to one,
  
 Prob density ( column
  j
  of
  C
  =
  v
 ) =
  
 1
  
 r
  
 v
 µ−
 1
  
 ,
  
  
 g
 (
 µ
 )
  
 l
 =1
  
 l
  
  
 where,
  g
 (
 µ
 ) is the normalizing constant so that the total probability mass is one. Since
 µ <
  
 1, if any
  v
 l
  = 0, then the probability density is infinite.
  
 Once
  C
  is generated, the Latent Dirichlet Allocation model hypothesizes that the matrix 
  
  
 P
  =
  BC
  
 acts as the probability matrix for the data matrix
  A
 , namely,
  
 E
 (
 A|P
 ) =
  P.
  
 Assume the model picks
  m
  terms from each document. Each trial is according to the 
 multinomial distribution with probability vector
  P
 (:
 , j
 ); so the probability that the first 
 term we pick to include in the document
  j
  is the
  i
 th
 term in the dictionary is
  p
 ij
 . Then, 
 a
 ij
  is 
 set equal to the fraction out of
  m
  of the number of times term
  i
  occurs in document
  j
 .
  
 The Dirichlet density favors low
  v
 l
 , but since the
  v
 l
  have to sum to one, there is at 
 least one component that is high. We show that if
  µ
  is small, then with high probability, 
 the highest entry of the column is typically much larger than the average. So, in each 
 document, one topic, which may be thought as the “primary topic” of the document, gets 
 disproportionately high weight. To prove this, we have to work out some properties of 
 the Dirichlet distribution. The first Lemma describes the marginal probability density of 
 each coordinate of a Dirichlet distributed random variable:
  
 Lemma 9.4
  Suppose the joint distribution of
  y
  = (
 y
 1
 , y
 2
 , . . . , y
 r
 )
  is the Dirichlet distri-bution 
 with concentration parameter µ. Then, the marginal probability density q
 (
 y
 )
  of y
 1 
 is given 
 by
  
  
  
  
 Γ(
 rµ
  + 1) 
  
 q
 (
 y
 ) =Γ(
 µ
 )Γ((
 r −
  1)
 µ
  + 1)
 y
 µ−
 1
 (1
  − y
 )
 (
 r−
 1)
 µ
  , µ ∗
  (0
 ,
  1]
 , 
  
 where,
  Γ
  is the Gamma function (see Appendix for the definition).
  
 320",NA
9.7 ,NA,NA
The Dominant Admixture Model,"In this section, we formulate a model with three key assumptions. The first two are 
 mo-tivated by Latent Dirichlet Allocation, respectively by (1) and (2) of the last section. 
 The third assumption is also natural; it is more realistic than the anchor words 
 assumptions discussed earlier. This section is self-contained and no familiarity with 
 Latent Dirichlet Allocation is needed.
  
 which is the frequency vector of the
  d
  terms in that document.
  m
  is the number of words We 
 first recall the notation.
  A
  is a
  d × n
  data matrix with one document per column,
  
 in each document.
  r
  is the “inner dimension”, i.e.,
  B
  is
  d × r
  and
  C
  is
  r × n
 . We always index 
 topics by
  l
  and
  l
 ′
 , terms by
  i
 , and documents by
  j
 .",NA
9.8 ,NA,NA
Formal Assumptions,"Parameters
  α, β, ρ
  and
  δ
  are real numbers in (0
 ,
  0
 .
 4] satisfying
  
 β
  +
  ρ ≤
  (1
  −
  3
 δ
 )
 α. 
 (9.5)
  
 (1)
  Primary Topic
  There is a partition of [
 n
 ] into
  T
 1
 , T
 2
 , . . . , T
 k
  with:
  
 c
 lj
  
 ≥ α
  
 ≤ β.
  
 for
  j ∗ T
 l 
  
 for
  j /∗ T
 l
 . .
  
 (9.6)
  
 (2)
  Pure Document
  For each
  l
 , there is some
  j
  with
  
 c
 lj
  ≥
  1
  − δ.
  
 (3)
  Catchwords
  For each
  l
 , there is at least one catchword
  i
  satisfying:
  
 b
 il
 ′
  ≤ ρb
 il 
  
 for
  l
 ′
 ̸
 =
  l 
  
 (9.7)
  
 b
 il
  ≥ µ ,
  where,
  µ
  =
 c
  log(10
 nd/δ
 ) 
 , c
  constant
 . 
 (9.8)
  
 Let
  
 r
  
 l
 (
 i
 ) = arg max 
 l
 ′
 =1
 b
 il
 ′
 . 
 (9.9)
  
 Another way of stating the assumption
  b
 il
  ≥ µ
  is that the expected number of times term 
 i
  
 occurs in topic
  l
  among
  m
  independent trials is at least
  c
  log(10
 nd/δ
 )
 /α
 2
 δ
 2
 which grows",NA
9.9 ,NA,NA
Finding the Term-Topic Matrix,"For this, we need an extra assumption, which we first motivate. Suppose as in Section
  
 9.8, we assume that there is a single pure document for each topic. In terms of the Figure
  
 327",NA
9.10,NA,NA
Hidden Markov Models,"A
  hidden Markov model
  (HMM) consists of a finite set of states with a transition
  
 between each pair of states. There is an initial probability distribution
  α
  on the states",NA
9.11 Graphical Models and Belief Propagation,"A graphical model is a compact representation of a probability distribution over
  n 
 variables
  x
 1
 , x
 2
 , . . . , x
 n
 . It consists of a graph, directed or undirected, whose vertices cor-
 respond to variables that take on values from some set. In this chapter, we consider the 
 case where the set of values the variables take on is finite, although graphical models are 
 often used to represent probability distributions with continuous variables. The edges of 
 the graph represent relationships or constraints between the variables.
  
 In the directed model, it is assumed that the directed graph is acyclic. This model 
 represents a joint probability distribution that factors into a product of conditional prob-
  
 abilities.
  
 p
  (
 x
 1
 , x
 2
 , . . . , x
 n
 ) =
  
 n
  
 p
  (
 x
 i
 |
 parents of
  x
 i
 )
  
 i
 =1
  
 The directed graphical model is called a
  Bayesian
  or
  belief network
  and appears 
 frequently in the artificial intelligence and the statistics literature.
  
 The undirected graphical model, called a
  Markov random field
 , can also represent a 
 joint probability distribution of the random variables at its vertices. In many 
 applications the Markov random field represents a function of the variables at the 
 vertices which is to be optimized by choosing values for the variables.
  
 A third model called the
  factor model
  is akin to the Markov random field, but here the 
 dependency sets have a different structure. In the following sections we describe all 
 these models in more detail.
  
 337",NA
9.12 Bayesian or Belief Networks,"A
  Bayesian network
  is a directed acyclic graph where vertices correspond to 
 variables and a directed edge from
  y
  to
  x
  represents a conditional probability
  p
 (
 x|y
 ). If a 
 vertex
  x 
 has edges into it from
  y
 1
 , y
 2
 , . . . , y
 k
 , then the conditional probability is
  p
  (
 x | y
 1
 , y
 2
 , 
 . . . , y
 k
 ). The variable at a vertex with no in edges has an unconditional probability 
 distribution. If the value of a variable at some vertex is known, then the variable is called
  
 evidence
 . An important property of a Bayesian network is that the joint probability is 
 given by the product over all nodes of the conditional probability of the node 
 conditioned on all its immediate predecessors.
  
 In the example of Fig. 9.1, a patient is ill and sees a doctor. The doctor ascertains the 
 symptoms of the patient and the possible causes such as whether the patient was in 
 contact with farm animals, whether he had eaten certain foods, or whether the patient 
 has an hereditary predisposition to any diseases. Using the above Bayesian network 
 where the variables are true or false, the doctor may wish to determine one of two 
 things. What is the marginal probability of a given disease or what is the most likely set 
 of diseases. In determining the most likely set of diseases, we are given a T or F 
 assignment to the causes and symptoms and ask what assignment of T or F to the 
 diseases maximizes the joint probability. This latter problem is called the
  maximum a 
 posteriori probability
  (MAP).
  
 Given the conditional probabilities and the probabilities
  p
  (
 C
 1
 ) and
  p
  (
 C
 2
 ) in Figure 
 9.1, the joint probability
  p
  (
 C
 1
 , C
 2
 , D
 1
 , . . .
 ) can be computed easily for any combination of 
 values of
  C
 1
 , C
 2
 , D
 1
 , . . .
 . However, we might wish to find the value of the variables of 
 highest probability (MAP) or we might want one of the marginal probabilities
  p
  (
 D
 1
 ) or 
 p
  
 (
 D
 2
 ). The obvious algorithms for these two problems require evaluating the probabil-ity
  
 p
  (
 C
 1
 , C
 2
 , D
 1
 , . . .
 ) over exponentially many input values or summing the probability 
 p
  (
 C
 1
 , 
 C
 2
 , D
 1
 , . . .
 ) over exponentially many values of the variables other than those for",NA
9.13 Markov Random Fields,"The Markov random field model arose first in statistical mechanics where it was 
 called the Ising model. It is instructive to start with a description of it. The simplest 
 version of the Ising model consists of
  n
  particles arranged in a rectangular
 √n × √n
  grid. 
 Each particle can have a spin that is denoted
  ±
 1. The energy of the whole system 
 depends on interactions between pairs of neighboring particles. Let
  x
 i
  be the spin,
  ±
 1
 ,
  of 
 the
  i
 th 
 particle. Denote by
  i ∗ j
  the relation that
  i
  and
  j
  are adjacent in the grid. In the Ising 
 model, the energy of the system is given by
  
 f
 (
 x
 1
 , x
 2
 , . . . , x
 n
 ) = exp 
 c 
  
 |x
 i
  − x
 j
 | 
  
 .
  
 i∗j 
  
 The constant
  c
  can be positive or negative. If
  c <
  0, then energy is lower if many adjacent 
 pairs have opposite spins and if
  c >
  0 the reverse holds. The model was first used to 
 model probabilities of spin configurations in physical materials.
  
 In most computer science settings, such functions are mainly used as objective func-
 tions that are to be optimized subject to some constraints. The problem is to find the 
 minimum energy set of spins under some constraints on the spins. Usually the 
 constraints just specify the spins of some particles. Note that when
  c >
  0, this is the 
 problem of minimizing 
 i∗j 
 |x
 i
  − x
 j
 |
  subject to the constraints. The objective function is 
 convex and so this can be done efficiently. If
  c <
  0, however, we need to minimize a 
 concave function for which there is no known efficient algorithm. The minimization of a 
 concave func-tion in general is NP-hard. Intuitively, this is because the set of inputs for 
 which
  f
 (
 x
 ) is less than some given value can be nonconvex or even consist of many 
 disconnected regions.
  
 A second important motivation comes from the area of vision. It has to to do with 
 reconstructing images. Suppose we are given noisy observations of the intensity of light 
 at individual pixels,
  x
 1
 , x
 2
 , . . . , x
 n
 ,
  and wish to compute the true values, the true 
 intensities, of these variables
  y
 1
 , y
 2
 , . . . , y
 n
 . There may be two sets of constraints, the first 
 stipulating that the
  y
 i
  should generally be close to the corresponding
  x
 i
  and the second, a 
 term correcting possible observation errors, stipulating that
  y
 i
  should generally be close 
 to the values of
  y
 j
  for
  j ∗ i
 . This can be formulated as
  
 min 
  
 y
  
 i
  
 |x
 i
  − y
 i
 |
  +
  
 i∗j
  
 |y
 i
  − y
 j
 |
  
 ,
  
 where the values of
  x
 i
  are constrained to be the observed values. The objective function is 
 convex and polynomial time minimization algorithms exist. Other objective functions",NA
9.14 Factor Graphs,"Factor graphs arise when we have a function
  f
  of a variables
  x
  = (
 x
 1
 , x
 2
 , . . . , x
 n
 ) that can be 
 expressed as
  f
  (
 x
 ) = 
  
  
 f
 α
  (
 x
 α
 ) where each factor depends only on some small
  
  
 α
  
 number of variables
  x
 α
 . The difference from Markov random fields is that the variables 
 corresponding to factors do not necessarily form a clique. Associate a bipartite graph 
 where one set of vertices correspond to the factors and the other set to the variables.",NA
9.15 Tree Algorithms,"Let
  f
 (
 x
 ) be a function that is a product of factors. When the factor graph is a tree 
 there are efficient algorithms for solving certain problems. With slight modifications, the 
 algorithms presented can also solve problems where the function is the sum of terms 
 rather than a product of factors.
  
 The first problem is called
  marginalization
  and involves evaluating the sum of
  f
  over 
 all variables except one. In the case where
  f
  is a probability distribution the algorithm 
 computes the marginal probabilities and thus the word marginalization. The second 
 prob-lem involves computing the assignment to the variables that maximizes the 
 function
  f
 . When
  f
  is a probability distribution, this problem is the maximum a posteriori 
 probabil-ity or MAP problem.
  
 If the factor graph is a tree (such as in Figure 9.4), then there exists an efficient al-
 gorithm for solving these problems. Note that there are four problems: the function
  f 
 is 
 either a product or a sum and we are either marginalizing or finding the maximizing 
 assignment to the variables. All four problems are solved by essentially the same algo-
 rithm and we present the algorithm for the marginalization problem when
  f
  is a product.
  
 Assume we want to “sum out” all the variables except
  x
 1
 ,
  leaving a function of
  x
 1
 .
  
 Call the variable node associated with some variable
  x
 i
  node
  x
 i
 . First, make the node 
 x
 1
  the root of the tree. It will be useful to think of the algorithm first as a recursive 
 algorithm and then unravel the recursion. We want to compute the product of all factors 
 occurring in the sub-tree rooted at the root with all variables except the root-variable 
 summed out. Let
  g
 i
  be the product of all factors occurring in the sub-tree rooted at node
  
 x
 i
  with all variables occurring in the subtree except
  x
 i
  summed out. Since this is a tree,
  x
 1
  
 will not reoccur anywhere except the root. Now, the grandchildren of the root are 
 variable nodes and suppose inductively, each grandchild
  x
 i
  of the root, has already 
 computed its
  g
 i
 . It is easy to see that we can compute
  g
 1
  as follows.
  
 Each grandchild
  x
 i
  of the root passes its
  g
 i
  to its parent, which is a factor node. Each 
 child of
  x
 1
  collects all its children’s
  g
 i
 , multiplies them together with its own factor and 
 sends the product to the root. The root multiplies all the products it gets from its 
 children and sums out all variables except its own variable, namely here
  x
 1
 .
  
 Unraveling the recursion is also simple, with the convention that a leaf node just re-
 ceives 1, product of an empty set of factors, from its children. Each node waits until it 
 receives a message from each of its children. After that, if the node is a variable node, it 
 computes the product of all incoming messages, and sums this product function over
  
 341",NA
9.16 Message Passing in General Graphs,"The simple message passing algorithm in the last section gives us the one variable 
 function of
  x
 1
  when we sum out all the other variables. For a general graph that is not
  
 342",NA
9.17 Graphs with a Single Cycle,"The message passing algorithm gives the correct answers on trees and on certain 
 other graphs. One such situation is graphs with a single cycle which we treat here. We 
 switch from the marginalization problem to the MAP problem as the proof of 
 correctness is simpler for the MAP problem. Consider the network in Figure 9.6a with a 
 single cycle. The message passing scheme will count some evidence multiply. The local 
 evidence at A will get passed around the loop and will come back to A. Thus, A will count 
 the local evidence multiple times. If all evidence is multiply counted in equal amounts, 
 then there is a possibility that though the numerical values of the marginal probabilities 
 (beliefs) are wrong, the algorithm still converges to the correct maximum a posteriori 
 assignment.
  
 Consider the unwrapped version of the graph in Figure 9.6b. The messages that the
  
 344",NA
9.18 Belief Update in Networks with a Single Loop,"In the previous section, we showed that when the message passing algorithm converges,
  
 it correctly solves the MAP problem for graphs with a single loop. The message passing
  
 algorithm can also be used to obtain the correct answer for the marginalization problem.
  
 Consider a network consisting of a single loop with variables
  x
 1
 , x
 2
 , . . . , x
 n
  and evidence 
 y
 1
 , y
 2
 , . . . , y
 n
  as shown in Figure 9.7. The
  x
 i
  and
  y
 i
  can be represented by vectors having a 
 component for each value
  x
 i
  can take on. To simplify the discussion assume the
  x
 i
  take on 
 values 1
 ,
  2
 , . . . , m
 .
  
 Let
  m
 i
  be the message sent from vertex
  i
  to vertex
  i
  + 1 mod
  n
 . At vertex
  i
  + 1 each 
 component of the message
  m
 i
  is multiplied by the evidence
  y
 i
 +1
  and the constraint 
 function Ψ. This is done by forming a diagonal matrix
  D
 i
 +1
  where the diagonal elements 
 are the evidence and then forming a matrix
  M
 i
  whose
  jk
 th
 element is Ψ (
 x
 i
 +1
  =
  j, x
 i
  =
  k
 ). The 
 message
  m
 i
 +1
  is
  M
 i
 D
 i
 +1
 m
 i
 .
  Multiplication by the diagonal matrix
  D
 i
 +1
  multiplies the 
 components of the message
  m
 i
  by the associated evidence. Multiplication by the matrix
  
 M
 i
  multiplies each component of the vector by the appropriate value of Ψ and sums over 
 the values producing the vector which is the message
  m
 i
 +1
 . Once the message
  
 346",NA
9.19 Maximum Weight Matching,"We have seen that the belief propagation algorithm converges to the correct solution 
 in trees and graphs with a single cycle. It also correctly converges for a number of prob-
 lems. Here we give one example, the maximum weight matching problem where there is 
 a unique solution.
  
 We apply the belief propagation algorithm to find the maximal weight matching 
 (MWM) in a complete bipartite graph. If the MWM in the bipartite graph is unique, then 
 the belief propagation algorithm will converge to it.
  
 Let
  G
  = (
 V
 1
 , V
 2
 , E
 ) be a complete bipartite graph where
  V
 1
  =
  {a
 1
 , . . . , a
 n
 } , V
 2
  = 
 {b
 1
 , . . . , b
 n
 } ,
  
 and (
 a
 i
 , b
 j
 )
  ∗ E, 
 mutation of
  {
 1
 , . . . , n}
 . The collection of edges 1
  ≤ i, j ≤ n.
  Let
  π
  =
  {π
  (1)
  , . . . 
 , π
  (
 n
 )
 }
  be a per-
  
  
  
  
 a
 1
 , b
 π
 (1) 
  
 , . . . , 
  
 a
 n
 , 
 b
 π
 (
 n
 ) 
  
 is called a 
 matching
  which is denoted by
  π
 . Let
  w
 ij
  be the weight 
 associated with the edge (
 a
 i
 , b
 j
 ).
  
  
  
  
  
 n 
  
 The weight of the matching
  π
  is
  w
 π
  = 
  
   
 w
 iπ
 (
 i
 )
 . The maximum weight matching
  π
 ∗
 is 
  
  
  
 i
 =1
  
 π
 ∗
 = arg max 
  
 w
 π
  
  
 π
  
 The first step is to create a factor graph corresponding to the MWM problem. Each 
 edge of the bipartite graph is represented by a variable
  c
 ij
  which takes on the value zero 
 or one. The value one means that the edge is present in the matching, the value zero 
 means that the edge is not present in the matching. A set of constraints is used to force 
 the set
  
 of edges to be a matching. The constraints are of the form
  
 j
  
 c
 ij
  = 1 and
  
 i
  
 c
 ij
  = 1. Any
  
 0,1 assignment to the variables
  c
 ij
  that satisfies all of the constraints defines a matching. 
 In addition, we have constraints for the weights of the edges.",NA
9.20 Warning Propagation,"Significant progress has been made using methods similar to belief propagation in 
 finding satisfying assignments for 3-CNF formulas. 
  
 Thus, we include a section on a 
 version of belief propagation, called warning propagation, that is quite effective in 
 finding assignments. Consider a factor graph for a SAT problem (Figure 9.10). Index the 
 variables by
  i
 ,
  j
 , and
  k
  and the factors by
  a
 ,
  b
 , and
  c
 . Factor
  a
  sends a message
  m
 ai
  to each 
 variable
  i 
 that appears in the factor
  a
  called a warning. The warning is 0 or 1 depending 
 on whether or not factor
  a
  believes that the value assigned to
  i
  is required for
  a
  to be 
 satisfied. A factor
  a
  determines the warning to send to variable
  i
  by examining all 
 warnings received by other variables in factor
  a
  from factors containing them.
  
 For each variable
  j
 , sum the warnings from factors containing
  j
  that warn
  j
  to take 
 value T and subtract the warnings that warn
  j
  to take value F. If the difference says that 
 j
  
 should take value T or F and this value for variable
  j
  does not satisfy
  a
 , and this is true 
 for all
  j
 , then
  a
  sends a warning to
  i
  that the value of variable
  i
  is critical for factor
  a
 .
  
 Start the warning propagation algorithm by assigning 1 to a warning with probability 
 1/2. Iteratively update the warnings. If the warning propagation algorithm converges, 
 then compute for each variable
  i
  the local field
  h
 i
  and the contradiction number
  c
 i
 . The 
 local field
  h
 i
  is the number of clauses containing the variable
  i
  that sent messages that 
 i
  
 should take value T minus the number that sent messages that
  i
  should take value F. The 
 contradiction number
  c
 i
  is 1 if variable
  i
  gets conflicting warnings and 0 otherwise. If the 
 factor graph is a tree, the warning propagation algorithm converges. If one of the 
 warning messages is one, the problem is unsatisfiable; otherwise it is satisfiable.",NA
9.21 Correlation Between Variables,"In many situations one is interested in how the correlation between variables drops 
 off with some measure of distance. Consider a factor graph for a 3-CNF formula. Measure 
 the distance between two variables by the shortest path in the factor graph. One might 
 ask if one variable is assigned the value true, what is the percentage of satisfying assign-
 ments of the 3-CNF formula in which the second variable also is true. If the percentage is 
 the same as when the first variable is assigned false, then we say that the two variables
  
 351",NA
9.22 Bibliographic Notes,"A formal definition of Topic Models described in this chapter as well as the LDA model 
 are from Blei, Ng and Jordan [BNJ03]; see also [Ble12]. Non-negative Matrix Factoriza-
 tion has been used in several contexts, for example [DS03]. Anchor terms were defined
  
 355",NA
9.23 Exercises,"Exercise 9.1
  Find a nonnegative factorization of the matrix
  
 Indicate the steps in your method and show the intermediate results. 
  
 A
  =
  
  
 1 
  
  
 7 
   
 6 
  
  
 6 
  
  
  
 10
  
 10 
  
 2
  
  
 8 
   
 11 
  
  
  
 3 
  
  
  
 7 
  
  
  
 4
  
  
  
  
  
 4 
  
 6 
   
 5
  
 Exercise 9.2
  Find a nonnegative factorization of each of the following matrices.
  
 (1)
  
  
 2 
  
  
 8 
  
  
 7 
  
  
 5 
  
  
 1 
   
 2 
    
 1 
  
  
  
 7 
  
  
  
 5 
  
  
  
 5 
   
  
 1 
  
  
  
 2 
  
  
  
  
 13 
  
  
  
  
 11 
  
  
   
 11 
  
  
  
  
  
 3
  
 3 
  
 2 
  
  
 11",NA
10 ,NA,NA
Other Topics,NA,NA
10.1 Ranking and Social Choice,"Combining feedback from multiple users to rank a collection of items is an important 
 task. We rank movies, restaurants, web pages, and many other items. Ranking has be-
 come a multi-billion dollar industry as organizations try to raise the position of their 
 web pages in the results returned by search engines to relevant queries. Developing a 
 method of ranking that cannot be easily gamed by those involved is an important task.
  
 A ranking of a collection of items is defined as a complete ordering. For every pair of 
 items
  a
  and
  b
 , either
  a
  is preferred to
  b
  or
  b
  is preferred to
  a
 . Furthermore, a ranking is 
 transitive in that
  a > b
  and
  b > c
  implies
  a > c
 .
  
 One problem of interest in ranking is that of combining many individual rankings 
 into one global ranking. However, merging ranked lists in a meaningful way is nontrivial 
 as the following example illustrates.
  
 Example:
  Suppose there are three individuals who rank items
  a
 ,
  b
 , and
  c
  as illustrated in 
 the following table.
  
 individual
  
 first item
  
 second item
  
 third item
  
 1
  
 a
  
 b
  
 c
  
 2
  
 b
  
 c
  
 a
  
 3
  
 c
  
 a
  
 b
  
 Suppose our algorithm tried to rank the items by first comparing
  a
  to
  b
  and then 
 comparing
  b
  to
  c
 . In comparing
  a
  to
  b
 , two of the three individuals prefer
  a
  to
  b
  and thus 
 we conclude
  a
  is preferable to
  b
 . In comparing
  b
  to
  c
 , again two of the three individuals 
 prefer
  b
  to
  c
  and we conclude that
  b
  is preferable to
  c
 . Now by transitivity one would 
 expect that the individuals would prefer
  a
  to
  c
 , but such is not the case, only one of the 
 individuals prefers
  a
  to
  c
  and thus
  c
  is preferable to
  a
 . We come to the illogical 
 conclusion that
  a
  is preferable to
  b
 ,
  b
  is preferable to
  c
 , and
  c
  is preferable to
  a
 .
  
 Suppose there are a number of individuals or voters and a set of candidates to be 
 ranked. Each voter produces a ranked list of the candidates. From the set of ranked lists 
 can one construct a reasonable single ranking of the candidates? Assume the method of 
 producing a global ranking is required to satisfy the following three axioms.
  
 Nondictatorship
  – The algorithm cannot always simply select one individual’s ranking 
 to use as the global ranking.
  
 Unanimity
  – If every individual prefers
  a
  to
  b
 , then the global ranking must prefer
  a
  to 
 b
 .
  
 360",NA
10.2 Compressed Sensing and Sparse Vectors,"Define a
  signal
  to be a vector
  x
  of length
  d
 , and define a
  measurement
  of
  x
  to be a dot-
 product of
  x
  with some known vector
  a
 i
 . If we wish to uniquely reconstruct
  x
  without 
 any assumptions, then
  d
  linearly-independent measurements are necessary and 
 sufficient.
  
 364",NA
10.3 Applications,"10.3.1 Biological
  
 There are many areas where linear systems arise in which a sparse solution is 
 unique. One is in plant breeding. Consider a breeder who has a number of apple trees 
 and for each tree observes the strength of some desirable feature. He wishes to 
 determine which genes are responsible for the feature so he can crossbreed to obtain a 
 tree that better expresses the desirable feature. This gives rise to a set of equations
  A
 x
  =
  
 b
  where each row of the matrix
  A
  corresponds to a tree and each column to a position on 
 the genone. See Figure 10.4. The vector
  b
  corresponds to the strength of the desired 
 feature in each tree. The solution
  x
  tells us the position on the genone corresponding to 
 the genes that account for the feature. It would be surprising if there were two small 
 independent sets of genes that accounted for the desired feature. Thus, the matrix 
 should have a property that allows only one sparse solution.
  
 368",NA
 Add references ,"Notice that 
 we do not need to know the rank of
  L
  or the elements that were corrupted. All we need 
 is that the low rank matrix
  L
  is not sparse and that the sparse matrix
  R
  is not low rank. 
 We leave the proof as an exercise.
  
 If
  A
  is a small matrix one method to find
  L
  and
  R
  by minimizing
  ||L||
 ∗
  +
  ||R||
 1
  is to find the 
 singular value decomposition
  A
  =
  U
 Σ
 V
 T
 and minimize
  ||
 Σ
 ||
 1
  +
  ||R||
 1
  subject to 
 A
  =
  L
  +
  R
  and
  
 U
 Σ
 V
 T
 being the singular value decomposition of
  A.
  This can be done using Lagrange 
 multipliers (
 ??
 ). Write
  R
  =
  R
 +
 +
  R
 −
 where
  R
 +
 ≥
  0 and
  R
 −
 ≥
  0. Let 
  
  
 n
  
 f
 (
 σ
 i
 , r
 ij
 ) =
  
 i
 =1
  
 σ
 i
  +
  
 ij
  
 r
 + 
 ij
 +
  
 ij
  
 r
 −ij
 .
  
 Write the Lagrange formula 
  
  
 l
  =
  f
 (
 σ
 i
 , r
 ij
 ) +
  σ
 i
 λ
 i
 g
 i
  
 where the
  g
 i
  are the required constraints
  
 1.
  r
 + 
 ij
 ≥
  0 
  
 2.
  r
 −ij
 ≥
  0
  
 3.
  σ
 i
  ≥
  0 
  
 4.
  a
 ij
  =
  l
 ij
  +
  r
 ij
  
 5.
  u
 T i
 u
 j
 =
  
  1
  
 i
  =
  j
  
 0
  
 i ̸
 =
  j
  
 6.
  v
 T i
 v
 j
 =
  
  1
  
 i
  =
  j
  
 0
  
 i ̸
 =
  j
  
 7.
  l
 i
  =
 u
 i
 σ
 i
 v
 T
  
  
 44
 To minimize the absolute value of
  x
  write
  x
  =
  u − v
  and using linear programming minimize
  u
  +
  v 
 subject to
  u ≥
  0 and
  v ≥
  0
 .
  
 369",NA
10.4 An Uncertainty Principle,"Given a function
  x
 (
 t
 )
 ,
  one can represent the function by the composition of sinusoidal 
 functions. Basically one is representing the time function by its frequency components. 
 The transformation from the time representation of a function to it frequency represen-
 tation is accomplished by a Fourier transform. The Fourier transform of a function
  x
 (
 t
 ) is 
 given by
  
 f
 (
 ω
 ) = 
  
 x
 (
 t
 )
 e
 −
 2
 πωt
 dt
  
 Converting the frequency representation back to the time representation is done by the 
 inverse Fourier transformation
  
 x
 (
 t
 ) = 
 f
 (
 ω
 )
 e
 2
 πωt
 dω
  
 In the discrete case,
  x
  = [
 x
 0
 , x
 1
 , . . . , x
 n−
 1
 ] and
  f
  = [
 f
 0
 , f
 1
 , . . . , f
 n−
 1
 ]
 .
  The Fourier trans-form is
  f
  
 =
  A
 x
  with
  a
 ij
  =
 √n
 ω
 ij
 where
  ω
  is the principal
  n
 th
 root of unity. The inverse transform is
  x
  =
  
 B
 f
  where
  B
  =
  A
 −
 1
 has the simple form
  b
 ij
  =
 √n
 ω
 −ij
 .
  
 There are many other transforms such as the Laplace, wavelets, chirplets, etc. In fact, any 
 nonsingular
  n × n
  matrix can be used as a transform.
  
 10.4.1 Sparse Vector in Some Coordinate Basis
  
 sidered as two representations of the same quantity. For example,
  x
  might be a discrete 
 Consider
  A
 x
  =
  b
  where
  A
  is a square
  n × n
  matrix. The vectors
  x
  and
  b
  can be con-
  
 time sequence,
  b
  the frequency spectrum of
  x
 ,
  and the matrix
  A
  the Fourier transform. 
 The quantity
  x
  can be represented in the time domain by
  x
  and in the frequency domain 
 by its Fourier transform
  b
 .
  
 Any orthonormal matrix can be thought of as a transformation and there are many 
 important transformations other than the Fourier transformation. Consider a transfor-
 mation
  A
  and a signal
  x
  in some standard representation. Then
  y
  =
  A
 x
  transforms the 
 signal
  x
  to another representation
  y
 . If
  A
  spreads any sparse signal
  x
  out so that
  
 370",NA
10.5,NA,NA
Gradient,"The gradient of a function
  f
 (
 x
 ) of
  d
  variables,
  x
  = (
 x
 1
 , x
 2
 , . . . , x
 d
 )
 ,
  at a point
  x
 0
  is denoted
  
 ∗f
 (
 x
 0
 ). It is a
  d
 -dimensional vector with components
 ∂f
 (
 x
 0
 )
 ∂f
  
  
 ∂x
 1
 ,
  ∂f
 (
 x
 0
 )
 ∂x
 2
 , . . . ,
  
 ∂f
 (
 x
 0
 )
 ∂x
 d
 ,
  
 where
 ∂x
 i
 are partial derivatives. Without explicitly stating, we assume that the deriva-
 tives referred to exist. The rate of increase of the function
  f
  as we move from
  x
 0
  in a
  
 direction
  u
  is
  ∗f
 (
 x
 0
 )
  ·
  u
 . So the direction of steepest descent is
  −∗f
 (
 x
 0
 ); this is a nat-ural 
 direction to move to minimize
  f
 . But by how much should we move? A large move
  
 may overshoot the minimum. See Figure 10.6. A simple fix is to minimize
  f
  on the line 
 from
  x
 0
  in the direction of steepest descent by solving a one dimensional minimization 
 problem. This gives us the next iterate
  x
 1
  and we repeat. We do not discuss the issue of 
 step-size any further. Instead, we focus on infinitesimal gradient descent, where, the 
 algorithm makes infinitesimal moves in the
  −∗f
 (
 x
 0
 ) direction. Whenever
  ∗
 f
  is not the 
 zero vector, we strictly decrease the function in the direction
  −∗
 f
 , so the current point is 
 not a minimum of the function
  f
 . Conversely, a point
  x
  where
  ∗
 f
  =
  0
  is called a 
 first-order 
 local optimum
  of
  f
 . A first-order local optimum may be a local minimum, local maximum, 
 or a saddle point. We ignore saddle points since numerical error is likely to prevent 
 gradient descent from stoping at a saddle point. 
  
 In general, local minima do not 
 have to be global minima, see Figure 10.6, and gradient descent may converge to a local 
 minimum that is not a global minimum. When the function
  f
  is convex, this is not the case.
  
 A function
  f
  of a single variable
  x
  is said to be convex if for any two points
  a
  and
  b
 , the 
 line joining
  f
 (
 a
 ) and
  f
 (
 b
 ) is above the curve
  f
 (
 ·
 ). A function of many variables is convex if 
 on any line segment in its domain, it acts as a convex function of one variable on the line 
 segment.
  
 Definition 10.1
  A function f over a convex domain is a convex function if for any two points
  
 x
  and
  y
  in the domain, and any λ in
  [0
 ,
  1]
  we have
  
 f
 (
 λ
 x
  + (1
  − λ
 )
 y
 )
  ≤ λf
 (
 x
 ) + (1
  − λ
 )
 f
 (
 y
 )
 .
  
 The function is concave if the inequality is satisfied with ≥ instead of ≤.",NA
10.6 Linear Programming,"Linear programming is an optimization problem that has been carefully studied and 
 is immensely useful. We consider linear programming problem in the following form 
 where 
 A
  is an
  m × n
  matrix,
  m ≤ n,
  of rank
  m
 ,
  c
  is 1
  × n
 ,
  b
  is
  m ×
  1
 ,
  and
  x
  is
  n ×
  1 :
  
 max
  c
  ·
  x
  
 subject to
  
 A
 x
  =
  b
 ,
  x
  ≥
  0
 .
  
 Inequality constraints can be converted to this form by adding slack variables. Also, we 
 can do Gaussian elimination on
  A
  and if it does not have rank
  m
 , we either find that the 
 system of equations has no solution, whence we may stop or we can find and discard 
 redundant equations. After this preprocessing, we may assume that
  A
  ’s rows are inde-
 pendent.
  
 The simplex algorithm is a classical method to solve linear programming problems. It 
 is a vast subject and is well discussed in many texts. Here, we will discuss the ellipsoid 
 algorithm which is in a sense based more on continuous mathematics and is closer to the 
 spirit of this book.
  
 10.6.1 The Ellipsoid Algorithm
  
 The first polynomial time algorithm for linear programming
 45
 was developed by 
 Khachiyan based on work of Iudin, Nemirovsky and Shor and is called the ellipsoid 
 algorithm. The algorithm is best stated for the seemingly simpler problem of determining 
 whether there is a solution to
  A
 x
  ≤
  b
  and if so finding one. The ellipsoid algorithm starts 
 with a large ball in
  d
 -space which is guaranteed to contain the polyhedron
  A
 x
  ≤
  b
 .
  Even 
 though we do not yet know if the polyhedron is empty or non-empty, such a ball can be 
 found. The algorithm checks if the center of the ball is in the polyhedron, if it is, we have 
 achieved our objective. If not, we know from the Separating Hyperplane Theorem of convex 
 geometry that there is a hyperplane called the separating hyperplane through the center of 
 the ball
  
 45
 Although there are examples where the simplex algorithm requires exponential time, it was shown by 
 Shanghua Teng and Dan Spielman that the expected running time of the simplex algorithm on an instance 
 produced by taking an arbitrary instance and then adding small Gaussian perturbations to it is 
 polynomial.",NA
10.7 Integer Optimization,"The problem of maximizing a linear function subject to linear inequality constraints, but 
 with the variables constrained to be integers is called integer programming.
  
 Max
  c
  ·
  x 
  
 subject to
  A
 x
  ≤
  b
  with
  x
 i
  integers
  
 This problem is NP-hard. One way to handle the hardness is to relax the integer con-
 straints, solve the linear program in polynomial time, and round the fractional values to 
 integers. The simplest rounding, round each variable which is 1/2 or more to 1, the rest 
 to 0, yields sensible results in some cases. The vertex cover problem is one of them. The 
 problem is to choose a subset of vertices so that each edge is covered with at least one of 
 its end points in the subset. The integer program is:
  
 Min
  
 i
  
 x
 i
  
 subject to
  x
 i
  +
  x
 j
  ≥
  1
  ∗
  edges (
 i, j
 );
  x
 i
  integers
  .
  
 Solve the linear program. At least one variable for each edge must be at least 1/2 and the 
 simple rounding converts it to one. The integer solution is still feasible. It clearly at most 
 doubles the objective function from the linear programming solution and since the LP 
 solution value is at most the optimal integer programming solution value, we are within 
 a factor of two of the optimal.
  
 377",NA
10.8 Semi-Definite Programming,"Semi-definite programs are special cases of convex programs. Recall that an
  n×n
  ma-
 trix
  A
  is positive semi-definite if and only if
  A
  is symmetric and for all
  x
  ∗
  R
 n
 ,
  x
 T
 A
 x
  ≥
  0. 
 There are many equivalent characterizations of positive semi-definite matrices. We men-
  
 tion one. A symmetric matrix
  A
  is positive semi-definite if and only if it can be expressed
  
 as
  A
  =
  BB
 T
 for a possibly rectangular matrix
  B
 .
  
 A semi-definite program (SDP) is the problem of minimizing a linear function
  c
 T
 x
  
 subject to a constraint that
  F
  =
  F
 0
  +
  F
 1
 x
 1
  +
  F
 2
 x
 2
  +
  · · ·
  +
  F
 d
 x
 d
  is positive semi-definite. Here
  
 F
 0
 , F
 1
 , . . . , F
 d
  are given symmetric matrices.
  
 This is a convex program since the set of
  x
  satisfying the constraint is a convex
  
 set. To see this, note that if
  F
 (
 x
 ) =
  F
 0
  +
  F
 1
 x
 1
  +
  F
 2
 x
 2
  +
  · · ·
  +
  F
 d
 x
 d
  and
  F
 (
 y
 ) = 
 F
 0
  +
  F
 1
 y
 1
  +
  F
 2
 y
 2
  +
  · 
 · ·
  +
  F
 d
 y
 d
  are positive semi-definite, then so is
  F 
 for 0
  ≤ α ≤
  1. that there are more efficient 
 algorithms for SDP’s than general convex programs and that 
  
  
 In principle, 
 SDP’s can be solved in polynomial time.
 α
 x
  + (1
  − α
 )
 y 
 It turns out
  
 many interesting problems can be formulated as SDP’s. We discuss the latter aspect here.
  
 Linear programs are special cases of SDP’s. For any vector
  v
 , let diag(
 v
 ) denote a
  
 diagonal matrix with the components of
  v
  on the diagonal. Then it is easy to see that
  
 the constraints
  v
  ≥
  0
  are equivalent to the constraint diag(
 v
 ) is positive semi-definite. 
 Consider the linear program:
  
 Minimize
  c
 T
 x
  subject to
  A
 x
  =
  b
 ;
  x
  ≥
  0
 .
  
 above to formulate this as an SDP. Rewrite
  A
 x
  =
  b
  as
  A
 x
  −
  b
  ≥
  0
  and
  b
  − A
 x
  ≥
  0
  and use the 
 idea of diagonal matrices
  
 A second interesting example is that of quadratic programs of the form:
  
  
 2 
 Minimize (
 c
 T
  x
 ) 
 d
 T
 x
  
 subject to
  A
 x
  +
  b
  ≥
  0
 .
  
 This is equivalent to
  
  
  
 2 
 Minimize
  t
  subject to
  A
 x
  +
  b
  ≥
  0
  and
  t ≥
 (
 c
 T
  x
 ) 
 d
 T
 x
 .
  
 This is in turn equivalent to the SDP",NA
10.9 Bibliographic Notes,"Arrow’s impossibility theorem, stating that any ranking of three or more items satisfying 
 unanimity and independence of irrelevant alternatives must be a dictatorship, is from 
 [Arr50]. For extensions to Arrow’s theorem on the manipulability of voting rules, see 
 Gibbard [Gib73] and Satterthwaite [Sat75]. A good discussion of issues in social choice 
 appears in [Lis13]. The results presented in Section 10.2.2 on compressed sensing are 
 due to Donoho and Elad [DE03] and Gribonval and Nielsen [GN03]. See [Don06] for 
 more details on issues in compressed sensing. The ellipsoid algorithm for linear 
 programming is due to Khachiyan [Kha79] based on work of Shor [Sho70] and Iudin and 
 Nemirovski [IN77]. For more information on the ellipsoid algorithm and on semi-
 definite programming, see the book of Gr¨otschel, Lov´asz, and Schrijver [GLS12]. The 
 use of SDPs for approximating the max-cut problem is due to Goemans and 
 Williamson[GW95], and the use of SDPs for learning a kernel function is due to [LCB
 +
 04].
  
 380",NA
10.10 Exercises,"Exercise 10.1
  Select a method that you believe is good for combining individual rankings 
 into a global ranking. Consider a set of rankings where each individual ranks b last. One by 
 one move b from the bottom to the top leaving the other rankings in place. Does there exist 
 a v as in Theorem 10.2 where v is the ranking that causes b to move from the bottom to the 
 top in the global ranking. If not, does your method of combing individual rankings satisfy 
 the axioms of unanimity and independence of irrelevant alternatives.
  
 Exercise 10.2
  Show that for the three axioms: non dictator, unanimity, and indepen-dence 
 of irrelevant alternatives, it is possible to satisfy any two of the three.
  
 Exercise 10.3
  Does the axiom of independence of irrelevant alternatives make sense? 
 What if there were three rankings of five items. In the first two rankings, A is number one 
 and B is number two. In the third ranking, B is number one and A is number five. One might 
 compute an average score where a low score is good. A gets a score of 1+1+5=7 and B gets 
 a score of 2+2+1=5 and B is ranked number one in the global ranking. Now if the third 
 ranker moves A up to the second position, A’s score becomes 1+1+2=4 and the global 
 ranking of A and B changes even though no individual ranking of A and B changed. Is there 
 some alternative axiom to replace independence of irrelevant alternatives? Write a 
 paragraph on your thoughts on this issue.
  
 Exercise 10.4
  Prove that in the proof of Theorem 10.2, the global ranking agrees with 
 column v even if item b is moved down through the column.
  
 Exercise 10.5
  Let A be an m by n matrix with elements from a zero mean, unit variance 
 Gaussian. How large must n be for there to be two or more sparse solutions to A
 x
  =
  b 
 with 
 high probability. You will need to define how small s should be for a solution with at most s 
 nonzero elements to be sparse.
  
 Exercise 10.6
  Section 10.2.1 showed that if A is an n × d matrix with entries selected at 
 random from a standard Gaussian, and n ≥
  2
 s, then with probability one there will be a 
 unique s-sparse solution to A
 x
  =
  b
 . Show that if n ≤ s, then with probability one there will
  
 not
  be a unique s-sparse solution. Assume d > s.
  
 Exercise 10.7
  Section 10.2.2 used the fact that n
  =
  O
 (
 s
 2
 log
  d
 )
  rows is sufficient so that if 
 each column of A is a random unit-length n-dimensional vector, then with high
  
 probability all pairwise dot-products of columns will have magnitude less than
  
 1 
  
 2
 s
 . Here,
  
 we show that n
  = Ω(log
  d
 )
  rows is necessary as well. To make the notation less confusing
  
 for this argument, we will use “m” instead of “d”.
  
 Specifically, prove that for m >
  3
 n
 , it is not possible to have m unit-length n-dimensional 
 vectors such that all pairwise dot-products of those vectors are less than
 1 2
 .
  
 Some hints: (1) note that if two unit-length vectors
  u
  and
  v
  have dot-product greater 
  
  
  
 2
 then
  u
 ,
  v
 , and the than or equal to
 1 
 origin form an equilateral triangle). 
 So, it is enough to prove that m >
  3
 n
 unit-length 
 2
 then |
 u
  −
  v
 | ≤
  1
  (if their dot-product is 
 equal to
  1",NA
11 ,NA,NA
Wavelets,"Given a vector space of functions, one would like an orthonormal set of basis 
 functions that span the space. The Fourier transform provides a set of basis functions 
 based on sines and cosines. Often we are dealing with functions that have local structure 
 in which case we would like the basis vectors to have finite support. Also we would like 
 to have an efficient algorithm for computing the coefficients of the expansion of a 
 function in the basis.",NA
11.1 Dilation,"We begin our development of wavelets by first introducing dilation. A
  dilation
  is a 
 mapping that scales all distances by the same factor.
  
  
 ∗
  
  
 A dilation equation is an equation where a function is defined in terms of a linear 
 combination of scaled, shifted versions of itself. For instance,
  
 f
 (
 x
 ) =
  
 d−
 1
  
 c
 k
 f
 (2
 x − k
 )
 .
  
 k
 =0
  
 An example of this is
  f
 (
 x
 ) =
  f
 (2
 x
 ) +
  f
 (2
 x −
  1) which has a solution
  f
 (
 x
 ) equal to one for 0
  ≤ 
 x <
  1 and is zero elsewhere. The equation is illustrated in the figure below. The solid 
 rectangle is
  f
 (
 x
 ) and the dotted rectangles are
  f
 (2
 x
 ) and
  f
 (2
 x −
  1)
 .
  
 f
 (
 x
 )
  
 f
 (2
 x
 ) 
  
 f
 (2
 x −
  1)
  
 0 
  
 1 
  
 2 
 1
  
 Another example is
  f
 (
 x
 ) =
 1 
 in the figure below. The function
  f
 (
 x
 ) is indicated by solid lines. 
 The functions
 1 2
 f
 (2
 x
 ) +
  f
 (2
 x −
  1) +
  1 2
 f
 (2
 x −
  2)
 .
  A solution is illustrated 
 2
 f
 (2
 x
 )
 , f
 (2
 x
  + 1)
 ,
  
 and
 1 2
 f
 (2
 x −
  2) are indicated by dotted lines.
  
 1
  
 f
 (
 x
 )
  
 0
  
 1
  
 2
  
 f
 (2
 x −
  1)
  
 1 
  
 2
 f
 (2
 x
 )
  
 1 
  
 2
 f
 (2
 x −
  2)",NA
11.2 The Haar Wavelet,"φ
  is called a
  scale function
  or
  scale vector
  and is used to generate the two dimensional Let
  
 φ
 (
 x
 ) be a solution to the dilation equation
  f
 (
 x
 ) =
  f
 (2
 x
 )+
 f
 (2
 x−
 1). The function
  
 family of functions,
  φ
 jk
 (
 x
 ) =
  φ
 (2
 j
 x − k
 ), where
  j
  and
  k
  are non-negative integers. Other 
 authors scale
  φ
 jk
  =
  φ
 (2
 j
 x − k
 ) by 2 educational purposes, simplifying the notation for ease of 
 understanding was preferred. 
  
 2
  so that the 2-norm,
  
  ∞−∞
 φ
 2 
 jk
 (
 t
 )
 dt,
  is 1. However, for
  
 For a given value of
  j,
  the shifted versions,
  {φ
 jk
 |k ≥
  0
 },
  span a space
  V
 j
 .
  The spaces 
 V
 0
 , 
 V
 1
 , V
 2
 , . . .
  are larger and larger spaces and allow better and better approximations to a 
 function. The fact that
  φ
 (
 x
 ) is the solution of a dilation equation implies that for any 
 fixed
  j
 ,
  φ
 jk
  is a linear combination of the
  {φ
 j
 +1
 ,k
 ′
 |k
 ′
 ≥
  0
 }
  and this ensures that
  V
 j
  ∗ V
 j
 +1
 . 
 It is 
 for this reason that it is desirable in designing a wavelet system for the scale function to 
 satisfy a dilation equation. For a given value of
  j,
  the shifted
  φ
 jk
  are orthogonal in the",NA
The Haar Wavelet,"φ
 (
 x
 ) =
  
  
 1
  
 0
  ≤ x <
  1
  
 φ
 (
 x
 )
  
 1
  
 x
  
 0
  
 otherwise
  
 ψ
 (
 x
 ) =
  
  
 1
  
 0
  ≤ x <
 1
  
 ψ
 (
 x
 )
  
 -1
  
 1
  
 1
  
   
  
 x
  
 −
 1
  
 1 
  
 2
 ≤ x <
  1
  
  
 0
  
 otherwise
  
  
  
  
    
  
   
 1
  
 1
  
 φ
 (
 x
 )
  
  
    
  
  
 ψ
 (
 x
 )
  
  
  
 1
  
 The basis set becomes
  
 φ
 00
  
 ψ
 10
  
 ψ
 34
  
 ψ
 36
  
 ψ
 48
  
 ψ
 4
 ,
 10
  
 ψ
 4
 ,
 12
  
 ψ
 4
 ,
 14
  
 ψ
 20
  
 ψ
 22
  
 ψ
 30
  
 ψ
 32
  
 ψ
 40
  
 ψ
 42
  
 ψ
 44
  
 ψ
 46
  
 To approximate a function that has only finite support, select a scale vector
  φ
 (
 x
 ) 
 whose scale is that of the support of the function to be represented. Next approximate 
 the function by the set of scale functions
  φ
 (2
 j
 x − k
 )
 , k
  = 0
 ,
  1
 , . . . ,
  for some fixed value of 
 j.
  
 The value of
  j
  is determined by the desired accuracy of the approximation. Basically the
  x
  
 axis has been divided into intervals of size 2
 −j
 and in each interval the function is 
 approximated by a fixed value. It is this approximation of the function that is expressed 
 as a linear combination of the basis functions.
  
 Once the value of
  j
  has been selected, the function is sampled at 2
 j
 points, one in each 
 interval of width 2
 −j
 .
  Let the sample values be
  s
 0
 , s
 1
 , . . . .
  The approximation to the func-
 tion is
 2
 j
 −
 1 
 now is to represent the approximation to the function using the basis vectors 
 rather than 
 k
 =0
 s
 k
 φ
 (2
 j
 x−k
 ) and is represented by the vector (
 s
 0
 , s
 1
  . . . , s
 2
 j
 −
 1
 )
 .
  The problem
  
 the nonorthogonal set of scale functions
  φ
 jk
 (
 x
 )
 .
  This is illustrated in the following example.",NA
11.3 Wavelet Systems,"So far we have explained wavelets using the simple-to-understand Haar wavelet. We 
 now consider general wavelet systems. A wavelet system is built from a basic scaling 
 function
  φ
 (
 x
 )
 ,
  which comes from a dilation equation. Scaling and shifting of the basic 
 scaling function gives a two dimensional set of scaling functions
  φ
 jk
  where
  
 φ
 jk
 (
 x
 ) =
  φ
 (2
 j
 x − k
 )
 .
  
 For a fixed value of
  j,
  the
  φ
 jk
  span a space
  V
 j
 .
  If
  φ
 (
 x
 ) satisfies a dilation equation
  
 φ
 (
 x
 ) =
  
 d−
 1
  
 c
 k
 φ
 (2
 x − k
 )
 ,
  
 k
 =0
  
 then
  φ
 jk
  is a linear combination of the
  φ
 j
 +1
 ,k
 ’s and this implies that
  V
 0
  ∗ V
 1
  ∗ V
 2
  ∗ V
 3
  · · · .",NA
11.4 Solving the Dilation Equation,"Consider solving a dilation equation
  
 φ
 (
 x
 ) =
  
 d−
 1
  
 c
 k
 φ
 (2
 x − k
 )
  
 k
 =0
  
 to obtain the scale function for a wavelet system. Perhaps the easiest way is to assume a 
 solution and then calculate the scale function by successive approximation as in the 
 following program for the Daubechies scale function:
  
 φ
 (
 x
 ) =
 1+ 4
 √
 3
  
 φ
 (2
 x
 ) +
 3+ 4
 √
 3
   
  
 √
 3
 φ
 (2
 x −
  1) +
 3
 −
 4
  
  
 √
 3
 φ
 (2
 x −
  2) +
 1
 −
 4
  
 φ
 (2
 x −
  3)
 ,
  
 The solution will actually be samples of
  φ
 (
 x
 ) at some desired resolution.
  
 Program Compute-Daubechies:
  
 Set the initial approximation to
  φ
 (
 x
 ) by generating a vector whose 
 components approximate the samples of
  φ
 (
 x
 ) at equally spaced values of
  x.
  
 Begin with the coefficients of the dilation equation.
  
 c
 1
  =
 1+ 4
 √
 3
  
 c
 2
  =
 3+ 4
 √
 3
  
 c
 3
  =
 3
 −
 4
 √
 3
  
 c
 4
  =
 1
 −
 4
 √
 3
  
 Execute the following loop until the values for
  φ
 (
 x
 ) converge.
  
 begin
  
 Calculate
  φ
 (2
 x
 ) by averaging successive values of
  φ
 (
 x
 ) together. Fill 
 out the remaining half of the vector representing
  φ
 (2
 x
 ) with zeros.
  
 390",NA
11.5 Conditions on the Dilation Equation,"We would like a basis for a vector space of functions where each basis vector has 
 finite support and the basis vectors are orthogonal. This is achieved by a wavelet system 
 consisting of a shifted version of a scale function that satisfies a dilation equation along 
 with a set of wavelets of various scales and shifts. For the scale function to have a 
 nonzero integral, Lemma 11.1 requires that the coefficients of the dilation equation sum 
 to two. Although the scale function
  φ
 (
 x
 ) for the Haar system has the property that
  φ
 (
 x
 ) 
 and
 φ
 (
 x − k
 )
 , k >
  0
 ,
  are orthogonal, this is not true for the scale function for the dilation 
 equation
  φ
 (
 x
 ) =
 1 
 scale function be orthogonal and that the scale function has finite 
 support puts additional 
 2
 φ
 (2
 x
 )+
 φ
 (2
 x−
 1)+
  1 2
 φ
 (2
 x−
 2)
 .
  The conditions that integer shifts 
 of the
  
 conditions on the coefficients of the dilation equation. These conditions are developed in 
 the next two lemmas.
  
 Lemma 11.2
  Let 
  
  
  
 d−
 1
  
  
 φ
 (
 x
 ) = 
   
 c
 k
 φ
 (2
 x − k
 )
 .
  
 k
 =0
  
 If φ
 (
 x
 )
  and φ
 (
 x − k
 )
  are orthogonal for k ̸
 = 0
  and φ
 (
 x
 )
  has been normalized so that
  ∞−∞
 φ
 (
 x
 )
 φ
 (
 x 
 − k
 )
 dx
  =
  δ
 (
 k
 )
 , then
 d−
 1 
 i
 =0
 c
 i
 c
 i−
 2
 k
  = 2
 δ
 (
 k
 )
 .
  
 Proof:
  Assume
  φ
 (
 x
 ) has been normalized so that
  
  ∞
  
 −∞
 φ
 (
 x
 )
 φ
 (
 x − k
 )
 dx
  =
  δ
 (
 k
 )
 .
  Then
  
  ∞
  
 φ
 (
 x
 )
 φ
 (
 x − k
 )
 dx
  =
  
  ∞
  
 d−
 1
  
 c
 i
 φ
 (2
 x − i
 )
  
 d−
 1
  
 c
 j
 φ
 (2
 x −
  2
 k − j
 )
 dx
  
 i
 =0
  
 j
 =0
  
 x
 =
 −∞
  
 x
 =
 −∞
  
 =
  
 d−
 1
  
 d−
 1
  
 c
 i
 c
 j
  
  ∞
  
 φ
 (2
 x − i
 )
 φ
 (2
 x −
  2
 k − j
 )
 dx
  
 i
 =0
  
 j
 =0
  
 x
 =
 −∞
  
 Since
  
  ∞
  
 x
 =
 −∞
  
  
  ∞
  
 φ
 (2
 x − i
 )
 φ
 (2
 x −
  2
 k − j
 )
 dx
  = 1
  
 = 1
  
 2 
  
 x
 =
 −∞ ∞
  
 φ
 (
 y − i
 )
 φ
 (
 y −
  2
 k − j
 )
 dy
  
 φ
 (
 y
 )
 φ
 (
 y
  +
  i −
  2
 k − j
 )
 dy
  
 x
 =
 −∞
  
 = 1 2
 δ
 (2
 k
  +
  j − i
 )
 ,",NA
Scale and wavelet coefficients equations,"φ
 (
 x
 ) =
 d−
 1 
 k
 =0
 c
 k
 φ
 (2
 x − k
 )
  
 ψ
 (
 x
 ) =
  
 d−
 1
  
 b
 k
 φ
 (
 x − k
 )
  
 k
 =0
  
 ∞
  
 −∞d
 −
 1
  
 φ
 (
 x
 )
 φ
 (
 x − k
 )
 dx
  =
  δ
 (
 k
 ) 
 c
 j
  
 = 2
  
  
 ∞
  
 x
 =
 −∞
  
  
 ∞
  
 φ
 (
 x
 )
 ψ
 (
 x − k
 ) = 
 0
 ψ
 (
 x
 )
 dx
  = 0
  
 j
 =0
  
 x
 =
 −∞
  
  
 ∞
  
 x
 =
 −∞
 ψ
 (
 x
 )
 ψ
 (
 x − k
 )
 dx
  =
  δ
 (
 k
 )
  
 d−
 1
  
 i
 =0 
 (
 −
 1)
 k
 b
 i
 b
 i−
 2
 k
  = 2
 δ
 (
 k
 )
  
 d−
 1 
 j
 =0
  
 c
 j
 c
 j−
 2
 k
  = 2
 δ
 (
 k
 )
  
 c
 k
  = 0 unless 0
  ≤ k ≤ d −
  1 
 d
  
 even
  
 d−
 1 
  
 j
 =0
  
 c
 j
 b
 j−
 2
 k
  = 0
  
 d−
 1
  
 c
 2
 j
  =
  
 d−
 1
  
 c
 2
 j
 +1
  
 d−
 1
  
 b
 j
  = 0
  
 j
 =0
  
 j
 =0
  
 j
 =0
  
 b
 k
  = (
 −
 1)
 k
 c
 d−
 1
 −k
  
 One designs wavelet systems so the above conditions are satisfied.
  
 Lemma 11.2 provides a necessary but not sufficient condition on the coefficients of 
 the dilation equation for shifts of the scale function to be orthogonal. One should note 
 that the conditions of Lemma 11.2 are not true for the triangular or piecewise quadratic
  
 solutions to
  
 and
  
 φ
 (
 x
 ) = 1 2
 φ
 (2
 x
 ) +
  φ
 (2
 x −
  1) + 1 2
 φ
 (2
 x −
  2)
  
 φ
 (
 x
 ) = 1 4
 φ
 (2
 x
 ) + 3 4
 φ
 (2
 x −
  1) + 3 4
 φ
 (2
 x −
  2) + 1 4
 φ
 (2
 x −
  3)
  
 which overlap and are not orthogonal.
  
 For
  φ
 (
 x
 ) to have finite support the dilation equation can have only a finite number of 
 terms. This is proved in the following lemma.
  
 Lemma 11.3
  If
  0
  ≤ x < d is the support of φ
 (
 x
 )
 , and the set of integer shifts, {φ
 (
 x −k
 )
 |k ≥
  0
 }, 
 are linearly independent, then c
 k
  = 0
  unless
  0
  ≤ k ≤ d −
  1
 .
  
 Proof:
  If the support of
  φ
 (
 x
 ) is 0
  ≤ x < d,
  then the support of
  φ
 (2
 x
 ) is 0
  ≤ x <
 d 
 2
 .
  If
  
  
  
 ∞
  
 φ
 (
 x
 ) = 
    
 c
 k
 φ
 (2
 x − k
 ) 
  
 k
 =
 −∞
  
 393",NA
11.6 Derivation of the Wavelets from the Scaling Function,"In a wavelet system one develops a mother wavelet as a linear combination of integer
  
 shifts of a scaled version of the scale function
  φ
 (
 x
 )
 .
  Let the mother wavelet
  ψ
 (
 x
 ) be given
  
  
 d−
 1 
  
 by
  ψ
 (
 x
 ) =
  
 k
 =0 
 b
 k
 φ
 (2
 x − k
 )
 .
  One wants integer shifts of the mother wavelet
  ψ
 (
 x − k
 ) to
  
 be orthogonal and also for integer shifts of the mother wavelet to be orthogonal to the
  
 scaling function
  φ
 (
 x
 )
 .
  These conditions place restrictions on the coefficients
  b
 k
  which are
  
 the subject matter of the next two lemmas.
  
  
 d−
 1 
  
 Lemma 11.4
  (Orthogonality of ψ
 (
 x
 )
  and ψ
 (
 x − k
 )
 ) Let ψ
 (
 x
 ) = 
 k
 =0 
 b
 k
 φ
 (2
 x − k
 )
 . If ψ
 (
 x
 )
  
 and ψ
 (
 x−k
 )
  are orthogonal for k ̸
 = 0
  and ψ
 (
 x
 )
  has been normalized so that k
 )
 dx
  =
  δ
 (
 k
 )
 , then
  
 ∞−∞
 ψ
 (
 x
 )
 ψ
 (
 x−
  
 d−
 1
  
 (
 −
 1)
 k
 b
 i
 b
 i−
 2
 k
  = 2
 δ
 (
 k
 )
 .
  
 i
 =0
  
 Proof:
  Analogous to Lemma 11.2.",NA
11.7 Sufficient Conditions for the Wavelets to be Orthogonal,"Section 11.6 gave necessary conditions on the
  b
 k
  and
  c
 k
  in the definitions of the scale 
 function and wavelets for certain orthogonality properties. In this section we show that 
 these conditions are also sufficient for certain orthogonality conditions. One would like a 
 wavelet system to satisfy certain conditions.
  
 1. Wavelets,
  ψ
 j
 (2
 j
 x − k
 )
 ,
  at all scales and shifts to be orthogonal to the scale function
 φ
 (
 x
 )
 .
  
 2. All wavelets to be orthogonal. That is
  
  ∞
  
 −∞
  
 ψ
 j
 (2
 j
 x − k
 )
 ψ
 l
 (2
 l
 x − m
 )
 dx
  =
  δ
 (
 j − l
 )
 δ
 (
 k − m
 )
  
 3.
  φ
 (
 x
 ) and
  ψ
 jk
 , j ≤ l
  and all
  k,
  to span
  V
 l
 ,
  the space spanned by
  φ
 (2
 l
 x − k
 ) for all
  k. 
 These 
 items are proved in the following lemmas. The first lemma gives sufficient conditions on 
 the wavelet coefficients
  b
 k
  in the definition
  
 ψ
 (
 x
 ) =
  
 k
  
 b
 k
 ψ
 (2
 x − k
 )
  
 for the mother wavelet so that the wavelets will be orthogonal to the scale function. The 
 lemma shows that if the wavelet coefficients equal the scale coefficients in reverse order 
 with alternating negative signs, then the wavelets will be orthogonal to the scale 
 function.
  
 Lemma 11.7
  If b
 k
  = (
 −
 1)
 k
 c
 d−
 1
 −k
 , then
  
  ∞−∞
 φ
 (
 x
 )
 ψ
 (2
 j
 x − l
 )
 dx
  = 0
  for all j and l.
  
 Proof:
  Assume that
  b
 k
  = (
 −
 1)
 k
 c
 d−
 1
 −k
 .
  We first show that
  φ
 (
 x
 ) and
  ψ
 (
 x − k
 ) are orthog-
  
 onal for all values of
  k.
  Then we modify the proof to show that
  φ
 (
 x
 ) and
  ψ
 (2
 j
 x − k
 ) are 
 orthogonal for all
  j
  and
  k.
  
 Assume
  b
 k
  = (
 −
 1)
 k
 c
 d−
 1
 −k
 .
  Then
  
  ∞
  
  ∞d−
 1 
  
 d−
 1
  
 −∞
 φ
 (
 x
 )
 ψ
 (
 x − k
 ) =
  
 −∞i
 =0 
  
  
  
 c
 i
 φ
 (2
 x − i
 )
  
 j
 =0 
  
  
 b
 j
 φ
 (2
 x −
  2
 k − j
 )
 dx
  
 d−
 1 
 d−
 1
  
  ∞
  
 =
  
 i
 =0 
  
 j
 =0 
   
 c
 i
 (
 −
 1)
 j
 c
 d−
 1
 −j
  
 −∞
 φ
 (2
 x − i
 )
 φ
 (2
 x −
  2
 k − j
 )
 dx
  
 d−
 1 
 d−
 1
  
 = 
  
 (
 −
 1)
 j
 c
 i
 c
 d−
 1
 −j
 δ
 (
 i −
  2
 k − j
 )
  
 i
 =0 
  
 j
 =0
  
 d−
 1
  
 = (
 −
 1)
 j
 c
 2
 k
 +
 j
 c
 d−
 1
 −j
  
 j
 =0
  
 =
  c
 2
 k
 c
 d−
 1
  − c
 2
 k
 +1
 c
 d−
 2
  +
  · · ·
  +
  c
 d−
 2
 c
 2
 k−
 1
  − c
 d−
 1
 c
 2
 k
  
 = 0",NA
11.8 Expressing a Function in Terms of Wavelets,"Given a wavelet system with scale function
  φ
  and mother wavelet
  ψ
  we wish to 
 express a function
  f
 (
 x
 ) in terms of an orthonormal basis of the wavelet system. First we 
 will ex-press
  f
 (
 x
 ) in terms of scale functions
  φ
 jk
 (
 x
 ) =
  φ
 (2
 j
 x − k
 )
 .
  To do this we will build a 
 tree similar to that in Figure 11.2 for the Haar system, except that computing the 
 coefficients will be much more complex. Recall that the coefficients at a level in the tree 
 are the coefficients to represent
  f
 (
 x
 ) using scale functions with the precision of the level.
  
 Let
  f
 (
 x
 ) =
 ∞k
 =0
 a
 jk
 φ
 j
 (
 x − k
 ) where the
  a
 jk
  are the coefficients in the expansion of 
 f
 (
 x
 ) using 
 level
  j
  scale functions. Since the
  φ
 j
 (
 x − k
 ) are orthogonal
  
  
  
  
  
  ∞
  
  
  
  
 a
 jk
  = 
  
  
  
  
 x
 =
 −∞
 f
 (
 x
 )
 φ
 j
 (
 x − k
 )
 dx.
  
 Expanding
  φ
 j
  in terms of
  φ
 j
 +1
  yields
  
 a
 jk
  =
  
 =
  
 =
  
  ∞
  
 f
 (
 x
 )
  
 d−
 1
  
 c
 m
 φ
 j
 +1
 (2
 x −
  2
 k − m
 )
 dx
  
 m
 =0
  
 x
 =
 −∞
  
 d−
 1
  
 c
 m
  
  ∞
  
 f
 (
 x
 )
 φ
 j
 +1
 (2
 x −
  2
 k − m
 )
 dx
  
 m
 =0
  
 x
 =
 −∞
  
 d−
 1
  
 c
 m
 a
 j
 +1
 ,
 2
 k
 +
 m
  
 m
 =0
  
 Let
  n
  = 2
 k
  +
  m.
  Now
  m
  =
  n −
  2
 k.
  Then
  
 a
 jk
  =
  
 d−
 1
  
 c
 n−
 2
 k
 a
 j
 +1
 ,n
  
 (11.4)
  
 n
 =2
 k
  
 In construction the tree similar to that in Figure 11.2, the values at the leaves are the 
 values of the function sampled in the intervals of size 2
 −j
 .
  Equation 11.4 is used to 
 compute values as one moves up the tree. The coefficients in the tree could be used if we 
 wanted to represent
  f
 (
 x
 ) using scale functions. However, we want to represent
  f
 (
 x
 ) using 
 one scale function whose scale is the support of
  f
 (
 x
 ) along with wavelets which gives us 
 an orthogonal set of basis functions. To do this we need to calculate the coefficients for 
 the wavelets. The value at the root of the tree is the coefficient for the scale function. We 
 then move down the tree calculating the coefficients for the wavelets.
  
 Finish by calculating wavelet coefficients 
 maybe add material on jpeg
  
 Example:
  Add example using
  D
 4
 . 
  
 Maybe example using sinc",NA
11.9 Designing a Wavelet System,"In designing a wavelet system there are a number of parameters in the dilation equa-
 tion. If one uses
  d
  terms in the dilation equation, one degree of freedom can be used to 
 satisfy
  
 d−
 1
  
 c
 i
  = 2
  
 i
 =0
  
 which insures the existence of a solution with a nonzero mean. Another 
  
 d 
  
  
   
 2
 degrees of freedom are used to satisfy 
  
  
 d−
 1 
  
  
  
 c
 i
 c
 i−
 2
 k
  =
  δ
 (
 k
 ) 
  
  
 i
 =0
  
 which insures the orthogonal properties. The remaining
 d 
 used to obtain some desirable 
 properties such as smoothness. Smoothness appears to be 
 2
 −
  1 degrees of freedom can be
  
 related to vanishing moments of the scaling function. Material on the design of systems is 
 beyond the scope of this book and can be found in the literature.",NA
11.10 Applications,"Wavelets are widely used in data compression for images and speech, as well as in 
 computer vision for representing images. 
  
 Unlike the sines and cosines of the Fourier 
 transform, wavelets have spatial locality in addition to frequency information, which can 
 be useful for better understanding the contents of an image and for relating pieces of 
 different images to each other. Wavelets are also being used in power line 
 communication protocols that send data over highly noisy channels.",NA
11.11 Bibliographic Notes,"In 1909 Alfred Haar presented an orthonormal basis for functions with finite support. 
 Ingrid Daubechies
  
 402",NA
11.12 Exercises,"Exercise 11.1
  Give a solution to the dilation equation f
 (
 x
 ) =
  f
 (2
 x
 )+
 f
 (2
 x−k
 )
  satisfying f
 (0) = 1
 . 
 Assume k is an integer.
  
 Exercise 11.2
  Are there solutions to f
 (
 x
 ) =
  f
 (2
 x
 ) +
  f
 (2
 x −
  1)
  other than a constant multiple of
  
 f
 (
 x
 ) =
  
  
  1
  
 0 
  
 0
  ≤ x <
  1 
 otherwise
 ?
  
 Exercise 11.3
  Is there a solution to f
 (
 x
 ) = 
 f
 (0) =
  
 f
 (1) = 1
  and f
 (2) = 0?
  
 2
 f
 (2
 x
 ) +
  f
 (2
 x −
  1) +
  1 2
 f
 (2
 x −
  2)
  with
  
 Exercise 11.4
  What is the solution to the dilation equation
  
 f
 (
 x
 ) =
  f
 (2
 x
 ) +
  f
 (2
 x −
  1) +
  f
 (2
 x −
  2) +
  f
 (2
 x −
  3)
 .
  
 Exercise 11.5
  Consider the dilation equation
  
 f
 (
 x
 ) =
  f
 (2
 x
 ) + 2
 f
 (2
 x −
  1) + 2
 f
 (2
 x −
  2) + 2
 f
 (2
 x −
  3) +
  f
 (2
 x −
  4)
  
 1. What is the solution to the dilation equation?
  
 2. What is the value of
  
  ∞
  
 −∞
 f
 (
 x
 )
 dx?
  
 Exercise 11.6
  What are the solutions to the following families of dilation equations.
  
 1.
  
 f
 (
 x
 ) =
 f
 (2
 x
 ) +
  f
 (2
 x −
  1)
  
 f
 (
 x
 ) =1 2
 f
 (2
 x
 ) + 1 2
 f
 (2
 x −
  1) + 1 2
 f
 (2
 x −
  2) + 1 2
 f
 (2
 x −
  3)
  
 f
 (
 x
 ) =1 4
 f
 (2
 x
 ) + 1 4
 f
 (2
 x −
  1) + 1 4
 f
 (2
 x −
  2) + 1 4
 f
 (2
 x −
  3) + 1 4
 f
 (2
 x −
  4) + 1 4
 f
 (2
 x −
  5)
  
 + 1 4
 f
 (2
 x −
  6) + 1 4
 f
 (2
 x −
  7)
  
 f
 (
 x
 ) =1 
 kf
 (2
 x
 ) + 1 
 kf
 (2
 x
 ) +
  · · ·
  + 1 
 kf
 (2
 x
 )
  
 2.
  
 f
 (
 x
 ) =1 3
 f
 (2
 x
 ) + 2 3
 f
 (2
 x −
  1) + 2 3
 f
 (2
 x −
  2) + 1 3
 f
 (2
 x −
  3)
  
 f
 (
 x
 ) =1 4
 f
 (2
 x
 ) + 3 4
 f
 (2
 x −
  1) + 3 4
 f
 (2
 x −
  2) + 1 4
 f
 (2
 x −
  3)
  
 f
 (
 x
 ) =1 5
 f
 (2
 x
 ) + 4 5
 f
 (2
 x −
  1) + 4 5
 f
 (2
 x −
  2) + 1 5
 f
 (2
 x −
  3)
  
 f
 (
 x
 ) =1 
 kf
 (2
 x
 ) +
  k −
  1 
 f
 (2
 x −
  1) +
 k −
  1 
 f
 (2
 x −
  2) + 1 
 kf
 (2
 x −
  3)
  
 403",NA
12 ,NA,NA
Appendix,NA,NA
12.1 Definitions and Notation,"nonnegative integers
  
 . . . , −
 3
 , −
 2
 , −
 1
 ,
  
 0
 ,
  1
 ,
  2
 ,
  3
 , . . . 
  
 positive integers
  
  
  
 integers
  
 substructures 
  
 A substring of a string is a continuous string of symbols from the original string. 
  
 A 
 subsequence of a sequence is a sequence of elements from the original sequence in order 
 but not necessarily continuous. With subgraphs there are two possible definitions. We 
 define a subgraph of a graph to be a subset of the vertices and a subset of the edges of the 
 graph induced by the vertices. An induced subgraph is a subset of the vertices and all the 
 edges of the graph induced by the subset of vertices.",NA
12.2 Asymptotic Notation,"We introduce the big
  O
  notation here. A motivating example is analyzing the running 
 time of an algorithm. The running time may be a complicated function of the input 
 length 
 n
  such as 5
 n
 3
 + 25
 n
 2
 ln
  n −
  6
 n
  + 22
 .
  Asymptotic analysis is concerned with the 
 behavior as
  n → ∞
  where the higher order term 5
 n
 3
 dominates. Further, the coefficient 5 
 of 5
 n
 3 
 is not of interest since its value varies depending on the machine model. So we say 
 that the function is
  O
 (
 n
 3
 ). The big
  O
  notation applies to functions on the positive integers 
 taking on positive real values.
  
 Definition 12.1
  For functions f and g from the natural numbers to the positive reals, f
 (
 n
 )
  is 
 O
 (
 g
 (
 n
 ))
  if there exists a constant c >0 such that for all n, f
 (
 n
 )
  ≤ cg
 (
 n
 )
 .
  
 For example, in this case
  f
 (
 n
 ) is also
  O
 (
 n
 4
 ). Note in our definition we require
  g
 (
 n
 ) to be Thus,
  
 f
 (
 n
 ) = 5
 n
 3
 + 25
 n
 2
 ln
  n −
  6
 n
  + 22 is
  O
 (
 n
 3
 ). The upper bound need not be tight.
  
 strictly greater than 0 for all
  n
 .
  
 To say that the function
  f
 (
 n
 ) grows at least as fast as
  g
 (
 n
 ), one uses a notation Omega. 
 For positive real valued
  f
  and
  g
 ,
  f
 (
 n
 ) is Ω(
 g
 (
 n
 )) if there exists a constant
  c >
  0 such that 
 for all
  n
 ,
  f
 (
 n
 )
  ≥ cg
 (
 n
 ). If
  f
 (
 n
 ) is both
  O
 (
 g
 (
 n
 )) and Ω(
 g
 (
 n
 )), then
  f
 (
 n
 ) is Θ(
 g
 (
 n
 )). Theta is 
 used when the two functions have the same asymptotic growth rate.
  
 Many times one wishes to bound the low order terms. To do this, a notation called 
  
  
  
 f
 (
 n
 ) 
  
 little
  o
  is used. We say
  f
 (
 n
 ) is
  o
 (
 g
 (
 n
 )) if lim 
   
 g
 (
 n
 )
 = 0
 .
  Note that
  f
 (
 n
 ) being
  O
 (
 g
 (
 n
 )) 
  
  
 n→∞
  
 means that asymptotically
  f
 (
 n
 ) does not grow faster than
  g
 (
 n
 )
 ,
  whereas
  f
 (
 n
 ) being 
 o
 (
 g
 (
 n
 )) 
 means that asymptotically
  f
 (
 n
 )
 /g
 (
 n
 ) goes to zero. If
  f
 (
 n
 ) = 2
 n
  +
 √n
 , then
  f
 (
 n
 )",NA
12.3 Useful Relations,"Summations
  
 n
  
 i
 =0
  
 ∞
  
 i
 =0
  
 ∞
  
 i
 =0
  
 ∞
  
 i
 =0
  
 n
  
 i
 =1
  
 n
  
 i
 =1
  
 ∞
  
 i
 =1
  
 a
 i
 = 1 +
  a
  +
  a
 2
 +
  · · ·
  = 1
  − a
 n
 +1 
 1
  − a ,
  
 a ̸
 = 1
  
 a
 i
 = 1 +
  a
  +
  a
 2
 +
  · · ·
  =
  
  
 1 
  
 1
  − a,
  
 |a| <
  1
  
 ia
 i
 =
  a
  + 2
 a
 2
 + 3
 a
 3
 · · ·
  =
  
  
 a 
  
 (1
  − a
 )
 2
 ,
  
 |a| <
  1
  
 i
 2
 a
 i
 =
  a
  + 4
 a
 2
 + 9
 a
 3
 · · ·
  =
 a
 (1 +
  a
 ) (1
  − 
 a
 )
 3
  ,
  
 |a| <
  1
  
 i
  =
 n
 (
 n
  + 1) 
  
  
 2
  
 i
 2
 =
 n
 (
 n
  + 1)(2
 n
  + 1) 
  
  
 6
  
 i
 2
  =
  π
 2
  
 We prove one equality:
  
 ∞
  
 i
 =0
  
 ia
 i
 =
  a
  + 2
 a
 2
 + 3
 a
 3
 · · ·
  =
  
  
 a 
  
 (1
  − a
 )
 2
 ,
  provided
  |a| <
  1
 .
  
 Proof: Write
  S
  =
  
 ∞
  
 ia
 i
 .
  So,
  
 i
 =0
  
 Thus,
  
 S − aS
  =
  
 aS
  =
  
 ∞
  
 ia
 i
 +1
 =
  
 ∞
  
 (
 i −
  1)
 a
 i
 .
  
  
 a 
  
 1
  − a,
  
 i
 =0
  
 i
 =1
  
 ∞
  
 i
 =1
  
 ia
 i
 −
  
 ∞
  
 (
 i −
  1)
 a
 i
 =
  
 ∞
  
 a
 i
 =
  
 i
 =1
  
 i
 =1
  
  
 from which the equality follows. The sum
  
 i
  
 method (left to the reader). Using generating functions, we will see another proof of both 
 these equalities by derivatives.
  
 ∞
  
  
 1
  
 =1+ 
  1
  
 +
  
  
  1 
 +
  1
  
 +
  
  
  1
  
  1
  
  1
  
 +
  1
  
  
  
 + +
 1
  
 +
  1
  
 + ndthu dvere.
  
  
 i
 =1
  
 i
 =1+ 
 2
 +
  
 3
 +
  
 +
  
 5
  
 6
  
 7
 +
  
  
 +
  · · · ≥
  +
  
 2
 +
  
 2
 +
  · · ·
  ndthu dvere.
  
  
 408",NA
12.4 Useful Inequalities,"1 +
  x ≤ e
 x
 for all real
  x
 .
  
 One often establishes an inequality such as 1 +
  x ≤ e
 x
 by showing that the dif-
 ference of the two sides, namely
  e
 x
 −
  (1 +
  x
 ), is always positive. This can be done by 
 taking derivatives. The first and second derivatives are
  e
 x
 −
  1 and
  e
 x
 . Since
  e
 x 
 is 
 always positive,
  e
 x
 −
  1 is monotonic and
  e
 x
 −
  (1 +
  x
 ) is convex. Since
  e
 x
 −
  1 is 
 monotonic, it can be zero only once and is zero at
  x
  = 0. Thus,
  e
 x
 −
  (1 +
  x
 ) takes on 
 its minimum at
  x
  = 0 where it is zero establishing the inequality.
  
 (1
  − x
 )
 n
 ≥
  1
  − nx
  for 0
  ≤ x ≤
  1
  
 Let
  g
 (
 x
 ) = (1
  − x
 )
 n
 −
  (1
  − nx
 )
 .
  We establish
  g
 (
 x
 )
  ≥
  0 for
  x
  in [0
 ,
  1] by taking the 
 derivative.
  
 g
 ′
 (
 x
 ) =
  −n
 (1
  − x
 )
 n−
 1
 +
  n
  =
  n 
  
 1
  −
  (1
  − x
 )
 n−
 1
  
 ≥
  0
  
 for 0
  ≤ x ≤
  1
 .
  Thus,
  g
  takes on its minimum for
  x
  in [0
 ,
  1] at
  x
  = 0 where
  g
 (0) = 0 
 proving the inequality.
  
 (
 x
  +
  y
 )
 2
 ≤
  2
 x
 2
 + 2
 y
 2
  
 The inequality follows from (
 x
  +
  y
 )
 2
 + (
 x − y
 )
 2
 = 2
 x
 2
 + 2
 y
 2
 .
  
 Lemma 12.1
  For any nonnegative reals a
 1
 , a
 2
 , . . . , a
 n
  and any ρ ∗
  [0
 ,
  1]
 , 
  
 i
 .
  
 n 
  
 i
 =1
 a
 i
  
 ρ
  ≤
  
 Proof:
  We will see that we can reduce the proof of the lemma to the case when only one
  
 of the
  a
 i
  is nonzero and the rest are zero. To this end, suppose
  a
 1
  and
  a
 2
  are both positive
  
 and without loss of generality, assume
  a
 1
  ≥ a
 2
 . Add an infinitesimal positive amount
  ϵ
 to
  a
 1
  
 and subtract the same amount from
  a
 2
 . This does not alter the left hand side. We
  
 claim it does not increase the right hand side. To see this, note that
  
 (
 a
 1
  +
  ϵ
 )
 ρ
 + (
 a
 2
  − ϵ
 )
 ρ
 − a
 ρ
 1
 − a
 ρ
 2
 =
  ρ
 (
 a
 ρ−
 1
  
 − a
 ρ−
 1
  
 )
 ϵ
  +
  O
 (
 ϵ
 2
 )
 ,
  
 and since
  ρ −
  1
  ≤
  0, we have
  a
 ρ−
 1
 − a
 ρ−
 1
 ≤
  0, proving the claim. Now by repeating this 
 process, we can make
  a
 2
  = 0 (at that time
  a
 1
  will equal the sum of the original
  a
 1
  and
  
 a
 2
 ). Now repeating on all pairs of
  a
 i
 , we can make all but one of them zero and in the
  
 process, we have left the left hand side the same, but have not increased the right hand",NA
12.5 Probability,"Consider an experiment such as flipping a coin whose outcome is determined by 
 chance. To talk about the outcome of a particular experiment, we introduce the notion of 
 a
  ran-dom variable
  whose value is the outcome of the experiment. The set of possible 
 outcomes is called the
  sample space
 . If the sample space is finite, we can assign a 
 probability of occurrence to each outcome. In some situations where the sample space is 
 infinite, we can
  
  
 6
  
 1
  
 assign a probability of occurrence. The probability
  p
  (
 i
 ) =
  
 π
 2
  
 i
 2
  for
  i
  an integer greater
  
 than or equal to one is such an example. The function assigning the probabilities is called
  
 a
  probability distribution function
 .
  
 In many situations, a probability distribution function does not exist. For example, 
 for the uniform probability on the interval [0,1], the probability of any specific value is 
 zero. What we can do is define a
  probability density function p
 (
 x
 ) such that
  
 b
  
 Prob(
 a < x < b
 ) = 
  
 p
 (
 x
 )
 dx
  
 a
  
 If
  x
  is a continuous random variable for which a density function exists, then the
  cumu-
 lative distribution function f
  (
 a
 ) is defined by
  
  
  
  a 
  
  
 f
 (
 a
 ) = 
  
  
 p
 (
 x
 )
 dx
  
  
 −∞
  
 which gives the probability that
  x ≤ a
 .
  
 12.5.1 Sample Space, Events, and Independence
  
 There may be more than one relevant random variable in a situation. For example, if 
 one tosses
  n
  coins, there are
  n
  random variables,
  x
 1
 , x
 2
 , . . . , x
 n
 , taking on values 0 and 1, a 
 1 for heads and a 0 for tails. The set of possible outcomes, the sample space, is
  {
 0
 ,
  1
 }
 n
 . An
  
 event
  is a subset of the sample space. The event of an odd number of heads, consists of 
 all elements of
  {
 0
 ,
  1
 }
 n
 with an odd number of 1’s.",NA
12.6 Bounds on Tail Probability,"12.6.1 Chernoff Bounds
  
 Markov’s inequality bounds the probability that a nonnegative random variable exceeds
  
 a value
  a.
  
 or
  
 p
 (
 x ≥ a
 )
  ≤E
 (
 x
 )
  
 .
  
 p
  
 x ≥ aE
 (
 x
 )
  
 ≤
 1
  
 If one also knows the variance,
  σ
 2
 , then using Chebyshev’s inequality one can bound the 
 probability that a random variable differs from its expected value by more than
  a
  
 standard deviations. Let
  m
  =
  E
 (
 x
 ). Then Chebyshev’s inequality states that
  
 p
 (
 |x − m| ≥ aσ
 )
  ≤
 1 
 a
 2
  
 If a random variable
  s
  is the sum of
  n
  independent random variables
  x
 1
 , x
 2
 , . . . , x
 n
  of finite 
 variance, then better bounds are possible. Here we focus on the case where the 
 n
  
 independent variables are binomial. In the next section we consider the more general 
 case where we have independent random variables from any distribution that has a 
 finite variance.
  
 Let
  x
 1
 , x
 2
 , . . . , x
 n
  be independent random variables where
  
 x
 i
  =
  
  0
  
 Prob 1
  − p 
 Prob 
  
 p
  
 .
  
 1
  
 n
  
 Consider the sum
  s
  =
  
 x
 i
 . Here the expected value of each
  x
 i
  is
  p
  and by linearity
  
 i
 =1
  
 of expectation, the expected value of the sum is
  m
 =
 np
 .
  
 Chernoff bounds bound the
  
 probability that the sum
  s
  exceeds (1 +
  δ
 )
  m
  or is less than (1
  − δ
 )
  m
 . We state these 
 bounds as Theorems 12.3 and 12.4 below and give their proofs.
  
 Theorem 12.3
  For any δ >
  0
 , Prob
  
 s >
  (1 +
  δ
 )
 m
  
 <
  
 e
 δ
  
 m
  
  
 (1+
 δ
 )
 (1+
 δ
 )
  
 Theorem 12.4
  Let
  0
  < γ ≤
  1
 , then Prob
  
 s <
  (1
  − γ
 )
 m
  
 <
  
 e
 −γ
  
 m
  
 < e
 −
  γ
 2
 m 
 2
  .
  
 (1+
 γ
 )
 (1+
 γ
 )
  
 Proof (Theorem 12.3):
  For any
  λ >
  0, the function
  e
 λx
 is monotone. Thus,
  
 Prob
  
 s >
  (1 +
  δ
 )
 m
  
 = Prob
  
 e
 λs
 > e
 λ
 (1+
 δ
 )
 m
  
 .
  
 e
 λx
 is nonnegative for all
  x
 , so we can apply Markov’s inequality to get
  
 Prob
  
 e
 λs
 > e
 λ
 (1+
 δ
 )
 m
  
 ≤ e
 −λ
 (1+
 δ
 )
 m
 E
  
 e
 λs
  
 .
  
 430",NA
12.7,NA,NA
Applications of the Tail Bound,"Chernoff Bounds 
  
 Chernoff bounds deal with sums of Bernoulli random variables. Here we apply Theo-rem 
 12.5 to derive these.
  
 Theorem 12.6
  Suppose y
 1
 , y
 2
 , . . . , y
 n
  are independent 0-1 random variables with E
 (
 y
 i
 ) = 
 p 
 for all i. Let y
  =
  y
 1
  +
  y
 2
  +
  · · ·
  +
  y
 n
 . Then for any c ∗
  [0
 ,
  1]
 ,
  
 Prob
  
 |y − E
 (
 y
 )
 | ≥ cnp
  
 ≤
  3
 e
 −npc
 2
 /
 8
 .
  
 Proof:
  Let
  x
 i
  =
  y
 i
  − p
 . Then,
  E
 (
 x
 i
 ) = 0 and
  E
 (
 x
 2 
 i
 ) =
  E
 (
 y − p
 )
 2
  =
  p
 . For
  s ≥
  3,
  
 |E
 (
 x
 s i
 )
 |
  =
  |E
 (
 y
 i
 − p
 )
 s
 |
  
 =
  |p
 (1
  − p
 )
 s
 + (1
  − p
 )(0
  − p
 )
 s
 |
  
 = 
 p
 (1
  − p
 ) (1
  − p
 )
 s−
 1
 + (
 −p
 )
 s−
 1
  
 ≤ p.
  
 Apply Theorem 12.5 with
  a
  =
  cnp
 . Noting that
  a <√
 2
  np
 , completes the proof.
  
 Section (12.6.1) contains a different proof that uses a standard method based on 
 moment-generating functions and gives a better constant in the exponent.
  
 Power Law Distributions 
  
 The power law distribution of order
  k
  where
  k
  is a positive integer is
  
 f
 (
 x
 ) =
 k −
  1 
  
  
 x
 k
  
 for
  
 x ≥
  1
 .
  
 If a random variable
  x
  has this distribution for
  k ≥
  4, then
  
 µ
  =
  E
 (
 x
 ) =
 k −
  1 
  
 k −
  2
  
 and
  
 Var(
 x
 ) =
   
 k −
  1 
  
 (
 k −
  2)
 2
 (
 k −
  3)
 .
  
 436",NA
12.8,NA,NA
Eigenvalues and Eigenvectors,"nonzero vector
  x
  satisfying the equation
  A
 x
  =
  λ
 x
 . The vector
  x
  is called the eigenvector Let
  A
  
 be an
  n×n
  real matrix. The scalar
  λ
  is called an eigenvalue of
  A
  if there exists a
  
 of
  A
  associated with
  λ
 . The set of all eigenvectors associated with a given eigenvalue 
 form a subspace as seen from the fact that if
  A
 x
  =
  λ
 x
  and
  A
 y
  =
  λ
 y
 , then for any scalars
  c 
 and
  d
 ,
  A
 (
 c
 x
  +
  d
 y
 ) =
  λ
 (
 c
 x
  +
  d
 y
 ). The equation
  A
 x
  =
  λ
 x
  has a nontrivial solution only if det(
 A 
 − λI
 ) = 0. The equation det(
 A − λI
 ) = 0 is called the
  characteristic
  equation and has
  n
  not 
 necessarily distinct roots.
  
 Matrices
  A
  and
  B
  are similar if there is an invertible matrix
  P
  such that
  A
  =
  P
 −
 1
 BP
 .",NA
12.9,NA,NA
Generating Functions,"∞
  
 a
 i
 x
 i
 .
  The
  
 A sequence
  a
 0
 , a
 1
 , . . .
 , can be represented by a generating function
  g
 (
 x
 ) =
  
  
 i
 =0 
  
 advantage of the generating function is that it captures the entire sequence in a closed
  
 form that can be manipulated as an entity. For example, if
  g
 (
 x
 ) is the generating func-
  
 tion for the sequence
  a
 0
 , a
 1
 , . . .
 , then
  x
 d dx
 g
 (
 x
 ) is the generating function for the sequence
  
 0
 , a
 1
 ,
  2
 a
 2
 ,
  3
 a
 3
 , . . .
  and
  x
 2
 g
 ′′
 (
 x
 ) +
  xg
 ′
 (
 x
 ) is the generating function for the sequence for
  
 0
 , a
 1
 ,
  4
 a
 2
 ,
  9
 a
 3
 , . . .",NA
12.10 Miscellaneous,"12.10.1 Lagrange multipliers
  
 Lagrange multipliers are used to convert a constrained optimization problem into an un-
 constrained optimization. Suppose we wished to maximize a function
  f
 (
 x
 ) subject to a 
 constraint
  g
 (
 x
 ) =
  c
 . The value of
  f
 (
 x
 ) along the constraint
  g
 (
 x
 ) =
  c
  might increase for a 
 while and then start to decrease. At the point where
  f
 (
 x
 ) stops increasing and starts to 
 decrease, the contour line for
  f
 (
 x
 ) is tangent to the curve of the constraint
  g
 (
 x
 ) =
  c
 . 
 Stated another way the gradient of
  f
 (
 x
 ) and the gradient of
  g
 (
 x
 ) are parallel.
  
 g
  =
  c
 . These two conditions hold if and only if By introducing a new variable
  λ
  we can 
 express the condition by
  ∗
 x
 f
  =
  λ∗
 x
 g
  and
  
 ∗
 x
 λ
  
 f
  (
 x
 ) +
  λ
  (
 g
  (
 x
 )
  − c
 ) = 0
  
 The partial with respect to
  λ
  establishes that
  g
 (
 x
 ) =
  c
 . We have converted the constrained 
 optimization problem in
  x
  to an unconstrained problem with variables
  x
  and
  λ
 .
  
 456",NA
12.11 Exercises,"Exercise 12.1
  What is the difference between saying f
 (
 n
 )
  is O
  (
 n
 3
 )
  and f
 (
 n
 )
  is o
  (
 n
 3
 )
 ?
  
 Exercise 12.2
  If f
  (
 n
 )
  ∗ g
  (
 n
 )
  what can we say about f
 (
 n
 ) +
  g
 (
 n
 )
  and f
 (
 n
 )
  − g
 (
 n
 )
 ?
  
 Exercise 12.3
  What is the difference between ∗ and
  Θ
 ?
  
 Exercise 12.4
  If f
  (
 n
 )
  is O
  (
 g
  (
 n
 ))
  does this imply that g
  (
 n
 )
  is
  Ω (
 f
  (
 n
 ))
 ?
  
 Exercise 12.5
  What is
  lim 
 k→∞
  
  k−
 1 
 k−
 2
  
 k−
 2
 .
  
 Exercise 12.6
  Select a, b, and c uniformly at random from
  [0
 ,
  1]
 . The probability that b < a 
 is
 1
 /
 2
 . The probability that c<a is
 1
 /
 2
 . However, the probability that both b and c are less 
 than a is
 1 3
 not
  1
 /
 4
 . Why is this? Note that the six possible permutations abc, acb, bac, cab, 
 bca, and cba, are all equally likely. Assume that a, b, and c are drawn from the interval 
 (0,1]. Given that b < a, what is the probability that c < a?
  
 Exercise 12.7
  Let A
 1
 , A
 2
 , . . . , A
 n
  be events. Prove that Prob
 (
 A
 1
 ∗A
 2
 ∗· · · A
 n
 )
  ≤
  
 n
  
 Prob
 (
 A
 i
 )
  
 i
 =1
  
 460",NA
Index ,"2-universal, 184 Combining expert advice, 162
  
 4-way independence, 191
  
 Affinity matrix, 229 
  
 Algorithm 
  
 greedy k-clustering, 215 
  
 k-means, 211 
  
 singular value decomposition, 51 
 Almost surely, 253 
  
 Anchor Term, 317 
  
 Aperiodic, 77 
  
 Arithmetic mean, 417
  
 Commute time, 104 
  
 Conditional probability, 421 
  
 Conductance, 97 
  
 Coordinates 
  
  
 Cartesian, 17 
  
  
 polar, 17 
  
 Coupon collector problem, 107 
  
 Cumulative distribution function, 420 
 Current 
  
  
 probabilistic interpretation, 100 
 Cycles, 266
  
 Bad pair, 257 
  
 Bayes rule, 
 428
  
 emergence, 
 265 
  
 number of, 
 265
  
  
 Bayesian, 338 
  
 Bayesian network, 338 
  
 Belief Network, 338 
  
 belief propagation, 337 
  
 Bernoulli trials, 425 
  
 Best fit, 40 
  
 Bigoh, 406 
  
 Binomial distribution, 248 
  
 approximated by Poisson, 426 
 boosting, 158 
  
 Branching process, 272
  
 Cartesian coordinates, 17 
  
 Cauchy-Schwartz inequality, 414, 416 
 Central Limit Theorem, 423 
  
 Characteristic equation, 437 
  
 Characteristic function, 455 
  
 Chebyshev’s inequality, 13 
  
 Chernoff bounds, 430 
  
 Clustering, 208 
  
 k
 -center criterion, 215 
  
 k-means, 211 
  
 Sparse Cuts, 229 
  
 CNF 
  
 CNF-sat, 279 
  
 Cohesion, 232
  
 Data streams 
  
  
 counting frequent elements, 187 
  
  
 frequency moments, 182 
  
  
 frequent element, 188 
  
  
 majority element, 187 
  
  
 number of distinct elements, 183 
  
  
 number of occurrences of an element, 
  
  
 186 
  
  
 second moment, 189 
  
 Degree distribution, 248 
  
  
 power law, 248 
  
 Depth first search, 261 
  
 Diagonalizable, 438 
  
 Diameter of a graph, 256, 268 
  
 Diameter two, 266 
  
 dilation, 385 
  
 Disappearance of isolated vertices, 266 
  
 Discovery time, 102 
  
 Distance 
  
  
 total variation, 82 
  
 Distribution 
  
  
 vertex degree, 246 
  
 Document ranking, 62
  
 Effective resistance, 105 
  
 Eigenvalue, 437
  
 466",NA
References,"[AB15] Pranjal Awasthi and Maria-Florina Balcan. Center based clustering: A foun-
 dational perspective. In Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto 
 Rocci, editors,
  Handbook of cluster analysis
 . CRC Press, 2015.
  
 [ACORT11] Dimitris Achlioptas, Amin Coja-Oghlan, and Federico Ricci-Tersenghi. On the 
 solution-space geometry of random constraint satisfaction problems. 
 Random Structures & Algorithms
 , 38(3):251–268, 2011.
  
 [AGKM16] Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. Computing a 
 nonnegative matrix factorization - provably.
  SIAM J. Comput.
 , 45(4):1582–1611, 2016.
  
 [AK05] Sanjeev Arora and Ravindran Kannan. Learning mixtures of separated non-
 spherical gaussians.
  Annals of Applied Probability
 , 15(1A):69–92, 2005. Pre-liminary 
 version in STOC 2001.
  
 [Alo86] Noga Alon. Eigenvalues and expanders.
  Combinatorica
 , 6:83–96, 1986.
  
 [AM05] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of 
 distributions. In
  COLT
 , pages 458–469, 2005.
  
 [AMS96]
  
 Noga Alon, Yossi Matias, and Mario Szegedy.
  
 The space complexity of
  
 approximating the frequency moments. In
  Proceedings of the twenty-eighth 
 annual ACM symposium on Theory of computing
 , pages 20–29. ACM, 1996.
  
 [AN72] Krishna Athreya and P. E. Ney.
  Branching Processes
 , volume 107. Springer, Berlin, 
 1972.
  
 [AP03] Dimitris Achlioptas and Yuval Peres. The threshold for random k-sat is 2k (ln 2 - 
 o(k)). In
  STOC
 , pages 223–231, 2003.
  
 [Arr50] Kenneth J. Arrow. A difficulty in the concept of social welfare.
  Journal of Political 
 Economy
 , 58(4):328–346, 1950.
  
 [AV07] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of care-ful 
 seeding. In
  Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete 
 algorithms
 , pages 1027–1035. Society for Industrial and Applied Mathematics, 2007.
  
 [BA] Albert-Lszl Barabsi and Rka Albert. Emergence of scaling in random net-works.
  
 Science
 , 286(5439).
  
 [BB10]
  
 M.-F. Balcan and A. Blum.
  
 A discriminative model for semi-supervised
  
 learning.
  Journal of the ACM
 , 57(3):19:1–19:46, March 2010.
  
 470",NA
