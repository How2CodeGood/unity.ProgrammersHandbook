Larger Text,Smaller Text,Symbol
"LANGUAGE, ",NA,NA
PROOF AND ,NA,NA
LOGIC,NA,NA
JON BARWISE & JOHN ETCHEMENDY,In collaboration with,NA
Gerard Allwein ,NA,NA
Dave Barker-,NA,NA
Plummer ,NA,NA
Albert Liu,NA,NA
7 ,NA,NA
7,NA,NA
SEVEN BRIDGES PRESS ,"NEW 
 YORK • LONDON",NA
Acknowledgements,"Our primary debt of gratitude goes to our three main collaborators on this 
 project: Gerry Allwein, Dave Barker-Plummer, and Albert Liu. They have 
 worked with us in designing the entire package, developing and implementing 
 the software, and teaching from and refining the text. Without their intelli-
 gence, dedication, and hard work, LPL would neither exist nor have most of its 
 other good properties.
  
 In addition to the five of us, many people have contributed directly and in-
 directly to the creation of the package. First, over two dozen programmers 
 have worked on predecessors of the software included with the package, both 
 earlier versions of Tarski’s World and the program Hyperproof, some of 
 whose code has been incorporated into Fitch. We want especially to mention 
 Christopher Fuselier, Mark Greaves, Mike Lenz, Eric Ly, and Rick Wong, whose 
 outstand-ing contributions to the earlier programs provided the foundation of 
 the new software. Second, we thank several people who have helped with the 
 develop-ment of the new software in essential ways: Rick Sanders, Rachel 
 Farber, Jon Russell Barwise, Alex Lau, Brad Dolin, Thomas Robertson, Larry 
 Lemmon, and Daniel Chai. Their contributions have improved the package in a 
 host of ways.
  
 Prerelease versions of LPL have been tested at several colleges and uni-
 versities. In addition, other colleagues have provided excellent advice that we 
 have tried to incorporate into the final package. We thank Selmer Bringsjord, 
 Renssalaer Polytechnic Institute; Tom Burke, University of South Carolina; 
 Robin Cooper, Gothenburg University; James Derden, Humboldt State Uni-
 versity; Josh Dever, SUNY Albany; Avrom Faderman, University of Rochester; 
 James Garson, University of Houston; Christopher Gauker, University of Cin-
 cinnati; Ted Hodgson, Montana State University; John Justice, Randolph-Macon 
 Women’s College; Ralph Kennedy, Wake Forest University; Michael O’Rourke, 
 University of Idaho; Greg Ray, University of Florida; Cindy Stern, California 
 State University, Northridge; Richard Tieszen, San Jose State Uni-versity; Saul 
 Traiger, Occidental College; and Lyle Zynda, Indiana University at South Bend. 
 We are particularly grateful to John Justice, Ralph Kennedy, and their students 
 (as well as the students at Stanford and Indiana Uni-versity), for their patience 
 with early versions of the software and for their extensive comments and 
 suggestions.
  
 We would also like to thank Stanford’s Center for the Study of Language
  
 v",NA
Contents,"Acknowledgements 
  
 Introduction
  
 iii
  
 1 
  
 1 
  
 2 
  
 4 
  
 5 
  
 10 
  
 15
  
 17
  
 1
 9 
  
 1
 9 
  
 2
 0 
  
 2
 3 
  
 2
 8 
  
 3
 1 
  
 3
 7 
  
 3
 8 
  
 4
 0
  
 4
 1 
  
 4
 1 
  
 4
 6 
  
 5
 4 
  
 5
  
 The special role of logic in rational inquiry
  
 . . . . . . . . . . . . . .
  
 Why learn an artificial language? . . . . . . . . . . . . . . . . . . . . Consequence 
 and proof . . . . . . . . . . . . . . . . . . . . . . . . . . Instructions about homework 
 exercises (
 essential!
 ) . . . . . . . . . . To the instructor . . . . . . . . . . . . . . . . . . . . . . 
 . . . . . . .
  
 Web address
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 I
  
 Propositional Logic
  
 1
  
 Atomic Sentences
  
 1.1
  
 Individual constants
  
 . . . . . . . . . . . . . . . . . . . . . . . .
  
 1.2
  
 Predicate symbols
  
 . . . . . . . . . . . . . . . . . . . . . . . . .
  
 1.3
  
 Atomic sentences . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 1.4
  
 General first-order languages
  
 . . . . . . . . . . . . . . . . . . .
  
 1.5
  
 Function symbols (
 optional
 ) . . . . . . . . . . . . . . . . . . . .
  
 1.6
  
 The first-order language of set theory (
 optional
 )
  
 . . . . . . . .
  
 1.7
  
 The first-order language of arithmetic (
 optional
 )
  
 . . . . . . . .
  
 1.8
  
 Alternative notation (
 optional
 ) . . . . . . . . . . . . . . . . . .
  
 2
  
 The Logic of Atomic Sentences
  
 2.1
  
 Valid and sound arguments
  
 . . . . . . . . . . . . . . . . . . . .
  
 2.2
  
 Methods of proof . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 2.3
  
 Formal proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 2.4
  
 Constructing proofs in Fitch . . . . . . . . . . . . . . . . . . . .
  
 2.5
  
 Demonstrating nonconsequence . . . . . . . . . . . . . . . . . .
  
 2.6
  
 Alternative notation (
 optional
 ) . . . . . . . . . . . . . . . . . .
  
 3
  
 The Boolean Connectives
  
 3.
 1 
  
 3.
 2 
  
 3.
 3 
  
 3.
 4
  
 Negation symbol:
  ¬
  . . . . . . . . . . . . . . . . . . . . . . . . . Conjunction symbol:
  
 ∧
  . . . . . . . . . . . . . . . . . . . . . . . Disjunction symbol:
  ∧
  . . . . . . . . . . . . . . . . . . 
 . . . . . Remarks about the game 
  
 . . . . . . . . . . . . . . . . . . . . .
  
 v",NA
Introduction,NA,NA
The special role of logic in rational inquiry,"What do the fields of astronomy, economics, finance, law, mathematics, med-
 icine, physics, and sociology have in common? Not much in the way of sub-ject 
 matter, that’s for sure. And not all that much in the way of methodology.
  
 What they do have in common, with each other and with many other fields, is 
 their dependence on a certain standard of rationality. In each of these fields, it 
 is assumed that the participants can differentiate between rational argu-
 mentation based on assumed principles or evidence, and wild speculation or 
 nonsequiturs, claims that in no way follow from the assumptions. In other 
 words, these fields all presuppose an underlying acceptance of basic principles 
 of logic.
  
 For that matter,
  all
  rational inquiry depends on logic, on the ability of 
  
 logic and rational 
 people to reason correctly most of the time, and, when they fail to reason 
  
  
 inquiry
  
 correctly, on the ability of others to point out the gaps in their reasoning.
  
 While people may not all agree on a whole lot, they do seem to be able to 
 agree on what can legitimately be concluded from given information. 
 Acceptance of these commonly held principles of rationality is what 
 differentiates rational inquiry from other forms of human activity.
  
 Just what are the principles of rationality presupposed by these disciplines?
  
 And what are the techniques by which we can distinguish correct or 
 “valid”reasoning from incorrect or “invalid” reasoning? More basically, what is 
 it that
  makes
  one claim “follow logically” from some given information, while 
 some other claim does not?
  
 Many answers to these questions have been explored. Some people have 
  
 claimed that the laws of logic are simply a matter of convention. If this is so, 
 logic and convention 
 we could 
 presumably decide to change the conventions, and so adopt different 
  
 principles of logic, the way we can decide which side of the road we drive 
  
 on. But there is an overwhelming intuition that the laws of logic are somehow 
  
 more fundamental, less subject to repeal, than the laws of the land, or even the 
  
 laws of physics. We can imagine a country in which a red traffic light means 
  
 go
 , and a world on which water flows up hill. But we can’t even imagine a 
  
 world in which there both are and are not nine planets.
  
 The importance of logic has been recognized since antiquity. After all, no
  
 1",NA
Why learn an artificial language?,"This language of first-order logic is very important. Like Latin, the language is 
 not spoken, but unlike Latin, it is used every day by mathematicians, philoso-
 phers, computer scientists, linguists, and practitioners of artificial intelligence. 
 Indeed, in some ways it is the universal language, the
  lingua franca
 , of the 
 sym-bolic sciences. Although it is not so frequently used in other forms of 
 rational inquiry, like medicine and finance, it is also a valuable tool for 
 understanding the principles of rationality underlying these disciplines as 
 well.
  
 The language goes by various names: the lower predicate calculus, the
  
 FOL 
  
 functional calculus, the language of first-order logic, and fol. The last of
  
 Introduction",NA
Consequence and proof,"Earlier, we asked what makes one claim follow from others: convention, or 
 something else? Giving an answer to this question for fol takes up a signif-
 icant part of this book. But a short answer can be given here. Modern logic
  
 logical consequence 
  
 teaches us that one claim is a logical consequence of another if there is no way 
  
 the latter could be true without the former also being true.
  
 This is the notion of logical consequence implicit in all rational inquiry. All 
 the rational disciplines presuppose that this notion makes sense, and that we 
 can use it to extract consequences of what we know to be so, or what we think 
 might be so. It is also used in disconfirming a theory. For if a particular claim is 
 a logical consequence of a theory, and we discover that the claim is false, then 
 we know the theory itself must be incorrect in some way or other. If our 
 physical theory has as a consequence that the planetary orbits are circular 
 when in fact they are elliptical, then there is something wrong with our 
 physics. If our economic theory says that inflation is a necessary consequence 
 of low unemployment, but today’s low employment has not caused inflation, 
 then our economic theory needs reassessment.
  
 Rational inquiry, in our sense, is not limited to academic disciplines, and so
  
 Introduction",NA
Essential instructions about homework exercises,"This book came packaged with software that you must have to use the book.
  
 In the software package, you will find a CD-ROM containing four computer 
  
 applications—Tarski’s World, Fitch, Boole and Submit—and a manual that 
  
 Tarski’s World, Fitch, 
 explains how to use them. If you do not have the complete package, you will 
  
  
 Boole and Submit
  
 not be able to do many of the exercises or follow many of the examples used in 
  
 the book. The CD-ROM also contains an electronic copy of the book, in case 
  
 you prefer reading it on your computer. When you buy the package, you also 
  
 get access to the Grade Grinder, an Internet grading service that can check 
 the Grade Grinder 
 whether your 
 homework is correct.
  
 About half of the exercises in the first two parts of the book will be com-
 pleted using the software on the CD-ROM. These exercises typically require 
 that you create a file or files using Tarski’s World, Fitch or Boole, and then 
 submit these solution files using the program Submit. When you do this, your 
 solutions are not submitted directly to your instructor, but rather to our grad-
  
 Essential instructions about homework exercises",NA
To the instructor,"Students, you may skip this section. It is a personal note from us, the authors, 
 to instructors planning to use this package in their logic courses.
  
 Practical matters
  
 We use the
  Language, Proof and Logic
  package (LPL) in two very different 
 sorts of courses. One is a first course in logic for undergraduates with no 
 previous background in logic, philosophy, mathematics, or computer science.
  
 Introduction",NA
Web address,"In addition to the book, software, and grading service, additional material can 
 be found on the Web at the following address:
  
 http://www-csli.stanford.edu/LPL/
  
 Note the dash (-) rather than the more common period (.) after “www” in this 
 address.
  
 Web address",NA
Part I ,NA,NA
Propositional Logic,17,NA
Atomic Sentences,"In the Introduction, we talked about fol as though it were a single language.
  
 Actually, it is more like a family of languages, all having a similar grammar and 
 sharing certain important vocabulary items, known as the connectives and 
 quantifiers. Languages in this family can differ, however, in the specific 
 vocabulary used to form their most basic sentences, the so-called atomic sen-
 tences.
  
 Atomic sentences correspond to the most simple sentences of English, sen-
 atomic sentences 
 tences 
 consisting of some names connected by a predicate. Examples are
  Max 
  
 ran, Max saw Claire,
  and
  Claire gave Scruffy to Max.
  Similarly, in fol atomic 
  
 sentences are formed by combining names (or individual constants, as they 
  
 are often called) and predicates, though the way they are combined is a bit 
  
 different from English, as you will see.
  
 Different versions of fol have available different names and predicates. We 
  
 names and predicates 
 will frequently use a first-order language designed to describe blocks arranged 
  
 on a chessboard, arrangements that you will be able to create in the program 
  
 Tarski’s World. This language has names like b, e, and n
 2
 , and predicates 
  
 like Cube, Larger, and Between. Some examples of atomic sentences in this 
  
 language are Cube(b), Larger(c
 ,
  f), and Between(b
 ,
  c
 ,
 d). These sentences say, 
  
 respectively, that
  b
  is a cube, that
  c
  is larger than
  f
 , and that
  b
  is between
  c 
  
 and
  d
 .
  
 Later in this chapter, we will look at the atomic sentences used in two other 
 versions of fol, the first-order languages of set theory and arithmetic.
  
 In the next chapter, we begin our discussion of the connectives and quantifiers 
 common to all first-order languages.
  
 Section 1.1",NA
Individual constants,"Individual constants are simply symbols that are used to refer to some fixed 
 individual object. They are the fol analogue of names, though in fol we 
 generally don’t capitalize them. For example, we might use max as an individ-
 ual constant to denote a particular person, named Max, or 1 as an individual 
 constant to denote a particular number, the number one. In either case, they 
 would basically work exactly the way names work in English. Our blocks
  
 19",NA
Predicate symbols,"Predicate symbols are symbols used to express some property of objects or
  
 predicate or relation 
  
 some relation between objects. Because of this, they are also sometimes called
  
 symbols 
  
 relation symbols. As in English, predicates are expressions that, when com-
  
 bined with names, form atomic sentences. But they don’t correspond exactly 
  
 to the predicates of English grammar.
  
 Consider the English sentence
  Max likes Claire
 . In English grammar, this is 
 analyzed as a subject-predicate sentence. It consists of the subject
  Max 
 followed by the predicate
  likes Claire.
  In fol, by contrast, we view this as
  
 logical subjects
  
 a claim involving two “logical subjects,” the names
  Max
  and
  Claire
 , and a
  
 1
 There is, however, a variant of first-order logic called
  free logic
  in which this assumption
  
 is relaxed. In free logic, there can be individual constants without referents. This yields a
  
 language more appropriate for mythology and fiction.
  
 Chapter 1",NA
Atomic sentences,"In fol, the simplest kinds of claims are those made with a single predicate 
  
 and the appropriate number of individual constants. A sentence formed by a 
  
 predicate followed by the right number of names is called an
  atomic
  sentence. 
  
  
 atomic sentence 
 For example Taller(claire
 ,
  max) and Cube(a) are atomic sentences, provided 
  
 the names and predicate symbols in question are part of the vocabulary of 
  
 our language. In the case of the identity symbol, we put the two required 
  
  
 infix vs. prefix notation 
 names on either side of the predicate, as in a = b. This is called “infix” no-tation, since the predicate 
 symbol = appears in between its two arguments.
  
 With the other predicates we use “prefix” notation: the predicate precedes the 
 arguments.
  
 The order of the names in an atomic sentence is quite important. Just as
  
 Claire is taller than Max
  means something different from
  Max is taller than 
 Claire,
  so too Taller(claire
 ,
  max) means something completely different than 
 Taller(max
 ,
  claire). We have set things up in our blocks language so that the 
 order of the arguments of the predicates is like that in English. Thus LeftOf(b
 ,
  
 c) means more or less the same thing as the English sentence
  b
  is left of
  c
 ,
  and 
 Between(b
 ,
  c
 ,
  d) means roughly the same as the English
  b
  is between
  c
  and
  d
 .
  
 Predicates and names designate properties and objects, respectively. What
  
 Section 1.3",NA
General first-order languages,"First-order languages differ in the names and predicates they contain, and so 
 in the atomic sentences that can be formed. What they share are the 
 connectives and quantifiers that enable us to build more complex sentences 
 from these simpler parts. We will get to those common elements in later 
 chapters.
  
 translation 
  
  
 When you translate a sentence of English into fol, you will sometimes 
  
 have a “predefined” first-order language that you want to use, like the blocks 
  
 language of Tarski’s World, or the language of set theory or arithmetic de-
  
 scribed later in this chapter. If so, your goal is to come up with a translation 
  
 that captures the meaning of the original English sentence as nearly as pos-
  
 sible, given the names and predicates available in your predefined first-order 
  
 language.
  
 Other times, though, you will not have a predefined language to use for 
 your translation. If not, the first thing you have to do is decide what names and
  
 designing languages 
  
 predicates you need for your translation. In effect, you are designing, on the 
 fly, 
  
 a new first-order language capable of expressing the English sentence you 
 want 
  
 to translate. We’ve been doing this all along, for example when we introduced 
  
 Home(max) as the translation of
  Max is at home
  and Taller(claire
 ,
  max) as the 
  
 translation of
  Claire is taller than Max.
  
 When you make these decisions, there are often alternative ways to go. For 
 example, suppose you were asked to translate the sentence
  Claire gave Scruffy 
 to Max.
  You might introduce a binary predicate GaveScruffy(x
 ,
  y), meaning
  x 
 gave Scruffy to y
 , 
 and 
 then 
 translate 
 the 
 original 
 sentence 
 as 
 GaveScruffy(claire
 ,
  max). Alternatively, you might introduce a three-place 
 pred-icate Gave(x
 ,
 y
 ,
  z), meaning
  x gave y to z
 , and then translate the sentence 
 as Gave(claire
 ,
  scruffy
 ,
  max).
  
 There is nothing wrong with either of these predicates, or their resulting 
 translations, so long as you have clearly specified what the predicates mean. 
 Of course, they may not be equally useful when you go on to translate other
  
 choosing predicates 
  
 sentences. The first predicate will allow you to translate sentences like
  Max 
  
 gave Scruffy to Evan
  and
  Evan gave Scruffy to Miles.
  But if you then run into 
  
 the sentence
  Max gave Carl to Claire,
  you would be stuck, and would have 
  
 to 
 introduce an entirely new predicate, say, GaveCarl(x
 ,
 y). The three-place 
  
 predicate is thus more 
 flexible. A first-order language that contained it (plus 
  
 the relevant names) would be able to 
 translate any of these sentences.
  
 In general, when designing a first-order language we try to economize on 
 the predicates by introducing more flexible ones, like Gave(x
 ,
  y
 ,
 z), rather than",NA
Function symbols,"Some first-order languages have, in addition to names and predicates, other 
  
 expressions that can appear in atomic sentences. These expressions are called 
  
 function symbols
 . Function symbols allow us to form name-like terms from 
 function symbols 
 names and 
 other name-like terms. They allow us to express, using atomic 
  
 sentences, complex claims that could not be perspicuously expressed using 
  
 just names and predicates. Some English examples will help clarify this.
  
 English has many sorts of noun phrases, expressions that can be combined 
 with a verb phrase to get a sentence. Besides names like
  Max
  and
  Claire
 , other 
 noun phrases include expressions like
  Max’s father
 ,
  Claire’s mother
 , 
 Every girl 
 who knows Max
 ,
  No boy who knows Claire
 ,
  Someone
  and so forth.
  
 Each of these combines with a singular verb phrase such as
  likes unbuttered 
 popcorn
  to make a sentence. But notice that the sentences that result have 
 very different logical properties. For example,
  
 Claire’s mother likes unbuttered popcorn
  
 implies that someone likes unbuttered popcorn, while
  
 No boy who knows Claire likes unbuttered popcorn
  
 does not.
  
 Since these noun phrases have such different logical properties, they are 
  
 terms 
 treated differently in fol. Those that intuitively refer to an individual are
  
 Section 1.5",NA
The first-order language of set theory,"Fol was initially developed for use in mathematics, and consequently the 
  
 most familiar first-order languages are those associated with various branches 
  
 of mathematics. One of the most common of these is the language of set 
  
 theory. This language has only two predicates, both binary. The first is the 
  
 predicates of set theory 
 identity symbol, =, which we have already encountered, and the second is the 
  
 symbol
  ∧
 , for set membership. It is standard to use infix notation for both of these predicates. Thus, in 
  
 set theory, atomic sentences are always formed by placing individual constants 
  
 on either side of one of the two predicates. This allows us to make identity 
  
 claims, of the form a = b, and membership claims, of the form a
  ∧
  b (where a and b are individual 
 constants).
  
 a set, and the thing named by a is a member of that set. For example, suppose A sentence of the form a
  ∧
  b 
 is true if and only if the thing named by b is 
  
 membership
  (
 ∧
 )
  
 a names the number 2 and b names the set
  {
 2
 ,
  4
 ,
  6
 }
 . Then the following table 
 tells us which membership claims made up using these names are true and 
 which are false.
 2 
  
  
 a
  ∧
  a 
  
 false
  
 a
  ∧
  b 
  
 true
  
 b
  ∧
  a 
  
 false
  
 b
  ∧
  b 
  
 false
  
 Notice that there is one striking difference between the atomic sentences 
 of set theory and the atomic sentences of the blocks language. In the blocks 
 language, you can have a sentence, like LeftOf(a
 ,
 b), that is true in a world, but 
 which can be made false simply by moving one of the objects. Moving an 
 object does not change the way the name works, but it can turn a true 
 sentence into a false one, just as the sentence
  Claire is sitting down
  can go from 
 true to false in virtue of Claire’s standing up.
  
 In set theory, we won’t find this sort of thing happening. Here, the analog 
 of a world is just a domain of objects and sets. For example, our domain might 
 consist of all natural numbers, sets of natural numbers, sets of sets of natural 
 numbers, and so forth. The difference between these “worlds” and those of 
 Tarski’s World is that the truth or falsity of the atomic sentences is 
 determined entirely once the reference of the names is fixed. There is nothing 
 that corresponds to moving the blocks around. Thus if the universe contains
  
 2
 For the purposes of this discussion we are assuming that numbers are not sets, and that
  
 sets can contain either numbers or other sets as members.",NA
The first-order language of arithmetic,"While neither the blocks language as implemented in Tarski’s World nor the 
 language of set theory has function symbols, there are languages that use 
 them extensively. One such first-order language is the language of arithmetic. 
 This language allows us to express statements about the natural numbers 0
 ,
  1
 ,
  
 2
 ,
 3
 , . . .
 , and the usual operations of addition and multiplication.
  
 There are several more or less equivalent ways of setting up this language.
  
 predicates
  (=
 , <
 )
  and 
  
 The one we will use has two names, 0 and 1, two binary relation symbols, =
  
 functions
  (+
 , ×
 )
  of arithmetic 
  
 and
  <
 , and two binary function symbols, + and
  ×
 . The atomic sentences are 
 those that can be built up out of these symbols. We will use infix notation 
  
 both for the relation symbols 
 and the function symbols.
  
 Notice that there are infinitely many different terms in this language (for 
 example, 0, 1, (1 + 1), ((1 + 1) + 1), (((1 + 1) + 1) + 1), . . . ), and so an infinite 
 number of atomic sentences. Our list also shows that every natural number is 
 named by some term of the language. This raises the question of how we can 
 specify the set of terms in a precise way. We can’t list them all explicitly, since",NA
Alternative notation,"As we said before, fol is like a family of languages. But, as if that were not 
 enough diversity, even the very same first-order language comes in a variety 
 of dialects. Indeed, almost no two logic books use exactly the same notational 
 conventions in writing first-order sentences. For this reason, it is important to 
 have some familiarity with the different dialects—the different notational 
 conventions—and to be able to translate smoothly between them. At the end 
 of most chapters, we discuss common notational differences that you are 
 likely to encounter.
  
 Some notational differences, though not many, occur even at the level of 
 atomic sentences. For example, some authors insist on putting parentheses 
 around atomic sentences whose binary predicates are in infix position. So (a = 
 b) is used rather than a = b. By contrast, some authors omit parentheses 
 surrounding the argument positions (and the commas between them) when 
 the predicate is in prefix position. These authors use Rab instead of R(a
 ,
 b). We 
 have opted for the latter simply because we use predicates made up of several 
 letters, and the parentheses make it clear where the predicate ends and the 
 arguments begin: Cubed is not nearly as perspicuous as Cube(d).
  
 What is important in these choices is that sentences should be unambigu-
 ous and easy to read. Typically, the first aim requires parentheses to be used 
 in one way or another, while the second suggests using no more than is 
 necessary.",NA
The Logic of Atomic Sentences,"A major concern in logic is the concept of
  logical consequence
 : When does one 
 sentence, statement, or claim follow logically from others? In fact, one of the 
 main motivations in the design of fol was to make logical consequence as 
 perspicuous as possible. It was thought that by avoiding the ambiguities and 
 complexities of ordinary language, we would be able to recognize the 
 consequences of our claims more easily. This is, to a certain extent, true; but it 
 is also true that we should be able to recognize the consequences of our claims 
 whether or not they are expressed in fol.
  
 In this chapter, we will explain what we mean by “logical consequence,” or 
 equivalently, what we mean when we say that an argument is “logically 
 valid.”This is a fairly simple concept to understand, but it can also be devilishly 
 dif-ficult to apply in specific cases. Indeed, in mathematics there are many, 
 many examples where we do not know whether a given claim is a 
 consequence of other known truths. Mathematicians may work for years or 
 decades trying to answer such questions. After explaining the notion of 
 consequence, we will describe the principal techniques for showing that a 
 claim is or is not a con-sequence of other claims, and begin presenting what is 
 known as a formal system of deduction, a system that allows us to show that a 
 sentence of fol is a consequence of others. We will continue developing this 
 system as we learn more about fol in later chapters.
  
 Section 2.1",NA
Valid and sound arguments,"Just what do we mean by logical consequence? Or rather, since this phrase is 
 sometimes used in quite different contexts, what does a
  logician
  mean by 
 logical consequence?
  
 A few examples will help. First, let’s say that an
  argument
  is any series 
  
 arguments, premises, 
 of statements in which one (called the
  conclusion
 ) is meant to follow from, or 
  
  
 and conclusions
  
 be supported by, the others (called the
  premises
 ). Don’t think of two people 
 arguing back and forth, but of one person trying to convince another of some 
 conclusion on the basis of mutually accepted premises. Arguments in our 
 sense may appear as part of the more disagreeable sort of “arguments”—the 
 kind parents have with their children—but
  our
  arguments also appear
  
 41",NA
Methods of proof,"Our description of the logical consequence relation is fine, as far as it goes. But 
 it doesn’t give us everything we would like. In particular, it does not tell us 
 how to
  show
  that a given conclusion S follows, or does not follow, from some 
 premises P
 ,
  Q
 ,
  R
 , . . .
 . In the examples we have looked at, this may not seem 
 very problematic, since the answers are fairly obvious. But when we are 
 dealing with more complicated sentences or more subtle reasoning, things are 
 sometimes far from simple.
  
 In this course you will learn the fundamental methods of showing when 
 claims follow from other claims and when they do not. The main technique for 
 doing the latter, for showing that a given conclusion does
  not
  follow from 
 some premises, is to find a possible circumstance in which the premises are 
 true but the conclusion false. In fact we have already used this method to 
 show that the argument about Lucretius was invalid. We will use the method 
 repeatedly, and introduce more precise versions of it as we go on.
  
 What methods are available to us for showing that a given claim
  is
  a logical 
 consequence of some premises? Here, the key notion is that of a
  proof
 .
  
 proof 
  
 A proof is a step-by-step demonstration that a conclusion (say S) follows 
  
 from some premises (say P
 ,
 Q
 ,
 R). The way a proof works is by establishing a 
  
 series of intermediate conclusions, each of which is an obvious consequence of
  
 Chapter 2",NA
Formal proofs,"In this section we will begin introducing our system for presenting formal
  
 deductive systems 
  
 proofs, what is known as a “deductive system.” There are many different 
  
 styles of deductive systems. The system we present in the first two parts of
  
 the system F 
  
 the book, which we will call
  F
 , is a “Fitch-style” system, so called because 
 Frederic Fitch first introduced this format for giving proofs. We will look at 
  
 a very different deductive 
 system in Part IV, one known as the resolution 
  
 method, which is of considerable importance in 
 computer science.
  
 very much like an argument presented in Fitch format. The main difference is In the system
  F
 , a proof of a 
 conclusion S from premises P, Q, and R, looks
  
 that the proof displays, in addition to the conclusion S, all of the intermediate 
 conclusions S
 1
 , . . . ,
  S
 n
  that we derive in getting from the premises to the 
 conclusion S:
  
  
  
  
  
 P
  
 Justification 1 
  
 ...
  
 Q
  
 R
  
 S
 1 
  
 ...
  
 S
 n
  
 Justification n
  
 S
  
 Justification n+1
  
 There are two graphical devices to notice here, the vertical and horizontal 
 lines. The vertical line that runs on the left of the steps draws our attention to 
 the fact that we have a single purported proof consisting of a sequence of 
 several steps. The horizontal Fitch bar indicates the division between the 
 claims that are assumed and those that allegedly follow from them. Thus the 
 fact that P, Q, and R are above the bar shows that these are the premises of our 
 proof, while the fact that S
 1
 , . . . ,
  S
 n
 ,
  and S are below the bar shows that these 
 sentences are supposed to follow logically from the premises.",NA
Constructing proofs in Fitch,"Writing out a long formal proof in complete detail, let alone reading or check-
 ing it, can be a pretty tedious business. The system
  F
  makes this less painful 
 than many formal systems, but it’s still not easy. This book comes with a sec-
  
 the program Fitch 
  
 ond program, Fitch, that makes constructing formal proofs much less painful. 
  
 Fitch can also check your proof, telling you whether it is correct, and if it isn’t, 
  
 which step or steps are mistaken. This means you will never be in any doubt 
  
 about whether your formal proofs meet the standard of rigor demanded of 
  
 them. And, as a practical matter, you can make sure they are correct before 
  
 submitting them.
  
 There are other ways in which Fitch makes life simpler, as well. One is that
  
 Fitch vs. F
  
 Fitch is more flexible than the system
  F
 . It lets you take certain shortcuts that 
 are logically correct but do not, strictly speaking, fall under the rules of
  
 F
 . You can always go back and expand a proof in Fitch to a formally correct 
 F
  
 proof, but we won’t often insist on this. Let us now use Fitch to construct a 
 simple formal proof. Before going on,
  
 you will want to read the first few sections of the chapter on how to use Fitch 
 in the manual.
  
 You try it 
  
 .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. . .. .. . .. .. . .
  
 ▶
  
 1. We are going to use Fitch to construct the formal proof of SameRow(b
 ,
  a) 
 from premises SameRow(a
 ,
 a) and b = a. Launch Fitch and open the file
  
 Identity 1. Here we have the beginnings of the formal proof. The premises 
 appear above the Fitch bar. It may look slightly different from the proofs 
 we have in the book, since in Fitch the steps don’t have to be numbered, for 
 reasons we’ll soon find out. (If you would like to have numbered steps, you 
 can choose
  Show Step Numbers
  from the
  Proof
  menu. But don’t try this 
 yet.)
  
 ▶
  
 2. Before we start to construct the proof, notice that at the bottom of the proof 
 window there is a separate pane called the “goal strip,” contain-
  
 ing the goal of the proof. In this case the goal is to prove the sentence 
 SameRow(b
 ,
  a). If we successfully satisfy this goal, we will be able to get 
 Fitch to put a checkmark to the right of the goal.
  
 ▶
  
 3. Let’s construct the proof. What we need to do is fill in the steps needed to 
 complete the proof, just as we did at the end of the last section. Add",NA
Demonstrating nonconsequence,"Proofs come in a variety of different forms. When a mathematician proves 
  
 a theorem, or when a prosecutor proves a defendant’s guilt, they are show-
  
 ing that a particular claim follows from certain accepted information, the 
  
 information they take as given. This kind of proof is what we call a proof of 
  
  
 proofs of 
 consequence
 , a proof that a particular piece of information must be true if the 
 consequence
  
 given information, the premises of the argument, are correct.
  
 A very different, but equally important kind of proof is a proof of
  nonconse-
  
  
 proofs of 
 quence
 . When a defense attorney shows that the crime might have been com-
  
 nonconsequence
  
 mitted by someone other than the client, say by the butler, the attorney is 
 trying to prove that the client’s guilt
  does not
  follow from the evidence in the 
 case. When mathematicians show that the parallel postulate is not a conse-
 quence of the other axioms of Euclidean geometry, they are doing the same 
 thing: they are showing that it would be possible for the claim in question (the 
 parallel postulate) to be false, even if the other information (the remaining 
 axioms) is true.
  
 We have introduced a few methods for demonstrating the validity of an 
 argument, for showing that its conclusion is a consequence of its premises. We 
 will be returning to this topic repeatedly in the chapters that follow, adding 
 new tools for demonstrating consequence as we add new expressions to our 
 language. In this section, we discuss the most important method for demon-
 strating nonconsequence, that is, for showing that some purported conclusion 
 is not a consequence of the premises provided in the argument.
  
 Recall that logical consequence was defined in terms of the validity of 
 arguments. An argument is valid if every possible circumstance that makes the 
 premises of the argument true also makes the conclusion true. Put the other 
 way around, the argument is
  invalid
  if there is some circumstance that makes 
 the premises true but the conclusion false. Finding such a circumstance is the 
 key to demonstrating nonconsequence.
  
 To show that a sentence Q is not a consequence of premises P
 1
 , . . . ,
  P
 n
 , 
  
 we must show that the argument with premises P
 1
 , . . . ,
  P
 n
  and conclusion Q 
  
 is invalid. This requires us to demonstrate that it is possible for P
 1
 , . . . ,
  P
 n
  to 
  
 be true while Q is simultaneously false. That is, we must show that there is 
  
 a possible situation or circumstance in which the premises are all true while 
  
 the conclusion is false. Such a circumstance is said to be a
  counterexample
  to 
 counterexamples 
 the 
 argument.
  
 Informal proofs of nonconsequence can resort to many ingenious ways for
  
 Section 2.5",NA
Alternative notation,"You will often see arguments presented in the following way, rather than in 
 Fitch format. The symbol
  .·.
  (read “therefore”) is used to indicate the 
 conclusion:
  
 All men are mortal. 
  
 Socrates is a man.
  
 .·.
  Socrates is mortal.
  
 There is a huge variety of formal deductive systems, each with its own 
 notation. We can’t possibly cover all of these alternatives, though we describe 
 one, the resolution method, in Chapter 17.
  
 Chapter 2",NA
The Boolean Connectives,"So far, we have discussed only atomic claims. To form complex claims, fol pro-
  
 vides us with connectives and quantifiers. In this chapter we take up the three 
  
 simplest connectives: conjunction, disjunction, and negation, corresponding 
  
 to simple uses of the English
  and
 ,
  or
 , and
  it is not the case that.
  Because they 
 Boolean connectives 
 were first 
 studied systematically by the English logician George Boole, they 
  
 are called the Boolean operators or Boolean connectives.
  
 truth-functional 
 The Boolean connectives are also known as
  truth-functional
  connectives.
  
 There are additional truth-functional connectives which we will talk about 
  
 connectives
  
 later. These connectives are called “truth functional” because the truth value 
 of a complex sentence built up using these connectives depends on nothing 
 more than the truth values of the simpler sentences from which it is built.
  
 Because of this, we can explain the meaning of a truth-functional connective 
  
 in a couple of ways. Perhaps the easiest is by constructing a
  truth table
 , a 
  
 truth table 
 table that 
 shows how the truth value of a sentence formed with the connec-
  
 tive depends on the truth values of the sentence’s immediate parts. We will 
  
 give such tables for each of the connectives we introduce. A more interesting 
 Henkin-Hintikka game 
 way, 
 and one that can be particularly illuminating, is by means of a game, 
  
 sometimes called the Henkin-Hintikka game, after the logicians Leon Henkin 
  
 and Jaakko Hintikka.
  
 Imagine that two people, say Max and Claire, disagree about the truth 
 value of a complex sentence. Max claims it is true, Claire claims it is false. The 
 two repeatedly challenge one another to justify their claims in terms of 
 simpler claims, until finally their disagreement is reduced to a simple atomic 
 claim, one involving an atomic sentence. At that point they can simply examine 
 the world to see whether the atomic claim is true—at least in the case of 
 claims about the sorts of worlds we find in Tarski’s World. These successive 
 challenges can be thought of as a game where one player will win, the other 
 will lose. The legal moves at any stage depend on the form of the sentence. We 
 will explain them below. The one who can ultimately justify his or her claims 
 is the winner.
  
 When you play this game in Tarski’s World, the computer takes the side 
 opposite you, even if it knows you are right. If you are mistaken in your initial 
 assessment, the computer will be sure to win the game. If you are right, 
 though, the computer plugs away, hoping you will blunder. If you slip up, the 
 computer will win the game. We will use the game rules as a second way of 
 explaining the meanings of the truth-functional connectives.
  
 67",NA
Negation symbol:,NA,NA
 ¬,"The symbol
  ¬
  is used to express negation in our language, the notion we 
 commonly express in English using terms like
  not
 ,
  it is not the case that
 ,
  non-
 and
  un-
 . 
  
 In first-order logic, we always apply this symbol to the front of a 
 sentence to be negated, while in English there is a much more subtle system 
 for expressing negative claims. For example, the English sentences
  John isn’t 
 home
  and
  It is not the case that John is home
  have the same first-order 
 translation:
  
  
 ¬
 Home(john) 
  
 This sentence is true if and only if Home(john) isn’t true, that is, just in case 
 John isn’t home.
  
 In English, we generally avoid double negatives—negatives inside other 
 negatives. For example, the sentence
  It doesn’t make no difference
  is problem-
 atic. If someone says it, they usually mean that it doesn’t make any difference. 
 In other words, the second negative just functions as an intensifier of some 
 sort. On the other hand, this sentence could be used to mean just what it says, 
 that it does not make
  no
  difference, it makes
  some
  difference.
  
 Fol is much more systematic. You can put a negation symbol in front of any 
 sentence whatsoever, and it always negates it, no matter how many other 
 negation symbols the sentence already contains. For example, the sentence
  
 ¬¬
 Home(john) 
  
 negates the sentence
  
  
 ¬
 Home(john) 
  
 and so is true if and only if John is home.
  
 The negation symbol, then, can apply to complex sentences as well as to
  
 literals 
  
 atomic sentences. We will say that a sentence is a
  literal
  if it is either atomic 
  
 or the negation of an atomic sentence. This notion of a literal will be useful 
  
 later on.
  
 nonidentity symbol
  (
 ̸
 =)
  
 We will abbreviate negated identity claims, such as
  ¬
 (b = c), using
  ̸
 =, as in b
  ̸
 = 
 c. The symbol
  ̸
 = is available on the keyboard palettes in both Tarski’s World 
 and Fitch.
  
 Semantics and the game rule for negation
  
 Given any sentence P of fol (atomic or complex), there is another sentence
 ¬
 P. 
 This sentence is true if and only if P is false. This can be expressed in terms of 
 the following truth table.",NA
Conjunction symbol:,NA,NA
 ∧,"The symbol
  ∧
  is used to express conjunction in our language, the notion we 
 normally express in English using terms like
  and
 ,
  moreover
 , and
  but
 . In first-
 order logic, this connective is always placed between two sentences, whereas 
 in English we can also conjoin other parts of speech, such as nouns. For 
 example, the English sentences
  John and Mary are home
  and
  John is home and 
 Mary is home
  have the same first-order translation:
  
 Home(john)
  ∧
  Home(mary) 
  
 This sentence is read aloud as “Home John and home Mary.” It is true if and only 
 if John is home and Mary is home.
  
 In English, we can also conjoin verb phrases, as in the sentence
  John 
 slipped and fell
 . But in fol we must translate this the same way we would 
 translate 
 John slipped and John fell
 :
  
 Slipped(john)
  ∧
  Fell(john) 
  
 This sentence is true if and only if the atomic sentences Slipped(john) and 
 Fell(john) are both true.
  
 sign of conjunction in the English sentence at all. How, for example, do you A lot of times, a sentence of fol 
 will contain
  ∧
  when there is no visible
  
 think we might express the English sentence
  d is a large cube
  in fol? If you 
 guessed
  
 Large(d)
  ∧
  Cube(d) 
  
 you were right. This sentence is true if and only if
  d
  is large and
  d
  is a cube—
 that is, if
  d
  is a large cube.
  
 Some uses of the English
  and
  are not accurately mirrored by the fol conjunction 
 symbol. For example, suppose we are talking about an evening when Max and 
 Claire were together. If we were to say
  Max went home and Claire went to sleep
 , 
 our assertion would carry with it a temporal implication, namely that Max went 
 home
  before
  Claire went to sleep. Similarly, if we were to reverse the order and 
 assert
  Claire went to sleep and Max went home
  it would suggest a very different 
 sort of situation. By contrast, no such implication, implicit or explicit, is 
 intended when we use the symbol
  ∧
 . The sentence 
  
  
 WentHome(max)
  ∧
  FellAsleep(claire) 
  
 is true in exactly the same circumstances as
  
 FellAsleep(claire)
  ∧
  WentHome(max)",NA
Disjunction symbol:,NA,NA
 ∧,"The symbol
  ∧
  is used to express disjunction in our language, the notion we 
 express in English using
  or
 . In first-order logic, this connective, like the con-
 junction sign, is always placed between two sentences, whereas in English we 
 can also disjoin nouns, verbs, and other parts of speech. For example, the 
 English sentences
  John or Mary is home
  and
  John is home or Mary is home 
 both 
 have the same first-order translation:
  
 Home(john)
  ∧
  Home(mary)
  
 This fol sentence is read “Home John or home Mary.”
  
 exclusive vs. inclusive 
  
 Although the English
  or
  is sometimes used in an “exclusive” sense, to say
  
 disjunction 
  
 that
  exactly
  one (i.e., one but no more than one) of the two disjoined sentences 
  
 is true, the first-order logic
  ∧
  is always given an “inclusive” interpretation: it 
 means that at least one and possibly both of the two disjoined sentences is 
  
 true. Thus, our sample 
 sentence is true if John is home but Mary is not, if 
  
 Mary is home but John is not, or if both John and Mary 
 are home.",NA
Remarks about the game,"We summarize the game rules for the three connectives,
  ¬
 ,
  ∧
 , and
  ∧
 , in Table 3.1. The first column 
 indicates the form of the sentence in question,
  
 and the second indicates your current commitment, true or false. Which player 
 moves depends on this commitment, as shown in the third column.
  
 The goal of that player’s move is indicated in the final column. Notice that 
 commitment and rules 
 although 
 the player to move depends on the commitment, the goal of that 
  
 move does not depend on the commitment. You can see why this is so by 
  
 thinking about the first row of the table, the one for P
  ∧
  Q. When you are committed to true, it is clear that 
 your goal should be to choose a true 
  
 disjunct. But when you are committed to false, Tarski’s World is committed 
  
 to true, and so also has the same goal of choosing a true disjunct.
  
 There is one somewhat subtle point that should be made about our way of 
 describing the game. We have said, for example, that when you are committed 
 to the truth of the disjunction P
  ∧
  Q, you are committed to the truth of one of 
 the disjuncts. This of course is true, but does not mean you necessarily know 
 which of P or Q is true. For example, if you have a sentence of the form",NA
Ambiguity and parentheses,"When we first described fol, we stressed the lack of ambiguity of this language 
 as opposed to ordinary languages. For example, English allows us to say things 
 like
  Max is home or Claire is home and Carl is happy
 . This sentence can be 
 understood in two quite different ways. One reading claims that either Claire 
 is home and Carl is happy, or Max is home. On this reading, the sentence 
 would be true if Max was home, even if Carl was unhappy. The other reading 
 claims both that Max or Claire is home and that Carl is happy.
  
 Fol avoids this sort of ambiguity by requiring the use of parentheses, much 
 the way they are used in algebra. So, for example, fol would not have one 
 sentence corresponding to the ambiguous English sentence, but two:
  
  
 Home(max)
  ∧
  (Home(claire)
  ∧
  Happy(carl)) 
  
  
 (Home(max)
  ∧
  Home(claire))
  ∧
  Happy(carl) 
  
 The parentheses in the first indicate that it is a disjunction, whose second 
 disjunct is itself a conjunction. In the second, they indicate that the sentence is 
 a conjunction whose first conjunct is a disjunction. As a result, the truth 
 conditions for the two are quite different. This is analogous to the difference in 
 algebra between the expressions 2 + (
 x ×
  3) and (2 +
  x
 )
  ×
  3. This analogy 
 between logic and algebra is one we will come back to later.
  
 Section 3.5",NA
Equivalent ways of saying things,"Every language has many ways of saying the same thing. This is particularly 
 true of English, which has absorbed a remarkable number of words from other 
 languages in the course of its history. But in any language, speakers always 
 have a choice of many synonymous ways of getting across their point. The 
 world would be a boring place if there were just one way to make a given 
 claim.
  
 Fol is no exception, even though it is far less rich in its expressive capaci-
 ties than English. In the blocks language, for example, none of our predicates is 
 synonymous with another predicate, though it is obvious that we could do 
 without many of them without cutting down on the claims expressible in the 
 language. For instance, we could get by without the predicate RightOf by 
 expressing everything we need to say in terms of the predicate LeftOf, sys-
 tematically reversing the order of the names to get equivalent claims. This is 
 not to say that RightOf means the same thing as LeftOf—it obviously does 
 not—but just that the blocks language offers us a simple way to construct 
 equivalent claims using these predicates. In the exercises at the end of this 
 section, we explore a number of equivalences made possible by the predicates 
 of the blocks language.
  
 Some versions of fol are more parsimonious with their basic predicates 
 than the blocks language, and so may not provide equivalent ways of express-
 ing atomic claims. But even these languages cannot avoid multiple ways of 
 expressing more complex claims. For example, P
  ∧
  Q and Q
  ∧
  P express the 
 same claim in any first-order language. More interesting, because of the su-
 perficial differences in form, are the equivalences illustrated in Exercise 3.16,
  
 DeMorgan’s laws
  
 known as
  DeMorgan’s laws
 . The first of DeMorgan’s laws tells us that the
  
 negation of a conjunction,
  ¬
 (P
  ∧
  Q), is logically equivalent to the disjunction of 
 the negations of the original conjuncts:
  ¬
 P
  ∧ ¬
 Q. The other tells us that the 
 negation of a disjunction,
  ¬
 (P
  ∧
  Q), is equivalent to the conjunction of the 
 negations of the original disjuncts:
  ¬
 P
  ∧ ¬
 Q. These laws are simple con-
 sequences of the meanings of the Boolean connectives. Writing S
 1
  ∧
  S
 2
  to 
 indicate that S
 1
  and S
 2
  are logically equivalent, we can express DeMorgan’s",NA
Translation,"An important skill that you will want to master is that of translating from 
 English to fol, and vice versa. But before you can do that, you need to know 
 how to express yourself in both languages. The problems below are designed 
 to help you learn these related skills.
  
 correct translation 
  
  
 How do we know if a translation is correct? Intuitively, a correct 
 translation 
  
 is a sentence with the same meaning as the one being translated. But what 
  
 is 
 the meaning? Fol finesses this question, settling for “truth conditions.”
  
 What we require of a correct 
 translation in fol is that it be true in the same 
  
 circumstances as the original sentence. If two 
 sentences are true in exactly
  
 truth conditions 
  
 the same circumstances, we say that they have the same
  truth conditions
 . For 
  
 sentences of Tarski’s World, this boils down to being true in the very same 
  
 worlds.
  
 Note that it is not sufficient that the two sentences have the same truth 
 value in some
  particular
  world. If that were so, then any true sentence of 
 English could be translated by any true sentence of fol. So, for example, if 
 Claire and Max are both at home, we could translate
  Max is at home
  by means 
 of Home(claire). No, having the same actual truth value is not enough. They 
 have to have the same truth values in all circumstances.
  
 Remember
  
  
 In order for an fol sentence to be a good translation of an English sen-
  
  
 tence, it is sufficient that the two sentences have the same truth values
  
  
 in all possible circumstances, that is, that they have the same
  truth con-
  
  
 ditions
 .",NA
Alternative notation,"As we mentioned in Chapter 2, there are various dialect differences among 
 users of fol. It is important to be aware of these so that you will not be stymied 
 by superficial differences. In fact, you will run into alternate symbols being 
 used for each of the three connectives studied in this chapter.
  
 The most common variant of the negation sign,
  ¬
 , is the symbol known as 
 the tilde,
  ∧
 . Thus you will frequently encounter
  ∧
 P where we would write
 ¬
 P. A 
 more old-fashioned alternative is to draw a bar completely across the you to 
 avoid certain uses of parentheses, since the bar indicates its own scope 
 negated sentence, as in P. This has one advantage over
  ¬
 , in that it allows by 
 what lies under it. For example, where we have to write
  ¬
 (P
  ∧
  Q), the bar 
 equivalent would simply be P
  ∧
  Q. None of these symbols are available on all 
 keyboards, a serious problem in some contexts, such as programming 
 languages. Because of this, many programming languages use an exclamation 
 point to indicate negation. In the Java programming language, for example,
 ¬
 P 
 would be written !P.
  
 &, or sometimes (as in Java), &&. An older notation uses a centered dot, as There are only two common 
 variants of
  ∧
 . By far the most common is
  
 in multiplication. To make things more confusing still, the dot is sometimes 
 omitted, again as in multiplication. Thus, for P
  ∧
  Q you might see any of the 
 following: P&Q, P&&Q, P
  ·
  Q, or just PQ.
  
 encounter is a single or double vertical line, used in programming languages. Happily, the symbol
  ∧
  is 
 pretty standard. The only exception you may
  
 So if you see P
  |
  Q or P
  ∧
  Q, what is meant is probably P
  ∧
  Q. Unfortunately,",NA
The Logic of Boolean,NA,NA
Connectives,"The connectives
  ∧
 ,
  ∧
 , and
  ¬
  are truth-functional connectives. Recall what this means: the truth value of a 
 complex sentence built by means of one of
  
 these symbols can be determined simply by looking at the truth values of the 
 sentence’s immediate constituents. So to know whether P
  ∧
  Q is true, we need 
 only know the truth values of P and Q. This particularly simple behavior is 
 what allows us to capture the meanings of truth-functional connectives using 
 truth tables.
  
 Other connectives we could study are not this simple. Consider, the sen-
  
 tence
  it is necessarily the case that S
 . Since some true claims are necessarily 
  
 truth-functional vs. 
 true, that is, could not have been false (for instance, a = a), while other true 
  
 non-truth-functional
  
 claims are
  not
  necessarily true (for instance, Cube(a)), we can’t figure out the 
 operators
  
 truth value of the original sentence if we are only told the truth value of its 
 constituent sentence
  S
 .
  It is necessarily the case
 , unlike
  it is not the case
 , is not 
 truth-functional.
  
 The fact that the Boolean connectives are truth functional makes it very 
 easy to explain their meanings. It also provides us with a simple but power-ful 
 technique to study their logic. The technique is an extension of the truth tables 
 used to present the meanings of the connectives. It turns out that we can often 
 calculate the logical properties of complex sentences by construct-ing truth 
 tables that display all possible assignments of truth values to the atomic 
 constituents from which the sentences are built. The technique can, for 
 example, tell us that a particular sentence S is a logical consequence of some 
 premises P
 1
 , . . . ,
 P
 n
 . And since logical consequence is one of our main concerns, 
 the technique is an important one to learn.
  
 In this chapter we will discuss what truth tables can tell us about three 
 related logical notions: the notions of
  logical consequence
 ,
  logical equivalence
 , 
 and
  logical truth
 . Although we’ve already discussed logical consequence at 
 some length, we’ll tackle these in reverse order, since the related truth table 
 techniques are easier to understand in that order.",NA
Tautologies and logical truth,"We said that a sentence S is a logical consequence of a set of premises P
 1
 , . . . ,
  
 P
 n 
 if it is impossible for the premises all to be true while the conclusion S is 
 false. That is, the conclusion
  must
  be true if the premises are true.
  
 Notice that according to this definition there are some sentences that are 
 logical consequences of
  any
  set of premises, even the empty set. This will be 
 true of any sentence whose truth is itself a logical necessity. For example, 
 given our assumptions about fol, the sentence a = a is necessarily true. So of 
 course, no matter what your initial premises may be, it will be impossible for 
 those premises to be true and for a = a to be false—simply because it is 
 impossible for a = a to be false! We will call such logically necessary sentences
  
 logical truth 
  
 logical truths
 .
  
 The intuitive notions of logical possibility and logical necessity have al-
 ready come up several times in this book in characterizing valid arguments 
 and the consequence relation. But this is the first time we have applied them
  
 logical possibility and 
  
 to individual sentences. Intuitively, a sentence is logically possible if it could
  
 necessity 
  
 be (or could have been) true, at least on logical grounds. There might be some 
  
 other reasons, say physical, why the statement could not be true, but there are 
  
 no logical reasons preventing it. For example, it is not physically possible to 
  
 go faster than the speed of light, though it is logically possible: they do it on 
  
 Star Trek all the time. On the other hand, it is not even logically possible for 
  
 an object not to be identical to itself. That would simply violate the meaning 
  
 of identity. The way it is usually put is that a claim is logically possible if 
  
 there is some logically possible circumstance (or situation or world) in which 
  
 the claim is true. Similarly, a sentence is logically necessary if it is true in 
  
 every logically possible circumstance.
  
 These notions are quite important, but they are also annoyingly vague. As 
 we proceed through this book, we will introduce several precise concepts that 
 help us clarify these notions. The first of these precise concepts, which
  
 tautology 
  
 we introduce in this section, is the notion of a
  tautology
 .
  
 How can a precise concept help clarify an imprecise, intuitive notion? Let’s 
 think for a moment about the blocks language and the intuitive notion of 
 logical possibility. Presumably, a sentence of the blocks language is logically 
 possible if there could be a blocks world in which it is true. Clearly, if we can 
 construct a world in Tarski’s World that makes it true, then this demonstrates 
 that the sentence is indeed logically possible. On the other hand, there are 
 logically possible sentences that can’t be made true in the worlds you can
  
 Chapter 4",NA
Logical and tautological equivalence,"In the last chapter, we introduced the notion of logically equivalent sentences, 
 sentences that have the same truth values in every possible circumstance. 
 When two sentences are logically equivalent, we also say they have the same 
 truth conditions, since the conditions under which they come out true or false 
 are identical.
  
 The notion of logical equivalence, like logical necessity, is somewhat vague,
  
 logical equivalence 
  
 but not in a way that prevents us from studying it with precision. For here too 
  
 we can introduce precise concepts that bear a clear relationship to the 
 intuitive 
  
 notion we aim to understand better. The key concept we will introduce in this
  
 tautological equivalence 
  
 section is that of
  tautological equivalence
 . Two sentences are tautologically 
  
 equivalent if they can be seen to be equivalent simply in virtue of the meanings 
  
 of the truth-functional connectives. As you might expect, we can check for 
  
 tautological equivalence using truth tables.
  
 Suppose we have two sentences, S and S
 ′
 , that we want to check for tau-
 tological equivalence. What we do is construct a truth table with a reference 
 column for each of the atomic sentences that appear in either of the two sen-
 tences. To the right, we write both S and S
 ′
 , with a vertical line separating 
 them, and fill in the truth values under the connectives as usual. We call this
  
 joint truth tables 
  
 a
  joint
  truth table for the sentences S and S
 ′
 . When the joint truth table is 
  
 completed, we compare the column under the main connective of S with the 
  
 column under the main connective of S
 ′
 . If these columns are identical, then 
  
 we know that the truth conditions of the two sentences are the same.
  
 Let’s look at an example. Using A and B to stand for arbitrary atomic 
 sentences, let us test the first DeMorgan law for tautological equivalence. We 
 would do this by means of the following joint truth table.
  
 A
  
 B
   
 ¬
 (A
  ∧
  B)
  
 ¬
  A
  ∧ ¬
 B
  
 t
  
 t
  
 F
  
 t
  
 f
  
 F
  f
  
 t
  
 f
  
 T
  
 f
  
 f
  
 T
  t
  
 f
  
 t
  
 T
  
 f
  
 t
  
 T
  f
  
 f
  
 f
  
 T
  
 f
  
 t
  
 T
  t
  
  
 In this table, the columns in bold correspond to the main connectives of the
  
 Chapter 4",NA
Logical and tautological consequence,"Our main concern in this book is with the logical consequence relation, of 
 which logical truth and logical equivalence can be thought of as very special 
 cases: A logical truth is a sentence that is a logical consequence of any set of 
 premises, and logically equivalent sentences are sentences that are logical 
 consequences of one another.
  
 As you’ve probably guessed, truth tables allow us to define a precise notion 
 of
  tautological
  consequence, a strict form of logical consequence, just as they 
 allowed us to define tautologies and tautological equivalence, strict forms of 
 logical truth and logical equivalence.
  
 Let’s look at the simple case of two sentences, P and Q, both built from 
 atomic sentences by means of truth-functional connectives. Suppose you want 
 to know whether Q is a consequence of P. Create a joint truth table for P and Q, 
 just like you would if you were testing for tautological equivalence. After you 
 fill in the columns for P and Q, scan the columns under the main connectives 
 for these sentences. In particular, look at every row of the table in
  
 tautological consequence 
  
 which P is true. If each such row is also one in which Q is true, then Q is said 
  
 to be a
  tautological consequence
  of P. The truth table shows that if P is true, 
  
 then Q must be true as well, and that this holds simply due to the meanings 
  
 of the truth-functional connectives.
  
 Just as tautologies are logically necessary, so too any tautological conse-
 quence Q of a sentence P must also be a logical consequence of P. We can see 
 this by proving that if Q is
  not
  a logical consequence of P, then it can’t possibly 
 pass our truth table test for tautological consequence.
  
 Proof:
  Suppose Q is not a logical consequence of P. Then by our def-
 inition of logical consequence, there must be a possible circumstance 
 in which P is true but Q is false. This circumstance will determine 
 truth values for the atomic sentences in P and Q, and these values 
 will correspond to a row in the joint truth table for P and Q, since all 
 possible assignments of truth values to the atomic sentences are 
 represented in the truth table. Further, since P and Q are built up 
 from the atomic sentences by truth-functional connectives, and since 
 the former is true in the original circumstance and the latter false, P 
 will be assigned
  T
  in this row and Q will be assigned
  F
 . Hence, Q is 
 not a tautological consequence of P.
  
 Let’s look at a very simple example. Suppose we wanted to check to see 
 whether A
  ∧
  B is a consequence of A
  ∧
  B. The joint truth table for these sen-
  
 Chapter 4",NA
Tautological consequence in Fitch,"We hope you solved Exercise 4.24, because the solution gives you a sense of 
 both the power and the drawbacks of the truth table method. We were 
 tempted to ask you to construct a table requiring 64 rows, but thought better 
 of it. Constructing large truth tables may build character, but like most things 
 that build character, it’s a drag.
  
 Checking to see if Q is a tautological consequence of P
 1
 , . . . ,
  P
 n
  is a me-
 chanical procedure. If the sentences are long it may require a lot of tedious 
 work, but it doesn’t take any originality. This is just the sort of thing that 
 computers are good at. Because of this, we have built a mechanism into Fitch,
  
 Taut Con mechanism 
  
 called
  Taut Con
 , that is similar to
  Ana Con
  but checks to see whether a 
  
 sentence is a tautological consequence of the sentences cited in support. Like 
  
 Ana Con
 ,
  Taut Con
  is not really an inference rule (we will introduce infer-
  
 ence rules for the Boolean connectives in Chapter 6), but is useful for quickly 
  
 testing whether one sentence follows tautologically from others.
  
 ▶
  
 You try it 
 .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. . .. .. . .. .. . .
  
 1. Launch Fitch and open the file Taut Con 1. In this file you will find an
  
 argument that has the same form as the argument in Exercise 4.23. (Ignore
  
 the two goal sentences. We’ll get to them later.) Move the focus slider to 
 the last step of the proof. From the
  Rule?
  menu, go down to the
  Con 
 submenu and choose
  Taut Con
 .
  
 ▶
  
 2. Now cite the three premises as support for this sentence and check the 
 step. The step will not check out since this sentence is not a tautological
  
 consequence of the premises, as you discovered if you did Exercise 4.23, 
 which has the same form as this inference.
  
 ▶
  
 3. Edit the step that did not check out to read:
  
 Home(max)
  ∧
  Hungry(carl) 
  
 This sentence is a tautological consequence of two of the premises. Figure 
 out which two and cite just them. If you cited the right two, the step should 
 check out. Try it.
  
 ▶
  
 4. Add another step to the proof and enter the sentence:
  
 Hungry(carl)
  ∧
  (Home(max)
  ∧
  Hungry(pris))
  
 Chapter 4",NA
Pushing negation around,"When two sentences are logically equivalent, each is a logical consequence of 
 the other. As a result, in giving an informal proof, you can always go from an 
 established sentence to one that is logically equivalent to it. This fact makes 
 observations like the DeMorgan laws and double negation quite useful in 
 giving informal proofs.
  
 What makes these equivalences even more useful is the fact that logically 
  
 equivalent sentences can be substituted for one another in the context of a 
  
 substitution of logical 
 larger sentence and the resulting sentences will also be logically equivalent. 
  
  
 equivalents
  
 An example will help illustrate what we mean. Suppose we start with the 
 sentence:
  
 ¬
 (Cube(a)
  ∧ ¬¬
 Small(a)) 
  
 By the principle of double negation, we know that Small(a) is logically equiv-
 alent to
  ¬¬
 Small(a). Since these have exactly the same truth conditions, we can 
 substitute Small(a) for
  ¬¬
 Small(a) in the context of the above sentence, and the 
 result,
  
 ¬
 (Cube(a)
  ∧
  Small(a)) 
  
 will be logically equivalent to the original, a fact that you can check by con-
 structing a joint truth table for the two sentences.
  
 We can state this important fact in the following way. Let’s write S(P) for 
 an fol sentence that contains the (possibly complex) sentence P as a 
 component part, and S(Q) for the result of substituting Q for P in S(P). Then if 
 P and Q are logically equivalent:
  
  
 P
  ∧
  Q 
  
 it follows that S(P) and S(Q) are also logically equivalent:",NA
Conjunctive and disjunctive normal forms,"We have seen that with a few simple principles of Boolean logic, we can start 
 with a sentence and transform it into a logically equivalent sentence in 
 negation normal form, one where all negations occur in front of atomic 
 sentences. We can improve on this by introducing the so-called distributive 
 laws. These additional equivalences will allow us to transform sentences into 
 what are known as
  conjunctive normal form
  (CNF) and
  disjunctive normal form
  
 (DNF). These normal forms are quite important in certain applications of logic 
 in computer science, as we discuss in Chapter 17. We will also use disjunctive 
 normal form to demonstrate an important fact about the Boolean connectives 
 in Chapter 7.
  
 Recall that in algebra you learned that multiplication distributes over ad-
  
 dition:
  a×
 (
 b
 +
 c
 ) = (
 a×b
 )+(
 a×c
 ). The distributive laws of logic look formally 
  
 distribution
  
 Section 4.6",NA
Methods of Proof for Boolean,NA,NA
Logic,"Truth tables give us powerful techniques for investigating the logic of the 
 Boolean operators. But they are by no means the end of the story. Truth tables 
 are fine for showing the validity of simple arguments that depend only on 
 truth-functional connectives, but the method has two very significant lim-
 itations.
  
 First, truth tables get extremely large as the number of atomic sentences 
 limitations of truth 
   
 table methods 
 goes up. An argument involving seven atomic sentences is hardly unusual, but
  
 testing it for validity would call for a truth table with 2
 7
 = 128 rows. Testing an 
 argument with 14 atomic sentences, just twice as many, would take a table 
 containing over 16 thousand rows. You could probably get a Ph.D. in logic for 
 building a truth table that size. This exponential growth severely limits the 
 practical value of the truth table method.
  
 The second limitation is, surprisingly enough, even more significant. Truth 
 table methods can’t be easily extended to reasoning whose validity depends 
 on more than just truth-functional connectives. As you might guess from the 
 artificiality of the arguments looked at in the previous chapter, this rules out 
 most kinds of reasoning you’ll encounter in everyday life. Ordinary reasoning 
 relies heavily on the logic of the Boolean connectives, make no mistake about 
 that. But it also relies on the logic of other kinds of expressions. Since the truth 
 table method detects only tautological consequence, we need a method of 
 applying Boolean logic that can work along with other valid principles of 
 reasoning.
  
 Methods of proof, both formal and informal, give us the required exten-
 sibility. In this chapter we will discuss legitimate patterns of inference that 
 arise when we introduce the Boolean connectives into a language, and show 
 how to apply the patterns in informal proofs. In Chapter 6, we’ll extend our 
 formal system with corresponding rules. The key advantage of proof methods 
 over truth tables is that we’ll be able to use them even when the validity of our 
 proof depends on more than just the Boolean operators.
  
 The Boolean connectives give rise to many valid patterns of inference. 
 Some of these are extremely simple, like the entailment from the sentence P
  ∧
  
 Q to P. These we will refer to as
  valid inference steps
 , and will discuss
  
 127",NA
Valid inference steps,"Here’s an important rule of thumb: In an informal proof, it is always legiti-mate 
 to move from a sentence P to another sentence Q if both you and your
  
 important rule of thumb
  
 “audience” (the person or people you’re trying to convince) already know 
  
 that 
 Q is a logical consequence of P. The main exception to this rule is when 
  
 you give informal proofs to 
 your logic instructor: presumably, your instructor 
  
 knows the assigned argument is valid, so in these 
 circumstances, you have to 
  
 pretend you’re addressing the proof to someone who doesn’t already 
 know 
  
 that. What you’re really doing is convincing your instructor that
  you
  see that 
  
 the argument is valid and that you could prove it to someone who did not.
  
 The reason we start with this rule of thumb is that you’ve already learned 
 several well-known logical equivalences that you should feel free to use when 
 giving informal proofs. For example, you can freely use double negation or 
 idempotence if the need arises in a proof. Thus a chain of equivalences of the 
 sort we gave on page 119 is a legitimate component of an informal proof. Of 
 course, if you are asked to prove one of the named equivalences, say one of the 
 distribution or DeMorgan laws, then you shouldn’t presuppose it in your 
 proof. You’ll have to figure out a way to prove it to someone who doesn’t 
 already know that it is valid.
  
 A special case of this rule of thumb is the following: If you already know 
 that a sentence Q is a logical truth, then you may assert Q at any point in your 
 proof. We already saw this principle at work in Chapter 2, when we discussed 
 the reflexivity of identity, the principle that allowed us to assert a sentence of 
 the form a = a at any point in a proof. It also allows us to assert other simple 
 logical truths, like excluded middle (P
  ∧ ¬
 P), at any point in a proof. Of course, 
 the logical truths have to be simple enough that you can be sure your audience 
 will recognize them.
  
 There are three simple inference steps that we will mention here that don’t 
 involve logical equivalences or logical truths, but that are clearly supported by 
 the meanings of
  ∧
  and
  ∧
 . First, suppose we have managed to prove a 
 conjunction, say P
  ∧
  Q, in the course of our proof. The individual conjuncts P 
 and Q are clearly consequences of this conjunction, because there is no way 
 for the conjunction to be true without each conjunct being true. Thus, we",NA
Proof by cases,"The simple forms of inference discussed in the last section are all instances of 
 the principle that you can use already established cases of logical consequence 
 in informal proofs. But the Boolean connectives also give rise to two entirely 
 new methods of proof, methods that are explicitly applied in all types of 
 rigorous reasoning. The first of these is the method of
  proof by cases
 . In our 
 formal system
  F
 , this method will be called
  disjunction elimination
 , but don’t 
 be misled by the ordinary sounding name: it is far more significant than, say, 
 disjunction introduction or conjunction elimination.
  
 We begin by illustrating proof by cases with a well-known piece of math-
 ematical reasoning. The reasoning proves that there are irrational numbers
  b 
 and
  c
  such that
  b
 c
 is rational. First, let’s review what this means. A number is 
 said to be
  rational
  if it can be expressed as a fraction
  n/m
 , for integers
  
 n
  and
  m
 . If it can’t be so expressed, then it is irrational. Thus 2 is rational (2 = 
 2
 /
 1), but
 √
 2 is irrational. (We will prove this latter fact in the next sec-
  
 tion, to illustrate proof by contradiction; for now, just take it as a well-known 
 truth.) Here now is our proof:
  
 Proof:
  To show that there are irrational numbers
  b
  and
  c
  such that 
 b
 c
 is rational, we 
 will consider the number
 √
 2
 √
 2
 . We note that this
  
 number is either rational or irrational.",NA
Indirect proof: proof by contradiction,"One of the most important methods of proof is known as
  proof by contradic-
 tion
 . It is also called
  indirect proof
  or
  reductio ad absurdum
 . Its counterpart in
  
 F
  is called
  negation introduction
 . The basic idea is this. Suppose you want to 
 prove a negative sentence, say
  
 indirect proof or proof
  
 by contradiction
  
 ¬
 S, from some premises, say P
 1
 , . . . ,
  P
 n
 . One way to do this is by temporarily 
 assuming S and showing that a contradiction follows from this assumption. If 
  
 you can show this, 
 then you are entitled to conclude that
  ¬
 S is a logical conse-quence of the original premises. Why? Because 
 your proof of the contradiction 
  
 shows that S, P
 1
 , . . . ,
  P
 n
  cannot all be true simultaneously. (If they were, 
 the 
  
 contradiction would have to be true, and it can’t be.) Hence if P
 1
 , . . . ,
 P
 n
  are 
  
 true in any set of circumstances, then S must be false in those circumstances.
  
 Which is to say, if P
 1
 , . . . ,
  P
 n
  are all true, then
  ¬
 S must be true as well.
  
 Let’s look at a simple indirect proof. Assume Cube(c)
  ∧
  Dodec(c) and Tet(b). 
 Let us prove
  ¬
 (b = c).
  
 Proof:
  In order to prove
  ¬
 (b = c), we assume b = c and attempt to get 
 a contradiction. From our first premise we know that either Cube(c) 
 or Dodec(c). If the first is the case, then we conclude Cube(b) by the 
 indiscernibility of identicals, which contradicts Tet(b). But similarly, 
 if the second is the case, we get Dodec(b) which contra-dicts Tet(b). 
 So neither case is possible, and we have a contradiction.
  
 Thus our initial assumption that b = c must be wrong. So proof by 
 contradiction gives us our desired conclusion,
  ¬
 (b = c). (Notice that 
 this argument also uses the method of proof by cases.)
  
 Let us now give a more interesting and famous example of this method of 
 proof. The Greeks were shocked to discover that the square root of 2 could not 
 be expressed as a fraction, or, as we would put it, is irrational. The proof of 
 this fact proceeds via contradiction. Before we go through the proof, let’s 
 review some simple numerical facts that were well known to the Greeks. The 
 first is that any rational number can be expressed as a fraction
  p/q
  where at 
 least one of
  p
  and
  q
  is odd. (If not, keep dividing both the numerator and 
 denominator by 2 until one of them is odd.) The other fact follows from the 
 observation that when you square an odd number, you always get an odd 
 number. So if
  n
 2
 is an even number, then so is
  n
 . And from this, we see that",NA
Arguments with inconsistent premises,"What follows from an inconsistent set of premises? If you look back at our 
 definition of logical consequence, you will see that every sentence is a conse-
 quence of such a set. After all, if the premises are contradictory, then there are 
 no circumstances in which they are all true. Thus, there are no circum-stances 
 in which the premises are true and the conclusion is false. Which is to say, in 
 any situation in which the premises are all true (there aren’t any
  
 always valid 
  
 of these!), the conclusion will be true as well. Hence any argument with an 
  
 inconsistent set of premises is trivially valid. In particular, if one can establish 
  
 a contradiction
  ∧
  on the basis of the premises, then one is entitled to assert 
 any sentence at all.
  
 This often strikes students as a very odd method of reasoning, and for very 
 good reason. For recall the distinction between a valid argument and a sound 
 one. A
  sound
  argument is a valid argument with true premises. Even though 
 any argument with an inconsistent set of premises is valid, no such argument 
 is sound, since there is no way the premises of the argument can all be true. 
 For this reason, an argument with an inconsistent set of premises is not worth",NA
Formal Proofs and Boolean,NA,NA
Logic,"natural deduction
  
 The deductive system
  F
  is what is known as a system of
  natural deduction
 . 
 Such systems are intended to be models of the valid principles of reasoning
  
 used in informal proofs. In this chapter, we will present the inference rules of 
 F
  that correspond to the informal principles of Boolean reasoning discussed in 
 the previous chapter. You will easily recognize the rules as formal 
 counterparts of some of the principles we’ve already discussed.
  
 reasoning, they are also designed to be relatively spare or “stripped down”Although natural deduction 
 systems like
  F
  are meant to model informal
  
 versions of such reasoning. For example, we told you that in giving an informal 
 proof, you can always presuppose steps that you and your audience already 
 know to be logically valid. So if one of the equivalence laws is not at issue in a 
 proof, you can simply apply it in a single step of your informal proof. However, 
 in
  F
  we will give you a very elegant but restricted collection of inference rules 
 that you must apply in constructing a formal proof. Many of the valid inference 
 steps that we have seen (like the DeMorgan Laws) are not allowed as single 
 steps; they must be justified in terms of more basic steps. The advantage to 
 this “lean and mean” approach is that it makes it easier to prove results
  about
  
 the deductive system, since the fewer the rules, the simpler the system. For 
 example, one of the things we can prove is that anything you could 
 demonstrate with a system that contained rules for all of the named logical 
 equivalences of Chapter 4 can be proved in the leaner system
  F
 .
  
 one that allows us to prove statements containing the symbol, and one that Systems of natural deduction 
 like
  F
  use two rules for each connective,
  
 allows us to prove things
  from
  statements containing the symbol. The former
  
 introduction and 
  
 are called
  introduction
  rules since they let us introduce these symbols into
  
 elimination rules 
  
 proofs. By contrast, the latter are called
  elimination
  rules. This is similar to 
  
 our treatment of the identity predicate in Chapter 2. If you go on to study 
  
 proof theory in more advanced logic courses, you will see that that this elegant 
  
 pairing of rules has many advantages over systems that include more 
 inference 
  
 steps as basic.
  
 you to construct formal proofs much more easily than if you had to write The formal rules of
  F
  are all 
 implemented in the program Fitch, allowing
  
 them out by hand. Actually, Fitch’s interpretation of the introduction and",NA
Conjunction rules,"The simplest principles to formalize are those that involve the conjunction 
 symbol
  ∧
 . These are the rules of conjunction elimination and conjunction 
 introduction.
  
 Conjunction elimination
  
 The rule of conjunction elimination allows you to assert any conjunct P
 i
  of a 
 conjunctive sentence P
 1
  ∧ . . . ∧
  P
 i
  ∧ . . . ∧
  P
 n
  that you have already derived in 
 the proof. (P
 i
  can, by the way, be any conjunct, including the first or the last.) 
 You justify the new step by citing the step containing the conjunction.
  
 We abbreviate this rule with the following schema:
  
 Conjunction Elimination (
 ∧
  Elim):
  
 ∧
  
 P
 1
  ∧ . . . ∧
  P
 i
  ∧ . . . ∧
  P
 n 
 ...
  
 P
 i
  
 You try it 
  
 . .. . .. .. . .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . .. .. . ..
  
 1. Open the file Conjunction 1. There are three sentences that you are asked to 
 prove. They are shown in the goal strip at the bottom of the proof
  
 ◀
  
 window as usual.
  
 2. The first sentence you are to prove is Tet(a). To do this, first add a new step 
 to the proof and write the sentence Tet(a).
  
 ◀",NA
Disjunction rules,"We know: the conjunction rules were boring. Not so the disjunction rules, 
 particularly disjunction elimination.
  
 Disjunction introduction
  
 The rule of disjunction introduction allows you to go from a sentence P
 i
  to any 
 disjunction that has P
 i
  among its disjuncts, say P
 1
  ∧ . . . ∧
  P
 i
  ∧ . . . ∧
  P
 n
 . In 
 schematic form:
  
 Disjunction Introduction (
 ∧
  Intro):
  
 P
 i
 ...
  
 ∧
  
 P
 1
  ∧ . . . ∧
  P
 i
  ∧ . . . ∧
  P
 n
  
 Once again, we stress that P
 i
  may be the first or last disjunct of the conclusion. 
 Further, as with conjunction introduction, some thought ought to be given to 
 whether parentheses must be added to P
 i
  to prevent ambiguity.
  
 As we explained in Chapter 5, disjunction introduction is a less peculiar 
 rule than it may at first appear. But before we look at a sensible example of 
 how it is used, we need to have at our disposal the second disjunction rule.",NA
Negation rules,"Last but not least are the negation rules. It turns out that negation introduc-
  
 tion is our most interesting and complex rule.
  
 Chapter 6",NA
The proper use of subproofs,"Subproofs are the characteristic feature of Fitch-style deductive systems. It is 
 important that you understand how to use them properly, since if you are not 
 careful, you may “prove” things that don’t follow from your premises. For 
 example, the following formal proof looks like it is constructed according to
  
 our rules, but it purports to prove that A
  ∧
  B follows from (B
  ∧
  A)
  ∧
  (A
  ∧
  C), which is clearly 
 not right.
  
 1. (B
  ∧
  A)
  ∧
  (A
  ∧
  C)
  
  
 7. A
  
 8. A
  ∧
  B
  
  
  
 5. A
  ∧
  C
  
 6. A 
  
 2. B
  ∧
  A
  
  
 ∧
  Elim
 : 2
  
 ∧
  Elim
 : 2
  
 ∧
  Elim
 : 5
  
 ∧
  Elim
 : 1, 2–4, 5–6
  
 ∧
  Intro
 : 7, 3
  
 The problem with this proof is step 8. In this step we have used step 3, a 
 step that occurs within an earlier subproof. But it turns out that this sort of 
 justification—one that reaches back inside a subproof that has already 
 ended—is not legitimate. To understand why it’s not legitimate, we need to 
 think about what function subproofs play in a piece of reasoning.
  
 A subproof typically looks something like this:",NA
Strategy and tactics,"Many students try constructing formal proofs by blindly piecing together a se-
 quence of steps permitted by the introduction and elimination rules, a process 
 no more related to reasoning than playing solitaire. This approach occasion-
 ally works, but more often than not it will fail—or at any rate, make it harder 
 to find a proof. In this section, we will give you some advice about how to go 
 about finding proofs when they don’t jump right out at you. The advice 
 consists of two important strategies and an essential maxim.
  
 an important maxim 
 Here 
 is 
 the 
 maxim: 
 Always 
 keep 
 firmly 
 in 
 mind 
 what 
 the 
 sentences 
 in 
 your 
  
 proof 
 mean! 
 Students 
 who 
 pay 
 attention 
 to 
 the 
 meanings 
 of 
 the 
 sentences 
 avoid 
  
 innumerable 
 pitfalls, 
 among 
 them 
 the 
 pitfall 
 of 
 trying 
 to 
 prove 
 a 
 sentence 
 that 
  
 doesn’t 
 really 
 follow 
 from 
 the 
 information 
 given. 
 Your 
 first 
 step 
 in 
 trying 
 to 
  
 construct 
 a 
 proof 
 should
  
 always
  
 be 
 to 
 convince 
 yourself 
 that 
 the 
 claim 
 made 
  
 by 
 the 
 conclusion 
 is 
 a 
 consequence 
 of 
 the 
 premises. 
 You 
 should 
 do 
 this 
 even 
 if 
  
 the 
 exercise 
 tells 
 you 
 that 
 the 
 argument 
 is 
 valid 
 and 
 simply 
 asks 
 you 
 to 
 find 
 a 
  
 proof. 
 For 
 in 
 the 
 process 
 of 
 understanding 
 the 
 sentences 
 and 
 recognizing 
 the 
  
 argument’s validity, you will often get some idea how to prove it.
  
 After you’re convinced that the argument is indeed valid, the first strategy 
  
 for finding a formal proof is to try giving an informal proof, the kind you might 
  
  
 try informal proof 
 use to convince a fellow classmate. Often the basic structure of your informal 
  
 reasoning can be directly formalized using the rules of
  F
 . For example, if you find yourself using an 
 indirect proof, then that part of the reasoning will 
  
 you’ll almost surely formalize the proof using disjunction elimination. probably require negation 
 introduction in
  F
 . If you use proof by cases, then 
  
 Suppose you have decided that the argument is valid, but are having trou-
  
 ble finding an informal proof. Or suppose you can’t see how your informal 
  
 proof can be converted into a proof that uses just the rules of
  F
 . The second strategy is helpful in either of 
 these cases. It is known as “working backwards.”
 working backwards 
 What you do is look at the conclusion 
 and see what additional sentence or 
  
 sentences would allow you to infer that conclusion. Then you simply insert 
  
 these steps into your proof, not worrying about exactly how they will be jus-
  
 tified, and cite them in support of your goal sentence. You then take these 
  
 intermediate steps as new goals and see if you can prove them. Once you do, 
  
 your proof will be complete.
  
 Let’s work through an example that applies both of these strategies. Sup-pose 
 you are asked to give a formal proof of the argument:",NA
Proofs without premises,"Not all proofs begin with the assumption of premises. This may seem odd, but 
 in fact it is how we use our deductive system to show that a sentence is a 
 logical truth. A sentence that can be proven without any premises at all is
  
 necessarily true. Here’s a trivial example of such a proof, one that shows that a = 
 a
  ∧
  b = b is a logical truth.
  
 demonstrating 
 logical truth
  
  
  
  
 1. a = a
  
 2. b = b
  
 3. a = a
  ∧
  b = b
  
 =
  
 Intro 
  
 =
  
 Intro
  
 ∧
  Intro
 : 1, 2
  
 The first step of this proof is not a premise, but an application of
  = Intro
 . 
 You might think that any proof without premises would have to start with this 
 rule, since it is the only one that doesn’t have to cite any supporting steps 
 earlier in the proof. But in fact, this is not a very representative example of 
 such proofs. A more typical and interesting proof without premises is the 
 following, which shows that
  ¬
 (P
  ∧ ¬
 P) is a logical truth.
  
 Section 6.6",NA
Conditionals,"There are many logically important constructions in English besides the 
 Boolean connectives. Even if we restrict ourselves to words and phrases that 
 connect two simple indicative sentences, we still find many that go beyond the 
 Boolean operators. For example, besides saying:
  
 Max is home
  and
  Claire is at the library,
  
 and
  
 Max is home
  or
  Claire is at the library,
  
 we can combine these same atomic sentences in the following ways, among 
 others:
  
 Max is home
  if
  Claire is at the library, 
  
 Max is home
  only if
  Claire is at the library, 
  
 Max is home
  if and only if
  Claire is at the library, 
  
 Max is
  not
  home
  nor
  is Claire at the library, 
  
 Max is home
  unless
  Claire is at the library, 
  
 Max is home
  even though
  Claire is at the library, 
  
 Max is home
  in spite of the fact that
  Claire is at the library, Max is 
 home
  just in case
  Claire is at the library, 
  
 Max is home
  whenever
  Claire is at the library, 
  
 Max is home
  because
  Claire is at the library.
  
 And these are just the tip of the iceberg. There are also constructions that 
 combine three atomic sentences to form new sentences:
  
 If
  Max is home
  then
  Claire is at the library,
  otherwise
  Claire is 
 concerned,
  
 and constructions that combine four:
  
 If
  Max is home
  then
  Claire is at the library,
  otherwise
  Claire is 
 concerned
  unless
  Carl is with him,
  
 and so forth.
  
 Some of these constructions are truth functional, or have important truth-
 functional uses, while others do not. Recall that a connective is truth func-
 tional if the truth or falsity of compound statements made with it is 
 completely",NA
Material conditional symbol:,NA,NA
 →,"The symbol
  →
  is used to combine two sentences P and Q to form a new 
 sentence P
  →
  Q, called a
  material conditional
 . The sentence P is called the 
 antecedent
  of the conditional, and Q is called the
  consequent
  of the conditional. 
 We will discuss the English counterparts of this symbol after we explain its 
 meaning.
  
 Semantics and the game rule for the conditional
  
 truth table for →
  
 game rule for →
  
 The sentence P
  →
  Q is true if and only if either P is false or Q is true (or both). 
 This can be summarized by the following truth table.
  
 P
  
 Q
   
 P
  →
  Q
  
 t
  
 t
  
 T
  
 t
  
 f
  
 F
  
 f
  
 t
  
 T
  
 f
  
 f
  
 T
  
  
 A second’s thought shows that P
  →
  Q is really just another way of saying
 ¬
 P
  ∧
  
 Q. Tarski’s World in fact treats the former as an abbreviation of the latter. In 
 particular, in playing the game, Tarski’s World simply replaces a
  
 statement of the form P
  →
  Q by its equivalent
  ¬
 P
  ∧
  Q.
  
  
 Remember
  
  
 1. If P and Q are sentences of fol, then so is P
  →
  Q. 
  
 2. The sentence P
  →
  Q is false in only one case: if the antecedent P is true 
 and the consequent Q is false. Otherwise, it is true.
  
 English forms of the material conditional
  
 We can come fairly close to an adequate English rendering of the material
  
 if . . . then 
  
 conditional P
  →
  Q with the sentence
  If P then Q
 . At any rate, it is clear that
  
 Chapter 7",NA
Biconditional symbol:,NA,NA
 ↔,"Our final connective is called the material biconditional symbol. Given any 
 sentences P and Q there is another sentence formed by connecting these by
  
 Section 7.2",NA
Conversational implicature,"In translating from English to fol, there are many problematic cases. For 
 example, many students resist translating a sentence like
  Max is home unless 
 Claire is at the library
  as:
  
 ¬
 Library(claire)
  →
  Home(max)
  
 Section 7.3",NA
Truth-functional completeness,"We now have at our disposal five truth-functional connectives, one unary (
 ¬
 ), 
 and four binary (
 ∧
 ,
  ∧
 ,
  →
 ,
  ↔
 ). Should we introduce any more? Though we’ve 
 seen a few English expressions that can’t be expressed in fol, like 
 because
 , 
 these have not been truth functional. We’ve also run into others, like 
 neither. 
 ..nor...
  , that
  are
  truth functional, but which we can easily express using the 
 existing connectives of fol.
  
 The question we will address in the current section is whether there are 
 any truth-functional connectives that we need to add to our language. Is it 
 possible that we might encounter an English construction that is truth 
 functional but which we cannot express using the symbols we have 
 introduced so far? If so, this would be an unfortunate limitation of our 
 language.
  
 How can we possibly answer this question? Well, let’s begin by thinking 
 about binary connectives, those that apply to two sentences to make a third. 
 How many binary truth-functional connectives are possible? If we think about 
 the possible truth tables for such connectives, we can compute the total num-
 ber. First, since we are dealing with binary connectives, there are four rows in 
 each table. Each row can be assigned either true or false, so there are 2
 4
 = 16 
 ways of doing this. For example, here is the table that captures the truth 
 function expressed by
  neither... nor. ..
 .",NA
Alternative notation,"As with the other truth-functional connectives, there are alternative notations 
 for the material conditional and biconditional. The most common alternative 
 to P
  →
  Q is P
  ∧
  Q. Polish notation for the conditional is Cpq. The most com-
 mon alternative to P
  ↔
  Q is P
  ≡
  Q. The Polish notation for the biconditional is 
 Epq.",NA
The Logic of Conditionals,"One thing the theorem on page 193 tells us is that introducing the material 
 conditional and biconditional symbols did not increase the expressive power 
 of fol. Since
  →
  and
  ↔
  can be defined using the Boolean connectives, we could 
 always eliminate them from claims or proofs by means of these definitions. 
 Thus, for example, if we wanted to prove P
  →
  Q we could just prove
  ¬
 P
  ∧
  Q, 
 and then use the definition. In practice, though, this is a terrible idea. It is far 
 more natural to use rules that involve these symbols directly, and the resulting 
 proofs are simpler and easier to understand.
 1 
  
 The material conditional, in particular, is an extremely useful symbol to have. 
 For example, many claims and theorems that we will run across can only be 
 expressed naturally using the conditional. In fact, quite a few of the examples 
 we’ve already used are more naturally stated as conditional claims. Thus in an 
 earlier exercise we asked you to prove Even(n
  ×
  m) from the premise Odd(n + 
 m). But really, the fact we were interested in was that, no matter what 
 numbers
  n
  and
  m
  you pick, the following
  conditional
  claim is true:
  
 Odd(n + m)
  →
  Even(n
  ×
  m)
  
 Given the importance of conditional claims, and the frequency you’ll en-
 counter them, we need to learn how to prove these claims.
  
 Section 8.1",NA
Informal methods of proof,"As before, we will first look at informal proofs involving conditionals and later 
 incorporate the key methods into the system
  F
 . Among the informal methods, 
 we distinguish simple valid steps from more important methods of proof.
  
 Valid steps
  
 modus ponens or 
  
 conditional elimination
  
 The most common valid proof step involving
  →
  goes by the Latin name
  modus 
 ponens
 , or by the English
  conditional elimination
 . The rule says that if you",NA
Formal rules of proof for,NA,NA
 →,NA,NA
 and,NA,NA
 ↔,"We now turn to the formal analogues of the methods of proof involving the 
 conditional and biconditional. Again, we incorporate an introduction and elim-
 ination rule for each connective into
  F
 .
  
 Rules for the conditional
  
 The rule of
  modus ponens
  or conditional elimination is easily formalized. If you 
 have proven both P
  →
  Q and P then you can assert Q, citing as justification these 
 two earlier steps. Schematically:
  
 Conditional Elimination (
 →
  Elim):
  
 P
  →
  Q ...
  
 P...
  
 ∧
  
 Q
  
 The corresponding introduction rule is the formal counterpart of the 
 method of conditional proof. As you would expect, it requires us to construct a 
 sub-proof. To prove a statement of the form P
  →
  Q we begin our subproof with 
 the assumption of P and try to prove Q. If we succeed, then we are allowed to 
 discharge the assumption and conclude our desired conditional, citing the 
 subproof as justification. Schematically:
  
 Conditional Introduction (
 →
  Intro):
  
 ∧
  
  
  
  
  
 P 
  
 ...
  
 Q
  
 P
  →
  Q
  
 Strategy and tactics
  
 The strategy of working backwards usually works extremely well in proofs that 
 involve conditionals, particularly when the desired conclusion is itself a",NA
Soundness and completeness,"We have now introduced formal rules for all of our truth-functional connec-
 tives. Let’s step back for a minute and ask two important questions about the 
 formal system
  F
 . The questions get at two desirable properties of a deductive 
 system, which logicians call
  soundness
  and
  completeness.
  Don’t be confused by 
 the names, however. These uses of
  sound
  and
  complete
  are different from their 
 use in the notions of a sound argument and a truth-functionally complete set 
 of connectives.
  
 Soundness
  
 soundness of a 
  
 deductive system
  
 We intend our formal system
  F
  to be a correct system of deduction in the 
 sense that any argument that can be proven valid in
  F
  should be genuinely 
 valid. The first question that we will ask, then, is whether we have succeeded
  
 in this goal. Does the system
  F
  allow us to construct proofs only of genuinely 
 valid arguments? This is known as the soundness question for the deductive
  
 system
  F
 . The answer to this question may seem obvious, but it deserves a 
 closer look.
  
 After all, consider the rule of inference suggested in Exercise 7.32 on page 196. 
 Probably, when you first looked at this rule, it seemed pretty reasonable, even 
 though on closer inspection you realized it was not (or maybe you got the 
 problem wrong). How can we be sure that something similar might not be the 
 case for one of our official rules? Maybe there is a flaw in one of them but we 
 just haven’t thought long enough or hard enough to discover it.
  
 Or maybe there are problems that go beyond the individual rules, some-
 thing about the way the rules interact. Consider for example the following 
 argument:
  
  
  
  
  
 ¬
 (Happy(carl)
  ∧
  
 Happy(scruffy))
 ¬
 Happy(carl)
  
 We know this argument isn’t valid since it is clearly possible for the premise 
 to be true and the conclusion false. But how do we know that the rules of 
 proof we’ve introduced do not allow some very complicated and ingenious 
 proof of the conclusion from the premise? After all, there is no way to examine 
 all possible proofs and make sure there isn’t one with this premise and 
 conclusion: there are infinitely many proofs.
  
 To answer our question, we need to make it more precise. We have seen that",NA
Valid arguments: some review exercises,"There is wisdom in the old saying “Don’t lose sight of the forest for the trees.” 
 The forest in our case is an understanding of valid arguments. The trees are 
 the various methods of proofs, formal and informal, and the notions of 
 counterexample, tautology, and the like. The problems in this section are 
 intended to remind you of the relationship between the forest and the trees, 
 as well as to help you review the main ideas discussed so far.
  
 Since you now know that our introduction and elimination rules suffice to 
 prove any tautologically valid argument, you should feel free to use
  Taut Con 
 in doing these exercises. In fact, you may use it in your formal proofs from 
 now on, but with this important proviso: Make sure that you use it only in 
 cases where the inference step is obvious and would go by without notice in 
 an informal proof. For example, you may use it to introduce the law of 
 excluded middle or to apply a DeMorgan equivalence. But you should still use 
 rules like
 ∧
  Elim
 ,
  ¬
  Intro
 , and
  →
  Intro
  when your informal proof would use 
 proof by cases, proof by contradiction, or conditional proof. Any one-step 
 proofs that consist of a single application of
  Taut Con
  will be counted as 
 wrong!
  
 Before doing these problems, go back and read the material in the
  Re-
 member
  boxes, paying special attention to the strategy for evaluating argu-
 ments on page 171.
  
 Remember
  
  
 From this point on in the book, you may use
  Taut Con
  in formal proofs,
  
  
 but only to skip simple steps that would go unmentioned in an informal",NA
Part II ,NA,NA
Quantifiers,225,NA
Introduction to Quantification,"In English and other natural languages, basic sentences are made by combining 
  
 noun phrases and verb phrases. The simplest noun phrases are names, like
  Max 
  
 and
  Claire
 , which correspond to the constant symbols of fol. More complex 
  
 noun phrases are formed by combining common nouns with words known as 
  
 determiners,
  such as
  every, some, most, the, three,
  and
  no
 , giving us noun 
 determiners 
 phrases like
  every 
 cube
 ,
  some man from Indiana
 ,
  most children in the class
 , 
  
 the dodecahedron in the corner
 ,
  three blind mice
 , and
  no student of logic
 .
  
 Logicians call noun phrases of this sort
  quantified expressions,
  and sen-
  
 tences containing them
  quantified sentences.
  They are so called because they 
  
 quantified sentences 
 allow us to talk about quantities of things—every cube, most children, and so 
  
 forth.
  
 The logical properties of quantified sentences are highly dependent on which 
 determiner is used. Compare, for example, the following arguments:
  
 Every rich actor is a good actor.
  
  
  
 Brad Pitt is a rich actor. 
  
 Brad Pitt is a good actor.
  
 Many rich actors are good actors.
  
  
  
 Brad Pitt is a rich actor. 
  
 Brad Pitt is a good actor.
  
 No rich actor is a good actor.
  
  
  
 Brad Pitt is a rich actor. 
  
 Brad Pitt is a good actor.
  
 What a difference a determiner makes! The first of these arguments is obvi-
 ously valid. The second is not logically valid, though the premises do make the 
 conclusion at least plausible. The third argument is just plain dumb: in fact the 
 premises logically imply the
  negation
  of the conclusion. You can hardly get a 
 worse argument than that.
  
 Quantification takes us out of the realm of truth-functional connectives. 
 Notice that we can’t determine the truth of quantified sentences by looking at 
 the truth values of constituent sentences. Indeed, sentences like
  Every rich
  
 227",NA
Variables and atomic wffs,"Before we can show you how fol’s quantifier symbols work, we need to intro-
  
 variables 
  
 duce a new type of term, called a
  variable
 . Variables are a kind of auxiliary 
  
 symbol. In some ways they behave like individual constants, since they can 
  
 appear in the list of arguments immediately following a predicate or function 
  
 symbol. But in other ways they are very different from individual constants. In 
  
 particular, their semantic function is not to refer to objects. Rather, they are",NA
The quantifier symbols:,NA,NA
" ∧, ∧","The quantifier symbols
  ∧
  and
  ∧
  let us express certain rudimentary claims 
 about the number (or quantity) of things that satisfy some condition. Specif-
 ically, they allow us to say that
  all
  objects satisfy some condition, or that 
 at 
 least one
  object satisfies some condition. When used in conjunction with 
 identity (=), they can also be used to express more complex numerical claims, 
 for instance, that there are
  exactly three things
  that satisfy some condition.
  
 Universal quantifier (
 ∧
 )
  
 everything, each thing, 
  
 The symbol
  ∧
  is used to express universal claims, those we express in English 
 using quantified phrases like
  everything, each thing, all things,
  and
  anything
 .
  
 all things, anything 
  
 It is always used in connection with a variable, and so is said to be a variable 
  
 binding operator. The combination
  ∧
 x is read “for every object x,” or (some-
 what misleadingly) “for all x.”
 1
 If we wanted to translate the (rather unlikely) 
  
 English sentence
  
 Everything is at home
  into first-order logic, we would use the 
  
 fol sentence
  
  
 ∧
 x Home(x) 
  
  
 This says that every object x meets the following condition: x is at home. Or, 
  
 to put it more naturally, it says that everything whatsoever is at home.
  
 Of course, we rarely make such unconditional claims about absolutely ev-
 erything. More common are restricted universal claims like
  Every doctor is 
 smart
 . This sentence would be translated as
  
 ∧
 x (Doctor(x)
  →
  Smart(x))
  
 This fol sentence claims that given any object at all—call it x—if x is a doctor, 
 then x is smart. To put it another way, the sentence says that if you pick 
 anything at all, you’ll find either that it is not a doctor or that it is smart (or 
 perhaps both).
  
 Existential quantifier (
 ∧
 )
  
 something, at least one 
 thing, a, an
  
 The symbol
  ∧
  is used to express existential claims, those we express in English 
 using such phrases as
  something, at least one thing, a,
  and
  an
 . It too is always
  
  
 1
 We encourage students to use the first locution when reading formulas, at least for a
  
 few weeks, since we have seen many students who have misunderstood the basic function
  
 of variables as a result of reading them the second way.
  
 Chapter 9",NA
Wffs and sentences,"Notice that in some of the above examples, we formed
  sentences
  out of complex 
 expressions that were not themselves sentences, expressions like
  
 Doctor(x)
  ∧
  Smart(x)
  
 that contain variables not bound by any quantifier. Thus, to systematically 
 describe all the sentences of first-order logic, we first need to describe a larger 
 class, the so-called well-formed formulas, or
  wffs
 .
  
 We have already explained what an atomic wff is: any
  n
 -ary predicate 
  
 followed by
  n
  terms, where terms can now contain either variables or individ-
  
 ual constants. We will say that any variable that occurs in an atomic wff is 
  
 free
  or
  unbound
 . Using atomic wffs as our building blocks, we can construct 
  
 free variable 
 more complicated wffs by repeatedly applying the following rules. Note that 
  
 the last two clauses also explain how variables become
  bound
  when we apply 
 bound variable 
 quantifiers to 
 wffs.
  
 1. If P is a wff, so is
  ¬
 P.
  
 2. If P
 1
 , . . . ,
  P
 n
  are wffs, so is (P
 1
  ∧ . . . ∧
  P
 n
 ). 3. If 
 P
 1
 , . . . ,
  P
 n
  are wffs, so is (P
 1
  ∧ . . . ∧
  P
 n
 ). 4. If P 
 and Q are wffs, so is (P
  →
  Q).
  
 well-formed 
  
 formula (wff)",NA
Semantics for the quantifiers,"When we described the meanings of our various connectives, we told you how 
 the truth value of a complex sentence, say
  ¬
 P, depends on the truth values of 
 its constituents, in this case P. But we have not yet given you similar rules for 
 determining the truth value of quantified sentences. The reason is simple: the 
 expression to which we apply the quantifier in order to build a sentence is 
 usually not itself a sentence. We could hardly tell you how the truth value of
 ∧
 x 
 Cube(x) depends on the truth value of Cube(x), since this latter expression is 
 not a sentence at all: it contains a free variable. Because of this, it is
  neither 
 true
  nor
  false.
  
 To describe when quantified sentences are true, we need to introduce the
  
 satisfaction 
  
 auxiliary notion of
  satisfaction
 . The basic idea is simple, and can be illustrated 
  
 with a few examples. We say that an object satisfies the atomic wff Cube(x) 
  
 if and only if the object is a cube. Similarly, we say an object satisfies the 
  
 complex wff Cube(x)
  ∧
  Small(x) if and only if it is both a cube and small. As 
  
 a final example, an object satisfies the wff Cube(x)
  ∧ ¬
 Large(x) if and only if it 
 is either a cube or not large (or both).",NA
The four Aristotelian forms,"Long before fol was codified, Aristotle studied the kinds of reasoning associ-
 ated with quantified noun phrases like
  Every man
 ,
  No man
 , and
  Some man
 , 
 expressions we would translate using our quantifier symbols. The four main 
 sentence forms treated in Aristotle’s logic were the following.
  
 All P’s are Q’s
  
 Some P’s are Q’s 
  
 No P’s are Q’s
  
 Aristotelian forms
  
 Some P’s are not Q’s
  
 We will begin by looking at the first two of these forms, which we have 
 already discussed to a certain extent. These forms are translated as follows.
  
 The form
  All P’s are Q’s
  is translated as:
  
 ∧
 x (P(x)
  →
  Q(x))
  
 whereas the form
  Some P’s are Q’s
  is translated as:
  
 ∧
 x (P(x)
  ∧
  Q(x))
  
 Beginning students are often tempted to translate the latter more like the 
 former, namely as:
  
  
  
 ∧
 x (P(x)
  →
  Q(x)) 
  
 This is in fact an extremely unnatural sentence of first-order logic. It is mean-
 ingful, but it doesn’t mean what you might think. It is true just in case there is 
 an object which is either not a P or else is a Q, which is something quite",NA
Translating complex noun phrases,"The first thing you have to learn in order to translate quantified English 
  
 expressions is how to treat complex noun phrases, expressions like “a boy 
  
 living in Omaha” or “every girl living in Duluth.” In this section we will 
  
 learn how to do this. We concentrate first on the former sort of noun phrase, 
  
 whose most natural translation involves an existential quantifier. Typically, 
  
 these will be noun phrases starting with one of the determiners
  some
 ,
  a
 , and 
  
 an
 , including noun phrases like
  something
 . These are called existential noun 
  
  
 existential 
 phrases, since they assert the existence of something or other. Of course two 
 noun phrases
  
 of our four Aristotelian forms involve existential noun phrases, so we know 
 the general pattern: existential noun phrases are usually translated using
  ∧
 , 
 frequently together with
  ∧
 . Let’s look at a simple example. Suppose we wanted 
 to translate the sen-tence
  A small, happy dog is at home
 . This sentence claims 
 that there is an object which is simultaneously a small, happy dog, and at 
 home. We would translate it as
  
 ∧
 x [(Small(x)
  ∧
  Happy(x)
  ∧
  Dog(x))
  ∧
  Home(x)]
  
 We have put parentheses around the first three predicates to indicate that 
 they were all part of the translation of the subject noun phrase. But this is not 
 really necessary.
  
 Universal noun phrases are those that begin with determiners like
  every
 , 
  
 universal 
 each
 , and
  all
 . 
  
 These are usually translated with the universal quantifier. 
 noun phrases
  
 Sometimes noun phrases beginning with
  no
  and with
  any
  are also translated 
 with the universal quantifier. 
  
 Two of our four Aristotelian forms involve 
 universal noun phrases, so we also know the general pattern here: universal 
 noun phrases are usually translated using
  ∧
 , frequently together with
  →
 . Let’s 
 consider the sentence
  Every small dog that is at home is happy
 . This claims that 
 everything with a complex property, that of being a small dog at home, has 
 another property, that of being happy. This suggests that the
  
 Section 9.6",NA
Quantifiers and function symbols,"When we first introduced function symbols in Chapter 1, we presented them 
 as a way to form complex names from other names. Thus father(father(max)) 
 refers to Max’s father’s father, and (1 + (1 + 1)) refers to the number 3. Now 
 that we have variables and quantifiers, function symbols become much more 
 useful than they were before. For example, they allow us to express in a very 
 compact way things like:
  
 ∧
 x Nicer(father(father(x))
 ,
  father(x))
  
 This sentence says that everyone’s paternal grandfather is nicer than their 
 father, a false belief held by many children.
  
 Notice that even if our language had individual constants naming every-
 one’s father (and their fathers’ fathers and so on), we could not express the 
 above claim in a single sentence without using the function symbol father.
  
 Section 9.7",NA
Alternative notation,"The notation we have been using for the quantifiers is currently the most 
 popular. An older notation that is still in some use employs (x) for
  ∧
 x. Thus, for 
 example, in this notation our
  
 ∧
 x [Tet(x)
  →
  Small(x)]
  
 would be written: 
  
  
 (x) [Tet(x)
  →
  Small(x)] 
  
 Another notation that is occasionally used exploits the similarity between 
 universal quantification and conjunction by writingx instead of
  ∧
 x. In this 
 notation our sentence would be rendered:
  
 x [Tet(x)
  →
  Small(x)]
  
 Finally, you will sometimes encounter the universal quantifier written Πx, as 
 in:
  
  
 Πx [Tet(x)
  →
  Small(x)]
  
 versions writex or Σx. Thus the following are notational variants of one Similar variants of
  ∧
 x are in use. 
 One version writes (
 ∧
 x) or (Ex). Other
  
 another.
  
 ∧
 x [Cube(x)
  ∧
  Large(x)] 
  
 (Ex)[Cube(x)
  ∧
  
 Large(x)] 
  
    
 x [Cube(x)
  ∧",NA
The Logic of Quantifiers,"We have now introduced all of the symbols of first-order logic, though we’re 
  
 nowhere near finished learning all there is to know about them. Before we 
  
 go on, we should explain where the “first-order” in “first-order logic” comes 
  
 first-order logic 
 from. It has to do with the kinds of things that our quantifiers quantify over.
  
 In fol we are allowed to say things like
  ∧
 x Large(x), that is,
  there is something 
 that has the property of being large.
  But we can’t say things like
  there is some 
 First-order quantifiers allow us to make quantity claims about ordinary 
 property that Max has: ∧
 P P(max).
  
 objects: blocks, people, numbers, sets, and so forth. (Note that we are very 
  
 liberal about what an ordinary object is.) If, in addition, we want to make 
  
 quantity claims about properties of the objects in our domain of discourse—
  
 say we want to claim that Max and Claire share exactly two properties—then 
  
 we need what is known as
  second-order
  quantifiers. Since our language only 
 second-order 
  
  
 quantifiers 
 has first-order quantifiers, it is known as the language of first-order logic: fol.
  
 Now that we’ve learned the basics of how to express ourselves using first-
 order quantifiers, we can turn our attention to the central issues of logical 
 consequence and logical truth:
  
 What quantified sentences are logical truths?
  
 What arguments involving quantification are valid?
  
 What are the valid inference patterns involving quantifiers?
  
 How can we formalize these valid patterns of inference?
  
 In this chapter we take up the first two questions; the remaining two are 
 treated in Chapters 12 and 13.
  
 Section 10.1",NA
Tautologies and quantification,"Introducing quantifiers required a much more radical change to the language 
 than introducing additional truth-functional connectives. Because of the way 
 quantifiers work, we had to introduce the notion of a well-formed formula, 
 something very much like a sentence except that it can contain free vari-ables. 
 Quantifiers attach to these wffs, bind their variables, and thereby form",NA
First-order validity and consequence,"When we first discussed the intuitive notions of logical truth and logical con-
 sequence, we appealed to the idea of a logically possible circumstance. We 
 described a logically valid argument, for example, as one whose conclusion is 
 true in every possible circumstance in which all the premises are true. When 
 we needed more precision than this description allowed, we introduced truth 
 tables and the concepts of tautology and tautological consequence. These 
 concepts add precision by modeling possible circumstances as rows of a truth 
 table. We have seen that this move does a good job of capturing the intu-itive 
 notions of logical truth and logical consequence—provided we limit our 
 attention to the truth-functional connectives.
  
 Unfortunately, the concepts of tautology and tautological consequence 
 don’t get us far in first-order logic. We need a more refined method for ana-
 lyzing logical truths and logically valid arguments when they depend on the 
 quantifiers and identity. We will introduce these notions in this chapter, and 
 develop them in greater detail in Chapter 18. The notions will give us, for first-
 order logic, what the concepts of tautology and tautological consequence gave 
 us for propositional logic: precise approximations of the notions of logical 
 truth and logical consequence.
  
 First, a terminological point. It is a regrettable fact that there is no single 
 term like “tautological” that logicians consistently use when applying the 
 various logical notions to first-order sentences and arguments. That is, we 
 don’t have a uniform way of filling out the table:",NA
First-order equivalence and DeMorgan’s laws,"There are two ways in which we can apply what we learned about tautolog-
 ical equivalence to first-order sentences. First of all, if you apply the truth-
 functional form algorithm to a pair of sentences and the resulting forms are 
 tautologically equivalent, then of course the original sentences are first-order 
 equivalent. For example, the sentence:
  
  
 ¬
 (
 ∧
 x Cube(x)
  ∧ ∧
 y Dodec(y)) 
 is tautologically equivalent to:
  
 ¬∧
 x Cube(x)
  ∧ ¬∧
 y Dodec(y) 
  
 When you apply the truth-functional form algorithm, you see that this is just an 
 instance of one of DeMorgan’s laws.
  
 But it turns out that we can also apply DeMorgan, and similar principles, 
 inside the scope of quantifiers. Let’s look at an example involving the Law of 
 Contraposition. Consider the sentences:
  
  
  
 ∧
 x (Cube(x)
  →
  Small(x))
  
  
 ∧
 x (
 ¬
 Small(x)
  → ¬
 Cube(x)) 
  
 A moment’s thought will convince you that each of these sentences is a first-
 order consequence of the other, and so they are first-order equivalent. But 
 unlike the previous examples, they are not tautologically equivalent.
  
 To see why Contraposition (and other principles of equivalence) can be 
 applied in the scope of quantifiers, we need to consider the wffs to which the 
 principle was applied:
  
  
  
 Cube(x)
  →
  Small(x)
  
  
 ¬
 Small(x)
  → ¬
 Cube(x) 
  
 Or, more generally, consider the wffs:
  
  
  
 P(x)
  →
  Q(x)
  
  
 ¬
 Q(x)
  → ¬
 P(x) 
  
 where P(x) and Q(x) may be any formulas, atomic or complex, containing the 
 single free variable x.
  
 Now since these formulas are not sentences, it makes no sense to say they 
 are true in exactly the same circumstances, or that they are logical (or tauto-
 logical) consequences of one another. Formulas with free variables are neither
  
 Section 10.3",NA
Other quantifier equivalences,"The quantifier DeMorgan laws tell us how quantifiers interact with negation. 
 Equally important is the question of how quantifiers interact with conjunction 
 and disjunction. The laws governing this interaction, though less interesting 
 than DeMorgan’s, are harder to remember, so you need to pay attention!
  
 quantifiers and Boolean 
 connectives
  
 First of all, notice that
  ∧
 x (P(x)
  ∧
  Q(x)), which says that everything is both 
 P and Q, is logically equivalent to
  ∧
 x P(x)
  ∧ ∧
 x Q(x), which says that 
 everything is P and everything is Q. These are just two different ways of saying
  
 that every object in the domain of discourse has both properties P and Q. By 
 contrast,
  ∧
 x (P(x)
  ∧
  Q(x)) is
  not
  logically equivalent to
  ∧
 x P(x)
  ∧ ∧
 x Q(x). For 
 example, the sentence
  ∧
 x (Cube(x)
  ∧
  Tet(x)) says that everything is either a 
 cube or a tetrahedron, but the sentence
  ∧
 x Cube(x)
  ∧ ∧
 x Tet(x) says that 
 either everything is a cube or everything is a tetrahedron, clearly a very 
 different kettle of fish. We summarize these two observations, positive and 
 negative, as follows:
  
 ∧
 x (P(x)
  ∧
  Q(x))
  
 ∧
  
 ∧
 x P(x)
  ∧ ∧
 x 
 Q(x)
  
 ∧
 x (P(x)
  ∧
  Q(x))
  
  
 ∧/
  
 ∧
 x P(x)
  ∧ ∧
 x 
 Q(x)
  
 other way around. The claim that there is some object that is either P or Similar observations hold with
  
 ∧
 ,
  ∧
 , and
  ∧
 , except that it works the
  
 Q,
  ∧
 x (P(x)
  ∧
  Q(x)), is logically equivalent to the claim that something is P or 
 something is Q:
  ∧
 x P(x)
  ∧ ∧
 x Q(x). But this equivalence fails the mo-ment we 
 replace
  ∧
  with
  ∧
 . The fact that there is a cube and a tetrahedron,
 ∧
 x Cube(x)
  ∧ 
 ∧
 x Tet(x), hardly means that there is something which is both a cube and a 
 tetrahedron:
  ∧
 x (Cube(x)
  ∧
  Tet(x))! Again, we summarize both positive and 
 negative observations together:
  
  
 ∧
 x (P(x)
  ∧
  Q(x))
  
 ∧
  
 ∧
 x P(x)
  ∧ ∧
 x Q(x)
  
  
 ∧
 x (P(x)
  ∧
  Q(x))
  
  
 ∧/
  
 ∧
 x P(x)
  ∧ ∧
 x Q(x) 
  
 There is one circumstance when you can push a universal quantifier in past a 
 disjunction, or move an existential quantifier out from inside a conjunction. 
 But to explain this circumstance, we first have to talk a bit about a degenerate 
 form of quantification. In defining the class of wffs, we did not insist that the 
 variable being quantified actually occur free (or at all) in the wff to which the",NA
The axiomatic method,"As we will see in the coming chapters, first-order consequence comes much 
 closer to capturing the logical consequence relation of ordinary language than 
 does tautological consequence. This will be apparent from the kinds of sen-
 tences that we can translate into the quantified language and from the kinds of 
 inference that turn out to be first-order valid.
  
 Still, we have already encountered several arguments that are intuitively 
 valid but not first-order valid. Let’s look at an example where the replacement
  
 Section 10.5",NA
Multiple Quantifiers,"So far, we’ve considered only sentences that contain a single quantifier 
 symbol. This was enough to express the simple quantified forms studied by 
 Aristotle, but hardly shows the expressive power of the modern quantifiers of 
 first-order logic. Where the quantifiers of fol come into their own is in 
 expressing claims which, in English, involve several quantified noun phrases.
  
 Long, long ago, probably before you were even born, there was an 
 advertis-ing campaign that ended with the tag line:
  Everybody doesn’t like 
 something, but nobody doesn’t like Sara Lee.
  Now there’s a quantified sentence! 
 It goes without saying that this was every logician’s favorite ad campaign. Or 
 con-sider Lincoln’s famous line:
  You may fool all of the people some of the time; 
 you can even fool some of the people all of the time; but you can’t fool all of the 
 people all of the time.
  Why, the mind reels!
  
 To express claims like these, and to reveal their logic, we need to juggle 
 more than one quantifier in a single sentence. But it turns out that, like 
 juggling, this requires a fair bit of preparation and practice.
  
 Section 11.1",NA
Multiple uses of a single quantifier,"When you learn to juggle, you start by tossing balls in a single hand, not 
 crossing back and forth. We’ll start by looking at sentences that have multiple 
 instances of
  ∧
 , or multiple instances of
  ∧
 , but no mixing of the two. Here are a 
 couple of sentences that contain multiple quantifiers:
  
  
  
 ∧
 x
  ∧
 y [Cube(x)
  ∧
  Tet(y)
  ∧
  LeftOf(x
 ,
 y)]
  
  
 ∧
 x
  ∧
 y [(Cube(x)
  ∧
  Tet(y))
  →
  LeftOf(x
 ,
  y)] 
  
 Try to guess what these say. You shouldn’t have any trouble: The first says that 
 some cube is left of a tetrahedron; the second says that every cube is left of 
 every tetrahedron.
  
 In these examples, all the quantifiers are out in front (in what we’ll later 
 call
  prenex form
 ) but there is no need for them to be. In fact the same claims 
 could be expressed, perhaps more clearly, by the following sentences:",NA
Mixed quantifiers,"Ready to start juggling with both hands? We now turn to the important case in 
 which universal and existential quantifiers get mixed together. Let’s start with 
 the following sentence:
  
 ∧
 x [Cube(x)
  → ∧
 y (Tet(y)
  ∧
  LeftOf(x
 ,
 y))]
  
 This sentence shouldn’t throw you. It has the overall Aristotelian form
 ∧
 x 
 [P(x)
  →
  Q(x)], which we have seen many times before. It says that every cube 
 has some property or other. What property? The property expressed",NA
The step-by-step method of translation,"When an English sentence contains more than one quantified noun phrase, 
 translating it can become quite confusing unless you approach it in a very 
 systematic way. It often helps to go through a few intermediate steps, treating 
 the quantified noun phrases one at a time.
  
 Suppose, for example, we wanted to translate the sentence
  Each cube is to 
 the left of a tetrahedron
 . Here, there are two quantified noun phrases:
  each 
 cube
  and
  a tetrahedron
 . We can start by dealing with the first noun phrase, 
 temporarily treating the complex phrase
  is-to-the-left-of-a-tetrahedron
  as a 
 single unit. In other words, we can think of the sentence as a single quantifier 
 sentence, on the order of
  Each cube is small
 . The translation would look like 
 this:
  
 ∧
 x (Cube(x)
  →
  x
  is-to-the-left-of-a-tetrahedron
 ) 
  
 Of course, this is not a sentence in our language, so we need to translate the 
 expression x
  is-to-the-left-of-a-tetrahedron
 . But we can think of this expression 
 as a single quantifier sentence, at least if we pretend that x is a name. It has the 
 same general form as the sentence
  b
  is to the left of a tetrahedron
 , and would 
 be translated as
  
 ∧
 y (Tet(y)
  ∧
  LeftOf(x
 ,
  y))
  
 Substituting this in the above, we get the desired translation of the original 
 English sentence:
  
 ∧
 x (Cube(x)
  → ∧
 y (Tet(y)
  ∧
  LeftOf(x
 ,
  y)))
  
 This is exactly the sentence with which we began our discussion of mixed 
 quantifiers.
  
 This step-by-step process really comes into its own when there are lots of 
 quantifiers in a sentence. It would be very difficult for a beginner to trans-late 
 a sentence like
  No cube to the right of a tetrahedron is to the left of a larger 
 dodecahedron
  in a single blow. Using the step-by-step method makes it 
 straightforward. Eventually, though, you will be able to translate quite com-
 plex sentences, going through the intermediate steps in your head.
  
 Chapter 11",NA
Paraphrasing English,"Some English sentences do not easily lend themselves to direct translation 
 using the step-by-step procedure. With such sentences, however, it is often 
 quite easy to come up with an English paraphrase that is amenable to the 
 procedure. Consider, for example,
  If a freshman takes a logic class, then he or",NA
Ambiguity and context sensitivity,"There are a couple of things that make the task of translating between English 
 and first-order logic difficult. One is the sparseness of primitive concepts in 
 fol. While this sparseness makes the language easy to learn, it also means that 
 there are frequently no very natural ways of saying what you want to say. You 
 have to try to find circumlocutions available with the resources at hand. While 
 this is often possible in mathematical discourse, it is frequently impossible for 
 ordinary English. (We will return to this matter later.)
  
 ambiguity 
  
  
 The other thing that makes it difficult is that English is rife with ambi-
  
 guities, whereas the expressions of first-order logic are unambiguous (at least 
  
 if the predicates used are unambiguous). Thus, confronted with a sentence 
  
 of English, we often have to choose one among many possible interpretations 
  
 in deciding on an appropriate translation. Just which is appropriate usually 
  
 depends on context.
  
 The ambiguities become especially vexing with quantified noun phrases.
  
 Consider, for example, the following joke, taken from
  Saturday Night Live
 :
  
 Chapter 11",NA
Translations using function symbols,"Intuitively, functions are a kind of relation. One’s mother is one’s mother
  
 relations and functions 
  
 because of a certain relationship you and she bear to one another. Similarly, 
  
 2 + 3 = 5 because of a certain relationship between two, three, and five. 
  
 Building on this intuition, it is not hard to see that anything that can be 
  
 expressed in fol with function symbols can also be expressed in a version of 
  
 fol where the function symbols have been replaced by relation symbols.
  
 The basic idea can be illustrated easily. Let us use mother as a unary 
 function symbol, but MotherOf as a
  binary
  relation symbol. Thus, for example, 
 mother(max) = nancy and MotherOf(nancy, max) both state that Nancy is the 
 mother of Max.
  
 The basic claim is that anything we can say with the function symbol we 
 can say in some other way using the relation symbol. As an example, here is a 
 simple sentence using the function symbol:
  
 ∧
 x OlderThan(mother(x)
 ,
 x)
  
 Chapter 11",NA
Prenex form,"When we translate complex sentences of English into fol, it is common to end 
 up with sentences where the quantifiers and connectives are all scrambled 
 together. This is usually due to the way in which the translations of complex 
 noun phrases of English use both quantifiers and connectives:
  
  
 ∧
 x (P(x)
  → . . .
 )
  
  
  
 ∧
 x (P(x)
  ∧ . . .
 ) 
  
 As a result, the translation of (the most likely reading of) a sentence like 
 Every 
 cube to the left of a tetrahedron is in back of a dodecahedron
  ends up looking 
 like
  
  
 ∧
 x [(Cube(x)
  ∧ ∧
 y (Tet(y)
  ∧
  LeftOf(x
 ,
 y)))
  → ∧
 y (Dodec(y)
  ∧
  BackOf(x
 ,
  y))] 
  
  
 While this is the most natural translation of our sentence, there are sit-
  
 uations where it is not the most convenient one. It is sometimes important 
  
 that we be able to rearrange sentences like this so that all the quantifiers are 
  
 out in front and all the connectives in back. Such a sentence is said to be in 
 prenex form 
 prenex form
 , since 
 all the quantifiers come first.
  
 Stated more precisely, a wff is in
  prenex normal form
  if either it contains no 
 quantifiers at all, or else is of the form
  
 Q
 1
 v
 1
 Q
 2
 v
 2
  . . .
  Q
 n
 v
 n
 P
  
 where each Q
 i
  is either
  ∧
  or
  ∧
 , each v
 i
  is some variable, and the wff P is quantifier-free.
  
 Section 11.7",NA
Some extra translation problems,"Some instructors concentrate more on translation than others. For those who 
 like to emphasize this skill, we present some additional challenging exercises 
 here.
  
 Section 11.8",NA
Methods of Proof for Quantifiers,"In earlier chapters we discussed valid patterns of reasoning that arise from 
 the various truth-functional connectives of fol. This investigation of valid 
 inference patterns becomes more interesting and more important now that 
 we’ve added the quantifiers
  ∧
  and
  ∧
  to our language. Our aim in this chapter 
 and the next is to discover methods of proof that allow us to prove all and only 
 the first-order validities, and all and only the first-order consequences of a 
 given set of premises. In other words, our aim is to devise methods of proof 
 sufficient to prove everything that follows in virtue of the meanings of the 
 quantifiers, identity, and the truth-functional connectives. The resulting 
 deductive system does indeed accomplish this goal, but our proof of that fact 
 will have to wait until the final chapter of this book. That chapter will also 
 discuss the issue of logical consequence when we take into account the 
 meanings of other predicates in a first-order language.
  
 Again, we begin looking at informal patterns of inference and then present 
 their formal counterparts. As with the connectives, there are both simple 
 proof steps and more substantive methods of proof. We will start by 
 discussing the simple proof steps that are most often used with
  ∧
  and
  ∧
 . We 
 first discuss proofs involving single quantifier sentences and then explore 
 what happens when we have multiple and mixed quantifier sentences.
  
 Section 12.1",NA
Valid quantifier steps,"There are two very simple valid quantifier steps, one for each quantifier. They 
 work in opposite directions, however.
  
 Universal elimination
  
 Suppose we are given as a premise (or have otherwise established) that ev-
 erything in the domain of discourse is either a cube or a tetrahedron. And 
 suppose we also know that
  c
  is in the domain of discourse. It follows, of 
 course, that
  c
  is either a cube or a tetrahedron, since everything is.
  
 names an object in the domain of discourse. We may legitimately infer S(c). More generally, suppose we 
 have established
  ∧
 x S(x), and we know that c",NA
The method of existential instantiation,"Existential instantiation is one of the more interesting and subtle methods of 
 proof. It allows you to prove results when you are given an existential state-
 ment. Suppose our domain of discourse consists of all children, and you are 
 told that some boy is at home. If you want to use this fact in your reasoning, 
 you are of course not entitled to infer that Max is at home. Neither are you 
 allowed to infer that John is at home. In fact, there is no particular boy about 
 whom you can safely conclude that he is at home, at least if this is all you
  
 temporary names 
  
 know. So how should we proceed? What we could do is give a temporary name 
  
 to one of the boys who is at home, and refer to him using that name, as long 
  
 as we are careful not to use a name already used in the premises or the desired 
  
 conclusion.
  
 This sort or reasoning is used in everyday life when we know that 
 someone (or something) satisfies a certain condition, but do not know who (or 
 what) satisfies it. For example, when Scotland Yard found out there was a 
 serial killer at large, they dubbed him “Jack the Ripper,” and used this name in 
 reasoning about him. No one thought that this meant they knew who the killer 
 was; rather, they simply introduced the name to refer to whoever was doing 
 the killing. Note that if the town tailor were already called Jack the Ripper, 
 then the detectives’ use of this name would (probably) have been a gross 
 injustice.
  
 This is a basic strategy used when giving proofs in fol. If we have correctly 
 satisfying S(x), as long as the name is not one that is already in use. We may 
 proven that
  ∧
 x S(x), then we can give a name, say
  c
 , to one of the objects
  
 existential elimination 
  
 then assume S(c) and use it in our proof. This is the rule known as
  existential
  
 (instantiation) 
  
 instantiation
  or
  existential elimination
 .
  
 Generally, when existential instantiation is used in a mathematical proof, this 
 will be marked by an explicit introduction of a new name. For example, the 
 author of the proof might say, “So we have shown that there is a prime number 
 between
  n
  and
  m
 . Call it
  p
 .” Another phrase that serves the same function is: 
 “Let
  p
  be such a prime number.”
  
 Let’s give an example of how this rule might be used, by modifying our 
 preceding example. The desired conclusion is the same but one of the 
 premises is changed.",NA
The method of general conditional proof,"One of the most important methods of proof involves reasoning about an 
 arbitrary object of a particular kind in order to prove a universal claim about 
 all such objects. This is known as the method of
  general
  conditional proof. It is 
 a more powerful version of conditional proof, and similar in spirit to the 
 method of existential instantiation just discussed.
  
 Let’s start out with an example. This time let us assume that the domain of 
 discourse consists of students at a particular college. We suppose that we are 
 given a bunch of information about these students in the form of premises.",NA
Proofs involving mixed quantifiers,"There are no new methods of proof that apply specifically to sentences with 
 mixed quantifiers, but the introduction of mixed quantifiers forces us to be 
 more explicit about some subtleties having to do with the interaction of meth-
 ods that introduce new names into a proof: existential instantiation, general 
 conditional proof, and universal generalization. It turns out that problems can 
 arise from the interaction of these methods of proof.
  
 Let us begin by illustrating the problem. Consider the following argument:
  
  
  
  
 ∧
 y [Girl(y)
  ∧ ∧
 x (Boy(x)
  →
  Likes(x
 ,
  y))]
  
 ∧
 x [Boy(x)
  → ∧
 y (Girl(y)
  ∧
  Likes(x
 ,
  y))]
  
 If the domain of discourse were the set of children in a kindergarten class, the 
 conclusion would say every boy in the class likes some girl or other, while the 
 premise would say that there is some girl who is liked by every boy. Since this 
 is valid, let’s start by giving a proof of it.
  
 Proof:
  Assume the premise. Thus, at least one girl is liked by every 
 boy. Let
  c
  be one of these popular girls. To prove the conclusion we 
 will use general conditional proof. Assume that
  d
  is any boy in the 
 class. We want to prove that
  d
  likes some girl. But every boy likes 
 c
 , 
 so
  d
  likes
  c
 . Thus
  d
  likes some girl, by existential generalization.
  
 Since
  d
  was an arbitrarily chosen boy, the conclusion follows.
  
 Section 12.4",NA
Axiomatizing shape,"Let’s return to the project of giving axioms for the shape properties in Tarski’s 
 World. In Section 10.5, we gave axioms that described basic facts about the 
 three shapes, but we stopped short of giving axioms for the binary relation 
 SameShape. The reason we stopped was that the needed axioms require mul-
 tiple quantifiers, which we had not covered at the time.
  
 How do we choose which sentences to take as axioms? The main consid-
  
 correctness of axioms 
  
 eration is
  correctness
 : the axioms must be true in all relevant circumstances, 
  
 either in virtue of the meanings of the predicates involved, or because we have 
  
 restricted our attention to a specific type of circumstance.
  
 The two possibilities are reflected in our first four axioms about shape, which 
 we repeat here for ease of reference:
  
 Basic Shape Axioms:
  
  
 1.
  ¬∧
 x (Cube(x)
  ∧
  Tet(x)) 
  
  
 2.
  ¬∧
 x (Tet(x)
  ∧
  Dodec(x)) 
  
  
 3.
  ¬∧
 x (Dodec(x)
  ∧
  Cube(x)) 
  
  
 4.
  ∧
 x (Tet(x)
  ∧
  Dodec(x)
  ∧
  Cube(x)) 
  
 The first three of these are correct in virtue of the meanings of the predicates; 
 the fourth expresses a truth about all worlds of the sort that can be built in 
 Tarski’s World.
  
 completeness of axioms 
  
  
 Of second importance, just behind correctness, is
  completeness.
  We say 
  
 that 
 a set of axioms is complete if, whenever an argument is intuitively 
  
 valid (given the meanings of the 
 predicates and the intended range of cir-
  
 cumstances), its conclusion is a first-order consequence of its 
 premises taken 
  
 together with the axioms in question.
  
 The notion of completeness, like that of correctness, is not precise, depend-
 ing as it does on the vague notions of meaning and “intended circumstances.”
  
 Chapter 12",NA
Formal Proofs and Quantifiers,"Now that we have learned the basic informal methods of proof for quantifiers, 
 we turn to the task of giving formal rules that correspond to them. Again, we 
 can to do this by having two rules for each quantifier.
  
 Before getting down to the rules, though, we should emphasize that formal 
 proofs in the system
  F
  contain only sentences, never wffs with free variables. 
 This is because we want every line of a proof to make a definite claim. Wffs 
 with free variables do not make claims, as we have noted. Some deductive 
 systems do allow proofs containing formulas with free variables, where such 
 variables are interpreted universally, but that is not how the system
  F
  works.
  
 Section 13.1",NA
Universal quantifier rules,"The valid inference step of universal instantiation or elimination is easily for-
 malized. Here is the schematic version of the rule:
  
 Universal Elimination (
 ∧
  Elim):
  
 ∧
  
 ∧
 x S(x) ...
  
 S(c)
  
 Here x stands for any variable, c stands for any individual constant (whether 
 or not it has been used elsewhere in the proof), and S(c) stands for the result 
 of replacing free occurrences of x in S(x) with c. If the language contains 
 function symbols, c can also be any complex term that contains no variables.
  
 Next, let us formalize the more interesting methods of general conditional 
 proof and universal generalization. This requires that we decide how to rep-
 resent the fact that a constant symbol, say c, has been introduced to stand for 
 an arbitrary object satisfying some condition, say P(c). We indicate this by 
 means of a subproof with assumption P(c), insisting that the constant c in 
 question occur only within that subproof. This will guarantee, for example, 
 that the constant does not appear in the premises of the overall proof.",NA
Existential quantifier rules,"Recall that in our discussion of informal proofs, existential introduction was a 
 simple proof step, whereas the elimination of
  ∧
  was a subtle method of proof. 
 Thus, in presenting our formal system, we begin with the introduction rule.
  
 Existential Introduction (
 ∧
  Intro):
  
 S(c) 
  
 ...
  
 ∧∧
 x S(x)
  
 Here too x stands for any variable, c stands for any individual constant (or 
 complex term without variables), and S(c) stands for the result of replacing 
 free occurrences of x in S(x) with c. Note that there may be other occurrences 
 of c in S(x) as well.
  
 When we turn to the rule of existential elimination, we employ the 
 same,“boxed constant” device as with universal introduction. If we have 
 proven
 ∧
 x S(x), then we introduce a new constant symbol, say c, along with the 
 as-sumption that the object denoted by c satisfies the formula S(x). If, from 
 this",NA
Strategy and tactics,"We have seen some rather simple examples of proofs using the new rules. In 
 more interesting examples, however, the job of finding a proof can get pretty 
 challenging. So a few words on how to approach these proofs will be helpful.
  
 We have given you a general maxim and two strategies for finding sen-
  
 consider meaning 
  
 tential proofs. The maxim—to consider what the various sentences mean—is 
  
 even more important with the quantifiers. Only if you remember what they 
  
 mean and how the formal methods mirror common-sense informal methods 
  
 will you be able to do any but the most boring of exercises.
  
 informal proof as guide 
  
  
 Our first strategy was to try to come up with an informal proof of the 
  
 goal sentence from the premises, and use it to try to figure out how your 
  
 formal proof will proceed. This strategy, too, is even more important in proofs 
  
 involving quantifiers, but it is a bit harder to apply. The key skill in applying 
  
 the strategy is the ability to identify the formal rules implicit in your informal 
  
 reasoning. This takes a bit of practice. Let’s work through an example, to see 
  
 some of the things you should be looking out for.
  
 Suppose we want to prove that the following argument is valid:
  
  
  
  
  
 ∧
 x (Tet(x)
  ∧
  Small(x))
  
 ∧
 x (Small(x)
  →
  LeftOf(x
 ,
 b))
  
 ∧
 x LeftOf(x
 ,
  b)
  
 Obviously, the conclusion follows from the given sentences. But ask 
 yourself how you would prove it, say, to your stubborn roommate, the one 
 who likes to play devil’s advocate. You might argue as follows:
  
 Look, Bozo, we’re told that there is a small tetrahedron. So we know 
 that it is small, right? But we’re also told that anything that’s small is 
 left of
  b
 . So if it’s small, it’s got to be left of
  b
 , too. So something’s left 
 of
  b
 , namely the small tetrahedron.
  
 Now we don’t recommend calling your roommate “Bozo,” so ignore that bit. 
 The important thing to notice here is the implicit use of three of our quantifier 
 rules:
  ∧
  Elim
 ,
  ∧
  Elim
 , and
  ∧
  Intro
 . Do you see them?
  
 sentence. What we are doing there is introducing a temporary name (in this What indicates the use of
  ∧
  
 Elim
  is the “it” appearing in the second
  
 case, the pronoun “it”) and using it to refer to a small tetrahedron. That 
 corresponds to starting the subproof needed for an application of
  ∧
  Elim
 .
  
 Chapter 13",NA
Soundness and completeness,"In Chapter 8 we raised the question of whether the deductive system
  F
 T
  was sound and complete with 
 respect to tautological consequence. The same issues
  
 arise with the full system
  F
 , which contains the rules for the quantifiers and identity, in addition to the 
 rules for the truth-functional connectives. Here,
  
 the target consequence relation is the notion of first-order consequence, 
 rather than tautological consequence.
  
 The
  soundness
  question asks whether anything we can prove in
  F
  from 
 premises P
 1
 ,. . .,
  P
 n
  is indeed a first-order consequence of the premises. The
  
 completeness
  question asks the converse: whether every first-order consequence
  
 of a set of sentences can be proven from that set using the rules of
  F
 . It turns out 
 that both of these questions can be answered in the affirma-
  
 soundness of F 
 completeness of F
  
 tive. Before actually proving this, however, we need to add more precision to 
 the notion of first-order consequence, and this presupposes tools from set 
 theory that we will introduce in Chapters 15 and 16. We state and prove the 
 soundness theorem for first-order logic in Chapter 18. The completeness 
 theorem for first-order logic is the main topic of Chapter 19.
  
 Section 13.5",NA
Some review exercises,"In this section we present more problems to help you solidify your 
 understand-ing of the methods of reasoning involving quantifiers. We also 
 present some more interesting problems from a theoretical point of view.
  
 Exercises
  
 Some of the following arguments are valid, some are not. For each, either use Fitch to give a formal proof or 
 use Tarski’s World to construct a counterexample. In giving proofs, feel free to use
  Taut Con 
 if it helps.
  
 13.40
  
  
  
  
  
 ∧
 x Cube(x)
  ∧
  Small(d)
  
 13.41
  
  
  
  
  
 ∧
 x (Cube(x)
  ∧
  Small(x))
  
 Â
  
 Â
  
 ∧
 x (Cube(x)
  ∧
  Small(d))
  
 ∧
 x Cube(x)
  ∧ ∧
 x Small(x)
  
 Section 13.5",NA
More about Quantification,"Many English sentences take the form
  
 Q A B
  
 where
  Q
  is a
  determiner
  expression like
  every, some, the, more than half the, at 
 least three
 ,
  no
 ,
  many
 ,
  Max’s
 , etc.;
  A
  is a common noun phrase like
  cube
 , 
 student 
 of logic
 ,
  thing
 , etc.; and
  B
  is a verb phrase like
  sits in the corner
  or 
 is small
 .
  
 Such sentences are used to express quantitative relationships between the 
 set of objects satisfying the common noun phrase and the set of objects satis-
 fying the verb phrase. Here are some examples, with the determiner in bold:
  
 Every
  cube is small.
  
 Some
  cube is small.
  
 More than half the
  cubes are small. 
  
 At least three
  cubes are small.
  
 No
  cube is small.
  
 Many
  cubes are small.
  
 Max’s
  cube is small.
  
 These sentences say of the set
  A
  of cubes in the domain of discourse and the 
 set
  B
  of small things in the domain of discourse that
  
 every
  A
  is a
  B
 , 
  
 some
  A
  is a
  B
 , 
  
 more than half the
  A
 ’s are
  B
 ’s, 
  
 at least three
  A
 ’s are
  B
 ’s, 
  
 no
  A
  is a
  B
 , 
  
 many
  A
 ’s are
  B
 ’s, and 
  
 Max’s
  A
  is a
  B
 .
  
 Each of these can be thought of as expressing a kind of binary relation 
 between 
 A
  and
  B.
  
 determiners and 
  
 Linguistically, these words and phrases are known as
  determiners.
  The
  
 quantifiers 
  
 relation expressed by a determiner is usually, though not always, a
  quantita-
  
 tive
  relation between
  A
  and
  B
 . Sometimes this quantitative relation can be 
  
 captured using the fol quantifiers
  ∧
  and
  ∧
 , though sometimes it can’t. For
  
 364",NA
Numerical quantification,"We have already seen that many complex noun phrases can be expressed in 
 terms of
  ∧
  (which really means “everything”, not just “every”) and
  ∧
  (which 
 means “something,” not “some”). For example,
  Every cube left of
  b
  is small 
 can 
 be paraphrased as
  Everything that is a cube and left of
  b
  is small
 , a sentence 
 that can easily be translated into fol using
  ∧,∧
  and
  →
 . Similarly, 
 No cube is 
 small
  can be paraphrased as
  Everything is such that if it is a cube then it is not 
 small
 , which can again be easily translated into fol.
  
 Other important examples of quantification that can be indirectly ex-
  
 numerical claims 
  
 pressed in fol are numerical claims. By a “numerical claim” we mean a one 
  
 that 
 explicitly uses the numbers 1
 ,
  2
 ,
  3
 , . . .
  to say something about the rela-
  
 tion between the
  A
 ’s and the
  
 B
 ’s. Here are three different kinds of numerical 
  
 claims:
  
 At least two
  books arrived this week.
  
 At most two
  books are missing from the shelf. 
  
 Exactly two
  books are on the table.
  
 First-order languages do not in general allow us to talk directly about 
 numbers, only about elements of our domain of discourse. The blocks lan-
 guage, for example, only talks about blocks, not about numbers. Still, it is 
 possible to express these three kinds of numerical claims in fol.
  
 Recall that in fol, distinct names do not necessarily refer to distinct objects. 
 Similarly, distinct variables need not vary over distinct objects. For example, 
 both of the following sentences can be made true in a world with",NA
Proving numerical claims,"Since numerical claims can be expressed in fol, we can use the methods of 
 proof developed in previous chapters to prove numerical claims. However, as 
 you may have noticed in doing the exercises, numerical claims are not always 
 terribly perspicuous when expressed in fol notation. Indeed, expressing a 
 numerical claim in fol and then trying to prove the result is a recipe for 
 disaster. It is all too easy to lose one’s grasp on what needs to be proved.
  
 Suppose, for example, that you are told there are exactly two logic class-
 rooms and that each classroom contains exactly three computers. Suppose 
 you also know that every computer is in some logic classroom. From these 
 assump-tions it is of course quite easy to prove that there are exactly six 
 computers. How would the proof go?
  
 Proof:
  To prove there are exactly six computers it suffices to prove 
 that there are at least six, and at most six. To prove that there are
  
 Chapter 14",NA
The,NA,NA
",",NA,NA
 both,NA,NA
", and",NA,NA
 neither,"The English determiners
  the
 ,
  both
 , and
  neither
  are extremely common. Indeed, 
 the
  is one of the most frequently used words in the English language. (We used 
 it twice in that one sentence.) In spite of their familiarity, their logical 
 properties are subtle and, for that matter, still a matter of some dispute.
  
 To see why, suppose I say “The elephant in my closet is not wrinkling my 
 clothes.” What would you make of this, given that, as you probably guessed, 
 there is no elephant in my closet? Is it simply false? Or is there something else 
 wrong with it? If it is false, then it seems like its negation should be true. But 
 the negation seems to be the claim that the elephant in my closet
  is
  wrinkling 
 my clothes. Similar puzzles arise with
  both
  and
  neither
 :
  
 Both elephants in my closet are wrinkling my clothes. 
 Neither elephant in my closet is wrinkling my clothes.
  
 What are you to make of these if there are no elephants in my closet, or if there 
 are three?
  
 Early in the twentieth century, the logician Bertrand Russell proposed an 
  
 analysis of such sentences. He proposed that a sentence like
  The cube is small 
  
 the 
 should be analyzed as asserting that there is exactly one cube, and that it is 
  
 small. According to his analysis, the sentence will be false if there is no cube, 
  
 or if there is more than one, or if there is exactly one, but it’s not small. If 
  
 Russell’s analysis is correct, then such sentences can easily be expressed in 
  
 first-order logic as follows:
  
 ∧
 x [Cube(x)
  ∧ ∧
 y (Cube(y)
  →
  y = x)
  ∧
  Small(x)]
  
 More generally, a sentence of the form
  The A is a B
 , on the Russellian analysis, 
 would be translated as:
  
 ∧
 x [A(x)
  ∧ ∧
 y (A(y)
  →
  x = y)
  ∧
  B(x)]
  
 Noun phrases of the form
  the A
  are called
  definite descriptions
  and the above 
 definite descriptions 
 analysis is 
 called the
  Russellian analysis of definite descriptions
 .
  
 While Russell did not explicitly consider
  both
  or
  neither
 , the spirit of his 
  
 both, neither 
 analysis extends naturally to these determiners. We could analyze
  Both cubes 
  
 are small
  as saying that there are exactly two cubes and each of them is small:
  
 ∧
 !2
 x Cube(x)
  ∧ ∧
 x [Cube(x)
  →
  Small(x)]
  
 Section 14.3",NA
Adding other determiners to,NA,NA
 fol,"We have seen that many English determiners can be captured in fol, though 
  
 by somewhat convoluted circumlocutions. But there are also many determin-
  
 ers that simply aren’t expressible in fol. A simple example is the determiner 
  
 Most
 , as in
  Most cubes are large
 . There are two difficulties. One is that the 
  
 most, more than half 
 meaning of
  most
  is a bit indeterminate.
  Most cubes are large
  clearly implies 
  
 More than half the cubes are large
 , but does the latter imply the former? In-
  
 tuitions differ. But even if we take it to mean the same as
  More than half
 , 
  
 it cannot be expressed in fol, since the determiner
  More than half
  is not 
  
 expressible in fol.
  
 It is possible to give a mathematical proof of this fact. For example, con-sider 
 the sentence:
  
 More than half the dodecahedra are small.
  
 To see the problem, notice that the English sentence makes a claim about the 
 relative sizes of the set
  A
  of small dodecahedra and the set
  B
  of dodecahe-dra 
 that are not small. It says that the set
  A
  is larger than the set
  B
  and it does so 
 without claiming anything about how many objects there are in these sets or 
 in the domain of discourse. To express the desired sentence, we might try 
 something like the following (where we use A(x) as shorthand for Dodec(x)
  ∧
  
 Small(x), and B(x) as shorthand for Dodec(x)
  ∧ ¬
 Small(x)):
  
 [
 ∧
 x A(x)
  ∧ ∧
 x
  ¬
 B(x)]
  ∧
  [
 ∧
 ≥
 2
 x A(x)
  ∧ ∧
 ≤
 1
 x B(x)]
  ∧
  [
 ∧
 ≥
 3
 x A(x)
  ∧ ∧
 ≤
 2
 x B(x)]
  ∧ . . .
  
 The trouble is, there is no place to stop this disjunction! Without some 
 fixed finite upper bound on the total number of objects in the domain, we need 
 all of the disjuncts, and so the translation of the English sentence would be an 
 infinitely long sentence, which fol does not allow. If we knew there were a 
 maximum of twelve objects in the world, as in Tarski’s World, then we could 
 write a sentence that said what we needed; but without this constraint, the 
 sentence would have to be infinite.
  
 This is not in itself a proof that the English sentence cannot be expressed 
  
 in fol. But it does pinpoint the problem and, using this idea, one can actually 
 unexpressible in
  fol 
 give such a 
 proof. In particular, it is possible to show that for any first-order 
  
 sentence S of the blocks language, if S is true in every world where more than 
  
 half the dodecahedra are small, then it is also true in some world where less
  
 Section 14.4",NA
The logic of generalized quantification,"In this section we look briefly at some of the logical properties of determin-
 ers. Since different determiners typically have different meanings, we expect 
 them to have different logical properties. In particular, we expect the logical 
 truths and valid arguments involving determiners to be highly sensitive to the 
 particular determiners involved. Some of the logical properties of determiners 
 fall into nice clusters, though, and this allows us to classify determiners in 
 logically significant ways.
  
 We will assume that
  Q
  is some determiner of English and that we have 
 introduced a formal counterpart Q into fol in the manner described at the end 
 of the last section.
  
 Conservativity
  
 As it happens, there is one logical property that holds of virtually all single-
 word determiners in every natural language. Namely, for any predicates A and 
 B, the following are logically equivalent:
  
  
  
 Qx (A(x)
 ,
  B(x))
  ∧
  Q x (A(x)
 ,
 (A(x)
  ∧
  B(x))) 
  
 This is called the
  conservativity property
  of determiners. Here are two instances 
  
 conservativity property 
 of the
  ∧
  half of conservativity, followed by two instances of the
  ∧
  half: 
  
 If
  no
  doctor is a doctor and a lawyer, then
  no
  doctor is a lawyer.
  
 If
  exactly three
  cubes are small cubes, then
  exactly three
  cubes
  
 Section 14.5",NA
Other expressive limitations of first-order logic,"The study of generalized quantification is a response to one expressive limi-
 tation of fol, and so to its inability to illuminate the full logic inherent in 
 natural languages like English. The determiners studied in the preceding sec-
 tions are actually just some of the ways of expressing quantification that we 
 find in natural languages. Consider, for example, the sentences
  
 More
  cubes
  than
  tetrahedra are on the same row as e.
  
 Twice as many
  cubes
  as
  tetrahedra are in the same column as f. 
 Not 
 as many
  tetrahedra
  as
  dodecahedra are large.
  
 The expressions in bold take two common noun expressions and a verb ex-
  
 pression to make a sentence. The techniques used to study generalized quan-
  
 tification in earlier sections can be extended to study these determiners, but 
  
 we have to think of them as expressing
  three place
  relations on sets, not just 
  
 three place 
 two place relations. Thus, if we added these determiners to the language, they 
  
 quantification
  
 would have the general form Q x (A(x)
 ,
  B(x)
 ,
 C(x)).
  
 Section 14.6",NA
Part III ,NA,NA
Applications and ,NA,NA
Metatheory,403,NA
First-order Set Theory,"Over the past hundred years, set theory has become an important and useful 
 part of mathematics. It is used both in mathematics itself, as a sort of universal 
 framework for describing other mathematical theories, and also in 
 applications outside of mathematics, especially in computer science, 
 linguistics, and the other symbolic sciences. The reason set theory is so useful 
 is that it provides us with tools for modeling an extraordinary variety of 
 structures.
  
 Personally, we think of sets as being a lot like Tinkertoys or Lego blocks: 
 modeling in set theory 
  
 basic kits out of which we can construct models of practically anything. If you go on to study 
 mathematics, you will no doubt take courses in which natural numbers are modeled by sets of a 
 particular kind, and real numbers are modeled by sets of another kind. In the study of rational decision 
 making, economists use sets to model situations in which rational agents choose among competing 
 alternatives. Later in this chapter, we’ll do a little of this, modeling properties, relations, and functions as 
 sets. These models are used extensively in philosophy, computer science, and mathematics. In Chapter 
 18 we will use these same tools to make rigorous our notions of first-order consequence and first-order 
 validity.
  
 In this chapter, though, we will start the other way around, applying 
  
 what we have learned about first-order logic to the study of set theory. Since 
  
 logic and set theory 
 set theory is generally presented as an axiomatized theory within a first-
  
 order language, this gives us an opportunity to apply just about everything 
  
 we’ve learned so far. We will be expressing various set-theoretic claims in fol, 
  
 figuring out consequences of these claims, and giving informal proofs of these 
  
 claims. The one thing we won’t be doing very much is constructing formal 
  
 proofs of set-theoretic claims. This may disappoint you. Many students are 
  
 initially intimidated by formal proofs, but come to prefer them over informal 
  
 proofs because the rules of the game are so clear-cut. For better or worse, 
  
 however, formal proofs of substantive set-theoretic claims can be hundreds or 
  
 even thousands of steps long. In cases where the formal proof is manageable, 
  
 we will ask you to formalize it in the exercises.
  
 Set theory has a rather complicated and interesting history. Another ob-
  
 jective of this chapter is to give you a feeling for this history. We will start out 
  
 with an untutored, or “naive” notion of set, the one that you were no doubt 
  
 naive set theory 
 exposed to in elementary school. We begin by isolating two basic principles 
  
 that seem, at first sight, to be clearly true of this intuitive notion of set. The
  
 405",NA
Naive set theory,"The first person to study sets extensively and to appreciate the inconsistencies 
 lurking in the naive conception was the nineteenth century German mathe-
 matician Georg Cantor. According to the naive conception, a set is just a
  
 sets and membership 
  
 collection of things, like a set of chairs, a set of dominoes, or a set of numbers. 
  
 The things in the collection are said to be
  members
  of the set. We write
  a ∧ b
 , 
 and read “
 a
  is a member (or an element) of
  b
 ,” if
  a
  is one of the objects that 
  
 makes up the set
  b
 . If we can 
 list all the members of
  b
 , say the numbers 7, 8,
  
 list description 
  
 and 10, then we write
  b
  =
  {
 7
 ,
 8
 ,
  10
 }
 . This is called a
  list description
  of the set. As 
 you will recall, the first-order language of set theory has two relation
  
 sets and non-sets 
  
 symbols, = and
  ∧
 . Beyond that, there are some options. If we want our domain 
 of discourse to include not just sets but other things as well, then we need
  
 Chapter 15",NA
"Singletons, the empty set, subsets","There are two special kinds of sets that sometimes cause confusion. One is 
 where the set is obtained using the Axiom of Comprehension, but with a 
 property that is satisfied by exactly one object. The other is when there is no 
 object at all satisfying the property. Let’s take these up in turn.
  
 Suppose there is one and only one object
  x
  satisfying
  P
 (
 x
 ). According to the 
 Axiom of Comprehension, there is a set, call it
  a
 , whose only member is 
 x
 . That 
 is,
  a
  =
  {x}
 . Some students are tempted to think that
  a
  =
  x
 . But in that direction 
 lies, if not madness, at least dreadful confusion. After all, 
 a
  is a set (an abstract 
 object) and
  x
  might have been any object at all, say the Washington 
 Monument. The Washington Monument is a physical object,
  
 singleton set 
  
 not a set. So we must not confuse an object
  x
  with the set
  {x}
 , called the 
 singleton set containing x
 . Even if
  x
  is a set, we must not confuse it with its 
  
 own singleton. For example,
  
 x
  might have any number of elements in it, but 
  
 {x}
  has exactly one element:
  x
 . The other slightly 
 confusing case is when nothing at all satisfies
  P
 (
 x
 ).
  
 Suppose, for example, that
  P
 (
 x
 ) is the formula
  x ̸
 =
  x
 . What are we to make of 
 the set 
  
 {x | x ̸
 =
  x}
 ?
  
 Well, in this case the set is said to be
  empty
 , since it contains nothing. It is
  
 empty set
  (
 ∧
 )
  
 easy to prove that there can be at most one such set, so it is called
  the
  empty
  
 set, and is denoted by
  ∧
 . Some authors use 0 to denote the empty set. It can 
 also be informally denoted by
  {}
 . These special sets, singleton sets and the 
 empty set, may seem rather
  
 pointless, but set theory is much smoother if we accept them as full-fledged 
 sets. If we did not, then we would always have to prove that there were at 
 least two objects satisfying
  P
 (
 x
 ) before we could assert the existence of
  {x | 
 P
 (
 x
 )
 }
 , and that would be a pain. The next notion is closely related to the 
 membership relation, but importantly different. It is the subset relation, and is 
 defined as follows:",NA
Intersection and union,"There are two important operations on sets that you have probably seen 
 before: intersection and union. These operations take two sets and form a 
 third.
  
 Definition
  Let
  a
  and
  b
  be sets.
  
 1. The
  intersection
  of
  a
  and
  b
  is the set whose members are just those
  
 objects in both
  a
  and
  b
 . This set is generally written
  a ∩ b
 . (“
 a ∩ b
 ” is a 
 complex term built up using a binary function symbol
  ∩
  placed in infix 
 notation.
 2
 ) In symbols:
  
 intersection
  (
 ∩
 )
  
 ∧a ∧b∧z
 (
 z ∧ a ∩ b ↔
  (
 z ∧ a ∧ z ∧ b
 ))
  
 2. The
  union
  of
  a
  and
  b
  is the set whose members are just those objects in 
 either
  a
  or
  b
  or both. This set is generally written
  a ∧ b
 . In symbols:
  
 union
  (
 ∧
 )
  
 ∧a ∧b∧z
 (
 z ∧ a ∧ b ↔
  (
 z ∧ a ∧ z ∧ b
 ))
  
 At first sight, these definitions seem no more problematic than the defini-
 tion of the subset relation. But if you think about it, you will see that there is 
 actually something a bit fishy about them as they stand. For how do we know 
 that there
  are
  sets of the kind described? For example, even if we know that
  a
  
 and
  b
  are sets, how do we know that there is a set whose members are the 
 objects in both
  a
  and
  b
 ? And how do we know that there is exactly one such 
 set? Remember the rules of the road. We have to prove everything from 
 explicitly given axioms. Can we prove, based on our axioms, that there is such 
 a unique set?
  
 2
 Function symbols are discussed in the optional Section 1.5. You should read this section
  
 now if you skipped over it.
  
 Section 15.3",NA
Sets of sets,"The Axiom of Comprehension applies quite generally. In particular, it allows us 
 to form sets of sets. For example, suppose we form the sets
  {
 0
 }
  and
  {
 0
 ,
 1
 }
 .
  
 These sets can themselves be collected together into a set
  a
  =
  {{
 0
 }, {
 0
 ,
 1
 }}
 . More generally, we can prove the 
 following:
  
 Proposition 7.
  (Unordered Pairs)
  For any objects x and y there is a (unique) 
  
 set a
  =
  {x, y}. In symbols: 
 unordered pairs
  
 ∧x ∧y ∧
 !
 a ∧w
 (
 w ∧ a ↔
  (
 w
  =
  x ∧ w
  =
  y
 ))
  
 Proof:
  Let
  x
  and
  y
  be arbitrary objects, and let
  
 a
  =
  {w | w
  =
  x ∧ w
  =
  y}
  
 The existence of
  a
  is guaranteed by Comprehension, and its unique-
 ness follows from the Axiom of Extensionality. Clearly
  a
  has
  x
  and 
 y
  
 and nothing else as elements.
  
 It is worth noting that our previous observation about the existence of 
 singletons, which we did not prove then, follows from this result. Thus:",NA
Modeling relations in set theory,"Intuitively, a binary predicate like Larger expresses a binary relation between 
 objects in a domain
  D
 . In set theory, we model this relation by means of a set 
 of ordered pairs, specifically the set
  
 {∧x, y∧ | x ∧ D, y ∧ D,
  and
  x
  is larger than
  y}
  
 extension 
  
 This set is sometimes called the
  extension
  of the predicate or relation. More
  
 relation in set theory 
  
 generally, given some set
  D
 , we call any set of pairs
  ∧x, y∧
 , where
  x
  and
  y
  are in
  
 D
 , a binary
  relation on D
 . We model ternary relations similarly, as sets of 
  
 ordered triples, and so forth 
 for higher arities.
  
 It is important to remember that the extension of a predicate can depend 
 on the circumstances that hold in the domain of discourse. For example, if we 
 rotate a world 90 degrees clockwise in Tarski’s World, the domain of objects 
 remains unchanged but the extension of
  left of
  becomes the new extension of 
 back of
 . Similarly, if someone in the domain of discourse sits down, then the 
 ex-tension of
  is sitting
  changes. The binary predicates themselves do not 
 change, nor does what they express, but the things that stand in these 
 relations do, that is, their extensions change.
  
 properties of relations 
  
  
 There are a few special kinds of binary relations that it is useful to have 
  
 names for. In fact, we have already talked about some of these informally in 
  
 Chapter 2. A relation
  R
  is said to be
  transitive
  if it satisfies the following:
  
  
 Transitivity:
 ∧x∧y∧z
 [(
 R
 (
 x, y
 )
  ∧ R
 (
 y, z
 ))
  → R
 (
 x, z
 )] 
  
 As examples, we mention that the relation
  larger than
  is transitive, whereas 
 the relation
  adjoins
  is not. Since we are modeling relations by sets of ordered 
 pairs, this condition becomes the following condition on a set
  R
  of ordered 
 pairs: if
  ∧x, y∧ ∧ R
  and
  ∧y, z∧ ∧ R
  then
  ∧x, z∧ ∧ R
 . Here are several more 
 special properties of binary relations:
  
 Reflexivity: Irreflexivity: Symmetry: Asymmetry:
  
 ∧x R
 (
 x, x
 )
  
 ∧x ¬R
 (
 x, x
 )
  
 ∧x∧y
 (
 R
 (
 x, y
 )
  → R
 (
 y, x
 ))",NA
Functions,"The notion of a function is one of the most important in mathematics. We have 
 already discussed functions to some extent in Section 1.5.
  
 Intuitively, a function is simply a way of doing things to things or assign-
 ing things to things: assigning license numbers to cars, assigning grades to 
 students, assigning a temperature to a pair consisting of a place and a time, 
 and so forth. We’ve already talked about the
  father
  function, which assigns to 
 each person that person’s father, and the
  addition
  function, which assigns to 
 every pair of numbers another number, their sum.
  
 Like relations, functions are modeled in set theory using sets of ordered 
 functions as special 
 pairs. This is 
 possible because we can think of any function as a special type 
  
 kind of relation
  
 of binary relation: the relation that holds between the input of the function 
 and its output. Thus, a relation
  R
  on a set
  D
  is said to be a
  function
  if it satisfies 
 the following condition:
  
  
 Functional:
  
 ∧x∧
 ≤
 1
 y R
 (
 x, y
 ) 
  
 In other words, a relation is a function if for any “input” there is at most 
 one“output.” If the function also has the following property, then it is called a
  
 total
  function on
  D
 :
  
 Totality:
  
 ∧x∧y R
 (
 x, y
 )
  
 total functions
  
 Total functions give “answers” for every object in the domain. If a function 
  
 is not total on
  D
 , it is called a
  partial
  function on
  D
 .
 3 
 partial functions 
 Whether or not a function is total or 
 partial depends very much on just 
  
 what the domain
  D
  of discourse is. If
  D
  is the set of all people living or dead, 
  
 then intuitively the
  father of
  function is total, though admittedly things get a 
  
 bit hazy at the dawn of humankind. But if
  D
  is the set of living people, then 
  
 this function is definitely partial. It only assigns a value to a person whose 
  
 father is still living.
  
 There are some standard notational conventions used with functions. First, 
 it is standard to use letters like
  f, g, h
  and so forth to range over functions.
  
 Second, it is common practice to write
  f
 (
 x
 ) =
  y
  rather than
  ∧x, y∧ ∧ f
  when 
 f
  is a function. 
  
 f
 (
 x
 )
  
 The
  domain
  of a function
  f
  is the set 
 domain of function
  
  
 {x | ∧y
 (
 f
 (
 x
 ) =
  y
 )
 } 
  
 3
 Actually, usage varies. Some authors use “partial function” to include total functions,
  
 that is, to be synonymous with function.
  
 Section 15.6",NA
The powerset of a set,"Once we get used to the idea that sets can be members of other sets, it is 
  
 natural to form the set of all subsets of any given set
  b
 . The following theorem, 
  
 which is easy to prove, shows that there is one and only one such set. This 
  
 set is called the
  powerset
  of
  b
  and denoted
  ∧b
  or
  ∧
 (
 b
 ). 
  
 powersets
  (
 ∧
 )
  
 Proposition 10.
  (Powersets)
  For any set b there is a unique set whose mem-
 bers are just the subsets of b. In symbols:
  
 ∧b ∧c ∧x
 (
 x ∧ c ↔ x ∧ b
 )
  
 Proof:
  By the Axiom of Comprehension, we may form the set
  c
  = 
 {x | x 
 ∧ b}
 . This is the desired set. By the Axiom of Extensionality, there can 
 be only one such set.
  
 we need a set whose members are all the subsets of
  b
 . There are four of these. By way of example, let us 
 form the powerset of the set
  b
  =
  {
 2
 ,
  3
 }
 . Thus,
  
 The most obvious two are the singletons
  {
 2
 }
  and
  {
 3
 }
 . The other two are the 
 empty set, which is a subset of every set, as we saw in Problem 15.11, and the 
 set
  b
  itself, since every set is a subset of itself. Thus:
  
 ∧b
  =
  {∧, {
 2
 }, {
 3
 }, {
 2
 ,
  3
 }}
  
 Here are some facts about the powerset operation. We will ask you to prove 
 them in the problems.
  
 Proposition 11.
  Let a and b be any sets.
  
 1. b ∧ ∧b",NA
Russell’s Paradox,"We are now in a position to show that something is seriously amiss with the 
 theory we have been developing. Namely, we can prove the negation of 
 Proposition 12. In fact, we can prove the following which directly contradicts 
 Proposition 12.
  
 Proposition 14.
  There is a set c such that ∧c ∧ c.
  
 Proof:
  Using the Axiom of Comprehension, there is a universal set, a 
 set that contains everything. This is the set
  c
  =
  {x | x
  =
  x}
 . But then 
 every subset of
  c
  is a member of
  c
 , so
  ∧c
  is a subset of
  c
 .
  
 universal set
  (
 V
  ) 
  
  
 The set
  c
  used in the above proof is called the
  universal set
  and is usually 
  
 denoted by “
 V
  .” It is called that because it contains everything as a mem-
  
 ber, 
 including itself. What we have in fact shown is that the powerset of the 
  
 universal set both is and is 
 not a subset of the universal set.
  
 Let us look at our contradiction a bit more closely. Our proof of Propo-sition 
 12, applied to the special case of the universal set, gives rise to the set 
  
  
 Z
  =
  {x | x ∧ V ∧ x ̸∧ x} 
  
 This is just the Russell set for the universal set. But the proof of Proposition 12 
 shows that
  Z
  is a member of
  Z
  if and only if
  Z
  is not a member of
  Z
 . This set
  Z
  is 
 called the (absolute) Russell set, and the contradiction we have just
  
 Russell’s Paradox 
  
 established is called Russell’s Paradox.
  
 It would be hard to overdramatize the impact Russell’s Paradox had on set 
 theory at the turn of the century. Simple as it is, it shook the subject to its 
 foundations. It is just as if in arithmetic we discovered a proof that 23+27=50 
 and 23+27
 ̸
 = 50. Or as if in geometry we could prove that the area of a square 
 both is and is not the square of the side. But here we are in just that position. 
 This shows that there is something wrong with our starting assumptions of 
 the whole theory, the two axioms with which we began. There simply is no 
 domain of sets which satisfies these assumptions. This discovery was 
 regarded as a paradox just because it had earlier seemed to most 
 mathematicians that the intuitive universe of sets did satisfy the axioms.
  
 reactions to the paradox 
  
  
 Russell’s Paradox is just the tip of an iceberg of problematic results in 
  
 naive set theory. These paradoxes resulted in a wide-ranging attempt to clarify 
  
 the notion of a set, so that a consistent conception could be found to use in 
  
 mathematics. There is no one single conception which has completely won out
  
 Chapter 15",NA
Zermelo Frankel set theory,NA,NA
 zfc,"The paradoxes of naive set theory show us that our intuitive notion of set is 
 simply inconsistent. We must go back and rethink the assumptions on which 
 the theory rests. However, in doing this rethinking, we do not want to throw 
 out the baby with the bath water.
  
 Which of our two assumptions got us into trouble, Extensionality or Com-
 diagnosing the problem 
 prehension? If we examine the Russell Paradox closely, we see that it is ac-
  
 tually a straightforward refutation of the Axiom of Comprehension. It shows 
  
 that there is no set determined by the property of not belonging to itself. That
  
 Section 15.9",NA
Mathematical Induction,"In the first two parts of this book, we covered most of the important methods 
 of proof used in rigorous reasoning. But we left out one extremely important 
 method: proof by
  mathematical induction
 .
  
 By and large, the methods of proof discussed earlier line up fairly nicely 
 with various connectives and quantifiers, in the sense that you can often tell 
 from the syntactic form of your premises or conclusion what methods you will 
 be using. The most obvious exception is proof by contradiction, or its any form 
 of statement, no matter what its main connective or quantifier. This formal 
 counterpart
  ¬
  Intro
 . This method can in principle be used to prove is because 
 any sentence S is logically equivalent to one that begins with a
  
 form of statements 
  
 negation symbol, namely,
  ¬¬
 S. In terms of syntactic form, mathematical 
 induction is typically used to
  
 proved by induction 
  
 prove statements of the form
  
 ∧
 x [P(x)
  →
  Q(x)] 
  
 This is also the form of statements proved using general conditional proof. In 
 fact, proof by induction is really a pumped-up version of this method: general 
 conditional proof on steroids, you might say. It works when these statements 
 involve a predicate P(x) defined in a special way. Specifically, proof by induction 
 is available when the predicate P(x) is defined by what is called an
  
 inductive definition 
  
 inductive definition.
  For this reason, we need to discuss proof by induction and 
  
 inductive definitions side by side. We will see that whenever a predicate P(x) 
  
 is defined by means of an inductive definition, proof by induction provides a 
  
 much more powerful method of proof than ordinary general conditional proof.
  
 Before we can discuss either of these, though, we should distinguish both
  
 induction in science 
  
 from yet a third process that is also known as
  induction.
  In science, we use 
  
 the 
 term “induction” whenever we draw a general conclusion on the basis of a 
  
 finite number of 
 observations. For example, every day we observe that the sun 
  
 comes up, that dropped things fall 
 down, and that people smile more when 
  
 it is sunny. We come to infer that this is always the case: that 
 the sun comes 
  
 up every morning, that dropped things always fall, that people are always 
  
 happier when the sun is out.
  
 Of course there is no strict logical justification for such inferences. We may 
 have correctly inferred some general law of nature, or we may have simply ob-
 served a bunch of facts without any law that backs them up. Some time in
  
 442",NA
Inductive definitions and inductive proofs,"Inductive definitions involve setting things up in a certain methodical, step-by-
 step manner. Proofs by induction take advantage of the structure that results 
 from such inductive definitions. We begin with a simple analogy.
  
 Dominoes
  
 When they were younger, Claire and Max liked to build long chains of domi-
 noes, all around the house. Then they would knock down the first and, if things 
 were set up right, the rest would all fall down. Little did they know that in so 
 doing they were practicing induction. Setting up the dominoes is like giving an 
 inductive definition. Knocking them all down is like proving a theorem by 
 induction.
  
 There are two things required to make all the dominoes fall over. They 
 must be close enough together that when any one domino falls, it knocks 
 down the next. And then, of course, you need to knock down the first. In a 
 proof by induction, these two steps correspond to what are called the 
 inductive step (getting from one to the next) and the basis step (getting the 
 whole thing started).
  
 Section 16.1",NA
Inductive definitions in set theory,"The way we have been stating inductive definitions seems reasonably 
 rigorous. Still, you might wonder about the status of clauses like
  
 4. Nothing is an ambig-wff unless it can be generated by repeated applica-
 tions of (1), (2), and (3).
  
 This clause is quite different in character from the others, since it mentions not 
 just the objects we are defining, but the other clauses of the definition itself. 
 You might also wonder just what is getting packed into the phrase “repeated 
 applications.”
  
 One way to see that there is something different about clause (4) is to note that 
 the other clauses are obviously expressible using first-order formulas. For 
 example, if concat is a symbol for the concatenation function (that is, the 
 function that takes two expressions and places the first immediately to the left 
 of the second), then one could express (2) as
  
  
 ∧p
  [ambig-wff(
 p
 )
  →
  ambig-wff(concat(
 ¬
 ,
  p
 ))] 
  
 In contrast, clause (4) is not the sort of thing that can be expressed in fol.
  
 However, it turns out that if we work within set theory, then we can ex-
  
 making the final 
 press inductive definitions with first-order sentences. Here, for example, is a 
 clause more precise
  
 definition of the set of ambig-wffs that uses sets. It turns out that this defi-
 nition can be transcribed into the language of set theory in a straightforward 
 way. The English version of the definition is as follows:",NA
Induction on the natural numbers,"Many students come away from the study of induction in math classes with 
 the feeling that it has something special to do with the natural numbers. By 
 now, it should be obvious that this method of proof is far more general than 
 that. We can prove things about many different kinds of sets using induction. 
 In fact, whenever a set is defined inductively, we can prove general claims",NA
Axiomatizing the natural numbers,"In giving examples of informal proofs in this book, we have had numerous 
 occasions to use the natural numbers as examples. In proving things about the 
 natural numbers, we have made recourse to any fact about the natural 
 numbers that was obviously true. If we wanted to formalize these proofs, we 
 would have to be much more precise about what we took to be the 
 “obvious”facts about the natural numbers.
  
 Over the years, a consensus has arisen that the obviously true claims about
  
 Peano Arithmetic
  (pa) 
  
 the natural numbers can be formalized in what has come to be known as 
 Peano 
  
 Arithmetic, or pa for short, named after the Italian mathematician Giuseppe 
  
 Peano. This is a certain first-order theory whose main axiom states a form of 
  
 induction for natural numbers.
  
 pa is formulated in a first-order language that has constants 0 and 1 to-
 gether with the binary function symbols + and
  ×
  and the identity predicate. It 
 takes as axioms the following basic facts about the domain of natural num-
 bers.
  
 1.
  ∧x ∧y
  (
 x
  + 1 =
  y
  + 1
  → x
  =
  y
 ) 
  
 2.
  ∧x
  (
 x
  + 1
  ̸
 = 0) 
  
 3. 0 + 1 = 1
  
  
 4.
  ∧x
  (
 x
  + 0 =
  x
 ) 
  
  
 5.
  ∧x ∧y
  [
 x
  + (
 y
  + 1) = (
 x
  +
  y
 ) + 1] 
  
  
 6.
  ∧x
  (
 x ×
  0 = 0) 
  
  
 7.
  ∧x ∧y
  [
 x ×
  (
 y
  + 1) = (
 x × y
 ) +
  x
 ] 
  
 In addition, pa has an axiom scheme capturing the principle of mathematical
  
 induction scheme 
  
 induction on the natural numbers. This can be stated as follows:
  
 [
 Q
 (0)
  ∧ ∧x
  (
 Q
 (
 x
 )
  → Q
 (
 x
  + 1))]
  → ∧x Q
 (
 x
 )",NA
Proving programs correct,"Induction is an important technique for proving facts about computer pro-
 grams, particularly those that involve recursive definitions or loops. Imagine 
 that we ask a programmer to write a program that adds the first
  n
  natural 
 numbers. The program, when given a natural number
  n
  as its input, should
  
 program specification 
  
 return as its result the sum 0+1+2+
 . . .
 +
 n
 . We will call this the
  specification 
  
 of 
 the program.
  
 recursive program 
  
 One way to do this is to write a so-called
  recursive
  program like this:
  
 Chapter 16",NA
Advanced Topics in,NA,NA
Propositional Logic,"This chapter contains some more advanced ideas and results from proposi-
 tional logic, logic without quantifiers. The most important part of the chapter 
 is the proof of the Completeness Theorem for the propositional proof system 
 F
 T
  that you learned in Part I. This result was discussed in Section 8.3 and will 
 be used in the final chapter when we prove the Completeness Theorem for the 
 full system
  F
 . The final two sections of this chapter treat topics in 
 propositional logic of considerable importance in computer science.
  
 Section 17.1",NA
Truth assignments and truth tables,"In Part I, we kept our discussion of truth tables pretty informal. For example,
  
 modeling truth tables 
  
 we did not give a precise definition of truth tables. For some purposes this 
  
 informality suffices, but if we are going to prove any theorems about fol, 
  
 such as the Completeness Theorem for the system
  F
 T
 , this notion needs to be 
 modeled in a mathematically precise way. As promised, we use set theory 
  
 to do this modeling.
  
 We can abstract away from the particulars of truth tables and capture
  
 truth assignments 
  
 what is essential to the notion as follows. Let us define a
  truth assignment
  for 
  
 a first-order language to be any function
  h
  from the set of all atomic sentences 
  
 of that language into the set
  {
 true, false
 }
 . That is, for each atomic sentence A of 
 the language,
  h
  gives us a truth value, written
  h
 (A), either true or false. 
  
 Intuitively, we can think of 
 each such function
  h
  as representing one row of 
  
 the reference columns of a large truth table.
  
 modeling semantics 
  
  
 Given a truth assignment
  h
 , we can define what it means for
  h
  to make an 
  
 arbitrary sentence of the language true or false. There are many equivalent 
  
 ways to do this. One natural way is to extend
  h
  to a function ˆ
 h
  defined on 
  
 the 
 set of all sentences and taking values in the set
  {
 true
 ,
  false
 }
 . Thus if we think of
  h
  as giving us a row of the 
 reference column, then ˆ
 h
  fills in the 
  
 values of the truth tables for all sentences of the language, that 
 is, the values 
  
 corresponding to
  h
 ’s row. The definition of ˆ
 h
  is what you would expect, given",NA
Completeness for propositional logic,"We are now in a position to prove the Completeness Theorem for propositional
  
 Completeness of F
 T
  
 logic first stated on page 219. Recall that we used the notation
  F
 T
  to stand for 
 that part of
  F
  that uses only the introduction and elimination rules for
 ∧, ∧, 
 ¬,→, ↔
  and
  ∧
 . Given a set
  T
  of sentences and another sentence S, we write
  T ∧
 T
  
 S to mean that there is a formal proof of S in the system
  F
 T
  with premises 
 drawn from
  T
  . It is not assumed that every sentence in
  T
  is actually used in 
 the proof. For example, it might be that the set
  T
  is an infinite set of sentences 
 while only a finite number can be used in any one proof, of course.
  
 Notice that if
  T ∧
 T
  S and
  T
  is a subset of some other set
  T
 ′
 of sentences, then 
 T
 ′
 ∧
 T
  S. We restate the desired result as follows:
  
 Theorem
  (Completeness of
  F
 T
 ) If a sentence S is a tautological consequence of 
 a set
  T
  of sentences then
  T ∧
 T
  S.
  
 You might think that the way to prove the Completeness Theorem would
  
 be to assume that S is a tautological consequence of
  T
  and then try to con-
 struct a proof of S from
  T
  . But since we don’t know anything about the 
 meaning of S or of the sentences in
  T
  , this strategy would get us nowhere. In 
 fact, the way we will prove the theorem is by proving its converse: that if
  
 T ̸∧
 T
  S (that is, if there is no proof of S from
  T
  ), then S is not a tautological 
 consequence of
  T
  . That is to say, we will show that if
  T ̸∧
 T
  S, then there is a 
 truth assignment
  h
  that makes all of the sentences in
  T
  true, but S false. In 
 other words, we will show that
  T ∧ {¬
 S
 }
  is tt-satisfiable. The following lemma 
 will be helpful in carrying out this proof.
  
 Chapter 17",NA
Horn sentences,"In Chapter 4 you learned how to take any sentence built up without quantifiers 
  
 and transform it into one in conjunctive normal form (CNF), CNF, that is, one 
  
 which is a conjunction of one or more sentences, each of which is a disjunction 
  
 of one or more literals. Literals are atomic sentences and their negations. We 
  
 will call a literal
  positive
  or
  negative
  depending on whether it is an atomic 
  
 positive and 
 sentence or the negation of an atomic sentence, respectively. 
 negative literals
  
 A particular kind of CNF sentence turns out to be important in computer 
  
 science. These are the so-called “Horn” sentences, named not after their shape, 
  
 but after the American logician Alfred Horn, who first isolated them and stud-
  
 ied some of their properties. A
  Horn Sentence
  is a sentence in CNF that has 
  
 Horn sentences 
 the following additional property: every disjunction of literals in the sentence 
  
 contains
  at most one
  positive literal. Later in the section we will find that 
  
 there is a more intuitive way of writing Horn sentences if we use the connec-
  
 tive
  →
 . But for now we restrict attention to sentences involving only
  ∧
 ,
  ∧
 , 
  
 and
  ¬
 . The following sentences are all in CNF but none of them are Horn sen-
  
 tences:
  
 Section 17.3",NA
Resolution,"People are pretty good at figuring out when one sentence is a tautological 
 consequence of another, and when it isn’t. If it is, we can usually come up with 
 a proof, especially when we have been taught the important methods of proof. 
 And when it isn’t, we can usually come up with an assignment of truth values 
 that makes the premises true and the conclusion false. But for computer 
 applications, we need a reliable and efficient algorithm for determining when 
 one sentence is a tautological consequence of another sentence or a set of 
 sentences.
  
 Recall that S is a tautological consequence of premises P
 1
 , . . . ,
 P
 n
  if and only 
 if the set
  {
 P
 1
 , . . . ,
  P
 n
 ,¬S}
 is not tt-satisfiable, that is to say, its conjunc-tion is not 
 tt-satisfiable. Thus, the problem of checking for tautological conse-quence and 
 the problem of checking to see that a sentence is not tt-satisfiable amount to 
 the same thing. The truth table method provides us with a reliable method for 
 doing this. The trouble is that it can be highly expensive in terms of time and 
 paper (or computer memory). If we had used it in Fitch, there are many 
 problems that would have bogged down your computer intolerably.
  
 In the case of Horn sentences, we have seen a much more efficient method, 
 one that accounts for the importance of Horn sentences in logic programming. 
 In this section, we present a method that applies to arbitrary sentences in 
 CNF. It is not in general as efficient as the Horn sentence algorithm, but it is 
 often much more efficient than brute force checking of truth tables. It also has 
 the advantage that it extends to the full first-order language with quantifiers. 
 It is known as the resolution method, and lies at the heart of many 
 applications of logic in computer science. While it is not the algorithm that we 
 have actually implemented in Fitch, it is closely related to that algorithm.
  
 set of clauses 
  
  
 The basic notion in resolution is that of a
  set of clauses
 . A
  clause
  is just 
  
 any 
 finite set of literals. Thus, for example,
  
  
 C
 1
  =
  {¬
 Small(a)
 ,
  Cube(a)
 ,
  BackOf(b
 ,
  a)
 } 
 is 
 a clause. So is 
  
  
  
 C
 2
  =
  {
 Small(a)
 ,
  Cube(b)
 }",NA
Advanced Topics in FOL,"This chapter presents some more advanced topics in first-order logic. The first 
 three sections deal with a mathematical framework in which the semantics of 
 fol can be treated rigorously. This framework allows us to make our informal 
 notions of first-order validity and first-order consequence precise, and culmi-
 nates in a proof of the Soundness Theorem for the full system
  F
 . The later 
 sections deal with unification and resolution resolution method, topics of im-
 portance in computer science. The Completeness Theorem for
  F
  is taken up in 
 the next chapter, which does not presuppose the sections on unification and 
 resolution.
  
 Section 18.1",NA
First-order structures,"In our treatment of propositional logic, we introduced the idea of logical con-
 sequence in virtue of the meanings of the truth-functional connectives. We 
 developed the rigorous notion of tautological consequence as a precise 
 approx-imation of the intuitive notion. We achieved this precision thanks to 
 truth ta-ble techniques, which we later extended by means of truth 
 assignments. Truth assignments have two advantages over truth tables: First, 
 in assigning truth values to all atomic sentences at once, they thereby 
 determine the truth or falsity of every sentence in the language, which allows 
 us to apply the concept of tautological consequence to infinite sets of 
 sentences. Second, they allow us to do this with complete mathematical rigor.
  
 In Chapter 10, we introduced another approximation of the intuitive notion 
  
 of consequence, that of first-order consequence, consequence in virtue of the 
  
 meanings of
  ∧,∧
  and =, in addition to the truth-functional connectives. We described a vague technique 
 for determining when a sentence was a first-order 
  
 consequence of others, but did not have an analog of truth tables that gave 
  
 us enough precision to prove results about this notion, such as the Soundness 
  
 Theorem for
  F
 . Now that we have available some tools from set theory, we can solve this 
  
 problem. In this section, we define the notion of a first-order structure. A first-
 first-order structures 
 order 
 structure is analogous to a truth assignment in propositional logic. It",NA
"Truth and satisfaction, revisited","In Chapter 9, we characterized the notion of truth in a domain of discourse 
 rather informally. You will recall that in order to define what it means for a 
 quantified sentence (either
  ∧
 x S(x) or
  ∧
 x S(x)) to be true, we had to have 
 recourse to the notion of satisfaction, what it means for an object
  b
  to satisfy a 
 wff S(x) in a domain of discourse. This was defined in terms of what it means 
 for the simpler sentence S(c) to be true, where c was a new name.
  
 Now that we have defined the notion of a first-order structure, we can
  
 modeling satisfaction 
  
 treat truth and satisfaction more rigorously. The aim here is just to see how
  
 and truth 
  
 our informal treatment looks when you treat it mathematically. There should 
  
 be nothing surprising in this section, unless it is that these intuitive notions 
  
 are a bit complicated to define rigorously.
  
 In our earlier discussion, we explained what it meant for an object
  b
  in the 
 domain of discourse to satisfy a wff S(v) with one free variable. That was 
 enough to serve our needs in discussing truth and the game. However, for 
 more advanced work, it is important to understand what it means for some 
 objects to satisfy a wff P(x
 1
 , . . . ,
  x
 n
 ) with
  n
 -free variables, for any
  n ≥
  0. The 
 case of
  n
  = 0 is the important special case where there are
  no
  free variables, 
 that is, where P is a sentence.
  
 variable assignments 
  
  
 Let M be a first-order structure with domain
  D
 . A
  variable assignment 
  
 in 
 M is, by definition, some (possibly partial) function
  g
  defined on a set of 
  
 variables and taking values 
 in the set
  D
 . Thus, for example, if
  D
  =
  {a, b,c} 
 then the following would all be variable assignments in M:
  
 1. the function
  g
 1
  which assigns
  b
  to the variable x",NA
Soundness for,NA,NA
 fol,"Having made the notion of first-order consequence more precise using the 
 notion of first-order structure, we are now in a position to state and prove the 
 Soundness Theorem for fol. Given a set
  T
  of sentences we write
  T ∧
  S to mean 
 there is a proof of S from premises in
  T
  in the full system
  F
 .
 1
 As mentioned in 
 Chapter 17, this notation does not mean that all the sentences in
  T
  have to be 
 used in the formal proof of S, only that there is a proof of S whose premises are 
 all elements of
  T
  . In particular, the set
  T
  could be infinite (as in the case of 
 proofs from zfc or pa) whereas only a finite number of premises can be used in 
 any one proof. This notation allows us to state the Soundness Theorem as 
 follows.
  
 Theorem
  (Soundness of
  F
 ) If
  T ∧
  S, then S is a first-order consequence of set
  T
  .
  
 soundness of F
  
 Proof:
  The proof is very similar to the proof of the Soundness The-
 orem for
  F
 T
 , the propositional part of
  F
 , on page 215. We will show 
 that any sentence that occurs at any step in a proof
  p
  in
  F
  is a first-
 order consequence of the assumptions in force at that step (which 
 include the premises of
  p
 ). This claim applies not just to sentences at 
 the main level of proof
  p
 , but also to sentences appearing in sub-
 proofs, no matter how deeply nested. The theorem follows from this 
 claim because if S appears at the main level of
  p
 , then the only as-
 sumptions in force are premises drawn from
  T
  . So S is a first-order 
 consequence of
  T
  .
  
 Call a step of a proof
  valid
  if the sentence at that step is a first-order 
 consequence of the assumptions in force at that step. Our earlier
  
 proof of soundness for
  F
 T
  was actually a disguised form of induction on the number 
 of the step in question. Since we had not yet discussed
  
  
 1
 Recall that the formal proof system
  F
  includes all the introduction and elimination rules, but 
 not the
  Con
  procedures.",NA
The completeness of the shape axioms,"In Section 12.5 (on page 339), we promised to convince you that the ten
  
 axioms we gave for shape are complete, that is, that they completely bridged
  
 the gap between first-order consequence and the intuitive notion of logical
  
 consequence for the blocks language, as far as shape is concerned. We list the
  
 axioms again here for your convenience:
  
 Basic Shape Axioms:
  
 1.
  ¬∧
 x (Cube(x)
  ∧
  Tet(x))
  
 2.
  ¬∧
 x (Tet(x)
  ∧
  Dodec(x))
  
 3.
  ¬∧
 x (Dodec(x)
  ∧
  Cube(x))
  
 4.
  ∧
 x (Tet(x)
  ∧
  Dodec(x)
  ∧
  Cube(x))
  
 SameShape
  Introduction Axioms:",NA
Skolemization,"One important role function symbols play in first-order logic is as a way of 
 simplifying (for certain purposes) sentences that have lots of quantifiers 
 nested inside one another. To see an example of this, consider the sentence
  
 ∧
 x
  ∧
 y Neighbor(x
 ,
 y) 
  
 Given a fixed domain of discourse (represented by a first-order structure M, 
 say) this sentence asserts that every
  b
  in the domain of discourse has at least 
 one neighbor
  c
 . Let us write this as
  
 M
  |
 = Neighbor(x
 ,
  y)[
 b, c
 ]
  
 rather than the more formal M
  |
 = Neighbor(x
 ,
  y)[
 g
 ] where
  g
  is the variable assignment that assigns
  b
  to x 
 and
  c
  to y. Now if the original quantified",NA
Unification of terms,"the function symbol f.
  
 ∧
 z
  ∧
 y [(1 + (z
  ×
  z))
  <
  y]
  
 Which of the following functions on nat-
 ural numbers could be used as a Skolem 
 function for this sentence?
  
 1.
  f
 (
 z
 ) =
  z
 2 
  
 2.
  f
 (
 z
 ) =
  z
 2
 + 1 
  
 3.
  f
 (
 z
 ) =
  z
 2
 + 2 
  
 4.
  f
 (
 z
 ) =
  z
 3
  
 We now turn to a rather different topic, unification, that applies mainly to 
 languages that contain function symbols. Unification is of crucial importance 
 when we come to extend the resolution method to the full first-order 
 language.
  
 The basic idea behind unification can be illustrated by comparing a couple 
 of claims. Suppose first that Nancy tells you that Max’s father drives a Honda, 
 and that no one’s grandfather drives a Honda. Now this is not true, but there is 
 nothing logically incompatible about the two claims. Note that if Nancy went 
 on to say that Max was a father (so that Max’s father was a grandfather)
  
 Chapter 18",NA
"Resolution, revisited","In this section we discuss in an informal way how the resolution method for 
  
 propositional logic can be extended to full first-order logic by combining the 
  
 extending resolution 
 tools we have developed above. 
  
  
 to
  fol
  
 The general situation is that you have some first-order premises P
 1
 , . . . ,
  P
 n 
 and a potential conclusion Q. The question is whether Q is a first-order con-
 sequence of P
 1
 , .. . ,
  P
 n
 . This, as we have seen, is the same as asking if there is no 
 first-order structure which is a counterexample to the argument that Q 
 follows from P
 1
 , . . . ,
  P
 n
 . This in turn is the same as asking whether the 
 sentence
  
 P
 1
  ∧ . . . ∧
  P
 n
  ∧ ¬
 Q
  
 Section 18.7",NA
Completeness and,NA,NA
Incompleteness,"This introduction to first-order logic culminates in discussions of two very 
 famous and important results. They are the so-called Completeness Theorem 
 and Incompleteness Theorem of fol. Both are due to the logician Kurt G¨odel, 
 no doubt the greatest logician yet. We present a complete proof of the first, 
 together with a explanation of the second, with just a sketch of its proof.
  
 In this book you have learned the main techniques for giving proofs that 
 one statement is a logical consequence of others. There were simple valid rea-
 soning steps and more intricate methods of proof, like proof by contradiction 
 or the method of general conditional proof. But the definition of logical con-
 sequence was fundamentally a semantic one: S is a logical consequence of 
 premises P
 1
 ,. . . ,P
 n
  if there is no way for the premises to be true without the 
 conclusion also being true. The question arises as to whether the methods of 
 proof we have given are sufficient to prove everything we would like to prove. 
 Can we be sure that if S is a logical consequence of P
 1
 ,. . . ,P
 n
 , then we can find a 
 proof of S from P
 1
 ,. . . ,P
 n
 ?
  
 The answer is both yes and no, depending on just how we make the notion of 
 logical consequence precise, and what language we are looking at.
  
 The answer to our question is yes if by logical consequence we mean first-
  
 G¨odel’s Completeness 
 Theorem
  
 order consequence. G¨odel’s Completeness Theorem for fol assures us that if S
  
 is a first-order consequence of some set
  T
  of first-order sentences then there is 
 a formal proof of S using only premises from
  T
  . The main goal of this chapter 
 is to give a full proof of this important result. The first such completeness
  
 proof was given by G¨odel in his dissertation in 1929. (His proof was actually 
 about a somewhat different formal system, one used by Bertrand Russell and 
 Alfred North Whitehead in their famous work
  Pincipia Mathematica
 , but the 
 formal systems have the same power.) 
  
 Suppose, though, that we are using some specific first-order language and we 
 are interested in the logical consequence relation
  where the meaning of the 
 predicates of the language is taken into account.
  Do we need other methods
  
 G¨odel’s Incompleteness 
  
 of proof? If so, can these somehow be reduced to those we have studied? Or
  
 Theorem 
  
 is it conceivable that there simply is no complete formal system that captures 
  
 the notion of logical consequence for some languages? We will return to these
  
 526",NA
The Completeness Theorem for,NA,NA
 fol,"The first few sections of this chapter are devoted to giving a complete proof 
  
 of the G¨odel Completeness Theorem just referred to. We use the terms “the-
  
 ory” and “set of sentences” interchangeably. (Some authors reserve the term 
  
 theories
 “theory” for a set of first-order sentences which is “closed under provability,”
  
 that is, satisfying the condition that if
  T ∧
  S then S
  ∧ T
  .) In this section we 
 write
  T ∧
  S to mean there is a proof of S from the theory
  T
  in the full system 
 F
 .
 1
 As mentioned in Chapter 17, this notation does not mean that all the 
 sentences in
  T
  have to be used in the formal proof of S, only that there is a 
 proof of S whose premises are all elements of
  T
  . In particular, the set
  T 
 could 
 be infinite (as in the case of proofs from zfc or pa) whereas only a finite
  
 T ∧
  S
  
 number of premises can be used in any one proof. This notation allows us to 
 state the Completeness Theorem as follows.
  
 Theorem
  (Completeness Theorem for
  F
 ). Let
  T
  be a set of sentences of a first-
 order language
  L
  and let S be a sentence of the same language. If S is a
  
 first-order consequence of
  T
  , then
  T ∧
  S.
  
 Exactly as in the case of propositional logic, we obtain the following as an
  
 Completeness Theorem 
 for F
  
 immediate consequence of the Completeness Theorem.
  
 Theorem
  (Compactness Theorem for fol). Let
  T
  be a set of sentences of a first-
 order language
  L
 . If for each finite subset of
  T
  there is a first-order structure 
 making this subset of
  T
  true, then there is a first-order structure M that makes 
 all the sentences of
  T
  true.
  
 The Completeness Theorem for fol was first established by Kurt G¨odel, as
  
 Compactness Theorem
  
 we mentioned above. The proof of the Completeness Theorem for first-order 
  
 consequence is, as we shall see, considerably subtler than for tautological 
  
 consequence. The proof we give here is simpler than G¨odel’s original, though, 
  
 and is based on a proof known as the Henkin method, named after the logician 
 Henkin method 
 Leon Henkin 
 who discovered it.
  
 Recall from Section 10.1 that the truth table method is too blunt to take 
 account of the meaning of either the quantifiers
  ∧
  and
  ∧
  or the identity symbol 
 1
 Recall that the formal proof system
  F
  includes all the introduction and elimination rules, but not 
 the
  Con
  procedures.",NA
Adding witnessing constants,"Given any first-order language
  K
 , we construct a new first-order language
  K
 ′
 .
  
 The language
  K
 ′
 will have the same symbols as
  K
  except that it will have a
  
 lot of new constant symbols. For example, if
  K
  is our blocks language, then
  
 in
  K
 ′
 will be able to say things like the following:
  
 1.
  ∧
 x (Small(x)
  ∧
  Cube(x))
  →
  Small(c
 1
 )
  ∧
  Cube(c
 1
 )
  
 2.
  ∧
 z (z
  ̸
 = a
  ∧
  z
  ̸
 = b)
  →
  (c
 2
  ̸
 = a
  ∧
  c
 2
  ̸
 = b)
  
 3.
  ∧
 y Between(y
 ,
 a
 ,
 b)
  →
  Between(c
 3
 ,
  a
 ,
  b)
  
 4.
  ∧
 x
  ∧
 y Between(a
 ,
  x
 ,
  y)
  → ∧
 y Between(a
 ,
  c
 4
 ,
 y)
  
 More generally, for each wff P of
  L
  with exactly one free variable, form a new
  
 constant symbol c
 P
 , making sure to form different names for different wffs.
  
 This constant is called the
  witnessing constant
  for P.
  
 witnessing constant
  
 3
 In this chapter we are using the notions of tautology and tautological consequence
  
 for
  P
  
 defined in Section 10.1, in which every sentence starting with a quantifier is treated as
  
 atomic.",NA
The Henkin theory,"We have added witnessing constants for each wff P with exactly one free 
 variable. The free variable of P is going to be important in what follows so we 
 often write the wff in a way that reminds us of the free variable, namely, as 
 P(x).
 4
 Consequently, its witnessing constant is now denoted by c
 P(x)
 . Notice 
 that by iterating our construction infinitely often, we have managed to arrange 
 things so that for each wff P(x) of
  L
 H
  with exactly one free variable, the 
 witnessing constant c
 P(x)
  is also in
  L
 H
 . This allows us to form the sentence
  
 ∧
 x P(x)
  →
  P(c
 P(x)
 )
  
 in
  L
 H
 . This sentence is known as the
  Henkin witnessing axiom
  for P(x). The 
 witnessing axioms 
 intuitive idea is 
 that
  ∧
 x P(x)
  →
  P(c
 P(x)
 ) asserts that if there is something that 
  
 satisfies P(x), then the object named by c
 P(x)
  provides an example (or “wit-
  
 ness”) of one such.
  
 Lemma 2.
  (Independence lemma) If c
 P
  and c
 Q
  are two witnessing constants 
 and the date of birth of c
 P
  is less than or equal to that of c
 Q
 , then c
 Q
  does not 
 appear in the witnessing axiom of c
 P
 .
  
 Proof:
  If the date of birth of c
 P
  is less than that of c
 Q
 , the result 
 follows from the date of birth lemma. If they have the same date
  
 4
 Really, we should be writing this as P(
 ν
 ), where
  ν
  can be any of the variables of our
  
 language, not just the variable x. We are using x here as a representative variable of our
  
 language.",NA
The Elimination Theorem,"It follows from Proposition 3 that if a sentence S of
  L
  is a first-order conse-
 quence of
  T ∧ H
 , then it is a first-order consequence of
  T
  alone. (This is what 
 you established in Exercise 19.9.) This result shows that we’ve constructed
  H 
 in such a way that it doesn’t add any substantive new claims, things that give 
 us new first-order consequences (in
  L
 ) of
  T
  . The Elimination Theorem shows 
 us that our deductive system is strong enough to give us a similar result on the 
 formal system side of things.
  
 Elimination Theorem 
  
 Proposition 4.
  (The Elimination Theorem) Let
  p
  be any formal first-order 
  
 proof with a conclusion S that is a sentence of
  L
  and whose premises are",NA
The Henkin Construction,"Proposition 3 allows us to take any first-order structure for
  L
  and get from it 
 one for
  L
 H
  that makes all the same sentences true. This, of course, gives rise to 
 a truth assignment
  h
  to all the sentences of
  L
 H
  that respects the truth-
 functional connectives: just assign true to all the sentences that are true in the 
 structure, false to the others. (You may recall that you were asked to prove 
 this in Exercise 18.11.) The main step in the Henkin proof of the Completeness 
 Theorem is to show that we can reverse this process.
  
 Henkin Construction 
 Lemma
  
 Theorem
  (Henkin Construction Lemma) Let
  h
  be any truth assignment for 
 L
 H
  
 that assigns true to all the sentences of the Henkin theory
  H
 . There is a first-
 order structure M
 h
  such that M
 h
  |
 = S for all sentences S assigned true by the 
 assignment
  h
 .
  
 In giving the proof of this result, we will assume that our language
  L 
 contains only relation symbols and constants, no function symbols. We will 
 return at the end to explain how to modify the proof if there are function 
 symbols. The proof of this theorem has two parts. We must first show how
  
 constructing
  M
 h
  
 to construct M
 h
  from
  h
  and then show that M
 h
  does indeed make true all the 
 sentences to which
  h
  assigned true. To construct M
 h
 , we must do three
  
 things. We must define the domain
  D
  of M
 h
 , we must assign to each
  n
 -ary 
 relation symbol R some set
  R
  of
  n
 -tuples from
  D
 , and we must assign to each 
 name c of
  L
 H
  some element of
  D
 . We first give the basic idea of the 
 construction. This idea won’t quite work, so it will have to be modified, but it’s 
 useful to see the flawed idea before digging into the details that correct the 
 flaw.
  
 The basic (flawed) idea in constructing the first-order structure M is to use 
 the construction of Exercise 18.12, but to count on the fact that
  h
  assigns true 
 to all the sentences in
  H
  to get us past the quantifiers. In more detail, we build 
 M as follows:",NA
The L¨owenheim-Skolem Theorem,"One of the most striking things about the proof of completeness is the na-
 the 
 structure
  M
 h 
  
 ture of the first-order structure M
 h
 . Whereas our original language may be 
  
 talking about physical objects, numbers, sets, what-have-you, the first-order 
  
 structure M
 h
  that we construct by means of the Henkin construction has as 
  
 elements something quite different: equivalence classes of constant symbols.
  
 This observation allows us to exploit the proof to establish something known 
 as the L¨owenheim-Skolem Theorem for fol.
  
 Recall from our discussion of infinite sets in Chapter 15 that there are 
 different sizes of infinite sets. The smallest infinite sets are those that have the 
 same size as the set of natural numbers, those that can be put in one-to-one 
 correspondence with the natural numbers. A set is
  countable
  if it is finite or is 
 the same size as the set of natural numbers. Digging into the details of the 
 proof of completeness lets us prove the following important theorem, due 
 originally to the logicians L¨owenheim and Skolem. They proved it before 
 G¨odel proved the Completeness Theorem, using a very different method. 
 L¨owenheim proved it for single sentences, Skolem proved it for countably 
 infinite sets of sentences.
  
 L¨owenheim-Skolem
  
 Theorem 
  
 Theorem
  (L¨owenheim-Skolem Theorem) Let
  T
  be a set of sentences in a 
  
 countable language
  L
 . Then if
  T
  is satisfied by a first-order structure, it is 
 satisfied by one whose domain is countable.
  
 Proof:
  (Sketch) By the Soundness Theorem, if
  T
  is satisfiable, then it 
 is formally consistent. The proof of completeness shows that if 
 T
  is 
 formally consistent, it is true in a first-order structure of the form M
 h
 , 
 for some truth assignment
  h
  for
  L
 H
 . Let’s assume that our original 
 language
  L
  is countable and ask how big the structure M
 h
  is. The 
 answer to this is not hard to determine. There cannot be any more 
 elements of M
 h
  than there are constant symbols in the language
  L
 H
 . 
 Each of these can be written down using the symbol c, with 
 subscripts involving symbols from
  L
 , though the subscripts need to 
 be able to be iterated arbitrarily far. Still, if we can list the symbols of
  
 L
  in a list, we can use this list to alphabetize all the witnessing 
 constants of
  L
 H
 . In this way, it is possible to show that the domain of 
 the structure M
 h
  is countable.",NA
The Compactness Theorem,"As we mentioned before, one of the immediate consequences of the Complete-
 ness Theorem is the first-order Compactness Theorem:
  
 Compactness Theorem
  
 Theorem
  (Compactness Theorem for fol) Let
  T
  be a set of sentences of a first-
 order language
  L
 . If every finite subset of
  T
  is true in some first-order 
 structure, then there is a first-order structure M that makes every sentence
  
 of
  T
  true.
  
 This follows from Completeness for the simple reason that proofs in
  F
  are 
 finite and so can only use finitely many premises. If
  T
  is not satisfiable, then 
 (by Completeness) there is a proof of
  ∧
  from sentences in
  T
  , and this proof 
 can only use finitely many premises from
  T
  . So that subset of
  T
  can’t be 
 satisfiable (by Soundness).
  
 It turns out that this theorem, like the L¨owenheim-Skolem Theorem, 
 shows some important expressive limitations of fol. In particular, we can show 
 that it is impossible to come up with axioms in the first-order language of 
 arithmetic that characterize the structure of the natural numbers.
  
 nonstandard models 
  
 Theorem
  (Nonstandard models of arithmetic) Let
  L
  be the language of Peano 
  
 arithmetic. There is a first-order structure M such that
  
 1. M contains all the natural numbers in its domain,
  
 2. M also contains elements greater than all the natural numbers, but
  
 3. M makes true exactly the same sentences of
  L
  as are true about the 
 natural numbers.
  
 Proof:
  (Sketch) The proof of this result is fairly easy using the 
 Compactness Theorem. The language of Peano arithmetic, as we 
 defined it in Chapter 16, did not contain a symbol for greater than, 
 but we can define x
  >
  y by the wff
  ∧
 z (z
  ̸
 = 0
  ∧
  x = y + z). To say that an 
 element
  n
  of M is greater than all the natural numbers is to say that
  n
  
 satisfies all the wffs:",NA
The G¨odel Incompleteness Theorem,"The theorem showing the existence of nonstandard models of arithmetic 
 shows a kind of incompleteness of fol. There is, however, a far deeper form of 
 in-completeness that was discovered by Kurt G¨odel a few years after he 
 proved the Completeness Theorem. This is the famous result known as 
 G¨odel’s In-completeness Theorem.
  
 Students are sometimes puzzled by the fact that G¨odel first proved some-
  
 completeness vs. 
  
 thing called the Completeness Theorem, but then turned around and proved
  
 incompleteness 
  
 the Incompleteness Theorem. Couldn’t he make up his mind? Actually, though, 
  
 the senses of “completeness” involved in these two theorems are quite 
 different. 
  
 Recall that the Completeness Theorem tells us that our formal rules of proof 
  
 adequately capture first-order consequence. The Incompleteness Theorem, by 
  
 contrast, involves the notion of formal completeness introduced earlier. Re-
  
 member that a theory
  T
  is said to be
  formally complete
  if for any sentence S of 
  
 its language, either S or
  ¬
 S is provable from
  T
  . (We now know, by soundness 
  
 and completeness, that this is equivalent to saying that S or
  ¬
 S is a first-order 
  
 consequence of
  T
  .) In the early part of the twentieth century, logicians were 
 analyzing mathe-
  
 matics by looking at axiomatic theories like Peano arithmetic and formal proof 
  
 systems like
  F
 . The aim was to come up with a formally complete axiomati-
 zation of arithmetic, one that allowed us to prove all and only the sentences",NA
Summary of Rules,NA,NA
Propositional rules (,NA,NA
F,T,NA
),"Conjunction Introduction (
 ∧
  
 Intro)
  
 P
 1
  
 ∧
  
 P
 n 
  
 ...
  
 Conjunction Elimination (
 ∧
  
 Elim)
  
 P
 1
  ∧ . . . ∧
  P
 i
  ∧ . . . ∧
  P
 n 
 ...
  
 ∧
  
 P
 i
  
 Disjunction Elimination 
 (
 ∧
  Elim)
  
 P
 1
  ∧ . . . ∧
  P
 n 
 ...
  
 ∧
  
 P
 1
  ∧ .. . ∧
  P
 n
  
 Disjunction Introduction (
 ∧
  
 Intro)
  
 P
 i
 ...
  
 ∧
  
 P
 1
  ∧ .. . ∧
  P
 i
  ∧ . . . ∧
  P
 n
  
  
  
  
  
 P
 1 
  
 ...
  
 S
  
 ∧
  
 ∧
  
  
  
 ...
  
 P
 n
  
 ...
  
 S
  
 S
  
 557",NA
First-order rules (,NA,NA
F,NA,NA
),"Identity Introduction (
 =
  
 Intro)
  
 Identity Elimination 
  
 (
 =
  Elim)
  
 P(n) 
  
 ...
  
 ∧
  
 n = n
  
 n = m 
  
 ...
  
 ∧
 P(m)
  
 First-order rules (
 F
 )",NA
Inference Procedures (Con Rules),"Fitch also contains three, increasingly powerful inference procedures. They are 
 not technically inference rules.
  
 Tautological Consequence 
  
 (Taut Con)
  
 Taut Con
  allows you to infer any sentence that follows from the cited sen-
 tences in virtue of the meanings of the truth-functional connectives alone.
  
 First-order Consequence 
  
 (FO Con)
  
 FO Con
  allows you to infer any sentence that follows from the cited sentences 
 in virtue of the meanings of the truth-functional connectives, the quantifiers 
 and the identity predicate.
  
 Analytic Consequence 
  
 (Ana Con)
  
 In theory,
  Ana Con
  should allow you to infer any sentence that follows from 
 the cited sentences in virtue of the meanings of the truth-functional 
 connectives, the quantifiers, the identity predicate and the blocks language 
 predicates. The Fitch implementation of
  Ana Con
 , however, does not take into 
 account the meaning of Adjoins or Between due to the complexity these 
 predicates give rise to.
  
 Inference Procedures (
 Con
  Rules)",NA
Glossary,"Ambiguity:
  A feature of natural languages that makes it possible for a single 
 sentence to have two or more meanings. For example,
  Max is happy or 
 Claire is happy and Carl is happy
 , can be used to claim that either Max is 
 happy or both Claire and Carl are happy, or it can be used to claim that 
 at least one of Max and Claire is happy and that Carl is happy. Ambiguity 
 can also arise from words that have two meanings, as in the case of 
 puns. Fol does not allow for ambiguity.
  
 Antecedent:
  The antecedent of a conditional is its first component clause. In P
  
 →
  Q, P is the antecedent and Q is the consequent. 
  
 Argument:
  The word “argument” is ambiguous in logic.
  
 1. One kind of argument consists of a sequence of statements in which 
 one (the conclusion) is supposed to follow from or be supported by 
 the others (the premises).
  
 2. Another use of “argument” refers to the term(s) taken by a pred-
 icate in an atomic wff. In the atomic wff LeftOf(x
 ,
 a), x and a are the 
 arguments of the binary predicate LeftOf.
  
 Arity:
  The arity of a predicate indicates the number of arguments (in the 
 second sense of the word) it takes. A predicate with arity of one is called 
 unary. A predicate with an arity of two is called binary. It’s possible for a 
 predicate to have any arity, so we can talk about 6-ary or even 113-ary 
 predicates.
  
 Atomic sentences:
  Atomic sentences are the most basic sentences of fol, 
 those formed by a predicate followed by the right number (see arity) of 
 names (or complex terms, if the language contains function symbols). 
 Atomic sentences in fol correspond to the simplest sentences of English.
  
 Axiom:
  An axiom is a proposition (or claim) that is accepted as true about 
 some domain and used to establish other truths about that domain.
  
 Boolean connective (Boolean operator):
  The logical connectives conjunc-
 tion, disjunction, and negation allow us to form complex claims from 
 simpler claims and are known as the Boolean connectives after the logi-
 cian George Boole. Conjunction corresponds to the English word
  and
 ,
  
 562",NA
General Index,"=, 25, 37
  
 ̸
 =, 68
  
 ∧
 , 71
  
 ∧
 , 74
  
 ¬
 , 68
  
 ∧
 , 137, 155, 
 157
 →
 , 178
  
 ↔
 , 181
  
 ∧
 , 228, 230
  
 ∧
 , 228, 230–231
  
 ∧
 , 37, 406
  
 ∧
 , 413
  
 ∧
 , 412
  
 ∩
 , 415
  
 ∧
 , 415
 ∧
 , 
 429
  
 | b |
 , 437
  
 ∧
 , 83 
  
 F
 , 54, 142, 342 
  
 F
 T
 ,
  215
 , 470
  
 ∧
 T
 , 470 M, 498
  
 H
 , 532 
  
 g
 ∧
 , 501
  
 , 6, 7
  
 Â, 6, 7
  
 Â
 |
 , 6
 ∧
 , 7
  
 Reit
 , 56
  
 = Elim
 , 50, 51,
  56
  
 = Intro
 , 50, 51,
  55
  
 ∧
  Elim
 , 143
  
 ∧
  Intro
 , 144
  
 ∧
  Elim
 , 150
  
 ∧
  Intro
 , 148
  
 ¬
  Elim
 , 155, 161
  
 ¬
  Intro
 , 155
  
 ∧
  Elim
 , 159
  
 ∧
  Intro
 , 156
  
 →
  Elim
 , 206
  
 →
  Intro
 , 206
  
 ↔
  Elim
 , 209
  
 ↔
  Intro
 , 209
  
 ∧
  Elim
 , 342
  
 ∧
  Intro
 , 343
  
 ∧
  Elim
 , 348
  
 ∧
  Intro
 , 347
  
 Ana Con
 , 60, 61, 114, 115, 158, 
  
 272, 286 
  
 FO Con
 , 115, 158, 271, 272, 286, 
  
 524 
  
 Taut Con
 , 114–116, 158, 171, 221, 
  
 272
  
 absolute complement, 419, 440 
  
 ac, 436 
  
 Aczel, Peter, 441, 465 
  
 addition, 129 
  
 afa, 441 
  
 affirming the consequent, 203, 212 
  
 alternative notation, 40, 66, 89, 90, 
  
  
 196, 255–256 
  
 ambig-wffs, 444 
  
 ambiguity, 4, 79, 307 
  
 and context, 304 
  
 and inference, 308
  
 573",NA
Exercise Files Index,"Abelard’s Sentences, 183 
  
 Ackermann’s World, 76, 83, 279 
 Alan Robinson’s Sentences, 492 
 Allan’s Sentences, 241 
  
 Ana Con 1, 60 
  
 Anderson’s First World, 305
  
 Conditional Sentences, 203 
  
 Conjunction 1, 143 
  
 Conjunction 2, 145 
  
 Conjunction 3, 146 
  
 Conjunction 4, 147 
  
 Cooper’s World, 388
  
  
 Anderson’s Second World, 306 
 Aristotle’s Sentences, 241 
  
 Arnault’s Sentences, 296
  
 Deduction Thm 1, 535 
  
 DeMorgan’s Sentences, 81, 82
  
 Austin’s Sentences, 27 
 Axioms 1, 286
  
 Bernays’ Sentences, 83 
  
 Bernstein’s Sentences, 234 
  
 Between Sentences, 205 
  
 Bill’s Argument, 64 
  
 Bolzano’s World, 35, 83, 185, 273, 
 279, 300, 302, 372 
  
 Boole’s Sentences, 80
  
 DeMorgan’s Sentences 2, 279 
 Disjunction 1, 151 
  
 Disjunction 2, 153 
  
 DNF Example, 124 
  
 Dodgson’s Sentences, 245
  
 Edgar’s Sentences, 241 
  
 Edgar’s World, 241 
  
 Euler’s Sentences, 186 
  
 Existential 1, 349
  
 Boole’s World, 35, 70, 83, 86, 88
  
 Boolos’ Sentences, 186 
 Bozo’s Sentences, 234 
  
 Brouwer’s Sentences, 70 
 Buridan’s Sentences, 297
  
 Cantor’s Sentences, 290 
  
 Cantor’s World, 290 
  
 Carnap’s Sentences, 273 
  
 Carroll’s World, 296, 308 
  
 Church’s Sentences, 279 
  
 Claire’s World, 72, 249, 279, 300 
 CNF Sentences, 125 
  
 Conditional 1, 207 
  
 Conditional 2, 208 
  
 Conditional 3, 210
  
 Finsler’s World, 292 
  
 FO Con 1, 272 
  
 Frege’s Sentences, 291
  
 Game Sentences, 238 
  
 Game World, 238 
  
 G¨odel’s World, 283 
  
 G¨odel’s World, 303
  
 Henkin Construction, 545 
 Henkin’s Sentences, 534 
  
 Hercule’s Sentences, 296 
  
 Hilbert’s Sentences, 296 
  
 Horn’s Other Sentences, 486 
 Horn’s Sentences, 486
  
 585",NA
