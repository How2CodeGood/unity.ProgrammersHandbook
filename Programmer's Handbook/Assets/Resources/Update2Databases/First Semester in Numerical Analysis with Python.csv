Larger Text,Smaller Text,Symbol
First Semester in Numerical Analysis with Python,NA,NA
Yaning Liu ,NA,NA
Department of Mathematical and Statistical Sciences ,NA,NA
University of Colorado Denver ,NA,NA
Denver CO 80204,NA,NA
Giray Ökten ,NA,NA
Department of Mathematics ,NA,NA
Florida State University ,NA,NA
Tallahassee FL 32306,NA,NA
Contents,"1
  
 Introduction
  
 5
  
  
 2
  
 1.1
  
 Review of Calculus
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 5
  
 1.2
  
 Python basics
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 8
  
 1.3
  
 Computer arithmetic
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 21
  
 Solutions of equations: Root-finding
  
 42
  
 3
  
 2.1
  
 Error analysis for iterative methods
  . . . . . . . . . . . . . . . . . . . . . . .
  
 45
  
 2.2
  
 Bisection method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 46
  
 2.3
  
 Newton’s method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 50
  
 2.4
  
 Secant method
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 59
  
 2.5
  
 Muller’s method
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 61
  
 2.6
  
 Fixed-point iteration
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 64
  
 2.7
  
 High-order fixed-point iteration
  . . . . . . . . . . . . . . . . . . . . . . . . .
  
 72
  
 Interpolation
  
 75
  
 4
  
 3.1
  
 Polynomial interpolation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 76
  
 3.2
  
 High degree polynomial interpolation
  . . . . . . . . . . . . . . . . . . . . . .
  
 92
  
 3.3
  
 Hermite interpolation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 96
  
 3.4
  
 Piecewise polynomials: spline interpolation
  . . . . . . . . . . . . . . . . . . .
  
 104
  
 Numerical Quadrature and Differentiation
  
 12
 1
  
 4.1
  
 Newton-Cotes formulas
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 121
  
 4.2
  
 Composite Newton-Cotes formulas
  
 . . . . . . . . . . . . . . . . . . . . . . .
  
 127
  
 4.3
  
 Gaussian quadrature
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 131
  
 4.4
  
 Multiple integrals
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 137
  
 4.5
  
 Improper integrals
  
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 144
  
 4.6
  
 Numerical differentiation
  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  
 145
  
 2",NA
Preface,"The book is based on “First semester in Numerical Analysis with Julia”, written by 
 GirayÖkten
 1
 . The contents of the original book are retained, while all the algorithms are im-
 plemented in Python (Version 3.8.0). Python is an open source (under OSI), interpreted, 
 general-purpose programming language that has a large number of users around the 
 world. Python is ranked the third in August 2020 by the TIOBE programming community 
 index
 2
 , a measure of popularity of programming languages, and is the top-ranked 
 interpreted language. We hope this book will better serve readers who are interested in a 
 first course in Numerical Analysis, but are more familiar with Python for the 
 implementation of the algorithms.
  
 The first chapter of the book has a self-contained tutorial for Python, including how to 
 set up the computer environment. Anaconda, the open-source individual edition, is recom-
 mended for an easy installation of Python and effortless management of Python packages, 
 and the Jupyter environment, a web-based interactive development environment for 
 Python as well as many other programming languages, was used throughout the book and 
 is recom-mended to the readers for easy code developement, graph visualization and 
 reproducibility.
  
 The book was also inspired by a series of Open Educational Resources workshops at 
 Uni-versity of Colorado Denver and supported partially by the professional development 
 funding thereof. Yaning Liu also thanks his students in his Numerical Analysis classes, who 
 enjoyed using Python to implement the algorithms and motivated him to write a Numerical 
 Analysis textbook with codes in Python.
  
 Yaning Liu 
  
 August 2020 
  
 Denver, Colorado
  
 Giray 
 Ökten 
  
 August 
 2020 
  
 Tallahassee, 
 Florida
  
 1
 https://open.umn.edu/opentextbooks/textbooks/first-semester-in-numerical-analysis-with-
 julia 
 2
 https://www.tiobe.com/tiobe-index/",NA
Chapter 1,NA,NA
Introduction,NA,NA
1.1 ,NA,NA
Review of Calculus,"There are several concepts and facts from Calculus that we need in Numerical Analysis. In 
 this section we will list some definitions and theorems that will be needed later. For the 
 most part functions in this book refer to real valued functions defined on real numbers
  R
 , 
 or an interval (
 a, b
 )
  ⊂
  R
 .
  
 Definition 1. 
  
 1. A function
  f
  has the limit
  L
  at
  x
 0
 , written as lim
 x→x
 0
  f
 (
 x
 ) =
  L,
  if for any
  ϵ >
  
 0
 ,
  there exists
  δ >
  0 such that
  |f
 (
 x
 )
  − L| < ϵ
  whenever 0
  < |x − x
 0
 | < δ.
  
 2. A function
  f
  is continuous at
  x
 0
  if lim
 x→x
 0
  f
 (
 x
 ) =
  f
 (
 x
 0
 )
 ,
  and
  f
  is continuous on a set 
  
 A
  if it 
 is continuous at each
  x
 0
  ⊂ A.
  
 3. Let
  {x
 n
 }
 ∞n
 =1
 be an infinite sequence of real numbers. The sequence has the limit
  x, 
 i.e., 
 lim
 n→∞
  x
 n
  =
  x
  (or, written as
  x
 n
  → x
  as
  n → ∞
 ) if for any
  ϵ >
  0
 ,
  there exists an integer
  N >
  
 0 such that
  |x
 n
  − x| < ϵ
  whenever
  n > N.
  
 Theorem 2.
  The following are equivalent for a real valued function f
  :
  
 1.
  f
  is continuous at
  x
 0
  
 2. If
  {x
 n
 }
 ∞n
 =1
 is any sequence converging to
  x
 0
 ,
  then lim
 n→∞
 f
 (
 x
 n
 ) =
  f
 (
 x
 0
 )
 .
  
 Definition 3.
  We say
  f
 (
 x
 ) is differentiable at
  x
 0
  if
  
 f
 ′
 (
 x
 0
 ) = lim 
 x→x
 0
  
 f
 (
 x
 )
  − f
 (
 x
 0
 ) 
 x 
 − x
 0
  
 = lim 
 h→
 0
  
 f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 ) 
 h
  
 exists.
  
 5",NA
1.2,NA,NA
Python basics,"We recommend using the free and open-source Python package manager Anaconda 
 (Individ-ual Edition
  https://www.anaconda.com/products/individual
 ). Go to the webpage 
 and choose the installer according to your operating system. At the time of writing this 
 book, Python Version 3.8 is installed by default, which is also the version we use in this 
 book. There are different environments and editors to run Python. Here we will use the 
 Jupyter environment, which will be ready after Anaconda is installed. There are several 
 tutorials and other resources on Python at
  https://www.python.org/
  where one can find 
 up-to-date information on Python.
  
 The Jupyter environment uses the so-called Jupyter notebook where one can write and 
 edit a Python code, run the code, and export the work into various file formats including 
 Latex and pdf. Most of our interaction with Python will be through the Jupyter notebooks. 
 Note that Anaconda, once installed, has already preinstalled a large number of commonly 
 used Python packages, so it is not necessary to install new packages for the purpose of 
 running the codes in this book.
  
 After installing Anaconda, open a Jupyter notebook by following Anaconda
  →
  Jupyter 
 Notebook
  →
  new
  →
  Python 3 . Here is a screenshot of my notebook:",NA
NumPy arrays ,"NumPy is a useful Python package for array data structure, random number generation, 
 linear algebra algorithms, and so on. A NumPy array is a data structure that can be used to 
 represent vectors and matrices, for which the computations are also made easier. Import 
 the NumPy package and define an alias (np) for it.
  
 In [5]:
  import
  numpy
  as
  np 
  
 Here is the basic syntax to create a 1D NumPy array (representing a vector): 
  
 In [6]:
  x
  =
  np
 .
 array([
 10
 ,
  20
 ,
  30
 ]) 
  
  
  
 x 
  
 Out[6]:
  array([10, 20, 30]) 
  
 The following line of code shows the entries of the created array are integers of 64 bits.
  
 In [7]:
  x
 .
 dtype 
  
 Out[7]:
  dtype('int64') 
  
 If we input a real, Python will change the type accordingly: 
  
 In [8]:
  x
  =
  np
 .
 array([
 10
 ,
  20
 ,
  30
 ,
  0.1
 ]) 
  
  
  
 x 
  
 Out[8]:
  array([10. , 20. , 30. , 
  
 0.1]) 
  
 In [9]:
  x
 .
 dtype 
  
 Out[9]:
  dtype('float64') 
  
 A 1D NumPy array does not assume a particular row or column arrangement of the data, 
 and hence taking transpose for a 1D NumPy array is not valid. Here is another way to 
 construct a 1D array, and some array operations: 
  
 In [10]:
  x
  =
  np
 .
 array([
 10*
 i
  for
  i
  in
  range
 (
 1
 ,
  6
 )]) 
  
  
  
  
 x 
  
 Out[10]:
  array([10, 20, 30, 40, 50]) 
  
 In [11]:
  x[
 -1
 ]",NA
Plotting ,"There are several packages for plotting functions and we will use the PyPlot package. The 
 package is preinstalled in Anaconda. To start the package, use 
  
 In [19]:
  import
  matplotlib.pyplot
  as
  plt 
  
  
 %
 matplotlib
  inline 
  
 The following piece of code is a simple example of plotting with PyPlot.",NA
Matrix operations,"NumPy uses 2D arrays to represent matrices. Let’s create a 3
  ×
  3 matrix (2D array):
  
 In [22]:
  A
  =
  np
 .
 array([[
 -1
 ,
  0.26
 ,
  0.74
 ], [
 0.09
 ,
  -1
 ,
  0.26
 ], [
 1
 ,
 1
 ,
 1
 ]])
  
 A
  
 Out[22]:
  array([[-1. 
  
 , 
  
 0.26, 
  
 0.74],
  
 [ 0.09, -1. 
  
 , 0.26],
  
 [ 1. 
  
 , 
  
 1. 
  
 , 1. 
  
 ]])
  
 Transpose of
  A
  is computed as:
  
 In [23]:
  A
 .
 T
  
 Out[23]:
  array([[-1.
  
 ,
  
 0.09,
  
 1.
  
 ],
  
 [ 0.26, -1.
  
 ,
  
 1.
  
 ],
  
 [ 0.74,
  
 0.26,
  
 1.
  
 ]])
  
 Here is its inverse.
  
 In [24]:
  np
 .
 linalg
 .
 inv(A)",NA
Logic operations ,"Here are some basic logic operations:  
 In [31]:
  2 == 3 
  
 Out[31]:
  False  
 In [32]:
  2 <= 3 
  
 Out[32]:
  True  
 In [33]:
  (
 2==2
 )
  or
  (
 1<0
 )  
 Out[33]:
  True  
 In [34]:
  (
 2==2
 )
  and
  (
 1<0
 )  
 Out[34]:
  False  
 In [35]:
  (
 4 % 2
 )
  == 0
  # Check if 4 is an even number 
  
 Out[35]:
  True  
 In [36]:
  (
 5 % 2
 )
  == 0 
  
 Out[36]:
  False  
 In [37]:
  (
 5 % 2
 )
  == 1
  # Check if 5 is an odd number 
  
 Out[37]:
  True",NA
Defining functions ,"There are two ways to define a function. Here is the basic syntax:  
 In [38]:
  def
  squareit
 (x):  
  
  
 return
  x
 **2 
  
 In [39]:
  squareit(
 3
 )  
 Out[39]:
  9  
 There is also a compact form to define a function, if the body of the function is a short, simple 
 expression:",NA
Types,"In Python, there are several types for integers and floating-point numbers such as int8, int64, 
 float32, float64, and more advanced types for Boolean variables and strings. When we write a 
 function, we do not have to declare the type of its variables: Python figures what the correct type is 
 when the code is compiled. This is called a dynamic type system. For example, consider the
  
 squareit 
 function we defined before: 
 In [44]:
  def
  squareit
 (x):  
  
 return
  x
 **2
  
 The type of
  x
  is not declared in the function definition. We can call it with real or integer inputs, and 
 Python will know what to do: 
 In [45]:
  squareit(
 5
 ) 
 Out[45]:
  25 
 In [46]:
  squareit(
 5.5
 ) 
 Out[46]:
  30.25 
 Now suppose the type of the input is a floating-point number. We can write another version of 
 squareit
  that specifies the type.",NA
Control flow,"Let’s create a NumPy array of 10 entries of floating-type. A simple way to do it is by using the 
 function
  np.zeros(n)
 , which creates an array of size
  n
 , and sets each entry to zero. (A similar 
 function is
  np.ones(n)
  which creates an array of size
  n
  with each entry set to 1.) 
 In [50]:
  values
  =
  np
 .
 zeros(
 10
 )  
  
 values 
 Out[50]:
  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 
 Now we will set the elements of the array to values of sin function. 
 In [51]:
  for
  n
  in
  range
 (
 10
 ):  
  
 values[n]
  =
  np
 .
 sin((n
 +1
 )
 **2
 ) 
 In [52]:
  values 
 Out[52]:
  array([ 0.84147098, -0.7568025 ,-
 0.99177885, -0.95375265, 
 0.41211849, -0.28790332, -0.13235175, 
 0.92002604, -0.62988799, -0.50636564]) 
 Here is another way to do this. Start with creating an empty array:  
 In [53]:
  newvalues
  =
  np
 .
 array([])  
 Then use a
  while
  statement to generate the values, and append them to the array.",NA
Random numbers,"These are 5 uniform random numbers from (0,1). 
 In [62]:
  import
  numpy
  as
  np",NA
1.3,NA,NA
Computer arithmetic,"The way computers store numbers and perform computations could surprise the beginner. In 
 Python if you type (
 √
 3)
 2
 the result will be 2.9....96, where 9 is repeated 15 times. Here are two 
 obvious 
 but fundamental differences in the way computers do arithmetic: 
 • only finitely many numbers can be represented in a computer; 
 • a number represented in a computer can only have finitely many digits. 
 Therefore the numbers that can be represented in a computer exactly is only a subset of rational 
 numbers. Anytime the computer performs an operation whose outcome is not a number that can be 
 represented exactly in the computer, an approximation will replace the exact number. This is called 
 the
  roundoff error
 : error produced when a computer is used to perform real number calculations.",NA
Floating-point representation of real numbers,"Here is a general model for representing real numbers in a computer: 
 x
  =
  s
 (
 .a
 1
 a
 2
 ...a
 t
 )
 β
  × β
 e 
  
 (1.1)",NA
Representation of integers,"In the previous section, we discussed representing real numbers in a computer. Here we will give a 
 brief discussion of representing integers. How does a computer represent an integer
  n
 ? As in real 
 numbers, we start with writing
  n
  in base 2. We have 64 bits to represent its digits and sign. As in the 
 floating-point representation, we can allocate one bit for the sign, and use the rest, 63 bits, for the 
 digits. This approach has some disadvantages when we start adding integers. Another approach, 
 known as the
  two’s complement
 , is more commonly used, including in Python. 
 For an example, assume we have 8 bits in our computer. To represent 12 in two’s complement 
 (or any positive integer), we simply write it in its base 2 expansion: (00001100)
 2
 . To represent
  −
 12, 
 we do the following: flip all digits, replacing 1 by 0, and 0 by 1, and then add 1 to the result. When 
 we flip digits for 12, we get (11110011)
 2
 , and adding 1 (in binary), gives (11110100)
 2
 . 
 Therefore
 −
 12 is represented as (11110100)
 2
  in two’s complement approach. It may seem 
 mysterious to go through all this trouble to represent
  −
 12, until you add the representations of 12 
 and
  −
 12, 
 (00001100)
 2
  + (11110100)
 2
  = (
 1
 00000000)
 2
  
 and realize that the first 8 digits of the sum (from right to left), which is what the computer can only 
 represent (ignoring the red digit 1), is (00000000)
 2
 . So just like 12 + (
 −
 12) = 0 in base 10, the sum 
 of the representations of these numbers is also 0. 
 We can repeat these calculations with 64-bits, using Python. The function
  int2bin
  defined below 
 outputs the digits of an integer, using two’s complement for negative numbers: 
 In [1]:
  import
  struct 
  
 def
  int2bin
 (i):  
  
  
 (d,)
  =
  struct
 .
 unpack(
 "">Q""
 , struct
 .
 pack(
 "">q""
 , i))  
  
 return
  f'
 {d:064b}
 '
  
 In [2]:
  int2bin(
 12
 )",NA
Chopping & Rounding,"Let
  x
  be a real number with more digits the computer can handle:
  x
  = 0
 .d
 1
 d
 2
  . . . d
 k
 d
 k
 +1
  . . . ×
  10
 n
 . 
 How 
 will the computer represent
  x
 ? Let’s use the notation
  fl
 (
 x
 ) for the floating-point representation of
  x.
  
 There are two choices, chopping and rounding: 
 • In chopping, we simply take the first
  k
  digits and ignore the rest:
  fl
 (
 x
 ) = 0
 .d
 1
 d
 2
  . . . d
 k
 .
  
 • In rounding, if
  d
 k
 +1
  ≥
  5 we add 1 to
  d
 k
  to obtain
  fl
 (
 x
 )
 .
  If
  d
 k
 +1
  <
  5
 ,
  then we simply do as in 
 chopping.",NA
Absolute and relative error,"Since computers only give approximations to real numbers, we need to be clear on how we measure 
 the error of an approximation. 
 Definition 12.
  Suppose
  x
 ⊂
 is an approximation to
  x
 . 
 •
  |x
 ⊂
 − x|
  is called the
  absolute error
  
 • 
 |x
 ⊂
 −x| |x|
  is called the
  relative error
  (
 x ̸
 = 0) 
 Relative error usually is a better choice of measure, and we need to understand why. 
 Example 13.
  Find absolute and relative errors of 
 1.
  x
  = 0
 .
 20
  ×
  10
 1
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 1
  
 2.
  x
  = 0
 .
 20
  ×
  10
 −
 2
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 −
 2
  
 3.
  x
  = 0
 .
 20
  ×
  10
 5
 , x
 ⊂
 = 0
 .
 21
  ×
  10
 5
  
 Notice how the only difference in the three cases is the exponent of the numbers. The absolute 
 errors are: 0
 .
 01
 ×
 10 , 0
 .
 01
 ×
 10
 −
 2
 , 0
 .
 01
 ×
 10
 5
 . The absolute errors are different since the exponents are 
 different. However, the relative error in each case is the same: 0
 .
 05
 .
  
 Definition 14.
  The number
  x
 ⊂
 is said to approximate
  x
  to
  s
  significant digits (or figures) if
  s
  is 
 the largest nonnegative integer such that 
 |x − x
 ⊂
 | |x|≤
  5
  ×
  10
 −s
 .
  
 In Example
  13
  we had
 |x−x
 ⊂
 | 
   
 |x| 
  
 = 0
 .
 05
  ≤
  5
 ×
 10
 −
 2
 but not less than or equal to 5
 ×
 10
 −
 3
 .
  
 Therefore we say
  x
 ⊂
 = 0
 .
 21 approximates
  x
  = 0
 .
 20 to 2 significant digits (but not to 3 digits). 
 When the computer approximates a real number
  x
  by
  fl
 (
 x
 )
 ,
  what can we say about the error? 
 The following result gives an upper bound for the relative error.",NA
Machine epsilon,"Machine epsilon
  ϵ
  is the smallest positive floating point number for which
  fl
 (1+
 ϵ
 )
  >
  1
 .
  This means, if 
 we add to 1
 .
 0 any number less than
  ϵ
 , the machine computes the sum as 1
 .
 0. 
 The number 1.0 in its binary floating-point representation is simply (1
 .
 0
  . . .
  0)
 2
  where
  a
 2
  =
  a
 3
  = 
 ...
  =
  a
 53
  = 0
 .
  We want to find the smallest number that gives a sum larger than 1.0, when it is added 
 to 1.0. The answer depends on whether we chop or round. 
 If we are chopping, examine the binary addition",NA
Propagation of error,"We discussed the resulting error when chopping or rounding is used to approximate a real number 
 by its machine version. Now imagine carrying out a long calculation with many arithmetical 
 operations, and at each step there is some error due to say, rounding. Would all the rounding errors 
 accumulate and cause havoc? This is a rather difficult question to answer in general. For a much 
 simpler example, consider adding two real numbers
  x, y.
  In the computer, the numbers are 
 represented as 
 fl
 (
 x
 )
 , fl
 (
 y
 )
 .
  The sum of these number is
  fl
 (
 x
 ) +
  fl
 (
 y
 ); however, the computer can only 
 represent its floating-point version,
  fl
 (
 fl
 (
 x
 ) +
  fl
 (
 y
 ))
 .
  Therefore the relative error in adding two 
 numbers is: 
 (
 x
  +
  y
 )
  − fl
 (
 fl
 (
 x
 ) +
  fl
 (
 y
 )) 
 x
  +
  y
  
  .
  
 In this section, we will look at some specific examples where roundoff error can cause problems, 
 and how we can avoid them. 
 Subtraction of nearly equal quantities: Cancellation of leading digits
  
 The best way to explain this phenomenon is by an example. Let
  x
  = 1
 .
 123456
 , y
  = 1
 .
 123447
 .
  We will 
 compute
  x − y
  and the resulting roundoff error using rounding and 6-digit arithmetic. First, we find
  
 fl
 (
 x
 )
 , fl
 (
 y
 ) : 
 fl
 (
 x
 ) = 1
 .
 12346
 , fl
 (
 y
 ) = 1
 .
 12345
 .
  
 The absolute and relative error due to rounding is: 
 |x − fl
 (
 x
 )
 |
  = 4
  ×
  10
 −
 6
 , |y − fl
 (
 y
 )
 |
  = 3
  ×
  10
 −
 6 
  
 |x − fl
 (
 x
 )
 | 
   
 |x| 
 = 3
 .
 56
  ×
  10
 −
 6
 ,|y − 
 fl
 (
 y
 )
 | |y| 
  
 = 2
 .
 67
  ×
  10
 −
 6
 .
  
 From the relative errors, we see that
  fl
 (
 x
 ) and
  fl
 (
 y
 ) approximate
  x
  and
  y
  to six significant digits. Let’s 
 see how the error propagates when we subtract
  x
  and
  y.
  The actual difference is: 
 x − y
  = 1
 .
 123456
  −
  1
 .
 123447 = 0
 .
 000009 = 9
  ×
  10
 −
 6
 .
  
 The computer finds this difference first by computing
  fl
 (
 x
 )
 , fl
 (
 y
 )
 ,
  then taking their difference and 
 approximating the difference by its floating-point representation:
  fl
 (
 fl
 (
 x
 )
  − fl
 (
 y
 )) : 
 fl
 (
 fl
 (
 x
 )
  − fl
 (
 y
 )) =
  fl
  (1
 .
 12346
  −
  1
 .
 12345) = 10
 −
 5
 .",NA
Sources of error in applied mathematics,"Here is a list of potential sources of error when we solve a problem. 
 1. Error due to the simplifying assumptions made in the development of a mathematical model 
 for the physical problem. 
 2. Programming errors. 
 3. Uncertainty in physical data: error in collecting and measuring data. 
 4. Machine errors: rounding/chopping, underflow, overflow, etc. 
 5. Mathematical truncation error: error that results from the use of numerical methods in 
 solving a problem, such as evaluating a series by a finite sum, a definite integral by a 
 numerical integration method, solving a differential equation by a numerical method. 
 Example 23.
  The volume of the Earth could be computed using the formula for the volume of a 
 sphere,
  V
  = 4
 /
 3
 πr
 3
 , where
  r
  is the radius. This computation involves the following approximations: 
 1. The Earth is modeled as a sphere (modeling error) 
 2. Radius
  r ≈
  6370 km is based on empirical measurements (uncertainty in physical data) 
 3. All the numerical computations are done in a computer (machine error) 
 4. The value of
  π
  has to be truncated (mathematical truncation error) 
 Exercise 1.3-6:
  
 The following is from ""Numerical mathematics and computing"" by Cheney 
 & Kincaid [
 7
 ]: 
 In 1996, the Ariane 5 rocket launched by the European Space Agency exploded 40 seconds after lift-off 
 from Kourou, French Guiana. An investigation determined that the horizontal velocity required the 
 conversion of a 64-bit floating-point number to a 16-bit signed integer. It failed because the number 
 was larger than 32,767, which was the largest integer of this type that could be stored in memory. The 
 rocket and its cargo were valued at $500 million.
  
 Search online, or in the library, to find another example of computer arithmetic going very wrong! 
 Write a short paragraph explaining the problem, and give a reference.",NA
Chapter 2,NA,NA
Solutions of equations: Root-finding,"Arya and the mystery of the Rhind papyrus
  
 College life is full of adventures, some hopefully of intellectual nature, and Arya is doing her part by 
 taking a history of science class. She learns about the Rhind papyrus; an ancient Egyptian papyrus 
 purchased by an antiquarian named Henry Rhind in Luxor, Egypt, in 1858. 
  
 Figure 2.1: Rhind Mathematical Papyrus. (British Museum Image under a Creative Com-
 mons license.)
  
 The papyrus has a collection of mathematical problems and their solutions; a translation is given by 
 Chace and Manning [
 2
 ]. The following is Problem 26, taken from [
 2
 ]: 
 42",NA
2.1,NA,NA
Error analysis for iterative methods,"Assume we have an iterative method
  {p
 n
 }
  that converges to the root
  p
  of some function. How can 
 we assess the rate of convergence? 
 Definition 24.
  Suppose
  {p
 n
 }
  converges to
  p
 . If there are constants
  C >
  0 and
  α >
  1 such that 
 |p
 n
 +1
  − p| ≤ C|p
 n
  − p|
 α
 , 
  
 (2.1) 
 for
  n ≥
  1, then we say
  {p
 n
 }
  converges to
  p
  with order
  α
 . 
 Special cases:
  
 • If
  α
  = 1 and
  C <
  1, we say the convergence is linear, and the rate of convergence is
  C
 . In 
 this case, using induction, we can show 
 |p
 n
 +1
  − p| ≤ C
 n
 |p
 1
  − p|. 
  
 (2.2) 
 There are some methods for which Equation (
 2.2
 ) holds, but Equation (
 2.1
 ) does not hold 
 for any
  C <
  1. We still call these methods to be of linear convergence. An example is the 
 bisection method. 
 • If
  α >
  1, we say the convergence is superlinear. 
 quadratic convergence. 
 Example 25.
  Consider the sequences defined by 
 In particular, the case
  α
  = 2 is called 
 p
 n
 +1
  = 0
 .
 7
 p
 n
  and
  p
 1
  = 1  
 p
 n
 +1
  = 0
 .
 7
 p
 2 
 n
 and
  p
 1
 = 1
 .",NA
2.2 ,NA,NA
Bisection method,"Let’s recall the Intermediate Value Theorem (IVT), Theorem
  6
 : If a continuous function
  f
  defined on 
 [
 a, b
 ] satisfies
  f
 (
 a
 )
 f
 (
 b
 )
  <
  0
 ,
  then there exists
  p ⊂
  [
 a, b
 ] such that
  f
 (
 p
 ) = 0
 .
  
 Here is the idea behind the method. At each iteration, divide the interval [
 a, b
 ] into two subin-
 tervals and evaluate
  f
  at the midpoint. Discard the subinterval that does not contain the root and 
 continue with the other interval. 
 Example 26.
  Compute the first three iterations by hand for the function plotted in Figure (
 2.2
 ). 
 2.5
  
 -4
  
 -3
  
 -2
  
 -1
  
 0
  
 1
  
 2
  
 3
  
 4
  
 -2.5
  
 Figure 2.2
  
 Step 1 : To start, we need to pick an interval [
 a, b
 ] that contains the root, that is,
  f
 (
 a
 )
 f
 (
 b
 )
  <
  0. From 
  
 the plot, it is clear that [0
 ,
  4] is a possible choice. In the next few steps, we will be working 
 with  
 a sequence of intervals. For convenience, let’s label them as [
 a, b
 ] = [
 a
 1
 , b
 1
 ]
 ,
  [
 a
 2
 , b
 2
 ]
 ,
  [
 a
 3
 , b
 3
 ]
 , 
  
 etc. Our first interval is then [
 a
 1
 , b
 1
 ] = [0
 ,
  4]. Next we find the midpoint of the interval, 
  
 p
 1
  = 4
 /
 2 = 2
 ,
  and use it to obtain two subintervals [0
 ,
  2] and [2
 ,
  4]. Only one of them  contains 
 the root, and that is [2
 ,
  4].",NA
Python code for the bisection method,"In Example
  26
 , we kept track of the intervals and midpoints obtained from the bisection method, by 
 labeling them as [
 a
 1
 , b
 1
 ]
 ,
  [
 a
 2
 , b
 2
 ]
 , ...,
  and
  p
 1
 , p
 2
 , ...
 . So at step
  n
  of the method, we know we are 
 working on the interval [
 a
 n
 , b
 n
 ] and its midpoint is
  p
 n
 . This approach will be useful when we study 
 the convergence of the method in the next theorem. However, keeping track of the intervals and 
 midpoints is not needed in the computer code. Instead, in the Python code below, we will let [
 a, b
 ] 
 be the current interval we are working on, and when we obtain a new interval in the following step, 
 we will simply call the new interval [
 a, b
 ], overwriting the old one. Similarly, we will call the 
 midpoint
  p
 , and update it at each step. 
 In [1]:
  import
  numpy
  as
  np
  
 In [2]:
  def
  bisection
 (f, a, b, eps, N):  
  
 n
  = 1 
  
  
 p
  = 0.
  # to ensure the value of p carries out of the while loop 
  
 while
  n
  <=
  N:  
  
 p
  =
  a
  +
  (b
 -
 a)
 /2 
  
  
 if
  np
 .
 isclose(f(p),
  0
 )
  or
  np
 .
 abs(a
 -
 b)
 <
 eps: 
 1
 Notice how we label the midpoints, as well as the endpoints of the interval, with the step number.",NA
2.3,NA,NA
Newton’s method,"Suppose
  f ⊂ C
 2
 [
 a, b
 ]
 ,
  i.e.,
  f, f
 ′
 , f
 ′′
 are continuous on [
 a, b
 ]
 .
  Let
  p
 0
  be a ""good"" approximation to
  p 
 such 
 that
  f
 ′
 (
 p
 0
 )
  ̸
 = 0 and
  |p − p
 0
 |
  is ""small"". First Taylor polynomial for
  f
  at
  p
 0
  with the remainder term is  
  
 f
 (
 x
 ) =
  f
 (
 p
 0
 ) + (
 x − p
 0
 )
 f
 ′
 (
 p
 0
 ) + (
 x − p
 0
 )
 2 
 f
 ′′
 (
 ξ
 (
 x
 )) 
 where
  ξ
 (
 x
 ) is a number between
  x
  and
  p
 0
 .
  Substitute
  x
  =
  p
  and note
  f
 (
 p
 ) = 0 to get: 
 0 =
  f
 (
 p
 0
 ) + (
 p − p
 0
 )
 f
 ′
 (
 p
 0
 ) + (
 p − p
 0
 )
 2 
 f
 ′′
 (
 ξ
 (
 p
 )) 
 where
  ξ
 (
 p
 ) is a number between
  p
  and
  p
 0
 .
  Rearrange the equation to get 
 p
  =
  p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 −
  (
 p − p
 0
 )
 2
  
 f
 ′′
 (
 ξ
 (
 p
 ))  
 f
 ′
 (
 p
 0
 )
 .
  
 (2.4) 
 If
  |p − p
 0
 |
  is ""small"" then (
 p − p
 0
 )
 2
 is even smaller, and the error term can be dropped to obtain the 
 following approximation: 
 p ≈ p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 .
  
 The idea in Newton’s method is to set the next iterate,
  p
 1
 , to this approximation: 
 p
 1
  =
  p
 0
  −f
 (
 p
 0
 ) 
 f
 ′
 (
 p
 0
 )
 .",NA
Python code for Newton’s method,"The Python code below is based on Equation (
 2.6
 ). The variable
  pin
  in the code corresponds to 
 p
 n−
 1
 , 
 and
  p
  corresponds to
  p
 n
 . The code overwrites these variables as the iteration continues. Also notice 
 that the code has two functions as inputs;
  f
  and
  fprime
  (the derivative
  f
 ′
 ). 
 In [1]:
  def
  newton
 (f, fprime, pin, eps, N):  
  
 n
  = 1 
  
  
 p
  = 0.
  # to ensure the value of p carries out of the while loop 
  
 while
  n
  <=
  N:  
  
 p
  =
  pin
  -
  f(pin)
 /
 fprime(pin)  
  
 if
  np
 .
 isclose(f(p),
  0
 )
  or
  np
 .
 abs(p
 -
 pin)
  <
  eps:  
  
  
  
 print
 (
 'p is '
 , p,
  ' and the iteration number is '
 , n)  
  
  
 return 
  
  
 pin
  =
  p  
  
 n
  += 1 
  
  
 y
  =
  f(p)  
  
 print
 (
 'Method did not converge. The last iteration gives '
 ,  
  
  
 p,
  ' with function value '
 , y) 
 Let’s apply Newton’s method to find the root of
  f
 (
 x
 ) =
  x
 5
 + 2
 x
 3
 −
  5
 x −
  2, a function we considered 
 before. First, we plot the function. 
 In [2]:
  import
  matplotlib.pyplot
  as
  plt 
  
 %
 matplotlib
  inline  
 import
  numpy
  as
  np
  
 In [3]:
  x
  =
  np
 .
 linspace(
 -2
 ,
  2
 ,
  1000
 )  
 y
  =
  x
 **5+2*
 x
 **3-5*
 x
 -2 
  
 ax
  =
  plt
 .
 gca()  
 ax
 .
 spines[
 'left'
 ]
 .
 set_position(
 'center'
 )",NA
2.4 ,NA,NA
Secant method,"One drawback of Newton’s method is that we need to know
  f
 ′
 (
 x
 ) explicitly to evaluate
  f
 ′
 (
 p
 n−
 1
 ) in 
 p
 n
  =
  p
 n−
 1
  −f
 (
 p
 n−
 1
 ) 
 f
 ′
 (
 p
 n−
 1
 )
 , n ≥
  1
 .
  
 If we do not know
  f
 ′
 (
 x
 ) explicitly, or if its computation is expensive, we might approximate
  f
 ′
 (
 p
 n−
 1
 ) 
 by the finite difference 
 f
 (
 p
 n−
 1
  +
  h
 )
  − f
 (
 p
 n−
 1
 )  
 h
  
 (2.11) 
 for some small
  h.
  We then need to compute two values of
  f
  at each iteration to approximate
  f
 ′
 . 
 Determining
  h
  in this formula brings some difficulty, but there is a way to get around this. We will 
 use the iterates themselves to rewrite the finite difference (
 2.11
 ) as 
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 )  
 p
 n−
 1
  − p
 n−
 2
  
 .
  
 Then, the recursion for
  p
 n
  simplifies as 
 p
 n
  =
  p
 n−
 1
  −
  
 f
 (
 p
 n−
 1
 )  
 f
 (
 p
 n−
 1
 )
 −f
 (
 p
 n−
 2
 ) 
 p
 n−
 1
 −p
 n−
 2
  
 =
  p
 n−
 1
  − f
 (
 p
 n−
 1
 ) 
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 )
 , n ≥
  2
 . p
 n−
 1
  − 
 p
 n−
 2
  
 (2.12) 
 This is called the secant method. Observe that 
 1. No additional function evaluations are needed, 
 2. The recursion requires two initial guesses
  p
 0
 , p
 1
 .
  
 Geometric interpretation
 : The slope of the secant line through the points (
 p
 n−
 1
 , f
 (
 p
 n−
 1
 )) and (
 p
 n−
 2
 , 
 f
 (
 p
 n−
 2
 )) is
 f
 (
 p
 n−
 1
 )
 −f
 (
 p
 n−
 2
 ) 
 p
 n−
 1
 −p
 n−
 2 
  
  
 . The
  x
 -intercept of the secant line, which is set to
  p
 n
 , is 
 0
  − f
 (
 p
 n−
 1
 ) 
 p
 n
  − p
 n−
 1
  
 =
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 ) 
 p
 n−
 1
  
 − p
 n−
 2
  
 ⊂ p
 n
  =
  p
 n−
 1
  − f
 (
 p
 n−
 1
 ) 
 p
 n−
 1
  − p
 n−
 2 
  
 f
 (
 p
 n−
 1
 )
  − f
 (
 p
 n−
 2
 ) 
 which is the recursion of the secant method. 
 The following theorem shows that if the initial guesses are ""good"", the secant method has 
 superlinear convergence. A proof can be found in Atkinson [
 3
 ]. 
 Theorem 35.
  Let f ⊂ C
 2
 [
 a, b
 ]
  and assume f
 (
 p
 ) = 0
 , f
 ′
 (
 p
 )
  ̸
 = 0
 , for p ⊂
  (
 a, b
 )
 . If the initial guesses p
 0
 , p
 1
  
 are sufficiently close to p, then the iterates of the secant method converge to p with
  
  
  
  
  
 lim 
 |p − p
 n
 +1
 | 
 = 
 f
 ′′
 (
 p
 ) 
 r
 1
  
  
  
  
  
 n→∞
  
 |p − p
 n
 |
 r
 0
  = 
 2
 f
 ′
 (
 p
 )  
 where
  
 √
 5+1
   
 √
 5
 −
 1
  
 ≈
  0
 .
 62
 .
  
  
  
  r
 0
  = 
 2
  
 ≈
  1
 .
 62
 , r
 1
  = 
 2",NA
Python code for the secant method,"The following code is based on Equation (
 2.12
 ); the recursion for the secant method. The initial 
 guesses are called
  pzero
  and
  pone
  in the code. The same stopping criterion as in Newton’s method is 
 used. Notice that once a new iterate
  p
  is computed,
  pone
  is updated as
  p
 , and
  pzero
  is updated as
  
 pone
 . 
 In [1]:
  def
  secant
 (f, pzero, pone, eps, N):  
  
 n
  = 1 
  
  
 p
  = 0.
  # to ensre the value of p carries out of the while loop 
  
 while
  n
  <=
  
 N:  
  
 p
  =
  pone
  -
  f(pone)
 *
 (pone
 -
 pzero)
  /
  (f(pone)
 -
 f(pzero))  
 if
  np
 .
 isclose(f(p),
  0
 )
  
 or
  np
 .
 abs(p
 -
 pone)
 <
 eps:  
  
  
  
 print
 (
 'p is '
 , p,
  ' and the iteration number is '
 , n)  
  
  
 return 
  
  
 pzero
  =
  pone  
  
 pone
  =
  p  
  
 n
  += 1 
  
  
 y
  =
  f(p)  
  
 print
 (
 'Method did not converge. The last iteration gives '
 ,  
  
  
 p,
  ' with function value '
 , y) 
 Let’s find the root of
  f
 (
 x
 ) = cos
  x − x
  using the secant method, using 0.5 and 1 as the initial guesses. 
 In [2]:
  secant(
 lambda
  x: np
 .
 cos(x)
 -
 x,
  0.5
 ,
  1
 ,
  1e-4
 ,
  20
 ) 
 p is 
 0.739085132900112 
 and the iteration number is 
 4 
 Exercise 2.4-1:
  
 Use the Python codes for the secant and Newton’s methods to find solutions 
 for the equation sin
  x − e
 −x
 = 0 on 0
  ≤ x ≤
  1. Set tolerance to 10
 −
 4
 , and take
  p
 0
  = 0 in Newton, and
  p
 0
  = 
 0
 , p
 1
  = 1 in secant method. Do a visual inspection of the estimates and comment on the convergence 
 rates of the methods. 
 Exercise 2.4-2:
  
 a) The function
  y
  = log
  x
  has a root at
  x
  = 1. Run the Python code for Newton’s method with 
 p
 0
  = 
 2
 , ϵ
  = 10
 −
 4
 , N
  = 20, and then try
  p
 0
  = 3. Does Newton’s method find the root in each case? If 
 Python gives an error message, explain what the error is.",NA
2.5,NA,NA
Muller’s method,"The secant method uses a linear function that passes through (
 p
 0
 , f
 (
 p
 0
 )) and (
 p
 1
 , f
 (
 p
 1
 )) to find the 
 next iterate
  p
 2
 .
  Muller’s method takes three initial approximations, passes a parabola (quadratic 
 polynomial) through (
 p
 0
 , f
 (
 p
 0
 ))
 ,
  (
 p
 1
 , f
 (
 p
 1
 )), (
 p
 2
 , f
 (
 p
 2
 )), and uses
  one
  of the roots of the polynomial as 
 the next iterate. 
 Let the quadratic polynomial written in the following form 
 P
 (
 x
 ) =
  a
 (
 x − p
 2
 )
 2
 +
  b
 (
 x − p
 2
 ) +
  c. 
 (2.13)",NA
Python code for Muller’s method,"The following Python code takes initial guesses
  p
 0
 , p
 1
 , p
 2
  (written as
  pzero, pone, ptwo
  in the code), 
 computes the coefficients
  a, b, c
  from Equation (
 2.14
 ), and sets the root
  p
 3
  to
  p
 . It then updates the 
 three initial guesses as the last three iterates, and continues until the stopping criterion is satisfied. 
 We need to compute the square root, and the absolute value, of possibly complex numbers in 
 Equations (
 2.15
 ) and (
 2.16
 ). The Python function for the square root of a possibly complex number 
 z
  is
  complex(z)
 0
 .
 5
 , and its absolute value is
  np.abs(z)
 . 
 In [1]:
  def
  muller
 (f, pzero, pone, ptwo, eps, N):  
  
 n
  = 1 
  
  
 p
  = 0 
  
  
 while
  n
  <=
  N:  
  
 c
  =
  f(ptwo)  
  
 b1
  =
  (pzero
 -
 ptwo)
  *
  (f(pone)
 -
 f(ptwo))
  /
  ((pone
 -
 ptwo)
 *
 (pzero
 -
 pone))  b2
  =
  (pone
 -
 ptwo)
  *
  (f(pzero)
 -
 f(ptwo))
  /
  ((pzero
 -
 ptwo)
 *
 (pzero
 -
 pone))  
 b
  =
  b1
  -
  b2  
  
 a1
  =
  (f(pzero)
 -
 f(ptwo))
  /
  ((pzero
 -
 ptwo)
 *
 (pzero
 -
 pone))  
  
 a2
  =
  (f(pone)
 -
 f(ptwo))
  /
  ((pone
 -
 ptwo)
 *
 (pzero
 -
 pone))  
  
 a
  =
  a1
  -
  a2  
  
 d
  =
  (
 complex
 (b
 **2-4*
 a
 *
 c))
 **0.5 
  
  
 if
  np
 .
 abs(b
 -
 d)
  <
  np
 .
 abs(b
 +
 d):  
  
  
 inc
  = 2*
 c
 /
 (b
 +
 d)  
  
 else
 :  
  
  
 inc
  = 2*
 c
 /
 (b
 -
 d)  
  
 p
  =
  ptwo
  -
  inc  
  
 if
  np
 .
 isclose(f(p),
  0
 )
  or
  np
 .
 abs(p
 -
 ptwo)
 <
 eps:  
  
  
 print
 (
 'p is '
 , p,
  ' and the iteration number is '
 , n)  
  
  
 return",NA
2.6,NA,NA
Fixed-point iteration,"Many root-finding methods are based on the so-called fixed-point iteration, a method we discuss in 
 this section. 
 Definition 37.
  A number
  p
  is a fixed-point for a function
  g
 (
 x
 ) if
  g
 (
 p
 ) =
  p.
  
 We have two problems that are related to each other: 
 •
  Fixed-point problem
 : Find
  p
  such that
  g
 (
 p
 ) =
  p.
  
 •
  Root-finding problem
 : Find
  p
  such that
  f
 (
 p
 ) = 0
 .",NA
Python code for fixed-point iteration,"The following code starts with the initial guess
  p
 0
  (
 pzero
  in the code), computes
  p
 1
  =
  g
 (
 p
 0
 ), and 
 checks if the stopping criterion
  |p
 1
  − p
 0
 | < ϵ
  is satisfied. If it is satisfied the code terminates with the 
 value
  p
 1
 . Otherwise
  p
 1
  is set to
  p
 0
 , and the next iteration is computed. 
 In [1]:
  def
  fixedpt
 (g, pzero, eps, N):  
  
 n
  = 1 
  
  
 while
  n
 <
 N:  
  
 pone
  =
  g(pzero)  
  
 if
  np
 .
 abs(pone
 -
 pzero)
 <
 eps:  
  
  
 print
 (
 'p is '
 , pone,
  ' and the iteration number is '
 , n)  
  
 return 
  
  
 pzero
  =
  pone  
  
 n
  += 1 
  
  
 print
 (
 'Did not converge. The last estimate is p = '
 , pzero) 
 Let’s find the fixed-point of
  g
 (
 x
 ) =
  x
  where
  g
 (
 x
 ) = (2
 x
 2
 + 1)
 1
 /
 3
 , with
  p
 0
  = 1. We studied this 
 problem in Example
  43
  where we found that 23 iterations guarantee an estimate accurate to within 
 10
 −
 4
 . We set
  ϵ
  = 10
 −
 4
 , and
  N
  = 30, in the above code. 
 In [2]:
  fixedpt(
 lambda
  x: (
 2*
 x
 **2+1
 )
 **
 (
 1/3
 ),
  1
 ,
  1e-4
 ,
  30
 ) 
 p is 
 2.205472095330031 
 and the iteration number is 
 19 
 The exact value of the fixed-point, equivalently the root of
  x
 3
 −
  2
 x
 2
 −
  1, is 2.20556943. Then the exact 
 error is: 
 In [3]:
  2.205472095330031-2.20556943
  
 Out[3]:
  -9.733466996930673e-05 
 A take home message and a word of caution: 
 • The exact error,
  |p
 n
  −p|
 , is guaranteed to be less than 10
 −
 4
 after 23 iterations from Corollary 
 42
 , 
 but as we observed in this example, this could happen before 23 iterations. 
 • The stopping criterion used in the code is based on
  |p
 n
  − p
 n−
 1
 |
 , not
  |p
 n
  − p|
 , so the iteration 
 number that makes these quantities less than a tolerance
  ϵ
  will not be the same in general. 
 Theorem 44.
  Assume p is a solution of g
 (
 x
 ) =
  x, and suppose g
 (
 x
 )
  is continuously differentiable in 
 some interval about p with |g
 ′
 (
 p
 )
 | <
  1
 . Then the fixed-point iteration converges to p, provided p
 0 
 is 
 chosen sufficiently close to p. Moreover, the convergence is linear if g
 ′
 (
 p
 )
  ̸
 = 0
 .",NA
2.7 ,NA,NA
High-order fixed-point iteration,"In the proof of Theorem
  44
 , we showed 
 lim  
 n→∞
  
 |p
 n
 +1
  − p| 
 |p
 n
  − p|
  
 =
  |g
 ′
 (
 p
 )
 |
  
 which implied that the fixed-point iteration has linear convergence, if
  g
 ′
 (
 p
 )
  ̸
 = 0. 
 If this limit were zero, then we would have 
 lim  
 n→∞
  
 |p
 n
 +1
  − p| 
 |p
 n
  − p|
  
 = 0
 ,
  
 which means the denominator is growing at a larger rate than the numerator. We could then ask if 
 n→∞
 lim 
 |p
 n
 +1
  − p| |p
 n
  − p|
 α
  = nonzero constant 
 for some
  α >
  1
 .",NA
Application to Newton’s Method,"Recall Newton’s iteration 
 p
 n
  =
  p
 n−
 1
  −f
 (
 p
 n−
 1
 ) 
 f
 ′
 (
 p
 n−
 1
 )
 .
  
 Put
  g
 (
 x
 ) =
  x −
 f
 (
 x
 ) 
 f
 ′
 (
 x
 )
 .
  Then the fixed-point iteration
  p
 n
  =
  g
 (
 p
 n−
 1
 ) is Newton’s method. We have 
 g
 ′
 (
 x
 ) = 1
  −
 [
 f
 ′
 (
 x
 )]
 2
  − f
 (
 x
 )
 f
 ′′
 (
 x
 ) [
 f
 ′
 (
 x
 )]
 2
  
 =
 f
 (
 x
 )
 f
 ′′
 (
 x
 )  
  
 [
 f
 ′
 (
 x
 )]
 2
  
 and thus 
 g
 ′
 (
 p
 ) =
 f
 (
 p
 )
 f
 ′′
 (
 p
 )  
 [
 f
 ′
 (
 p
 )]
 2
  
 = 0
 .
  
 Similarly, 
 g
 ′′
 (
 x
 ) = (
 f
 ′
 (
 x
 )
 f
 ′′
 (
 x
 ) +
  f
 (
 x
 )
 f
 ′′′
 (
 x
 )) (
 f
 ′
 (
 x
 ))
 2
  − f
 (
 x
 )
 f
 ′′
 (
 x
 )2
 f
 ′
 (
 x
 )
 f
 ′′
 (
 x
 )  
 [
 f
 ′
 (
 x
 )]
 4",NA
Chapter 3,NA,NA
Interpolation,"In this chapter, we will study the following problem: given data (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
 , find a function
  f
  
 such that
  f
 (
 x
 i
 ) =
  y
 i
 .
  This problem is called the interpolation problem, and
  f
  is called the interpolating 
 function, or interpolant, for the given data. 
 Interpolation is used, for example, when we use mathematical software to plot a smooth curve 
 through discrete data points, when we want to find the in-between values in a table, or when we 
 differentiate or integrate black-box type functions. 
 How do we choose
  f
 ? Or, what kind of function do we want
  f
  to be? There are several options. 
 Examples of functions used in interpolation are polynomials, piecewise polynomials, rational func-
 tions, trigonometric functions, and exponential functions. As we try to find a good choice for
  f
  for 
 our data, some questions to consider are whether we want
  f
  to inherit the properties of the data 
 (for example, if the data is periodic, should we use a trigonometric function as
  f
 ?), and how we want
  
 f
  behave between data points. In general
  f
  should be easy to evaluate, and easy to integrate & 
 differentiate. 
 Here is a general framework for the interpolation problem. We are given data, and we pick a family 
 of functions from which the interpolant
  f
  will be chosen: 
 • Data: (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
  
 • Family: Polynomials, trigonometric functions, etc. 
 Suppose the family of functions selected forms a vector space. Pick a basis for the vector 
 space:
 φ
 0
 (
 x
 )
 , φ
 1
 (
 x
 )
 , ..., φ
 n
 (
 x
 )
 .
  Then the interpolating function can be written as a linear combination 
 of 
 the basis vectors (functions): 
 f
 (
 x
 ) = 
 n
  
 a
 k
 φ
 k
 (
 x
 )
 .
  
 k
 =0
  
 We want
  f
  to pass through the data points, that is,
  f
 (
 x
 i
 ) =
  y
 i
 . Then determine
  a
 k
  so that: 
  
  
 n 
  
 f
 (
 x
 i
 ) =    
 a
 k
 φ
 k
 (
 x
 i
 ) =
  y
 i
 , i
  = 0
 ,
  1
 , ..., n, 
  
  
 k
 =0
  
 75",NA
3.1,NA,NA
Polynomial interpolation,"In polynomial interpolation, we pick polynomials as the family of functions in the interpolation 
 problem. 
 • Data: (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
  
 • Family: Polynomials 
 The space of polynomials up to degree
  n
  is a vector space. We will consider three choices for the 
 basis for this vector space: 
 • Basis: 
 –
  Monomial basis:
  φ
 k
 (
 x
 ) =
  x
 k
  
 –
  Lagrange basis:
  φ
 k
 (
 x
 ) =
 n j
 =0
 ,j̸
 =
 k
  
  x−x
 j 
  
 x
 k
 −x
 j
  
 –
  Newton basis:
  φ
 k
 (
 x
 ) =
 k−
 1 
 j
 =0
 (
 x − x
 j
 ) 
 where
  k
  = 0
 ,
  1
 , ..., n
 . 
 Once we decide on the basis, the interpolating polynomial can be written as a linear combination 
 of the basis functions: 
 p
 n
 (
 x
 ) = 
 n
  
 a
 k
 φ
 k
 (
 x
 ) 
 k
 =0
  
 where
  p
 n
 (
 x
 i
 ) =
  y
 i
 , i
  = 0
 ,
  1
 , ..., n.
  
 Here is an important question. How do we know that
  p
 n
 , a polynomial of degree at most
  n 
 passing through the data points, actually exists? Or, equivalently, how do we know the system of 
 equations
  p
 n
 (
 x
 i
 ) =
  y
 i
 , i
  = 0
 ,
  1
 , ..., n
 , has a solution? 
 The answer is given by the following theorem, which we will prove later in this section. 
 Theorem 47.
  If points x
 0
 , x
 1
 , ..., x
 n
  are distinct, then for real values y
 0
 , y
 1
 , ..., y
 n
 , there is a unique 
 polynomial p
 n
  of degree at most n such that p
 n
 (
 x
 i
 ) =
  y
 i
 , i
  = 0
 ,
  1
 , ..., n.",NA
Monomial form of polynomial interpolation,"Given data (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
 , we know from the previous theorem that there exists a polynomial 
 p
 n
 (
 x
 ) of degree at most
  n
 , that passes through the data points. To represent
  p
 n
 (
 x
 ), we will use the 
 monomial basis functions, 1
 , x, x
 2
 , ..., x
 n
 , or written more succinctly, 
 φ
 k
 (
 x
 ) =
  x
 k
 , k
  = 0
 ,
  1
 , ..., n.
  
 The interpolating polynomial
  p
 n
 (
 x
 ) can be written as a linear combination of these basis functions 
 as  
  
 p
 n
 (
 x
 ) =
  a
 0
  +
  a
 1
 x
  +
  a
 2
 x
 2
 +
  ...
  +
  a
 n
 x
 n
 .
  
 We will determine
  a
 i
  using the fact that
  p
 n
  is an interpolant for the data: 
 p
 n
 (
 x
 i
 ) =
  a
 0
  +
  a
 1
 x
 i
  +
  a
 2
 x
 2 
 i
 +
  ...
  +
  a
 n
 x
 n i
 =
  y
 i
  
 for
  i
  = 0
 ,
  1
 , ..., n.
  Or, in matrix form, we want to solve 
 1 
 1 
 1 ... 
 x
 0
  
 x
 2 0
  
 ...
  
 x
 n 
 0
  
  
 a
 0
  
 a
 1
  
 a
 n 
 ... 
  
 = 
 y
 0
  
 y
 1
  
 y
 n 
 ... 
  
  
 x
 1
  
 x
 2 1
  
 x
 n 
 1
  
 x
 n
  
 x
 2 
 n
  
 x
 n n
  
  
  
 A
   
  
  
 a
    
 y
   
 for [
 a
 0
 , ..., a
 n
 ]
 T
 where [
 ·
 ]
 T
 stands for the transpose of the vector. The coefficient matrix
  A
  is known as 
 the van der Monde matrix. This is usually an ill-conditioned matrix, which means solving the system 
 of equations could result in large error in the coefficients
  a
 i
 .
  An intuitive way to understand the ill-
 conditioning is to plot several basis monomials, and note how less distinguishable they are as the 
 degree increases, making the columns of the matrix nearly linearly dependent.",NA
Lagrange form of polynomial interpolation,"The ill-conditioning of the van der Monde matrix, as well as the high complexity of solving the 
 resulting matrix equation in the monomial form of polynomial interpolation, motivate us to explore 
 other basis functions for polynomials. As before, we start with data (
 x
 i
 , y
 i
 )
 , i
  = 0
 ,
  1
 , ..., n
 , and call our 
 interpolating polynomial of degree at most
  n
 ,
  p
 n
 (
 x
 ). The Lagrange basis functions up to degree 
 n
  
 (also called cardinal polynomials) are defined as 
 l
 (
 x
 ) = 
 n
  
  x − x
 j
  
 , k
  = 0
 ,
  1
 , ..., n.
  
 k
  
 j
 =0
 ,j̸
 =
 k
  
 x
 k
  − x
 j
  
  
 We write the interpolating polynomial
  p
 n
 (
 x
 ) as a linear combination of these basis functions as 
 p
 n
 (
 x
 ) =
  a
 0
 l
 0
 (
 x
 ) +
  a
 1
 l
 1
 (
 x
 ) +
  ...
  +
  a
 n
 l
 n
 (
 x
 )
 .
  
 1
 The formal definition of the big O notation is as follows: We write
  f
 (
 n
 ) =
  O
 (
 g
 (
 n
 )) as
  n → ∞
  if and only if 
 there exists a positive constant
  M
  and a positive integer
  n
 ⊂
 such that
  |f
 (
 n
 )
 | ≤ Mg
 (
 n
 ) for all
  n ≥ n
 ⊂
 .",NA
Newton’s form of polynomial interpolation,"The Newton basis functions up to degree
  n
  are 
 k−
 1
  
 π
 k
 (
 x
 ) =  (
 x − x
 j
 )
 , k
  = 0
 ,
  1
 , ..., n 
  
 j
 =0
  
 where
  π
 0
 (
 x
 ) =
 −
 1 
 j
 =0
 (
 x − x
 j
 ) is interpreted as 1. The interpolating polynomial
  p
 n
 , written as a linear 
 combination of Newton basis functions, is 
 p
 n
 (
 x
 ) =
  a
 0
 π
 0
 (
 x
 ) +
  a
 1
 π
 1
 (
 x
 ) +
  ...
  +
  a
 n
 π
 n
 (
 x
 ) 
 =
  a
 0
  +
  a
 1
 (
 x − x
 0
 ) +
  a
 2
 (
 x − x
 0
 )(
 x − x
 1
 ) +
  ...
  +
  a
 n
 (
 x − x
 0
 )
  · · ·
  (
 x − x
 n−
 1
 )
 .
  
 We will determine
  a
 i
  from 
 p
 n
 (
 x
 i
 ) =
  a
 0
  +
  a
 1
 (
 x
 i
  − x
 0
 ) +
  ...
  +
  a
 n
 (
 x
 i
  − x
 0
 )
  · · ·
  (
 x
 i
  − x
 n−
 1
 ) =
  y
 i
 ,
  
 for
  i
  = 0
 ,
  1
 , ..., n,
  or in matrix form 
 1 
 1 
 1 
   ...  
 1 
 0 
 0 
 . . .
  
 0 0 
  
 n−
 1 
 i
 =0
 (
 x
 n
  − x
 i
 ) 
  
  ...
  
  0 
 a
 0
  
 a
 1
  
 a
 n
  
 a 
  
 ... 
  
 = 
 y
 0
  
 y
 1
  
 y
 n
  
 y 
  
 ... 
  
  
 (
 x
 1
  − x
 0
 ) (
 x
 2
  
 − x
 0
 ) ... 
 0 
 . . .
  
 (
 x
 2
  − x
 0
 )(
 x
 2
  − x
 1
 ) ... 
 (
 x
 n
  − x
 0
 ) 
 (
 x
 n
  − x
 0
 )(
 x
 n
  − x
 1
 ) 
 A
  
 for [
 a
 0
 , ..., a
 n
 ]
 T
 .
  Note that the coefficient matrix
  A
  is lower-triangular, and
  a
  can be solved by 
 forward substitution, which is shown in the next example, in
  O
 (
 n
 2
 ) operations. 
 Example
  
 49.
  Find 
 the interpolating polynomial using Newton’s basis for the data: 
 (
 −
 1
 , −
 6)
 ,
  (1
 ,
  0)
 ,
  (2
 ,
  6)
 .
  
 Solution.
  We have
  p
 2
 (
 x
 ) =
  a
 0
  +
  a
 1
 π
 1
 (
 x
 ) +
  a
 2
 π
 2
 (
 x
 ) =
  a
 0
  +
  a
 1
 (
 x
  + 1) +
  a
 2
 (
 x
  + 1)(
 x −
  1)
 .
  Find 
 a
 0
 , a
 1
 , a
 2
  
 from 
 p
 2
 (
 −
 1) =
  −
 6
  ⊂ a
 0
  +
  a
 1
 (
 −
 1 + 1) +
  a
 2
 (
 −
 1 + 1)(
 −
 1
  −
  1) =
  a
 0
  =
  −
 6 
 p
 2
 (1) = 0
  ⊂ a
 0
  +
  a
 1
 (1 + 1) +
  a
 2
 (1 + 1)(1
  −
  1) =
  a
 0
  + 2
 a
 1
  = 0 
 p
 2
 (2) = 6
  ⊂ a
 0
  +
  a
 1
 (2 + 1) +
  a
 2
 (2 + 1)(2
  −
  1) =
  a
 0
  + 3
 a
 1
  + 3
 a
 2
  = 6",NA
Python code for Newton interpolation,"Consider the following finite difference table. 
 There are 2 + 1 = 3 divided differences in the table, not counting the 0th divided differences. In 
 general, the number of divided differences to compute is 1 +
  ...
  +
  n
  =
  n
 (
 n
  + 1)
 /
 2. However, to 
 construct Newton’s form of the interpolating polynomial, we need only
  n
  divided differences and 
 the 0th divided difference
  y
 0
 . These numbers are displayed in red in Table
  3.1
 . The important",NA
3.2,NA,NA
High degree polynomial interpolation,"Suppose we approximate
  f
 (
 x
 ) using its polynomial interpolant
  p
 n
 (
 x
 ) obtained from (
 n
  + 1) data 
 points. We then increase the number of data points, and update
  p
 n
 (
 x
 ) accordingly. The central 
 question we want to discuss is the following: as the number of nodes (data points) increases, does 
 p
 n
 (
 x
 ) become a better approximation to
  f
 (
 x
 ) on [
 a, b
 ]? We will investigate this question numerically, 
 using a famous example: Runge’s function, given by
  f
 (
 x
 ) = 
 1+
 x
 2
  . 
 We will interpolate Runge’s function using polynomials of various degrees, and plot the func- 
  
 We are interested to see tion, together with its interpolating polynomial and the data points. 
 what happens as the number of data points, and hence the degree of the interpolating polynomial, 
 increases. 
 In [8]:
  import
  matplotlib.pyplot
  as
  plt 
  
 %
 matplotlib
  inline 
 We start with taking four equally spaced
  x
 -coordinates between -5 and 5, and plot the corre-
 sponding interpolating polynomial and Runge’s function. Matplotlib allows typing mathematics in 
 captions of a plot using Latex. (Latex is a typesetting program this book is written with.) Latex 
 commands need to be enclosed by a pair of dollar signs, in addition to a pair of quotation marks.",NA
Divided differences and derivatives,"The following theorem shows the similarity between divided differences and derivatives. 
 Theorem 58.
  Suppose f ⊂ C
 n
 [
 a, b
 ]
  and x
 0
 , x
 1
 , ..., x
 n
  are distinct numbers in
  [
 a, b
 ]
 . Then there exists ξ ⊂
  
 (
 a, b
 )
  such that 
  
  
 f
 [
 x
 0
 , ..., x
 n
 ] =
 f
 (
 n
 )
 (
 ξ
 ) 
 .
  
 n
 ! 
 To prove this theorem, we need the generalized Rolle’s theorem. 
 Theorem 59
  (Rolle’s theorem)
 .
  Suppose f is a differentiable function on
  (
 a, b
 )
 . If f
 (
 a
 ) =
  f
 (
 b
 )
 , then 
 there exists c ⊂
  (
 a, b
 )
  such that f
 ′
 (
 c
 ) = 0
 .
  
 Theorem 60
  (Generalized Rolle’s theorem)
 .
  Suppose f has n derivatives on
  (
 a, b
 )
 . If f
 (
 x
 ) = 0
  at 
 (
 n
  + 1)
  
 distinct numbers x
 0
 , x
 1
 , ..., x
 n
  ⊂
  [
 a, b
 ]
 , then there exists c ⊂
  (
 a, b
 )
  such that f
 (
 n
 )
 (
 c
 ) = 0
 .
  
 Proof of Theorem
  58
  .
  Consider the function
  g
 (
 x
 ) =
  p
 n
 (
 x
 )
  − f
 (
 x
 )
 .
  Observe that
  g
 (
 x
 i
 ) = 0 for 
 i
  = 0
 ,
  1
 , ..., n.
  
 From generalized Rolle’s theorem, there exists
  ξ ⊂
  (
 a, b
 ) such that
  g
 (
 n
 )
 (
 ξ
 ) = 0
 ,
  which implies  
  
 p
 (
 n
 ) 
 n
 (
 ξ
 )
  − f
 (
 n
 )
 (
 ξ
 ) = 0
 .
  
 Since
  p
 n
 (
 x
 ) =
  f
 [
 x
 0
 ] +
  f
 [
 x
 0
 , x
 1
 ](
 x − x
 0
 ) +
  ...
  +
  f
 [
 x
 0
 , ..., x
 n
 ](
 x − x
 0
 )
  · · ·
  (
 x − x
 n−
 1
 ),
  p
 (
 n
 ) 
 n
 (
 x
 ) equals
  n
 ! 
 times the leading coefficient
  f
 [
 x
 0
 , ..., x
 n
 ]
 .
  Therefore 
 f
 (
 n
 )
 (
 ξ
 ) =
  n
 !
 f
 [
 x
 0
 , ..., x
 n
 ]
 .",NA
3.3,NA,NA
Hermite interpolation,"In polynomial interpolation, our starting point has been the
  x
 - and
  y
 -coordinates of some data we 
 want to interpolate. Suppose, in addition, we know the derivative of the underlying function at 
 these
  x
 -coordinates. Our new data set has the following form. 
 Data:
  
 x
 0
 , x
 1
 , ..., x
 n
  
 y
 0
 , y
 1
 , ..., y
 n
 ;
  y
 i
  =
  f
 (
 x
 i
 )",NA
Computing the Hermite polynomial,"We do not use Theorem
  61
  to compute the Hermite polynomial: there is a more efficient method 
 using divided differences for this computation. 
 We start with the data: 
 x
 0
 , x
 1
 , ..., x
 n
  
 y
 0
 , y
 1
 , ..., y
 n
 ;
  y
 i
  =
  f
 (
 x
 i
 ) 
 y
 ′
 0
 , y
 ′
 1
 , ..., y
 ′n
 ;
  y
 ′i
 =
  f
 ′
 (
 x
 i
 ) 
 and define a sequence
  z
 0
 , z
 1
 , ..., z
 2
 n
 +1
  by 
 z
 0
  =
  x
 0
 , z
 2
  =
  x
 1
 , z
 4
  =
  x
 2
 , ..., z
 2
 n
  =
  x
 n
  
 z
 1
  =
  x
 0
 , z
 3
  =
  x
 1
 , z
 5
  =
  x
 2
 , ..., z
 2
 n
 +1
  =
  x
 n
  
 i.e.,
  z
 2
 i
  =
  z
 2
 i
 +1
  =
  x
 i
 ,
  for
  i
  = 0
 ,
  1
 , ..., n.",NA
Python code for computing Hermite interpolating polynomial,"In [1]:
  import
  numpy
  as
  np 
  
 import
  matplotlib.pyplot
  as
  plt 
  
 %
 matplotlib
  inline 
 The following function
  hdiff
  computes the divided differences needed for Hermite 
 interpolation. It is based on the function
  diff
  for computing divided differences for Newton 
 interpolation. The inputs to
  hdiff
  are the
  x
 -coordinates, the
  y
 -coordinates, and the derivatives
  
 yprime
 . 
 In [2]:
  def
  hdiff
 (x, y, yprime):  
  
 m
  =
  x
 .
 size
  # here m is the number of data points. Note n=m-1 
  
 # and 
 2n+1=2m-1 
  
  
 l
  = 2*
 m  
  
 z
  =
  np
 .
 zeros(l)  
  
 a
  =
  np
 .
 zeros(l)  
  
 for
  i
  in
  range
 (m):  
  
 z[
 2*
 i]
  =
  x[i]  
  
 z[
 2*
 i
 +1
 ]
  =
  x[i]  
  
 for
  i
  in
  range
 (m):  
  
 a[
 2*
 i]
  =
  y[i]",NA
3.4 ,NA,NA
Piecewise polynomials: spline interpolation,"As we observed in Section
  3.2
 , a polynomial interpolant of high degree can have large oscillations, 
 and thus provide an overall poor approximation to the underlying function. Recall that the degree 
 of the interpolating polynomial is directly linked to the number of data points: we do not have the 
 freedom to choose the degree of the polynomial. 
 In spline interpolation, we take a very different approach: instead of finding a single polynomial 
 that fits the given data, we find one low-degree polynomial that fits every
  pair
  of data. This results 
 in several polynomial pieces joined together, and we typically impose some smoothness conditions 
 on different pieces. The term
  spline function
  means a function that consists of polynomial pieces 
 joined together with some smoothness conditions. 
 In
  linear spline interpolation
 , we simply join data points (the nodes), by line segments, that is, 
 linear polynomials.  
 For example, consider the following figure that plots three data 
 points (
 x
 i−
 1
 , y
 i−
 1
 )
 ,
  (
 x
 i
 , y
 i
 )
 ,
  (
 x
 i
 +1
 , y
 i
 +1
 ). We fit a linear polynomial
  P
 (
 x
 ) to the first pair of data points 
 (
 x
 i−
 1
 , y
 i−
 1
 )
 ,
  (
 x
 i
 , y
 i
 ), and another linear polynomial
  Q
 (
 x
 ) to the second pair of data points (
 x
 i
 , y
 i
 )
 ,
  (
 x
 i
 +1
 , 
 y
 i
 +1
 ). 
 y
 i 
  
  
 Q(x) 
  
  
 P(x) 
  
 y
 i+1
  
 y
 i-1
  
 x
 i-1
  
  x
 i
  
 x
 i+1
  
 Figure 3.4: Linear spline
  
 Let
  P
 (
 x
 ) =
  ax
  +
  b
  and
  Q
 (
 x
 ) =
  cx
  +
  d
 . We find the coefficients
  a, b, c, d
  by solving 
 P
 (
 x
 i−
 1
 ) =
  y
 i−
 1 
  
 P
 (
 x
 i
 ) =
  y
 i
  
 Q
 (
 x
 i
 ) =
  y
 i
  
 Q
 (
 x
 i
 +1
 ) =
  y
 i
 +1
  
 which is a system of four equations and four unknowns. We then repeat this procedure for all data 
 points, (
 x
 0
 , y
 0
 )
 ,
  (
 x
 1
 , y
 1
 )
 , ...,
  (
 x
 n
 , y
 n
 ), to determine all of the linear polynomials.",NA
Cubic spline interpolation,"This is the most common spline interpolation. It uses cubic polynomials to connect the nodes. 
 Consider the data 
 (
 x
 0
 , y
 0
 )
 ,
  (
 x
 1
 , y
 1
 )
 , ...,
  (
 x
 n
 , y
 n
 )
 ,
  
 where
  x
 0
  < x
 1
  < ... < x
 n
 . In the figure below, the cubic polynomials interpolating pairs of data are 
 labeled as
  S
 0
 , ..., S
 n−
 1
  (we ignore the
  y
 -coordinates in the plot).",NA
Python code for spline interpolation,"In [1]:
  import
  numpy
  as
  np 
  
 import
  matplotlib.pyplot
  as
  plt 
  
 %
 matplotlib
  inline 
 The function
  CubicNatural
  takes the
  x
 - and
  y
 -coordinates of the data as input, and computes the 
 natural cubic spline interpolating the data, by solving the resulting matrix equation. The code is 
 based on Algorithm 3.4 of Burden, Faires, Burden [
 4
 ].  
 The output is the coefficients of the 
 m 
 −
  1 cubic polynomials,
  a
 i
 , b
 i
 , c
 i
 , d
 i
 , i
  = 0
 , ..., m −
  2 where
  m
  is the number of data points. These 
 coefficients are stored in the arrays
  a, b, c, d
  and returned at the end of the function, so that we can 
 access these arrays later to evaluate the spline for a given value
  w
 . 
 In [2]:
  def
  CubicNatural
 (x, y):  
 m
  =
  x
 .
 size
  # m is the number of data points 
 n
  =
  m
 -1",NA
Chapter 4,NA,NA
Numerical Quadrature and,NA,NA
Differentiation,"Estimating 
  b 
  
 a
 f
 (
 x
 )
 dx
  using sums of the form
 n i
 =0
 w
 i
 f
 (
 x
 i
 ) is known as the quadrature problem. 
 Here
  w
 i
  are called weights, and
  x
 i
  are called nodes. The objective is to determine the nodes and 
 weights to minimize error.",NA
4.1 ,NA,NA
Newton-Cotes formulas,"b 
  
 The idea is to construct the polynomial interpolant
  P
 (
 x
 ) and compute  
 a
 P
 (
 x
 )
 dx
  as an approxima- 
  
  b 
  
 tion to  
 a
 f
 (
 x
 )
 dx
 . Given nodes
  x
 0
 , x
 1
 , ..., x
 n
 , the Lagrange form of the interpolant is 
 n
  
 P
 n
 (
 x
 ) =  
 f
 (
 x
 i
 )
 l
 i
 (
 x
 ) 
 i
 =0
  
 and from the interpolation error formula Theorem
  51
 , we have 
 f
 (
 x
 ) =
  P
 n
 (
 x
 ) + (
 x − x
 0
 )
  · · ·
  (
 x − x
 n
 )
 f
 (
 n
 +1)
 (
 ξ
 (
 x
 )) 
 ,
  
 where
  ξ
 (
 x
 )
  ⊂
  [
 a, b
 ]. (We have written
  ξ
 (
 x
 ) instead of
  ξ
  to emphasize that
  ξ
  depends on the value of 
 x
 .) 
 Taking the integral of both sides yields 
  b
  
 f
 )
 x
  = 
  b
  
 + 
 1 
  b
  
 n
  
 x 
 (
 n
 +1)
 (()
 d.
  
 (4.1) 
 P
 ()
 x
  
 a
  
 f
 )
 x
  = 
 P
 n
 ()
 x
  + (
 n
  + 1)! 
 a
  
 i
 =0
  
 x − 
 i
 (()
 d.
  
  
 a
  
  
  
  
  
  
  
  
 error term
  
  
 quadrature rule
  
 121",NA
4.2 ,NA,NA
Composite Newton-Cotes formulas,"If the interval [
 a, b
 ] in the quadrature is large, then the Newton-Cotes formulas will give poor 
 approximations. The quadrature error depends on
  h
  = (
 b − a
 )
 /n
  (closed formulas), and if
  b − a
  is 
 large, then so is
  h,
  hence error. If we raise
  n
  to compensate for large interval, then we face a 
 problem discussed earlier: error due to the oscillatory behavior of high-degree interpolating 
 polynomials that use equally-spaced nodes. A solution is to break up the domain into smaller 
 intervals and use a Newton-Cotes rule with a smaller
  n
  on each subinterval: this is known as a 
 composite rule. 
 Example 74.
  Let’s compute 
  2 
  
 0
 e
 x
  sin
  xdx.
  The antiderivative can be computed using integration 
 by parts, and the true value of the integral to 6 digits is 5.39689. If we apply the Simpson’s rule we 
 get: 
  2
  
 e
 x
 sin
  xdx ≈
 1 3(
 e
 0
  sin 0 + 4
 e
  sin 1 +
  e
 2
  sin 2) = 5
 .
 28942
 .
  
 0
  
 If we partition the integration domain (0
 ,
  2) into (0
 ,
  1) and (1
 ,
  2), and apply Simpson’s rule to each 
 domain separately, we get 
  2 
  
 0
  
  1
  
  2
  
 e
 x
 sin
  xdx
  = 
 e
 x
 sin
  xdx
  + 
 e
 x
 sin
  xdx
  
 0
  
 1
  
 ≈
 1 6(
 e
 0
  sin 0 + 4
 e
 0
 .
 5
  sin(0
 .
 5) +
  e
  sin 1) + 1 6(
 e
  sin 1 + 4
 e
 1
 .
 5
  sin(1
 .
 5) +
  e
 2
  sin 2) 
 = 5
 .
 38953
 ,
  
 improving the accuracy significantly. Note that we have used five nodes, 0
 ,
  0
 .
 5
 ,
  1
 ,
  1
 .
 5
 ,
  2, which split 
 the domain (0
 ,
  2) into four subintervals. 
 The composite rules for midpoint, trapezoidal, and Simpson’s rule, with their error terms, are: 
 •
  Composite Midpoint rule 
  
 Let
  f ⊂ C
 2
 [
 a, b
 ]
 , n
  be even,
  h
  =
 b−a n
 +2
 ,
  and
  x
 j
  =
  a
  + (
 j
  + 1)
 h
  for
  j
  =
  −
 1
 ,
  0
 , ..., n
  + 1
 .
  The composite 
 Midpoint rule for
  n
  + 2 subintervals is 
  b 
  
 n/
 2
  
  
 f
 (
 x
 )
 dx
  = 2
 h 
   
 f
 (
 x
 2
 j
 ) +
 b − a h
 2
 f
 ′′
 (
 ξ
 )  
 (4.2) 
 a 
   
 j
 =0 
   
  
  
 6 
 for some
  ξ ⊂
  (
 a, b
 )
 .
  
 •
  Composite Trapezoidal rule 
  
 Let
  f ⊂ C
 2
 [
 a, b
 ]
 , h
  =
 b−a n
 ,
  and
  x
 j
  =
  a
  +
  jh
  for
  j
  = 0
 ,
  1
 , ..., n.
  The composite Trapezoidal rule for
  n
  
 subintervals is 
  b
  
 a 
  
 f
 (
 x
 )
 dx
  =
 h 
  2
 f
 (
 a
 ) + 2  
  
  
 n−
 1",NA
Python codes for Newton-Cotes formulas ,"We write codes for the trapezoidal and Simpson’s rules, and the composite Simpson’s rule. Coding 
 trapezoidal and Simpson’s rule is straightforward. 
 Trapezoidal rule 
  
 In [1]:
  def
  trap
 (f, a, b):  
  
  
 return
  (f(a)
 +
 f(b))
 *
 (b
 -
 a)
 /2 
  
 Let’s verify the calculations of Example
  72
 :  
 In [2]:
  trap(x
 ->
 x
 ^
 x,
 0.5
 ,
 1
 )  
 Out[2]:
  0.42677669529663687 
 Simpson’s rule 
  
 In [3]:
  def
  simpson
 (f, a, b):  
  
  
 return
  (f(a)
 +4*
 f((a
 +
 b)
 /2
 )
 +
 f(b))
 *
 (b
 -
 a)
 /6 
  
 In [4]:
  simpson(
 lambda
  x: x
 **
 x,
  0.5
 ,
  1
 )  
 Out[4]:
  0.4109013813880978  
 Recall that the degree of accuracy of Simpson’s rule is 3. This means the rule integrates poly-
 nomials 1
 , x, x
 2
 , x
 3
 exactly, but not
  x
 4
 . We can use this as a way to verify our code:  
 In [5]:
  simpson(
 lambda
  x: x,
  0
 ,
  1
 )  
 Out[5]:
  0.5  
 In [6]:
  simpson(
 lambda
  x: x
 **2
 ,
  0
 ,
  1
 )  
 Out[6]:
  0.3333333333333333  
 In [7]:
  simpson(
 lambda
  x: x
 **3
 ,
  0
 ,
  1
 )  
 Out[7]:
  0.25  
 In [8]:
  simpson(
 lambda
  x: x
 **4
 ,
  0
 ,
  1
 )  
 Out[8]:
  0.20833333333333334",NA
Composite rules and roundoff error,"As we increase
  n
  in the composite rules to lower error, the number of function evaluations 
 increases, and a natural question to ask would be whether roundoff error could accumulate and 
 cause problems. Somewhat remarkably, the answer is no. Let’s assume the roundoff error 
 associated with computing 
 f
 (
 x
 ) is bounded for all
  x,
  by some positive constant
  ϵ
 . And let’s try to 
 compute the roundoff error in composite Simpson rule. Since each function evaluation in the 
 composite rule incorporates an error of (at most)
  ϵ
 , the total error is bounded by 
 h
  
 ϵ
  + 2 
 n 
  
 2
 −
 1
  
 ϵ
  + 4 
 n/
 2
  
 ϵ
  +
  ϵh
  
 ϵ
  + 2 
  
 n
   1 
  
 ϵ
  + 4 
 n
  
 ϵ
  +
  ϵ
  
 =
 h 
 (3
 nϵ
 ) =
  hnϵ.
  
 3 
 ϵ
  + 2 
 j
 =1
  
  
 j
 =1
  
  ≤
  
  
 2
 −
  
  
 2  
 3",NA
4.3,NA,NA
Gaussian quadrature,"Newton-Cotes formulas were obtained by integrating interpolating polynomials with equally-
 spaced nodes.  
 The equal spacing 
 is convenient in deriving simple expressions for the composite rules. However, this placement of 
 nodes is not necessarily the optimal placement.  
 For example, the 
 trapezoidal rule approximates the integral by integrating a linear function that joins the endpoints 
 of the function. The fact that this is not the optimal choice can be seen by sketching a simple 
 parabola. 
 The idea of Gaussian quadrature is the following: in the numerical quadrature rule 
  b
  
 f
 (
 x
 )
 dx ≈
  
 n
  
 w
 i
 f
 (
 x
 i
 ) 
 a
  
 i
 =1
  
 choose
  x
 i
  and
  w
 i
  in such a way that the quadrature rule has the highest possible accuracy. Note that 
 unlike Newton-Cotes formulas where we started labeling the nodes with
  x
 0
 , in the Gaussian 
 quadrature the first node is
  x
 1
 . This difference in notation is common in the literature, and each 
 choice makes the subsequent equations in the corresponding theory easier to read. 
 Example 76.
  Let (
 a, b
 ) = (
 −
 1
 ,
  1)
 ,
  and
  n
  = 2
 .
  Find the “best”
  x
 i
  and
  w
 i
 . 
 There are four parameters to determine:
  x
 1
 , x
 2
 , w
 1
 , w
 2
 .
  We need four constraints. Let’s require 
 the rule to integrate the following functions exactly:
  f
 (
 x
 ) = 1
 , f
 (
 x
 ) =
  x, f
 (
 x
 ) =
  x
 2
 , and
  f
 (
 x
 ) =
  x
 3
 .
  
  
  
  
  
  1 
  
 If the rule integrates
  f
 (
 x
 ) = 1 exactly, then 
  
  1
  
  
 −
 1
 dx
  =
 2 
 i
 =1
 w
 i
 , i.e.,
  w
 1
  +
  w
 2
  = 2
 .
  If 
 the rule integrates
  f
 (
 x
 ) =
  x
  exactly, then 
 −
 1
 xdx
  =
 2 
 i
 =1
 w
 i
 x
 i
 , i.e.,
  w
 1
 x
 1
  +
  w
 2
 x
 2
  = 0
 .
  Continuing this 
 for",NA
Python code for Gauss-Legendre rule with five nodes,"The following code computes the Gauss-Legendre rule for 
 and weights are from Table
  4.1
 . 
 In [1]:
  def
  gauss
 (f): 
  1
  
 −
 1
 f
 (
 x
 )
 dx
  using
  n
  = 5 nodes. The nodes 
 return
  0.2369268851*
 f(
 -0.9061798459
 )
  + 0.2369268851*
 f(
 0.9061798459
 )
  +
  \ 
 0.5688888889*
 f(
 0
 )
  + 0.4786286705*
 f(
 0.5384693101
 )
  +
  \  
 0.4786286705*
 f(
 -0.5384693101
 )",NA
4.4 ,NA,NA
Multiple integrals,"The numerical quadrature methods we have discussed can be generalized to higher dimensional 
 integrals. We will consider the two-dimensional integral 
 f
 (
 x, y
 )
 dA. 
  
 R
  
 The domain
  R
  determines the difficulty in generalizing the one-dimensional formulas we learned 
 before. The simplest case would be a rectangular domain
  R
  =
  {
 (
 x, y
 )
 |a ≤ x ≤ b, c ≤ y ≤ d}
 . We can then 
 write the double integral as the iterated integral 
 R
  
 f
 (
 x, y
 )
 dA
  = 
  b
  
  d
  
 f
 (
 x, y
 )
 dy
  
 dx.
  
 a
  
 c
  
 Consider a numerical quadrature rule 
  b
  
 f
 (
 x
 )
 dx ≈
  
 n
  
 w
 i
 f
 (
 x
 i
 )
 .
  
 i
 =1
  
 a
  
 Apply the rule using
  n
 2
  nodes to the inner integral to get the approximation 
  b
  
  
 a
  
 j
 =1 
  
  
  
  
 n
 2 
  
  
     
 w
 j
 f
 (
 x, y
 j
 )
  dx 
  
 where the
  y
 j
 ’s are the nodes. Rewrite, by interchanging the integral and summation, to get 
 n
 2
   
  b 
  
  
 w
 j 
  
 f
 (
 x, y
 j
 )
 dx 
  
 j
 =1 
   
 a
  
 and apply the quadrature rule again, using
  n
 1
  nodes, to get the approximation 
 n
 2
  
 w
 j
  
  n
 1
  
 w
 i
 f
 (
 x
 i
 , y
 j
 ) 
 .
  
 j
 =1
  
 i
 =1
  
 This gives the two-dimensional rule 
  b
  
  d
  
 f
 (
 x, y
 )
 dy
  
 dx ≈
  
 n
 2
  
 n
 1
  
 w
 i
 w
 j
 f
 (
 x
 i
 , y
 j
 )
 .
  
 j
 =1
  
 i
 =1
  
 a
  
 c
  
 For simplicity, we ignored the error term in the above derivation; however, its inclusion is straight-
 forward.",NA
4.5 ,NA,NA
Improper integrals,"The quadrature rules we have learned so far cannot be applied (or applied with a poor 
 performance)
  b 
  
 to integrals such as 
 a
 f
 (
 x
 )
 dx
  if
  a, b
  =
  ±∞
  or if
  a, b
  are finite but
  f
  is not continuous at one or both of the 
 endpoints: recall that both Newton-Cotes and Gauss-Legendre error bound theorems require the 
 integrand to have a number of continuous derivatives on the closed interval [
 a, b
 ]. For 
 example, an integral in the form 
  1 
  
 f
 (
 x
 ) 
 −
 1
 √
 1
  − x
 2
  dx
  
 clearly cannot be approximated using the trapezoidal or Simpson’s rule without any modifications, 
 since both rules require the values of the integrand at the end points which do not exist. One could 
 try using the Gauss-Legendre rule, but the fact that the integrand does not satisfy the smoothness 
 conditions required by the Gauss-Legendre error bound means the error of the approximation 
 might be large. 
 A simple remedy to the problem of improper integrals is to change the variable of integration and 
 transform the integral, if possible, to one that behaves well. 
 Example 82.
  Consider the previous integral 
  1
  
 f
 (
 x
 ) 
  
 dx.
  Try the transformation
  θ
  = cos
 −
 1
  x.
  
  
 −
 1
  
 √
 1
 −x
 2",NA
4.6,NA,NA
Numerical differentiation,".
  
 The derivative of
  f
  at
  x
 0
  is 
 f
 ′
 (
 x
 0
 ) = lim 
  
 h→
 0
  
 f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 ) 
 h
  
 This formula gives an obvious way to estimate the derivative by 
 f
 ′
 (
 x
 0
 )
  ≈f
 (
 x
 0
  +
  h
 )
  − f
 (
 x
 0
 ) 
 for small
  h
 . What this formula lacks, however, is it does not give any information about the error of 
 the approximation.",NA
Numerical differentiation and roundoff error,"Arya and the mysterious black box
  
 College life is full of mysteries, and Arya 
 faces one in an engineering class: a black box! 
 What is a black box? It is a computer program, 
 or some device, which produces an output when 
 an input is provided. We do not know the inner 
 workings of the system, and hence comes the 
 name black box. Let’s think of the black box as a 
 function 
 f
 , and represent the input and output as
  
 x, f
 (
 x
 ). Of course, we do not have a formula for
  f
 . 
  
 What Arya’s engineering classmates want to do is compute the derivative information of the 
 black box, that is,
  f
 ′
 (
 x
 ), when
  x
  = 2
 .
  (The input to this black box can be any real number.) Students 
 want to use the three-point midpoint formula to estimate
  f
 ′
 (2): 
 f
 ′
 (2)
  ≈
 1 2
 h
 [
 f
 (2 +
  h
 )
  − f
 (2
  − h
 )]
  .
  
 They argue how to pick
  h
  in this formula. One of them says they should make
  h
  as small as possible, 
 like 10
 −
 8
 . Arya is skeptical. She mutters to herself, ""I know I slept through some of my numerical 
 analysis lectures, but not all!"" 
  
 She tells her classmates about the cancellation of leading digits phenomenon, and to make her 
 point more convincing, she makes the following experiment: let
  f
 (
 x
 ) =
  e
 x
 , and suppose we want",NA
Chapter 5,NA,NA
Approximation Theory,NA,NA
5.1 ,NA,NA
Discrete least squares,"Arya’s adventures in the physics lab
  
 College life is expensive, and Arya is happy to land a job working at a physics lab for some extra 
 cash. She does some experiments, some data analysis, and a little grading. In one experiment she 
 conducted, where there is one independent variable
  x
 , and one dependent variable
  y
 , she was asked 
 to plot
  y
  against
  x
  values. (There are a total of six data points.) She gets the following plot: 
  
 Figure 5.1: Scatter plot of data
  
 Arya’s professor thinks the relationship between the variables should be linear, but we do not 
 see data falling on a perfect line because of measurement error. The professor is not happy, 
 professors are usually not happy when lab results act up, and asks Arya to come up with a linear 
 formula, 
 153",NA
Python code for least squares approximation,"In [1]:
  import
  numpy
  as
  np 
  
 import
  matplotlib.pyplot
  as
  plt 
  
 %
 matplotlib
  inline 
 The function
  leastsqfit
  takes the
  x
 - and
  y
 -coordinates of the data, and the degree of the polynomial 
 we want to use,
  n
 , as inputs. It solves the matrix Equation (
 5.2
 ). 
 In [2]:
  def
  leastsqfit
 (x, y, n):  
  
 m
  =
  x
 .
 size
  # number of data points 
  
  
 d
  =
  n
 +1
  # number of coefficients to be determined 
  
  
 A
  =
  np
 .
 zeros((d, d))  
  
 b
  =
  np
 .
 zeros(d)  
  
 # the linear system we want to solve is Ax=b 
  
  
 p
  =
  np
 .
 zeros(
 2*
 n
 +1
 )  
  
 for
  k
  in
  range
 (d):  
  
 sum
  = 0 
  
  
 for
  i
  in
  range
 (m):  
  
  
 sum
  +=
  y[i]
 *
 x[i]
 **
 k  
  
 b[k]
  =
  sum 
  
  
 # p[i] below is the sum of the i-th power of the x coordinates 
  
 p[
 0
 ]
  =
  m  
  
 for
  i
  in
  range
 (
 1
 ,
  2*
 n
 +1
 ):  
  
 sum
  = 0 
  
  
 for
  j
  in
  range
 (m):  
  
  
 sum
  +=
  x[j]
 **
 i  
  
 p[i]
  =
  sum 
  
  
 # We next compute the upper triangular part of the coefficient 
  
 # matrix A, 
 and its diagonal 
  
  
 for
  k
  in
  range
 (d):  
  
 for
  j
  in
  range
 (k, d):  
  
  
 A[k, j]
  =
  p[k
 +
 j]  
  
 # The lower triangular part of the matrix is defined using the 
  
 # fact the 
 matrix is symmetric 
  
  
 for
  i
  in
  range
 (
 1
 , d):  
  
 for
  j
  in
  range
 (i):",NA
Least squares with non-polynomials,"The method of least squares is not only for polynomials. For example, suppose we want to find the 
 function 
 f
 (
 t
 ) =
  a
  +
  bt
  +
  c
  sin(2
 πt/
 365) +
  d
  cos(2
 πt/
 365)  
 (5.3) 
 that has the best fit to some data (
 t
 1
 , T
 1
 )
 , ...,
  (
 t
 m
 , T
 m
 ) in the least-squares sense. This function is used 
 in modeling weather temperature data, where
  t
  denotes time, and
  T
  denotes the temperature. The 
 following figure plots the daily maximum temperature during a period of 1,056 days, from 2016 
 until November 21, 2018, as measured by a weather station at Melbourne airport, Australia
 1
 . 
  
 To find the best fit function of the form (
 5.3
 ), we write the least squares error term 
 E
  = 
 m
  
 (
 f
 (
 t
 )
  T
 )
 2
 = 
 m
  
 a
  +
  bt
 +
  c
  sin 
 2
 πt
 i
  
 +
  d
  cos 
 2
 πt
 i
  
  T
  
 2
  
 ,
  
  
 i
 =1
  
 i
  −
 i
  
 i
 =1
  
 i
   
 365  
 365 
 −
 i
  
   
 and set its partial derivatives with respect to the unknowns
  a, b, c, d
  to zero to obtain the normal 
 1
 http://www.bom.gov.au/climate/data/",NA
5.2,NA,NA
Continuous least squares,"In discrete least squares, our starting point was a set of data points. Here we will start with a 
 continuous function
  f
  on [
 a, b
 ] and answer the following question: how can we find the ""best"" 
 polynomial
  P
 n
 (
 x
 ) =
 n j
 =0
 a
 j
 x
 j
  of degree at most
  n
  that approximates
  f
  on [
 a, b
 ]? As before, ""best"" 
 polynomial will mean the polynomial that minimizes the least squares error: 
 E
  = 
  b
  
 f
 (
 x
 )
  −
  
 n
  
 a
 j
 x
 j
  
  
  
 2
  
 dx.
  
 (5.10) 
 j
 =0
  
 a
  
 Compare this expression with that of the
  discrete least squares
 : 
 E
  = 
 i
 =1
 y
 i
  −
 j
 =0 
  
  
 a
 j
 x
 j 
 i
  
  
  
    
 .",NA
5.3 ,NA,NA
Orthogonal polynomials and least squares,"Our discussion in this section will mostly center around the continuous least squares problem; 
 however, the discrete problem can be approached similarly. Consider the set
  C
 0
 [
 a, b
 ], the set of all 
 continuous functions defined on [
 a, b
 ]
 ,
  and
  P
 n
 ,
  the set of all polynomials of degree at most
  n
  on [
 a, 
 b
 ]
 . 
 These two sets are vector spaces, the latter a subspace of the former, under the usual operations 
 of function addition and multiplying by a scalar. An inner product on this space is defined as 
 follows: 
 given
  f, g ⊂ C
 0
 [
 a, b
 ] 
 ⊂f, g⊂
  = 
  b
  
 w
 (
 x
 )
 f
 (
 x
 )
 g
 (
 x
 )
 dx
  
 (5.12) 
 a
  
 and the norm of a vector under this inner product is 
 ⊂f⊂
  =
  ⊂f, f⊂
 1
 /
 2
 = 
  b
  
 w
 (
 x
 )
 f
 2
 (
 x
 )
 dx
  
 1
 /
 2
  
 .
  
 a
  
 Let’s recall the definition of an inner product: it is a real valued function with the following prop-
 erties: 
 1.
  ⊂f, g⊂
  =
  ⊂g, f⊂
  
 2.
  ⊂f, f⊂ ≥
  0
 ,
  with the equality only when
  f ≡
  0 
 3.
  ⊂βf, g⊂
  =
  β ⊂f, g⊂
  for all real numbers
  β
  
 4.
  ⊂f
 1
  +
  f
 2
 , g⊂
  =
  ⊂f
 1
 , g⊂
  +
  ⊂f
 2
 , g⊂
  
 The mysterious function
  w
 (
 x
 ) in (
 5.12
 ) is called a
  weight function
 . Its job is to assign different 
 importance to different regions of the interval [
 a, b
 ]
 .
  The weight function is not arbitrary; it has to 
 satisfy some properties. 
 Definition 87.
  A nonnegative function
  w
 (
 x
 ) on [
 a, b
 ] is called a weight function if 
  
  b 
  
 1.  
 a
 |x|
 n
 w
 (
 x
 )
 dx
  is integrable and finite for all
  n ≥
  0 
  
  b 
  
 2. If 
 a
 w
 (
 x
 )
 g
 (
 x
 )
 dx
  = 0 for some
  g
 (
 x
 )
  ≥
  0
 ,
  then
  g
 (
 x
 ) is identically zero on (
 a, b
 )
 .
  
 With our new terminology and set-up, we can write the least squares problem as follows: 
 Problem
  (Continuous least squares) Given
  f ⊂ C
 0
 [
 a, b
 ]
 ,
  find a polynomial
  P
 n
 (
 x
 )
  ⊂
  P
 n
  that minimizes 
  
 b
  
 a 
  
 w
 (
 x
 )(
 f
 (
 x
 )
  − P
 n
 (
 x
 ))
 2
 dx
  =
  ⊂f
 (
 x
 )
  − P
 n
 (
 x
 )
 , f
 (
 x
 )
  − P
 n
 (
 x
 )
 ⊂ .
  
 We will see this inner product can be calculated easily if
  P
 n
 (
 x
 ) is written as a linear combi-
 nation of orthogonal basis polynomials:
  P
 n
 (
 x
 ) =
 n j
 =0
 a
 j
 φ
 j
 (
 x
 ).",NA
Python code for orthogonal polynomials,"Computing Legendre polynomials
  
 Legendre polynomials satisfy the following recursion: 
 L
 n
 +1
 (
 x
 ) = 2
 n
  + 1 
 n
  + 1
 xL
 n
 (
 x
 )
  −n
  + 1
 L
 n−
 1
 (
 x
 ) 
 n
  
 for
  n
  = 1
 ,
  2
 , . . .
  , with
  L
 0
 (
 x
 ) = 1, and
  L
 1
 (
 x
 ) =
  x
 . 
 The Python code implements this recursion, with a little modification: the index
  n
 +1 is shifted 
 down to
  n
 , so the modified recursion is:
  L
 n
 (
 x
 ) =
 2
 n−
 1 
 n
  
 In [1]:
  import
  numpy
  as
  np 
  
 import
  matplotlib.pyplot
  as
  plt 
 %
 matplotlib
  inline 
 In [2]:
  def
  leg
 (x, n):  
 if
  n
  == 
 0
 :  
 return
  1 
  
 elif
  n
  == 1
 : 
  
 return
  x 
  
 else
 : 
 xL
 n−
 1
 (
 x
 )
  −
 n−
 1 
 n
 L
 n−
 2
 (
 x
 ), for
  n
  = 2
 ,
  3
 , . . .
  . 
 return
  ((
 2*
 n
 -1
 )
 /
 n)
 *
 x
 *
 leg(x,n
 -1
 )
 -
 ((n
 -1
 )
 /
 n)
 *
 leg(x,n
 -2
 ) 
 Here is a plot of the first five Legendre polynomials: 
 In [3]:
  xaxis
  =
  np
 .
 linspace(
 -1
 ,
  1
 ,
  200
 )  
 legzero
  =
  np
 .
 array(
 list
 (
 map
 (
 lambda
  x: leg(x,
 0
 ), xaxis))) legone
  =
  
 leg(xaxis,
  1
 )  
 legtwo
  =
  leg(xaxis,
  2
 )  
 legthree
  =
  leg(xaxis,
  3
 )  
 legfour
  =
  leg(xaxis,
  4
 )  
 plt
 .
 plot(xaxis, legzero, label
 =
 '$L_0(x)$'
 )  
 plt
 .
 plot(xaxis, legone, label
 =
 '$L_1(x)$'
 )  
 plt
 .
 plot(xaxis, legtwo, label
 =
 '$L_2(x)$'
 )  
 plt
 .
 plot(xaxis, legthree, label
 =
 '$L_3(x)$'
 )  
 plt
 .
 plot(xaxis, legfour, label
 =
 '$L_4(x)$'
 )  
 plt
 .
 legend(loc
 =
 'lower right'
 );",NA
References,"[1] Abramowitz, M., and Stegun, I.A., 1965. Handbook of mathematical functions: with formulas, 
 graphs, and mathematical tables (Vol. 55). Courier Corporation. 
 [2] Chace, A.B., and Manning, H.P., 1927. The Rhind Mathematical Papyrus: British Museum 10057 
 and 10058. Vol 1. Mathematical Association of America. 
 [3] Atkinson, K.E., 1989. An Introduction to Numerical Analysis, Second Edition, John Wiley & Sons. 
 [4] Burden, R.L, Faires, D., and Burden, A.M., 2016. Numerical Analysis, 10th Edition, Cengage. [5] 
 Capstick, S., and Keister, B.D., 1996. Multidimensional quadrature algorithms at higher degree 
 and/or dimension. Journal of Computational Physics, 123(2), pp.267-273. 
 [6] Chan, T.F., Golub, G.H., and LeVeque, R.J., 1983. Algorithms for computing the sample variance: 
 Analysis and recommendations. The American Statistician, 37(3), pp.242-247. 
 [7] Cheney, E.W., and Kincaid, D.R., 2012. Numerical mathematics and computing. Cengage 
 Learning. 
 [8] Glasserman, P., 2013. Monte Carlo methods in Financial Engineering. Springer. 
 [9] Goldberg, D., 1991. What every computer scientist should know about floating-point arith-
 metic. ACM Computing Surveys (CSUR), 23(1), pp.5-48. 
 [10] Heath, M.T., 1997. Scientific Computing: An introductory survey. McGraw-Hill. 
 [11] Higham, N.J., 1993. The accuracy of floating point summation. SIAM Journal on Scientific 
  
 Computing, 14(4), pp.783-799. 
 [12] Isaacson, E., and Keller, H.B., 1966. Analysis of Numerical Methods. John Wiley & Sons. 
 187",NA
Index ,"Absolute error,
  30 
  
 Julia code,
  68
  
 Beasley-Springer-Moro,
  142 
  
 Biased exponent,
  22 
  
 Big O notation,
  78 
  
 Bisection method,
  46 
  
  
 error theorem,
  48 
  
  
 Julia code,
  47 
  
  
 linear convergence,
  49 
  
 Black-Scholes-Merton formula,
  55
  
 Chebyshev nodes,
  96 
  
 Chebyshev polynomials,
  175 
  
 Julia code,
  181 
  
 Chopping,
  29 
  
 Composite Newton-Cotes,
  127 
  
 midpoint,
  127 
  
  
 roundoff,
  130 
  
  
 Simpson,
  127
  
 relation to Newton’s method,
  73 
 Floating-point,
  21 
  
 decimal,
  29 
  
 IEEE 64-bit,
  22 
  
 infinity,
  24 
  
 NAN,
  24 
  
 normalized,
  22 
  
 toy model,
  25 
  
 zero,
  24
  
 Gamma function,
  87 
  
 Gaussian quadrature,
  131 
  
 error theorem,
  136 
  
 Julia code,
  135 
  
 Legendre polynomials,
  132 
 Gram-Schmidt process,
  174
  
 Hermite interpolation,
  96
  
 trapezoidal,
  127 
  
 Degree of accuracy,
  124
  
 computation,
  99 
  
 Julia code,
  101
  
  
 Divided differences,
  85 
  
  
 derivative formula,
  
 96
  
 Implied volatility,
  56 
  
 Improper integrals,
  144
  
 Dr. Seuss,
  117 
  
 Intermediate value theorem,
  6
  
 Extreme value theorem,
  6 
 Fixed-point iteration,
  64
 ,
  65 
  
 error theorem,
  68
  
 Interpolation,
  75 
  
 Inverse interpolation,
  92 
  
 Iterative method,
  44 
  
 stopping criteria,
  44
  
 geometric interpretation,
  66 
 high-order,
  72 
  
 high-order error theorem,
  73
  
 Julia  
 abs,
  63",NA
